{
  "title": "Structured information extraction from scientific text with large language models",
  "url": "https://openalex.org/W4391836235",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2546941172",
      "name": "John Dagdelen",
      "affiliations": [
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2096959900",
      "name": "Alexander Dunn",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2124393352",
      "name": "Sang-Hoon Lee",
      "affiliations": [
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A1982222761",
      "name": "Nicholas Walker",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2690833346",
      "name": "Andrew S. Rosen",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1970654099",
      "name": "Gerbrand Ceder",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2133845934",
      "name": "Kristin A. Persson",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2333800602",
      "name": "Anubhav Jain",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2546941172",
      "name": "John Dagdelen",
      "affiliations": [
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2096959900",
      "name": "Alexander Dunn",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2124393352",
      "name": "Sang-Hoon Lee",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1982222761",
      "name": "Nicholas Walker",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2690833346",
      "name": "Andrew S. Rosen",
      "affiliations": [
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A1970654099",
      "name": "Gerbrand Ceder",
      "affiliations": [
        "Lawrence Berkeley National Laboratory",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2133845934",
      "name": "Kristin A. Persson",
      "affiliations": [
        "University of California, Berkeley",
        "Lawrence Berkeley National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2333800602",
      "name": "Anubhav Jain",
      "affiliations": [
        "Lawrence Berkeley National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3026048580",
    "https://openalex.org/W3208687975",
    "https://openalex.org/W4284699179",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W4224442790",
    "https://openalex.org/W4214535912",
    "https://openalex.org/W4206078246",
    "https://openalex.org/W4307139584",
    "https://openalex.org/W4283074682",
    "https://openalex.org/W4281617541",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W4225409008",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W4224909481",
    "https://openalex.org/W3036481679",
    "https://openalex.org/W4281476575",
    "https://openalex.org/W3047398431",
    "https://openalex.org/W2992302948",
    "https://openalex.org/W4225378608",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W3016077617",
    "https://openalex.org/W2949112260",
    "https://openalex.org/W2952179106",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W4307309666",
    "https://openalex.org/W4385567177",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W3090350559",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W4221155590",
    "https://openalex.org/W4285239949",
    "https://openalex.org/W4223491992",
    "https://openalex.org/W3214342214",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W4311130499",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W4386269388",
    "https://openalex.org/W4376644341",
    "https://openalex.org/W4380686968",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4292289324",
    "https://openalex.org/W6841950445",
    "https://openalex.org/W2912512851",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W4224051134",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2973153050",
    "https://openalex.org/W3130479782",
    "https://openalex.org/W3039136197",
    "https://openalex.org/W6779966058",
    "https://openalex.org/W2953641512",
    "https://openalex.org/W6982728064",
    "https://openalex.org/W6911396176",
    "https://openalex.org/W6892453975",
    "https://openalex.org/W6977254979"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-024-45563-x\nStructured information extraction from\nscientiﬁc text with large language models\nJohn Dagdelen 1,2,3, Alexander Dunn 1,2,3, Sanghoon Lee1,2, Nicholas Walker1,\nAndrew S. Rosen 1,2, Gerbrand Ceder1,2, Kristin A. Persson1,2 &\nAnubhav Jain 1\nExtracting structured knowledge from scientiﬁc text remains a challenging\ntask for machine learning models. Here, we present a simple approach to joint\nnamed entity recognition and relation extraction and demonstrate how pre-\ntrained large language models (GPT-3, Llama-2) can beﬁne-tuned to extract\nuseful records of complex scientiﬁc knowledge. We test three representative\ntasks in materials chemistry: linking dopants and host materials, cataloging\nmetal-organic frameworks, and general composition/phase/morphology/\napplication informationextraction. Records are extracted from single sen-\ntences or entire paragraphs, and the output can be returned as simple English\nsentences or a more structured formatsuch as a list of JSON objects. This\napproach represents a simple, accessible, and highlyﬂexible route to obtain-\ning large databases of structured specialized scientiﬁc knowledge extracted\nfrom research papers.\nThe majority of scientiﬁc knowledge about solid-state materials is\nscattered across the text, tables, andﬁgures of millions of academic\nresearch papers. Thus, it is difﬁcult for researchers to properly\nunderstand the full body of past work and effectively leverage existing\nknowledge when designing experiments. Moreover, machine learning\nmodels for direct property prediction are being increasingly employed\nas screening steps for materials discovery and design workﬂows\n1–3,b u t\nthis approach is limited by the amount of training data available in\ntabulated databases. While databases of materials property data\nderived from ab initio simulations are relatively common, they are\nlimited to the subset of computationally accessible properties whereas\ndatabases of experimental property measurements and other useful\nexperimental data are comparatively small (if they exist at all).\nIn recent years, researchers have made signiﬁcant advances in the\napplication of natural language processing (NLP) algorithms for\nmaterials towards structuring the existing body of textual materials\nscience knowledge\n4–7. The majority of this work has focused on named\nentity recognition (NER), where entity labels such as“material\" or\n“property\" are applied to words from the text. These tagged sequences\nof words can sometimes be used with additional post-processing to\nconstruct auto-generated tabular databases of materials property data\naggregated from text entries8–12. Prior information extraction studies\nin the domain of solid-state materials include NER labeling of chemical\nsynthesis parameters in methods section texts\n13–16, quantitative results\nof battery cycling experiments17, or peak absorption wavelengths for\nUV-Vis experiments18,a m o n go t h e r s4,5,9–12,19. Regular expressions,\nBiLSTM recurrent neural networks, and smaller transformer-based\nlanguage models such as BERT are sufﬁcient for such tasks. In these\nstudies, entities (e.g., LiCoO\n2, \"350K\") rather than relations (e.g.,\n\"350K\" is an experimental synthesis parameter for LiCoO2)a r et h e\nprimary target of extraction.\nYet, a key challenge in scientiﬁc natural language processing is the\ndevelopment of robust, simple, and general relation extraction (RE)\ntechniques to accurately extract the relationships between named\nentities. Downstream tasks such as supervised machine learning or the\nconstruction of knowledge graphs require the transformation of\nunstructured text into sets of st ructured relationships between\nsemantic entities of interest. RE models are used to determine which\nentities are linked by a predeﬁned set of relations. For example, in the\nsentence “LiCoO2 is studied as a Li-ion battery material\", the material\nentity“LiCoO2\" is linked to the application entity“Li-ion battery\". Until\nrecently, there has been relatively little work on relation extraction in\nReceived: 17 March 2023\nAccepted: 22 January 2024\nCheck for updates\n1Lawrence Berkeley National Laboratory, Berkeley, CA, USA.2Materials Science and Engineering Department, University of California, Berkeley, CA, USA.\n3These authors contributed equally: John Dagdelen, Alexander Dunn.e-mail: ajain@lbl.gov\nNature Communications|         (2024) 15:1418 1\n1234567890():,;\n1234567890():,;\nmaterials science text, but there has been much research interest in RE\non general-purpose text, especially related to linking people, organi-\nzations, locations, and dates\n20,21. These methods have traditionally\nrelied on pipeline-based approaches where named entity recognition\nis theﬁrst step followed by one or more additional steps and aﬁnal\nrelation classiﬁcation step (see Fig.1, top row). Each of these steps\ntypically uses a separate machine learning model, which may or may\nnot share weights or architectures with each other. State-of-the-art\ntransformer-based implementations of pipeline implementations have\nbeen shown to perform document level relation extraction relatively\nwell on a variety of general-knowledge corpora\n22 and more specialized\ndomains such as chemical-disease relations 23 and gene-disease\nrelations24. Recently, this kind of two-step approach was demon-\nstrated on a benchmark dataset of procedures for the synthesis of\npolycrystalline materials encoded as directed graphs extracted from\nmaterials science text\n25.\nHowever, scientiﬁc information often cannot be modeled as\nsimple pairwise relations between entities. This is particularly apparent\nin inorganic materials science, where a compound’s properties are\ndetermined by a complex combination of its elemental composition,\natomic geometry, microstructure, morphology (e.g., nanoparticles,\nheterostructures, and interfaces), processing history, and environ-\nmental factors such as temperature and pressure. Furthermore, inor-\nganic materials knowledge is often inherently intertwined such that\nthe relations may only be valid between one entity type and a com-\npound entity (itself comprised of several entities and relationships).\nFor example, we may consider zinc oxide nanoparticles (a compostion\n“ZnO\" linked to the morphology“nanoparticles\") to be a catalyst, but\n“ZnO\" and “nanoparticles\" alone are not necessarily catalysts in\nthemselves. When parts of these compound relations are lost, scien-\ntiﬁc meaning will change. A sample of an“epitaxial La-doped thinﬁlm\"\nof HfZrO\n4 will have different physical properties than a“La-doped thin\nﬁlm\" of HfZrO4 and a “La-doped\" sample of HfZrO4. In theory, rela-\ntionships betweenn entities can be modeled asn-tuples (e.g., (\"ZnO\",\n“nanoparticles\", “catalyst\")), but comprehensively enumerating all\npossible variations is both impractical and not amenable to conven-\ntional relation extraction methods, since a sufﬁcient number of train-\ning examples is required for each relation type. For example, a model\nextracting 10 distinct entity classes may have\n10C3 = 120 3-tuple entity\nrelation types, each requiring at least several annotation examples.\nCurrent relation extraction models are not designed to practically\nextract or preserve such kinds of highly complex, intricately related,\nand hierarchical relationships between arbitrary numbers of named\nentities; a moreﬂexible strategy is required.\nLarge language models (LLMs) such as GPT-3/4\n26,27,P a L M28,\nMegatron29, LLaMA 1/230,31,O P T32,G o p h e r33, and FLAN34 have been shown\nto have remarkable ability to leverage semantic information between\ntokens in natural language sequences of varying length. They are parti-\ncularly adept at sequence-to-sequence (seq2seq) tasks, where text input\nis used to seed a text response from the model. In this paper, we will refer\nto these inputs as“prompts\" and the outputs as“completions.\" Use cases\nfor seq2seq are broad\n35 and include machine translation36,a n s w e r i n g\ng e n e r a lf a c t u a lk n o w l e d g eq u e s t i o n s33,37, performing simple arithmetic33,\ntranslating between languages36,38, summarizing text28,39, and chatbot\napplications26,40. It stands to reason that these models may also be adept\nat complex scientiﬁc information extraction.\nRecently, end-to-end methods that use a single machine learning\nmodel have been investigated for joint named entity recognition and\nFig. 1 | Schematic comparison of previous relation extraction (RE) methods to\nthis work.The objective of each method is to extract entities (colored text) and\ntheir relationships from unstructured text.a An example multi-step pipeline\napproach ﬁrst performs entity recognition, then intermediate processing such as\ncoreference resolution, andﬁnally classiﬁcation of links between entities.b seq2seq\napproaches encode relationships as 2-tuples in the output sequence. Named enti-\nties and relationship links are tagged with special symbols (e.g.,“@FORMULA@\",\n“@N2F@\"). c The method shown in this work outputs entities and their relationships\nas JSON documents or other hierarchical structures.\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 2\nrelation extraction (NERRE)41–43. These methods take a sequence-to-\nsequence approach where a model is trained to output tuples of two or\nmore named entities and the relation label belonging to the predeﬁned\nset of possible relations between them (Fig.1,m i d d l er o w ) .T h e s e\nmethods perform well on relation extraction, but they fundamentally\nremain n-ary relation extraction systems that are not suited to highly\nintricate and hierarchical NERRE.\nIn the domain of materials science, Huang & Cole recentlyﬁne-\ntuned a BERT model on battery publications and trained a model to\nenhance a database of NLP-extracted battery data\n11. Their approach\nemployed a “question and answer\" (Q/A) approach that extracted\nlimited device-level information (e.g.,“What is the cathode?\",“What is\nthe anode?\",“What is the electrolyte?\") in tandem with conventional\ninformation extraction methods11.W en o t et h a tt h i sa p p r o a c hc a n n o t\nbe used on passages that contain information about more than one\ndevice, and it required the BERT language model to be trained on\nhundreds of thousands of battery research papers before beingﬁne-\ntuned on the Q/A task. More recently, Zheng et al.44 designed a\nprompt-engineering approach (ChemPrompt w/ ChatGPT 45)f o r\nextracting data from scientiﬁc papers. This method is focused on\nstructuring text into tabular forms, creating semi-structured summa-\nries, and collating existing knowledge from the pretraining corpus.\nSimilarly, Castro Nascimento and Pimentel\n46 examined ChatGPT’s\ngeneral knowledge of chemistry; however, theyﬁnd that, as opposed\nto methods using considerable prompt engineering47,C h a t G P Tw i t h -\nout prompting “tricks\" performs poorly on several simple tasks in\nchemistry. Xie et al.’s48 approach utilizes LLMsﬁne-tuned on a large,\nbroad materials science corpus for a range of Q/A, inverse design,\nclassiﬁcation, and regression tasks. While these methods\n44,46–48\ndemonstrate LLMs might act as materials science knowledge engines,\nthey have not been shown to extract structured representations of\ncomplex hierarchical entity relationships generalizing outside of the\npretraining corpus.\nIn this work, we investigate a simple approach to complex infor-\nmation extraction where a large language model isﬁne-tuned to\nsimultaneously extract named entities and their relationships. This\nmethod is able toﬂexibly handle complex inter-relations (including\ncases where information exists as lists of multiple items) without\nrequiring enumeration of all of possiblen-tuple relations or pre-\nliminary NER. Our approach differs from the supervised learning (e.g.,\nregression and classiﬁcation for chemistry) and inverse design\napproaches of Jablonka et al.\n49,50 and Xie et al.48; rather than using LLMs\nto directly inﬂuence design or predict properties, we aim to (accu-\nrately) extract structured hierarchies of information for use with\ndownstream models. Weﬁne-tune a pretrained large language model\n(e.g., GPT-3\n26 or Llama-231) to accept a text passage (for example, a\nresearch paper abstract) and write a precisely formatted“summary\" of\nknowledge contained in the prompt. This completion can be for-\nmatted as either English sentences or a more structured schema such\nas a list of JSON documents. To use this method, one only has to deﬁne\nthe desired output structure— for example, a list of JSON objects with a\npredeﬁned set of keys— and annotate ~100–500 text passages using\nthis format. The LLM is thenﬁne-tuned on these examples, and the\nresulting model is able to accurately output extracted information in\nthe same structured representation, such as the format shown in Fig.1.\nIn essence, a domain expert can show an LLM-NERRE model both what\nit should extract and how that information should be represented, and\nthen the model learns how to perform the task independently.\nThis method shows strong performance using both OpenAI’sG P T - 3\n(closed source) and Llama-2 (open access) on both sentence-level and\ndocument-level materials information extraction. Moreover, the\nmethod can leverage online LLM APIs, which allows users to train\nbespoke models without extensive knowledge of how LLMs work\ninternally; the LLM may be simply treated by the user as a black-box that\ntransforms passages into precisely-formatted, structured summaries of\nscientiﬁc text. Therefore, researchers may use this method with little\nNLP experience. We also discuss how intermediate models can be used\nto pre-suggest entities for annotation, vastly increasing the speed and\nease of annotating documents so that large training sets can be con-\nstructed relatively quickly. Although the example tasks shown are from\nmaterials science, the generality and accessibility of the method implies\nit may be readily applied to other domains such as chemistry, health\nsciences, or biology. In particular, this approach does not appear to\nrequireﬁne-tuning on a large corpus of domain-speciﬁcd a t a( e . g . ,m i l -\nlions of article abstracts or paragraphs) as in previous methods; rather,\nthe comprehensive pretraining of the LLMs along with the user-provided\nannotations are sufﬁcient to accomplish a broad array of complex tasks.\nResults\nWe use the described approach on three joint named entity recogni-\ntion and relation extraction (NERRE) materials information extraction\ntasks: solid-state impurity doping, metal–organic frameworks (MOFs),\nand general materials information extraction. Details for each dataset\nare summarized in Table1. Further details of each task are presented in\nthe Methods section. Brieﬂy, the solid-state impurity task is to identify\nhost materials, dopants, and potentially additional related information\nfrom text passages (sentences). The MOF task is to identify chemical\nformulae, applications, guest species, and further descriptions of MOF\nmaterials from text (materials science abstracts). The general materials\ninformation task is to identify inorganic materials, their formulae,\nacronyms, applications, phase labels, and other descriptive informa-\ntion from text (materials science abstracts). The general and MOF\nmodels were trained on data including normalization and error cor-\nrection, while doping models were trained to extract data exactly as it\nappears in text. Each base LLM model isﬁne-tuned per-task to adhere\nto a particular schema that encapsulates the entities of interest, rele-\nvant relationships, and format. All schemas are shown in Table1 and\nfurther details are available in the Methods and Supplementary Note 1.\nRelation extraction performance\nA comparison between GPT-3 and Llama-2 on NERRE precision, recall,\nand F\n1 scores across the three tasks using a JSON schema is shown in\nTable2. Details on each of the task’s JSON schemas are explained in the\nMethods section. The performances are calculated with an exact word-\nTable 1 | Overview of approaches tested on the three materials information extraction tasks\nTask Schema Training samples Task level Completion format\nDoping Doping-JSON 413 sentences Sentence JSON\nDoping Doping-English 413 sentences Sentence English sentences\nDoping DopingExtra-English 413 sentences Sentence English sentences\nMOFs MOF-JSON 507 abstracts Abstract JSON\nGeneral Materials General-JSON 634 abstracts Abstract JSON\nAll three tasks are tested with a JSON schema, and we additionally test the doping task with alternate schemas resembling written English. The MOF and general materials models are trained and\nevaluated on abstracts, while doping tasks are evaluated on sentences.\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 3\nmatch basis, a lower-bound metric described in more detail in the\nMethods section. Because this joint task involves both named-entity\nrecognition and relation extraction, it reﬂects both the NER and RE\nperformance of the models (a relation cannot be correctly identiﬁed if\nthe entities are not correct.) GPT-3 achieves the highestF\n1 scores for\nthe General and MOF tasks across all the entity relationships we tested.\nExact matchF1 scores for these two extraction tasks are generally ~30%\nlower than in the host-dopant task. The highestF1 for the general task is\nfound for relationships between formulae and applications (F1 =0 . 5 3 7 )\nwhile formula-acronym and formula-description relationships are\nmuch less reliable. A similarﬁnding occurs for the MOF task, where the\nname-application (F\n1 = 0.573) and name-guest species (F1 = 0.616)\nrelationships are extracted most accurately. The Llama-2 NERRE scores\nare on average 20− 30% lower than their GPT-3 counterparts, indi-\ncating a signiﬁcant advantage for GPT-3. In the dopant task, Llama-2\nhas the highest precision (0.836), recall (0.807), and F\n1 (0.821),\nrepresenting an improvement of 13% over GPT-3 wrt.F1.\nThe F1 scores for the general and MOF tasks in Table2 are gen-\nerally 0.3–0.6, which is, onﬁrst inspection, seemingly too low to be\nuseful for a large scale information extraction task. However, the\nscores for the MOF and general tasks are subject to an important\ncaveat. These tasks’annotations include implicit normalization (e.g.\n“Lithium ion\"→ “Li-ion\") and error correction (\"silcion\"→ “silicon\"),\nwhile the doping task aims to extract hosts and dopants exactly as they\nappear in text. Thus, the exact word-match basis scores shown above\nare an approximate lower bound on information extraction perfor-\nmance, since this metric compares only exact matches between words.\nWhen outputs of the general and MOF models are read by human\nexperts, it becomes obvious that the models are often extracting true\ninformation with slight changes in phrasing or notation. There is also\nan effect on performance from inherent ambiguity in real-world\ninformation extraction tasks. For example, in MOF information\nextraction, MOF names (e.g.,“ZIF-8\") are qualitatively easier to delimit\nthan descriptions (e.g.,“mesostructured MOFs formed by Cu2+ and 5-\nhydroxy-1,3-benzenedicarboxylic acid\"), which can be written with\nmany different wordings.\nTo account for these factors, we manually scored outputs against\nthe original human (true) annotations for a random 10% test set of the\ngeneral materials information extraction dataset. We calculated\n“manual scores\" by marking extractions as correct if the core infor-\nmation from entities is extracted in the correct JSON object (i.e.,\ngrouped with the correct material formula) and incorrect if they are in\nthe wrong JSON object, are not extracted at all, or are not plausibly\ninferred from the original abstract. In contrast to the exact match\nscores (Table2), manual scores allow forﬂexibility with respect to\nthree aspects: (1) entity normalization, (2) error correction, and (3)\nmultiple plausible annotations of an entity under different labels (e.g.,\n“thermoplastic elastomer\" may be considered either an application or\ndescription). Whereas Table2 assesses whether the model can extract\npairs of words exactly as they appear in the true annotation, the\nmanual scores shown in Table3 assess if the model extracts equivalent\ninformation to that of the true annotation - regardless of the exact\nform. Simply, if a domain expert would agree the model’s extraction\nand the true extraction are equivalent, the model’s extraction is\nmarked as correct. We provide precise details on this procedure in the\nMethods section and detailed examples with explanations in Supple-\nmentary Discussion 4.\nTable 3 shows the adjusted scores based on manual scoring. We\nstratify these scores by entity; the“name\", “acronym\", “application\",\n“structure\", and “description\" manual scores can be compared to\nTable 2’s exact-matchformula-{name, application, structure,\ndescription} relation scores. For example,“description\" reﬂects\nhow often the model extracts a description entity which is both\nequivalent in meaning to that of the true annotation (according to a\ndomain expert) and is grouped in the correct JSON object (linked to\nthe correct formula). We see that exact-match scoring severely under-\npredicts performance for materials’names (0.456 vs 0.818), applica-\ntions (0.537 vs 0.832), structures/phases (0.482 vs 0.829), and\ndescriptions (0.354 vs 0.704). Manual scoring reveals that our models\nare actually able to correctly extract structured knowledge from sci-\nentiﬁc text on a wide variety of materials science topics, and readers\ncan inspect the model’s output on test set examples (included in the\nSupplementary Discussion 4) for themselves. We observe that\nTable 2 | Named entity recognition and relation extraction scores for three tasks in materials science using models with a JSON\noutput schema\nTask Relation E.M. Precision\n(GPT-3)\nE.M. Recall\n(GPT-3)\nE.M. F1\n(GPT-3)\nE.M. Precision\n(Llama-2)\nE.M. Recall\n(Llama-2)\nE.M. F1 (Llama-2)\nDoping host-dopant 0.772 0.684 0.726 0.836 0.807 0.821a\nGeneral formula-name 0.507 0.429 0.456 0.462 0.417 0.367\nGeneral formula-acronym 0.500 0.250 0.333 0.333 0.250 0.286\nGeneral formula-struc-\nture/phase\n0.538 0.439 0.482 0.551 0.432 0.47\nGeneral formula-application 0.542 0.543 0.537 0.545 0.496 0.516\nGeneral formula-description 0.362 0.35 0.354 0.347 0.342 0.340\nMOFs name-formula 0.425 0.688 0.483 0.460 0.454 0.276\nMOFs name-guest specie 0.789 0.576 0.616 0.497 0.407 0.408\nMOFs name-application 0.657 0.518 0.573 0.507 0.562 0.531\nMOFs name-description 0.493 0.475 0.404 0.432 0.411 0.389\nExact match (E.M.) scores are evaluated on a per-word basis, and links are only correct if both entities and the relationship are correct. The exact match metric scores output that contains the correct\ninformation but is written differently as incorrect, making such scores a rough lower bound on the true performance of models.F1, precision, and recall reﬂect the scores on a hold out test set for\ndoping models and averages overﬁve cross-validation sets for the general and MOF models.\naBest F1 scores for each task are shown in bold.\nTable 3 | Manual scores for the general materials task using\nGPT-3 with General-JSON schema\nEntity Extraction recall Extraction\nprecision\nExtractionF1\nformula 0.943 0.943 0.943\nname 0.692 1.0 0.818\nacronym 0.667 0.400 0.500\napplications 0.797 0.870 0.832\nstructure or phase 0.754 0.920 0.829\ndescription 0.576 0.905 0.704\nScores measure the model’s ability to extract inter-related data together (i.e. assigning entities\ncorrect labels and grouping them appropriately).\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 4\nacronyms have the lowest information extraction scores, which we\nattribute to the fact that acronyms are relatively rare in the training\nclass compared to the others (appearing in only 52 abstracts across the\nentire dataset, ~9% of the documents) and that the model can confuse\nacronyms with chemical formulae (e.g.,“AuNP\" is the acronym for gold\nnanoparticle but is also a valid chemical formula). Usually, context\nclues are the only way to disambiguate cases like this, and we expect\nincluding more training data with acronyms may improve the acronym\nextraction score.\nOverall, these scores indicate the model is highly capable at\nextracting meaningfully complex representations of material knowl-\nedge from passages. Precision scores for the various categories (other\nthan acronyms) are all roughly 0.87 or better, which indicates that\nwhen information is extracted, it contains true relational information\nfrom the passage rather than spurious connections.\nThe advantage of the LLM-NERRE method reﬂected in these\nmanual scores is the ability to automatically correct errors and nor-\nmalize common entity patterns. While the doping models were trained\nto extract snippets of text exactly as they appeared in the text prompt,\nthe General-JSON model’s training data included simple normal-\nizations and error corrections of entities. For example, the erroneous\ninclusion of white spaces in chemical formulae is common in the raw\nabstract text. We observe that including corrected formulae instead of\nthe raw string in the output training sequences results in LLMs that\nautomatically resolve extracted entities to cleaner forms. For example,\n“Li Co O2\" is corrected to“LiCoO2\" by the model without additional\npost-processing. Similarly, because there are suf ﬁcient training\nexamples, the models usingGeneral-JSONschema resolve text such\nas “PdO functionalized with platinum\" to a normalized form such as\n{formula: “PdO\", description: [\"Pt-functionalized\"]}. The\nbuilt-in normalization and correction abilities of LLM models may\nprove useful for domain specialists who desire structured entity for-\nmats rather than exact string excerpts pulled directly from the text, as\nentity normalization is a common post-processing task.\nEffect of different schemas\nFor the host-dopant extraction task, we evaluated three different\noutput schemas to determine whether one format of output is exclu-\nsively better than any other. The models using the Doping-English\nschema output English sentences with a particular structure (e.g.,“the\nhost ’<host entity>’ was doped with ’<dopant entity>’.\") and the\nDopingExtra-English models likewise output English sentences but\nalso includes some additional information (e.g., if one of the hosts is a\nsolid solution and/or the concentration of a particular dopant). For the\nDoping-JSON schema, we used a JSON object schema with keys“hosts\",\n“dopants\", and“hosts2dopants\" (which in turn has a key-value object\nas its corresponding value). For readers familiar with the Python pro-\ngramming language, these are identical to python dictionary objects\nwith strings as keys and strings or other dictionaries as values. We\ninclude a baseline comparison to seq2rel\n41, a comparable sequence-to-\nsequence method, trained on the same doping dataset. We also com-\npare to MatBERT-Doping\n5, an NER model trained on ~450 abstracts,\ncombined with a simple heuristic for determining host-dopant rela-\ntionships; that is, all hosts and dopants within the same sentence\n(sample) are related. We refer to this model as MatBERT-Proximity. Full\ndescriptions and examples of all schemas are available in the Methods\nsection, and further details on seq2rel and MatBERT-Proximity are\navailable in Supplementary Notes 4–5. Because the general materials\ninformation extraction and MOF information extraction tasks are far\nmore complex, we did not attempt to train models to output English\nsentences (as opposed to JSON formatted strings), as the resulting\nsequences would be difﬁcult to parse into structured database entries.\nWe ﬁnd that all three of our LLM-NERRE host-dopant extraction\nmodels perform signi ﬁcantly better than either the MatBERT-\nProximity or seq2rel baseline models. Of the two baselines, the\nseq2rel model achieves higher precision (0.420) and recall (0.605)\nresulting inF\n1 = 0.496, which is slightly higher than MatBERT-Proximity\n(0.390) but substantially lower than any of the LLM-NERRE models.\nThis seq2rel benchmark model is derived from the PubMedBERT\n51\npretrained BERT model as per the original implementation41,a n di t\nmay be possible to improve the seq2rel method by using a BERT model\npretrained exclusively on materials text rather than biomedical text.\nHowever, this improvement is not expected to be dramatic because\nprevious comparisons between SciBERT and MatBERT show relatively\nminor differences in materials NER tasks\n5. We also observe that all\nthree LLM-NERRE models exceed the performance of the two baselines\nin pure NER performance (see Supplementary Discussion 2) despite\nbeing trained on less text than the MatBERT-NER model (413 sentences\nvs. 455 abstracts.) Of the six LLM-based models, the Llama-2 model\nwith Doping-JSON schema performs the best (F\n1 =0 . 8 2 1 ) w i t h G P T - 3 /\nDopingExtra-English ( F1 = 0.809) and Llama-2/Doping-English\n(F1 = 0.814) both within a 2% margin. We summarize both LLMs' per-\nformances with all three schemas alongside the baseline models in\nTable 4.\nWithin the GPT-3 results, the DopingExtra-English and Doping-\nE n g l i s hs c h e m a sh a v et h eh i g h e s tF\n1. In particular, GPT-3/DopingExtra-\nEnglish tops the GPT-3 models despite being trained on the same\nnumber of samples as the Doping-English and Doping-JSON models.\nThis is notable because GPT-3/DopingExtra-English is both more\naccurate and more capable (i.e., this model extracts“results\" and\n“modiﬁers\" entities in addition to host-dopant relationships) than the\nGPT-3 models using other schemas. The opposite observation is true\nof the Llama-2 models, where the JSON format outperforms both\nEnglish schemas and the DopingExtra-English schema suffers from low\nprecision (0.694). Roughly, the GPT-3 models tend to perform opti-\nmally when using natural language like schemas, while Llama-2 per-\nforms optimally using JSON.\nHuman-in-the-loop annotation\nAs a separate experiment, we evaluated the use of partially trained\nLLMs in a“human-in-the-loop\" annotation process for constructing\noutputs with the GPT-3/General-JSON, as seen in Fig.2. In each trial of\nthe experiment, the human annotator received 10 abstracts and\n10 schemas that were pre-populated by an intermediate version of the\nmodel which was trained onn samples of training data (n = 1, 10, 50,\n100, 300). Instead of completing annotations from scratch, the human\nannotator corrected these intermediate models’suggestions, and the\ntime to complete each annotation was recorded. As shown in Fig.3,t h e\nannotation time sharply decreases as the number of training samples\nused in the intermediate models increases; then =3 0 0 i n t e r m e d i a t e\nmodel was able to reduce the averageannotation time per abstract by\n57% in comparison with then = 1 model, indicating that the model was\ncompleting many sections of the annotation correctly.\nAt low numbers of training samples, the models’predictions are\nnot valid JSON objects, and the annotator had to redo annotations\nfrom scratch. At higher numbers of training samples, particularly those\nabove 50, the intermediate model predictions required very little error\ncorrection from the annotator. As a lower bound, we also report the\ntime needed by the annotator to simply verify whether an entry was\nentirely correct (veriﬁcation time) which reﬂe c t st h ea n n o t a t i o nr a t eo f\na human annotator using a perfect model, which only requires the\nhuman annotator to check the outputs. Weﬁnd that by three metrics\n(time per abstract, time per material entry, and time per prompt\ntoken), the human annotator annotated substantially faster with a well-\ntrained model in the loop (n samples > 50) than with a poorly trained\nmodel (n samples < 50) or no model. For example, then = 300 model\nreduced the annotation time per token ~60% compared to then =1\nmodel and is only 38% slower than the veriﬁcation time. Given addi-\ntional training samples for intermediate models, we expect the anno-\ntation to asymptotically approach the veriﬁcation time. Thus, this\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 5\nmethod may serve as a useful tool for building even larger benchmark\ndatasets for information extraction tasks.\nWe note that the LLM-NERRE method requires the model to learn\nboth the correct structure of the output data as well as the information\nto populate into that data structure, particularly when asking the\nmodel to output English sentences that can later be parsed to a\nstructured format. To determine the minimum number of training\nexamples required for models with sentence-format outputs that have\na parseable sentence structure, we trained intermediate models on\nvarying training set sizes for the GPT-3/Doping-English model. Preci-\nsion, recall, andF\n1 scores as a function of training set size are plotted in\nFig. 4. We observe that output sequences are not properly structured\nfor training set sizes below ~10 samples, but there is a sharp increase in\nthe number of correctly structured outputs at ~20 samples, which\nseems to be the minimum number of examples GPT-3 needs to learn a\ndesired output format when using simple sentence-type schemas.\nDiscussion\nOverall, we ﬁnd excellent performance on three diverse tasks for\nmaterials science and engineering: solid-state impurity doping,\nmetal–organic frameworks, and general materials relations. The non-\ntechnical nature of this approach implies scientists without NLP\ntraining can utilize existing models such as GPT-3 to extract large\nstructured relational datasets for highly-speciﬁc problems. As the LLM\nis treated essentially as a black-box, we anticipate this approach may\nbe used for LLMs other than GPT-3 or Llama-2, including LLMs released\nin the near future. We hope this approach enables domain specialists\nto rapidly extract relational datasets for the advancement of scientiﬁc\nknowledge.\nThe NERRE scores in Tables2–4 provide a quantitative score for\nperformance, but some of the best features of this method are not\ndirectly shown byF\n1 scores. The primary advantage of this method is\nits accessibility and ease-of-use, as LLM-NERRE requires only\nFig. 3 | Annotation time as a function of intermediate large language model\n(LLM) ﬁne-tuning samples for the named entity recognition and relation\nextraction (NERRE) method.We show the time taken for a domain expert to\nannotate new abstracts for the general materials chemistry task with assistance\nfrom intermediate (partially-trained) LLM-NERRE models on a (a) word basis, (b)\nmaterial entry basis, and (c) token basis. Outputs from models trained on more data\ncontain fewer mistakes and require less time to correct. Source data are provided as\na Source Dataﬁle.\nFig. 2 | Overview of the proposed sequence-to-sequence approach to\ndocument-level joint named entity recognition and relationship\nextraction task.In theﬁrst step, lists of JSON documents are prepared from\nabstracts according to a predeﬁned schema, and the large language model (LLM) is\ntrained. In the second step, this preliminary (intermediate) model is used to\naccelerate the preparation of additional training data by pre-annotation with the\npartially trained model and manual correction. An example error is shown high-\nlighted in red. This step may be repeated multiple times with each subsequent\npartialﬁne-tuning improving in performance. In theﬁnal step, the LLM isﬁne-tuned\non the complete dataset and used for inference to extract desired information from\nnew text.\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 6\nspecifying a basic schema, annotating a minimal number of examples,\nand ﬁne-tuning a model via a publicly available API without extensive\nNLP knowledge or hyperparameter tuning; theﬁnal result is a useful\nmodel with the ability to extract specialized technical information that\nis nuanced and semantically complex. Additionally, error correction\nand normalization may be embedded directly into training examples\nto reduce the need for post-processing. In essence, one can show an\nLLM-NERRE model both what it should extract and how it can be\ncondensed and presented.\nLike many others, we have found using a human-in-the-loop\nprocess can help decrease the time required to collect a full training\nset\n52. Our particular process is shown in Fig.2. Annotation for scientiﬁc\ninformation extraction tasks is often a tedious and error-prone process\nwhereas checking intermediate model outputs for errors is qualita-\ntively easier than creating annotations from scratch. Additionally,ﬁne\ntuning GPT-3/Llama-2 requires fewer training examples to match or\nexceed the performance of BERT-based models. Figure4 shows how\nperformance of theﬁne-tuned models improves quickly at relatively\nsmall training set sizes. However, more and more text-completion\npairs are required to achieve the same rate of improvement as training\nset size is increased.\nOne limitation of our model is that valid output schema format-\nting is not rigorously enforced in the generation step. The LLM may,\nfor any given sample, output an unparsable sequence. This is parti-\ncularly apparent when the inference token limit is less than 512 tokens\nand the schema is JSON, as JSON schema typically requires a larger\nnumber of tokens for correct formatting. For example, a nearly-correct\noutput sequence containing 10+ correct entities may be missing a“}\"\nending character and therefore will not be parsable. Outputs are nearly\nalways parsable (~99% success rate), especially as the number of\ntraining examples increases. Failures predominantly occur when the\nsample exceeds the prompt-completion token limit of the LLM (early\ntermination), which in this work was 512-1024 tokens for both GPT-3\nand Llama-2. Because of this, some abstracts that are too long or too\ndense with information to be processed with this method. This was the\ncase in the few unparseable completions where the passage and partial\ncompletion exceed the token limit and cut off early before the full\ncompletion could be output by the model. This limitation may be\nmitigated by increasing the token limit up to 2048 (GPT-3) or 4096\n(Llama-2); we expect the token limitation will become less of a concern\nas the maximum token size of such models increase.\nAnother limitation is the tendency of LLMs to generate or invent\ninformation that is not actually present in the input text, a phenom-\nenon termed“hallucination\"\n53,54 in LLM literature. The main manifes-\ntation of hallucination we observed was the addition of names or\nchemical formulae for a material when only one or the other was\nmentioned (for example writing“SiO2\" in the formula ﬁeld even\nthough the paragraph only mentions“silica\"). Although these halluci-\nnations could potentially be correct, because the source text does not\ninclude them, we believe they should not be included in the output of\ninformation extraction models. We could enforce this by the require-\nment that all extracted entities should occur word-for-word in the\nsource text, but the fact that these models do not extract phrases\nexactly can also be a useful feature because it allows for automatic\nentity normalization. For example, an abstract may mention both“p-\nZnSe doped with N\" and“nitrogen-doped ZnSe\" in the same passage. Is\n“doped with N\" or“nitrogen-doped\" the correct description to extract?\nClearly, both are correct and either one could be reasonably chosen.\nMoreover, “N-doped\" could also be extracted and would be factually\ncorrect even though“N-doped\" never occurs in the passage. Because\nLLMs can learn implicit normalization rules, if the annotator is con-\nsistent in how they normalize cases like this (such as always using“X-\ndoped\" and/or “p(n)-type\"), the model generally follows the same\nnormalization scheme and it can greatly reduce the amount of entity\nnormalization post-processing required later. We differentiate this\nfrom hallucination in that the inference is fully justiﬁed by the content\nin the source text rather than simply plausible.\nFinally, the choice of LLM poses a practical tradeoff for\nresearchers: essentially, ease of use vs. control. Using a proprietary\nLLM such as GPT-3 through an online API enables the LLM in our\nmethod to be treated as a“black box\", and abstracting away LLMﬁne-\ntuning details allows researchers to focus entirely on their domain-\nspeciﬁc information extraction tasks. However, the underlying LLM is\nexclusively controlled by a private entity, posing problems of\nTable 4 | Comparison of large language models with different\njoint named entity recognition and relation extraction\n(NERRE) schemas to baseline models on host-dopant\nextraction task\nModel Schema Precision\n(exact match)\nRecall\n(exact\nmatch)\nF1\n(exact\nmatch)\nMatBERT-\nProximity\nn/a 0.377 0.403 0.390\nSeq2rel n/a 0.420 0.605 0.496\nGPT-3 Doping-JSON 0.772 0.684 0.725\nGPT-3 Doping-English 0.803 0.754 0.778\nGPT-3 DopingExtra-\nEnglish\n0.820 0.798 0.809\nLlama-2 Doping-JSON 0.836a 0.807 0.821\nLlama-2 Doping-English 0.787 0.842 0.814\nLlama-2 DopingExtra-\nEnglish\n0.694 0.815 0.750\nNERRE exact match scores are evaluated on a per-word basis, and links are only correct if both\nentities and relationship are correct. DopingExtra-English scores here refer to only host-dopant\nrelation prediction. We note that exact match scores output that contains the correct information\nbut is written differently as incorrect, making such scores an approximate lower bound on the\ntrue performance of models.F\n1, precision, and recall are computed on a hold-out test set from 77\nsentences. Best scores for precision, recall, andF1 are shown in bold.\naBest scores among all models in each category (exact match precision, recall,F1) are shown\nin bold.\nFig. 4 | Test set performance vs. number of training samples for the doping\nextraction task using GPT-3 with the Doping-English schema.This schema\nspeciﬁcally requires the model to learn a new and speciﬁc sentence structure to use\nas the output. We separate scores by (a) host-dopant links (relations), (b)h o s t\nentities alone, and (c) dopant entities alone. We note that below approximately\n10 samples, the scores are zero because the model has not learned the speciﬁc\nstructure of the desired output sentences. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 7\nreproducibility and security. Regarding security, potentially sensitive\nor conﬁdential data must be sent to the entity for processing;\nregarding reproducibility, the models cannot be shared, and the entity\ncontrolling the LLM may at any time change the model, amend theﬁne-\ntuning method, or revoke access to the model altogether. More, the\ncost for inference on large datasets using trained models may be\nprohibitive. In contrast, using self-hosted models such as Llama-2\n31 or\nGPT-NeoX 20B55 favors control over ease of use. The weights and code\nfor the model are fully accessible, and inference cost is restricted only\nby the user’s budget on a cluster with capable GPUs. However, suc-\ncessfully running,ﬁne-tuning, and deploying LLMs such as Llama-2 on\ncluster infrastructure is non-trivial for many scientists. Cloud-hosted\nopen-access models (e.g., Llama-2 hosted on a managed cloud\ninstance) may provide a solution to the ease of use vs. control tradeoff,\nas the technical details ofﬁne-tuning are abstracted away from the user\nbut theﬁne-tuned models themselves can remain open-access. Simi-\nlarly, zero-shot approaches withoutﬁne-tuning may make scientiﬁc\ninformation extraction more accessible at the expense of accuracy (see\nSupplementary Discussion 6). Methods for reducing the number of\nparameters needed for LLM inference andﬁne-tuning\n56–59 are also a\npromising avenue for reducing the complexity and cost of self-hosting\nLLMs. As these methods advance and LLM codebases become more\nmature, we expectﬁne-tunable models compatible with LLM-NERRE\nwill become simultaneously powerful, easy to self-host, reproducible,\nand under researchers’full control. We hope the code examples of\nboth ﬁne-tuning and running inference using the published model\nweights we provide in Methods are aﬁrst step in the direction of\npowerful and open source NERRE models.\nIn summary, this work demonstrates that LLMs that areﬁne-tuned\non a few hundred training examples are capable of extracting scientiﬁc\ninformation from unstructured text and formatting the information in\nuser-deﬁned schemas. This is in contrast to past models which were\nsuccessful in extracting entities from text but struggled to relate those\nentities or structure them in meaningful ways. The proposed method is\nsimple and broadly accessible given the APIs and interfaces currently\navailable such as GPT-3. Furthermore, we have made the Llama-2 LoRA\nweights of all models shown in this paper available for download (see\nMethods and Code Availability), allowing researchers to investigate\nthe LLM-NERRE method on their own hardware. We expect these\nadvancements to greatly facilitate the rate and accuracy by which\nhistorical scientiﬁc text can be converted to structured forms.\nMethods\nGeneral sequence-to-sequence NERRE\nWe ﬁne-tune Llama-2 and GPT-3 models to perform NERRE tasks using\n400−650 manually annotated text-extraction (prompt-completion)\npairs. Extractions contain the desired information formatted with a\npredeﬁned, consistent schema across all training examples. These\nschemas can range in complexity from English sentences with pre-\ndeﬁned sentence structures to lists of JSON objects or nested JSON\nobjects. In principle, many other potential schemas (e.g., YAML,\npsuedocode) may also be valid, though we do not explore those here.\nOnceﬁne-tuned on sufﬁcient data adhering to the schema, a model will\nbe capable of performing the same information extraction task on new\ntext data with high accuracy. The model outputs completions in the\nsame schema as the training examples. We refer to this approach\ngenerally as“LLM-NERRE\".\nOur general workﬂow for training GPT-3 and Llama-2 to perform\nNERRE tasks is outlined in Fig.2. Annotations are performed by human\ndomain experts to create an initial training set, and then a partially\ntrained model (GPT-3) is used to accelerate the collection of additional\ntraining examples. Fine-tuning is then performed on these examples to\nproduce a“partially trained\" model, which is used to pre-ﬁll annota-\ntions that are subsequently corrected by the human annotator before\nbeing added to the training set. Once a suf ﬁcient number of\nannotations have been completed, theﬁnal ﬁne-tuned model is cap-\nable of extracting information in the desired format without human\ncorrection. Optionally, as illustrated in Figs.5 and 6, the structured\noutputs may be further decoded and post-processed into hierarchical\nknowledge graphs.\nTask and schema design\nSolid-state impurity doping schema. The Doping-English and\nDoping-JSON schemas aim to extract two entity types (host and\ndopant) and the relations between them (host-dopant), returned as\neither English sentences or a list of one or more JSON objects. Hosts are\ndeﬁned as the host crystal, sample, or material class along with crucial\ndescriptors in its immediate context (e.g., “ZnO2 nanoparticles\",\n“LiNbO3\", “half-Heuslers\"). Dopants are taken to be any elements or\nions that are minority species, intentionally added impurities, or spe-\nciﬁc point defects or charge carriers (\"hole-doped\",“Sv a c a n c i e s \" ) .O n e\nhost may be doped with more than one dopant (e.g., separate single-\ndoping or co-doping), or the same dopant may be linked to more than\none host material. There may also be many independent pairs of\ndopant-host relations, often within a single sentence, or many unre-\nlated dopants and hosts (no relations). We impose no restriction on the\nnumber or structure of the dopant-host relations beyond that each\nrelation connects a host to a dopant. The Doping-JSON schema\nrepresents the graph of relationships between hosts and dopants\nwithin a single sentence, where unique keys identify dopant and host\nstrings. The model aims to learn this relatively loose schema during\nﬁne-tuning. A separate key,“hosts2dopants\", describes the pairwise\nrelations according to those unique keys. The Doping-English schema\nencodes the entity relationships as quasi-natural language summaries.\nThe Doping-English schema represents the same information as the\nDoping-JSON schema, but more closely mimics the natural language\npre-training distribution of the LLMs we tested. When there are mul-\ntiple items to extract from the same sentence, the output sentences are\nseparated by newlines.\nFor the DopingExtra-English schema, we introduce two additional\nentities: modiﬁers and result, without explicit linking (i.e., NER\nonly). The results entity represents formulae with algebra in the\nstoichiometric coefﬁcients such as Al\nxGa1−xAs, which are used for\nexperiments with samples from a range of compositions or crystalline\nsolid solutions (e.g., CaCu\n3−xCoxTi4O12). We also include stoichiome-\ntries where the algebra is substituted (i.e.,x value speciﬁed) and the\ndoped result is a speciﬁc composition (e.g., CaCu2.99Co0.1Ti4O12).\nModiﬁers are loosely bounded entity encapsulating other descriptors\nof the dopant-host relationship not captured bydopant, host,o r\nresult. These can be things like polarities (e.g.,“n-type\", “n-SnSe\"),\ndopant quantities (e.g.,“5a t . % \" ,“x < 0.3\"), defect types (e.g.,“sub-\nstitutional\",“antisite\", “vacancy\") and other modiﬁers of thehost to\ndopant relationship (e.g., “high-doping\", “degenerately doped\").\nThese entities (host, dopant, result,a n dmodiﬁers)w e r ec h o s e nt o\ndeﬁne a minimal effective schema for extracting basic doping\ninformation.\nAll doping-related models are trained to work only on single\nsentences. The main motivation for this design choice is that the vast\nmajority of dopant-related data can be found within single sentences,\nand the remaining relational data is often difﬁcult to resolve con-\nsistently for both human annotators and models. We expand on pro-\nblems with annotations and ambiguity in Supplementary Discussion 5\nand we further explain the doping task schemas in Supplemen-\ntary Note 1.\nGeneral materials information schema. In our previous work\n4,5,w e\nfocused on NER for a speciﬁc set of entity types that are particularly\nrelevant in materials science: materials, applications, structure/phase\nlabels, synthesis methods, etc. However, we did not link these labeled\nentities together to record their relations beyond a simple“bag-of-\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 8\nentities\" approach. In this work, we train an LLM to perform a“general\nmaterials information extraction\" task that captures both entities and\nthe complex network of interactions between them.\nThe schema we have designed for this task encapsulates an\nimportant subset of information about solid compounds and their\napplications. Each entry in the list, a self-contained JSON document,\ncorresponds one-to-one with a material mentioned in the text. Mate-\nrials entries are ordered by appearance in the text. The root of each\nentry starts with a compound’s name and/or its chemical formula. If a\nname or formula is not mentioned for a material, no information about\nthat material is extracted from the text. We also extract acronyms\nmentioned for a material’s name/formula, although in cases where\nonly an acronym is mentioned we do not create a material entry for the\ncompound. Compounds that are not solids (ions, liquids, solvents,\nsolutions, etc) are generally not extracted. Thename, formula,a n d\nacronymﬁelds are exclusively given string value in the JSON document\nFig. 6 | Diagrams of general information extraction and metal organic frame-\nwork (MOF) information extraction using large language models (LLMs) for\njoint named entity and relation extraction (NERRE).In both panels, an LLM\ntrained using a particular schema (desired output structure, far left) is prompted\nwith raw text and produces a structured completion as JSON. This completion can\nthen be parsed to construct relational diagrams (far right). Each task uses a dif-\nferent schema representing the desired output text structure from the LLM.\na Schema and labeling example for the general materials-chemistry extraction task.\nMaterials science research paper abstracts are passed to an LLM using General-\nJSON schema, which outputs a list of JSON objects representing individual material\nentries ordered by appearance in the text. Each material may have a name, formula,\nacronym, descriptors, applications, and/or crystal structure/phase information.\nb Schema and labeling example for the metal-organic frameworks extraction task.\nSimilar to the General-JSON model, the MOF-JSON model takes in full abstracts\nfrom materials science research papers and outputs a list of JSON objects. In the\nexample, only MOF name and application were present in the passage, and both\nMOFs (LaBTB and ZrPDA) are linked to both applications (luminescent and VOC\nsensor).\nFig. 5 | Diagrams of doping information extraction using large language\nmodels (LLMs) for joint named entity and relation extraction (NERRE).In all\nthree panels, an LLM trained to output a particular schema (far left) reads a raw text\nprompt and outputs a structured completion in that schema. The structured\ncompletion can then be parsed, decoded, and formatted to construct relational\ndiagrams (far right). We show an example for each schema (desired output struc-\nture). Parsing refers to the reading of the structured output, while decoding refers\nto the programmatic (rule-based) conversion of that output into JSON form. Nor-\nmalization and postprocessing are programmatic steps which transform raw\nstrings (e.g.,“Co+2\") into structured entities with attributes (e.g., Element: Co,\nOxidation state +2).a Raw sentences are passed to the model with Doping-English\nschema, which outputs newline-separated structured sentences that contain one\nhostand one or moredopantentities.b Raw sentences are passed to a model with\nDoping-JSON schema, which outputs a nested JSON object. Eachhost entity has its\nown key-value pair, as does eachdopantentity. There is also a list ofhost2dopant\nrelations that links the corresponding dopant keys to each host key.c Example for\nthe extraction with a model using the DopingExtra-English schema. Thisﬁrst part of\nthe schema is the same as ina, but additional information is contained indoping\nmodiﬁers,a n dresults-bearing sentences are included at the end of the schema.\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 9\nfor each material whereas thedescription, structure_or_phase,\nand applicationsﬁelds are lists of an arbitrary number of strings.\nWe label this modelGeneral-JSON, and an example is shown in\nFig. 6 (a).\nDescriptionentities are deﬁned as details about a compound’s\nprocessing history, defects, modiﬁcations, or the sample’sm o r p h o l -\nogy. For example, consider the hypothetical text“Pt supported on\nCeO2 nanoparticles infused with Nb...\". In this case, thedescription\nvalue for the material object referring to“Pt\" might be annotated as\n\"['supported on CeO2']\",a n dt h edescriptionentities listed for\n“CeO2\" would be\"['nanoparticles', 'Nb-doped']\".\nStructure_or_phaseentities are deﬁned as information that\ndirectly implies the crystalline structure or symmetry of the com-\npound. Crystal systems such as “cubic\" or “tetragonal\", structure\nnames such as“rutile\" or“NASICON\", and space groups such as“Fd3m\"\nor “space group No. 93\" are all extracted in thisﬁeld. We also include\nany information about crystal unit cells, such as lattice parameters and\nthe angles between lattice vectors.“Amorphous\" is also a valid struc-\nture/phase label.\nApplications are deﬁned as high-level use cases or major\nproperty classes for the material. For example, a battery cathode\nmaterial may have \"['Li-ion battery', 'cathode']\" as its\napplications entry. Generally, applications are mentioned in the order\nthey are presented in the text, except for certain cases such as battery\nmaterials, in which case the type of device is generally mentioned\nbefore the electrode type, and catalysts, where the reaction catalyzed\nis generally listed following the “catalyst\" entity in the list (e.-\ng.,\"['catalyst', 'hydrogenation of citral']\").\nMore details about the general materials information task schema\nare provided in the Supplementary Discussion 4.\nMetal– organic framework (MOF) schema.T h es c h e m au s e df o rt h e\nMOF cataloging task is based on the general materials information\nschema described in the previous section, which was modiﬁed to\nbetter suit the needs of MOF researchers. We developed this schema to\nextract MOF names (name), an entity for which there is no widely\naccepted standard\n60, and chemical formulae (formula), which form\nthe root of the document. If no name or formula is present, no infor-\nmation is extracted for that instance. In addition, because there is a\ngreat deal of interest in using MOFs for ion and gas separation61,62,w e\nextract guest species, which are chemical species that have been\nincorporated, stored, or adsorbed in the MOF. We extractapplica-\ntions the MOF is being studied for as a list of strings (e.g.,\"['gas-\nseparation']\" or \"['heterogeneous catalyst', 'Diels-\nAlder reactions']\") as well as a relevantdescription for the\nMOF, such as its morphology or processing history, similar to the\ngeneral information extraction schema. Entries in the list are generally\nadded in the order the material names/formulae appear in the text. The\nMOF extraction model is labeledMOF-JSON, and an example is shown\nin Fig.6 (b).\nComparison baselines and evaluation\nTo compare our model with other sequence-to-sequence approaches\nto information extraction, we perform a benchmark of two methods\non the doping task to compare to the LLM-NERRE models. Theﬁrst\nemploys the seq2rel method of Giorgi et al.\n41 for the host-dopant task.\nWe formatted host-dopant relationships under tags labeled@DOPANT@\nand @BASEMAT@ (base/host material), with their relationship signiﬁed\nby @DBR@ (\"dopant-base material relationship\"); these sequences were\nconstructed from the same training data as theDoping-JSON and\nDoping-English models. We trained seq2rel to perform sentence-\nlevel extraction with 30 epochs, batch size of 4, encoder learning rate\n2×1 0\n−5, decoder learning rate 5 × 10−4, and pretrained BiomedNLP\nBERT tokenizer51 (further training details can be found in the Supple-\nmentary Note 4). Additionally, we compare against the previously\npublished MatBERT doping-NER model5 combined with proximity-\nbased heuristics for linking (see Supplementary Note 5). With this\nmethod, a MatBERT NER model pretrained on ~50 million materials\nscience paragraphs andﬁne-tuned on 455 separate manually anno-\ntated abstractsﬁrst extracts hosts and dopants and then links them if\nthey co-occur in the same sentence.\nDatasets\nDatasets were prepared from our database of more than 8 million\nresearch paper abstracts\n63. Annotations were performed by human\nannotators using a graphical user interface built using Jupyter64,\nalthough in principle annotations could be conducted via a simple text\neditor. To accelerate the collection of training data, new annotations\nare collected via a“human in the loop\" approach where models are\ntrained on small datasets and their outputs are used as starting points\nand corrected by human annotators (see Fig.2.) This process of\ntraining and annotation is completed multiple times until a sufﬁciently\nlarge set of training data was achieved. Each dataset was annotated by\na single domain expert annotator. Class support for each annotated\ndataset is provided in Supplementary Tables 1-3.\nDoping dataset. Training and evaluation data was gathered from our\ndatabase of research paper abstracts using the keywords“n-type\", “p-\ntype\", “-dop\", “-codop\", “doped\", “doping\", and“dopant\" (with exclu-\nsions for common irrelevant keywords such as“-dopamine\"), resulting\nin ~375k total abstracts. All doping tasks were trained on text from 162\nrandomly selected abstracts, comprising 1215 total sentences andﬁl-\ntered with regular expressions to only include 413 relevant (potentially\nincluding doping information) sentences. Doping tasks were tested on\nan additional 232 sentences (77 relevant by regex) from a separate\nholdout test set of 31 abstracts.\nGeneral materials dataset. Training and evaluation data was gathered\nfrom our abstract database by usingkeywords for a variety of materials\nproperties and applications (e.g.,“magnetic\", “laser\", “space group\",\n“ceramic\",“fuel cell\",“electrolytic\", etc). For each keyword a materials\nscience domain expert annotated ~10–50 abstracts, which resulted in\n≈650 entries manually annotated according to the general materials\ninformation schema. Results were evaluated using a 10% random sam-\nple for validation, and this procedure was averaged overﬁve trials using\ndifferent random train/validation splits with no hyperparameter tuning.\nMetal– organic framework dataset. Training and evaluation data was\nselected from our database using the keywords “MOF\", “MOFs\",\n“metal-organic framework\",“metal organic framework\",“ZIF\", “ZIFs\",\n“porous coordination polymer\", and “framework material\", which\nproduced approximately 6,000 results likely containing MOF-related\ninformation. From these, 507 abstracts were randomly selected and\nannotated by a MOF domain expert. Results were evaluated using the\nsame repeated random split procedure as the general materials data-\nset in the previous section.\nGPT-3 ﬁne tuning details\nFor all tasks, weﬁne-tune GPT-3 (‘davinci’, 175B parameters)26 using the\nOpenAI API, which optimizes the cross-entropy loss on predicted\ntokens. Doping models were trained for 7 epochs at a batch size of 1,\nwith inference temperature of 0 and output limited to a maximum\nlength of 512 tokens (all doping models) or 1024 tokens (General-JSON,\nMOF-JSON). The intermediate models shown in Fig.4 were trained with\na number of epochs depending on the number of training samplest:2\nepochs for 2\n0≤t <2 6,4e p o c h sf o r26 < t ≤ 27,a n d7e p o c h sf o rt ≥ 28.\nModels for the MOF and general materials extraction tasks were\ntrained for 4 epochs with a batch size of 1. We use a learning rate\nmultiplier of 0.1 and a prompt loss weight of 0.01 but have not per-\nformed hyperparameter tuning for these hyperparameters. For all\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 10\ntasks, the start and end tokens used were\"\\n\\n\\n###\\n\\n\\n\"and\n\"\\n\\n\\nEND\\n\\n\\n\".\nLlama-2 ﬁne-tuning details\nLlama-231 ﬁne-tunes were performed using a modiﬁed version of the\nMeta Research Llama-2 recipes repository; the modiﬁed repository can\nbe found athttps://github.com/lbnlp/nerre-llama. Llama-2ﬁne-tunes\nwere performed using the 70 billion parameter version of Llama-2\n(llama-2-70b-hf) with quantization (8 bit precision). The number of\nepochs was set to 7 for doping tasks and 4 for the MOF/general tasks.\nLlama-2 ﬁne-tunes used parameter efﬁcient ﬁne-tuning (PEFT) using\nlow rank adaptation (LoRA)\n58 with LoRAr =8 ,α =3 2a n dL o R Ad r o p o u t\nof 0.05. Further hyperparameter tuning was not performed. Decoding\nwas done without sampling using greedy decoding to be consistent\nwith GPT-3 decoding setting of temperature = 0, with max tokens = 512\nfor doping task and 1024 for general and MOF task. More details on the\nﬁne tuning and inference parameters are available in the modiﬁed\nrepository and Supplementary Note 3. Allﬁne-tuning and inference\nwas performed on a single A100 (Ampere) tensor core GPU with\n80GB VRAM.\nThe ﬁne-tuned weights for each model are provided in the NERRE-\nLlama repository (url above) along with code and instructions for\ndownloading the weights, instantiating the models, and running\ninference.\nEvaluation criteria\nThe fuzzy and complex nature of the entities and relationships detailed\nin the previous section necessitates the use of several metrics for\nscoring. We evaluate the performance of all models on two levels:\n1. A relationF\n1 computed on a stringent exact word-match basis (i.e.,\nhow many words are correctly linked together exactly as they\nappear in the source text prompt).\n2. A holistic information extractionF\n1 based on manual inspection\nby a domain expert, which doesn’tr e q u i r ew o r d st om a t c h\nexactly.\nWe separately provide a sequence-level error analysis in Supple-\nmentary Note 7 and Supplementary Discussion 1.\nNERRE performance. We measure NERRE performance as the ability\nof the model to jointly recognize entities and the relationships\nbetween them.\nExact word-match basis scoring\nWe score named entity relationships on a word-basis byﬁrst\nconverting an entityE into a set of constituentk whitespace-separated\nwordsE ={ w\n1, w2, w3, … , wk}. When comparing two entitiesEtrue and Etest\nthat do not contain chemical formulae, we count the number of exactly\nmatching words in both sets as true positives (Etrue ∩ Etest)a n dt h e\nmathematical set differences between the sets as false positives\n(Etest − Etrue) or false negatives (Etrue − Etest). For example, if the true entity\nis “Bi2Te3 thinﬁlm\" and the predicted entity is“Bi2Te3 ﬁlm sample\",\nwe record two true positive word exact matches (\"Bi2Te3\",“ﬁlm\"), one\nfalse negative (\"thin\"), and one false positive (\"sample\"). Formula-type\nentities are crucial for identifying materials, so in cases where entities\ncontain chemical formulae, E\ntest must contain all wi that can be\nrecognized as stoichiometries for any ofwi ∈ Etest to be considered\ncorrect. For example, if the true entity is“Bi2Te3 thinﬁlm\", and the\npredicted entity is“thin ﬁlm\", we record three false negatives. Thus,\nany formula-type entity (Doping host,D o p i n gdopant,G e n e r a l\nformula,a n dM O Fmof_formula) containing a chemical composition\nis entirely incorrect if the composition is not an exact match. This\nchoice of evaluation was made to avoid metrics measuring the per-\nformance of the model in a misleading way. For example,“Bi2Te3\nnanoparticles\" and“Bi2Se3 nanoparticles\" have very high similarities\nvia Jaro-Winkler (0.977) and character-level BLEU-4 (0.858), but these\ntwo phrases mean entirely different things— the material’sc h e m i s t r yi s\nwrong. Under our scoring system, they are recorded as entirely\nincorrect because the compositions do not match.\nWe score relationships between entities on a word-by-word basis\nto determine the number of correct relation triplets. Triplets are\n3-tuples relating wordw\nn\nj of an entityEn to wordwm\nk of an entityEm by\nrelationshipr, represented asðwn\nj ,wm\nk ,rÞ. The total set of correct rela-\ntionships Ttrue for a text contains many of these triplets. A test set of\nrelationships Ttest is evaluated by computing the number of triplets\nfound in both sets (Ttrue ∩ Ttest) as true positives and the differences\nbetween these sets as false positives (Ttest − Ttrue)o rf a l s en e g a t i v e s\n(Ttrue − Ttest). Entity triplets are also bound to the same requirement for\ncomposition correctness if either of the words in the triplet belong to\nan formula-type entity (host, dopant, formula, mof_formula), i.e.,\nwe count all triplets for two entities as incorrect if the formula is not an\nexact string match. With correct and incorrect triplets identiﬁed, F\n1\nscores for each relation are calculated as:\nprecision =No. of correct relations retrieved\nNo. of relations retrieved\nð1Þ\nrecall =No. of correct relations retrieved\nNo. of relations in test set\nð2Þ\nF1 = 2ð precision/C1 recallÞ\nprecision + recall ð3Þ\nTo compute triplet scores across entire test sets in practice, we\nﬁrst select a subset of relations to evaluate. We note that this is not a\nfull evaluation of the task we are training the model to perform, which\ninvolves linking many interrelated entities simultaneously, but is rather\nprovided to help give a general sense of its performance compared to\nother NERRE methods. For the doping task, we evaluatehost-dopant\nrelationships. For the general materials and MOF tasks, we evaluate\nrelationships between the formulaﬁeld (formula for general materi-\nals, mof_formula f o rM O F s )a n da l lo t h e rr e m a i n i n gﬁelds. For\ndescription, structure_or_phase,a n dapplicationsﬁelds, all\nof which may contain multiple values, all of the possible formula-value\npairs are evaluated.\nManual evaluation\nThe metrics provided in prior sections demonstrate automatic\nand relatively strict methods for scoring NERRE tasks, but the under-\nlying capabilities of the LLM models are best shown with manual\nevaluation. This is most apparent in the case of theGeneral-JSON\nmodel, where exact boundaries on entities are fuzzier, precise deﬁni-\ntions are difﬁcult to universally deﬁne, and annotations include some\nimplicit entity normalization. For example, the text“Pd ions were\nintercalated into mesoporous silica\" may have equivalently have a\ncorrect description ﬁeld for the material “silica\" including “Pd-\nintercalated\", “Pd ion-intercalated\",“intercalated with Pd ions\", etc.;\nthe exact choice of which particular string is used as the“correct\"\nanswer is arbitrary.\nTo better address scoring of these fuzzy tasks, we introduce an\nadjusted score based on a domain expert’s manual evaluation of\nwhether the information extracted is a valid representation of the\ninformation actually contained in the passage. We term this adjusted\nscore “manual score\"; it constitutes a basis for precision, recall, andF\n1\nthat quantiﬁes the quality of overall information capture for cases\nwhere there may be equivalent or multiple ways of representing the\nsame concept. This score was constructed to better estimate the per-\nformance of our model for practical materials information\nextraction tasks.\nWe score entities extracted by annotators but not present in the\nmodel’s output as false negatives, except when reasonable variations\nare present. The criteria for a true positive are as follows:\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 11\n1. The entity comes from the original passage or is a reasonable\nvariation of the entity in the passage (e.g.,“silicon\"⟶“Si\"). It is\nnot invented by the model.\n2. The entity is a root entity or is grouped with a valid root entity. For\nthe General-JSON model, a root entity is either a material’s\nformula or name. If both are present, the formula is used at\nthe root.\n3. The entity is in the correctﬁeld in the correct root entity’sg r o u p\n(JSON object).\nManual scores are reported per-entity as if they were NER scores.\nHowever, the requirements for a true positive implicitly include rela-\ntional information, since an entity is only correct if is grouped with the\ncorrect root entity.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nAll data used for this study are available at https://github.com/\nLBNLP/NERREand via Zenodo65, which contains the annotated data-\nsets and test and train splits. Intermediateﬁles for each step of the\npipeline reported in this method are stored in this repository with\ncorresponding documentation. Data for running Llama-2 models are\navailable in the supplementary repositoryhttps://github.com/lbnlp/\nnerre-llama\n66; LoRA weights for all Llama-2 models reported in this\npaper can be downloaded directly from Figshare (https://doi.org/10.\n6084/m9.ﬁgshare.24501331.v1)67. Source data are provided with\nthis paper.\nCode availability\nT h ec o d eu s e df o rt h i ss t u d yi sa v a i l a b l ea thttps://github.com/LBNLP/\nNERREand via Zenodo65 alongside the data. This code includes Jupyter\nnotebooks for annotation as well as Python scripts for annotation,\npreprocessing, model training, and model evaluation on the train and\ntest sets presented in this publication. The supplementary repository\nhttps://github.com/lbnlp/nerre-llama\n66 contains code and data for\nﬁne-tuning and inference with Llama-2 models trained in this study,\nincluding access to the complete sets of weights via a script.\nReferences\n1. Saal, J. E., Oliynyk, A. O. & Meredig, B. Machine learning in materials\ndiscovery: conﬁrmed predictions and theirunderlying approaches.\nA n n u .R e v .M a t e r .R e s .50,4 9–69 (2020).\n2. Choudhary, K. et al. Recent advances and applications of deep\nlearning methods in materials science.npj Comput. Mater.8,\n59 (2022).\n3 . O l i v e i r a ,O .N .&O l i v e i r a ,M .C .F .M a t e r i a l sd i s c o v e r yw i t hm a c h i n e\nlearning and knowledge discovery.Front. Chem.10,\n930369 (2022).\n4. Weston, L. et al. Named entity recognition and normalization\napplied to large-scale information extraction from the materials\nscience literature.J. Chem. Inform. Modeling59, 3692–3702\n(2019).\n5. Trewartha, A. et al. Quantifying the advantage of domain-speciﬁc\npre-training on named entity recognition tasks in materials science.\nPatterns 3, 100488 (2022).\n6. Isazawa, T. & Cole, J. M. Single model for organic and inorganic\nchemical named entity recognition in ChemDataExtractor.J. Chem.\nInform. Modeling62,1 2 0 7–1213 (2022).\n7. Zhao, X., Greenberg, J., An, Y. & Hu, X. T. Fine-tuning BERT model for\nmaterials named entity recognition. In:2021 IEEE International\nConference on Big Data (Big Data)(IEEE, 2021).https://doi.org/10.\n1109/bigdata52589.2021.9671697.\n8. Sierepeklis, O. & Cole, J. M. A thermoelectric materials database\nauto-generated from the scientiﬁc literature using chemdataex-\ntractor.Sci. data9, 648 (2022).\n9. Beard, E. J. & Cole, J. M. Perovskite- and dye-sensitized solar-cell\ndevice databases auto-generated using chemdataextractor.Sci.\nData 9, 329 (2022).\n10. Kumar, P., Kabra, S. & Cole, J. M. Auto-generating databases of yield\nstrength and grain size using chemdataextractor.Sci. Data9,\n292 (2022).\n11. Huang, S. & Cole, J. M. BatteryBERT: A pretrained language model\nfor battery database enhancement.J. Chem. Inform. Modeling62,\n6365–6377 (2022).\n12. Dong, Q. & Cole, J. M. Auto-generated database of semiconductor\nband gaps using chemdataextractor.Sci. Data9, 193 (2022).\n13. Kononova, O. et al. Text-mined dataset of inorganic materials\nsynthesis recipes.Sci. Data6,2 0 3( 2 0 1 9 ) .\n14. Huo, H. et al. Machine-learning rationalization and prediction of\nsolid-state synthesis conditions.Chem. Mater.34,\n7323–\n7336 (2022).\n15. He, T. et al. Similarity of precursors in solid-state synthesis as text-\nmined from scientiﬁc literature.Chem. of Mater.32,\n7861–7873 (2020).\n16. Wang, Z. et al. Dataset of solution-based inorganic materials\nsynthesis procedures extracted from the scientiﬁcl i t e r a t u r e .Sci.\nData 9, 231 (2022).\n17. Huang, S. & Cole, J. M. A database of battery materials auto-\ngenerated using ChemDataExtractor.Sci. Data7,2 6 0( 2 0 2 0 ) .\n18. Beard, E. J., Sivaraman, G., Vázquez-Mayagoitia, Á., Vishwanath, V. &\nCole, J. M. Comparative dataset of experimental and computational\nattributes of UV/vis absorption spectra.Sci. Data6,3 0 7\n(2019).\n19. Zhao, J. & Cole, J. M. A database of refractive indices and dielectric\nconstants auto-generated using ChemDataExtractor.Sci. Data9,\n192 (2022).\n20. Bekoulis, G., Deleu, J., Demeeste r ,T .&D e v e l d e r ,C .J o i n te n t i t y\nrecognition and relation extraction as a multi-head selection pro-\nblem. Expert Syst. Appl.114,3 4–45 (2018).\n21. Han, X. et al. More data, more relations, more context and more\nopenness: a review and outlook for relation extraction. In:Pro-\nceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the\nAssociation for Computational Linguistics and the 10th International\nJoint Conference on Natural Language Processing,7 4 5–758.\n(Association for Computational Linguistics, 2020).https://\naclanthology.org/2020.aacl-main.75.\n22. Yao, Y. et al. DocRED: A large-scale document-level relation\nextraction dataset. In:Proc. 57th Annual Meeting of the Association\nfor Computational Linguistics,7 6 4–777 (Association for Computa-\ntional Linguistics, 2019).https://aclanthology.org/P19-1074.\n23. Li, J. et al. Biocreative v cdr task corpus: a resource for chemical\ndisease relation extraction.Database2016 (2016).\n24. Bravo, Á., Piñero, J., Queralt-Rosinach, N., Rautschka, M. & Furlong,\nL. I. Extraction of relations between genes and diseases from text\nand large-scale data analysis: Implications for translational\nresearch.BMC Bioinformatics16,1 –17 (2015).\n25. Yang, X. et al. Pcmsp: A dataset for scientiﬁca c t i o ng r a p h s\nextraction from polycrystalline materials synthesis procedure text.\nIn: Findings of the Association for Computational Linguistics: EMNLP\n2022,6 0 3 3–6046 (Association for Computational Linguistics,\n2022). https://aclanthology.org/2022.ﬁndings-emnlp.446.\n26. Brown, T. B. et al. Language models are few-shot learners. Preprint\nat https://browse.arxiv.org/abs/2005.14165\n(2020).\n27. OpenAI. Gpt-4 technical report. Preprint athttps://browse.arxiv.\norg/abs/2303.08774(2023).\n28. Chowdhery, A. et al. Palm: Scaling language modeling with path-\nways. Journal of Machine Learning Research24,1 –113 (2023).\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 12\n29. Smith, S. et al. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. Preprint\nat https://arxiv.org/abs/2201.11990(2022).\n30. Touvron, H. et al. Llama: Open and efﬁcient foundation language\nmodels. Preprint athttps://arxiv.org/abs/2302.13971(2023).\n31. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. Preprint athttps://arxiv.org/abs/2307.09288(2023).\n32. Zhang, S. et al. Opt: Open pre-trained transformer language mod-\nels. Preprint athttps://browse.arxiv.org/abs/2205.01068(2022).\n33. Hoffmann, J. et al. Training compute-optimal large language\nmodels. Preprint athttp://arxiv.org/abs/2203.15556(2022).\n34. Wei, J. et al. Finetuned language models are zero-shot learners. In:\nInternational Conference on Learning Representations(2022).\nhttps://openreview.net/forum?id=gEZrGCozdqR.\n35. BIG-bench collaboration. Beyond the imitation game: measuring\nand extrapolating the capabilities of language models. In prepara-\ntion. https://github.com/google/BIG-bench/(2021).\n36. Dabre, R., Chu, C. & Kunchukuttan, A. A survey of multilingual\nneural machine translation.ACM Comput. Surv.53,1 –38 (2020).\n37. Petroni, F. et al. Language models as knowledge bases? In:Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),2 4 6 3–2473 (Asso-\nciation for Computational Linguistics, 2019).https://aclanthology.\norg/D19-1250.\n38. Han, J. M. et al. Unsupervised neural machine translation with\ngenerative language models only.https://openreview.net/forum?\nid=SVwbKmEg7M(2022).\n39. Zhang, H., Xu, J. & Wang, J. Pretraining-based natural language\ngeneration for text summarization. In:Proceedings of the 23rd\nConference on Computational Natural Language Learning (CoNLL),\n789–797 (Association for Computational Linguistics, 2019).https://\naclanthology.org/K19-1074.\n40. Liu, Z. et al. Multi-stage prompting for knowledgeable dialogue\ngeneration. In:Findings of the Association for Computational Lin-\nguistics: ACL 2022,1 3 1 7–1337 (Association for Computational Lin-\nguistics, 2022).https://aclanthology.org/2022.ﬁndings-acl.104.\n4 1 . G i o r g i ,J . ,B a d e r ,G .&W a n g ,B .As e q u e n c e - t o - s e q u e n c ea p p r o a c h\nfor document-level relation extraction. In:Proc. 21st Workshop on\nBiomedical Language Processing,1 0–25 (Association for Compu-\ntational Linguistics, 2022).https://aclanthology.org/2022.bionlp-\n1.2.\n42. Cabot, P.-L. H. & Navigli, R. REBEL: Relation extraction by end-to-\nend language generation. In:Findings of the Association for Com-\nputational Linguistics: EMNLP 2021(Association for Computational\nLinguistics, 2021).https://doi.org/10.18653/v1/2021.\nﬁndings-\nemnlp.204.\n43. Townsend, B., Ito-Fisher, E., Zhang, L. & May, M. Doc2dict: Infor-\nmation extraction as text generation. Preprint athttp://arxiv.org/\nabs/2105.07510(2021).\n44. Zheng, Z., Zhang, O., Borgs, C., Chayes, J. T. & Yaghi, O. M. ChatGPT\nchemistry assistant for text mining and the prediction of MOF\nsynthesis.J. Am. Chem. Soc.145,1 8 0 4 8–18062 (2023).\n45. OpenAI et al. Introducing chatgpthttps://openai.com/blog/\nchatgpt (2022).\n46. Castro Nascimento, C. M. & Pimentel, A. S. Do large language\nmodels understand chemistry? a conversation with chatgpt.J.\nChem. Inform. Modeling63,1 6 4 9–1655 (2023).\n47. White, A. D. et al. Assessment of chemistry knowledge in large\nlanguage models that generate code.Digital Discov.2,\n368–376 (2023).\n4 8 . X i e ,T .e ta l .D a r w i ns e r i e s :D o m a i ns p e c iﬁc large language models\nfor natural science. Preprint athttps://arxiv.org/abs/2308.\n13565 (2023).\n4 9 . J a b l o n k a ,K .M . ,S c h w a l l e r ,P . ,O r t e g a - G u e r r e r o ,A .&S m i t ,B .I sG P T\nall you need for low-data discovery in chemistry?https://doi.org/10.\n26434/chemrxiv-2023-fw8n4-v2(2023).\n50. Jablonka, K. M. et al. 14 examples of how llms can transform\nmaterials science and chemistry: a reﬂection on a large\nlanguage model hackathon.Digital Discov. 2,1 2 3 3–1250\n(2023).\n51. Gu, Y. et al. Domain-speciﬁc language model pretraining for bio-\nmedical natural language processing.ACM Trans. Comput.\nHealthcare3,1( 2 0 2 1 ) .\n52. Mosqueira-Rey, E., Hernández-Pereira, E., Alonso-Ríos, D., Bobes-\nBascarán, J. & Fernández-Leal, Á. Human-in-the-loop machine\nlearning: a state of the art.Artif. Intel. Rev.56,1 –50\n(2022).\n53. Maynez, J., Narayan, S., Bohnet, B. & McDonald, R. On faithfulness\nand factuality in abstractive summarization. In:Proc. 58th Annual\nMeeting of the Association for Computational Linguistics(Associa-\ntion for Computational Linguistics, 2020).https://doi.org/10.\n18653/v1/2020.acl-main.173.\n54. Ji, Z. et al. Survey of hallucination in natural language generation.\nACM Comput. Surv.55, 12 (2023).\n55. Black, S. et al. Gpt-neox-20b: An open-source autoregressive lan-\nguage model. Preprint athttps://browse.arxiv.org/abs/2204.\n06745 (2022).\n56. Frantar, E. & Alistarh, D. Sparsegpt: Massive language models can\nbe accurately pruned in one-shot. In:Proceedings of the 40th\nInternational Conference on Machine Learning,1 0 3 2 3–\n10337\n(JLMR.org, 2023).https://proceedings.mlr.press/v202/frantar23a/\nfrantar23a.pdf.\n5 7 . S u n ,M . ,L i u ,Z . ,B a i r ,A .&K o l t e r ,J .Z .As i m p l ea n de f f e c t i v ep r u n i n g\napproach for large language models. In:Workshop on Efﬁcient\nSystems for Foundation Models @ ICML2023.https://openreview.\nnet/forum?id=tz9JV2PRSv(2023).\n58. Hu, E. J. et al. Lora: Low-rank adaptation of large language models.\nPreprint athttps://arxiv.org/abs/2106.09685(2021).\n59. Ma, X., Fang, G. & Wang, X. Llm-pruner: On the structural pruning of\nlarge language models. In:Thirty-seventh Conference on Neural\nInformation Processing Systems(2023). https://openreview.net/\nforum?id=J8Ajf9WfXP.\n60. Bucior, B. J. et al. Identiﬁcation schemes for metal–organic frame-\nworks to enable rapid search and cheminformatics analysis.Cryst.\nGrowth Des.19,6 6 8 2–6697 (2019).\n61. Li, X., Hill, M. R., Wang, H. & Zhang, H. Metal–organic framework-\nbased ion-selective membranes.Adv. Mater. Technol.6,\n2000790 (2021).\n62. Qian, Q. et al. Mof-based membranes for gas separations.Chem.\nRev. 120,8 1 6 1–8266 (2020).\n63. Tshitoyan, V. et al. Unsupervised word embeddings capture latent\nknowledge from materials science literature.Nature 571,\n95–98 (2019).\n64. Kluyver, T. et al. Jupyter notebooks— a publishing format for\nreproducible computational workﬂows. In:Positioning and Power in\nAcademic Publishing: Players, Agents and Agendas(eds. Loizides, F.\n&S c h m i d t ,B . )8 7–90 (IOS Press, 2016).\n65. Dagdelen, J. et al. Structured information extraction from scientiﬁc\ntext with large language modelshttps://doi.org/10.5281/zenodo.\n10421174(2023).\n66. Dagdelen, J. et al. Llama 2 code for structured information extrac-\ntion from scientiﬁc text with large language models.https://doi.\norg/10.5281/zenodo.10421187(2023).\n67. Dagdelen, J. et al. Llama 2 weights for structured information\nextraction from scientiﬁc text with large language\nmodels. https://doi.org/10.6084/m9. ﬁgshare.24501331.v1\n(2023).\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 13\nAcknowledgements\nThis work was supported by Toyota Research Institute through the\nAccelerated Materials Design and Discovery program. A.S.R.\nacknowledges support via a Miller Research Fellowship from the\nMiller Institute for Basic Research in Science, University of Cali-\nfornia, Berkeley. Funding for training and evaluating the Llama-2\nmodel was provided by the U.S. Department of Energy, Ofﬁce of\nScience, Ofﬁce of Basic Energy Sciences, Materials Sciences and\nEngineering Division under Contract No. DE-AC02-05CH11231\n(D2S2 program KCD2S2). This research used resources of the\nNational Energy Research Scientiﬁc Computing Center (NERSC), a\nU.S. Department of Energy Ofﬁce of Science User Facility located\nat Lawrence Berkeley National Laboratory, operated under Con-\ntract No. DE-AC02-05CH11231 using NERSC award BES-\nERCAP0024004. We thank Anna Sackmann (Science Data and\nEngineering Librarian at UC Berkeley) for helping us to obtain Text\nand Data Mining agreements with the speciﬁed publishers and we\nalso thank J. Montoya and A. Trewartha for helpful discussions.\nAuthor contributions\nJ.D., A.D., N.W., and A.J. developed the information extraction method\npresented in this paper and J.D. and A.D. collected the abstract dataset\nused. J.D originated and performed supporting experiments to justify\nthe sequence-to-sequence approach and use of GPT-3 for document-\nlevel information extraction from materials science text. A.D. expanded\non JD’s initial experiments to further develop the approach. N.W. and J.D.\ndeveloped the sequence-to-JSONmethod. A.D. created the doping\nschemas, developed the sequence-to-sentence method for doping, and\nannotated the doping dataset. J.D. created the general materials infor-\nmation schema, annotated the general materials dataset, trained the\ngeneral materials information extraction model, and manually scored\nthe information extraction results for the General-JSON model validation\nset results. A.D. trained all Doping-* LLM-NERRE models and imple-\nmented the MatBERT + Proximity doping model. S.L. trained and col-\nlected data from the Llama-2 and seq2rel models. A.D. performed the\nlearning curve experiments, and collected data on annotation times.\nA.S.R. and J.D. co-created the MOF schema and annotated the MOF\ndataset while J.D. trained the MOF-JSON model. In addition to intellec-\ntual leadership and overall researchdirections, A.J. also contributed to\ntask design and task scoring metrics.G.C., K.P., and A.J. supervised this\nwork. All authors contributed to writing the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-45563-x.\nCorrespondenceand requests for materials should be addressed to\nAnubhav Jain.\nPeer review informationNature Communicationsthanks the anon-\nymous, reviewers for their contribution to the peer review of this work. A\npeer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-45563-x\nNature Communications|         (2024) 15:1418 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8170351982116699
    },
    {
      "name": "Relationship extraction",
      "score": 0.6265901923179626
    },
    {
      "name": "Information extraction",
      "score": 0.5928383469581604
    },
    {
      "name": "Task (project management)",
      "score": 0.5847753882408142
    },
    {
      "name": "Information retrieval",
      "score": 0.5606008172035217
    },
    {
      "name": "JSON",
      "score": 0.5281914472579956
    },
    {
      "name": "Natural language processing",
      "score": 0.5113422274589539
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5064793825149536
    },
    {
      "name": "Knowledge extraction",
      "score": 0.4225511848926544
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3870672583580017
    },
    {
      "name": "World Wide Web",
      "score": 0.22554254531860352
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}