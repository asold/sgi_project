{
  "title": "A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation",
  "url": "https://openalex.org/W3195396028",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2273756988",
      "name": "Zhang, Yinglin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889812864",
      "name": "Higashita Risa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750508952",
      "name": "Fu, Huazhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2332101520",
      "name": "Xu, Yanwu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999911198",
      "name": "Zhang Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2652436855",
      "name": "Liu, Haofeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1746823114",
      "name": "Zhang Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105262696",
      "name": "LIU Jiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2802597987",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2943196448",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2891688681",
    "https://openalex.org/W2792730798",
    "https://openalex.org/W2086843392",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2893801697",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2963803174"
  ],
  "abstract": "Corneal endothelial cell segmentation plays a vital role inquantifying clinical indicators such as cell density, coefficient of variation,and hexagonality. However, the corneal endothelium's uneven reflectionand the subject's tremor and movement cause blurred cell edges in theimage, which is difficult to segment, and need more details and contextinformation to release this problem. Due to the limited receptive field oflocal convolution and continuous downsampling, the existing deep learn-ing segmentation methods cannot make full use of global context andmiss many details. This paper proposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the transformer and body-edgebranch. Firstly, We use the convolutional block to focus on local tex-ture feature extraction and establish long-range dependencies over space,channel, and layer by the transformer and residual connection. Besides,We use the body-edge branch to promote local consistency and to provideedge position information. On the self-collected dataset TM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art (SOTA)methods, the proposed method achieves an improvement.",
  "full_text": "A Multi-Branch Hybrid Transformer Network\nfor Corneal Endothelial Cell Segmentation\nYinglin Zhang1,6, Risa Higashita1,5⋆, Huazhu Fu7, Yanwu Xu8, Yang Zhang1,\nHaofeng Liu1, Jian Zhang6, and Jiang Liu 1,2,3,4⋆\n1 Department of Computer Science and Engineering, Southern University of Science\nand Technology, Shenzhen 518055, China\n2 Cixi Institute of Biomedical Engineering, Chinese Academy of Sciences, China\n3 Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation,\nDepartment of Computer Science and Engineering, Southern University of Science\nand Technology, Shenzhen 518055, China\n4 Research Institute of Trustworthy Autonomous Systems, Southern University of\nScience and Technology, Shenzhen 518055, China\n5 Tomey Corporation, Nagoya 451-0051, Japan\n6 Global Big Data Technologies Centre, University of Technology Sydney, NSW,\nAustralia\n7 Inception Institute of Artiﬁcial Intelligence, UAE\n8 Intelligent Healthcare Unit, Baidu, Beijing 100085, China\nk-chen@tomey.co.jp\nAbstract. Corneal endothelial cell segmentation plays a vital role in\nquantifying clinical indicators such as cell density, coeﬃcient of variation,\nand hexagonality. However, the corneal endothelium’s uneven reﬂection\nand the subject’s tremor and movement cause blurred cell edges in the\nimage, which is diﬃcult to segment, and need more details and context\ninformation to release this problem. Due to the limited receptive ﬁeld of\nlocal convolution and continuous downsampling, the existing deep learn-\ning segmentation methods cannot make full use of global context and\nmiss many details. This paper proposes a Multi-Branch hybrid Trans-\nformer Network (MBT-Net) based on the transformer and body-edge\nbranch. Firstly, We use the convolutional block to focus on local tex-\nture feature extraction and establish long-range dependencies over space,\nchannel, and layer by the transformer and residual connection. Besides,\nWe use the body-edge branch to promote local consistency and to provide\nedge position information. On the self-collected dataset TM-EM3000 and\npublic Alisarine dataset, compared with other State-Of-The-Art (SOTA)\nmethods, the proposed method achieves an improvement.\nKeywords: Corneal endothelial cell segmentation· Deep learning · Trans-\nformer · Multi-branch.\n1 Introduction\nCorneal endothelial cell abnormalities may be related to many corneal and sys-\ntemic diseases. Quantifying corneal endothelial cell density, the coeﬃcient of\narXiv:2106.07557v1  [cs.CV]  21 May 2021\nMICCAI 2021 early accepted\nvariation, and hexagonality have essential clinical signiﬁcance [2]. Cell segmen-\ntation is a crucial step to quantify the above parameters. Nevertheless, manual\nsegmentation is time-consuming, laborious, and unstable. Therefore, an accurate\nand fully automatic corneal endothelial cell segmentation method is essential to\nimprove diagnosis eﬃciency and accuracy.\nFig. 1.Segmentation results on TM-EM3000.(a) is the input equalized corneal en-\ndothelium cell image.(b), (c), (d), (e) are the segmentation results of LinkNet, DinkNet,\nUNet, and our method. The red line represents the prediction result, and the green\nline represents ground truth, orange when the two overlap.\nThe main challenge of accurate segmentation is the blurred cell edges, which\nare diﬃcult to segment, as shown in Fig.1, and needs more details and context\ninformation to release this problem. UNet [12] captures contextual semantic in-\nformation through the contracting path and combines high-resolution features\nin the contracted path with upsampled output to achieve precise localization.\nUNet++ [18] optimizes it through a series of nested, dense skip connections\nto reduce the semantic gap between the encoder and decoder’s feature maps.\nFabij´ anska [7] ﬁrst applied UNet to the task of corneal endothelial cell segmen-\ntation. Vigueras-Guill´ en et al. [15] applied the complete convolution method\nbased on UNet and the sliding window version to the analysis of cell images ob-\ntained by SP-1P Topcon corneal endothelial microscope. Fu et al. [8] proposed a\nmulti-conetxt deep network, by combining prior knowledge of regions of interest\nand clinical parameters. However, due to the limited receptive ﬁeld of local con-\nvolution and continuous downsampling, they cannot make full use of the global\ncontext and still miss many details.\nThe transformer has been proved to be an eﬀective method for establishing\nlong-range dependencies. Vaswani et al. [14] proposed a transformer structure\nsystem for language translation tasks through a complete attention mechanism\nto establish the global dependence of input and output among time, space, and\nlevels. Prajit et al. [11] explored the use of the transformer mechanism on vi-\nsual classiﬁcation tasks, replacing all spatial convolutional layers in ResNet with\nstand-alone self-attention layers. However, local self-attention will still lose part\nof the global information. Wang et al. [16] establish a stand-alone attention layer\nby using two decomposed axial attention blocks, to reduce the number of pa-\nrameters and calculations, and allow performing attention in a larger or even\nglobal range.\nMICCAI 2021 early accepted\nSome previous works obtain better segmentation results by taking full ad-\nvantage of edge information. Chen et al. [1] proposed the deep contour-aware\nnetwork, using a multi-task learning framework to study the complementary\ninformation of gland objects and contours, which improves the discriminative\ncapability of intermediate features. Chen et al.[5] improved the network output\nby learning the reference edge map of CNN intermediate features. Ding et al.\n[6] proposed to use boundary as an additional semantic category to introduce\nboundary layout constraints and promote intra-class consistency through the\nboundary feature propagation module based on unidirectional acyclic graphs.\nWe need to preserve more local details and make full use of the global context.\nIn this paper, we propose a Multi-Branch hybrid Transformer Network(MBT-\nNet). At ﬁrst, we apply a hybrid residual transformer feature extraction module\nto give full play to the advantages of convolution block and transformer block\nin terms of local details and global semantics. Speciﬁcally, we use the convolu-\ntional block to focus on local texture feature extraction and establish long-range\ndependencies over space, channel, and layer by the transformer and residual con-\nnection. Besides, we deﬁne the corneal endothelial cell’s segmentation task more\nentirely from the perspective of edge and body. Body-edge branches provide pre-\ncise edge location information and promote local consistency. The experimental\nresults show that the proposed method is superior to other state-of-the-art meth-\nods and has achieved better performance on two corneal endothelial datasets.\n2 Method\nFig. 2.The pipeline of multi-branch hybrid transformer network. Conv-e1, Conv-e2,\nConv-d1 and Conv-d2 represent encoder and decoder layer based on convolution block.\nTr-e3, Tr-e4, Tr-d3, and Tr-d4 are based on transformer blocks.\nIn this paper, we propose the MBT-Net, as shown in Fig. 2. Firstly, the fea-\nture F of equalized corneal endothelium cell image is extracted by the hybrid\nMICCAI 2021 early accepted\nresidual transformer encoder-decoder module. Each convolution layer contains\ntwo basic residual blocks with a kernel size = 3 × 3. Each transformer layer con-\ntains two residual transformer blocks with kernel size = 1× 48. Then, the feature\nis decoupled into two parts, body and edge. Also, the edge texture information\nfrom Conv-e1 is fused into the edge feature. Finally, we take the maximum re-\nsponse of edge feature E-F, body feature B-F, and feature F to obtain the fused\nfeature to predict the ﬁnal segmentation result. The training process of these\nthree branches is explicitly supervised.\nIn this pipeline, the convolutional layer focuses on local texture feature ex-\ntraction, which retains more details. The residual connection and transformer\nmake full use of the feature map’s global context information in a more exten-\nsive range of space, diﬀerent channels, and layers. The edge perspective helps\npreserve boundary details, and the body perspective promotes local consistency.\nThe low-resolution feature map of dx+1 is reﬁned by features from ex by con-\ncatenating and addition operation.\n2.1 Residual Transformer Block\nFig. 3.The residual transformer block contains two 1 × 1 convolutions, a height-axial\nand a width-axial Multi-Head Self-Attention block (MHSA). MHSA compute axial self-\nattention (SA) with eight head. r, WQ, WK, and WV are learnable vectors, where r\nrelated to the relative position\nThe residual transformer block [16] is shown in Fig.3, which contains two 1×1\nconvolution to control the number of channels to be calculated and a height-axial\nand a width-axial Multi-Head Self-Attention block (MHSA), which signiﬁcantly\nreduces the amount of calculation. This setting allows the transformer layer\nto consider the global spatial context in feature map size straightly. The axial\nSelf-Attention(SA) module is deﬁned as:\nyo =\n∑\np∈N1×m(o)\nsoftmaxp(qT\no kp + qT\no rq\np−o + kT\np rk\np−o)(vp + rv\np−o) (1)\nFor a given input feature mapx, queries q= WQx, keys k= WKx, values v=\nWV x are linear projections of feature map x, where WQ,WK,WV are learnable\nparameters. rq\n(p−o),rk\n(p−o),rv\n(p−o) measure the compatibility from position p to\no in query, key and value. They are also learnable paramters. The softmaxp\ndenotes a softmax function applied to all possibleppositions. N1×m(o) represents\nMICCAI 2021 early accepted\nthe local 1 × m square region centered around location o, yo is the output at\nposition o.\nal2 = al1 +\nl2−1∑\ni=l1\nf(ai) (2)\nBesides, all the block used in encoder-decoder is in residual form, which can\npropagate input signal directly from any low layer to the high layer, optimizing\ninformation interaction [9], [10]. Taking any two layersl2 >l1 into consideration,\nthe forward information propagation process is formulated as Eq.(2).\n2.2 The Body, Edge, and Final Branches\nThe information from the body and edge perspectives is combined to better\ndeﬁne corneal endothelial cell segmentation. The body branch provides general\nshape and overall consistency information to promote local consistency, while the\nedge branch provides edge localization information to improve the segmentation\naccuracy of image details.\nWe decouple the feature F extracted by the hybrid residual transformer\nencoder-decoder module into Fbody = φ(F) and Fedge = F − Fbody, where φ\nis implemented by convolution layer. Also, the low level information from en-\ncoder Conv-e1 is fused into the edge feature, Fedge = Fedge + ψ(Fe1), where\nψ is dimension operation. Finally, the above three feature maps are fused into\nFfinal = ϕ(F,Fedge,Fbody) for ﬁnal segmentation prediction, where ϕrepresents\nthe maximum response.\nThe training process of these three branches is explicitly supervised. The\nthree masks used in training are shown in Fig.4. The ﬁnal prediction mask is the\nground truth annotated by experts. The edge prediction mask is extracted from\nthe ﬁnal prediction mask by the canny operator. The body prediction mask is\nobtained by inverting the ﬁnal prediction and then performing Gaussian blurring\nat the edges.\nFig. 4.Three kinds of masks on TM-EM3000.(a) The ﬁnal prediction mask from the\nannotation of an expert, (b) The edge prediction mask extracted from (a) through the\ncanny operator, (c) The body prediction mask by relaxing the edge of invert image of\n(a) with a Gaussian kernel.\n2.3 Loss Function\nLoss= λ1Lb(ˆyb,yb) + λ2Le(ˆye,ye) + λ3Lf (ˆyf ,yf ) (3)\nMICCAI 2021 early accepted\nIn this paper, we jointly optimize the body, edge, and ﬁnal losses, as shown\nin Eq.(3), where λ1,λ2,λ3 are hyper parameters to adjust the weight of three\ndiﬀerent losses. As the ﬁnal prediction is the output we ﬁnally use to compare\nwith ground truth, we give it a higher weight than edge and body branch. In\nour experiment, we set λ1 = 0 .5,λ2 = 0 .5,λ3 = 1 .2. yb,ye,yf represent the\nground truth of body, edge and ﬁnal prediction respectively, and ˆ yb,ˆye,ˆyf are\ncorresponding prediction from model. The binary cross entropy loss is used, as\nshown in Eq.(4).\nL= 1\nN\n∑\ni\n[yi ln ˆyi + (1 − yi) ln(1− ˆyi)] (4)\nWhere N represents the total number of pixels, yi denotes target label for\npixel i, ˆyi is the predicted probability.\n3 Experiments\n3.1 Datasets and implementation Details\nTM-EM3000 contains 184 images of corneal endothelium cell and its corre-\nsponding segmentation ground truth, with size = 266 ×480, collected by specu-\nlar microscope EM3000, Tomey, Japan. To reduce the interference of lesions and\nartifacts and build a data set with almost the same imaging quality, we select a\npatch with a size of 192 ×192 from each image. This dataset is manually anno-\ntated and reviewed by three independent experts. We split it into the training\nset 155 patches, the validation set 10 patches, and the test set 19 patches.\nAlizarine Dataset is collected by inverse phase-contrast microscope (CK 40,\nOlympus) at 200 × magniﬁcation [13]. It consists of 30 images of corneal en-\ndothelium acquired from 30 porcine eyes and its corresponding segmentation\nground truth, with image size = 768×576, and mean area assessed per cornea =\n0.54 ± 0.07 mm2. Since each image in this dataset is only partly annotated, we\nselect ten patches of size 192×192 from each image to have 300 patches in total.\nAnd then split it into the training set 260 patches, validation set 40 patches.The\ntraining set and validation set do not overlap.\nImplementation Details. We use the RMSprop optimization strategy during\nmodel training. The initial learning rate is 2e-4, epochs = 100 , batch size = 1.\nThe learning rate optimization strategy is ReduceLROnPlateau, and the network\ninput size is 192 ×192. All the models are trained and tested with PyTorch on\nthe platform of NVIDIA GeForce TITAN XP.\n3.2 Comparison with SOTA methods\nWe compare performance of the proposed method with LinkNet [3], DinkNet\n[17], UNet [12], UNet++[18] and TransUNet[4] on TM-EM3000 and Alis-\narine dataset. We use dice coeﬃcient(DICE), F1 score(F1), sensitivity(SE), and\nMICCAI 2021 early accepted\nTable 1.Quantitative evaluation of diﬀerent methods. The proposed method achieves\nthe best performance.\nModel TM-EM3000 Alisarine\nDICE F1 SE SP DICE F1 SE SP\nLinkNet34 [3] 0.711 0.712 0.719 0.941 0.766 0.801 0.805 0.956\nDinkNet34 [17] 0.717 0.718 0.724 0.944 0.767 0.805 0.821 0.953\nUNet [12] 0.730 0.743 0.763 0.945 0.775 0.811 0.814 0.960\nUNet++ [18] 0.728 0.739 0.775 0.938 0.773 0.811 0.850 0.947\nTransUNet [4] 0.734 0.742 0.769 0.941 0.783 0.821 0.866 0.948\nProposed 0.747 0.747 0.768 0.946 0.786 0.821 0.8770.944\nspeciﬁcity(SP) as evaluation indicators, where DICE and F1 are most important\nbecause they are related to the overall performance.\nAs shown in Table 1, The proposed method has obtained the best overall\nperformance on both TM-EM3000 and Alisarine data sets. On TM-EM3000,\nthe DICE accuracy and F1 score of our approach are 0.747 and 0.747. On the\nAlisarine data set, the Dice accuracy and F1 score of our method are 0.786 and\n0.821. UNet++[18] is modiﬁed from UNet, through a series of nested, dense skip\nconnections to capture more semantic information. However, in general, there is\nno noticeable improvement observed in this experiment. TransUNet[4] optimized\nthe UNet by using the transformer layer to capture the global context in the\nencoder part, but our method has achieved better performance. It is mainly due\nto the following advantages. 1) Long-range dependencies are established through\nthe transformer in both the encoder and decoder. 2) Performing transformer\nlayer on the whole feature map, further reducing the loss of semantic information.\n3) The body-edge branch encourages the network to learn more general features\nand provide edge localization information.\nAs shown in Fig.5, on the left side of the TM-EM3000 image, the cell bound-\nary is clear. The segmentation performance of diﬀerent methods is not much\ndiﬀerent. Nevertheless, on the right side with uneven illumination and the blur\ncell boundary, the proposed method achieves better segmentation results, closer\nto the ground truth, and in line with the real situation.\nThere is no extensive range of fuzzy area in the Alisarine image, and all meth-\nods obtained satisﬁed segmentation results. However, the baselines have varying\ndegrees of loss in details, which lead to discontinuous cell edge segmentation\nas the white arrow indicated, and the segmentation results do not match the\nground truth well as the yellow arrow indicated. The proposed method obtains\nbetter segmentation accuracy.\n3.3 Ablation Study\nThe ablation experiment is conducted to explore the transformer’s replacement\ndesign, as shown in Table 2. In this process, we gradually replace the encoder-\ndecoder structure’s convolution layer with the transformer from inside to out-\nMICCAI 2021 early accepted\nFig. 5.Qualitative results on TM-EM3000 and the Alisarine Dataset.(a) is the input\nequalized corneal endothelium cell image. The red line represents the prediction result,\nand the green line represents ground truth, orange when the two overlap. The white\narrow indicates the missing segmentation location, and the yellow arrow indicates the\nlocation where the segmentation result does not ﬁt well with the ground truth\nside. In the beginning, the model captures more semantic information, and the\nperformance is improved. Then, with the transformer replacing the shallow con-\nvolutional layer further, the model starts to lose local information, resulting in\nthe decline of performance. Model 2-2-TR achieves the best balance between\nlocal details and global context.\nTable 2.Ablation study on the replacement design of transformer. 0-0-TR means no\ntransformer is used. 1-1-TR means e4, d4 is transformer layer. 2-2-TR means e3, e4,\nand d3, d4 is transformer layer. 3-3-TR means e2, e3, e4, and d2, d3, d4 is transformer\nlayer. 4-4-TR means complete transformer structure.\nModel TM-EM3000 Alisarine\nDICE F1 SE SP DICE F1 SE SP\n0-0-TR 0.731 0.737 0.774 0.937 0.776 0.813 0.852 0.948\n1-1-TR 0.737 0.746 0.778 0.941 0.778 0.816 0.857 0.948\n2-2-TR 0.747 0.7470.768 0.946 0.786 0.821 0.8770.944\n3-3-TR 0.702 0.714 0.742 0.935 0.777 0.812 0.874 0.940\n4-4-TR 0.687 0.707 0.717 0.940 0.769 0.802 0.869 0.936\nWe also study the inﬂuence of transformer and body-edge branch on per-\nformance on TM-EM3000 dataset, as shown in Table 3. When neither trans-\nformer nor body-edge branches are used, the DICE accuracy and F1 score on\nTM-EM3000 and Alisarine are 0.720 and 0.733, respectively. After adding the\nbody-edge branch, the performance is improved to 0.731 and 0.737. When the\ntransformer is used, the DICE accuracy and F1 score are 0.736 and 0.741. Using\nMICCAI 2021 early accepted\nTable 3.Ablation study on transformer and body-edge branch on TM-EM3000.TR\nmeans transformer, and B-E means body-edge branch. When transformer is used, it\nmeans 2-2-TR.\nTR B-E Dice F1 SE SP\n% % 0.720 0.733 0.746 0.945\n% ! 0.731 0.737 0.774 0.937\n! % 0.736 0.741 0.786 0.936\n! ! 0.747 0.747 0.768 0.946\nboth the body-edge branch and transformer, we improve the performance by\n2.7% and 1.4% in total to 0.747 and 0.747.\n4 Conclusion\nThis paper proposes a multi-branch hybrid transformer network for corneal en-\ndothelial cell segmentation, which combines the convolution and transformer\nblock’s advantage and uses the body-edge branch to promote local consistency\nand provide edge localization information. Our method achieves superior perfor-\nmance to various state-of-the-art methods, especially in the fuzzy region. The\nablation study shows that both the well-designed transformer replacement and\nbody-edge branches contribute to improved performance.\nReferences\n1. 0011, C.H., Qi, X., Yu, L., Heng, P.A.: Dcan: Deep contour-aware networks for\naccurate gland segmentation. CVPR pp. 2487–2496 (2016)\n2. Al-Fahdawi, S., Qahwaji, R., Al-Waisy, S.A., Ipson, S.S., Ferdousi, M., Malik, A.R.,\nBrahma, A.: A fully automated cell segmentation and morphometric parameter\nsystem for quantifying corneal endothelial cell morphology. Computer Methods\nand Programs in Biomedicine pp. 11–23 (2018)\n3. Chaurasia, A., Culurciello, E.a.: Linknet: Exploiting encoder representations for\neﬃcient semantic segmentation. VCIP pp. 1–4 (2017)\n4. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, L.A., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation\n(2021)\n5. Chen, L.C., Barron, T.J., Papandreou, G., 0002, M.K., Yuille, L.A.: Semantic\nimage segmentation with task-speciﬁc edge detection using cnns and a discrimina-\ntively trained domain transform. computer vision and pattern recognition (2016)\n6. Ding, H., Jiang, X., Liu, Q.A., Magnenat-Thalmann, N., Wang, G.: Boundary-\naware feature propagation for scene segmentation. ICCV pp. 6819–6829 (2019)\n7. Fabijanska, A.: Segmentation of corneal endothelium images using a u-net-based\nconvolutional neural network. Artiﬁcial Intelligence in Medicine pp. 1–13 (2018)\nMICCAI 2021 early accepted\n8. Fu, H., Xu, Y., Lin, S., Wong, D.W.K., Mani, B., Mahesh, M., Aung, T., Liu, J.:\nMulti-context deep network for angle-closure glaucoma screening in anterior seg-\nment oct. In: International Conference on Medical image computing and computer-\nassisted intervention. pp. 356–363. Springer (2018)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. pp. 770–778 (2016)\n10. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\nIn: ECCV. pp. 630–645. Springer (2016)\n11. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., Shlens, J.:\nStand-alone self-attention in vision models. NIPS (2019)\n12. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. MICCAI (2015)\n13. Ruggeri, A., Scarpa, F., Luca, D.M., Meltendorf, C., Schroeter, J.: A system\nfor the automatic estimation of morphometric parameters of corneal endothelium\nin alizarine red-stained images. British Journal of Ophthalmology pp. 643.0–647\n(2010)\n14. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, N.A., Kaiser,\nL., Polosukhin, I.: Attention is all you need. NIPS pp. 5998–6008 (2017)\n15. Vigueras-Guill´ en, J.P., Sari, B., Goes, S.F., Lemij, H.G., van Rooij, J., Vermeer,\nK.A., van Vliet, L.J.: Fully convolutional architecture vs sliding-window cnn for\ncorneal endothelium cell segmentation. BMC Biomedical Engineering 1(1), 1–16\n(2019)\n16. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., Chen, L.C.: Axial-deeplab:\nStand-alone axial-attention for panoptic segmentation. In: ECCV. pp. 108–126.\nSpringer (2020)\n17. Zhou, L., Zhang, C., Wu, M.: D-linknet: Linknet with pretrained encoder and\ndilated convolution for high resolution satellite imagery road extraction. CVPR\nWorkshops pp. 182–186 (2018)\n18. Zhou, Z., Siddiquee, M.R.M., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. DLMIA/ML-CDS@MICCAI pp. 3–\n11 (2018)",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.684099555015564
    },
    {
      "name": "Computer science",
      "score": 0.6189284324645996
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5132594108581543
    },
    {
      "name": "Corneal endothelium",
      "score": 0.45395559072494507
    },
    {
      "name": "Convolutional neural network",
      "score": 0.41390740871429443
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.373900830745697
    },
    {
      "name": "Computer vision",
      "score": 0.3737691342830658
    },
    {
      "name": "Endothelium",
      "score": 0.1498478353023529
    },
    {
      "name": "Medicine",
      "score": 0.10406714677810669
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    }
  ]
}