{
    "title": "VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization",
    "url": "https://openalex.org/W3154719617",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5101561398",
            "name": "Pankaj Kumar Mishra",
            "affiliations": [
                "University of Udine"
            ]
        },
        {
            "id": "https://openalex.org/A5012992975",
            "name": "Riccardo Verk",
            "affiliations": [
                "University of Udine"
            ]
        },
        {
            "id": "https://openalex.org/A5032900062",
            "name": "Daniele Fornasier",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5004078798",
            "name": "Claudio Piciarelli",
            "affiliations": [
                "University of Udine"
            ]
        },
        {
            "id": "https://openalex.org/A5078277002",
            "name": "Gian Luca Foresti",
            "affiliations": [
                "University of Udine"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2124136621",
        "https://openalex.org/W2809705434",
        "https://openalex.org/W3017026857",
        "https://openalex.org/W3044215473",
        "https://openalex.org/W6769594638",
        "https://openalex.org/W6786114270",
        "https://openalex.org/W6685777803",
        "https://openalex.org/W2963061824",
        "https://openalex.org/W6753123669",
        "https://openalex.org/W2925312408",
        "https://openalex.org/W6748366383",
        "https://openalex.org/W2784032999",
        "https://openalex.org/W3013207792",
        "https://openalex.org/W2037660732",
        "https://openalex.org/W2936404407",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3021182036",
        "https://openalex.org/W3034314048",
        "https://openalex.org/W2171771147",
        "https://openalex.org/W2950119225",
        "https://openalex.org/W2110934250",
        "https://openalex.org/W2971501370",
        "https://openalex.org/W2963049059",
        "https://openalex.org/W2948982773",
        "https://openalex.org/W2783748519",
        "https://openalex.org/W2340896621",
        "https://openalex.org/W2962849408",
        "https://openalex.org/W3151160911",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1579853615",
        "https://openalex.org/W3109715690",
        "https://openalex.org/W3034648032",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2789159078",
        "https://openalex.org/W2181347294",
        "https://openalex.org/W2860343088",
        "https://openalex.org/W3105939760"
    ],
    "abstract": "We present a transformer-based image anomaly detection and localization network. Our proposed model is a combination of a reconstruction-based approach and patch embedding. The use of transformer networks helps to preserve the spatial information of the embedded patches, which are later processed by a Gaussian mixture density network to localize the anomalous areas. In addition, we also publish BTAD, a real-world industrial anomaly dataset. Our results are compared with other state-of-the-art algorithms using publicly available datasets like MNIST and MVTec.",
    "full_text": "VT-ADL: A Vision Transformer Network for Image\nAnomaly Detection and Localization\nPankaj Mishra\nUniversity of Udine, Italy\nEmail: mishra.pankaj@spes.uniud.it\nRiccardo Verk\nUniversity of Udine, Italy\nEmail: verk.riccardo@spes.uniud.it\nDaniele Fornasier\nbeanTech srl, Italy\nEmail: daniele.fornasier@beantech.it\nClaudio Piciarelli\nUniversity of Udine, Italy\nEmail: claudio.piciarelli@uniud.it\nGian Luca Foresti\nUniversity of Udine, Italy\nEmail: gianluca.foresti@uniud.it\nAbstract—We present a transformer-based image anomaly\ndetection and localization network. Our proposed model is\na combination of a reconstruction-based approach and patch\nembedding. The use of transformer networks helps preserving\nthe spatial information of the embedded patches, which is later\nprocessed by a Gaussian mixture density network to localize the\nanomalous areas. In addition, we also publish BTAD, a real-world\nindustrial anomaly dataset. Our results are compared with other\nstate-of-the-art algorithms using publicly available datasets like\nMNIST and MVTec.\nIndex Terms—Anomaly Detection, Anomaly segmentation,\nVision transformer, Gaussian density approximation, Anomaly\ndataset\nI. I NTRODUCTION\nIn computer vision, an anomaly is any image or image\nportion which exhibits signiﬁcant variation from the pre-\ndeﬁned characteristics of normality. Anomaly Detection is thus\nthe task of identifying these novel samples in supervised or\nunsupervised ways. A system which can perform this task in\nan intelligent way is hugely in demand, as its applications\nrange from video surveillance [1] to defect segmentation [2],\n[3], inspection [2], quality control [4], medical imagining [5],\nﬁnancial transactions [6] etc. As it can be seen from the\nexamples, anomaly detection is particularly signiﬁcant in the\nindustrial ﬁeld, where it can be used to automatically identify\ndefective products.\nRecent efforts have been made to improve the anomaly\ndetection task in the ﬁeld of deep learning. Most of the works\ntry to learn the manifold of a single class representing normal\ndata[7], using an encoding-decoding scheme, and their output\nis a classiﬁcation of the input image as either normal or\nanomaly, while fewer works deal with the task to segment\nthe local anomalous region in an image[8]. Majorly, the\nmethods either use a reconstruction-based approach, or learn\nthe distribution of the latent features extracted by a pre-trained\nnetwork or trained in end-to-end fashion.\nMotivated from the above facts and industrial needs, we\ndeveloped a Vision-Transformer-based image anomaly detec-\ntion and localization network (VT-ADL), which learns the\nThis work is partially supported by beanTech srl.\nFig. 1. The three products of BTAD dataset. First column shows an example of\nnormal images, second column shows anomalous images, third column shows\nthe anomalous image with pixel-level ground truth labels, fourth column\nshows the predicted heat map by our proposed method.\nmanifold of normal class data in a semi-supervised way, thus\nrequiring only normal data in the training process. The vision\ntransformer network model, recently proposed by Dosovitskiy\net al. [9], is a network designed to work on image patches\ntrying to preserve their positional information. In our work\nwe show how an adapted transformer network can be used for\nanomaly localization using Gaussian Approximation [10], [11]\nof the latent features and also how different conﬁgurations can\nbe tweaked to win some of the shortcomings of the vision\ntransformer network. In addition to this, we are also pub-\nlishing a real-world industrial dataset (the beanTech Anomaly\nDetection dataset — BTAD) for the anomaly detection task.\nThe dataset contains a total of 2830 real-world images of 3\nindustrial products showcasing body and surface defects.\nII. R ELATED WORK\nImage-based anomaly detection is not a new topic in the\nindustrial use cases, as it has been used in many inspection and\nquality control schemes, however is still under investigation\nwith modern deep learning techniques. Historically, several\narXiv:2104.10036v1  [cs.CV]  20 Apr 2021\nclassical image processing and machine learning methods\nhave been used to perform anomaly detection tasks, such as\nBayesian networks, rule-based systems, clustering algorithms\netc. However, in the recent years the trend has been shifted to\nthe use of deep learning methods, as the convolutional layers\nhave revolutionized this ﬁeld. Most of the proposed approaches\nare based on image reconstruction: in this case, the network is\ntrained to reconstruct the input image. If the network is trained\non normal data only, it is assumed it will fail at properly re-\nconstructing anomalies. Network architectures mostly consist\nof various conﬁguration of autoencoders [12], [13], [14], [15],\n[16] or Generative Adversarial Network (GAN) [17], [18]. At\nimage level, the simplest way is to train using MSE loss, and\nin turn expect higher reconstruction loss for the anomalous\nimages. Additional information of the latent space [19] are also\nused for better classiﬁcation. Yet for the anomaly localization,\npixel-wise reconstructed error is taken as the anomaly score.\nSome methods also tried to use visual attention maps [20],\n[13] from the latent space. Reconstruction based methods are\nvery intuitive and explainable, but their performance is limited\nwhen it comes to capture small localized anomalies[21].\nRegarding the learning method, few works adopt a fully\nsupervised approach. It consist in training a binary classiﬁer,\nin which two classes represent the normal and the anomalous\ndata. However real-world anomaly datasets are extremely\nimbalanced, since the number of anomalies is typically orders\nof magnitude smaller than the number of normal data. This\nrequires specialized approaches to handle data imbalance [22],\n[23]. The majority of the solutions however rely on a semi-\nsupervised approach, in which only normal data are available\nin the training step. In this case the system tries to learn a “nor-\nmality” model from the training data and thus subsequently\nclassify new samples as anomaly if they don’t ﬁt the model [1],\n[16], [12], [24]. Recently P.Bergmann et.al [8] developed a\nnovel network and training scheme for both image anomaly\ndetection and localization. The approach uses a student-teacher\nlearning scheme and knowledge distillation for achieving state-\nof-the-art results and a single network for both classiﬁcation\nand pixel level anomaly localization. The work achieves good\nresults, but it uses a complex training scheme with high num-\nber of student networks, which again demands high computing\nresources for the industrial applications. Finally, some models\nare based on unsupervised learning: in this case the most\ncommon approach is to use the deep network only for feature\nextraction and then later use some clustering algorithm, such\nas one-class SVM or SVDD for the ﬁnal classiﬁcation. Some\nof the works handled these two steps independently [25], [26],\nwhile others achieved better results by doing the two steps\ncollectively, in order to extract the best features for subsequent\nanomaly detection [27], [28].\nIII. P ROPOSED MODEL\nThe proposed model combines the traditional\nreconstruction-based methods with the beneﬁts of a patch-\nbased approach. The input image is subdivided in patches\nand encoded using a Vision Transformer. The resulting\nfeatures are then fed into a decoder to reconstruct the original\nimage, thus forcing the network to learn features that are\nrepresentative of the aspect of normal images (the only\ndata on which the network is trained). At the same time, a\nGaussian mixture density network models the distribution\nof the transformer-encoded features in order to estimate\nthe distribution of the normal data in this latent space.\nDetecting anomalies with this model automatically allows\ntheir localization, since transformer-encoded features are\nassociated to positional information.\nAn overview of the model is depicted in Figure 2. To handle\na 2D image X ∈ RH×W×C, we break the image into a\nsequence of 2D patches Xp ∈RN×(P×P×C), where (H,W )\nis the original image resolution, C is the number of channels,\n(P,P ) is the patch dimensions and N is the resulting number\nof patches N = HW/P2. These patches are then embedded\nto a D-dimensional embedding space through a linear layer.\nPositional embedding is added to the patch embedding to\npreserve the positional information.\n• Transformer Encoder: The transformer encoder layer\nis based on the work by Vaswani et al [29] and its\napplication to images by Dosovitskiy et al [9]. The input\npatches are ﬁrst mapped to the embedding space and\nare augmented with positional information (eq. 1), then\npassed to a Multi-headed Self-Attention block (eq 2)\nand a MLP block (eq. 3). Layer normalization (LN) is\napplied before the two blocks and residual connections\nare added after the two blocks. We didn’t use the dropout\nlayer throughout the network, as this causes instability in\nthe Gaussian approximation network. MLP contains two\nlinear layers with a GELU activation function.\nZ0 = [X1\npE; X2\npE; ...; XN\np E] + Epos, (1)\nE ∈R(P2.C)×D, Epos ∈R(N+1)×D\nZ\n′\nl = MSA(LN(Zl−1)) + Zl−1,l = 1..L (2)\nZl = MLP(LN(Z\n′\nl)) + Z\n′\nl,l = 1..L (3)\nThe ﬁnal encoded patches are reshaped and projected in\nto a reconstruction vector via learned projection matrix.\n• Decoder: The decoder is used to decode the reconstruc-\ntion vector back to the original image shape. It maps\nR512 −→RH×W×C. In our experiments with the MVTec\nand BTAD dataset, we used 5 transposed convolutional\nlayers, with batch normalization and ReLU in-between,\nexcept for the last layer, we use tanh as the ﬁnal non-\nlinearity.\n• Gaussian Mixture Density Network: This kind of net-\nwork estimates the conditional distribution p(y|x) [10] of\na mixture density model. In particular, the parameters of\nthe unconditional mixture distribution p(y) are estimated\nby the neural network, which takes the image embedding\n(conditional variable x) as the input. For our purpose we\nemploy the Gaussian Mixture Model (GMM) with full\nFig. 2. Left image: model overview. Image is split into patches, which are augmented with positional embedding. The resulting sequence is fed to the\nTransformer encoder. Then encoded features are summed into a reconstruction vector which is fed to decoder. The transformer encoded features are also fed\ninto an Gaussian approximation network [10], which is later used to localize the anomaly. Right image: detailed structure of the tranformer encoder (image\nfrom [9]).\nco-variance matrix Σk as the density model. The density\nestimate ˆp(y|x) follows the weighted sum of K Gaussian\nfunctions.\nˆp(y|x) =\nK∑\nk=1\nwk(x; θ)N(y|µk(x; θ),σ2\nk(x; θ)) (4)\nwherein, wk(x; θ) denotes the weight, µk(x; θ) the mean,\nσ2\nk(x; θ) the variance of the k-th Gaussian. All the GMM\nparameters are estimated using the neural network with\nparameters θ and input x. The mixing weights of the\nGaussians must satisfy the constraints ∑K\nk=1 wk(x; θ) =\n1 and wk(x; θ) ≥ 0 ∀k. This is achieved using the\nsoftmax function to the output of weight estimation:\nwk(x) = exp(aw\nk(x))∑K\nk=1 exp(aw\ni (x))\n(5)\nwherein aw\nk(x) ∈R is the logit scores emitted by the\nneural network. Additionally, standard deviation σk(x)\nmust be positive. To satisfy this, a softplus non-linearity\nis applied to the output of the neural network.\nσk(x) = log(1 + exp(β×x)); β = 1 (6)\nSince, mean µk(x; θ) doesn’t have any constraint, we\nused linear layer without any non-linearity for the re-\nspective output neurons.\nIV. O BJECTIVE AND LOSSES\nTraining the network has two objectives: on one side we\nwant the decoder output to resemble the network input, as\nin reconstruction-based anomaly detection. This forces the\nencoder to catch features that are relevant to describe the\nnormal data. On the other side, the goal is to train the\nGaussian mixture density network to model the manifold\nwhere the encoded features of normal images reside. For the\nreconstruction-based part we adopted a combination of two\nlosses:\n• Mean Squared Error (MSE): it is a pixel-level loss, which\nassumes independence between pixels. MSE loss is com-\nputed as the average of the squared pixel-wise differences\nof the two images, and can be formally deﬁned in terms\nof the Frobenius norm as 1\nWH ∥X − ˆX∥2\nF, where X is\nthe input and ˆX is the output of the decoder network\n(respectively the original and the reconstructed image),\nand W,H are the image width and height respectively.\n• Structural Similarity Index - The Structural Similarity\nIndex (SSIM) [30] is used to measure the image similarity\nby considering visual properties that are lost in the\nstandard MSE approach:\nSSIM(x,y) = (2µxµy + c1)(2σxy + c2)\n(µ2x + µ2y + c1)(σ2x + σ2y + c2) (7)\nwhere, µx, µy, are the average values of input and\nreconstruction image, σ2\nx, σ2\ny are the variance of input and\nreconstructed image, σxy is their co-variance and c1,c2\nare the two constants used for numerical stability.\nFor the Gaussian mixture density network training we\nused the Log-Likelihood Loss (LL). The parameter θ of the\nGaussian estimation network are ﬁtted through maximum\nlikelihood estimation. We minimize the negative conditional\nlog-likelihood of the normal class training data.\nθ∗ = −arg min\nθ\nK∑\nk=1\nlog pθ(yn|xn) (8)\nFor the purpose of regularization, we also add Gaussian noise\nN(0,0.2) to the transformer embedded features before feeding\nit to the GMM model. Adding noise during training is seen\nas a form of data augmentation and regularization that biases\ntowards smooth functions [10], [31].\nFig. 3. Anomaly detection on MVTec dataset. First row shows the actual\nanomalous image of bottle, cable, capsule, metal nut and brush. second row\nshows the actual ground truth and third row shows the generated anomaly\nscore and anomaly localization by our method\nHence, the ﬁnal objective function to minimize is the\nweighted addition of the above three losses.\nL(X) = −LL+ λ1MSE(X, ˆX) + λ2SSIM(X, ˆX) (9)\nwherein, λ1 = 5 and λ2 = 0.5 for all the datasets used in this\nstudy.\nV. E XPERIMENTAL RESULTS\nIn this section, we present the experimental results obtained\nby our proposed network VT-ADL. We ﬁrst describe the\nused datasets, training and testing procedures and comparative\nresults. We also introduce the beanTech Anomaly Detection\nDataset1 (BTAD), a novel dataset of real-world, industry-\noriented images composed of both normal and defective\nproducts. The defective images have been pixel-wise manually\nlabeled with a ground-truth anomaly localization mask.\nA. Datasets\n• MNIST: MNIST dataset consists of 60K gray images\nof hand written digits. Although this dataset was not\noriginally developed for anomaly detection tasks, it has\noften been used as a baseline dataset, thus we used it\nto compare with other state-of-the-art approaches. For\ntraining, one class has been considered as normal, while\nall others as anomaly.\n• MVTec Dataset: It’s a real-world anomaly detection\ndataset. It contains 5,354 high-resolution color images\nof different textures and objects categories. It has normal\nand anomalous images which showcase 70 different types\nof anomalies of different real-world products. It contains\ngray scale images as well as RGB images. Gray scale\nimages are quite common in industrial scenarios. With\nthis dataset, all the images were ﬁrst scales to 550 ×550\npixels and then center cropped to 512 ×512pixels before\nbeing passed to the model.\n1http://avires.dimi.uniud.it/papers/btad/btad.zip\n• BTAD Dataset: It contains RGB images of three in-\ndustrial products. Product 1 is of 1600 ×1600 pixels,\nproduct 2 is of 600 ×600 and product 3 is of 800 ×600\npixels in size. Product 1, 2 and 3 have 400, 1000 and 399\ntrain images respectively. While training all the images\nwere ﬁrst scaled to 512 before passing to the model. For\neach anomalous image, a pixel-wise ground truth mask\nis given.\nWhile training, we fed our model using the normal class data\nonly. While testing, a combination of reconstruction losses and\nthe maximum of the log-likelihood loss are used to perform\nglobal anomaly detection, while the log-likelihood loss alone\nis used for anomaly localization. In this second case, we\nstored the log-likelihood loss for all the patch positions and\nthen upsample it using 2D bilinear-upsampling, to input image\nsize, to obtain the heatmap. Then we employed the PRO (Per\nRegion Overlap) [8], [24] as the evaluation metric for the\nMVTec and BTAD datasets. For computing PRO, heatmaps\nare ﬁrst thresholded at a given anomaly score to make the\nbinary decision for each pixel. Then the percentage of overlap\nwith the ground truth (GT) is computed. We followed the same\napproach as in [8], to ﬁnd the PRO value for a large number of\nincreasing thresholds until an average per-pixel positive rate\nof 30% is reached. For the MNIST dataset, we adopted AUC\n(area under ROC curve) as a performance metric in order to\nshow comparative results.\nThe hyper-parameters used in the training are show in\ntable I.\nAdams lr rate 0.0001\nWeight decay 0.0001\nBatch Size 8\nEpochs 400\nNo. of Gaussian’s 150\nPatch Dimension P = 64\nTABLE I\nTRAINING HYPERPARAMETERS\nB. Results\nBefore considering the problem of anomaly localization,\nwe tested our model on the MNIST dataset, which has been\nwidely used as a reference dataset for anomaly detection. In\nthis case, one class is selected as normal and anything else\nis considered anomalous. The anomalies are thus deﬁned at a\nglobal level, rather than being localized in speciﬁc, possibly\nsmall image patches as in the more challenging MVTec and\nBTAD datasets. For this reason, anomaly detection is per-\nformed only by using the global reconstruction losses, without\nmeasuring the localization output. The results are reported in\ntable II, where they are compared with the performances of\nother popular anomaly detection techniques (results taken from\n[19], [14]). As it can be seen, the proposed method almost\nalways outperforms the competitors.\nTable III shows the results results for MVTec dataset. The\nvalue shows the PRO curve up to an average false positive rate\nper-pixel of 30% is reported. It measures the average overlap\nClass OC\nSVM KDE DAE V AE\nPix\nCNN\nGAN\nLSA Deep\nSVDD\nPyr.\nAE VT-ADL\n0 0.988 0.885 0.991 0.994 0.531 0.993 0.98 0.995 0.99\n1 0.999 0.996 0.999 0.999 0.995 0.999 0.997 0.999 1\n2 0.902 0.71 0.89 0.96 0.478 0.959 0.917 0.941 0.976\n3 0.950 0.693 0.935 0.947 0.517 0.966 0.919 0.966 0.976\n4 0.955 0.844 0.921 0.965 0.739 0.956 0.949 0.960 0.984\n5 0.968 0.776 0.937 0.963 0.542 0.964 0.885 0.972 0.971\n6 0.978 0.861 0.981 0.995 0.592 0.994 0.983 0.993 0.995\n7 0.965 0.884 0.964 0.974 0.789 0.980 0.946 0.993 0.99\n8 0.853 0.669 0.841 0.905 0.340 0.953 0.939 0.895 0.974\n9 0.995 0.825 0.96 0.978 0.662 0.981 0.965 0.989 0.99\nMean 0.95 0.81 0.94 0.97 0.62 0.97 0.948 0.97 0.984\nTABLE II\nAUC RESULTS OF ANOMALY CLASSIFICATION USING MNIST, E ACH ROW SHOWS THE NORMAL CLASS OF THE TRAINED MODEL . COMPARATIVE\nRESULTS ARE TAKEN FROM [19], [14]\nCategory 1-NN OC\nSVM\nK\nMeans\nAE\nMSE V AE AE\nSSIM\nAno\nGAN\nCNN\nFeat.\nDic\nUni.\nStud.\nVT-ADL\n(Ours)\nCarpet 0.512 0.355 0.253 0.456 0.501 0.647 0.204 0.469 0.695 0.773\nGrid 0.228 0.125 0.107 0.582 0.224 0.849 0.226 0.183 0.819 0.871\nLeather 0.446 0.306 0.308 0.819 0.635 0.561 0.378 0.641 0.819 0.728\nTile 0.822 0.722 0.779 0.897 0.87 0.175 0.177 0.797 0.912 0.796\nWood 0.502 0.336 0.411 0.727 0.628 0.605 0.386 0.621 0.725 0.781\nBottle 0.898 0.85 0.495 0.91 0.897 0.834 0.62 0.742 0.918 0.949\nCable 0.806 0.431 0.513 0.825 0.654 0.478 0.383 0.558 0.865 0.776\nCapsule 0.631 0.554 0.387 0.862 0.526 0.86 0.306 0.306 0.916 0.672\nHazelnut 0.861 0.616 0.698 0.917 0.878 0.916 0.698 0.844 0.937 0.897\nMetal Nut 0.705 0.319 0.351 0.83 0.576 0.603 0.32 0.358 0.895 0.726\nPill 0.725 0.544 0.514 0.893 0.769 0.83 0.776 0.46 0.935 0.705\nScrew 0.604 0.644 0.55 0.754 0.559 0.887 0.466 0.277 0.928 0.928\nToothbrush 0.675 0.538 0.337 0.822 0.693 0.784 0.749 0.151 0.863 0.901\nTransistor 0.68 0.496 0.399 0.728 0.626 0.725 0.549 0.628 0.701 0.796\nZipper 0.512 0.355 0.253 0.839 0.549 0.665 0.467 0.703 0.933 0.808\nMeans 0.64 0.479 0.423 0.79 0.639 0.694 0.443 0.515 0.857 0.807\nTABLE III\nCOMPARATIVE RESULTS ON MVT EC DATASET . COMPARATIVE RESULTS TAKEN FROM [8].\nPrdt\nPRO\nScore\nours\nPR\nAUC\nours\nAE\nMSE\nAE\nMSE+SSIM\n0 0.92 0.99 0.49 0.53\n1 0.89 0.94 0.92 0.96\n2 0.86 0.77 0.95 0.89\nMean 0.89 0.90 0.78 0.79\nTABLE IV\nRESULTS ON BTAD DATASET. WE ALSO COMPARE OUR PR-AUC WITH\nTHE RESULTS OF CONVOLUTIONAL AUTOENCODERS TRAINED WITH MSE\nLOSS AND MSE+SSIM LOSS .\nof each ground truth region with the predicted anomaly region\nfor multiple thresholds. Our proposed methods performed at\npar with the most recent state of the art algorithms (results\ntaken from [8]), and even outperformed them in 7 product\ncategories. for our newly published BTAD dataset, we are also\nreporting the ﬁrst results in table IV with the similar model\nconﬁguration as of MVTec. For comparison we also report\nPR-AUC of a basic convolutional autoencoder on BTAD with\nMSE and MSE+SSIM loss.\nFig. 4. Plot shows the PRO score for the different no of Gaussians used in\nthe Gaussian approximation.\nC. Gaussian mixture model tuning\nHere we justify the choice of number of Gaussians for our\nmixture model. For this we trained on MVTec dataset with\nincreasing number of Gaussians and calculated the PRO-score\n(ﬁg.4). we found that with increasing number of Gaussians,\nPRO-score increases and then becomes constant. We also tried\nto see effect of noise addition in the transformer encoded\nfeatures for generalisation. With noise added, PRO score\nwith 150 Gaussians is 0.897 in contrast to 0.807 without\nnoise. Hence, noise addition actually helps in generalizing the\nlearning procedure.\nVI. C ONCLUSIONS\nWe proposed a transformer-based framework which uses\nreconstruction and patch-based learning for image anomaly\ndetection and localization. The anomalies can be detected at\na global level using a reconstruction-based approach, and can\nbe localized with the application of a Gaussian mixture model\napplied to the encoded image patches. The achieved results\nare at par with or outperform other state-of-the-art techniques.\nWe also published BTAD, a real world industrial dataset for\nthe anomaly detection task.\nREFERENCES\n[1] C. Piciarelli, C. Micheloni, and G. L. Foresti, “Trajectory-based anoma-\nlous event detection,” IEEE Transaction on Circuits and Systems for\nVideo Technology, vol. 18, no. 11, pp. 1544–1554, 2008.\n[2] C. Piciarelli, D. Avola, D. Pannone, and G. L. Foresti, “A vision-based\nsystem for internal pipeline inspection,”IEEE Transactions on Industrial\nInformatics, vol. 15, no. 6, pp. 3289–3299, 2019.\n[3] P. Chen, S. Yang, and J. A. McCann, “Distributed real-time anomaly\ndetection in networked industrial sensing systems,” IEEE Transactions\non Industrial Electronics, vol. 62, no. 6, pp. 3832–3842, 2015.\n[4] P. Napoletano, F. Piccoli, and R. Schettini, “Anomaly detection in\nnanoﬁbrous materials by cnn-based self-similarity,” Sensors, vol. 18,\nno. 1, p. 209, 2018.\n[5] X. Ma, Y . Niu, L. Gu, Y . Wang, Y . Zhao, J. Bailey, and\nF. Lu, “Understanding adversarial attacks on deep learning based\nmedical image analysis systems,” Pattern Recognition , vol. 110,\np. 107332, 2021. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0031320320301357\n[6] P. Yu and X. Yan, “Stock price prediction based on deep neural\nnetworks,”Neural Computing and Applications, vol. 32, no. 6, pp. 1609–\n1628, 2020.\n[7] D. Wulsin, J. Blanco, R. Mani, and B. Litt, “Semi-supervised anomaly\ndetection for eeg waveforms using deep belief nets,” in 2010 Ninth\nInternational Conference on Machine Learning and Applications, 2010,\npp. 436–441.\n[8] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “Uninformed\nstudents: Student-teacher anomaly detection with discriminative latent\nembeddings,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 4183–4192.\n[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” 2020.\n[10] C. M. Bishop, Mixture density networks. Aston University, 1994.\n[11] N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton, “Split and\nmerge em algorithm for improving gaussian mixture density estimates,”\nin Neural Networks for Signal Processing VIII. Proceedings of the 1998\nIEEE Signal Processing Society Workshop (Cat. No. 98TH8378). IEEE,\n1998, pp. 274–283.\n[12] P. Mishra, C. Piciarelli, and G. L. Foresti, “A neural network for image\nanomaly detection with deep pyramidal representations and dynamic\nrouting.” International Journal of Neural Systems, vol. 30, no. 10, pp.\n2 050 060–2 050 060, 2020.\n[13] W. Liu, R. Li, M. Zheng, S. Karanam, Z. Wu, B. Bhanu, R. J. Radke,\nand O. Camps, “Towards visually explaining variational autoencoders,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 8642–8651.\n[14] P. Mishra, C. Piciarelli, and G. L. Foresti, “Image anomaly detection\nby aggregating deep pyramidal representations,” in 25th International\nConference on Pattern Recognition (ICPR), Industrial Machine Learning\nWorkshop, 2021.\n[15] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning. MIT Press,\n2016, http://www.deeplearningbook.org.\n[16] P. Baldi, “Autoencoders, unsupervised learning, and deep architectures,”\nin Proceedings of ICML workshop on unsupervised and transfer learn-\ning, 2012, pp. 37–49.\n[17] M. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli, “Adversarially\nlearned one-class classiﬁer for novelty detection,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 3379–3388.\n[18] S. Pidhorskyi, R. Almohsen, D. A. Adjeroh, and G. Doretto, “Generative\nprobabilistic novelty detection with adversarial autoencoders,” arXiv\npreprint arXiv:1807.02588, 2018.\n[19] D. Abati, A. Porrello, S. Calderara, and R. Cucchiara, “Latent space\nautoregression for novelty detection,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp. 481–\n490.\n[20] S. Venkataramanan, K.-C. Peng, R. V . Singh, and A. Mahalanobis, “At-\ntention guided anomaly localization in images,” in European Conference\non Computer Vision. Springer, 2020, pp. 485–503.\n[21] P. Perera, R. Nallapati, and B. Xiang, “Ocgan: One-class novelty detec-\ntion using gans with constrained latent representations,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2019, pp. 2898–2906.\n[22] C. Piciarelli, P. Mishra, and G. L. Foresti, “Image anomaly detection with\ncapsule networks and imbalanced datasets,” in International Conference\non Image Analysis and Processing. Springer, 2019, pp. 257–267.\n[23] P. Perera and V . M. Patel, “Learning deep features for one-class\nclassiﬁcation,” IEEE Transactions on Image Processing, vol. 28, no. 11,\npp. 5450–5463, 2019.\n[24] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, “Mvtec ad–a\ncomprehensive real-world dataset for unsupervised anomaly detection,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 9592–9600.\n[25] C. Aytekin, X. Ni, F. Cricri, and E. Aksu, “Clustering and unsupervised\nanomaly detection with l 2 normalized deep auto-encoder represen-\ntations,” in 2018 International Joint Conference on Neural Networks\n(IJCNN). IEEE, 2018, pp. 1–6.\n[26] S. M. Erfani, S. Rajasegarar, S. Karunasekera, and C. Leckie, “High-\ndimensional and large-scale anomaly detection using a linear one-class\nsvm with deep learning,” Pattern Recognition, vol. 58, pp. 121–134,\n2016.\n[27] Z. Ghafoori and C. Leckie, “Deep multi-sphere support vector data\ndescription,” in Proceedings of the 2020 SIAM International Conference\non Data Mining. SIAM, 2020, pp. 109–117.\n[28] R. Chalapathy, A. K. Menon, and S. Chawla, “Anomaly detection using\none-class neural networks,” arXiv preprint arXiv:1802.06360, 2018.\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, 2017, pp. 6000–6010.\n[30] P. Bergmann, S. L ¨owe, M. Fauser, D. Sattlegger, and C. Steger, “Improv-\ning unsupervised defect segmentation by applying structural similarity\nto autoencoders,” in International joint conference on computer vision,\nimaging and computer graphics theory and applications, 2019.\n[31] G. An, “The effects of adding noise during backpropagation training on\na generalization performance,” Neural Computation, vol. 8, no. 3, pp.\n643–674, 1996."
}