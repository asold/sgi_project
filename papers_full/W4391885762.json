{
  "title": "DataAgent: Evaluating Large Language Models’ Ability to Answer Zero-Shot, Natural Language Queries",
  "url": "https://openalex.org/W4391885762",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4288673713",
      "name": "Mishra, Manit",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Braham, Abderrahman",
      "affiliations": [
        "University of Sousse"
      ]
    },
    {
      "id": null,
      "name": "Marsom, Charles",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": null,
      "name": "Chung, Bryan",
      "affiliations": [
        "Windsor Dermatology"
      ]
    },
    {
      "id": null,
      "name": "Griffin, Gavin",
      "affiliations": [
        "Bellarmine University"
      ]
    },
    {
      "id": null,
      "name": "Sidnerlikar, Dakshesh",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": null,
      "name": "Sarin, Chatanya",
      "affiliations": [
        "Bellarmine University"
      ]
    },
    {
      "id": null,
      "name": "Rajaram, Arjun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3139250374",
    "https://openalex.org/W4213074563",
    "https://openalex.org/W2899709844",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4308244910"
  ],
  "abstract": "Conventional processes for analyzing datasets and extracting meaningful\\ninformation are often time-consuming and laborious. Previous work has\\nidentified manual, repetitive coding and data collection as major obstacles\\nthat hinder data scientists from undertaking more nuanced labor and high-level\\nprojects. To combat this, we evaluated OpenAI's GPT-3.5 as a \"Language Data\\nScientist\" (LDS) that can extrapolate key findings, including correlations and\\nbasic information, from a given dataset. The model was tested on a diverse set\\nof benchmark datasets to evaluate its performance across multiple standards,\\nincluding data science code-generation based tasks involving libraries such as\\nNumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in\\ncorrectly answering a given data science query related to the benchmark\\ndataset. The LDS used various novel prompt engineering techniques to\\neffectively answer a given question, including Chain-of-Thought reinforcement\\nand SayCan prompt engineering. Our findings demonstrate great potential for\\nleveraging Large Language Models for low-level, zero-shot data analysis.\\n",
  "full_text": "DataAgent: Evaluating Large Language Models’\nAbility to Answer Zero-Shot, Natural Language\nQueries\nManit Mishra\nIrvington High School\nFremont, United States\nmshmanit@gmail.com\nAbderrahman Braham\nPioneer High School\nSousse, Tunisia\nbr.abderrahman.contact@gmail.com\nCharles Marsom\nDavis Senior High School\nDavis, United States\ncharleshenrymarsom@gmail.com\nBryan Chung\nThe Loomis Chaffee School\nWindsor, United States\nbryan chung@loomis.org\nGavin Griffin\nBellarmine College Preparatory\nSunnyvale, United States\ngavgriffin563@gmail.com\nDakshesh Sidnerlikar\nRutgers University\nNew Brunswick, United States\ndakshesh.sid@gmail.com\nChatanya Sarin\nBellarmine College Preparatory\nSunnyvale, United States\nchatanya.sarin@gmail.com\nArjun Rajaram\nUniversity of Maryland, College Park\nFrisco, United States\narajara1@terpmail.umd.edu\nAbstract—Conventional processes for analyzing datasets and\nextracting meaningful information are often time-consuming\nand laborious. Previous work has identified manual, repetitive\ncoding and data collection as major obstacles that hinder data\nscientists from undertaking more nuanced labor and high-level\nprojects. To combat this, we evaluated OpenAI’s GPT-3.5 as\na “Language Data Scientist” (LDS) that can extrapolate key\nfindings, including correlations and basic information, from a\ngiven dataset. The model was tested on a diverse set of bench-\nmark datasets to evaluate its performance across multiple\nstandards, including data science code-generation based tasks\ninvolving libraries such as NumPy, Pandas, Scikit-Learn, and\nTensorFlow, and was broadly successful in correctly answering\na given data science query related to the benchmark dataset.\nThe LDS used various novel prompt engineering techniques\nto effectively answer a given question, including Chain-of-\nThought reinforcement and SayCan prompt engineering. Our\nfindings demonstrate great potential for leveraging Large Lan-\nguage Models for low-level, zero-shot data analysis.\nIndex Terms—GPT, data science, natural language pro-\ncessing, large language model, data processing, RefleXion,\nChain-of-Thought, SayCan, action plan generation, zero-shot\nprompting, plain language\nI. I NTRODUCTION\nWith a rampant increase in the demand for data pro-\ncessing, specific interest in quickly extrapolating key\nconnections in datasets has grown exponentially. How-\never, the ability of data scientists to meet these demands\nis waning; though the average number of data scientists\nwithin a company has grown from 28 to 50 in the last\n9 years [1], and is only expected to continue increasing,\nthat trend cannot compensate for the exponential levels\nof growth in demand. Put simply, such an increase is\nunsustainable as interest in processing large amounts of\ndata skyrockets. However, much of the field consists of\nconducting relatively simple tasks; namely, the act of\nuncovering patterns and correlations. Data science tasks\nare often synonymous with repetitive labor, taking much\nlonger than the growing industry desires - analyzing\ndatasets for hidden patterns can be a tedious task, making\nit a prime candidate for automation through machine\nlearning (ML) techniques. This study aims to examine\nthe efficacy of Large Language Models (LLMs), when\npaired with an action loop and prompting framework,\nfor analyzing datasets to accomplish various data science\ntasks.\nSignificant work already exists in this field. Automatic\nPrompt Engineers (APEs) have proved useful in demon-\nstrating the power of LLMs to extrapolate correlatory\ndata; when given a set of inputs, APEs are able to\nidentify the most likely “instruction” for the specific set\nof inputs [2]. However, in their current form, APEs have\nonly been identified to work as instruction generators,\nrather than generators for a roadmap of how to complete\na task given an inputted dataset and a human-engineered\nprompt.\nAutoML models - a general class of interfaces de-\nsigned to bring ML models to non-ML experts - have\nalso provided another foray into this space [3]. However,\nAutoML focuses on building deep learning systems (and\nother high-level tasks like hyperparameter optimization)\nwithout human input (and other high-level tasks like\nhyperparameter optimization), while our LLM workflow\nis far more flexible for completing zero-shot data sci-\nence tasks with specific human instruction. Additionally,\na major weakness of AutoML models concerns their\nprompting; natural-language queries are impractical for\nmodels like Auto-PyTorch [4]. Unlike Large Language\narXiv:2404.00188v1  [cs.CL]  29 Mar 2024\nModels (LLMs) that are specifically designed to process,\nunderstand, and generate human-like text, AutoML mod-\nels primarily focus on automated machine learning tasks,\nsuch as model selection and hyperparameter tuning. For\nexample, Auto-PyTorch can’t easily understand a ques-\ntion in plain English, unlike Large Language Models\n(LLMs). This becomes a prominent issue when users\nwho aren’t experts in machine learning need to use\nthese systems. This poses a challenge because AutoML\nmodels can’t handle conversational language.\nFig. 1. Some differences between common AutoML models and the Language\nData Scientist.\nThough existing efforts into improving the accessi-\nbility of ML models for non-ML experts are generally\nwell-supported, such efforts are rarely directed towards\nameliorating direct, user-generated queries in the field\nof data science. We have approached this issue using\na 3-stage application that heavily integrates GPT lan-\nguage modeling. Upon being prompted with an input\ndataset and a query related to that dataset, the Language\nData Scientist (LDS) will first gather basic background\ninformation on the data using Pandas functions, such\nas pandas.DataFrame.head(), pandas.DataFrame.info(),\nand pandas.DataFrame.describe(). After acquiring the\nnecessary background information, a GPT model will be\nprompted to create an “action plan,” which will generate\na list of plain-language steps to complete the given task.\nThese steps build on top of each other until finally reach-\ning the final answer to the query. Next, the LDS will use\nthose tasks as a guide to generate lines of code, which\nwill then be run on a low-level executor to complete the\ntask, outputting the answer to the original query.\nTo measure the model’s accuracy, we generated a set\nof 15 benchmark datasets along with a corresponding\nset of manually-created questions and answers for each\ndataset. Through refinement, reinforcement, and itera-\ntion using novel mechanisms - like Chain-of-Thought\nPrompting and SayCan prompt engineering - the LDS’s\nperformance was able to significantly improve [5] [6]\n[7].\nThe remainder of this paper is organized as follows:\nSection II provides both a high-level summary and de-\ntailed description of the methodology and workflow,\nin addition to underscoring metrics for measuring the\nmodel’s accuracy, reinforcement, and efficacy. Section\nIII elucidates overall results, Section IV offers a com-\nprehensive discussion and conclusion of our work, and\nSection V gives summary acknowledgments.\nII. M ETHODOLOGY\nA. Summary\nBroadly, the methodology for evaluating the LDS’s\nperformance consists of three phases: taking in a query\nand gathering background on a given dataset, formulat-\ning a natural-language action plan with the GPT-based\naction plan generator (AcPG), and systematically feed-\ning the action plan’s steps into the LDS for it to execute,\neventually determining a final output. For accuracy mea-\nsurement, a set of the LDS’s predicted outputs for a set\nof queries is compared to a set of manually calculated\nand supervised answers for a given benchmark dataset,\nthus generating an accuracy score.\n1) Benchmark Datasets [8]: The LDS is primarily eval-\nuated on 15 benchmark datasets: sets of entries generated\nusing GPT-3.5, ranging from 50 to 300 rows, with differ-\nent columns consisting of both numerical and categorical\ndata. To best simulate a wide range of data (as would be\nexpected in diverse work environments), the benchmark\ndatasets cover different areas of common data entry, like\nuser phone numbers, names of people, and revenue on\nadvertisements. The datasets were categorized into three\nsize groups: small, medium, and large. Small datasets\nconsist of fewer than 100 rows, while medium datasets\nrange between 100 and 200 rows, and large datasets\nencompass more than 200 rows. Ground truths for the\nanswers to the questions related to each of the datasets\nwere calculated by hand, using Pandas, NumPy, and\nvarious other machine learning libraries.\n2) Queries: Each benchmark dataset was paired with a\nset of 15 questions with varying levels of difficulty; an\neasy question might ask for the number of rows of a Toy\nDataset, while a more difficult question might ask about\nusing linear regression to predict a value of the dataset.\nA sample benchmark dataset is shown in Figure 2.\nUnless otherwise specified, queries were judged on\nwhether the LDS was able to correctly answer a question,\nin full, without any additional prompting. Any numerical\nanswers with extremely negligible rounding errors (less\nthan 0.001 percent off from the ground truth) were\nmarked as correct.\nCity Temp Humidity Wind Clouds\nNew York 25.3 60.0 10.9 PartShade\nLos Angeles 30.3 50.7 Sun\nBeijing 32.7 45.2 10.0 PartShade\nParis 75.8 10.8 Shade\nSao Paulo 35.9 60.1 12.2\nMoscow 15.9 55.6 6.4 Shade\nDubai 40.5 20.7 25.3 Sun\nSingapore 30.5 15.0 PartShade\nMumbai 35 70 10 Sun\nFig. 2. A chunk of a sample benchmark dataset, labeled ”Cities,” containing\nboth numerical and categorical data, along with missing values. The original\ndataset was medium-sized and had 165 rows.\nB. Gathering Background Information on a Dataset\nBefore an action plan is generated for a given dataset,\nthe LDS gathers basic background information on the in-\nputted data as a part of its context. Our original intention\nwas to feed the entire dataset into the OpenAI API, but\nthis approach was not feasible due to token and query\nlimitations set in place by the API [9]. As an alternative,\nwhen a query is presented for a dataset, the LDS uses\nGPT-3.5 to generate the code to retrieve any preliminary\ninformation needed to answer the question at hand.\nAlong with the background information needed to solve\nthe query, some general information about the dataset us-\ning Pandas functions (such as pandas.DataFrame.head()\nand pandas.DataFrame.info()) are stored as context. This\ncode is then executed and the result is stored in a Python\ndictionary to be passed as context to the AcPG. The\ndictionary acts as a way to store the code used to generate\nthe context, along with the context itself.\nC. Action Plan Generation\n1) Specifics: The primary portion of the model is a\nGPT-dependent Action Plan Generator (AcPG), a plain-\nlanguage roadmap that will be interpreted and fed into\nthe LDS. The AcPG is given basic information about\nthe dataset as well as preliminary information previously\ngenerated in past queries, which were all stored in a\ndictionary. To improve accuracy and help generate action\nplans, the AcPG utilizes Chain of Thought reasoning [6],\na technique where a complex task is broken down into a\nsequence of smaller, explainable steps chained together.\nFinally, the list of smaller steps is converted into an array\nto be parsed by the low-level executor.\n2) SayCan: The prompts provided to the AcPG are\nstructured based on the SayCan framework [7], with\ndistinct sections outlining the situation context, desired\nresponse action, declared capabilities, and stipulated\nneeds. While going through the AcPG, GPT-3.5 is in-\nstructed to provide code snippets without executing\nthem, focusing solely on preparing the steps required\nfor data analysis. This approach directly addresses the\ndeclared capabilities and stipulated needs of our system.\nAs the process unfolds, the context is updated with\neach step’s output, gradually building towards executing\nthe final step that directly answers the initial question.\nThis incremental building of context and action, guided\nby the SayCan framework, ensures that each step is\npurposeful and directly contributes to solving the larger\nquery. When compared with traditional natural language\nprompting, SayCan-formulated prompts assist in dras-\ntically improving the AcPG outputs that work with the\nremaining portion of the LDS. Using SayCan, we struc-\ntured the answers to suit our purpose of passing the code\nthrough the low-level executor.\nFig. 3. An example of how natural language steps generated by the AcPG\nare then translated to code for the executor\nD. Low-Level Execution\nOnce the AcPG generates a sufficient plain-language\naction plan, along with the specific code from libraries\nsuch as NumPy and Pandas necessary to implement that\nplan, the code will be run on a local low-level executor\nwhich calculates a numerical or categorical response to\nthe original query.\nFig. 4. A broad overview of the model. A natural-language query, paired with\nan inputted dataset, is sent both to the AcPG and the LDS. The AcPG then\ngenerates a plan of action for answering the question with the given data, and\nan executor in the LDS computes the final output.\nE. Benchmark Dataset Answer Checker\nIn addition to developing the primary model, we also\ncreated an answer checker tool to facilitate usage of\nour benchmark datasets. This tool is designed to assist\nusers in verifying the accuracy of their responses when\nutilizing our datasets for their own research or practice.\nTo construct this tool, we first compiled comprehen-\nsive dictionaries for each benchmark dataset, cataloging\nall associated questions and their corresponding correct\nanswers. When a user submits an answer, the tool re-\ntrieves the question along with the correct response from\nthe dictionaries. It then employs GPT-4, along with a\npredefined margin of error, to assess the accuracy of\nthe user’s answer. This approach ensures a reliable and\nefficient means of verification, enabling users to gauge\nthe correctness of their responses effectively.\nThe incorporation of the GPT-4 API in this process\nadds an additional layer of sophistication, allowing the\ntool to handle a range of answer types and nuances.\nThe margin of error parameter provides flexibility, ac-\ncommodating slight variations in answers that are still\nfundamentally correct.\nIII. R ESULTS\nWe tested the LDS’s ability to accurately extrapolate\ninformation within numerical and categorical datasets of\nthree sizes: small, medium, and large, and questions of\nvarying difficulty, language, and style.\nOverall, the LDS answered 74 out of 225 questions\ncorrectly, for an accuracy of 32.89 percent. There was a\nlot of variation in the accuracy rates between Toy Dataset\nsizes; all individual datasets had accuracies between 20\nand 60 percent, and no size or question difficulty level\npresented an anomaly concerning accuracy. The LDS\nperformed best on Large Benchmark Datasets, answer-\ning 36 percent (27 of 75) questions correctly. A more\ndetailed breakdown of the results is shown in Figure 5.\nSmall Medium Large\nCorrect Queries 25 22 27\nTotal Queries 75 75 75\nPercent Correct 33.33 29.33 36\nFig. 5. A summary of our results, which were generally stable across different\nToy Dataset sizes and queries of varying difficulty.\nAs illustrated in Figure 5, minimal variations in model\naccuracy are observed across various dataset sizes. The\nLDS consistently demonstrates its performance when\nexposed to datasets containing varying numbers of rows,\nspanning from 50 to 300.\nOut of the 74 incorrect responses, a notable portion\nwas attributed to two primary issues encountered dur-\ning the query processing. Firstly, there were instances\nwhere the GPT model generated incorrect code, involv-\ning variables that did not exist in the dataset and calling\nfunctions that are not found in Pandas or NumPy. This\nled to responses that were not only incorrect but also not\nexecutable within the framework of our established data\nanalysis environment.\nFig. 6. Model’s Accuracy by Dataset Size\nSecondly, we faced challenges with the token limits\nimposed by the GPT API. The limit of 4096 tokens re-\nstricted our ability to provide comprehensive context for\nlarger datasets. This limitation mainly impacted ques-\ntions that required large amounts of data as context.\nIV. D ISCUSSION AND CONCLUSION\nTo better understand the feasibility of using Large\nLanguage Models (LLMs) to find answers for zero-\nshot queries, specifically in the field of data science, we\ncreated and evaluated a sample ”LLM Data Scientist”\n(LDS) that utilized an action plan generator and vari-\nous methods of reinforcement. Our study demonstrated\na large amount of promise in using Large Language\nModels to accurately perform low-level data analysis,\nboth for numerical and categorical datasets. Below is a\nbrief discussion of notable points of our workflow and\nits results, along with our takeaways and future plans.\nA. Prompt Rewording\nIn any instance where the original question had to\nbe altered to produce correct results, the query was\nmarked as incorrect for accuracy purposes. However,\nmost of these questions produced correct results with\nminor tweaking; as an example, questions with incorrect\nanswers that asked for “results” often produced correct\nresponses when “overall results” was used instead.\nFurther work on the benchmark datasets could involve\na revision of the questions to enhance their clarity and\nspecificity. By refining the phrasing and structure of the\nquestions, we could potentially reduce ambiguity and\nimprove the model’s ability to comprehend and accu-\nrately respond to them.\nB. Multiple Answers in One Prompt\nUnexpectedly, the LDS often failed to produce com-\nplete answers for queries that required multiple outputs.\nMost commonly, the model correctly produced one an-\nswer to the question but left the rest incomplete. These\nresults were marked as incorrect, but when the questions\nwith multiple parts were rephrased into multiple ques-\ntions asking for one answer and re-fed into the LDS, the\nmodel generally produced correct results.\nC. Edge Cases\nWhen prompted with questions that didn’t fully apply\nto the context of a dataset, the LDS frequently produced\nincorrect results as a result of flawed action plan gen-\neration. This is consistent with popular findings con-\ncerning GPT-model-based generators. Frequently, this\nmanifested in questions that asked for the median value\nfor a categorical (non-numeric) dataset, or in questions\nthat asked for the column with the greatest number of\nmissing values for a dataset that contained no missing\nvalues; in this case, the LDS often incorrectly returned\nthe first column in the dataset.\nD. Computing Details and Future Plans\nThe relative simplicity of the LDS’s structure greatly\ninfluenced the types of questions it was able to an-\nswer. Though the LDS had shortcomings when given\ncomplicated, multi-prompt queries, the model excelled\nin drawing simple non-obvious connections, data, and\nconclusions from various Toy Datasets.\nThe restrictions on the LDS’s ability to execute com-\nplex queries may also be a result of the GPT model\nused for the AcPG. We relied on OpenAI’s GPT-3.5, a\nsomewhat inferior and outdated model when compared\nto GPT-4 and other cutting-edge GPT actors. This deci-\nsion was mainly limited by our budget.\nConsidering the vastly improved computing power of\nnovel models when compared to GPT-3.5, we anticipate\nthe AcPG to produce more accurate results when able to\nharness more capable LLMs; GPT-4 is 82 percent less\nlikely to produce factual errors [10], and includes much\ngreater capabilities for recognizing nuance, context, and\ncomplex, multi-part instructions in prompts. Specifically\nwith regard to this increased capability for multi-part\nprompts, we anticipate that using a GPT-4-based AcPG\nwould allow the LDS to accurately evaluate questions\nthat ask for multiple answers, one of the main drawbacks\nof the current LDS.\nAdditionally, we plan for the AcPG to incorporate\nrefleXion, a prompt engineering technique that improves\nmodel robustness through linguistic feedback rather than\nweight modification. During training, we will implement\na separate refleXion prompt used to generate targeted\nlinguistic adversarial examples that expose biases, over-\nsights, and flaws in the AcPG’s reasoning. This prompt\nwill run concurrently with the orignal query. The probing\nexamples given will then be provided as input to the\nAcPG in successive rounds of generation, pushing it to\nconfront its own limitations as a means of reinforce-\nment. To enable the model to learn from this linguistic\nfeedback over time, we will also an episodic memory\nbuffer that stores examples along with the main model’s\nresponses.\nFurthermore, while our study has demonstrated the ef-\nficacy of Large Language Models (LLMs) for data anal-\nysis across a range of dataset sizes, a natural progression\nfor future research would be to explore the performance\nof these models on even larger datasets. This exploration\nis crucial as it can reveal how scalable these models are\nand whether their performance is maintained or even\nenhanced when dealing with larger volumes of data.\nThe investigation into larger dataset sizes could po-\ntentially unveil unique challenges and opportunities. For\ninstance, larger datasets might introduce more complex\npatterns or noise, which could test the limits of the\nmodel’s analytical capabilities. Conversely, they could\nalso provide a richer context for the model to learn from,\npossibly leading to more accurate or nuanced analy-\nses. Additionally, assessing the performance on larger\ndatasets would provide valuable insights into the compu-\ntational efficiency and resource utilization of these mod-\nels, which is a critical factor in real-world applications.\nV. A CKNOWLEDGMENTS\nWe would like to thank our mentor, Arjun Rajaram,\nfor his invaluable guidance and support. We also like to\nthank OpenAI for providing access to their API, which\nplayed a crucial role in the development and testing of\nour workflow.\nVI. A BBREVIATIONS\nLLM, Large Language Model; LDS, LLM Data Sci-\nentist; AcPG, Action Plan Generator; APEs, Automatic\nPrompt Engineers.\nREFERENCES\n[1] “How much do data scientists earn in germany in 2022,” 2022.\n[2] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba,\n“Large language models are human-level prompt engineers,” 2022.\n[3] B. Liu, “A very brief and critical discussion on automl,” 2018.\n[4] L. Zimmer, M. Lindauer, and F. Hutter, “Auto-pytorch tabular: Multi-\nfidelity metalearning for efficient and robust autodl,” 2020.\n[5] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and\nS. Yao, “Reflexion: Language agents with verbal reinforcement learn-\ning,” 2023.\n[6] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\nQ. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in\nlarge language models,” 2022.\n[7] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho,\nJ. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey,\nS. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y . Kuang, K.-H. Lee,\nS. Levine, Y . Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao,\nJ. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev,\nV . Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, “Do\nas i can, not as i say: Grounding language in robotic affordances,” 2022.\n[8] M. Mishra, A. Braham, C. Marsom, B. Chung, G. Griffin, D. Sidnerlikar,\nC. Sarin, and A. Rajaram, “Dataagent benchmark files,” 2023.\n[9] OpenAI, “Gpt-4 technical report,” 2023.\n[10] “Gpt-4,” 2023.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8250042200088501
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7438108921051025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5439654588699341
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.5268401503562927
    },
    {
      "name": "Natural language",
      "score": 0.5057356357574463
    },
    {
      "name": "Source code",
      "score": 0.4912664294242859
    },
    {
      "name": "Natural language processing",
      "score": 0.48114287853240967
    },
    {
      "name": "Language model",
      "score": 0.45325759053230286
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4248698353767395
    },
    {
      "name": "Information retrieval",
      "score": 0.4132651090621948
    },
    {
      "name": "Code (set theory)",
      "score": 0.41090402007102966
    },
    {
      "name": "Data science",
      "score": 0.37766146659851074
    },
    {
      "name": "Data mining",
      "score": 0.3222547471523285
    },
    {
      "name": "Programming language",
      "score": 0.13807642459869385
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}