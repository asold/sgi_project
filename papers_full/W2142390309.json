{
  "title": "Bayesian Learning of a Language Model from Continuous Speech",
  "url": "https://openalex.org/W2142390309",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2158355418",
      "name": "Masato Mimura",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A370184058",
      "name": "Shinsuke Mori",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2137932240",
      "name": "Tatsuya Kawahara",
      "affiliations": [
        "Kyoto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W397920122",
    "https://openalex.org/W17921661",
    "https://openalex.org/W2069712814",
    "https://openalex.org/W4237938692",
    "https://openalex.org/W2088857750",
    "https://openalex.org/W175734176",
    "https://openalex.org/W2114347655",
    "https://openalex.org/W30845872",
    "https://openalex.org/W2148876114",
    "https://openalex.org/W2123815913",
    "https://openalex.org/W2140991203",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W2087309226",
    "https://openalex.org/W1895481600",
    "https://openalex.org/W2122598882",
    "https://openalex.org/W2074546930",
    "https://openalex.org/W2126377586",
    "https://openalex.org/W1975638594",
    "https://openalex.org/W2020999234",
    "https://openalex.org/W2090361527",
    "https://openalex.org/W1975113979",
    "https://openalex.org/W1610605641",
    "https://openalex.org/W2033436836",
    "https://openalex.org/W1991274470",
    "https://openalex.org/W2011238950",
    "https://openalex.org/W2169895393",
    "https://openalex.org/W2799033464",
    "https://openalex.org/W2785552256",
    "https://openalex.org/W1904457459",
    "https://openalex.org/W2146939522",
    "https://openalex.org/W3140968660",
    "https://openalex.org/W1606268232",
    "https://openalex.org/W2097931979",
    "https://openalex.org/W89093025",
    "https://openalex.org/W2107917162",
    "https://openalex.org/W2005902041",
    "https://openalex.org/W2107469355",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W1779834323",
    "https://openalex.org/W2009475852",
    "https://openalex.org/W1548450879"
  ],
  "abstract": "We propose a novel scheme to learn a language model (LM) for automatic speech recognition (ASR) directly from continuous speech. In the proposed method, we first generate phoneme lattices using an acoustic model with no linguistic constraints, then perform training over these phoneme lattices, simultaneously learning both lexical units and an LM. As a statistical framework for this learning problem, we use non-parametric Bayesian statistics, which make it possible to balance the learned model's complexity (such as the size of the learned vocabulary) and expressive power, and provide a principled learning algorithm through the use of Gibbs sampling. Implementation is performed using weighted finite state transducers (WFSTs), which allow for the simple handling of lattice input. Experimental results on natural, adult-directed speech demonstrate that LMs built using only continuous speech are able to significantly reduce ASR phoneme error rates. The proposed technique of joint Bayesian learning of lexical units and an LM over lattices is shown to significantly contribute to this improvement.",
  "full_text": "614\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nPAPER\nBayesian Learning of a Language Model from Continuous Speech\nGraham NEUBIG†a), Masato MIMURA†, Shinsuke MORI†, Nonmembers, and Tatsuya KAWAHARA†, Member\nSUMMARY We propose a novel scheme to learn a language model\n(LM) for automatic speech recognition (ASR) directly from continuous\nspeech. In the proposed method, we ﬁrst generate phoneme lattices us-\ning an acoustic model with no linguistic constraints, then perform train-\ning over these phoneme lattices, simultaneously learning both lexical units\nand an LM. As a statistical framework for this learning problem, we use\nnon-parametric Bayesian statistics, which make it possible to balance the\nlearned model’s complexity (such as the size of the learned vocabulary) and\nexpressive power, and provide a principled learning algorithm through the\nuse of Gibbs sampling. Implementation is performed using weighted ﬁnite\nstate transducers (WFSTs), which allow for the simple handling of lattice\ninput. Experimental results on natural, adult-directed speech demonstrate\nthat LMs built using only continuous speech are able to signiﬁcantly re-\nduce ASR phoneme error rates. The proposed technique of joint Bayesian\nlearning of lexical units and an LM over lattices is shown to signiﬁcantly\ncontribute to this improvement.\nkey words: language modeling, automatic speech recognition, Bayesian\nlearning, weighted ﬁnite state transducers\n1. Introduction\nA language model (LM) is an essential part of automatic\nspeech recognition (ASR) systems, providing linguistic con-\nstraints on the recognizer and helping to resolve the ambigu-\nity inherent in the acoustic signal. Traditionally, these LMs\nare learned from digitized text, preferably text that is similar\nin style and content to the speech that is to be recognized.\nIn this paper, we propose a new paradigm for LM learn-\ning, using not digitized text but audio data of continuous\nspeech. The proposition of learning an LM from continu-\nous speech is motivated from a number of viewpoints. First,\nthe properties of written and spoken language are very dif-\nferent [1], and LMs learned from continuous speech can\nbe expected to naturally model spoken language, removing\nthe need to manually transcribe speech or compensate for\nthese di ﬀerences when creating an LM for ASR [2]. Sec-\nond, learning words and their context from speech can al-\nlow for out-of-vocabulary word detection and acquisition,\nwhich has been shown to be useful in creating more adapt-\nable and robust ASR or dialog systems [3], [4]. Learning\nLMs from speech could also prove a powerful tool in e ﬀorts\nfor technology-based language preservation [5], particularly\nfor languages that have a rich oral, but not written tradition.\nFinally, as human children learn language from speech, not\nManuscript received June 21, 2011.\nManuscript revised September 26, 2011.\n†The authors are with the Graduate School of Informatics,\nKyoto University, Kyoto-shi, 606–8501 Japan.\na) E-mail: neubig@ar.media.kyoto-u.ac.jp\nDOI: 10.1587/transinf.E95.D.614\ntext, computational models for learning from speech are of\ngreat interest in the ﬁeld of cognitive science [6].\nThere has been a signiﬁcant amount of work on learn-\ning lexical units from speech data. These include statistical\nmodels based on the minimum description length or max-\nimum likelihood frameworks, which have been trained on\none-best phoneme recognition results [7]–[9] or recognition\nlattices [10]. There have also been a number of works that\nuse acoustic matching methods combined with heuristic cut-\noﬀs that may be adjusted to determine the granularity of\nthe units that need to be acquired [11]–[13]. Finally, many\nworks, inspired by the multi-modal learning of human chil-\ndren, use visual and audio information (or at least abstrac-\ntions of such) to learn words without text [6], [14], [15].\nThis work is di ﬀerent from these other approaches in\nthat it is the ﬁrst model that is able to learn a full word-based\nn-gram model from raw audio. In order to learn an LM from\ncontinuous speech, we ﬁrst generate lattices of phonemes\nwithout any linguistic constraints using a standard ASR\nacoustic model. To learn an LM from this data, we build on\nrecent work in unsupervised word segmentation of text [16],\nproposing a novel inference procedure that allows for mod-\nels to be learned over lattice input. For LM learning, we use\nthe hierarchical Pitman-Yor LM (HPYLM) [17], a variety\nof LM that is based on non-parametric Bayesian statistics.\nNon-parametric Bayesian statistics are well suited to this\nlearning problem, as they allow for automatically balancing\nmodel complexity and expressiveness, and have a principled\nframework for learning through the use of Gibbs sampling.\nTo perform sampling over phoneme lattices, we repre-\nsent all of our models using weighted ﬁnite state transducers\n(WFSTs), which allow for simple and e ﬃcient combination\nof the phoneme lattices with the LM. Using this combined\nlattice, we use a variant of the forward-backward algorithm\nto e ﬃciently sample a phoneme string and word segmen-\ntation according to the model probabilities. By perform-\ning this procedure on each of the utterances in the corpus\nfor several iterations, it is possible to e ﬀectively discover\nphoneme strings and lexical units appropriate for LM learn-\ning, even in the face of acoustic uncertainty.\nIn order to evaluate the feasibility of the proposed\nmethod, we performed an experiment on learning an LM\nfrom only audio ﬁles of ﬂuent adult-directed meeting speech\nwith no accompanying text. We demonstrate that, despite\nthe lack of any text data, the proposed model is able to both\ndecrease the phoneme recognition error rate over a separate\ntest set and acquire a lexicon with many intuitively reason-\nCopyright c⃝2012 The Institute of Electronics, Information and Communication Engineers\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n615\nable lexical entries. Moreover, we demonstrate that the pro-\nposed lattice processing approach is e ﬀective for overcom-\ning acoustic ambiguity present during the training process.\nIn Sect. 2 we brieﬂy overview ASR, including language\nmodeling and representation of ASR models in the WFST\nframework. Section 3 describes previous research on LM-\nbased unsupervised word segmentation, which learns LMs\neven when there are no clear boundaries between words. In\nSect. 4 we propose a method for formulating LM-based un-\nsupervised word segmentation using a combination of WF-\nSTs and Gibbs sampling. We conclude the description in\nSect. 4.3 by showing that the WFST-based formulation al-\nlows for LM learning directly from speech, even in the pres-\nence of acoustic uncertainty. Section 5 describes the results\nof an experimental evaluation demonstrating the e ﬀective-\nness of the proposed method, and Sect. 6 concludes the pa-\nper and discusses future directions.\n2. Speech Recognition and Language Modeling\nThis section provides an overview of ASR and language\nmodeling and provides deﬁnitions that will be used in the\nrest of the paper.\n2.1 Speech Recognition\nASR can be formalized as the task of ﬁnding a series of\nwords W given acoustic features X of a speech signal con-\ntaining these words. Most ASR systems use statistical meth-\nods, creating a model for the posterior probability of the\nwords given the acoustic features, and searching for the\nword sequence that maximizes this probability\nˆW =argmax\nW\nP(W|X). (1)\nAs this posterior probability is di ﬃcult to model di-\nrectly, Bayes’s law is used to decompose the probability\nˆW =argmax\nW\nP(X|W)P(W)\nP(X) (2)\n=argmax\nW\nP(X|W)P(W). (3)\nHere, P(X|W) is computed by the acoustic model\n(AM), which makes a probabilistic connection between\nwords and their acoustic features. However, directly mod-\neling the acoustic features of the thousands to millions of\nwords in large-vocabulary ASR systems is not realistic due\nto data sparsity issues. Instead, AMs are trained to recog-\nnize sequences of phonemes Y, which are then mapped into\nthe word sequence W. Phonemes are deﬁned as the small-\nest perceptible linguistic unit of speech. Thus, the entire\nASR process can be described as ﬁnding the optimal word\nsequence according to the following formula\nˆW =argmax\nW\n∑\nY\nP(X|Y)P(Y|W)P(W). (4)\nThis is usually further approximated by choosing the single\nmost likely phoneme sequence to allow for e ﬃcient search:\nˆW =argmax\nW,Y\nP(X|Y)P(Y|W)P(W). (5)\nHere, P(X|Y) indicates the AM probability and P(Y|W)i sa\nlexicon probability that maps between words and their pro-\nnunciations. P(W) is computed by the LM, which we will\ndescribe in more detail in the following section. It should be\nnoted that in many cases a scaling factor αis used\nˆW =argmax\nW,Y\nP(X|Y)P(Y|W)P(W)α. (6)\nThis allows for the adjustment of the relative weight put on\nthe LM probability.\n2.2 Language Modeling\nThe goal of the LM probability P(W)i st op r o v i d eap r e f e r -\nence towards “good” word sequences, assigning high prob-\nability to word sequences that the speaker is likely to say,\nand low probability to word sequences that the speaker is\nunlikely to say. By doing so, this allows the ASR system to\nselect linguistically proper sequences when purely acoustic\ninformation is not enough to correctly recognize the input.\nThe most popular form of LM is the n-gram, which\nis notable for its simplicity, computational e ﬃciency, and\nsurprising power [18]. n-gram LMs are based on the fact\nthat it is possible to calculate the joint probability of W =w\nI\n1\nsequentially by conditioning on all previous words in the\nsequence using the chain rule\nP(W) =\nI∏\ni=1\nP(wi|wi−1\n1 ). (7)\nConditioning on previous words in the sequence allows\nfor the consideration of contextual information in the prob-\nabilistic model. However, as few sentences will contain ex-\nactly the same words as any other, conditioning on all pre-\nvious words in the sentence quickly leads to data sparseness\nissues. n-gram models resolve this problem by only condi-\ntioning on the previous ( n−1) words when choosing the next\nword in the sequence\nP(W) ≈\nI∏\ni=1\nP(wi|wi−1\ni−n+1). (8)\nThe conditional probabilities are generally trained from\na large corpus of word sequences W.F r o m Wwe calcu-\nlate the counts of each subsequence of n words wi\ni−n+1 (an\n“n-gram”). From these counts, it is possible to compute con-\nditional probabilities using maximum likelihood estimation\nPml(wi|wi−1\ni−n+1) =c(wi\ni−n+1)\nc(wi−1\ni−n+1). (9)\nHowever, even if we set n to a relatively small value, we\nwill never have a corpus large enough to exhaustively cover\n616\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nall possible n-grams. In order to deal with this data spar-\nsity issue, it is common to use a framework that references\nhigher order n-gram probabilities when they are available,\nand falls back to lower order n-gram probabilities according\nto a fallback probability P(FB|wi−1\ni−n+1):\nP(wi|wi−1\ni−n+1) =\n⎧⎪⎪⎨⎪⎪⎩\nPs(wi|wi−1\ni−n+1)i f c(wi\ni−n+1) >0,\nP(FB|wi−1\ni−n+1)P(wi|wi−1\ni−n+2) otherwise .\n(10)\nBy combining more accurate but sparse higher-order\nn-grams with less accurate but more reliable lower-order n-\ngrams, it is possible to create LMs that are both accurate\nand robust. To reserve some probability for P(FB|w\ni−1\ni−n+1),\nwe replace Pml with the smoothed probability distribution\nPs. Ps can be deﬁned according to a number of smoothing\nmethods, which are described thoroughly in [19].\n2.3 Bayesian Language Modeling\nWhile traditional methods for LM smoothing are based on\nheuristics (often theoretically motivated), it is also possi-\nble to motivate language modeling from the perspective of\nBayesian statistics [17], [20]. In order to perform smooth-\ning in the Bayesian framework, we ﬁrst deﬁne a variable\ng\nwi|wi−1\ni−m+1\nthat speciﬁes n-gram probabilities\ngwi|wi−1\ni−m+1\n=P(wi|wi−1\ni−m+1) (11)\nwhere 0 ≤ m ≤ n −1 is the length of the context being\nconsidered.\nAs we are not sure of the actual values of the n-gram\nprobabilities due to data sparseness, the standard practice\nof Bayesian statistics suggests we treat all probabilities as\nrandom variables G that we can learn from the training data\nW. Formally, this learning problem consists of estimating\nthe posterior probability P(G|W). This can be calculated in\na Bayesian fashion by placing a prior probability P(G) over\nG and combining this with the likelihood P(W|G) and the\nevidence P(W)\nP(G|W) =P(W|G)P(G)\nP(W) (12)\n∝P(W|G)P(G). (13)\nWe can generally ignore the evidence probability, as the\ntraining data is ﬁxed throughout the entire training process.\nIt should be noted that LMs are a collection of multi-\nnomial distributions Gwi−1\ni−m+1\n= {gwi=1|wi−1\ni−m+1\n,...,g wi=N|wi−1\ni−m+1\n}\nwhere N is the number of words in the vocabulary. There\nis one multinomial for each history wi−1\ni−m+1, with the length\nof wi−1\ni−m+1 being 0 through n −1. As the variables in Gwi−1\ni−m+1\nbelong to a multinomial distribution, it is natural to use pri-\nors based on the Pitman-Yor process [21]. † The Pitman-Yor\nprocess is useful in that it is able to assign probabilities to\nthe space of variables that form multinomial distributions.\nFormally, this means that if we deﬁne the prior over G\nwi−1\ni−m+1\nusing a Pitman-Yor process, we will be guaranteed that its\nelements will add to one\nN∑\nx=1\ngwi=x|wi−1\ni−m+1\n=1 (14)\nand be between zero and one\n∀N\nx=10 ≤gwi=x|wi−1\ni−m+1\n≤1. (15)\nThe Pitman-Yor process has three parameters: the dis-\ncount parameter dm, the strength parameter θm, and the base\nmeasure Gwi−1\ni−m+2\nGwi−1\ni−m+1\n∼PY(dm,θm,Gwi−1\ni−m+2\n). (16)\nThe discount dm is subtracted from observed counts, and\nwhen it is given a large value (close to one), the model will\ngive more probability to frequent words. The strength θm\ncontrols the overall sparseness of the distribution, and when\nit is given a small value the distribution will be sparse. ††The\nbase measure Gwi−1\ni−m+2\nof the Pitman-Yor process indicates the\nexpected value of the probability distributions it generates,\nand is essentially the “default” value used when there are no\nwords in the training corpus for context w\ni−1\ni−m+1.\nIt should be noted that here, we are setting the base\nmeasure of each Gwi−1\ni−m+1\nto that of its parent context Gwi−1\ni−m+2\n.\nThis forms a hierarchical structure that is referred to as the\nhierarchical Pitman-Yor LM (HPYLM, [17]) and shown in\nFig. 1. This hierarchical structure implies that each set of\nm-gram (e.g., trigram) probabilities will be using its corre-\nsponding (m−1)-gram (e.g., bigram) probabilities as a start-\ning point when no or little training data is available. As a\nresult, we achieve a principled probabilistic interpolation of\nm-gram and ( m −1)-gram smoothing similar to the heuristic\nmethods described in Sect. 2.2. Finally, the base measure of\nthe unigram model G\n0 indicates the prior probability over\nwords in the vocabulary. If we have a vocabulary of all the\nwords that the HPYLM is expected to generate, we can sim-\nply set this so that a uniform probability is given to each\nword in the vocabulary.\nFor the Pitman-Yor process, the actual probabilities of\nthe LM can be calculated through Gibbs sampling and the\nChinese Restaurant Process (CRP) formulation, the details\nFig. 1 An example of the hierarchical structure of the HPYLM.\n†The better-known Dirichlet process is a speciﬁc case of the\nPitman-Yor process, where the discount parameter is set to zero.\n††Following [17], we give the strength and discount parameters\na prior and allow them to be chosen automatically.\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n617\nof which are beyond the scope of this paper but described\nin [17]. The important thing to note is that for each n-gram\nprobability, it is possible to calculate the expectation of the\nprobability given a set of su ﬃcient statistics S\nP(w\ni|wi−1\ni−n+1,S ) =\n∫ 1\n0\ngwi|wi−1\ni−n+1\nP(gwi|wi−1\ni−n+1\n|S )dgwi|wi−1\ni−n+1\n.\n(17)\nThe statistics S mainly consist of n-gram counts, but also\nsome auxiliary variables that summarize the conﬁguration\nof the CRP. These can be easily computed given a word-\nsegmented corpus W. The practical implication of this is\nthat we do not need to directly estimate the parameters G,\nbut only need to keep track of the su ﬃcient statistics needed\nto calculate this expectation of P(w\ni|wi−1\ni−n+1,S ). This fact be-\ncomes useful when using this model in unsupervised learn-\ning, as described in later sections.\n2.4 Weighted Finite State ASR\nIn recent years, the paradigm of weighted ﬁnite state trans-\nducers (WFSTs) has brought about great increases in the\nspeed and ﬂexibility of ASR systems [22]. Finite state trans-\nducers are ﬁnite automata with transitions labeled with input\nand output symbols. WFSTs also assign a weight to transi-\ntions, allowing for the deﬁnition of weighted relations be-\ntween two strings. These weights can be used to represent\nprobabilities of each model for ASR including the AM, lex-\nicon, and the LM, examples of which are shown in Fig. 2. In\nﬁgures of the WFSTs, edges are labeled as “ a/b:c”, where\na indicates the input, b indicates the output, and c indicates\nthe weight. b may be omitted when a and b are the same\nvalue, and c will be omitted when it is equal to 1.\nThe standard AM for P(X|Y) in most ASR systems is\nbased on a Hidden Markov Model (HMM), and its WFST\nFig. 2 The WFSTs for ASR including (a) the acoustic model A,( b )t h e\nlexicon L, and (c) the language model G.\nrepresentation, which we will call A. A simpliﬁed exam-\nple of this model is shown in Fig. 2 (a). As input, this takes\nacoustic features, and after several steps through the HMM\noutputs a single phoneme such as “e-” or “s.” The transi-\ntion and emission probabilities are identical to the standard\nHMM used in ASR acoustic models, but we have omitted\nthem from the ﬁgure for simplicity.\nThe WFST formulation for the lexicon, which we will\ncall L, shown in Fig. 2 (b), takes phonemes as input and out-\nputs words along with their corresponding lexicon probabil-\nity P(Y|W). Excluding the case of homographs (words with\nthe same spelling but di ﬀerent pronunciations), the proba-\nbility of transitions in the lexicon will be 1.\nFinally, the LM probability P(W) can also be repre-\nsented in the WFST format. Figure 2 (c) shows an example\nof a bigram LM with only two words w\n1 and w2 in the vocab-\nulary. Each node represents a unique n-gram context wi−1\ni−m+1,\nand the outgoing edges from the node represent the proba-\nbility of symbols given this context P(w\ni|wi−1\ni−m+1). In order\nto handle the fallback to lower-order contexts as described\nin Sect. 2.2, edges that fall back from wi−1\ni−m+1 to wi−1\ni−m+2 are\nadded, weighted with the fallback probability (marked with\n“FB” in the ﬁgure). The label ϵon these edges indicates the\nempty string, which means they can be followed at any time,\nregardless of the input symbol.\nThe main advantage of using WFSTs to describe the\nASR problem is the existence of e ﬃcient algorithms for op-\nerations such as composition, intersection, determinization,\nand minimization. In particular, composition (written X ◦Y)\nallows the combination of two WFSTs in sequence, so if we\ncompose A◦L◦G together, we can create a single WFST that\ntakes acoustic features as input and outputs weighted strings\nof words entailed by the acoustic features. We use this prop-\nerty of WFSTs later to facilitate the implementation of our\nlearning of LMs from continuous speech.\n3. Learning LMs from Unsegmented Text\nWhile Sects. 2.2 and 2.3 described how to learn LMs when\nwe are given a corpus of word sequences W, there are some\ncases when the word sequence is not obvious. For example,\nwhen human babies learn words they do so from continu-\nous speech, even though there often are not explicit bound-\naries between words in the phoneme stream. In addition,\nmany languages such as Japanese and Chinese are written\nwithout boundaries between words, and thus the deﬁnition\nof words is not uniquely ﬁxed. These two facts have led to\nsigniﬁcant research interest in unsupervised word segmenta-\ntion (WS), the task of ﬁnding words and learning LMs from\nunsegmented phoneme or character strings with no manual\nintervention [7], [16], [23]–[26].\n3.1 Unsupervised WS Modeling\nIn this work, we follow [16] in taking an LM-based ap-\nproach to unsupervised WS, learning a word-based LM G\nfrom a corpus of unsegmented phoneme strings Y.T h i s\n618\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nproblem can be speciﬁed as ﬁnding a model according to\nthe posterior probability of the LM P(G|Y), which we can\ndecompose using Bayes’s law\nP(G|Y) ∝P(Y|G)P(G). (18)\nHowever, as G is a word-based LM, we also assume\nthat there are hidden word sequences W, and model the\nprobability given these sequences\nP(G|Y) ∝\n∑\nW\nP(Y|W)P(W|G)P(G). (19)\nHere, P(Y|W) indicates that the words in Wmust corre-\nspond to the phonemes in Y, and will be 1 if and only if Y\ncan be recovered by concatenating the words in Wtogether.\nP(W|G) is the likelihood given the LM probabilities, and is\nidentical to that described in Eq. (8).\nP(G) can be set using the previously described\nHPYLM, with one adjustment. With the model we de-\nscribed in Sect. 2 .3, it was necessary to know the full vo-\ncabulary in advance so that we could set the base measure\nG\n0 to a uniform distribution over all the words in the vocab-\nulary. However, when learning an LM from unsegmented\ntext, W is not known in advance, and thus it is impossible\nto deﬁne a closed vocabulary before training starts. As a re-\nsult, it is necessary to ﬁnd an alternative method of deﬁning\nG\n0 that allows the model to ﬂexibly decide which words to\ninclude in the vocabulary as training progresses.\nIn order to do so, [16] uses a “spelling model” H, which\nassigns prior probabilities over words by using an LM spec-\niﬁed over phonemes. If we have a word wi that consists of\nphonemes, y1,...,y J , we deﬁne the spelling model proba-\nbility of wi according to the n-gram probabilities of H:\nG0(wi) =P(wi =y1,...,y J |H) =\nJ∏\nj=1\nHyj|yj−1\nj−n+1\n(20)\nWe assume that H is also distributed according to the\nHPYLM, and that the set of phonemes is closed and thus\nwe are able to deﬁne a uniform distribution over phonemes\nH\n0. The probabilities of H can be calculated from the set\nof phoneme sequences of words generated from the spelling\nmodel, much like the probabilities of G can be calculated\nfrom the set of word sequences contained in the corpus.\nThis gives us a full generative model for the corpus Y\nthat ﬁrst generates the LM probabilities\nH ∼HPYLM (dH ,θH ,H0) (21)\nG ∼HPYLM (dG,θG,P(w|H)) (22)\nthen generates each word sequence W ∈W and concate-\nnates it into a phoneme sequence\nW ∼P(W|G) (23)\nY ←concat(W). (24)\nThis generative story is important in that it allows for\nthe creation of LMs that are both highly expressive and\ncompact (and thus have high generalization capacity). The\nHPYLM priors for H and G have a preference for simple\nmodels, and thus will tend to induce compact models, while\nthe likelihoods for W bias towards larger and more expres-\nsive models that describe the data well.\n3.2 Inference for Unsupervised WS\nThe main di ﬃculty in learning LM G from the phoneme\nstring Yis solving Eq. (19). Here, it is necessary to sum over\nall possible conﬁgurations of W, which represent all possi-\nble segmentations of Y. However, for all but the smallest\nof corpora, the number of possible segmentations is astro-\nnomical and thus it is impractical to explicitly enumerate all\npossible W.\nInstead, we can turn to Gibbs sampling [27], [28], a\nmethod for calculating this sum approximately. Gibbs sam-\npling approximates the integral or sum over multivariate dis-\ntributions by stepping through each variable in the distribu-\ntion and sampling it given all of the other variables to be\nestimated. As we are interested in calculating W, for each\nstep of the algorithm we take a single sentence W\nk ∈W\nand sample it according to a distribution P(Wk|Yk,S −Wk ). S\nindicates the su ﬃcient statistics calculated from the current\nconﬁguration of Wrequired to calculate language model\nprobabilities (as described in Sect. 2 .3). S −Wk indicates the\nsuﬃcient statistics after subtracting the n-gram counts and\ncorresponding CRP conﬁgurations that were obtained from\nthe sentence W\nk.† These su ﬃcient statistics allow us to cal-\nculate the conditional probability of Wk given all other sen-\ntences, a requirement to properly perform Gibbs sampling.\nIt should be noted that each Wk contains multiple variables\n(words), so this is a variant of “blocked Gibbs sampling,”\nwhich samples multiple variables simultaneously [29]. The\nfull sampling procedure is shown in Fig. 3, and we further\ndetail how a single sentence W\nk can be sampled according\nto this distribution in the following section.\nBy repeating Gibbs sampling for many iterations, the\nsampled values of each sentence Wk, and the LM su ﬃcient\nstatistics S calculated therefrom, will gradually approach\nthe high-probability areas speciﬁed by the model. As men-\nFig. 3 The algorithm for Gibbs sampling of the word sequence Wand\nthe suﬃcient statistics S necessary for calculating LM probabilities.\n†On the ﬁrst iteration, we start with an empty S , and gradually\nadd the statistics for each sentence as they are sampled.\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n619\ntioned previously, the HPYLM-based formulation prefers\nhighly expressive, compact models. Lexicons that contain\nmany words are penalized by the HPYLM prior, preventing\nsegmentations of Wthat result in a large number of unique\nwords. On the other hand, if the lexicon is too small, it will\nresult in low descriptive power. Thus the sampled values\nare expected to be those with a consistent segmentation for\nwords, and with common phoneme sequences grouped to-\ngether as single words.\n3.3 Calculating Predictive Probabilities\nAs the main objective of an LM is to assign a probability to\nan unseen phoneme string Y, we are interested in calculating\nthe predictive distribution\nP(Y|Y) =\n∫\nG\n∑\nW∈{˜W:concat( ˜W)=Y}\nP(W|G)P(G|Y)dG. (25)\nHowever, computing this function directly is computation-\nally diﬃcult. To reduce this computational load we approxi-\nmate the summation over W with the maximization, assum-\ning that the probability of Y is equal to that of its most likely\nsegmentation.\nIn addition, assume we have I eﬀective samples of the\nsuﬃcient statistics obtained after iterations of the previous\nsampling process. † Using these samples, we can approxi-\nmate the integral over G with the mean of the probabilities\ngiven the su ﬃcient statistics {S 1,..., S I }\nP(Y|Y) ≈1\nI\nI∑\ni=1\nmax\nW∈{˜W:concat( ˜W)=Y}\nP(W|S i). (26)\nWhile Eq. (26) approximates the probability using the\naverage maximum-segmentation probability of each S i,\nsearch for such a solution at decoding time is a non-trivial\nproblem. As an approximation to this sum, we ﬁnd the one-\nbest solution mandated by each of the samples, and combine\nthe separate solutions using ROVER [30].\n4. WFST-based Sampling of Word Sequences\nWhile the previous section described the general ﬂow of the\ninference process, we still require an e ﬀective method to\nsample the word sequence W according to the probability\nP(W|Y,S\n−W ). One way to do so would be to explicitly enu-\nmerate all possible segmentations for Y, calculate their prob-\nabilities, and sample based on these probabilities. However,\nas the number of possible segmentations of Y grows expo-\nnentially in the length of the sentence, this is an unrealistic\nsolution. Thus, the most di ﬃcult challenge of the algorithm\nin Fig. 3 is e ﬃciently obtaining a word sequence W given a\nphoneme sequence Y according to the language model prob-\nabilities speciﬁed by S\n−W .\nOne solution is proposed by [16], who use a dynamic\nprogramming algorithm that allows for e ﬃcient sampling\nof a value for W according to the probability P(W|Y,S −W ).\nWhile this method is applicable to unsegmented text strings,\nit is not applicable to situations where uncertainty exists in\nthe input, such as the case of learning from speech. Here\nwe propose an alternative formulation that uses the WFST\nframework. This is done by ﬁrst creating a WFST-based\nformulation of the WS model (Sect. 4.1), then describing a\ndynamic programming method for sampling over WFSTs\n(Sect. 4.2). This formulation is critical for learning from\ncontinuous speech, as it allows for sampling a word string W\nfrom not only one-best phoneme strings, but also phoneme\nlattices that are able to encode the uncertainty inherent in\nacoustic matching results.\n4.1 A WFST Formulation for Word Segmentation\nOur formulation for sampling word sequences consists of\nﬁrst generating a lattice of all possible segmentation candi-\ndates using WFSTs, then performing sampling over this lat-\ntice. The three WFSTs used for WS (Fig. 4) are quite similar\nto the ASR WFSTs shown in Fig. 2.\nIn place of the acoustic model WFST used in ASR, we\nFig. 4 The WFSTs for word segmentation including (a) the input Y,\n(b) the lexicon L, and (c) the language model GH.\n†Some samples may be skipped during the early stages of sam-\npling (a process called “burn-in”) to help ensure that samples are\nlikely according to the HPYLM.\n620\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nsimply use a linear chain representing the phonemes in Y,\nas shown in Fig. 4 (a). The lexicon WFST L in Fig. 4 (b) is\nidentical to the lexicon WFST used in ASR, except that in\naddition to creating words from phonemes, it also allows all\nphonemes in the input to be passed through as-is. This al-\nlows words in the lexicon to be assigned word-based proba-\nbilities according to the language model G, and all words (in\nthe lexicon or not) to be assigned probabilities according to\nthe spelling model H. This is important in the unsupervised\nWS setting, where the lexicon is not deﬁned in advance, and\nwords outside of the lexicon are still assigned a small prob-\nability.\nThe training process starts with an empty lexicon, and\nthus no paths emitting words are present. When a word that\nis not in the lexicon is sampled as a phoneme sequence, L\nis modiﬁed by adding a path that converts the new word’s\nphonemes into its corresponding word token. Conversely,\nwhen the last sample containing a word in the lexicon is sub-\ntracted from the distribution and the word’s count becomes\nzero, its corresponding path is removed from L. It should be\nnoted that we assume that each word can be mapped onto a\nsingle spelling, so P(Y|W) will always be 1.\n†\nMore major changes are made to the LM WFST, which\nis shown in Fig. 4 (c). Unlike the case in ASR, where we are\ngenerally only concerned with words that exist in the vocab-\nulary, it is necessary to model unknown words that are not\nincluded in the vocabulary. The key to the representation is\nthat the word-based LM G and the phoneme-based spelling\nmodel H are represented in a single WFST, which we will\ncall GH. GH has weighted edges falling back from the base\nstate of G to H, and edges accepting the terminal symbol for\nunknown words and transitioning from H to the base state\nof G. This allows for the WFST to transition as necessary\nbetween the known word model and the spelling model.\nBy composing together these three WFSTs as Y ◦L ◦\nGH, it is possible to create a WFST representing a lattice\nof segmentation candidates weighted with probabilities ac-\ncording to the LM.\n4.2 Sampling over WFSTs\nOnce we have a WFST lattice representing the model prob-\nabilities, we can sample a single path through the WFST\naccording to the probabilities assigned to each edge. This is\ndone using a technique called forward-ﬁltering/backward-\nsampling, a concept similar to that of the forward-backward\nalgorithm for hidden Markov models (HMM). This algo-\nrithm can be used to acquire a sample from all probabilis-\ntically weighted, acyclic WFSTs deﬁned by a set of states S\nand a set of edges E.\nThe ﬁrst step of the algorithm consists of choosing an\nordering for the states in S , which we will write s\n1,..., sI .\nThis ordering must be chosen so that all states included in\npaths that travel to state si should be processed before si\nitself. Each edge in E is deﬁned as ek =⟨si,sj,wk⟩traveling\nfrom si to sj and weighted by wk. Assuming the graph is\nacyclic, we can choose the ordering so that for all edges in\nFig. 5 A WFSA representing a unigram segmentation (words of length\ngreater than three are not displayed).\nE, i < j. Given this ordering, if all states are processed in\nascending order, we can be ensured that all states will be\nprocessed after their predecessors.\nNext, we perform the forward ﬁltering step, identical\nto the forward pass of the forward-backward algorithm for\nHMMs, where probabilities are accumulated from the start\nstate to following states. The initial state s\n0 is given a for-\nward probability f0 =1, and all following states are updated\nwith the sum of the forward probabilities of each of the in-\ncoming states multiplied by the weights of the edges to the\ncurrent state\nfj =\n∑\nek =⟨si,s˜j,wk ⟩∈{E:˜j=j}\nfi ∗wk. (27)\nThis forward probability can be interpreted as the total prob-\nability of all paths that travel to fj from the initial state.\nWe provide an example of this process using a\nweighted ﬁnite state acceptor (WFSA) for the unigram seg-\nmentation model of “e- e s a r” (“ASR”) shown in Fig. 5. In\nthis case, the forward step will push probabilities from the\nﬁrst state as follows:\nf\n1 =P(e-) ∗f0 (28)\nf2 =P(e-e) ∗f0 +P(e) ∗f1 (29)\n...\nThe backward sampling step of the algorithm consists\nof sampling a path starting at the ﬁnal state sI of the WFST.\nFor the current state, sj, we can calculate the probability of\nall incoming edges\nP(ek =⟨si,sj,wk⟩) = fi ∗wk\nfj\n, (30)\nand sample a single incoming edge according to this prob-\nability. Here wk considers the likelihood of ek itself, while\nfi considers the likelihood of all paths traveling up to si,a l -\nlowing for the correct sampling of an edge ek according to\nthe probability of all paths that travel through it to the cur-\nrent state s\nj. In the example, the edge incoming to state s5\nis sampled according to\nP(s4 →s5) =P(r) ∗f4 (31)\nP(s3 →s5) =P(ar) ∗f3 (32)\n...\n†In this work, we assume that all words are represented by\ntheir phonetic spelling, not considering the graphemic represen-\ntation used in usual text. For example, the word “ASR” will be\ntranscribed as “e-esar” in the learned model.\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n621\nFig. 6 A WFSA representing a phoneme lattice.\nThrough this process, a path representing the segmen-\ntation of the phoneme string can be sampled according to\nthe probability of the models included in the lattice. Given\nthis path, it is possible to recover Y and W by concatenating\nthe phonemes and words represented by the input and output\nof the sampled path respectively.\n4.3 Extension to Continuous Speech Input\nWhen learning from continuous speech, the input is not a\nset of phoneme strings Y, but a set of spoken utterances X.\nAs a result, instead of sampling just the word sequences W,\nwe now need to additionally sample the phoneme strings Y.\nIf we can create a single lattice representing the probabil-\nity of both W and Y for a particular X, it is possible to use\nthe forward-ﬁltering /backward-sampling algorithm to sam-\nple phoneme strings and their segmentations together.\nWith the WFST-based formulation described in the pre-\nvious section, it is straight-forward to create this lattice rep-\nresenting candidates for Y and W. In fact, all we must do is\nreplace the string of phonemes Y that was used in the WS\nmodel in Fig. 4 (a) with the acoustic model HMM A used for\nASR in Fig. 2. As a result, the composed lattice A ◦L ◦GH\ncan take acoustic features as input, and includes both the\nacoustic and language model probabilities. Using this value,\nwe can sample appropriate new values of Y and W, and plug\nthis into the learning algorithm of Fig. 3.\nHowever, as with traditional ASR, if we simply expand\nall hypotheses allowed by the acoustic model during the\nforward-ﬁltering step, the hypothesis space will grow un-\nmanageably large. As a solution to this, before starting train-\ning we ﬁrst perform ASR using only the acoustic model and\nno linguistic information, generating trimmed phoneme lat-\ntices representing candidates for each Y such as those shown\nin Fig. 6.\nIt should be noted that this dependence on an acous-\ntic model to estimate P(X|Y) indicates that this is not an\nentirely unsupervised method. However, some work has\nbeen done on language-independent acoustic model train-\ning [31], as well as the unsupervised discovery and cluster-\ning of acoustic units from raw speech [32]. The proposed\nLM acquisition method could be used in combination with\nthese AM acquisition methods to achieve fully unsupervised\nspeech recognition, a challenge that we leave to future work.\n5. Experimental Evaluation\nWe evaluated the feasibility of the proposed method on con-\ntinuous speech from meetings of the Japanese Diet (Parlia-\nment). This was chosen as an example of naturally spoken,\ninteractive, adult-directed speech with a potentially large vo-\ncabulary, as opposed to the simpliﬁed grammars or infant-\ndirected speech used in some previous work [6], [14].\n5.1 Experimental Setup\nWe created phoneme lattices using a triphone acoustic\nmodel, performing decoding with a vocabulary of 385 syl-\nlables that represent the phoneme transitions allowed by the\nsyllable model.\n† No additional linguistic information was\nused during the creation of the lattices, with all syllables in\nthe vocabulary being given a uniform probability.\nIn order to assess the amount of data needed to e ﬀec-\ntively learn an LM, we performed experiments using ﬁve\ndiﬀerent corpora of varying sizes: 7.9, 16.1, 31.1, 58.7, and\n116.7 minutes. The speech was separated into utterances,\nwith utterance boundaries being delimited by short pauses\nof 200 ms or longer. According to this criterion, the training\ndata consisted of 119, 238, 476, 952, and 1,904 utterances\nrespectively. An additional 27.2 minutes (500 utterances) of\nspeech were held out as a test set.\nAs a measure of the quality of the LM learned by the\ntraining process, we used phoneme error rate (PER) when\nthe LM was used to re-score the phoneme lattices of the test\nset. We chose PER as word-based accuracy may depend\nheavily on a particular segmentation standard. Given no lin-\nguistic information, the PER on the test set was 34.20%.\nThe oracle PER of the phoneme lattice was 8.10%, indicat-\ning the lower bound possibly obtainable by LM learning.\nFifty samples of the word sequences Wfor each train-\ning utterance (and the resulting su ﬃcient statistics S )w e r e\ntaken after 20 iterations of burn-in, the ﬁrst 10 of which were\nannealed according to the technique presented by [25]. For\nthe LM scaling factor of Eq. (6), αwas set arbitrarily to 5,\nwith values between 5 and 10 producing similar results in\npreliminary tests.\n5.2 E ﬀect of n-gram Context Dependency\nIn the ﬁrst experiment, the e ﬀect of using context informa-\ntion in the learning process was examined. The n of the\nHPYLM language model was set to 1, 2, or 3, and n of the\nHPYLM spelling model was set to 3 for all models. The\nresults with regards to PER are shown in Fig. 7.\nFirst, it can be seen that an LM learned directly from\nspeech was able to improve the accuracy by 7% absolute\nPER or more compared to a baseline using no linguistic in-\nformation. This is true even with only 7.9 minutes of train-\ning speech. In addition, the results show that the bigram\nmodel outperforms the unigram, and the trigram model out-\nperforms the bigram, particularly as the size of the training\ndata increases. We were also able to conﬁrm the observa-\ntion of [25] that the unigram model tends to undersegment,\n†Syllable-based decoding was a practical consideration due to\nthe limits of the decoding process, and is not a fundamental part of\nthe proposed method. Phoneme-based decoding will be examined\nin the future.\n622\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nFig. 7 Phoneme error rate by model order.\nTable 1 The size of the vocabulary, and the number of n-grams in the\nword-based model G, and the phoneme-based model H when trained on\n116.7 minutes of speech.\n1-gram 2-gram 3-gram\nV ocabulary size 4480 1351 708\nG entries 4480 16150 38759\nH entries 9624 3869 2426\ngrouping together “multi-word” phrases instead of actual\nwords. This is reﬂected in the vocabulary and n-gram sizes\nof the three models after the ﬁnal iteration of the learning\nprocess, which are displayed in Table 1. It can also be seen\nthat the vocabulary size increases when the LM is given a\nsmaller n, with the lack of complexity in the word-based\nLM being transferred to the phoneme-based spelling model.\n5.3 E ﬀect of Joint and Bayesian Estimation\nThe proposed method has two major di ﬀerences from previ-\nous methods such as [10], which estimates multigram mod-\nels from speech lattices. The ﬁrst is that we are perform-\ning joint learning of the lexicon and n-gram context, while\nmultigram models do not consider context, similarly to the\n1-gram model presented in this paper [23]. However, it is\nconceivable that a context insensitive model could be used\nfor learning lexical units, and its results used to build a tra-\nditional LM. In order to test the e ﬀect of context-sensitive\nlearning, we experiment with not only the proposed 1-gram\nand 3-gram models from Sect. 5.2, but also use the 1-gram\nmodel to acquire samples of Wand use these to train a stan-\ndard 3-gram LM.\nThe second major di ﬀerence is that we are performing\nlearning using Bayesian methods. This allows us to con-\nsider the uncertainty of the acquired W through the sum in\nEq. (26). Previous multigram approaches are based on max-\nimum likelihood estimation, which only allows for a unique\nsolution to be considered. To test the e ﬀect of this, we also\ntake the one-best results acquired by the sampled LMs, but\ninstead of combining them together to create a better result\nas explained in Sect. 3.3, we simply report the average PER\nof these one-best results.\nTable 2 shows the results of the evaluation (performed\non the 116.7 minute training data). It can be seen that\nTable 2 The eﬀects on accuracy of the n-gram length used to acquire the\nlexicon and train the language model, as well as Bayesian sample combina-\ntion. The proposed method signiﬁcantly exceeds italicized results accord-\ning to the two-proportions z-test ( p <0.05).\nLexicon LM\nSingle Combined\n1-gram 1-gram 26.28% 26.08%\n1-gram 3-gram 26.06% 25.41%\n3-gram 3-gram 25.85% 25.28%\nFig. 8 Phoneme error rate for various training methods.\nthe proposed method using Bayesian sample combination\nand incorporating LMs directly into training (3-gram /3-\ngram/combined) is e ﬀective in reducing the error rate com-\npared to a model that does not use these proposed improve-\nments (1-gram/3-gram/single).\n5.4 E ﬀect of Lattice Processing\nWe also compare the proposed lattice processing method\nwith four other LM construction methods. First, we trained\na model using the proposed method, but instead of using\nword lattices, used one-best ASR results to provide a com-\nparison with previous methods that have used one-best re-\nsults [7], [9]. Second, to examine whether the estimation of\nword boundaries is necessary when acquiring an LM from\nspeech, we trained a syllable trigram LM using these one-\nbest results. Moreover, we show two other performance\nresults for reference. One is an LM that was built using\na human-created verbatim transcription of the utterances.\nWS and pronunciation annotation were performed with the\nKyTea toolkit [33], and pronunciations of unknown words\nwere annotated by hand. Trigram language and spelling\nmodels were created on the segmented word and phoneme\nstrings using interpolated Kneser-Ney smoothing. For the\nsecond reference, we created an “oracle” model by training\non the lattice path with the lowest possible PER for each\nutterance. This demonstrates an upper bound of the accu-\nracy achievable by the proposed model if it picks all the best\nphoneme sequences in the training lattice.\nThe PER for the four methods is shown in Fig. 8. It can\nbe seen that the proposed method signiﬁcantly outperforms\nthe model trained on one-best results, demonstrating that lat-\ntice processing is critical in reducing the noise inherent in\nacoustic matching results. It can also be seen that on one-\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n623\nFig. 9 Entropy comparison for various LM learning methods.\nbest results, the model using acquired units achieves slightly\nbut consistently better results than the syllable-based LM for\nall data sizes.\nAs might be expected, the proposed method does not\nperform as well as the model trained on gold-standard tran-\nscriptions. However, it appears to improve at approximately\nthe same rate as the model trained on the gold-standard tran-\nscriptions as more data is added, which is not true for one-\nbest transcriptions. Furthermore, it can be seen that the\noracle results fall directly between those achieved by the\nproposed model and the results on the gold-standard tran-\nscriptions. This indicates that approximately one half of the\ndiﬀerence between the model learned on continuous speech\nand that learned from transcripts can be attributed to the lat-\ntice error. By expanding the size of the lattice, or directly\nintegrating the calculation of acoustic scores with sampling,\nit will likely be possible to further close this gap.\nAnother measure commonly used for evaluating the ef-\nfectiveness of LMs is cross-entropy on a test set [18]. We\nshow entropy per syllable for the LMs learned with each\nmethod in Fig. 9. It can be seen that the proposed method\nonly slightly outperforms the model trained on one-best\nphoneme recognition results. This di ﬀerence can be ex-\nplained by systematic pronunciation variants that are not\naccounted for in the verbatim transcript. For example,\nkangaete\norimasu (“I am thinking”) is often pronounced\nwith a dropped e as kangaetorimasu in ﬂuent conversation.\nAs a whole word will fail to match the reference, this will\nhave a large e ﬀect on entropy results, but less of an e ﬀect\non PER as only a single phoneme was dropped. In fact, for\nmany applications such as speech analysis or data prepa-\nration for acoustic model training, the proposed method,\nwhich managed to properly learn pronunciation variants, is\npreferable to one that matches the transcript correctly.\n5.5 Lexical Acquisition Results\nFinally, we present a qualitative evaluation of the lexical ac-\nquisition results. Typical examples of the words that were\nacquired in the process of LM learning are shown in Ta-\nble 3. These are split into four categories: function words,\nsubwords, content words, spoken language expressions.\nTable 3 An example of words learned from continuous speech.\nFunction Words no (genitive marker), ni (locative marker), to (“and”)\nSubwords ka (kyoka “reinforcement”, interrogative marker)\nsai (kokusai “international”,seisai “sanction”)\nContent Words koto (“thing”),hanashi (“speak”),kangae (“idea”),\nchi-ki (“region”),shiteki (“point out”)\nSpoken Expressions yu- (“say (colloquial)”), e- (ﬁller), desune (ﬁller),\nmo-shiage (“say (polite)”)\nIn the resulting vocabulary, function words were the\nmost common of the acquired words, which is reasonable\nas function words make the majority of the actual spoken\nutterances. Subwords are the second most frequent category,\nand generally occur when less frequent content words share\na common stem.\nAn example of the content words discovered by the\nlearning method shows a trend towards the content of dis-\ncussions made in meetings of the Diet. In particular, chi-ki\n(“region”) and shiteki (“point out”) are good examples of\nwords that are characteristic of Diet speech and acquired by\nthe proposed model. While this result is not surprising, it is\nsigniﬁcant in that it shows that the proposed method is able\nto acquire words that match the content of the utterances on\nwhich it was trained. In addition to learning the content of\nthe utterances, the proposed model also learned a number of\nstylistic characteristics of the speech in the form of ﬁllers\nand colloquial expressions. This is also signiﬁcant in that\nthese expressions are not included in the o ﬃcial verbatim\nrecords in the Diet archives, and thus would not be included\nin an LM that was simply trained on these texts.\n6. Conclusions and Future Work\nThis paper presented a method for unsupervised learning of\nan LM given only speech and an acoustic model. Speciﬁ-\ncally, we adapted a Bayesian model for word segmentation\nand LM learning so that it could be applied to speech in-\nput. This was achieved by formulating all elements of LM\nlearning as WFSTs, which allows for lattices to be used as\ninput to the learning algorithm. We then formulated a Gibbs\nsampling algorithm that allows for learning over composed\nlattices that represent acoustic and LM probabilities.\nAn experimental evaluation showed that LMs acquired\nfrom continuous speech with no accompanying transcrip-\ntions were able to signiﬁcantly reduce the error rates of ASR\nover when no such models were used. We also showed that\nthe proposed technique of joint Bayesian learning of lexical\nunits and an LM over lattices signiﬁcantly contributes to this\nimprovement.\nThis work contributes a basic technology that opens up\na number of possible directions for future research into prac-\ntical applications. The ﬁrst and most immediate application\nof the proposed method would be for use in semi-supervised\nlearning. In the semi-supervised setting, we have some text\nalready available, but want to discover words from untran-\nscribed speech that may be in new domains, speaking styles,\nor dialects. This can be formulated in the proposed model\n624\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.2 FEBRUARY 2012\nby treating the phoneme sequences Y (and possibly word\nboundaries W) of existing text as observed variables and the\nY and W of untranscribed speech as hidden variables. In ad-\ndition, if it is possible to create word dictionaries but not a\ntraining corpus, these dictionaries could be used as a com-\nplement or replacement to the spelling model, allowing the\nproposed method to favor words that occur in the dictionary.\nThe combination of the proposed model with informa-\ntion from modalities other than speech is another promising\nfuture direction. For example, while the model currently\nlearns words as phoneme strings, it is important to learn the\northographic forms of words for practical use in ASR. One\npossibility is that speech could be grounded in text data such\nas television subtitles to learn these orthographic forms. In\norder to realize this in the proposed model, an additional\nFST layer that maps between phonetic transcriptions and\ntheir orthographic forms could be introduced to allow for\na single phonetic word to be mapped into multiple ortho-\ngraphic words and vice-versa.\nIn addition, the proposed method could be used to dis-\ncover a lexicon and LM for under-resourced languages with\nlittle or no written text. In order to do so, it will be nec-\nessary to train not only an LM, but also an acoustic model\nthat is able to recognize the phonemes or tones in the target\nlanguage. One promising approach is to combine the pro-\nposed method with cross-language acoustic model adapta-\ntion, an active area of research that allows for acoustic mod-\nels trained in more resource-rich languages to be adapted to\nresource-poor languages [31], [34].\nThe proposed method is also of interest in the frame-\nwork of computational modeling of lexical acquisition by\nchildren. In its current form, which performs multiple\npasses over the entirety of the data, the proposed model is\nless cognitively plausible than previous methods that have\nfocused on incremental learning [35]–[37]\n†However, work\nby [35] has demonstrated that similar Bayesian methods\n(which were evaluated on raw text, not acoustic input) can\nbe adapted to an incremental learning framework. This sort\nof incremental learning algorithm is compatible with the\nproposed method as well, and may be combined to form a\nmore cognitively plausible model.\nThe ﬁnal interesting challenge is how to scale the\nmethod to larger data sets. One possible way to improve\nthe e ﬃciency of sampling would be to use beam sampling\ntechniques similar to those developed for non-parametric\nMarkov models [39]. Another promising option is parallel\nsampling, which would allow sampling to be run on a num-\nber of di ﬀerent CPUs simultaneously [40].\n†On the other hand, phonemic acquisition is generally consid-\nered to occur in the early stages of infancy, prior to lexical acquisi-\ntion [6], [38], and thus our reliance on a pre-trained acoustic model\nis largely plausible.\nReferences\n[1] D. Tannen, Spoken and Written Language: Exploring Orality and\nLiteracy, ABLEX, 1982.\n[2] Y . Akita and T. Kawahara, “Statistical transformation of lan-\nguage and pronunciation models for spontaneous speech recogni-\ntion,” IEEE Trans. Audio Speech Language Process., vol.18, no.6,\npp.1539–1549, 2010.\n[3] I. Bazzi and J. Glass, “Learning units for domain-independent out-\nof-vocabulary word modelling,” Proc. 7th European Conference on\nSpeech Communication and Technology (EuroSpeech), pp.61–64,\n2001.\n[4] T. Hirsimaki, M. Creutz, V . Siivola, M. Kurimo, S. Virpioja, and J.\nPylkkonen, “Unlimited vocabulary speech recognition with morph\nlanguage models applied to ﬁnnish,” Comput. Speech Lang., vol.20,\nno.4, pp.515–541, 2006.\n[5] S. Abney and S. Bird, “The human language project: Building a uni-\nversal corpus of the world’s languages,” Proc. 48th Annual Meet-\ning of the Association for Computational Linguistics, pp.88–97,\nUppsala, Sweden, July 2010.\n[6] D. Roy and A. Pentland, “Learning words from sights and sounds: a\ncomputational model,” Cognitive Science, vol.26, no.1, pp.113–146,\n2002.\n[7] C. de Marcken, “The unsupervised acquisition of a lexicon from\ncontinuous speech,” tech. rep., Massachusetts Institute of Technol-\nogy, Cambridge, MA, USA, 1995.\n[8] S. Deligne and F. Bimbot, “Inference of variable-length linguistic\nand acoustic units by multigrams,” Speech Commun., vol.23, no.3,\npp.223–241, 1997.\n[9] A. Gorin, D. Petrovska-Delacretaz, G. Riccardi, and J. Wright,\n“Learning spoken language without transcriptions,” Proc. 1999\nIEEE Automatic Speech Recognition and Understanding Workshop,\n1999.\n[10] J. Driesen and H.V . Hamme, “Improving the multigram algorithm\nby using lattices as input,” Proc. 9th Annual Conference of the Inter-\nnational Speech Communication Association (InterSpeech), 2008.\n[11] L. ten Bosch and B. Cranen, “A computational model for unsuper-\nvised word discovery,” Proc. 8th Annual Conference of the Interna-\ntional Speech Communication Association (InterSpeech), pp.1481–\n1484, 2007.\n[12] A. Park and J. Glass, “Unsupervised pattern discovery in speech,”\nIEEE Trans. Audio Speech Language Process., vol.16, no.1, 2008.\n[13] A. Jansen, K. Church, and H. Hermansky, “Towards spoken term\ndiscovery at scale with zero resources,” Proc. 11th Annual Confer-\nence of the International Speech Communication Association (Inter-\nSpeech), 2010.\n[14] N. Iwahashi, “Language acquisition through a human-robot inter-\nface by combining speech, visual, and behavioral information,” Inf.\nSci., vol.156, no.1-2, pp.109–121, 2003.\n[15] C. Yu and D.H. Ballard, “A multimodal learning interface for\ngrounding spoken language in sensory perceptions,” ACM Trans.\nApplied Perception, vol.1, pp.57–80, July 2004.\n[16] D. Mochihashi, T. Yamada, and N. Ueda, “Bayesian unsupervised\nword segmentation with nested Pitman-Yor modeling,” Proc. 47th\nAnnual Meeting of the Association for Computational Linguistics,\n2009.\n[17] Y .W. Teh, “A Bayesian interpretation of interpolated kneser-ney,”\ntech. rep., School of Computing, National Univ. of Singapore, 2006.\n[18] J.T. Goodman, “A bit of progress in language modeling,” Comput.\nSpeech Lang., vol.15, no.4, pp.403–434, 2001.\n[19] S.F. Chen and J. Goodman, “An empirical study of smoothing tech-\nniques for language modeling,” Proc. 34th Annual Meeting of the\nAssociation for Computational Linguistics, 1996.\n[20] D.J.C. Mackay and L.C.B. Petoy, “A hierarchical Dirichlet language\nmodel,” Natural Language Engineering, vol.1, pp.1–19, 1995.\n[21] J. Pitman and M. Yor, “The two-parameter Poisson-Dirichlet distri-\nNEUBIG et al.: BAYESIAN LEARNING OF A LANGUAGE MODEL FROM CONTINUOUS SPEECH\n625\nbution derived from a stable subordinator,” The Annals of Probabil-\nity, vol.25, no.2, pp.855–900, 1997.\n[22] M. Mohri, F. Pereira, and M. Riley, “Speech recognition with\nweighted ﬁnite-state transducers,” in Handbook on speech process-\ning and speech communication, Part E: Speech recognition, 2008.\n[23] F. Bimbot, R. Pieraccini, E. Levin, and B. Atal, “Variable-length\nsequence modeling: Multigrams,” IEEE Signal Process. Lett., vol.2,\nno.6, pp.111–113, 1995.\n[24] M.R. Brent, “An e ﬃcient, probabilistically sound algorithm for seg-\nmentation and word discovery,” Mach. Learn., vol.34, pp.71–105,\n1999.\n[25] S. Goldwater, T.L. Gri ﬃths, and M. Johnson, “A Bayesian frame-\nwork for word segmentation: Exploring the e ﬀects of context,” Cog-\nnition, vol.112, no.1, pp.21–54, 2009.\n[26] H. Poon, C. Cherry, and K. Toutanova, “Unsupervised morpholog-\nical segmentation with log-linear models,” Proc. North American\nChapter of the Association for Computational Linguistics - Human\nLanguage Technology (NAACL HLT), pp.209–217, 2009.\n[27] S. Geman and D. Geman, “Stochastic relaxation, Gibbs distributions\nand the Bayesian restoration of images,” IEEE Trans. Pattern Anal.\nMach. Intell., vol.6, no.6, pp.721–741, 1984.\n[28] D.J. MacKay, Information theory, inference, and learning algo-\nrithms, pp.357–386, Cambridge University Press, 2003.\n[29] C.S. Jensen, U. Kjærul ﬀ, and A. Kong, “Blocking Gibbs sampling\nin very large probabilistic expert systems,” Int. J. Human Comput.\nStudies, vol.42, no.6, pp.647–666, 1995.\n[30] J. Fiscus, “A post-processing system to yield reduced word er-\nror rates: Recognizer output voting error reduction (ROVER),”\nProc. 1997 IEEE Automatic Speech Recognition and Understand-\ning Workshop, 1997.\n[31] L. Lamel, J. Gauvain, and G. Adda, “Lightly supervised and unsu-\npervised acoustic model training,” Comput. Speech Lang., vol.16,\npp.115–129, 2002.\n[32] J.R. Glass, Finding acoustic regularities in speech: Application to\nphonetic recognition, Ph.D. thesis, Massachusetts Institute of Tech-\nnology, Cambridge, MA, USA, 1988.\n[33] G. Neubig and S. Mori, “Word-based partial annotation for e ﬃcient\ncorpus construction,” Proc. 7th International Conference on Lan-\nguage Resources and Evaluation, 2010.\n[34] T. Schultz and A. Waibel, “Language-independent and language-\nadaptive acoustic modeling for speech recognition,” Speech Com-\nmun., vol.35, no.1, pp.31–52, 2001.\n[35] L. Pearl, S. Goldwater, and M. Steyvers, “How ideal are we? incor-\nporating human limitations into Bayesian models of word segmen-\ntation,” Proc. 34th Annual Boston University Conference on Child\nLanguage Development, pp.315–326, Somerville, MA, 2010.\n[36] F.R. McInnes and S. Goldwater, “Unsupervised extraction of recur-\nring words from infant-directed speech,” Proc. 33rd Annual Confer-\nence of the Cognitive Science Society, 2011.\n[37] O. R ¨as¨anen, “A computational model of word segmentation from\ncontinuous speech using transitional probabilities of atomic acoustic\nevents,” Cognition, vol.120, no.2, pp.149–176, 2011.\n[38] P.D. Eimas, E.R. Siqueland, P. Jusczyk, and J. Vigorito, “Speech\nperception in infants,” Science, vol.171, no.3968, p.303, 1971.\n[39] J. Van Gael, Y . Saatci, Y . Teh, and Z. Ghahramani, “Beam sampling\nfor the inﬁnite hidden Markov model,” Proc. 25th International Con-\nference on Machine Learning, 2008.\n[40] A. Asuncion, P. Smyth, and M. Welling, “Asynchronous distributed\nlearning of topic models,” Proc. 22nd Annual Conference on Neural\nInformation Processing Systems, vol.21, 2008.\nGraham Neubig received his B.E.\nfrom University of Illinois, Urbana-Champaign,\nU.S.A, in 2005, and his M.E. in informatics\nfrom Kyoto University, Kyoto, Japan in 2010,\nwhere he is currently pursuing his Ph.D. He is\na recipient of the JSPS Research Fellowship for\nYoung Scientists (DC1). His research interests\ninclude speech and natural language processing,\nwith a focus on unsupervised learning for ap-\nplications such as automatic speech recognition\nand machine translation.\nMasato Mimura received the B.E. and M.E.\ndegrees from Kyoto University, Kyoto, Japan, in\n1996 and 2000, respectively. Currently, he is a\nresearcher in the Academic Center for Comput-\ning and Media Studies, Kyoto University. His\nresearch interests include spontaneous speech\nrecognition and spoken language processing.\nShinsuke Mori received B.S., M.S., and\nPh.D. degrees in electrical engineering from\nKyoto University, Kyoto, Japan in 1993, 1995,\nand 1998, respectively. After joining Tokyo Re-\nsearch Laboratory of International Business Ma-\nchines (IBM) in 1998, he studied the language\nmodel and its application to speech recognition\nand language processing. He is currently an as-\nsociate professor of Academic Center for Com-\nputing and Media Studies, Kyoto University.\nTatsuya Kawahara received B.E. in 1987,\nM.E. in 1989, and Ph.D. in 1995, all in infor-\nmation science, from Kyoto University, Kyoto,\nJapan. In 1990, he became a Research As-\nsociate in the Department of Information Sci-\nence, Kyoto University. From 1995 to 1996, he\nwas a Visiting Researcher at Bell Laboratories,\nMurray Hill, NJ, USA. Currently, he is a Pro-\nfessor in the Academic Center for Computing\nand Media Studies and an A ﬃliated Professor in\nthe School of Informatics, Kyoto University. He\nhas also been an Invited Researcher at ATR and NICT. He has published\nmore than 200 technical papers on speech recognition, spoken language\nprocessing, and spoken dialogue systems. He has been managing several\nspeech-related projects in Japan including a free large vocabulary continu-\nous speech recognition software project (http: //julius.sourceforge.jp/). Dr.\nKawahara received the 1997 Awaya Memorial Award from the Acoustical\nSociety of Japan and the 2000 Sakai Memorial Award from the Informa-\ntion Processing Society of Japan. From 2003 to 2006, he was a member\nof IEEE SPS Speech Technical Committee. From 2011, he is a secretary\nof IEEE SPS Japan Chapter. He was a general chair of IEEE Automatic\nSpeech Recognition & Understanding workshop (ASRU 2007). He also\nserved as a tutorial chair of INTERSPEECH 2010. He is a senior member\nof IEEE.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8535374402999878
    },
    {
      "name": "Gibbs sampling",
      "score": 0.5877723693847656
    },
    {
      "name": "Speech recognition",
      "score": 0.5752003192901611
    },
    {
      "name": "Vocabulary",
      "score": 0.5636716485023499
    },
    {
      "name": "Acoustic model",
      "score": 0.5302029848098755
    },
    {
      "name": "Language model",
      "score": 0.49822044372558594
    },
    {
      "name": "Bayesian probability",
      "score": 0.47541576623916626
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47234976291656494
    },
    {
      "name": "Parametric statistics",
      "score": 0.4499668478965759
    },
    {
      "name": "Natural language",
      "score": 0.41868114471435547
    },
    {
      "name": "Natural language processing",
      "score": 0.3920022249221802
    },
    {
      "name": "Speech processing",
      "score": 0.25512266159057617
    },
    {
      "name": "Linguistics",
      "score": 0.08118903636932373
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}