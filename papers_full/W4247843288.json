{
  "title": "Voice Activity Detection Applied to Hands-Free Spoken Dialogue Robot based on Decoding usingAcoustic and Language Model",
  "url": "https://openalex.org/W4247843288",
  "year": 2007,
  "authors": [
    {
      "id": "https://openalex.org/A5101708377",
      "name": "Hiroyuki Sakai",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5058481406",
      "name": "Tobias Cincarek",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5005388852",
      "name": "Hiromichi Kawanami",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5003814223",
      "name": "Hiroshi Saruwatari",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5021868389",
      "name": "Kiyohiro Shikano",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5035473671",
      "name": "Akinobu Lee",
      "affiliations": [
        "Nagoya Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130827473",
    "https://openalex.org/W2009422037",
    "https://openalex.org/W2162170885",
    "https://openalex.org/W6634685263",
    "https://openalex.org/W2114645826",
    "https://openalex.org/W1502984613",
    "https://openalex.org/W2032474878"
  ],
  "abstract": "Speech recognition and speech-based dialogue are means for realizing communication between humans and robots. In case of conventional system setup a headset or a directional microphone is used to collect speech with high signal-to-noise ratio (SNR). However, the user must wear a microphone or has to",
  "full_text": "V oice Activity Detection Applied to Hands-Free\nSpoken Dialogue Robot based on Decoding using\nAcoustic and Language Model\nHiroyuki Sakai, Tobias Cincarek\nHiromichi Kawanami, Hiroshi Saruwatari, Kiyohiro Shikano\nGraduate School of Information Science,\nNara Institute of Science and Technology\n8916–5, Takayama-Cho, Ikoma-City, Nara, 630–0192, Japan\nEmail: hiroyuki-s@is.naist.jp\nAkinobu Lee\nNagoya Institute of Technology, Japan\nEmail: ri@nitech.ac.jp\nAbstract—Speech recognition and speech-based dialogue\nare means for realizing communication between humans and\nrobots. In case of conventional system setup a headset or a\ndirectional microphone is used to collect speech with high\nsignal-to-noise ratio (SNR). However, the user must wear\na microphone or has to approach the system closely for\ninteraction. Therefore it’s preferable to develop a hands-free\nspeech recognition system which enables the user to speak to\nthe system from a distant point. To collect speech from distant\nspeakers a microphone array is usually employed. However,\nthe SNR will degrade in a real environment because of the\npresence of various kinds of background noise besides the\nuser’s utterance. This will most often decrease speech recog-\nnition performance and no reliable speech dialogue would be\npossible. Voice Activity Detection (V AD) is a method to detect\nthe user utterance part in the input signal. If V AD fails, all\nfollowing processing steps including speech recognition and\ndialogue will not work. Conventional V AD based on amplitude\nlevel and zero cross count is difﬁcult to apply to hands-free\nspeech recognition, because speech detection will most often\nfail due to low SNR.\nThis paper proposes a V AD method based on the acoustic\nmodel (AM) for background noise and the speech recognition\nalgorithm applied to hands-free speech recognition. There will\nalways be non-speech segments at the beginning and end of\neach user utterance. The proposed V AD approach compares\nthe likelihood of phoneme and silence segments in the top\nrecognition hypotheses during decoding. We implemented the\nproposed method for the open-source speech recognition en-\ngine Julius. Experimental results for various SNRs conditions\nshow that the proposed method attains a higher V AD accuracy\nand higher recognition rate than conventional V AD.\nI. INTRODUCTION\nRecently, automatic speech recognition (ASR) technol-\nogy has been applied to real environment applications such\nas speech guidance system, robots, car navigation systems,\nportable speech translators, etc. Speech is the easiest, most\nnatural and effective way for humans to communicate.\nTherefore speech should also be considered as an effective\nmethod for natural communication of humans with robots.\nIn a conventional speech recognition system, a headset or a\ndirectional microphone is employed to collect speech with\nhigh SNR. However, the user must wear the microphone or\nhas to approach the system closely for interaction. Therefore\nthe introduction of a hands-free ASR system that can be\nused without these burdens is expected for robots, etc. We\nhave been operating the speech oriented guidance system\n“Takemaru-kun”[1] (Fig.1) and “Kita-chan, Kita-robot”[2]\n(Fig.2) in real-environment for several years. Presently, each\nsystem uses a directional microphone. We are going to\nreplace it with a microphone array to realize a hands-free\ninterface.\nIn a hands-free automatic speech recognition system,\nSNR of the input signal will be worse than in a conven-\ntional systems due to the background noise from the real\nenvironment and that the user is not standing in front of the\nmicrophone. Low SNR is very likely to cause degradation\nin voice activity detection (V AD) and speech recognition\nperformance when using a conventional V AD[3] approach.\nThe purpose of V AD is to detect the user utterance part\nin the input signal. V AD is very important because if\nV AD fails, all following processing steps including speech\nrecognition and dialogue will also fail. Amplitude level\n(AL) and zero cross count (ZC) are employed for V AD in a\nconventional speech recognition system. Speech recognition\nis only carried out when the amplitude level of the input\nsignal exceeds a certain threshold. The lower the SNR,\nthe more likely that this conventional approach to speech\ndetection fails.\nThe employment of a “Push and Talk” interface would\nbe a different approach to realize a hand-free ASR\nsystem which is also effective in noisy environments.\nHowever, the requirement for the user to push a but-\nton once before and once after talking is very inconve-\nnient and would even require extra equipment. Therefore\n“Push and Talk” is no decent alternative to a real hands-\nfree ASR system. Speech/non-speech discrimination using\nframe-base GMMs[4], optimization of discrimination using\nAdaboost[5] and the Speech Starter interface[6] which\nis based on the detection of lip movements are further\nFig. 1. Real-Environment Speech Information Guidance System\n“Takemaru-kun” : System installed at the local community center since\nNovember 2002.\nFig. 2. Real-Environment Speech Train Information Guidance System\n“Kita-chan(back), Kitarobo(front)” : System installed at the local railway\nstation since March 2006.\nexamples of methods which can be employed for speech\ndetection. However, when considering the employment in a\nreal environment these methods are not enough to realize\npractical V AD considering both performance and real-time\ncapabilities.\nIn this paper, we propose a V AD method for hands-free\nspeech recognition which is based on the speech recognition\nalgorithm. For decoding the spoken text from the input\nsignal an acoustic model (AM) and a language model (LM)\nare employed. The acoustic model is employed to model\nthe characteristics of each phoneme of the target language\nand non-speech segments such as the background noise.\nThe language models determines which sentences should be\nrecognized by the system. As the conventional approach, the\nproposed method also employs a statistical model (GMM)\nto reject unwanted speech inputs, e.g. laughter, coughing\nand many other kinds of noise. Therefore, robust V AD can\nbe expected. The effectiveness of the proposed method is\nevaluated in this paper.\nFig. 3. Structure of Standard Speech Recognition System\nII. STANDARD SPEECH R ECOGNITION SYSTEM\nThe structure of a standard speech recognition and the\nprocessing steps of the system from speech input to utter-\nance veriﬁcation are shown in Figure 3. Input speech is\ncollected with a microphone and digitized by the computer\nhardware. Additionally, acoustic signal processing can be\napplied to improve SNR, e.g. by using noise suppression\nand blind source separation (BSS)[7]. After that, acoustic\nfeature extraction is carried out. As next step, V AD is em-\nployed to detect the user utterance part in the input signal.\nNon-speech input, e.g. laughing, coughing and other kinds\nof noise can be rejected after segmentation by V AD based\non utterance classiﬁcation using a statistical model, e.g. a\nGaussian mixture model (GMM)[8]. In parallel to voice\nactivity detection, the search for recognition hypotheses is\ncarried out most often using a HMM-based acoustic model\nand a statistical n-gram language model. The acoustic model\nmodels the acoustic characteristics of phonemes and the\nbackground noise. The language model models the connec-\ntion of words and the structure of sentences. Recognition\nworks by calculating the similarity between the segments\nof the input signal and a dynamically generated graph of\nrecognition hypotheses using both acoustic and language\nmodel. Finally, utterance veriﬁcation, i.e. classifying the\ninput either as speech or as noise, is conducted. Most often\nGMMs are employed to represent speech and many kinds\nof noise. In case of a speech-oriented dialogue system, the\nsystem also analyzes a user’s intention using the recognition\nresult and generates a system answer to respond to a\nuser’s request (dialogue management). In the following the\nconventional and proposed V AD method are explained in\ndetail.\nIII. THE A PPROACH\nA. Conventional Method\nV AD is carried out as preprocessing before actual speech\ndecoding. Although a few conventional V AD methods exist\nwe restrict their treatment to one example of a real-time\nFig. 4. Summary of Conventional VAD Method\nFig. 5. Block Diagram : Proposed Voice Activity Detection\nV AD method because this work considers the introduction\nof V AD into a real environment speech dialogue system.\nConsequently, V AD based on amplitude level (AL) and\nzero cross count (ZC) is considered here. The method is\ndesigned to make use of the difference of the amplitude\nlevel between the user utterance part and the background\nnoise, i.e. the signal-to-noise ratio (SNR). If SNR is high,\nit is easy to set a threshold to separate the utterance from\nthe background noise with high performance, because the\ndifference between the utterance amplitude level and the\nbackground noise level is large. However, for ASR under\nhands-free condition, adjustment of the amplitude threshold\nis very difﬁcult due to a low SNR. Therefore, performance\nof user utterance detection is likely to degrade. Fig. 4\ngives an overview to the conventional V AD method. The\nwaveform of the same utterance with two different SNRs is\nshown in Fig. 4 (left: high SNR, right: low SNR). This is\nan example where the low SNR utterance can be detected\nseparately using the same threshold as for the high SNR\nutterance. In order to prevent cutting of the start and end of\nthe utterance, start margin and end margin are employed\nbefore the beginning and after the end of the detection\npoints.\nB. Proposed Method\nWe propose a two-stage V AD for a hands-free ASR\napplication. Fig. 5 gives an overview to the proposed\nmethod. Segments of the input containing only laughter,\nFig. 6. Flow of VAD by decoding in First stage\nFig. 7. Concept of Recognition Flow in Proposed Method\ncoughing, car horn honk, clapping, etc. are deﬁned as\n“noise” segments. A noise segment has a temporarily high\namplitude level but it is not part of the utterance. Segments\ncontaining user speech are deﬁned as “voice” or “phoneme”\nsegments. Otherwise, a segment is deﬁned as “silence (non-\nutterance)”, i.e. it contains only background noise but no\nuser speech.\nThe following explains the two stages of the proposed\nV AD. Additionally, Fig. 6 shows the ﬂow of processing of\nthe proposed V AD which is carried out in parallel with the\ndecoding algorithm. Fig.7 shows the main steps of V AD\nand the decoding algorithm.\nProposed V AD:\n² First Stage\nV oice Activity Detection and Preliminary Decoding.\nThere will always be non-speech segments (silence)\nbefore and after user utterances. Frame Synchronous\nBeam Search (FSBS) is carried out using language\nmodel (LM) and acoustic model (AM). FSBS classiﬁes\nsegments as phonemes, silence or noise. A frame\nor a series of frames which are recognized as part\nof phonemes or series of phonemes are considered\nas “voice activity”. Otherwise, a sequence of silence\nframes of about 300 to 400 msec in duration is con-\nsidered as noise (cf. Fig. 6). There are two processing\nsteps. The ﬁrst is V AD based on decoding using AM\nand LM. The other is to search for one or more\nrecognition hypotheses.\n² Second Stage\nV oice activity segments from the ﬁrst stage are re-\nclassiﬁed either as voice or noise based on likelihood\nscores of GMMs for voice and noise. If the classiﬁca-\ntion result is noise, the input is rejected and V AD is\nrestarted. If the result is voice, the ﬁnal recognition\nhypothesis is calculated. The GMM-based classiﬁ-\ncation has the positive effect of preventing useless\ncomputation for noise segment which are rejected in\nany case.\nThere are two differences between the conventional\nmethod and the proposed method. Firstly, the proposed\nmethod is not considering amplitude level (AL) and zero\ncross count (ZC). Acoustic and linguistic models are em-\nployed instead. Secondly, V AD is carried out by decoding\nand is part of the actual speech recognition algorithm.\nThis is different to the conventional approach where V AD\nis carried out independently from the recognition process.\nWhile in the conventional method the search for recognition\nhypotheses is only carried out for segments actually cut\nby initial V AD, the complete input stream is processed\nby the recognition algorithm in the proposed approach.\nSince background noise and other noise segments should\nbe rejected, the start position for decoding has to be reset\nwhenever a longer background noise segment is detected.\nAfter a reset, the search is restarted at the beginning of the\nbackground noise segment detected last. A fatal processing\ndelay does not occur because the computational complexity\nof frame synchronous beam search (FSBS) is adjustable by\nchanging the beamwidth, etc. Consequently, the proposed\nV AD method works in real-time.\nThe acoustic model (AM) employed for decoding has\nbeen adapted to the target environment background noise\nby MLLR[9] using speech data collected in the target\nenvironment. This improves detection performance over\na conventional acoustic model because the environmental\nbackground noise can be recognized as silence effectively.\nMoreover GMM-based noise classiﬁcation makes it possible\nto reject various kinds of noise. We integrated the proposed\nV AD method into the open-source speech recognition de-\ncoder Julius[10][11].\nC. Overview of Julius\nFig. 8. Outline of System Organization of Julius\nJulius is a high-performance, real-time, two-pass large\nvocabulary continuous speech recognition (LVCSR) engine\nfor research and practical system development. Julius is\nopen-source software, may be distributed freely and even\nincluded in commercial software. It works on various\nsoftware platforms such as Windows, FreeBSD, Linux and\nother Unix derivatives. Julius can be employed for different\nlanguages by changing the acoustic (AM) and language\nmodels (LM). Therefore, it can be applied to a wide range\nof applications and is usable for arbitrary target languages.\nThe system’s organization and the ﬂow of processing dur-\ning recognition are shown in Fig. 8. Julius’ conventional\nV AD method uses amplitude level (AL) and zero cross\ncount (ZC). Mel-frequency cepstrum coefﬁcients (MFCC)\nare extracted as acoustic features. The search algorithm\nfor recognition hypotheses employs HMM-based acoustic\nmodel and a statistical n-gram language model. It is a two\npass algorithm. In the ﬁrst pass, frame synchronous beam\nsearch (FSBS) is carried out using a bi-gram language\nmodel to determine a preliminary recognition hypothesis.\nAfter the ﬁrst pass is ﬁnished, GMM-based classiﬁcation\nof the input into voice and noise is carried out. If the\nclassiﬁcation result is voice, the second pass is carried out\nwith stack decoding using a tri-gram language model. If the\nclassiﬁcation result is noise, the second pass is skipped and\nthe input rejected.\nD. Julius with the Proposed Method\nThe system organization of Julius that integrated the\nproposed method is shown in Fig.9. MFCC features for\neach input speech frames is extracted. The proposed V AD\nmethod is carried out using synchronous beam search\n(FSBS) based on acoustic model (AM) and language model\n(LM). The search also determines a preliminary recognition\nhypothesis. Next, the voice activity segments are reclassiﬁed\nusing the voice and noise GMMs. If the classiﬁcation result\nis voice stack decoding using the tri-gram language model\nis carried out. If the classiﬁcation result is noise, the input\nFig. 9. Julius that integrated Proposed Method\nis rejected, and frame synchronous beam search (FSBS) is\nrestarted.\nIV. EVALUATION E XPERIMENTS\nIn the following the experimental setup for evaluating the\nrecognition performances of the proposed and conventional\nV AD method is explained. For the conventional method\nseveral thresholds for amplitude level (AL) and zero cross\ncount (ZC) are considered.\nA. Experimental Conditions\nJulius as shown in Fig. 8 is employed for evaluating\nconventional V AD. Fig.9 shows Julius for evaluating the\nproposed V AD method. The Kita-chan system (cf. Fig.1)\nwas employed to collect the evaluation speech data for\nthree different SNR conditions which are controlled by\nchanging distance between the speaker and the microphone.\nKita-chan is a speech-oriented guidance system operated\nat a railway station near the author’s university in Nara,\nJapan. The acoustic model (AM) was constructed using\nthe data collected during two years by second speech-\noriented guidance system, Takemaru-kun (cf. Fig.2). The\nnumber of utterances for training an initial acoustic models\nwas 23,417 for adults and 120,671 for children. After that\nMLLR adaptation was carried out to adapt the AM to\nthe environment of the railway station. The background\nnoise level at the railway station was between 56 and 63\ndB(A). The adaptation data was collected during six weeks\nof regular system operation in the Kita-chan environment.\nThe amount of adaptation data is 6,661 utterances for adults\nand 9,472 utterances for children. Moreover, cepstrum mean\nnormalization (CMN) was employed to reduce acoustic mis-\nmatch due to speaker characteristics. Further experimental\nconditions are given in Table I.\nB. Feature of input data\nExamples of input speech waveforms for conditions 1 to\n3 are shown in Figs. 10-12, respectively. In Condition 1,\nthe SNR is high with about 50 dB. The difference between\nTABLE I\nE XPERIMENTAL C ONDITION\nInput Speech Condition 1 Close talk\n(2&3 are Condition 2 Collected from about 1m distance\nHands-Free) Condition 3 Collected from about 1.5m distance\nThreshold in amplitude 100ʙ1000\nConvention zero cross 0ʙ100 (times/sec) (default=60)\nAcoustic Model(AM) 2000 states, PTM, Gaussian\nAcoustic Features 12MFCC, 12 ¢MFCC, ¢E\nAM Training Baum-Welch, 3 Iterations\nAM Adaptation MLLR-MAP, 3 Iterations, 256 Classes\nLanguage Model(LM) 3-gram, Kneser-Ney smoothing\nV ocabulary size is 40k.\nTask guidance of railway station, train information,\nsightseeing, institutions, local area information,\nnews, weather forecast, greetings and conversation\nEvaluation 1 speaker, 204 Utterances, 1024 words\nData OOV = 0% (no unknown word).\nFig. 10. Input Wave in Condition 1 (SNR = 50dB, close talk)\nFig. 11. Input Wave in Condition 2 (SNR = 10dB, 1m distance)\nFig. 12. Input Wave in Condition 3 (SNR = 6dB, 1.5m distance)\nthe amplitude of the utterance and the background noise\nis large. In Condition 2, SNR is about 10dB and the level\ndifference between the utterance and the background noise\nis quite small. The part which corresponds to the start and\nthe end of the utterance has almost been buried in the\nbackground noise. In Condition 3, SNR is only about 6dB\nwhich is worse than Condition 2.\nV. EXPERIMENTAL R ESULT\nFigures 13 to 15 show word accuracy in Conditions 1-3.\nThe results of the proposed method are shown as planes\nfor various thresholds of the amplitude level (AL) and\nzero cross count (ZC). The effectiveness of the proposed\nmethod is clear from the experiment result. In hands-\nfree condition (2 and 3), the recognition performance of\n 80\n 82\n 84\n 86\n 88\n 90\n 92\n 94\n 96\n 0  200  400  600  800  1000 0 20 40 60 80 100\n 80\n 82\n 84\n 86\n 88\n 90\n 92\n 94\n 96\n 98\n 100 Proposed Method\nConventional\nMethod\nAmplitude Level Threshold\nZero Cross\nThreshold\n[times/sec]\nWord Accuracy[%]\n 100\n 95.88\nFig. 13. Result of Condition 1 (50dB SNR, close talk)\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n 0  200  400  600  800  1000 0 20 40 60 80 100\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n 100 Proposed Method\nConventional\nMethod\nAmplitude Level Threshold\nZero Cross\nThreshold\n[times/sec]\nWord Accuracy[%]\n 100\n 82.96\nFig. 14. Result of Condition 2 (10dB SNR, 1m distance)\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 0  200  400  600  800  1000 0 20 40 60 80 100 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n 100Word Accuracy[%]\nProposed Method\nConventional\nMethod\nAmplitude Level Threshold\nZero Cross\nThreshold\n[times/sec]\n 100\n 71.37\nFig. 15. Result of Condition 3 (6dB SNR, 1.5m distance)\nthe conventional V AD method depends greatly on the AL\nthreshold. Moreover, the performance is lower than the\nproposed method. The dependency on ZC is small. It has\nonly a small inﬂuence on the recognition performance. The\nresults show that the performance of the proposed method\ndegrades to some extent in hands-free condition, but it is\nmuch higher than when using the conventional method.\nMoreover, the experimental results show that the proposed\nmethod is also better than the conventional method even in\ncase of condition 1. The results for ASR performance are\ngiven by the word correct rate (cf. Figs. 16 and 17), since it\nmore related related to the system’s response performance\nthan word accuracy and the investigation in this paper\nconsiders introduction of hands-free speech recognition to a\nspeech dialogue systems. Fig. 16 shows ASR performance\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 45\n 50\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 95\n 100\n 0  200  400  600  800  1000\nWord Correct Rate[%]\nLevel Threshold\n 100\nConventional Method(condition 1)\nConventional Method(condition 3)\nConventional Method(condition 2)\nFig. 16. Recognition rate of conventional method (ZC threshold 60)\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 45\n 50\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 95\n 100\n 0  200  400  600  800  1000\nWord Correct Rate[%]\nLevel Threshold\nProposed Method(condition 1)\nProposed Method(condition 3)\nProposed Method(condition 2)\nFig. 17. Recognition rate of proposed method (ZC threshold 60)\n 50\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 95\n 100\n 0 10 20 30 40 50\nWord Accuracy[%]\nSNR[dB]\n(close talk)\nCondition1\nProposed Method\nConventional Method\n 6\n(1m) (1.5m)\nCondition2 Condition3\nFig. 18. Dependency of Recognition rate on SNR\nof the the conventional method. Fig. 17 shows results for\nthe proposed method. Furthermore, the change of speech\nrecognition rate depending on conditions 1-3 is shown in\nFigs. 18. It is clear that the proposed method improves the\nperformance in hands-free condition remarkably.\nFinally, it is conﬁrmed whether V AD itself is effective.\nAs indicators to evaluate V AD performance, the number of\n“False Rejections” and the number of “False Acceptions”\nare deﬁned as follows:\n² False Rejection\nUtterance detection fails, i.e. a segment which is actu-\nally voice is rejected as noise or background noise. For\nsuch segments speech recognition is not carried out in\nthe end. For the conventional V AD method, detection\nof the user utterance fails if AL threshold is higher\nthan the utterance AL.\n² False Acceptance\nA segment which is not part of the user utterance is\ndetected as part of a user utterance, e.g. a background\nnoise segment is recognized as part of a user’s utter-\nance. In the conventional method, background noise is\nvery likely to be classiﬁed as part of the utterance if\nthe AL threshold is lower than the background noise.\nFigure.19 shows the number of false rejections, Fig. 20\nshows the number of false acceptions. Fig. 19 and Fig. 20\nboth show the typical recognition result for conditions 1\nand 3. For ZC a threshold of 60 is selected because the\nresult is not very dependent on the ZC threshold. In the\nproposed V AD method, the number of errors occurring\nis constant, and performance improves in comparison to\nthe conventional method. Some errors may occur without\naffecting SNR because the proposed method processes the\ncomplete input stream and many kinds of noises exist in\na real environment. The conventional method needs an\nappropriate threshold setting. However, it is difﬁcult to\nrealize that for low SNR.\nVI. C ONCLUSION\nIn this paper, we proposed a method for V AD using\nacoustic model and language model by speech decoding for\nreal-environment hands-free speech recognition. Moreover,\nwe integrated the proposed method into the open-source\nspeech recognition decoder Julius. We evaluated the pro-\nposed method and compared its performance to the con-\nventional V AD method which is based on amplitude level\nand zero cross count. Experimental results show that for\nhigh SNR, both the conventional and the proposed method\nwork well. However, when SNR is low due to a hands-\nfree setup the conventional V AD causes more misdetections.\nThe proposed V AD method outperformed the conventional\nmethod w.r.t to both V AD and ASR performance.\nThe proposed method requires to adapt the acoustic\nmodel to the target environment in order to recognize\nthe background noise as silence effectively. Adaptation is\npossible with a small amount of data collected in the target\nenvironment. Consequently, the proposed method can be\neasily applied for various kinds of real-environment ASR\napplications. Apart from that, the system will always have a\nhigh computational load with the proposed method since it\nhas to process the complete input stream. In future work, we\nare going to employ face detection to detect whether a user\nis present to control termination and restart of the recog-\nnition engine. This will decrease the computational load\nof the system and may further improve ASR performance.\n 0\n 20\n 40\n 60\n 80\n 100\n 0  200  400  600  800  1000\nFalse Rejection Count[%]\nLevel Threshold\nConventional Method(condition 1)\nConventional Method(condition 3)\nProposed Method(condition 1)\nProposed Method(condition 3)\n 100\n 10\n 30\n 50\n 70\n 90\nFig. 19. False Rejection Count\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 0  200  400  600  800  1000\nNumber of Occurrences[times]\nLevel Threshold\nConventional Method(condition 1)\nConventional Method(condition 3)\nProposed Method(condition 1)\nProposed Method(condition 3)\n 100\nFig. 20. Number of False Acceptance\nMoreover, the speaker’s position can be obtained from\nimage processing and employed as initial value for direction\nof arrival estimation which is necessary for many signal\nprocessing techniques. On the other hand, there is also\nnew potential by recognizing the complete input stream.\nFor example, sound of running water, blowing of the wind,\nblaze and whistling kettle, etc. could be detected in order\nto asses the environment in which the system is currently\nused.\nA CKNOWLEDGMENT\nThis work is partly supported by MEXT e-society leading\nproject.\nR EFERENCES\n[1] R. Nisimura, A. Lee, H. Saruwatari, and K. Shikano, “Public Speech-\noriented Guidance System with Adult and Child Discrimination Ca-\npability”, proc.of ICASSP, pp.433–436, 2004.\n[2] H. Kawanami, M. Kida, N. Hayakawa, T. Cincarek, T. Kitamura,\nT. Kato and K. Shikano, “Spoken Guidance Systems Kita-chan\nand Kita-chan robot.Their Development and Operation in a Railway\nStation” , tech.rep., IEICE, SP2006–14, 2006.\n[3] L. R. Rabiner, M. R. Sambur: “An Algorithm for Determining the\nEndpoints of Isolated Utterances”, BSTJ, vol.54, No.2, pp.297–315,\n1975.\n[4] Norbert Binder, Konstantin Markov, Rainer Gruhn, Satoshi Nakamura,\n“Speech/Non-Speech Separation with GMMs” , Proc.of ASJ Fall\nMeeting, V ol1, pp.141–142, 2001\n[5] Oh-Wook Kwon, Te-Won Lee, “Optimizing speech/non-speech classi-\nﬁer design using AdaBoost”, Acoustic, Speech, and Signal Processing,\n2003.Proceedings.(ICASSP;03).2003 IEEE International Conference\non V olume 1, Issue, 6–10 April 2003 Page(s):l436-l.439 vol.1\n[6] K. Kitayama, M. Goto, K. Itou, T. Kobayashi, “Speech Starter :\n“SWITCH” on Speech”, IPSJ SIG Technical Report, 2003–SLP–46–\n12, V ol.2003, No.58, pp.67–72, May 2003.\n[7] S. Haykin, Ed., “Unsupervised Adaptive Filtering (V olume I, Blind\nSource Separation)”, John Wiley & Sons, 2000.\n[8] G. McLachlan, “Finite Mixture Models”, Wiley-Interscience, 2000.\n[9] M. Yamada, A. Baba, S. Yoshizawa, Y . Mera, A. Lee, H. Saruwatari,\nK. Shikano, “Unsupervised Acoustic Model Adaptation Algorithm\nUsing MLLR in Noisy Environment”\n[10] Open-Source Large V ocabulary CSR Engine Julius\ndeveloped by A.LEE\nintroductory web pages at:http://julius.sourceforge.jp/\n[11] A. Lee, T. Kawahara and K. Shikano. “Julius - an open source\nreal-time large vocabulary recognition engine”, Proc Eurospeech2001,\npp1691–1694, 2001",
  "topic": "Headset",
  "concepts": [
    {
      "name": "Headset",
      "score": 0.8992680311203003
    },
    {
      "name": "Microphone",
      "score": 0.8288719654083252
    },
    {
      "name": "Computer science",
      "score": 0.7464961409568787
    },
    {
      "name": "Speech recognition",
      "score": 0.7291294932365417
    },
    {
      "name": "Decoding methods",
      "score": 0.6866185665130615
    },
    {
      "name": "Robot",
      "score": 0.607008159160614
    },
    {
      "name": "Voice activity detection",
      "score": 0.5709720253944397
    },
    {
      "name": "SIGNAL (programming language)",
      "score": 0.4856361448764801
    },
    {
      "name": "Noise (video)",
      "score": 0.43679118156433105
    },
    {
      "name": "Speech processing",
      "score": 0.4202635884284973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3546275198459625
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3497743010520935
    },
    {
      "name": "Telecommunications",
      "score": 0.07056796550750732
    },
    {
      "name": "Sound pressure",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I75917431",
      "name": "Nara Institute of Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I197274945",
      "name": "Nagoya Institute of Technology",
      "country": "JP"
    }
  ],
  "cited_by": 8
}