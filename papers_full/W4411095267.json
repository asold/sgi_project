{
  "title": "Vision-language foundation models for medical imaging: a review of current practices and innovations",
  "url": "https://openalex.org/W4411095267",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4318149815",
      "name": "Ji Seung Ryu",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2415205131",
      "name": "Hyunyoung Kang",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A3208451250",
      "name": "Yuseong Chu",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2123470950",
      "name": "Sejung Yang",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A4318149815",
      "name": "Ji Seung Ryu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2415205131",
      "name": "Hyunyoung Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3208451250",
      "name": "Yuseong Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123470950",
      "name": "Sejung Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4312533035",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W3095986980",
    "https://openalex.org/W3197217317",
    "https://openalex.org/W4404509535",
    "https://openalex.org/W4387580644",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4399393690",
    "https://openalex.org/W2157640836",
    "https://openalex.org/W2963967185",
    "https://openalex.org/W2948141910",
    "https://openalex.org/W6814982423",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3205071530",
    "https://openalex.org/W6600553734",
    "https://openalex.org/W6600617704",
    "https://openalex.org/W4312605942",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4402727705",
    "https://openalex.org/W6629651325",
    "https://openalex.org/W4401879424",
    "https://openalex.org/W4399952271",
    "https://openalex.org/W3164654615",
    "https://openalex.org/W4283821415",
    "https://openalex.org/W4392795030",
    "https://openalex.org/W4403069390",
    "https://openalex.org/W4387211350",
    "https://openalex.org/W4402671465",
    "https://openalex.org/W4403805874",
    "https://openalex.org/W4400136387",
    "https://openalex.org/W4401749938",
    "https://openalex.org/W6601462914",
    "https://openalex.org/W4401752707",
    "https://openalex.org/W4408352617",
    "https://openalex.org/W4403650311",
    "https://openalex.org/W4402753927",
    "https://openalex.org/W4403015917",
    "https://openalex.org/W6609098157",
    "https://openalex.org/W4405102058",
    "https://openalex.org/W4403150348",
    "https://openalex.org/W4404584849",
    "https://openalex.org/W4404788908",
    "https://openalex.org/W4396494945",
    "https://openalex.org/W4403791409",
    "https://openalex.org/W4408616263",
    "https://openalex.org/W4403649762",
    "https://openalex.org/W6966891254",
    "https://openalex.org/W4394867514",
    "https://openalex.org/W4307006119",
    "https://openalex.org/W4396766375",
    "https://openalex.org/W4387789891",
    "https://openalex.org/W4323066559",
    "https://openalex.org/W4304092062",
    "https://openalex.org/W4399758263",
    "https://openalex.org/W4386195020",
    "https://openalex.org/W4401749553",
    "https://openalex.org/W4382998948",
    "https://openalex.org/W4402781391",
    "https://openalex.org/W4401397240",
    "https://openalex.org/W4405022378",
    "https://openalex.org/W4396819284",
    "https://openalex.org/W4404986253",
    "https://openalex.org/W4405626241",
    "https://openalex.org/W4404648570",
    "https://openalex.org/W4401750760",
    "https://openalex.org/W4403650337",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W2977650145",
    "https://openalex.org/W4280488276",
    "https://openalex.org/W2979448322",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W3101156210"
  ],
  "abstract": "The online version contains supplementary material available at 10.1007/s13534-025-00484-6.",
  "full_text": "REVIEW ARTICLE\nBiomedical Engineering Letters (2025) 15:809–830\nhttps://doi.org/10.1007/s13534-025-00484-6\nadvancements spurred by the development of founda -\ntion models, large language models (LLMs), and vision-\nlanguage models (VLMs). These cutting-edge innovations \nhave transformed the fields of computer vision and natu -\nral language processing (NLP), introducing versatile and \nefficient methodologies to address a wide array of visual \nunderstanding tasks. AI/ML systems have become indis -\npensable in achieving significant progress across various \ndomains, including object detection, image segmentation, \nor multimodal applications such as visual question answer -\ning (VQA) and cross-modal retrieval. Foundation models \nrepresent a fundamental shift in AI/ML approaches. Unlike \ntraditional deep learning models, which rely heavily on \ntask-specific annotated datasets, these models utilize exten-\nsive and diverse datasets during pre-training [ 1]. This pre-\ntraining spans multiple data modalities, including images, \ntext, and their multimodal combinations, which allows \nfoundation models to develop generalized representations \nthat require minimal additional training for downstream \ntasks. Models such as CLIP [ 2] and DINO [ 3] epitomize \n1 Introduction\n1.1 History of foundation models and recent trends\nOver the past decade, artificial intelligence (AI) and \nmachine learning (ML) have experienced groundbreaking \nJi Seung Ryu and Hyunyoung Kang equally contributed to this work.\n \r Sejung Yang\nsyang@yonsei.ac.kr\nJi Seung Ryu\nryujissss@yonsei.ac.kr\nHyunyoung Kang\nsonya23@yonsei.ac.kr\n1 Department of Precision Medicine, Yonsei University Wonju \nCollege of Medicine, Wonju, Korea\n2 Department of Medical Informatics and Biostatistics, Yonsei \nUniversity Wonju College of Medicine, Wonju, Republic of \nKorea\nAbstract\nFoundation models, including large language models and vision-language models (VLMs), have revolutionized artificial \nintelligence by enabling efficient, scalable, and multimodal learning across diverse applications. By leveraging advance -\nments in self-supervised and semi-supervised learning, these models integrate computer vision and natural language pro -\ncessing to address complex tasks, such as disease classification, segmentation, cross-modal retrieval, and automated report \ngeneration. Their ability to pretrain on vast, uncurated datasets minimizes reliance on annotated data while improving \ngeneralization and adaptability for a wide range of downstream tasks. In the medical domain, foundation models address \ncritical challenges by combining the information from various medical imaging modalities with textual data from radiol -\nogy reports and clinical notes. This integration has enabled the development of tools that streamline diagnostic workflows, \nenhance accuracy (ACC), and enable robust decision-making. This review provides a systematic examination of the recent \nadvancements in medical VLMs from 2022 to 2024, focusing on modality-specific approaches and tailored applications \nin medical imaging. The key contributions include the creation of a structured taxonomy to categorize existing models, \nan in-depth analysis of datasets essential for training and evaluation, and a review of practical applications. This review \nalso addresses ongoing challenges and proposes future directions for enhancing the accessibility and impact of foundation \nmodels in healthcare.\nKeywords Foundation model · Vision-language model · Medical imaging · Deep learning\nReceived: 15 January 2025 / Revised: 30 April 2025 / Accepted: 20 May 2025 / Published online: 6 June 2025\n© The Author(s) 2025\nVision-language foundation models for medical imaging: a review of \ncurrent practices and innovations\nJi Seung Ryu1 · Hyunyoung Kang2 · Yuseong Chu1 · Sejung Yang1,2\n1 3\nBiomedical Engineering Letters (2025) 15:809–830\nthis paradigm by employing large-scale, self-supervised \nlearning to align visual and textual data, thereby performing \nefficiently across a variety of applications.\nCritical differentiators between foundation models \nand earlier deep learning architectures are their scalabil -\nity, adaptability, and efficiency. Traditional models often \nrequire large, labeled datasets and significant computational \nresources for task-specific training. In contrast, foundation \nmodels leverage self-supervised or unsupervised learning \ntechniques, drawing on large, uncurated datasets such as \nweb-crawled image-text pairs [4]. This approach minimizes \nthe reliance on annotated data while enabling the extraction \nof rich, transferable representations. Consequently, founda-\ntion models not only reduce the computational overhead \nbut also address a broader spectrum of vision-related tasks. \nMoreover, these models demonstrate a remarkable abil -\nity to generalize visual features across different domains \nand tasks. The modular architecture of foundation models \nfurther enhances their utility by supporting incremental \nfine-tuning, thereby enabling seamless adaptation to new \ndomains or tasks with minimal computational effort.\nIn computer vision and NLP, foundation models have \ndriven revolutionary advancements in complex multimodal \napplications. Tasks such as cross-modal retrieval, action \nrecognition, and high-level semantic understanding benefit \nfrom the robustness of these models. LLMs, including GPT-3 \n[5], PaLM [6], Galactica [7], and LLaMA [8] are pretrained \non vast text corpora using self-supervised learning tech -\nniques. These models are particularly adept at zero-shot and \nfew-shot learning, allowing them to perform a wide range of \ntasks with minimal fine-tuning. Unlike LLMs, VLMs focus \non integrating visual and language modalities. By leverag -\ning paired datasets during pre-training, models such as CLIP \n[2] align images with text, making them highly effective for \ntasks requiring multimodal reasoning.\nHealthcare is a field that naturally demands diverse data \ntypes—medical imaging, clinical records, and laboratory \nresults, to name a few. Foundation models such as LLMs and \nVLMs are well-suited for addressing this complexity. For \ninstance, LLMs reduce the reliance on task-specific training \nby efficiently extracting critical insights from unstructured \ntextual data. They enable the seamless analysis of electronic \nhealth records (EHR) and support natural language-driven \ndecision-making [9]. Simultaneously, VLMs excel in bridg-\ning textual and visual data and tackling tasks such as cross-\nmodal retrieval, disease diagnosis, and automated medical \nreport generation. Their extensive pretraining allows them \nto generalize across applications, thereby minimizing man -\nual efforts and enhancing accuracy (ACC) [10].\nThese foundation models contribute to the transformative \nreshaping of healthcare workflows. GatorTron [9] optimizes \nEHR analysis, improves clinical documentation, and enables \nfaster access to critical patient data. In interactive settings, \nChatDoctor enhances patient-provider communication with \nconversational AI capabilities, bridging gaps in understand-\ning [11]. Visually, VLMs have proven to be indispensable \nfor multimodal applications. BioViL [10] combines imaging \nand textual data to support disease classification and report-\ning, which are critical requirements for modern diagnostics. \nBy enhancing diagnostic ACC, reducing manual workloads, \nand delivering comprehensive insights across multiple \nmodalities, these foundation models can redefine the future \nof healthcare. Their ability to integrate and analyze diverse \ndatasets not only improves efficiency but also paves the way \nfor more personalized and effective patient care.\nThis article presents a comprehensive review of foun -\ndation models, emphasizing their applications in medical \nimaging and the recent advancements in VLMs within the \nmedical domain. We have organized and evaluated exist -\ning studies to provide a structured and insightful overview. \nOur analysis highlights the applications and strengths of \nthese models, focusing specifically on research published \nbetween 2022 and 2024, to capture the latest developments \nin this dynamic field. A key highlight of this review is the \nmeticulously curated summary of the datasets used for \ntraining and evaluation, which provide a valuable resource \nfor researchers. Additionally, by categorizing models based \non medical imaging modalities, we offer in-depth insights \ninto the unique challenges and tailored solutions associated \nwith each imaging modality.\nSpecifically, this paper focuses on the application of \nVLMs in the medical imaging domain, offering a structured \nanalysis of studies in this field. To complement these find -\nings, Fig. 1 presents a four-part visual taxonomy that clas -\nsifies the reviewed studies by imaging modality (Fig. 1a), \nanatomical target (Fig. 1b), task type (Fig. 1c), and data \nsource (Fig. 1d). \nThis review is intended to serve as a guiding framework \nfor researchers, to foster deeper exploration and collabo -\nration between the vision and medical communities. The \nmajor contributions of this study are as follows:\n ● This review presents a structured taxonomy and thor -\nough analysis of vision-language foundation models in \nmedical imaging, with a focus on groundbreaking re -\nsearch conducted between 2022 and 2024 (Fig. 1).\n ● By categorizing the models according to their medical \nimaging modalities, we provide detailed insights into \nthe modality-specific challenges and innovative solu -\ntions designed to address them.\n ● Furthermore, through a comparative evaluation of mod-\nel performance across tasks and modalities, we empha -\nsize the clinical applicability and practical implications \nof VLMs.\n1 3\n810\nBiomedical Engineering Letters (2025) 15:809–830\n ● This review highlights the key applications and strengths \nof existing methodologies and proposes directions for \nfuture research.\n1.2 Prior reviews on foundation models and the \nmedical domain\nWang et al. [12] explored the impact of deep learning meth-\nodologies on medical image analysis, with a particular focus \non advances in convolutional neural networks (CNNs). \nTheir review delved into applications such as disease \ndetection, image segmentation, and classification, while \naddressing critical challenges such as data scarcity, model \ninterpretability, and the integration of these techniques into \nclinical workflows. Suganyadevi et al. [13] provided a broad \nanalysis of deep learning approaches across various medical \nimaging modalities, including magnetic resonance imaging \n(MRI), CT, and X-rays. By covering the entire technical \npipeline—from preprocessing and model development to \nevaluation—their work also highlighted practical imple -\nmentation barriers. To address these challenges, they offered \nactionable recommendations aimed at facilitating real-\nworld adoption. Azad et al. [ 14] focused on the emerging \nrole of foundation models in medical imaging, emphasizing \ntheir scalability and adaptability to downstream tasks. Their \nreview categorized the existing foundation models based on \nthe architectural design and pretraining strategies, offering a \ncritical assessment of their limitations, and proposed future \nresearch directions to enhance the efficacy of these models \nand broaden their applicability in medical contexts. Hartsock \net al. [15] examined the application of VLMs to tasks such \nas medical report generation and VQA. By investigating \nthe advancements in aligning visual and textual data, their \nreview highlighted commonly used datasets and evaluation \nmetrics. They also discussed the potential of these models \nin streamlining healthcare workflows by improving clini -\ncal documentation and decision support. Zhang et al. [ 16] \naddressed the challenges of deploying foundation models \nfor medical image analysis, particularly those related to data \navailability, bias, and clinical validation. These issues often \nhinder the transition from research to practical application. \nTheir review emphasized the need for model interpretabil -\nity and robust evaluation frameworks to ensure clinical rel -\nevance, offering a forward-looking perspective on bridging \nthe gap between innovation and implementation.\n2 Research approach\nWe conducted an extensive search using Google Scholar \nand Arxiv, utilizing the advanced search tools available on \nthese platforms. Custom queries were developed to compile \na diverse and comprehensive collection of academic studies. \nThis process encompassed multiple types of publications, \nincluding peer-reviewed journal articles, conference papers, \nworkshop materials, preprints, and other non-peer-reviewed \nwork. To ensure this breadth and diversity, our search crite-\nria were carefully tailored to capture the full scope of rel -\nevant research.\nFig. 1 Distribution of founda-\ntion model in medical field. The \ndiagrams provide an analysis of \nthe training datasets utilized in the \nreviewed studies. Each subfigure \nillustrates the distribution of key \naspects: a imaging modalities, b \ntarget classifications, c organs of \nfocus, and d data sources. The total \nnumber of papers included in the \nanalysis is 61\n \n1 3\n811\nBiomedical Engineering Letters (2025) 15:809–830\nensuring coherence and clarity in the synthesis of findings. \nUltimately, this review aims to serve as a resource for both \nresearchers and clinicians by offering a comprehensive \nunderstanding of the current state of medical VLMs, iden -\ntifying prevailing limitations, and outlining potential direc -\ntions for future research.\n2.1 Review organization\nThe remainder of this review is organized, as follows. Sec -\ntion 2 provides an overview of the foundational principles \nunderlying foundation models and their significance in the \nhealthcare domain. It also summarizes the major tasks in \nmedical imaging and classifies the primary frameworks of \nthe foundation models used in this field. Section 3 focuses \non VLMs used in medical imaging. It distinguishes between \nSpecialist VLMs tailored for specific imaging modali -\nties, such as CT, X-ray, and fundus; and Generalist VLMs \ndesigned to handle multiple imaging modalities for diverse \napplications (Fig. 2). Section 4 addresses the challenges \nin medical VLMs, including dataset bias, inadequate mul -\ntilingual representation, and the limitations of evaluation \nmetrics, along with an analysis of the overall trends in \nmethodology adoption. It highlights the widespread use of \ncross-modal alignment for scalability, whereas multimodal \nattention and encoder–decoder integration face computa -\ntional challenges.\nThe queries were carefully crafted to include the follow-\ning keywords: (foundation*| generalist*| medical*| [Task]), \n(med-[FM]| medical vision language), and (foundation*| \nbiomedical*| image*| model*). Here, [FM] denotes well-\nknown foundation models such as PaLM and CLIP and \n[Task] denotes specific tasks, such as segmentation and \nquestion answering, within the context of medical imaging.\nTo provide a structured overview of this emerging field, \nthis review adopts a narrative synthesis approach, focus -\ning specifically on VLMs applied in medical imaging. The \nobjective is to analyze recent advances in architectures, \ndata modalities, and clinical applications, with emphasis \non interpretability, scalability, and domain-specific chal -\nlenges. Studies were selected for inclusion based on the \nfollowing criteria: (1) publication between 2020 and 2024, \n(2) application of VLMs to medical domains including \nradiology, pathology, and ophthalmology, and (3) the pres -\nence of experimental results or evaluations conducted on \nclinical datasets. Studies that were purely theoretical or not \ndirectly related to medical tasks were excluded. Reflecting \nthe rapid and ongoing developments in vision-language \nmodels within the medical imaging domain, the literature \nsearch strategy incorporated preprints published between \n2022 and 2024. Scientific rigor and reliability were pre -\nserved by applying a critical appraisal process, through \nwhich only preprints demonstrating sound methodological \nquality and adequate experimental validation were retained. \nIn addition, non-English studies were excluded in order to \nminimize the risk of misinterpretation stemming from lin -\nguistic ambiguity or inconsistencies in translation, thereby \nFig. 2 Organization of the review \npaper. The proposed taxonomy \norganizes foundational models \nin medical field into two broad \ncategories. Specific-domain trans-\nfer applications, which include \nX-ray, CT, fundus imaging, MRI, \nand other medical imaging types. \nMulti-domain integrated applica-\ntions, which combine insights \nacross multiple imaging modalities\n \n1 3\n812\nBiomedical Engineering Letters (2025) 15:809–830\nmedical images. These tasks are essential for early diagnosis \nand treatment planning. MedYOLO [19], a 3D object detec-\ntion framework based on the YOLO family, was introduced \nand specifically tailored for medical imaging applications. \nThe model has demonstrated an exceptional performance in \ndetecting various medical structures, highlighting its poten-\ntial for use in clinical workflows.\n3.1.4 Retrieval\nRetrieval tasks focus on identifying visually or semantically \nsimilar images from medical datasets and play a critical \nrole in comparative diagnosis and research. This capability \nis particularly valuable in fields such as radiology, pathol -\nogy, and dermatology, where historical cases often guide \ndiagnostic decisions. Lehmann et al. [ 20] proposed a com -\nprehensive framework for content-based image retrieval \nin medical applications. By incorporating feature extrac -\ntion and relevance feedback mechanisms, their system can \nsignificantly improve the retrieval ACC across multimodal \ndatasets.\n3.1.5 VQA\nVQA integrates visual understanding with clinical reason -\ning to address natural language questions regarding medical \nimages. This task is particularly vital in domains such as \nradiology and pathology, where clinicians require targeted \ninsights from imaging data. Ben Abacha et al. [21] achieved \nsignificant strides in this area by developing the VQA-Med \nbenchmark dataset. Designed to evaluate the VQA mod -\nels in medical imaging, the dataset features clinically rel -\nevant questions related to imaging findings and diagnostic \ntasks. By providing a standardized resource, VQA-Med has \nbecome instrumental in advancing VQA systems for medi -\ncal applications.\n3.1.6 Image captioning\nImage captioning automates the generation of textual \ndescriptions for medical images, enhancing documenta -\ntion and communication among healthcare professionals. \nWang et al. [ 22] introduced TieNet, a model that embeds \nradiological images and reports into a shared representation \nspace to produce descriptive captions for chest radiograph. \nBy aligning visual data with textual representations, TieNet \ncan improve the efficiency of automated reporting systems \nand support streamlined radiological workflows.\n3 Preliminary information\nThe concept of “foundation models” was first introduced \nby the Stanford Institute for Human-Centered AI, which \ndefined them as “base models trained on large-scale data \nin a self-supervised or semi-supervised manner, adaptable \nfor various downstream tasks” [ 1]. These models are built \non the principles of deep learning, such as deep neural net -\nworks and self-supervised learning, and are influenced by \nthe development of LLMs. Their growth has been driven \nby the scaling up of both data and model sizes, thereby \nenabling their use across many fields. This section discusses \nthe main tasks that foundation models address in the medi -\ncal field, their underlying architectures, and the factors that \nmake them effective for medical applications.\n3.1 Primary tasks in the medical field\n3.1.1 Classification and zero-shot classification\nClassification is a cornerstone task in medical imaging, \nin which models predict categories such as disease types \nor imaging conditions. Zero-shot classification, a more \nadvanced approach, utilizes pre-trained VLMs to classify \nimages without requiring fine-tuning using task-specific \ndata. This capability is particularly valuable in scenarios \nwhere labeled datasets are scarce. One notable example \nis CheXNet [ 17] which achieves a radiologist-level per -\nformance in detecting pneumonia from chest radiographs. \nBy leveraging DenseNet architecture and the large-scale \nlabeled dataset ChestX-ray14, the study highlights the criti-\ncal role of extensive datasets in achieving high diagnostic \naccuracy.\n3.1.2 Segmentation\nSegmentation focuses on identifying and delineating spe -\ncific anatomical structures or regions of interest such as \ntumors, organs, or lesions. This task is crucial for applica -\ntions such as treatment planning and surgical procedures. \nThe U-Net architecture introduced by Ronneberger et al. \n[18] has become the gold standard for biomedical image \nsegmentation. U-Net features an encoder–decoder design \nenhanced with skip connections and excels in precise \nboundary delineation, even with limited training data. Its \nadaptability makes it an indispensable tool for a wide range \nof medical imaging tasks.\n3.1.3 Detection\nDetection tasks are centered on identifying and localizing \nabnormalities, such as nodules, fractures, or tumors, within \n1 3\n813\nBiomedical Engineering Letters (2025) 15:809–830\ncapabilities across various tasks by pre-training on 400 mil-\nlion image-text pairs. CLIP utilizes a vision transformer \n(ViT) or ResNet as its image encoder and a transformer for \ntext encoding, jointly optimizing them using contrastive \nloss. ALIGN [25] has extended this approach using a larger \ndataset, demonstrating state-of-the-art results in image-text \nretrieval. Subsequent advancements, such as CLOOB [ 26] \nand DeCLIP [27] have focused on improving robustness and \nefficiency by integrating self-supervised learning objectives \nand better sampling strategies for contrastive pairs. In medi-\ncal imaging, encoder based alignment models facilitate the \ndevelopment of robust retrieval systems that match medi -\ncal images with their corresponding textual annotations or \nreports. This capability can significantly enhance the effi -\nciency of case-based reasoning and diagnostic support in \nradiology.\n3.2.2 Encoder based multimodal attention\nEncoder based multi-modal attention combines the visual \nand textual inputs within a unified encoder architecture \n(Fig. 3b). By embedding both modalities into a single \nencoder, the model learns joint representations that capture \ntheir contextual relationships through layer-wise interaction. \nUnlike cross-modal alignment, which processes the modali-\nties separately, this approach uses self-attention mecha -\nnisms to model cross-modal interactions directly within an \nencoder, thereby enabling joint representation learning. An \nexample of this methodology is SimVLM [28], which treats \nimage patches and text tokens as inputs to a shared encoder, \nusing attention layers to capture the dependencies between \nthe two modalities. Similarly, VisualBERT [29] employs a \ntransformer encoder to jointly encode image regions and \ntext tokens, allowing it to excel in tasks such as VQA and \nvisual entailment. By fully integrating the information in \neach layer, these models perform exceptionally well in tasks \nrequiring complex cross-modal reasoning. In the medical \ncontext, encoder based multimodal attention models are \nhighly effective for tasks such as medical VQA, in which \nnuanced interactions between clinical images and associ -\nated textual data are critical. This approach is particularly \nuseful for tasks that require contextual understanding, such \nas combining diagnostic imaging with clinical notes to pro-\nvide comprehensive insights.\n3.2.3 Encoder–decoder based multimodal integration\nEncoder–decoder based multi-modal integration models \nadopt a generative approach, making them highly effec -\ntive for tasks such as image captioning, report generation, \nand text-conditioned image creation. Unlike models that \nsimply align or jointly embed inputs, this architecture is \n3.1.7 Image and report generation\nImage generation focuses on synthesizing realistic medical \nimages to augment datasets, particularly in cases involv -\ning rare conditions or limited training data. Hou et al. [ 23] \ndeveloped a hybrid synthesis pipeline for histopathology \nimage segmentation that combines real histopathology \ntextures with generative adversarial networks (GANs). \nThis innovative approach generates diverse training image \npatches across various tissue types, enhancing generaliza -\ntion performance. By improving the heterogeneity of syn -\nthetic datasets, this method is especially valuable for cancer \ntypes lacking annotated training data.\nReport generation automates the creation of structured \ndiagnostic reports by summarizing the key imaging find -\nings. Jing et al. [ 24] designed a model that learns the joint \nrepresentations of imaging data and textual information and \nproduces radiology reports. By bridging the gap between \nimage analysis and textual synthesis, this approach con -\ntributes to more accurate and efficient reporting in clinical \nradiology.\n3.2 Model architecture\nVLMs represent a groundbreaking category of AI systems \ndesigned to process and reason across both visual and tex -\ntual modalities. These models support a wide range of tasks \nincluding image captioning, cross-modal retrieval, VQA, \nand text-conditioned image generation. Methodologically, \nVLMs can be divided into three main approaches: encoder-\nbased cross-modal alignment, encoder-based multimodal \nattention, and encoder–decoder based multimodal inte -\ngration (Fig. 3). This section explores each approach in \ndetail, focusing on the architecture, learning strategies, and \nexpected effects in the medical domain.\n3.2.1 Encoder based cross-modal alignment\nEncoder based cross-modal alignment employs separate \nencoders for visual and textual inputs and aligns their rep -\nresentations in a shared embedding space, shown in Fig. 3a. \nThis alignment enables the model to compute semantic \nsimilarity between modalities—such as visual features in \nan X-ray and corresponding medical terms in a diagnos -\ntic report—without requiring pixel-level annotations. By \ncomparing the similarity between encoded features, the \nmodel learns to associate paired inputs and distinguish \nthem from unpaired examples. This methodology relies pri-\nmarily on contrastive learning in which paired inputs are \nbrought closer together in the embedding space and mis -\nmatched pairs are pushed apart. CLIP [ 2] by OpenAI is a \nseminal model in this category that has achieved zero-shot \n1 3\n814\nBiomedical Engineering Letters (2025) 15:809–830\nrepresentations. Some implementations also allow one \nmodality, such as text, to conditionally influence another, \nsuch as images, during the intermediate stages of process -\ning the intermediate stages of processing, as illustrated as \nFig. 3c. In its encoder–decoder configuration, SimVLM \ndesigned to actively generate outputs, allowing the model \nto produce natural language or synthesized images condi -\ntioned on mult-imodal input. These models typically pro -\ncess visual and textual inputs within a shared encoder and \nutilize a decoder to generate outputs based on the encoded \nFig. 3 Detailed illustration of \nmodel architecture. a Encoder-\nbased cross-modal alignment \nmethod employs separate encoders \nfor images and text, aligning their \nembeddings across modalities to \nfacilitate integration. b In encoder-\nbased multi-modal attention, both \nimage and text inputs are processed \nwithin a unified model, using the \nencoder alone to execute tasks. c \nEncoder–decoder-based multi-\nmodal integration combines images \nand text as simultaneous joint \ninputs to the encoder, adopting a \ngenerative approach for decoding \noutputs. d In another encoder–\ndecoder-based multi-modal \nintegration approach, text serves as \na conditional prompt, directing the \ngeneration process by attention-\nbased mechanisms\n \n1 3\n815\nBiomedical Engineering Letters (2025) 15:809–830\nmodel demonstrated enhanced precision and interpretabil -\nity, making it suitable for a wide range of medical tasks.\nIn the domain of encoder based multi-modal atten -\ntion, Moon et al. [ 36] introduced the Medical Vision Lan -\nguage Learner (MedViLL), a framework that bridges the \nunderstanding and generation of medical images and text. \nThrough an innovative self-attention mechanism, Med -\nViLL effectively captures joint representations and achieves \nsuperior performance across a variety of medical tasks. \nWang et al. [ 37] proposed ECAMP, a model designed to \nenhance the interpretation of medical data by emphasizing \nentity-specific contexts within radiology reports. By lever -\naging advanced language models, ECAMP extracts and \nrefines entity-centered information from medical reports, \nthereby strengthening the interaction between the textual \nand visual modalities to improve diagnostic insights. Yan \net al. [38] adapted the bidirectional encoder representations \nfrom transformers (BERT) architecture for clinical text by \npretraining it on extensive medical corpora, including the \nmedical information mart for intensive care III (MIMIC-III) \nclinical notes. The resulting ClinicalBERT model excels in \nunderstanding the unique language patterns and specialized \nterminology of the medical domain, making it highly effec-\ntive for various clinical text-processing applications.\nIn the domain of encoder–decoder based multi-modal \nintegration, Chambon et al. [ 39] presented RoentGen, a \nvision-language foundation model specifically designed \nto produce clinically accurate and descriptive chest X-ray \nreports. This model bridges the gap between imaging and \ntext by generating detailed radiological insights, making \nit a robust tool for automated report generation. Huemann \net al. [ 40] developed ConTEXTual Net, a multi-modal \nvision-language foundation model that integrates radiol -\nogy reports into the segmentation process for chest radio -\ngraphs. By incorporating a free-form textual context, the \nmodel can enhance pneumothorax segmentation, surpassing \nthe performance of vision-only models, and demonstrating \nthe value of combining visual and textual modalities. Li et \nal. [41] introduced an Anatomical Structure-Guided (ASG) \nframework that integrates anatomical knowledge into a \nmedical vision-language foundation model. This innovative \napproach aligns the anatomical regions in images with the \ncorresponding textual descriptions, enabling superior per -\nformance in classification and segmentation tasks across \nmultiple datasets. Liu et al. [42] proposed M-FLAG, which \nfocuses on improving training stability and efficiency. By \nfreezing the language models and optimizing the latent \nspace geometry with a novel orthogonality loss, the model \nachieves significant advancements in medical tasks. Tha -\nwakar et al. [ 43] introduced XrayGPT, which was tailored \nfor radiology applications. By combining the MedClip \nvisual encoder with a fine-tuned Vicuna language model, \n[28] treats image patches as pseudo-text tokens and inte -\ngrates them seamlessly into prefixed language modeling for \ntasks such as conditional text generation. Expanding on this \nconcept, VisualGPT [ 30] conditions pre-trained language \nmodels on visual inputs, enabling the generation of detailed \ncaptions or answers. Similarly, DeepMind’s Flamingo [ 31] \nleverages cross-attention modules to fuse images and text \nmodalities dynamically, achieving impressive few-shot \nperformance across a variety of vision-language tasks. A \nrepresentative architectural structure of these models is \nshown in Fig. 3d. In medical applications, encoder–decoder \nmodels have significant potential for automating diagnostic \nreport generation, thereby reducing the workload of radi -\nologists. For instance, given a chest radiograph, such mod -\nels can produce comprehensive findings and impressions, \nimprove workflow efficiency, and minimize human error. \nFurthermore, text-conditioned image generation can be \nused to simulate rare pathological cases, thereby enhancing \nthe diversity of training datasets for medical education and \nmodel development.\n4 Foundation models in medical imaging\n4.1 Specific domain transfer applications\n4.1.1 X-ray imaging\nIn the domain of X-ray imaging using encoder based cross-\nmodal alignment (Table 1), Phan et al. [ 32] proposed a \nnovel medical foundation model that breaks down dis -\nease descriptions into fundamental visual components. \nThis model, which is primarily trained on X-ray images, \naligns visual data with key pathological features, thereby \nsignificantly improving its ability to detect and interpret \npathological findings. Similarly, Luo et al. [ 33] introduced \nDeViDe, a transformer-based approach that enhanced the \nperformance of medical foundation models. The integration \nof diverse medical knowledge sources, such as radiographic \ndescriptions, enables this model to establish a stronger con-\nnection between visual data and textual representations. \nFocusing on clinical knowledge, Liu et al. [34] developed a \nhierarchical foundation model, IMITATE. With a structure \nthat relies on X-ray images, the model uses the findings and \nimpressions sections of medical reports to align multilevel \nvisual features with descriptive and conclusive text, thereby \nachieving effective integration of clinical insights. Finally, \nWang et al. [ 35] presented multi-modal collaborative \nprompt learning (MCPL), a framework aimed at refining \nthe relationship between medical texts and image represen-\ntations. By employing collaborative prompt learning, this \n1 3\n816\nBiomedical Engineering Letters (2025) 15:809–830\n4.1.2 Computed tomography imaging\nChen et al. [45] presented 3D-CT-GPT, a cutting-edge VQA-\nbased medical VLM developed to generate radiology reports \nfrom 3D CT scans, with a specific focus on chest computed \ntomography (CT) using encoder-based cross-modal align -\nment in CT imaging (Table 2). By employing advanced \nVQA techniques, this model improves the interpretability \nand ACC of automated radiological assessments, thereby \nproviding a significant step forward in generating detailed \nand clinically meaningful reports. Building on the need for \ntheir approach excels in radiology report generation and \ninteractive reasoning, offering state-of-the-art performance \nin these areas. Zhang et al. [ 44] designed Libra, a tempo -\nrally aware multi-modal LLM aimed at improving radiol -\nogy report generation. Libra effectively captures temporal \nchanges in radiological data, achieving good performance \nwith the MIMIC-CXR dataset across lexical and clinical \nevaluation metrics.\nTable 1 Summary of foundation models in X-ray imaging\nModality Model Dataset Prompt \ntype\nTask Metrics Mean (evaluation \ndataset)\nEncoder \nbased \ncross-modal \nalignment\nMA VL MIMIC-CXR v2 Text Zero-shot \nclassification\nDetection\nAUROC, F1, \nACC\nIoU, Dice, \nACC\n0.735, 26.25, 82.77 \n(ChestX-ray14)\n21.97, 34.11, 84.29 \n(COVID Rural)\nDeViDe MIMIC-CXRv2 Text Zero-shot \nclassification\nSegmentation\nAUROC, F1, \nACC\nDice\n0.777, 31.5, 82.3 \n(ChestX-ray14)\n70.27 (ChexDet)\nIMITATE MIMIC-CXR, CheXpert, RSNA, \nSIIM, COVIDx, ChestX-ray14\nText Classi-\nfication, \nSegmentation\nDetection, \nRetrieval\nAUROC, Dice\nmAP, \nPrecision@5\n0.897 (CheXpert), 64.5 \n(SIIM)\n26.4 (RSNA), 71.83 \n(CheXpert 5 × 200)\nMCPL MIMIC-CXR Report, \nHand-craft\nClassification\nDetection\nACC, AUROC\nmAP, mIoU\n83.3, 0.843 (CheXpert)\n20.1, 27.5 (Object-CXR)\nEncoder \nbased \nmulti-modal \nattention\nMedViLL MIMIC-CXR, Open-I Report Classification\nRetrieval\nAvg AUROC, \nF1\nMRR, H@5, \nR@5\n0.980, 0.839 \n(MIMIC-CXR)\n56.5, 77.0, 47.4 \n(MIMIC-CXR)\nECAMP MIMIC-CXR Text gener-\nated by \nChatGPT\nClassification\nSegmentation\nAUROC\nDice\n0.867, 0.851 \n(ChestX-ray14)\n84.5 (SIIM-ACR \nPneumothorax)\nClinical-BERT MIMIC-CXR, IU X-Ray,\nCOV-CTR, NIH ChestXray14\nReport Image \nCaptioning\nClassification\nBLUE1,CIDEr\nAUROC\n0.383,0.151 \n(MIMIC-CXR)\n0.845 (NIH \nChestXray14)\nEncoder–\ndecoder based\nmulti-modal \nintegration\nRoentGen MIMIC-CXR Text Image \ngeneration\nClassification\nFID\nAUROC\n3.6 (MIMIC-CXR)\n0.824 (CheXpert)\nConTEXTual Net CANDID-PTX Report Segmentation Dice 0.716 (CANDID-PTX)\nASG MIMIC-CXR Report Classification\nSegmentation\nAUROC\nDice\n0.836 (NIH Chest \nX-ray)\n73 (RSNA Pneumonia)\nM-FLAG MIMIC-CXR Report Classification\nSegmentation\nAUC\nDice\n69.50 (MIMIC-CXR)\n64.80 (SIIM-ACR)\nXrayGPT MIMIC-CXR, Open-I Report Image \ncaptioning\nClassification\nBLEU\nAUROC\n17.8 (OpenI)\n0.832 (CheXpert)\nLibra MIMIC-CXR, \nMedical-Diff-VQA,\nMIMIC-Ext-MIMIC-CXR-VQA\nReport Report \ngeneration\nBLEU-1, \nBLEU-4,\n51.3, 24.5 \n(MIMIC-CXR)\nAUROC, area under receiver operating characteristic curve; ACC, accuracy; IoU, intersection over union; mAP, mean average precision; mIoU, \nmean intersection over union; MRR, mean reciprocal rank; H, Hit Rate; R, Recall; CIDEr, consensus-based image description evaluation; FID, \nfréchet inception distance; BLEU, bilingual evaluation understudy\n1 3\n817\nBiomedical Engineering Letters (2025) 15:809–830\nan innovative text-guided interslice (TG-IS) scoring module \nthat mimics the attention mechanisms used by radiologists \nwhen analyzing CT images. This approach enables Med-\n2E3 to excel in tasks such as report generation and VQA \nusing large-scale multi-modal benchmarks. Zhou et al. [51] \nproposed a sophisticated vision-language framework that \nmerges LLMs with hierarchical attention mechanisms. By \neffectively integrating multi-modal inputs, the model excels \nin fine-grained abnormality detection and the generation of \nnatural language descriptions for medical CT images. This \napproach significantly improves the clinical relevance and \ndetection ACC, establishing a new benchmark for precision \nin medical imaging tasks.\n4.1.3 Fundus imaging\nIn the domain of fundus imaging using encoder based cross-\nmodal alignment (Table 3), Cherukuri et al. [52] employed a \nguided context self-attention mechanism to integrate visual \nand textual features within a vision-language foundation \nmodel designed for retinal image captioning. The GCS-\nM3VLT architecture effectively captures intricate visual \ndetails and a broader clinical context, even with limited data. \nEvaluations of the DeepEyeNet dataset have demonstrated \nimprovements in BLEU-4 scores, indicating its capability to \ngenerate accurate and comprehensive medical captions. Du \net al. [53] developed RET-CLIP, a vision-language founda-\ntion model pre-trained on a large dataset of color fundus \nphotographs paired with clinical diagnostic reports. The \nmodel employs a tripartite optimization strategy to extract \nfeatures at three levels: the left eye, right eye, and report \ndata. This multilevel approach facilitates effective represen-\ntation learning, leading to enhanced diagnostic performance \nin diseases such as diabetic retinopathy and glaucoma. Luo \net al. [54] addressed demographic biases in VLMs by intro-\nducing FairCLIP, a framework designed to promote fairness \nrobust datasets, Hamamci et al. [ 46] introduced CT-RATE, \nwhich is the first open-source multi-modal dataset that \npairs 3D CT scans with the corresponding textual reports. \nBy leveraging this dataset, the authors also developed CT-\nCLIP and CT-CHAT, two innovative foundation models that \nexcel in tasks such as zero-shot multi-abnormality detection \nand multi-modal AI assistance for 3D medical imaging. To \naddress the challenges of extracting high-quality 3D visual \nfeatures, Lai et al. [ 47] proposed E3D-GPT, an enhanced \n3D visual foundation model tailored for medical vision-\nlanguage applications. The model is built on a substantial \ncorpus of unlabeled 3D CT data utilized in a self-supervised \nlearning framework to extract robust 3D visual features. By \nincorporating 3D spatial convolutions, E3D-GPT efficiently \naggregates and projects high-level image features while \nreducing computational complexity.\nIn the domain of encoder–decoder based multi-modal \nintegration, Blankemeier et al. [ 48] presented Merlin, a \ncomputationally efficient 3D vision-language foundation \nmodel specifically designed for interpreting abdominal CT \nscans. Merlin achieves exceptional performance across a \nwide range of downstream tasks by integrating supervi -\nsion from both structured EHR and unstructured radiology \nreports. Notably, Merlin achieves state-of-the-art results \nwhile maintaining minimal computational resource require-\nments, making it a practical and scalable solution. To address \nthe challenges of 3D medical image segmentation, Li et al. \n[49] introduced ProMISe, a framework driven by prompt \nengineering that adapts general VLMs for domain-specific \napplications. By leveraging the flexibility of prompts, this \nmethod demonstrates both high effectiveness and versatil -\nity, thereby establishing a new standard for segmentation in \ncomplex medical imaging. Focusing on multi-modal inte -\ngration, Shi et al. [ 50] developed Med-2E3, a vision-lan -\nguage foundation model that combines 3D and 2D encoders \nto enhance medical-image analysis. The model incorporates \nTable 2 Summary of foundation models in CT imaging\nModality Model Dataset Prompt \ntype\nTask Metrics Mean (evaluation dataset)\nEncoder based\ncross-modal \nalignment\n3D-CT-GPT CT-RATE, Dataset-XY Text Report generation BLEU, ROUGE-1 13.27, 25.94 (CT-RATE)\nCT-CLIP,\nCT-CHAT\nCT-RATE Text Detection\nZero-shot classification\nMAP@1\nMAP@1\n0.886 (CT-RATE)\n0.886 (CT-RATE)\nE3D-GPT BIMCV-R, CT-RATE\nUnlabeled 3D CT\nText Report generation\nVQA\nBLEU\nACC\n18.19 (BIMCV-R)\n42.24 (BIMCV-R-VQA)\nEncoder–\ndecoder based\nmulti-modal \nintegration\nMerlin Abdominal CT Report Zero-shot classification F1 0.741 (Abdominal CT)\nProMISe Medical Segmentation \nDecathlon (MSD)\nPoint Segmentation Dice, NSD 66.81, 81.24 (MSD)\nMed-2E3 M3D-Cap, M3D-VQA Report Report generation\nVQA\nBLEU-1, ROUGE-1\nBLEU-1, ROUGE-1\n51.51, 54.48 (M3D-Cap)\n58.55, 62.04 (M3D-VQA)\nProposed \nMethods\nMIMIC-CXR, Open-I,\nCT-KIDNEY\nText Detection AUROC, Precision 0.96,0.95 (MIMIC-CXR)\nBLEU, bilingual evaluation understudy; ROUGE, recall-oriented understudy for gisting evaluation; MAP, mean average precision; ACC, accu -\nracy; NSD, normalized surface dice; AUROC, area under receiver operating characteristic curve\n1 3\n818\nBiomedical Engineering Letters (2025) 15:809–830\ndetection and segmentation tasks and highlights its utility in \nprecision diagnostics.\nIn the domain of encoder–decoder based multi-modal \nintegration, Li et al. [ 58] introduced VisionUnite, which is \ndesigned specifically for ophthalmology, to address criti -\ncal challenges in multi-disease diagnosis, user interaction, \nand interpretability. The model is trained on MMFundus, \nthe largest multi-modal fundus dataset to date that contains \nmore than 1.24 million image-text pairs, including high-\nresolution fundus images and simulated doctor-patient \ndialogues.\n4.1.4 MRI imaging\nIn the domain of MRI imaging using encoder based multi-\nmodal attention (Table 4), Chen et al. [ 59] introduced \nMedBLIP, a vision-language foundation model aimed at \nseamlessly integrating 3D medical imaging with textual \ndata derived from EHRs. By leveraging vision language \npre-training, this model effectively captures the intricate \nrelationships between volumetric medical images and the \nassociated textual information. Consequently, MedBLIP has \nachieved significant breakthroughs in applications such as \nacross diverse data distributions. Using optimal transport \nmethods, the model mitigates performance disparities \nbetween demographic groups, ensuring more equitable out-\ncomes in medical image analysis while maintaining robust \ndiagnostic capabilities. Silva-Rodriguez et al. [55] incorpo-\nrated domain-specific retinal knowledge into the training \nprocess of FLAIR, a vision-language foundation model for \nmedical image analysis. The model embeds expert clinical \ninsights into text supervision and demonstrates improved \ninterpretative abilities, resulting in an enhanced performance \nin disease classification and anomaly detection tasks. Wei et \nal. [56] utilized synthetic fundus images paired with natu -\nral language descriptions to develop VisionCLIP, a vision-\nlanguage foundation model for retinal image analysis. This \nstrategy enabled the model to effectively generalize to \nreal-world datasets while preserving patient confidentiality. \nYang et al. [57] designed ViLReF, a vision-language foun -\ndation model optimized for detecting fine-grained abnor -\nmalities in retinal images. By leveraging expert-driven label \nextraction and implementing weighted similarity coupling \nloss, the model effectively captures subtle yet clinically sig-\nnificant patterns. This approach improves the ACC of lesion \nTable 3 Summary of foundation models in Fundus imaging\nModality Model Dataset Prompt type Task Metrics Mean (evaluation dataset)\nEncoder based\ncross-modal \nalignment\nGCS-M3VLT DeepEyeNet Text Report \ngeneration\nBLEU-1, BLEU-2 0.430, 0.345 \n(DeepEyeNet)\nRET-CLIP Private Dataset Report Classification AUROC, AUPR 0.856 0.616 (IDRID)\nFairCLIP Harvard-FairVLMed Report \ngenerated\nby ChatGPT\nClassification AUROC, ES-AUC 0.702, 0.655 \n(Harvard-FairVLMed)\nFLAIR 37 Combined datasets Text Detection\nSegmentation\nACA/κ\nAUROC\n0.604/0.772 (MESSIDOR)\n0.92 (FIVES)\nVisionCLIP SynFundus-1 M Text Zero-shot \nclassification\nACC 43.1 (MESSIDOR)\nViLReF Private Dataset Report Classification\nSegmentation\nAUROC, mAP\nDSC, IoU\n94.29, 63.62 (RFMiD)\n52.65, 38.38 (IDRiD)\nEncoder–\ndecoder based\nmulti-modal \nintegration\nVisionUnite MMFundus Text Classification ACC,\nDiagnostic \nRelevance\n77.8, 2.937 (MMFundus)\nBLEU, bilingual evaluation understudy; AUROC, area under receiver operating characteristic curve; AUPR, area under the precision-recall \ncurve; ES-AUC, early stopping area under the curve; ACA, average classification accuracy; κ, Cohen's Kappa; ACC, accuracy; mAP, mean \naverage precision; DSC, dice similarity coefficient; IoU, intersection over union\nTable 4 Summary of foundation models in MRI imaging\nModality Model Dataset Prompt type Task Metrics Mean (evaluation dataset)\nEncoder based\nmulti-modal attention\nMedBLIP ADNI, NACC, \nOASIS\nText generated\nby EHRs\nClassification\nZero-shot \nclassification\nACC\nACC\n78.7 (ADNI)\n80.8 (AIBL)\nEncoder–decoder based\nmulti-modal integration\nMed-UniC MIMIC-CXR, \nPadChest\nReport Image captioning\nClassification\nBLEU\nAUROC\n18.25 (MIMIC-CXR)\n0.832 (CheXpert)\nFM-ABS Left Atrium, \nBrain Tumor\nBbox generated \nby MobileSAM\nSegmentation Dice, \nJaccard\n86.14, 75.85 (Left \nAtrium)\nBbox, bounding box; ACC, accuracy; BLEU, bilingual evaluation understudy\n1 3\n819\nBiomedical Engineering Letters (2025) 15:809–830\nThis innovative approach shows promise in supporting flex-\nible diagnostic workflows that align seamlessly with clinical \nrequirements. V o et al. [63] investigated the utilization of \nfrozen, large-scale, pretrained vision-language foundation \nmodels as foundational backbones for multi-modal breast \ncancer prediction. Rather than retraining the models, this \nmethod preserves the pretrained parameters while incor -\nporating domain-specific mammography data, leading to \nimproved predictive ACC for breast cancer diagnosis. This \nstudy highlights the practical advantages of repurposing \nlarge-scale VLMs for medical imaging, showcasing their \neffectiveness in addressing domain-specific diagnostic chal-\nlenges. Building on the EchoCLIP model, Christensen et al. \n[64] introduced EchoCLIP-R, a vision-language foundation \nmodel specifically designed for echocardiographic analysis. \nThis updated model features a customized echocardiogra -\nphy report text tokenizer, enabling a more precise alignment \nof multi-modal data. EchoCLIP-R achieves impressive \nresults across various tasks, including identifying individual \npatients across multiple videos, detecting clinical transi -\ntions, and delivering robust image-to-text retrieval with top-\ntier cross-modal ranking. These advancements underscore \nits versatility and reliability in echocardiographic interpreta-\ntion and report generation.\nIn the domain of encoder–decoder based multi-modal \nintegration, Yin et al. [ 62] investigated the use of prompt \nengineering to customize vision foundation models for \nanalyzing pathology images. Task-specific prompts are \nincorporated within the QAP framework, enabling the \nmodel to excel in pathology-oriented tasks such as tissue \nautomated radiology report generation and clinical decision \nmaking.\nIn response to the biases often present in multilingual med-\nical datasets, Wan et al. [60] developed Med-UniC, a vision-\nlanguage foundation model that employs cross-lingual text \nalignment regularization. This innovative framework aligns \ntextual representations across languages, thereby enhanc -\ning inclusivity and optimizing performance in a variety of \nvision-language tasks. In particular, Med-UniC excels in \nmultilingual diagnostic reporting and image-text retrieval, \nunderscoring its adaptability to diverse clinical contexts. Xu \net al. [61] proposed foundation model-driven active barely \nsupervised (FM-ABS), a vision-language foundation model \ndesigned to address the complexities of 3D medical image \nsegmentation under minimal supervision. By incorporat -\ning a prompt-driven architecture alongside active learning \nmethodologies, FM-ABS significantly reduces the reliance \non large, annotated datasets while maintaining high seg -\nmentation precision.\n4.1.5 Other medical imaging\nIn the domain of other medical imaging using encoder \nbased cross-modal alignment (Table 5), Ferber et al. [ 62] \nexplored the potential of in-context learning within multi-\nmodal LLMs to classify cancer pathology images without \nthe need for task-specific fine-tuning. By harnessing the \ncontextual information embedded in both visual and tex -\ntual data, the model demonstrates its capability to analyze \ncomplex pathology slides with adaptability and efficiency. \nTable 5 Summary of foundation models in other medical imaging\nModality Model Dataset Image type Prompt type Task Metrics Mean (evaluation \ndataset)\nEncoder based\ncross-modal \nalignment\nGPT-4 V Private dataset attrib-\nuted to company\nPathology Slides Text Zero-shot \nclassification\nACC 32.5 (CRC-V AL-\nHE-7 K)\nProposed \nMethods\nCBIS-DDSM, \nEMBED\nMammography Text gener-\nated by \nTab2Text\nClassification ACC, \nAUROC\n79.6, 0.907 \n(CBIS-DDSM)\nEchoCLIP-R Cedars-Sinai Medical \nCenter\nEchocardiography Report Retrieval\nRegression\nMCMRR\nMAE\n206.1 (Cedars-Sinai \nMedical Center)\n16.9 (Cedars-Sinai \nMedical Center)\nEncoder based\nmulti-modal \nattention\nQAP NAFLD-Anomaly Pathology Slides Morpho-\nlogical \nAttributes\nClassification\nScoring\nF1\nAvg F1\n99.58 \n(NAFLD-Anomaly)\n83.37 \n(NAFLD-Anomaly)\nLLaV A-Ultra US-Hospital Ultrasound Text VQA F1, \nPrecision\n76.85, 81.88 \n(SLAKE)\nGP-VLS 11 Combined datasets Surgical Imaging Text VQA ACC 46.1 (MedQA)\nEncoder–\ndecoder based\nmulti-modal \nintegration\nSkinGEN Fitzpatrick17k, SCIN Clinical image Text Image \ngeneration\nClassification\nCLIP, \nDINOV2 \nscore\n0.76,0.82 \n(Fitzpatrick17k)\nACC, accuracy; AUROC, area under receiver operating characteristic curve; MCMRR, mean cumulative mean reciprocal rank; MAE, mean \nabsolute error\n1 3\n820\nBiomedical Engineering Letters (2025) 15:809–830\ndiagnostic ACC in tasks such as classification and anom -\naly detection. By embedding explainability into its design, \nSkinGEN not only improves clinical outcomes, but also \nstrengthens communication between clinicians and patients, \nfostering greater trust and understanding in medical \nconsultations.\n4.2 Multi-domain integrated applications\n4.2.1 Encoder based cross-modal alignment\nIn the domain of foundation models with encoder based \ncross-modal alignment (Table 6), Ghosh et al. [68] introduced \nMammo-CLIP, a pioneering vision-language foundation \nmodel pre-trained on an extensive dataset of mammogram-\nreport pairs. By capitalizing on the inherent alignment \nbetween the visual and textual data in mammography, the \nmodel achieves improvements in data efficiency and robust-\nness. Its enhanced performance in tasks such as abnormality \ndetection and image-text alignment underscores its potential \nfor integration into breast cancer screening workflows. Liu \net al. [69] developed T3D, which is a vision-language frame-\nwork tailored for high-resolution 3D medical imaging. This \nmodel uses text-informed contrastive learning and advanced \nimage restoration techniques to capture intricate visual \ndetails without down sampling. Consequently, T3D excels \nin representation learning for volumetric datasets, making \nit particularly effective for classification and segmentation \nclassification and anomaly detection without the need for \nextensive fine-tuning. This innovative approach emphasizes \nthe adaptability and efficiency of prompt-based techniques \nfor streamlining medical imaging workflows for patho -\nlogical slides. Guo et al. [ 65] introduced LLaV A-Ultra, a \nvision-language foundation model specifically designed for \nultrasound imaging in Chinese healthcare. This model inte-\ngrates sophisticated vision and language functionalities to \naddress critical challenges unique to ultrasound, including \nthe variability in interpretation and the demands of real-time \ninteraction. Optimized for tasks such as image interpreta -\ntion, diagnostic decision-making, and interactive querying, \nLLaV A-Ultra is effective in advancing clinical ultrasound \npractices. In surgical applications, Schmidgall et al. [ 66] \ndeveloped GP-VLS, a versatile vision-language foundation \nmodel that combines domain-specific medical and surgical \nknowledge with advanced visual scene comprehension. This \nmodel supports key tasks such as surgical phase recogni -\ntion, instrument detection, and intraoperative decision-mak-\ning. GP-VLS offers real-time, context-sensitive assistance \nand can enhance surgical workflows, improve clinical effi -\nciency, and support more informed decision-making in sur-\ngical environments.\nLin et al. [ 67] introduced SkinGEN, a vision-language \nfoundation model augmented with stable diffusion, to \nadvance dermatological diagnostics through interactive \nand explainable visualizations. The model generates life -\nlike depictions of potential skin conditions, enhancing the \nTable 6 Summary of foundation models with encoder-based cross-modal alignment\nModel Dataset Image type Prompt \ntype\nTask Metrics Mean (evaluation \ndataset)\nMammo-CLIP UPMC, VinDr X-ray, CT Report Zero-shot \nclassification\nACC 62.0, 76.0, 15.0 \n(RSNA)\nT3D BIMCV-VLP X-ray, CT, MRI Text Segmentation\nClassification\navgDice\nmacro-avg \nAUROC\n79.5 (BTCV)\n58.1 (MDLT)\nBLIP PubMed Image-Text Xray, CT, MRI, Micros-\ncopy, Fundus Imaging\nCaption Retrieval i2t@1 i2t@10 36.52 72.62 (PubMed \nImage-Text)\nPM2 BACH, Figshare MRI \nBrain Tumor, DR\nMRI, Fundus Imaging, \nPathology Slides\nText \ngener-\nated by \nCoOp\nZero-shot \nclassification\nACC 47.5 (BACH)\nMedclip MIMIC-CXR, \nCheXpert, Unpaired \nText, COVID, RSNA \nPneumodia\nX-ray, CT Text Zero-shot classifi-\ncation Retrieval\nACC\nP@1,P@2\n59.4 (MIMIC-CXR)\n45,49 \n(CheXpert5 × 200)\nUniDCP ROCO, MIMIC-CXR X-ray, CT, MRI, Ultra-\nsound, Pathology Slides\nText VQA\nReport generation\nACC\nBLEU-1, \nBLEU-2\n74.5 (VQA-RAD)\n0.527, 0.349 (IU \nX-Ray)\nMPMA ROCO, MIMIC-CXR X-ray, CT, MRI, \nUltrasound,\nPathology Slides\nText Classification\nReport generation\nAUROC\nBLEU-1, \nBLEU-2\n0.906 (CheXpert)\n0.518, 0.337 (IU \nX-Ray)\nBiomedCLIP PMC-15 M X-ray, CT, MRI, Ultra-\nsound, PET, Microscopy, \nPathology Slides\nText Retrieval\nVQA\nR@1, R@5\nACC\n56.0, 77.9 \n(PMC-15 M)\n72.7 (VQA-RAD)\nACC, accuracy; AUROC, area under receiver operating characteristic curve; i2t, image-to-text; P, precision; R, Recall\n1 3\n821\nBiomedical Engineering Letters (2025) 15:809–830\ntask-specific fine-tuning. UniDCP performs exceptionally \nwell in tasks such as report generation and cross-modal \nretrieval. Zhang et al. [ 74] proposed MPMA, a vision-\nlanguage foundation model that integrates cross-modal \nalignment into joint image-text reconstruction. By foster -\ning enhanced interactions between modalities, this method \nimproves the performance in tasks such as classification and \nreport generation, particularly when applied to multi-modal \ndatasets. Finally, Zhang et al. [75] introduced BiomedCLIP, \na multi-modal biomedical foundation model pre-trained on \nPMC-15 M [ 75], a comprehensive dataset containing 15 \nmillion image-text pairs sourced from PubMed Central. \nThe model benefits from extensive pretraining and excels in \nbiomedical tasks such as image-text retrieval and zero-shot \nclassification. Its ability to address complex medical que -\nries with remarkable precision highlights its potential for \nadvancing biomedical research and applications.\n4.2.2 Encoder based multi-modal attention\nIn the domain of foundation models with encoder based \nmulti-modal attention (Table 7), Chen et al. [76] devised an \napproach that integrates domain-specific knowledge. Their \nmethod refines the alignment between the visual and textual \ndata, enabling more accurate reasoning for complex tasks. \nThis advancement has proven to be particularly effective in \ntasks involving 3D modalities, such as CT scans. Monajati-\npoor et al. [70] proposed BLIP, a pipeline designed to align \nmedical images with textual data through subfigure-caption \nmatching and multi-modal pretraining. Particularly adept at \nanalyzing brain abnormalities, this model enhances tasks \nsuch as image-text retrieval and multi-modal understand -\ning. Its architecture emphasizes precise alignment between \nvisual inputs and textual descriptions, enabling superior \nanalysis of complex brain imaging datasets. Wang et al. [71] \nintroduced PM2, a multi-modal prompting paradigm that \naddresses the challenges of few-shot medical image classi -\nfication. By integrating cross-modal information, PM2 dem-\nonstrates flexibility and robust performance, particularly in \nscenarios with limited labeled data. This versatility makes \nit a valuable tool for various medical imaging modalities. \nWang et al. [ 72] presented MedCLIP, a vision-language \nfoundation model designed to learn from unpaired medical \nimages and text. Employing a semantic similarity matrix for \ncontrastive learning, MedCLIP bypasses the need for paired \ndatasets, achieving notable success in zero-shot image-text \nretrieval and classification across modalities such as X-rays \nand pathology slides. Zhan et al. [73] introduced UniDCP, a \nVLM that utilizes dynamic cross-modal learnable prompts. \nThis approach harmonizes inputs from diverse pretrain -\ning tasks, enabling the model to adapt to a wide range of \nvision-language tasks in medical imaging without requiring \nTable 7 Summary of foundation models with encoder based multi-modal attention\nModel Dataset Image type Prompt type Task Metrics Mean (evaluation \ndataset)\nProposed \nMethods\nROCO, MedICaT, \nMIMIC-CXR\nX-ray, CT, MRI, Ultrasound Text, Graph VQA\nClassification\nACC\nACC\n67.60 (VQA-RAD)\n80.51 (MELINDA)\nLlama3-Med Claude 3 Opu, \nLLaMA 3 70B\nX-ray, CT, MRI, Ultrasound, \nPET\nText VQA Recall 31.20 (VQA-RAD)\nPPE COCO X-ray, Microscopy,\nPathology Slides, RGB \nimage\nText generated \nby BLIP, Hand-\ncraft, Mask \nlabel generated \nby LViT\nSegmentation Dice, mIoU 80.59, 67.59 \n(MoNuSeg)\nLLaV A-Med PMC-15 M X-ray, CT, MRI, Ultrasound, \nPET\nText generated \nby GPT-4\nVQA Recall 64.75 (VQA-RAD)\nTFA-LT ISIC2018, \nAPTOS2019\nDermoscopy, Fundus \nImaging\nText Classification ACC 70.48 (ISIC2018)\nLViT Private dataset attrib-\nuted to company\nX-ray, CT Report Segmentation Dice, mIoU 83.66, 75.11 \n(MosMed Data +)\nOne-Prompt \nSegmentation\n78 Combined datasets X-ray, CT, MRI,\nFundus Imaging, CBCT\nClick, Bbox, \nDoodles, Mask \nlabel\nSegmentation Avg Dice 67.30 (KiTS23)\nMed-VLFM ROCOv2 X-ray, CT Text Report \ngeneration\nBERT \nScore, \nROUGE-1\n0.638, 0.304 \n(ROCOv2)\nBiomedGPT-B IU X-ray, MIMIC-\nCXR, Peir Gross, \nSLAKE, VQA-RAD, \nPathVQA\nX-ray, CT, MRI, Pathology \nSlides\nReport Image \ncaptioning\nVQA\nROUGE-L, \nMETEOR\nACC\n28.50, 12.90 (IU \nX-ray)\n88.7 (SLAKE)\nBbox, bounding box; ACC, accuracy; mIoU, mean intersection over union; ROUGE, recall-oriented understudy for gisting evaluation; \nMETEOR, metric for evaluation of translation with explicit ordering\n1 3\n822\nBiomedical Engineering Letters (2025) 15:809–830\ntransformers with language guidance, the model achieves \nprecise, context-aware segmentation. Its success demon -\nstrates the benefits of combining multi-modal understanding \nwith advanced techniques. Wu et al. [ 82] innovated a sin -\ngle-prompt framework that simplifies medical image seg -\nmentation across diverse imaging modalities. Its versatility \nand straightforward design make it a promising choice for \ntasks such as organ segmentation and lesion identification. \nYang et al. [83] achieved recognition with Med-VLFM (also \nknown as Pclmed), a vision-language foundation model that \ntriumphed in the ImageCLEFmedical 2024 Caption Predic-\ntion Challenge. The model improves both interpretability \nand clinician-patient communication by generating detailed, \ncontext-aware captions for medical images. Finally, Zhang \net al. [84] introduced BiomedGPT-B, a multi-modal founda-\ntion model designed for biomedical applications. The model \nuses extensive pretraining to excel in tasks such as VQA and \nmulti-modal analysis, thus solidifying its role as a robust \ntool for biomedical research.\n4.2.3 Encoder–decoder based multi-modal integration\nIn the domain of foundation models with encoder–decoder \nbased multi-modal integration (Table 8), Jiang et al. [ 85] \nimproved the zero-shot segmentation capabilities for multi-\nmodal medical images by integrating GPT-4-generated \ndescriptive prompts into the text-visual-prompt segment \nanything model (TV-SAM) framework. This innovation \nmedical applications such as diagnostic support and anom -\naly detection. Llama3-Med, a vision-language foundation \nmodel crafted by Chen et al. [ 77], is designed for biomedi -\ncal tasks. The model utilizes a hierarchical image-encoding \nstrategy and an enriched biomedical image-text dataset, \nsignificantly enhancing its capacity to analyze intricate \nbiomedical imagery. Its strong performance in generating \ndiagnostic reports and supporting clinical decisions high -\nlights its potential. Focusing on the segmentation ACC and \nadaptability across imaging modalities, Han et al. [ 78] cre-\nated prior prompt encoder (PPE), a VLM guided by textual \nprompts at multiple scales. The integration of contextually \nrelevant guidance has been invaluable for tasks involving \nX-rays, CT scans, and MRIs. Li et al. [ 79] streamlined the \ntraining of LLaV A-Med, a foundation model optimized \nfor multi-modal biomedical conversations. The model was \ntrained in less than one day by using an efficient pipeline \nthat combines biomedical figure-caption pairs and GPT-\n4-generated instruction data. This efficiency, paired with \nconversational fluency, has made it stand out in biomedical \ncontexts. Li et al. [80] addressed the challenge of long-tailed \nmedical image classification using text-guided foundation \nmodel adaptation for long-tailed medical (TFA-LT), which \nis a text-guided framework. Their system employs light -\nweight adapters and a two-stage training strategy and excels \nin handling imbalanced datasets while maintaining com -\nputational efficiency. Li et al. [ 81] introduced LViT, which \nadvances medical image segmentation. By fusing vision \nTable 8 Summary of foundation models with encoder–decoder based multi-modal integration\nModel Dataset Image type Prompt \ntype\nTask Metrics Mean (evaluation \ndataset)\nTV-SAM Private dataset attrib-\nuted to company\nX-ray, CT, MRI, Ultra-\nsound, Microscopy, \nDermoscopy\nText gen-\nerated by \nGPT-4, \nBbox \ngener-\nated by \nGLIP\nSegmentation Avg Dice 0.831 (Polyp \nbenchmark)\nSERPENT-VLM IU X-Ray, ROCO X-ray, CT Text Report \ngeneration\nBLEU4, \nROUGE-L\n0.190,0.326 (IU X-Ray)\nBiomedCoOp CTKidney, DermaM-\nNIST, Kvasir, RETINA, \nLC25000\nCT, Dermoscopy, Endos-\ncopy, Fundus Imaging, \nPathology Slides\nText Classification ACC, Har-\nmonic Mean\n86.93, 82.74 \n(CTKidney)\nMS-VLM CT-RATE, In-house \nRectal MRI\nCT, MRI Report Report genera-\ntion VQA\nBLEU-4, \nROUGE-L \nPrecision, \nRecall\n0.232, 0.438 (CT-RATE)\n0.222, 0.329 (CT-RATE)\nVILA-M3 MIMIC-CXR, SLAKE, \nPathVQA, CheXpert\nX-ray, CT, MRI, Pathol-\nogy Slides\nText Segmentation \nVQA\nDice ACC 0.95 (RSNA Pneumonia)\n84.20 (SLAKE)\nMAKEN ImageCLEFmedical \n2023\nX-ray, CT, MRI, Ultra-\nsound, PET, Endoscopy\nText Report \ngeneration\nBLEU-1, \nROUGE-1\n0.189, 0.275 (Image-\nCLEFmedical 2023)\nProposed \nMethods\nTN3K, Kvasir-SEG, \nQaTa-COV19\nUltrasound, Endoscopic, \nCT\nBbox Segmentation mDice, mIoU 93.67, 89.44 (TN3K)\nBbox, bounding box; BLEU, bilingual evaluation understudy; ROUGE, recall-oriented understudy for gisting evaluation; ACC, accuracy; \nmDice, mean dice similarity coefficient; mIoU, mean intersection over union\n1 3\n823\nBiomedical Engineering Letters (2025) 15:809–830\nas data availability, clinical relevance, and the technical \nfeasibility of integrating these modalities into VLM frame -\nworks. Similarly, the architecture of the model, including \nencoder–decoder designs, attention mechanisms, and multi-\nmodal fusion techniques, significantly affects its ability to \nprocess and analyze diverse medical data effectively. This \ndiscussion explores the key trends in modern healthcare \nVLMs, focusing on advancements in their applications and \nstrategies. Additionally, the ongoing challenges in applying \nVLMs to the medical domain are also addressed, highlight-\ning areas that require further development.\n5.1 Frequently used medical image modalities\nX-rays are the most widely used imaging modalities in \nresearch, serving as a foundation for numerous medical \napplications. This can be attributed to several factors. First, \nThe availability of large-scale datasets, such as MIMIC-\nCXR [ 92], CheXpert [ 93], and NIH ChestX-ray14 [ 94], \nprovides millions of X-ray images paired with radiology \nreports. These datasets are instrumental for VLM training, \nfacilitating robust cross-modal alignment, and supporting \ntasks such as automated report generation. In addition, the \nstructured nature of radiology reports aligns well with the \nrequirements of cross-modal tasks, further enhancing their \nutility. Second, the simplicity and consistency of X-ray \nimaging make it particularly well-suited for scalable model \ndevelopment. Unlike CT or MRI, which produce complex \n3D volumetric data, X-rays are 2D single-view images. This \nlower dimensionality significantly reduces computational \ndemands and helps mitigate the risk of overfitting, espe -\ncially when working with limited data. \nWhile CT and MRI are indispensable for diagnosing \ncomplex conditions, such as cancer staging and neuro -\nlogical disorders, their use in VLM research remains rela -\ntively limited compared to X-rays. A major barrier is the \ncomputational demands of the modalities. CT and MRI \ngenerate high-resolution volumetric data, requiring exten -\nsive processing power and sophisticated algorithms, which \nincreases the complexity of training VLMs. Thus, despite \ntheir clinical significance, CT and MRI are seldom used in \nlarge-scale VLM studies. Efforts to incorporate 3D imag -\ning into vision-language pretraining have faced scalabil -\nity issues due to GPU memory limitations and the lack of \nstandardized radiology report formats across institutions \n[45]. These challenges hinder model generalization and \nunderscore a key limitation, that clinically valuable imag -\ning modalities cannot be fully leveraged without adequate \ncomputational resources and standardized datasets.\nFundus imaging is a specialized niche in research. Its \nclinical applications, such as the diagnosis of diabetic reti -\nnopathy and glaucoma, highlight its importance. Paired \neliminated the reliance on human annotations, making seg -\nmentation workflows more efficient while maintaining high \nACC across imaging modalities such as X-rays, CT scans, \nand MRIs. Kapadnis et al. [86] introduced SERPENT-VLM, \na self-refining framework designed for generating radiology \nreports. Employing a novel self-supervised loss function, \nthe model aligned generated text with the corresponding \ninput images, thereby effectively minimizing hallucina -\ntions and bolstering robustness. Even when handling noisy \nor incomplete inputs, SERPENT-VLM delivered consistent \nresults across multiple radiology benchmarks. Koleilat et al. \n[87] addressed the challenges of biomedical image classi -\nfication using BiomedCoOp, a vision-language foundation \nmodel. By blending BiomedCLIP with prompt ensembles \nderived from LLMs and employing selective knowledge \ndistillation, the framework excelled in few-shot classifica -\ntion tasks. Its effectiveness has been demonstrated using \ndiverse imaging modalities, including pathology slides and \nmammograms. For 3D medical imaging interpretation, Lee \net al. [ 88] introduced MS-VLM, a model optimized using \na slice-by-slice embedding strategy powered by Z-former. \nThis innovative design seamlessly integrated multi-view \nand multi-phase data to overcome the computational chal -\nlenges typically encountered by traditional 3D vision encod-\ners. MS-VLM has also achieved impressive performance \nin generating clinically relevant radiology reports. Nath et \nal. [89] expanded the potential of vision-language founda -\ntion models with VILA-M3, which incorporated domain-\nspecific medical knowledge. Task-specific optimization \nallowed the model to excel in VQA, report generation, and \nmedical image classification, particularly when used with \ncomplex multi-modal datasets. Wu et al. [ 90] participated \nin the ImageCLEFmedical 2023 challenge and utilized the \nMAKEN framework to focus on internal validation because \nof the absence of ground truth labels for external test data -\nsets. By prioritizing reliable internal benchmarking, their \napproach ensured robust performance even with data limi -\ntations. Zheng et al. [ 91] explored the segmentation chal -\nlenges in medical imaging through a curriculum-prompting \nstrategy for vision-language foundation models. This \nframework gradually increased the task complexity dur -\ning training, leading to superior segmentation results across \nimaging modalities, such as CT and ultrasound. This sys -\ntematic approach offered an effective pathway to enhance \nthe segmentation performance.\n5 Discussion\nIn the medical field, the use of VLMs is closely tied to both \nimaging modalities and the underlying model architectures. \nThe choice of imaging modality is shaped by factors such \n1 3\n824\nBiomedical Engineering Letters (2025) 15:809–830\ninteractions between visual and textual data. Although \nthis approach increases computational costs, it excels in \nscenarios that require simultaneous reasoning over both \nmodalities. A notable example is MedViLL [ 36], which \ndemonstrates strong classification performance by combin -\ning X-ray images with clinical notes. This method performs \nwell in tasks that require understanding both image and text \ntogether such as matching clinical findings with correspond-\ning visual patterns because it directly models interactions \nbetween the two. However, this comes at a cost. The inter -\nnal workings of the attention mechanism are hard to inter -\npret, making it difficult for clinicians to understand why the \nmodel made a certain prediction. This lack of transparency \ncan be a major drawback in medical settings where trust and \naccountability are essential. In addition, these models often \nneed large amounts of training data to perform well. When \ntrained on smaller datasets, their performance tends to pla -\nteau early, limiting their usefulness in low-resource domains \nlike rare diseases or specialized imaging modalities.\nEncoder–decoder based multi-modal integration is \namong the least commonly applied methodologies in VLMs \nwithin the medical domain, despite its significant potential \nfor generative tasks. Its limited adoption can be attributed \nto the considerable computational power and large-scale \npaired datasets required for effective training. Generative \ntasks, such as radiology report generation, often depend on \nstructured text outputs; however, such datasets are scarce, \nparticularly for modalities such as MRI and pathology. Even \nin the case of widely available modalities, such as X-rays, \ndatasets such as MIMIC-CXR [ 92] offer only partially \nstructured text, further complicating the training process. \nThe high computational demands of encoder–decoder mod-\nels present another major challenge, particularly for institu-\ntions that lack a robust infrastructure. Consequently, such \nmodels are often limited to niche applications in resource-\nrich environments. However, their capabilities are limited \nto tasks for which structured and coherent outputs are indis-\npensable. For instance, RoentGen [39] demonstrates strong \nperformance in radiology report generation by producing \nclinically relevant and coherent text. Similarly, XrayGPT \n[43] has demonstrated its potential for automating diag -\nnostic reporting workflows, thereby reducing the manual \neffort required for such labor-intensive processes. While the \npromise of encoder–decoder based integration for genera -\ntive applications is evident, its current reliance on extensive \npaired datasets and computationally intensive training limits \nits broader adoption. Addressing these challenges is essen -\ntial for making this methodology more accessible and appli-\ncable across diverse medical contexts.\nimage-text datasets, such as IDRiD [ 95] and MMFundus \n[96, 97] support research in this area by enabling vision-lan-\nguage applications. However, fundus imaging is confined \nto ophthalmology, which restricts its broad applicability in \ndiverse clinical contexts. Pathology and ultrasound imag -\ning are less researched because of the unique challenges \nthey pose. Pathology datasets require detailed expert anno -\ntations, such as cell types or cancer grades, making them \ntime-consuming and costly. Additionally, the visual com -\nplexity of pathology images complicates data preparation \nand model training. Particularly, the extremely high reso -\nlution of whole-slide pathology images, gigapixel scale, \nimposes significant memory demands. Although tiling strat-\negies are often used to manage this, they frequently lead \nto the loss of spatial context that is essential for accurate \ndiagnosis. In contrast, ultrasound imaging faces challenges \nrelated to variability in image quality. Operator skills signif-\nicantly affect the consistency of the ultrasound data, creat -\ning inconsistencies that make model training more difficult. \nFurthermore, the lack of large-scale paired datasets limits \nthe use of VLMs.\n5.2 Frequently used methodologies\nEncoder based cross-modal alignment is the most widely \nused VLM methodology in the medical domain. Its popu -\nlarity arises from its simplicity, scalability, and efficiency \nin addressing tasks such as classification and retrieval, \nparticularly when large paired datasets such as X-rays and \nradiology reports are available. By separating the image \nand text encoders, this approach reduces the computational \noverhead, making it an attractive choice for resource-con -\nstrained settings. The strength of this methodology in zero-\nshot learning has revolutionized case-based reasoning and \ndiagnostics. For example, DeViDe [ 33] excels in both seg -\nmentation and classification tasks, whereas RET-CLIP [53] \ndemonstrates high performance in fundus imaging classi -\nfication. Its effectiveness is primarily due to the fact that \nmany widely used medical datasets, such as MIMIC-CXR \n[92] and CheXpert [ 93], contain loosely aligned image-\ntext pairs rather than fully annotated or structured reports. \nDespite these advantages, the independent processing of \nvisual and textual modalities remains a notable limitation. \nThis separation hinders the model’s ability to capture com -\nplex interactions between modalities, making it less effec -\ntive for tasks that demand deep semantic understanding, \nsuch as those involving nuanced cross-modal reasoning.\nIn the medical domain, encoder based multi-modal atten-\ntion is moderately used, primarily in tasks that demand \nnuanced reasoning and rich contextual understanding. In \ncontrast to cross-modal alignment, which processes modali-\nties independently, multi-modal attention fosters deeper \n1 3\n825\nBiomedical Engineering Letters (2025) 15:809–830\n5.4 Lack of standardized evaluation metrics\nEvaluation metrics, such as BLEU and ROUGE, are widely \nused to assess the generative performance of medical VLMs. \nThese metrics serve as evaluation benchmarks for most mod-\nels [38, 50, 90]. However, these metrics often fail to reflect \nclinically important findings. BLEU and ROUGE focus on \nsurface-level matching by evaluating n-grams (words or \nphrases) based on their overlap with reference texts. This \napproach is limited because clinical reports often describe \nthe same conditions or findings using various terminologies \nor expressions. As a result, clinically accurate texts may still \nreceive poor evaluations. Moreover, clinical reports fre -\nquently emphasize specific disease names or findings that \ncarry greater clinical significance compared to other words. \nBecause BLEU and ROUGE treat all n-grams equally, they \ncannot assign appropriate weights to clinically critical terms \nor phrases. For instance, if a clinical report states “no malig-\nnancy found” but rephrases it as “malignancy not detected,” \nthe two sentences convey identical clinical meaning. How -\never, BLEU and ROUGE may assign low scores because of \ndifferences in word choice or phrasing. Consequently, met -\nrics should prioritize ACC, relevance, and interpretability, \nwhich reflect the clinical importance of findings, over simple \ntextual similarity. To address these limitations, alternative \nmetrics such as the CLIP and Dinov2 scores have been pro-\nposed, focusing on the similarity between medical text and \nimages [67]. Although these metrics represent an improve -\nment, they still fail to fully guarantee ACC for clinical sig -\nnificance and lack sufficient evaluation of specific details or \nkey terms in medical texts. Therefore, future studies should \nconsider developing evaluation metrics that better reflect \nthe way medical professionals understand clinical reports. \nFor example, using medical term databases such as Unified \nMedical Language System (UMLS) or RadLex could help \ngive more weight to important disease related terms during \nevaluation. It is also important to recognize that different \nexpressions can mean the same thing in clinical language. \nIn addition, involving clinicians or radiologists in the evalu-\nation process could help judge whether a generated report is \ntruly useful and accurate in a medical context. Finally, cre -\nating benchmark datasets that include multiple correct ver -\nsions of a report for the same image would allow for more \nfair and realistic scoring, since there is often more than one \nway to describe the same medical finding.\n6 Conclusion\nThis review of VLMs in the medical domain provides a vital \nsynthesis of the rapidly evolving landscape of foundation \nmodels in healthcare. Exploring the diverse applications of \n5.3 Bias and variance in VLMs\nThe bias and variance issues in VLMs for medical imaging \nremains a significant challenge. Bias arises from training \ndatasets that do not adequately represent diverse popula -\ntions, leading to an imbalanced model performance across \ndifferent groups. For example, biases related to race, eth -\nnicity, sex, socioeconomic factors, and language can result \nin unreliable predictions regarding underrepresented com -\nmunities. Variance, on the other hand, refers to the sensitiv-\nity of the model to variations in training data, which limits \nits ability to effectively generalize across different patient \npopulations or healthcare settings. In VLM datasets, Eng -\nlish continues to dominate, despite the fact that most of \nthe world’s population does not speak English as their pri -\nmary language. This dominance restricts the performance \nof monolingual VLMs in multilingual tasks and introduces \ncommunity bias, which disproportionately affects non-Eng-\nlish speakers. This bias is particularly concerning in medical \napplications and can have serious consequences [60].\nRecent developments in VLMs have indicated a shift \ntowards emphasizing the diversity and representativeness of \ndatasets to address these challenges. For example, datasets \nsuch as FairCLIP [54], PadChest [98], PMC-15 M [75], and \nMammo-CLIP [ 68] include racially and demographically \ndiverse data to reduce bias and ensure fairness. Specifi -\ncally, PadChest [98] can construct reports that incorporate \nnon-English languages, such as Spanish, to integrate cross-\nlingual representations and improve performance on non-\nEnglish tasks. The VLMs applied to these datasets include \nMA VL [32], Medunic [ 60], BioMedCLIP [ 75], DeViDe \n[33], IMITATE [34], LLaV A-Med [79], and Mammo-CLIP \n[68]. These models demonstrate the potential to address \nbiases, improve multilingual capabilities, and enhance real-\nworld performance.\nDespite ongoing efforts to mitigate bias in recent models, \nexisting datasets and methodologies remain inadequate for \nfully addressing this issue. A lack of diversity in training \ndata, such as under representation of different racial groups, \nlanguages, or clinical settings, can lead to uneven model per-\nformance, thereby increasing the risk of inaccurate or biased \noutcomes for marginalized populations. This limitation is \nparticularly concerning in clinical contexts, where fairness, \nreliability, and generalizability are critical for safe deploy -\nment. To overcome this challenge, it is essential to develop \nmore representative and inclusive datasets that accurately \nreflect the heterogeneity of real-world patient populations. \nAdditionally, robust evaluation frameworks are needed to \nassess model performance across diverse demographic and \nlinguistic subgroups.\n1 3\n826\nBiomedical Engineering Letters (2025) 15:809–830\ncredit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if you modified the licensed \nmaterial. You do not have permission under this licence to share \nadapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Cre-\native Commons licence, unless indicated otherwise in a credit line to \nthe material. If material is not included in the article’s Creative Com -\nmons licence and your intended use is not permitted by statutory regu-\nlation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit  \nh t t p  : / /  c r e a  t i  v e c  o m m o  n s .  o r g  / l i  c e n  s e s /  b y  - n c - n d / 4 . 0 /.\nReferences\n1. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von \nArx S, et al. On the opportunities and risks of foundation models. \narXiv preprint arXiv:2108.07258. 2021. arXiv:2108.07258.\n2. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, \net al. Learning transferable visual models from natural language \nsupervision. Int Conf Mach Learn. 2021;139:8748–63.\n3. Caron M, Touvron H, Misra I, Jégou H, Mairal J, Bojanowski P, \net al. Emerging properties in self-supervised vision transformers. \nProceedings of the IEEE/CVF international conference on com -\nputer vision; 2021; 9650–60.\n4. Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan \nA, et al. Language models are few-shot learners. arXiv preprint \narXiv:2005.14165. 2020;1. arXiv:2005.14165.\n5. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal \nP, et al. Language models are few-shot learners. Adv Neural Inf \nProcess Syst. 2020;33:1877–901.\n6. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts \nA, et al. Palm: scaling language modeling with pathways. J Mach \nLearn Res. 2023;24(240):1–113.\n7. Taylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Sara -\nvia E, et al. Galactica: A large language model for science. arXiv \npreprint arXiv:2211.09085. 2022. arXiv:2211.09085.\n8. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lac-\nroix T, et al. Llama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971. 2023. arXiv:2302.13971.\n9. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien \nC, et al. Gatortron: A large clinical language model to unlock \npatient information from unstructured electronic health records. \narXiv preprint arXiv:2203.03540. 2022. arXiv:2203.03540.\n10. Boecking B, Usuyama N, Bannur S, Castro DC, Schwaighofer \nA, Hyland S, et al., editors. Making the most of text semantics to \nimprove biomedical vision–language processing. European con -\nference on computer vision; 2022: Springer.\n11. Li Y , Li Z, Zhang K, Dan R, Jiang S, Zhang Y . ChatDoctor: a \nmedical chat model fine-tuned on a large language model \nmeta-AI (LLaMA) using medical domain knowledge. Cureus. \n2023;15(6):e40895.  h t t p  s : /  / d o i  . o  r g /  1 0 . 7  7 5 9  / c u  r e u s . 4 0 8 9 5.\n12. Wang J, Zhu H, Wang SH, Zhang YD. A review of deep learning \non medical image analysis. Mobile Netw Appl. 2021;26(1):351–\n80.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  1 0 3 6 - 0 2 0 - 0 1 6 7 2 - 7.\n13. Suganyadevi S, Seethalakshmi V , Balasamy K. A review on deep \nlearning in medical image analysis. Int J Multimed Inf Retr. \n2022;11(1):19–38.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  3 7 3 5 - 0 2 1 - 0 0 2 1 8 - 1.\n14. Azad B, Azad R, Eskandari S, Bozorgpour A, Kazerouni A, \nRekik I, et al. Foundational models in medical imaging: a \ncomprehensive survey and future vision. arXiv. arXiv preprint \narXiv:2310.18689. 2023;10. arXiv:2310.18689.\n15. Hartsock I, Rasool G. Vision-language models for medical report \ngeneration and visual question answering: a review. Front Artif \nVLMs across key medical imaging tasks, such as segmenta-\ntion, classification, and report generation, highlights their \ntransformative potential in enhancing diagnostic ACC and \nclinical workflows. The modality-specific categorization of \nVLMs, coupled with a detailed analysis of their strengths, \nand a systematic mapping of their clinical applications offer \na structured and comprehensive perspective on the cur -\nrent state of the art. This review aims to serve as both a \nresource and roadmap to guide researchers and practitioners \nin advancing the development and application of VLMs to \naddress the complex challenges of modern medicine.\nSupplementary Information  The online version contains \nsupplementary material available at  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  3 5 3 4 - 0 \n2 5 - 0 0 4 8 4 - 6.\nAcknowledgements We would like to thank Editage ( www.editage.\nco.kr) for the English language editing.\nAuthor contributions  Conceptualization was carried out by Sejung \nYang. Literature search was conducted by Ji Seung Ryu, Hyunyoung \nKang. Formal analysis was performed by Ji Seung Ryu, Hyunyoung \nKang. Funding acquisition was secured by Sejung Yang. Investigation \nwas undertaken by Ji Seung Ryu, Yuseong Chu. Project administration \nwas managed by Sejung Yang. Supervision was provided by Sejung \nYang. The original draft of the manuscript was written by Ji Seung \nRyu. All authors contributed to the methodology, validation, and visu-\nalization. The review and editing of manuscript were collaboratively \nperformed by all authors.\nFunding This research was supported by the National Research Foun-\ndation of Korea (NRF) grant funded by the Korea government (MSIT) \n(2022R1A2C2091160). Additional support was provided through a \ngrant from the Information and Communications Promotion Fund via \nthe National IT Industry Promotion Agency (NIPA), funded by the \nMinistry of Science and ICT (MSIT), Republic of Korea. Furthermore, \nthis research received funding from the Bio & Medical Technology \nDevelopment Program of the National Research Foundation (NRF), \nsupported by the Korean government (MSIT) (Grant No. RS-2024-\n00440802).\nDeclarations\nConflict of of interest  The authors declare that they have no relevant \nfinancial or non-financial interests to disclose.\nEthical approval  This article is a review and does not contain any \nstudies with human participants or animals performed by any of the \nauthors. Therefore, ethics approval was not required.\nConsent to publish Not applicable. This article does not contain any \nindividual person’s data in any form.\nConsent to participate Not applicable. This article does not report on \nstudies involving human participants performed by the authors.\nOpen Access   This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and \nreproduction in any medium or format, as long as you give appropriate \n1 3\n827\nBiomedical Engineering Letters (2025) 15:809–830\nProceedings of the IEEE/CVF Conference on Computer Vision \nand Pattern Recognition. 2024.\n33. Luo H, Zhou Z, Royer C, Sekuboyina A, Menze B. DeViDe: Fac-\neted medical knowledge for improved medical vision-language \npre-training. arXiv preprint arXiv:2404.03618. 2024. https:/doi.\norg/10.48550/arXiv:2404.03618.\n34. Liu C, Cheng S, Shi M, Shah A, Bai W, Arcucci R. IMITATE: \nclinical prior guided hierarchical vision-language pre-training. \nIEEE Trans Med Imaging. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / T M  I . 2 0 \n2 4 . 3 4 4 9 6 9 0.\n35. Wang P, Zhang H, Yuan Y . MCPL: multi-modal collaborative \nprompt learning for medical vision-language model. IEEE Trans \nMed Imaging. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / T M  I . 2 0 2 4 . 3 4 1 8 4 0 8.\n36. Moon JH, Lee H, Shin W, Kim Y-H, Choi E. Multi-modal \nunderstanding and generation for medical images and text via \nvision-language pre-training. IEEE J Biomed Health Inform. \n2022;26(12):6070–80.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / J B  H I . 2 0 2 2 . 3 2 0 7 5 \n0 2.\n37. Wang R, Yao Q, Lai H, He Z, Tao X, Jiang Z, et al. ECAMP: \nentity-centered context-aware medical vision language \npre-training. arXiv preprint arXiv:2312.13316. 2023. \n10.48550/arXiv:2312.13316.\n38. Yan B, Pei M. Clinical-bert: vision-language pre-training for \nradiograph diagnosis and reports generation. Proc AAAI Conf \nArtif Intell. 2022.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  6 0 9  / a a  a i . v 3 6 i 3 . 2 0 2 0 4.\n39. Chambon P, Bluethgen C, Delbrouck J-B, Van der Sluijs R, \nPołacin M, Chaves JMZ, et al. Roentgen: vision-language \nfoundation model for chest x-ray generation. arXiv preprint \narXiv:2211.12737. 2022. 10.48550/arXiv:2211.12737.\n40. Huemann Z, Tie X, Hu J, Bradshaw TJ. ConTEXTual net: a mul-\ntimodal vision-language model for segmentation of pneumotho -\nrax. J Imaging Inf Med. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  0 2 7 8 - 0 2 \n4 - 0 1 0 5 1 - 8.\n41. Li Q, Yan X, Xu J, Yuan R, Zhang Y , Feng R, et al. Anatomical \nstructure-guided medical vision-language pre-training. In: Inter -\nnational Conference on Medical Image Computing and Com -\nputer-Assisted Intervention; 2024: Springer.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  \n0 0 7  / 9 7  8 - 3 - 0 3 1 - 7 2 1 2 0 - 5 _ 8.\n42. Liu C, Cheng S, Chen C, Qiao M, Zhang W, Shah A, et al. M-flag: \nmedical vision-language pre-training with frozen language mod -\nels and latent space geometry optimization. In: International \nconference on medical image computing and computer-assisted \nintervention; 2023: Springer.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / 9 7  8 - 3 - 0 3 1 - \n4 3 9 0 7 - 0 _ 6 1.\n43. Thawakar OC, Shaker AM, Mullappilly SS, Cholakkal H, Anwer \nRM, Khan S, et al., editors. XrayGPT: Chest radiographs sum -\nmarization using large medical vision-language models. In: Pro -\nceedings of the 23rd workshop on biomedical natural language \nprocessing. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  8 6 5  3 / v  1 / 2 0 2 4 . b i o n l p - 1 . 3 5.\n44. Zhang X, Meng Z, Lever J, Ho ES. Libra: Leveraging tempo -\nral images for biomedical radiology analysis. arXiv preprint \narXiv:2411.19378. 2024. arXiv:2411.19378.\n45. Chen H, Zhao W, Li Y , Zhong T, Wang Y , Shang Y , et al. 3d-ct-\ngpt: generating 3d radiology reports through integration of large \nvision-language models. arXiv preprint arXiv:2409.19330. 2024. \narXiv:2409.19330.\n46. Hamamci IE, Er S, Almas F, Simsek AG, Esirgun SN, Dogan I, et \nal. Developing generalist foundation models from a multimodal \ndataset for 3D computed tomography. 2024;  h t t p  s : /  / d o i  . o  r g /  1 0 . 2  1 \n2 0  3 / r  s . 3 . r s - 5 2 7 1 3 2 7 / v 1.\n47. Lai H, Jiang Z, Yao Q, Wang R, He Z, Tao X, et al. E3D-GPT: \nenhanced 3D visual foundation for medical vision-language model. \narXiv preprint arXiv:2410.14200. 2024. arXiv:2410.14200.\n48. Blankemeier L, Cohen JP, Kumar A, Van Veen D, Gardezi SJS, \nPaschali M, et al. Merlin: a vision language foundation model for \nIntell. 2024;7:1430984.  h t t p  s : /  / d o i  . o  r g /  1 0 . 3  3 8 9  / f r  a i . 2 0 2 4 . 1 4 3 0 9 8 \n4.\n16. Zhang S, Metaxas D. On the challenges and perspectives of \nfoundation models for medical image analysis. Med Image Anal. \n2024;91:102996.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  / j .  m e d i a . 2 0 2 3 . 1 0 2 9 9 6.\n17. Rajpurkar P. CheXNet: Radiologist-Level Pneumonia Detec -\ntion on Chest X-Rays with Deep Learning. ArXiv abs/1711. \n2017.5225. arXiv:1711.05225.\n18. Ronneberger O, Fischer P, Brox T. U-net: convolutional networks \nfor biomedical image segmentation. Medical image computing \nand computer-assisted intervention–MICCAI 2015. 18th inter -\nnational conference, Munich, Germany, October 5-9, 2015, pro -\nceedings, part III 18. Springer; 2015. pp. 234-41.  h t t p  s : /  / d o i  . o  r g /  1 \n0 . 1  0 0 7  / 9 7  8 - 3 - 3 1 9 - 2 4 5 7 4 - 4 _ 2 8.\n19. Sobek J, Medina Inojosa JR, Medina Inojosa BJ, Rassoulinejad-\nMousavi S, Conte GM, Lopez-Jimenez F, et al. MedYOLO: a \nmedical image object detection framework. J Imaging Inform \nMed. 2024;37(6):3208–16.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  0 2 7 8 - 0 2 4 - 0 \n1 1 3 8 - 2.\n20. Lehmann TM, Güld MO, Thies C, Fischer B, Spitzer K, Keysers \nD, et al. Content-based image retrieval in medical applications. \nMethods Inf Med. 2004;43(4):354–61.\n21. Ben Abacha A, Hasan SA, Datla VV , Demner-Fushman D, Müller \nH. Vqa-med: Overview of the medical visual question answering \ntask at imageclef 2019. Proceedings of CLEF (Conference and \nLabs of the Evaluation Forum) 2019 Working Notes; 2019:9–12.\n22. Wang X, Peng Y , Lu L, Lu Z, Summers RM. Tienet: text-image \nembedding network for common thorax disease classification and \nreporting in chest x-rays. Proceedings of the IEEE conference on \ncomputer vision and pattern recognition. 2018:9049–58.\n23. Hou L, Agarwal A, Samaras D, Kurc TM, Gupta RR, Saltz JH. \nRobust histopathology image analysis: To label or to synthesize?. \nIn: Proceedings of the IEEE/CVF conference on computer vision \nand pattern recognition; 2019;2019:8533-42.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  \n1 0 9  / C V  P R . 2 0 1 9 . 0 0 8 7 3.\n24. Jing B, Xie P, Xing E. On the automatic generation of medi -\ncal imaging reports. arXiv preprint arXiv:1711.08195. 2017. \n10.48550/arXiv:1711.08195.\n25. Jia C, Yang Y , Xia Y , Chen Y-T, Parekh Z, Pham H, et al. Scaling \nup visual and vision-language representation learning with noisy \ntext supervision. In: International conference on machine learn -\ning, PMLR. 2021.\n26. Fürst A, Rumetshofer E, Lehner J, Tran VT, Tang F, Ramsauer H, \net al. Cloob: Modern hopfield networks with infoloob outperform \nclip. Adv Neural Inf Process Syst. 2022;35:20450–68.\n27. Li Y , Liang F, Zhao L, Cui Y , Ouyang W, Shao J, et al. Super -\nvision exists everywhere: a data efficient contrastive language-\nimage pre-training paradigm. arXiv preprint arXiv:2110.05208. \n2021. arXiv:2110.05208.\n28. Wang Z, Yu J, Yu AW, Dai Z, Tsvetkov Y , Cao Y . Simvlm: simple \nvisual language model pretraining with weak supervision. arXiv \npreprint arXiv:2108.10904. 2021. 10.48550/arXiv:2108.10904.\n29. Li LH, Yatskar M, Yin D, Hsieh C-J, Chang K-W. Visualbert: a \nsimple and performant baseline for vision and language. arXiv \npreprint arXiv:1908.03557. 2019. 10.48550/arXiv:1908.03557.\n30. Chen J, Guo H, Yi K, Li B, Elhoseiny M. Visualgpt: Data-effi -\ncient adaptation of pretrained language models for image caption-\ning. In: Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition. 2022.\n31. Alayrac J-B, Donahue J, Luc P, Miech A, Barr I, Hasson Y , et al. \nFlamingo: a visual language model for few-shot learning. Adv \nNeural Inf Process Syst. 2022;35:23716–36.\n32. Phan VMH, Xie Y , Qi Y , Liu L, Liu L, Zhang B, et al. Decom -\nposing disease descriptions for enhanced pathology detection: \na multi-aspect vision-language pre-training framework. In: \n1 3\n828\nBiomedical Engineering Letters (2025) 15:809–830\n64. Christensen M, Vukadinovic M, Yuan N, Ouyang D. Vision–lan-\nguage foundation model for echocardiogram interpretation. Nat \nMed. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / s 4  1 5 9 1 - 0 2 4 - 0 2 9 5 9 - y.\n65. Guo X, Chai W, Li S-Y , Wang G, editors. LLaV A-ultra: large Chi-\nnese language and vision assistant for ultrasound. In: Proceedings \nof the 32nd ACM international conference on multimedia; 2024.  \nh t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 4 5  / 3 6  6 4 6 4 7 . 3 6 8 1 5 8 4.\n66. Schmidgall S, Cho J, Zakka C, Hiesinger W. GP-VLS: a gen -\neral-purpose vision language model for surgery. arXiv preprint \narXiv:2407.19305. 2024. 10.48550/arXiv:2407.19305.\n67. Lin B, Xu Y , Bao X, Zhao Z, Zhang Z, Wang Z, et al. SkinGEN: \nan explainable dermatology diagnosis-to-generation frame -\nwork with interactive vision-language models. arXiv preprint \narXiv:2404.14755. 2024. 10.48550/arXiv:2404.14755.\n68. Ghosh S, Poynton CB, Visweswaran S, Batmanghelich K. \nMammo-clip: a vision language foundation model to enhance \ndata efficiency and robustness in mammography. In: International \nconference on medical image computing and computer-assisted \nintervention; 2024: Springer.\n69. Liu C, Ouyang C, Chen Y , Quilodrán-Casas CC, Ma L, Fu J, et \nal. T3d: towards 3d medical image understanding through vision-\nlanguage pre-training. arXiv preprint arXiv:2312.01529. 2023. \n10.48550/arXiv:2312.01529.\n70. Monajatipoor M, Dou Z-Y , Chien A, Peng N, Chang K-W. Medi-\ncal vision-language pre-training for brain abnormalities. arXiv \npreprint arXiv:2404.17779. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . \n2 4 0 4 . 1 7 7 7 9.\n71. Wang Z, Sun Q, Zhang B, Wang P, Zhang J, Zhang Q. PM2: A \nnew prompting multi-modal model paradigm for few-shot medi -\ncal image classification. arXiv preprint arXiv:2404.08915. 2024.  \nh t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 4 0 4 . 0 8 9 1 5.\n72. Wang Z, Wu Z, Agarwal D, Sun J. Medclip: Contrastive learn -\ning from unpaired medical images and text. arXiv preprint \narXiv:2210.10163. 2022.  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 2 1 0 . 1 0 \n1 6 3.\n73. Zhan C, Zhang Y , Lin Y , Wang G, Wang H. Unidcp: unifying \nmultiple medical vision-language tasks via dynamic cross-modal \nlearnable prompts. IEEE Trans Multimed. 2024.  h t t p  s : /  / d o i  . o  r g /  1 \n0 . 1  1 0 9  / T M  M . 2 0 2 4 . 3 3 9 7 1 9 1.\n74. Zhang K, Yang Y , Yu J, Jiang H, Fan J, Huang Q, et al. Multi-\ntask paired masking with alignment modeling for medical vision-\nlanguage pre-training. IEEE Trans Multimed. 2023.  h t t p  s : /  / d o i  . o  r \ng /  1 0 . 1  1 0 9  / T M  M . 2 0 2 3 . 3 3 2 5 9 6 5.\n75. Zhang S, Xu Y , Usuyama N, Xu H, Bagga J, Tinn R, et al. \nBiomedCLIP: a multimodal biomedical foundation model pre -\ntrained from fifteen million scientific image-text pairs. arXiv pre-\nprint arXiv:2303.00915. 2023.  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 3 \n0 3 . 0 0 9 1 5.\n76. Chen Z, Li G, Wan X, editors. Align, reason and learn: Enhancing \nmedical vision-and-language pre-training with knowledge. In: \nProceedings of the 30th ACM international conference on multi -\nmedia. 2022.\n77. Chen Z, Pekis A, Brown K. Advancing high resolution vision-lan-\nguage models in biomedicine. arXiv preprint arXiv:2406.09454. \n2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 4 0 6 . 0 9 4 5 4.\n78. Han X, Chen Q, Xie Z, Li X, Yang H. Multiscale progressive text \nprompt network for medical image segmentation. Comput Graph. \n2023;116:262–74.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  / j .  c a g . 2 0 2 3 . 0 8 . 0 3 0.\n79. Li C, Wong C, Zhang S, Usuyama N, Liu H, Yang J, Naumann T, \nPoon H, Gao J. Llava-med: training a large language-and-vision \nassistant for biomedicine in one day. Adv Neural Inf Process Syst. \n2023;36:28541–64.\n80. Li S, Lin L, Huang Y , Cheng P, Tang X. Text-guided foundation \nmodel adaptation for long-tailed medical image classification. \nIn: 2024 IEEE international symposium on biomedical imaging \n3d computed tomography. Res Square. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 2  \n1 2 0  3 / r  s . 3 . r s - 4 5 4 6 3 0 9 / v 1.\n49. Li H, Liu H, Hu D, Wang J, Oguz I, editors. Promise: prompt-\ndriven 3D medical image segmentation using pretrained image \nfoundation models. In: 2024 IEEE international symposium on \nbiomedical imaging (ISBI); 2024: IEEE.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / \nI S  B I 5  6 5 7  0 . 2 0  2 4  . 1 0 6 3 5 2 0 7.\n50. Shi Y , Zhu X, Hu Y , Guo C, Li M, Wu J. Med-2E3: A 2D-enhanced \n3D medical multimodal large language model. arXiv preprint \narXiv:2411.12783. 2024. arXiv:2411.12783.\n51. Zhou Z, Xia S, Shu M, Zhou H. Fine-grained abnormality detec -\ntion and natural language description of medical CT images using \nlarge language models. Int J Innov Res Comput Sci Technol. \n2024;12(6):52–62.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / I C  H I 6 1 2 4 7 . 2 0 2 4 . 0 0 0 \n8 0.\n52. Cherukuri TK, Shaik NS, Bodapati JD, Ye DH. GCS-M3VLT: \nguided context self-attention based multi-modal medical vision \nlanguage transformer for retinal image captioning. arXiv preprint \narXiv:2412.17251. 2024. arXiv:2412.17251.\n53. Du J, Guo J, Zhang W, Yang S, Liu H, Li H, et al., editors. Ret-\nclip: a retinal image foundation model pre-trained with clinical \ndiagnostic reports. In: International conference on medical image \ncomputing and computer-assisted intervention; 2024: Springer.\n54. Luo Y , Shi M, Khan MO, Afzal MM, Huang H, Yuan S, et al. \nFairclip: harnessing fairness in vision-language learning. In: Pro-\nceedings of the IEEE/CVF conference on computer vision and \npattern recognition; 2024.\n55. Silva-Rodriguez J, Chakor H, Kobbi R, Dolz J, Ayed IB. A \nfoundation language-image model of the retina (flair): encod -\ning expert knowledge in text supervision. Med Image Anal. \n2025;99:103357.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  / j .  m e d i a . 2 0 2 4 . 1 0 3 3 5 7.\n56. Wei H, Liu B, Zhang M, Shi P, Yuan W. VisionCLIP: an Med-\nAIGC based ethical language-image foundation model for gener-\nalizable retina image analysis. arXiv preprint arXiv:2403.10823. \n2024. arXiv:2403.10823.\n57. Yang S, Du J, Guo J, Zhang W, Liu H, Li H, et al. ViLReF: an expert \nknowledge enabled vision-language retinal foundation model. \narXiv preprint arXiv:2408.10894. 2024. arXiv:2408.10894.\n58. Li Z, Song D, Yang Z, Wang D, Li F, Zhang X, et al. VisionUnite: \na vision-language foundation model for ophthalmology enhanced \nwith clinical knowledge. arXiv preprint arXiv:2408.02865. 2024. \narXiv:2408.02865.\n59. Chen Q, Hong Y . Medblip: Bootstrapping language-image pre-\ntraining from 3d medical images and texts. In: Proceedings of the \nAsian conference on computer vision; 2024.\n60. Wan Z, Liu C, Zhang M, Fu J, Wang B, Cheng S, Ma L, Quilo -\ndrán-Casas C, Arcucci R. Med-unic: unifying cross-lingual medi-\ncal vision-language pre-training by diminishing bias. Adv Neural \nInf Process Syst. 2023;36:56186–97.\n61. Xu Z, Chen C, Lu D, Sun J, Wei D, Zheng Y , et al. FM-ABS: \npromptable foundation model drives active barely supervised \nlearning for 3D medical image segmentation. In: International \nconference on medical image computing and computer-assisted \nintervention; 2024: Springer.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / 9 7  8 - 3 - 0 3 1 - \n7 2 1 1 1 - 3 _ 2 8.\n62. Ferber D, Wölflein G, Wiest IC, Ligero M, Sainath S, Ghaffari \nLaleh N, et al. In-context learning enables multimodal large lan -\nguage models to classify cancer pathology images. Nat Commun. \n2024;15(1):10104.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / s 4  1 4 6 7 - 0 2 4 - 5 1 4 6 5 - 9.\n63. V o HQ, Wang L, Wong KK, Ezeana CF, Yu X, Yang W, et al. \nFrozen large-scale pretrained vision-language models are the \neffective foundational backbone for multimodal breast cancer \nprediction. IEEE J Biomed Health Inform. 2024.  h t t p  s : /  / d o i  . o  r g \n/  1 0 . 1  1 0 9  / J B  H I . 2 0 2 4 . 3 5 0 7 6 3 8.\n1 3\n829\nBiomedical Engineering Letters (2025) 15:809–830\nIn: 2024 IEEE international symposium on biomedical imaging \n(ISBI); 2024: IEEE.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / I S  B I 5  6 5 7  0 . 2 0  2 4  . 1 0 6 \n3 5 4 2 1.\n91. Zheng X, Zhang Y , Zhang H, Liang H, Bao X, Jiang Z, et al. \nCurriculum prompting foundation models for medical image seg-\nmentation. In: International conference on medical image com -\nputing and computer-assisted intervention; 2024: Springer.\n92. Johnson AE, Pollard TJ, Berkowitz SJ, Greenbaum NR, Lun -\ngren MP, Deng C-Y , et al. MIMIC-CXR, a de-identified publicly \navailable database of chest radiographs with free-text reports. Sci \nData. 2019;6(1):317.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / s 4  1 5 9 7 - 0 1 9 - 0 3 2 2 \n- 0.\n93. Irvin J, Rajpurkar P, Ko M, Yu Y , Ciurea-Ilcus S, Chute C, et al. \nChexpert: A large chest radiograph dataset with uncertainty labels \nand expert comparison. Proceedings of the AAAI conference on \nartificial intelligence; 2019.\n94. Wang X, Peng Y , Lu L, Lu Z, Bagheri M, Summers RM. Chestx-\nray8: Hospital-scale chest x-ray database and benchmarks on \nweakly-supervised classification and localization of common tho-\nrax diseases. Proceedings of the IEEE conference on computer \nvision and pattern recognition; 2017.\n95. Porwal P, Pachade S, Kokare M, Deshmukh G, Son J, Bae W, \net al. Idrid: diabetic retinopathy–segmentation and grading chal -\nlenge. Med Image Anal. 2020;59:101561.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  \n/ j .  m e d i a . 2 0 1 9 . 1 0 1 5 6 1.\n96. Liu R, Wang X, Wu Q, Dai L, Fang X, Yan T, et al. Deepdrid: \nDiabetic retinopathy—grading and image quality estimation \nchallenge. Patterns. 2022.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 6  / j .  p a t t e r . 2 0 2 2 . 1 \n0 0 5 1 2.\n97. Orlando JI, Fu H, Breda JB, van Keer K, Bathula DR, Diaz-Pinto \nA, et al. Refuge challenge: a unified framework for evaluating \nautomated methods for glaucoma assessment from fundus photo-\ngraphs. Med Image Anal. 2020;59:101570.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 1 \n6  / j .  m e d i a . 2 0 1 9 . 1 0 1 5 7 0.\n98. Bustos A, Pertusa A, Salinas J-M, De La Iglesia-Vaya M. \nPadchest: a large chest x-ray image dataset with multi-label anno-\ntated reports. Med Image Anal. 2020;66:101797.  h t t p s :   /  / d o  i . o  r  g  /  1 \n0  . 1 0   1  6  / j . m e  d i a .  2 0 2 0 . 1 0 1 7 9 7.\nPublisher's Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\n(ISBI); 2024: IEEE.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / I S  B I 5  6 5 7  0 . 2 0  2 4  . 1 0 6 \n3 5 4 6 2.\n81. Li Z, Li Y , Li Q, Wang P, Guo D, Lu L, et al. Lvit: language meets \nvision transformer in medical image segmentation. IEEE Trans \nMed Imaging. 2023.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  1 0 9  / T M  I . 2 0 2 3 . 3 2 9 1 7 1 9.\n82. Wu J, Xu M. One-prompt to segment all medical images. In: Pro-\nceedings of the IEEE/CVF conference on computer vision and \npattern recognition; 2024.\n83. Yang B, Yu Y , Zou Y , Zhang T. Pclmed: champion solution for \nimageclefmedical 2024 caption prediction challenge via medi -\ncal vision-language foundation models. CLEF2024 Working \nNotes, CEUR Workshop Proceedings, CEUR-WS org, Grenoble, \nFrance; 2024.\n84. Zhang K, Zhou R, Adhikarla E, Yan Z, Liu Y , Yu J, et al. A gen-\neralist vision–language foundation model for diverse biomedical \ntasks. Nat Med. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 3 8  / s 4  1 5 9 1 - 0 2 4 - 0 3 1 8 \n5 - 2.\n85. Jiang Z, Cheng D, Qin Z, Gao J, Lao Q, Ismoilovich AB, et al. \nTV-SAM: increasing zero-shot segmentation performance on \nmultimodal medical images using GPT-4 generated descrip -\ntive prompts without human annotation. Big Data Min Analyt. \n2024;7(4):1199–211.\n86. Kapadnis MN, Patnaik S, Nandy A, Ray S, Goyal P, Sheet D. \nSERPENT-VLM: self-refining radiology report generation using \nvision language models. arXiv preprint arXiv:2404.17912. 2024.  \nh t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 4 0 4 . 1 7 9 1 2.\n87. Koleilat T, Asgariandehkordi H, Rivaz H, Xiao Y . BiomedCoOp: \nlearning to prompt for biomedical vision-language models. arXiv \npreprint arXiv:2411.15232. 2024.  h t t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . \n2 4 1 1 . 1 5 2 3 2.\n88. Lee C, Park S, Shin C-I, Choi WH, Park HJ, Lee JE, et al. Read \nlike a radiologist: efficient vision-language model for 3D medical \nimaging interpretation. arXiv preprint arXiv:2412.13558. 2024.  h \nt t p  s : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 4 1 2 . 1 3 5 5 8.\n89. Nath V , Li W, Yang D, Myronenko A, Zheng M, Lu Y , et al. \nVILA-M3: enhancing vision-language models with medical \nexpert knowledge. arXiv preprint arXiv:2411.12915. 2024.  h t t p  \ns : /  / d o i  . o  r g /  1 0 . 4  8 5 5  0 / a  r X i v . 2 4 1 1 . 1 2 9 1 5.\n90. Wu S, Yang B, Ye Z, Wang H, Zheng H, Zhang T. MAKEN: \nimproving medical report generation with adapter tuning and \nknowledge enhancement in vision-language foundation models. \n1 3\n830",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7759244441986084
    },
    {
      "name": "Current (fluid)",
      "score": 0.6739912033081055
    },
    {
      "name": "Engineering",
      "score": 0.44333091378211975
    },
    {
      "name": "Engineering ethics",
      "score": 0.4133642911911011
    },
    {
      "name": "Computer science",
      "score": 0.406350702047348
    },
    {
      "name": "Medical physics",
      "score": 0.36380183696746826
    },
    {
      "name": "Systems engineering",
      "score": 0.3326285481452942
    },
    {
      "name": "Medicine",
      "score": 0.32225847244262695
    },
    {
      "name": "Electrical engineering",
      "score": 0.17499014735221863
    },
    {
      "name": "Political science",
      "score": 0.17128649353981018
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    }
  ]
}