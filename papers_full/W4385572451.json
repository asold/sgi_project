{
  "title": "Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers",
  "url": "https://openalex.org/W4385572451",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2609011906",
      "name": "James Michaelov",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2115683131",
      "name": "Benjamin Bergen",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2119728020",
    "https://openalex.org/W2996107880",
    "https://openalex.org/W4286988498",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3102485638",
    "https://openalex.org/W3184030040",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W4297412056",
    "https://openalex.org/W2974597461",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4226056216",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962678860",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2977268464",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2130184303",
    "https://openalex.org/W4286850188",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2997734032",
    "https://openalex.org/W4296413749",
    "https://openalex.org/W2072893832",
    "https://openalex.org/W4308245303",
    "https://openalex.org/W2038959141",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3133702157"
  ],
  "abstract": "How well do language models deal with quantification? In this study, we focus on 'few'-type quantifiers, as in 'few children like toys', which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 14162–14174\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRarely a problem? Language models exhibit inverse scaling in their\npredictions following few-type quantifiers\nJames A. Michaelov\nDepartment of Cognitive Science\nUniversity of California, San Diego\nj1michae@ucsd.edu\nBenjamin K. Bergen\nDepartment of Cognitive Science\nUniversity of California, San Diego\nbkbergen@ucsd.edu\nAbstract\nHow well do language models deal with quan-\ntification? In this study, we focus on few-type\nquantifiers, as in few children like toys, which\nmight pose a particular challenge for language\nmodels because the sentence components with-\nout the quantifier are likely to co-occur, and\nfew-type quantifiers are rare. We present 960\nEnglish sentence stimuli from two human neu-\nrolinguistic experiments to 22 autoregressive\ntransformer models of differing sizes. Not only\ndo all the models perform poorly on few-type\nquantifiers, but overall the larger the model, the\nworse its performance. This inverse scaling is\nconsistent with previous work suggesting that\nlarger models increasingly reflect online rather\nthan offline human processing, and we argue\nthat the decreasing performance of larger mod-\nels may challenge uses of language models as\nthe basis for natural language systems.\n1 Introduction\nQuantifiers can dramatically alter the meaning of\nan utterance. Consider the sentences in (1).\n(1) (a) Most sharks are harmless.\n(b) Most sharks are dangerous.\n(c) Few sharks are harmless.\n(d) Few sharks are dangerous.\nDespite the fact that (a) and (c) have the same\ncontent words in the same syntactic arrangement,\nthe statements have starkly different meanings. The\nsame is true of (b) and (d). Being able to success-\nfully comprehend these differences is useful, and\nin an example such as this one, vitally important1.\nYet current work suggests that language mod-\nels deal poorly with quantifiers—they struggle to\npredict which quantifier is used in a given context\n(Pezzelle et al., 2018; Talmor et al., 2020), and also\n1Note that most sharks are in fact harmless to hu-\nmans; see, e.g., https://www.floridamuseum.ufl.edu/\ndiscover-fish/sharks/shark-attack-faq/ .\nperform poorly at generating appropriate continu-\nations following logical quantifiers (Kalouli et al.,\n2022). This is especially concerning given the re-\ncent trend of using large language models (some-\ntimes referred to as ‘foundation models’; Bom-\nmasani et al., 2021) as general systems that can per-\nform multiple tasks, including question answering,\nwithout specific training (Brown et al., 2020; Raffel\net al., 2020; Lin et al., 2021; Srivastava et al., 2022;\nHoffmann et al., 2022; Rae et al., 2022; Zhang\net al., 2022; Chowdhery et al., 2022). It is thus cru-\ncial that such systems be able to distinguish among\nsentences like those in (1) in human-like ways both\nduring training and when generating responses.\nThe aim of the present study is to evaluate how\nwell language models take into account the mean-\ning of a quantifier when generating the text that\nfollows it, and to investigate whether this scales\nwith model size. We are particularly interested in\nthe question of whether language models exhibit\ninverse scaling—that is, whether as model size\nincreases, performance decreases rather than in-\ncreases (Perez et al., 2022; McKenzie et al., 2022a).\nInverse scaling is an issue of serious concern for de-\nveloping and training new language models, since\ninverse scaling could indicate ‘outer misalignment’\n(Perez et al., 2022)—that the training approach is\nleading to models that produce undesirable outputs,\nwhich may get worse as performance at training ob-\njectives increases. Inverse scaling is also a concern\nfor models’ ultimate use. As models increase in\nsize and perform better at a wider range of bench-\nmarks (for recent examples, see, e.g., Srivastava\net al., 2022; Chowdhery et al., 2022), they may be\nincreasingly assumed to be trustworthy and general-\npurpose, and thus able to perform well tasks on\nwhich they have not been tested (Raji et al., 2021).\nThis could lead to a range of possible harms, from\nmisidentifying whether something is dangerous or\nnot (as in the opening example), to amplifying bi-\nases (Bender et al., 2021).\n14162\nTo test how well language models deal with\nquantifiers, we follow the approach of Ettinger\n(2020) in using sentences from a study on human\nlanguage comprehension to inform our evaluation.\nEttinger (2020) found that following a negation,\nthe predictions of BERT BASE and BERTLARGE in\nsimple sentences expressing a proposition with\nor without negation (from Fischler et al., 1984)\ndo not appear sensitive to negation—for example,\nBERTLARGE predicts the final word of a robin is a\nbird to be more likely than a robin is atree, but\nalso predicts that a robin is not abird is more likely\nthan a robin is not atree. In this way, the mod-\nels’ predictions more closely match those made\nby humans ‘online’—that is, incrementally during\nthe process of language comprehension—than our\nfully-formed ‘offline’ judgements: in their original\nstudy, Fischler et al. (1984) found that the word\nbird elicited an N400 response of smaller ampli-\ntude than tree in both contexts, indicating that it\nwas more strongly predicted.\nSimilar effects have been reported (Kassner and\nSchütze, 2020; Kalouli et al., 2022) for other trans-\nformers such as Transformer-XL (Dai et al., 2019),\nRoBERTa (Liu et al., 2019), and ALBERT (Lan\net al., 2020), as well as ELMo (Peters et al., 2018).\nWorse, recent work suggests that as language mod-\nels increase in size, their ability to deal with nega-\ntion may degrade: an inverse scaling relationship\nhas been reported for performance at a wide range\nof tasks when prompts include negation (McKen-\nzie et al., 2022b; Jang et al., 2023), though it is\npossible that this may reverse at extremely large\nscales (Wei et al., 2022).\nNegation may be particularly challenging for\nstatistical language models because its presence\nradically alters the meaning of a sentence, but\nnegation occurs in only about 10% of sentences\n(Jiménez-Zafra et al., 2020). Quantifiers similarly\nimpose radical modulations to meaning while also\nbeing relatively infrequent (see Appendix B). In\nthe present study, we focus on quantifiers indicat-\ning typicality such as most and few. To the best of\nour knowledge, only one study has evaluated model\npredictions following any quantifiers (Kalouli et al.,\n2022), and it focused on words corresponding to\nlogical quantifiers such as all, every, and some.\nThe few studies involving the quantifiers we ad-\ndress either focus on predicting the quantifier it-\nself (Pezzelle et al., 2018; Talmor et al., 2020), or\nuse RNNs to investigate modeling significant ef-\nfects on the N400 without any form of evaluation\n(Michaelov and Bergen, 2020). This study, there-\nfore, represents the first attempt to explicitly evalu-\nate the predictions of language models following\nmost and few-type quantifiers.\nIn the present study, we carry out two experi-\nments. In the first, following Ettinger (2020), we\nuse the stimuli from a previously published N400\nstudy (Urbach and Kutas, 2010). In it, Urbach\nand Kutas (2010) found that while most and few-\ntype quantifiers do impact N400 amplitude, it is\nnot enough to reverse predictions— few farmers\ngrow crops elicits a smaller N400 response than\nfew farmers growworms, indicating that crops was\nmore strongly predicted than worms, even though\nexperimental participants judged it to be less plau-\nsible off-line. We test whether language models\nshow the same pattern of insensitivity towards the\nquantifiers that humans do in online measures. In\nthis way, we test how closely the predictions of\nlanguage models correlate with those underlying\nthe human N400 response.\nIn our second experiment, we extend our study\nfurther. Experiment 1 aims to replicate the original\nN400 results of Urbach and Kutas (2010); how-\never, one thing that it does not account for is that\nwhile a given complete sentence (e.g., few farmers\ngrow crops.) can be highly unlikely and implau-\nsible, sentences beginning with the same words\nmay not be (for example, in the plausible sentence\nfew farmers growcrops in the winter). Experiment\n1 does not distinguish between these possibilities,\nand while it is important to test the sensitivity of\nlanguage models to few-type quantifiers, if they\nfail to show a difference for complete sentences\nincluding the final period (e.g., few farmers grow\ncrops.), this is more concerning. Thus, in Exper-\niment 2, we run the same stimuli as Experiment\n1, but including a period following the final word\n(e.g., crops./worms.).\n2 Experiment 1: Replication of Urbach\nand Kutas (2010)\n2.1 Materials\nIn this experiment, we use all the stimuli from\ntwo experiments carried out by Urbach and Kutas\n(2010). These are made up of 120 sentence frames\nwith 8 different sentence types falling into 4 experi-\nmental conditions, for a total of 960 sentences. The\n4 conditions had a 2x2 design—each stimulus was\neither typical (T) or atypical (A), and had either\n14163\nFew-type Accuracy Most-type Accuracy Sensitivity\n108 109 1010 1011 108 109 1010 1011 108 109 1010 1011\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.85\n0.90\n0.95\n0.05\n0.10\n0.15\nNumber of Parameters\nScore\nModel Series\nGPT-2\nGPT-3\nGPT-Neo\nInstructGPT\nOPT\nFigure 1: Accuracy and sensitivity of all models.\na most-type or few-type quantifier. An example\nof the 8 sentence types comprising one sentence\nframe is shown in (2).\n(2) (a) Most squirrels gather nuts... (T, most)\n(b) Most squirrels gather nails... (A, most)\n(c) Few squirrels gather nuts... (T, few)\n(d) Few squirrels gather nails... (A, few)\n(e) Squirrels often gather nuts... (T, most)\n(f) Squirrels often gather nails... (A, most)\n(g) Squirrels rarely gather nuts... (T, few)\n(h) Squirrels rarely gather nails... (A, few)\nThe quantifiers used in sentences (a)-(d) differed\nby sentence frame; see Appendix B for a full list.\n2.2 Language Models\nTo cover a range of language models with different\ntraining data and numbers of parameters, we run\nour analyses on the GPT-2 (Radford et al., 2019),\nGPT-3 (Brown et al., 2020), GPT-Neo (Black et al.,\n2021; including GPT-J, Wang and Komatsuzaki,\n2021), and OPT (Zhang et al., 2022) language mod-\nels. We also include an analysis of the first series\nof InstructGPT models (text-davinci-001 etc.),\nwhich were finetuned on human-written and highly-\nrated model-generated responses (OpenAI, 2023).\n2.3 Evaluation\nFor each stimulus sentence, we calculate the sur-\nprisal of the critical word, that is, the word for\nwhich the N400 response was measured in the orig-\ninal study. Because humans only encounter the\ncontext preceding the critical word when process-\ning the word, and because the language models we\nanalyze are all autoregressive, we only consider the\nsurprisal of the critical word given its preceding\ncontext. To do this we truncated the sentence be-\nfore the critical word, and then used the relevant\nlanguage model to calculate the probabilityp of the\ntarget word given the preceding context, which was\nthen converted to surprisal S following Equation 1.\nS = −log p(wi|w1...wi−1) (1)\nIn previous work of this type (e.g., Ettinger,\n2020), only words that were single tokens in the\nmodels’ vocabularies were used. In this study,\nall models are autoregressive, so for multi-token\nwords, consecutive sub-word tokens can be pre-\ndicted, the product of which is a well-defined prob-\nability for the whole word. The surprisal of such\nwords, then, is the sum of the surprisals of the sub-\nword tokens. Calculating surprisal this way allows\nus to compare the predictions of all the models for\nall the stimuli in the original experiment.\nIn order to evaluate how well each model takes\ninto account the quantifier in its predictions, we\ncompared which of the two possible critical words\n(typical or atypical) had a lower surprisal, i.e., was\nmore strongly predicted by the model. To align\nwith human plausibility judgements, following a\nmost-type quantifier, the typical continuation was\njudged to be correct, and following afew-type quan-\ntifier, the atypical continuation was judged to be\ncorrect. Accuracy was calculated as the fraction of\nthe stimulus pairs for which the model predicted\nthe appropriate critical word—that is, predicted the\ncorrect continuation more strongly than the incor-\nrect one. For example, the set of stimuli presented\nin (2) is made up of 4 pairs of stimuli, and for a\nmodel to achieve 100% accuracy (4/4), it would\nneed to predict (a) over (b), (d) over (c), (e) over (f),\nand (h) over (g). This design intrinsically controls\nfor any differences in unconditioned probability\namong the final words themselves.\nFollowing Ettinger (2020), we also analyzed\nmodel sensitivity to the quantifiers. In the present\nstudy, this corresponds to the question of whether,\n14164\nfor a given sentence frame, the model makes a\ndifferent prediction following a few-type quanti-\nfier than it does following a most-type quantifier.\nWe defined sensitivity as the proportion of stimuli\nfor which the model correctly predicts the critical\nword following both themost-type and the few-type\nquantifier. Thus, the stimuli in each sentence frame\nprovide 2 data points for sensitivity: in (2), sensi-\ntivity is calculated for (a)-(d) and for (e)-(h). For\nthe (a)-(d) stimuli, a model would be considered\nsensitive to the quantifier if it correctly predicted\n(a) over (b) and (d) over (c). Code and data are\navailable at https://osf.io/vjyw9.\n2.4 Results\nEach model’s accuracy at predicting the critical\nwords following most- and few-type quantifiers is\nshown in Figure 1. All model series show the same\ngeneral tendencies in accuracy: (1) they perform\nquite poorly for few-type quantifiers but relatively\nwell for most-type quantifiers; and (2) as model\nsize increases, word prediction followingmost-type\nquantifiers improves, but it degrades following few-\ntype quantifiers. Figure 1 does show small excep-\ntions to this pattern. From GPT-2 762M to 1542M\nand from InstructGPT 13B to 175B, while most-\nperformance increases, few-performance does not\ndecrease. Furthermore, from OPT 125M to 350M,\nand from OPT 2.7B to 6.7B, there is actually a\nslight improvement. Nonetheless, these differences\nare small compared to the overall decreases in per-\nformance, and the general trends are still clear—\nfor example, no model performs better on few-type\nquantifiers than a model two or more sizes smaller.\nWith sensitivity, as shown in Figure 1, some\nmodels improve as they increase in size, and some\nget worse; however, even the greatest distance be-\ntween the sensitivity of two models in the same\nseries (InstructGPT 2.7B and 13B) is only 3.4%.\nThus, other than the general fact that sensitivity is\nlow across all models, there does not appear to be\nany clear pattern, suggesting that sensitivity does\nnot drive the effects seen in accuracy. All accuracy\nand sensitivity scores can be found in Appendix A.\n2.5 Discussion\nThese results show that contemporary autoregres-\nsive transformer models perform poorly on few-\ntype quantifiers, and that as these models increase\nin size, they tend to improve at predicting words\nfollowing most-type quantifiers but get worse at\npredicting words following few-type quantifiers. In\nfact, we see that models that better predicted the\nmore typical word after amost-type quantifier were\nalso worse at predicting the less typical word fol-\nlowing a least-type quantifier. The fact that models\nwere evaluated on which of the two options they\npredicted to be more likely, combined with gener-\nally poor and largely invariant sensitivity (peaking\nat 5%), suggests that the larger models generally\nmade predictions increasingly in accordance with\ntypicality, overwhelming any sensitivity to quanti-\nfier type. This aligns with previous work on nega-\ntion and logical quantifiers in language models (Et-\ntinger, 2020; Kassner and Schütze, 2020; Kalouli\net al., 2022), as well as the N400 results of the\noriginal study by Urbach and Kutas (2010).\n3 Experiment 2: Sentence-final nouns\n3.1 Method\nThe models and evaluation approach were identical\nto Experiment 1. The materials were identical to\nExperiment 1 with the single difference that all\nnouns were followed by a period, and the surprisal\nof this period was included when calculating the\ntotal surprisal of the critical word (e.g., nuts. or\nnails. for the example presented in (2)). Thus,\nsurprisal reflected both the surprisal of the critical\nword in context and the surprisal of the word being\nfollowed by a period, i.e., being the last word in\nthe sentence. For a discussion of modeling the\nprobability of sentence-final words in this way, see\nSzewczyk and Federmeier (2022).\n3.2 Results\nResults are shown in Figure 2. As in Experiment\n1, larger models perform worse overall. How-\never, there is a small improvement in the very\nlargest GPT-3 and InstructGPT models relative to\nthe second-largest models of the same type, both\nin few-type accuracy and sensitivity. Performance\nalso increases on these metrics between OPT 2.7B\nand OPT 6.7B; however, this decreases with OPT\n13B. All accuracy and sensitivity scores can be\nfound in Appendix A.\n3.3 Discusion\nOverall, the results are similar to those of Experi-\nment 1: Larger models of the same type perform\nworse than smaller models. Whether the small im-\nprovement of the largest GPT-3 and InstructGPT\nmodels relative to the second-largest models is a\nfluctuation like that seen for OPT or the beginnings\n14165\nFew-type Accuracy Most-type Accuracy Sensitivity\n108 109 1010 1011 108 109 1010 1011 108 109 1010 1011\n0.02\n0.04\n0.06\n0.80\n0.85\n0.90\n0.95\n0.05\n0.10\n0.15\n0.20\n0.25\nNumber of Parameters\nScore\nModel Series\nGPT-2\nGPT-3\nGPT-Neo\nInstructGPT\nOPT\nFigure 2: Accuracy and sensitivity of all models on stimuli with added periods (e.g., Few squirrels gathernuts.).\nof a U-shaped curve (see Wei et al., 2022) is a\nquestion for further research.\n4 General Discussion\nIn this study, we investigated whether language\nmodels show the same insensitivity towards few-\ntype and most-type quantifiers observed in the pre-\ndictions made by humans during language com-\nprehension, as indexed by the N400 response. We\nfind that when tested on the same stimuli, they do,\npredicting the ostensibly implausible few squirrels\ngather nuts to be more likely than few squirrels\ngather nails. Moreover, we find that as language\nmodels increase in size, they tend to show this\neffect to a greater extent, an example of inverse\nscaling. Based on our analysis of sensitivity and\naccuracy with most-type quantifiers, we hypothe-\nsize that these results are due to a low degree of\nsensitivity to quantifiers and an increase in sensitiv-\nity to typicality. In other words, language models\nappear to be increasingly sensitive to the fact that\nsquirrels gathernuts is more plausible than squir-\nrels gathernails, but not to the effect on meaning\nthat is caused by a preceding most or few.\nIt is often assumed that as models increase in size\nand are trained on more data, their performance on\nnatural language tasks generally improves—indeed,\nevidence supports this (Brown et al., 2020; Raffel\net al., 2020; Lin et al., 2021; Srivastava et al., 2022;\nHoffmann et al., 2022; Rae et al., 2022; Zhang\net al., 2022; Chowdhery et al., 2022). However, the\npredictions of larger models and those trained on\nmore data also increasingly correlate with human\nincremental online predictions, in particular those\nindexed by N400 amplitude (Frank et al., 2015;\nAurnhammer and Frank, 2019a,b; Michaelov and\nBergen, 2020; Merkx and Frank, 2021; Michaelov\net al., 2021, 2022). The two are often aligned—it is\neasier for humans to process well-formed sentences\nwith plausible semantics (Frisch and Schlesewsky,\n2005; Nieuwland et al., 2020). But in cases such\nas the present study, the two are not aligned, and\nwe see instead that the predictions of larger mod-\nels correlate better with human online predictions,\neven when these are contrary to offline judgements.\nThus, the increased performance we see at tasks\ncorresponding to offline human judgements—and\nnote that virtually all manually-annotated tasks are\nbased on offline human judgements—may in fact\nbe a by-product of the models’ predictions resem-\nbling the online predictions.\nFortunately, the literature boasts a wealth of psy-\ncholinguistic studies where metrics of online pre-\ndiction such as the N400 appear to conflict with\noffline judgements. Future work could use these to\nidentify phenomena where language models may\nstruggle to make predictions in line with human\njudgements. Such cases are important to detect\nas use of LMs becomes more widespread. But by\nthe same token, the present study shows that as\nlanguage models increase in size, even when aug-\nmented by finetuning on desirable responses, they\ncan make predictions that align less and less with\nexplicit human judgements.\nThis may be a clear indication of an inherent\n‘outer misalignment’ present in language models:\nwhile humans might like language models to gen-\nerate plausible sentences, by their nature they can\nonly generate the most statistically probable ones.\nJust as there is no guarantee of accuracy or coher-\nence (Bender et al., 2021), there is no guarantee\nof plausibility. While it may be possible to tai-\nlor training to avoid specific known issues, this\nmisalignment between probability and plausibility\nmay pose a fundamental challenge with current\napproaches that aim to use language models as\ngeneral-purpose natural language systems.\n14166\nLimitations\nThere are two main limitations to our study. The\nfirst is that the stimuli used were limited to those\nprovided by Urbach and Kutas’s (2010) study. This\nis because, as stated, we wanted to be able to com-\npare the patterns in the language models’ predic-\ntions to the patterns in the human N400 response.\nThus, we do not look at logical quantifiers like\nKalouli et al. (2022), or any others that have previ-\nously been studied (in, e.g., Pezzelle et al., 2018;\nTalmor et al., 2020).\nThe other (and perhaps more important) limita-\ntion is in the models we were able to use. Crucially,\nwe were not able to access models larger than GPT-\n3 175B such as PaLM 540B (Chowdhery et al.,\n2022). This is important because recent work has\nshown that some inverse scaling patterns become\nU-shaped (i.e., as language model size increases,\nperformance degrades and then improves again)\nwith such larger models (Wei et al., 2022).\nEthics Statement\nOur work complies with the ACL Ethics Policy.\nBeyond this, we are not aware of any way in which\nthe results of this study may be harmful—in fact,\nif anything, identifying the limitations of large lan-\nguage models is something that is likely to reduce\npossible harms by demonstrating cases where their\nuse is not suitable.\nFrom an environmental perspective, we did not\ntrain any models; we only used pretrained models\nfor analysis, limiting energy consumption. With the\nexception of the GPT-3 and InstructGPT models\nand OPT 13B, all analyses were run on an NVIDIA\nRTX A6000 GPU, taking a total of 43 minutes.\nOPT 13B was too large to run on this GPU, and\nthus was run on an Intel Dual Xeon E7-4870 CPU\nfor a total of 22 hours and 39 minutes. Finally, the\nGPT-3 and the InstructGPT models were run using\nthe OpenAI API, and thus we do not have access\nto information about the GPUs used.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their helpful comments. We would also like to\nacknowledge the other members of the Language\nand Cognition Lab at UCSD for their valuable dis-\ncussion, as well as Roger Levy and attendees of the\nMIT Computational Psycholinguistics Laboratory\nmeeting. Finally, we would like to thank the San\nDiego Social Sciences Computing Facility Team\nfor the use of the Social Sciences Research and\nDevelopment Environment (SSRDE) cluster. The\nRTX A6000 used for this research was donated by\nthe NVIDIA Corporation.\nReferences\nChristoph Aurnhammer and Stefan L. Frank. 2019a.\nComparing Gated and Simple Recurrent Neural Net-\nwork Architectures as Models of Human Sentence\nProcessing. In Proceedings of the 41st Annual Meet-\ning of the Cognitive Science Society (CogSci 2019).\nChristoph Aurnhammer and Stefan L. Frank. 2019b.\nEvaluating information-theoretic measures of word\nprediction in naturalistic sentence reading. Neuropsy-\nchologia, 134:107198.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\nDangers of Stochastic Parrots: Can Language Models\nBe Too Big? &#x1f99c;. In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’21, pages 610–623, New York,\nNY , USA. Association for Computing Machinery.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nZenodo.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, S. Buch, Dallas Card,\nRodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen A. Creel, Jared Davis, Dora Dem-\nszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\nmus, Stefano Ermon, John Etchemendy, Kawin Etha-\nyarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lau-\nren E. Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, O. Khat-\ntab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak,\nMina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Ma-\nlik, Christopher D. Manning, Suvir P. Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Benjamin Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJ. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim-\nitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Robert Re-\nich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani,\nCamilo Ruiz, Jack Ryan, Christopher R’e, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishna Parasuram Srinivasan, Alex Tamkin,\nRohan Taori, Armin W. Thomas, Florian Tramèr,\n14167\nRose E. Wang, William Wang, Bohan Wu, Jiajun\nWu, Yuhuai Wu, Sang Michael Xie, Michihiro Ya-\nsunaga, Jiaxuan You, Matei A. Zaharia, Michael\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,\nLucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021.\nOn the opportunities and risks of foundation models.\nArXiv.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling Language\nModeling with Pathways.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models beyond\na Fixed-Length Context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAllyson Ettinger. 2020. What BERT Is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics for\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nIra Fischler, Paul A. Bloom, Donald G. Childers, A. An-\ntonio Arroyo, and Nathan W. Perry. 1984. Brain po-\ntentials during sentence verification: Late negativity\nand long-term memory strength. Neuropsychologia,\n22(5):559–568.\nStefan L. Frank, Leun J. Otten, Giulia Galli, and\nGabriella Vigliocco. 2015. The ERP response to\nthe amount of information conveyed by words in\nsentences. Brain and Language, 140:1–11.\nStefan Frisch and Matthias Schlesewsky. 2005. The\nresolution of case conflicts from a neurophysiological\nperspective. Cognitive Brain Research, 25(2):484–\n498.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training Compute-Optimal\nLarge Language Models.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2023. Can\nLarge Language Models Truly Understand Prompts?\nA Case Study with Negated Prompts. In Proceedings\nof The 1st Transfer Learning for Natural Language\nProcessing Workshop, pages 52–62. PMLR.\nSalud María Jiménez-Zafra, Roser Morante, María\nTeresa Martín-Valdivia, and L. Alfonso Ureña-López.\n2020. Corpora Annotated with Negation: An\nOverview. Computational Linguistics, 46(1):1–52.\nAikaterini-Lida Kalouli, Rita Sevastjanova, Christin\nBeck, and Maribel Romero. 2022. Negation, Co-\nordination, and Quantifiers in Contextualized Lan-\nguage Models. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 3074–3085, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nMisprimed Probes for Pretrained Language Models:\nBirds Can Talk, But Cannot Fly. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7811–7818, Online.\nAssociation for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2021. Few-shot Learning with\nMultilingual Language Models. arXiv:2112.10668\n[cs].\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\n14168\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022a. The inverse scal-\ning prize.\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022b. Inverse scaling\nprize: First round winners.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer Sentinel Mixture Mod-\nels. In International Conference on Learning Repre-\nsentations.\nDanny Merkx and Stefan L. Frank. 2021. Human Sen-\ntence Processing: Recurrence or Attention? In Pro-\nceedings of the Workshop on Cognitive Modeling\nand Computational Linguistics, pages 12–22, Online.\nAssociation for Computational Linguistics.\nJames A. Michaelov, Megan D. Bardolph, Seana Coul-\nson, and Benjamin K. Bergen. 2021. Different kinds\nof cognitive plausibility: Why are transformers bet-\nter than RNNs at predicting N400 amplitude? In\nProceedings of the 43rd Annual Meeting of the Cog-\nnitive Science Society, pages 300–306, University of\nVienna, Vienna, Austria (Hybrid).\nJames A. Michaelov and Benjamin K. Bergen. 2020.\nHow well does surprisal explain N400 amplitude\nunder different experimental conditions? In Pro-\nceedings of the 24th Conference on Computational\nNatural Language Learning, pages 652–663, Online.\nAssociation for Computational Linguistics.\nJames A. Michaelov, Seana Coulson, and Benjamin K.\nBergen. 2022. So Cloze yet so Far: N400 Amplitude\nis Better Predicted by Distributional Information than\nHuman Predictability Judgements. IEEE Transac-\ntions on Cognitive and Developmental Systems.\nMante S. Nieuwland, Dale J. Barr, Federica Bartolozzi,\nSimon Busch-Moreno, Emily Darley, David I. Don-\naldson, Heather J. Ferguson, Xiao Fu, Evelien Hey-\nselaar, Falk Huettig, E. Matthew Husband, Aine Ito,\nNina Kazanina, Vita Kogan, Zdenko Kohút, Eugenia\nKulakova, Diane Mézière, Stephen Politzer-Ahles,\nGuillaume Rousselet, Shirley-Ann Rueschemeyer,\nKatrien Segaert, Jyrki Tuomainen, and Sarah V on\nGrebmer Zu Wolfsthurn. 2020. Dissociable ef-\nfects of prediction and integration during language\ncomprehension: Evidence from a large-scale study\nusing brain potentials. Philosophical Transac-\ntions of the Royal Society B: Biological Sciences,\n375(1791):20180522.\nOpenAI. 2023. Model index for researchers.\nEthan Perez, Ian McKenzie, and Sam Bowman. 2022.\nAnnouncing the Inverse Scaling Prize ($250k Prize\nPool).\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nSandro Pezzelle, Shane Steinert-Threlkeld, Raffaella\nBernardi, and Jakub Szymanik. 2018. Some of Them\nCan be Guessed! Exploring the Effect of Linguistic\nContext in Predicting Quantifiers. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 114–119, Melbourne, Australia. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage Models are Unsupervised Multitask Learners.\npage 24.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scal-\ning Language Models: Methods, Analysis & Insights\nfrom Training Gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research,\n21(140):1–67.\nDeborah Raji, Emily Denton, Emily M. Bender, Alex\nHanna, and Amandalynne Paullada. 2021. AI and\nthe Everything in the Whole Wide World Benchmark.\n14169\nProceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks, 1.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ek-\nmekci, Bill Yuchen Lin, Blake Howald, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick Ar-\ngueta, César Ferri Ramírez, Chandan Singh, Charles\nRathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu,\nChris Callison-Burch, Chris Waites, Christian V oigt,\nChristopher D. Manning, Christopher Potts, Cindy\nRamirez, Clara E. Rivera, Clemencia Siro, Colin Raf-\nfel, Courtney Ashcraft, Cristina Garbacea, Damien\nSileo, Dan Garrette, Dan Hendrycks, Dan Kilman,\nDan Roth, Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek Tam,\nDieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dim-\nitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Ko-\nco´n, Jana Thompson, Jared Kaplan, Jarema Radom,\nJascha Sohl-Dickstein, Jason Phang, Jason Wei, Ja-\nson Yosinski, Jekaterina Novikova, Jelle Bosscher,\nJennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse En-\ngel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jil-\nlian Tang, Joan Waweru, John Burden, John Miller,\nJohn U. Balis, Jonathan Berant, Jörg Frohberg, Jos\nRozen, Jose Hernandez-Orallo, Joseph Boudeman,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸ Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mo-\nhit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh\nGheini, Mukund Varma T, Nanyun Peng, Nathan\nChi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas\nCameron, Nicholas Roberts, Nick Doiron, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Ni-\ntish Shirish Keskar, Niveditha S. Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar\nAgha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale\nFung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter Chang,\nPeter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin\nBanjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel\nHabacker, Ramón Risco Delgado, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Rohan\nSikand, Roman Novak, Roman Sitelew, Ronan Le-\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Rus-\nlan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Sto-\nvall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M.\nMohammad, Sajant Anand, Sam Dillavou, Sam\nShleifer, Sam Wiseman, Samuel Gruetter, Samuel R.\nBowman, Samuel S. Schoenholz, Sanghyun Han,\nSanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian,\nSayan Ghosh, Sean Casey, Sebastian Bischoff, Sebas-\ntian Gehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\n14170\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Timothy Telleen-Lawton,\nTitus Tunduny, Tobias Gerstenberg, Trenton Chang,\nTrishala Neeraj, Tushar Khot, Tyler Shultz, Uri Sha-\nham, Vedant Misra, Vera Demberg, Victoria Nyamai,\nVikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fe-\ndus, William Saunders, William Zhang, Wout V ossen,\nXiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu,\nXudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz,\nYangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov,\nYu Hou, Yufang Hou, Yuntao Bai, Zachary Seid,\nZhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui\nWang, and Ziyi Wu. 2022. Beyond the Imitation\nGame: Quantifying and extrapolating the capabilities\nof language models.\nJakub M. Szewczyk and Kara D. Federmeier. 2022.\nContext-based facilitation of semantic access fol-\nlows both logarithmic and linear functions of stimu-\nlus probability. Journal of Memory and Language,\n123:104311.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-On What Language\nModel Pre-training Captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nThomas P. Urbach and Marta Kutas. 2010. Quantifiers\nmore or less quantify on-line: ERP evidence for par-\ntial incremental interpretation. Journal of Memory\nand Language, 63(2):158–179.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A\n6 billion parameter autoregressive language model.\nJason Wei, Yi Tay, and Quoc V . Le. 2022. Inverse\nscaling can become U-shaped.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open Pre-\ntrained Transformer Language Models.\n14171\nA Scores\nThe performance of each model is presented in\nTable 1.\nCritical word Critical word + period\nAccuracy Sens. Accuracy Sens.\nModel most few most few\nGPT-2 117M (gpt2) 0.850 0.142 0.013 0.846 0.158 0.021\nGPT-2 345M (gpt2-medium) 0.908 0.108 0.025 0.887 0.121 0.025\nGPT-2 762M (gpt2-large) 0.921 0.088 0.021 0.917 0.108 0.029\nGPT-2 1542M (gpt2-xl) 0.942 0.088 0.038 0.917 0.092 0.033\nGPT-3 2.7B (ada) 0.917 0.092 0.017 0.917 0.1 0.021\nGPT-3 6.7B (babbage) 0.942 0.083 0.029 0.917 0.104 0.038\nGPT-3 13B (curie) 0.954 0.042 0.008 0.954 0.058 0.021\nGPT-3 175B (davinci) 0.975 0.038 0.025 0.958 0.067 0.038\nInstructGPT 2.7B (text-ada-001) 0.829 0.179 0.050 0.775 0.242 0.075\nInstructGPT 6.7B (text-babbage-001) 0.950 0.088 0.042 0.908 0.121 0.042\nInstructGPT 13B (text-curie-001) 0.967 0.042 0.017 0.958 0.083 0.054\nInstructGPT 175B (text-davinci-001) 0.975 0.042 0.021 0.963 0.112 0.075\nGPT-Neo 125M (EleutherAI/gpt-neo-125m) 0.829 0.179 0.029 0.821 0.183 0.033\nGPT-Neo 1.3B (EleutherAI/gpt-neo-1.3B) 0.933 0.079 0.033 0.921 0.088 0.029\nGPT-Neo 2.7B (EleutherAI/gpt-neo-2.7B) 0.950 0.067 0.025 0.942 0.088 0.042\nGPT-J 6B (EleutherAI/gpt-j-6b) 0.963 0.062 0.029 0.954 0.079 0.038\nOPT 125M (facebook/opt-125m) 0.867 0.129 0.021 0.854 0.133 0.013\nOPT 350M (facebook/opt-350m) 0.883 0.133 0.025 0.875 0.142 0.033\nOPT 1.3B (facebook/opt-1.3b) 0.925 0.075 0.021 0.921 0.092 0.025\nOPT 2.7B (facebook/opt-2.7b) 0.950 0.046 0.004 0.933 0.058 0.013\nOPT 6.7B (facebook/opt-6.7b) 0.963 0.050 0.017 0.946 0.075 0.029\nOPT 13B (facebook/opt-13b) 0.967 0.033 0 0.954 0.058 0.017\nTable 1: Accuracy and sensitivity scores for all models.\nB Quantifiers\nTable 2 lists all quantifiers used and the proportion\nof sentences in WikiText-103 that contain them.\nMost-type Few-type\nQuantifier Frequency (sentences) Quantifier Frequency (sentences)\nmost 0.025177 few 0.005870\nalmost all 0.000305 almost no 0.000098\npractically all 0.000009 practically no 0.000008\na large number of 0.000300 a small number of 0.000131\nnearly all 0.000170 rather few 0.000001\nlots of 0.000153 hardly any 0.000017\na lot of 0.000745 a very few 0.000010\nmany 0.015874 few 0.005870\noften 0.005766 rarely 0.000610\nTotal 0.046809 0.006717\nTable 2: In each sentence frame, most and few-type quantifiers were matched based on their meanings as length\nin number of words (Urbach and Kutas, 2010). Matched quantifiers are shown beside each other. As can be seen,\nfew is matched to both most and many. The frequency of each quantifier is given in terms of the proportion of\nsentences in WikiText-103 (Merity et al., 2017) that contain it. The total frequencies are the number of sentences in\nWikiText-103 that contain at least one of either the few-type or most-type quantifiers; not the sum of the individual\nquantifier frequencies.\n14172\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, 1 (Introduction)\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n1 (Introduction), 2 (Method), 3 (Results)\n□\u0013 B1. Did you cite the creators of artifacts you used?\n1 (Introduction), 2 (Experiment 1)\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSupplementary Materials (OSF repository linked in paper)\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSupplementary Materials (OSF repository linked in paper)\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We only used the stimuli constructed by researchers from a previously-published\nstudy (Urbach and Kutas, 2010)\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAbstract, 1 (Introduction), 2 (Experiment 1), 3 (Experiment 2)\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nDiscussed in section 2.1 (Materials)\nC □\u0013 Did you run computational experiments?\n2 (Experiment 1), 3 (Experiment 2)\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nParameters: Appendix; Computational budget and infrastructure: Ethics Statement\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14173\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n2 (Experiment 1), 3 (Experiment 2)\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n2 (Experiment 1), 3 (Experiment 2)\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAll data, code for running the models, and analyses are provided as supplementary materials in an\nOSF repository (link in paper).\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14174",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7347212433815002
    },
    {
      "name": "Sentence",
      "score": 0.6774904727935791
    },
    {
      "name": "Quantifier (linguistics)",
      "score": 0.6609519720077515
    },
    {
      "name": "Natural language processing",
      "score": 0.5650172233581543
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5263059139251709
    },
    {
      "name": "Language model",
      "score": 0.5119117498397827
    },
    {
      "name": "Natural language",
      "score": 0.502453088760376
    },
    {
      "name": "Focus (optics)",
      "score": 0.470766544342041
    },
    {
      "name": "Type (biology)",
      "score": 0.4568822979927063
    },
    {
      "name": "Sentence processing",
      "score": 0.45418962836265564
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}