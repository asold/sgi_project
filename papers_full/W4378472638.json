{
  "title": "Web-Scale Semantic Product Search with Large Language Models",
  "url": "https://openalex.org/W4378472638",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3109225483",
      "name": "Aashiq Muhamed",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2119970127",
      "name": "Sriram Srinivasan",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2108769517",
      "name": "Choon-Hui Teo",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2131789145",
      "name": "Qingjun Cui",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3167139966",
      "name": "Belinda Zeng",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2077038638",
      "name": "Trishul Chilimbi",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2513861011",
      "name": "S. V. N. Vishwanathan",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4213009331",
    "https://openalex.org/W2136189984",
    "https://openalex.org/W2964369530",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W3094444847",
    "https://openalex.org/W3036320503",
    "https://openalex.org/W3166125679",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3038572442",
    "https://openalex.org/W3172750682",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3098468692"
  ],
  "abstract": "Abstract Dense embedding-based semantic matching is widely used in e-commerce product search to address the shortcomings of lexical matching such as sensitivity to spelling variants. The recent advances in BERT-like language model encoders, have however, not found their way to realtime search due to the strict inference latency requirement imposed on e-commerce websites. While bi-encoder BERT architectures enable fast approximate nearest neighbor search, training them effectively on query-product data remains a challenge due to training instabilities and the persistent generalization gap with cross-encoders. In this work, we propose a four-stage training procedure to leverage large BERT-like models for product search while preserving low inference latency. We introduce query-product interaction pre-finetuning to effectively pretrain BERT bi-encoders for matching and improve generalization. Through offline experiments on an e-commerce product dataset, we show that a distilled small BERT-based model (75M params) trained using our approach improves the search relevance metric by up to 23% over a baseline DSSM-based model with similar inference latency. The small model only suffers a 3% drop in relevance metric compared to the 20x larger teacher. We also show using online A/B tests at scale, that our approach improves over the production model in exact and substitute products retrieved.",
  "full_text": "Web-Scale Semantic Product Search\nwith Large Language Models\nAashiq Muhamed1(B), Sriram Srinivasan1, Choon-Hui Teo1, Qingjun Cui1,\nBelinda Zeng2, Trishul Chilimbi2, and S. V. N. Vishwanathan1\n1 Amazon, Palo Alto, CA, USA\n{muhaaash,srirs,choonhui,qingjunc,vishy}@amazon.com\n2 Amazon, Seattle, WA, USA\n{zengb,trishulc}@amazon.com\nAbstract. Dense embedding-based semantic matching is widely used\nin e-commerce product search to address the shortcomings of lexical\nmatching such as sensitivity to spelling variants. The recent advances\nin BERT-like language model encoders, have however, not found their\nway to realtime search due to the strict inference latency requirement\nimposed on e-commerce websites. While bi-encoder BERT architectures\nenable fast approximate nearest neighbor search, training them eﬀec-\ntively on query-product data remains a challenge due to training insta-\nbilities and the persistent generalization gap with cross-encoders. In this\nwork, we propose a four-stage training procedure to leverage large BERT-\nlike models for product search while preserving low inference latency. We\nintroduce query-product interaction pre-ﬁnetuning to eﬀectively pretrain\nBERT bi-encoders for matching and improve generalization. Through\noﬄine experiments on an e-commerce product dataset, we show that a\ndistilled small BERT-based model (75M params) trained using our app-\nroach improves the search relevance metric by up to 23% over a baseline\nDSSM-based model with similar inference latency. The small model only\nsuﬀers a 3% drop in relevance metric compared to the 20x larger teacher.\nWe also show using online A/B tests at scale, that our approach improves\nover the production model in exact and substitute products retrieved.\nKeywords: Matching\n· Retrieval · Search · Pretrained Language\nModels\n1 Introduction\nAn e-commerce product search engine typically serves queries in two stages—\nmatching and ranking, for eﬃciency and latency reasons. In the matching stage,\na query is processed and matched against hundreds of millions of products to\nretrieve thousands of products that are relevant to the query. In the subsequent\nranking stage, the retrieved products are scored against one or more objectives\nA. Muhamed and S. Srinivasan—Equal contribution.\nc⃝ The Author(s) 2023\nH. Kashima et al. (Eds.): PAKDD 2023, LNAI 13937, pp. 73–85, 2023.\nhttps://doi.org/10.1007/978-3-031-33380-4\n_6\n74 A. Muhamed et al.\nand then sorted to increase the likelihood of satisfying the customer query in\nthe top positions. Matching is therefore a critical ﬁrst step towards a delightful\ncustomer experience in terms of search latency and relevance, and the focus of\nthis paper. Lexical matching using an inverted index [ 1] has been the industry\nstandard approach for e-commerce retrieval applications. This type of matching\nretrieves products that have one or more query keywords appear in their textual\nattributes such as title and description. Lexical matching is favorable because\nof its simplicity, explainability, low latency, and ability to scale to catalogs with\nbillions of products. Despite the advantages, lexical matching has several short-\ncomings such as sensitivity to spelling variants (e.g. “grey” vs “gray”) or mistakes\n(e.g. “sheos” instead of “shoes”), proneness to vocabulary mismatch (e.g. hyper-\nnyms, synonyms), and lack of semantic understanding (e.g. “latex free exami-\nnation gloves” does not match the intent of “latex examination gloves”). These\nissues are largely caused by the underlying term-based distributional represen-\ntation for query and product that fails to capture the ﬁne-grained relationship\nbetween terms. Researchers and practitioners typically resort toquery expansion\ntechniques to address these issues.\nDense embedding based semantic matching [2] has been shown to signiﬁcantly\nalleviate the shortcomings of lexical matching due to its distributed represen-\ntation that admits granular proximity between the terms of a query-product\npair in low dimensional vector space [3]. To fulﬁll the low latency requirement,\nthese semantic matching models are predominantly shallow and use a bi-encoder\narchitecture. Bi-encoders have separate encoders for generating query and prod-\nuct embeddings and use cosine similarity to deﬁne the proximity of queries and\nproducts. Such an architecture allows product embeddings to be indexed oﬄine\nfor fast approximate nearest neighbor (ANN) search [4] with the query embed-\nding generated in realtime. Recently, BERT-based models [ 5] have advanced\nthe state-of-the-art in natural language processing but due to latency consider-\nations, their use in online e-commerce information retrieval is largely limited to\nthe bi-encoder architecture [6–8] which does not beneﬁt from the early interac-\ntion between the query and product representations.\nIn this work, we propose a multi-stage training procedure to train a small\nBERT-based matching model for online inference that leverages a large pre-\ntrained BERT-based matching model. A large BERT encoder (750M parameters)\nis ﬁrst pretrained with the masked language modeling (MLM) objective on the\nproduct catalog data (details in Sect. 2.1), we refer to the trained model as ds-\nbert. Next, the ds-bert model is pre-ﬁnetuned using our novel query-product\ninteraction pre-ﬁnetuning (QPI) task (see Sect.2.2), the trained model is referred\nto asqpi-bert. We ﬁnd that interaction pre-ﬁnetuning greatly improves training\nstability of bi-encoders downstream as well as signiﬁcantly improves generaliza-\ntion. qpi-bert is then cloned into a bi-encoder model architecture and ﬁnetuned\nwith query-product purchase signal, we refer to this model as qpi-bert-ft(see\nSect.2.3). Finally, a smaller qpi-bert bi-encoder student model (75M parame-\nters) is distilled from theqpi-bert-ft teacher by matching the cosine similarity\nWeb-Scale Semantic Product Search with Large Language Models 75\nscore on the query-product pairs used for ﬁnetuning (see Sect. 2.4), we refer to\nthis model as small-qpi-bert-dis.\nThrough our oﬄine experiments on a large e-commerce dataset, we show that\nthe small-qpi-bert-dis model (75M) suﬀers only a 3% drop in search relevance\nmetric, compared to the qpi-bert model with 20x its number of parameters\n(1.5B). This small-qpi-bert-dis model improves search relevance by 23%, over\na baseline DSSM-based matching model [2] with similar number of parameters\nand inference latency. Using an online A/B test we also show that the small-\nqpi-bert-dis model outperforms the production model with 2% lift in both\nrelevance and sales metrics.\nOur work is closely related to the literature on semantic matching with deep\nlearning. Some of the initial pre-BERT works include the Deep Semantic Sim-\nilarity Model (DSSM) [ 2] , that constructs vector representations for queries\nand documents using a feedforward network and uses cosine similarity as the\nscoring function. DSSM-based models are widely used for real-time matching at\nweb-scale [9,10]. This was later specialized for online product matching [3]. Post-\nBERT techniques leverage Pretrained Language Models (PLMs), such as BERT\n[5] to construct bi-encoders for matching tasks [8,11,12]. These techniques have\nbroadly been applied to question-answering where the question and answer are\nfrom similar domains and interaction pre-ﬁnetuning is less essential. A recent\nwork [13], proposes a multi-stage semantic matching training pipeline for web\nretrieval. However, unlike our approach, their focus is on deploying an ERNIE\nmodel (220M), while we study how large bi-encoders (1.5B) can be compressed\nto much smaller bi-encoders (70M) at web-scale using interaction pre-ﬁnetuning.\nIn summary, the key contributions of this work are:\n– We propose a multi-stage training procedure to eﬀectively train a small\nBERT-based matching model for online inference from a much larger model\n(750 million to 1.5 billion parameters).\n– We introduce a novel pre-ﬁnetuning task where a span masking and ﬁeld\npermutation equivariant objective is used on joint query-product input text\nto help align the query and product representations. This task helps stabilize\ntraining and improve generalization of bi-encoders.\n– We show using oﬄine and online experiments at scale on an e-commerce\nwebsite, that the proposed approach helps the small BERTsmall-qpi-bert-\ndis model signiﬁcantly outperform both a DSSM-based model (by 23%) in\noﬄine experiments and a production model in an online A/B test.\n2 Methodology\nIn this section we describe our proposed four-stage training paradigm that con-\nsists of 1) domain-speciﬁc pretraining, 2) query-product interaction preﬁnetun-\ning, 3) ﬁnetuning for retrieval, and 4) knowledge distillation to a smaller model.\nIn the ﬁrst three stages, we train a large BERT model for product matching\nand in the ﬁnal stage we distill this knowledge to a smaller model that can be\ndeployed eﬃciently in production (See Fig.1).\n76 A. Muhamed et al.\n2.1 Domain-Speciﬁc Pretraining\nIn the ﬁrst stage of training, we pretrain a large BERT model on a domain\nspeciﬁc dataset for product matching. The language used to describe products\n(catalog ﬁelds) in the e-commerce domain signiﬁcantly diﬀers from the language\nused on the larger web. Product titles and descriptions use a subset of the entire\nvocabulary, are often structured to follow a speciﬁc pattern, and in general have\na diﬀerent distribution from the sources that publicly available language models\nare trained on. Therefore, using an oﬀ-the-shelf pretrained BERT-based model\ndoes not perform well when ﬁnetuned for the product matching task.\nInstead of using an oﬀ-the-shelf pretrained BERT model, we construct a BPE\nvocabulary [14] from the catalog corpus comprising of billions of products in e-\ncommerce domain. We then pretrain the model on text from the catalog, and use\nall of the catalog text ﬁelds such as title and description of products available\nby concatenating them along with their ﬁeld names. Our pretraining objective is\nthe standard masked-language-modeling (MLM) loss [5,15,16] .W er e f e rt ot h e\nmodel trained with this strategy as the ds-bert model (See Fig. 1a).\n(a) Stage 1:\nBERT model is\npretrained to\nproduce ds-bert\n(b) Stage 2:\nds-bert is pre-\nﬁnetuned to\nproduce qpi-bert\n(c) Stage 3: Bi-encoder\nqpi-bert is trained\nwith three-part hinge\nloss to produce\nqpi-bert-ft\n(d) Stage 4:\nqpi-bert-ft is dis-\ntilled to produce\nsmall-qpi-bert-\ndis\nFig. 1. This ﬁgure shows the four stages involved in training an eﬀective deployable\nmodel for semantic matching.\n2.2 Query-Product Interaction Pre-ﬁnetuning\nBi-encoders are preferred over cross-encoder models with full interaction for\nmatching due to their eﬃciency and feasibility at runtime. Bi-encoders are how-\never notoriously diﬃcult to train on query-product pairs due to training instabil-\nities arising from gradient variance between the two inputs. Losing the capability\nto explicitly model the interaction between queries and products also results in\nworse generalization than the cross-encoder.\nIn the second stage of training we propose a novel self-supervised approach\nto incorporate query-product interaction in the large encoder which is critical to\nimproving the performance on the product matching task. We use query-product\nWeb-Scale Semantic Product Search with Large Language Models 77\npaired data to help the encoder learn the relationship between a query and a\nproduct using full cross-attention. To construct such a dataset, we ﬁrst identify\nquery-product pairs that share a relevant semantic relationship, for example,\nall products purchased for a given query can be considered relevant or query-\nproduct pairs can be manually labeled for relevance. In this paper, the dataset\nis constructed such that the query-product pairs are semantically relevant with\na high probability α> 0.8. The pre-ﬁnetuning dataset size (a few million exam-\nples) is much smaller than the pretraining dataset (a billion examples).\nTo perform pre-ﬁnetuning, we perform span MLM on the concatenated query\nand product text with a “[SEP]” token between them. At each iteration, we\nselect spans from either the query text or product text (never both) to mask\ntokens. We sample span length (number of words) from a geometric distribution,\ntill a predetermined percentage of tokens have been masked. The start of the\nspan is uniformly sampled within the query or the product. During training\nwe also observed that permuting the ﬁelds within the query and product, a\nform of ﬁeld permutation equivariant training, also helped the model generalize\nbetter. We refer to the model trained with this strategy as qpi-bertmodel\n(See Fig.1b). Pre-ﬁnetuning with self-supervision on semantically relevant paired\ndataset boosts generalization for matching when a large noisy training set is\navailable. This diﬀers from previous works that use supervision on manually\nlabeled data.\n2.3 Finetuning for Matching\nThe third stage of training is to ﬁnetune the large teacher encoder qpi-bert\nmodel in a bi-encoder setting for matching. We train a bi-encoder teacher as\nopposed to a cross-encoder teacher for retrieval as the extreme ineﬃciency in\ngenerating predictions for evaluation and slow training convergence rate makes\nit impractical to train cross-encoders for web-scale data and large models.\nLet us denote the qpi-bert model as M, query encoder as M\nq , and product\nencoder as Mp, where the weights between query encoder and product encoder\nare shared. In our experiments sharing weights performed comparably to inde-\npendently training them. For any query-product pairQ and P as inputs, we ﬁrst\ngenerate the embedding Q\nemb for query Q using Mq and embedding Pemb for\nproduct P using Mp using their “[CLS]” token representation. A cosine similarity\nscore sQ,P =c o s (Qemb,Pemb) is used to compute relevance between them.\nWe train the bi-encoder using a three-part hinge loss. This loss requires the\nground-truth data (yQ,P ) to be labeled with one of three possible values referred\nto as positive (1), hard negative (0) and random negative ( −1). We use the\npurchased products for a given query as positive and any product uniformly\nsampled from the catalog as random negative. Identifying hard negatives is non-\ntrivial [12,17], and in this work we choose a simple yet eﬀective approach [ 3],\nwhere for a given query, all products that were shown to the user but did not\n78 A. Muhamed et al.\nreceive any interaction is a hard negative. The loss takes the following form:\nlossQ,P (yQ,P ,sQ,P )=\n⎧\n⎪⎪\n⎪\n⎨\n⎪⎪\n⎪\n⎩\nmax(δ\npos − sQ,P ,0), if yQ,P =1 .\nmax(δ−\nhn − sQ,P ,0) if yQ,P =0 .\n+ max(sQ,P − δ+\nhn,0),\nmax(sQ,P − δrn ,0), if yQ,P = −1.\n(1)\nwhere δpos and δ−\nhn are the lower thresholds for the positive and hard negative\ndata scores respectively and δ+\nhn and δrn are the upper thresholds for the hard\nnegative and random negative data scores respectively. We refer to the model\ntrained with this strategy as the qpi-bert-ft model (see Fig. 1c).\n2.4 Distillation and Realtime Inference\nThe ﬁnal stage of our framework is to distill the knowledge of teacherqpi-bert-\nft to a smaller student bi-encoder BERT model (75M to 150M parameters) that\nmeets the online latency constraint. We ﬁrst pretrain and preﬁnetune the small\nmodel similar toqpi-bert to generatesmall-qpi-bert model M. Then we clone\nthe encoder to create a query encoder ˜M\nQ and a product encoder ˜MP . Unlike\nthe large model case, for the small model we observe that sharing parameters\nbetween encoders helps improve performance signiﬁcantly. The query embedding\n˜Q\nemb and product embedding ˜Pemb for the student model are computed by\naveraging all token embeddings in the query Q and product P respectively.\nThe relevance score for a query-product pair is compute using cosine similarity\ni.e., ˜sQ,P = cos( ˜Qemb, ˜Pemb). The model is trained by minimizing the distance\nbetween the scores generated by qpi-bert-ft teacher and the model using the\nmean squared error (MSE) loss function.\nlossQ,P (sQ,P , ˜sQ,P )=( sQ,P − ˜sQ,P )2 (2)\nIn practice we observed that simple score matching using MSE outperformed\nother approaches such as using L2 loss on the embeddings directly, Margin-MSE\n[18] with random negatives, or contrastive losses like SimCLR [ 19] with ran-\ndom negatives. We refer to the model distilled with this strategy as thesmall-\nqpi-bert-ft model (see Fig. 1d). At runtime, for every query entered by the\ncustomer, we compute the query embedding and then retrieve top K products\nusing ANN search [4]. To serve traﬃc in realtime, we cache the product embed-\ndings and compute only the query embedding online. The retrieved products are\nserved directly to customers or mixed with other results and re-ranked before\ndisplaying to the customer.\nWeb-Scale Semantic Product Search with Large Language Models 79\n3 Empirical Evaluation\n3.1 Experimental Setup\nData. We use the following multilingual datasets for diﬀerent stages of training:\nDomain-Speciﬁc Pretraining Data: We use∼1 billion product titles and descrip-\ntions from 14 diﬀerent languages. This data is also used to construct a senten-\ncepiece [20] tokenizer with 256K vocab size.\nInteraction Pre-ﬁnetuning Data: We use∼15M query-product pairs from 12 lan-\nguages and use weak supervision in the form of rules to label them as relevant\nor irrelevant. ∼80% of the pairs are relevant query-product pairs.\nFinetuning for Matching Data: We use ∼330M query-product pairs subsampled\nfrom a live e-commerce service to train the model for matching. We maintain\na positive to hard negative to negative ratio of 1:10:11. The pairs are collected\nfrom multiple countries with at least 4 languages. We use a validation dataset to\ncompute recall that contains 28K queries and 1M products from the subsampled\ncatalog. Human evaluation (Sect.3.1) uses a held-out set of 100 queries.\nModels. We experiment with several model variants, both small and large\nsummarized in Table1. All large models we train are based on ds-bert,w h i c h\nis a multilingual BERT model with 38 layers, 1024 output dimensions and 4096\nhidden dimensions. When the parameters for the query and product encoder are\nnot shared, the model has twice the parameters of the encoder. The small models\nwe train are multilingual BERT models with 2 layers, 256 output dimensions,\nand 1024 hidden dimensions. In addition, we use dssm and xlmroberta as\nbaselines. • xlmroberta: Publicly available XLMRoberta [21] model which is\nﬁnetuned for matching as described in Sect.2.3. • dssm: Bi-encoder model with a\nshared embedding layer (output dimension of 256) followed by batch norm and\naveraged token embedding to represent the query and product [ 3]. To ensure\neﬀective use of vocabulary for DSSM, we create a diﬀerent sentencepiece model\nwith 300k tokens using the matching training data.\nMetrics. R@100: This is the average purchase recall computed on the validation\ndata for the top 100 products retrieved.\nRelevance Metrics: To understand the true improvement in the quality of\nmatches retrieved by the model, we use Toloka (toloka.yandex.com) to label\nthe results produced by our models. For every query we retrieve 100 results\nand ask the annotators to label them as exact match, substitute, or other. We\nreport the average percentage of exact (E@100). substitute (S@100), and other\n(O@100). We use E@100 + S@100 (E+S) to measure semantic improvement in\nthe model.\n80 A. Muhamed et al.\nTable 1. Bi-encoder model variants. Diﬀerences are number of parameters (Params),\nembedding dimensionality (ED), embedding type (ET), domain-speciﬁc pretraining\n(DS PT), QPI preﬁnetuning (QPI PFT), whether encoders share parameters (Shared),\nwhether model is distilled from qpi-bert-ft (Dis).\nModels Params ED ET DS PT QPI PFT Shared Dist\nLarge Models\nxlmroberta 1.1B 1024 CLS N N N N\nds-bert 1.5B 1024 CLS Y N N N\nqpi-bert-ft 1.5B 1024 CLS Y Y N N\nqpi-bert-ft* 1.5B 1024 CLS Y Ya N N\nqpi-bert-ft-sh 750M 1024 AVG Y Y Y N\nSmaller Models\nsmall-qpi-bert-ft 150M 256 CLS Y Y N N\nsmall-qpi-bert-ft-avg 150M 256 AVG Y Y N N\nsmall-qpi-bert-ft-sh 75M 256 CLS Y Y Y N\nsmall-qpi-bert-ft-sh-avg 75M 256 AVG Y Y Y N\nsmall-qpi-bert-dis 75M 256 AVG Y Y Y Y\ndssm 75M 256 AVG N N Y N\na Classiﬁcation objective instead of span masking objective on pre-ﬁnetuning data.\nTraining. We use Deepspeed (deepspeed.ai) and PyTorch for training models on\nAWS P3DN instances. We used LANS optimizer [22] with learning rate between\n1e−4 and 1e−6 based on the model and for all models we use a batch size of\n8192. During pre-ﬁnetuning, we use validation MLM accuracy to perform early\nstopping and for ﬁnetuning we use validation recall for stopping. When using\nthe three-part hinge-loss in Eq.1, δ\npos =0 .9, δ+\nhn =0 .55 and δrn = δ−\nhn =0 .2.\n3.2 Oﬄine and Online Results\nDoes our Training Strategy Help Improve Semantic Matching Perfor-\nmance Oﬄine?For large models, we compareqpi-bert-ft with xlmroberta,\nds-bert,a n d qpi-bert-ft*, and for small models, we compare small-qpi-\nbert-ft small-qpi-bert-ft-sh with dssm (Table2). a) qpi-bert outperforms\nother approaches both in R@100 and E+S. Among large models, the perfor-\nmance of ds-bert is better than xlmroberta and qpi-bert-ft* is better\nthan ds-bert. This clearly indicates progressive improvement with the diﬀerent\nstages in our approach. b) We observe is that dssm outperforms xlmroberta\nin all metrics indicating a vocabulary and domain mismatch between the cata-\nlog data and web data. Domain-speciﬁc pretraining is essential to performance\nwhen training the large models. c) We see that qpi-bert-ft signiﬁcantly out-\nperforms qpi-bert-ft* in all metrics, validating the importance of interaction\npre-ﬁnetuning over mere supervision alone for matching. d) For small models,\nWeb-Scale Semantic Product Search with Large Language Models 81\nwe observe that the performance ofsmall-qpi-bert-ft is very similar todssm,\nwith small-qpi-bert-ft showing ∼45% relative lift in S@100 but, ∼8% rela-\ntive drop in E@100, ∼4% relative lift in E+S, and∼1% relative drop in R@100.\nWhen sharing parameters between the query and product encoder, and averaging\nembeddings, small-qpi-bert-ft-sh-avg outperforms dssm by ∼38% relative\nlift in S@100, ∼2% relative lift in E@100, ∼10% relative lift in E+S, and ∼2%\nrelative lift in R@100. The results indicate that our strategy helps improve the\nperformance overall and the improvements are higher for larger models ( ∼23%\nrelative lift in E+S over dssm). This reinforces our proposed approach: train\na large model and distill the knowledge to a smaller model, instead of directly\ntraining a smaller model.\nCan Distillation Preserve Large Model Performance? Given the large\nimprovement in matching metrics for large models, we would ideally like to retain\nthis improvement in smaller models using distillation. We compare small-qpi-\nbert-dis with qpi-bert-ft (Table2)a n do b s e r v ea∼3% relative drop in E+S\nand R@100. This shows that while there is small gap, it is possible to transfer\nmost of the information from a 1.5B parameter large qpi-bert-ft model to a\n20x smaller small-qpi-bert-dis model (75M parameter) using our approach.\nDoes Sharing Parameters in the Bi-encoder have an Impact on\nRetrieval Task Performance?To understand the eﬀect of sharing parameters\nbetween query and product encoders in the bi-encoder setting, we compareqpi-\nbert-ft-sh with qpi-bert-ft among the large models and small-qpi-bert-\nft-shwith small-qpi-bert-ft,a n dsmall-qpi-bert-ft-sh-avg, with small-\nqpi-bert-ft-avgamong the small models (Table 2). We observe that sharing\nencoders has almost no impact on the performance of large models and the\nTable 2. Oﬄine metrics of models on a multi-lingual e-commerce dataset\nModels R@100 E@100 S@100 O@100 E+S\nLarge Models\nxlmroberta 68.43 29.52 17.46 53.02 46.98\nds-bert 73.98 43.17 17.55 39.28 60.72\nqpi-bert-ft 82.2 50.36 20.5 29.14 70.86\nqpi-bert-ft* 75.6 48.35 19.66 31.99 68.01\nqpi-bert-ft-sh 83.35 51.08 19.81 29.11 70.89\nSmaller Models\nsmall-qpi-bert-ft 77.06 40.28 18.16 41.56 58.44\nsmall-qpi-bert-ft-avg 50.84 33.63 13 53.37 46.63\nsmall-qpi-bert-ft-sh 79.3 43.98 18.52 37.5 62.5\nsmall-qpi-bert-ft-sh-avg 80.17 44.45 17.33 38.22 61.78\nsmall-qpi-bert-dis 80 48.04 20.78 31.18 68.82\ndssm 78.1 43.56 12.49 43.95 56.05\n82 A. Muhamed et al.\nmaximum relative drop in E+S and recall is∼1% with qpi-bert-ft-shwinning\nmarginally. However, in the smaller models we observe that sharing parameters\ngives a large boost in performance with a relative lift of upto ∼32% in the E+S\nmetric and ∼60% in R@100. When the model size is large enough, it is capable\nof learning independent encoders for both inputs. But, when the model is small,\nthe model beneﬁts from sharing parameters.\nHow Does our Approach Improve over a non-BERT-Based Model?\nTo visualize the diﬀerence in matching quality between our BERT-based model\nand DSSM, we look at results for two queries, with DSSM retrieving more rel-\nevant products on one query and vice-versa on the other (Fig. 2). We observe\nthat for query “sailor ink” qpi-bert-ft performs better as all results are rele-\nvant products. For this query, dssm behaves like a lexical matcher and fetches\nresults for both“sailor” and “ink” . For query “omron sale bp monitor machine”,\ndssm retrieves all relevant matches.qpi-bert-ft however, retrieves an irrelevant\nproduct (a ﬁtness watch). While irrelevant, it still falls into the product type of\n“personal health” implying an error in semantic generalization. The signiﬁcantly\nhigher increase in S@100 compared to E@100 indicates that qpi-bert-ft is a\nbetter semantic model as the representations must incorporate high-level con-\ncepts to match substitutes, that token-level exact matches cannot achieve.\nWhat is the Latency Improvement of the Smaller BERT Model Com-\npared to the Large Model? We have seen earlier that the large model can\nbe eﬀectively compressed to a 20x smaller model that incurs much lower infer-\nence latency. We compare the inference latencies of our models while generating\nquery embeddings which is representative of realtime latency as the product\nembeddings are generated oﬄine and indexed for ANN. We ignore the ANN\nlatency as modern ANN search can be computed eﬀectively in realtime (∼1m s )\n(a) Query: ”sailor\nink”; Method:\ndssm.\n(b) Query: ”sailor\nink”;M e t h o d :qpi-\nbert-ft.\n(c) Query: ”omron\nsale bp moni-\ntor machine” ;\nMethod: dssm.\n(d) Query: ”om-\nron sale bp\nmonitor ma-\nchine”; Method:\nqpi-bert-ft.\nFig. 2. Top 6 results obtained by dssm and qpi-bert-ft for queries “sailor ink” and\n“omron sale bp monitor machine”\nWeb-Scale Semantic Product Search with Large Language Models 83\n[4]. Figure 3 shows the time it takes to compute query embedding (inference)\nfor diﬀerent query lengths (query length computed as number of tokens after\ntokenization) on an r5.4xlarge AWS instance. As expected,dssm has the lowest\ninference time and qpi-bert has the largest. Both small-qpi-bert and dssm\nhave embedding generation time of under 1ms upto 32 tokens making it feasi-\nble to serve realtime traﬃc.small-qpi-bert reduces the latency time by∼60×\ncompared to qpi-bert with a relevance metric performance drop of only ∼3%.\nFig. 3. Inference time for qpi-bert, small-qpi-bert,a n ddssm on r5.4xlarge\nTable 3. A/B test results for small-qpi-bert-dis rel. to production system.\nPS Units E@16 S@16 E+S@16 SR Latency P99\n+2.07% +1.47% − 1.19% +3.37% +2.18% − 16.9% +4 ms\nHow Well Does the Approach Perform Online? To measure the impact\nof our approach online, we experiment with small-qpi-bert-dis in a multi-\nlingual large e-commerce service. The service augments matching results from\nseveral sources like lexical matchers, semantic matchers, upstream machine learn-\ning models, and advertised products. We replace only the production semantic\nmatcher with our small-qpi-bert-dis and perform an A/B test. We measure\nboth customer engagement metrics and relevance quality metrics. For customer\nengagement metrics, we look at the change in number of units purchased and the\namount of product sales (PS). For quality metric, we look at the change in user\nevaluated E@16, S@16, E+S@16 and sparse results (SR) which is the percent-\nage of queries with less than 16 products retrieved. We observe (Table 3) that\nour approach signiﬁcantly improves over the production semantic matcher and\nlead to a signiﬁcant drop in SR. The reduction in E@16 and increase in S@16\nsuggests that our approach is learning latent semantic meaning to increase sub-\nstitutes displayed to customers. We also observe that our model does not have\na signiﬁcant impact on latency (∼4 ms) and can be used at runtime.\n84 A. Muhamed et al.\n4 Conclusion\nIn this work we develop a four-stage training paradigm to train an eﬀective BERT\nmodel that can be deployed online to improve product matching. We introduce\na new pre-ﬁnetuning task that incorporates the interaction between queries and\nproducts prior to training for retrieval which we show is critical to improving\nperformance. Using a simple yet eﬀective approach, we distill a large model\nto a smaller model and show through oﬄine and online experiments that our\napproach can signiﬁcantly improve customer experience. As future work, it would\nbe interesting to incorporate other structured data from the e-commerce service\nto enhance representation learning, such as brand and product dimensions, as\nwell as customer interaction data such as reviews.\nAcknowledgement. We would like to thank Priyanka, Mutasem, Huajun, Jaspreet,\nDhivya, Giovanni, Hemant, Anton, Tina, and RJ from Amazon Search.\nReferences\n1. Sch¨utze, H., Manning, C.D., Raghavan, P.: Introduction to Information Retrieval.\nCambridge University Press, Cambridge (2008)\n2. Huang, P.-S., He, X., Gao, J., Deng, L., Acero, A., Heck, L.P.: Learning deep\nstructured semantic models for web search using clickthrough data. In: CIKM\n(2013)\n3. Nigam, P., et al.: Semantic product search. In: SIGKDD (2019)\n4. Malkov Y.A., Yashunin, D.A.: Eﬃcient and robust approximate nearest neighbor\nsearch using hierarchical navigable small world graphs. IEEE Trans. Pattern Anal.\nMach. Intell. 42(4), 824–836 (2018)\n5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: pre-training of deep\nbidirectional transformers for language understanding. In: NACCL (2019)\n6. Reimers, N., Gurevych, I.: Sentence-BERT: sentence embeddings using Siamese\nBERT-networks. In: EMNLP-IJCNLP (2019)\n7. Khattab, O., Zaharia, M.: Colbert: eﬃcient and eﬀective passage search via con-\ntextualized late interaction over BERT. In: SIGIR (2020)\n8. Lu, W., Jiao, J., Zhang, R.: TwinBERT: distilling knowledge to twin-structured\nBERT models for eﬃcient retrieval. ArXiv, vol. abs/2002.06275 (2020)\n9. Huang, J.-T., et al.: Embedding-based retrieval in Facebook search. In: SIGKDD\n(2020)\n10. Li, S., et al.: Embedding-based product retrieval in Taobao search. In: SIGKDD\n(2021)\n11. Karpukhin, V., et al.: Dense passage retrieval for open-domain question answering.\nIn: EMNLP (2020)\n12. Xiong, L., et al.: Approximate nearest neighbor negative contrastive learning for\ndense text retrieval. In: ICLR (2021)\n13. Liu, Y., et al.: Pre-trained language model for web-scale retrieval in baidu search.\nIn: SIGKDD (2021)\nWeb-Scale Semantic Product Search with Large Language Models 85\n14. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with\nsubword units. In :Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), Berlin, Germany, pp. 1715–\n1725, Association for Computational Linguistics, Aug. 2016\n15. Liu, Y., et al.: RoBERTa: a robustly optimized BERT pretraining approach, ArXiv,\nvol. abs/1907.11692 (2019)\n16. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: AlBERT: a\nlite BERT for self-supervised learning of language representations. In: ICLR (2020)\n17. Ji, S., Vishwanathan, S.V.N., Satish, N., Anderson, M.J., Dubey, P.: BlackOut:\nspeeding up recurrent neural network language models with very large vocabularies.\nIn: ICLR (2016)\n18. Hofst¨atter, S., Althammer, S., Schr¨oder, M., Sertkan, M., Hanbury, A.: Improv-\ning eﬃcient neural ranking models with cross-architecture knowledge distillation.\nArXiv, vol. abs/2010.02666 (2020)\n19. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-\ntrastive learning of visual representations. In: ICML (2020)\n20. Kudo, T., Richardson, J.: SentencePiece: a simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In: EMNLP (2018)\n21. Conneau, A.: Unsupervised cross-lingual representation learning at scale. In: ACL\n(2020)\n22. Zheng, S., Lin, H., Zha, S., Li, M.: Accelerated large batch optimization of BERT\npretraining in 54 minutes. ArXiv, vol. abs/2006.13484 (2020)\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the\nchapter’s Creative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8797928094863892
    },
    {
      "name": "Inference",
      "score": 0.6579875349998474
    },
    {
      "name": "Encoder",
      "score": 0.6048651337623596
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5453247427940369
    },
    {
      "name": "Language model",
      "score": 0.4994699954986572
    },
    {
      "name": "Information retrieval",
      "score": 0.48028990626335144
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47942274808883667
    },
    {
      "name": "Embedding",
      "score": 0.4448942542076111
    },
    {
      "name": "Theoretical computer science",
      "score": 0.43256497383117676
    },
    {
      "name": "Metric (unit)",
      "score": 0.41077035665512085
    },
    {
      "name": "Machine learning",
      "score": 0.3924816846847534
    },
    {
      "name": "Natural language processing",
      "score": 0.3672257661819458
    },
    {
      "name": "Data mining",
      "score": 0.35590386390686035
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}