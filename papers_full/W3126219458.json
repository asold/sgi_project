{
  "title": "Teachers‚Äô attitudes towards chatbots in education: a technology acceptance model approach considering the effect of social language, bot proactiveness, and users‚Äô characteristics",
  "url": "https://openalex.org/W3126219458",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5086843578",
      "name": "Raquel Chocarro",
      "affiliations": [
        "Universidad Publica de Navarra"
      ]
    },
    {
      "id": "https://openalex.org/A5015471988",
      "name": "M√≥nica Corti√±as",
      "affiliations": [
        "Universidad Publica de Navarra"
      ]
    },
    {
      "id": "https://openalex.org/A5100601168",
      "name": "Gustavo Marcos‚ÄêMat√°s",
      "affiliations": [
        "Universidad Publica de Navarra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3012220842",
    "https://openalex.org/W2795037203",
    "https://openalex.org/W2894533043",
    "https://openalex.org/W2077870824",
    "https://openalex.org/W2020524408",
    "https://openalex.org/W2911698949",
    "https://openalex.org/W1574489312",
    "https://openalex.org/W2967793662",
    "https://openalex.org/W2961720344",
    "https://openalex.org/W3034267466",
    "https://openalex.org/W2790685724",
    "https://openalex.org/W1968323069",
    "https://openalex.org/W1791587663",
    "https://openalex.org/W2033943395",
    "https://openalex.org/W2051626970",
    "https://openalex.org/W2024291455",
    "https://openalex.org/W2964543620",
    "https://openalex.org/W1922191855",
    "https://openalex.org/W2566111605",
    "https://openalex.org/W2904976895",
    "https://openalex.org/W2891751675",
    "https://openalex.org/W2914188784",
    "https://openalex.org/W2077201361",
    "https://openalex.org/W2001829221",
    "https://openalex.org/W2288415441",
    "https://openalex.org/W2296274542",
    "https://openalex.org/W2143403090",
    "https://openalex.org/W1487801033",
    "https://openalex.org/W2973537858",
    "https://openalex.org/W3014007670",
    "https://openalex.org/W2020789613",
    "https://openalex.org/W3036081459",
    "https://openalex.org/W1987461999",
    "https://openalex.org/W2122724418",
    "https://openalex.org/W2793624681",
    "https://openalex.org/W1984449688",
    "https://openalex.org/W3009335665",
    "https://openalex.org/W2096294317",
    "https://openalex.org/W2109015436",
    "https://openalex.org/W2121107665",
    "https://openalex.org/W2119192275",
    "https://openalex.org/W1969888080",
    "https://openalex.org/W2100379340",
    "https://openalex.org/W2166982098",
    "https://openalex.org/W2908999732",
    "https://openalex.org/W1980336349",
    "https://openalex.org/W2014624785",
    "https://openalex.org/W2960616218",
    "https://openalex.org/W1840218356",
    "https://openalex.org/W2606724194",
    "https://openalex.org/W2939403596",
    "https://openalex.org/W2790099794",
    "https://openalex.org/W2014443601",
    "https://openalex.org/W2149568515",
    "https://openalex.org/W2594908799"
  ],
  "abstract": "The appearance of Artificial Intelligence implementations, such as text-based virtual assistants (chatbots) in education is relatively new. These implementations can be useful for helping teachers and students to solve both educational questions and routine tasks. This paper examines the factors that explain teachers‚Äô acceptance of chatbots through the dimensions of the Technology Acceptance Model (perceived usefulness and perceived ease of use), its conversational design (use of social language and proactiveness), and the teachers‚Äô age and digital skills. The data collection process included a pre-test and an online survey with four different types of chatbots. We analyse 225 responses of primary and secondary education teachers. The results show that the perceived easiness and perceived usefulness leads to greater acceptance of chatbots. As for the chatbots‚Äô features, formal language by a chatbot leads to a higher intention of using them. These results can help in chatbot design and communication decisions, improving the acceptance of the educational community.",
  "full_text": "1 \n \nRaquel Chocarro, M√≥nica Corti√±as & Gustavo Marcos-Mat√°s (2021) Teachers‚Äô \nattitudes towards chatbots in education: a technology acceptance model approach \nconsidering the effect of social language, bot proactiveness, and users‚Äô \ncharacteristics, Educational Studies, DOI: 10.1080/03055698.2020.1850426 \n \nThis is an Accepted Manuscript of an article published by Taylor & Francis in \nEducational Studies on 04 Feb 2021, available online: \nhttps://doi.org/10.1080/03055698.2020.1850426.\n \n \n \n \n \nAbstract \nThe appearance of Artificial Intelligence implementations, such as text-based virtual \nassistants (chatbots) in education is relatively new. These implementations can be useful for \nhelping teachers and students to solve both educational questions and routine tasks. This \npaper examines the factors that explain teachers‚Äô acceptance of chatbots through the \ndimensions of the Technology of Acceptance Model (perceived usefulness and perceived ease \nof use), its conversational design (use of social language and proactiveness), and the teachers‚Äô \nage and digital skills. The data collection process included a pretest and an online survey with \nfour different types of chatbots. We analyze 225 responses of primary and secondary \neducation teachers. The results show that the perceived easiness and perceived usefulness \nleads to greater acceptance of chatbots. As for the chatbots‚Äô features, formal language by a \nchatbot leads to a higher intention of using them. These results can help in chatbot design and \ncommunication decisions, improving the acceptance of the educational community. \nKeywords: chatbot, education, teachers, TAM, proactiveness, social language, digital skills \n \n \n \n \n2 \n \nIntroduction \nWhen people access some universities‚Äô web pages, they find a pop-up saying: ‚ÄúHello! I am \nthe virtual assistant of the University Information Service. I am here to help you‚Äù. For \nexample, this service is called Quizbot at the University of Stanford and Carol at the \nComplutense University of Madrid. Specifically, Quizbot and Carol are chatbot programs that \nintegrate Artificial Intelligence, and they are capable of maintaining a level of conversation \nwith people and save time and effort.  \nThis illustration is an example of how information and communication technologies are \ntransforming society. People no longer interact with each other, buy products or seek \ninformation, learn or study in the same way, and computer-mediated environments have \nbecome pervasive. In this ambiance, chatbots emerge as a software tool to maintain a text \nconversation with users on a particular topic or specific field. \nDue to advances in language processing and artificial intelligence (AI), today‚Äôs chatbots have \nsophisticated natural language processing tools that allow them to understand complex \nconversations and address user requests with depth, compassion, and even humor (Wilson et \nal., 2018). Moreover, chatbots can consistently talk in a friendly way because they do not \nhave bad days and never get frustrated or tired like humans (Gao et al., 2018). \nChatbots or conversational agents help humans in customer service in many fields, from retail \ncommerce, financial advice, healthcare to education and teaching, because they provide \nconvenience and cost-efficiency. However, new challenges have also appeared. In particular, \nchatbots are still perceived as a machine and not human beings, and many individuals are \nreluctant to interact with them (Araujo, 2018; Go & Sundar, 2019). Human beings still prefer \nto have contact with real people, for example, when users know that they are interacting with \na bot instead of a human, they perceive the agent as less knowledgeable and less empathetic \nand behave more abruptly (Luo et al., 2019).  \nIncreasing the affective human-likeness similarity of agents can also improve their level of \nacceptance and use (Catrambone et al., 2002), as they are intended to interact with humans \nmore naturally than in a traditional human-computer interaction (Bernard & Arnold, 2019). \nThe humanity of agents can be suggested through visual attributes, identity and \nconversational attributes that mimic human language (Go & Sundar, 2019). \nSince chatbots are relatively new, there is little research on their general acceptance (Rese et \nal., 2020) and even less on educational chatbots (Chen et al., 2020). Specifically, it is \nimportant to analyze how chatbots are accepted and valued by teachers, key participants in the \neducational process, including the factors that can increase or decrease the probability of \nacceptance of this technology. \nTherefore, our first objective is to analyze teachers‚Äô intention to adopt virtual agents in an \neducational context from the perspective of a general technology adoption model (TAM) \n(Davis, 1989). Secondly, we also study how conversational attributes of the agents, social \nlanguage and proactivity, affect this process. Finally, our third objective is to examine the \nadditional effect of specific user characteristics on using the assistants (digital skills and age).  \nThe creation of these assistants is a complex effort that requires research in various areas such \nas AI, computer animation, interface design, sociology, and psychology, among others. \n3 \n \nIncorporating chatbots into the educational area during the last decade have implied an \nincrease in interest in the characteristics that should be implemented for teaching and \nlearning. Progress in the area can significantly benefit from a better understanding of the \nfactors that guide this technology‚Äôs adoption. We contribute to this area with a novel analysis \nof the determinants of the intention to use virtual text assistants in education, studying general \nperceptions about the assistants, their conversational features, and teachers‚Äô characteristics.  \nIn the section below, we present the conceptual framework along with our hypotheses. \nAfterwards, we describe the research design used in this study. Then, we depict and discuss \nthe results, and our conclusions are summarized.  \nConceptual framework \nChatbots in the educational environment \nThe term chatbot combines two words: ‚Äúchat‚Äù for conversation and ‚Äúbot‚Äù for robot. This type \nof communication technology has received numerous names since its appearance. Luo et al. \n(2019) define chatbots as computer programs that simulate human conversations using natural \nlanguages through voice or text commands and serve as virtual assistants to users. Other \ninterchangeable terms are dialog systems, conversational agents, digital agents, and virtual \nassistants. Here, we use this term for agents operating mainly through text since voice virtual \nassistants generally receive specific names such as Voice Activated Personal Agents or \nVAPAs (Easwara Moorthy & Vu, 2015).  \nEducational chatbots are those designed with a specific educational purpose (Bii, 2013). Their \ngoals can be purely educational or oriented to administrative and supportive tasks. \nAdministrative tasks involving automating the processes and administrative consultations \ninclude calendar management, reminders of assignments and deliveries, and frequently asked \nquestions about admissions, registration, or technical problems with email or the virtual \ncampus. Chatbots with educational intent show high potential as a teaching-learning tool in \ndistance education (Heller et al., 2005) and favor the teaching and learning processes directly, \nacting as tutors that accompany the learning process. Bork (2001) notes how this interactive \ntutoring can make a change in the educational paradigm. These educational tasks include the \nfacilitation and adaptation of content, such as knowing the difference between two concepts \nor practicing a language by chatting with a mobile phone (Fryer et al., 2019; Lu et al., 2006). \nChatbots can create an individualized learning environment with the benefits of having a two-\nway, one-to-one conversation: immediate feedback and unlimited conversation time. \nNowadays, most text-based educational chatbots are implemented in mobile phone messaging \nservices such as Facebook Messenger, WhatsApp, or Skype. Mobile phone messaging \nplatforms are increasingly used as a communication channel and can reach 2.5 billion people \n(Clement, 2019). These platforms bring convenience to chatbots, decreasing the cost of starting \nusing this technology, as there is no need to install a particular app on mobile devices (Baier et \nal., 2018) . They also allow users to learn from anywhere and at any time and get quick \nresponses. Smutny & Schreiberova (2020)  have studied 47 educational chatbots available in \nFacebook Messenger platform based on attributes of the quality of teaching and accessibility, \namong others. However, their results show that chatbots are still in the early stages of becoming \nteaching assistants.  \n4 \n \nTo date, research on the application of virtual assistants and their use in education is scarce, \nalthough the few existing studies show the benefits of chatbots. Goda et al. (2014) found higher \nquality debates when students have had a previous conversation about the topic with an online \nagent. Young Oh et al. (2020)  showed that attitudes towards bullying problems were more \npositive after implementing a chatbot. Gulz (2004)  established that educational chatbots \nprovide increased motivation, greater fluency in the information and communication process, \nand gains in memory, understanding, and problem -solving. Finally, Veletsianos & Miller \n(2008) also concluded that there was substantial evidence of chatbots ‚Äô positive effects as \neducational agents. However, the factors determining the acceptance of thi s technology have \nnot been fully studied yet. \nFactors in the adoption of chatbots for education- The Technology Acceptance Model \nThe Technology Acceptance Model (TAM) (Davis et al., 1989; Moussawi et al., 2020)  is \ndesigned to explain the process of infor mation technology acceptance and has been widely \napplied in different contexts for understanding users‚Äô behavior regarding the acceptance of new \ntechnology. TAM is tailored to information systems contexts, and it is the most recognized \nmodel by far in terms of research on user acceptance behavior (Liu et al., 2009). TAM is highly \ngeneralizable (Park, 2010) , and it allows easy transfer and application to different contexts \n(Venkatesh, 2000). \nTAM explains the users‚Äô behavioral intention with two factors: the perceived usefulness of the \nnew technology and its perceived ease of use. Perceived usefulness refers to the degree to which \npeople believe that using a particular system would enhance their job performance (Davis et \nal., 1989). Perceived ease of use applies to the degree to which potential users believe that using \na particular system would be free of effort (Davis et al., 1989). Warshaw & Davis (1985) define \nusers‚Äô behavioral intention as ‚Äúthe degree to which a person has formulated conscious plans to \nperform or not perform some specified future behavior‚Äù (p. 214). The TAM‚Äôs main idea is that \nthe user‚Äôs behavioral intention determines the acceptance of the technology by the user, which, \nin turn, is defined by the perceived usefulness and the perceived ease of use (Farahat, 2012) . \nChen et al.  (2020) have successfully applied the TAM model to explore a new chatbot \ndeveloped to learn Chinese vocabulary. \nTherefore, according to TAM, we propose our first two hypotheses: \nH1. Chatbot perceived usefulness is positively related to its usage intention, and \nH2. Chatbot perceived effort is positively related to its usage intention. \nThe TAM model has been extended by adding different factors to fit different contexts better. \nFor example, the Unified Theory of Acceptance and Use of Technology (UTAUT) \n(Venkatesh et al., 2003) includes, as additional determinants of behavioral intention, the \nfacilitating conditions, social influence, and personal characteristics. Chen et al. (2020) added \nperceived enjoyment as a predictor of perceived ease of use, which means that if users enjoy \nusing chatbots, they will probably perceive that they are easier to use, which will affect their \nadoption.  \nIn the context of chatbots, two characteristics can also impact behavioral intentions: the type \nof social bond established with the chatbot and its level of intrusiveness. First, concerning \nsocial bonds, people‚Äôs preference for communicating with each other results in rejecting many \n5 \n \nnew automatization technologies such as chatbots or shelf service technologies (Collier & \nKimes, 2013; Lee & Lyu, 2016). Castelo et al. (2019) provide a review of human versus \nalgorithmic advice preference, which supports greater preference of human advice for varied \ntopics ranging from medical recommendations to the stock market. However, when a virtual \nagent shows signs of human identity, it is perceived as more anthropomorphic and is \nevaluated more favorably (Sundar et al., 2016; Xu & Lombard, 2017). The increase in \nanthropomorphism drives a positive effect in perceived social presence, an essential factor in \nhuman-computer interactions (Araujo, 2018) and a key predictor of positive attitudes and \nbehaviors (Araujo, 2018; Bente et al., 2008).  \nDesigners can improve customers‚Äô perceptions of the agents‚Äô anthropomorphic level through \ndifferent types of social cues: visual attributes, such as the agent having the shape of a human \nfigure, identity attributes such as personal names, and conversational features using language \nthat mimic the human one (Go & Sundar, 2019).  This paper will pay attention to the last type \nof social cue: conversational attributes because they present more challenges than visual and \nidentity characteristics due to human language complexity. For that purpose, we use the \ncomputer-mediated communication (CMC) framework and the taxonomy of social attributes \nestablished by Feine et al., (2019) for conversational agents. In this framework, \nconversational attributes are considered ‚Äúinvisible cues.‚Äù  \nIn CMC, the perception of dialogue is fundamental in two-way communication (Seltzer & \nMitrook, 2007). The parties will understand each other if they speak a similar language. A \nvital characteristic of this human communication is the style of language. Agents who use \nsocial language get better responses from consumers than when they use a functional \nlanguage (Adam et al., 2020).  \nEmoticons are combinations of ASCII characters used to represent a particular emotional state \nsuch as happiness or sadness (Baron, 2003) and increase the perception of interpersonal \nrelationships (Liebman & Gergle, 2016). Conversational agents can submit these textual or \npictorial depictions of facial expressions to increase social language perception and obtain \nbetter responses (Liebman & Gergle, 2016). Using emoticons adds personality and makes the \nconversation more entertaining, visually appealing, and more enjoyable. Moreover, customers \nthemselves use these emoticons when communicating with virtual agents. (Hill et al., 2015) \nanalyzed communication changes when people communicate with an intelligent agent and \nwith another human, and their results showed no statistically significant differences in the \nmean number of emoticons. \nWe establish our third hypothesis as: \nH3. The chatbots‚Äô use of a social language style, including emoticons is positively related to \nits usage intention. \nAnother important element in the category of invisible social cues is the proactiveness in the \nconversation, that is, who takes the first turn in the discussion (Feine et al., 2019). Proactive \nassistants start the conversation, make themselves known, and do not wait for the user to take \nthe initiative to request information. The effects of proactive assistants have been studied in \nvarious task settings, such as interactive learning, web searches, and educational software. \nProactive help may cause some users to perceive chatbots as intrusive, annoying, distracting, \nand offensive rather than useful (Rickenberg & Reeves, 2000), and therefore, they are \n6 \n \nperceived as less useful. Xiao et al. (2003) provided evidence from their text-editing assistant \nexperiment. They found that although individuals feel that both reactive and proactive \nassistants are valuable, they perceive that the proactive assistant would not improve the result \nand would be intrusive. Therefore, we hypothesize that the reaction of users to an assistant \nmay vary as a function of the assistant‚Äôs initiative. An assistant who makes unsolicited \nsuggestions could cause the user to feel discomfort, intrusion, and distraction and be less \nhelpful compared to an assistant who responds only to user questions, which leads us to \nestablish a fourth hypothesis: \nH4. The proactiveness of the chatbot is negatively related to its usage intention. \nFinally, we include two personal characteristics in our model of technology adoption: digital \nskills and age. The lack of digital skills is one of the teachers‚Äô common reasons for not using \ntechnology and a major barrier to technology integration (Snoeyink & Ertmer, 2001; Williams \net al., 2000). In our case, teachers‚Äô particular backgrounds and experience will make them \nmore or less comfortable interacting with an assistant. Chen & Wang (2018) confirmed that \nuser experience and technical knowledge affect conversational agents‚Äô perceived usability. \nSpecifically, technical knowledge seems to increase perceived usability for inexperienced \nusers, which leads us to establish the fifth hypothesis as follows: \nH5. The level of teachers‚Äô digital skills is positively related to the chatbots‚Äô usage intention. \nThe age of teachers is the last factor influencing the process included in our model of \nadoption. There is considerable evidence of differences by age in the degree of acceptance of \nthe technology. Fern√°ndez-Cruz & Fern√°ndez-D√≠az (2016) found that older teachers have \nmuch lower ICT profiles than younger ones. Younger teachers have more technical skills and \nintegrate them more into their teaching practices (Russell et al., 2000; Su√°rez-Rodr√≠guez et \nal., 2012). Therefore, as the last hypothesis, we propose the following:  \nH6. The teachers‚Äô age is negatively related to the chatbots‚Äô usage intention. \nWe illustrate the research model in Figure 1. \n \nFigure 1. Conceptual Framework and Hypotheses \n \n7 \n \n \nResearch design \nSample \nTo test our hypotheses, we developed a questionnaire distributed during November 2019 in 28 \nprimary and secondary education centers located in the Navarre region (Spain). In five of \nthese centers, we contacted the management team asking for collaboration in the study, and \nthey sent the questionnaire to the teachers through their center‚Äôs email system. We \ncomplemented these data with additional ones collected directly from teachers through \nsnowball sampling. We gathered a total of 225 valid responses. The participants have an \naverage age of 43 years and 15 years working as a teacher. More than half (65%) were \nwomen. \nMaterials: chatbots \nOur study used a 2x2 experiment: with two treatments: \n‚Ä¢ Social vs. functional language: we varied both the language and the use (or not) of \nemoticons. \n‚Ä¢ Proactive vs. reactive language: the chatbot initiates the conversation in the first case \nand the user in the second case. \nThe combination of these two treatments results in four types of chatbots. \nWe implemented the experimental design by creating four simulated conversations with the \nchatbot in a Whatsapp interface, one for each of the four different types of chatbots. We chose \nthe WhatsApp mobile application because it is one of the three most used globally, making it \na familiar interface for the participants. \nEach individual was randomly exposed to only one of these four designs. One example is \nshown in Figure 2, and the four possible screenshots are presented in Appendix 1. After being \n\n8 \n \nexposed to the screenshot, the participants complete a series of statements about using the \nchatbots in the questionnaire. \n  \n9 \n \nFigure 2. Image provided to respondents as part of the conversation with a chatbot. \nScreenshot 4: Example of conversation (social language, human first turn) \n \n \n \n \n \n \nTranscript of screenshot 4 (the text was in Spanish in the original experiment): \n- Human: Edubot What classes do I have today? \n- Chatbot: Hello, Lidia üñêüñê, how are you? Today you have classÔøΩüè´üè´ in 2¬∫A from \n9 to 10 AMü•àü•à, with the group of 3¬∫B at 11 AMü•âü•âand with the group of 1¬∫D at \n1:00 PM ü•áü•á. Have a great day! üòÄüòÄ \n- Human: And what is the main office number? \n- Chatbot: Hello, Lidia üñêüñê. The main office number is 948-874598 ‚òé I hope I \nhave been useful! üòÄüòÄ \n \n \nProcedure \nThe ad hoc questionnaire was hosted by the Qualtrics survey service and accessible from \ndifferent platforms (i.e., cellphone, tablet, or computer). In the questionnaire, respondents \ncould visualize the screenshot of the chatbot conversation and a series of questions. As an \nHuman \nquestions and \nresponses to the \nchatbot are \npresented on \nthe right, in \ngreen boxes\nChatbot on the \nleft-hand side. \nYou will see the \nchatbot logo and \nthe chatbot \nresponses, which \nwill be in white \nboxes \n10 \n \nincentive, we stated that the five collaborator centers would receive a 120‚Ç¨ bonus to buy \nschool supplies if the global center response rate reaches 90%. The data collection, including \nthe conversation display and questionnaire completion, took approximately 15 minutes per \nperson. \nMethod \nThis experiment enabled us to obtain questionnaire responses from each subject. The \nquestionnaire included information regarding TAM items and digital skills and teachers‚Äô age. \nFinally, we included other subjects‚Äô characteristics, such as subjects taught and training cycle, \nand demographics for control purposes. Given our objectives, the metrics employed in this \nstudy about chatbots are as follows. \nFirst, we use as metrics for usage intentions, perceive usefulness and perceive ease of use the \noriginal questionnaire items of the TAM model from Davis et al. (1989), adapted to the \nchatbot context: \n- Assistant usage intention: The respondents were asked to indicate on a 1 to 7 scale their \nlevel of agreement (1-disagree strongly, 7-agree strongly) with one statement:  \n¬∑ ‚ÄúI would like to use virtual assistants in the future in my teaching work.‚Äù  \n- Perceived usefulness: The respondents were asked to indicate on a scale of 1-7 their level of \nagreement (1-disagree strongly, 7-agree strongly) with four statements:  \n¬∑ Utility: ‚ÄúI think that chatbots like this can be useful for my work.‚Äù \n¬∑ Performance: ‚ÄúUsing chatbots like this would improve my performance.‚Äù  \n- Perceived ease of use: The respondents were asked to indicate on a scale of 1-7 their level of \nagreement (1-disagree strongly, 7-agree strongly) with four statements:  \n¬∑ Easiness: ‚ÄúI find it easy to use chatbots like this.‚Äù  \n¬∑ Mental effort: ‚ÄúUsing chatbots like this would not require much mental effort on \nmy part.‚Äù \nAlso, we include the following measures for teachers‚Äô characteristics. \n- Digital skills: To assess the respondents‚Äô skills, we used the standardized scale of Digital \nCompetence Framework for Educators (DigCompEdu) (Caena & Redecker, 2019; \nRedecker, 2017) that was used to evaluate educators in the European Union. As a global \nmeasure, the respondents were asked to indicate on a scale of 1-6 their self-assessed skill \n11 \n \nlevel according to the DigiComp categories: newcomer, explorer, integrator, expert, leader \nor pioneer.  \n- Age: Each teacher had to indicate their age expressed in numerical value from 21 to 70 \nyears. \nResults \nTable 1 shows the descriptive statistics for the variables involved in this study. First, regarding \nthe attitudes toward chatbots, participants have a moderately high usage intention with a mean \nvalue of 4.2 out of 7. The indicators of perceived usefulness and ease of use also have similar \nmean values, with the highest one for perce ived easiness (5.16/7) and the lowest one for the \nperceived impact on performance (3.98/7). Surprisingly, the perceived mental effort of using \nthe chatbot presents a value relatively high (4.58/7) and also has the highest standard deviation \nof these variables, indicating a lower level of agreement between participants in this aspect.  \nThe next two variables show the proportion of participants exposed to each experimental \ncondition, and, as expected, they match the 50% fixed by the experimental design.  \nFinally, the last two rows are participants‚Äô characteristics: age in years and the self -declared \ndigital competence level. On average, the respondents are 43 years old, with a minimum of 23 \nand a maximum of 65. This maximum value is consistent with its theoretical value, the age for \nretirement in Spain. Self -declared digital competence level has a mean value of 3 on a range \nfrom 1 to 6, which means that, on average, teachers considered themselves as an ‚Äúintegrator‚Äù \nregarding their digital skills.  \nTable 1. Descriptive Statistics \nVariable Mean Min. Max. SD N \nUse Intentions 4.24 1 7 1.66 225 \nPU1. Perceive Utility 4.36 1 7 1.50 225 \nPU2. Perceive Impact on Performance 3.98 1 7 1.42 225 \nPEU1. Easiness 5.16 1 7 1.30 225 \nPEU2. Perceive Mental Effort 4.58 1 7 1.62 225 \nBot with Social Language (dummy) 0.51 0 1 0.50 225 \nProactive Bot (dummy) 0.50 0 1 0.50 225 \nAge (years) 43.09 23 65 9.92 225 \nDigital Skills 3.00 1 6 1.04 225 \nPU: Perceive usefulness. PEU: perceived ease of use.  \n  \n12 \n \nWe estimated a regression model (Wooldridge, 2006) with the R software (version 3.6.2). Our \ndependent variable is the chatbot usage intention and we have eight independent variables. The \nmodel results are presented in Table 2. \nTable 2. Regression Model. \nVariable Estimate Std. Error Statistic P value \nIntercept 0.175 0.527 0.333 0.739 \nPU1. Perceive Utility 0.409 0.093 4.413 0.000 \nPU2. Perceive Impact on Performance 0.430 0.095 4.528 0.000 \nPEU1. Easiness 0.206 0.066 3.124 0.002 \nPEU2. Perceive Mental Effort -0.010 0.054 -0.176 0.861 \nBot with Social Language (dummy) -0.320 0.146 -2.193 0.029 \nProactive Bot (dummy) -0.011 0.149 -0.075 0.940 \nAge (years) -0.002 0.007 -0.214 0.831 \nDigital Skills -0.073 0.076 -0.956 0.340 \nR Squared 0.610  \nAdj R squared 0.595  \nÍ≠ì2 39.165  \nP value 0.000  \nDegrees of freedom 8.000  \nPU: Perceive utility. PEU: perceived ease of use \nThe last five rows in the table show the statistics for the goodness of fit of the model. The \npercentage of variance explained by the R2 and adjusted R2 show that the model has good \nexplanatory power, with around 60% of the variance in the usage intentions explained by our \nmodel.  \nConsidering our two indicators of usefulness, both are significant at the 1% level, with a \npositive sign, therefore, confirming our hypothesis 1: the perceived usefulness of chatbots \nincreases the intention to use them. The effect is slightly higher for the perceived chatbots‚Äô \nimpact on performance. Regarding the effect of the perceived ease of use, perceived easiness \nhas, as expected, a significant positive parameter value. However, we do not find evidence of \na significant negative impact of perceived mental effort, although the parameter sign is \nnegative, as expected. We partially confirm hypothesis 2. \n13 \n \nThe next two rows show the effect of the two experimental conditions: social language and \nproactive versus reactive behavior. Contrary to our expectations, using social language with \nemoticons has a significant negative influence on the intention to use assistants. Therefore, we \nreject our hypothesis 3. Furthermore, the proactive conditioned effect has the predicted sign \nbut is not significant, and consequently, we cannot confirm hypothesis 4. Finally, regarding \nthe individual characteristics: digital skills and age, neither one significantly affects using a \nchatbot, and we cannot confirm hypotheses 5 and 6. \nConclusions, discussion and limitations \nIn this study, we empirically test the TAM model of technology acceptance (Davis, 1986) to \nexplain teachers‚Äô usage intention of a chatbot in an educational context, including the effect of \nconversational design decisions and teachers‚Äô characteristics.  \nOur results confirm a positive and substantial impact of the perceived usefulness for using the \nchatbot on teachers‚Äô technology usage intention. Improving the performance and usefulness of \nchatbots is a critical determinant for teachers when considering adopting this technology for \ntheir jobs. \nFurthermore, our results also show that chatbots‚Äô ease of use does relate to higher intention of \nuse. On the contrary, low required mental effort does not have a significant relationship with \nthe intention of use. We think that in an educational environment, mental effort is an \nendogenous constant for teachers. The evidence shows that users often adopt technology for \npleasure (Toubia & Stephen, 2013). Researchers such as Fosso Wamba et al. (2017) noticed \nthat enjoyment and playfulness emerged as constructs responsible for explaining usage \nintention in the Technology Acceptance Model. However, in our context, the chatbot is a tool \nfor more efficient work, and mental effort is a part of the job. Therefore, it could take a \nsecondary role in explaining their usage intention. \nIn hypothesis 3, we posit that there would be greater acceptance of virtual assistants that use \nsocial language versus assistants with a more functional one. However, the results show a \nnegative impact of social language with emoticons. Therefore, the acceptance of chatbots with \nformal language assistant is higher than that for a social one. There are two possible \nalternative explanations for this unexpected result. First, in some cases, technology and \nanthropomorphism can be perceived as a threaten to humanity and generate strongly adverse \nemotional reactions, what has been called the Uncanny Valley theory (Wang et al., 2015). An \nincrease in human similarity through a more social language could reduce the intention of \nadopting the chatbot. The second possible explanation may be due to the characteristics of our \npopulation for the study. Teachers may be more likely to use formal language and avoid the \nuse of emoticons, and therefore, in this case, social language can increase the perceived \ndistance with the chatbot. More research will be needed to investigate these two alternative \nexplanations for our results. \nRegarding the second variable of conversational design, the chatbot‚Äôs proactivity, we do not \nfind a significant effect. The effect is very close to zero, although it shows a negative sign. In \nthis case, the participants do not perceive this chatbot as intrusive, maybe because of the \nprofessional nature of the interaction. \nFinally, teachers‚Äô digital skills (H5) and age (H6) are not significant predictors of their usage \nintentions to use chatbots. Regardless of the existing evidence that younger teachers and more \n14 \n \ndigitally skilled ones more actively use IT in their practices, in our model, age and digital \nskills do not seem to influence their intention to use chatbots. S√°nchez-Mena et al. (2019), in \ntheir research on teachers‚Äô intention to use videogames, also found that neither age nor gender \nmoderate teachers‚Äô behavioral intention.  \nGiven the results concerning both the conversational design and the teachers‚Äô characteristics, \nit will be convenient to replicate this research with students as a population to see the \ndifferential impact of the variables under study.  Furthermore, other conversational cues, as \nthe type of content (educational vs administrative) or language style, could be investigated. \nAdditionally, it would be interesting to examine how TAM factors might affect student \nlearning outcomes. Barneche Naya & Hern√°ndez Ib√°√±ez (2015) suggested that perceived \nusefulness and ease of use could influence learning outcomes. Recently, Chen et al. (2020) \nshowed statistically significant improvement in learning achievement with the chatbot. These \nprevious studies further advance the present investigation to analyze the correlations between \nTAM variables and the learning performance of students. \nOur findings suggest that educational chatbot developers or teaching programs seeking to \nencourage the use of chatbots should highlight their utility at improving teachers‚Äô task \nperformance. Teachers can find in chatbots an opportunity to increase efficiency and promote \nthe tool as support when taking charge of certain instructions. Class time could be maximized \nby downloading certain tasks such as questions about homework submissions and information \nrequests in digital assistants such as chatbots. Also, students could learn with chatbots in a \npersonalized way and on-demand when there is no teacher available.  \nReferences \nAdam, M., Wessel, M., & Benlian, A. (2020). AI-based chatbots in customer service and their \neffects on user compliance. Electronic Markets, 1‚Äì19. https://doi.org/10.1007/s12525-\n020-00414-7 \nAraujo, T. (2018). Living up to the chatbot hype: The influence of anthropomorphic design \ncues and communicative agency framing on conversational agent and company \nperceptions. Computers in Human Behavior, 85, 183‚Äì189. \nhttps://doi.org/10.1016/j.chb.2018.03.051 \nBaier, D., Rese, A., & R√∂glinger, M. (2018). Conversational User Interfaces for Online \nShops? A Categorization of Use Cases. ICIS. \nBarneche Naya, V., & Hern√°ndez Ib√°√±ez, L. A. (2015). Evaluating user experience in joint \nactivities between schools and museums in virtual worlds. Universal Access in the \nInformation Society, 14(3), 389‚Äì398. https://doi.org/10.1007/s10209-014-0367-y \nBaron, N. S. (2003). Language of the Internet. The Stanford Handbook for Language \nEngineers, 59‚Äì127. \nBente, G., R√ºggenberg, S., Kr√§mer, N. C., & Eschenburg, F. (2008). Avatar-Mediated \nNetworking: Increasing Social Presence and Interpersonal Trust in Net-Based \n15 \n \nCollaborations. Human Communication Research, 34(2), 287‚Äì318. \nhttps://doi.org/10.1111/j.1468-2958.2008.00322.x \nBernard, D., & Arnold, A. (2019). Cognitive interaction with virtual assistants: From \nphilosophical foundations to illustrative examples in aeronautics. Computers in Industry, \n107, 33‚Äì49. https://doi.org/10.1016/j.compind.2019.01.010 \nBii, P. (2013). Chatbot technology: A possible means of unlocking student potential to learn \nhow to learn. Educational Research, 4(2), 218‚Äì221. \nBork, A. (2001). Tutorial learning for the new century. Journal of Science Education and \nTechnology, 10(1), 57‚Äì71. https://doi.org/10.1023/A:1016620611003 \nCaena, F., & Redecker, C. (2019). Aligning teacher competence frameworks to 21st century \nchallenges: The case for the European Digital Competence Framework for Educators ( \nDigcompedu). European Journal of Education, 54(3), 356‚Äì369. \nhttps://doi.org/10.1111/ejed.12345 \nCastelo, N., Bos, M. W., & Lehmann, D. R. (2019). Task-Dependent Algorithm Aversion. \nJournal of Marketing Research, 56(5), 809‚Äì825. \nhttps://doi.org/10.1177/0022243719851788 \nCatrambone, R., Stasko, J., & Xiao, J. (2002). Anthropomorphic Agents as a User Interface \nParadigm: Experimental Findings and a Framework for Research. In Proceedings of the \nAnnual Meeting of the Cognitive Science Society (Vol. 24, Issue 24). \nChen, H. L., Vicki Widarso, G., & Sutrisno, H. (2020). A ChatBot for Learning Chinese: \nLearning Achievement and Technology Acceptance. Journal of Educational Computing \nResearch, 58(6), 1161‚Äì1189. https://doi.org/10.1177/0735633120929622 \nChen, M. L., & Wang, H. C. (2018). How personal experience and technical knowledge affect \nusing conversational agents. International Conference on Intelligent User Interfaces, \nProceedings IUI, 31, 17‚Äì18. https://doi.org/10.1145/3180308.3180362 \nClement, J. (2019). Global number of mobile messaging users 2018-2022. \nhttps://www.statista.com/statistics/483255/number-of-mobile-messaging-users-\nworldwide/ \nCollier, J. E., & Kimes, S. E. (2013). Only If It Is Convenient. Journal of Service Research, \n16(1), 39‚Äì51. https://doi.org/10.1177/1094670512458454 \nDavis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of \ninformation technology. MIS Quarterly: Management Information Systems, 13(3), 319‚Äì\n339. https://doi.org/10.2307/249008 \nDavis, F. D., Bagozzi, R. P., & Warshaw, P. R. (1989). User acceptance of computer \n16 \n \ntechnology‚ÄØ: a comparison of two theoretical models. Management Science, 35(8), 982‚Äì\n1003. \nEaswara Moorthy, A., & Vu, K. P. L. (2015). Privacy Concerns for Use of Voice Activated \nPersonal Assistant in the Public Space. International Journal of Human-Computer \nInteraction, 31(4), 307‚Äì335. https://doi.org/10.1080/10447318.2014.986642 \nFarahat, T. (2012). Applying the Technology Acceptance Model to Online Learning in the \nEgyptian Universities. Procedia - Social and Behavioral Sciences, 64, 95‚Äì104. \nhttps://doi.org/10.1016/j.sbspro.2012.11.012 \nFeine, J., Gnewuch, U., Morana, S., & Maedche, A. (2019). A Taxonomy of Social Cues for \nConversational Agents. International Journal of Human Computer Studies, \n132(September 2018), 138‚Äì161. https://doi.org/10.1016/j.ijhcs.2019.07.009 \nFern√°ndez-Cruz, F. J., & Fern√°ndez-D√≠az, M. J. (2016). Generation z‚Äôs teachers and their \ndigital skills. Comunicar, 24(46), 97‚Äì105. https://doi.org/10.3916/C46-2016-10 \nFosso Wamba, S., Bhattacharya, M., Trinchera, L., & Ngai, E. W. T. (2017). Role of intrinsic \nand extrinsic factors in user social media acceptance within workspace: Assessing \nunobserved heterogeneity. International Journal of Information Management, 37(2), 1‚Äì\n13. https://doi.org/10.1016/j.ijinfomgt.2016.11.004 \nFryer, L. K., Nakao, K., & Thompson, A. (2019). Chatbot learning partners: Connecting \nlearning experiences, interest and competence. Computers in Human Behavior, 93, 279‚Äì\n289. https://doi.org/10.1016/j.chb.2018.12.023 \nGao, D., Chen, J., & Wang, Y. (2018). Study on Omnichannel service for Time-sensitive and \nPrice-sensitive Demand. 15th International Conference On Service Systems And Service \nManagement (ICSSSM). \nGo, E., & Sundar, S. S. (2019). Humanizing chatbots: The effects of visual, identity and \nconversational cues on humanness perceptions. Computers in Human Behavior, 97(June \n2018), 304‚Äì316. https://doi.org/10.1016/j.chb.2019.01.020 \nGoda, Y., Yamada, M., Matsukawa, H., Hata, K., & Yasunami, S. (2014). Conversation with \na Chatbot before an Online EFL Group Discussion and the Effects on Critical Thinking. \nThe Journal of Information and Systems in Education, 13(1), 1‚Äì7. \nhttps://doi.org/10.12937/ejsise.13.1 \nGulz, A. (2004). Benefits of virtual characters in computer based learning environments: \nClaims and evidence. International Journal of Artificial Intelligence in Education, 14(3, \n4), 313‚Äì334. \nHeller, B., Proctor, M., Mah, D., Jewell, L., & Cheung, B. (2005). Freudbot: An investigation \n17 \n \nof chatbot technology in distance education. EdMedia+ Innovate Learning, 3913‚Äì3918. \nHill, J., Randolph Ford, W., & Farreras, I. G. (2015). Real conversations with artificial \nintelligence: A comparison between human-human online conversations and human-\nchatbot conversations. Computers in Human Behavior, 49, 245‚Äì250. \nhttps://doi.org/10.1016/j.chb.2015.02.026 \nLee, H. J., & Lyu, J. (2016). Personal values as determinants of intentions to use self-service \ntechnology in retailing. Computers in Human Behavior, 60, 322‚Äì332. \nhttps://doi.org/10.1016/j.chb.2016.02.051 \nLiebman, N., & Gergle, D. (2016). It‚Äôs (Not) simply a matter of time: The relationship \nbetween CMC cues and interpersonal affinity. Proceedings of the ACM Conference on \nComputer Supported Cooperative Work, CSCW, 27, 570‚Äì581. \nhttps://doi.org/10.1145/2818048.2819945 \nLiu, S. H., Liao, H. L., & Pratt, J. A. (2009). Impact of media richness and flow on e-learning \ntechnology acceptance. Computers and Education, 52(3), 599‚Äì607. \nhttps://doi.org/10.1016/j.compedu.2008.11.002 \nLu, C. H., Chiou, G. F., Day, M. Y., Ong, C. S., & Hsu, W. L. (2006). Using instant \nmessaging to provide an intelligent learning environment. Lecture Notes in Computer \nScience (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes \nin Bioinformatics), 4053 LNCS, 575‚Äì583. https://doi.org/10.1007/11774303_57 \nLuo, X., Tong, S., Fang, Z., & Qu, Z. (2019). Frontiers: Machines vs. Humans: The Impact of \nArtificial Intelligence Chatbot Disclosure on Customer Purchases. Marketing Science, \nOctober. https://doi.org/10.1287/mksc.2019.1192 \nMoussawi, S., Koufaris, M., & Benbunan-Fich, R. (2020). How perceptions of intelligence \nand anthropomorphism affect adoption of personal intelligent agents. Electronic \nMarkets. https://doi.org/10.1007/s12525-020-00411-w \nPark, N. (2010). Adoption and use of computer-based voice over Internet protocol phone \nservice: Toward an integrated model. Journal of Communication, 60(1), 40‚Äì72. \nhttps://doi.org/10.1111/j.1460-2466.2009.01440.x \nRedecker, C. (2017). European framework for the digital competence of educators: \nDigCompEdu. In Joint Research Centre (JRC) Science for Policy report. \nhttps://doi.org/10.2760/159770 \nRese, A., Ganster, L., & Baier, D. (2020). Chatbots in retailers‚Äô customer communication: \nHow to measure their acceptance? Journal of Retailing and Consumer Services, \n56(June), 102176. https://doi.org/10.1016/j.jretconser.2020.102176 \n18 \n \nRickenberg, R., & Reeves, B. (2000). The effects of animated characters on anxiety, task \nperformance, and evaluations of user interfaces. Conference on Human Factors in \nComputing Systems - Proceedings, 2(1), 49‚Äì56. https://doi.org/10.1145/332040.332406 \nRussell, G., Finger, G., & Russell, N. (2000). Information technology skills of australian \nteachers: Implications for teacher education. Journal of Information Technology for \nTeacher Education, 9(2), 149‚Äì166. https://doi.org/10.1080/14759390000200087 \nS√°nchez-Mena, A., Mart√≠-Parre√±o, J., & Ald√°s-Manzano, J. (2019). Teachers‚Äô intention to use \neducational video games: The moderating role of gender and age. Innovations in \nEducation and Teaching International, 56(3), 318‚Äì329. \nhttps://doi.org/10.1080/14703297.2018.1433547 \nSeltzer, T., & Mitrook, M. A. (2007). The dialogic potential of weblogs in relationship \nbuilding. Public Relations Review, 33(2), 227‚Äì229. \nhttps://doi.org/10.1016/j.pubrev.2007.02.011 \nSmutny, P., & Schreiberova, P. (2020). Chatbots for learning: A review of educational \nchatbots for the Facebook Messenger. Computers and Education, 151, 103862. \nhttps://doi.org/10.1016/j.compedu.2020.103862 \nSnoeyink, R., & Ertmer, P. A. (2001). Thrust into Technology: How Veteran Teachers \nRespond. Journal of Educational Technology Systems, 30(1), 85‚Äì111. \nhttps://doi.org/10.2190/ydl7-xh09-rlj6-mtp1 \nSu√°rez-Rodr√≠guez, J. M., Almerich, G., D√≠az-Garc√≠a, I., & Fern√°ndez-Piqueras, R. (2012). \nICT Competences of teachers. Influence of personal and contextual factors. Universitas \nPsychologica, 11(1), 293‚Äì309. https://doi.org/10.11144/javeriana.upsy11-1.cpif \nSundar, S. S., Bellur, S., Oh, J., Jia, H., & Kim, H.-S. (2016). Theoretical Importance of \nContingency in Human-Computer Interaction. Communication Research, 43(5), 595‚Äì\n625. https://doi.org/10.1177/0093650214534962 \nTerzopoulos, G., & Satratzemi, M. (2019). Voice assistants and artificial intelligence in \neducation. ACM International Conference Proceeding Series. \nhttps://doi.org/10.1145/3351556.3351588 \nToubia, O., & Stephen, A. T. (2013). Intrinsic vs. image-related utility in social media: Why \ndo people contribute content to Twitter? Marketing Science, 32(3), 368‚Äì392. \nhttps://doi.org/10.1287/mksc.2013.0773 \nVeletsianos, G., & Miller, C. (2008). Conversing with pedagogical agents: A \nphenomenological exploration of interacting with digital entities. British Journal of \nEducational Technology, 39(6), 969‚Äì986. https://doi.org/10.1111/j.1467-\n19 \n \n8535.2007.00797.x \nVenkatesh, Morris, Davis, & Davis. (2003). User Acceptance of Information Technology: \nToward a Unified View. MIS Quarterly, 27(3), 425. https://doi.org/10.2307/30036540 \nVenkatesh, V. (2000). Determinants of Perceived Ease of Use: Integrating Control, Intrinsic \nMotivation, and Emotion into the Technology Acceptance Model. Information Systems \nResearch, 11(4), 342‚Äì365. https://doi.org/10.1287/isre.11.4.342.11872 \nWang, S., Lilienfeld, S. O., & Rochat, P. (2015). The Uncanny Valley: Existence and \nExplanations. Review of General Psychology, 19(4), 393‚Äì407. \nhttps://doi.org/10.1037/gpr0000056 \nWarshaw, P. R., & Davis, F. D. (1985). Disentangling behavioral intention and behavioral \nexpectation. Journal of Experimental Social Psychology, 21(3), 213‚Äì228. \nhttps://doi.org/10.1016/0022-1031(85)90017-4 \nWilliams, D., Coles, L., Wilson, K., Richardson, A., & Tuson, J. (2000). Teachers and ICT: \ncurrent use and future needs. British Journal of Educational Technology, 31(4), 307‚Äì\n320. https://doi.org/10.1111/1467-8535.00164 \nWilson, H. J., Daugherty, P. R., & Morini-Bianzino, N. (2018). The Jobs That Artificial \nIntelligence Will Create. In What the Digital Future Holds (Vol. 58, Issue 4, pp. 14‚Äì16). \nThe MIT Press. https://doi.org/10.7551/mitpress/11645.003.0020 \nWooldridge, J.‚ÄØ;. (2006). WooldrigeCap17.pdf. In Paraninfo (Ed.), Introducci√≥n a la \neconometr√≠a (2a, pp. 619‚Äì671). \nXiao, J., Xiao, J., Catrambone, R., & Stasko, J. (2003). Be Quiet? Evaluating Proactive and \nReactive User Interface Assistants. Proccedings of Interact 2003, 383--390. \nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.7309 \nXu, K., & Lombard, M. (2017). Persuasive computing: Feeling peer pressure from multiple \ncomputer agents. Computers in Human Behavior, 74, 152‚Äì162. \nhttps://doi.org/10.1016/j.chb.2017.04.043 \nYoung Oh, E., Song, D., & Hong, H. (2020). Interactive Computing Technology in Anti-\nBullying Education: The Effects of Conversation-Bot‚Äôs Role on K-12 Students‚Äô Attitude \nChange Toward Bullying Problems. Journal of Educational Computing Research, 58(1), \n200‚Äì219. https://doi.org/10.1177/0735633119839177 \n \n  \n20 \n \nAppendix \nDetail of the four treatments \nScreenshot 1: Example of conversation (no emoticons, human first turn) \n \n \n \n \n \n \nTranscript of screenshot 1 (the text was in Spanish in the original experiment): \n- Chatbot: Reminder: Today you have class in 2¬∫A from 9 to 10, with the group of \n3¬∫B at 11 and the group of 1¬∫D at 1:00 PM \n- Human: Ok \n- Chatbot: More information at the main office, the phone number is 948-874598 \n- Human: Ok \n- Chatbot: Do you have any other questions? \n \n \n  \nHuman \nquestions and \nresponses to the \nchatbot are \npresented on \nthe right, in \ngreen boxes\nChatbot on the \nleft-hand side. \nYou will see the \nchatbot logo and \nthe chatbot \nresponses, which \nwill be in white \nboxes \n21 \n \nScreenshot 2: Example of conversation (no emoticons, human first turn) \n \n \n \n \n \n \nTranscript of screenshot 2 (the text was in Spanish in the original experiment): \n- Human: Edubot, what classes do I have today? \n- Chatbot: Today you have class in 2¬∫A from 9 to 10, with the group of 3¬∫B at 11 and \nthe group of 1¬∫D at 1:00 PM \n- Human: And what is the main office number? \n- Chatbot: The main office number is 948-874598 \n \n \n  \nHuman \nquestions and \nresponses to the \nchatbot are \npresented on \nthe right, in \ngreen boxes\nChatbot on the \nleft-hand side. \nYou will see the \nchatbot logo and \nthe chatbot \nresponses, which \nwill be in white \nboxes \n22 \n \nScreenshot 3: Example of conversation (emoticons, chatbot first turn) \n \n \n \n \n \n \nTranscript of screenshot 3 (the text was in Spanish in the original experiment): \n- Chatbot: Hello, Lidia üñêüñê, how are you? Remember, today you have \nclassÔøΩüè´üè´ in 2¬∫A from 9 to 10 ü•àü•à, with the group of 3¬∫B at 11 ü•âü•âand with \nthe group of 1¬∫D at 1:00 PM ü•áü•á. Have a great day! üòÄüòÄ \n- Human: Ok \n- Chatbot: Hello, Lidia üñêüñê, remember! If you have any questions, the main \noffice phone is 948-874598 ‚òé I hope I have been useful! üòÄüòÄ \n- Human: Ok \n \n \n  \nHuman \nquestions and \nresponses to the \nchatbot are \npresented on \nthe right, in \ngreen boxes\nChatbot on the \nleft-hand side. \nYou will see the \nchatbot logo and \nthe chatbot \nresponses, which \nwill be in white \nboxes \n23 \n \nScreenshot 4: Example of conversation (emoticons, human first turn) \n \n \n \n \n \n \nTranscript of screenshot 4 (the text was in Spanish in the original experiment): \n- Human: Edubot What classes do I have today? \n- Chatbot: Hello, Lidia üñêüñê, how are you? Today you have classÔøΩüè´üè´ in 2¬∫A from \n9 to 10 ü•àü•à, with the group of 3¬∫B at 11 ü•âü•âand with the group of 1¬∫D at 1:00 PM \nü•áü•á. Have a great day! üòÄüòÄ \n- Human: And what is the main office number? \n- Chatbot: Hello, Lidia üñêüñê. The main office number is 948-874598 ‚òé I hope I \nhave been useful! üòÄüòÄ \n \n \n \nHuman \nquestions and \nresponses to the \nchatbot are \npresented on \nthe right, in \ngreen boxes\nChatbot on the \nleft-hand side. \nYou will see the \nchatbot logo and \nthe chatbot \nresponses, which \nwill be in white \nboxes ",
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.9829550981521606
    },
    {
      "name": "Proactivity",
      "score": 0.8737322092056274
    },
    {
      "name": "Implementation",
      "score": 0.7898867130279541
    },
    {
      "name": "Technology acceptance model",
      "score": 0.5505840182304382
    },
    {
      "name": "Process (computing)",
      "score": 0.5140095949172974
    },
    {
      "name": "Psychology",
      "score": 0.4871194362640381
    },
    {
      "name": "Computer science",
      "score": 0.48523855209350586
    },
    {
      "name": "Usability",
      "score": 0.39109885692596436
    },
    {
      "name": "Mathematics education",
      "score": 0.33421990275382996
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.24803578853607178
    },
    {
      "name": "World Wide Web",
      "score": 0.19619527459144592
    },
    {
      "name": "Social psychology",
      "score": 0.1627466380596161
    },
    {
      "name": "Software engineering",
      "score": 0.10421004891395569
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}