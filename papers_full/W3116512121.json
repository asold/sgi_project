{
  "title": "Pre-trained Language Model Based Active Learning for Sentence Matching",
  "url": "https://openalex.org/W3116512121",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2016241076",
      "name": "Gui-Rong Bai",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2126153884",
      "name": "Shizhu He",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2072929885",
      "name": "Kang Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2028683064",
      "name": "Jun Zhao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2138546940",
      "name": "Zaiqing Nie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946728200",
    "https://openalex.org/W2950193743",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2889968917",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2736885633",
    "https://openalex.org/W2171671120",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2890802255",
    "https://openalex.org/W2152748974",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2756386045",
    "https://openalex.org/W2964082993",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W2951147191",
    "https://openalex.org/W2876111955",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2952113915",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2963742748",
    "https://openalex.org/W2413794162"
  ],
  "abstract": "Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria from the pre-trained language model to measure instances and help select more effective instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1495–1504\nBarcelona, Spain (Online), December 8-13, 2020\n1495\nPre-trained Language Model Based Active Learning\nfor Sentence Matching\nGuirong Bai1,2, Shizhu He1,2, Kang Liu1,2, Jun Zhao1,2, Zaiqing Nie3\n1 National Laboratory of Pattern Recognition, Institute of Automation,\nChinese Academy of Sciences\n2 School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences\n3 Alibaba AI Labs\n{guirong.bai, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn\nzaiqing.nzq@alibaba-inc.com\nAbstract\nActive learning is able to signiﬁcantly reduce the annotation cost for data-driven techniques.\nHowever, previous active learning approaches for natural language processing mainly depend\non the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In\nthis paper, we propose a pre-trained language model based active learning approach for sentence\nmatching. Differing from previous active learning, it can provide linguistic criteria from the\npre-trained language model to measure instances and help select more effective instances for\nannotation. Experiments demonstrate our approach can achieve greater accuracy with fewer\nlabeled training instances.\n1 Introduction\nSentence matching is a fundamental technology in natural language processing. Over the past few years,\ndeep learning as a data-driven technique has yielded state-of-the-art results on sentence matching (Wang\net al., 2017; Chen et al., 2016; Gong et al., 2017; Yang et al., 2016; Parikh et al., 2016; Gong et al.,\n2017; Kim et al., 2019). However, this data-driven technique typically requires large amounts of manual\nannotation and brings much cost. If large labeled data can’t be obtained, the advantages of deep learning\nwill signiﬁcantly diminish.\nTo alleviate this problem, active learning is proposed to achieve better performance with fewer labeled\ntraining instances (Settles, 2009). Instead of randomly selecting instances, active learning can measure\nthe whole candidate instances according to some criteria, and then select more efﬁcient instances for\nannotation (Zhang et al., 2017; Shen et al., 2017; Erdmann et al., ; Kasai et al., 2019; Xu et al., 2018).\nHowever, previous active learning approaches in natural language processing mainly depend on the\nentropy-based uncertainty criterion (Settles, 2009), and ignore the characteristics of natural language.\nTo be more speciﬁc, if we ignore the linguistic similarity, we may select redundant instances and waste\nmany annotation resources. Thus, how to devise linguistic criteria to measure candidate instances is an\nimportant challenge.\nRecently, pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018;\nYang et al., 2019) have been shown to be powerful for learning language representation. Accordingly,\npre-trained language models may provide a reliable way to help capture language characteristics. In this\npaper, we devise linguistic criteria from a pre-trained language model to capture language characteristics,\nand then utilize these extra linguistic criteria (noise, coverage and diversity) to enhance active learning. It\nis shown in Figure 1. Experiments on both English and Chinese sentence matching datasets demonstrate\nthe pre-trained language model can enhance active learning.\n2 Methodology\nIn a general active learning scenario, there is a small set of labeled training data P and a large pool of\navailable unlabeled data Q. Active learning is to select instances in Qaccording to some criteria, and then\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1496\nAnnotator Unlabeled DataLabeled Data\nClassifier Criteria\nuncertainty criterion\nnoise criterion*\ncoverage criterion*\ndiversity criterion*\nActive Learning\nMeasuring\nLabeling\nTraining\nQuery\nLanguage Model\nlinguistic\nlinguistic\nlinguistic\nFigure 1: Pipeline of our pre-trained language model based active learning. Noise, coverage and diversity\nare proposed linguistic criteria from a pre-trained language model.\nlabel them and add them into P, so as to maximize classiﬁer M performance and minimize annotation\ncost. More details of preliminaries about sentence matching and active learning are in the Appendix.\n2.1 Pre-trained Language Model\nWe choose the widely used language model BERT (Devlin et al., 2018) as the pre-trained language model.\nFrom BERT, we can obtain two kinds of information to provide linguistic criteria. One is the cross entropy\nloss sai of reconstructing of the i-th word ai in sentence A(the same with another B) by masking only\nai and predicting ai again. The other is word embeddings (contextual representations of the last layer)\na=[e(a1),e(a2),. . . ,e(alA)] in the sentence, where lA is the length of sentence A.\n2.2 Criteria for Instance Selection\n(1) Uncertainty: The uncertainty criterion indicates classiﬁcation uncertainty of an instance and is the\nstandard criterion in active learning. Instances with high uncertainty are more helpful to optimize the\nclassiﬁer and thus are worthier to be selected. The uncertainty is computed as the entropy, and we can\nobtain uncertainty rank rankuncer(xi) for the i-th instance in Qbased on the entropy. Formally,\nrankuncer(xi) ∝−Ent(xi) (1)\nwhere Ent(xi) =−∑\nk P(yi = k|xi) logP(yi = k|xi).\n(2) Noise: The noise criterion indicates how much potential noise there is in an instance. Intuitively,\ninstances with noise may degrade the labeled data P, and we want to select noiseless instances. Noisy\ninstances usually have rare expression with low generating probability. Thus, tokens in noisy instances\nmay be hard to be reconstructed with context by the pre-trained language model. Based on this assumption,\nnoise criterion is formulated about losses of reconstructing masked tokens:\nranknoise(xi) ∝−P(A) −P(B) (2)\nwhere P(A) =P(a1a2 ...a lA) ∝ lA∑\ni∈lA sai\n. P(B) is similar. ranknoise(xi) denotes noise rank of the\ni-th instance in Q, sai/sbi is the reconstruction loss of the i-th word ai/bi in sentence A/B from the\npre-trained language model.\n(3) Coverage: The coverage criterion indicates whether the language expression of the current instance\ncan enrich representation learning. On the one hand, some tokens like stop words are meaningless and easy\nto model (high coverage). On the other hand, the classiﬁer needs fresh instances (low coverage) to enrich\nrepresentation learning. These fresh instances like relatively low-frequency professional expressions\nusually have lower generating probabilities than common ones. Thus, we can employ reconstruction\nlosses to capture the low coverage ones as follows:\nrankcover(xi) ∝−\n∑\nj∈lA caj saj\n∑\nj∈lA caj\n−\n∑\nj∈lB cbj sbj\n∑\nj∈lB cbj\n(3)\ncaj =\n{ 0 if saj >β\n1 others ,cbj =\n{ 0 if sbj >β\n1 others (4)\n1497\nwhere βdenotes a hyperparameter to distinguish noise and is set as 10.0.\n(4) Diversity: The diversity criterion indicates the diversity of instances. Redundant instances are\ninefﬁcient and waste annotation resources. In contrast, diverse ones can help learn more various language\nexpressions and matching patterns.\nFirst, we use a vector vi for instance representation of a sentence pair instance xi. To model the\ndifference between two sentences, we employ the subtraction of word embeddings between “Delete\nSequence” LD and “Insert Sequence” LI from Levenshtein Distance (when we transform sentence A\nto sentence B by deleting and inserting tokens, these tokens are added into LD and LI respectively).\nIt is illustrated in the Appendix. Besides, the word embeddings in the subtraction are weighted by\nreconstruction losses. Intuitively, meaningless tokens such as preposition should have less weight, and\nthey are usually easier to predict with lower reconstruction losses. Formally,\nvi =\n∑\nj∈LI\nwbj e(bj) −\n∑\nj∈LD\nwaj e(aj) (5)\nwaj = saj∑\nk∈lA sak\n,wbj = sbj∑\nk∈lB sbk\n(6)\nwhere sai/sbj is the reconstruction loss of the i/j-th word of sentence A/B. e(aj)/e(bj) denotes word\nembdeddings. wai/wbj denotes the weight for tokens.\nWith instance representation, we want to select diverse ones that are representative and different from\neach other. Speciﬁcally, we employ k-means clustering algorithm for diversity rank as follows:\nrankdiver(xi) =\n{ 0 if vi ◦vi ∈Odiver\nn others (7)\nwhere Odiver are the centers of nclusters of {vi ◦vi}. ◦denotes multiplication on element.\n2.3 Instance Selection\nIn practice, according to different effectiveness of criteria, we combine ranks of criteria and select the\ntop ncandidate instances in unlabeled data Q. Speciﬁcally, we sequentially use rankuncer, rankdiver,\nrankcover, ranknoise to select top 8n, 4n, 2n, ncandidate instances, and add the ﬁnal ninstances into\nlabeled data P for training at every round.\n3 Experiments\n3.1 Settings and Comparisons\nWe conduct experiments on Both English and Chinese datasets, including SNLI (Bowman et al., 2015),\nMultiNLI (Williams et al., 2017), Quora (Iyer et al., 2017), LCQMC (Liu et al., 2018), BQ (Chen et al.,\n2018). The number of instances to select at every round is n= 100. We choose (Devlin et al., 2018) as\nclassiﬁer M and perform 25 rounds of active learning. There is a held-out test set for evaluation after all\nrounds. We compare the following active learning approaches:\n(1)Random sampling (Random) randomly selects instances for annotation and training at each round.\n(2)Uncertainty sampling (Entropy) is the standard entropy criterion (Tong and Koller, 2001; Zhu et al.,\n2008).\n(3)Expected Gradient Length (EGL) aims to select instances expected to result in the greatest change\nto the gradients of tokens (Settles and Craven, 2008; Zhang et al., 2017).\n(4)Pre-trained language model (LM) is our proposed active learning approach.\n3.2 Results\nTable 1 and Figure 2 (1-5) report accuracy and learning curves of each approach on the ﬁve datasets.\nOverall, our approach obtains better performance on both English and Chinese datasets. We can know that\nextra linguistic criteria are effective, demonstrating that a pre-trained language model can substantially\ncapture language characteristics and provide more efﬁcient instances for training. Besides, active learning\n1498\nSNLI MultiNLI Quora LCQMC BQ\nRandom 77.90 67.83 79.01 82.04 71.44\nEntropy 79.80 70.27 80.21 83.25 73.60\nEGL 77.86 66.80 77.91 80.35 71.59\nLM 80.99 71.79 81.79 84.29 74.73\nEnt E+Cov E+Noi E+Div E+All\nAblation 79.80 80.99 81.11 81.45 80.99\nTable 1: The upper part lists accuracy of different approaches on ﬁve datasets. The low part lists accuracy\nof combining different linguistic criterion with uncertainty on SNLI dataset for ablation.\nFigure 2: The ﬁgures 1-5 are learning curves of comparisons on the ﬁve datasets. The 6-th ﬁgure illustrates\nlearning curves on four SNLI subsets to show the relation between data size and accuracy.\napproaches always obtain better performance than random sampling. It demonstrates that the amount of\nlabeled data for sentence matching can be substantially reduced by active learning. And EGL performs\nworse than the standard approach active learning, maybe gradient based active learning is not suitable\nfor sentence matching. In fact, sentence matching needs to capture the difference between sentences and\ngradients of a single token can’t reﬂect the relation. Moreover, we show the relation between the size of\nunlabeled data and accuracy in Figure 2 (6), we can see the superiority of the pre-trained model based\napproach is more signiﬁcant for larger data size.\n3.3 Ablation Study\nTo validate the effectiveness of extra linguistic criteria, we separately combining them with standard\nuncertainty criterion. “Ent” denotes the standard uncertainty criterion, “E+Noi/E+Cov/E+Div/E+All”\ndenotes combining uncertainty with noise/coverage/diversity/all criteria. Table 1 reports the accuracy.\nCurves are also illustrated in the Appendix.\nWe can see each combined criterion performs better than a single uncertainty criterion. It demonstrates\nthat each linguistic criterion from a pre-trained language model helps capture language characteristics and\nenhances selection of instances. More ablation discussions are shown in the Appendix.\n4 Conclusion\nIn this paper, we combine active learning with a pre-trained language model. We devise extra linguistic\ncriteria from a pre-trained language model, which can capture language characteristics and enhance active\nlearning. Experiments show that our proposed active learning approach obtains better performance.\n1499\nAcknowledgements\nThe work is supported by the National Natural Science Foundation of China under Grant Nos.61533018,\nU1936207, 61976211, and 61702512. This research work was also supported by the independent research\nproject of National Laboratory of Pattern Recognition and the Youth Innovation Promotion Association\nCAS.\nReferences\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2016. Enhanced lstm for natural\nlanguage inference. arXiv.\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018. The bq corpus: A large-scale\ndomain-speciﬁc chinese corpus for sentence semantic equivalence identiﬁcation. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv.\nAlexander Erdmann, David Joseph Wrisley, Benjamin Allen, Christopher Brown, Sophie Cohen-Bod´en`es, Micha\nElsner, Yukun Feng, Brian Joseph, B ´eatrice Joyeux-Prunel, and Marie-Catherine de Marneffe. Practical, efﬁ-\ncient, and customizable active learning for named entity recognition in the digital humanities. In NAACL.\nYichen Gong, Heng Luo, and Jian Zhang. 2017. Natural language inference over interaction space. arXiv.\nShankar Iyer, Nikhil Dandekar, and Korn´el Csernai. 2017. First quora dataset release: Question pairs. data. quora.\ncom.\nJungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian Popa. 2019. Low-resource deep entity resolution\nwith transfer and active learning. In ACL.\nSeonhoon Kim, Inho Kang, and Nojun Kwak. 2019. Semantic sentence matching with densely-connected recur-\nrent and co-attentive information. In AAAI.\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang. 2018. Lcqmc: A\nlarge-scale chinese question matching corpus. In COLING.\nAnkur P Parikh, Oscar T ¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model\nfor natural language inference. arXiv.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. arXiv.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nwith unsupervised learning. Technical report, Technical report, OpenAI.\nBurr Settles and Mark Craven. 2008. An analysis of active learning strategies for sequence labeling tasks. In\nEMNLP.\nBurr Settles. 2009. Active learning literature survey. Technical report, University of Wisconsin-Madison Depart-\nment of Computer Sciences.\nYanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod, and Animashree Anandkumar. 2017. Deep active\nlearning for named entity recognition. arXiv.\nSimon Tong and Daphne Koller. 2001. Support vector machine active learning with applications to text classiﬁca-\ntion. JMLR.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural language\nsentences. arXiv.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence\nunderstanding through inference. arXiv.\n1500\nYang Xu, Yu Hong, Huibin Ruan, Jianmin Yao, Min Zhang, and Guodong Zhou. 2018. Using active learning to\nexpand training data for implicit discourse relation recognition. In EMNLP.\nLiu Yang, Qingyao Ai, Jiafeng Guo, and W Bruce Croft. 2016. anmm: Ranking short answer texts with attention-\nbased neural matching model. In ACM international on conference on information and knowledge management.\nACM.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv.\nYe Zhang, Matthew Lease, and Byron C Wallace. 2017. Active discriminative text representation learning. In\nAAAI.\nJingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin K. Tsou. 2008. Active learning with sampling by\nuncertainty and density for word sense disambiguation and text classiﬁcation. COLING.\n1501\nAppendix A: More Details and Discussions\nSentence Matching Task: Given a pair of sentences as input, the goal of the task is to judge the relation\nbetween them, such as whether they express the same meaning. In formal, we have two sentences\nA=[a1,a2,. . . ,alA] and B=[b1,b2,. . . ,blB ], where ai and bj denote the i-th and j-th word respectively in\ncorresponding sentences, and lA and lB denote the length of corresponding sentences.\nThrough a shared word embedding matrix We ∈Rne×d, we can obtain word embeddings of input\nsentences a=[e(a1),e(a2),. . . ,e(alA)] and b=[e(b1),e(b2),. . . ,e(blB )], where ne denotes the vocabulary\nsize, ddenotes the embedding size and e(ai) and e(bj) denote the word embedding of the i-th and j-th\nword respectively in corresponding sentences. And there is a sentence matching model M to predict a\nlabel ˆybased on a and b. When testing, we choose the label with the highest probability in prediction\ndistribution P(yi|a,b; θM ) as output, where θM denotes parameters of the model M and yi denotes a\npossible label. When training, the model M is optimized by minimizing cross entropy:\nLoss= −P(y|a,b; θM ) logP(y|a,b; θM ) (8)\nwhere ydenotes the golden label.\nStandard Active Learning: In a general active learning scenario, there exists a small set of labeled\ndata P and a large pool of available unlabeled data Q. P is for training a classiﬁer and can absorb new\ninstances from Q. The task for the active learning is to select instances in Qbased on some criteria, and\nthen label them and add them into P, so as to maximize classiﬁer performance and minimize annotation\ncost. In the selection criteria, a measure is used to score all candidate instances in Q, and instances\nmaximizing this measure are selected into P.\nThe process is illustrated in Algorithm 1. The instance selection process is iterative, and the process\nwill repeat until a ﬁxed annotation budget is reached. At every round, there are ninstances to be selected\nand labeled.\nAlgorithm 1 Active learning algorithm ﬂow.\nInput:\nlabeled data set P={∅}, unlabeled data set Q={qi}, the classiﬁer M, criteria of instance selection C,\nthe number of instances for annotation at every round n\nOutput:\nlabeled data set P={pi}, the classiﬁer M\n1: repeat\n2: Sort Qbased on M and C\n3: Select top ninstances from Qto label, update Q\n4: Add labeled ninstances into P, update P\n5: Train and update classiﬁer M based on P\n6: until The annotation budget is exhausted\nWith the same amount of labeled data P, criteria for instance selection in active learning determine the\nclassiﬁer performance. Commonly, the criteria is mainly based on uncertainty criterion ( uncertainty\nsampling), in which ones near decision boundaries have priority to be selected. A general uncertainty\ncriterion uses entropy, which is deﬁned as follows:\nEnt(xi) =−\n∑\nk\nP(yi = k|xi) logP(yi = k|xi) (9)\nwhere kindexes all possible labels, xi denotes a candidate instance that is made up of a pair of sentences\nAand Bin available unlabeled data Q.\nVisualization of Delete Sequence and Insert Sequence:To model the difference between two sentences,\nwe employ the subtraction of word embeddings between “Delete Sequence” and “Insert Sequence” from\nLevenshtein Distance (when we transform sentence A to sentence B by deleting and inserting tokens,\nthese tokens are added into “Delete Sequence” and “Insert Sequence” espectively). We illustrate it in\n1502\n\"What shall I do to improve my IQ ? How can I expand my IQ ?\"\n-Delete\n+Insert\nX remain\nX\nX\nDelete Sequence\nInsert Sequence\nare you now\nHow are you\n-Delete\nA Sentence B\nEditing\nWhere\n-Delete\nInsert Sequence:\nDelete Sequence:\nRemain Sequence:\nSentence A\nWhere\nare you\nRemainRemain\nHow\n+Insert\nnow\nFigure 3: “Delete Sequence” and “Insert Sequence”.\nFigure 3.\nDatasets: We conduct experiments on three English datasets and two Chinese dataset. Table 2 provides\nstatistics of these datasets.\n(1)SNLI: an English natural language inference corpus based on image captioning.\n(2)MultiNLI: an English natural language inference corpus with greater linguistic difﬁculty and\ndiversity.\n(3)Quora: an English question matching corpus from the online question answering forum Quora.\n(4)LCQMC: an open-domain Chinese question matching corpus from the community question answer-\ning website Baidu Knows.\n(5)BQ: an in-domain Chinese corpus question matching corpus from online bank custom service logs.\ntraining validation test\nSNLI 549,367 9,842 9,824\nMultiNLI 392,702 9,815 9,832\nQuora 384,348 10,000 10,000\nLCQMC 238,766 8,802 12,500\nBQ 100,000 1,000 1,000\nTable 2: Statistics of sentence matching datasets.\nFigure 4: Learning curves of combining each proposed linguistic criterion with uncertainty on SNLI\ndataset.\nConﬁguration: The number of instances to select nis 100 at every round and we perform 25 rounds of\nactive learning, that is there are total of 2500 labeled instances for training in the end. Batch size is 16 for\nEnglish and 32 for Chinese, Adam is used for optimization. We evaluated performance by calculating\naccuracy and learning curves on a held-out test set (classes are fairly balanced in datasets) after all rounds.\n1503\nFigure 5: Learning curves of different instance representation methods.\nFigure 6: Learning curves of subtraction operation on Levenshtein Distance.\nCurves of Ablation Study: Figure 4 shows learning curves of combining each proposed linguistic\ncriterion with uncertainty on SNLI dataset.\nDiscussion:\n(1)Effectiveness of different instance representation methods: We validate the effectiveness of\ndifferent instance representation methods in diversity criterion on SNLI dataset. We compare our method\nwith 4 baselines: (a) using the ﬁrst word embedding layer in BERT as context-dependent representations\n(Uncontext); (b) using the subtraction between sentence vectors from auto-encoding (AE); (c) using\nthe subtraction between sentence vectors from topic model (Topic); (d) using the subtraction between\nsentence vectors from Skip-Thoughts (Skip).\nEntroy Uncontext AE Topic Skip LM\n79.80 80.63 80.42 80.54 80.71 80.99\nTable 3: Accuracy of different instance representation methods.\nTable 3 and Figure 5 report accuracy and learning curves respectively. We can see contextual repre-\nsentations are better than context-dependent representations. In intuition, contextual representations are\nmore exact especially when dealing with polysemy. Next, we ﬁnd our proposed method outperforms\nsentence vector based methods (Topic, AE, and Skip). It is possibly because BERT used more data to\nlearn language representations.\n(2)Effectiveness of subtraction operation on Levenshtein Distance: Here we validate the effective-\nness of the operation that uses the subtraction of word embeddings between “Delete Sequence” and “Insert\nSequence” in diversity criterion on SNLI dataset. We compare it with 4 baselines: (a) using the sum of\nword embeddings of the two sentences (Sum); (b) directly using the subtraction of word embeddings of\n1504\nthe two sentences without “Delete Sequence” and “Insert Sequence” (Sub); (c) without weight for word\nembeddings (Nowei); (d) without absolute value operation for symmetry (Noabs).\nEntroy Sum Sub Nowei Noabs LM\n79.80 80.35 80.67 80.29 80.44 80.99\nTable 4: Accuracy of subtraction operation on Levenshtein Distance.\nTable 4 and Figure 6 report accuracy and learning curves respectively. We can see subtraction operation\nis better than sum operation. It demonstrates that subtraction has better ability to capture the difference\nbetween two sentences, and provides better instance representation for diversity rank. We can see the\nresults without “Delete Sequence” and “Insert Sequence” performs a little worse, proving its necessity.\nAnd the results without weight operation for word embeddings perform worse. We can know weight\nfor meaningless tokens is effective. Besides, we can see the results without absolute value operation for\nsymmetry is worse, demonstrating absolute value operation is necessary.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8279508352279663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7202627658843994
    },
    {
      "name": "Sentence",
      "score": 0.6765791773796082
    },
    {
      "name": "Annotation",
      "score": 0.6218748688697815
    },
    {
      "name": "Natural language processing",
      "score": 0.6165897846221924
    },
    {
      "name": "Matching (statistics)",
      "score": 0.6002350449562073
    },
    {
      "name": "Active learning (machine learning)",
      "score": 0.5782846808433533
    },
    {
      "name": "Natural language",
      "score": 0.5048893094062805
    },
    {
      "name": "Language model",
      "score": 0.4480769634246826
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.44764232635498047
    },
    {
      "name": "Machine learning",
      "score": 0.38647645711898804
    },
    {
      "name": "Mathematics",
      "score": 0.05594980716705322
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}