{
  "title": "TrEP: Transformer-Based Evidential Prediction for Pedestrian Intention with Uncertainty",
  "url": "https://openalex.org/W4382240287",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2113233021",
      "name": "Zhengming Zhang",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2144151552",
      "name": "Renran Tian",
      "affiliations": [
        "Indiana University – Purdue University Indianapolis"
      ]
    },
    {
      "id": "https://openalex.org/A2235073056",
      "name": "Zhengming Ding",
      "affiliations": [
        "Tulane University"
      ]
    },
    {
      "id": "https://openalex.org/A2113233021",
      "name": "Zhengming Zhang",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2144151552",
      "name": "Renran Tian",
      "affiliations": [
        "Indiana University – Purdue University Indianapolis",
        "University of Indianapolis"
      ]
    },
    {
      "id": "https://openalex.org/A2235073056",
      "name": "Zhengming Ding",
      "affiliations": [
        "Tulane University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6755171240",
    "https://openalex.org/W6768698167",
    "https://openalex.org/W3183809202",
    "https://openalex.org/W6691241951",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W3208930250",
    "https://openalex.org/W3214126613",
    "https://openalex.org/W3206120323",
    "https://openalex.org/W2594843741",
    "https://openalex.org/W3002302528",
    "https://openalex.org/W2579937995",
    "https://openalex.org/W2883770893",
    "https://openalex.org/W2968524820",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3113216216",
    "https://openalex.org/W3182906273",
    "https://openalex.org/W4312511187",
    "https://openalex.org/W3119361198",
    "https://openalex.org/W3119170582",
    "https://openalex.org/W3007443069",
    "https://openalex.org/W3029267566",
    "https://openalex.org/W3048462507",
    "https://openalex.org/W4210546004",
    "https://openalex.org/W3091804291",
    "https://openalex.org/W2024529169",
    "https://openalex.org/W3091129905",
    "https://openalex.org/W2991484432",
    "https://openalex.org/W2771583656",
    "https://openalex.org/W3191907322",
    "https://openalex.org/W3110317294",
    "https://openalex.org/W2997563872",
    "https://openalex.org/W2806471870",
    "https://openalex.org/W3146879304",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W4318465483",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W3151748102",
    "https://openalex.org/W3201612248",
    "https://openalex.org/W2799059904",
    "https://openalex.org/W6746474542",
    "https://openalex.org/W3157832040",
    "https://openalex.org/W3045724617",
    "https://openalex.org/W4285139742",
    "https://openalex.org/W4200087236",
    "https://openalex.org/W2921286075",
    "https://openalex.org/W3159019800",
    "https://openalex.org/W6846968578",
    "https://openalex.org/W4309701360",
    "https://openalex.org/W4313159670",
    "https://openalex.org/W4312430207",
    "https://openalex.org/W2963694068",
    "https://openalex.org/W4294007906",
    "https://openalex.org/W3214535627",
    "https://openalex.org/W2252355370",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2964185119",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963353290",
    "https://openalex.org/W4303859916",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W4318621194",
    "https://openalex.org/W3163910697",
    "https://openalex.org/W4307717803",
    "https://openalex.org/W4287778673",
    "https://openalex.org/W4214751560",
    "https://openalex.org/W4210457203",
    "https://openalex.org/W4282813766",
    "https://openalex.org/W3127710918",
    "https://openalex.org/W4200630266",
    "https://openalex.org/W3008700642",
    "https://openalex.org/W3100019193",
    "https://openalex.org/W4318963850",
    "https://openalex.org/W2963818059",
    "https://openalex.org/W3177765762"
  ],
  "abstract": "With rapid development in hardware (sensors and processors) and AI algorithms, automated driving techniques have entered the public’s daily life and achieved great success in supporting human driving performance. However, due to the high contextual variations and temporal dynamics in pedestrian behaviors, the interaction between autonomous-driving cars and pedestrians remains challenging, impeding the development of fully autonomous driving systems. This paper focuses on predicting pedestrian intention with a novel transformer-based evidential prediction (TrEP) algorithm. We develop a transformer module towards the temporal correlations among the input features within pedestrian video sequences and a deep evidential learning model to capture the AI uncertainty under scene complexities. Experimental results on three popular pedestrian intent benchmarks have verified the effectiveness of our proposed model over the state-of-the-art. The algorithm performance can be further boosted by controlling the uncertainty level. We systematically compare human disagreements with AI uncertainty to further evaluate AI performance in confusing scenes. The code is released at https://github.com/zzmonlyyou/TrEP.git.",
  "full_text": "TrEP: Transformer-Based Evidential Prediction for\nPedestrian Intention with Uncertainty\nZhengming Zhang1, Renran Tian2, Zhengming Ding3\n1 School of Industrial Engineering, Purdue University, West Lafayette, Indiana, USA\n2 Department of Computer Information Technology, Indiana University Purdue University Indianapolis, Indiana, USA\n3 Department of Computer Science, Tulane University, New Orleans, Louisiana, USA\nzhan3988@purdue.edu, rtian@iupui.edu, zding1@tulane.edu\nAbstract\nWith rapid development in hardware (sensors and proces-\nsors) and AI algorithms, automated driving techniques have\nentered the public’s daily life and achieved great success in\nsupporting human driving performance. However, due to the\nhigh contextual variations and temporal dynamics in pedes-\ntrian behaviors, the interaction between autonomous-driving\ncars and pedestrians remains challenging, impeding the de-\nvelopment of fully autonomous driving systems. This pa-\nper focuses on predicting pedestrian intention with a novel\ntransformer-based evidential prediction (TrEP) algorithm.\nWe develop a transformer module towards the temporal cor-\nrelations among the input features within pedestrian video se-\nquences and a deep evidential learning model to capture the\nAI uncertainty under scene complexities. Experimental re-\nsults on three popular pedestrian intent benchmarks have veri-\nfied the effectiveness of our proposed model over the state-of-\nthe-art. The algorithm performance can be further boosted by\ncontrolling the uncertainty level. We systematically compare\nhuman disagreements with AI uncertainty to further evaluate\nAI performance in confusing scenes. The code is released at\nhttps://github.com/zzmonlyyou/TrEP.git.\nIntroduction\nWith the rapid progress in AI technologies, the numerous\nsuccesses in intelligent transportation systems have made\nautonomous driving promising (Liu et al. 2022; Tang et al.\n2023; Cui et al. 2022; Liu et al. 2021; Zeng et al. 2021).\nThese transformative technologies have the potential to fun-\ndamentally change daily life for everyone and create vast\nsocial and individual benefits (Litman 2017). Mercedes has\nrecently begun selling their Level 3 self-driving system (de-\nfined by SAE International as Conditional Driving Automa-\ntion) on their S-Class, marking a significant milestone as\nhigher-level automated driving techniques enter ordinary\npeople’s lives. However, while Level 2 and 3 automated sys-\ntems can drive autonomously under human supervision and\nwithin the Operational Design Domain (ODD), the main\nchallenge for fully automated cars to safely and efficiently\ndrive in urban settings remains interactions with pedestri-\nans (Domeyer, Lee, and Toyoda 2020; Herman et al. 2021;\nZhang, Tian, and Duffy 2023).\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nA large number of studies have concentrated on model-\ning and predicting pedestrian behaviors, with various deep\nlearning techniques and benchmark data sets constructed in\nthe past few years (Chen and Tian 2021). In general, tra-\njectory prediction represents the majority of pedestrian be-\nhavior modeling efforts. While traditional algorithms pre-\ndict trajectory in a fixed bird eye view from mainly surveil-\nlance cameras (Xu, Piao, and Gao 2018; Zhang et al. 2019;\nShi et al. 2021; Ma, Karimpour, and Wu 2020; Liu et al.\n2020b,c), many recent studies focus on the moving ego-\ncentric view in front of the vehicles to better serve the\nneeds of automated driving (Rasouli et al. 2019; Rasouli,\nRohani, and Luo 2021; Yagi et al. 2018; Chen, Tian, and\nDing 2021). Although recent trajectory prediction algo-\nrithms have achieved improved accuracies, inherent limita-\ntions prevent satisfying prediction accuracy in longer pre-\ndiction horizon (Herman et al. 2021), including the behav-\nior temporal dynamics, uncertainty related to the scene com-\nplexity, and accumulated position prediction errors over time\nsteps. Such limitations restrict the common trajectory pre-\ndiction horizon to about 1-2 seconds.\nA Limited trajectory prediction horizon may be sufficient\nfor automatic braking features focusing on last-second brak-\ning to improve safety, but cannot support efficient motion\nplanning for higher-level automatic cars to interact with\npedestrians smoothly. Some studies have shown that hu-\nman drivers need at least 3 seconds of prediction horizon to\nplan driving behaviors during pedestrian interactions (Her-\nman et al. 2021; Zhang et al. 2022a, 2021b; Pang, Guo,\nand Zhuang 2022), indicating a similar requirement for au-\ntomatic driving algorithms. Also, in the case to detect the\nout-of-ODD event and start the transition from automatic\ncontrol to manual driving, drivers need up to 20 seconds to\nfully control the car given a sudden automatic driving failure\n(Eriksson and Stanton 2017; Merat et al. 2014), which poses\nhigh requirements of pedestrian behavior prediction horizon\nas well to ensure driving safety.\nSolutions are needed to address the limitations of pedes-\ntrian trajectory prediction. Besides some task-specific prob-\nabilistic behavior prediction metrics such as In-ROI Sensi-\ntivity (IRS) (Herman et al. 2021), many studies started to\nfocus on pedestrian intention prediction. The goal of inten-\ntion prediction is to help identify crossing pedestrians (Fang\nand L´opez 2018), anticipating crossing timing (Zhang et al.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3534\n2021a), and improving trajectory and action prediction with\nintentions as guidance and boundaries (Rasouli et al. 2019;\nYao et al. 2021; Wang et al. 2022; Jing et al. 2022; Zhang\net al. 2022c; Ding et al. 2018; Zhang et al. 2022b).\nAlthough pedestrian intention prediction appears to be\na promising and vital research direction, there are signifi-\ncant challenges of uncertainty that are neglected in the cur-\nrent research frontier. Many studies have found uncertainty\nand disagreement among human annotators (Wu et al. 2022;\nJi et al. 2021). In human annotations (Rasouli et al. 2019;\nChen et al. 2021), significant disagreements among human\ndrivers in estimating pedestrian crossing intentions were ob-\nserved. Given the same driving scenes, two studies reported\nthat human drivers not only disagreed on pedestrian cross-\ning intentions at the same pre-determined critical frames\nbut also tended to estimate crossing/non-crossing at dif-\nferent timings. These phenomena reflect the uncertainty of\nunderstanding complex pedestrian-crossing driving scenes,\nwhich are highly dynamic, non-deterministic, and context-\ndependent. We argue that without modeling uncertainty in\nintention prediction, deep learning algorithms will struggle\nto achieve higher accuracies and predictability.\nOur Contributions. We propose a transformer-based evi-\ndential prediction (TrEP) algorithm for uncertainty-aware\nestimation of pedestrian intentions. Taking ego-centric\npedestrian encountering scene videos as input, the algorithm\nautomatically learns the evidence 1 towards different inten-\ntion categories from the motion information of the car and\ntargeted pedestrian. Trained evidence distributions in the\nhigh-dimensional spatial-temporal-mixed feature space are\nthen employed for intention prediction and uncertainty esti-\nmation. The study has achieved four main contributions:\n• The proposed TrEP is able to capture more temporal cor-\nrelation and be aware of pedestrian intention uncertainty\nso that it outperforms the state-of-the-art algorithms on\nthree benchmark datasets with large margins.\n• Strong negative relationship has been observed consis-\ntently between the uncertainty levels and algorithm pre-\ndiction accuracies, with the uncertainty-aware prediction\nhelping to secure high-level accuracy reliably by filtering\nout the cases with higher uncertainty levels.\n• The uncertainty associated with pedestrian intention es-\ntimation results improves the predictability and trustwor-\nthiness of the algorithm behavior, which can significantly\nenhance human-AI coordinated automatic driving.\n• Our data-driven pedestrian intention estimation uncer-\ntainty learned by the model is comparable with corre-\nsponding human disagreements in certain situations, al-\nthough human annotation disagreement levels are not in-\ncluded during the model training.\nRelated Works\nPedestrian intention prediction received a lot of attention\nin recent years to facilitate the interactions between au-\ntonomous cars and vulnerable road users. Based on some\n1A higher-order coding scheme for scene features following the\nDirichlet distribution, with details described in later sections.\npioneering benchmark datasets on pedestrian intention, like\nthe JAAD (Rasouli, Kotseruba, and Tsotsos 2017) and PIE\n(Rasouli et al. 2019), a lot of pedestrian intention predic-\ntion algorithms have been proposed (Rasouli, Kotseruba,\nand Tsotsos 2017; Liu et al. 2020a; Kotseruba, Rasouli, and\nTsotsos 2020). One early work used a CNN to extract fea-\ntures from a static frame of driving scenes to predict pedes-\ntrian intention(Rasouli, Kotseruba, and Tsotsos 2017). In an-\nother study, Fang et al. used a pre-trained pose estimation\nnetwork to estimate pedestrian pose and then predict the\ncrossing intention (Fang and L ´opez 2018). More recently, a\ngraph convolution network was trained to model the pedes-\ntrian pose along with visual features for intention prediction\n(Chen, Tian, and Ding 2021). Although different deep learn-\ning structures have been implemented in the domain, most\npresent studies consider the input as a sequence of frames\nand the output as a single probability of crossing (Rasouli\net al. 2019; Gujjar and Vaughan 2019; Liu et al. 2020a; Ra-\nsouli, Kotseruba, and Tsotsos 2020).\nIn a comparison study (Kotseruba, Rasouli, and Tsotsos\n2021), results show that both 3D convolution networks and\ntwo stream networks are capable of dealing with the tempo-\nral visual information (Simonyan and Zisserman 2014; Tran\net al. 2015; Carreira and Zisserman 2017). One proposed a\nnetwork fusing the temporal-spatial features from a 3D CNN\nalong with the bounding box coordinates and vehicle speed\npredictions (Kotseruba, Rasouli, and Tsotsos 2021). A re-\ncent work utilized the self-attention mechanism to capture\nthe spatial-temporal feature and fused it with semantic seg-\nmented context (Yang et al. 2022). Similarly, (Rasouli et al.\n2022) adopted an attention mechanism to fuse the multi-\nmodal features that achieved state-of-the-art performance.\nMost previous studies have utilized the RNN-based\nencoder-decoder framework to develop their models. While\nRNN-variants, such as LSTM, have incorporated mecha-\nnisms to capture the temporal relationships across the frame\nseries (Qu et al. 2020), the pedestrian intention prediction\ndomain has not fully explored other modern techniques,\nsuch as transformer-based sequential models. The latter op-\ntion has the potential to capture longer temporal patterns.\nMore importantly, none of the existing algorithms has\nadopted pedestrian intention estimation uncertainty as inputs\nor outputs of their models. In particular, we want to empha-\nsize that existing algorithms rely solely on the accuracy or\nF1 scores to evaluate intention prediction performance, ig-\nnoring the facts that the ground-truth labels in benchmark\ndatasets (Rasouli et al. 2019; Chen et al. 2021) contain in-\nherent uncertainties. The disagreement levels among human\nannotators shall be considered when developing and evalu-\nating corresponding algorithms.\nOur Proposed Method\nPreliminary & Motivation\nThe goal of intention prediction is to determine if the inter-\nested pedestrian is crossing or not given the raw input (Chen\net al. 2021; Rasouli et al. 2019). Thus, it can be formulated\nas a binary classificationI ∈ {0,1}, where 1 indicates cross-\ning and vice versa. Given a sequence of ego-centric frames\n3535\n{s1, s2, ..sl} with length l, there is a bounding box repre-\nsented by a quaternion, bi, in each frame i annotating the\nsame pedestrian. Each quaternion contains the 2D coordi-\nnates for the upper-left and bottom-right points of the bound-\ning box. In addition to the bounding box and visual infor-\nmation, each frame comes with an action annotation of the\nego-vehicle, ai, such as speed for PIE and driver action for\nJAAD (Rasouli, Kotseruba, and Tsotsos 2017).\nHowever, existing methods ignore the pedestrian intents\nto be conflicting in terms of various drivers, defined as\nhuman disagreement. It is intuitive to capture such intent\nuncertainty so that AI can mimic human cognition. To\nachieve this, evidential learning models second-order prob-\nabilities and uncertainty (Sensoy, Kaplan, and Kandemir\n2018; Amini et al. 2020; Bao, Yu, and Kong 2021), instead\nof modeling the probability assignment of a given sample.\nIn other words, a Dirichlet distribution parameterized over\nevidence represents the density of each such probability as-\nsignment, where the predicted evidence (parameters of the\nDirichlet distribution) is the model output. In this sense, we\ncould consider the uncertainty as a variance estimation of\nthe Dirichlet distribution.\nFramework Overview\nCompared to (Chen et al. 2021; Rasouli et al. 2019), our\nmodel is more compact. Since our objective is to devise an\nintention prediction model using purely tabular data, we dis-\ncarded the visual information {s1, s2, ..sl}. The input is a\nsequence of tabular information including bounding box and\nego-vehicle action. The first type is to simply consider the\nquaternion bi a type of feature. The second one is inspired by\nthe SORT tracking algorithm (Bewley et al. 2016). We first\ncalculate the center of the bounding box denoted as a tuple\nci and then compute the area and ratio between the length\nand width of the bounding box (ai, ri) for each frame i. The\noverall structure of our model is shown in Figure 1.\nBase Model Firstly, we concatenate all the input fea-\ntures bi, ci, ai, ri at each frame i to get the feature xi.\nyi is its corresponding ground-truth intent label. RNN-\nbased encoder-decoder captures the temporal correlation\nthrough model parameters (like memories), transformer-\nbased model (Vaswani et al. 2017a; Han et al. 2021; Xu\net al. 2021; Yi and Qu 2022; Wu et al. 2023) design attention\nmodules to capture all the possible relationships. In other\nwords, for a trained model, the attention mechanism relies\non the data itself explicitly to capture the temporal correla-\ntion, while LSTM/RNN memorizes the temporal informa-\ntion implicitly through model parameters.\nThus, a shared feed-forward layer is used to extend the\nfeature dimension for a later transformer layer with multi-\nhead attention (where the output is fi.) Before the sequence\nfusion, we include a positional encoder to add temporal in-\nformation gi. The positional encoder injects some informa-\ntion about the relative or absolute position of the frames\nin the sequence. The positional encodings are summed\nwith the inputs of the transformer, ki = xi + gi. We\nuse sine and cosine functions of different frequencies to\nencode the temporal order. The later layers until the last\nPositional Encoder\nTransformer\nEvidential Layer\nIntent: Xing\nuncertainty\n!!\n!\"\n!#\nFigure 1: Overview of the proposed model, where trans-\nformer module aims to capture temporal correlation explic-\nitly from {x1, x2, ··· , xl} while the evidential layer is to\ngenerate the model uncertainty u, providing one metric to\nreject the model prediction.\nfeed-forward layer could be considered a simple version\nof the transformer encoder (Vaswani et al. 2017b). The\ntransformer encoder includes blocks of one residual con-\nnected self-attention layer and one fully connected layer\nwith layer normalization. It transformed each input vector\naccording to self-attention. Then, we simply flatten the out-\nput for each component at frame i to merge the sequence\nto get final embeddings f as the following formulation:\nf = Flatten(Transformer(k1, k2, . . . , kl)).\nAfter that, we deploy the softmax as the activation func-\ntion on concatenated feature f, and apply the cross-entropy\nloss function as follows:\nL = −\nXN\nj=1\nyj log\n\u0010\nSoftmax(fj)\n\u0011\n. (1)\nUncertainty-Aware Evidential Learning In our Base\nModel for intent prediction, the softmax function is used to\npredict intent assignment probabilities. However, it provides\nonly a point estimate for the intent probabilities of a sam-\nple and does not provide the associated uncertainty for this\nprediction. On the other hand, Dirichlet distributions can be\nused to model a probability distribution for the class proba-\nbilities (Sensoy, Kaplan, and Kandemir 2018; Sensoy et al.\n2020). Therefore, we can use the variance estimates of the\nDirichlet distribution to calculate the model uncertainties.\nWe replaced our last activation function (softmax) with\na rectified linear unit (ReLU) to ascertain non-negative out-\nputs. The outputs are no longer modeled as the probability of\nthe classes. Instead, they are considered the evidence for the\npredicted Dirichlet distribution. In addition, we did not use\nthe cross-entropy loss, since the training goal is not to maxi-\nmize the likelihood of the model parameters given samples.\nBy decomposing the loss function in Equation (2), the first\npart aims to achieve the goals of minimizing the prediction\nerror while reducing the variance of the Dirichlet experiment\ngenerated by the model, specifically for each sample in the\ntraining set (Sensoy, Kaplan, and Kandemir 2018).\nGiven any sample i, the evidence ei refers to the output,\nand the uncertainty estimation loss is modeled as\nLi(Θ) =\nXK\nj=1\n(yij − E[pij])2 + Var(pij), (2)\n3536\nwhere yij is the j-th element of ground-truth labelyi, and pij\nis the j-th element of the probabilistic predictionpi referring\nto a simplex for probability assignments for sample i. E[·],\nand Var(·) are the operators for the expectation and vari-\nance over the Dirichlet distribution. Notepi is not the output\nof the model but a random vector following the Dirichlet dis-\ntribution. To estimate the pj, we use the following equation\nE[pij] = αij\nS , where S = PK\nj=1 αij = PK\nj=1(eij + 1).\nVar[pij] = E[pij ](1−E[pij ])\n(S+1) . K indicates the number of\nclasses and Θ represent the model parameters.\nIn this sense, the overall objective function integrated with\nKullback–Leibler (KL) divergence is formulated as:\nL =\nXN\ni=1\n\u0010\nLi(Θ) + λKL\n\u0000\nD(pi|αi)||D(pi|1)\n\u0001\u0011\n, (3)\nwhere N indicates the number of samples and D(pi|1)\nmeans a uniform prior if there is no evidence for the assign-\nment. KL(·) denotes the operators for the KL divergence,\nwhich aims to regularize our predictive distribution by pe-\nnalizing those divergences from the “I do not know” state\nthat do not contribute to data fit. the λ is the trade-off pa-\nrameter (by default we set it to 10.). The model uncertainty\nu is computed as u = K\nS .\nExperiment\nDataset\nIn our experiments, three intention/action prediction bench-\nmarks are explored, which are JAAD (Rasouli, Kotseruba,\nand Tsotsos 2017), PIE (Rasouli et al. 2019), and PSI (Chen\net al. 2021). To our best knowledge, those three benchmarks\nare the most representative datasets regarding the intent pre-\ndiction task. Specifically, JAAD and PSI are collected in a\nsimilar sense where both of them consist of recorded dash-\ncam video clips. In contrast, PIE is collected in a continuum\nfashion, where the entire dataset is from a 4-hour drive in\nToronto downtown. PIE and JAAD have a similar number\nof annotated pedestrians (> 1k), while PSI is smaller-scale.\nFurthermore, both PIE and JAAD datasets utilized a simi-\nlar annotation pipeline, wherein each pedestrian was labeled\nwith crossing action and crowdsourcing-labeled intention la-\nbels, and we used the crossing labels as a substitute for\nestimating pedestrian intention. However, the PSI dataset\nidentified weaknesses in the above-mentioned approach and\nadopted an intention segmentation methodology to tackle\nthe issue of intention dynamics. Specifically, PSI annotated\nthe crossing intention of each pedestrian for every frame.\nEvaluation and Metrics\nTo compare with the existing works fairly, we applied the\nevaluation protocol for both PIE and JAAD datasets (Kot-\nseruba, Rasouli, and Tsotsos 2021). In short, we sampled\nclips at least one second before the appearance of the cross-\ning action and predict the crossing intention. We set the over-\nlap ratio as 0.5 for both datasets. In total, PIE has 3,980\ntraining sequences 995 of which are crossing cases. On the\nother hand, JAAD has 3,955 training sequences including\n805 crossing cases. For the ego-vehicle action annotation,\nJAAD offered the driver’s behaviors while PIE recorded the\nspeed of the ego-vehicle. We calculated the F1 score, ac-\ncuracy, the area under the receiver operating characteris-\ntic (AUC), and precision to comprehensively evaluate the\nmodel performance.\nBecause of the different annotations between PSI and\nJAAD/PIE, we followed the original PSI for the task set-\nup (Chen et al. 2021). We sampled the clips with an overlap\nratio of 0.8 across the whole video as long as the pedestrian\nappears. Differently from one intention label for each pedes-\ntrian in PIE/JAAD, the annotated pedestrians in PSI have a\ncrossing intention label for each frame. The prediction task\nis to assign the crossing intention at the 16th frame given 15\nframes as input. Note that PSI does not provide any kind of\nego-vehicle action annotations. There are 6,262 training se-\nquences with 3,927 crossing cases. For the convenience of\ncomparing with the others, we reported accuracy, F1 score,\nand balanced accuracy for the models trained on PSI.\nImplementation Details\nDue to the different annotations and feature engineering for\nthe model on each dataset, the input dimensions (b×t ×fd)\nare slightly different (where b refers to batch size (we set\nb = 64), t and fd refers to the size of time span and feature\ndimension). We projected the input features dimensions fd\nto 8 dimensions in the first linear layer. The fully connected\nlayers in the transformer projected the 8 dimensions to 16.\nThere is one layer of multi-head attention (2 heads) for PIE\nand PSI and two layers for JAAD. The dropout rates are set\nto 0.1. All the models are trained by Adam optimizer with a\nlearning rate of 5e-3 for 2,000 epochs.\nComparison Results\nResults on PIE/JAAD The benchmark results for mod-\nels trained on PIE and JAAD are shown in Table ??, where\nwe compare with ATGC (Rasouli, Kotseruba, and Tsot-\nsos 2017), I3D (Carreira and Zisserman 2017), MM-LSTM\n(Aliakbarian et al. 2018), SF-GRU (Rasouli, Kotseruba,\nand Tsotsos 2020), PCPA (Kotseruba, Rasouli, and Tsot-\nsos 2021), MMHA (Rasouli et al. 2022), and BiPed (Ra-\nsouli, Rohani, and Luo 2021). ATGC is the only model\nwith a static input (one frame of the sequence). I3D is a\nwell-known 3D convolution network for video action recog-\nnition. PCPA, MMHA, and BiPed used multi-modality as\ntheir input, where the main difference is the incorporation\nof the fusion methods. Our proposed models outperformed\nall the existing models on both benchmark datasets. BiPed\n(Rasouli, Rohani, and Luo 2021) performed close to our\nproposed model on the PIE dataset, which adopts multi-\nmodality sources as input. Our model simply used bound-\ning box information along with ego-vehicle actions. How-\never, our model dominated the others on JAAD datasets for\nall metrics. Especially, the AUC score increased by 9%. On\nboth datasets, the performance of our base and evidential\nmodels are similar, which demonstrates the feasibility of ev-\nidential deep learning.\nResults on PSI Table ?? listed the performance for all\nmodels trained on the PSI dataset, where we compare with\n3537\nPIE JAAD\nModel\\Metric Accuracy AUC F1 Precision Accuracy AUC F1 Precision\nATGC 0.59 0.55 0.36 0.35 0.64 0.60 0.53 0.50\nI3D 0.79 0.75 0.64 0.61 0.82 0.75 0.55 0.49\nMM-LSTM 0.84 0.84 0.75 0.68 0.80 0.77 0.58 0.51\nSF-GRU 0.86 0.83 0.75 0.73 0.83 0.77 0.58 0.51\nPCPA 0.86 0.84 0.76 0.73 0.83 0.77 0.57 0.50\nMMHA 0.89 0.88 0.81 0.77 0.84 0.80 0.62 0.54\nBiPed 0.91 0.90 0.85 0.82 0.83 0.79 0.60 0.52\nOurs 0.91 0.93 0.85 0.84 0.87 0.88 0.63 0.63\nOurs (u = 1) 0.92 0.94 0.85 0.88 0.88 0.86 0.61 0.70\nOurs (u = 0.6; PIE = 96%, JAAD = 89%) 0.93 0.94 0.87 0.89 0.91 0.86 0.69 0.71\nTable 1: Performance of the proposed models and the other existing models on the JAAD and PIE datasets. u refers to the\nuncertainty threshold, where we reject the predictions with higher uncertainties. When u = 1 , all the samples are included.\nWhen u = 0.6, 96% of the PIE dataset and 89% of the JAAD dataset are included.\nModel\\Metric Accuracy Balanced\nAccuracy F1\nVR-GCN 0.74 0.61 0.64\nPIE-Intention 0.69 0.58 0.79\nPSI-Intention 0.76 0.67 0.66\nOurs 0.83 0.75 0.88\nOurs (u = 1) 0.82 0.75 0.87\nOurs (u = 0.6; 75%) 0.85 0.77 0.90\nTable 2: Performance of the proposed models and the other\nexisting models on the PSI dataset.urefers to the uncertainty\nthreshold, where we reject the predictions with higher un-\ncertainties. When u = 1, all the samples are included. When\nu = 0.6, 75% of the PSI dataset is included.\nVR-GCN (Chen, Tian, and Ding 2021), PIE-Intention (Ra-\nsouli et al. 2019) and PSI-Intention (Chen et al. 2021). VR-\nGCN used graph neural networks to model the pedestrian\nposes, whereas PSI-intention is based on multi-task learning\n(reasoning, trajectory, and intention). Similar to the JAAD\nand PIE datasets, our proposed model performed better than\nthe existing works. The F1 scores are boosted by 12%, and\nthe accuracy is increased by 7%. Again, the evidential and\nbase models performed comparably on the PSI dataset.\nAblation Study\nIn the ablation study, all the reported results are based on the\nbase model, because we did not find any significant differ-\nence between the base and evidential models. We test mod-\nels with different combinations of features. Also, we trained\nmodels with and without positional encoder. All the results\nare shown in Table 3. Besides the bounding box feature, we\nfound that the center coordinates of the bounding box are a\nvery useful feature, which boosted at least 8% of the accu-\nracy for all datasets. On the other hand, the ratio between the\nPIE JAAD PSI\nModel\\Metric Acc F1 Acc F1 Acc F1\nBbox+Action 0.80 0.72 0.79 0.58 0.72 0.69\nBbox+Action\n+Center 0.91 0.85 0.87 0.63 0.80 0.85\nBbox+Action\n+Center+Ratio 0.89 0.81 0.86 0.65 0.83 0.88\nNo Pos. Encoder 0.90 0.83 0.85 0.61 0.81 0.87\nTable 3: Performance for each variation of the base model on\nthree datasets. “bbox” refers to bounding box coordinates.\n“action” refers to ego-vehicle action. ”center” refers to the\ncoordinates of the bounding box center. “ratio” refers to the\nbounding box area and the ratio between length and width.\nwidth and length of the bounding box and the bounding box\narea is helpful in PIE while decreasing the performance in\nPSI. One reason might be the number of sample sequences.\nThe sequences sampled from the PSI are nearly twice the se-\nquences from PIE. At last, our results proved that the use of a\npositional encoder increased the performance in all datasets\nbecause it allows the model to capture the temporal changes.\nUncertainty Analysis\nIntuitively, we hypothesize that the samples with higher\nuncertainty generated by the evidential model have lower\nscores using the metrics because the evidence in our frame-\nwork is a measure of the amount of support collected from\ndata in favor of a sample to be classified into a certain class.\nIn other words, our evidential mode is not confident in the\nprediction when having a high uncertainty score.\nThe test samples were grouped based on predicted un-\ncertainty, as depicted by the blue bars in Fig. 2, with each\nbar representing the proportion of samples falling within the\ncorresponding uncertainty range. For the top three graphs in\nFig. 2, the uncertainty value refers to the range that the un-\ncertainty that is less than the value and 0.1 larger than the\n3538\nJAAD PIE\n PSI\nFigure 2: AI Uncertainty vs. Metrics on three datasets. The bars in the top three graphs are the proportion of the samples grouped\nby the uncertainty score (where 0.1 refers to the samples with uncertainty greater than 0 and less than 0.1). Each colored curve\ndenotes the performance using a specific metric. The graphs at the bottom are the corresponding cumulative version.\nvalue. For the bottom three graphs, the range is simply less\nthan the corresponding value. We found that the uncertainty\ndistributions in JAAD and PIE datasets have long right tails\nwhile the distribution in PSI is right-tailed.\nAs the uncertainty values increased for all three datasets,\nmost metrics exhibited a decrease. In JAAD and PSI, the\nprecision score reached 1 when the uncertainty was 1, as the\nmodel predicted all samples as ”crossing,” resulting in a low\nF1 score. In the case of the PIE dataset, it might seem like\nthe model performs worse when the uncertainty is low, given\nthe low F1 score and AUC on the left side. However, we ob-\nserved that the accuracy was very high on the left side, and\nthe low scores were due to the small number of ”not cross-\ning” samples. The cumulative graphs showed more stabil-\nity and gradual decrease as the uncertainty increased. These\nfindings support our hypothesis that ”the models perform\nbetter on samples with lower uncertainties”.\nDisagreement Analysis\nSince the PIE and PSI datasets provide the distribution of\nthe annotators’ decisions. For example, we know the num-\nber of annotators reporting the given pedestrian is crossing\nand vice versa. We use the entropy to measure disagree-\nment among the annotators. When all annotators have the\nsame intention estimation, the entropy is zero. On the other\nhand, entropy is one when the predictions are grouped into\nhalf and half. We present Fig. 3 using a similar fashion with\nFig. 2 while the samples are grouped by the human disagree-\nment scores. In addition, the green bars indicate the average\nuncertainty values for the corresponding groups.\nSince both figures indicated a trend of decreasing per-\nformance with larger human disagreement scores, we con-\ncluded that our model performed worse on the human con-\nflicting cases. We calculated the correlation coefficient be-\ntween human disagreement and model uncertainty to tes-\ntify whether the predicted uncertainties represent human dis-\nagreements. We found a weak negative correlation (correla-\ntion = -0.17, p-value < 0.001) and a strong positive corre-\nlation (correlation = 0.60, p-value < 0.001) for the PIE and\nPSI datasets, respectively. One possible explanation is that\nthe intention segmentation (annotation in PSI) gives each\nframe a crowd-sourced label delivering more information to\nthe model and allowing the model to capture patterns similar\nto the human. In contrast, PIE provides each pedestrian with\none fixed label across the whole time span might supervise\nthe model to ignore some discriminative features.\nCase Study\nIn Fig. 4, we select some interesting cases from the test set of\nPIE datasets 4 to qualitatively analyze our proposed model.\nThe qualitative analyses for the other two datasets are in the\nsupplemental materials. The first two top figures are in the\nsame scene, where a group of pedestrians were crossing in\nfront of the ego vehicle to get on the bus. Though the ground-\ntruth label from PIE was “not crossing” for both pedestrians,\nour model predicted “crossing”. Admittedly, the trajectories\nof the pedestrians in the first two figures are very like the\ncrossing case, and those could be considered as “crossing”\nin some sense. Moreover, these cases are very rare where\nthe training set does not have similar cases to supervise the\nmodel’s learning for this specific case. The third figure at\nthe top and the first figure at the bottom are cases where the\nmodel successfully predicts the crossing intention with low\nuncertainty. The prior one is a typical situation for the “not\ncrossing” case. However, the previous bounding boxes of the\nlater case demonstrated large lateral movements. The model\n3539\nHumanDisagreement[PSI]HumanDisagreement[PIE]\n HumanDisagreement\nAI Uncertainty\nFigure 3: Human disagreement vs. Metrics on PIE and PSI datasets [Left two sub-figures], where blue bars are the proportion of\nthe samples grouped by the uncertainty score (where 0.1 refers to the samples with uncertainty greater than 0 and less than 0.1)\nand green bars are the corresponding average uncertainty values. Each colored curve denotes the performance using a specific\nmetric. The rightmost sub-figure shows the correlation between AI uncertainty and human disagreement.\nU:0.48;P:0.76;I:X U:0.39;P:0.8;I:X U:0.1;P:0.95;I:X\nU:0.15;P:0.93;I:X U:1.0;P:0.5;I:X U:0.67;P:0.67;I:X\nFigure 4: Case study on PIE dataset, where U denotes the uncertainty, P means probability and I represents the decision ( X is\ncrossing, while the crossed X is not crossing. The red bounding boxes indicate the cases where our model predicts wrongly,\nwhile the green bounding boxes are the correct cases. The yellow bounding boxes are those in the previous 3 frames.\nrecognized the cause of the large lateral movement as from\nthe ego vehicle’s turning and predicted it correctly.\nThe last two figures in fig. 4 are also from the same sce-\nnario, where an adult takes a child to cross the street. Our\nmodel is extremely unsure about the crossing intention of\nthe child (uncertainty = 1) and gave a useless prediction\n(“crossing” = “not crossing” = 0.5). It might be because of\nthe deficiency of the samples of children because the model\npredicts the intention of the adult correctly with less uncer-\ntainty. Moreover, we examined the video clip and found that\nthe pedestrians were negotiating with the ego vehicle where\nboth sides did not carry a firm intention. However, the anno-\ntations in the PIE dataset do not show this intention dynam-\nics. Overall, we believe the model performance is limited by\nthe diversity of the training sample.\nConclusion\nIn this paper, we proposed a novel transformer-based eviden-\ntial prediction (TrEP) algorithm for pedestrian intentions,\naiming to capture the temporal correlation and model the AI\nuncertainty. We did comprehensive evaluations using three\npopular datasets for both existing and our proposed mod-\nels. Our model outperformed all the existing works on all\nthree datasets. Moreover, we utilized the crowd-sourced an-\nnotations in PIE and PSI to represent human disagreement\nand compared human disagreement with AI uncertainty. We\nfound that our model shared the same uncertainty pattern\nwith various human annotators provided in the PSI dataset.\nAcknowledgments\nThis paper is based upon work supported by the National\nScience Foundation under Grant No.2145565.\n3540\nReferences\nAliakbarian, M. S.; Saleh, F. S.; Salzmann, M.; Fernando, B.; Pe-\ntersson, L.; and Andersson, L. 2018. VIENA2: A Driving Antici-\npation Dataset. In Asian Conference on Computer Vision, 449–466.\nSpringer.\nAmini, A.; Schwarting, W.; Soleimany, A.; and Rus, D. 2020. Deep\nevidential regression. Advances in Neural Information Processing\nSystems, 33: 14927–14937.\nBao, W.; Yu, Q.; and Kong, Y . 2021. Evidential deep learning for\nopen set action recognition. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 13349–13358.\nBewley, A.; Ge, Z.; Ott, L.; Ramos, F.; and Upcroft, B. 2016. Sim-\nple online and realtime tracking. In 2016 IEEE International Con-\nference on Image Processing (ICIP), 3464–3468.\nCarreira, J.; and Zisserman, A. 2017. Quo vadis, action recog-\nnition? a new model and the kinetics dataset. In proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition,\n6299–6308.\nChen, T.; and Tian, R. 2021. A survey on deep-learning methods\nfor pedestrian behavior prediction from the egocentric view. In\n2021 IEEE International Intelligent Transportation Systems Con-\nference (ITSC), 1898–1905. IEEE.\nChen, T.; Tian, R.; Chen, Y .; Domeyer, J.; Toyoda, H.; Sherony,\nR.; Jing, T.; and Ding, Z. 2021. PSI: A Pedestrian Behavior\nDataset for Socially Intelligent Autonomous Car. arXiv preprint\narXiv:2112.02604.\nChen, T.; Tian, R.; and Ding, Z. 2021. Visual reasoning using\ngraph convolutional networks for predicting pedestrian crossing in-\ntention. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, 3103–3109.\nCui, Y .; Cao, Z.; Xie, Y .; Jiang, X.; Tao, F.; Chen, Y . V .; Li, L.;\nand Liu, D. 2022. Dg-labeler and dgl-mots dataset: Boost the au-\ntonomous driving perception. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, 58–67.\nDing, Y .; Harirchi, F.; Yong, S. Z.; Jacobsen, E.; and Ozay, N. 2018.\nOptimal input design for affine model discrimination with applica-\ntions in intention-aware vehicles. In 2018 ACM/IEEE 9th Interna-\ntional Conference on Cyber-Physical Systems (ICCPS), 297–307.\nIEEE.\nDomeyer, J. E.; Lee, J. D.; and Toyoda, H. 2020. Vehicle\nautomation–Other road user communication and coordination:\nTheory and mechanisms. IEEE Access, 8: 19860–19872.\nEriksson, A.; and Stanton, N. A. 2017. Takeover time in highly au-\ntomated vehicles: noncritical transitions to and from manual con-\ntrol. Human factors, 59(4): 689–705.\nFang, Z.; and L´opez, A. M. 2018. Is the pedestrian going to cross?\nanswering by 2d pose estimation. In2018 IEEE intelligent vehicles\nsymposium (IV), 1271–1276. IEEE.\nGujjar, P.; and Vaughan, R. 2019. Classifying pedestrian actions in\nadvance using predicted video of urban driving scenes. In 2019\nInternational Conference on Robotics and Automation (ICRA),\n2097–2103. IEEE.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y . 2021.\nTransformer in transformer. Advances in Neural Information Pro-\ncessing Systems, 34.\nHerman, M.; Wagner, J.; Prabhakaran, V .; M¨oser, N.; Ziesche, H.;\nAhmed, W.; B ¨urkle, L.; Kloppenburg, E.; and Gl ¨aser, C. 2021.\nPedestrian Behavior Prediction for Automated Driving: Require-\nments, Metrics, and Relevant Features. IEEE Transactions on In-\ntelligent Transportation Systems.\nJi, W.; Yu, S.; Wu, J.; Ma, K.; Bian, C.; Bi, Q.; Li, J.; Liu, H.;\nCheng, L.; and Zheng, Y . 2021. Learning calibrated medical im-\nage segmentation via multi-rater agreement modeling. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 12341–12351.\nJing, T.; Xia, H.; Tian, R.; Ding, H.; Luo, X.; Domeyer, J.; Sherony,\nR.; and Ding, Z. 2022. Inaction: Interpretable action decision mak-\ning for autonomous driving. In European Conference on Computer\nVision, 370–387. Springer.\nKotseruba, I.; Rasouli, A.; and Tsotsos, J. K. 2020. Do they want\nto cross? understanding pedestrian intention for behavior predic-\ntion. In 2020 IEEE Intelligent Vehicles Symposium (IV), 1688–\n1693. IEEE.\nKotseruba, I.; Rasouli, A.; and Tsotsos, J. K. 2021. Benchmark\nfor evaluating pedestrian action prediction. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision,\n1258–1268.\nLitman, T. 2017. Autonomous vehicle implementation predictions.\nVictoria Transport Policy Institute Victoria, BC, Canada.\nLiu, B.; Adeli, E.; Cao, Z.; Lee, K.-H.; Shenoi, A.; Gaidon, A.;\nand Niebles, J. C. 2020a. Spatiotemporal relationship reasoning\nfor pedestrian intent prediction. IEEE Robotics and Automation\nLetters, 5(2): 3485–3492.\nLiu, D.; Cui, Y .; Chen, Y .; Zhang, J.; and Fan, B. 2020b. Video\nobject detection for autonomous driving: Motion-aid feature cali-\nbration. Neurocomputing, 409: 1–11.\nLiu, D.; Cui, Y .; Guo, X.; Ding, W.; Yang, B.; and Chen, Y . 2021.\nVisual localization for autonomous driving: Mapping the accurate\nlocation in the city maze. In 2020 25th International Conference\non Pattern Recognition (ICPR), 3170–3177. IEEE.\nLiu, X.; Masoud, N.; Zhu, Q.; and Khojandi, A. 2022. A markov\ndecision process framework to incorporate network-level data in\nmotion planning for connected and automated vehicles. Trans-\nportation Research Part C: Emerging Technologies, 136: 103550.\nLiu, X.; Zhao, G.; Masoud, N.; and Zhu, Q. 2020c. Trajectory plan-\nning for connected and automated vehicles: Cruising, lane chang-\ning, and platooning. arXiv preprint arXiv:2001.08620.\nMa, X.; Karimpour, A.; and Wu, Y .-J. 2020. Statistical evaluation\nof data requirement for ramp metering performance assessment.\nTransportation Research Part A: Policy and Practice, 141: 248–\n261.\nMerat, N.; Jamson, A. H.; Lai, F. C.; Daly, M.; and Carsten, O. M.\n2014. Transition to manual: Driver behaviour when resuming con-\ntrol from a highly automated vehicle. Transportation research part\nF: traffic psychology and behaviour, 27: 274–282.\nPang, Y .; Guo, Z.; and Zhuang, B. 2022. ProspectNet: Weighted\nConditional Attention for Future Interaction Modeling in Behavior\nPrediction. arXiv preprint arXiv:2208.13848.\nQu, X.; Mei, Q.; Liu, P.; and Hickey, T. 2020. Using EEG to dis-\ntinguish between writing and typing for the same cognitive task.\nIn Brain Function Assessment in Learning: Second International\nConference, BFAL 2020, Heraklion, Crete, Greece, October 9–11,\n2020, Proceedings 2, 66–74. Springer.\nRasouli, A.; Kotseruba, I.; Kunic, T.; and Tsotsos, J. K. 2019. PIE:\nA Large-Scale Dataset and Models for Pedestrian Intention Esti-\nmation and Trajectory Prediction. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV).\nRasouli, A.; Kotseruba, I.; and Tsotsos, J. K. 2017. Are they going\nto cross? a benchmark dataset and baseline for pedestrian cross-\nwalk behavior. In Proceedings of the IEEE International Confer-\nence on Computer Vision Workshops, 206–213.\n3541\nRasouli, A.; Kotseruba, I.; and Tsotsos, J. K. 2020. Pedestrian ac-\ntion anticipation using contextual feature fusion in stacked rnns.\narXiv preprint arXiv:2005.06582.\nRasouli, A.; Rohani, M.; and Luo, J. 2021. Bifold and Seman-\ntic Reasoning for Pedestrian Behavior Prediction. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision\n(ICCV), 15600–15610.\nRasouli, A.; Yau, T.; Rohani, M.; and Luo, J. 2022. Multi-Modal\nHybrid Architecture for Pedestrian Action Prediction. In 2022\nIEEE Intelligent Vehicles Symposium (IV), 91–97.\nSensoy, M.; Kaplan, L.; Cerutti, F.; and Saleki, M. 2020.\nUncertainty-aware deep classifiers using generative models. In\nProceedings of the AAAI Conference on Artificial Intelligence, vol-\nume 34, 5620–5627.\nSensoy, M.; Kaplan, L.; and Kandemir, M. 2018. Evidential deep\nlearning to quantify classification uncertainty. Advances in neural\ninformation processing systems, 31.\nShi, L.; Wang, L.; Long, C.; Zhou, S.; Zhou, M.; Niu, Z.; and\nHua, G. 2021. SGCN: Sparse Graph Convolution Network for\nPedestrian Trajectory Prediction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR),\n8994–9003.\nSimonyan, K.; and Zisserman, A. 2014. Two-stream convolutional\nnetworks for action recognition in videos. Advances in neural in-\nformation processing systems, 27.\nTang, Y .; Song, S.; Gui, S.; Chao, W.; Cheng, C.; and Qin, R. 2023.\nActive and Low-Cost Hyperspectral Imaging for the Spectral Anal-\nysis of a Low-Light Environment. Sensors, 23(3): 1437.\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri, M.\n2015. Learning spatiotemporal features with 3d convolutional net-\nworks. In Proceedings of the IEEE international conference on\ncomputer vision, 4489–4497.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017a. Attention is\nall you need. Advances in neural information processing systems,\n30.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017b. Attention is\nAll you Need. In Guyon, I.; Luxburg, U. V .; Bengio, S.; Wallach,\nH.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances\nin Neural Information Processing Systems, volume 30. Curran As-\nsociates, Inc.\nWang, C.; Wang, Y .; Xu, M.; and Crandall, D. J. 2022. Stepwise\ngoal-driven networks for trajectory prediction. IEEE Robotics and\nAutomation Letters, 7(2): 2716–2723.\nWu, J.; Fang, H.; Shang, F.; Wang, Z.; Yang, D.; Zhou, W.; Yang,\nY .; and Xu, Y . 2022. Learning self-calibrated optic disc and\ncup segmentation from multi-rater annotations. arXiv preprint\narXiv:2206.05092.\nWu, J.; Fu, R.; Fang, H.; Zhang, Y .; and Xu, Y . 2023. MedSegDiff-\nV2: Diffusion based Medical Image Segmentation with Trans-\nformer. arXiv preprint arXiv:2301.11798.\nXu, T.; Chen, W.; Pichao, W.; Wang, F.; Li, H.; and Jin, R. 2021.\nCDTrans: Cross-domain Transformer for Unsupervised Domain\nAdaptation. In International Conference on Learning Represen-\ntations.\nXu, Y .; Piao, Z.; and Gao, S. 2018. Encoding Crowd Interaction\nWith Deep Neural Network for Pedestrian Trajectory Prediction.\nIn Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nYagi, T.; Mangalam, K.; Yonetani, R.; and Sato, Y . 2018. Fu-\nture Person Localization in First-Person Videos. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR).\nYang, D.; Zhang, H.; Yurtsever, E.; Redmill, K.; and Ozguner, U.\n2022. Predicting pedestrian crossing intention with feature fusion\nand spatio-temporal attention. IEEE Transactions on Intelligent\nVehicles.\nYao, Y .; Atkins, E.; Johnson-Roberson, M.; Vasudevan, R.; and Du,\nX. 2021. BiTraP: Bi-Directional Pedestrian Trajectory Prediction\nWith Multi-Modal Goal Estimation. IEEE Robotics and Automa-\ntion Letters, 6(2): 1463–1470.\nYi, L.; and Qu, X. 2022. Attention-Based CNN Capturing EEG\nRecording’s Average V oltage and Local Change. InArtificial Intel-\nligence in HCI: 3rd International Conference, AI-HCI 2022, Held\nas Part of the 24th HCI International Conference, HCII 2022, Vir-\ntual Event, June 26–July 1, 2022, Proceedings, 448–459. Springer.\nZeng, Z.; Zhao, W.; Qian, P.; Zhou, Y .; Zhao, Z.; Chen, C.; and\nGuan, C. 2021. Robust Traffic Prediction From Spatial–Temporal\nData Based on Conditional Distribution Learning. IEEE Transac-\ntions on Cybernetics, 52(12): 13458–13471.\nZhang, P.; Ouyang, W.; Zhang, P.; Xue, J.; and Zheng, N. 2019.\nSR-LSTM: State Refinement for LSTM Towards Pedestrian Tra-\njectory Prediction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR).\nZhang, S.; Abdel-Aty, M.; Wu, Y .; and Zheng, O. 2021a. Pedes-\ntrian crossing intention prediction at red-light using pose estima-\ntion. IEEE Transactions on Intelligent Transportation Systems,\n23(3): 2331–2339.\nZhang, Z.; Shen, D.; Tian, R.; Li, L.; Chen, Y .; Sturdevant, J.; and\nCox, E. 2021b. Implementation and Performance Evaluation of In-\nvehicle Highway Back-of-Queue Alerting System Using the Driv-\ning Simulator. In 2021 IEEE International Intelligent Transporta-\ntion Systems Conference (ITSC), 1753–1759.\nZhang, Z.; Tian, R.; and Duffy, V . G. 2023. Trust in Automated\nVehicle: A Meta-Analysis, 221–234. Cham: Springer International\nPublishing. ISBN 978-3-031-10784-9.\nZhang, Z.; Tian, R.; Duffy, V . G.; and Li, L. 2022a. The Comfort of\nthe Soft-Safety Driver Alerts: Measurements and Evaluation. In-\nternational Journal of Human–Computer Interaction, 0(0): 1–11.\nZhang, Z.; Tian, R.; Elahi, F. M.; Luo, X.; Domeyer, J.; and\nSherony, R. 2022b. Modeling Pedestrian Situated Intent in Dy-\nnamic Driving Scenes from the Driver’s Perspective. Available at\nSSRN 4281923.\nZhang, Z.; Tian, R.; Sherony, R.; Domeyer, J.; and Ding, Z.\n2022c. Attention-Based Interrelation Modeling for Explainable\nAutomated Driving. IEEE Transactions on Intelligent Vehicles.\n3542",
  "topic": "Pedestrian",
  "concepts": [
    {
      "name": "Pedestrian",
      "score": 0.8048890829086304
    },
    {
      "name": "Transformer",
      "score": 0.6927434802055359
    },
    {
      "name": "Artificial intelligence",
      "score": 0.661489725112915
    },
    {
      "name": "Computer science",
      "score": 0.6118561029434204
    },
    {
      "name": "Machine learning",
      "score": 0.5432641506195068
    },
    {
      "name": "Pedestrian detection",
      "score": 0.43636566400527954
    },
    {
      "name": "Engineering",
      "score": 0.26897311210632324
    },
    {
      "name": "Transport engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I55769427",
      "name": "Indiana University – Purdue University Indianapolis",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114832834",
      "name": "Tulane University",
      "country": "US"
    }
  ],
  "cited_by": 51
}