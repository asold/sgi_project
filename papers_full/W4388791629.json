{
  "title": "Application and accuracy of artificial intelligence-derived large language models in patients with age related macular degeneration",
  "url": "https://openalex.org/W4388791629",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2794787477",
      "name": "Lorenzo Ferro Desideri",
      "affiliations": [
        "University of Bern",
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2551381938",
      "name": "Janice Roth",
      "affiliations": [
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2646453899",
      "name": "Martin Zinkernagel",
      "affiliations": [
        "University of Bern",
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A1984988652",
      "name": "Rodrigo Anguita",
      "affiliations": [
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2794787477",
      "name": "Lorenzo Ferro Desideri",
      "affiliations": [
        "University Hospital of Bern",
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2551381938",
      "name": "Janice Roth",
      "affiliations": [
        "University Hospital of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2646453899",
      "name": "Martin Zinkernagel",
      "affiliations": [
        "University Hospital of Bern",
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A1984988652",
      "name": "Rodrigo Anguita",
      "affiliations": [
        "University Hospital of Bern",
        "Moorfields Eye Hospital NHS Foundation Trust"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3200699924",
    "https://openalex.org/W3135840606",
    "https://openalex.org/W4324020464",
    "https://openalex.org/W3083027974",
    "https://openalex.org/W1997293755",
    "https://openalex.org/W2946661638",
    "https://openalex.org/W6762316754",
    "https://openalex.org/W2918145471",
    "https://openalex.org/W2898192966",
    "https://openalex.org/W4290613782",
    "https://openalex.org/W3022060579",
    "https://openalex.org/W4386776401",
    "https://openalex.org/W4386861318",
    "https://openalex.org/W4386726071",
    "https://openalex.org/W4379209901",
    "https://openalex.org/W4367834585",
    "https://openalex.org/W2138848349",
    "https://openalex.org/W2515104264",
    "https://openalex.org/W1778362155",
    "https://openalex.org/W3110054227",
    "https://openalex.org/W4386209519",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4385718801",
    "https://openalex.org/W4385514573",
    "https://openalex.org/W4389049692",
    "https://openalex.org/W4379509109"
  ],
  "abstract": "Abstract Introduction Age-related macular degeneration (AMD) affects millions of people globally, leading to a surge in online research of putative diagnoses, causing potential misinformation and anxiety in patients and their parents. This study explores the efficacy of artificial intelligence-derived large language models (LLMs) like in addressing AMD patients' questions. Methods ChatGPT 3.5 (2023), Bing AI (2023), and Google Bard (2023) were adopted as LLMs. Patients’ questions were subdivided in two question categories, (a) general medical advice and (b) pre- and post-intravitreal injection advice and classified as (1) accurate and sufficient (2) partially accurate but sufficient and (3) inaccurate and not sufficient. Non-parametric test has been done to compare the means between the 3 LLMs scores and also an analysis of variance and reliability tests were performed among the 3 groups. Results In category a) of questions, the average score was 1.20 (± 0.41) with ChatGPT 3.5, 1.60 (± 0.63) with Bing AI and 1.60 (± 0.73) with Google Bard, showing no significant differences among the 3 groups (p = 0.129). The average score in category b was 1.07 (± 0.27) with ChatGPT 3.5, 1.69 (± 0.63) with Bing AI and 1.38 (± 0.63) with Google Bard, showing a significant difference among the 3 groups (p = 0.0042). Reliability statistics showed Chronbach’s α of 0.237 (range 0.448, 0.096–0.544). Conclusion ChatGPT 3.5 consistently offered the most accurate and satisfactory responses, particularly with technical queries. While LLMs displayed promise in providing precise information about AMD; however, further improvements are needed especially in more technical questions.",
  "full_text": "Ferro Desideri et al. \nInternational Journal of Retina and Vitreous  (2023) 9:71 \nhttps://doi.org/10.1186/s40942-023-00511-7\nORIGINAL ARTICLE\nApplication and accuracy of artificial \nintelligence-derived large language models \nin patients with age related macular \ndegeneration\nLorenzo Ferro Desideri1,2*, Janice Roth1, Martin Zinkernagel1,2 and Rodrigo Anguita1,3 \nAbstract \nIntroduction Age-related macular degeneration (AMD) affects millions of people globally, leading to a surge \nin online research of putative diagnoses, causing potential misinformation and anxiety in patients and their parents. \nThis study explores the efficacy of artificial intelligence-derived large language models (LLMs) like in addressing AMD \npatients’ questions.\nMethods ChatGPT 3.5 (2023), Bing AI (2023), and Google Bard (2023) were adopted as LLMs. Patients’ questions were \nsubdivided in two question categories, (a) general medical advice and (b) pre- and post-intravitreal injection advice \nand classified as (1) accurate and sufficient (2) partially accurate but sufficient and (3) inaccurate and not sufficient. \nNon-parametric test has been done to compare the means between the 3 LLMs scores and also an analysis of vari-\nance and reliability tests were performed among the 3 groups.\nResults In category a) of questions, the average score was 1.20 (± 0.41) with ChatGPT 3.5, 1.60 (± 0.63) with Bing AI \nand 1.60 (± 0.73) with Google Bard, showing no significant differences among the 3 groups (p = 0.129). The average \nscore in category b was 1.07 (± 0.27) with ChatGPT 3.5, 1.69 (± 0.63) with Bing AI and 1.38 (± 0.63) with Google Bard, \nshowing a significant difference among the 3 groups (p = 0.0042). Reliability statistics showed Chronbach’s α of 0.237 \n(range 0.448, 0.096–0.544).\nConclusion ChatGPT 3.5 consistently offered the most accurate and satisfactory responses, particularly with techni-\ncal queries. While LLMs displayed promise in providing precise information about AMD; however, further improve-\nments are needed especially in more technical questions.\nKeywords LLMs, Large language models, Artificial Intelligence, Artificial intelligence in ophthalmology, Macular \nedema, Wet macular degeneration, Dry macular degeneration\nIntroduction\nAge-related macular degeneration (AMD) represents a \nleading cause of visual loss affecting around 200 million \npeople worldwide and its prevalence is steadily increas -\ning [1]. In 2040 AMD prevalence is expected to raise up \nto 288 million people worldwide [2].\nGiven this alarming epidemiological data, AMD \nrepresent an important social and economic burden; \nOpen Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nInternational Journal\nof Retina and Vitreous\n*Correspondence:\nLorenzo Ferro Desideri\nlorenzoferrodes@gmail.com\n1 Department of Ophthalmology, Inselspital, University Hospital of Bern, \nBern, Switzerland\n2 Bern Photographic Reading Center, Inselspital, Bern University Hospital, \nUniversity of Bern, Bern, Switzerland\n3 Moorfields Eye Hospital NHS Foundation Trust, City Road, London EC1V \n2PD, UK\nPage 2 of 6Ferro Desideri et al. International Journal of Retina and Vitreous  (2023) 9:71\nnonetheless, a growing trend of AMD patients seek -\ning diagnosis online is expected and this scenario poses \na multifaceted challenge [3]. This represents a social \nissue as it can lead to misinformation and unnecessary \nanxiety for patients. In fact, many patients affected with \nAMD often seek online answers about their disease, the \npossible treatment options, and their visual prognosis, \nbut often the information reported can be wrong, inac -\ncurate, and sometimes misleading [4]. Addressing this \nissue requires promoting digital health literacy, offering \nreliable online resources, and educating patients on the \nsignificance of consulting healthcare professionals for \naccurate diagnosis and proper care [5, 6]. An integrated \napproach is essential to harness the benefits of digitaliza -\ntion while mitigating its challenges in healthcare [7].\nIn recent years, there has been a significant increase \nin the use of artificial intelligence (AI) in healthcare sec -\ntor and in ophthalmological field [8]. This growth is due \nin part to the advancements in AI subfields such as data \nvisualization, speech recognition, and natural language \nprocessing, which facilitates patients to access clini -\ncal information through large language models (LLMs) \n[9]. LLMs are AI- derived models trained extensively \non text data using deep learning (DL) techniques and \nthey are capable to understand and replicate human-\nlike responses by analyzing patterns and context in their \ntraining data. LLMs are adept at generating relevant \nresponses to a wide range of prompts or questions [10].\nRecent studies have investigated the role of LLMs in \ngenerating reliable information for the patients with sev -\neral ophthalmological diseases, including uveitis, ocular \ntumors, glaucoma, and others [11–14]. A recent study \nshowed the potential of ChatGPT 3.5 in creating ophthal-\nmic discharge summaries and operative notes, conclud -\ning that an adequate training of LLMs on these task with \nhuman verification may have a positive impact on health-\ncare [15].\nIn this study, we tasked with responding 3 of the most \ncommon LLMs with the most frequent questions of \npatients with AMD. The aim of this study is to assess the \naccuracy and feasibility of LLMs in addressing patients \nwith AMD and helping them to acquire more validated \ninformation about their health status condition, prog -\nnosis, and doubts regarding their available treatment \noptions.\nMethods\nIn our investigation into the quality and reliability of \ninformation provided by LLMs. In this study the authors \nselected three of the most widely used and freely available \nLLMs, all of which were posed with the most common \nquestions formulated by patients suffering from AMD. \nThe LLMs under scrutiny were ChatGPT 3.5 (2023) by \nOpenAI, Bing AI (2023) powered by GPT-4 (2023) and \ndeveloped by Microsoft, and Google Bard by Google. To \nsystematically assess their performance, we elaborated a \nset of questions, dividing them into two distinct catego -\nries: 15 questions related to medical advice and the most \ncommon questions of patients, as outlined in Table 1, and \n13 technical questions regarding pre- and post-intravit -\nreal injections advice, detailed in Table 2. \nThe responses generated by these LLMs were discussed \nand evaluated after common agreement by three expe -\nrienced retina specialists (with at least 8  years of clini -\ncal experience). Their evaluations led to categorizations \nbased on accuracy and sufficiency. Responses were classi-\nfied as ’Accurate and Sufficient’ if they were both correct \nand comprehensive. ’Partially Accurate and Sufficient’ \nwas assigned when responses contained minor inaccu -\nracies but still provided substantial and understandable \ninformation. Lastly, ’Inaccurate’ denoted answers that \nwere entirely incorrect or contained critical errors ren -\ndering them unreliable.\nStatistical analysis was conducted by using the SPSS \nprogram (IBM SPSS Statistics, version 25). Descriptive \nanalysis (including frequency, means and standard devia -\ntion) and normality distribution test (Shapiro–Wilk) have \nbeen done. A non-parametric Kruskal–Wallis test has \nbeen subsequently performed, given the abnormal distri -\nbution of the data, to compare average scores across the \nthree LLMs. Reliability test was also performed by meas -\nuring Cronbach α coefficient. A p-value of less than 0.05 \nwas considered statistically significant.\nResults\nIn the group of medical advice general questions, Chat -\nGPT 3.5 showed that 80.0% (n = 12) of the response \nwere classified as accurate and sufficient and the remain -\ning 20% (n = 3) as partially accurate and sufficient. Bing \nAI reported 46.7% (n = 7) of the response classified as \naccurate and sufficient and another 46.7% classified as \npartially accurate and sufficient, while only 6.7% (n = 1) \nwere reputed inaccurate and insufficient. Google Bard \nwas referred with 53.3% (n = 8) of the answers accurate \nand sufficient, 33.3% (n = 5) as partially accurate and suf -\nficient and the remaining 13.3% (n = 2) were inaccurate \nand insufficient (Fig.  1). In this first group of question, \nthe average score was 1.20 (± 0.41) with ChatGPT 3.5, \n1.60 (± 0.63) with Bing AI and 1.60 (± 0.73) with Google \nBard, showing no significant differences among the 3 \ngroups (p = 0.129).\nIn the second group of questions (pre- and post-\nintravitreal injections advice questions), ChatGPT 3.5 \nanswered 76.9% (n = 10) of the questions accurately and \nsufficiently and 23.1% (n = 3) partially accurately and \nsufficiently. Differently, Bing AI showed 30.8% (n = 4) of \nPage 3 of 6\nFerro Desideri et al. International Journal of Retina and Vitreous  (2023) 9:71\n \nthe response as accurate and sufficient, 61.5% (n = 8) of \nthem as partially accurate and sufficient and the remain -\ning 7.7% (n = 1) of them as inaccurate and insufficient. \nGoogle Bard answered accurately and sufficiently in \n75.0% (n = 9) of the questions, partially accurately and \nsufficiently in 8.3% (n = 1) of them and inaccurately and \ninsufficiently in the remaining 26.7% (n = 2). The average \nscore was 1.07 (± 0.27) with ChatGPT 3.5, 1.69 (± 0.63) \nwith Bing AI and 1.38 (± 0.63) with Google Bard, showing \na significant difference among the 3 groups (p = 0.0042).\nReliability statistics showed Chronbach’s α of 0.237 \n(range 0.448, 0.096–0.544), indicating an overall low \nagreement between the 3 LLMs.\nDiscussion\nOur research offers a comprehensive assessment of Chat-\nGPT 3.5, Bing AI, and Google Bard in their ability to \nrespond effectively to commonly asked questions about \nAMD from patients or their parents. To improve the \nintegrity of our evaluation, the Chatbot LLMs-generated \nresponses were thoroughly reviewed by 3 distinct experi -\nenced retina specialists. Our results showed that on aver -\nage these 3 LLMs have the potential to provide accurate \nanswers to AMD-related queries; however, the relatively \nlow results in reliability test showed a relatively low level \nof agreement between the 3 LLMs. Our results empha -\nsize that ChatGPT 3.5 consistently performed well in \nproviding accurate and sufficient information, particu -\nlarly excelling in technical questions related to pre- and \npost-intravitreal injections. Nonetheless, no response \nfrom ChatGPT 3.5 were characterized as inaccurate and \ninsufficient. Differently, Bing AI displayed mixed perfor -\nmance, while Google Bard showed strength in certain \naspects but also exhibited some inaccuracies. Although \nChatGPT 3.5 has outperformed the other 2 LLMs in \nterms of accuracy and reliability of the answers, our \nfindings suggest that LLMs still give different levels of \nperformance and they cannot still be considered inter -\nchangeable tools in the providing accurate information \nfor patients with AMD.\nTable 1 Medical advice general questions in patients with macular degeneration\na Lilliefors Significance Correction\n1 = Accurate and sufficient\n2 = Partially accurate and sufficient\n3 = Inaccurate and insufficient\nQuestions ChatGPT Bing AI Google Bard\n1. How common is AMD? 1 1 1\n2. In a patient with established diagnosis of AMD, what is the chance the other eye is affected \nwith AMD?\n2 1 3\n3. What is the underlying cause of AMD? 1 2 2\n4. Is AMD inherited? 1 1 1\n5. What is dry AMD? 1 2 1\n6. What is wet AMD? 1 1 1\n7. What are the chances it converts into wet AMD? 2 1 2\n8. What is the best treatment for dry and wet AMD? 2 2 3\n9. How can I know that my dry AMD converted into wet AMD? What are the symptoms? 1 2 1\n10. How can I test myself for AMD? How often should I perform Amsler grid examination? 1 1 2\n11. I have been diagnosed with AMD. Are there any eyeglasses or contact lenses I can wear \nto improve my condition?\n1 2 2\n12. How can I slow down AMD progression naturally? 1 3 1\n13. Will I lose vision/go blind? 1 1 1\n14. Do vitamins and oral nutritional supplements help for AMD? 1 2 1\n15. What happens if AMD is left untreated? 1 2 2\n16. Can I drive with AMD? 1 1 1\nTests of Normality\nKolmogorov-Smirnova Sharpiro-Wilk\nStatistic df Sig Statistic df Sig\nChatGPT ,492 16 ,000 ,484 16 ,000\nBingAl ,314 16 ,000 ,750 16 ,001\nGoogleBard ,343 16 ,000 ,738 16 ,000\nPage 4 of 6Ferro Desideri et al. International Journal of Retina and Vitreous  (2023) 9:71\nTo the best of our knowledge, this the first study to \ninvestigate the utility of LLMs focusing specifically \non addressing patients with AMD with general ques -\ntions on technical questions on pre-and post-oper -\native management. We found that LLMs may provide \na promising supportive role to patients, which may be \nsometimes lost and confused about their condition, its \nmanagement, treatment options and prognosis. It has \nbeen widely reported that patient’s satisfaction is highly \ndependent on an appropriate information regard -\ning their condition [16]; however, previous studies \nhave reported that the online information about oph -\nthalmological conditions may be often inaccurate and \nmisleading [17, 18]. Nowadays, we are presented with \nsignificant worldwide challenges and prospects as a \nresult of several factors: the global population is grow -\ning with a shift to an aging demographic, diagnostic \ncapabilities are improving, and treatment options are \nTable 2 Pre- and post-intravitreal injections advice questions in patients with macular degeneration\n1 = Accurate and sufficient\n2 = Partially accurate and sufficient\n3 = Inaccurate and insufficient\nQuestions ChatGPT Bing AI Google Bard\n1. What is it an intravitreal injection? 1 2 1\n2. What are the risks associated with intravitreal injections? 1 2 1\n3. How do these anti-VEGF agents work? Do they treat only the wet AMD form? 1 2 2\n4. Are there any medications against the dry form? How do they work? 2 1 3\n5. I have problems to come every month to the hospital for AMD intravitreal injections. Are there any drugs allow-\ning me a more extended treatment interval?\n1 2 3\n7. Should I take any medicaments after the intravitreal injection? For how long? 1 2 1\n8. What should avoid doing after intravitreal injection? 1 2 1\n9. Can I exercise and/or lift objects after intravitreal injection? And can I go swimming? 1 2 1\n10. Can I wear my contact lenses after anti-VEGF injection? 1 3 1\n11. I see a mobile bubble moving in the visual field since I have been injected. Should I worry about that? 1 2 1\n12. After the anti-VEGF intravitreal injection, a large blood effusion has appeared in my conjunctiva. This blood \neffusion is really scaring me. What can it be? What should I do?\n1 1 1\n13. My eye keeps on tearing after the anti-VEGF intravitreal injection and it still seems to be reddened. What can it \nbe? What should I do?\n1 1 1\n14. I have been injected some days ago with an anti-VEGF intravitreal injection. Now I feel severe pain in my eye, \nwhich is reddened, and I noticed a severe visual impairment. What can it be? What should I do?\n1 1 1\nFig. 1 Accuracy of response among the 3 Chatbot large language models in patients with macular degeneration\nPage 5 of 6\nFerro Desideri et al. International Journal of Retina and Vitreous  (2023) 9:71\n \nexpanding [19]. Considering the increasing requests, \nophthalmologists may not always be readily accessible, \nin contrast with internet and Chatbot LLMs platforms, \nwhich are already widely used by the global community \n[20].\nChatGPT 3.5, BingAI and Bard, accounting as 3 of the \nmost prominent LLMs are AI-based services that can \nbe easily accessed via internet. These LLMs have been \ndeveloped in a way allowing to understand and respond \nto user questions and instructions. Furthermore, they \nhave been extensively trained on diverse text sources, \nincluding articles, books, and websites, enabling them \nto generate responses that mimic human language when \nprompted [21].\nIn this scenario, LLMs offer the advantage of acces -\nsibility, allowing patients to quickly access information \nand obtain answers at their convenience, a particularly \nsignificant advantage in remote or isolated areas, and in \nsome cases translating medical information into patients’ \nnative languages [10, 15, 22]. Additionally, responses gen-\nerated by LLMs are more comprehensible than medical \njargon, further enhancing their utility [23].\nA previous study evaluated the general responses \ngenerated by ChatGPT 3.5 regarding different retinal \ndiseases, including AMD, central serous chorioretin -\nopathy and retinal vein occlusions. They rated 45% of \nthe LLM-generated answers as very good, 26% as minor \nnon-harmful inaccurate and only 17% as markedly misin-\nterpretable [3]. In another study published by Anguita et \nal., LLMs were shown to potentially play a beneficial role \nin vitreoretinal care, also if proper patient education on \ntheir use is still needed [12].\nAnother study evaluated the accuracy of GTP at diag -\nnosing glaucoma based on specific clinical case descrip -\ntions with comparison to the performance of senior \nophthalmology resident trainees. In this study, ChatGPT \n3.5 demonstrated a diagnostic accuracy of 72.7% when \ndiagnosing primary and secondary glaucoma cases, out -\nperforming some senior ophthalmology residents who \nachieved an accuracy of 54.5% to 72.7%. These findings \nsuggested that ChatGPT 3.5 has the potential to assist in \nclinical settings for efficient and objective glaucoma diag-\nnoses, particularly in primary care offices and eye care \npractices [13].\nAnother study evaluated the capacity of ChatGPT 3.5 \nto improve the readability of patient-targeted health \ninformation on uveitis. ChatGPT 3.5 generated responses \nwith significantly lower Flesch Kincaid Grade Level \nscores and fewer complex words when asked to sim -\nplify the language, making the content more accessible \nto the average American reader. The findings suggested \nthat ChatGPT 3.5 has the potential to assist healthcare \nprofessionals in creating more understandable uveitis \ninformation for patients and enhancing the overall acces-\nsibility of healthcare content [11].\nFurthermore, it might be important to question that \nreadability and simplifying language might come at the \ncost of accuracy of information. It should be further \ninvestigated if ChatGPT3.5 and the others 2 LLMs can \ncorrectly decide which part of the information should be \nomitted and accurately translate medical knowledge to \nsimple terms without compromising the facts.\nNonetheless, some limitations are present in the study \nincluding the relative low sample of tasks for LLMs and \nthe adoption of only 3 LLMs. Further studies should \ninvestigate the applicability of other advanced LLMs, \nincluding ChatGPT 4.0, with a larger sample of tasks in \npatients with AMD.\nIn a healthcare landscape where accessibility and \npatient education are crucial, LLMs offer a valuable tool, \nbridging communication gaps and providing understand-\nable medical information. This study contributes to the \ngrowing body of evidence highlighting LLMs’ utility in \nhealthcare, particularly in addressing specific patient \nqueries within the context of AMD.\nConclusion\nThe future integration of Chatbots LLMs into the oph -\nthalmologists’ daily clinical practice may represent a \npriceless opportunity for both eye specialists and patients \nwith AMD. Our study showed that ChatGPT 3.5 consist -\nently offered the most accurate responses, particularly \nwith technical queries. Overall the 3 LLMs displayed \npromise in providing precise information about AMD; \nhowever, further improvements are warranted especially \nin more technical questions. Future, larger-scale, and \nreal-life studies, possibly adopting questionnaire directly \ninterrogating patients’ satisfaction and feasibility to adopt \nLLMs in their everyday life, may address us on the reach \nof these novel AI-tools to improve patients and physi -\ncians’ life.\nAcknowledgements\nNone\nAuthor contributions\nAll authors contributed to manuscript preparation. LFD and RA did conceptu-\nalization of the study. Methodology, software and validation and data curation \nwere done by RA, MZ and JR. MZ did formal analysis. LFD and RA did investiga-\ntion. Resources and funding acquisition were provided by MZ. supervision and \nconceptualization of the study, JR did analysis of the data. LFD did writing- \noriginal draft and RA and MZ did writing-review and editing. MZ and RA did \nvisualization and supervision.\nFunding\nThis research was self-funded and did not receive specific grant from any \nfunding agency in the public, commercial or not-for-profit sectors.\nAvailability of data and materials\nAll data are available and kept in Inselspital protected database and.\nPage 6 of 6Ferro Desideri et al. International Journal of Retina and Vitreous  (2023) 9:71\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nDeclarations\nEthics approval and consent to participate\nAn ethical approval involving the adoption of AI in retinal diseases has been \nobtained at Inselspital, Bern (Switzerland). No patients ‘consent is needed since \nthe study did not involve human subjects. Was sul.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors report no competing interests.\nReceived: 13 October 2023   Accepted: 11 November 2023\nPublished: 18 November 2023\nReferences\n 1. Schultz NM, Bhardwaj S, Barclay C, et al. Global Burden of dry age-\nrelated macular degeneration: a targeted literature review. Clin Ther. \n2021;43(10):1792–818.\n 2. Deng Y, Qiao L, Du M, et al. Age-related macular degeneration: epidemi-\nology, genetics, pathophysiology, diagnosis, and targeted therapy. Genes \nDis. 2022;9(1):62–79.\n 3. Potapenko I, Boberg-Ans LC, Stormly Hansen M, et al. Artificial intelli-\ngence-based chatbot patient information on common retinal diseases \nusing ChatGPT. Acta Ophthalmol. 2023. https:// doi. org/ 10. 1111/ aos. \n15661.\n 4. Li JO, Liu H, Ting DSJ, et al. Digital technology, tele-medicine and artificial \nintelligence in ophthalmology: a global perspective. Prog Retin Eye Res. \n2021;82: 100900.\n 5. Kaiser PK, Wang YZ, He YG, et al. Feasibility of a novel remote daily \nmonitoring system for age-related macular degeneration using mobile \nhandheld devices: results of a pilot study. Retina. 2013;33(9):1863–70.\n 6. Mathews SC, McShea MJ, Hanley CL, et al. Digital health: a path to valida-\ntion. NPJ Digit Med. 2019;2:38.\n 7. Buchan JC, Norman P , Shickle D, et al. Failing to plan and planning to fail. \nCan we predict the future growth of demand on UK Eye Care Services? \nEye. 2019;33(7):1029–31.\n 8. Ting DSW, Pasquale LR, Peng L, et al. Artificial intelligence and deep learn-\ning in ophthalmology. Br J Ophthalmol. 2019;103(2):167–75.\n 9. Chen JS, Baxter SL. Applications of natural language processing in oph-\nthalmology: present and future. Front Med. 2022;9: 906554.\n 10. Zand A, Sharma A, Stokes Z, et al. An exploration into the use of a chatbot \nfor patients with inflammatory bowel diseases: retrospective cohort \nstudy. J Med Internet Res. 2020;22(5): e15589.\n 11. Kianian R, Sun D, Crowell EL, et al. The use of large language models to \ngenerate education materials about uveitis. Ophthalmol Retina. 2023. \nhttps:// doi. org/ 10. 1016/j. oret. 2023. 09. 008.\n 12. Anguita R, Makuloluwa A, Hind J, et al. Large language models in vitreo-\nretinal surgery. Eye. 2023. https:// doi. org/ 10. 1038/ s41433- 023- 02751-1.\n 13. Delsoz M, Raja H, Madadi Y, et al. The use of ChatGPT to assist in diagnos-\ning glaucoma based on clinical case reports. Ophthalmol Ther. 2023. \nhttps:// doi. org/ 10. 1007/ s40123- 023- 00805-x.\n 14. Momenaei B, Wakabayashi T, Shahlaee A, et al. Appropriateness and read-\nability of ChatGPT-4-generated responses for surgical treatment of retinal \ndiseases. Ophthalmol Retina. 2023;7(10):862–8.\n 15. Singh S, Djalilian A, Ali MJ. ChatGPT and ophthalmology: exploring its \npotential with discharge summaries and operative notes. Semin Ophthal-\nmol. 2023;38(5):503–7.\n 16. Britten N, Stevenson FA, Barry CA, et al. Misunderstandings in \nprescribing decisions in general practice: qualitative study. BMJ. \n2000;320(7233):484–8.\n 17. Borgersen NJ, Henriksen MJ, Konge L, et al. Direct ophthalmoscopy on \nYouTube: analysis of instructional YouTube videos’ content and approach \nto visualization. Clin Ophthalmol. 2016;10:1535–41.\n 18. Subhi Y, Bube SH, RolskovBojsen S, et al. Expert involvement and adher-\nence to medical evidence in medical mobile phone apps: a systematic \nreview. JMIR Mhealth Uhealth. 2015;3(3): e79.\n 19. Blindness GBD. Vision impairment C, vision loss expert group of the \nGlobal Burden of Disease S. Trends in prevalence of blindness and dis-\ntance and near vision impairment over 30 years: an analysis for the Global \nBurden of Disease Study. Lancet Glob Health. 2021;9(2):130–43.\n 20. Biswas S, Logan NS, Davies LN, et al. Assessing the utility of ChatGPT as \nan artificial intelligence-based large language model for information to \nanswer questions on myopia. Ophthalmic Physiol Opt. 2023. https:// doi. \norg/ 10. 1111/ opo. 13227.\n 21. Sallam M. ChatGPT utility in healthcare education, research, and practice: \nsystematic review on the promising perspectives and valid concerns. \nHealthcare (Basel). 2023. https:// doi. org/ 10. 3390/ healt hcare 11060 887.\n 22. Khanna RK, Ducloyer JB, Hage A, et al. Evaluating the potential of Chat-\nGPT-4 in ophthalmology: the good, the bad and the ugly. J Fr Ophtalmol. \n2023;46(7):697–705.\n 23. Caranfa JT, Bommakanti NK, Young BK, et al. Accuracy of vitreoretinal \ndisease information from an artificial intelligence Chatbot. JAMA Oph-\nthalmol. 2023. https:// doi. org/ 10. 1001/ jamao phtha lmol. 2023. 3314.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.8025925159454346
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.44746312499046326
    },
    {
      "name": "Psychology",
      "score": 0.40748605132102966
    },
    {
      "name": "Medicine",
      "score": 0.40397414565086365
    },
    {
      "name": "Demography",
      "score": 0.38425737619400024
    },
    {
      "name": "Clinical psychology",
      "score": 0.32343918085098267
    },
    {
      "name": "Computer science",
      "score": 0.2387373447418213
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ]
}