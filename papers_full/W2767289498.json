{
  "title": "Dual Language Models for Code Mixed Speech Recognition",
  "url": "https://openalex.org/W2767289498",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2122846389",
      "name": "Saurabh Garg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2624127707",
      "name": "Tanmay Parekh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2088806615",
      "name": "Preethi Jyothi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2094655846",
    "https://openalex.org/W2099808146",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2407467516",
    "https://openalex.org/W2802201485",
    "https://openalex.org/W2153433699",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2031292349",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2002342963",
    "https://openalex.org/W2117671523",
    "https://openalex.org/W2013489815",
    "https://openalex.org/W2114569717",
    "https://openalex.org/W2536834305",
    "https://openalex.org/W2034585809",
    "https://openalex.org/W2123162330"
  ],
  "abstract": "In this work, we present a simple and elegant approach to language modeling for bilingual code-switched text. Since code-switching is a blend of two or more different languages, a standard bilingual language model can be improved upon by using structures of the monolingual language models. We propose a novel technique called dual language models, which involves building two complementary monolingual language models and combining them using a probabilistic model for switching between the two. We evaluate the efficacy of our approach using a conversational Mandarin-English speech corpus. We prove the robustness of our model by showing significant improvements in perplexity measures over the standard bilingual language model without the use of any external information. Similar consistent improvements are also reflected in automatic speech recognition error rates.",
  "full_text": "Dual Language Models for\nCode Switched Speech Recognition\nSaurabh Garg, Tanmay Parekh, Preethi Jyothi\nDepartment of Computer Science and Engineering, Indian Institute of Technology Bombay\n{saurabhgarg,tanmayb,pjyothi}@cse.iitb.ac.in\nAbstract\nIn this work, we present a simple and elegant approach to lan-\nguage modeling for bilingual code-switched text. Since code-\nswitching is a blend of two or more different languages, a stan-\ndard bilingual language model can be improved upon by using\nstructures of the monolingual language models. We propose\na novel technique called dual language models, which involves\nbuilding two complementary monolingual language models and\ncombining them using a probabilistic model for switching be-\ntween the two. We evaluate the efﬁcacy of our approach using a\nconversational Mandarin-English speech corpus. We prove the\nrobustness of our model by showing signiﬁcant improvements\nin perplexity measures over the standard bilingual language\nmodel without the use of any external information. Similar\nconsistent improvements are also reﬂected in automatic speech\nrecognition error rates.\nIndex Terms: Code-switching, language modeling, speech\nrecognition\n1. Introduction\nCode-switching is a commonly occurring phenomenon in multi-\nlingual communities, wherein a speaker switches between lan-\nguages within the span of a single utterance. Code-switched\nspeech presents many challenges for automatic speech recogni-\ntion (ASR) systems, in the context of both acoustic models and\nlanguage models. Mixing of dissimilar languages leads to loss\nof structure, which makes the task of language modeling more\ndifﬁcult. Our focus in this paper is on building robust language\nmodels for code-switched speech from bilingual speakers.\nA naïve approach towards this problem would be to sim-\nply use a bilingual language model. However, the complex-\nity of a full-ﬂedged bilingual language model is signiﬁcantly\nhigher than that of two monolingual models, and is unsuitable\nin a limited data setting. More sophisticated approaches rely-\ning on translation models have been proposed to overcome this\nchallenge (see Section 2), but they rely on external resources\nto build the translation model. In this paper, we introduce an\nalternate – and simpler – approach to address the challenge of\nlimited data in the context of code-switched text without use of\nany external resources.\nAt the heart of our solution is adual language model(DLM)\nthat has roughly the complexity of two monolingual language\nmodels combined. A DLM combines two such models and\nuses a probabilistic model to switch between them. Its sim-\nplicity makes it amenable for generalization in a low-data con-\ntext. Further there are several other beneﬁts of using DLMs. (1)\nThe DLM construction does not rely on any prior information\nabout the underlying languages. (2) Since the structure of our\ncombined model is derived from monolingual language mod-\nels, it can be implemented as a ﬁnite-state machine and easily\nincorporated within an ASR system. (3) The monolingual lan-\nguage model for the primary language can be trained further\nwith large amounts of monolingual text data (which is easier to\nobtain compared to code-switched text).\nOur main contributions can be summarized as follows:\n• We formalize the framework of DLMs (Section 3).\n• We show signiﬁcant improvements in perplexity using DLMs\nwhen compared against smoothed n-gram language models\nestimated on code-switched text (Section 4.3). We provide a\ndetailed analysis of DLM improvements (Section 5).\n• We evaluate DLMs on the ASR task. DLMs capture sufﬁ-\ncient complementary information which we leverage to show\nimprovements on error rates. (Section 4.4).\n2. Related Work\nPrior work on building ASR systems for code-switched speech\ncan be broadly categorized into two sets of approaches: (1) De-\ntecting code-switching points in an utterance, followed by the\napplication of monolingual acoustic and language models to the\nindividual segments [1, 2, 3]. (2) Employing a universal phone\nset to build acoustic models for the mixed speech and pair-\ning it with standard language models trained on code-switched\ntext [4, 5, 6, 7, 8].\nThere have been many past efforts towards enhancing the\ncapability of language models for code-switched speech us-\ning additional sources of information such as part-of-speech\n(POS) taggers and statistical machine translation (SMT) sys-\ntems. Yeh et al. [7] employed class-based n-gram models that\ncluster words from both languages into classes based on POS\nand perplexity-based features. Vu et al. [9] used an SMT sys-\ntem to enhance the language models during decoding. Li et\nal. [10] propose combining a code-switch boundary predictor\nwith both a translation model and a reconstruction model to\nbuild language models. (Solorio et. al. [11] were one of the ﬁrst\nworks on learning to predict code-switching points.) Adel et\nal. [12] investigated how to effectively use syntactic and seman-\ntic features extracted from code-switched data within factored\nlanguage models. Combining recurrent neural network-based\nlanguage models with such factored language models has also\nbeen explored [13].\n3. Dual language models\nWe deﬁne a dual language model (DLM) to have the follow-\ning 2-player game structure. A sentence (or more generally, a\nsequence of tokens) is generated via a co-operative game be-\ntween the two players who take turns. During its turn a player\ngenerates one or more words (or tokens), and either terminates\nthe sentence or transfers control to the other player. Optionally,\nwhile transferring control, a player may send additional infor-\nmation to the other player (e.g., the last word it produced), and\narXiv:1711.01048v2  [cs.CL]  3 Aug 2018\nGiven two language models L1 and L2 with conditional probabilities P1 and P2 that satisfy the following conditions:\nP1[⟨/s⟩|⟨ s⟩] =P2[⟨/s⟩|⟨ s⟩] = 0 (1) P1[⟨sw⟩|⟨ s⟩] +P2[⟨sw⟩|⟨ s⟩] = 1 (2)\nP1[⟨sw⟩|⟨ sw⟩] =P2[⟨sw⟩|⟨ sw⟩] = 0 (3) P1[⟨/s⟩|⟨ sw⟩] =P2[⟨/s⟩|⟨ sw⟩] = 0 (4)\nWe deﬁne a combined language model L, with conditional probabilities P, as follows:\nP[w′|w] =\n\n\n\nP1[w′|⟨s⟩] if w′∈V1\nP2[w′|⟨s⟩] if w′∈V2\n0 if w′= ⟨/s⟩\nfor w = ⟨s⟩\nP[w′|w] =\n{\nP1[w′|w] if w′∈V1 ∪{⟨/s⟩}\nP1[⟨sw⟩| w] ·P2[w′|⟨sw⟩] if w′∈V2\nfor w ∈V1\nP[w′|w] =\n{\nP2[w′|w] if w′∈V2 ∪{⟨/s⟩}\nP2[⟨sw⟩| w] ·P1[w′|⟨sw⟩] if w′∈V1\nfor w ∈V2\nFigure 1: Deﬁnition of a bigram-based DLM for code-switched text.\nalso may retain some state information (e.g., cached words) for\nits next turn. At the beginning of the game one of the two play-\ners is chosen probabilistically.\nIn the context of code-switched text involving two lan-\nguages, we consider a DLM wherein the two players are each in\ncharge of generating tokens in one of the two languages. Sup-\npose the two languages have (typically disjoint) vocabularies\nV1 and V2. Then the alphabet of the output tokens produced by\nthe ﬁrst player in a single turn is V1 ∪{⟨sw⟩, ⟨/s⟩}, ⟨sw⟩de-\nnotes the switching – i.e., transferring control to the other player\n– and ⟨/s⟩denotes the end of sentence, terminating the game.\nWe shall require that a player produces at least one token before\nswitching or terminating, so that when V1 ∩V2 = ∅, any non-\nempty sentence in(V1∪V2)∗uniquely determines the sequence\nof corresponding outputs from the two players when the DLM\nproduces that sentence. (Without this restriction, the players\ncan switch control between each other arbitrarily many times,\nor have either player terminate a given sentence.)\nIn this paper, we explore a particularly simple DLM that is\nconstructed from two given LMs for the two languages. More\nprecisely, we shall consider an LM L1 which produces ⟨/s⟩-\nterminated strings in (V1 ∪{⟨sw⟩})∗ where ⟨sw⟩indicates a\nspan of tokens in the other language (so multiple ⟨sw⟩tokens\ncannot appear adjacent to each other), and symmetrically an\nLM L2 which produces strings in (V2 ∪{⟨sw⟩})∗. In Sec-\ntion 4.2, we will describe how such monolingual LMs can be\nconstructed from code-switched data. Given L1 and L2, we\nshall splice them together into a simple DLM (in which players\ndo not retain any state between turns, or transmit state informa-\ntion to the other player at the end of a turn). Below we explain\nthis process which is formally described in Fig. 1 (for bi-gram\nlanguage models).\nWe impose conditions (1)-(4) on the given LMs. Condition\n(1) which disallows empty sentences in the given LMs (and the\nresulting LM) is natural, and merely for convenience. Condition\n(2) states the requirement that L1 and L2 agree on the proba-\nbilities with which each of them gets the ﬁrst turn. Conditions\n(3) and (4) require that after switching at least one token should\nbe output before switching again or terminating. If the two LMs\nare trained on the same data as described in Section 4.2, all these\nconditions would hold.\nTo see that P[w′ |w] deﬁned in Fig. 1 is a well-deﬁned\nprobability distribution, we check that ∑\nw′ P[w′|w] = 1for\nall three cases of w, where the summation is over w′ ∈V1 ∪\nP 1 [ h sw i | w ]\nP 2 [ w 0\n| h sw i ]\nw\n0\nw\nL 1\nL 2\nFigure 2: DLM using two monolingual LMs, L1 and L2, imple-\nmented as a ﬁnite-state machine.\nV2 ∪{⟨/s⟩}. When w = ⟨s⟩, ∑\nw′ P[w′|w] equals\n∑\nw′∈V1\nP1[w′|⟨s⟩] +\n∑\nw′∈V2\nP2[w′|⟨s⟩]\n= (1−P1[⟨sw⟩|⟨ s⟩]) + (1−P2[⟨sw⟩|⟨ s⟩]) = 1\nwhere the ﬁrst equality is from (1) and the second equality is\nfrom (2).\nWhen w ∈V1, ∑\nw′ P[w′|w] is\n∑\nw′∈V1∪⟨/s⟩\nP1[w′|w] +P1[⟨sw⟩| w]\n∑\nw′∈V2\nP2[w′|⟨sw⟩]\n=\n∑\nw′∈V1∪⟨/s⟩\nP1[w′|w] +P1[⟨sw⟩| w] = 1.\nThe case of w ∈V2 follows symmetrically.\nFigure 2 illustrates how to implement a DLM as a ﬁnite-\nstate machine using ﬁnite-state machines for the monolingual\nbigram LMs, L1 and L2. The start states in both LMs, along\nwith all the arcs leaving these states, are deleted; a new start\nstate and end state is created for the DLM with accompanying\narcs as shown in Figure 2. The two states maintaining informa-\ntion about the ⟨sw⟩token can be split and connected, as shown\nin Figure 2, to create paths between L1 and L2.\n4. Experiments and Results\n4.1. Data description\nWe make use of the SEAME corpus [14] which is a conversa-\ntional Mandarin-English code-switching speech corpus.\nPreprocessing of data.Apart from the code-switched speech,\nthe SEAME corpus comprises of a) words of foreign origin\n(other than Mandarin and English) b) incomplete words c) un-\nknown words labeled as ⟨unk⟩, and d) mixed words such as\nbleach跟, cause就是, etc.. Since it was difﬁcult to obtain pro-\nnunciations for these words, we removed utterances that con-\ntained any of these words. A few utterances contained markers\nfor non-speech sounds like laughing, breathing, etc. Since our\nfocus in this work is to investigate language models for code-\nswitching, ideally without the interference of these non-speech\nsounds, we excluded these utterances from our task.\nData distribution. We construct training, development and\ntest sets from the preprocessed SEAME corpus data using a 60-\n20-20 split. Table 1 shows detailed statistics of each split. The\ndevelopment and evaluation sets were chosen to have 37 and 30\nrandom speakers each, disjoint from the speakers in the training\ndata.1 The out-of-vocabulary (OOV) rates on the development\nand test sets are 3.3% and 3.7%, respectively.\nTrain Dev Test\n# Speakers 90 37 30\nDuration (hrs) 56.6 18.5 18.7\n# Utterances 54,020 19,976 19,784\n# Tokens 539,185 195,551 196,462\nTable 1: Statistics of the dataset\n4.2. Monolingual LMs for the DLM construction\nGiven a code-switched text corpus D, we will derive two com-\nplementary corpora, D1 and D2, from which we construct bi-\ngram models L1 and L2 as required by the DLM construction\nin Figure 1, respectively. In D1, spans of tokens in the second\nlanguage are replaced by a single token⟨sw⟩. D2 is constructed\nsymmetrically. Standard bigram model construction on D1 and\nD2 ensures conditions (1) and (2) in Figure 1. The remaining\ntwo conditions may not naturally hold: Even though the data in\nD1 and D2 will not have consecutive ⟨sw⟩tokens, smoothing\noperations may assign a non-zero probability for this; also, both\nLMs may assign non-zero probability for a sentence to end right\nafter a ⟨sw⟩token, corresponding to the sentence having ended\nwith a non-empty span of tokens in the other language. These\ntwo conditions are therefore enforced by reweighting the LMs.\n4.3. Perplexity experiments\nWe used the SRILM toolkit [15] to build all our LMs. The base-\nline LM is a smoothed bigram LM estimated using the code-\nswitched text which will henceforth be referred to as mixed\nLM. Our DLM was built using two monolingual bigram LMs.\n(The choice of bigram LMs instead of trigram LMs will be jus-\ntiﬁed later in Section 5). Table 2 shows the perplexities on\nthe validation and test sets using both Good Turing [16] and\n1We note that choosing fewer speakers in the development and test\nsets led to high variance in the observed results.\nKneser-Ney [17] smoothing techniques. DLMs clearly outper-\nform mixed LMs on both the datasets. All subsequent experi-\nments use Kneser-Ney smoothed bigram LMs as they perform\nbetter than the Good Turing smoothed bigram LMs.\nSmoothing\nTechnique\nDev Test\nmixed LM DLM mixed LM DLM\nGood Turing 338.2978 329.1822 384.5164 371.1112\nKneser-Ney 329.6725 324.9268 376.0968 369.9355\nTable 2: Perplexities on the dev/test sets using mixed LMs and\nDLMs with different smoothing techniques.\nWe also evaluate perplexities by reducing the amount of\ntraining data to 1\n2 or 1\n3 of the original training data (shown in\nTable 3). As we reduce the training data, the improvements\nin perplexity of DLM over mixed LM further increase, which\nvalidates our hypothesis that DLMs are capable of generalizing\nbetter. Section 5 elaborates this point further.\nTraining\ndata\nDev Test\nmixed LM DLM mixed LM DLM\nFull 329.6725 324.9268 376.0968 369.9355\n1/2 362.0966 350.5860 400.5831 389.7618\n1/3 368.6205 356.012 408.562 394.2131\nTable 3: Kneser-Ney smoothed bigram dev/test set perplexities\nusing varying amounts of training data\n4.4. ASR experiments\nAll the ASR systems were built using the Kaldi toolkit [18].\nWe used standard mel-frequency cepstral coefﬁcient\n(MFCC)+delta+double-delta features with feature space\nmaximum likelihood linear regression (fMLLR) [19] trans-\nforms to build speaker-adapted triphone models with 4200\ntied-state triphones, henceforth referred to as “SAT” models.\nWe also build time delay neural network (TDNN [20])-based\nacoustic models using i-vector based features (referred to as\n“TDNN+SAT\"). Finally, we also re-scored lattices generated\nby the “TDNN+SAT\" model with an RNNLM [21] (referred to\nas “RNNLM Rescoring\"), trained using Tensorﬂow [22] on the\nSEAME training data. 2 We trained a single-layer RNN with\n200 hidden units in the LSTM [23] cell.\nThe pronunciation lexicon was constructed from CMU-\ndict [24] and THCHS30 dictionary [25] for English and Man-\ndarin pronunciations, respectively. Mandarin words that did not\nappear in THCHS30 were mapped into Pinyin using a freely\navailable Chinese to Pinyin converter. 3 We manually merged\nthe phone sets of Mandarin and English (by mapping all the\nphones to IPA) resulting in a phone inventory of size105.\nTo evaluate the ASR systems, we treat English words and\nMandarin characters as separate tokens and compute token er-\nror rates (TERs) as discussed in [9]. Table 4 shows TERs on\nthe dev/test sets using both mixed LMs and DLMs. DLM per-\nforms better or on par with mixed LM and at the same time, cap-\ntures a signiﬁcant amount of complementary information which\nwe leverage by combining lattices from both systems. The im-\nprovements in TER after combining the lattices are statistically\nsigniﬁcant (at p < 0.001) for all three systems, which justi-\nﬁes our claim of capturing complementary information. Tri-\ngram mixed LM performance was worse than bigram mixed\n2This rescoring was implemented using the tfrnnlm binary pro-\nvided by Kaldi [18] developers.\n3https://www.chineseconverter.com/en/\nconvert/chinese-to-pinyin\nASR system Data mixed LM DLM combined\nSAT Dev 45.59 45.59 44.93∗\nTest 47.43 47.48 46.96∗\nTDNN+SAT Dev 35.20 35.26 34.91∗\nTest 37.42 37.35 37.17∗\nRNNLM Rescoring Dev 34.21 34.11 33.85∗\nTest 36.64 36.52 36.37∗\nTable 4: TERs using mixed LMs and DLMs\nLM; hence we adopted the latter in all our models (further dis-\ncussed in Section 5). This demonstrates that obtaining signif-\nicant performance improvements via LMs on this task is very\nchallenging. Table 5 shows all the TER numbers by utiliz-\ning only 1\n2 of the total training data. The combined models\ncontinue to give signiﬁcant improvements over the individual\nmodels. Moreover, DLMs consistently show improvements on\nTERs compared to mixed LMs in the 1\n2 training data setting.\nASR system Data mixed LM DLM combined\nSAT Dev 48.48 48.17 47.671\nTest 49.07 49.04 48.52∗\nTDNN+SAT Dev 40.59 40.48 40.12∗\nTest 41.34 41.32 41.13∗\nRNNLM Rescoring Dev 40.20 40.09 39.84∗\nTest 40.98 40.90 40.72∗\nTable 5: TERs with 1\n2 training data\n5. Discussion\nCode-switched data corpora tend to exhibit very different lin-\nguistic characteristics compared to standard monolingual cor-\npora, possibly because of the informal contexts in which code-\nswitched data often occurs, and also possibly because of the\ndifﬁculty in collecting such data. It is possible that the gains\nmade by our language model are in part due to such character-\nistics of the corpus we use, SEAME. (We note that this corpus\nis by far the most predominant one used to benchmark speech\nrecognition techniques for code-switched speech.)\nIn this section we analyze the SEAME corpus and try to\nfurther understand our results in light of its characteristics.\nCode-switching boundaries. Code-switched bigrams with\ncounts of ≤ 10 occupy 87.5% of the total number of code-\nswitched bigrams in the training data. Of these, 55% of the\nbigrams have a count of 1. This suggests that context across\ncode-switching boundaries cannot signiﬁcantly help a language\nmodel built from this data. Indeed, the DLM construction in\nthis work discards such context, in favor of a simpler model.\nn-gram token distribution.We compare the unigram distribu-\ntion of a code-switched corpus (SEAME) with a standard mono-\nlingual corpus (PTB [26]). A glaring difference is observed in\ntheir distributions (Figure 3-a) with signiﬁcantly high occur-\nrence of less-frequent unigrams in the code-switched corpus,\nwhich makes them rather difﬁcult to capture using standard n-\ngram models (which often fall back to a unigram model). The\nDLM partially compensates for this by emulating a “class-based\nlanguage model,” using the only class information readily avail-\nable in the data (namely, the language of each word).\nIllustrative examples. Below, we analyze perplexities of the\nmixed LM and the DLM on some representative sentences from\nthe SEAME corpus, to illustrate how the performances of the\ntwo models compare.\n1statistically signiﬁcant improvement (at p <0.001)\n(a)\n (b)\nFigure 3: Comparison of fraction of data vs. frequency of n-\ngrams in code-mixed text (SEAME) and monolingual English\n(PTB) text. (The X-axis uses the log2 scale; 1 indicates uni-\ngrams with frequencies ≤21, etc.)\nSentence Mixed LM DLM\nperplexity perplexity\n我们的total是五十七 920.8 720.4\n哦我没有meeting了 92.2 75.9\nokay kay让我拿出我的calculator 1260.3 1284.6\nthe roomie lives in serangoon right 2302.1 1629.3\noh 他拿third class他差一点点他\n的 f. y. p. screwed up 他 拿 到 b\nminus c plus\n299.7 257.1\nWe observe that when less frequent words appear at switch-\ning points (like total, meeting, etc.), the DLM outperforms the\nmixed LM by a signiﬁcant margin as illustrated in the ﬁrst\ntwo sentences above. In cases of highly frequent words oc-\ncurring at switching points, the DLM performs on par with or\nslightly worse than the mixed LM, as seen in the case of the\nthird sentence. The DLM also performs slightly better within\nlong stretches of monolingual text as seen in the fourth sen-\ntence. On the ﬁnal sentence, which has multiple switches and\nlong stretches of monolingual text, again the DLM performs\nbetter. As these examples illustrate, DLMs tend to show im-\nproved performance at less frequent switching points and within\nlong stretches of monolingual text.\nEffect of Trigrams.In standard monolingual datasets, trigram\nmodels consistently outperform bilingual models. However, in\nthe SEAME corpus we did not ﬁnd a pronounced difference be-\ntween a bigram and a trigram model. This could be attributed\nto the fact that the number of highly frequent trigrams in our\ncorpus is lower in comparison to that in the PTB dataset (Fig-\nure 3-b). As such, we have focused on bigram LMs in this work.\n6. Conclusions\nWe introduced DLMs and showed robust improvements over\nmixed LMs in perplexity for code-switched speech. While the\nperformance improvements for the ASR error rates are mod-\nest, they are achieved without the aid of any external language\nresources and without any computational overhead. We ob-\nserve signiﬁcant ASR improvements via lattice combination of\nDLMs and the standard mixed LMs. Future directions include\ninvestigating properties of code-switched text which can be in-\ncorporated within DLMs, using monolingual data to enhance\neach DLM component and demonstrating the value of DLMs\nfor multiple code-switched language pairs.\nAcknowledgements. The last author gratefully acknowledges\nﬁnancial support from Microsoft Research India for this project,\nas well as access to Microsoft Azure cloud computing services.\n7. References\n[1] J. Y . Chan, P. Ching, T. Lee, and H. M. Meng, “Detection of lan-\nguage boundary in code-switching utterances by bi-phone proba-\nbilities,” in Proceedings of International Symposium on Chinese\nSpoken Language Processing. IEEE, 2004, pp. 293–296.\n[2] D.-C. Lyu and R.-Y . Lyu, “Language identiﬁcation on code-\nswitching utterances using multiple cues,” in Proceedings of In-\nterspeech, 2008.\n[3] C.-J. Shia, Y .-H. Chiu, J.-H. Hsieh, and C.-H. Wu, “Language\nboundary detection and identiﬁcation of mixed-language speech\nbased on MAP estimation,” in Proceedings of ICASSP , vol. 1.\nIEEE, 2004, pp. I–381.\n[4] D. Imseng, H. Bourlard, M. M. Doss, and J. Dines, “Language\ndependent universal phoneme posterior estimation for mixed lan-\nguage speech recognition,” in Proceedings of ICASSP, 2011, pp.\n5012–5015.\n[5] Y . Li, P. Fung, P. Xu, and Y . Liu, “Asymmetric acoustic modeling\nof mixed language speech,” in Proceedings of ICASSP, 2011, pp.\n5004–5007.\n[6] K. Bhuvanagiri and S. Kopparapu, “An approach to mixed lan-\nguage automatic speech recognition,” Proceedings of Oriental\nCOCOSDA, Kathmandu, Nepal, 2010.\n[7] C. F. Yeh, C. Y . Huang, L. C. Sun, and L. S. Lee, “An inte-\ngrated framework for transcribing Mandarin-English code-mixed\nlectures with improved acoustic and language modeling,” in Pro-\nceedings of Chinese Spoken Language Processing (ISCSLP),\n2010 7th International Symposium on . IEEE, 2010, pp. 214–\n219.\n[8] S. Yu, S. Hu, S. Zhang, and B. Xu, “Chinese-English bilingual\nspeech recognition,” in Proceedings of Natural Language Pro-\ncessing and Knowledge Engineering. IEEE, 2003, pp. 603–609.\n[9] N. T. Vu, D.-C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F. Blaicher,\nE.-S. Chng, T. Schultz, and H. Li, “A ﬁrst speech recognition sys-\ntem for Mandarin-English code-switch conversational speech,” in\nProceedings of ICASSP. IEEE, 2012, pp. 4889–4892.\n[10] Y . Li and P. Fung, “Improved mixed language speech recognition\nusing asymmetric acoustic model and language model with code-\nswitch inversion constraints,” inProceedings of ICASSP. IEEE,\n2013, pp. 7368–7372.\n[11] T. Solorio and Y . Liu, “Learning to predict code-switching\npoints,” inProceedings of EMNLP, 2008, pp. 973–981.\n[12] H. Adel, N. T. Vu, K. Kirchhoff, D. Telaar, and T. Schultz, “Syn-\ntactic and semantic features for code-switching factored language\nmodels,” Proceedings of IEEE Transactions on Audio, Speech,\nand Language Processing, vol. 23, no. 3, pp. 431–440, 2015.\n[13] H. Adel, D. Telaar, N. T. Vu, K. Kirchhoff, and T. Schultz, “Com-\nbining recurrent neural networks and factored language models\nduring decoding of code-switching speech,” inProceedings of In-\nterspeech, 2014.\n[14] D.-C. Lyu, T.-P. Tan, E.-S. Chng, and H. Li, “An analysis of a\nMandarin-English code-switching speech corpus: SEAME,” Pro-\nceedings of Age, vol. 21, pp. 25–8, 2010.\n[15] A. Stolcke, “SRILM – an extensible language modeling toolkit.”\nin Proceedings of Interspeech, 2002.\n[16] S. Katz, “Estimation of probabilities from sparse data for the lan-\nguage model component of a speech recognizer,” in Proceedings\nof IEEE Transactions on Acoustics, Speech, and Signal Process-\ning. IEEE, 1987.\n[17] R. Kneser and H. Ney, “Improved backing-off for m-gram lan-\nguage modeling,” inProceedings of ICASSP. IEEE, 1995.\n[18] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarzet al.,\n“The kaldi speech recognition toolkit,” in Proceedings of ASRU,\n2011.\n[19] M. J. F. Gales, “Maximum likelihood linear transformations for\nhmm-based speech recognition,” in Proceedings of Computer\nSpeech and Language. ELSEVIER, 1998.\n[20] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang,\n“Phoneme recognition using time-delay neural network,” inRead-\nings in speech recognition. ELSEVIER, 1990.\n[21] X. Hainan, C. Tongfei, G. Dongji, W. Yiming, L. Ke, G. Na-\ngendra, C. Yishay, P. Daniel, and K. Sanjeev, “A pruned rnnlm\nlattice-rescoring algorithm for automatic speech recognition,” in\nProceedings of ICASSP. IEEE, 2017.\n[22] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isardet al., “Tensorﬂow: A\nsystem for large-scale machine learning.” inProceedings of OSDI,\nvol. 16, 2016, pp. 265–283.\n[23] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[24] R. L. Weide, “The CMU pronouncing dictionary,” URL:\nhttp://www. speech. cs. cmu. edu/cgibin/cmudict, 1998.\n[25] Z. Z. Dong Wang, Xuewei Zhang, “THCHS-30 : A free\nChinese speech corpus,” 2015. [Online]. Available: http:\n//arxiv.org/abs/1512.01882\n[26] M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini, “Building\na large annotated corpus of english: The penn treebank,” Compu-\ntational linguistics, vol. 19, no. 2, pp. 313–330, 1993.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.959667444229126
    },
    {
      "name": "Computer science",
      "score": 0.8349766731262207
    },
    {
      "name": "Language model",
      "score": 0.7652634382247925
    },
    {
      "name": "Natural language processing",
      "score": 0.6166712045669556
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5638763904571533
    },
    {
      "name": "Mandarin Chinese",
      "score": 0.563042938709259
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5065208673477173
    },
    {
      "name": "Cache language model",
      "score": 0.49193692207336426
    },
    {
      "name": "Speech recognition",
      "score": 0.4828958511352539
    },
    {
      "name": "Code-switching",
      "score": 0.46307623386383057
    },
    {
      "name": "Code (set theory)",
      "score": 0.4340953230857849
    },
    {
      "name": "Probabilistic logic",
      "score": 0.41536104679107666
    },
    {
      "name": "Natural language",
      "score": 0.35197877883911133
    },
    {
      "name": "Universal Networking Language",
      "score": 0.3226349949836731
    },
    {
      "name": "Programming language",
      "score": 0.1809961199760437
    },
    {
      "name": "Linguistics",
      "score": 0.17872467637062073
    },
    {
      "name": "Comprehension approach",
      "score": 0.13144400715827942
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": []
}