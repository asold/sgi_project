{
    "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
    "url": "https://openalex.org/W4385572282",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2903011371",
            "name": "Enes Altinisik",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2256769809",
            "name": "Hassan Sajjad",
            "affiliations": [
                "Dalhousie University"
            ]
        },
        {
            "id": "https://openalex.org/A2478772260",
            "name": "Husrev Sencar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2654308594",
            "name": "Safa Messaoud",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105491359",
            "name": "Sanjay Chawla",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4224035735",
        "https://openalex.org/W4293846201",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W4321005039",
        "https://openalex.org/W2243397390",
        "https://openalex.org/W4307868887",
        "https://openalex.org/W3009870288",
        "https://openalex.org/W3036267641",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W4287813862",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2964232431",
        "https://openalex.org/W4385564898",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2963496101",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W3006622081",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2774644650",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2964116600",
        "https://openalex.org/W2913266441",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W1673923490",
        "https://openalex.org/W4318959511",
        "https://openalex.org/W2964159205",
        "https://openalex.org/W3035164976",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2786163515",
        "https://openalex.org/W3099617520",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W2970078867",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W3187071356",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2962718684",
        "https://openalex.org/W2180612164",
        "https://openalex.org/W2996344901",
        "https://openalex.org/W3197868468",
        "https://openalex.org/W3035736465",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2970449623",
        "https://openalex.org/W2952186591",
        "https://openalex.org/W2963400886",
        "https://openalex.org/W3196986263",
        "https://openalex.org/W3174318528",
        "https://openalex.org/W3104350794",
        "https://openalex.org/W3104423855",
        "https://openalex.org/W4280538731",
        "https://openalex.org/W4205758343"
    ],
    "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveal that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7828–7840\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nImpact of Adversarial Training on Robustness and Generalizability of\nLanguage Models\nEnes Altinisik Hassan Sajjad ♣ Husrev Taha Sencar\nSafa Messaoud Sanjay Chawla\n{ealtinisik,hsencar,smessaoud,schawla}@hbku.edu.qa\nQatar Computing Research Institute, HBKU Research Complex, Doha, Qatar\nhsajjad@dal.ca\n♣Faculty of Computer Science, Dalhousie University, Halifax, Canada\nAbstract\nAdversarial training is widely acknowledged as\nthe most effective defense against adversarial\nattacks. However, it is also well established\nthat achieving both robustness and generaliza-\ntion in adversarially trained models involves\na trade-off. The goal of this work is to pro-\nvide an in depth comparison of different ap-\nproaches for adversarial training in language\nmodels. Specifically, we study the effect of pre-\ntraining data augmentation as well as training\ntime input perturbations vs. embedding space\nperturbations on the robustness and general-\nization of transformer-based language models.\nOur findings suggest that better robustness can\nbe achieved by pre-training data augmentation\nor by training with input space perturbation.\nHowever, training with embedding space per-\nturbation significantly improves generalization.\nA linguistic correlation analysis of neurons of\nthe learned models reveal that the improved\ngeneralization is due to ‘more specialized’ neu-\nrons. To the best of our knowledge, this is the\nfirst work to carry out a deep qualitative analy-\nsis of different methods of generating adversar-\nial examples in adversarial training of language\nmodels.\n1 Introduction\nLanguage Models (LMs) have emerged as the back-\nbone of many tasks in AI and have extended their\nreach beyond NLP applications into vision and\neven reinforcement learning (Brown et al., 2020;\nReed et al., 2022; Ramesh et al., 2022). Thus it is\nimperative that the generalizability and robustness\nof LMs be carefully assessed and evaluated.\nGeneralizability is the ability of a model to per-\nform well on unseen data. Transformer-based mod-\nels that are pre-trained on large unlabeled text have\nshown remarkable generalization ability. However,\nwhen confronted with carefully designed adversar-\nial samples, their robustness - the ability to grace-\nfully deal with small perturbations, suffers signifi-\ncantly. For example, a recent study has shown that\non a classification task on a YELP data set, accu-\nracy dropped by almost 90%, when a standard test\nset was replaced by an adversarial counterpart (Jin\net al., 2020; Yoo and Qi, 2021; Yuan et al., 2021).\nAdversarial training is a pragmatic approach to\nattain both generalizability and robustness. The\nidea is straightforward. For a given model M, gen-\nerate adversarial samples that target M and then\nuse the samples to incrementally re-train the model.\nThis can be done either at the pre-training or the\nfine-tuning stage (Liu et al., 2020).\nAdversarial samples can be generated both in the\ninput space and in the embedding space. The origi-\nnal work on the creation of adversarial samples for\ncomputer vision was in the input space. For exam-\nple, the fast gradient sign method (FGSM) (Good-\nfellow et al., 2014) that perturbs a data point x\nalong the direction of the sign gradient of the loss\nfunction with respect to the input is an example of\na perturbation in the input space. In the context of\nnatural language inputs, perturbing text is challeng-\ning due to its discrete nature. Unlike continuous\ndata, there is no systematical way to guarantee an\nincrease in the loss function when perturbing text.\nFor instance, if we aim to make a small modifica-\ntion to the word “robust” we can choose to replace\na single letter within the word or substitute it with\na near synonym. However, both of these perturba-\ntions may seem ad-hoc and not sufficiently prin-\ncipled to intentionally increase the loss function.\nTherefore, in language settings, it is often more\nappropriate to perform perturbations in the embed-\nding space, where continuous representations can\nbe manipulated in a more structured manner.\nFurthermore, despite the widespread use of ad-\nversarial training to increase the robustness of mod-\nels, it is not clear what their impact is on down-\nstream tasks beyond the model’s overall accuracy.\nFor example, a deeper analysis of language models\nhas shown that different parts of the network are\nresponsible for different parts of speech (Belinkov\n7828\net al., 2017; Conneau et al., 2018; Liu et al., 2019;\nDalvi et al., 2022; Durrani et al., 2020). In this re-\ngard, the change in the network due to adversarial\ntraining has not yet been investigated.\nOverall our contributions in this paper are three-\nfold. Firstly, we introduce two techniques in\nthe context of adversarial training in the embed-\nding space, representing the regularization- and\ngradient-based approaches commonly used by la-\ntent space techniques. We compare these tech-\nniques using a simple one-dimensional model and\nhypothesize their behavior in adversarial scenarios.\nSecondly, we evaluate the effectiveness of input-\nand embedding-space adversarial training methods\nin terms of their generalization ability and robust-\nness against various types of adversarial attacks\nin sentiment analysis. Lastly, we conduct a thor-\nough linguistic analysis of an adversarially trained\nmodel and demonstrate that incorporating robust-\nness through adversarial training leads to more “fo-\ncused\" neurons that are associated with distinct\nPart of Speech (POS) tags.\nThe rest of the paper is organized as follows. In\nSection 2, we discuss adversarial attacks and de-\nfenses, with a specific focus on the NLP domain.\nSection 3 provides a detailed explanation of em-\nbedding space adversarial techniques. In Section\n4, we conduct experiments to analyze the trade-off\nbetween robustness and generalization achieved by\ndata augmentation, input-space training, and em-\nbedding space training approaches, considering var-\nious well-known adversarial attacks. Additionally,\nwe present our findings from linguistic correlation\nanalysis of neurons in robust models within the\nsame section. Finally, we finalized the paper in the\nconcluding section.\n2 Related Work\nAdversarial Attacks: The purpose of an adversar-\nial attack is to cause a model to output conflicting\ndecisions for an input and its ‘imperceptibly’ mod-\nified version. An adversarial sample is defined as:\nx′ = x+ δ; ||δ||≤ ϵ∧f(x,θ) ̸= f(x′,θ) (1)\nwhere x′ is the adversarial sample, δis the pertur-\nbation added to the original datax, ||δ||is a generic\nnorm, ϵis the limit of the maximum norm of the\nperturbation, and f(x,θ) is the output of the model\nparameterized by θfor input x. The quality of an\nadversarial sample is typically evaluated depend-\ning on how well δis minimized, i.e., the minimum\ndistortion that changes the prediction of the model\non a sample.\nObtaining an exact solution for the perturba-\ntion δ is a very challenging problem. Further,\neven when close approximations are considered,\nthe solution gets computationally very expensive\n(Szegedy et al., 2013). To solve this problem\nmore efficiently, gradient-based methods were in-\ntroduced. Accordingly, the perturbation δis com-\nputed by taking one (Goodfellow et al., 2014) or\nmore steps iteratively (Madry et al., 2017; Dong\net al., 2018) in the direction of the gradient to max-\nimize the loss function. Then, this high loss point\nis projected back onto the input space to determine\nthe norm-bounded perturbation. In practice, pro-\njected gradient descent (PGD) approaches that, take\nseveral small steps in the direction of the gradient,\nare used most frequently to create strong adversar-\nial samples (Madry et al., 2017; Papernot et al.,\n2016).\nOther than gradient based approaches, Jacobian-\nbased Saliency Map Attack (JSMA) (Papernot\net al., 2016) uses the Jacobian matrix created\nfrom forward derivation of input to identify to\nimportance of each input component to the target\nattack. DeepFool (Moosavi-Dezfooli et al., 2016),\nalternatively, iteratively linearizes the classifier\nto identify the minimum perturbation that causes\na change in the classification label. Carlini &\nWagner Attack (C&W) proposed defensive distil-\nlation strategy (Hinton et al., 2015) based approach.\nAdversarial Attacks in NLP: Running adversarial\nattacks against Natural language processing (NLP)\nmodels is more challenging than widely used vision\nmodels. The discrete nature of word representa-\ntions, combined with the tokenization of words into\nword pieces, effectively invalidates any algorithm\nthat applies differential changes on the model input\nwhen generating an adversarial sample. Moreover,\nquantification of the extent to which semantic sim-\nilarity and contextual relations are preserved be-\ntween a text input and its modified version is not\ntrivial.\nTo circumvent these limitations, many adver-\nsarial sample generation algorithms adopted the\napproach of substituting one or more words in\nthe input until a misprediction occurs. The crux\nof this attack lies in identification of alternative\nwords or phrases that retain the semantic intact-\nness of the original input. For this, several meth-\n7829\nods based on word-embedding similarity (Jin et al.,\n2020), word synonymity (Ren et al., 2019; Zang\net al., 2019), and masked language model predic-\ntions (Li et al., 2020) are proposed. However, find-\ning appropriate word candidates may get computa-\ntionally very intensive. For a sentence consisting\nof mwords with ncandidates to substitute each\nword, there are (n+ 1)m possible combinations\nto test. To perform this search efficiently, greedy\nsearch (Ren et al., 2019), genetic algorithm (Alzan-\ntot et al., 2018), and particle swarm optimization-\nbased (PSO) (Zang et al., 2019) approaches are\nproposed and incorporated with word importance\nas determined by gradient measurements (Yoo and\nQi, 2021) and word deletion (Ren et al., 2019).\nAn alternative approach to above substitution-\nbased approach is applying perturbations in the\nembedding space directly to word embeddings.\nThis approach avoids the expensive search step\nto identify the best word substitution config-\nuration, but it requires devising a mapping\nfrom perturbed embeddings to the text domain\nin order to create an adversarial sample. To\nrealize this, recent work (Yuan et al., 2021)\nadapted a gradient-based adversarial sample\ngeneration method to compute perturbations\nassociated with each word embedding. Perturbed\nembeddings are then translated to input domain\nusing a pre-trained masked-language modeling\n(MLM) head, as in (Li et al., 2020; Garg and\nRamakrishnan, 2020), to create an adversarial sam-\nple that is semantically similar to the original input.\nAdversarial Defence in NLP: The most com-\nmonly deployed method for attaining robustness\nagainst an adversarial attack is through addition of\nadversarial samples into the training set (Szegedy\net al., 2013). This approach is known to increase\nmodel robustness in both computer vision and NLP\ndomains. Further, it is also reported that this de-\nfence approach decreases the generalization error\nof a model in the absence of any attack (Yuan et al.,\n2021), which contradicts the commonly held opin-\nion that there is a trade-off between generalization\nand robustness E: (Tsipras et al., 2019). This find-\ning can essentially be attributed to the use of a\nlarger training set enhanced with adversarial sam-\nples. The second approach augments the train-\ning set with newly constructed, synthetic samples.\nWhile this may seem equivalent to adding adver-\nsarial samples to the training set, data augmenta-\ntion methods do not need to have an adversarial\nnature. Common data augmentation methods in-\nclude word replacement, i.e., substituting words\nwith their synonyms or inserting random words,\nrandom word deletions, and swapping of words be-\ntween sentences (Wei and Zou, 2019). Rather than\nusing manually-designed heuristics, the power of\nexisting NLP models can also be harnessed for data\naugmentation. Reverse translation, which involves\nre-translation of samples from a target language\nback to their source language constitutes one such\nmethod that ideally preserves the semantic simi-\nlarity of original and augmented samples (Edunov\net al., 2018; Xie et al., 2020). The use of MLM via\nmasking words in a sentence and replacing them\nwith model predictions (Ng et al., 2020) is another\naugmentation method.\nThe third approach to adversarial training in-\nvolves applying perturbations in the latent space\n(Zhu et al., 2019; Liu et al., 2020; Li and Qiu, 2021;\nPan et al., 2022). This yields a simpler training\nprocedure as it removes the need for generating ad-\nversarial samples in the input space. In (Zhu et al.,\n2019), a model is incrementally fine-tuned on sets\nof adversarially perturbed word embeddings com-\nputed after each fine-tuning step. Li et al. (2021)\ndemonstrate that this method performs better when\nno constraint on the amount of perturbation is im-\nposed. In Li and Qiu (2021), it is observed that\nrather than initializing the PGD step with random\nnoise when computing perturbations for each to-\nken, using a token-dependent random noise that is\nfixed across all inputs is more effective. Recently,\nPan et al. (2022) proposed the use of contrastive\nobjective (Oord et al., 2018) for ensuring invari-\nant representations by forcing the model to learn\nthe differences between the normal input and its\nadversarial version.\nIn addition to empirical methods, certified de-\nfense methods are proposed to identify and elim-\ninate adversarial samples. These techniques min-\nimize misclassification within an l∞ ball bound,\nparticularly in the vision domain (Raghunathan\net al., 2018; Wong and Kolter, 2018). In the NLP\ndomain, two main categories of certified defense\nmethods have emerged: Interval Bound Propaga-\ntion (IBP) (Jia et al., 2019; Huang et al., 2019; Shi\net al., 2020) and randomized smoothing (Ye et al.,\n2020; Zeng et al., 2021). IBP techniques estimate\nthe output range by iteratively applying interval\nconstraints from the input layer to subsequent lay-\n7830\ners. However, the requirement to modify the model\nstructure poses challenges in incorporating these\nmethods into pre-trained models.\nRandomized smoothing-based methods offer an\nalternative approach that is independent of the\nmodel structure. These methods utilize stochastic\nensembles of input texts and leverage the statis-\ntical properties of these ensembles to offer prov-\nable robustness certification. A common approach\nto achieve this is by generating a few randomly\nmodified versions of the original sample. This can\nbe done through techniques such as random word\nsubstitutions using synonyms, as demonstrated in\nSAFER (Ye et al., 2020), or by employing a mask\nlanguage model to substitute words, as shown in\nRanMASK (Zeng et al., 2021). The final prediction\nis then made based on the decisions made by these\nrandomly generated samples.\nThroughout the rest of the paper, we do not delve\ninto a detailed discussion of these techniques for\nseveral reasons. Firstly, the main focus of this paper\nis on empirical methods and evaluating their impact.\nSecondly, randomized smoothing methods can be\nintegrated into various techniques, making them\napplicable in different contexts. Lastly, previous\nfindings suggest that while randomized smoothing\nmethods demonstrate strong defense performance,\nthey tend to underperform compared to latent space\nadversarial training (Li et al., 2021).\n3 AT with Embedding Space\nPerturbations\nAmong all adversarial defenses developed for lan-\nguage processing models, moving the adversarial\ntraining from the input space to the embedding\nspace offers the most advantage. This essentially\nallows the adoption of gradient-based adversarial\ntraining approaches that are computationally less\ndemanding than input space methods. Although a\nplethora of such adversarial training methods ex-\nists, they are all essentially guided by two main\nprinciples in their approach. The first one essen-\ntially sets the training objective to minimize the\nloss due to worst-case perturbation induced on the\ntraining samples, instead of the average loss com-\nputed from training samples by the standard train-\ning. This group of methods essentially differ in the\nway they approximate the worst-case perturbation\n(Madry et al., 2017; Miyato et al., 2018; Zhang\net al., 2019) as well as the extent and nature of per-\nturbation applied during generation of adversarial\nsamples (Ding et al., 2018; Wang et al., 2019; Liu\net al., 2020).\nThe second approach primarily relies on the\npremise that smoothness is an important require-\nment of a robust model. To this objective, these\nmethods focus on minimization of a regularized\nversion of the loss instead of optimizing only the\nstandard, training loss. The regularization term\nhere ensures that there is a wide enough margin\naround each training data point with the decision\nboundary of the model through minimizing the\ndifference between the predictions of natural and\nadversarial samples. Methods following this ap-\nproach are distinguished based on their formula-\ntion of regularization (Szegedy et al., 2016; Zhang\net al., 2019) and their coupling with the training\nloss described above (Gan et al., 2020; Pan et al.,\n2022).\nIn our analysis, we consider two representative\nmethods that most effectively exemplify each ap-\nproach. In practice, due to its computational effi-\nciency, the PGD attack is most frequently used for\nthe creation of adversarial samples. We will refer to\nthis generic adversarial training approach as PGD-\nAT. The latter approach is also best characterized\nby the use of PGD in ensuring local distribution\nsmoothness around natural samples. This alterna-\ntive method will be referred to as LDS. We must\nnote that improved variants of the two base meth-\nods should be expected to perform better. In this\nregard, robustness-generalization performance of\nthe PGD-AT and LDS can be interpreted as lower-\nbounds.\nThe steps of both methods are presented in Al-\ngorithm 1 where the lines that differ between the\ntwo methods are highlighted as pink for PGD-AT\nand blue for LDS. Both methods start by randomly\ninitializing δwith normal distribution with a mean\nof zero and standard deviation of σ. The loss is\nthen calculated between the model’s output of the\nperturbed input depending on the method, PGD-AT\nor LDS. The δ value is then updated by the gra-\ndient and clipped to within ±ϵby the projection\nfunction Π. These steps are repeated for S times.\nThe loss value is then updated by combining the\nstandard loss with the loss associated with each\nmethod. Gradient update is then applied to model\nparameters.\nTo better examine the behavior of the two meth-\nods, we analyze a simple one-dimensional linear\n7831\nAlgorithm 1 PGD-AT and LDS based adversarial\ntraining\nInput: E: the number of epochs, D= {(x(i),y(i))}n\ni=1: the\ndataset, f(x,θ): the machine learning model parametrized by\nθ, δ: the perturbation initialized by σand limited by ϵ, τ: the\nglobal learning rate, µ: the adversarial learning rate, S: the\nnumber of PGD step, and Π is the projection function.\nfor e= 1,..,E do\nfor (x,y) ∈D do\nδ∼N(0,σ2)\nfor s= 1,..,S do\ngadv = ∇δl(f(x+ δ,θ),y) %PGD-AT\ngadv = ∇δl(f(x,θ),f(x+ δ,θ)) %LDS\nδ= Π||δ||≤ϵ(δ+ µgadv)\nend\ngθ ←∇θl(f(x,θ),y)\n+∇θl(f(x+ δ,θ),y) %PGD-AT\ngθ ←∇θl(f(x,θ),y)\n+∇θl(f(x,θ),f(x+ δ,θ)) %LDS\nθ←θ−τgθ\nend\nend\nOutput: θ\nModel Loss Function Parameter\nOLS 1\nn\n∑n\ni=1(θ.xi−yi)2 θ=\n∑\ni xiyi∑\ni x2\ni\nPGD-AT\n1\nn\n∑n\ni=1\n{(θ.xi−yi)2 + θ=\n∑\ni 2xiyi+yiδ∑\ni x2\ni +(xi+δ)2(θ.(xi+ δ) −yi)2}\nLDS\n1\nn\n∑n\ni=1\n{(θ.xi−yi)2 + θ=\n∑\ni xiyi∑\ni x2\ni +δ2(θ.(xi+ δ) −θ.xi)2}\nTable 1: Closed form solutions of the model parame-\nter of a one-dimensional linear regression model under\nvarious loss functions\nregression model:\ny= θ.x+ ϵ, ϵ ∼N(0,σ2)\nAssuming a fixed perturbationδ, we determine how\nthe two loss functions, given in Algorithm 1, es-\ntimate the model parameter θ under noisy obser-\nvations. Table 1 presents the loss functions cor-\nresponding to PGD-AT and LDS as well as the\none corresponding to the standard ordinary least\nsquares (OLS) estimation in the absence of δ. The\nestimates for the parameter θ for the three loss\nfunctions are also given in the table (third column).\nComparing PGD-AT and LDS, it can be deduced\nthat LDS will converge to OLS only as the noise\nϵgets severe, suppressing the effect of δin the de-\nnominator. Whereas PGD-AT can be expected to\nfollow OLS more closely at all noise levels as δ\nappears both at the numerator and the denominator,\nthereby absorbing its effect on the estimate.\nWe also designed an experimental setup to test\nthese hypotheses. A single neuron is trained based\non randomly generated (x,y) pairs as defined above\n(a)\n (b)\nFigure 1: The resulting distribution for θvalues related\nto three different models, trained using OLS, LDS, and\nPGD-AT methods, when σ is set to (a) 0.01 and (b)\n0.1. A small standard deviation indicates the model’s\nrobustness and clustering around 0.5 implies better gen-\neralizability.\nFigure 2: Evaluation pipeline of models learned using\ndifferent adversarial training approaches.\nassuming θ = 1\n2 and for two different noise dis-\ntributions, (σ = 0.01 and σ = 0.1) for each loss\nfunction. The models are trained for 2K epochs at\na learning rate of 0.005 starting with the OLS loss.\nFor PGD-AT and LDS models, the OLS loss is sub-\nstituted by their loss function after epoch 1750 and\nδvalues are computed as defined in Algorithm 1.\nThe distributions of the estimated scalar model\nparameter θobtained after 25 runs is displayed in\nFig. 1. Essentially, the spread of the distribution\nsignifies the robustness of a model against adver-\nsarial samples and the distribution mean relates to\nthe generalizability of the model. In this regard,\nPGD-AT is seen to perform better than LDS as it\nyields a tighter spread in both cases. However, at\nhigher noise levels, it can be seen that LDS pro-\nvides a more accurate estimate of θ. Overall, we\ncan expect that a model trained with PGD-AT to\nbe more robust while yielding a generalizability\nbehavior closer to that of LDS.\n4 Experiments\nWe first compare the robustness, generalization and\nrun-time complexity of different AT strategies, fol-\nlowing the pipeline in Fig. 2. Then, we perform a\nLinguistic Correlation Analysis (LCA, Dalvi et al.,\n7832\nFigure 3: LCA pipeline of models learned using different adversarial training approaches.\nTable 2: Robustness results. Models are evaluated using ASR (lower is better) on the MR and IMDB datasets.\nAttack Dataset BERT AT-IP AT-DA AT-EP\nA2T A2T_MLM BERT_Attack SSMBA BackTranslation LDS PGD-AT\nTextFooler MR 82.1 77.9 79.7 79.3 79.8 72.8 88.6 87.8\nIMDB 80.6 86.5 65.3 72.8 81.3 45.9 91.5 94.7\nA2T MR 33.8 27.6 30.8 26.5 34.2 30.4 22.3 20.9\nIMDB 59.5 51.1 43.7 49.4 59.2 36.4 56.9 43.0\nBAE MR 52.1 44.1 45.5 44.0 49.2 47.1 55.3 52.9\nIMDB 68.8 65.0 52.5 57.9 61.5 41.4 66.5 61.0\nPSO MR 79.8 75.0 72.7 74.7 78.1 75.6 79.8 80.7\nIMDB 46.4 35.3 35.4 30.2 41.8 42.8 70.8 66.2\nAverage MR 62.0 56.1 57.2 56.1 60.3 56.5 61.5 60.5\nIMDB 63.8 59.5 49.2 52.6 61.0 41.6 71.4 66.2\n2019) as implemented in the NeuroX toolkit (Dalvi\net al., 2023) to gain better insights into the dynam-\nics of the learned models, as illustrated in Fig. 3.\nBaselines: We compare standard BERT (Devlin\net al., 2018) with seven versions of adversarially\ntrained BERT models using methods from three\nfamilies of AT approaches: (1) AT with pre-training\ndata augmentation (AT-DA), (2) AT with input\nspace perturbations (AT-IP) and (3) AT with em-\nbedding space perturbations (AT-EP), on the task\nof sentiment classification. Specifically, for AT-\nDA, we experiment with SSMBA (Ng et al., 2020)\nand BackTranslation (Xie et al., 2020). For AT-IP,\nwe use A2T, A2T_MLM (Yoo and Qi, 2021) and\nBERT_attack (Li et al., 2020). For AT-EP, we re-\nport results on LDS (Szegedy et al., 2016; Zhang\net al., 2019) and PGD-AT (Gan et al., 2020; Pan\net al., 2022).\nDatasets: We fine-tune all models on the Inter-\nnet Movie Database (IMDB, Maas et al., 2011)\nand Movie Reviews (MR, Pang and Lee, 2005)\ndatasets and test on the corresponding testing splits,\nas well as on YELP dataset (Zhang et al., 2015) for\nout-of-distribution assessment of the models.\nAttack methods: We assess the robustness of the\nmodels under four different attacks which replace\nwords in the input space using different strategies.\n(1) TextFooler (Jin et al., 2020) first searches for the\nword that results in the highest change in the senti-\nment score, when removed, then replaces it with the\nnearest neighbouring word in the embedding space.\n(2) BAE (Garg and Ramakrishnan, 2020) masks a\nportion of the text and using a BERT masked lan-\nguage model to generate alternatives for the masked\nwords. (3) A2T (Yoo and Qi, 2021) selects the word\nwith the largest loss gradient w.r.t its embedding\nand replaces it with a synonym generated from a\ncounterfitted word embedding (Mrkši´c et al., 2016).\n(4) PSO (Zang et al., 2019) uses sememe-based\nword substitution and particle swarm optimization-\nbased search algorithm to find good adversarial\nexamples for a given input text.\nEvaluation metrics: we assess (1) generaliza-\ntion via computing the accuracy values on in-\ndistribution and out-of-distribution datasets, (2)\nrobustness using the Attack Success Rate (ASR)\nrepresenting the ratio of the number of successful\nattacks to the number of samples, as well as (3)\nthe time complexity measured via the fine-tuning\nrun-time of the BERT model over 4 epochs.\nImplementation details: For AT-DA and AT-IP\nmethods, we use the parameters proposed by the\ncorresponding papers. For our PGD-AT and LDS\napproaches, we limit the number of PGD steps to 3\nand the perturbations L2-norm to 0.003. All exper-\niments are conducted on Nvidia v100 Tensor Core\nGPU.\nRun-time results: We report the time for fine-\n7833\ntuning the models over 4 epochs in Tab. 3. The AT-\nDA approaches results in the shortest fine-tuning\ntime as adversarial examples are generated once for\nevery sample before the training, unlike in AT-IP\nand AT-EP where adversarial examples are gener-\nated at every training iteration. AT-EP methods,\nare around 1.5 times slower to fine-tune than the\nstandard BERT model as generating the adversarial\nexamples requires an additional backward pass for\ncomputing the gradient of the loss, at every training\niteration. As expected, AT-IP methods are the most\ntime consuming as they involve a combinatorial\nsearch over a large number of input space configu-\nrations. For example, the fastest approach in this\nclass, A2T, needs 6 seconds for a single adversar-\nial example generation, which is around 10 times\nslower than the other approaches.\nTable 3: Run-time results. We report the fine-tuning run-\ntime over 4 episodes on the MR and IMDB datasets.\nModels Run Time (in min)\nIMDB MR\nBERT 79.0 38.2\nAT-DA\nSSMBA 112.8 46.4\nBackTranslation 210.5 66.0\nAT-IP\nA2T 1600.5 448.5\nA2T_MLM 1494.3 504.7\nBERT_Attack 1495.2 461.5\nAT-EP\nLDS 163.4 64.2\nPGD-AT 158.2 69.0\nRobustness results are shown in Tab. 2. The lower\nthe ASR the better is the model in withstanding\nthe attack. As expected, the most effective meth-\nods against adversarial attacks are the AT-IP ones.\nThis is due to the fact that the only class of ap-\nproaches were it’s possible to match the attack and\nthe defense strategies, i.e., train on perturbations\ngenerated from the attack strategies, is AT-IP, as\nattacks in language models operate in the input\nspace. Among AT-AD methods, BackTranslation\nis the most robust method on the IMDB dataset. We\nfound that this is due to IMDB having in average\nlong sentences which makes it easier to generate\ngood and diverse adversarial examples to train on,\nvia back translation. Our results show that AT-EP\nmethods are the least robust. In particles, LDS-AT\nstruggle in the sentiment classification task due to\nnoisy ground-truth label, i.e., sentiments are mostly\nnot binary but the ground truth labels are.\nGeneralization results are reported in Tab. 4.\nAT-DA accuracy values are comparable to BERT.\nHence, it looks like AT-DA generalization capa-\nbilities are not traded-off for better robustness\nas it is the case of AT-IP approaches. This is\ndue to the fact that adversarial examples from\nSSMBA (self-supervised-based) and BackTrans-\nlation (translation-based) are generated while tak-\ning the global context into account. So they are\nunlikely to change the semantics of the input text\nand hence the decision boundaries. These meth-\nods are however unpractical for usage inside of\nthe training loop. More efficient techniques, e.g.,\nbased on local search in the embedding space, are\nused by AT-IP methods. This however might not\nalways lead to preserving the semantics of the orig-\ninal input text, which also means that assigning the\nlabel of the ground truth input to these adversarial\nexamples might be inappropriate or noisy. Such\nhard examples are well known to encourage over-\nfitting and hence reduce the generalization ability\nof the model. This explains the significant drop in\nboth in and out-of-distribution accuracy values of\nAT-IP approaches. The best generalization results\nare obtained using AT-EP methods. We notice that\nPGD-AT consistently improves upon BERT. This\nphenomena doesn’t occur in vision where general-\nization is well know to drop in adversarially trained\nmodels. To the best of our knowledge, we are\nthe first to report this in language models trained\nwith embedding space perturbation. In order to\ngain a better understanding of the reasons behind\nthis phenomena, we investigate the learned dynam-\nics of deepnets trained with AT-EP methods using\nLinguistic Correlation Analysis (next paragraph).\nSpecifically, we want to validate that the achieved\naccuracy was due to better learning to solve of the\ntask at hand and not just due to memorizing the\ntraining data.\nLinguistic Correlation Analysis (LCA, Dalvi\net al., 2019) is used to identify the most salient neu-\nrons for a given linguistic property like a Parts-of-\nSpeech (POS) tag (Sajjad et al., 2022). To achieve\nthis, we first match words to neurons, then assess if\nthe matched words have the linguistic property of\ninterest. As the sentiment prediction task is not ap-\npropriate for word level analysis, i.e., same words\ncan be part of different sentiment classes, we focus\non POS tagging task. We fine-tune BERT mod-\nels using AT-EP methods on the publicly available\nPenn Treebank dataset (Marcinkiewicz, 1994). We\nuse LCA to generate a list of the top-5 firing neu-\n7834\nTable 4: Generalization results. We report the accuracy\nvalues on IMDB/MR (in-distribution) and YELP (out-\nof-distribution) datasets for BERT models fine-tuned on\nIMDB/MR for the task of sentiment classification.\nModels\nIMDB MR\nIMDB YELP MR YELP\nBERT 93.49 91.24 85.27 87.06\nAT-DA\nSSMBA 93.49 91.17 85.24 87.72\nBackTranslation 93.44 91.50 84.96 87.77\nAT-IP\nA2T 92.59 89.97 83.58 83.62\nA2T_MLM 92.70 89.15 83.90 81.79\nBERT_Attack 92.63 90.04 84.61 80.41\nAT-EP\nLDS 93.24 92.09 86.49 81.80\nPGD-AT 93.80 92.11 86.59 88.16\nTable 5: LCA results. The association strength between\nPOS tags and neurons.\nPOS BERT LDS PGD-AT\nMatch Total % Match Total % Match Total %\nJJ 2 15 13.33 2 15 13.33 6 10 60.00JJR 3 9 33.33 4 9 44.44 7 13 53.84MD 0 5 0.00 3 5 60.00 2 5 40.00VBD 5 5 100.000 5 0.00 0 5 0.00: 0 5 0.00 1 5 20.00 3 5 60.00VBZ 4 10 40.00 7 9 77.77 9 10 90.00RB 9 10 90.00 9 10 90.00 6 10 60.00VBG 10 12 83.33 14 18 77.77 15 15 100.00\nrons for every POS tag and leverage these lists to\nperform two types of analysis: (1) neurons-POS\ntags association strength analysis and a (2) a neu-\nral ablation analysis. To assess the neurons-tag\nassociation strength, given the list of the top-firing\nneurons from LCA, we next generate a list of the\nwords in the testing data with the highest activa-\ntion values for these neurons. Then, we compute\nthe intersection between the generated word list\nand the ground-truth one, i.e., the list of words\nwith label being the POS tag of interest in the test-\ning data. A large intersection set means that the\nneurons learned to specialize in predicting specific\nPOS tags, i.e., they learned the linguistic nuances\nof the task and are unlikely to have just memorized\nthe training data. Results in Tab. 51 show that our\nAT-EP learn more ‘focused’ neurons as measured\nby the intersection ratio (match/total). In particular,\nPGD-AT significantly improves upon the standard\nBERTB model.\nTable 6 provides words corresponding to select\n1Definitions of POS tags with their order in the table: ad-\njective; adjective, comparative; modal; verb, past tense; colon,\nsemi-colon; verb, 3rd person singular present; adverb; verb,\ngerund or present participle\nPOS tags obtained from the models trained with\nthe BERTB, the LDS, and PGD-AT methods.\nFor the second analysis, i.e., the neural ablation\nstudy, we create a linear regression model using\nonly activations of the top 10 ranked neurons. Re-\nsults are shown in Tab. 7. PGD-AT and LDA\nachieve a significantly higher performance than\nBERT, which further support the observation that\nAT helped better learn the intricacies of the tasks\nand explains the improvement of the generalization\nabilities of the AT-EP approaches (e.g., in Tab. 4).\n5 Conclusions\nIn this paper we have carried out an extensive study\nof adversarial training methods (ATMs) to under-\nstand their impact on robustness and generalizabil-\nity for transformer-based deep language models.\nWe can draw the following conclusions from our\nstudy. First, non-adversarial data augmentation im-\nproves both generalization and robustness over the\nbaseline BERT model. Adversarial training in the\ninput space yields better robustness compared to\nboth non-adversarial data augmentation and em-\nbedding space adversarial training. In contrast, ad-\nversarial training in the embedding space exhibits\nbest generalization behavior. Among PGD-AT and\nLDS methods, our results show that the PGD-AT is\nconsistently more robust and generalizable. Over-\nall, our results show that unlike in computer vision\ndomain where gradient-based adversarial training\nyields the best robustness and generalization trade-\noff, for language processing models input-space\ntraining methods are indispensable.\nFor future work we will consider combining data\naugmentation, input-space training, and embedding\nspace training approaches together. We would also\nlike to extend our theoretical understanding of the\ntrade-off between robustness and generalizability\nfor language models. In connection, the impact of\nATMs for other downstream applications needs to\nbe studied.\nLimitations\nAll our experiments are performed using the BERT-\nsmall language model due to the computational\nrequirements of generating and testing models con-\nsidering many configurations of adversarial train-\ning and attack methods. Although using larger\nlanguage models might have provided different per-\nformance measurements, our findings that compare\ninput- and embedding-space adversarial training\n7835\nTable 6: Examples of the most related words for different POS tags for models trained with the BERTB, the LDS,\nand PGD-AT methods. The words are bolded when their actual tags match with the associated tag, where the actual\ntags correspond to the most frequent tags of the words based on the POS-tagged training data.\nPOS BERTB LDS PGD-AT\nVBZ\nindicatesteenage Andbegins indicates denies erodes explainsindicates accounts refuses agrees\nreflects explainsevil Previously resemblesAndruns is has believesAnd\nautomatic reckless addstrains adds begins\nJJ\nRae awaylittleSprings Nelson Aktiebolaget least plummeted Do policiesbrightaway whathigh\nliveequalWhat explain Giants littletold Whatequalsecurities strong coldskyrocketed\nWho Aktiebolaget skyrocketed what rungDallara added said most cardboard greenWhatsame\nJJR\nnewermeaninggreaterpunish includednewer greater smallerincludednewer strongermeaning\nincluded banking close indicated shipbuilding arrangedsmaller greaterindicated planning\nsmallerher Higherher highercloseHigher lowerleast\nMD associated bright required severe deniedapartshall might mustfallen fallenshallexpectedmightapart\nVBD restored bothered notched mixed beganexpire face exist become buy expire face become exist disagree\nTable 7: LCA results. Neural ablation study.\nBERT LDS PGD-AT\n34.2% 38.6% 35.3%\nmethods are expected to remain unchanged. An-\nother limitation of our work is the semantic gap be-\ntween attacks in input and embedding space needs\nfurther research. Specifically, how do perturbations\nin the embedding space get translated in the input\nspace? Finally, other forms of robustness tech-\nniques, besides adversarial training, in the context\nof large language models require examination.\nEthics Statement\nThe work studied the impact of several adversarial\ntraining methods on robustness and generalization.\nThe work did not result in any new dataset and\nmodel and it has no potential ethical issues. On\nthe positive side, the work targets two important\nattributes of trustworthy AI i.e. robustness and gen-\neralization. Our work provides an insightful com-\nparison of the input-space and embedding space\nadversarial training approaches and will positively\nimpact the future research work in this area.\nReferences\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial exam-\nples. arXiv preprint arXiv:1804.07998.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do Neural\nMachine Translation Models Learn about Morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (ACL),\nVancouver. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, D. Anthony Bau, and James Glass. 2019.\nWhat is one grain of sand in the desert? analyzing in-\ndividual neurons in deep nlp models. In Proceedings\nof the Thirty-Third AAAI Conference on Artificial\nIntelligence (AAAI, Oral presentation).\nFahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Dur-\nrani, Jia Xu, and Hassan Sajjad. 2022. Discovering\nlatent concepts learned in BERT. In International\nConference on Learning Representations.\nFahim Dalvi, Hassan Sajjad, and Nadir Durrani. 2023.\nNeurox library for neuron analysis of deep nlp mod-\nels. In Proceedings of the Association for Computa-\ntional Linguistics (ACL).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nGavin Weiguang Ding, Yash Sharma, Kry Yik Chau\nLui, and Ruitong Huang. 2018. Mma training: Direct\ninput space margin maximization through adversarial\ntraining. arXiv preprint arXiv:1812.02637.\nYinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su,\nJun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting\nadversarial attacks with momentum. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 9185–9193.\n7836\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4865–4880, Online. Association for Computational\nLinguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. arXiv preprint arXiv:1808.09381.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale adver-\nsarial training for vision-and-language representation\nlearning. Advances in Neural Information Process-\ning Systems, 33:6616–6628.\nSiddhant Garg and Goutham Ramakrishnan. 2020. Bae:\nBert-based adversarial examples for text classifica-\ntion. arXiv preprint arXiv:2004.01970.\nIan J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2014. Explaining and harnessing adver-\nsarial examples. arXiv preprint arXiv:1412.6572.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nPo-Sen Huang, Robert Stanforth, Johannes Welbl,\nChris Dyer, Dani Yogatama, Sven Gowal, Krish-\nnamurthy Dvijotham, and Pushmeet Kohli. 2019.\nAchieving verified robustness to symbol substitu-\ntions via interval bound propagation. arXiv preprint\narXiv:1909.01492.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and Percy\nLiang. 2019. Certified robustness to adversarial word\nsubstitutions. arXiv preprint arXiv:1909.00986.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification\nand entailment. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, pages\n8018–8025.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang\nXue, and Xipeng Qiu. 2020. Bert-attack: Adver-\nsarial attack against bert using bert. arXiv preprint\narXiv:2004.09984.\nLinyang Li and Xipeng Qiu. 2021. Token-aware vir-\ntual adversarial training in natural language under-\nstanding. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 8410–8418.\nZongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiao-\nqing Zheng, Qi Zhang, Kai-Wei Chang, and Cho-Jui\nHsieh. 2021. Searching for an effective defender:\nBenchmarking defense against adversarial word sub-\nstitution. arXiv preprint arXiv:2108.12777.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu\nChen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n2020. Adversarial training for large neural language\nmodels. arXiv preprint arXiv:2004.08994.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2017.\nTowards deep learning models resistant to adversarial\nattacks. arXiv preprint arXiv:1706.06083.\nMary Ann Marcinkiewicz. 1994. Building a large anno-\ntated corpus of english: The penn treebank. Using\nLarge Corpora, 273.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii. 2018. Virtual adversarial training:\na regularization method for supervised and semi-\nsupervised learning. IEEE transactions on pattern\nanalysis and machine intelligence, 41(8):1979–1993.\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,\nand Pascal Frossard. 2016. Deepfool: a simple and\naccurate method to fool deep neural networks. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2574–2582.\nNikola Mrkši´c, Diarmuid O Séaghdha, Blaise Thom-\nson, Milica Gaši´c, Lina Rojas-Barahona, Pei-Hao Su,\nDavid Vandyke, Tsung-Hsien Wen, and Steve Young.\n2016. Counter-fitting word vectors to linguistic con-\nstraints. arXiv preprint arXiv:1603.00892.\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\n2020. Ssmba: Self-supervised manifold based data\naugmentation for improving out-of-domain robust-\nness. arXiv preprint arXiv:2009.10195.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nLin Pan, Chung-Wei Hang, Avirup Sil, and Saloni Pot-\ndar. 2022. Improved text classification via contrastive\nadversarial training. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 36, pages\n11130–11138.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. arXiv preprint cs/0506075.\n7837\nNicolas Papernot, Patrick McDaniel, Somesh Jha, Matt\nFredrikson, Z Berkay Celik, and Ananthram Swami.\n2016. The limitations of deep learning in adversarial\nsettings. In 2016 IEEE European symposium on secu-\nrity and privacy (EuroS&P), pages 372–387. IEEE.\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang.\n2018. Certified defenses against adversarial exam-\nples. In International Conference on Learning Rep-\nresentations.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125.\nScott Reed, Konrad Zolna, Emilio Parisotto, Ser-\ngio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, et al. 2022. A\ngeneralist agent. arXiv preprint arXiv:2205.06175.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn Proceedings of the 57th annual meeting of the as-\nsociation for computational linguistics, pages 1085–\n1097.\nHassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022.\nNeuron-level interpretation of deep NLP models: A\nsurvey. Transactions of the Association for Compu-\ntational Linguistics, 10:1285–1303.\nZhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie\nHuang, and Cho-Jui Hsieh. 2020. Robustness verifi-\ncation for transformers. In International Conference\non Learning Representations.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2818–2826.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. 2013. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199.\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom,\nAlexander Turner, and Aleksander Madry. 2019. Ro-\nbustness may be at odds with accuracy. In Interna-\ntional Conference on Learning Representations.\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey,\nXingjun Ma, and Quanquan Gu. 2019. Improving ad-\nversarial robustness requires revisiting misclassified\nexamples. In International Conference on Learning\nRepresentations.\nJason Wei and Kai Zou. 2019. Eda: Easy data augmenta-\ntion techniques for boosting performance on text clas-\nsification tasks. arXiv preprint arXiv:1901.11196.\nEric Wong and Zico Kolter. 2018. Provable defenses\nagainst adversarial examples via the convex outer\nadversarial polytope. In International conference on\nmachine learning, pages 5286–5295. PMLR.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33:6256–6268.\nMao Ye, Chengyue Gong, and Qiang Liu. 2020. Safer:\nA structure-free approach for certified robustness\nto adversarial word substitutions. arXiv preprint\narXiv:2005.14424.\nJin Yong Yoo and Yanjun Qi. 2021. Towards improving\nadversarial training of nlp models. arXiv preprint\narXiv:2109.00544.\nLifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei.\n2021. Bridge the gap between cv and nlp! a gradient-\nbased textual adversarial attack framework. arXiv\npreprint arXiv:2110.15317.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan\nLiu, Meng Zhang, Qun Liu, and Maosong Sun.\n2019. Word-level textual adversarial attacking\nas combinatorial optimization. arXiv preprint\narXiv:1910.12196.\nJiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li,\nLiping Yuan, and Xuanjing Huang. 2021. Certified\nrobustness to text adversarial attacks by randomized\n[mask]. arXiv preprint arXiv:2105.03743.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing,\nLaurent El Ghaoui, and Michael Jordan. 2019. Theo-\nretically principled trade-off between robustness and\naccuracy. In International conference on machine\nlearning, pages 7472–7482. PMLR.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2019. Freelb: Enhanced\nadversarial training for language understanding.\n7838\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nUnder the Limitation\n□\u0013 A2. Did you discuss any potential risks of your work?\nUnder the Ethics Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nAll datasets are publicly available\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. All datasets are publicly available\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7839\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7840"
}