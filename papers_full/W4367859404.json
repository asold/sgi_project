{
    "title": "Towards autonomous system: flexible modular production system enhanced with large language model agents",
    "url": "https://openalex.org/W4367859404",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5102771100",
            "name": "Yuchen Xia",
            "affiliations": [
                "University of Stuttgart"
            ]
        },
        {
            "id": "https://openalex.org/A5071243263",
            "name": "Manthan Shenoy",
            "affiliations": [
                "University of Stuttgart"
            ]
        },
        {
            "id": "https://openalex.org/A5042926074",
            "name": "Nasser Jazdi",
            "affiliations": [
                "University of Stuttgart"
            ]
        },
        {
            "id": "https://openalex.org/A5072168528",
            "name": "Michael Weyrich",
            "affiliations": [
                "University of Stuttgart"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2279846777",
        "https://openalex.org/W4312499804",
        "https://openalex.org/W4310925484",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4312571801",
        "https://openalex.org/W3216708762",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W4312300824",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W3158551399",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W3121192324",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4364379393",
        "https://openalex.org/W2972498556",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W3170148270"
    ],
    "abstract": "In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. We retrofit the automation system for a modular production facility and create executable control interfaces of fine-granular functionalities and coarse-granular skills. Low-level functionalities are executed by automation components, and high-level skills are performed by automation modules. Subsequently, a digital twin system is developed, registering these interfaces and containing additional descriptive information about the production system. Based on the retrofitted automation system and the created digital twins, LLM-agents are designed to interpret descriptive information in the digital twins and control the physical system through service interfaces. These LLM-agents serve as intelligent agents on different levels within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production process, and execute the operations. This research highlights the potential of integrating LLMs into industrial automation systems in the context of smart factory for more agile, flexible, and adaptive production processes, while it also underscores the critical insights and limitations for future work. Demos at: https://github.com/YuchenXia/GPT4IndustrialAutomation",
    "full_text": "This is the pre-print draft manuscript for early researcher discussion. The peer-reviewed version will be published exclusively \nby IEEE after the ETFA2023 conference, which is set to take place from September 12th to 15th, 2023. We've made several  \ncritical improvements to the final version of the paper based on valuable feedback and suggestions from other researchers.  \nTowards autonomous system: flexible modular \nproduction system enhanced with large language \nmodel agents\nYuchen Xia \nInstitute of Industrial Automation \nand Software Engineering \nUniversity of Stuttgart \nStuttgart, Germany \nyuchen.xia@ias.uni-stuttgart.de \nManthan Shenoy \nInstitute of Industrial Automation \nand Software Engineering \nUniversity of Stuttgart \nStuttgart, Germany \nst175289@stud.uni-stuttgart.de \nNasser Jazdi \nInstitute of Industrial Automation \nand Software Engineering \nUniversity of Stuttgart \nStuttgart, Germany \nnasser.jazdi@ias.uni-stuttgart.de \nMichael Weyrich \nInstitute of Industrial Automation \nand Software Engineering \nUniversity of Stuttgart \nStuttgart, Germany \nmichael.weyrich@ias.uni-stuttgart.de \n \nAbstract — In this paper, we present a novel framework that \ncombines large language models (LLMs) , digital twin s and \nindustrial automation system to enable intelligent planning and \ncontrol of production process es. We retrofit the automation \nsystem for a modular production facility and create executable \ncontrol interfaces of fine-granular functionalities  and co arse-\ngranular skills. Low-level function alities are executed by \nautomation components, and high-level skills are performed by \nautomation modules. Subsequently, a digital twin system  is \ndeveloped, registering these interfaces  and containing \nadditional descriptive information about the production system. \nBased on the retrofitted automation system and the created \ndigital twins, LLM-agents are designed to interpret descriptive \ninformation in the digital twins and control the physical system \nthrough service interfaces. These LLM-agents serve as \nintelligent agents on different levels within an automation \nsystem, enabling autonomous planning and control of flexible \nproduction. Given a task instruction as input, the LLM-agents \norchestrate a sequence of atomic functionalities and skills to \naccomplish the task. We demonstrate how our implemented \nprototype can handle un-predefined tasks, plan a production \nprocess, and execute the operations. This research highlights the \npotential of integrating LLMs into industrial automation \nsystems in the context of smart factory for more agile, flexible, \nand adaptive production processes, while it also underscores the \ncritical insights and limitations for future work.  Demos at: \nhttps://github.com/YuchenXia/GPT4IndustrialAutomation \nKeywords— autonomous system, intelligent agent, GPT , \ndigital twin, Asset Administration Shell, smart factory \nI. INTRODUCTION \nFlexible production has emerged as a significant aspect of \nmodern manufacturing environments in response to changing \nmarket demands and product customiz ation requirements. \nManufacturers need to adapt quickly to market changes and to \nstay competitive.  This leads the manufacturer to  consider \ndiversifying their products and provid ing customized \nmanufacturing services, which requires an agile production \nsystem and efficient management of  the complexity of the  \nproduction.  \nHowever, there are several technical challenges for \ndeployment of agile and flexible production in reality: First of \nall, flexible production requires  seamless integration  of \ndiverse technologies solu tion, e.g., robotics, automation, \nplanning algorithms etc. Secondly, the production equipment \nand manufacturing processes need to  be reconfigurable \n[1][2], which requires modular processes and systems as well \nas reconfigurable machines. Furthermore, automated flexible \nproduction also requires quick changeover [3] after decision-\nmaking to adapt the production against the changing \nrequirements. Eventually, a highly knowledgeable \nworkforce in every complicated technology with high \navailability to manage and supervise the complex system is \ntoo luxurious to be true . Traditional production systems \nfrequently face difficulties in fulfilling these requirements due \nto their inflexible  [1], dedicated workflows and restricted \nadaptability, as well as the absence of domain -specific \nknowledge in reconfiguring the production facility.  \nTo tackle these challenges and requi rements, we propose a \nnovel solution: a large language model (LLM) enhanced \nautomated modular production system  for flexible \nmanufacturing.  \nOur messages and contributions from this paper are \nsummarized as follows: \n(1) We demonstrate with a representative use case \nexplaining why and how large language models can \nbe used to achieve a higher level of intelligence  and \nadaptability of industrial automation systems  by \nplanning and controlling the production, especially in \nthe context of flexible production scenarios.  \n(2) We structure the system design according to  the \nautomation pyramid, illustrating a feasible technical \napproach to integrate LLMs into automation system. \n(3) We prefer  the more scalable  in-context-learning \napproach over the fine-tuning approach, and the task-\nspecific knowledge is injected into a LLM in prompt.  \nAs prompt engineering is an emerging field with little \nstandardization, we devise a structured prompt \ntemplate for this use case, drawing on insights from \nexisting research in Natural Language Processing. \nII. BACKGROUND \nIn this section, we start by discussing why and how  \nmodular production systems can meet the requirements  \nseamless integration and reconfigurability  for flexible \nproduction. Then we emphasize the importance of modular \nquery and control interfaces to allow  the LLM  to access \ninformation about the physical production processes and to \nadapt the production to changing requirements.  Last but not \nleast, we provide a brief overview of LLMs and the \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \nfundamental reasons why they have the potential to handle \ndomain-specific tasks in industrial automation. \nA. Modular Production Systems \nModular production systems are developed to address the \nchallenges of flexibility, scalability, and adaptability in \nmanufacturing. These systems consist of a series of modular \nmodules, which can be easily reconfigured, replaced, or \nupdated to accommodate varying production requirements. In \nthe following introduction, w e categorize them into three \ntypes: linear, parallel and matrix modular production. \n1) Linear modular production \nThe production process follows a step-by-step sequence, with \neach module performing its designated task before passing the \nworkpiece to the next module.  In discrete production like \nautomotive assembly  [4], the material can be processed \ndifferently with variable process module. In continuous \nproduction in the process industry, the process plants can be \ndesigned modular to  decrease efforts and cost in  system \nplanning, integration, and configuration [5]. \n2) Parallel modular production \nThe production system enables multiple modules and lines to \noperate concurrently on a variety of tasks. In comparison, \nparallel modular production supports simultaneous production \noperations, allowing a workpiece to be processed by multiple \nmodules [4], which further increases flexibility. To effectively \ncombine various production modules, additional \ntransportation systems are necessary for seamless process \nautomation. \n3) Matrix modular production \nThe matrix modular production  [6] decouples the l ogistics \ntasks from production and changes the rigid line structure into \nmatrix structure, which consist of modular production cells \nand automated transportation sys tems (often by applying \nAutomated Guided Vehicle). These systems comprise \nindependent modules that can be reconfigured and combined \nto execute a wider range of production tasks. As various  \nproduction modules with different specialties can be \nrearranged, added, or removed with minimal impact on the \noverall system, the matrix production has the potential to \nquickly adapt to diverse requirements, customer preferences, \nand market demands.  Some literature also refers to this \nproduction type as Matrix Manufacturing Systems (MMS) [7]. \nDespite the structural superiority of matrix modular \nproduction for flexible reconfiguration, planning and process \norchestration for customized production tasks still rely on the \naccumulated expertise within a company. Identifying a \nfeasible solution to a problem can be time -consuming if any \npart of the required knowledge is unavailable or if there is a \nlack of effective communication among experts. \nTable 1 Comparison of different types of modular production \nagainst the changing customer demand examples \nLarge Language Models (LLMs) possess the capability to \ninterpret information, generate reasoning insights, and assist \nin decision -making proces ses. Trained on vast amounts of \ndata, LLMs can understand and process complex information \nacross various domains. By harnessing the interpretation and \nreasoning abilities of LLMs, the planning and process \norchestration can be streamlined. This can lead to faster \nproblem-solving and better adaptation to customer demands. \nB. Digital Twins \nDespite the vast knowledge and reasoning capabilities of \nlarge language models (LLMs), a critical question remains: \nHow can the LLMs access real -world information and \neffectively address tasks in practical settings? \nThe current state-of-the-art automation systems are not fully \nequipped to offer comprehensive descriptive information \nabout production and unified accessible interfaces for \nquerying and controlling physical processes. We developed a \ndigital twin system to bridge the gap between LLMs and the \nphysical world , as shown in Figure 2 . Digital twins are \nsynchronized virtual representations of physical assets or \nprocesses [8]. The  digital twin system contains  descriptive \ninformation about the production and exposes unified \ninterfaces to LLM for manipulating the physical system. We \nlay special stress on the synchronization characteristics \nbecause it is fundamental to allow  the reactive intelligent \nbehavior of an autonomous system.  \nCase examples Requirements Linear \nMP \nParallel \nMP \nMatrix \nMP \nCustomer wants the \npackaging material to be \npaper instead of plastics. \nVariation of \nmachine \nfunctionalities \n+ + + \nCustomer wants an \nengraved logo on the \nproduct instead of painted \nlogo. \nVariation of \ncertain process \nsteps \n- + + \nCustomer wants a special \nHiL-quality test on the \nproduct in the middle of \nproduction process. \nVariation of \nochestration of \nprocesses \n- - + \nCustomer returns the \nproduct due to a quality \nfault and demands \nreprocessing \nVariation of \nproblem-\nsolving process \n- - ○ \n+ : \n– : \n○ : \nRequirements fulfilled without change of production system  \nRequirements hardly fulfilled due to the inflexible material flows  \nRequirements can be fulfilled with experts intervene and effort \n \nIN OUT\nIN OUT\nIN\nOUT\nAGV\nAGV\ncustomizable \npainting Inspection customizable \npackaging\nprogrammable \nCNCInspection\nInspection programmable \nCNC\nforming Laser\nengraving\ncustomizable \npainting Inspection customizable \npackaging\nInspection forming\nLaser\nengraving\ncustomizable \npainting\nprogrammable \nCNC\ncustomizable \npackaging\nLinear modular production\nParallel modular production\nMatrix modular production\nworkpieces\nProduct A\nProduct B\nProduct A\nProduct B\nVariable products\nworkpieces\nworkpieces\n• Separation of module specialty\n• Decoupling of transportation\n→Organization flexibility\n→ Adaptability to changes\n…\nFigure 1 Three structure types of modular production system  \n \nDigital Twin System\nAutomation System Intelligent Agents\n(Large Language Model)\nHigh-Fidelity Vitual\nRepresentation & Synchronization\nKnowledge Access &\nInterface Access\nMediate Interaction with Physical World\nFigure 2 The overall framework of the general concepts and \ntheir relationships \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \nOn one hand, automation systems are enhanced with \ndigital twins and LLMs to unlock the potential of  data- and \nAI-driven smart factories. On the other hand, LLMs interact \nwith physical environments by having an  embodiment in \nreality through the established infrastructure that combines  \nautomation systems and digital twins . This approach equips \nan artificial “brain” with mechatronic “hands” and “eyes” for \nmore intelligent interaction. \nC. LLM in automated Production Systems \nLLMs can be utilized to interpret complicated information, \ngenerate insights, and support decision -making processes in \nindustrial automation systems.  \nAs illustrated in Figure 3, LLMs are deep learning models \ntrained on vast amounts of text data, enabling them to generate \nhuman-like responses and handle complex language patterns \nacross various NLP tasks. Recent advancements in NLP \nresearch have uncovered a promising finding: as the neuron \nsize of LLMs in creases, well-trained models gain the ability \nto interpret the meaning conveyed through language and \ndemonstrate a capability of approximating human \nknowledge behind the language representation—a capability \nbeyond the languages processing and not observed in smaller \nneural networks [9][10]. This development allows LLMs to \nperform general reasoning tasks effectively. Furthermore, as \nthe training data for LLMs includes scientific papers, books, \nQ&A forums, and software code, LLMs are als o informed \nwith diverse domain -specific knowledge, which can be \nutilized for executing engineering related tasks. \nBy employing prompt engineering techniques  [11], we \ndevelop multiple intelligent agents at both the MES level and \nthe automation module level within the automation pyramid. \nThese agents are specifically designed to manage production \ntasks within their respective scopes. \nIII. METHODS \nIn this section, we explain how we connect the LLM to the \ndigital twin infrastructure with prompt engineering, allowing \nintelligent agents to manage and control the production  \noperations to solve an unforeseen problem. \nA. Integrate the information and expose the service \ninterfaces of the digital twin \nFirst and foremost, the large language model agent requires \nhigh-fidelity information to accurately comprehend the \nproduction system. Thus, a data infrastructure that houses \n \n1 The Asset Administration Shell comprises extensive information pertaining \nto an asset, and it organizes this data into sub -models based on various \naspects. \ncomprehensive information about the production system is \nfundamental. We model the production system  in a digital \ntwin system in a modular and cascaded manner. These \nmodular digital twins contain detailed information about their \nrepresented assets.  \nThe descriptive information in the digital twin  system is \nmodeled in the form of Asset Administration Shells (AAS) 1 \nand managed  with an AAS-middleware 2. Within the AAS, \nquery and command services are referenced as URLs, which \nare semantically annotated with interface description in the \nskill sub-model. These interfaces enable the querying of asset \nstates and control over automation system functionalities \nthrough RESTful service calls.  \nAs shown in Figure 5, the digital twin of an automation \nmodule “Transport Robot Robotino_7” contains the \ncascaded information about its “skills”, the references to its \n“components”, callable “functionalities” interface, and other \ncomprehensive information in sub-models “technical data” \n“operational data”  “documentation” and “software” for \nfurther information. \nBased on the descriptive knowledge about the assets and the \ncallable interfaces provided by the digital twin system , it is \npossible to build two types of intelligent agents: A manager \n2 We used Basyx AAS-middelware. \n \nERP\nLLM enhanced MES\nLLM enhanced\nAutomation \nModules\nSensors /\nActuators\n+\nServices Services Services\n \nHuman knowledge\nDomain knowledge\n…\nKnowledge required \nto perform a specific \ntask\nDomain & task \nadaptation\nContext-\nualization\nArtificial\nGeneral\nIntelligence\nArtificial\nSpecial \nIntelligence\nPre-training on a wide range of texts \nencompassing knowledge from various domains\n \nFigure 3 Integration of LLM-agents and digital twins in \nautomation systems for enhanced intelligence \nFigure 5 Digital twin representation of the production system \nmodeled with asset administration shell. \nFigure 4 Underlying general mechanism enabling LLMs to \naddress domain-specific task. \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \nagent that works on the top of  the automation modules, \norchestrating diverse skills of the automation modules to plan \nthe production; and several operator agents work within a \nparticular automation module, orchestrating diverse \nfunctionalities to execute a given skill, as shown in Figure 5 \nand 6. Designing more than one agent is necessary to break \ndown the challenging task into several manageable sub-tasks. \nB. Creating a LLM agent to adapt to a specific task with \nprompt engineering \nWe create these agents by contextualizing a GPT-model with \nprompts. The structure  of the designed  prompt and the \ninteractions between digital twin and GPT-agent through the \nprompt are shown in Figure 7. More detailed examples are \nshown in the next implementation section. \nThe digital twin and the GPT-agents are connected via a \nprompt, which is sent to  a LLM to initiate a response from \nthe model. The prompt also serves as a trigger for the LLM \nto generate contextually appropriate text based on the given \ninput. As shown in Figure 7, o ur designed prompts are \ncomposed of five distinct sections  that target specific core \nworking mechanisms of the GPT model s. In the following \ntexts, w e explain them along with the design reasons. To \n \n3 This portion of the prompt targets the zero -shot learning capabil ity [16], \nwhich enable large language models (LLMs) to perform previously \nunencountered tasks without explicit examples, particularly when the model \nhas undergone instruct-fine-tuning optimization [17]. \n4 A token can be defined as a basic meaningful unit of the text in input and \noutput. \n5 Attention mechanism is analyzed and visualized in [18]. \nincrease the readability, we put further in-depth explanations \nrelated to NLP in footnotes.  \n1) The role and the goal \nThis section of the prompt outlines the role and objective of \nan agent in natural language 3 , providing the model with \nclarity regarding the particular task that is expected to carry \nout.  \nBy defining the role and objective concisely, the text can \neffectively convey intentions and expectations with fewer \ntokens4. This offers two benefits: Firstly, it enables the model \nto better align with the general requirements, producing \noutputs that adhere to the desired role (e.g., operator, manager, \nadvisor) and goal (e.g., performing operations, finding \nsolutions, providing suggestions). Secondly, concise text \nconcentrates meaning within fewer tokens, allowing t he \nmodel to infer stronger related connections  between text \nelements with higher attention weight 5 while preventing the \ndilution of the model’s attention. \n2) Context \nThis section presents information derived from digital twins, \naiming to supply descriptive information about the particular \nproduction system t hat the model needs for effective \nreasoning. As shown in table 2 and 3 in the Implementation \nSection, the  knowledge should at least contain the objects \ndescription, the skills and functions of the objects and the \nmappings to the service interfaces of the executable \noperations. However, as the knowledge representation in \ndigital twins’ software system and texts in natural language \nare differen t, the information from the digital twin model \nshall be converted in text form in natural language, e.g., with \nfill-in-the-template mechanism  and conca tenation of text \nstrings. \nThe converted information from digital twins  offered in this \ncontext section serves two purposes: first, it enables the \nmodel to comprehend the production system ’s operations, \nincorporating additional information about the particular \nsystem. Secondly, as GPT has been trained across a wide \nrange of subjects, it possesses extensive general knowledge \nthat is implicitly stored within its model weights  [12]. The \ndescriptive information provided in the prompt guide GPT to \n“concentrate” 6 on the related knowledge embedded within \nthe model  when generating text . In this sense, this process \nactualizes the combination of the general knowledge of GPT \nwith the special knowledge specified in prompt to execute  \nreasoning for specific task. \n3) Instructions \nThis section  aims to  guide the GPT -agent’s behavior by \nspecifying the desired output formats and establishing \nboundaries for the generated content. We also encourage the \nmodel to “think step -by-step”7, a widely adopted strategy \namong researchers, to facilitate logically structured reasoning \n6 This concentration is based on the mechanism of auto-regression process in \ntext generation [13], where predicted text is continuously generated based on \nprevious seen tokens. \n7 Also referred to as “chain-of-thought-prompting” [19] which significantly \nimproves accuracy in performing complex reasoning [20][21]. Models that \ntrained on code generation typically exhibit superior performance in step-by-\nstep reasoning [19]. \n \nAutomation Module\nManager Agent\nProvide Context\nOperator Agent\n Operator Agent\nFunctionalities\nOrchestration\nResult\n…\nInvoke Skill\nInvoke Skill\nProvide Context\n& a skill objective\nProvide Context\n& a skill objective\nFunctionalities\nOrchestration\nResult\nSkills\nOrchestration\nResult\nMES\nAutomation Module\nTask Input\n \n• The role and goal\n• Context\n• Objects description\n• Callable skills/functions\n• Mapping between RESTful\ninterface and skills/functions\n• Other information\n• Instruction\n• Constraints and instructions\n• Illustrative examples\n• Input: [an example of input that the agent gets]\n• Output: [an example of output to be generated]\n(… multiple examples …)\n• Current task:\n• Input: [a task]\n• Output: \nprovide \ncontext \ninformation\nInteraction\nServices Services\nThe prompt template for stateless GPT-Agent\n: Information in dynamic \ninteraction with digital \ntwin system\n: Static information\n……\nDigital Twin System\nFigure 6 Interactions between LLM-agents and digital twins of \nautomation modules and components  \nFigure 7 structure and content elements of the prompt and the \ninteraction between the digital twin and GPT-agent through the \nprompt \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \nand break down complex problems into a series of smaller \nintermediate tasks. \n4) Illustrative examples \nThis section provides  verified concrete illustrative instances \nthat demonstrate the desired input-output pattern. This can be \nbeneficial in several ways: first of all, it constrains the \nstructure of the text to be generated. Furthermore, the model’s \nperformance can be improved even with a limited number of \nexamples8. Last but not least,  the examples can help further \nspecify the context information and disambiguate the abstract \ninformation provided so far, and during our experimenting, \nwe observed an increase in misunderstood or irrelevant \nresponse from the model  without representative instance \nexamples.  \n5) Input and Output interaction pattern \nWhile the previous prompt sections focus on configuring the \nLLM to comprehend the task, this prompt section focuses on \ninstructing the LLM to generate texts based on specific input. \nThe input can be a user request to the “manager agent” to \nperform production task by orchestrating the skills, or it could \nbe a skill demand to an “operator agent” to perform a skill \nby orchestrating the functionalit ies within an automation \nmodule.  \nWe intentionally leave the prompt incomplete, ending with \n“Output:”. By this means, the  GPT’s fundamental \nmechanism of next -token prediction  [12][13] is addressed , \nupon which the model has been trained and optimized. \nEssentially, t he agent carries out its designated task by \ncompleting the entire prompt and continuing writing the texts \nafter the cue-word “Output:”. \nIV. IMPLEMENTATION AND EXPERIMENTS \nIn this  section, we  first illustrate our methods with two \nexamples of the prompt, then explain the system components \nand their interaction with diagram and show the implemented \ndemonstrator of a matrix modular production facility in our \nlaboratory.   \nTable 2 and 3 contain minimum prompts w e specified to \nprompt the GPT-model “text-davinci-003” to solve the \nproduction planning and execut ion problem. Readers can \nreproduce the results of agents output by sending the prompt \ntext to the GPT-model for text generation9. \nTable 2 Prompt input example of the stateless manager-agent. \nThe Prompt for the manager agent: \nRole and goal: \nYou are a manager of a production system. Your goal is to design an efficient production \nprocess based on a given task. You should take into account the provided context, instructions, \nand examples. Following these, you generate an output of a production process. \n \nContext: \n(1) A production process consists of one or more process steps. \n(2) There are two types of process steps, one type is transportation process step, another \ntype is production process step. \n(3) If the next production process is executed in a different production module, \ntransportation process between two production processes is necessary.   \n(4) The transportation step can be executed with a transport robot.  \n(5) Transportation step is not considered as production process step. \n(6) A production process always begins with a skill of the storage module and ends with \na skill of the storage module. \n(7) This production system that you manage consists of several production modules. \nEach of these production modules has one or more skills to execute a production \nprocess step. \n(8) Each process step can be executed with one skill of a module. \n \n8 This ability is also termed as termed as “few-shot learning” [22]. \n9 These prompts are simplified from our hardware-dependent use case, and \nin order to improve the readability of the paper while retain the transferability \nof the method, too detailed information is cut short, such as the long \n(9) The production process should only contain the necessary steps that are necessary to \nsatisfy a task specified in the input. \nThe production modules are described as following: \n(10) An inspection module. It has the following skills: (I1) check the raw material, (I2) \ncheck the faulty material, (I3) test the quality of the material. \n(11) A storage module. It has the following skills: (S1) retrieve a workpiece, (S2) store a \nworkpiece. \n(12) A transport robot. It has the following skills: (T1) transport workpiece between \ndifferent modules. (T2) leave the production area. \n(13) A CNC machine module. It has the following capabilities: (M1) drilling, (M2) \nmilling, (M3) polishing. \n(14) A painting module. It has the following skills: (P1) coat the material, (P2) print a \npattern on the surface with paint. \n(15) A laser machine module. It has the following skills: (L1) print a pattern on the \nsurface with laser. \nInstructions: \nAs a manager of this production system, please arrange a production process based on the \ninput.  \nOnly use the skills that are given in the context section. \nOnly use the skills that are necessary to carry out the task.  \nGive an explanation with short reason in list form. \nYou should think step-by-step. \n \nExamples: \nInput:  \n{produce a steel sheet with a hole} \nOutput:  \n{(S1) – (T1) – (I1) – (T1) – (M1) – (T1) – (I3) – (T1) – (S2)} \nExplanation:  \n(S1) retrieve a workpiece from storage module.  \n(T1) transport the workpiece from storage module to inspection module.  \n(I1) check the raw material to make sure it's suitable for production.  \n(T1) transport the material to the CNC machine module.  \n(M1) drill a hole on the steel sheet.  \n(T1) transport the material back to the inspection module.  \n(I3) test the quality of the material to ensure the final product meets the desired specifications.  \n(CT1) transport the material back to storage module to store the final product. \n(…) \nInput: {produce a steel nameplate and there should be a painted logo.} \nOutput:  \n{(S1) – (T1) – (I1) – (T1) – (M2) – (T1) – (L1) – (T1) – (I3) – (T1) – (S2)} \nExplanation: \n(S1) retrieve a workpiece from storage module. \n(T1) transport the workpiece from storage module to inspection module. \n(I1) check the raw material to make sure it's suitable for production. \n(T1) transport the material to the CNC machine module. \n(M2) mill the steel sheet to create the nameplate. \n(T1) transport the material to the laser machine module. \n(L1) print the logo on the nameplate using laser. \n(T1) transport the material back to the inspection module. \n(I3) test the quality of the material to ensure the final product meets the desired specifications. \n(T1) transport the material back to storage module to store the final product. \nInput: {the customer returned a wood nameplate and said there should be a painted customer logo \non the backside. The wood nameplate is now in the storage module.} \nOutput:  \nThe generated output by the agent: \n{(S1) – (T1) – (P2) – (T1) – (I3) – (T1) – (S2)} \nExplanation:  \n(S1) retrieve the wood nameplate from storage module.  \n(T1) transport the workpiece from storage module to painting module.  \n(P2) print the customer logo on the backside of the nameplate with paint.  \n(T1) transport the material back to the inspection module.  \n(I3) test the quality of the material to ensure the final product meets the desired specifications.  \n(T1) transport the material back to storage module to store the final product. \nTable 3 Prompt input example of the stateless operator-agent for \nthe transport robot \nThe Prompt for an operator agent (a transport robot): \nRole and goal: \nYou are an operator of a transport robot of a production system that performs skills to fulfill a \ntransportation task. Your goal is to orchestrate the functionalities of this robot to perform a \nskill. You should take into account the provided context, instructions, and examples. \nFollowing these, you generate an output of a series of functionalities and provide the correct \nURLs to these functionalities.    \nContext: \nThere are several production modules in this production system, and they are storage module, \ninspection module, painting module, CNC machine module and laser machine module. \nThe transport robot only transports workpiece between two of the following modules: the \ninspection module, the painting module, the CNC machine module and the laser machine \nmodule. \nThe transport robot can perform the following skill(s): (T1) Transport workpiece. \nA component named \"functionality handler\" is a logical component, and it is identified as \n\"functionality_handler_001\". It can control the actions of the transport robot. \nThis component \"functionality_handler_001\" can execute the following functionalities of \nRobotino_7: \n(1) Functionality \"move_dock\" will move the transport robot to a module and dock it to the \nmodule. This functionality can be called using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\". \n(2) Functionality \"load\" will take a workpiece from a module and load it to the transport robot. \nThis functionality can be called using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/load\". \n(3) Functionality \"unload\" will unload a workpiece from the transport robot and give it to a \nmodule. This functionality can be called using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/unload\". \n(4) Functionality \"undock\" will detach the transport robot from a module, if the transport \nrobot is docked to a module. This functionality can be called using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/undock\". \nInstructions: \nAs an operator of this transport robot, please arrange a series of functionalities to solve the \ntask specified in the input. \nOnly use the functionalities that are given. \nOnly use the functionalities that are necessary to carry out the task. \nSpecify the start module and target module in the output steps when necessary. \nGive an explanation with short reason in list form. \nYou shall think step-by-step. \nFollow the text structure and syntax in the examples. \nExamples: \nInput:  \n(T1) Transport the workpiece from laser machine module to the CNC module. \nOutput:  \nTo transport the workpiece from the laser machine module to the CNC module, the following \nsteps shall be executed: \nidentifiers and  the amount of  examples. The most convenient way  to \nreproduce the generated results could be using the ChatGPT web-application \nand paste the prompt input into the conversation.  Demos at: \nhttps://github.com/YuchenXia/GPT4IndustrialAutomation \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \n(1) Move the transport robot to the laser machine module and dock it. Call the functionality \n\"move_dock\" using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\" to move the robot to the \nlaser machine module and dock it. \n(2) Load the workpiece from the laser machine module onto the transport robot. Call the \nfunctionality \"load\" using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/load\" to load the workpiece onto the \nrobot. \n(3) Undock the transport robot from the laser machine module. Call the functionality \n\"undock\" using the URL \"http://129.69.102.129:5010/robotino_7/functionalities/undock\" to \ndetach the robot from the laser machine module. \n(4) Move the transport robot to the CNC module and dock it. Call the functionality \n\"move_dock\" using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\" to move the robot to the \nCNC module and dock it. \n(5) Unload the workpiece from the transport robot onto the CNC module. Call the \nfunctionality \"unload\" using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/unload\" to unload the workpiece from the \nrobot onto the CNC module. \nInput: {(T1) Transport the workpiece from the storage module to the painting module.} \nOutput:  \nThe generated output by the agent: \nTo transport the workpiece from the storage module to the painting module, the following steps \nshall be executed: \n(1) Move the transport robot to the storage module and dock it. Call the functionality \"move_dock\" \nusing the URL \"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\" to move the \nrobot to the storage module and dock it. \n(2) Load the workpiece from the storage module onto the transport robot. Call the functionality \n\"load\" using the URL \"http://129.69.102.129:5010/robotino_7/functionalities/load\" to load the \nworkpiece onto the robot. \n(3) Undock the transport robot from the storage module. Call the functionality \"undock\" using the \nURL \"http://129.69.102.129:5010/robotino_7/functionalities/undock\" to detach the robot from the \nstorage module. \n(4) Move the transport robot to the painting module and dock it. Call the functionality \"move_dock\" \nusing the URL \"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\" to move the \nrobot to the painting module and dock it. \n(5) Unload the workpiece from the transport robot onto the painting module. Call the \nfunctionality \"unload\" using the URL \n\"http://129.69.102.129:5010/robotino_7/functionalities/unload\" to unload the workpiece from \nthe robot onto the painting module. \nThe parsed result in JSON-file for control services invocation: \n(by using regular expression to convert the texts into JSON in python)10 \n[{\"step\": 1, \"description\": \"Move the transport robot to the storage module and dock it\", \"action\": \n\"move_dock\", \"url\": \"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\"},{\"step\": \n2, \"description\": \"Load the workpiece from the storage module onto the transport robot\", \"action\": \n\"load\", \"url\": \"http://129.69.102.129:5010/robotino_7/functionalities/load\"},{\"step\": 3, \n\"description\": \"Undock the transport robot from the storage module\", \"action\": \"undock\", \"url\": \n\"http://129.69.102.129:5010/robotino_7/functionalities/undock\"},{\"step\": 4, \"description\": \"Move \nthe transport robot to the painting module and dock it\", \"action\": \"move_dock\", \"url\": \n\"http://129.69.102.129:5010/robotino_7/functionalities/move_dock\"},{\"step\": 5, \"description\": \n\"Unload the workpiece from the transport robot onto the painting module\", \"action\": \"unload\", \"url\": \n\"http://129.69.102.129:5010/robotino_7/functionalities/unload\"}] \nIn the prompts, we allow updates to the information in the \ncontext section in accordance with the descriptive \ninformation and semantic annotations  in the digital twin \nsystem. This is implemented in a python program with query-\nbased template-filling and tex t string concatenation (cf. \nFigure 8). The program sends the prompt to GPT-model and \nparses the returned text into structured production steps with \nidentifiers. \nSeveral software components are implemented to realize our \nsystem design. The sequence diagram in Figure 8 shows the \nessential software components in our prototypical \ndemonstrator and their interactions. \nAt the beginning of the process, a task input is handled by the \nmanager agent. It queries a digital twin system (AAS-System) \nto retrieve the skills description of the automated production \n \n10 Used regular expression in Python code: \n\\((\\d+)\\) (.+)\\. Call the functionality \"(\\w+)\" using the URL \"(.+)\" \nsystem and uses this data to update the context information in \nthe specified prompt . The specified prompt is transmitted to \nthe service API of  a GPT-Model 11 , and the GPT-model \nreturns with the generated output texts. By parsing the  \ngenerated texts, the manager agent obtains a  sequence of \nskills to fulfill the task. The orchestrated skills are passed to \nthe MES-System, based on which the MES-system invokes \nthe skill service calls on one or more operator agent(s). On \nreceiving the skill service  calls, the operator agent retrieves \nthe necessary information  about the functionalities in an \nautomation module  from the digital twin system  (AAS-\nSystem), and then orchestrates the functionalities to execute \nthe requested skill.  The orchestrated process is finally \nexecuted by the automation modules, as shown in Figure 9. \nV. DISCUSSION \nIn the previous sections , we introduced a novel framework \nthat integrates large language models (L LMs) with digital \ntwin systems to enable intelligent management and control in \nindustrial automation . Our method leverages prompt \nengineering to create LLM agents capable of adapting to \nspecific tasks based on the information provided by digital \ntwins. Through a case study involving a matrix modular \nproduction facility demonstrator, we showcased how our \napproach can address unforeseen problem tasks, \nautonomously orchestrate a production plan, and execute \nthem to provide customizable production services. \nSeveral key insights emerged from our study, and we explain \nthem in three parts: positive insights, difficulties and lessons \nlearned, as well as limitations and future works. \nA. Positive Insights \n1) Enhanced reasoning and decision-making capabilities \nfor flexible/agile production \nBy contextualizing the LLM agents using prompt engineering, \nwe observed the human-like problem-solving capabilities in \ndomain-specific tasks of production management and control. \nBased on an instruct-finetuned model “text-davinci-003”, we \niteratively refined the proposed prompt template, which \nconsists of five distinct sections. Using this template and the \ninformation provided by the digital twins , the defined LLM -\nagent effectively generates contextually appropriate \nresponses to plan and control the production system \nautonomously. This reduces the need for human effort and \n11 We used the API of OpenAI’s GPT-Model “text-davinci-003” for text \ngeneration in our prototypical implementation.  \n \nPainting StationPainting StationPainting StationTransport RobotModule\n./print_pattern./print_pattern./print_pattern./transport_workpieceSkill\n./print./robot_arm_move./position_workpiece./unload\nTransport RobotTransport RobotTransport RobotTransport RobotModule\n./transport_workpiece./transport_workpiece./transport_workpiece./transport_workpieceSkill\n./move_dock./undock./load./move_dockFunct-\nionality\nFunct-\nionality\n \n \nFigure 9 Sequence diagram illustrating the interaction between \ndifferent components of the prototype, from task input to \noperations execution. \nFigure 8 Execution of production following the orchestrated \nprocesses directed by LLM-agents. \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \ncan lead to increased productivity, reduced operational costs, \nand minimized delay time in production processes. \n2) Digital twin system bridge s the information gaps for \nLLM-agent.  \nAs the traditional automation systems are not designed to host \ncomprehensive information and knowledge about the \nproduction and the automation system itself, the development \nof a digital twin system (implemented with AAS) is necessary \nto provide the missing knowledge for the intelligent agent s. \nThe proposed architecture demonstrates the feasibility of \nestablishing a bridge between LLMs and the physical \nproduction infrastructure  with digital twin system . The \nmodular and casc aded digital twin s enable scalable \ncommunication, allowing the LLM -agents to access \ncomprehensive information about the production system and \ninteract with the physical world through service interfaces. \n3) Scalability, reuse of  modular functionalities/skills to \nachieve adaptability \nOur approach demonstrates the necessities of developing \nscalable services interface of atomic functionalities /skills of \nautomation components/modules. These interfaces are \nindispensable to enable intelligent LLM-agents to interact \nwith the physical world. By dynamically orchestrating those \natomic functionalities and skills, a higher level of adaptability \nof the production  system can be achieved to meet the \ncustomized production demand.  \nB. Difficulteis and Lessons Learned \n1) Retrofitting the system to acquire modular interfaces. \nRetrofitting existing production facilities to accommodate \nmodular interfaces can be a challenging process, \nnecessitating time-consuming engineering efforts. While \ncreating query interfaces is typically straightforward, \ndeveloping control interfaces demand s a higher level of \ncaution due to the complexity involved.  In comparison to \ncoarse-granular skill, fine -granular functionalities exhibit \nmore dependencies and require a closer examination of \nhardware-related aspects.  Some required i nterfaces for our \nuse case have application-level dependencies and cannot be \nexecuted independently without using the delivered vendor -\nspecific software application.  The modularity of the \nfunctionalities might not have been  considered during the \ndesign phase when the system was developed . These \ndependencies across different components and on different \nsystem levels are preventing us from easily creating scalable \ninteraction interfaces for our intelligent agents.  \nIn order to solve this problem, we are looking into the code \nmodularization on the device level to create the functionality \ninterfaces from the bottom. By building from the bottom up , \nthese “atomic” code-level functionalities can be orchestrated \nto create  the modular skills of a n automation module , \nallowing for greater reusability, flexibility and adaptability in \nsystem integration. \n2) Knowledge representation and conversion in prompts \nThe conversion between knowledge representation in digital \ntwins’ software systems and natural language is a challenging \ntask. Inaccurate or lossy conversion might result in inaccurate \ninterpretation of the LLM agents for the production system \nand negatively affect their performance.  Moreover, it is \ndifficult to assess whether the LLM agents have accurately \ninterpreted the context information provided in the prompt. \nHow to determine the point at which refining the prompt \nwould no longer yield significant performance improvement \nremains an open question. In our case study , we refined our \nprompts until the agents could generate effective results \nrepeatedly (confer section C.3.). \nWhen working on the iterative refinement of the prompts, we \nbelieve that the essential task is to “translate the languages” \n(e.g., translate the code and information models into text in \nnatural language). In the translation, the knowledge conveyed \nby both representation  forms shall pertain. Notably, we also \nuse our own domain knowledge in automation and production \nengineering to design and additionally guide the LLM-agents. \nTo be accurate  in detail, LLM-agents don’t understand the \nknowledge, but rather they approximate the knowledge  \nconveyed by the representation. \n3) High-quality data and high-fidelity digital twins \nSupplying high-quality data and accurate knowledge \nrepresentation in digital twin is essential to allow the LLM-\nagents to perform correct decision . However, creating high -\nfidelity digital twins can be labor -intensive and time -\nconsuming, which  also requires a comprehensive \nunderstanding of the system’s architecture, behavior, and \ndependencies. \nIn our implementation, the digital twin  system provides the \nasset information and exposes the updated interface s \ndescription. The dynamic operational data are not replicated \nin digital twin s to avoid data -inconsistency issues. The \noperational data should be provided by the MES system and \nthe reverse-engineered RESTful-interfaces. In this sense, the \ndigital twin system supplies descriptive information and \nannotations about the production system and components  to \nhelp the LLMs to interpret how the production works and \nhow to control it.  However, we have not included  the \noperational data into the prompt so far , because the GPT -\nagent has diffi culty interpreting the dynamic numeric data \nand it would require extra memory for the model to cache the \ntime series data. It is also due to the fact that we designed the \nLLM-agent to perform stateless interactions. \nC. Limitation and Future Work \n1) Stateless interaction \nOne notable limitation of our approach is the stateless \ninteraction of the LLM agents  because we only give all the \ncontext information at once through the API-call. The agent \nitself does not know how its output affect s the production \nsystem. In order to keep track of the dynamic effects on \nenvironment and to perform more informed decision, the  \nagent need to maintain a memory of the data about the \nproduction, which could require an extra software component \n(e.g., a database)  or new mechanism to merge the LLM \nagents into the digital twin system.  \n2) Non-deterministic results \nAnother limitation of utilizing large language models (LLMs) \nis the inherent non -deterministic nature of the generated \nresults. LLMs, such as GPT, are designed to predict the most \nprobable next token in a sequence based on the input prompt \nand their extensive training data. Consequently, the outputs \nproduced by LLMs can vary each time, even when the input \nremains the same.  Although it is possible to set the model \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \ntemperature to 0 to let the model generate more invariable \noutput when given the same input, a decision-making process \nthat only based on unexplained probability estimation would \nstill be unreliable. As the task  for customized pro duction \nplanning inherently involves non-deterministic input,  \nadditional me chanisms (extra constraints or guidance) are \nrequired to ensure a deterministic output and  the \npredictability of the results.  \n3) Data dilema for comprehensive testing and evaluation \nIn contrast to general NLP tasks, where standard benchmarks \nand datasets are openly available, industrial automation tasks \noften involve heterogeneous data from specific components, \ncomplex hardware -level dependencies, a wide range of use \ncases, limited data collections, diversity of knowledge \nrepresentation, and compliance with safety and reliability \nstandards. Consequently, developing benchmarks for \nperformance evaluation of LLM agents in these tasks \nnecessitates future collaborative efforts. A t this stage, we \nhave not yet established an evaluation benchmark to \ncomprehensively quantify the performance of LLMs in \nindustrial automation tasks and can only provide a use-case-\nlevel proof-of-concept. For the given manager agent example \nin the implementation section, we evaluated 50 generated \nsamples by the model “text-davinci-003”: 96% of all the skill \nsequences are executable without error, 88% can solve the \nparticular task and are able to produce the right product , \nhowever, only 6% of all the generated skills sequences use \nthe minimal required steps to solve the task efficiently  \nwithout unnecessary steps. \n4) The computational complexity of LLM models \nThe most capable LLMs often come w ith considerable size \nand computational requirements, which can pose challenges \nfor their local deployment in real -world industrial settings . \nFine-tuning a smaller model with dedicated data for a specific \ndomain could be a potential solution. However, we \nhypothesize that  well-trained larger models with more \nneurons and trained on data from diverse knowledge domains \npossess a stronger general intelligence, which benefits the \ninterpretation and reasoning in specialized domains and tasks. \nIn other words, we assume that the larger models that perform \nbetter in general tasks may also yield better outcomes in \ndomain-specific tasks. For simpler tasks, such as identifying \nsemantic similarity in search queries or providing auto -\ncompletion recommendations  [14][15], smaller embedding \nmodels are sufficient for functions with lower complexity.  \nVI. CONCLUSION AND OUTLOOK \nIn conclusion, we have presented a novel framework that \nintegrates large language models (LLMs) with digital twin \nsystems for intelligent management and control in industrial \nautomation. Our approach demonstrates the potential of LLM \nagents in making informed decisions based on the \ninformation provided by digital twins, leading to improved \nproductivity, reduced operational costs, and minimized delay \ntime in production planning processes. \nOur study highlighted several positive insights, such as using \nLLMs to enhance the intelligence of  the automation system, \nintegrating of the LLM s into production system via digital \ntwin systems, and the necessity of scalable  and modular  \ninterfaces to increase the adaptability of production system.  \nWe also encountered challenges, including retrofitting \nexisting systems, translating knowledge representat ion in \nprompts, and accommodating high-quality data and high -\nfidelity models with digital twins. \nLast but not least, we identified limitations and future work \ndirections, such as stateless interaction, non -deterministic \nresults, data dilemma for benchmarki ng, and the \ncomputational requirements of LLMs. As the field of AI and \nindustrial automation technology advances, we believe that \nintegrating LLMs into industrial automation systems will \nlead to more efficient, flexible, and adaptive production \nsystems. To further uncover and realize the potential of \napplication of large language models in autonomous systems \nfor industrial applications, it is crucial to engage in \ncollaborative research efforts spanning across \ninterdisciplinary fields.  \nACKNOWLEDGMENT  \nThis work was supported by Stiftung der Deutschen \nWirtschaft (SDW) and the Ministry of Science, Research and \nthe Arts of the State of Baden-Wuerttemberg within the \nsupport of the projects of the Exzellenzinitiative II.  \nREFERENCES \n \n[1] T. Müller, B. Lindemann, T. Jung, N. Jazdi, and M. Weyrich, \n“Enhancing an Intelligent Digital Twin with a Self -organized \nReconfiguration Management based on Adaptive Process \nModels,” Procedia CIRP, vol. 104, pp. 786 –791, Jan. 2021, doi: \n10.1016/J.PROCIR.2021.11.132. \n[2] T. Müller, N. Jazdi , J. P. Schmidt, and M. Weyrich, “Cyber -\nphysical production systems: enhancement with a self -organized \nreconfiguration management,” Procedia CIRP, vol. 99, pp. 549 –\n554, Jan. 2021, doi: 10.1016/J.PROCIR.2021.03.075. \n[3] M. Müller, T. Müller, B. Ashtari Talkhestani, P. Marks, N. Jazdi, \nand M. Weyrich, “Industrial autonomous systems: A survey on \ndefinitions, characteristics and abilities,” At-\nAutomatisierungstechnik, vol. 69, no. 1, pp. 3–13, Jan. 2021. \n[4] P. Foith -Förster and I. Thomas Bauernhansl, “Changeab le and \nreconfigurable assembly systems – A structure planning approach \nin automotive manufacturing,” pp. 1173 –1192, 2015, doi: \n10.1007/978-3-658-08844-6_81. \n[5] A. Markaj, A. Fay, N. Schoch, K. Stark, and M. Hoernicke, \n“Intention-based engineering for the early design phases and the \nautomation of modular process plants,” IEEE International \nConference on Emerging Technologies and Factory Automation, \nETFA, vol. 2022 -September, 2022, doi: \n10.1109/ETFA52439.2022.9921599. \n[6] P. Greschke , “Matrix -Produktion als Konzept einer \ntaktunabhängigen Fließfertigung,” 2016. \n[7] M. Trierweiler and T. Bauernhansl, “Reconfiguration of \nProduction Equipment of Matrix Manufacturing Systems,” pp. 20–\n27, 2021, doi: 10.1007/978-3-662-62962-8_3. \n[8] D. Dittler, P. Lierhammer, D. Braun, T. Müller, N. Jazdi, and M. \nWeyrich, “An Agent -based Realisation for a continuous Model \nAdaption Approach in intelligent Digital Twins,” 2022, Accessed: \nMar. 21, 2023. [Online]. Available: \nhttps://arxiv.org/abs/2212.03681v1 \n[9] G. Jawahar, B. Sagot, and D. Seddah, “What Does BERT Learn \nabout the Structure of Language?,” ACL 2019 - 57th Annual \nMeeting of the Association for Computational Linguistics, \nProceedings of the Conference , pp. 3651 –3657, 2019, doi: \n10.18653/V1/P19-1356. \n[10] J. Wei et al., “Emergent Abilities of Large Language Models,” Jun. \n2022, Accessed: Mar. 21, 2023. [Online]. Available: \nhttps://arxiv.org/abs/2206.07682v2 \n[11] T. Gao, A. Fisch, and D. Chen, “Making Pre -trained Language \nModels Better Few-shot Learners,” ACL-IJCNLP 2021 - 59th Ann. \nMeet. Assoc. Comput. Linguist. & 11th Int. Joint Conf. Nat. Lang. \nProcess., Proceedings of the Conference , pp. 3816 –3830, 2021, \ndoi: 10.18653/V1/2021.ACL-LONG.295. \nThis is the pre-print draft manuscript. The completed peer-reviewed version will be published exclusively by IEEE. \n[12] A. Radford and K. Narasimhan, “Improving Language \nUnderstanding by Generative Pre-Training,” 2018. \n[13] A. Vaswani et al., “Attention Is All You Need,” Adv Neural Inf \nProcess Syst , vol. 2017 -December, pp. 5999 –6009, Jun. 2017, \nAccessed: Mar. 21, 2023. [Online]. Available: \nhttps://arxiv.org/abs/1706.03762v5 \n[14] M. Both, J. Muller, and C. Diedrich, “Reducing configuration \nefforts in energy management systems based on natural language \nprocessing methods and asset administration shells,” IEEE \nInternational Conference on Emerging Technologies and Factory \nAutomation, ETFA , vol. 2022 -September, 2022, doi: \n10.1109/ETFA52439.2022.9921479. \n[15] Y. Xia, N. Jazdi, and M. Weyrich, “Automated generation of Asset \nAdministration Shell: a transfer learning approach with neural \nlanguage model and semantic fingerprints,” IEEE International \nConference on Emerging Technologies and Factory Automation, \nETFA, vol. 2022 -September, 2022, doi: \n10.1109/ETFA52439.2022.9921637. \n[16] V. Sanh et al., “Multitask Prompted Training Enables Zero -Shot \nTask Generalization,” Oct. 2021, Accessed: Mar. 21, 2023. \n[Online]. Available: https://arxiv.org/abs/2110.08207v3 \n[17] J. Wei et al. , “Finet uned Language Models Are Zero -Shot \nLearners,” Sep. 2021, Accessed: Mar. 21, 2023. [Online]. \nAvailable: https://arxiv.org/abs/2109.01652v5 \n[18] J. Vig, Y. Belinkov, H. John, and A. Paulson, “Analyzing the \nStructure of Attention in a Transformer Language Model,” pp. 63–\n76, Jun. 2019, doi: 10.18653/v1/w19-4808. \n[19] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in \nLarge Language Models,” Jan. 2022, Accessed: Mar. 21, 2023. \n[Online]. Available: https://arxiv.org/abs/2201.11903v6 \n[20] T. Kojima, S . Shane Gu, M. Reid Google Research, Y. Matsuo, \nand Y. Iwasawa, “Large Language Models are Zero -Shot \nReasoners,” May 2022, Accessed: Apr. 11, 2023. [Online]. \nAvailable: https://arxiv.org/abs/2205.11916v4 \n[21] B. Prystawski and N. D. Goodman, “Why think ste p-by-step? \nReasoning emerges from the locality of experience,” Apr. 2023, \nAccessed: Apr. 11, 2023. [Online]. Available: \nhttps://arxiv.org/abs/2304.03843v1 \n[22] T. B. Brown et al., “Language Models are Few -Shot Learners,” \nAdv Neural Inf Process Syst , vol. 2 020-December, May 2020, \nAccessed: Mar. 21, 2023. [Online]. Available: \nhttps://arxiv.org/abs/2005.14165v4 \n "
}