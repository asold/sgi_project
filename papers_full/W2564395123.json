{
  "title": "A Character-Word Compositional Neural Language Model for Finnish",
  "url": "https://openalex.org/W2564395123",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Lankinen, Matti",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287415915",
      "name": "Heikinheimo, Hannes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4298606264",
      "name": "Takala, Pyry",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294484404",
      "name": "Raiko, Tapani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743822152",
      "name": "Karhunen Juha",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2345764277",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W1669302834",
    "https://openalex.org/W2949563612",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2949679234",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2950075229",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2116599427",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Inspired by recent research, we explore ways to model the highly morphological Finnish language at the level of characters while maintaining the performance of word-level models. We propose a new Character-to-Word-to-Character (C2W2C) compositional language model that uses characters as input and output while still internally processing word level embeddings. Our preliminary experiments, using the Finnish Europarl V7 corpus, indicate that C2W2C can respond well to the challenges of morphologically rich languages such as high out of vocabulary rates, the prediction of novel words, and growing vocabulary size. Notably, the model is able to correctly score inflectional forms that are not present in the training data and sample grammatically and semantically correct Finnish sentences character by character.",
  "full_text": "A Character-Word Compositional Neural Language\nModel for Finnish\nMatti Lankinen\nReaktor\nmatti.lankinen@reaktor.com\nHannes Heikinheimo\nReaktor\nhannes.heikinheimo@reaktor.com\nPyry Takala\nAalto University\npyry.takala@aalto.fi\nTapani Raiko\nAalto University\ntapani.raiko@aalto.fi\nJuha Karhunen\nAalto University\njuha.karhunen@aalto.fi\nAbstract\nInspired by recent research, we explore ways to model the highly morphological\nFinnish language at the level of characters while maintaining the performance of\nword-level models. We propose a new Character-to-Word-to-Character (C2W2C)\ncompositional language model that uses characters as input and output while still\ninternally processing word level embeddings. Our preliminary experiments, using\nthe Finnish Europarl V7 corpus, indicate that C2W2C can respond well to the\nchallenges of morphologically rich languages such as high out of vocabulary rates,\nthe prediction of novel words, and growing vocabulary size. Notably, the model\nis able to correctly score inﬂectional forms that are not present in the training data\nand sample grammatically and semantically correct Finnish sentences character\nby character.\n1 Introduction\nCharacter level language models have attracted signiﬁcant research attention recently in the deep\nlearning community [18, 5, 20, 2, 14, 15]. Character level language models have certain advantages\nover word level models, which tend to struggle with applications that require a large vocabulary or\nneed to tackle high out-of-vocabulary word rates. In many cases increasing vocabulary size quickly\nmultiplies the parameter count, making word-level models complex and slow to train. Furhermore,\nwith highly inﬂectional languages such as Turkish, Russian, Hungarian or Finnish word level models\nfail to encode part-of-speech information embedded into the morphology of inﬂected words.\nAmong highly morphological languages Finnish is one of the most complex. Hence the Finnish\nlanguage provids an interesting testbed for research in character level language models. It has a large\nnumber of inﬂectional types for both nouns and verbs and it uses sufﬁxes to express grammatical\nrelations. As an example, the single Finnish word talossanikin corresponds to the English phrase\nin my house, too . Bojanowski et al. [2] report the vocabulary size and out-of-vocabulary rate of\nvarious European languages for a set of experiments on the Europarl dataset [17]. For Finnish these\nvalues were reported to be 10 % and 37 % higher respectively, compared to Hungarian, which had\nthe second hightest reported values.\n1\narXiv:1612.03266v1  [cs.CL]  10 Dec 2016\nFigure 1: C2W2C model overall structure\nIn this study we propose a new Character-to-Word-to-Character (C2W2C) compositional lan-\nguage model for modeling Finnish. The model combines ideas from [20] and [5], and consists of\nthree logical parts: (1) the Character-to-Word C2W model inspired by [20], (2) a traditional LSTM\nLanguage Model [28], and (3) a Word-to-Character W2C decoder model inspired by [5]. The entire\nmodel is implemented sequentially, so the output from any logical part can be used as an input for\nthe next one. Thus, the model can be trained and used with a single pass without any preliminary\nsub-word tokenization or word classiﬁcation. Also, all the parts are independent, so they can be\ntrained individually. The structure of C2W2C model is described in Figure 1.\nThe rest of our discussion is organised as follows: Section 2 discusses related work. Section 3\ndescribes our language model architecture in more detail. Section 4 and 5 present our experiment\nsetup and the model performance results. Section 6 contains the conclusions.\n2 Related work\nThe usual approach in neural language modeling is to use so-called one-hot vectors [25] to represent\ninput and output words of the model. However, the number of parameters linearly follow the size\nof the vocabulary, hence resulting into scalability issues for large vocabularies. Ling et .al [20]\nproposed a new Character-to-Word model (C2W) to solve this issue. In C2W, the traditional word-\nlevel projection layer is replaced with a character-level Bidirectional LSTM unit that produced word\nembeddings to the language model LSTM. As a result, the input layer parameters are reduced from\n4M to 180k with a dataset from the Wikipedia corpus. Also noticeable performance improvements\nwere reported compared to traditional word-level models. Here we use the C2W model of [20] as\npart of our C2W2C model.\nIn [15] Kim et al. propose a convolutional neural network and a highway network over characters\n(CharCNN), whose output is given to a LSTM language model. With this setup, the authors managed\nin par performance with state-of-art models with 60% fewer parameters. The work in [14] proposes\nalso a solution by using convolution networks, but use a CharCNN as a part of the output layer. The\narchitecture of this model followes the original work of [15], but instead of having the projection\nlayer for output, they use a CharCNN to create word embeddings, which they combine with a context\nvector from a language model layer to produce an output word logit function. Other alternative\napproaches in dealing with the large output vocabulary size are the hierachical softmax [11, 23],\nimportance sampling [1], and noice contrast estimation [12, 24]. Although these methods work well\nand give speedups without signiﬁcant performance penalties, they do not remove the challenges of\nout-of-vocabulary words.\n2\nFigure 2: The structure of C2W model\nCho et al. [5] proposed a new two-stage encoder-decoder RNN performing machine translations\nfrom characters to characters. They use the previous encoder-decoder work of [4] and gated-\nfeedback networks [6] as a basis and built an adaptive ”Bi-Scale Recurrent Neural Network”, where\nthe idea is to model sentences by using two layers. The ”faster layer” models the fast-changing\ntimescale (i.e. words), and the ”slower layer” modeles the slower-changing timescale (i.e. charac-\nters). The layers were gated so that the produced context vector was an adaptive combination of\nthe activations of both layers, and that context was used to decode the translation characters. In this\npaper we use a character-level decoder as a part of its C2W2C model similar to [5].\nRecent research, including [22, 2, 21] have indicated the need for more research on subword level\nmodels for morphologically rich languages. However, sub-word level language models have been\nstudies previously. Creutz and Lagus [7] introduced an unsupervised morpheme segmentation and\nmorphology induction algorithm called Morfessor. Vilar et al. [29] translated sentences character by\ncharacter, but the results of this technique were inferior to those of word-based approaches. Botha\nand Blunsom [3] proposed a word segment-level language model.\nIn this paper we apply neural language modelling especilly to the Finnish language. Previous work\ndone towards tackling the complexites of Finnish morpoholgy include [19] and [26]. Most recent\nwork on neural language modeling applied to the Finnish language are [9] and [8]. Chung et al. [5]\ndemonstrate their machine translation work using Finnish as one of their target languages.\n3 A compositional language model\n3.1 Character to Word model\nThe character-to-word (C2W) model is the ﬁrst part of the C2W2C model. The implementation\nmainly follows [20]. The responsibility of C2W in C2W2C is to transform the input context word\nsequence ω1...ωn into dW -dimensional word embeddings w1...wn. The word embeddings capture\n3\nFigure 3: The C2W2C language model structure\nthe characteristics of each of the words in the given context. These characteristics contain, for\nexample, part-of-speech information (noun, verb, adjective, etc.), grammatical tense, and inﬂection\ninformation that is usually encoded into the words in highly inﬂectional languages (e.g. Finnish\nwords auto, autossa, autosta as opposed to their English counterpartsa car, in the car, from the car).\nC2W2C contains the ﬁrst two layers from the original C2W model implementation: (1) charac-\nter lookup table and (2) Bi-LSTM generating the word embeddings for the given word sequence\nω1...ωn. These ﬁrst two C2W layers are illustrated in Figure 2. More details can be found from\n[20].\n3.2 Language Model\nThe language model component of C2W2C is a standard 2-layer LSTM network [13] that takes word\nembeddings w1...wn as an input and yields the state sequencesW\n0 ...sW\nn as an ouput. In training time,\nthe intermediate states sW\n0 ...sW\nn−1 are discarded and the actual context embedding c is selected by\ntaking the last state of the computed state sequence. The overall structure of the LM of C2W2C is\nshown in Figure 3.\n3.3 Word to Character model\nThe Word to Character (W2C) model is the ﬁnal part of C2W2C mode. Its goal is to decompose the\ncontext embedding c back to a sequence of characters along the lines of the work of [5] and [4].\nThe full implementation of the character-level encoder-decoder [5] contains two adaptive decoder-\nRNN [4] layers whose goal is to translate characters from a source word sequence x1...xn into a\ntarget word sequence y1...yn. However, since C2W2C tries to predict only a single target word, our\nW2C implementation has only one layer following the details of the original decoder-RNN.\n4\nFigure 4: The structure of W2C model\nW2C works as follows: The layer receives the context embedding c from the language model and\ncombines it with the predecessor character ct−1 of the predicted character ci. By using these two\ncomponents, the RNN decoder maintains a hidden state h over every predicted character. The used\nRNN implementation is a variant of Gated Recurrent Unit [4], with the difference that instead of\nrelying only on the hidden stateht−1, the RNN also uses an embedding of the predecessor character\ncE\nt−1 to get the next state ht by using the following formula:\nht = ztht−1 + (1−zt)h′\nt\nh′\nt = tanh(WhcE\nt−1 + rt(Uhht−1 + Chc))\nzt = σ(WzcE\nt−1 + Uzht−1 + Czc)\nrt = σ(WrcE\nt−1 + Urht−1 + Crc)\n(1)\nThe initial cE\n0 is a zero vector, and the initial hidden state is initialized by using the context vector\nand tanh activation:\nh0 = tanh(Vc)\nEach state ht of the received state sequence h1...hm is combined with the context vector and the\nprevious character embeddingcE\nt−1, and the logit for the predicted character index (inVC) is received\nby running the combined embedding through a 2-feature Maxout network [10] and projecting the\nresult to the VC index space:\ncI\nt = PIst\nst = max{s′1\nt ,s′2\nt }\ns′i\nt = Oi\nhht + Oi\necE\nt−1 + Oi\ncc + b\n(2)\n5\nProbabilities of the predicted characters are received by running Softmax activation over the logit\nvectors. The overall structure of W2C is illustrated in Figure 4.\n4 Experiment: Finnish Language Modeling\nWe examine how well the C2W2C model can model the Finnish language. We are interested in\nthe models capability to learn word-level relationships, ﬁnd the meaning of the different inﬂection\nforms (e.g. auton, ‘of the car’,autossa, ‘in the car’) as well as cope with part of speech information\n(e.g. Pekka ajaa punaista autoa, ‘Pekka drives a red car’) of a word in the context of a sentence.\n4.1 Dataset\nWe chose the text corpus of Finnish translations of Europarl Parallel Corpus V7 1 [17] as our test\ndataset. The text data was was tokenized with Apache OpenNLP open-source tool 2 and using\nFinnish tokenizing model from Finnish Dependency Parser project 3. The tokenized text data was\nconverted into a lower-case form. No further pre-processing or word segmentation was done to the\ndataset.\nThe training set was selected by using the ﬁrst one million tokens from the tokenized lower-case\ncorpus. The properties of the selected dataset reﬂected the nature of highly inﬂectional languages;\nthe one million tokens included 88,000 unique tokens which cover 8.8% from the dataset. 5,000\nmost frequent tokens covered 78.41 % from the dataset, 10k most frequent tokens 84.97 % and 20k\nmost common frequent tokens 90.55 %.\n1-5 11-15 21-25 31-35 35+\n0\n1\n2\n3\n4\n·105\nWord length (characters)\nNumber of words in dataset\nFigure 5: Number of words in dataset by word length\nIn addition to the training data, extra 10k tokens were fetched from the same Europarl Finnish\ncorpus for test and validation purposes. Both training and validation datasets were pre-processed as\ndescribed above and sentences were split per line and shufﬂed. The training dataset sentences were\nalso shufﬂed at the beginning of each epoch.\n1http://www.statmt.org/europarl\n2https://opennlp.apache.org\n3https://github.com/TurkuNLP/Finnish-dep-parser\n6\n4.2 Hyper-parameters\nThe hyper-parameters of C2W model were set to follow the values from the original C2W proposed\nby [20]. The projection layer PE ∈RdC×|VC|for the character embeddings was set to use character\nproperties dC = 50. The intermediate Bi-LSTM cell states sf\nm and sb\n0 were both set to use dWI =\n|sf\nm|= |sb\n0|= 150. Word features number dW was set to 50. The language model LSTM hidden\nstate dL was set to 500 for both nodes. W2C decoder’s hidden state was set to |h|= 500 and\ncharacter embeddings for both cE\nt−1 and st were set to use dC.\nAnother variable that has a great inﬂuence on the model speed is the maximum length of the input\nand output words: the longer the maximum length, the more inner LSTM steps must be performed\nto construct the word embeddings. Fig. 5 shows the word frequency distribution by word length.\nWe chose the maximum word length to be 20, which covers over 98% from the training dataset.\nThe training was performed by using Backpropagation through time with unlimited context size\nand Adam optimizer [16] with learning rate of 0.0001and norm clipping with value 2.0. Gradient\nupdates were performed after each time step, and language model’s LSTM states were reset before\neach epoch. In order to speed up the calculation, the data was arranged into batches so that each\nsample xt\ni in the batch was a predecessor of the next sample xt+1\ni . Size of these mini-batches were\nset to 150. To prevent overﬁtting we used dropout layers [27] after each submodel (W2C, LM, W2C)\nwith friction value 0.5.\n4.3 Perplexity\nFor character-level perplexity the probability of a word is seen as a product of the probabilities\nof its characters. Because C2W2C already produces categorical probability distributions by using\nsoftmax as its output layer activation, the perplexity for C2W2C was calculated by using the received\nprobability distribution and getting the loss character by character. Those character losses are then\nadded per word, and ﬁnally, the word-level cross-entropies are combined and normalised with the\ntesting set size to get overall validation perplexity.\n4.4 Environment\nThe model was trained and validated by using a single Ubuntu 14.04 server having 8GB RAM,\nsingle Intel i5 for the model building, and one nVidia GeForce GTX 970 GPU Unit with 3.5GB\ngraphics memory and CUDA (cMEM and cuDNN disabled) for the model training.\nThe model was implemented by using Python 2.7, Theano 0.8.2 4 and Keras 1.0.4 5. The source\ncodes of the ﬁnal C2W2C model implementation can be found at GitHub: https://github.\ncom/milankinen/c2w2c under MIT license.\n5 Results and Analysis\nThe model was run by using the dataset, hyperparameters, and environment described in the previ-\nous section. In addition to C2W2C, a traditional word-level model (”Word-LSTM” onwards) was\ntrained for comparison. Word-LSTM had the same two-level LSTM language model as C2W2C (as\ndescribed in subsection 3.2) and typical feedforward projection layers for both inputs and outputs.\nFor the sake of efﬁciency, context vector c|dim(c)=500 was projected into 150 dimensional space\nbefore output projection (removing 31M parameters from the model).\n5.1 Quantitative analysis\n5.1.1 Model Complexity and training times\nAs shown in Table 1 the C2W2C model operates in our experiment by using only 25% of the tradi-\ntional word-level model parameters. The C2W layer, in particular, seems very useful as it reduces\nthe input parameters by a factor of 20 without affecting model performance negatively.\n4http://deeplearning.net/software/theano\n5http://keras.io\n7\nC2W2C Word-LSTM\nC2W LM W2C FF-NN LM FF-NN\n0.26M 3.1M 2.04M 4.5M 3.1M 13.2M\n5.41M 20.8M\nTable 1: Model parameters per sub-model\nHowever, even if there are signiﬁcantly fewer parameters, the training times are roughly equal: Both\nmodels can process approximately 900 words per second, resulting in 20 minutes per epoch with\nthe given training data set. However, for the C2W2C the gradient is less stable compared to the\nWorld-LSTM model. To remendy this we had to resort to a smaller learning rate, hence requiring\nmore iterations for similar results.\n5.1.2 Perplexity\nWhen measured with perplexity, the classic word-level model surpasses C2W2C. With the given\ndataset, word-level model achieved PP 392.28, whereas PP for C2W2C was 410.95. One possible\nfactor that raises the PP of C2W2C is that it gives predictions to all possible character sequence,\nwhich means |VC|maxlen different combinations. However, the predicted vocabulary size is much\nsmaller. The work done in [14] eliminated this by doing an expensive normalisation over the training\nvocabulary, which indeed improved character-level performance a little bit. However, such normal-\nisation was not done in the scope of this study.\n5.2 Qualitative analysis\n5.2.1 Grammar\nThe following table displays some example scores (a length-normalized log-loss of the sentence,\nwhere a lower score is better) of sentences with different properties. The bolded word w in the ﬁrst\ncolumn represents the examined word and the second column lists the tested variants for w. None\nof the listed sentences is found from the training dataset.\nThe results indicate that C2W2C can learn the stucture of Finnish grammar and capture morpholog-\nical information that is encoded into words. Especially the sense of the word (e.g. esitt¨a¨a, esitti, on\nesitt¨anyt’to present, presented, has presented’ ) is a property where correct words yield better scores\nthan incorrect ones. Same applies to inﬂection forms (e.g. v ¨aitteist¨a, v ¨aitteilt¨a, v ¨aitteille, ‘from\nthe arguments’, ‘of the arguments’, ‘for the arguments’) especially when there are more than two\nsubsequent words having same inﬂection form.\nHowever, C2W2C is less good at predicting dual forms (whether a word is in singular or plural\nform) — usually the model prefers singular forms. One possible reason for this might be the score\nfunction; word score in C2W2C is not length-normalized, which prefers shorter singular words (e.g.\ntalo versus talot ‘house’ vesus ‘houses’,¨a¨anestyksen versus ¨a¨anestyksien ‘vote’ versus ‘votes’).\nPerhaps the most interesting feature of the C2W2C model is how well it can predict words that\nare not part of the dataset. Whereas traditional word-level models would use an unknown token\nfor such words, C2W2C can give scores to the actual words as long as they do not contain any\nunknown characters. For example, even if words sijoitusrahasto ‘mutual Fund’ andmonivuotisella\n‘multiannual’ in Table 2 do not belong to the training data, C2W2C can still assign a better scores\nto the grammatical correct unseen wordform than to their morphological variants found from the\ndataset!\n5.2.2 Semantics\nResults indicate that C2W2C can learn semantic meanings and part-of-speech information — for\nexample, if a verb is replaced with a noun, it usually yields worse scores than the correct sentence.\nThe same applies to semantics: for example, entities that can do something usually yield better\nscores when placed before verbs than passive entities in the same position. However, punctuation\nseems to be harder for C2W2C. Probably due to a signiﬁcant number of statements and the nature\n8\nSentence Variants Score\n<S> Olen huolestunut esitetyist¨a w . </S> v¨aitteist¨a* 5.143\nv¨aite 6.315\n<S> Komission on w aloitteen . </S> esitt¨anyt* 2.972\nesitt¨a¨a 4.254\n<S> N¨am¨a esitykset vaikuttavat w . </S> j¨arkevilt¨a* 5.557\nj¨arkev¨alt¨a 5.045\n<S> T¨am¨a w vaikuttaa kannattavalta . </S> sijoitusrahasto* 6.503\nsijoitusrahastot 7.605\n<S> S¨a¨ast¨ot aiotaan saavuttaa t¨all¨a w\nohjelmalla . </S>\nmonivuotisella* 6.503\nmonivuotista 7.658\nmonivuotinen 7.605\nTable 2: Word variant scores (lower is better) with different inﬂection forms. The grammatically\ncorrect alternative marked with *.\nof the dataset the model typically assigns better scores to periods than question marks, even if the\nsentence starts with a question word.\nA few example sentences and their scores are shown in Table 3. The notation and used scoring are\nsame as in Table 2.\n5.3 Sampling text with C2W2C\nAs an example application, the trained C2W2C model was conﬁgured to generate the Finnish polit-\nical text character by character. Two different strategies were tested: stochastic sampling and beam\nsearch sampling. Stochastic sampling is a simple approach where given a context of words, the next\nmost likely word is chosen and used as a part of the context when predicting the next character.\nBeam search is a heuristic search algorithm that holds k most likely word sequences and expands\nthem until all sequences are terminated (</S>is encountered).\nWithin those two strategies, beam search gave signiﬁcantly better results, whereas stochastic sam-\npling usually ended up looping certain phrases. Beam search also had some phrases and words\nit preferred over others, but the generated samples were much more natural, and the inﬂection of\nwords was better than with stochastic sampling. A two-layer search beam was used for the text\nsampling. Individual words were sampled with the beam of k = 20, and the sampled words and\ntheir probabilities were used with the sentence-level beam of k= 10.\nThe C2W2C model can produce, grammatically and semantically, sensible text, character by char-\nacter. Some of the best example sentences are displayed below (capitalization was added, and extra\nspaces were removed afterwards). The ﬁrst two words of each sentence were given as an initial\ncontext, and the rest of the words were sampled by the model.\n<S> Haluan kiitt ¨a¨a syd ¨amellisesti puheenjohtajavaltio Portugalia h ¨anen mi-\netinn¨ost¨a¨an. 6 </S>\n<S> N¨am¨a ovat eritt ¨ain t ¨arkeit¨a eurooppalaisia mahdollisuuksia Euroopan\nUnionin j¨asenvaltioiden perustamissopimuksen yhteydess¨a.7 </S>\n<S> Siit¨a huolimatta haluaisin onnitella esittelij ¨a¨a h ¨anen erinomaisesta mi-\netinn¨ost¨a¨an. 8</S>\n<S> T¨am¨a merkitsee sit ¨a, ett ¨a Euroopan Unionin perustamissopimuksen\nvoimaantulon tarkoituksena on kuitenkin v¨altt¨am¨at¨ont¨a eurooppalaisille j¨asenval-\ntioille.9 </S>\n6I would like to cordially thank the Portuguese Presidency for his report.\n7These are very important European opportunities in the context of the Treaty of the European Union Mem-\nber States.\n8Nevertheless I would like to congratulate the rapporteur on his excellent report.\n9This means that the entry into force of the Treaty of the European Union’s aim is, however, indispensable\nfor the European Member States.\n9\nSentence Variants Score\n<S> w on pyyt¨anyt uutta ¨a¨anestyst¨a . </S>\nBelgia* 5.160\nelokuu 6.963\nilmi¨o 5.345\n<S> Parlamentti w yksimielisesti asetuksesta . </S>\np¨a¨atti* 5.095\nkomissio 7.339\nsuuri 6.628\n<S> Oletteko tietoisia , ett¨a uhka on ohi w </S>\n?* 5.455\n. 5.164\n, 6.154\nTable 3: Sentence scores (lower is better) with different semantical variants. The grammatically\ncorrect alternative marked with *.\n6 Conclusions\nWe propose a new C2W2C character-to-word-to-character compositional language model and es-\ntimate its performance compared to the traditional word-level LSTM model. The experiments and\ntheir validations are done using Finnish language and the Europarl V7 corpus.\nOur experiments are sill preliminary, however, the discoveries mainly follow the ﬁndings of [14] and\n[5] — it is indeed possible to model succesfully a morphologically rich language such as Finnish\ncharacter by character and to signiﬁcantly reduce model parameters, especially with large corpuses.\nLarger scale experiments still need to be conducted, including other data sets and languages, to make\nfurther conclusions from the results.\nWith our setup the traditional word-level model still slightly outperforms the C2W2C model in\nterms of perplexity and convergence speed, but the C2W2C model compensates by successfully\nhandling out-of-vocabulary words. The model can also learn the Finnish grammar and assign better\nscores to grammatically correct sentences than incorrect ones, even though the inﬂection forms in\nthe sentence are not present in training data. The same applies to text sampling: C2W2C can sample\ngrammatically and semantically correct Finnish sentences character by character.\nReferences\n[1] Yoshua Bengio and Jean-S ´ebastien Sen ´ecal. Adaptive importance sampling to accelerate\ntraining of a neural probabilistic language model. IEEE Transactions on Neural Networks ,\n19(4):713–722, 2008.\n[2] Piotr Bojanowski, Armand Joulin, and Tomas Mikolov. Alternative structures for character-\nlevel rnns. CoRR, abs/1511.06303, 2015.\n[3] Jan A. Botha and Phil Blunsom. Compositional morphology for word representations and\nlanguage modelling. CoRR, abs/1405.4273, 2014.\n[4] Kyunghyun Cho, Bart van Merrienboer, C ¸ aglar G¨ulc ¸ehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statisti-\ncal machine translation. CoRR, abs/1406.1078, 2014.\n[5] Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. A character-level decoder without\nexplicit segmentation for neural machine translation. CoRR, abs/1603.06147, 2016.\n[6] Junyoung Chung, Caglar G ¨ulc ¸ehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback\nrecurrent neural networks. CoRR, abs/1502.02367, 2015.\n[7] Mathias Creutz and Krista Lagus. Unsupervised morpheme segmentation and morphology\ninduction from text corpora using Morfessor 1.0. Helsinki University of Technology, 2005.\n[8] Seppo Enarvi and Mikko Kurimo. Theanolm-an extensible toolkit for neural network language\nmodeling. arXiv preprint arXiv:1605.00942, 2016.\n[9] Filip Ginter and Jenna Kanerva. Fast training of word2vec representations using n-gram cor-\npora, 2014.\n10\n[10] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron C Courville, and Yoshua Bengio.\nMaxout networks. International Conference on Machine Learning, 28:1319–1327, 2013.\n[11] Joshua Goodman. Classes for fast maximum entropy training. In Acoustics, Speech, and\nSignal Processing, 2001. Proceedings.(ICASSP’01). 2001 IEEE International Conference on,\nvolume 1, pages 561–564. IEEE, 2001.\n[12] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive estimation: A new estimation prin-\nciple for unnormalized statistical models. InInternational Conference on Artiﬁcial Intelligence\nand Statistics, volume 1, page 6, 2010.\n[13] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–\n1780, Nov 1997.\n[14] Rafal J ´ozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. CoRR, abs/1602.02410, 2016.\n[15] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural\nlanguage models. CoRR, abs/1508.06615, 2015.\n[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2014.\n[17] Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In MT summit,\nvolume 5, pages 79–86, 2005.\n[18] Jason Lee, Kyunghyun Cho, and Thomas Hofmann. Fully character-level neural machine\ntranslation without explicit segmentation. arXiv preprint arXiv:1610.03017, 2016.\n[19] Krister Lind ´en, Erik Axelson, Sam Hardwick, Tommi A Pirinen, and Miikka Silfverberg.\nHfst—framework for compiling and applying morphologies. In International Workshop on\nSystems and Frameworks for Computational Morphology, pages 67–85. Springer, 2011.\n[20] Wang Ling, Tiago Lu ´ıs, Lu´ıs Marujo, Ram´on Fernandez Astudillo, Silvio Amir, Chris Dyer,\nAlan W. Black, and Isabel Trancoso. Finding function in form: Compositional character mod-\nels for open vocabulary word representation. CoRR, abs/1508.02096, 2015.\n[21] Thang Luong, Richard Socher, and Christopher D Manning. Better word representations with\nrecursive neural networks for morphology. In CoNLL, pages 104–113, 2013.\n[22] Tom ´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and J Cer-\nnocky. Subword language modeling with neural networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf), 2012.\n[23] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In\nAdvances in neural information processing systems, pages 1081–1088, 2009.\n[24] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efﬁciently with noise-\ncontrastive estimation. In Advances in Neural Information Processing Systems , pages 2265–\n2273, 2013.\n[25] David Money and Sarah L Harris. Digital design and computer architecture. Morgan Kauf-\nmann Publishers, 2007.\n[26] Miikka Silfverberg, Teemu Ruokolainen, Krister Lind ´en, and Mikko Kurimo. Finnpos: an\nopen-source morphological tagging and lemmatization toolkit for ﬁnnish.Language Resources\nand Evaluation, pages 1–16, 2015.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: a simple way to prevent neural networks from overﬁtting.Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[28] Martin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney. Lstm neural networks for language\nmodeling. In Interspeech, pages 194–197, 2012.\n[29] David Vilar, Jan-T Peter, and Hermann Ney. Can we translate letters? In Proceedings of the\nSecond Workshop on Statistical Machine Translation, pages 33–39. Association for Computa-\ntional Linguistics, 2007.\n11",
  "topic": "Character (mathematics)",
  "concepts": [
    {
      "name": "Character (mathematics)",
      "score": 0.8512148857116699
    },
    {
      "name": "Computer science",
      "score": 0.7552019357681274
    },
    {
      "name": "Word (group theory)",
      "score": 0.7297806739807129
    },
    {
      "name": "Vocabulary",
      "score": 0.7089836597442627
    },
    {
      "name": "Natural language processing",
      "score": 0.6664832234382629
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5897521376609802
    },
    {
      "name": "Linguistics",
      "score": 0.4900749921798706
    },
    {
      "name": "Language model",
      "score": 0.48414143919944763
    },
    {
      "name": "Mathematics",
      "score": 0.07614082098007202
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}