{
  "title": "Gaussian Radar Transformer for Semantic Segmentation in Noisy Radar Data",
  "url": "https://openalex.org/W4310925503",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2543752370",
      "name": "Zeller, Matthias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3182877758",
      "name": "Behley, Jens",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A4310925560",
      "name": "Heidingsfeld, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2631474168",
      "name": "Stachniss, Cyrill",
      "affiliations": [
        "University of Bonn"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137210930",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W2962912109",
    "https://openalex.org/W2968557240",
    "https://openalex.org/W2963046128",
    "https://openalex.org/W2959771705",
    "https://openalex.org/W4221163325",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W2968370607",
    "https://openalex.org/W2963281829",
    "https://openalex.org/W6637187546",
    "https://openalex.org/W6796562092",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W2966975099",
    "https://openalex.org/W3003437478",
    "https://openalex.org/W3137763535",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3093434340",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W3181190968",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3039448353",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W6718683173",
    "https://openalex.org/W2991216808",
    "https://openalex.org/W2940129212",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W3177330511",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W6840940796",
    "https://openalex.org/W2891649842",
    "https://openalex.org/W2963057320",
    "https://openalex.org/W2994788716",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2990710319",
    "https://openalex.org/W3035275207",
    "https://openalex.org/W4230808100",
    "https://openalex.org/W1667072054",
    "https://openalex.org/W4287025408",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3035481945",
    "https://openalex.org/W4298395628",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W4287593861",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W4313145913",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3127091182",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3168492867",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4287255572"
  ],
  "abstract": "Scene understanding is crucial for autonomous robots in dynamic environments\\nfor making future state predictions, avoiding collisions, and path planning.\\nCamera and LiDAR perception made tremendous progress in recent years, but face\\nlimitations under adverse weather conditions. To leverage the full potential of\\nmulti-modal sensor suites, radar sensors are essential for safety critical\\ntasks and are already installed in most new vehicles today. In this paper, we\\naddress the problem of semantic segmentation of moving objects in radar point\\nclouds to enhance the perception of the environment with another sensor\\nmodality. Instead of aggregating multiple scans to densify the point clouds, we\\npropose a novel approach based on the self-attention mechanism to accurately\\nperform sparse, single-scan segmentation. Our approach, called Gaussian Radar\\nTransformer, includes the newly introduced Gaussian transformer layer, which\\nreplaces the softmax normalization by a Gaussian function to decouple the\\ncontribution of individual points. To tackle the challenge of the transformer\\nto capture long-range dependencies, we propose our attentive up- and\\ndownsampling modules to enlarge the receptive field and capture strong spatial\\nrelations. We compare our approach to other state-of-the-art methods on the\\nRadarScenes data set and show superior segmentation quality in diverse\\nenvironments, even without exploiting temporal information.\\n",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2022. 1\nGaussian Radar Transformer\nfor Semantic Segmentation in Noisy Radar Data\nMatthias Zeller1 Jens Behley2 Michael Heidingsfeld3 Cyrill Stachniss4\nAbstract—Scene understanding is crucial for autonomous\nrobots in dynamic environments for making future state predic-\ntions, avoiding collisions, and path planning. Camera and LiDAR\nperception made tremendous progress in recent years, but face\nlimitations under adverse weather conditions. To leverage the full\npotential of multi-modal sensor suites, radar sensors are essential\nfor safety critical tasks and are already installed in most new\nvehicles today. In this paper, we address the problem of semantic\nsegmentation of moving objects in radar point clouds to enhance\nthe perception of the environment with another sensor modality.\nInstead of aggregating multiple scans to densify the point clouds,\nwe propose a novel approach based on the self-attention mech-\nanism to accurately perform sparse, single-scan segmentation.\nOur approach, called Gaussian Radar Transformer, includes the\nnewly introduced Gaussian transformer layer, which replaces the\nsoftmax normalization by a Gaussian function to decouple the\ncontribution of individual points. To tackle the challenge of the\ntransformer to capture long-range dependencies, we propose our\nattentive up- and downsampling modules to enlarge the receptive\nﬁeld and capture strong spatial relations. We compare our\napproach to other state-of-the-art methods on the RadarScenes\ndata set and show superior segmentation quality in diverse\nenvironments, even without exploiting temporal information.\nIndex Terms—Semantic Scene Understanding, Deep Learning\nMethods\nI. I NTRODUCTION\nA\nUTONOMOUS vehicles need to understand their sur-\nroundings to safely navigate in dynamic, real-world\nenvironments. To achieve holistic perception and enhance\nsafety, the sensor suites of autonomous vehicles are versatile\nto explore redundant information of individual sensors such as\ncameras, LiDAR, or radar. Particularly in autonomous driving,\nwhere a malfunction of one modality can result in lethal\nconsequences, redundancy is key. Widely explored cameras\nand LiDAR sensors capture the environment precisely but face\nlimitations under adverse weather such as fog, rain, and snow.\nAdditional information is required, which is accessible via\nradar sensors, and hence, makes them crucial to enable safe\nautonomous mobility.\nManuscript received: July 5, 2022; Revised: Sept 29, 2022; Accepted: Nov\n17, 2022. This paper was recommended for publication by Editor Markus\nVincze upon evaluation of the Associate Editor and Reviewers’ comments.\n1Matthias Zeller is with CARIAD SE and with the University of Bonn,\nGermany. 2Jens Behley is with the University of Bonn, Germany. 3Michael\nHeidingsfeld is with CARIAD SE, Germany. 4Cyrill Stachniss is with the\nUniversity of Bonn, Germany, with the Department of Engineering Science\nat the University of Oxford, UK, and with the Lamarr Institute for Machine\nLearning and Artiﬁcial Intelligence, Germany\nThis is a preprint of the article accepted at Robotics and Automation Letters\n(RA-L). © 2022 IEEE.\nDigital Object Identiﬁer (DOI) 10.1109/LRA.2022.3226030\ncar bike static truck\nradar cross section input\noutput\nvelocity vectors\n[dBm2]\n30\n−30\nour approach\nFig. 1: Our method performs semantic segmentation of moving\nobjects (bottom) from 4D sparse, single-scan radar point clouds (top)\nexploiting additional information including the velocity and the radar\ncross section. In the bottom image, each color represents a different\nsemantic class for moving objects (static is grey).\nIn this work, we investigate the semantic segmentation\nof moving objects in radar point clouds. This task requires\ndifferentiating between detections of moving and static ob-\njects and assigning a class label to each radar detection,\nas illustrated in Fig. 1. Compared to LiDAR point clouds,\nradar point clouds are noisier due to sensor noise and multi-\npath propagation and more sparse. However, radar sensors\nprovide additional information such as the relative velocity to\ndirectly indicate moving objects, making the sensor inherently\nsuitable for single-scan processing. Furthermore, the radar\ncross section depends on the structure, material, and surface\nof the reﬂections, which helps to differentiate objects.\nMost state-of-the-art methods for estimating semantics from\nradar data [23], [26] strongly rely on the aggregation of\ninformation over multiple scans to accurately perform seman-\ntic segmentation. However, aggregation inherently introduces\nlatency, making it unsuitable for tasks requiring immediate\ninformation about the vehicle’s vicinity, such as collision\navoidance. Therefore, this work investigates the processing of\nsingle scans by exploiting the additional information provided\nby radar sensors.\nThe main contribution of this paper is a new method for\naccurate, single-scan, radar-only semantic segmentation of\nmoving objects. It takes sparse point cloud representations\narXiv:2212.03690v1  [cs.CV]  7 Dec 2022\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2022.\nof radar scans as input and outputs a semantic label for\neach point. To extract discriminative point-wise features, we\nbuild on the self-attention mechanism, a fully attentive neural\nnetwork with our novel Gaussian transformer layer, and our\nattentive up- and downsampling modules as central building\nblocks. We optimize the transformer layer and enable the\ndecoupling via the usage of a Gaussian. Furthermore, our\nattentive sampling enables the capturing of complex local\nstructures and progressively increases the receptive ﬁeld of\nindividual points. We combine these building blocks in our\nnew backbone, called Gaussian radar transformer, to enhance\nfeature extraction on sparse and noisy radar point clouds.\nIn sum, we make three key claims: Firstly, our approach\ndemonstrates state-of-the-art performance for semantic seg-\nmentation of moving objects in sparse, single-scan radar point\nclouds without aggregating multiple scans and without ex-\nploiting temporal dependencies. Secondly, the Gaussian trans-\nformer layer and the attentive up-and downsampling modules\nimprove feature extraction by decoupling individual points and\nenlarging the receptive ﬁeld to enhance accuracy. Thirdly, our\nfully attentive network is able to extract discriminable features\nfrom additional sensor information such as Doppler velocity\nand radar cross section.\nII. R ELATED WORK\nThere is extensive literature on semantic segmentation of\npoint clouds, mostly, however, working on LiDAR data. The\nworks can be categorized into projection-based, voxel-based,\npoint-based, and hybrid methods [8].\nProjection-based methods are inspired by the successful\nconvolutional neural networks (CNNs) [13], [16]. Squeeze-\nSeg [33], SqueezeSegV2 [32], RangeNet++ [17], and Sal-\nsaNext [4] project the point cloud into frontal view images\nor 2D range images to exploit 2D convolutions. Milioto et\nal. [17] further alleviate the problem of blurry CNN output\nand discretization errors by efﬁcient GPU-enabled projective\nnearest neighbor search as a post-processing step to enhance\nsegmentation results. However, projection-based methods face\nseveral problems due to intermediate representation including\ndiscretization errors and occlusion.\nVoxel-based. To maintain the 3D geometric information\nbetween the data points, voxel-based encoding can be used.\nV oxSegNet [31] voxelizes the point clouds as dense cuboids\nand leverages atrous 3D convolution and attention-based ag-\ngregation to enhance feature extraction under limited resolu-\ntion. Since outdoor point clouds are sparse and vary in density,\njust a small percentage of voxels are occupied. This makes it\ninefﬁcient to apply dense convolution neural networks. To re-\nduce the computational burden, Graham et al. [6] propose sub-\nmanifold sparse convolutional networks which only generate\noutputs for occupied voxels. Following Polarnet [43], Zhu et\nal. [47] introduce the cylindrical partitioning, which does not\nalter the 3D topology compared to the 2D approach, and pro-\ncesses the features by asymmetrical 3D convolution networks.\nThe advancement in 3D point cloud processing has led to\nstate-of-the-art results of (AF)2-S3Net [3] and RPVNet [39] in\nthe SemanticKITTI LiDAR point cloud semantic segmentation\nbenchmark [1]. Xu et al. [39] combine the voxel-based method\nwith point- and projection-based encoding, utilizing a gated\nfusion module to adaptively merge the features leading to a\nhybrid approach. Since voxel-based methods inherently intro-\nduce discretization artifacts and information loss, the hybrid\nmethod utilizes point-wise information to alleviate the lossy\nencoding of information.\nPoint-based. To leverage the full potential of 3D points, es-\npecially for sparse point clouds, and keep the geometric infor-\nmation intact, point-based methods [12], [20], [27] have been\nintroduced. The pioneering work of Qi et al. [20] consumes\npoint clouds directly by shared multi-layer perceptrons (MLPs)\nand aggregates nearby information by symmetrical pooling\nfunctions. The successor PointNet++ [21] groups points hi-\nerarchically and progressively extracts features from larger\nlocal regions. Schumann et al. [24] adapt the approach and\noptimize the network for sparse radar point cloud processing.\nHowever, the ability to capture local 3D structures is limited,\nespecially in sparse point clouds. To circumvent, Schumann et\nal. [26] aggregate scans, include additional features, or exploit\nstrong temporal relationships. To combine local features and\nreduce the computational cost point-based methods beneﬁt\nfrom effective sampling strategies [10], [35], [40], [41]. The\nmost frequently used methods for small-scale point clouds are\nfarthest point sampling [21] and inverse density sampling [35].\nAnother approach to learning per-point local features is\nkernel-based convolutions. PointConv [35] uses an MLP\nwhereas KPConv [27] deﬁnes an explicit convolution to di-\nrectly learn the kernel. Nobis et al. [18] extended KPConv [27]\nand exploit the time dimension of multiple radar scans to\nperform object detection. Another method to elaborate a\nstronger connection of the individual points is graph-based,\nconducting message passing on the constructed graphs [12].\nPointWeb [46] uses adaptive feature adjustment to represent\nregions and capture local interactions. However, graph-based\nnetworks capture edge relationships of local patches which are\ninvariant to the deformation of these. Velickovic et al. [29] and\nWang et al. [30] utilize the self-attention mechanism which is\ninherently permutation invariant to leverage the limitations and\nfurther improve the accuracy.\nSelf-attention models have revolutionized natural language\nprocessing [5], [28] and inspired self-attention modules for\nimage recognition [11], [22], [44] and point cloud pro-\ncessing [36], [41]. Recent point transformer networks [7],\n[37], [42], [45] enhance state-of-the-art performance for 3D\npoint cloud understanding by elaborating the self-attention\nmechanism. PCT [7] proposes offset-attention to sharpen the\nattention weights by element-wise subtraction of the self-\nattention features and the input features. Point Transformer\nuses the vector-based subtraction attention [44] to aggregate\nlocal features whereas Stratiﬁed Transformer applies dot-\nproduct attention and increases the effective receptive ﬁeld\nby a window-based key-sampling strategy. Furthermore, recent\nwork elaborates positional encoding to enhance accuracy and\nkeep position information throughout the network [14].\nIn contrast to the related work, we propose a novel archi-\ntecture inspired by self-attention and point transformers. With\nour newly introduced Gaussian Radar Transformer we are\nZELLER et al.: GAUSSIAN RADAR TRANSFORMER FOR SEMANTIC SEGMENTATION IN NOISY RADAR DATA 3\nFCL (N, 32)\ninput (N,D) output (N,C)\nPS\nADS (N/2, 64)\nADS (N/4, 128)\nADS (N/8, 256)\nADS (N/16, 512)\nAUS (N/8, 256)\nAUS (N/4, 128)\nAUS (N/2, 64)\nAUS (N, 32)\nFCL (N, 16)\nFCL (N, nclasses)\nFCL (N/16, 512)\nGTB\nGTB\nGTB\nGTB\nGTB\nGTB\nGTB\nGTB\nGTB\nGTB\nFig. 2: The architecture of our Gaussian Radar Transformer for semantic segmentation of moving objects. FCL: fully connected layer, ADS:\nattentive downsampling, AUS: attentive upsampling, GTB: Gaussian transformer block\nable to capture complex structures in sparse point clouds and\nfurther extend the capabilities of the self-attention mechanism.\nFurthermore, our proposed fully attentive network includes\nadvanced sampling strategies and substantially enhances state-\nof-the-art performance for semantic segmentation of moving\nobjects in radar point clouds.\nIII. O UR APPROACH\nThe goal of our approach is to achieve accurate semantic\nsegmentation of moving objects in single-scan, sparse radar\npoint clouds to enhance scene understanding of autonomous\nvehicles. To accomplish this, we introduce a point-based\nframework to directly processes the input point cloud to omit\ninformation loss, and builds upon the successful self-attention\nmechanism throughout the network. Fig. 2 depicts our Gaus-\nsian Radar Transformer (GRT). We adopt the encoder-decoder\nstructure of the Point Transformer [45]. We replace each\nmodule and use our Gaussian transformer layer as the central\nbuilding block of each stage, which enables decoupled ﬁne-\ngrained feature aggregation. Furthermore, we introduce atten-\ntive up- and downsampling modules to enlarge the receptive\nﬁeld and extract discriminative features.\nA. Transformers\nBefore presenting our contribution, we shortly revisit trans-\nformers as they are a key ingredient in our work. Transformers\nand self-attention networks rely on the encoded representation\nof the input features xF ∈RD within the queries q, the keys\nk, and the values v, as follows:\nq = WqxF , k = WkxF , v = WvxF , (1)\nwhere Wq ∈ RD×D, Wk ∈ RD×D and Wv ∈ RD×D\nare the corresponding learned matrices of fully connected\nlayers or multi-layer perceptrons (MLPs). To calculate the\nattention scores Ai,j, different methods exist such as scalar\ndot-product [28] and vector attention [44]. The scaling by the\nfactor dC is intended to counteract the effect of small gradients\nfor the softmax if it grows large in magnitude and is deﬁned\nas follows:\nAi,j = softmax\n(\nqik⊤\nj√dC\n)\n. (2)\nThere is an alternative way for the weighting of individual\nfeature channels by vector attention that utilizes relation\nfunctions f such as addition or subtraction. To keep ﬁne-\ngrained position information throughout the network, Wu et\nal. [34] and Zhao et al. [45] use relative positional encoding\nri,j = pi −pj,1 ≤i,j ≤Nl. The ﬁnal attention weights Ai,j\nare determined by the softmax function:\nAi,j = softmax(f(qi,kj) +ri,j). (3)\nSince global self-attention leads to unacceptable memory\nconsumption and computational cost the inputs are restricted\nto local areas with Nl points determined by farthest point\nsampling and k nearest neighbor ( kNN) [21], [45]. The\nintermediate representation yj utilizing vector attention is\ncalculated as follows:\nyj =\nNl∑\ni=1\nAi,j ⊙vi. (4)\nThe aggregated features y are processed by an MLP with a\nlearnable weight matrix Wy ∈RD×D:\no = Wyy, (5)\nto calculate the ﬁnal output o.\nB. Gaussian Transformer Layer\nIn sparse radar point clouds, individual reﬂections contain\nessential information for downstream tasks such as seman-\ntic segmentation of moving objects. To enable independent\nand precise feature aggregation, we introduce the Gaussian\ntransformer layer (GTL) based on the Point Transformer\nlayer [45] including vector self-attention as illustrated in\nFig. 3 (a). Contrary to other approaches, including the Point\nTransformer [45], which focuses on dense point clouds, we do\nnot utilize the softmax function, which is deﬁned as:\nsi = exp(zi)∑Nl\nj=1 exp(zj)\n. (6)\nThe softmax function leads to a coupling of points since\nindividual outputs si are dependent on all inputs zj with\nj ∈{1,...,N l}, which is why the softmax function is also not\nscale invariant (weighting for dot-product attention Sec. III-A).\nFurthermore, the backpropagation of the loss Lthrough the\nsoftmax function to obtain the partial derivative ∂L\n∂zj\nto de-\ntermine the gradients at the input is dependent on all output\nvalues. The calculation of the chain rule of derivatives for the\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2022.\nxF\ns\nPl+1\nPs\nps\nFPS + kNN\nAttentive Sampling\nAttentive Sampling\n(c) attentive upsampling module\n(b) attentive downsampling module\nFCL\nFCL\nFCL\nFCL\naggregation\nxF\nl\nPl\npl\nFPS + kNNFCL\nPl+1\n+\nconcat.\nnorm.\nFCL\naggregation\nconcat.\nnorm.\nxF\nl\nPl\npl\nxF\nl\nPl pl\nr\nFCL\nPl+1\n-\npos. enc.\nk q v\nFCL, Wqkv\nGaussian\naggregation\n(a) Gaussian transformer layer\nFig. 3: The detailed design of each module of our Gaussian Radar Transformer (a) shows the Gaussian transformer layer, (b) the attentive\ndownsampling module, and (c) the attentive upsampling module. FCL: fully connected layer, pos. enc.: positional encoding, concat.:\nconcatenation, norm.: normalization, FPS: farthest point sampling, kNN: k nearest neighbor\nsoftmax can be expressed by the Jacobian matrix Jsoftmax as\nfollows:\n∂L\n∂z = Jsoftmax\n∂L\n∂s. (7)\nIf the output values grow in magnitude the gradients di-\nminish since the Jacobian converges to a zero matrix. Hence,\nthe error propagation is restricted, which slows down the\nlearning process. In contrast, we argue that points belonging\nto the same class should aggregate the information, whereas\npoints belonging to different classes reduce the information\naggregation to a minimum, both of which can lead to a close to\nzero Jacobian matrix. To overcome this limitation, we replace\nthe softmax function in Eq. (3) by a Gaussian function G,\nwhich is executed on every dimension of the vector, for vector\nself-attention:\nAi,j = G(f(qi,kj) +ri,j), (8)\nto assess ﬁne-grained information ﬂow for sparse radar point\nclouds. Since the Gaussian function is applied to each feature\nindividually, the points are decoupled, which enables a precise\ninformation aggregation to enhance feature extraction and\nperformance. Moreover, the partial derivative of the Gaussian\ndepends on a single output value sj. Hence, vanishing gradi-\nents may inﬂuence individual points but not whole local areas,\nwhich can be seen by the chain rule:\n∂L\n∂zj\n= ∂L\n∂sj\n∂sj\n∂zj\n. (9)\nTo derive the output o, we calculate the sum of the element-\nwise multiplication:\noj =\nNl∑\ni=1\nAi,j ⊙vi, (10)\nwithout further processing by a linear layer reducing compu-\ntational cost in contrast to Eq. (5). Following Qi et al. [21]\nand Zhao et al. [45], we determine the local areas by far-\nthest point sampling and kNN algorithm with k = Nl. We\ndirectly derive the queries qi, the keys ki, and the values\nvi by applying a fully connected layer with weight matrix\nWqkv ∈RD×3D. For the positional encoding, we adopt the\napproach of Zhao et al. [45]. We process the relative position\nby two fully connected layers and replace the activation\nfunction by the Gaussian error linear unit (GELU) [9] to\ndetermine the positional encoding.\nC. Gaussian Transformer Block\nOur Gaussian transformer layer (GTL) is embedded into\nthe center of the Gaussian transformer block (GTB) which is\na residual block, similar to the Point Transformer block [45],\nwith two fully connected layers processing the input and the\noutput. We replace the activation function with GELU after\neach fully connected layer. The GTB processes point clouds\nP with point coordinates pi ∈R2 and point-wise features\n(XF = {xF\n1 ,..., xF\nN }), where xF\ni ∈RD with feature dimen-\nsion D. The features of the individual points xF\ni are enriched\nby the information aggregation within the block enhanced by\nthe GTL. The point coordinates pi are utilized to calculate\nthe positional encoding but not further transformed to keep\ndetailed position information.\nD. Attentive Downsampling Layer\nTo reduce the cardinality of the point cloud Pl+1 ⊂Pl and\nthereby the number of points N, we process the point cloud by\nthe attentive downsampling layer, depicted in Fig. 3 (b). Our\napproach aims to enable adequate sampling and feature pro-\ncessing by applying the self-attention mechanism throughout\nthe network. To reduce computational complexity, we follow\nYang et al. [40] and calculate the attention weights by a single\nfeed-forward layer with the weight matrix Wf ∈R(D+2)×D\nand no direct representation of keys, queries, and values. We\nconcatenate the input features xF\ni and the point coordinates\npi to include positional information to calculate the attention\nweights Ai,j. Additionally, we normalize the attention weights\nover the whole point cloud to amplify the contribution of\nvaluable points. The ﬁnal weights are multiplied with the\ninput features xF within local areas which are determined\nby farthest point sampling and the kNN algorithm [21], with\nk= Nd resulting in:\nyi =\nNd∑\nj=1\nAi,j ⊙xF\nj . (11)\nThe features are fed into another fully connected layer\nwith LayerNorm [38] and a GELU activation function. In\nZELLER et al.: GAUSSIAN RADAR TRANSFORMER FOR SEMANTIC SEGMENTATION IN NOISY RADAR DATA 5\ncontrast to Point Transformer [45], which utilizes farthest point\nsampling and max pooling [21], our attentive downsampling\nincludes the information of nearby points, which we assume\nas valuable for sparse point clouds.\nE. Attentive Upsampling Layer\nTo deduce discriminative features, we argue that the up-\nsampling and feature concatenation of the skip connection is\ncrucial to further enhance performance. The common method\nfor upsampling, also utilized by Point Transformer [45], is\nan interpolation of the k = 3 nearest neighbors based on an\ninverse distance weighted average [21]. The interpolated points\nNu are concatenated with the features of the points, which\nare passed through the skip connection. The inverse distance\nweighted average does not include further feature-based infor-\nmation. Hence, the interpolation combines the features only\nbased on their relative position. This is reasonable for dense\npoint clouds because nearby points often belong to the same\nclass.\nHowever, this might be problematic for sparse point clouds,\nespecially for small instances, which are represented by single\npoints. Therefore, we consider upsampling as an important\npart to improve feature extraction and propose the attentive\nupsampling layer. The upsampling layer, which is illustrated\nin Fig. 3 (c), ﬁrst processes the features of the skip connection\nand the proceeding GTB by two separate fully connected\nlayers with LayerNorm and GELU activation function. To\npropagate the points from Pl to Pl+1 where Pl ⊂Pl+1 with\nNl ≤ Nl+1, we feed the position information of the two\npoint sets and the corresponding features into our attentive\nupsampling layer. We calculate the k nearest neighbors of\nthe individual points for the point set of the skip connection\nPs within the point cloud which has to be upsampled Pl.\nThe attention mechanism enables information aggregation of\nlarger local areas since the attention weights will control the\ninformation ﬂow and not reduce the discriminability which is\npossible if large local regions are interpolated. To integrate the\npositional information we calculate the relative position of the\nkNN of the two point sets given by:\nri,j = pi −pj, (12)\nwhere pj ∈Ps and pi ∈Pl. The relative distances ri,j are\nconcatenated with the features. Following our downsampling\nlayer, we calculate the attention weights directly by processing\nthe concatenated features with a fully connected layer and\nnormalizing the weights over the whole point cloud. The\noutput of the summation is processed by a fully connected\nlayer with LayerNorm and GELU activation function. The\nself-attention mechanism turns into an inter-attention between\nthe two point clouds to enable attentive feature aggregation.\nThe upsampling is repeated until we have broadcasted the\nfeatures to the original set of points. We optimize the infor-\nmation aggregation by determining the weighting based on\nthe relative position and the features. We emphasize that the\nsampling steps are essential for appropriate feature extraction\nof transformer architectures for sparse point clouds.\nF . Input Features\nThe input is a sparse radar point cloud with N points,\nfeature dimension D, and batch size b. Each point pi is deﬁned\nby two spatial coordinates xi, yi. Additionally, the radar\nsensors provide the ego-motion compensated Doppler velocity\nvi and the radar cross section σi resulting in a 4-dimensional\ninput vector xF\ni = (xi,yi,vi,σi)⊤.\nIV. I MPLEMENTATION DETAILS\nWe construct our architecture based on the self-attention\nmechanism. The central building blocks are the GTL and the\nattentive down- and upsampling modules to extract discrimi-\nnative features for point cloud understanding. The backbone\nadopts the U-Net architecture of Point Transformer [45] with\nan encoder-decoder architecture including skip connections.\nFirst, we directly extract features of the sparse input point\ncloud by a GTB and increase the per-point feature dimension\nto 32. The resulting features are progressively down-sampled\nby four consecutive stages where each reduces the cardinality\nof the point cloud by a factor of two resulting in [N/2, N/4,\nN/8, N/16] points. The per-point features are further gradually\nincreased to 64, 128, 256, and 512. The individual stages\ninclude the GTB and attentive downsampling modules in the\nencoder part, which are replaced by attentive upsampling mod-\nules in the decoder part of the network. The per-point features\nmaps of the ﬁnal decoder layer are processed by an MLP\nwith two fully connected layers to obtain point-wise semantic\nclasses PS = {pS\n1 ,...,p S\nN }, where pS\ni ∈{1,...,C }.\nWe implement the Gaussian Radar Transformer in Py-\nTorch [19]. To train the network, we utilize the SGD optimizer\nwith an initial learning rate of 0.05, a momentum of 0.9, and a\ncosine annealing learning rate scheduler [15]. The batch size\nb is set to 32. The loss combines the Lov ´asz loss [2] and\nweighted cross-entropy. We follow Schumann et al. [26] and\nset the weights of the cross-entropy loss for dynamic objects\nto 8.0 and for static to 0.5 to account for the class imbalance of\nthe data set. For the attentive sampling operations, we deﬁne\nk= 9for the kNN operation, and for the Gaussian transformer\nlayer, we restrict the local area to Nl = 16. We deﬁne G(x)\nas:\nG(x) = exp\n(−x2\n2\n)\n, (13)\nsuch that for x = 0 the attention weight is G(x) = 1 .\nAdditionally, we apply data augmentation, which includes\nscaling, rotation around the origin, jitter augmentation of the\ncoordinate features, and instance augmentation.\nV. E XPERIMENTAL EVALUATION\nThe main focus of this work is to enhance the semantic\nsegmentation of moving objects in sparse and noisy radar\npoint clouds. We present our experiments to show the ca-\npabilities of our method and to support our key claims that\nour approach achieves state-of-the-art performance in semantic\nsegmentation of moving objects in single-scan radar point\nclouds without exploration of temporal dependencies or the\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2022.\nIoU F1Method Input mIoU F1 static car ped. ped. grp. bike truck static car ped. ped. grp. bike truck\nRadarPNv1 [24] 61.0 74.3 98.7 58.2 36.0 58.7 58.4 56.1 99.4 73.6 52.9 74.0 73.8 71.9\nRadarPNv2 [26] aggregation 61.9 75.0 98.7 63.8 38.8 58.5 51.0 61.0 99.4 77.9 55.9 73.8 67.5 75.8\nPoint V oxel Transformer [42] 45.9 57.5 99.3 47.5 7.3 47.5 54.6 19.2 99.6 64.4 13.6 64.4 70.6 32.2\nPoint Transformer [45] 55.6 68.1 99.3 58.1 15.2 56.8 55.1 48.9 99.6 73.5 26.4 72.5 71.1 65.6\nGaussian Radar Transformer\nsingle-scan\n68.5 79.8 99.4 69.6 36.3 71.2 71.2 62.8 99.7 82.1 53.2 83.2 83.2 77.1\nTABLE I: Semantic segmentation results of moving objects on the RadarScenes test set in terms of IoU and F1 scores. The results of\nRadarPNv1 [24] and RadarPNv2 [26] are calculated based on the reported confusion matrix.\nGaussian Radar Transformer\nPoint Transformer\nPoint V oxel Transformer\nground truth\nreference images\ncar bike static truckped. ped. grp.\nFig. 4: Qualitative results of Point Transformer [45], Point V oxel Transformer [42], and Gaussian Radar Transformer on the test set of\nRadarScenes [25].\naggregation of scans. Moreover, we demonstrate that the Gaus-\nsian transformer layer and the attentive up- and downsampling\nmodules improve feature extraction and contribute to the ﬁnal\nperformance. Our fully attentive network is able to extract\nvaluable features from the Doppler velocity and radar cross\nsection provided by the radar sensor.\nA. Experimental Setup\nWe train and evaluate our method on RadarScenes [25],\nwhich is the only large-scale, open-source radar data set\nincluding point-wise annotations for varying scenarios. The\ndata set consists of 158 annotated sequences. We use the rec-\nommended 130 sequences for training and split the remaining\n28 sequences into validation (sequences: 6, 42, 58, 85, 99,\n122) and test set. The RadarScenes [25] data set is split into\nseparate scans for each of the four sensors. Since the ﬁeld-\nof-view of the sensors is restricted to certain areas, we derive\ndetailed information about the surrounding by merging the\nindividual sensor data from the four sensors into a single radar\npoint cloud. The measurement times and the pose information\nare given, which enables a transformation into a common\ncoordinate system. We aggregate four scans, one of each\nsensor, which results in the ﬁnal input point clouds with\ntransformed local coordinates. To evaluate the performance,\nSchumann et al. [25] propose the point-wise macro-averaged\nF1 scores based on all ﬁve moving object classes and the\nstatic background class ( C = 6 ). We further report the\nIoU = TP\nTP +FN +FP and mIoU = 1\nC\n∑C\ni=1 IoUi scores,\nwhich are common for semantic segmentation tasks [1]. We\ntrain each network using its speciﬁc hyperparameters with two\nNvidia RTX A6000 GPUs over 50 epochs on the training set\nand report the results on the test set. For more details on the\ntraining regime for Point Transformer1, we refer to the original\npaper [45].\n1https://github.com/POSTECH-CVLab/point-Transformer\nB. Semantic Segmentation of Moving Objects\nThe ﬁrst experiment presents the performance of our ap-\nproach on the RadarScenes test set to investigate the claim\nthat we achieve state-of-the-art results for semantic segmen-\ntation of moving objects in sparse and noisy radar point\nclouds without the aggregation of scans or the exploration\nof temporal dependencies. In this experiment, we compare\nour Gaussian Radar Transformer with the recent and high-\nperforming Point Transformer by Zhao et al. [45] as well\nas the baselines provided by Schumann et al. [24], [26]. We\nselected Point Transformer as a reference since the method\nmeets the following requirements: (1) single-scan input for\ncomparability; (2) point-based method, since the voxelization\nleads to discretization artifacts and hence a loss of infor-\nmation, see Point V oxel Transformer in Tab. I; (3) very\ngood performance on different benchmarks including semantic\nscene understanding. Furthermore, the Point Transformer [45]\nutilizes vector attention, which is beneﬁcial for point cloud\nunderstanding.\nOur Gaussian Radar Transformer outperforms the existing\nmethods in terms of both, mIoU and F1 score, as displayed\nin Tab. I. Especially, we achieve superior performance on\nﬁve of the six classes, except pedestrian. We assume that\nthe individual detection in radar scans contains important\ninformation, which is why strict point-based methods enhance\nthe performance compared to Point V oxel Transformer. The\nbaselines exploit temporal dependencies of consecutive radar\nscans within a memory feature map, utilize additional global\ncoordinates or densify the point clouds by aggregation. The\nexact comparison of the results is difﬁcult because Schumann\net al. work on a subset of the ofﬁcially released data set.\nHowever, the IoU for the class pedestrian indicates that the\nexploration of temporal information is beneﬁcial for small\ninstances. We suspect that the consistent detection of pedes-\ntrians over the whole sequence, which is difﬁcult for strict\nsingle-scan approaches, further improves the performance.\nNevertheless, the Gaussian Radar Transformer considerably\nimproves the IoU for the class pedestrian as opposed to Point\nZELLER et al.: GAUSSIAN RADAR TRANSFORMER FOR SEMANTIC SEGMENTATION IN NOISY RADAR DATA 7\n# ADS AUS GTL F1 mIoU\nA 74.0 61.0\nB ✓ 77.0 64.7\nC ✓ 77.3 65.5\nD ✓ ✓ 78.8 66.8\nE ✓ ✓ ✓ 79.4 68.3\nTABLE II: Inﬂuence of the different components of the approach in\nterms of mIoU and F1 score on the RadarScenes validation set.\nTransformer by more than 19 absolute percentage points.\nFig. 4 shows some qualitative results on the test set. Notably,\nour approach achieves superior performance under adverse\nweather including rain and fog.\nC. Ablation Studies on Method Components\nThe ﬁrst ablation study presented in this section is designed\nto support our second claim that our proposed self-attention\nmodules each contribute to the advancements of the Gaussian\nRadar Transformer. To assess the inﬂuence of the different\ncomponents of our fully attentive backbone, we evaluate the\nperformance in terms of mIoU and F1 score on the validation\nset. To replace our proposed modules, we follow commonly\nused network designs. We substitute the Gaussian function\nby the softmax function and keep the rest of the Gaussian\ntransformer layer as it is. For the attentive downsampling, we\nutilize local max pooling and we exchange attentive upsam-\npling by trilinear interpolation based on an inverse distance\nweighted average. Tab. II summarizes the inﬂuence of different\ncomponents on the performance in terms of mIoU on the\nvalidation set.\nIn conﬁguration (A), we replace each module by its sub-\nstitute, which leads to a noticeable decrease in mIoU. We\nsuspect that the commonly used modules are highly optimized\nfor denser point clouds but struggle to capture ﬁne-grained\ninformation from sparse and noisy radar point clouds. In\n(B), we add attentive downsampling (ADS), see Sec. III-D,\nwhich introduces a smooth information exchange within the\ndownsampling step of individual points, visibly improving the\nresults. In (C), we add the attentive upsampling (AUS) module\nto enlarge the receptive ﬁeld and include encoded feature infor-\nmation to optimize the information aggregation, see Sec. III-E.\nThe larger receptive ﬁeld resulting from the increased local\narea from three (trilinear) to nine points improves the F1\nscore by 3.3 and the mIoU by 4.5 absolute percentage points.\nAlthough the AUS only affects the features of the decoder part\nit leads to an additional improvement of mIoU by 0.8 absolute\npercentage points compared to AUS in (B). In (D), we add\nthe attentive up- and downsampling which further enhance\nthe performance. This shows the importance of the attentive\nsampling modules for sparse radar point cloud processing. In\n(E), we utilize the fully attentive network to illustrate the\nimprovement due to the usage of the Gaussian function by\ndecoupling individual points, see Sec. III-B, resulting in the\nbest performance. In conclusion, the Gaussian function and\nthe attentive up- and downsampling are essential to extract\nvaluable features from sparse and noisy radar point clouds.\nInput Features F1 mIoU\nxF = (x,y) 56.0 43.7\nxF = (x,y,σ ) 63.7 50.1\nxF = (x,y,v ) 75.0 62.0\nxF = (x,y,v,σ ) 79.4 68.3\nTABLE III: Inﬂuence of the different input features in terms of mIoU\nand F1 score on the RadarScenes validation set.\nD. Ablation Studies on Input Features\nThe third experiment evaluates the performance depending\non the provided information by the radar sensor and demon-\nstrates that our approach is capable of capturing complex\nlocal structures within the features to enhance mIoU. For this\nexperiment, we utilize our Gaussian Radar Transformer and\nadd to the position information of x and y coordinates, the\nego-motion compensated Doppler velocity v, the radar cross\nsection σ, or both. Tab. III displays the inﬂuence of the input\nfeatures xF on the validation set performance. As we presume,\nthe ego-motion compensated Doppler velocity is especially\nvaluable for semantic segmentation of moving objects since\nthe feature inherently distinguishes between moving and non-\nmoving parts of the environment resulting in an increase of\nmIoU of 18.2 absolute percentage points. Moreover, we further\nimprove the mIoU if we add the radar cross section features σ\nsuggesting that our approach extracts valuable features for the\ndownstream task from additional sensor information. Hence,\nthe Gaussian Radar Transformer achieves the best performance\nincluding radar cross section and ego-motion compensated\nDoppler velocity.\nIn summary, our evaluation supports our statement that\nour method provides competitive semantic segmentation per-\nformance of moving objects in single-scan, sparse radar\npoint clouds. At the same time, our method exploits self-\nattention modules which enhance the performance in multi-\ndimensional radar data processing outperforming state-of-the-\nart approaches. Thus, we support all our claims with this\nexperimental evaluation.\nVI. C ONCLUSION\nIn this paper, we presented a novel approach to perform se-\nmantic segmentation of moving objects in sparse, noisy, single-\nscan radar point clouds obtained from automotive radars.\nOur method exploits the self-attention mechanism throughout\nthe network and replaces the softmax normalization of the\ntransformer by a Gaussian. This allows us to successfully\nsegment moving objects and improve the feature extraction by\ndecoupling individual points. We implemented and evaluated\nour approach on the RadarScenes data set, providing compar-\nisons to other methods and supporting all claims made in this\npaper. The experiments suggest that the proposed architecture\nachieves good performance on semantic segmentation of mov-\ning objects. We assessed the different parts of our approach\nand compared them to other existing techniques. Overall, our\napproach outperforms the state of the art both in F1 score and\nmIoU, taking a step forward towards sensor redundancy for\nsemantic segmentation for autonomous robots and vehicles.\n8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED NOVEMBER, 2022.\nREFERENCES\n[1] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss,\nand J. Gall. SemanticKITTI: A Dataset for Semantic Scene Under-\nstanding of LiDAR Sequences. In Proc. of the IEEE/CVF Intl. Conf. on\nComputer Vision (ICCV) , 2019.\n[2] M. Berman, A.R. Triki, and M.B. Blaschko. The lov ´asz-softmax loss:\nA tractable surrogate for the optimization of the intersection-over-union\nmeasure in neural networks. In Proc. of the IEEE/CVF Conf. on\nComputer Vision and Pattern Recognition (CVPR) , 2018.\n[3] R. Cheng, R. Razani, E. Taghavi, E. Li, and B. Liu. (af)2-s3net: Atten-\ntive feature fusion with adaptive feature selection for sparse semantic\nsegmentation network. In Proc. of the IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR) , 2021.\n[4] T. Cortinhal, G. Tzelepis, and E. Erdal Aksoy. Salsanext: Fast,\nuncertainty-aware semantic segmentation of lidar point clouds. In\nProc. of the Int. Symp. on Visual Computing , 2020.\n[5] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. Le, and R. Salakhut-\ndinov. Transformer-XL: Attentive language models beyond a ﬁxed-\nlength context. In Proc. of the Annual Meeting of the Association for\nComputational Linguistics, 2019.\n[6] B. Graham, M. Engelcke, and L. van der Maaten. 3D Semantic\nSegmentation with Submanifold Sparse Convolutional Networks. In\nProc. of the IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[7] M.H. Guo, J. Cai, Z.N. Liu, T.J. Mu, R.R. Martin, and S. Hu. Pct: Point\ncloud transformer. Comput. Vis. Media , 7(2):187–199, 2021.\n[8] Y . Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun. Deep\nlearning for 3d point clouds: A survey. IEEE Trans. on Pattern\nAnalalysis and Machine Intelligence (TPAMI), 43(12):4338–4364, 2021.\n[9] D. Hendrycks and K. Gimpel. Bridging nonlinearities and stochastic\nregularizers with gaussian error linear units. arXiv preprint:1606.08415,\n2016.\n[10] Q. Hu, B. Yang, L. Xie, S. Rosa, Y . Guo, Z. Wang, N. Trigoni, and\nA. Markham. Randla-net: Efﬁcient semantic segmentation of large-scale\npoint clouds. In Proc. of the IEEE/CVF Conf. on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[11] A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold, J. Uszkor-\neit, L. Beyer, M. Minderer, M. Dehghani, N. Houlsby, S. Gelly, T. Un-\nterthiner, and X. Zhai. An image is worth 16x16 words: Transformers\nfor image recognition at scale. In Proc. of the Int. Conf. on Learning\nRepresentations (ICLR), 2021.\n[12] L. Landrieu and M. Simonovsky. Large-scale Point Cloud Semantic\nSegmentation with Superpoint Graphs. In Proc. of the IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR) , 2018.\n[13] Y . LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker,\nH. Drucker, I. Guyon, U. Muller, E. S¨ackinger, P. Simard, and V . Vapnik.\nComparison of learning algorithms for handwritten digit recognition. In\nProc. of the Int. Conf. on Artiﬁcial Neural Networks , 1995.\n[14] Y . Li, S. Si, G. Li, C.J. Hsieh, and S. Bengio. Learnable fourier\nfeatures for multi-dimensional spatial positional encoding. In Proc. of\nthe Conf. on Neural Information Processing Systems (NeurIPS) , 2021.\n[15] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm\nrestarts. In Proc. of the Int. Conf. on Learning Representations (ICLR) ,\n2017.\n[16] A. Milioto and C. Stachniss. Bonnet: An Open-Source Training and\nDeployment Framework for Semantic Segmentation in Robotics using\nCNNs. In Proc. of the IEEE Intl. Conf. on Robotics & Automation\n(ICRA), 2019.\n[17] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss. RangeNet++: Fast\nand Accurate LiDAR Semantic Segmentation. In Proceedings of the\nIEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) , 2019.\n[18] F. Nobis, F. Fent, J. Betz, and M. Lienkamp. Kernel point convolution\nlstm networks for radar point cloud segmentation. Applied Sciences ,\n11:2599–2618, 2021.\n[19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. In Proc. of\nthe Conf. on Neural Information Processing Systems (NeurIPS) , 2019.\n[20] C.R. Qi, H. Su, K. Mo, and L.J. Guibas. PointNet: Deep Learning on\nPoint Sets for 3D Classiﬁcation and Segmentation. In Proc. of the IEEE\nConf. on Computer Vision and Pattern Recognition (CVPR) , 2017.\n[21] C. Qi, K. Yi, H. Su, and L.J. Guibas. PointNet++: Deep Hierarchical\nFeature Learning on Point Sets in a Metric Space. In Proc. of the\nConf. on Neural Information Processing Systems (NeurIPS) , 2017.\n[22] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens. Stand-alone self-attention in vision models. In Proc. of the\nConf. on Neural Information Processing Systems (NeurIPS) , 2019.\n[23] N. Scheiner, F. Kraus, F. Wei, B. Phan, F. Mannan, N. Appenrodt,\nW. Ritter, J. Dickmann, K. Dietmayer, B. Sick, and F. Heide. Seeing\naround street corners: Non-line-of-sight detection and tracking in-the-\nwild using doppler radar. In Proc. of the IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR) , 2020.\n[24] O. Schumann, M. Hahn, J. Dickmann, and C. W ¨ohler. Semantic\nsegmentation on radar point clouds. In Proc. of the Int. Conf. on\nInformation Fusion, 2018.\n[25] O. Schumann, M. Hahn, N. Scheiner, F. Weishaupt, J.F. Tilly, J. Dick-\nmann, and C. W ¨ohler. Radarscenes: A real-world radar point cloud data\nset for automotive applications. In Proc. of the Int. Conf. on Information\nFusion, 2021.\n[26] O. Schumann, J. Lombacher, M. Hahn, C. W ¨ohler, and J. Dickmann.\nScene understanding with automotive radar. IEEE Trans. on Intelligent\nVehicles, 5(2):188–203, 2020.\n[27] H. Thomas, C. Qi, J. Deschaud, B. Marcotegui, F. Goulette, and\nL. Guibas. KPConv: Flexible and Deformable Convolution for Point\nClouds. In Proc. of the IEEE/CVF Intl. Conf. on Computer Vision\n(ICCV), 2019.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez,\nŁ. Kaiser, and I. Polosukhin. Attention is all you need. In Proc. of the\nConf. on Neural Information Processing Systems (NeurIPS) , 2017.\n[29] P. Veli ˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li `o, and\nY . Bengio. Graph Attention Networks. In Proc. of the Int. Conf. on\nLearning Representations (ICLR) , 2018.\n[30] X. Wang, S. Liu, X. Shen, C. Shen, and J. Jia. Associatively Segmenting\nInstances and Semantics in Point Clouds. In Proc. of the IEEE/CVF\nConf. on Computer Vision and Pattern Recognition (CVPR) , 2019.\n[31] Z. Wang and F. Lu. V oxsegnet: V olumetric cnns for semantic part\nsegmentation of 3d shapes. IEEE Trans. on Visualization and Computer\nGraphics, 26(9):2919–2930, 2019.\n[32] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer. SqueezeSegV2:\nImproved Model Structure and Unsupervised Domain Adaptation for\nRoad-Object Segmentation from a LiDAR Point Cloud. In Proc. of the\nIEEE Intl. Conf. on Robotics & Automation (ICRA) , 2019.\n[33] B. Wu, A. Wan, X. Yue, and K. Keutzer. Squeezeseg: Convolutional\nneural nets with recurrent crf for real-time road-object segmentation\nfrom 3d lidar point cloud. In Proc. of the IEEE Intl. Conf. on Robotics\n& Automation (ICRA) , 2018.\n[34] K. Wu, H. Peng, M. Chen, J. Fu, and H. Chao. Rethinking and\nimproving relative position encoding for vision transformer. In Proc. of\nthe IEEE/CVF Intl. Conf. on Computer Vision (ICCV) , 2021.\n[35] W. Wu, Z. Qi, and L. Fuxin. Pointconv: Deep convolutional networks\non 3d point clouds. In Proc. of the IEEE/CVF Conf. on Computer Vision\nand Pattern Recognition (CVPR) , 2019.\n[36] S. Xie, S. Liu, Z. Chen, and Z. Tu. Attentional shapecontextnet for point\ncloud recognition. In Proc. of the IEEE/CVF Conf. on Computer Vision\nand Pattern Recognition (CVPR) , 2018.\n[37] L. Xin, L. Jianhui, J. Li, W. Liwei, Z. Hengshuang, L. Shu, Q. Xiaojuan,\nand J. Jiaya. Stratiﬁed transformer for 3d point cloud segmentation.\nIn Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2022.\n[38] R. Xiong, Y . Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang,\nY . Lan, L. Wang, and T. Liu. On layer normalization in the transformer\narchitecture. In Proc. of the Int. Conf. on Machine Learning (ICML) ,\n2020.\n[39] J. Xu, R. Zhang, J. Dou, Y . Zhu, J. Sun, and S. Pu. Rpvnet: A deep\nand efﬁcient range-point-voxel fusion network for lidar point cloud\nsegmentation. In Proc. of the IEEE/CVF Intl. Conf. on Computer Vision\n(ICCV), 2021.\n[40] B. Yang, S. Wang, A. Markham, and N. Trigoni. Robust attentional\naggregation of deep feature sets for multi-view 3d reconstruction.\nIntl. Journal of Computer Vision (IJCV) , 128(1):53–73, 2020.\n[41] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian. Modeling\npoint clouds with self-attention and gumbel subset sampling. In Proc. of\nthe IEEE/CVF Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2019.\n[42] C. Zhang, H. Wan, X. Shen, and Z. Wu. Pvt: Point-voxel transformer\nfor point cloud learning. arXiv preprint, 2019.\n[43] Y . Zhang, Z. Zhou, P. David, X. Yue, Z. Xi, B. Gong, and H. Foroosh.\nPolarnet: An improved grid representation for online lidar point clouds\nsemantic segmentation. In Proc. of the IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR) , 2020.\nZELLER et al.: GAUSSIAN RADAR TRANSFORMER FOR SEMANTIC SEGMENTATION IN NOISY RADAR DATA 9\n[44] H. Zhao, J. Jia, and V . Koltun. Exploring self-attention for image\nrecognition. In Proc. of the IEEE/CVF Conf. on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[45] H. Zhao, L. Jiang, J. Jia, P.H. Torr, and V . Koltun. Point transformer. In\nProc. of the IEEE/CVF Intl. Conf. on Computer Vision (ICCV) , 2021.\n[46] T. Zhao, Y . Xu, M. Monfort, W. Choi, C. Baker, Y . Zhao, Y . Wang,\nand Y . Wu. Multi-Agent Tensor Fusion for Contextual Trajectory\nPrediction. In Proc. of the IEEE/CVF Conf. on Computer Vision and\nPattern Recognition (CVPR), 2019.\n[47] X. Zhu, H. Zhou, T. Wang, F. Hong, Y . Ma, W. Li, H. Li, and\nD. Lin. Cylindrical and asymmetrical 3d convolution networks for lidar\nsegmentation. In Proc. of the IEEE/CVF Conf. on Computer Vision and\nPattern Recognition (CVPR), 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7516011595726013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6368867754936218
    },
    {
      "name": "Segmentation",
      "score": 0.5796200037002563
    },
    {
      "name": "Computer vision",
      "score": 0.5606048107147217
    },
    {
      "name": "Radar",
      "score": 0.5576053261756897
    },
    {
      "name": "Point cloud",
      "score": 0.4462505578994751
    },
    {
      "name": "Telecommunications",
      "score": 0.10037034749984741
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135140700",
      "name": "University of Bonn",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4387154616",
      "name": "Lamarr Institute for Machine Learning and Artificial Intelligence",
      "country": null
    },
    {
      "id": "https://openalex.org/I4923324",
      "name": "Fraunhofer Society",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 28
}