{
  "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks",
  "url": "https://openalex.org/W3131755153",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5066076737",
      "name": "Nikunj Saunshi",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A5077971483",
      "name": "Sadhika Malladi",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A5103209777",
      "name": "Sanjeev Arora",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2752172973",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3046882683",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W1768488313",
    "https://openalex.org/W2964230347",
    "https://openalex.org/W1508567213",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2890487780",
    "https://openalex.org/W3149173402",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2995335514",
    "https://openalex.org/W102708294",
    "https://openalex.org/W44262497",
    "https://openalex.org/W2785524755",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2605035112",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3009571263",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2949988325",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that ϵ-optimal language models in cross-entropy (log-perplexity) learn features that are O(ϵ)-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.",
  "full_text": "A Mathematical Exploration of Why Language Models\nHelp Solve Downstream Tasks\nNikunj Saunshi1, Sadhika Malladi1, and Sanjeev Arora1,2\n1Department of Computer Science, Princeton University\n{nsaunshi, smalladi, arora}@cs.princeton.edu\n2Institute for Advanced Study\nAbstract\nAutoregressive language models, pretrained using large text corpora to do well on next word\nprediction, have been successful at solving many downstream tasks, even with zero-shot usage.\nHowever, there is little theoretical understanding of this success. This paper initiates a mathematical\nstudy of this phenomenon for the downstream task of text classiﬁcation by considering the following\nquestions: (1) What is the intuitive connection between the pretraining task of next word prediction\nand text classiﬁcation? (2) How can we mathematically formalize this connection and quantify the\nbeneﬁt of language modeling? For (1), we hypothesize, and verify empirically, that classiﬁcation\ntasks of interest can be reformulated as sentence completion tasks, thus making language modeling a\nmeaningful pretraining task. With a mathematical formalization of this hypothesis, we make progress\ntowards (2) and show that language models that areϵ-optimal in cross-entropy (log-perplexity)\nlearn features that canlinearly solve such classiﬁcation tasks withO(√ϵ) error, thus demonstrating\nthat doing well on language modeling can be beneﬁcial for downstream tasks. We experimentally\nverify various assumptions and theoretical ﬁndings, and also use insights from the analysis to design\na new objective function that performs well on some classiﬁcation tasks.\n1 Introduction\nThe construction of increasingly powerful language models has revolutionized natural language processing\n(NLP). Using gigantic text corpora and a cross-entropy objective, language models are trained to predict\na distribution over thenext wordto follow a given context (piece of text). Pretrained language models\nare useful for many downstream NLP tasks, either as initializations [Ramachandran et al., 2017, Howard\nand Ruder, 2018] or as a source of contextual word embeddings [McCann et al., 2017, Peters et al.,\n2018]. Recent models [Radford et al., 2019, Brown et al., 2020] have even bypassed the need for careful\nﬁne-tuning and have demonstrated strong performance on downstream tasks without ﬁne-tuning. This\nwork aims to understand this incredible success of language models.\nSince next word prediction is a powerful test of language understanding, at an intuitive level it is\nbelievable that doing well on language modeling can help with many diverse NLP tasks. At the same\ntime, it is quite intriguing how improvements in the test perplexity of language models translate to\nbetter downstream performance. Attempting to understand this phenomenon naturally raises the\nfollowing questions:(a) why should training on the next-word prediction task, with the cross-entropy\nobjective, result in useful features for downstream tasks? (b) what role do inductive biases of the model\narchitecture and training algorithms play in this empirical success?Given the nascency of deep learning\ntheory, it is very challenging to say anything mathematically precise about (b) for deep networks.\nGiven these diﬃculties, this paper focusses on the mathematical study of (a) by exploring if and how\nquantitative improvements on downstream NLP tasks can bemathematically guaranteedfor language\n1\narXiv:2010.03648v2  [cs.CL]  14 Apr 2021\nmodels that do well on the cross-entropy objective. As a ﬁrst cut analysis, we restrict attention to\ntext classiﬁcation tasksand the striking observation that they can be solved fairly well withlinear\nclassiﬁers on top of ﬁxed language models features, i.e. without ﬁnetuning (Table 1). Although we\ntreat models as black boxes, just ﬁrst-order optimality conditions of the cross-entropy objective reveal\ninteresting properties of learned features, leading to an understanding of their success on classiﬁcation\ntasks. Insights from the analysis help us construct a simple objective (Quad), that provably learns\nuseful features for classiﬁcation tasks, as also veriﬁed empirically. We summarize our contributions\nalong with an overview of the paper below.\nIn Section 2, we set up notation and formally describe language modeling and the ubiquitous low-\ndimensional softmax parametrization, along with a description of the cross-entropy objective and\nproperties of its optimal solutions. We then describe the observation, in Section 3.1, that text\nclassiﬁcation tasks of interest can be reformulated as sentence completion tasks. Amenability to such\na reformulation is mathematically formalized (Section 3.2) as the classiﬁcation task being anatural\ntask: tasks that can be solvedlinearly using conditional distribution over words following an input\ntext. Section 4 presents our main results, theorems 4.1 and 4.2, that use the above formalization to\nmathematically quantify the utility of language model features on natural tasks:ϵ-optimal language\nmodel (in cross-entropy) will doO(√ϵ)-well on such tasks. Theorem 4.2 shows a stronger result for\nlow-dimensional softmax models by leveraging a new tool,conditional mean features(Deﬁnition 4.1),\nwhich we show (Section 6) to be eﬀective in practice. The usefulness of the language model features\nthemselves is demonstrated by arguing a weak linear relationship between them and conditional mean\nfeatures. In Section 5.2, we present a new mathematically motivated objective (Quad) that has formal\nguarantees. Experiments in Section 6 verify the sentence completion reformulation idea and the good\nperformance of conditional mean features on standard benchmarks.\n1.1 Related work\nText embedding methods:Prior to language models, large text corpora like Wikipedia [Merity et al.,\n2016] were used to learn low-dimensional embeddings for words [Mikolov et al., 2013b,a, Pennington\net al., 2014] and subsequently for sentences [Kiros et al., 2015, Arora et al., 2017, Pagliardini et al.,\n2018, Logeswaran and Lee, 2018] for downstream task usage. These methods were inspired by the\ndistributional hypothesis [Firth, 1957, Harris, 1954], which posits that meaning of text is determined in\npart by the surrounding context. Recent methods like BERT [Devlin et al., 2018] and variants [Lan et al.,\n2019, Yang et al., 2019, Liu et al., 2019] learn models from auxiliary tasks, such as sentence completion,\nand are among the top performers on downstream tasks. In this work we consider autoregressive models\nand make a distinction from masked language models like BERT; Table 2 shows that language model\nand BERT features have comparable performances.\nLanguage models for downstream tasks:We are interested in language models [Chen and Good-\nman, 1999], especially those that use neural networks to compute low-dimensional features for contexts\nand parametrize the next word distribution using softmax [Xu and Rudnicky, 2000, Bengio et al., 2003].\nLanguage models have shown to be useful for downstream tasks as initializations [Ramachandran et al.,\n2017, Howard and Ruder, 2018] or as learned feature maps [Radford et al., 2017, McCann et al., 2017,\nPeters et al., 2018]. The idea of phrasing classiﬁcation tasks as sentence completion problems to use\nlanguage models is motivated by recent works [Radford et al., 2019, Puri and Catanzaro, 2019, Schick\nand Schütze, 2020] that show that many downstream tasks can be solved by next word prediction for\nan appropriately conditioned language model. This idea also shares similarities with work that phrase\na suite of downstream tasks as question-answering tasks [McCann et al., 2018] or text-to-text tasks\n[Raﬀel et al., 2019] and symbolic reasoning as ﬁll-in-the-blank tasks [Talmor et al., 2019]. Our work\nexploits this prevalent idea of task rephrasing to theoretically analyze why language models succeed on\n2\ndownstream tasks.\nRelevant theory:Since the success of early word embedding algorithms like word2vec [Mikolov et al.,\n2013a] and GloVe [Pennington et al., 2014], there have been attempts to understand them theoretically.\nLevy and Goldberg [2014] argue that word2vec algorithm implicitly factorizes the PMI matrix. Noise\nContrastive Estimation (NCE) theory is used to understand word embeddings [Dyer, 2014] and to show\nparameter recovery for negative sampling based conditional models [Ma and Collins, 2018]. A latent\nvariable model [Arora et al., 2016] is used to explain and unify various word embedding algorithms.\nTheoretical justiﬁcation is provided for sentence embedding methods either by using a latent variable\nmodel [Arora et al., 2017] or through the lens of compressed sensing [Arora et al., 2018]. Also relevant is\nrecent work on theory for contrastive learning [Arora et al., 2019, Tosh et al., 2020b,a, Wang and Isola,\n2020] and reconstruction-based methods [Lee et al., 2020], which analyze the utility of self-supervised\nrepresentations learned for downstream tasks. Our work is the ﬁrst to analyze the eﬃcacy of language\nmodel features on downstream tasks.\n2 Language modeling and optimal solutions\nWe useSto denote the discrete set of all contexts, i.e. complete or partial sentences (preﬁxes),Wto\ndenote the vocabulary of words, withV = |W|being the vocabulary size. For a discrete setA, let∆A\ndenote the set of distributions onA. We usep,pL ∈∆S to denote probability distributions overS, and\np·|s,p∗\n·|s ∈∆W to denote conditional distributions, wherep·|s(w) is the predicted probability of wordw\nfollowing contexts and p∗\n·|s(w) denotes the true conditional probability. Boldfacep·|s,p∗\n·|s ∈RV denote\nvectors of probabilities forp·|s,p∗\n·|s ∈∆W. Forv∈RV, v(w) indexes the coordinate forw∈W; p·|s(w)\nis the probability ofw according top·|s. We useφw ∈Rd to denote ad-dimensional embedding for word\nw; word embeddings are stacked into the columnsΦ ∈Rd×V. We usef : S→ Rd for a feature map\nfrom contexts tod-dimensional embeddings, e.g.f(s) can be the output of a Transformer model for\ninput contexts∈S. For embeddings{θs}s∈S with θs ∈RD (any D), we use{θs}to denoteg: S→ RD\nsuch thatg(s) = θs.\n2.1 Language modeling using cross-entropy\nLanguage model aims to learn the true distribution of a text corpus and a popular approach to do so is\nthrough next word prediction. Given a context (e.g., a sentences∈S), it predicts a distributionp·|s\nover the word to follow, e.g. for the context “The food was ”, the model could place high probabilities on\nwords “delicious”, “expensive”, “bland”, etc. We usepL to denote the true distribution over the context\nset Sin the language modeling corpus. A standard approach is to minimize the expected cross-entropy\nloss between the true distributionp∗\n·|s and the model predictionp·|s. We deﬁne the cross-entropy loss\nfor a language model with output vector of probabilities{p·|s}s∈S as\nℓxent({p·|s}) = E\ns∼pL\nE\nw∼p∗\n·|s\n[\n−log(p·|s(w))\n]\n= E\ns∼pL\n[\nℓxent,s(p·|s)\n]\n(1)\nTo understand what language models learn, we look at the optimal solution of the cross-entropy objective.\nWhile one cannot practically hope to learn the optimal solution due to optimization, statistical and\nexpressivity limitations, the optimal solution at least tells us the best that language modeling can hope\nto do. A well-known property of cross-entropy objective is that its optimal solution isp∗\n·|s, which can\nbe proved by noting thatℓxent,s(p·|s) = DKL(p∗\n·|s,p·|s) + C.\nProposition 2.1(Cross-entropy recoversp∗\n·|s). The unique minimizer ofℓxent({p·|s}) is p·|s = p∗\n·|s for\nevery s∈support(pL).\n3\n2.2 Softmax parametrized language modeling\nUnlike traditional language models like n-gram models, neural language models parametrize the\nconditional distributionp·|s as a softmax computed usinglow dimensionalembeddings. For an embedding\nθ ∈Rd, the softmax distribution overWusing word embeddingsΦ ∈Rd×V is pθ,Φ(w) = eθ⊤φw/Zθ,\nwhere Zθ = ∑\nw′∈Weθ⊤φw′ is the partition function. Whilepθ,Φ depends onΦ, we will usepθ instead\nwhenever Φ is clear from context. Just likep∗\n·|s, we can interpretpθ ∈RV as a vector of probabilities\nfor the distributionpθ.\nWe now describe the abstraction for softmax models that is applicable to most neural models. A language\nmodel ﬁrst embeds a contexts into f(s) ∈Rd using a feature mapf : S→ Rd that is parametrized by\nan architecture of choice (e.g. Transformer [Vaswani et al., 2017]). The output conditional distribution\nis set to be the softmax distribution induced by the context embeddingf(s) and word embeddingsΦ,\ni.e. p·|s = pf(s). The cross-entropy in its familiar form is presented below\nℓxent(f,Φ) = E\ns∼pL\nE\nw∼p∗\n·|s\n[\n−log(pf(s)(w))\n]\n= E\ns∼pL\n[\nE\nw∼p∗\n·|s\n[−f(s)⊤φw] + log(Zf(s))\n]\n(2)\nWe rewrite it asℓxent(f,Φ) = E\ns∼pL\n[ℓxent,s(f(s),Φ)], whereℓxent,s(θ,Φ) = ℓxent,s(pθ,Φ) is the cross-entropy\nloss for a contexts that uses embeddingθ. Analogous to Proposition 2.1, we would like to know the\noptimal d-dimensional feature mapf∗and the induced conditional distributionpf∗(s)1.\nProposition2.2 (Softmaxmodelsrecover p∗\n·|s onasubspace) . Fix a ﬁxedΦ, iff∗∈arg minf:S→Rd ℓxent(f,Φ)\nexists, thenΦpf∗(s) = Φp∗\n·|s for everys∈support(pL).\nUnlike Proposition 2.1,pf∗(s) ∈RV is only guaranteed to be equal top∗\n·|s ∈RV on thed-dimensional\nsubspace spanned by rows ofΦ ∈Rd×V. We may not learnp∗\n·|s exactly whend<V , but this result\nat least guarantees learningp∗\n·|s on alinear subspacedetermined by word embeddingsΦ. This forms\nthe basis for our main results later and is proved by using the ﬁrst-order optimality condition, i.e.\n∇θℓxent,s(f∗(s)) = 0 , ∀s ∈S. The gradient of cross-entropy is∇θℓxent,s(θ) = −Φp∗\n·|s + ∇θZθ/Zθ =\n−Φp∗\n·|s + Φpθ. Setting it to 0 completes the proof. We use the properties of optimal solutions to\nunderstand why language models help with classiﬁcation tasks.\n3 Using language models for classiﬁcation tasks\nSections 2.1 and 2.2 suggest that language models aim to learnp∗\n·|s, or a low-dimensional projection\nΦp∗\n·|s. Thus to understand why language models help with downstream tasks, a natural starting point\nis to understand how access top∗\n·|s can help with downstream tasks. In a thought experiment, we\nuse oracle access top∗\n·|s for anys and demonstrate that sentence classiﬁcation task can be solved by\nreformulating it as a sentence completion problem and usingp∗\n·|s to get completions to predict the label.\nThis sentence completion reformulation is mathematically formalized asnatural tasks.\n3.1 Sentence completion reformulation\nFor exposition, we consider the sentence classiﬁcation task of sentiment analysis, where the inputs are\nmovie reviews (subset ofS) and labels belongs to{±1}, denoting positive and negative reviews.\nClassiﬁcation task as sentence completion:Can we predict the label for a movie reviews by\nusing p∗\n·|s? One way is to usep∗\n·|s to compare probabilities of “:)” and “:(” following a movie review and\n1A ﬁnite minimizer may not always exist. This is handled in Section 4 that deals withϵ-optimal solutions.\n4\nto predict sentiment based on which is higher. This seems like a reasonable strategy, since “:)” is likelier\nthan “:(” to follow a positive movie review. One issue, however, is thatp∗\n·|s will place much higher\nprobability on words that start sentences, like “The”, rather than discriminative words useful for the\ntask. To allow a larger set of grammatically correct completions, we can append a prompt like “This\nmovie is ” at the end of all movie reviews and query probabilities of indicative adjectives like good, bad,\ninteresting, boring etc. that are better indicators of sentiment. This approach of adding a prompt can\nalso work for other classiﬁcation tasks. For the AG news dataset [Zhang et al., 2015] containing news\narticles from 4 categories (world, science/tech., sports, business), a prompt like “This article is about ”\ncan help solve the task. The theoretical and practical relevance of prompts is discussed in Theorem 4.1,\nand Section 6 respectively. We note that the choice of prompts and completion words is less important\nthan the underlying idea of sentence completion reformulation and its formalization.\nSolving tasks using a linear function ofp∗\n·|s: The above process is actually a sub-case of using a\nlinear classiﬁer on top ofp∗\n·|s ∈RV. For sentiment analysis, ifw+ = “:)”and w−= “:(”, then the sign\nof p∗\n·|s(w+) −p∗\n·|s(w−) can predict the sentiment. This strategy can be expressed asv⊤p∗\n·|s, where the\nlinear classiﬁerv∈RV has v(w+) = 1, v(w−) = −1 and v(w′) = 0 for w′∈W\\{w+,w−}. Similarly\nwith the prompt, we can assign positive weights invto adjectives like “good” and negative weights to\nadjectives like “boring”. Strength of sentiment in diﬀerent adjectives (e.g., “good” vs “amazing”) can be\ncaptured through diﬀerent weights. This equivalence between sentence completion reformulation and\nlinear classiﬁer onp∗\n·|s is further explored in Section D.1. Other tasks can be similarly solved with a\ndiﬀerent set of words for each class. We verify experimentally that SST and AG news tasks can be\nsolved by a linear function of probabilities of just a small subset of words in Section 6 and for many\nother classiﬁcation tasks in Section F.1, thus lending credibility to the sentence completion view.\n3.2 Natural classiﬁcation tasks\nWe now translate the above sentence completion reformulation into a reasonable mathematical charac-\nterization for classiﬁcation tasks of interest. Firstly we formally deﬁne text classiﬁcation tasks and the\nstandard metric for performance of linear classiﬁcation on ﬁxed features. A binary classiﬁcation task2\nT is characterized by a distributionpT over S×{± 1}, where the inputs is a piece of text fromSand\nthe labely is in{±1}. Given a feature mapg: S→ RD (arbitrary D), T is solved by ﬁtting a linear\nclassiﬁer v∈RD on top ofg(s) and the metric of classiﬁcation loss is\nℓT(g,v) = E(s,y)∼pT\n[\nℓ(v⊤g(s),y)\n]\n; ℓT(g) = inf\nv∈RD\nℓT(g,v) (3)\nwhere ℓ is a 1-Lipschitz surrogate to the 0-1 loss, like the hinge lossℓ(ˆy,y) = (1 −yˆy)+ or the\nlogistic lossℓ(ˆy,y) = log(1 + e−yˆy). For given embeddings{θs}s∈S, the classiﬁcation loss is written as\nℓT({θs},v) = E(s,y)∼pT[ℓ(v⊤θs,y)].\nWe now formalize classiﬁcation tasks amenable to sentence completion reformulation, from Section 3.1),\nas (τ,B)-natural tasks, i.e. tasks that achieve a small classiﬁcation loss ofτ by using a linear classiﬁer\nwith ℓ∞-norm bounded3 by B on top of featuresp∗\n·|s ∈RV.\nDeﬁnition 3.1.A classiﬁcation taskT is (τ,B)-natural if min\nv∈RV,∥v∥∞≤B\nℓT({p∗\n·|s},v) ≤τ.\nWhile we motivated this formalization of linear classiﬁcation overp∗\n·|s in Section 3.1, we provide a\nmathematical justiﬁcation in Section D.1, along with interpretations forτ and B that relate them to\nthe Bayes optimal predictor and probability mass of indicative words respectively. Low dimensional\n2Extending tok-way tasks is straightforward.\n3ℓ∞makes sense since∥p∗\n·|s∥1 = 1 & ∥·∥∞is dual norm of∥·∥1.\n5\nsoftmax models, however, only learnp∗\n·|s in the subspace ofΦ, per Proposition 2.2. Thus we are also\ninterested in subset of tasks that this subspace can solve.\nDeﬁnition 3.2.Task T is (τ,B)-natural w.r.t.Φ ∈Rd×V if min\nv∈row-span(Φ),∥v∥∞≤B\nℓT({p∗\n·|s},v) ≤τ.\nNote that every(τ,B)-natural task w.r.t.Φ is trivially(τ,B)-natural, though the converse may not hold.\nHowever it can be argued that ifΦ has some “nice properties”, then(τ,B)-natural tasks of interest will\nroughly also be(τ,B)-natural w.r.t. Φ. Capturing the synonym structure of words can be such a nice\nproperty, as discussed in Section D.2. A better understanding of these properties of word embeddingsΦ\ncan potentially enable better performance of language models on downstream tasks. In fact, Section 5.2\ndescribes a carefully designed objective that can learn word embeddings with desirable properties like\nsynonyms having similar embeddings. In the subsequent sections, we use the above formalization to\nshow guarantees for language models on natural tasks.\n4 Guarantees for language models on natural tasks\nWe now show guarantees for features from language models on natural tasks in two cases: 1) for an\narbitrary language model{p·|s}where we useV-dimensional featuresp·|s ∈RV for downstream tasks\nand 2) for softmax language model(f,Φ) where we use newd-dimensional featuresΦpf(s) ∈Rd. Since\nwe cannot practically hope to learn the optimal solutions described in propositions 2.1 and 2.2, we only\nassume that the language models areϵ-optimal in cross-entropy. We ﬁrst deﬁneℓ∗\nxent to be the minimum\nachievable cross-entropy andℓ∗\nxent(Φ) to be the minimum achievable cross-entropy by ad-dimensional\nsoftmax language model usingΦ; clearlyℓ∗\nxent ≤ℓ∗\nxent(Φ).\nℓ∗\nxent = ℓxent({p∗\n·|s}), ℓ∗\nxent(Φ) = E\ns∼pL\n[\ninf\nθ∈Rd\nℓxent,s(θ,Φ)\n]\n(4)\nWe ﬁrst present the results for arbitrary language models with a proof sketch that describes the main\nideas, following which we present our main results for softmax language models.\n4.1 Arbitary language models\nWe show guarantees for a language model that isϵ-optimal, i.e.ℓxent({p·|s})−ℓ∗\nxent ≤ϵ, on(τ,B)-natural\ntasks. An important consideration is that the language model distributionpL of contexts is often\na diverse superset of the downstream distributionpT (deﬁned in Section 2.2) over sentences, thus\nrequiring us to show how guarantees ofp·|s ≈p∗\n·|s on averageover the distributions∼pL transfer to\nguarantees on a subsetpT. In the worst case, all of theϵ error in cross-entropy by{p·|s}is incurred on\nsentences from the subsetpT, leading to pessimistic bounds4. In practice, however, the errors might be\nmore evenly distributed acrosspL, thus bypassing this worst case bound. As a ﬁrst step, we present the\nworst case bound here; stronger guarantees are in Section 5.1. The worst-case coeﬃcientγ(pT), deﬁned\nbelow, captures thatpT is aγ(pT)-fraction ofpL.\nγ(pT) = sup{γ ∈(0,1] : pL(s) ≥γpT(s) ∀s∈S} (5)\nWe now present our results that applies to any language model, regardless of the parametrization\n(e.g., n-gram models, softmax models). The result suggests that small test cross-entropy (hence test\nperplexity) is desirable to guarantee good classiﬁcation performance, thus formalizing the intuition that\nbetter language models will be more useful for downstream tasks.\n4For instance ifpT is 0.001 fraction ofpL, {p·|s}could have1000ϵ error onpT and 0 error on rest ofpL.\n6\nTheorem 4.1.Let {p·|s}be a language model that isϵ-optimal, i.e. ℓxent({p·|s}) −ℓ∗\nxent ≤ϵ, for some\nϵ> 0. For a classiﬁcation taskT that is(τ,B)-natural, we have\nℓT\n(\n{p·|s}\n)\n≤τ +\n√\n2B2ϵ(γ(pT))−1\nThis upper bounds classiﬁcation loss on taskT for V-dimensional features{p·|s}from anϵ-optimal\nlanguage model. We discuss factors that lead to small upper bound and corresponding intuitions.\n•ϵ is small: learned language model has smaller cross-entropy (log-perplexity)\n•τ is small: task can be solved well through a sentence completion reformulation with a set of indicative\nwords as completions, as in Section 3.1, and has small Bayes error (cf. Section D.1)\n•B is small: set of indicative words has high probability mass inp∗\n·|s (cf. Section D.1). This could\npotentially explain the superior performance when prompts are added (Section 6).\n•γ(pT) is large: pT is closer topL; note thatγ(pT) ≤1 with equality if and only ifpT = pL\nThus the bound captures meaningful intuitions about good performance of language models on down-\nstream tasks. We provide a detailed proof sketch in Section E.1 and a strengthened version of this\n(Theorem B.1) is presented in Section E.6. Proving this result requires connecting the classiﬁcation\nloss with language modeling cross-entropy loss and dealing with distribution mismatch; we present a\nrough outline to do so below. SinceT is (τ,B)-natural, letv∗be the classiﬁer with∥v∗∥∞≤B and\nℓT({p∗\n·|s},v∗) ≤τ. The result follows from the following 3 inequalities:\nℓT\n(\n{p·|s},v∗)\n−ℓT({p∗\n·|s},v∗) ≤\n√\nE\ns∼pT\n[(v∗⊤(p·|s −p∗\n·|s))2] ... Lipschitzness + Jensen’s\nE\ns∼pT\n[(v∗⊤(p·|s −p∗\n·|s))2] ≤γ(pT)−1 E\ns∼pL\n[(v∗⊤(p·|s −p∗\n·|s))2] ... TransferpT to pL\n∀v∈RV, (v⊤(p·|s −p∗\n·|s))2 ≤2∥v∥2\n∞(ℓxent,s(p·|s) −ℓxent,s(p∗\n·|s)) ... Pinsker’s inequality\nThe ﬁrst and third inequalities (Lemma E.8 and Lemma E.3) connect the classiﬁcation loss to the\ncross-entropy loss in language modeling, while the second inequality deals with distribution mismatch\nbetween pL and pT. We now present a stronger result for softmax models.\n4.2 Softmax language model with conditional mean features\nWe now consider a softmax language model with feature mapf that satisﬁesℓxent(f,Φ) −ℓ∗\nxent(Φ) ≤ϵ;\nsuboptimality is measured w.r.t. the best d-dimensional model, unlike Theorem 4.1,. Note that\nTheorem 4.1 can be invoked here to give a bound ofℓT({pf(s)}) ≤τ + O(B√ϵ+ ϵ∗\nΦ) on (τ,B)-natural\ntasks, whereϵ∗\nΦ = ℓ∗\nxent(Φ) −ℓ∗\nxent is the suboptimality of the bestd-dimensional model. The ﬁxed error\nof O(B√ϵ∗\nΦ) (even whenϵ= 0), however, is undesirable. We improve on this by proving a stronger\nresult speciﬁcally for softmax models. Inspired by Proposition 2.2, our guarantees are for features\nΦpf(s) ∈Rd called conditional mean features.\nDeﬁnition 4.1(Conditional Mean Features). For a feature mapf : S→ Rd and Φ ∈Rd×V, we deﬁne\nconditional mean featuresΦpf : S→ Rd, whereΦpf(s) = Φpf(s), wherepf(s) ∈RV.\nWe now present the result for softmax language models that has similar implications as Theorem 4.1,\nbut with above-mentioned subtle diﬀerences.\nTheorem 4.2.For a ﬁxedΦ, letf be features from anϵ-optimal d-dimensional softmax language model,\ni.e. ℓxent(f,Φ) −ℓ∗\nxent(Φ) ≤ϵ. For a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ,\nℓT(Φpf) ≤τ +\n√\n2B2ϵ(γ(pT))−1\n7\nThis result guarantees good performance of conditional mean featuresΦpf on some natural tasks,\nthereby suggesting a novel way to extract features for downstream tasks. We empirically verify the\ngood performance ofΦpf(s) on classiﬁcations tasks (Section 6) and also ﬁnd aO(√ϵ)-like behavior\n(Section F.5). The proof (Section E.3) is similar to that of Theorem 4.1, the main diﬀerence being the use\nof the following inequality, proved using asoftmax variant of Pinsker’s inequality(Lemma E.4).\n∀v∈row-span(Φ), (v⊤(pf(s) −p∗\n·|s))2 ≤2∥v∥2\n∞(ℓxent,s(pf(s)) − inf\nf∗(s)∈Rd\nℓxent,s(pf∗(s)))\nThe more general result (Theorem 5.1) replacesγ(pT) with a more reﬁned coeﬃcient (Section 5.1).\nWhile guarantees are only for natural tasks w.r.t.Φ, Section D.2 discusses why this might be enough\nfor tasks of interestif word embeddingsΦ satisfy nice properties.\n4.3 Φpf(s) is a linear function off(s)\nTheorem 4.2 shows thatΦpf is useful for linear classiﬁcation. However, using feature mapf directly\nis more standard and performs better in practice (Section 6). Here we argue that there is a linear\nrelation betweenf and Φpf if word embeddingsΦ satisfy a certain Gaussian-like property, which we\nshow implies that tasks solvable linearly withΦpf are also solvable linearly usingf.\nAssumption 4.1.There exists a symmetric positive semideﬁnite matrixA∈Rd×d, a vectorb∈Rd\nand a constantc∈R such thatlog(Zθ) = 1\n2 θ⊤Aθ+ θ⊤b+ c for anyθ∈Rd.\nIf word embeddings were distributed as Gaussians, i.e.V columns of Φ are sampled fromN(µ,Σ)\nindependently, it is not hard to show (Lemma E.1) thatlog(Zθ) ≈1\n2 θ⊤Σθ+ θ⊤µ+ log(V). While\nsome papers [Arora et al., 2016, Mu and Viswanath, 2018] have noted that word embeddings are fairly\nrandom-like in the bulk to argue that the log partition function is constant for∥θ∥2 = 1, our quadratic\nassumption is a bit stronger. However, empirically we ﬁnd the ﬁt to be very good, as evident in Figure 1.\nUnder the above assumption, we can show a linear relation betweenf and Φpf.\nLemma 4.3.Under Assumption 4.1, feature mapf satisﬁes Φpf(s) = Af(s) + b,∀s∈S.\nCorollary 4.1.Under same setting as Lemma 4.3 and Theorem 4.2,ℓT(f) ≤τ + O(B√ϵ).\nThis shows thatf itself is good for natural classiﬁcation tasks. However, in practice, the linearity\nbetween f and Φpf only weakly holds on features from pretrained GPT-2 [Radford et al., 2018]. The\nfractional residual norm of the best linear ﬁt, i.e.r =\nE\ns∼p\n∥Φpf(s)−Af(s)−b∥2\nE\ns∼p\n∥Φpf(s)∥2 , measured for diﬀerent\ndistributions (r = 0 is perfect ﬁt) are 0.28 for SST, 0.39 for AG News, and 0.18 for IMDb contexts.\nThis non-trivial linear relationship, although surprising, might not completely explain the success off,\nwhich usually performs better thanΦpf; we leave exploring this to future work.\n5 Extensions\n5.1 Better handling of distributional shift\nThe bounds in the previous section use the coeﬃcientγ(pT) to transfer guarantees frompL to pT and\nwe deﬁne a more reﬁned notion of transferability here. The coeﬃcientγ(pT) is independent of the\nlearned model and assumes a worst case distribution of errors. For the reﬁned coeﬃcient, we ﬁrst deﬁne\nthe error made in predicted probabilities by a softmax language modelf as ∆{pf(s)}(s) = pf(s) −p∗\n·|s.\nFor any distributionp∈∆S, we deﬁne uncentered covariance of a functiong : S→ RD as Σp(g) =\n8\nFigure 1: Learned quadratic function v/s log partition function on various datasets for features computed\nfrom pre-trained GPT-2 to verify Assumption 4.1. We also plot they= x line for reference.\nEs∼p\n[\ng(s)g(s)⊤]\n. The reﬁned transferability coeﬃcient is then deﬁned as\nγ(p; Φpf) :=\n(ΣpL(Φ∆{pf(s)})−1\n2 Σp(Φ∆{pf(s)})ΣpL(Φ∆{pf(s)})−1\n2\n\n2\n)−1\nWe state the reﬁned result for softmax language models; detailed results are deferred to Section B.\nTheorem 5.1(Simpliﬁed). In the same setting as Theorem 4.2,ℓT(Φpf) ≤τ +\n√\n2B2ϵ\nγ(pT;Φpf)\nIt is easy show thatγ(pT; Φpf) ≥γ(pT), so this is indeed a stronger bound. The coeﬃcientγ(pT; Φpf)\nmeasures how average error onf on pL can propagate topT. This can potentially be much smaller than\nγ(pT) due to some inductive biases off. For instance, if errors made by the model are random-like,\ni.e. ∆{pf(s)}(s) ∼ρ, independently ofs, thenΣpL(Φ∆{pf(s)}) ≈Σp(Φ∆{pf(s)}) ≈Eη∼ρ[ηη⊤], making\nγ(p; Φpf) ≈1. Independence prevents accumulation of language modeling error on contexts frompT,\nbypassing the worst case transfer ofγ(pT).\n5.2 Quad: A new objective function\nIn Deﬁnition 3.2 we discuss how low dimensional softmax language models learn a linear projection\nof p∗\n·|s, only solving tasks that lie in the row span of word embeddingsΦ. Although Φ deﬁnes tasks\nthat language model features can solve, the standard cross-entropy objective does not lend a simple\nclosed form expression for optimalΦ. This motivates the construction of our Quad objective, that has\ntwo nice properties: (1) the optimal feature mapf∗is a linear function ofp∗\n·|s and thus can solve some\nnatural tasks, and (2) the optimalΦ∗has an intuitively meaningful closed-form solution.\nℓquad(f,Φ) = E\ns∼pL\n[\nE\nw∼p∗\n·|s\n[−f(s)⊤φw] + 1\n2∥Φ⊤f(s)∥2\n]\n(6)\nThe Quad objective is very similar to the cross-entropy objective from Equation (2), with the log\npartition function replaced by a quadratic function, inspired in part by Assumption 4.1. We can derive\nthe optimal solutionΦ∗that depends on the eigen-decomposition of asubstitutability matrix.\nDeﬁnition 5.1. The substitutability matrix is deﬁned to be Ω∗ := E\ns∼pL\n[\np∗\n·|s p∗\n·|s\n⊤\n]\n∈ RV×V. If\nΩ∗= USU⊤ is the eigendecomposition, thenUd ∈RV×d is matrix of topd eigenvectors ofΩ∗.\nThe matrixΩ∗captures substitutability between pairs of words. Wordswand w′are substitutable if they\nhave identical conditional probabilities for every contexts∈S and thus can replace occurrences of each\nother while still providing meaningful completions. By deﬁnition, these words satisfyΩ∗[w] = Ω∗[w′].\nSuch pairs of words were called “free variants\" in the work on distributional semantics [Harris, 1954],\nand capture the notion of synonyms; more in Section D.2.\n9\nTable 1: Accuracy (%) onk-way linear classiﬁcationusing ﬁxed GPT-2 features. Good performance of\nfeatures f(s), conditional mean featuresΦpf(s) and meaningful subset of≤30 (and ≤2k) coordinates\nof pf(s) verify the sentence completion reformulation and main results. The numbers right below the\nfeatures denote dimensionality of the features. An asterisk indicates that we added a task-speciﬁc\nprompt. Other baselines are ﬁne-tuning (FT, Section F.2) and random projection ofpf(s) (rand. proj.).\nSentence version of SST (train/test: 6.9K/1.8K) is used.\nTask k f(s)\n768\nΦpf (s)\n768\npf(s) (subset)\n≤30\npf(s) (class words)\n≤2k\npf(s) (rand. proj.)\n768 FT\nSST 2 87.5 83.3 82.6 78.7 67.5 91.4\nSST* 2 89.4 87.3 85.4 79.1 76.4 92.3\nSST ﬁne 5 49.2 43.5 44.0 39.2 23.1 50.2\nSST ﬁne* 5 49.4 48.6 47.6 40.3 28.8 53.5\nAG 4 90.7 84.6 83.8 75.4 58.5 94.5\nAG* 4 91.1 88.2 86.1 75.1 63.7 94.4\nTheorem 5.2. Let f∗,Φ∗= arg minf,Φ ℓquad(f,Φ). Then Φ∗= BU⊤\nd , for full rankB∈Rd×d. Also,\nfor a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ∗, we haveℓT(f∗) ≤τ.\nThus f∗excels on natural tasks w.r.t.Φ∗, which in turn, is the bestd-dimensional projection ofΩ∗.\nThus wordsw,w′∈W that are synonyms (hence substitutable) will satisfyφ∗\nw = φ∗\nw′, fulﬁlling the\ndesired property for word embeddings discussed in Deﬁnition 3.2.\nWe train using the Quad objective and compare its performance to a similarly trained GPT-2 language\nmodel. The results in Table 3 suggest that Quad performs comparably toΦpf from the cross-entropy\nobjective, which ﬁts our theory since both are linear functions ofp∗\n·|s. Section F.3 has more details and\nexperiments. The goal of testing Quad is to demonstrate that theoretical insights can aid the design of\nprovably eﬀective algorithms. Refer to Section C for more details on Quad.\n6 Experiments\nWe use experiments to verify (1) linear classiﬁcation on ﬁxed language model features does comparably\nto ﬁne-tuning the features, (2) sentence completion reformulation (Section 3.1), i.e. tasks can be solved\nusing probabilities for indicative words, (3) conditional mean features are eﬀective.\nTasks using linear function ofp∗\n·|s: We validate our claims from Section 3 that classiﬁcation tasks\ncan be solved by linear functions ofp∗\n·|s. Since p∗\n·|s is never available, we instead use the output features\nf(s) and probabilitiesp·|s := pf(s) from a small pretrained GPT-2 model [Radford et al., 2019]. Table 1\ndemonstrates that on binary and ﬁne-grained Stanford Sentiment Treebank (SST) [Socher et al., 2013]\nand AG News [Zhang et al., 2015] tasks, probabilitiespf(s) of just 30 or so task-relevant tokens (see\nSection F.1) can solve the tasks. Even just one/two token per class (“class words”) yields non-trivial\nperformance. Furthermore, we validate the sentence completion reformulation in Section 3.1 by using the\nprobabilities pf(s) after adding a task speciﬁc prompt and consistently observing improved performance,\nincluding for ﬁne-tuning (FT) with small datasets.\nΦpf and f are good features:We ﬁrst note that linear classiﬁcation over ﬁxed featuresf(s) from\nthe pretrained model performs comparably to the FT baseline. We further validate Theorem 4.2 by\nverifying that the conditional mean featuresΦpf(s) also linearly solve downstream tasks fairly well.\nThis performance is comparable to, but always worse thanf(s), as seen in columns 3 and 4 of Table 1.\nWe again ﬁnd that adding a prompt improves performance. Note that a random projection ofpf(s) to\nsame dimensions asΦpf(s) has very poor performance. Section E.5 has results for a wider range of\nclassiﬁcation tasks. Evidence for Assumption 4.1 is provided by learning a quadratic function to ﬁt the\n10\nlog partition function of features from pretrained GPT-2 model (see Section F.4). Figure 1 demonstrates\nthat the ﬁt holds for its training and unseen data (e.g., WebText [Radford et al., 2019]).\n7 Conclusions and future work\nWe provide intuitive and mathematical explanations for the success of language model features on\nclassiﬁcation tasks by reformulating them as sentence completion problems. This reformulation is\nformalizedas natural tasks: thosethatcanbesolvedlinearlyusingtheconditionalprobabilitydistribution\np∗\n·|s. Insights from our analysis help design the Quad objective that provably learns good features for\nthese natural tasks. We hope our analysis will inspire other mathematical insights into language models.\nWhile Section 4.3 argues linearity between conditional mean featuresΦpf and f, it is insuﬃcient to\nexplain the observed superiority off over Φpf. We leave exploring this limitation of our analysis to\nfuture work. Guarantees for softmax models are for natural tasks w.r.t.Φ, thus knowing the optimal\nd-dimensional word embeddingsΦ∗ for ℓxent(f,Φ) is also important. Other meaningful directions\ninclude providing guarantees for other successful models like BERT [Devlin et al., 2018] and more\ndiverse downstream tasks. Although we would like to show stronger guarantees by exploiting model\nand algorithmic inductive biases, as well as study the setting of ﬁne-tuning language model features,\nlack of a good theory of deep learning is the current bottleneck.\nAcknowledgments: Sanjeev Arora, Sadhika Malladi and Nikunj Saunshi are supported by NSF, ONR,\nSimons Foundation, Amazon Research, DARPA and SRC.\n11\nReferences\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable\nmodel approach to pmi-based word embeddings.Transactions of the Association for Computational\nLinguistics, 2016.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence\nembeddings. In Proceedings of the International Conference on Learning Representations, 2017.\nSanjeev Arora, Mikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli. A compressed sensing view\nof unsupervised text embeddings, bag-of-n-grams, and LSTMs. InProceedings of the International\nConference on Learning Representations, 2018.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A\ntheoretical analysis of contrastive unsupervised representation learning. InProceedings of the 36th\nInternational Conference on Machine Learning, 2019.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.\nDbpedia: A nucleus for a web of open data. InProceedings of the 6th International The Semantic\nWeb and 2nd Asian Conference on Asian Semantic Web Conference, 2007.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language\nmodel. Journal of machine learning research, 2003.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165, 2020.\nStanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language\nmodeling. Computer Speech & Language, 13, 1999.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018.\nChris Dyer. Notes on noise contrastive estimation and negative sampling.arXiv preprint arXiv:1410.8251,\n2014.\nJohn R Firth. A synopsis of linguistic theory, 1930-1955.Studies in linguistic analysis, 1957.\nZellig Harris. Distributional structure.Word, 1954.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146, 2018.\nMinqing Hu and Bing Liu. Mining and summarizing customer reviews. InProceedings of the Tenth\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004.\nMikhail Khodak, Nikunj Saunshi, Yingyu Liang, Tengyu Ma, Brandon Stewart, and Sanjeev Arora.\nA la carte embedding: Cheap but eﬀective induction of semantic feature vectors. InProceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\n2018.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint\narXiv:1412.6980, 2014.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Skip-thought vectors. InAdvances in neural information processing systems, 2015.\n12\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of language representations.arXiv preprint\narXiv:1909.11942, 2019.\nJason D. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:\nprovable self-supervised learning.arXiv preprint arXiv:2008.01064, 2020.\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. InAdvances\nin neural information processing systems, 2014.\nXin Li and Dan Roth. Learning question classiﬁers. InProceedings of the 19th international conference\non Computational linguistics-Volume 1, 2002.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nLajanugen Logeswaran and Honglak Lee. An eﬃcient framework for learning sentence representations.\nIn Proceedings of the International Conference on Learning Representations, 2018.\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional\nmodels: Consistency and statistical eﬃciency. InProceedings of the Conference on Empirical Methods\nin Natural Language Processing, 2018.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. InProceedings of the 49th Annual Meeting of\nthe ACL: Human Language Technologies, 2011.\nJulian J. McAuley, Rahul Pandey, and Jure Leskovec. Inferring networks of substitutable and comple-\nmentary products. CoRR, 2015.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contex-\ntualized word vectors. InAdvances in Neural Information Processing Systems, 2017.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering.arXiv preprint arXiv:1806.08730, 2018.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843, 2016.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word representations\nin vector space.arXiv preprint arXiv:1301.3781, 2013a.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. Distributed representations of\nwords and phrases and their compositionality. InAdvances in neural information processing systems,\n2013b.\nJiaqi Mu and Pramod Viswanath. All-but-the-top: Simple and eﬀective postprocessing for word\nrepresentations. InProceedings of the International Conference on Learning Representations, 2018.\nMatteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embeddings\nusing compositional n-gram features. Proceedings of the North American Chapter of the ACL: Human\nLanguage Technologies, 2018.\nJeﬀrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), 2014.\n13\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations.arXiv preprint arXiv:1802.05365, 2018.\nRaul Puri and Bryan Catanzaro. Zero-shot text classiﬁcation with generative language models.arXiv\nprepring arXiv:1912.10165, 2019.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444, 2017.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understand-\ning by generative pre-training. 2018. URLhttps://s3-us-west-2.amazonaws.com/openai-assets/\nresearchcovers/languageunsupervised/languageunderstandingpaper.pdf.\nAlec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners.OpenAI Blog, 2019.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nPrajit Ramachandran, Peter Liu, and Quoc Le. Unsupervised pretraining for sequence to sequence\nlearning. InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,\n2017.\nTimo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118, 2020.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical methods in natural language processing, 2013.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics–on what language model\npre-training captures. arXiv preprint arXiv:1912.13283, 2019.\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy,\nand linear models.arXiv preprint arXiv:2008.10150, 2020a.\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic\nposterior information to linear models.arXiv preprint arXiv:2003.02234, 2020b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information processing\nsystems, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding.arXiv preprint\narXiv:1804.07461, 2018.\nTongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment\nand uniformity on the hypersphere.arXiv preprint arXiv:2005.10242, 2020.\nTheresa Wilson and Janyce Wiebe. Annotating opinions in the world press. InProceedings of the\nFourth SIGdial Workshop of Discourse and Dialogue, 2003.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers:\nState-of-the-art natural language processing.arXiv preprint arXiv:1910.03771, 2019.\n14\nWei Xu and Alex Rudnicky. Can artiﬁcial neural networks learn language models? InSixth international\nconference on spoken language processing, 2000.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. InAdvances in neural information\nprocessing systems, 2019.\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁca-\ntion. In Advances in Neural Information Processing Systems 28. 2015.\n15\nA Overview\nSection B is a more detailed version of Section 5.1 and Section C is a detailed version of Section 5.2.\nSection D.1 has a discussion about whynatural tasksare a reasonable formalization for the sentence\ncompletion reformulation and also interpretations forτ and B in the deﬁnition of natural tasks.\nSection D.2 discusses desirable properties of word embeddingsΦ like capturing synonym structure\nin words. Section E contains proofs for all results, including proof sketches for the main results in\nSection E.1. Lemma E.4 is the softmax variant of Pinsker’s inequality that we prove and use for our\nmain results.\nSection F contains many more experimental ﬁndings that consolidate many of our theoretical results.\nSection F.1 provides the information about subsets of words used for results in Table 1 and also\nadditional experiments to test the performance of pretrained language model embeddingsf on more\ndownstream tasks and also verifying that conditional mean embeddingsΦpf do well on these tasks. In\nSection F.3, we present additional results for Quad objective trained on a larger corpus and tested on\nSST. Section F.4 provides additional details on howA, band c from Assumption 4.1 are learned and\nalso further veriﬁcation of the assumption on more datasets. Finally, Section F.5 experimentally veriﬁes\nthe O(√ϵ) dependence from Theorem 4.2.\nB Better handling of distributional shift\nWhile the bounds above usedγ(pT) to transfer from the distributionpL to pT, we deﬁne a more reﬁned\nnotion of transferability here. Whileγ(pT) only depends onpL and pT, the more reﬁned notions depend\nalso on the learned language model, thus potentially exploiting some inductive biases. We ﬁrst deﬁne\nthe notion of error made in the predicted probabilities by any predictorp·|s as ∆{p·|s}(s) = p·|s −p∗\n·|s.\nThus for any softmax language modelf we have∆{pf(s)}(s) = pf(s) −p∗\n·|s. For any distributionp∈∆S,\nwe deﬁne the covariance5 of a functiong: S→ RD as Σp(g) = E\ns∼p\n[\ng(s)g(s)⊤]\n. We deﬁne 3 coeﬃcients\nfor the results to follow\nDeﬁnition B.1.For any distributionp∈∆S, we deﬁne the following\nγ(p; {p·|s}) :=\n(ΣpL(∆{p·|s})−1\n2 Σp(∆{p·|s})ΣpL(∆{p·|s})−1\n2\n\n2\n)−1\n(7)\nγΦ(p; {p·|s}) :=\n(ΣpL(Φ∆{p·|s})−1\n2 Σp(Φ∆{p·|s})ΣpL(Φ∆{p·|s})−1\n2\n\n2\n)−1\n(8)\nγ(p; Φpf) := γΦ(p; {pf(s)}) (9)\nWe notice thatΣp(∆{p·|s}) = E\ns∼p\n[\n(p·|s −p∗\n·|s)(p·|s −p∗\n·|s)⊤\n]\n, Σp(Φ∆{p·|s}) = ΦΣp(∆{p·|s})Φ⊤. We are\nnow ready to state the most general results.\nTheorem B.1 (Strengthened Theorem 4.1). Let {p·|s}be a language model that isϵ-optimal, i.e.\nℓxent({p·|s}) −ℓ∗\nxent ≤ϵ for someϵ> 0. For a classiﬁcation taskT that is(τ,B)-natural, we have\nℓT\n(\n{p·|s}\n)\n≤τ +\n√\n2B2ϵ\nγ(pT; {p·|s})\n5This is not exactly the covariance since the mean is not subtracted, all results hold even for the usual covariance.\n16\nFor a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ, we have\nℓT\n(\n{p·|s}\n)\n≤ℓT\n(\n{Φp·|s}\n)\n≤τ +\n√\n2B2ϵ\nγΦ(pT; {p·|s})\nTheorem 5.1 (Strengthened Theorem 4.2). For a ﬁxedΦ, let f be features from anϵ-optimal d-\ndimensional softmax language model, i.e. ℓxent(f,Φ) −ℓ∗\nxent(Φ) ≤ ϵ, where ℓ∗\nxent(Φ) is deﬁned in\nEquation (4). For a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ, we have\nℓT\n(\n{pf(s)}\n)\n≤ℓT(Φpf) ≤τ +\n√\n2B2ϵ\nγ(pT; Φpf)\nDiscussions: It is not hard to show that the coeﬃcients satisfyγΦ(pT; {p·|s}) ≥γ(pT; {p·|s}) ≥γ(pT)\nand γ(pT; Φpf) ≥γ(pT), thus showing that these results are strictly stronger than the ones from the\nprevious section. The transferability coeﬃcient is a measure of how guarantees onpL using a language\nmodel can be transferred to another distribution of contexts and it only depends on the distribution of\ncontexts and not the labels. Unlikeγ(pT), the coeﬃcients in Deﬁnition B.1 depend on the learned models,\neither {p·|s}or {pf(s)}, and can be potentially much smaller due to the inductive bias of the learned\nmodels. For instance, if errors made by the model are random-like, i.e.∆{p·|s}(s) ∼ρ, independently\nof s, thenΣpL(∆{p·|s}) ≈Σp(∆{p·|s}) ≈Eη∼ρ[ηη⊤], makingγ(p; {p·|s}) ≈1. Independence prevents\nlanguage modeling error from accumulating on contexts frompT, bypassing the worst case transfer of\nγ(pT).\nC Quad: A new objective function\nIn Deﬁnition 3.2 we discuss how low dimensional softmax language models learn a linear projection\nof p∗\n·|s, only solving tasks that lie in the row span of word embeddingsΦ. Although Φ deﬁnes tasks\nthat language model features can solve, the standard cross-entropy objective does not lend a simple\nclosed form expression for optimalΦ. This motivates the construction of our Quad objective, that has\ntwo nice properties: (1) the optimal feature mapf∗is a linear function ofp∗\n·|s and thus can solve some\nnatural tasks, and (2) the optimalΦ∗has an intuitively meaningful closed-form solution.\nℓquad,s(θ,Φ) = E\nw∼p∗\n·|s\n[−θ⊤φw] + 1\n2∥Φ⊤θ∥2 = −θ⊤Φp∗\n·|s + 1\n2∥Φ⊤θ∥2 (10)\nℓquad(f,Φ) = E\ns∼pL\n[ℓquad,s(f(s),Φ)] (11)\nThe Quad objective is very similar to the cross-entropy objective from Equation (2), with the log\npartition function replaced by a quadratic function, inspired in part by Assumption 4.1. We can derive\nthe optimal solutionΦ∗that depends on the eigen-decomposition of asubstitutability matrix.\nDeﬁnition 5.1. The substitutability matrix is deﬁned to be Ω∗ := E\ns∼pL\n[\np∗\n·|s p∗\n·|s\n⊤\n]\n∈ RV×V. If\nΩ∗= USU⊤ is the eigendecomposition, thenUd ∈RV×d is matrix of topd eigenvectors ofΩ∗.\nThe matrixΩ∗captures substitutability between pairs of words. Wordswand w′are substitutable if they\nhave identical conditional probabilities for every contexts∈S and thus can replace occurrences of each\nother while still providing meaningful completions. By deﬁnition, these words satisfyΩ∗[w] = Ω∗[w′].\nSuch pairs of words were called “free variants\" in the work on distributional semantics [Harris, 1954],\n17\nand capture the notion of synonyms in the distributional hypothesis. We now derive expressions for the\noptimal solution of the Quad objective described in Equation (11). The proof of all results from this\nsection are in Section E.5.\nTheorem C.1.The optimal solutionf∗,Φ∗= arg minf,Φ ℓquad(f,Φ) satisﬁes\nΦ∗= BU⊤\nd , for full rankB∈Rd×d\nf∗(s) = (Φ∗Φ∗⊤)−1/2Φ∗p∗\n·|s = CU⊤\nd p∗\n·|s, for full rankC∈Rd×d\nIf Φ is ﬁxed, then the optimal solution isf∗(s) = (ΦΦ⊤)−1/2Φp∗\n·|s.\nTheorem 5.2. Let f∗,Φ∗= arg minf,Φ ℓquad(f,Φ). Then Φ∗= BU⊤\nd , for full rankB∈Rd×d. Also,\nfor a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ∗, we haveℓT(f∗) ≤τ.\nThus f∗excels on natural tasks w.r.t.Φ∗, which in turn, is the bestd-dimensional projection ofΩ∗.\nThus wordsw,w′∈W that are synonyms (hence substitutable) will satisfyφ∗\nw = φ∗\nw′, fulﬁlling the\ndesired property for word embeddings discussed in Deﬁnition 3.2. We train using the Quad objective\nand compare its performance to a similarly trained language model, ﬁnding Quad to be reasonably\neﬀective. The goal of testing Quad is not to obtain state-of-the-art results, but to demonstrate that\ntheoretical insights can aid the design of provably eﬀective algorithms.\nD More on natural tasks\nThe discussions in this section may not be formal and precise in places, they are meant to provide more\nintuition for some of the deﬁnitions and results.\nD.1 Sentence completion reformulation ≡natural task\nWe provide informal justiﬁcation for why the sentence completion reformulation can be formalized as\nbeing able to solve using a linear classiﬁer overp∗\n·|s ∈RV. The analysis will also end up providing\nsome intuitions forτ and B in Deﬁnition 3.1 and Theorem 4.1. In particular, we will show that\na task that is amenable to the sentence completion reformulation will be(τ,B)-natural, with τ =\nO(Bayes-Error(T)), i.e. τ is small if the Bayes error for the task error, andB = O(α(Windicative)−1)\nis inversely proportional to the probability mass of the set of indicative words for the task. This is\nformalized in Proposition D.2.\nLinear classiﬁer overp∗\n·|s\nConsider a binary classiﬁcation taskT and that can be solved with a sentence completion reformulation\nafter adding a prompt as in Section 3.1, for e.g. sentiment classiﬁcation can be solved by adding a\nprompt “This movie is” at the end of every movie review and use the completions to solve the task.\nRecall thatpT is the distribution overS×{±1}for the taskT. We abuse notation and usepT to denote\nthe distribution over inputs where a prompt is added to each to input, for e.g. “I loved the movie.” is\ntransformed to “I loved the movie. This movie is”. For anys∼pT, letpT(y= 1|s) and pT(y= −1|s)\ndenote the conditional probabilities of the sentiment of reviews (with an added prompt) being positive\nand negative respectively. By law of total probability we can write this conditional probability as\npT(y= 1|s) =\n∑\nw∈W\nPr(y= 1|(s,w)) Pr(w|s) =\n∑\nw∈W\nPr(y= 1|(s,w)) p∗\n·|s(w) (12)\nFor any taskT we can roughly partition the vocabulary setWinto the following\n18\nIndicative wordsWindicative: w can be anindicative completionfor the task, like “good”, “boring”,\n“trash” etc, after a movie review likes=“I loved the movie. This movie is”. In this case the sentence\ncompletion reformulation can be interpreted as the following: the completionw after a reviews is\nsuﬃcient to determine the sentiment of the review, i.e. we do not need to know the content of the\nreview sto predict the label if we know the completionw. This can be formalized asPr(y= 1|(s,w)) ≈\nP(y= 1|w) for some ﬁxed distributionP for indicative completionsw.\nIrrelevant wordsWirrelevant: w can be anirrelevant completionfor the task, like “a”, “very”, “not”.\nIn this case the completions, on the other hand, do not reveal anything more about the sentiment for\nthe review thans itself, i.e. Pr(y= 1|(s,w)) ≈pT(y= 1|s) for irrelevant completionsw.\nThus from Equation (12) we get\npT(y= 1|s) =\n∑\nw∈Windicative\nPr(y= 1|(s,w)) p∗\n·|s(w) +\n∑\nw∈Wirrelevant\nPr(y= 1|(s,w)) p∗\n·|s(w)\n≈\n∑\nw∈Windicative\nP(y= 1|w) p∗\n·|s(w) +\n∑\nw∈Wirrelevant\npT(y= 1|s) p∗\n·|s(w)\n=\n∑\nw∈Windicative\nv1(w) p∗\n·|s(w) + pT(y= 1|s)\n∑\nw∈Wirrelevant\np∗\n·|s(w)\n= v⊤\n1 p∗\n·|s + pT(y= 1|s)p∗\n·|s(Wirrelevant)\nwhere v1 ∈RV is deﬁned asv1(w) = P(y= 1|w) for w∈Windicative and v1(w) = 0 for w∈Wirrelevant.\nSimilarly we can deﬁnev−1 ∈RV with v−1(w) = P(y = −1|w) for w ∈Windicative, v−1(w) = 0 for\nw∈Wirrelevant. From the earlier calculation, and a similar one fory= −1, we get\npT(y= b|s) ≈ 1\n1 −p∗\n·|s(Wirrelevant)v⊤\nb p∗\n·|s = 1\np∗\n·|s(Windicative)v⊤\nb p∗\n·|s, forb∈{±1}\nIf we assumep∗\n·|s(Windicative) ≈α(Windicative) is roughly the same for alls, i.e. probability mass of\nindicative words following a modiﬁed review is approximately the same, then we get\npT(y= 1|s) −pT(y= −1|s) ≈v⊤\nTp∗\n·|s , wherevT = 1\nα(Windicative)(v1 −v−1) (13)\nThus we can approximately express the diﬀerence in conditional probabilities of the 2 classes as a linear\nfunction ofp∗\n·|s. While it is intuitively clear why knowingpT(y = 1|s) −pT(y = −1|s) is useful for\nsolving the task, we show precisely why in the next part.\nInterpretation forτ and B\nBased on the above discussed, we will show that the taskT from earlier is(τ,B)-natural according to\nthe Deﬁnition 3.1 and will also give us an interpretation forτ and B. First we show that the following\npredictor from Equation (13) is eﬀective for taskT\ngT(s) = pT(y= 1|s) −pT(y= −1|s) ≈v⊤\nTp∗\n·|s (14)\nWe reuse the notation from Equation (3) and deﬁne the task loss for any predictorg: S→ R as\nℓT(g) = E(s,y)∼pT [ℓ(g(s),y)] (15)\nFurthermore letBayes-Error(T) := infg:S→R E(s,y)∼pT[1{g(s) ̸= y}] denote the Bayes error of the task\nT, i.e. the optimal0 −1 error achievable on the task.\n19\nProposition D.1. For any taskT and for the hinge lossℓ, ℓT(gT) ≤4 Bayes-Error(T), where\ngT(s) = pT(y= 1|s) −pT(y= −1|s).\nThus if a task is easily solvable, i.e. has small Bayes error, then it will be solvable by the predictor\ngT(s). Since we argued above that sentence reformulation implies thatgT(s) is a linear function ofp∗\n·|s,\nwe can now show thatT is anatural taskas formalized in Deﬁnition 3.1.\nProposition D.2(Informal). TaskT that can be reformulated as a sentence completion task (described\nabove) is a(τ,B)-natural task w.r.t. the hinge loss, with the follow parameters\nτ ≤4 Bayes-Error(T) and B= α(Windicative)−1\nHere Bayes-Error(T) is the Bayes error of taskT and α(Windicative) is the total mass of the indicative\nwords for the task.\nIf the taskT can be reformulated as sentence completion, thenT is (τ,B)-natural where\n• τ is small if the task is unambiguous, i.e. it has small Bayes error\n• B is small if the probability mass of the set of indicative wordsWindicative is large, i.e. the task\ndepends on a large set of frequent words\nThus the upper bound in Theorem 4.1 is smaller if the task can be reformulated as sentence completion\ntask with a large and frequent set of completions, and we can ever hope to solve it well (Bayes error is\nsmall). The proofs for the above propositions are in Section D.1.\nD.2 Nice properties of word embeddingsΦ\nWe argue here that if the word embeddingsΦ satisfy certainnice properties, then(τ,B)-natural tasks\nof interestwill be(τ′,B′)-natural w.r.t. Φ, where we will provide informal quantiﬁcations for thenice\npropertiesand tasks of interestthat lead to a small value forτ′and B′. Thenice propertywill be related\nto Φ capturing the semantic meaning (synonym structure) of words andtasks of interestwill be those\nthat try to distinguish word completion (in the sentence completion reformulation) with very diﬀerent\nmeanings, i.e. tries to distinguish more coarse-grained semantic notions rather than very ﬁne-grained\nones. Note that the results here are informal and qualitative, rather than quantitative.\nConsider a taskTthat is(τ,B)-natural task and letv∗∈RV be the classiﬁer such thatℓT({p∗\n·|s},v∗) ≤τ\nand ∥v∗∥∞≤B. We want to ﬁnd properties ofΦ and v∗that will makeT to be(τ′,B′)-natural w.r.t.\nΦ such thatτ′and B′are not too large.6\nWe will show thatT is (τ′,B′)-natural w.r.t. Φ by ﬁnding a classiﬁerv such thatv = Φ⊤λ ∈RV,\n∥v∥∞ ≤B′ and ℓT({p∗\n·|s},v) ≤τ′. First we deﬁnePΦ := Φ†Φ ∈RV×V to be the projection matrix\nfor the row-span ofΦ and P⊥\nΦ := IV −PΦ to be orthogonal projection matrix. We will show that the\nclassiﬁer v= PΦv∗suﬃces for our case, under some intuitive conditions onv∗and Φ.\nTo computeB′, we ﬁrst look at theℓ∞norm ofv= PΦv∗\nB′= ∥v∥∞= ∥PΦv∗∥∞= ∥v∗−P⊥\nΦ v∗∥∞≤∥v∗∥∞+ ∥P⊥\nΦ v∗∥∞≤B+ ∥P⊥\nΦ v∗∥2\nTo ﬁnd the upper boundτ′, we upper bound the classiﬁcation loss ofv= PΦv∗. We ﬁrst deﬁne the\nsubstitutability matrixΩ∗\np = E\ns∼p\n[\np∗\n·|sp∗\n·|s\n⊤\n]\n, similar to the one in Deﬁnition 5.1. Then\nℓT({p∗\n·|s},v) = E\n(s,y)∼pT\n[\nℓ(v⊤p∗\n·|s,y)\n]\n= E\n(s,y)∼pT\n[\nℓ((PΦv∗)⊤p∗\n·|s,y)\n]\n6Note that the converse is trivially true, i.e. a(τ,B)-natural task w.r.t.Φ is also(τ,B)-natural.\n20\n≤(a) E\n(s,y)∼pT\n[\nℓ(v∗⊤p∗\n·|s,y)\n]\n+ E\ns∼pT\n[|(v∗−PΦv∗)⊤p∗\n·|s|]\n= ℓT({p∗\n·|s},v∗) + E\ns∼pT\n[\n|v∗⊤P⊥\nΦ p∗\n·|s|\n]\n≤(b) τ +\n√\nE\ns∼pT\n[\n(v∗⊤P⊥\nΦ p∗\n·|s)2\n]\n= τ +\n√\nE\ns∼pT\n[\nv∗⊤P⊥\nΦ p∗\n·|sp∗\n·|s\n⊤P⊥\nΦ v∗\n]\n=(c) τ +\n√\nv∗⊤P⊥\nΦ Ω∗pTP⊥\nΦ v∗≤(d) τ + ∥P⊥\nΦ v∗∥2\n√P⊥\nΦ Ω∗pTP⊥\nΦ\n\n2\nwhere (a) follows from 1-Lipschitz property ofℓ, (b) from Jensen’s inequality and thatℓT({p∗\n·|s},v∗) ≤τ,\n(c) from the deﬁnition of substitutability matrixΩ∗\npT and(d) by deﬁnition of spectral norm of a symmetric\nPSD matrix.\nThus we have shown thatT is (τ′,B′)-natural w.r.t. Φ, where\nτ′= τ + ∥P⊥\nΦ v∗∥2\n√P⊥\nΦ Ω∗pTP⊥\nΦ\n\n2, B′= B+ ∥P⊥\nΦ v∗∥2 (16)\nWe will now show that ifΦ captures the notion of synonyms, then\nP⊥\nΦ Ω∗\npTP⊥\nΦ\n\n2 will be small leading to\nτ′being small. Furthermore we also shed some light on what it means for∥P⊥\nΦ v∗∥2 to be small, which\nwill in turn makeB′ small andτ′ smaller. We do so with the following arguments, 1)Ω∗\npT captures\nsemantic meaning of words and thus its top eigen-directions will capture more dominant semantic\nconcepts, 2) ifΦ captures the “top-d” directions of meaning, i.e. the top-d eigen-directions ofΩ∗\npT, thenP⊥\nΦ Ω∗\npTP⊥\nΦ\n\n2 = O(1/d), 3) if additionallyv∗cares about the “top-d” directions of meaning, i.e. top-d\neigen-directions ofΩ∗\npT then ∥P⊥\nΦ v∗∥2 will be small. We expand on these points below\n1. Substitutability matrix (Ω∗\npT) captures semantic meaning: We use a similar argument to the\none in Section 5.2 right after Deﬁnition 5.1 that is based on distributional semantics [Harris, 1954].\nHarris [1954] posits that meaning for elements (words) can be derived from the environments (contexts)\nin which they occur. Thus Harris [1954] argues that words that occur in almost identical set of\ncontexts have the same meaning, i.e. are synonyms. On the other hand, if two words share some\ncontexts but not all, then they have diﬀerent meanings and the amount of diﬀerence in meaning roughly\ncorresponds to amount of diﬀerence in contexts. In our setting, the similarity of wordsw and w′can\nthen be determined by the probabilities assigned to them by diﬀerent contextss. In particular, if\np∗\n·|s(w) = p∗\n·|s(w′) for all or mosts ∈supp(pT), then w and w′ have essentially the same meaning\nw.r.t. the distribution of contextspT and the closer[p∗\n·|s(w)]s∈supp(pT) and [p∗\n·|s(w′)]s∈supp(pT) are, the\ncloser the meaning ofw and w′ are. For the substitutability matrixΩ∗\npT = E\ns∼pT\n[p∗\n·|sp∗\n·|s\n⊤] ∈RV×V,\nit is not hard to show thatΩ∗\npT(w) = Ω ∗\npT(w′) is equivalent top∗\n·|s(w) = p∗\n·|s(w′) ∀s ∼pT, where\nΩ∗\npT(w) is the row ofΩ∗\npT corresponding to wordw. To show this, we can deﬁneβw ∈R|supp(pT)|\nto be an embedding of w that looks like βw = [ p∗\n·|s(w)\n√\npT(s)]s∈supp(pT). It is easy to see that\nβ⊤\nw1βw2 = E\ns∼pT\n[\np∗\n·|s(w1)p∗\n·|s(w2)\n]\n= Ω ∗\npT(w1,w2). Thus βw = βw′ =⇒ Ω∗\npT(w) = Ω ∗\npT(w′) is\nstraightforward to see. For the converse,\nΩ∗\npT(w) = Ω∗\npT(w′) =⇒ Ω∗\npT(w,w) = Ω∗\npT(w′,w) = Ω∗\npT(w,w′) = Ω∗\npT(w′,w′) (17)\n=⇒ β⊤\nwβw = β⊤\nwβw′ = β⊤\nw′βw′ =⇒ βw = βw′ (18)\nThus Ω∗\npT indeed does capture the synonyms structure between words, and the top eigen-directions of\nit capture the most signiﬁcant “semantic meaning” directions.\n21\n2. Φ has nice properties: if Φ roughly respects this synonym structure by aligning with the top-d\neigen-directions ofΩ∗\npT, we have\nP⊥\nΦ Ω∗\npTP⊥\nΦ\n\n2\n≤λd+1(Ω∗\npT) ≤ 1\nd+ 1\nd+1∑\ni=1\nλi(Ω∗\npT) ≤ 1\nd+ 1tr(Ω∗\npT) (19)\n≤ 1\nd+ 1 E\ns∼pT\ntr(p∗\n·|sp∗\n·|s\n⊤) ≤ 1\nd+ 1 (20)\nFrom Equation (16), we then haveτ′≤τ + ∥P⊥\nΦ v∗∥2√\nd\n3. Tasks of interest: It is more likely for a classiﬁerv∗to separate words with big diﬀerences in\nmeaning rather than small diﬀerences. For e.g., it is more likely for a task to separate word completions\n“good” and “bad” rather than “good” and “nice”. Since top eigen-directions ofΩ∗\npT capture more\ndominant semantic meanings, this could correspond tov∗ aligning with the top eigen-directions of\nΩ∗\npT. In combination with the above property aboutΦ, this could suggest that∥P⊥\nΦ v∗∥2 is small, thus\nleading toτ′and B′being small.\nNote that they above arguments are informal and qualitative, and we leave exploring desirable properties\nof Φ more formally to future work.\nD.3 Proofs for Section D.1\nProposition D.1.Let pb(s) = pT(y = b|s) for b ∈ {±1}, pmin(s) = minb∈{±1}pb(s), pmax(s) =\nmaxb∈{±1}pb(s) and g∗(s) = arg maxb∈{±1}pb(s) denote the Bayes optimal predictor. We ﬁrst no-\ntice that there is a simple well-known closed form expression for the Bayes risk\nBayes-Error(T) = E\n(s,y)∼pT\n[1 {g∗(s) ̸= y}]\n= E\n(s,y)∼pT\n[\n1\n{\narg max\nb∈{±1}\npb(s) ̸= y\n}]\n= E\ns∼pT\n[pmin(s)]\nWe now analyze the hinge loss of the predictorgpT deﬁned in Equation (14). Note that sincegpT(s) ≤1,\nthe hinge lossℓ(gpT(s),y) = (1 −ygpT(s))+ = 1 −ygpT(s) for everys,y. Thus the total loss is\ngpT(s) = E\n(s,y)∼pT\n[(1 −ygpT(s))+] = E\n(s,y)∼pT\n[(1 −ygpT(s))]\n=(a) E\ns∼pT\n[p1(s) (1−gpT(s)) + p−1(s) (1 +gpT(s))] = E\ns∼pT\n[1 −(p1(s) −p−1(s))gpT(s)]\n=(b) E\ns∼pT\n[\n1 −(p1(s) −p−1(s))2]\n= E\ns∼pT\n[\n(p1(s) + p−1(s))2 −(p1(s) −p−1(s))2]\n= E\ns∼pT\n[4p1(s)p−1(s)] = 4 E\ns∼pT\n[pmin(s)pmax(s)]\n≤(c) 4 E\ns∼pT\n[pmin(s)] = 4 Bayes-Error(T)\nwhere (a) follows by splitting the expectation overy|s, (b) follows from the deﬁnition ofgpT(s) in\nEquation (14) and(c) follows frompmax(s) ≤1. This completes the proof.\n22\nProposition D.2.Let B = α(Windicative)−1. We ﬁrst note the following using the deﬁnition ofvfrom\nEquation (13).\n∥vT∥∞= α(Windicative)−1 max\nw∈W\n|v1(w) −v−1(w)|= Bmax\nw∈W\n|P(y= 1|w) −P(y= −1|w)|≤ B (21)\nTo ﬁnd the value ofτ that makes the task(τ,B)-natural (Deﬁnition 3.1), we observe the following\nmin\nv∈RV,∥v∥≤B\nℓT({p∗\n·|s},v) =(a) ℓT({p∗\n·|s},vT) = E\n(s,y)∼pT\n[ℓ(v⊤\nTp∗\n·|s,y)]\n=(b) E\n(s,y)∼pT\n[ℓ(gT(s),y)] = ℓT(gT)\n≤(c) 4 Bayes-Error(T)\nwhere (a) follows from the calculation in Equation (21),(b) follows from Equation (13) and(c) follows\nfrom Proposition D.1.\nE Proofs\nE.1 Proof sketch\nWe ﬁrst present a sketch of the arguments that help us show our main results, theorems 4.1 and 4.2. The\nsubsections after the next one contain the full proofs for strengthened versions of these results.\nE.1.1 Proof sketch for arbitrary language models: Theorem 4.1\nHere we want to show guarantees for features{p·|s}on a(τ,B)-natural taskT. From the deﬁnition of\nnatural tasks, we know\n∃v∗∈RV,∥v∗∥∞≤B s.t. ℓT({p∗\n·|s},v∗) ≤τ (22)\nWe wish to upper bound the classiﬁcation errorℓT({p·|s}) and do so using the following sequence of\ninequalities.\nℓT({p·|s}) −τ = inf\nv∈RV\nℓT({p·|s},v) −τ ≤ℓT({p·|s},v∗) −ℓT({p∗\n·|s},v∗)\n=\nℓT({p·|s},v∗) −ℓT({p∗\n·|s},v∗)\n√\nE\ns∼pT\n[(v∗⊤(p·|s −p∗\n·|s))2]\n·\n√\nE\ns∼pT\n[(v∗⊤(p·|s −p∗\n·|s))2]\nE\ns∼pL\n[(v∗⊤(p·|s −p∗\n·|s))2] ·\n√\nE\ns∼pL\n[(v∗⊤(p·|s −p∗\n·|s))2]\n=\nℓT({p·|s},v∗) −ℓT({p∗\n·|s},v∗)\n√\nv∗⊤ΣpT(∆{p·|s})v∗\n  \nα1(v∗)\nClassiﬁcation loss→error covariance onpT\nUse Lipschitzness ofℓ and Jensen’s inequality\n·\n√v∗⊤ΣpT(∆{p·|s})v∗\nv∗⊤ΣpL(∆{p·|s})v∗\n  \nα2(v∗)\nError covariance frompT →pL\nUse transferability coeﬃcient\n·\n√\nE\ns∼pL\n[(v∗⊤(p·|s −p∗\n·|s))2]\n  \nα3(v∗)\nError covariance→cross-entropy loss\nUse (modiﬁed) Pinsker’s inequality\n(23)\nwhere Σp(g) := E\ns∼p\n[g(s)g(s)⊤] is the uncentered covariance ofg w.r.t. distribution p∈∆S, as deﬁned\nin Section 5.1. We upper boundℓT({p·|s}) −τ by upper bounding each ofα1(v∗),α2(v∗),α3(v∗) as\nfollows\n23\n• Classiﬁcation loss → prediction error covariance: α1(v∗) is upper bounded by using\nLipschitzness of the lossℓ used in the deﬁnition ofℓT, e.g. hinge loss or logistic loss, and then\nfollowed by an application of Jensen’s inequality\nLemma E.8 =⇒ α1(v) ≤1 for allv∈RV\n• Error covariance frompT →pL: α2(v∗) handles the mismatch in distributionspT and pL\nover which the classiﬁcation loss and cross-entropy losses are measured respectively. It is upper\nbounded by the transferability coeﬃcient\nLemma E.10 and Lemma E.9=⇒ α2(v) ≤\n√\nγ(pT)−1 for allv∈RV\n• Error covariance→cross-entropy loss (arbitrary language models):This is arguably\nthe most important step that connects the error in prediction to the cross-entropy loss. For the\narbitrary language model case, this is proved using Pinsker’s inequality and taking expectation\nover the distributionpL.\nLemma E.3 =⇒ α3(v) ≤\n√\n2∥v∥2∞(ℓxent({p·|s}) −ℓxent(p∗\n·|s)) for allv∈RV\nE.1.2 Proof sketch for softmax language models: Theorem 4.2\nHere we want to show guarantees for featuresΦpf = {Φpf(s)}on a(τ,B)-natural taskT w.r.t Φ. From\nthe deﬁnition of natural tasks w.r.t.Φ, we know\n∃v∗= Φ⊤λ∈RV,∥v∗∥∞≤B s.t. ℓT({p∗\n·|s},v∗) ≤τ (24)\nNote that the diﬀerence here is thatv∗is in the span ofΦ rather than an arbitrary vector inRV. We\nwish to upper bound the classiﬁcation errorℓT({Φpf(s)}) and do so using the following sequence of\ninequalities.\nℓT({Φpf(s)}) −τ = inf\nλ∈Rd\nℓT({Φpf(s)},λ) −τ\n= inf\nv=Φ⊤λ∈RV\nℓT({pf(s)},v) −τ\n≤ℓT({pf(s)},v∗) −ℓT({p∗\n·|s},v∗)\n≤α1(v∗) ·α2(v∗) ·α3(v∗) (25)\nwhere the ﬁrst inequality follows becausev∗ is in the span ofΦ and second inequality follows from\nEquation (23). The bounds forα1(v∗) and α2(v∗) are the same as arbitrary language models. The\nmain diﬀerence is the bound onα3(v∗) which will be a stronger bound for softmax models.\n• Error covariance→cross-entropy loss (softmax language models):For softmax language\nmodels, we need to prove a modiﬁed version of Pinsker’s inequality speciﬁcally for softmax models.\nThis version will show a bound that only works whenv∗is in the span ofΦ and if the evaluated\nmodel pf(s) computes softmax usingΦ as well.\nLemma E.4 =⇒ α3(v) ≤\n√\n2∥v∥2∞(ℓxent({pf(s)}) −inf\nf∗\n({pf∗(s)})) ∀v= Φ⊤λ∈RV\nThus we suﬀer the suboptimality of the language model{pf(s)}w.r.t. the best softmax model{pf∗(s)}\nrather than the absolute best language model{p∗\n·|s}. This is done using the softmax variant of Pinsker’s\ninequality in Lemma E.4. We now present the detailed proofs for all results.\n24\nE.2 Proofs for arbitrary language models\nTheorem B.1 (Strengthened Theorem 4.1). Let {p·|s}be a language model that isϵ-optimal, i.e.\nℓxent({p·|s}) −ℓ∗\nxent ≤ϵ for someϵ> 0. For a classiﬁcation taskT that is(τ,B)-natural, we have\nℓT\n(\n{p·|s}\n)\n≤τ +\n√\n2B2ϵ\nγ(pT; {p·|s})\nFor a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ, we have\nℓT\n(\n{p·|s}\n)\n≤ℓT\n(\n{Φp·|s}\n)\n≤τ +\n√\n2B2ϵ\nγΦ(pT; {p·|s})\nProof. The proof has two main steps that we summarize by the following two lemmas. The ﬁrst one\nupper bounds the downstream performance on natural tasks with the covariance of errors.\nLemma E.2.For a language model{p·|s}, ifT is (τ,B)-natural,\nℓT({p·|s}) ≤τ + sup\nv∈RV,∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγ(pT; {p·|s})\nIf T is (τ,B)-natural w.r.t.Φ ∈Rd×V,\nℓT({Φp·|s}) ≤τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγΦ(pT; {p·|s})\nwhere γ(·) and γΦ(·) are from Deﬁnition B.1.\nThe second lemma upper bounds the covariance of error with the suboptimality of the language model.\nLemma E.6.For a language model{p·|s}and classiﬁerv∈RV,\nv⊤ΣpL(∆{p·|s})v≤2∥v∥2\n∞\n(\nℓxent({p·|s}) −ℓ∗\nxent\n)\nwhere ΣpL(∆{p·|s}) = E\ns∼pL\n[\n(p·|s −p∗\n·|s)(p·|s −p∗\n·|s)⊤\n]\nas deﬁned in Section B.\nWe prove both the above lemmas in Section E.6. We ﬁrst use these to prove the main result.\nCombining the two lemmas, we get the following inequality\nℓT({p·|s}) ≤(a) τ + sup\nv∈RV,∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγ(pT; {p·|s})\n≤(b) τ + sup\nv∈RV,∥v∥∞≤B\n√\n2∥v∥2∞\n(\nℓxent({p·|s}) −ℓ∗\nxent\n)\nγ(pT; {p·|s})\n≤(c) τ +\n√\n2B2ϵ\nγ(pT; {p·|s})\n25\nwhere (a) uses ﬁrst part of Lemma E.2,(b) uses Lemma E.6 and(c) uses theϵ-optimality of{p·|s}.\nThis proves the ﬁrst part of the result. The second part can also be proved similarly.\nℓT({Φp·|s}) ≤(a) τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγΦ(pT; {p·|s})\n≤(b) τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\n2∥v∥2∞\n(\nℓxent({p·|s}) −ℓ∗\nxent\n)\nγΦ(pT; {p·|s})\n≤τ + sup\nv∈RV,∥v∥∞≤B\n√\n2∥v∥2∞\n(\nℓxent({p·|s}) −ℓ∗\nxent\n)\nγΦ(pT; {p·|s}) ≤(c) τ +\n√\n2B2ϵ\nγΦ(pT; {p·|s})\nwhere (a) uses second part of Lemma E.2,(b) uses Lemma E.6 and(c) uses theϵ-optimality of{p·|s}.\nThe proof of the lemmas can be found in Section E.6.\nTheorem 4.1.Let {p·|s}be a language model that isϵ-optimal, i.e. ℓxent({p·|s}) −ℓ∗\nxent ≤ϵ, for some\nϵ> 0. For a classiﬁcation taskT that is(τ,B)-natural, we have\nℓT\n(\n{p·|s}\n)\n≤τ +\n√\n2B2ϵ\nγ(pT)\nProof. This follows from the ﬁrst part of Theorem B.1 if we can also show thatγ(pT; {p·|s})−1 ≤γ(pT)−1.\nFor that we use the following lemma that we prove in Section E.6.\nLemma E.9.For anyg: S→ RD and pT ∈∆S, we have∥ΣpL(g)−1\n2 ΣpT(g)ΣpL(g)−1\n2 ∥2 ≤γ(pT)−1\nInstantiating this forg = ∆{p·|s} and using Equation (7), we getγ(pT; {p·|s})−1 ≤γ(pT)−1, which\ncompletes the proof.\nE.3 Proofs for softmax language models\nTheorem 5.1 (Strengthened Theorem 4.2). For a ﬁxedΦ, let f be features from anϵ-optimal d-\ndimensional softmax language model, i.e. ℓxent(f,Φ) −ℓ∗\nxent(Φ) ≤ ϵ, where ℓ∗\nxent(Φ) is deﬁned in\nEquation (4). For a classiﬁcation taskT that is(τ,B)-natural w.r.t.Φ, we have\nℓT\n(\n{pf(s)}\n)\n≤ℓT(Φpf) ≤τ +\n√\n2B2ϵ\nγ(pT; Φpf)\nProof. Instantiating Lemma E.2 forp·|s = pf(s), we get\nℓT({Φpf(s)}) ≤τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpL(∆{pf(s)})v\nγΦ(pT; {pf(s)})\n=(a) τ +\n√\nsup\n∥Φ⊤λ∥∞≤B\nλ⊤ΦΣpL(∆{pf(s)})Φ⊤λ\nγ(pT; Φpf)\n26\n= τ +\n√\nsup\n∥Φ⊤λ∥∞≤B\nλ⊤ΣpL(Φ∆{pf(s)})λ\nγ(pT; Φpf)\nwhere (a) follows from Equation (9) that saysγ(pT; Φpf) = γΦ(pT; {pf(s)}). We now prove a similar\nresult for the second term in the following lemma that we prove in Section E.6.\nLemma E.7.For a ﬁxedΦ and a softmax language model with featuresf and λ∈Rd,\nλ⊤ΣpL(Φ∆{pf(s)})λ≤2∥Φ⊤λ∥2\n∞(ℓxent(f,Φ) −ℓ∗\nxent(Φ))\nwhere ΣpL(Φ∆{pf(s)}) = E\ns∼pL\n[\n(Φpf(s) −Φp∗\n·|s)(Φpf(s) −Φp∗\n·|s)⊤\n]\nas deﬁned in Section B.\nUsing Lemma E.7 directly gives usℓT(Φpf) = ℓT({Φpf(s)}) ≤τ +\n√\nB2(ℓxent(f,Φ)−ℓ∗\nxent(Φ))\nγΦ(pT;Φpf) , and theϵ-\noptimality almost completes the proof. The only thing remaining to show is thatℓT({pf(s)}) ≤ℓT(Φpf)\nwhich follows from the following sequence.\nℓT({pf(s)}) = inf\nv∈RV,b∈R\nℓT({pf(s)},v) ≤ inf\nΦ⊤λ∈RV,b∈R\nℓT({pf(s)},(Φ⊤λ,b))\n= inf\nλ∈Rd,b∈R\nℓT({Φpf(s)},(λ,b)) = ℓT(Φpf)\nTheorem 4.2.For a ﬁxedΦ, letf be features from anϵ-optimal d-dimensional softmax language model,\ni.e. ℓxent(f,Φ) −ℓ∗\nxent(Φ) ≤ϵ, whereℓ∗\nxent(Φ) is deﬁned in Equation (4). For a classiﬁcation taskT that\nis (τ,B)-natural w.r.t.Φ, we have\nℓT\n(\n{pf(s)}\n)\n≤ℓT(Φpf) ≤τ +\n√\n2B2ϵ\nγ(pT)\nProof. This result follows directly from Theorem 5.1, if we can also show thatγ(pT; Φpf)−1 ≤γ(pT)−1\njust like in the proof of Theorem 4.1. For that we again use Lemma E.9 withg = Φ∆{pf(s)} and\nEquation (9) and this completes the proof.\nE.4 Proofs for Section 4.3\nWe ﬁrst show why Assumption 4.1 is approximately true when word embeddings are gaussian like.\nLemma E.1. Suppose word embeddingsφw are independent samples from the distributionN(µ,Σ).\nThen for anyθ∈Rd such thatλ2 = θ⊤Σθ= O(1) we have that|log(Zθ) −1\n2 θ⊤Σθ−θ⊤µ−log(V)|≤ ϵ\nwith probability1 −δ for ϵ= ˜O\n(\neλ2\n√\nV\n)\nand δ= 1 −exp(−Ω(log2(V))).\nProof. We ﬁrst note thatlog(Zθ) = log\n(∑\nweθ⊤φw\n)\n= θ⊤µ+ log\n(∑\nweθ⊤(φw−µ)\n)\n, thus we can simply\ndeal with the case whereφw are sampled fromN(0,Σ). Furthermore the only random variable of\ninterest isXw = θ⊤φw which is a gaussian variableN(0,θ⊤Σθ) = N(0,λ2). Thus the problem reduces\nto showing that forV samples ofXw ∼N(0,λ2), log(Z) is concentrated aroundλ2 + log(V) where\nZ = ∑\nwexp(Xw). This can be proved similarly to the proof of Lemma 2.1 in Arora et al. [2016]. It is\n27\neasy to see that E\nXw∼N(0,λ2)\n[exp(Xw)] = eλ2\n. However the variableexp(Xw) is neither sub-gaussian nor\nsub-exponential and thus standard inequalities cannot be used directly. We use the same technique\nas Arora et al. [2016] to ﬁrst observe thatE[Z] = Ve\n1\n2 λ2\nand Var[Z] ≤E[exp(2Xw)] = Ve2λ2\n. After\nconditioning on the event thatXw ≤1\n2 λlog(V) and applying Berstein’s inequality just like in Arora\net al. [2016] completes the proof.\nWe next prove Lemma 4.3 that establishes a linear relationship betweenΦpf and f (under Assump-\ntion 4.1) and also the guarantees forf on natural tasks.\nLemma 4.3.Under Assumption 4.1, any feature mapf : S→ Rd satisﬁes Φpf(s) = Af(s) + b, for\nall s∈S.\nProof. Assumption 4.1 gives us thatlog(Zθ) = 1\n2 θ⊤Aθ+ θ⊤b+ c. We prove this lemma by matching\nthe gradients oflog(Zθ) and the quadratic function on the R.H.S.\n∇θlog(Zθ) = ∇θZθ\nZθ\n=\n∑\nw∈Weφ⊤\nwθφw\nZθ\n=\n∑\nw∈W\npθ(w)φw = Φpθ\nWhereas the gradient of the quadratic part is∇θ[1\n2 θ⊤Aθ+ θ⊤b+ c] = Aθ+ b. Matching the two for\nθ= f(s) gives usΦpf(s) = Φpf(s) = Af(s) + b.\nCorollary 4.1.Using Lemma 4.3, for anyϵ-optimal f, as deﬁned in Theorem 4.2, for classiﬁcation\ntasks that are(τ,B)-natural w.r.t.Φ we haveℓT(f) ≤τ + O(√ϵ).\nProof. The main idea is that Lemma 4.3 gives us thatΦpf(s) = Af(s) +band thus any linear function\nof Φpf will also be a linear function off(s). From Theorem 5.1 (or Theorem 4.2), we also know that\nΦpf will do well onT, i.e. ℓT(Φpf) ≤τ + O(B√ϵ). We formalize7 the intuition as\nℓT(Φpf) = inf\nλ∈Rd,b\nℓT(Φpf,(λ,b)) = inf\nλ∈Rd,b\nℓT(Af + b,(λ,b)) = inf\nλ∈Rd,b\nℓT(f,(A⊤λ,b + λ⊤b))\n≥ inf\nv∈Rd,b′\nℓT(f,(v,b′)) = ℓT(f)\nThis shows thatℓT(f) ≤ℓT(Φpf) ≤τ + O(B√ϵ) and completes the proof.\nE.5 Proofs for Section C\nTheorem C.1.The optimal solutionf∗,Φ∗= arg minf,Φ ℓquad(f,Φ) satisﬁes\nΦ∗= BU⊤\nd , for full rankB∈Rd×d\nf∗(s) = (Φ∗Φ∗⊤)−1/2Φ∗p∗\n·|s = CU⊤\nd p∗\n·|s, for full rankC∈Rd×d\nIf Φ is ﬁxed, then the optimal solution isf∗(s) = (ΦΦ⊤)−1/2Φp∗\n·|s.\nProof. FromEquations(10)and(11)weknowthat, ℓquad,s(θ,Φ) = −θ⊤Φp∗\n·|s+1\n2 ∥Φ⊤θ∥2 andℓquad(f,Φ) =\nE\ns∼pL\n[ℓquad,s(f(s),Φ)]. For a ﬁxedΦ, we deﬁnef∗\nΦ(s) = arg minθ∈Rd ℓquad,s(θ,Φ).\n7Note that here we assume that we learn both a linear classiﬁer and an intercept for a downstream classiﬁcation task.\nAll results in the paper essentially remain the same with an intercept in the deﬁnition of classiﬁcation loss.\n28\nWe use the ﬁrst-order optimality condition to getf∗\nΦ(s), by using the fact that∇θℓquad,s(θ,Φ) =\n−Φp∗\n·|s + ΦΦ⊤θ. Setting the gradient to zero, we getf∗\nΦ(s) = (ΦΦ⊤)−1Φp∗\n·|s\n8. To get the optimalΦ∗\nfor this objective, we plug in this expression forf∗\nΦ in ℓquad and ﬁndΦ∗= arg minΦ ℓquad(f∗\nΦ,Φ).\nℓquad(f∗\nΦ,Φ) = E\ns∼p∗\n[ℓquad,s(f∗\nΦ(s),Φ)] = E\ns∼p∗\n[\n−f∗\nΦ(s)⊤Φp∗\n·|s + 1\n2∥Φ⊤f∗\nΦ(s)∥2\n]\n= E\ns∼p∗\n[\n−((ΦΦ⊤)−1Φp∗\n·|s)⊤Φp∗\n·|s + 1\n2∥Φ⊤(ΦΦ⊤)−1Φp∗\n·|s∥2\n]\n= E\ns∼p∗\n[\n−p∗\n·|s\n⊤Φ⊤(ΦΦ⊤)−1Φp∗\n·|s + 1\n2p∗\n·|s\n⊤Φ⊤(ΦΦ⊤)−1ΦΦ⊤(ΦΦ⊤)−1Φp∗\n·|s\n]\n= E\ns∼p∗\n[\n−1\n2p∗\n·|s\n⊤Φ⊤(ΦΦ⊤)−1Φp∗\n·|s\n]\n= −1\n2 E\ns∼p∗\n[\ntr\n(\np∗\n·|s\n⊤Φ⊤(ΦΦ⊤)−1Φp∗\n·|s\n)]\n= −1\n2tr\n(\nΦ⊤(ΦΦ⊤)−1Φ E\ns∼p∗\n[\np∗\n·|sp∗\n·|s\n⊤\n])\n= −1\n2\n⟨\nΦ⊤(ΦΦ⊤)−1Φ, E\ns∼p∗\n[\np∗\n·|sp∗\n·|s\n⊤\n]⟩\n= −1\n2\n⣨\nΦ⊤(ΦΦ⊤)−1Φ,Ω∗\n⟩\nwhere Ω∗is the substitutability matrix deﬁned in Deﬁnition 5.1. LetΦ = NTV ⊤be the SVD. Then the\nabove objective reduces toℓquad(f∗\nΦ,Φ) = −1\n2\n⟨\nVV ⊤,Ω∗⟩\nAnd hence learning the optimalΦ∗reduces\nto learning an optimalV∗such that\nV∗= arg min\nV ∈RV×d,V ⊤V =Id\n−⟨VV ⊤,Ω∗⟩\nWe will now show that the best such matrix is the matrix of topd eigenvectors ofΩ∗, i.e. V∗= Ud\n(cf. Deﬁnition 5.1). Here we will assume that the eigenvalues ofΩ∗ are all distinct for simplicity of\npresentation. First we note that⟨VV ⊤,Ω∗⟩= ∥VV ⊤Ω∗1\n2 ∥2\nF, whereΩ∗1\n2 = US\n1\n2 U⊤, withU, Ud and\nS deﬁne in Deﬁnition 5.1. This can be shown by the following sequence of steps\n⟨VV ⊤,Ω∗⟩= tr(VV ⊤Ω∗) = tr(VV ⊤VV ⊤Ω∗) = tr(VV ⊤Ω∗VV ⊤)\n= tr(VV ⊤USU⊤VV ⊤) = tr(VV ⊤US\n1\n2 U⊤US\n1\n2 U⊤VV ⊤)\n= tr(VV ⊤Ω∗1\n2 Ω∗1\n2 VV ⊤) = ⟨VV ⊤Ω∗1\n2 ,VV ⊤Ω∗1\n2 ⟩\n= ∥VV ⊤Ω∗1\n2 ∥2\nF\nFurthermore, we notice that∥VV ⊤Ω∗1\n2 ∥2\nF = ∥Ω∗1\n2 ∥2\nF −∥Ω∗1\n2 −VV ⊤Ω∗1\n2 ∥2\nF as shown below\n∥Ω∗1\n2 −VV ⊤Ω∗1\n2 ∥2\nF = ∥Ω∗1\n2 ∥2\nF + ∥VV ⊤Ω∗1\n2 ∥2\nF −2tr(Ω∗1\n2 VV ⊤Ω∗1\n2 )\n= ∥Ω∗1\n2 ∥2\nF + ∥VV ⊤Ω∗1\n2 ∥2\nF −2tr(Ω∗1\n2 VV ⊤VV ⊤Ω∗1\n2 )\n= ∥Ω∗1\n2 ∥2\nF + ∥VV ⊤Ω∗1\n2 ∥2\nF −2∥VV ⊤Ω∗1\n2 ∥2\nF\n= ∥Ω∗1\n2 ∥2\nF −∥VV ⊤Ω∗1\n2 ∥2\nF\nThus we get arg min\nV ∈RV×d,V ⊤V =Id\n−⟨VV ⊤,Ω∗⟩= arg min\nV ∈RV×d,V ⊤V =Id\n∥Ω∗1\n2 −VV ⊤Ω∗1\n2 ∥2\nF.\n8It will be clear later that the optimal solution will have as high a rank as possibleΦ. All inverses can be replaced by\npseudo-inverses for low-rank matrices.\n29\nNote thatVV ⊤Ω∗1\n2 has columns that are columns ofΩ∗1\n2 projected on the space spanned by columns\nV. It is folklore that the best such subspaceV∗ is the subspace spanned by the topd eigenvectors\nof Ω∗1\n2 , which is the same as topd eigenvectors ofΩ∗, thus giving usV∗V∗⊤= UdU⊤\nd . Thus we get\nV∗= UdM for M = U⊤\nd V∗.\nThis tells us that the optimal solutionΦ∗will have SVD of the formΦ∗= N∗T∗V∗⊤, thus giving us\nΦ∗= BU⊤\nd for matrixB = N∗T∗M⊤∈Rd×d. This directly givesf∗= f∗\nΦ∗ = (Φ∗Φ∗⊤)−1Φ∗p∗\n·|s =\nN∗T−1V∗⊤p∗\n·|s = CU⊤\nd p∗\n·|s for C= N∗T∗−1M⊤.\nE.6 Proofs for supporting lemmas\nLemma E.2.For a language model{p·|s}, ifT is (τ,B)-natural,\nℓT({p·|s}) ≤τ + sup\nv∈RV,∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγ(pT; {p·|s})\nIf T is (τ,B)-natural w.r.t.Φ ∈Rd×V,\nℓT({Φp·|s}) ≤τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγΦ(pT; {p·|s})\nwhere γ(·) and γΦ(·) are from Deﬁnition B.1.\nProof. We note the following upper bounds onℓT({p·|s}) and ℓT({Φp·|s}).\nℓT({p·|s}) = inf\nv∈RV\n{\nℓT({p·|s},v)\n}\n≤ inf\nv∈RV,\n∥v∥∞≤B\n{\nℓT({p·|s},v)\n}\n(26)\nℓT({Φp·|s}) = inf\nv=Φ⊤λ∈RV\n{\nℓT({p·|s},v)\n}\n≤ inf\nv=Φ⊤λ∈RV,b∈R,\n∥v∥∞≤B\n{\nℓT({p·|s},v)\n}\n(27)\nWhen T is (τ,B)-natural, by Deﬁnition 3.1 we know thatinf\nv∈RV\n∥v∥∞≤B\n[\nℓT({p∗\n·|s},v)\n]\n≤τ. We now upper\nbound ℓT({p·|s},v) using Lemma E.8. Taking inﬁmum w.r.t.v∈RV,∥v∥∞≤B from the inequality\nin Lemma E.8.\nℓT({p·|s},v) ≤ℓT({p∗\n·|s},v) +\n√\nv⊤ΣpT(∆{p·|s})v\ninf\nv∈RV\n∥v∥∞≤B\nℓT({p·|s},v) ≤ inf\nv∈RV\n∥v∥∞≤B\nℓT({p∗\n·|s},v) + sup\nv∈RV,∥v∥∞≤B\n√\nv⊤ΣpT(∆{p·|s})v\nThis, combined with Equation (26), gives us\nℓT({p·|s}) ≤τ + sup\nv∈RV,∥v∥∞≤B\n√\nv⊤ΣpT(∆{p·|s})v (28)\nUsing Lemma E.10 and the deﬁnition ofγ(pT; {p·|s}) in Equation (7), we get that\nv⊤ΣpT(∆{p·|s})v≤\nΣpL(∆{p·|s})−1\n2 ΣpT(∆{p·|s})ΣpL(∆{p·|s})−1\n2\n\n2\n(\nv⊤ΣpL(∆{p·|s})v\n)\n30\n=\nv⊤ΣpL(∆{p·|s})v\nγ(pT; {p·|s}) (29)\nWe have thus successfully transferred the bound from the distributionpT to pL. Combining this with\nEquation (28) completes the proof of the ﬁrst part of the lemma.\nWe now prove the second part of the lemma where we only assume thatT is (τ,B)-natural w.r.t. Φ.\nHere we instead take the inﬁmum over classiﬁers in the span ofΦ in Lemma E.8 to get\ninf\nv=Φ⊤λ∈RV,b∈R,\n∥v∥∞≤B\n{\nℓT({p·|s},v)\n}\n≤ inf\nv=Φ⊤λ∈RV,b∈R,\n∥v∥∞≤B\n{\nℓT({p∗\n·|s},v)\n}\n+\nsup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpT(∆{p·|s})v (30)\nThis, combined with deﬁnition of(τ,B)-natural task w.r.t.Φ and Equation (27) gives us\nℓT({Φp·|s}) ≤τ + sup\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpT(∆{p·|s})v (31)\nFor the last term, for anyv= Φ⊤λ,λ ∈Rd we notice that\nv⊤ΣpT(∆{p·|s})v= λ⊤ΦΣpT(∆{p·|s})Φ⊤λ= λ⊤ΣpT(Φ∆{p·|s})λ\n≤(a)\nΣpL(Φ∆{p·|s})−1\n2 ΣpT(Φ∆{p·|s})ΣpL(Φ∆{p·|s})−1\n2\n\n2\n(\nλ⊤ΣpL(Φ∆{p·|s})λ\n)\n=\nλ⊤ΣpL(Φ∆{p·|s})λ\nγΦ(pT; {p·|s}) =\nv⊤ΣpL(∆{p·|s})v\nγΦ(pT; {p·|s})\nThis combined with Equation (31), we get\nℓT({Φp·|s}) ≤τ + inf\nv=Φ⊤λ∈RV,\n∥v∥∞≤B\n√\nv⊤ΣpL(∆{p·|s})v\nγΦ(pT; {p·|s})\nLemma E.3 (Pinsker’s inequality). For discrete distributions q,q∗ ∈∆V, let q,q∗ ∈RV be the\ncorresponding vector of probabilities. Then we have\nmax\n∥v∥∞≤1\n|v⊤(q−q∗)|≤\n√\n2DKL(q∗,q)\nProof. This basically follows from Pinsker’s inequality which upper bounds the total variation distance\nbetween distributions by their KL-divergence\nmax\n∥v∥∞≤1\n|v⊤(q−q∗)|= ∥q−q∗∥1 = 2 TV(q∗,q) ≤\n√\n2DKL(q∗,q)\nWe remind the reader that for an embedding matrixΦ ∈Rd×V, pθ,Φ := softmax(Φ⊤θ)\n31\nLemma E.4(Softmax variant of Pinsker’s inequality). Consider a matrixΦ ∈Rd×V with d≤V. For\nany discrete distributionq∗∈∆V and softmax distributionpθ,Φ = softmax(Φ⊤θ) ∈∆V for θ∈Rd, let\nq∗,pθ,Φ ∈RV be the corresponding vector of probabilities. Then we have\nmax\nv=Φ⊤λ,\n∥v∥∞≤1\n|v⊤(pθ,Φ −q∗)|≤\n√\n2\n(\nDKL(pθ,Φ,q∗)− inf\nθ∗∈Rd\nDKL(pθ∗,Φ,q∗)\n)\n(32)\nPinsker’s inequality (Lemma E.3), on the other hand, gives\nmax\n∥v∥∞≤1\n|v⊤(pθ,Φ −q∗)|≤\n√\n2DKL(pθ,Φ,q∗)\nProof. Deﬁne the lossρ(θ) := DKL(pθ,Φ,q∗). The statement in Equation (32) to prove reduces to\nmax\n∥Φ⊤λ∥∞≤1\n|λ⊤(Φpθ,Φ −Φq∗)|≤\n√\n2\n(\nρ(θ) − inf\nθ∗∈Rd\nρ(θ∗)\n)\n(33)\nTo prove this, we compute the gradient and hessian ofρ(θ) w.r.t. θ. We can simplifyρ(θ) as follows\nρ(θ) = DKL(pθ,Φ,q∗) = E\nw∼q∗\n[−log(pθ,Φ(w))] = E\nw∼q∗\n[\n−log\n(\neθ⊤φw\n∑\nw′eθ⊤φw′\n)]\n= −θ⊤Φq∗+ log\n(∑\nw′\neθ⊤φw′\n)\n= −θ⊤Φq∗+ log (Zθ)\nThe gradient is\n∇ρ(θ) = ∇\n[\n−θ⊤Φq∗+ log(Zθ)\n]\n= −Φq∗+ ∇Zθ\nZθ\n= −Φq∗+ ∇∑\nweθ⊤φw\nZθ\n= −Φq∗+\n∑\nweθ⊤φwφw\nZθ\n= −Φq∗+ Φpθ,Φ\nSimilarly the Hessian can be computed\n∇2ρ(θ) = ∇(∇ρ(θ)) = ∇[−Φq∗+ Φpθ,Φ] = ∇\n∑\nw∈W\npθ,Φ(w)φw =\n∑\nw∈W\n∇eθ⊤φw\nZθ\nφw\n=\n∑\nw∈W\neθ⊤φw\nZθ\nφwφ⊤\nw −eθ⊤φw\nZ2\nθ\nφw\n(∑\nw′\neθ⊤φw′φw′\n)⊤\n= E\nw∼pθ,Φ\n[φwφ⊤\nw] −\n(\nE\nw∼pθ,Φ\n[φw]\n)(\nE\nw∼pθ,Φ\n[φw]\n)⊤\n= Covw∼pθ,Φ[φw]\nWhere Covw∼pθ,Φ[φw] denotes the covariance of the word embeddingsφw when measured w.r.t. the\ndistribution pθ,Φ. This directly gives us that∇2ρ(θ) ≽ 0, since the covariance is always psd, and thusρ\nis convex inθ.\nWe return to the statement in Equation (33) that we need to prove. With the expression for gradient of\nρ at hand, we can rewrite Equation (33) as trying to prove\n|λ⊤∇ρ(θ)|≤∥ Φ⊤λ∥∞\n√\n2\n(\nρ(θ) − inf\nθ∗∈Rd\nρ(θ∗)\n)\n(34)\n32\nFurthermore, using the deﬁnition of the Hessian, it is not hard to see for someλ,˜θ ∈ Rd that\nλ⊤∇2ρ(˜θ)λ= Covw∼p˜θ,Φ[λ⊤φw] ≤ E\nw∼p˜θ,Φ\n[(λ⊤φw)2] ≤∥Φ⊤λ∥2\n∞. Thus we can evoke Lemma E.5 with\nℓ= ρ and L= ∥Φ⊤λ∥2\n∞to prove Equation (34) and thus completing the proof. Intuitively Lemma E.5\nexploits the smoothness of the function to argue that small suboptimality (i.e. being close to optimal\nsolution in function value) is suﬃcient to guarantee small norm of the gradient, a property that is\nwell-known in the optimization literature. We now present this lemma\nLemma E.5.If a functionℓ: Rd →R and λ∈Rd satisfy λ⊤∇2ℓ(˜θ)λ≤L,∀˜θ∈Rd (L-smoothness in\nthe direction ofλ) and ifℓ∗= infθ∈Rd ℓ(θ), then|λ⊤∇ℓ(θ)|2 ≤2L(ℓ(θ) −ℓ∗)\nProof. This is a variant of a classical result used in optimization and we prove it here for completeness.\nFor anyη∈R we have\nℓ(θ) −ℓ∗≥(a) ℓ(θ) −ℓ(θ−ηλ)\n≥(b) ℓ(θ) −\n(\nℓ(θ) + ⟨∇ℓ(θ),−ηλ⟩+ η2\n2 λ⊤∇2ℓ(˜θ)λ\n)\n≥(c) η(λ⊤∇ℓ(θ)) −η2L\n2\nwhere (a) follows from the deﬁnition of inﬁmum and(b) follows from Taylor’s expansion for some\n˜θ∈[θ−ηλ,θ] and (c) follows from the smoothness condition in the statement of the lemma. Picking\nη= λ⊤∇ℓ(θ)\nL gives usℓ(θ) −ℓ∗≥ 1\n2L|λ⊤∇ℓ(θ)|2, thus completing the proof.\nLemma E.6.For a language model{p·|s}and classiﬁerv∈RV,\nv⊤ΣpL(∆{p·|s})v≤2∥v∥2\n∞\n(\nℓxent({p·|s}) −ℓ∗\nxent\n)\nwhere ΣpL(g) = E\ns∼pL\n[g(s)g(s)⊤] and ∆{p·|s}(s) = p·|s −p∗\n·|s are deﬁned in Section B\nProof. We ﬁrst note that\nℓxent({p·|s}) −ℓxent({p∗\n·|s}) = E\ns∼pL\nE\nw∼p∗\n·|s\n[\nlog\n(p∗\n·|s(w)\np·|s(w)\n)]\n= E\ns∼pL\n[\nDKL(p∗\n·|s,p·|s)\n]\n(35)\nWe boundv⊤ΣpL(∆{p·|s})vbelow\nv⊤ΣpL(∆{p·|s})v= E\ns∼pL\n[(\nv⊤(p·|s −p∗\n·|s)\n)2]\n≤(a) ∥v∥2\n∞ E\ns∼pL\n[\n2DKL(p∗\n·|s,p·|s)\n]\n=(b) 2∥v∥2\n∞\n(\nℓxent({p·|s}) −ℓxent({p∗\n·|s})\n)\nwhere (a) follows from Lemma E.3 (Pinsker’s inequality),(b) uses Equation (35).\nLemma E.7.For a ﬁxedΦ, a softmax language model with featuresf and λ∈Rd,\nλ⊤ΣpL(Φ∆{pf(s)})λ≤2∥Φ⊤λ∥2\n∞(ℓxent(f,Φ) −ℓ∗\nxent(Φ))\nwhere ΣpL(Φ∆{pf(s)}) = E\ns∼pL\n[\n(Φpf(s) −Φp∗\n·|s)(Φpf(s) −Φp∗\n·|s)⊤\n]\nas deﬁned in Section B.\n33\nProof. We start by nothing that\nλ⊤ΣpL(Φ∆{pf(s)})λ= λ⊤ E\ns∼pL\n[\n(Φpf(s) −Φp∗\n·|s)(Φpf(s) −Φp∗\n·|s)⊤\n]\nλ\n= E\ns∼pL\n[|λ⊤(Φpf(s) −Φp∗\n·|s)|2] = E\ns∼pL\n[|(Φ⊤λ)⊤(pf(s) −p∗\n·|s)|2]\nWe will use the variant of Pinsker’s inequality from Lemma E.4 to bound each term on the right hand\nside. Notice thatℓxent(f,Φ) −ℓ∗\nxent(Φ) = E\ns∼pL\n[ℓxent,s(f(s),Φ) − inf\nθ∈Rd\nℓxent,s(θ,Φ)].\nλ⊤ΣpL(Φ∆{pf(s)})λ= E\ns∼pL\n[|(Φ⊤λ)⊤(pf(s) −p∗\n·|s)|2]\n≤(a) 2∥Φ⊤λ∥2\n∞ E\ns∼pL\n[\nDKL(p∗\n·|s,pf(s),Φ) − inf\nθ∈Rd\nDKL(p∗\n·|s,pθ,Φ)\n]\n≤2∥Φ⊤λ∥2\n∞ E\ns∼pL\n[\nℓxent,s(f(s),Φ) − inf\nθ∈Rd\nℓxent,s(θ,Φ)\n]\n≤2∥Φ⊤λ∥2\n∞(ℓxent(f,Φ) −ℓ∗\nxent(Φ))\nwhere (a) follows from Lemma E.4. This completes the proof.\nE.6.1 Classiﬁcation loss to covariance of error\nLemma E.8.For any taskT and classiﬁerv∈RV and predicted probabilities{p·|s}\nℓT({p·|s},v) ≤ℓT({p∗\n·|s},v) +\n√\nE\ns∼pT\n[\n(v⊤(p·|s −p∗\n·|s))2\n]\n= ℓT({p∗\n·|s},v) +\n√\nv⊤ΣpT(∆{p·|s})v\nwhere ΣpT(g) = E\ns∼pT\n[g(s)g(s)⊤] and ∆{p·|s}(s) = p·|s −p∗\n·|s are deﬁned in Section B.\nProof. The following sequence of inequalities proves it\nℓT({p·|s},v) = E\n(s,y)∼pT\n[\nℓ(v⊤p·|s,y)\n]\n≤(a) E\n(s,y)∼pT\n[\nℓ(v⊤p∗\n·|s,y) + |v⊤(p∗\n·|s −p·|s)|\n]\n≤(b) E\n(s,y)∼pT\n[\nℓ(v⊤p∗\n·|s,y)\n]\n+\n√\nE\ns∼pT\n[⏐⏐⏐v⊤(p∗\n·|s −p·|s)\n⏐⏐⏐\n2]\n= ℓT({p∗\n·|s},v) +\n√\nv⊤\n(\nE\ns∼pT\n[\n(p∗\n·|s −p·|s)(p∗\n·|s −p·|s)⊤\n])\nv\n= ℓT({p∗\n·|s},v) +\n√\nv⊤ΣpT(∆{p·|s})v\nwhere (a) follows from 1-lipschitzness ofℓ, (b) follows from Jensen’s inequality.\nE.6.2 Handling distribution shift\nLemma E.9.For anyg: S→ RD and pT ∈∆S, we have∥ΣpL(g)−1\n2 ΣpT(g)ΣpL(g)−1\n2 ∥2 ≤γ(pT)−1\nProof. By deﬁnition ofγ(pT), we have that\nΣpL(g) = E\ns∼pL\n[g(s)g(s)⊤] =\n∑\ns∈S\npL(s)g(s)g(s)⊤\n34\n≽ γ(pT)\n∑\ns∈S\npT(s)g(s)g(s)⊤= γ(pT) E\ns∼pT\n[g(s)g(s)⊤] = γ(pT)ΣpT(g)\nThus 1\nγ(pT) ΣpL(g) ≽ ΣpT(g) and hence 1\nγ(pT) ΣpL(g)−1\n2 ΣpL(g)ΣpL(g)−1\n2 ≽ ΣpL(g)−1\n2 ΣpT(g)ΣpL(g)−1\n2 ,\nwhich is equivalent to 1\nγ(pT) ID ≽ ΣpL(g)−1\n2 ΣpT(g)ΣpL(g)−1\n2 . This ﬁnishes the proof.\nLemma E.10. For matrices X,Y ∈ RD×D s.t. X,Y ≽ 0 and Y is full rank, we have that\nmax\na∈RD,0<∥a∥≤λ\na⊤Xa\na⊤Y a= ∥Y−1\n2 XY−1\n2 ∥2 for any norm∥·∥.\nProof. Note that a⊤Xa\na⊤Y a is independent of the scaling ofa. The following sequence of inequalities\ncompletes the proof\nmax\na∈RD,0<∥a∥≤λ\na⊤Xa\na⊤Ya = max\na∈RD\na⊤Xa\na⊤Ya = max\na∈RD\na⊤Xa\n(Y\n1\n2 a)⊤(Y\n1\n2 a)\n= max\na∈RD,∥Y\n1\n2 a∥2=1\na⊤Xa= max\nb∈RD,∥b∥2=1\n(Y−1\n2 b)⊤X(Y−1\n2 b)\n= max\nb∈RD,∥b∥2=1\nb⊤Y−1\n2 XY−1\n2 b= ∥Y−1\n2 XY−1\n2 ∥2\nF Experiment Details\nFor all experiments9, we use the 117M parameter “small” GPT-2 model proposed in Radford et al.\n[2019] and implemented in HuggingFace [Wolf et al., 2019]. Linear classiﬁcation experiments (except\nfor ﬁne-tuning baseline in Table 1) are performed onﬁxed output features from GPT-2.\nWe note that the binary SST-2 dataset used in all experiments is comprised of complete sentences, and\nthere are 6,920 train examples and 1,821 test examples. In particular, this dataset is smaller than the\nversion included with the GLUE benchmark [Wang et al., 2018]. This smaller version of SST-2 better\nﬁts the sentence completion hypothesis we propose.\nF.1 Solving downstream tasks using f and Φpf\nThe featuresf from GPT-2 for any input sequence(w1,...,w N) is the output embedding of the ﬁnal\ntoken wN at the ﬁnal layer, whereN is the input length and can be diﬀerent for diﬀerent inputs. This is\nalso the embedding that is directly multiplied by the word embeddings to get the softmax distribution\nfor language modeling, as in the theoretical setting. To use a prompt, the same prompt is added at the\nend of all inputs and the features are extracted for this modiﬁed input.\nWe use theLogisticRegressionCV class from the scikit-learn package to ﬁt linear classiﬁers to all\nﬁxed features (i.e., no ﬁnetuning). We use the liblinear solver and one-vs-rest loss function unless it\ncatastrophically fails (e.g., close to random performance) on a particular multi-class task. In that case,\nwe use the stochastic average gradient (SAG) algorithm with multinomial loss. We use 5-fold cross\nvalidation for all experiments and test values for the regularization parameterC between 1e−6 and 1e4\nfor small datasets (i.e., fewer than 10K examples) and between1e−3 and 1e3 for larger datasets.\n9Link to code:https://github.com/sadhikamalladi/mathematical-exploration-downstream-tasks .\n35\nDetails about word subsets:For all of the results presented in Table 1, we use a pre-trained GPT-2\nmodel. For SST, we use the prompt “This movie is ” when indicated. For AG News, we use the prompt\n“This article is about ” when indicated.\nWe compute the conditional probability of selecting a subset of words to complete the sentence. For\nAG News, this subset is: ’world’, ’politics’, ’sports’, ’business’, ’science’, ’ﬁnancial’, ’market’, ’foreign’,\n’technology’, ’international’, ’stock’, ’company’, ’tech’, ’technologies’. For SST, this subset is: ’:)’, ’:(’,\n’great’, ’charming’, ’ﬂawed’, ’classic’, ’interesting’, ’boring’, ’sad’, ’happy’, ’terrible’, ’fantastic’, ’exciting’,\n’strong’. For AG News, the class words we use are: ’foreign’, ’sports’, ’ﬁnancial’, ’scientiﬁc’. For SST,\nthe class words we use are ‘:)’ and ‘:(’.\nWe account for BPE tokenization by using the encoding of the word directly and the encoding of the\nword with a space prepended. We then ﬁlter to use only words that encode to a single BPE token.\nTests on additional datasets: We also test the performance of pre-trained GPT-2 embeddingsf\nand the conditional mean embeddingsΦpf on the DBPedia [Auer et al., 2007], Yahoo Answers [Zhang\net al., 2015], TREC [Li and Roth, 2002], IMDb [Maas et al., 2011], Customer Review (CR) [Hu and\nLiu, 2004], and MPQA polarity [Wilson and Wiebe, 2003] datasets in Table 2. We limited the training\nset size to 250K for larger datasets (i.e., DBPedia and Yahoo Answers). For CR and MPQA, we\nfollow Zhang et al. [2015] and average the performance across 10 random 90-10 train-test splits of the\ndataset.\nWe ﬁnd thatΦpf consistently has comparable performance tof across non-sentiment and sentiment\ndownstream classiﬁcation tasks. We include baseline results of bag ofn-grams (BonG) for most tasks\nand the mLSTM model [Radford et al., 2017] for sentiment tasks. BonG performs quite well on the\nlarger datasets, but not as well on smaller datasets, due to the high dimensionality of features.\nFor sentiment tasks, adding a prompt almost always boosts performance. We also demonstrate that\nmuch of the performance can be recovered by only looking at “positive” and “negative” or “:)” and “:(”\nas class words. Using these 2-dimensional features is even more sample-eﬃcient than the standard\n768-dimensional ones.\nWe also include results using the pre-trained BERT base cased model [Devlin et al., 2018, Wolf et al.,\n2019], using the embedding at the ﬁrst token as input to the downstream task. We also tried using\nthe mean embedding and last token embedding and found that the ﬁrst token embedding is often the\nbest. Moreover, the ﬁrst token embedding is what is extracted in the traditional usage of BERT on\ndownstream tasks, though we note that it is rare to use BERT without ﬁne-tuning.\nF.2 Finetuning Experiments\nAs a strong baseline, we ﬁnetune the GPT-2 features along with learning a linear classiﬁer for the\nSST and AG News classiﬁcation tasks and report accuracy numbers in Table 1. We use a maximum\nsequence length of 128 BPE tokens for downstream inputs of SST-2 and a maximum length of 400 BPE\ntokens for AG News inputs. We use the end of sentence token as the padding token. The datasets are\ndescribed below.\n1. AG News has 108K train examples, 12K dev examples, 7600 test examples. We split the train\nset for AG News into train and dev (90-10) and use the same test set as the non-ﬁnetuning\nexperiments.\n2. The sentence version of SST-2 has 6,920 train examples (same as non-ﬁnetuning), and 810\nexamples for dev and test each (split the original test set in half).\n36\nTable 2: GPT-2 performance without ﬁne-tuning on downstream task test sets withk classes. We\nprovide the performance of bag ofn-grams (BonG) as an approximate baseline for these tasks. AG\nNews, DBPedia and Yahoo performances were reported in Zhang et al. [2015], and the other tasks were\nreported in Khodak et al. [2018]. We also include results from mLSTM (Sentiment Neuron) [Radford\net al., 2017] for the sentiment-related classiﬁcation tasks (SST, IMDb, CR, and MPQA) with numbers\nreported from Khodak et al. [2018]. Furthermore, we include results for BERT [Devlin et al., 2018]\nfeatures without ﬁne-tuning, where we use the output features for the ﬁrst position of an input for linear\nclassiﬁcation. An asterisk indicates we add a standard sentiment prompt “The sentiment is” to each\ninput, but for AG News we used the prompt “This article is about”. We also tested the performance of\nthe conditional probability distribution over “positive” and “negative” as well as “:)” and “:(” on the\nsentiment-related tasks with and without the prompt.\nTask k f(s) Φ pf(s) p·|s: pos,neg p·|s: :),:( BonG mLSTM BERT\nNon-sentiment\nAG News 4 90.7 84.6 - - 92.4 (n= 5) - 88.9\nAG News* 4 91.1 88.2 - - - - 89.9\nDBPedia 14 97.2 88.2 - - 98.6 (n= 5) - 98.7\nYahoo 10 69.2 56.7 - - 68.5 (n= 5) - 65.0\nTREC 6 93.6 87.8 - - 89.8 (n= 3) - 90.6\nSentiment\nSST 2 87.5 83.3 74.9 78.7 80.9 (n= 2) 91.8 85.8\nSST* 2 89.4 87.3 80.8 79.1 - - 84.1\nSST ﬁne 5 49.2 43.5 37.5 39.2 42.3 (n= 3) 52.9 43.5\nSST ﬁne* 5 49.4 48.0 41.5 40.2 - - 43.3\nIMDb 2 88.1 82.7 73.8 76.2 89.8 (n= 3) 92.3 82.2\nIMDb* - 88.4 85.3 81.8 80.9 - - 84.0\nCR 2 86.8 84.6 74.9 80.0 78.3 (n= 3) 91.4 85.5\nCR* - 87.9 87.1 82.5 79.4 - - 84.6\nMPQA 2 86.0 79.2 75.6 70.7 85.6 (n= 3) 88.5 87.3\nMPQA* - 87.8 86.1 80.3 71.4 - - 88.1\n3. Fine-grained SST-2 has 8,544 train examples (same as non-ﬁnetuning), and 1,105 examples each\nfor the dev and test data (split the original test set in half).\nTo select the best hyperparameter conﬁguration, we run a grid search over learning rate and batch size.\nWe train each model for 10 epochs. For all datasets, we test learning rates5e−5, 1e−4, and3e−4. For\nboth version of SST-2, we try batch sizes 8, 16, and 32, and for AG News, we try batch sizes 8, 12,\nand 16. We note that the longer sequence length of AG News inputs required us to use parallelization\nacross multiple GPUs to simulate larger batch sizes, which made batch size 32 prohibitively expensive\nto test.\nWe take the hyperparameter conﬁguration that achieves the best performance on the dev set and then\nperform ﬁne-tuning using those settings with three diﬀerent random seeds: 8, 33, and 42. We then\nreport the average performance on the test set in Table 1.\nWe perform the hyperparameter grid search over the standard datasets and then perform ﬁne-tuning\nusing the best settings on the dataset with task-speciﬁc prompts added. For SST-2, we use the prompt\n“This movie is ”, and for AG News we use “This article is about ”.\n37\nTable 3: Comparing Quad features to cross-entropy features for GPT-2 trained on the IMDb unlabeled\ncorpus [Maas et al., 2011]. In this experiment we ﬁxΦ to be the word embeddings from prertained\nGPT-2 model for the cross-entropy objective. For the Quad objective, we initializeΦ to be the SVD of\nthe pre-trained embeddings. An asterisk indicates that we added the prompt “This movie is ” to each\ninput.\nTask f(s) (xent) Φpf(s) (xent) f(s) (Quad)\nSST 82.1% 79.9% 77.3%\nSST* 83.1% 81.1% 80.7%\nTable 4: Comparing Quad features to cross-entropy features for GPT-2 trained on the Amazon corpus.\nAn asterisk indicates that we added the prompt “This movie is ” to each input. Note that the validation\nloss was still decreasing at the time of measurement.\nTask f(s) (xent) Φpf(s) (xent) f(s) (Quad, learnedΦ)\nSST 89.4% 89.7% 79.2%\nSST* 89.7% 89.2% 84.3%\nF.3 Testing Quad objective\nWe test two models with the same parametrizations, one trained using our Quad objective and another\ntrained with the standard cross-entropy objective using the unlabeled IMDb corpus [Maas et al., 2011]\nand the Amazon product review corpus [McAuley et al., 2015]. We slightly modify the standard\narchitecture of GPT-2 to generate Tables 3 and 4. First we add a single linear layer (that is trained) on\ntop of the output features of the standard Transformer architecture. Furthermore, instead of tying the\ninput and output word (token) embeddings, we learn them separately so thatf and Φ are independent\nfunctions; this is more in line with out theoretical setup. We ﬁx the input embeddings and the positional\nembeddings to be the parameters from the pre-trained GPT-2.\nFor Quad, we initialize Φ, the output embeddings, using the singular vectors of the pre-trained\nword embeddings Φ. For the cross-entropy models, we initializeΦ to be the full pre-trained word\nembeddings Φ, because we found that initializing with the singular vectors harmed performance. Given\nour parameterization, initializing with the singular vectors is as expressive as initializing with the\npretrained embeddingsΦ themselves; however it potentially lends a better optimization landscape and\nspeeds up training for our new objective Quad. As described in Section 5.2, we minimize the following\nobjective\nℓquad(f,Φ) = E\n(s,w)\n[\n−f(s)⊤φw + 1\n2∥Φ⊤f(s)∥2\n]\n(36)\nwhere (s,w) are sampled from the text corpus. The implementation of the Quad loss is the same as\nthe standard cross-entropy loss, the main diﬀerence being the second term: it is1\n2 ∥Φ⊤f(s)∥2 for Quad\ninstead of the log-partition functionlog\n(∑\nw′ef(s)⊤φw′\n)\nin the cross-entropy objective.\nBecause IMDb is a smaller dataset, we ﬁxΦ at its initialization and only trainf to generate Table 3.\nWhen training on the Amazon dataset, we initializedΦ the same way as we did for the IMDb dataset,\nbut we allowedf and Φ to both be trained, since more data was available. To train the models, we use\nthe standard learning rate schedule as in in Radford et al. [2019]. To learn a model on IMDb, we use a\ncontext size of 512 BPE tokens, and for the Amazon reviews dataset [McAuley et al., 2015], we use the\nstandard context length of 1,024 BPE tokens.\n38\nFigure 2: Fit of the learned quadratic function to the log partition function on various datasets for\nfeatures computed by the full, pre-trained GPT-2. We also plot they = x line for reference. These\nplots are meant to verify Assumption 4.1.\nWe observe that training using Quad, in both cases, yields comparable performance to the language\nmodel on the SST task, but always slightly worse. According to the theory, featuresf(s) from Quad\nshould learnp∗\n·|s on a subspace, just likeΦpf from cross-entropy models, thus making the comparison\nbetween these two important. Furthermore, adding a prompt consistently improves performance for\nboth objectives. While Quad did not beat the cross-entropy in either case, its good performs at least\ndemonstrates that insights from the theoretical analysis can translate to practical algorithms. We leave\nexploring the gap in performance between Quad and cross-entropy and a more extensive evaluation of\nQuad for future work.\nF.4 Learning the quadratic approximation of the log-partition function\nIn Assumption 4.1, we assert that there is a quadratic ﬁt for the log partition function, which allows us\nto show in Lemma 4.3 that a linear relation holds betweenf and Φpf. We validate these theoretical\nﬁndings by ﬁtting a quadratic function to the log partition function for a subset of embeddings from\nthe IMDb, SST, and AG News datasets (Figure 1). Here, we describe how we learnedA, band c. To\nensure Ais symmetric and positive semi-deﬁnite as required, we parametrizeA= UUT. As deﬁned\nearlier, the partition functionZθ = ∑\nw′eθ⊤φw′ and Φpθ = ∑\nw′\neθ⊤φw′\nZθ\nφw′ for anyθ∈Rd. We minimize\nthe following objective function:\nL(U,b,c) = E\nθ\n[\nλ1\n(\nlog(Zθ) −1\n2θ⊤UU⊤θ−θ⊤b−c\n)2\n+ λ2\nΦpθ −UU⊤θ−b\n\n2\n]\n(37)\n39\n(a) Trained on IMDb [Maas et al., 2011]\n (b) Trained on Amazon [McAuley et al., 2015]\nFigure 3: Logistic loss of conditional mean features on the SST-2 task for various checkpoints of a\nGPT-2 architecture trained on IMDb and Amazon. The reported cross-entropy is measured on the\nvalidation set. The red trend shows the ﬁt of a square-root function, which is what the upper bound in\nTheorem 4.2 looks like.\nIn practice, we train only on the regression loss (i.e.,λ1 = 0, λ2 = 1) for the most promising results.\nNote that the regression term is trying to learn a linear relationship between betweenθ and Φpθ that\nLemma 4.3 aims to prove. This ends up learning a matrixA= UU⊤and vectorbthat also satisfy the\nquadratic form oflog(Zθ) from Assumption 4.1.\nWe use 20,000 examples from a mix of IMDb, SST, and AG News embeddings as the training set. Thus\nwe sampleθ by samplingsfrom the aforementioned datasets and setθ= f(s), f being the feature map\nfrom pretrained GPT-2. We use the Adam [Kingma and Ba, 2014] optimizer with learning rate1e−3\nfor U and learning rate1e−4 for band c. We decay the learning rate every 50 steps by a factor of 0.1.\nWe use theU obtained after 8 epochs of training. We further demonstrate the quality of the learned\nﬁt by plotting the true log partition and estimated log partition function for embeddings from other\ndatasets in Figure 2.\nF.5 Experimentally Checking Theorem 4.2\nTheorem 4.2 can be informally summarized as stating that anϵ suboptimality in the cross-entropy of a\nd-dimensional language model propagates to a√ϵ increase in the logistic loss. We note that theτ,B,\nand γ(pT) factors are ﬁxed for a given pre-training corpus and downstream task, so we can empirically\ntest if this square root relationship holds in practice. In particular, Theorem 4.2 says\nℓT(Φpf) ≤τ +\n√\n2B2 (γ(pT))−1 (ℓxent(f,Φ) −ℓ∗\nxent) (38)\nOf these,τ,B,γ (pT)−1 and ℓ∗\nxent are independent of the language model(f,Φ) and only depend on the\ntask T and language modeling distribution. Thus we can rewrite this asℓT(Φpf) ≤c+a\n√\nℓxent(f,Φ) −b\nfor suitable constantsa,b,c ∈R. The left hand side,ℓT(Φpf), is the logistic loss of conditional mean\nfeatures from language model(f,Φ) on taskT and ℓxent(f,Φ) is the cross-entropy loss of the language\nmodel, both of which can be measured in practice.\nWe train a 117M parameter GPT-2 model from scratch on the IMDb and Amazon corpora, described\nin Section F.3. We maintain checkpoints during training, and for each checkpoint, we measure the\ncross-entropy of the model on the validation set as well as the performance of the conditional mean\nfeatures Φpf on SST-2. Plotting these values together yields Figure 3.\n40\nWe furthermore ﬁt a square root trend, shown in red, to these points. We learna,b,c such that\ny ≈a\n√\nx−b+ c, wherey = ℓT(Φpf) is the logistic loss andx= ℓxent(f,Φ) is the cross-entropy loss.\nFor this, we perform a grid search over 100 evenly spaced valid values ofb, and for eachb, we perform\nlinear regression on\n√\nx−b to ﬁnda and c. We choose thea,b,c that maximizes ther-value of the\nregression. While Theorem 4.2 only provides an upper bound on the logistic loss, this experiment shows\nthat some square-root trend is observable in practice.\n41",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8311188220977783
    },
    {
      "name": "Perplexity",
      "score": 0.7132917642593384
    },
    {
      "name": "Language model",
      "score": 0.6434404253959656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6013144254684448
    },
    {
      "name": "Natural language",
      "score": 0.5707393288612366
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.5451483130455017
    },
    {
      "name": "Machine learning",
      "score": 0.541577160358429
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.5127869248390198
    },
    {
      "name": "Task (project management)",
      "score": 0.4718104600906372
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.46398022770881653
    },
    {
      "name": "Autoregressive model",
      "score": 0.4603000283241272
    },
    {
      "name": "Natural language processing",
      "score": 0.4385646879673004
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}