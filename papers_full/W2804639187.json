{
    "title": "Using Morphological Knowledge in Open-Vocabulary Neural Language Models",
    "url": "https://openalex.org/W2804639187",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2164133841",
            "name": "Austin Matthews",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2107310219",
            "name": "Chris Dyer",
            "affiliations": [
                "DeepMind (United Kingdom)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2155607551",
        "https://openalex.org/W2575512508",
        "https://openalex.org/W2964325845",
        "https://openalex.org/W2963022149",
        "https://openalex.org/W2510842514",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2963735467",
        "https://openalex.org/W28766783",
        "https://openalex.org/W2066201260",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2149741699",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2399344342",
        "https://openalex.org/W2417736714",
        "https://openalex.org/W2004316800",
        "https://openalex.org/W2411934291",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W627498308",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2963824800",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2963582782",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W1505680913",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W1990645831",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963324947"
    ],
    "abstract": "Austin Matthews, Graham Neubig, Chris Dyer. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",
    "full_text": "Proceedings of NAACL-HLT 2018, pages 1435–1445\nNew Orleans, Louisiana, June 1 - 6, 2018.c⃝2018 Association for Computational Linguistics\nUsing Morphological Knowledge in\nOpen-Vocabulary Neural Language Models\nAustin Matthews and Graham Neubig\nLanguage Technologies Institute\nCarnegie Mellon University\nPittsburgh, PA, USA\n{austinma,gneubig}@cs.cmu.edu\nChris Dyer\nDeepMind\nLondon, UK\ncdyer@google.com\nAbstract\nLanguages with productive morphology pose\nproblems for language models that generate\nwords from a ﬁxed vocabulary. Although\ncharacter-based models allow any possible\nword type to be generated, they are linguis-\ntically naïve: they must discover that words\nexist and are delimited by spaces—basic lin-\nguistic facts that are built in to the structure\nof word-based models. We introduce an open-\nvocabulary language model that incorporates\nmore sophisticated linguistic knowledge by\npredicting words using a mixture of three gen-\nerative processes: (1) by generating words as a\nsequence of characters, (2) by directly gener-\nating full word forms, and (3) by generating\nwords as a sequence of morphemes that are\ncombined using a hand-written morphological\nanalyzer. Experiments on Finnish, Turkish,\nand Russian show that our model outperforms\ncharacter sequence models and other strong\nbaselines on intrinsic and extrinsic measures.\nFurthermore, we show that our model learns\nto exploit morphological knowledge encoded\nin the analyzer, and, as a byproduct, it can\nperform effective unsupervised morphological\ndisambiguation.\n1 Introduction\nLanguage modelling of morphologically rich lan-\nguages is particularly challenging due to the vast\nset of potential word forms and the sparsity with\nwhich they appear in corpora. Traditional closed\nvocabulary models are unable to produce word\nforms unseen in training data and unable to gen-\neralize sub-word patterns found in data.\nThe most straightforward solution is to treat\nlanguage as a sequence of characters (Sutskever\net al., 2011). However, models that operate at\ntwo levels—a character level and a word level—\nhave better performance (Chung et al., 2016). An-\nother solution is to use morphological information,\nwhich has shown beneﬁts in non-neural models\n(Chahuneau et al., 2013). In this paper, we present\na model that combines these approaches in a fully\nneural framework.\nOur model incorporates explicit morphological\nknowledge (e.g. from a ﬁnite-state morphological\nanalyzer/generator) into a neural language model,\ncombining it with existing word- and character-\nlevel modelling techniques, in order to create a\nmodel capable of successfully modelling morpho-\nlogically complex languages. In particular, our\nmodel achieves three desirable properties.\nFirst, it conditions on all available (intra-\nsentential) context, allowing it, in principle, to\ncapture long-range dependencies, such as that the\nverb agreement between “students” and “are” in\nthe sentence “The students who studied the hardest\nare getting the highest grades”. While traditional\nn-gram based language models lack this property,\nRNN-based language models fulﬁll it.\nSecond, it explicitly captures morphological\nvariation, allowing sharing of information be-\ntween variants of the same word. This allows\nfaster, smoother training as well as improved pre-\ndictive generalization. For example, if the model\nsees the phrase “gorped the ball” in data, it is able\nto infer that “gorping the ball” is also likely to\nbe valid. Similarly, the model is capable of un-\nderstanding that morphological consistency within\nnoun phrases is important. For example in Rus-\nsian, one might say malen’kaya chërniya koshka\n(\"small black cat\", nominative), or malen’kuyu\nchërniyu koshku (accusative), but malen’kiy chër-\nnuyu koshke (mixing nominative, accusative and\ndative) would have much lower probability.\nThird, the language model seamlessly handles\nout of vocabulary items and their morphological\nvariants. For example, even if the word Obama\nwas never seen in a Russian corpus, we expect\nYa dal eto prezidentu Obame (“I gave it to presi-\n1435\ndent Obama”) to have higher probability using the\ndative Obame than Ya dal eto prezidentu Obama,\nwhich uses the nominative. The model can also\nlearn to decline proper nouns, including OOVs.\nHere it can recognize that dal (“gave”) requires a\ndative, and that nouns ending with “a” generally\ndo not meet that requirement.\nIn order to capture these properties, our model\ncombines two pieces: an alternative embedding\nmodule that uses sub-word information such as\ncharacter and morpheme-level information, and a\ngeneration module that allows us to output words\nat the word, morpheme, or character-level. The\nembedding module allows for the model to share\ninformation between morphological variants of\nsurface forms, and produce sensible word embed-\ndings for tokens never seen during training. The\ngeneration model allows us to emit tokens never\nseen during training, either by combining a lemma\nand a sequence of afﬁxes to create a novel sur-\nface form, or by directly spelling out the desired\nword character by character. We then demonstrate\nthe effectiveness both intrinsically, showing re-\nduced perplexity on several morphologically rich\nlanguages, and extrinsically on machine transla-\ntion and morphological disambiguation tasks.\n2 Multi-level RNNLMs\nRecurrent neural network language models are\ncomposed of three parts: (a) an encoder, which\nturns a context word into a vector, (b) a recur-\nrent backbone that turns a sequence of word vec-\ntors that represent the ordered sequence of con-\ntext vectors into a single vector, and (c) a genera-\ntor, which assigns a probability to each word that\ncould follow the given context. RNNLMs often\nuse the same process for (a) and (c), but there is\nno reason why these processes cannot be decou-\npled. For example, Kim et al. (2016) and Ling\net al. (2015) compose character-level representa-\ntions for their word encoder, but generate words\nusing a softmax whose probabilities rely on inner\nproducts between the current context vector and\ntype-speciﬁc word embeddings.\nIn our model both the word generator (§2.1) and\nthe word encoder (§2.2) compute representations\nthat leverage three different views of words: fre-\nquent words have their own parameters, words that\ncan be analyzed/generated by an analyzer are rep-\nresented in terms of sequences of abstract mor-\nphemes, and all words are represented as a se-\nquence of characters.\n2.1 Word generation mixture model\nIn typical RNNLMs the probability of theith word\nin a sentence, wi given the preceding words is\ncomputed by using an RNN to encode the context\nfollowed by a softmax:\np(wi |w<i) =p(wi |hi = ϕRNN(w1, . . . , wi−1))\n= softmax(Whi + b)\nwhere ϕRNN is an RNN that reads a sequence of\nwords and returns a ﬁxed sized vector encoding,\nW is a weight matrix, and b is a bias.\nIn this work, we will use a mixture model over\nM different models for generating words in place\nof the single softmax over words (Miyamoto and\nCho, 2016; Neubig and Dyer, 2016):\np(wi |hi) =\nM∑\nmi=1\np(wi, mi |hi)\n=\nM∑\nmi=1\np(mi |hi)p(wi |hi, mi),\nwhere mi ∈ [1, M] indicates the model used to\ngenerate word wi. To ensure tractability for train-\ning and inference, we assume thatmi is condition-\nally independent of allm<i, given the sequence of\nword forms w<i.\nWe use three ( M = 3 ) component models:\n(1) directly sampling a word from a ﬁnite vocabu-\nlary (mi = WORD ), (2) generating a word as a se-\nquence of characters (mi = CHARS ), and (3) gen-\nerating as a sequence of (abstract) morphemes\nwhich are then stitched together using a hand-\nwritten morphological transducer that maps from\nabstract morpheme sequences to surface forms\n(mi = MORPHS ). Figure 1 illustrates the model\ncomponents, and we describe in more detail here:\nWord generator. Select a word by directly sam-\npling from a multinomial distribution over surface\nform words. Here the vocabulary is the |Vw|most\ncommon full-form words seen during training. All\nless frequent words are assigned zero probability\nby this model, and must be generated by one of\nthe remaining models.\nCharacter sequence generator. Generate a\nword as a sequence of characters. Each character\nis predicted conditioned on the LM hidden state\nhi and the partial word generated so far, encoded\n1436\nwith an RNN. The product of these probabilities is\nthe total probability assigned to a full word form.\nMorpheme sequence generator. Similarly to\nthe character sequence generator, we can gener-\nate a word as a sequence of morphemes. We ﬁrst\ngenerate a rootr, followed by a sequence of afﬁxes\na1, a2, . . .. For example the word “devours” might\nbe generated as devour+3P+SG+EOW. Since mul-\ntiple sequences of abstract morphemes may in\ngeneral give rise to a single output form, 1 we\nmarginalize these, i.e.,\np(wi |hi,mi = MORPHS ) =\n∑\nai∈{a|GEN (a)=wi}\npmorphs(ai |hi).\nwhere GEN (a) gives the surface word form pro-\nduced from the morpheme sequence a.\nDue to the model’s ability to produce output at\nthe character level, it is able to produce any out-\nput sequence at all within the language’s alpha-\nbet. This is critical as it allows the model to gen-\nerate unknown words, such as novel names or de-\nclensions thereof. Furthermore, the morphologi-\ncal level facilitates the model’s generation of word\nforms whose lemmas may be known, but whose\nsurface form was nevertheless unattested in the\ntraining data. Finally the word-level generation\nmodel handles generating words that the model\nhas seen many times during training.\n2.2 Morphologically aware context vectors\nWord vectors are typically learned with a single,\nindependent vector for each word type. This inde-\npendence means, for example, that the vectors for\nthe word “apple” and the word “apples” are com-\npletely unrelated. Seeing the word “apple” gives\nno information at all about the word “apples”.\nIdeally we would like to share information be-\ntween such related words. Nevertheless, some-\ntimes words have idiomatic usage, so we’d like not\nto tie them together too tightly.\nWe accomplish this by again using three differ-\nent types of word vectors for each word in the vo-\ncabulary. The ﬁrst is a standard per-type word vec-\ntor. The second is the output of a character-level\n1In general analyzers encode many-to-many relations, but\nour model assumes that any sequence of morphs in the under-\nlying language generates a single surface form. This is gen-\nerally true, although free spelling variants of a morph (e.g.,\nAmerican -ize vs. British -ise as well as alternative realiza-\ntions like shined/shone and learned/learnt) violate this as-\nsumption.\nd o g s\ndog\ndogs\n+ ᴘʟ\np(m i  | h i )\np( w i  | h i , m i )\nh i\n</w>\n</w>\ndog + 3 ᴘ + s ɢ </w>\nsum\n]\nFigure 1: We allow the model to generate an output\nword at the word, morpheme, or character level,\nand marginalize over these three options to ﬁnd\nthe total probability of a word.\nRNN using Long Short-Term Memory (LSTM)\nunits (Hochreiter and Schmidhuber, 1997). The\nthird is the output of a morphology-level LSTM\nover a lemma and a sequence of afﬁxes, as output\nby a morphological analyzer.\nTypically language models ﬁrst generate a word\nwi given some (initially empty) prior contextci−1,\nand then that word is combined with the context\nto generate a new context ci that includes the new\nword. Since we have just used one or more of our\nthree modes to generate each word, intuitively we\nwould like to use the same mode(s) to generate the\nembedding used to progress the context.\nUnfortunately, doing so introduces dependen-\ncies among the latent variables p(mode | ci) in\nour model, making exact inference intractable. As\nsuch, we instead drop the dependency on how\na word was generated and instead represent the\nword at all three levels, regardless of the mode(s)\nactually used to generate it, and combine them by\nconcatenating the three representations. A visual\nrepresentation of the embedding process is shown\nin Figure 2.\nAdditionally, should a morphological analyzer\nproduce more than one valid analysis for a surface\nform, we independently produce embeddings for\neach candidate analysis, and combine them using\na per-dimension maximum operation. Mathemati-\n1437\nd o g s\ndog\ndogs\n+ ᴘʟ </w>\n</w>\ndog + 3 ᴘ + s ɢ\nmax\n</w>\nFigure 2 : We concatenate word- morpheme- and\ncharacter-level vectors to build a better input vec-\ntor for our RNNLM.\ncally, the ith dimension of the morphological em-\nbedding em is given by\nemi = max\nj\neaj i\nwhere eaj is the embedding of the jth possi-\nble analysis, as computed by the LSTM over the\nlemma and its sequence of afﬁxes.\nThe intuition behind the use of all analyses plus\na pooling operation can be seen by observing the\ncase of the word “does”, which could be do+3-\nperson+singular or doe+plural. If this word ap-\npears after the word “he”, what we care about\nmore is whether “does” could feasibly be a third\nperson singular verb, thus agreeing with the sub-\nject. The max-pooling operation captures this in-\ntuition by ensuring that if a feature is active for\none of these two analyses, it will also be active in\nthe pooled representation. This procedure affords\nus the capability to efﬁciently marginalize over all\nthree possible values of the latent variable at each\nstep, and compute the full marginal of the wordwi\ngiven the context ci−1 during generation.\nThis formulation allows words with the same\nstem to share vector information through the char-\nacter or morphological embeddings, but still af-\nfords each word the ability to capture idiomatic us-\nages of individual words through the word embed-\ndings. Furthermore, it allows a language model to\nexplicitly capture morphological information, for\nexample that third person singular subjects should\nco-occur with third person singular verbs. Finally,\nthe character-level segment of the embedding al-\nlows the model to at least attempt to build sensible\nembeddings for completely unknown words. For\nexample in Russian where names can decline with\ncase this formulation allows the model to know\nthat Obame is probably dative, even if it’s an OOV\nat the word level, and even if the morphological\nanalyzer is unable to produce any valid analyses.\nWe combine our three-layer input vectors, our\nfactored output model, and a standard LSTM\nbackbone to create a morphologically-enabled\nRNNLM that, as we will see in the next section,\nperforms well on morphologically complex lan-\nguages.\n3 Intrinsic Evaluation\nWe demonstrate the effectiveness of our model by\nexperimenting on three languages: Finnish, Turk-\nish, and Russian. For Finnish we use version 8 of\nthe Europarl corpus, for Turkish we use the SE-\nTIMES2 corpus, and for Russian we use version\n12 of the News Commentary corpus. Statistics of\nour experimental corpora can be found in Table 1.\nEach data set was pre-processed by UNK ing all\nbut the top ≈20k words and lemmas by frequency.\nNo characters or afﬁxes were UNK ed. This step is\nnot strictly required—our model is, after all, ca-\npable of producing arbitrary words— but it speeds\nup training immensely by reducing the size of the\nword and lemma softmaxes. Since the morphol-\nogy and/or character-level embeddings can still\ncapture information about the original forms of\nthese words, the degradation in modelling perfor-\nmance is minimal.\nFor morphological analysis we use Omorﬁ2 for\nFinnish, the analyzer of Oﬂazer (1994) for Turk-\nish, and PyMorphy3 for Russian.\n3.1 Baseline Models\nSince models are not accurately comparable un-\nless they share output vocabularies, our baselines\nmust also allow for the generation of arbitrary\nword forms, including out-of-vocabulary items.\nWe compare to three such models: an improved\nKneser-Ney (Kneser and Ney, 1995) 4-gram base-\nline, with an additional character-level backoff\nmodel for OOVs, an RNNLM with character-level\nbackoff, and a pure character-based RNN lan-\nguage model (Sutskever et al., 2011).\n2https://github.com/flammie/omorfi\n3https://github.com/kmike/pymorphy\n1438\nvenäjänpresidentillävladimirputinillaon yksiässähihassaanukrainansuhteen.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context)\nvenäjänpresidentillävladimirputinillaon yksiässähihassaanukrainansuhteen.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context, w)\nFinnish: “Russian President Vladimir Putin has an ace up his sleeve in the Ukrainian relationship.”\nFigure 3: An example of the priors (left) and posteriors (right) over modes used to generate each word in\na sample sentences. Probability given to the word-, morpheme-, and character-level models are shown in\nred, blue, and gold respectively. More examples can be found in the appendix.\nSince Kneser-Ney language models (and other\ncount-based models) are typically word-level and\ndo not model out-of-vocabulary items, we employ\na two-level approach with separate Kneser-Ney\nmodels at the word and character levels. We train\nthe word-level model after UNK ing low frequency\nwords, and we train the character-level model on\nthe same list of low frequency words. Now when\nwe want to predict a word wi given some con-\ntext c we can use the word-level model to directly\npredict p(wi|c) unless wi is an out-of-vocabulary\nitem. In that case we model p(wi |c) as\np(wi |c) =p(UNK |c) ·p(wi |UNK )\nwhere the ﬁrst factor is the probability of the word-\nlevel model emitting UNK , and the second is the\nprobability of the actual out-of-vocabulary word\nform under the character-level model.\nSecondly we compare to a similar hybrid RNN\nmodel that ﬁrst predicts the word-level proba-\nbility for each word, and if it predicted UNK\nthen also predicts a sequence of characters us-\ning a separate network. This model uses 256-\ndimensional character and word embeddings, and\na 1024-dimensional recurrent hidden layer.\nFinally we also compare to a standard RNN lan-\nguage model trained purely on the character level.\nFor this baseline we also use 256-dimensional\ncharacter embeddings and a 1024-dimensional re-\ncurrent hidden layer.\n3.2 Multi-factored Models\nFor our model we use 128-dimensional word and\nroot embeddings, 64-dimensional afﬁx and char-\nacter embeddings, 128-dimensional word-internal\nrecurrent hidden layers for characters and mor-\nphemes, and a 256-dimensional recurrent hidden\nlayer for the main inter-word LSTM.\nThe network is trained to stochastically opti-\nmize the log likelihood of the training data using\nAdam (Kingma and Ba, 2014). After each 10k\ntraining examples (Finnish, Turkish) or 100k train-\ning examples (Russian) we evaluate the model on\na development set. 4 If the perplexity on the de-\nvelopment set represents a new best, we save the\ncurrent model to disk, thereby mitigating overﬁt-\nting via early stopping. No other regularization is\nused.\nFor each language we run four variants of our\nmodel. In order to preserve the ability to model\nand emit any word in the modelled language, it\nis essential that we keep the character-level part\nof our model intact. The morpheme- and word-\nlevel models, however, may be removed without\ncompromising the generality of the model. As\nsuch, we present our model using only character-\nlevel input and outputs (C), using character- and\nmorpheme-level inputs and outputs (CM), using\ncharacter- and word-level inputs, but no morphol-\nogy (CW), and using all three levels as per the full\nmodel (CMW).\n3.3 Results and Analysis\nOur experimental results (Table 2) show that our\nmulti-modal model signiﬁcantly outperforms all\nthree baselines: a naïve n-gram model, a purely\ncharacter-level RNNLM, and a hybrid RNNLM\n4We evaluate less frequently on Russian since the dev set\nis much larger.\n1439\nFinnish Russian Turkish\nTrain Sents 2.1M 1.1M 188K\nTrain Words 38M 26M 3.9M\nDev Sents 1K 38K 1K\nDev Words 16K 705K 16K\nTest Sents 500 91K 3K\nTest Words 7.6K 1.6M 51K\nWord V ocab 20K 21K 42K\nLemma V ocab 20K 20K 13K\nAfﬁx V ocab 140 34 180\nChar V ocab 229 150 80\nTable 1: Details of our data sets. Each cell indi-\ncates the number of sentences and the number of\nwords in each set.\nfor open-vocabulary language models. Further-\nmore, they conﬁrm that morphological analyz-\ners can improve performance of such language\nmodels on particularly morphologically rich lan-\nguages. We observe that across all three lan-\nguages the space-aware character-level model out-\nperforms the purely character-based model that\ntreats spaces just as any other character. Further-\nmore we see that the Kneser-Ney language model\nperforms admirably well on this task, underscor-\ning the difference in setting between the familiar,\ntraditional closed-vocabulary LMs, and the open-\nvocabulary language modelling task. Additionally\nwe ﬁnd that the relative success of the n-gram\nmodel and the hybrid model over the character\nonly models underscores the importance of access\nto word-level information, even when using a less\nsophisticated model.\nTable 3 shows some examples of sentences on\nwhich our model heavily outperforms the RNN\nbaseline and vice-versa. We ﬁnd that the sentences\non which our model peforms well contain much\nless frequent word forms. For each sentence we\nexamine the frequency with which each token ap-\npears in our training corpus. The sentences on\nwhich our model performs best have a median to-\nken frequency of 305 times, while the sentences\nwhere the RNN performs better has an average to-\nken frequency of 3031 times. Overall our model\nhas better log-likelihood than the RNN baseline on\n88.1% of sentences.\nOur methods outperform the n-gram model in\nall languages with either set of just two mod-\nels, CM or CW. The same models outperform\nthe hybrid baseline in Turkish and Russian, and\nachieves comparable results in Finnish. Finally, in\nthe agglutinative languages, using all three modes\nperforms best, while in Russian, a fusional lan-\nguage, characters and words alone edge out the\nmodel with morphology. We hypothesize that\nour morphology model is better able to model the\nlong strings of morphemes found in Turkish and\nFinnish, but gains little from the more idiosyn-\ncratic fusional morphemes of Russian.\nSome examples of the priors and posteriors of\nthe modes used to generate some randomly se-\nlected sentences from the held out test set can be\nseen in Figure 3 and the appendix (Figure 4). The\nﬁgures show that all of the models tend a priori\nto prefer to generate words directly when possi-\nble, but that context can certainly inﬂuence its pri-\nors. In Finnish, after seeing the word Vladmir, the\nmodel suddenly assigns signiﬁcantly more weight\nto the character-level model to generate the fol-\nlowing word, which is likely to be a surname.\nIn Russian, after the preposition o, the following\nnoun is required to be in a rare case. As such,\nthe model suddenly assigns more probability mass\nto the following word being generated using the\nmorpheme-level model.\nThe posteriors tell a similarly encouraging\nstory. In Finnish we see that the word presiden-\ntillä is overwhelmingly likely to be produced by\nthe morphology model due to its peculiar adessive\n(“at”) case marking. Vladmir is common enough\nin the data that it can be generated wholly, but the\nlast name Putin is again inﬂected into the adessive\ncase, forming Putinilla. Unfortunately the mor-\nphological analyzer is unfamiliar with the stem\nPutin, forcing the word to be generated by the\ncharacter-level model. In our Turkish example, all\nof the short words are generated at the word level,\nwhile the primary nouns internetten (“internet”)\nand ders (“lecture”) are possible to generate ei-\nther as words or as a sequence of morphemes. The\nverb, which has much more complex morphology\n(progressive past tense with a third person singular\nagent), is generated via the morphological model.\n4 Extrinsic Evaluation\nIn addition to evaluating our model intrinsically\nusing perplexity, we evaluate it on two down-\nstream tasks. The ﬁrst is machine translation be-\ntween English and Turkish. The second is Turkish\nmorphological analysis disambiguation.\n1440\n(a) Finnish\nModel Dev Test\nKN4+OOV 2.04 1.94\nRNN+OOV 2.03 1.92\nPureC 2.69 2.63\nC 2.40 2.32\nCM 1.95 1.85\nCW 2.03 1.94\nCWM 1.91 1.81\n(b) Turkish\nModel Dev Test\nKN4+OOV 2.01 2.06\nRNN+OOV 1.99 2.05\nPureC 2.21 2.30\nC 2.05 2.16\nCM 1.88 1.99\nCW 1.78 1.85\nCWM 1.74 1.82\n(c) Russian\nModel Dev Test\nKN4+OOV 1.68 1.70\nRNN+OOV 1.62 1.66\nPureC 1.91 2.05\nC 1.85 1.87\nCM 1.47 1.50\nCW 1.44 1.47\nCWM 1.49 1.52\nTable 2: Intrinsic evaluation of language models for three morphologically rich languages. Entropy for\neach test set is given in bits per character on the tokenized data. Lower is better, with 0.0 being perfect.\nKomünizm pe¸ sinde ko¸ sanArnavut pek yok\nParlaklı˘gınıkaybeden mücevher: Kirlilik Karadeniz’iesir alıyor\nOlaylarınbaskısıyla kar¸ sıla¸ sanrejim tutumunu yava¸ sça yumu¸ sattı, 1991 yılında çok partili\n→seçimleri düzenledi ve sonunda da ertesi yıltümden iktidarı bıraktı.\nSoutheast European Times için Belgrad’dan Dusan Kosanoviç’in haberi - 24/06/04\n23 Temmuz’dan bu yana Balkanlar’la ilgili i¸ s ve ekonomi haberlerine genel bakı¸ s:\nAB’nin Geni¸ slemeden Sorumlu Komisyon Üyesi Olli Rehn (solda) Arnavutluk Ba¸ sbakanı Sali\n→Beri¸ sa ile 15 Mart Per¸ sembe günü Tiran’da bir araya geldi.\nTable 3: Some examples of Turkish sentence on which our morphological model heavily outperforms\nthe baseline RNNLM (top) and some examples of the opposite (bottom). The sentences that our model\nperforms well on have many particularly rare words, whereas the sentences the RNNLM performs well\non were seen hundreds or thousands of times in the training corpus. Words in bold were seen fewer than\n25 times in the training corpus. Arrows indicate line wrapping.\n4.1 Machine Translation\nAs an extrinsic evaluation we test whether our\nlanguage model improves machine translation be-\ntween Turkish and English. While we could trans-\nform our model into a source-conditioned trans-\nlation model, we choose here to focus on testing\nour model as an external unconditional language\nmodel, leaving the conditional version for future\nwork. Since neural machine translation systems\nstruggle with low-resource languages (Koehn and\nKnowles, 2017), we choose to introduce the score\nof our LM as an additional feature to a cdec (Dyer\net al., 2010) hierarchical MT system. We train on\nthe WMT 2016 Turkish–English data set, and per-\nform n-best reranking after re-tuning weights with\nthe new feature.\nThe results, shown in Table4 demonstrate small\nbut signiﬁcant gains in both directions, particu-\nlarly into Turkish, where modelling productive\nmorphology should be more important.\n4.2 Morphological Disambiguation\nOur model is a joint model over words and the\nlatent processes giving rise to those words (i.e.,\nwhich generation process was selected and, for the\nLang. Pair System BLEU\nTR-EN Baseline 15.0\nMorph. Input 15.2\nEN-TR Baseline 10.1\nMorph. Output 10.5\nTable 4: Machine Translation Results\nmorpheme process, which morpheme sequence\nwas generated). While our model is not directly\ntrained to perform morphological disambiguation,\nit still performs this task quite admirably. Given\na trained morphological language model, a sen-\ntence s, and a set of morphological analyses\nz, we can query the model to ﬁnd p(s, z) =\np(w1, w2, . . . , wN ) for a given sentence. Most no-\ntably, each wi may have a set of possible mor-\nphological analyses {a1, a2, . . . aMi }from which\nwe would like to choose the most likely a pos-\nteriori. To perform this task, we simply query\nthe model Mi times, each time hiding all but\nthe jth possible analysis from the model. We\ncan then re-normalize the resulting probabilities to\nﬁnd p(aj|s) for each j ∈1 . . . Mi.\nTo make training and inference with our model\n1441\nModel Supervised? Ambiguous Words All words\nRandom Chance no 34.08% 52.66%\nUnidirectional no 55.15% 80.28%\nBidirectional no 63.85% 84.11%\nShen et. al yes 91.03% 96.43%\nTable 5: Morphological disambiguation accuracy results for Turkish.\ntractable, we have assumed independence between\nprevious adjacent events and the next word gen-\neration given the previous surface word forms\n(§2.1). Thus, the posterior probability over the\nanalysis is only determined by the left context—\nsubsequent decisions are independent of the pro-\ncess used to generate a word at time t. However,\nsince disambiguating information may be present\nin either direction, we introduce a model variant\nthat conditions on information in both directions.\nBidirectional dependencies mean that we can no\nlonger use the chain rule to factor the probability\ndistribution from left-to-right. Rather we have to\nswitch to a globally normalized, undirected model\n(i.e., a Markov random ﬁeld) to deﬁne the prob-\nabilities of selecting the mode of generation and\ngeneration probability (conditional on the mode).\nThe factors used to parameterize the model are de-\nﬁned in terms of two LSTMs, one encoding from\nleft-to-right the preﬁx of the ith word (hi, deﬁned\nexactly as above), and a second encoding from\nright-to-left its sufﬁx ( h′\ni). These two vector rep-\nresentations are used to compute a score using a\nlocally normalized mixture model for each word.\nIntuitively, the morphological analysis generated\nat each time step should be compatible with both\nthe preceding words and the following words.\nOptimizing this model with the same MLE cri-\nterion we used in the direct model is, unfortu-\nnately, intractable since a normalizer would need\nto be computed. Instead, we use a pseudo-\nlikelihood objective (Besag, 1975).\nLPL =\n∏\ni\np(wi |w−i)\n=\n∏\ni\n∑\nm\np(mi = m |w−i)p(wi |m, w−i)\nWe note that although this model has a very differ-\nent semantics from the directed one, the PL train-\ning objective is identical to the directed model’s,\nthe only difference is that features are based both\non the past and future, rather than only the past.\nSimilarly to training, evaluating sentence like-\nlihoods using this model is intractable, but poste-\nrior inference over mi and ai is feasible since the\nnormalization factors cancel and therefore do not\nneed to be computed.\nFor our experiments we use the data set of\nYuret and Türe (2006) who manually disam-\nbiguated from among the possible forms identiﬁed\nby an FST. We signiﬁcantly out-perform the sim-\nple baseline of randomly guessing, and our results\nare competitive with Yatbaz and Yuret (2009), al-\nthough they evaluated on a different dataset so\nthey are not directly comparable. Furthermore, we\nalso compare to a supervised model (Shen et al.,\n2016). While unsupervised techniques can’t hope\nto exceed supervised accuracies, this comparison\nprovides insight into the difﬁculty of the problem.\nSee Table 5 for results.\n5 Related Work\nPurely Character-based or Subword-based\nLMs have a rich history going all the way\nback to Markov (1906)’s work modelling Russian\ncharacter-by-character with his namesake models.\nMore recently Sutskever et al. (2011) were the ﬁrst\nto apply RNNs to character-level language mod-\nelling, leveraging their ability to handle the long-\nrange dependencies required to model language at\nthe character level. It is also possible to alleviate\nthe closed vocabulary problem by training models\non automatically acquired subword units (Mikolov\net al., 2012; Sennrich et al., 2015). While these ap-\nproaches allow for an open vocabulary (or nearly\nopen, in the case of subwords) they discard a\nlarge amount of higher-level information, inhibit-\ning learning.\nCharacter-aware language models, which\ncombine character- and word-level information\nhave shown promise (Kang et al., 2011; Ling\net al., 2015; Kim et al., 2016). Unsupervised\nmorphology has also been shown to improve the\nrepresentations used by a log-bilinear LM (Botha\nand Blunsom, 2014). Jozefowicz et al. (2016)\nexplore many interesting such architectures,\nand compare with fully character-based models.\n1442\nWhile these models allow for the elegant encoding\nof novel word forms they lack an open vocabulary.\nOpen-vocabulary hybrid models alleviate this\nproblem, extending the beneﬁts of character-level\nrepresentations to the generation. Such hybrid\nmodels with open vocabularies have been around\nsince Brown et al. (1992). More recently, Chung\net al. (2016) and Hwang and Sung (2016) describe\nmethods of modelling sentences at both the word\nand character levels, using mechanisms to allow\nboth a word-internal model that captures short-\nrange dependencies and a word-external model to\ncapture longer-range dependencies. These mod-\nels have been successfully applied to machine\ntranslation by Luong and Manning (2016), who\nuse a character-level model to predict translations\nof out of vocabulary words. Our work falls in\nthis category—we combine multiple representa-\ntion levels while maintaining the ability to gener-\nate any character sequence. In contrast to these\nprevious works, we demonstrate the utility of in-\ncorporating morphological information in these\nopen-vocabulary models.\nMixture model language generation where the\nmixture coefﬁcients are predicted by a neural net\nare becoming quite common. Neubig and Dyer\n(2016) use this strategy to combine a count-based\nmodel and a neural language model. Ling et al.\n(2016) interpolate between character- and word-\nbased models to translate between natural lan-\nguage text and computer code. Merity et al. (2016)\nalso use multiple output models, allowing a word\nto either be generated by a standard softmax or by\ncopying a word from earlier in the input sentence.\n6 Conclusion\nWe have demonstrated a technique for language\nmodelling that works particularly well on mor-\nphologically rich languages where having an open\nvocabulary is desirable. We achieve this by us-\ning a multi-modal architecture that allows words\nto be input and output at the word, morpheme,\nor character levels. We show that knowledge of\nthe existence of word boundaries is of critical im-\nportance for language modelling tasks, even when\notherwise operating entirely at the character level,\nresulting in a surprisingly large reduction in per-\ncharacter entropy across all languages studied.\nFurthermore, we demonstrate that if we have ac-\ncess to a morphological analyzer we can leverage\nit to further improve our LM, reinforcing the no-\ntion that the explicit inclusion of linguistic infor-\nmation can indeed aid learning of neural models.\nAcknowledgements\nWe would like to thank Sebastian Mielke for his\ninsightful discussion and feedback on this work.\nThis work is sponsored by Defense Advanced\nResearch Projects Agency Information Innovation\nOfﬁce (I2O). Program: Low Resource Languages\nfor Emergent Incidents (LORELEI). Issued by\nDARPA/I2O under Contract No. HR0011-15-\nC0114. The views and conclusions contained in\nthis document are those of the authors and should\nnot be interpreted as representing the ofﬁcial poli-\ncies, either expressed or implied, of the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation\nhere on.\nReferences\nJulian Besag. 1975. Statistical analysis of non-lattice\ndata. The statistician pages 179–195.\nJan A Botha and Phil Blunsom. 2014. Compositional\nmorphology for word representations and language\nmodelling. In ICML. pages 1899–1907.\nPeter F Brown, Vincent J Della Pietra, Robert L Mer-\ncer, Stephen A Della Pietra, and Jennifer C Lai.\n1992. An estimate of an upper bound for the entropy\nof english. Computational Linguistics 18(1):31–40.\nVictor Chahuneau, Noah A Smith, and Chris Dyer.\n2013. Knowledge-rich morphological priors for\nbayesian language models. Association for Com-\nputational Linguistics.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704 .\nChris Dyer, Jonathan Weese, Hendra Setiawan, Adam\nLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-\nitkevitch, Phil Blunsom, and Philip Resnik. 2010.\ncdec: A decoder, alignment, and learning framework\nfor ﬁnite-state and context-free translation models.\nIn Proceedings of the ACL 2010 System Demon-\nstrations. Association for Computational Linguis-\ntics, pages 7–12.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735–1780.\nKyuyeon Hwang and Wonyong Sung. 2016. Character-\nlevel language modeling with hierarchical recurrent\nneural networks. arXiv preprint arXiv:1609.03777 .\n1443\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410 .\nMoonyoung Kang, Tim Ng, and Long Nguyen. 2011.\nMandarin word-character hybrid-input neural net-\nwork language model. In INTERSPEECH. pages\n625–628.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Proceedings of the Thirtieth AAAI\nConference on Artiﬁcial Intelligence, February 12-\n17, 2016, Phoenix, Arizona, USA. . pages 2741–\n2749.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nReinhard Kneser and Hermann Ney. 1995. Im-\nproved backing-off for m-gram language model-\ning. In Acoustics, Speech, and Signal Processing,\n1995. ICASSP-95., 1995 International Conference\non. IEEE, volume 1, pages 181–184.\nPhilipp Koehn and Rebecca Knowles. 2017. Six\nchallenges for neural machine translation. arXiv\npreprint arXiv:1706.03872 .\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tomáš Ko ˇcisk`y, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2016. Latent predic-\ntor networks for code generation. arXiv preprint\narXiv:1603.06744 .\nWang Ling, Tiago Luís, Luís Marujo, Ramón Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. arXiv preprint\narXiv:1508.02096 .\nMinh-Thang Luong and Christopher D Manning. 2016.\nAchieving open vocabulary neural machine trans-\nlation with hybrid word-character models. arXiv\npreprint arXiv:1604.00788 .\nAndrey Andreyevich Markov. 1906. Extension of the\nlaw of large numbers to dependent quantities. Izv.\nFiz.-Matem. Obsch. Kazan Univ.(2nd Ser) 15:135–\n156.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843 .\nTomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, and Stefan Kombrink. 2012. Subword lan-\nguage modeling with neural networks .\nYasumasa Miyamoto and Kyunghyun Cho. 2016.\nGated word-character recurrent language model.\narXiv preprint arXiv:1606.01700 .\nGraham Neubig and Chris Dyer. 2016. Generalizing\nand hybridizing count-based and neural language\nmodels. arXiv preprint arXiv:1606.00499 .\nKemal Oﬂazer. 1994. Two-level description of turk-\nish morphology. Literary and linguistic computing\n9(2):137–148.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909 .\nQinlan Shen, Daniel Clothiaux, Emily Tagtow, Patrick\nLittell, and Chris Dyer. 2016. The role of context in\nneural morphological disambiguation .\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International\nConference on Machine Learning (ICML-11). pages\n1017–1024.\nMehmet Ali Yatbaz and Deniz Yuret. 2009. Unsu-\npervised morphological disambiguation using sta-\ntistical language models. In Pro. of the NIPS\n2009 Workshop on Grammar Induction, Representa-\ntion of Language and Language Learning, Whistler,\nCanada. pages 321–324.\nDeniz Yuret and Ferhan Türe. 2006. Learning mor-\nphological disambiguation rules for turkish. In Pro-\nceedings of the main conference on Human Lan-\nguage Technology Conference of the North Amer-\nican Chapter of the Association of Computational\nLinguistics. Association for Computational Linguis-\ntics, pages 328–334.\n1444\nA Example sentences\nvenäjänpresidentillävladimirputinillaon yksiässähihassaanukrainansuhteen.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context)\nvenäjänpresidentillävladimirputinillaon yksiässähihassaanukrainansuhteen.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context, w)\nFinnish: “Russian President Vladimir Putin has an ace up his sleeve in the Ukrainian relationship.”\nbu y l\n sadeceinternetteniki dersveriyordu.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context)\nbu y l\n sadeceinternetteniki dersveriyordu.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context, w)\nTurkish: “He gave only two lectures on the internet this year.”\n.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context)\n.0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9p(mode | context, w)\nRussian: “The investigation does not theorize about the attacker’s motives.”\nFigure 4: Some examples of the priors (left) and posteriors (right) over modes used to generate each word\nin some sample sentences. Probability given to the word-, morpheme-, and character-level models are\nshown in red, blue, and gold respectively. The Finnish example is a reprint of Figure 3.\n1445"
}