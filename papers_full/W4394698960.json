{
    "title": "Toward Accurate Infrared Small Target Detection via Edge-Aware Gated Transformer",
    "url": "https://openalex.org/W4394698960",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2137655535",
            "name": "YiMing Zhu",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2114285212",
            "name": "Yong Ma",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2104945026",
            "name": "Fan Fan",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2097175565",
            "name": "Jun Huang",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2807731379",
            "name": "Kangle Wu",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2116246449",
            "name": "Ge Wang",
            "affiliations": [
                "Wuhan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2407220925",
        "https://openalex.org/W4382401167",
        "https://openalex.org/W4313318703",
        "https://openalex.org/W3208364069",
        "https://openalex.org/W3088317060",
        "https://openalex.org/W4289792517",
        "https://openalex.org/W3109262695",
        "https://openalex.org/W4319303166",
        "https://openalex.org/W2006851788",
        "https://openalex.org/W1992873714",
        "https://openalex.org/W3097913527",
        "https://openalex.org/W3128049713",
        "https://openalex.org/W2041560658",
        "https://openalex.org/W2341998679",
        "https://openalex.org/W3178409548",
        "https://openalex.org/W2604768956",
        "https://openalex.org/W1978993121",
        "https://openalex.org/W2900678331",
        "https://openalex.org/W2912919760",
        "https://openalex.org/W4281681392",
        "https://openalex.org/W2780449407",
        "https://openalex.org/W3127708505",
        "https://openalex.org/W3118249006",
        "https://openalex.org/W4313065862",
        "https://openalex.org/W3171950886",
        "https://openalex.org/W4308193099",
        "https://openalex.org/W3010079414",
        "https://openalex.org/W3168491317",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4381415904",
        "https://openalex.org/W3125738281",
        "https://openalex.org/W4386898833",
        "https://openalex.org/W3118934234",
        "https://openalex.org/W4313506322",
        "https://openalex.org/W4226043641",
        "https://openalex.org/W4387623848",
        "https://openalex.org/W4317794940",
        "https://openalex.org/W4380902443",
        "https://openalex.org/W4386634626",
        "https://openalex.org/W4387831814",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3211135382",
        "https://openalex.org/W4220815323",
        "https://openalex.org/W4225113999",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W4376109647",
        "https://openalex.org/W4387951721",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2545065926",
        "https://openalex.org/W2734349601",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W4235187403"
    ],
    "abstract": "Extracting small targets from complex backgrounds is the eventual goal of single-frame infrared small target detection, which has many potential applications in defense security and marine rescue. Recently, methods utilizing deep learning have shown their superiority over traditional theoretical approaches. However, they do not consider both the global semantics and specific shape information, thereby limiting their performance. To overcome this proplem, we propose a gated-shaped TransUnet (GSTUnet), designed to fully utilize shape information while detecting small target detection. Specifically, we have proposed a multiscale encoder branch to extract global features of small targets at different scales. Then, the extracted global features are passed through a gated-shaped stream branch that focuses on the shape information of small targets through gate convolutions. Finally, we fuse their features to obtain the final result. Our GSTUnet learns both global and shape information through the aforementioned two branches, establishing global relationships between different feature scales. The GSTUnet demonstrates excellent evaluation metrics on various datasets, outperforming current state-of-the-art methods.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nTowards Accurate Infrared Small Target Detection\nvia Edge-Aware Gated Transformer\nYiming Zhu, Yong Ma, Fan Fan*, Jun Huang, Kangle Wu, and Ge Wang\nAbstract—Extracting small targets from complex backgrounds\nis the eventual goal of single infrared small target detection\n(SISTD), which has many potential applications in defense\nsecurity and marine rescue. Recently, methods utilizing deep\nlearning have shown their superiority over traditional theoretical\napproaches. However, they do not consider both the global\nsemantics and specific shape information, thereby limiting their\nperformance. To overcome this proplem, we propose a Gated\nShaped TransUnet (GSTUnet), designed to fully utilize shape\ninformation while detecting small target detection. Specifically,\nwe have proposed a multi-scale encoder branch to extract\nglobal features of small targets at different scales. Then, the\nextracted global features are passed through a gated-shaped\nstream branch that focuses on the shape information of small\ntargets through gate convolutions. Finally, we fuse there features\nto obtain the final result. Our GSTUnet learns both global and\nshape information through the aforementioned two branches,\nestablishing global relationships between different feature scales.\nThe GSTUnet demonstrates excellent evaluation metrics on\nvarious datasets, outperforming current state-of-the-art methods.\nTo access our code and datasets, please visit the following link:\nhttps://github.com/\nIndex Terms—Infrared small target, swin transformer, gated-\nshaped stream.\nI. I NTRODUCTION\nS\nINGLE-FRAME infrared small target detection(SISTD)\nis a critical task that separates small and dim targets\nfrom complex backgrounds like the sky, ocean, and urban\nstructures. It plays an essential role in various fields, encom-\npassing defense security [1], [2], maritime surveillance [2]–\n[4], and precision guidance [2], [5]. Nevertheless, it poses\nparticular challenges. As shown in the red boxes in Fig.\n1(a) and (b), small targets occupy only a small portion of\nthe pixels, and their low signal-to-clutter ratios (SCRs) cause\nthem susceptible to blending with complex backgrounds [6].\nAdditionally, as shown in Fig.1(c) and (d), small targets lack\ntexture information, rending traditional object detection meth-\nods that focus on this information become not well feasible\n[7]. Meanwhile, as shown in Fig. 1, the shape and size of\ntargets vary tremendously (5-50 pixels) in different scenarios\nand backgrounds, potentially resulting in missing detection,\nfalse alarm, inaccurate localization, etc. Consequently, SISTD\nposes a challenging problem. In addressing these challenges,\nThis work was supported by the National Natural Science Foundation of\nChina (No. 62075169, 62003247 and 62061160370) and the Key Research\nand Development Program 2021BBA235 of Hubei Province.\nThe authors are with the Electronic Information School, Wuhan\nUniversity, Wuhan, 430072, China (e-mail: mayong@whu.edu.cn, jun-\nhwong@whu.edu.cn, fanfan@whu.edu.cn(corresponding author)).\nManuscript received April 19, 2005; revised August 26, 2015.\nFig. 1. Unique challenges in SISTD are depicted with the image name from\nthe NUAA-SIRST [8] dataset in the bottom left corner, and the red box\nrepresents a local zoom. (a) Small and gray point target against terrestrial\nbackgrounds. (b) Point target with low Signal-to-Clutter Ratios(SCRs) against\nsky backgrounds. (c) Small target without texture information but with distinct\nshape in sky backgrounds. (d) Small targets with discernible shape information\nagainst ocean backgrounds.\nit is necessary to devise a method, particularly on learning\nshape feature to facilitate the accurate detection of infrared\nsmall targets.\nTraditional SISTD methods can be classified into three\nclasses, including filter-based, human vision system(HVS)-\nbased, and low-rank matrix(LRM)-based methods. Filter-based\nmethods [9]–[12] employ specifically designed filters to extract\nsmall target from the backgrounds. While they are effective\nin filtering out smooth backgrounds clutter, their performance\ndegrades significantly when encountering noise and back-\nground interference of varying intensity. HVS-based methods\n[6], [7], [13]–[15] rely on the local brightness and darkness\ncontrast difference between targets and backgrounds, making\nthem particularly suitable for the detecting small targets with\nrelatively high brightness. However, in the presence of strong\nbright noise in the backgrounds, they may result in higher\nfalse alarm rates. LRM-based methods [16]–[20] treat the\ninfrared image as a low-rank sparse matrix, and introduce the\nlow-rank matrix reconstruction to filter the targets with the\nbackgrounds. Since the intensity of the target is not significant\nwith respect to the intensity of the backgrounds, these methods\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\ndo not detect targets under various shapes well. Overall,\nWhen dealing with complex scenarios, the traditional methods\nheavily rely on manual features, which makes it difficult to\ncope with complex scenes.\nDeep learning methods [21]–[27] have revolutionized the\nfield of SISTD by applying data-driven strategies, which are\nwidely used due to their ability to learn infrared small target\nfeatures. Among deep learning-based methods, both CNN-\nbased and Transformer-based approaches have been explored.\nCNN-based methods have achieved excellent results by im-\nproved convolution for small target scenarios. However, the\nconvolution operations have limited local receptive fields [28],\nand the introduced pooling operations will ignore small targets\nduring the downsampling process [24]. Transformer-based\napproaches address these issues by integrating global receptive\nfields and self-attention mechanisms, but they currently do not\nspecifically consider local feature information such as edge\nand shape, which may not only lead to miss detection, but also\naffect the recognition of target types in practical applications\n(e.g. Fig. 1 (c) and (d) can be further identified as UA V and\nShip). Due to the low contrast and low SCR between infrared\nsmall targets and the background, it is challenging to extract\nuseful edge and shape features of the targets. Thus, obtaining\naccurate edges and shapes of infrared small targets remains a\nchallenging task.\nTo overcome the aforementioned drawbacks, we consider\nto explore a novel paradigm of incorporating target shape\nlearning into SISTD. we propose Gated Shaped TransUnet\n(GSTUnet) for SISTD through semantic segmentation. The\nfeature extractor of GSTUnet consists of two key branches,\nin the first branch, we proposed a multi-scale feature ex-\ntraction network built on Swin transformer [29] to overcome\nthe receptive field limitation of convolution operations. This\nenables the extraction of global semantic features of small\ntargets at different scales. In the second branch, we design a\nedge-aware gated shaped stream that focuses specifically on\ncapturing the shape and edge information of small targets by\nusing a gated convolution layer composed of two convolution\nlayers and a sigmoid layer with ResNet [30]. Furthermore,\nGSTUnet integrates the semantic and shape information from\nthe aforementioned two branches and establishes global de-\npendencies among different feature scales, allowing for highly\ndiscriminate detection of small infrared targets, and to obtain\nthe exact shape of the target. The contributions of our work\ncan be summarized as follows:\n1) We propose a novel architectural paradigm that in-\ncorporates and meticulously considers edge and shape\ninformation within the ambit of SISTD.\n2) We have engineered an edge-aware gating mechanism\narchitecture that adeptly extracts edge and shape features\nof diminutive targets.\n3) We propose an edge-aware loss function specifically\ntailored to accentuate the extraction of edge and shape\ninformation against complex backgrounds while concur-\nrently suppressing false alarms.\n4) We have conducted extensive experiments on relevant\nbenchmarks, substantiating the efficacy of our method,\nwhich has culminated in state-of-the-art (SOTA) results.\nII. R ELATED WORK\nA. Single Infrared Small Target Detection\nTraditional SISTD methods are based on maintaining con-\nsistency in backgrounds and enhancing the salience of small\ninfrared targets, relying mainly on manual features such as\nHVS-based, filter-based, and LRM-based methods.\nHVS-based methods primarily rely on the salience of local\ngray value contrast between small targets and backgrounds.\nFor instance, Chen et al. [13] introduced the local contrast\nmeasure (LCM), and Wei et al. [14] further considered\nthe multi-scale feature extraction and designed a multi-scale\npatches contrast module(MPCM) to enhance the LCM for\ncomputing the salience of small targets salience. However,\nMPCM neglects global information, prompting Qiu et al.[6] to\ndeveloped the global structure-based LCM (GSLCM), which\nappropriately incorporates global information. Building upon\nGSLCM, Qiu et al. [7] also proposed an adaptive structure\npatch contrast Measure(ASPCM). However, the performance\nof HVS-based methods degrades significantly in the presence\nof noise or clutter in the image.\nFilter-based methods focus on suppressing backgrounds\nclutter, For instance, Bai et al.[9] proposed the Top-Hat filter\nas a filtering-based method. Li et al. [12] utilize both max-\nmean/max-median filter to detection small targets. However,\nthis type of methods have some limitations, such as the\nmodel parameters enable to automatically adjusted on complex\nbackgrounds, making it suitable only for single backgrounds\nand uniform scenes.\nLRM-based methods have been developed to utilize the\nproperties of targets and backgrounds for decomposing the\noriginal image into target and backgrounds components. For\ninstance, Dai et al. [16] proposed the re-weighted infrared\npatch tensor, and Wuet al.[31] proposed a tensor train/ring ex-\npansion methods, which utilize alternating direction multiplier\nmethod (ADMM) to expand the components by weighting\nthe kernel norm and balancing the constraints. While LRM-\nbased methods can achieve superior detection performance,\nthey often require multiple iterations to converge and attain\noptimal solutions, resulting in a less efficient process.\nTraditional methods are highly interpretable and can provide\nsatisfactory results without the need for large amounts of\ndata. However, they rely on prior assumptions and exhibit low\nrobustness in complex backgrounds.\nB. CNN-based Infrared Detection Framework\nWith the development of CNN, which enable the extrac-\ntion of features from data-driven and provide end-to-end\nprocessing, CNN-based methods have surpassed traditional\napproaches in suppressing false alarms, more precision, and\nenhancing robustness. For instance, Ju et al.[32] used ISTDet,\nwhich consists of an image filtering module and an infrared\nsmall target detection module. Dai et al. [8] proposed a one-\nstage cascade refinement network that cascades the CNN\nfeatures for detecting small targets in infrared images. Qian\n[33] et al. proposed SiamIST, which combines side window\nfilter with the improved SiamRPN model. Han et al. [3], [4]\nporposed the Efficient Information Reuse Network (EIRNet)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nin marine backgrounds, which combines a dense feature fusion\nnetwork (DFF-Net) with two fusion directions in feature\nextraction, and a Dual Mask Attention Module (DMAM) to\nrefine the fused feature map. Meanwhile, facing the special\nsituation of small samples, they proposed a Balanced Feature\nFusion Network (BFF-Net) and a Context Attention Network\n(CA-Net). However, object detection techniques only provide\nbounding box information and lack detailed shape information,\nwhich is important for further analysis and understanding of\ntarget characteristics in the SISTD.\nSince the semantic segmentation can provide detailed seg-\nmentation results at the pixel level, where the shape of\ntargets can be more accurately detected, addressing the afore-\nmentioned issue. Therefore, some works modeled SISTD\nas a semantic segmentation task. [23]–[25], [27], [34]–[40].\nFor instance, Dai et al. [23] [34] consider the feature of\nsmall targets and construct up-sampling asymmetric contextual\nmodulation(ACM) and attention local contrast(ALC) modules,\nintegrating them into CNN structure. Zhang et al. [38] pro-\nposed attention guided pyramid context network(AGPCNet),\nutilizing attention mechanisms to explore contextual infor-\nmation and preserve detailed information. Kou et al. [39],\n[40]proposed a multi strategy fusion model that integrates\nvarious class convolution modules, including post-processing\nof eight neighborhood clustering, achieved real-time infrared\nsmall target detection and tracking. As for the shape feature\nlearning, Zhang et al. [24]designed an end-to-end Taylor\ndifferential operator for improved edge and shape detection.\nLin [41] proposed a framework to fully consider shaped-biased\nlearning for accurate detect infrared small target. However, the\naforementioned methods are all based on CNN, which suffers\nfrom a limited ability to focus on global information due to\nconvolution and multiple down-sampling operations through\npooling in the network. To address this issue, a suitable feature\nextractor of framework should be employed.\nC. Transformer-based Infrared Detection Framework\nThe vision transformer(ViT) [42] has proven highly ef-\nfective in computer vision tasks. To extract cross-windows\nfeatures, the Swin Transformer [29] was developed using\nsliding window attention and has achieved satisfaction results\ntasks such as image classification [43], [44], object detection\n[26], [45], [46], and semantic segmentation [47], [48]. The\nSwin transformer consists of two series blocks [29]. The first\nblock, shown in the left part of Fig. 2, utilizes a W-MSA\nmodule, while the second block, shown in the right part of\nFig. 2, utilizes the SW -MSA module. The W-MSA and\nSW -MSA modules employ windows attention and sliding\nwindows attention, respectively. The W-MSA layer separates\nthe input infrared image into non-overlapping windows, with\neach window having a size of M ×M. The different windows\nsplitting strategies aim to minimize the effects of windows\nlocalization, as illustrated in Fig. 4. The W-MSA module uti-\nlizes localized windows to compute the self-attention of each\nindividual window, as shown in Figure 4(a). The SW -MSA\nlayer is designed to facilitate the creation and extraction of\nadequate global information between windows without intro-\nducing additional computation costs. The SW -MSA layer\nFig. 2. Two successive Swin transformer blocks. W-MSA and SW-MSA are\nthe multi-head self-attention modules with windows and sliding windows self-\nattention.\nemploys the sliding window method, as shown in Fig. 4(b).\nThe SW -MSA employs the sliding window method and\niterates from upper left to bottom right. Additionally, the\nSwin transformer module consists of a multi-Layer perception\n(MLP ) module, a layer-norm ( LN) module, and residual\nconnections. The computational procedure for two consecutive\nswin transformer is as follows:\nˆzl+1 = SW -MSA (LN(zl)) +zl,\nzl+1 = MLP (LN(ˆzl+1)) + ˆzl+1,\nˆzl = W-MSA (LN(zl−1)) +zl−1,\nzl = MLP (LN(ˆzl)) + ˆzl,\n(1)\nwhere ˆzl represents the output of W-MSA or SW -MSA\nmodule of l-th block and zl represents the output of MLP\nmodule.\nThe Swin transformer computes self-attention from the\nfeature map using batch windows. Each batch is composed of\nmultiple non-adjacent sub-windows, as shown in Fig. 4. The\npatches that include small targets are marked with red bound-\ning boxes, and self-attention can be computed in different sub-\nwindows. The calculation of self-attention in a sub-window is\nas follows:\nQ, K, V ={TWq, TWk, TWv}\nAttention(Q, K, V) =Softmax (QKT√\nd\n+ B)V, (2)\nwhere T ∈ RM2×d represents the input of self-attention,\nand Wq, Wk, Wv ∈ Rd×d represent the weights of three\nlinear projection layers, which implemented using a MLP ,\nQ, K, V ∈ RM2×D indicates the query, key and value\nmatrices, respectively. M2 denotes the size of T, Q, K, and\nV and D denote the channel dimensions. In Swin transformer,\na bias matrix ˆB ∈ R(2M−1)×(2M−1) is parameterized, and the\nvalues of B are taken from this bias matrix.\nBuilding upon ViT for SISTD, Lin [26] et al. proposed U-\nTransformer based on Swin transformer and CNNs, which has\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nFig. 3. The overall structure of our GSTUnet, the upper-left is a multi-scale feature extraction module, the bottom part is a gated-shaped stream, and the\nupper-right is feature fusion module and up-sampling module.\ndemonstrated satisfactory results in SISTD. Lin [49] et al.pro-\nposed IST-TransNet combines vision transformer with CNN\nblocks anti-aliasing contextual feature fusion (ACFF) module\nand spatial and channel attention (SCA) module. Liu [50]\net al. proposed a ViT-based feature embedding module with\na feature enhancement module to learn discriminate features\nof small targets and prevent missed detection. These findings\nsuggest that combining CNN with Transformer models holds\ngreat potential for feature extraction and can address the\nlimitations of CNNs. However, the above transformer-based\nmethods do not specially consider the local shape information\nof small targets. Drawing inspiration from the above methods,\nOur approach have explored a new idea of incorporating target\nshape reconstruction into small infrared target detection, which\nsignificantly improves over existing methods and holds great\npotential for advancing the field of SISTD.\nIII. M ETHODOLOGY\nThe overall structure is illustrated in Fig. 3. GSTUnet\ncomprises of three components: a multi-scale feature extrac-\ntion module, a gated-shaped stream module, and a feature\nfusion module. In the multi-scale feature extraction module,\nwe utilize four sequential stages to extract multi-scale semantic\nfeature maps (section III-A). The gated-shaped stream is\ndesigned to extract the shape information from small targets\nthrough gate convolutions (section III-B). In the feature fusion\nmodule, we fuse the semantic feature with shape features at\nmulti-scale to obtain the final results (section III-C). In section\nIII-D, we present the loss function for end-to-end training of\nour network.\nThe proposed GSTUnet aims to generate detected binary\nmask Iout ∈ RH×W×1 to indicate the detected regions.\nSpecifically, let Iin ∈ RH×W×Cin represent the dimensions of\nthe input infrared image. Here, H, W, and Cin represent the\nheight, width, and channels of the input image, respectively.\nThe notation used in this paper is as\nfollows:XS1 , XS2 , XS3 , and XS4 represent the feature\nmaps of each encoder stage. The feature in the Swin\nTransformer is denoted as zl, while the XGi represents the\ndifferent size feature map of the gated shaped stream, the\nup-sampling feature maps are XU1 , XU2 , XU3 , and XU4 .\nA. Multi-scale Feature Extraction Module\nThe detection of infrared small targets in the presence\nof complex and variable background becomes increasingly\nchallenging due to the scale variation. Therefore, feature ex-\ntraction must consider this issue. And since multi-scale feature\nrepresentation has been shown to be effective in adapting\nthe scale variation [6], [8], [51], we incorporate a multi-\nscale mechanism in the design our feature extractor. Moreover,\nSwin Transformer preforms well in SISTD due to its excellent\nglobal attention mechanism and semantic feature extraction\ncapability. Therefore, we design the feature extractor using\nmultiple Swin transformers, which are capable of extracting\nmulti-scale global semantic features.\nThe feature extractor consists of four stages. Each stage con-\ntains two Swin transformers blocks, that utilize W −MSA and\nSW −MSA to capture self-attention for windows and sliding\nwindows, respectively. This allows for a global receptive\nfield view during self-attention computation, and more details\nhave been illustrated in Sec. II-C. Subsequently, the output\nfeature maps of the swin transformer are downsampled at\neach stage to capture multi-scale global semantic information.\nThe general approach for downsampling is through maximum\nor average pooling. However, this may result in the loss of\ndetailed features for small targets when the targets are too\nsmall [25]. Therefore, we utilized patch merging instead of\npooling to downsample the feature map. Patch merging has\nbetter performance in feature retention as it integrate all the\nfeature information of each image patches.\nIt is worth noting that the self-attention calculations in the\nSwin transformer, as shown in Eq. 2, are complicated. Feeding\nthe input image directly to the multiple Swin transformer is\ncomputationally complex and time-consuming. Therefore, we\nsequentially introduce a patch partition (PP) layer and a linear\nembedding (LE) layer before it. The PP layer transforms the\ninput image Iin ∈ RH×W×Cin into image patches to reduce\ncomputational complexity. Then, the LE layer map the chan-\nnels of each patch to a specified dimension C. Specifically,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nFig. 4. The Window partition strategy of W-MSA module and SW-MSA.(red\nrectangle is the patch include small target.)\nthe resolution of each patch is H\n4 × W\n4 , and convolution is\nutilized in the patch partition layer and the linear embedding\nlayer. The convolution kernel size and stride are both H\n4 , the\ninput channels of the infrared image are 1, and the output\nchannels of the linear embedding layer channel are C.\nOverall, the multi-scale feature extraction module aims to\ncapture detailed information and adapt to different target scales\nto enhance the performance of SISTD. The structure is shown\nin the top-left of Fig. 3. First, it utilize PP and LE layer\ntransform the input image into patches and obtain an initial\nfeature map. This step can be expressed as follows:\nXS0 = ConvPP &LE(Iin), (3)\nSecondly, the initial feature map is fed into the four-\nstage swin transformer to calculate the self-attention of each\nwindow, and then down-sampling the feature map using a\npatch merging layer. The step can be expressed as follows\nEq. 4:\nXSi = SwinStage [XSi−1 ](i = 1, 2, 3, 4), (4)\nwhere XSi (i = 1, 2, 3, 4) is the output feature maps of each\nswin transformer stages. At each stage, the output feature map\nsize is halved compared to the input feature map size, while\nthe channel size is doubled. After the four stages, the size\nof the output feature map decreases from (H/4) × (W/4) to\n(H/32) × (W/32), and the channel size increases from C to\n8C.\nFinally, these four feature maps are concatenated along\nthe channel dimension to produce a feature map of size\n(H/2i+1) × (W/2i+1) × 2i+1C, which is then transformed\nto 2iC channels using a linear projection layer.\nSpecifically, at the i-th stage, interval sampling is performed\non the input feature map XSi ∈ R(H/2i+1)×(W/2i+1)×2i−1C,\nresulting in four feature maps of size (H/2i+1)×(W/2i+1)×\n2i−1C. The output feature maps of each stage are referred to\nas XS1 , XS2 , XS3 , XS4 .\nB. Edge-Aware Gated Shaped Stream Module\nThe environment backgrounds are dynamic and perplex,\nwith various types and orientations of targets, leading to\ndiverse shapes of small targets [41]. The previously extracted\nmulti-scale features are obtained by processing the semantic\nfeatures of the context in the patch sequence. However, they\n1*1Conv\n 1*1Conv\n 1*1Conv\nResNet18\n ResNet18\nGCL\n GCL\nResNet18\nGCL\nGated Shaped StreamShare Weights\nASPP\nFig. 5. Specifically of Gated Shaped Stream, which include 1*1 convolution,\nResNet, GCL, and ASPP.\nlack the ability to capture the shape of the target since it does\nnot directly process the spatial information. This results in the\ndirect extraction of small targets from the multi-scale features,\nwhich leads to a network that lacks both robustness and\nsensitivity to changes in target shape. Therefore, Extracting\nshape information can increase the robustness of SISTD.\nFor this reason, we design a second auxiliary branch, called\nthe edge-aware Gated Shaped Stream, aiming to accurately\nextract the shape features of small targets and improving the\ndetection accuracy. It is an end-to-end network and its structure\nis illustrated in Fig. 5. It comprises Gated Convolutional Layer\n(GCL), ResNet [30] and 1*1 convolution and ASPP [52]\nmodules. The details of this structure are as follows.\nFirstly, considering that the edges are important structural\nelements in images that implies the shape information of the\ntarget, it is essential to extract the shape features from the\nedge region. Therefore, we utilize the Sobel operator [53] to\nextract edges from the original infrared image, resulting in the\nedge prior graph XE. Next, XE is fed into the Gated Shaped\nStream through the input arrow in the lower left corner of Fig.\n5.\nXE = Sobel(I), (5)\nThen, previous studies [24], [41] have demostrated that\nincorporating contextual semantic information enhances the\nability of model to perceive edges. Based on this insight,\nwe feed multi-scale feature maps, comprising XS1 , XS2 and\nXS3 , into the Gated Shaped Stream, facilitating the fusion\nof the edge information and the semantic feature information.\nMoreover, to extract the local details and edge information of\nsmall targets and improve the target detection performance,\nwe further extract deep features from these multi-scale feature\nmaps. In this process, considering that the residual network\ncan prevent small target features from being lost during down-\nsampling [25], we concatenate 1*1 convolution and ResNet\n[30] after XSi . The network extracts deep features of multi-\nscale feature maps XS1 , XS2 and XS3 by concatenating the\nresiduals, respectively. This deep feature extraction formula is\nas follows:\nX′\nSi = ResNet(Conv1∗1(XSi )), (i = 1, 2, 3) (6)\nwhere ResNet denotes the residual network, XS′\ni\ndenotes the\nfeature after deep feature extraction. As a deep ResNet may\nresult in extensive computational requirements, we employ\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\na lightweight ResNet18 module. Its parameters are obtained\nfrom the pre-trained model weights on ImageNet [54], and the\nthree ResNet18 networks share the same set of weight.\nAfter acquiring the deep semantic features X′\nSi and the\nedge map XE, the next step is to fuse them. It is important\nto note that the edge information of infrared small targets in\nsemantic features tends to blurred [24]. Consequently, directly\nfusing the edge maps with deeper features can introduce\nnoise and redundancy. To address this, we design the GCL\nmodule to selectively process edge-related information in deep\nsemantic features and fuse them level by level. The input of\nthe first level GCL consists of XE and XS1 . The inputs to\nthe second and third GCLs are composed of the outputs of\nthe previous GCLs and the corresponding multi-scale deep\nsemantic features, respectively.\nThe structure of GCL is illustrated in Fig. 6, which firstly\nconcatenates the deep semantic features and edge features. As\nshown in the blue box on the left side of Fig. 6, where the\nedge semantic features (represented by light green blocks) and\ndeep semantic features (represented by dark green blocks) are\ncombined. Next, the stacked features are processed through\ntwo consecutive 1*1 convolution and batch normalization op-\nerations, both followed by ReLU activation function. Finally,\nthe sigmoid function to implement the gated convolution oper-\nation. Additionally, we integrate the two residual connections\ninto the input feature map by element-wise multiplication and\nelement-wise addition, the formula for GCL is:\nαi = σ(ReLU(BN (Conv1∗1(Xi||Yi)))),\nXout\nSi = Conv1∗1(Xi ⊙ αi + Xi), (i = 1, 2, 3) (7)\nwhere X1 denotes the edge prior graph XE, X2 and X3\ndenote the outputs from the previous level of GCL, Yi denotes\nthe feature graph after deep semantic feature extraction, BN\nrepresents batch normalization, ReLU denotes the activation\nfunction, σ denotes the sigmoid function, ⊙ denotes element-\nwise multiplication, || denotes the concating of the feature\nmap, and Xout\nSi denotes the output of the GCL module. When\nin the flat non-edge region, the value of αi will be relatively\nsmall, and the formula will be calculated to get relatively small\nXout\nSi , making the corresponding output of the Gated Shaped\nStream be relatively small. In summary, GCL controls the flow\nof information by using sigmoid function as a gating unit,\nwhich can effectively filter out the details of flat regions in\nthe feature map and put more focus on image features such as\nedges and shapes, and therefore can output feature maps that\ncontain only edge information.\nFinally, to achieve multi-scale feature fusion during the up-\nsampling process of the extracted feature maps contain edge\ninformation, we generate multi-scale edge feature maps by up-\nsampling the feature maps containing edge features with the\npurpose of fusing edges and small targets. Specifically, we\nintroduce the ASPP [52] module to up-sampling the output of\nthe final level of GCL to match size as the XS1 , XS2 , XS3 , as\nshown in Fig. 7. Additionally, a 1*1 convolution is applied to\nmatch the output channel with the input channel. We represent\nthe feature map after up-sampling, as XG1 , XG2 , XG3\nEdge Semantic \nFeature\nDeep Semantic Feature\nOutput Feature\nFeature\nConcatenate 1*1Conv BN ReLU Sigmoid\nFunction\nElement-wise\nProduct\nElement-wise\nAdd\nFig. 6. Specifically of Gate Convolution Layer, input edge semantic feature\nand deep semantic feature, with 1*1 conv, BN, and ReLU.\n3*3Conv\nRate=6\n3*3Conv\nRate=6\n3*3Conv\nRate=6\n 1*1Conv\nXG\nXG1\nXG2\nXG3\nFig. 7. Specifically of ASPP.\nC. The Up-sampling Module\nTo integrate multi-scale features and shape information of\nthe target, our upsampling process is illustrated in the upper\nright corner of Fig. 3. The upsampling process comprises\nthree stages, each including a fusion module and two swin\ntransformer modules. First, we introduce a fusion module that\nintegrates features from multi-scale feature extraction and the\noutput characteristics of the Gated Shaped Stream. The fusion\nblock employs a fully connected layer and a linear projection\nlayer to merge and upsample the features, thereby reducing the\nfeature dimensional. Second, the Swin Transformer module is\ndesigned to learn finer feature relationships effectively.\nThe feature maps of each stage are denoted as XUi (where\ni = 1, 2, 3). To mitigate information loss caused by upsam-\npling, the feature fusion module comprises a fully connected\nFig. 8. The structure of a up-sampling stage.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\n(a) Original Infrared Image (b) Ground Truth (c) Ground Truth of \nGated-Shaped Stream\nFig. 9. The input infrared image(a), and Ground Truth of Feature Extractor(b)\nand Gated-Shaped Stream(c).\nMLP and a linear projection Fproj, as shown in the Fig. 8.\nThe fusion block utilizes a patch reshape block to perform\nthe upsampling operation. Subquently, the upsampled feature\nmaps XUi (where i = 1, 2, 3) are concatenated with the\nfeature maps XGi generated by the Gated Shaped Stream.\nAfterwards, Fproj is applied to increase the dimensionality\nof XUi . The dimension is increased from 24−iC to 25−iC,\nwhere C represents the channel dimension. The upsampling\nprocess is formulated as follows in Eq. 8:\nXUi+1 = Fproj[MLP [Concat(XUi , XGi)]]. (8)\nDuring the upsampling process, after each stage, the channel\ndimension of XUi is halfved compared of XUi−1 , and the\nspatial resolution is doubled. The size of the feature map\nincreases from H/32 × W/32 to H/4 × W/4, while the\nchannels decreases from 8C to C. The ultimate output is the\ndetection image Iout, where Iout ∈ RH×W×1.\nD. Edge-Aware Loss Function Design\nGSTUnet is trained end-to-end with a hybrid edge-aware\nloss function. During the training, we proposed a loss function\nto supervise the output and the boundary map. Here, we\nutilize the Sobel operator on the ground truth to indicate the\ncontours of the small targets. We follow the standard Binary\nCross-Entropy (BCE) loss [24] on the predicted boundary\nmap s and BCE loss on the predicted semantic output map\ns. Our loss function comprises two components: one for\nthe output and the other for the gated-shaped stream, which\nground truth is obtained by applying the Soble [55] operator\nto the Ground Truth of the input image, as shown in Fig.\n9. We simultaneously train two data streams, incorporating\nboth the loss function for semantic segmentation and the loss\nfunction for small target boundaries. The loss function for the\noutput map is a combination of Dice [56] loss and BCE loss\nfunctions.\nLtotal = λ1LBCE (∇y, Xout\nS ) +L2(y, Xout),\nL2 = λ2LDice(y, Xout) +λ3LBCE (y, Xout), (9)\nwhere the ∇y is the edge ground truth. Xout\nS is the output\nof the gated shaped stream, y is the ground truth, Xout is\nthe output map of GSUnet, Xout\nS is the output of the Gated\nShaped Stream.\nIV. E XPERIMENTAL RESULTS AND ANALYSIS\nIn section IV, we conducted experiments on two infrared\ndatasets and compared the results of SOTA methods to evalu-\nate the learning and generalization capabilities of GSTUnet. In\nsection IV-A, we first provide an overview of the datasets. In\nsection IV-B, we describe implementation details. In section\nIV-C, We introduce the evaluation metrics and thoroughly\nanalyze the experimental results both quantitatively and qual-\nitatively. Additionally, we provide ablation studies for the\neffectiveness of each module.\nA. Datasets\n1) Infrared Datasets Descriptions:To validate the method-\nology of this paper, we evaluate our method on two public\ndatasets: NUAA-SIRST [23] and NUDT-SIRST [25].\nNUAA-SIRST is a small open-source SISTD dataset, which\ncontains 427 representative images from hundreds of real-\nworld videos captured at short-mid wave-950nm wavelength,\nannotated in 5 ways, such as VOC annotation and mask anno-\ntation, to support image detection and semantic segmentation\nor instance segmentation tasks. Most small infrared targets\nare dim and hidden in complex backgrounds such as the sky,\nocean, and buildings. Additionally, nearly 40% of the images\nbelong to the brightest pixels.\nNUDT-SIRST is a large open SISTD dataset, which is a\nsynthetic dataset and contains 1327 images created by collect-\ning high-resolution natural scene images, cropping different\nareas from these images to get different backgrounds, and then\noverlaying small targets by using a 2D Gaussian function onto\nthe backgrounds to form synthetic images.\n2) Data Augmentation Techniques:We leverage data aug-\nmentation to generate additional instances from raw data via\ntransformations, including rotation, translation, and scaling.\nOur experiment involved rotating the dataset by 90, 180, and\n270 degrees, and applying random cropping and scaling to\ndiversify the dataset further. Given the significant contrast dis-\ncrepancies observed in infrared small target detection datasets\nacross various backgrounds, we employed image brightness\nand contrast perturbation techniques for data augmentation.\nThe data augmentation methods can enhance the quantity\nof training data and improve the network’s generalization\ncapabilities.\nB. Experimental details\nGSTUnet was implemented by the PyTorch framework and\ntrained on NVIDIA RTX 3090Ti GPU, running on the Ubuntu\nenvironment. The model was trained for 200 epochs. Simulta-\nneously, we applied gradient clipping and cosine annealing to\nassist the network training. The specific training parameters\nare listed in Table I. Our model has three variants: The\nsmall-scale, basic-scale, and large-scale versions, and they use\nSwin-Tiny, Swin-Base, and Swin-Large as feature extraction\nencoders, respectively. These Swin transformer models use the\npre-trained model weights from [29]. Different scales of Swin\ntransformer blocks, including many layers and attention layers,\nare shown in table II.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE I\nTRAINING PARAMETERS , MORE DETAILS ARE SHOWN IN GITHUB\nPROJECT .\nTraining Parameters Details\nFramework PyTorch1.8.1\nOperating System Ubuntu\nGPU RTX3090Ti\nAccleration Tools CUDA11.3\nEpochs 200\nλ1 1\nλ2 1\nλ3 0.5\nLearning rate 2 × 10−4\nWeight Decay 1 × 10−5\nOptimizer Adam\nC. Experiment Results\nIn section IV-C, we compare the performance of GSTUnet\nwith other methods. We present quantitative results alongside\nqualitative visualizations in different backgrounds and target\nscales to evaluate our method. Furthermore, we conducted\nan ablation study to analyze the contribution of individual\ntechnical components in GSTUnet.\n1) Evaluation Metric: To evaluate shape description abil-\nity, our experiments utilize intersection over union(IoU) and\nnormalized intersection over union(nIoU). Meanwhile, we\nadopt probability detection(Pd) and flase alarm(Fa) to evaluate\nthe localization and detection ability. Furthermore, we draw\nreceiver operation characteristics(ROC) curves to describe how\ndetection rates evolve with varying false alarm rates.\n1)IoU: IoU is a pixel-level evaluation metric to describe\nthe contour of a small target, which calculates the ratio of\nintersection area and union area between the prediction map\nof small targets and the pixel annotated ground truth, i.e.:\nIoU = Ai\nAu\n, (10)\nwhere Ai and Au denote the size of the intersection and union\narea of the infrared small target prediction mask and ground\ntruth, respectively.\n2)nIoU: nIoU is the normalized IoU, which is calculated as\nfollows:\nnIoU = 1\nN\nNX\ni=1\n(T P[i]/(T[i] +P[i] − T P[i])), (11)\nwhere N is the total number of datasets, T Pdenotes the\nnumber of true positive pixels, T, and P denotes the number\nof ground truth and predicts positive pixels, respectively.\n3)Pd: Pd denotes the probability of detection, and we\nspecified that if the deviation of the predicted mask’s regional\ncenter and the center of the ground truth is less than a threshold\n(set to 3 in this paper), these targets are considered as correctly\npredicted targets, which is calculated as follows:\nPd = Npred\nNall\n(12)\nFig. 10. Roc and Prec-Recall curves of different methods on the NUAA-\nSIRST dataset, Our method is blue solid line, other compare methods are\ndashed line.\nwhere Npred is the correctly predicted point, and all targets\nnumber is the Nall.\n4)Fa:, Fa denotes the false alarm rate. We specified that\nif the deviation of the predicted mask’s regional center and\nthe center of the ground truth is more than the predefined\nthreshold(set 3 in this paper), we consider those pixels as\nfalsely predicted ones\nFa = Nfalse\nNall\n(13)\nwhere Nfalse is the false alarm points, and all targets number\nis the Nall.\n5)ROC: ROC curve describe the change tendency of the\nP dunder different F a. The area of the ROC curve expresses\nthe robustness of the model, with a larger area indicating a\nstronger model.\n2) Compare with State-of-art Methods: Our comparison\ninclude quantitative results and qualitative results.\nQuantitative results:\nWe select MDvsFAcGAN [27], ALCNet [34], ACMNet\n[23], and IAANet [36], UIUNet [35], and CGRANet [37] as\nthe CNN-based methods. To compare the traditional meth-\nods, we select the fliter-based method SRTHT [11], Max-\nmean/median based method TLMS [12], the HVS-based\nmethod MPCM [14], and the LRM-based method: IPI [17],\nRIPT [16]. The key parameters configuration are shown in\nTab. III:\nThe quantitative results are shown in Tab. IV, We selected\nthe top-performing from the three GSTUnet variants(-T, -\nB, and -L) as our method results. Our quantitative results\ndemonstrate SOTA performance in table IV. Our method\nachieves 82.61% and 84.51% in the IoU and nIoU metrics\nachieves 80.97% and 82.19% in the nIoU, respectively, indi-\ncating the effectiveness of our approach in detecting the shapes\nof small targets. It also achieves 98.91% and 99.91% in Pd,\nrespectively, compared to the lower Fa of 5.32 × 10−6 and\n4.92 × 10−6, reflecting the Pd of small targets with fewer Fa\nenabled by our method.\nWe also plot ROC and Precision-Recall curves for com-\nparison experiments, as Figure 10, which demonstrates that\nGSTUnet significantly outperforms other methods, with a\nlarger AUC than filter-based, HVS-based, and LRM-based\nmethods. Meanwhile, the method performs better than CNN-\nbased methods, which proves the effectiveness of the proposed\nmodel.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE II\nDETAILS OF SWIN TRANSFORMER BACKBONE VARIANTS\nModels Hidden Size C MLP Size D Layer Num Head Num Window Size\nSwin-Tiny 96 384 [2,2,6,2] [3,6,12,24] 7\nSwin-Base 128 512 [2,2,18,2] [4, 8, 12, 24] 7\nSwin-Large 192 768 [2,2,18,2] [6, 12, 24, 48] 7\nTABLE III\nCONFIGURATIONS FOR ALL COMPARATIVE EXPERIMENTS\nMethods Source Key parameters configurations\nSRTHT [11] PR’2021 Structure size: 12 × 12, Ring top-hat, M-estimator, local entropy\nTLMS [12] CEE’2021 Filter size: 15 × 15, Center Area:3 × 3\nIPI [17] TIP’2013 Patch Size: 50, sliding step: 10, λ = 1/\np\nmax(m, n), ϵ = 10−6\nRIPT [16] JSTARS’ 2017 Patch Size: 50, sliding step:10, λ = L/\np\nmin(I, J, P), L=1, ϵ = 10−2, ω = 10−7\nMPCM [14] PR’2016 Patch scale size: 3, 5, 7, 9\nMDvsFAcGAN [27] ICCV’2019 Image size: 128 × 128, λ1 = 100, λ2 = 10\nALCNet [34] TGRS’2021 Image size: 256 × 256, contextual scale:Global, Module:TLA-FPN\nACMNet [23] W ACV’2021 Image size: 256 × 256, backbone:FPN, fuse mode:asymbi.\nIAANet [36] TGRS’2022 Image size: 128 × 128, Module:RPN\nUIUNet [35] TIP’2022 Image size: 320 × 320, backbone:ResUnet, Module: RSU, IC-A\nCGRANet [37] JSTARS’2023 Image size: 128 × 128,backbone:Res2Net,Module:CGM,MAB,RAM\nTABLE IV\nEVALUATION INDEX OF COMPARE EXPERIMENT (DISPLAY THE BEST\nRESULT IN RED FONT AND THE SECOND -BEST RESULT IN BLUE , THE UP\nARROW MEANS THE LARGER IS BETTER , THE DOWN ARROW MEANS THE\nLESS IS BETTER .)\nNUAA-SIRST IoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nSRTHT [11] 23.46 22.12 79.51 23.31\nTLMS [12] 24.11 23.18 80.21 30.1\nIPI [17] 25.67 23.24 85.55 11.47\nRIPT [16] 52.82 49.52 86.72 10.31\nMPCM [14] 50.30 48.26 79.35 29.11\nMDvsFAcGAN [27] 72.11 70.23 92.17 10.89\nALCNet [34] 73.69 71.19 97.01 12.21\nACMNet [23] 75.18 73.54 95.91 9.325\nIAANet [36] 77.89 77.21 97.52 12.56\nUIUNet [35] 78.89 78.21 98.55 8.324\nCGRANet [37] 81.52 80.18 98.41 9.564\nGSTUnet 82.61 80.97 98.91 5.32\nNUDT-SIRST IoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nSRTHT [11] 29.57 28.22 75.51 25.31\nTLMS [12] 25.12 22.18 78.21 34.18\nIPI [17] 27.76 24.98 75.28 13.68\nRIPT [16] 67.76 64.21 82.28 12.68\nMPCM [14] 64.16 62.69 77.55 19.87\nMDvsFAcGAN [27] 73.11 72.23 94.17 14.89\nALCNet [34] 76.35 75.64 95.97 10.18\nACMNet [23] 81.15 78.92 96.54 9.231\nIAANet [36] 78.81 77.93 98.54 14.56\nUIUNet [35] 82.89 81.21 98.05 8.324\nCGRANet [37] 83.52 82.18 98.09 9.564\nGSTUnet 84.51 82.19 99.11 4.92\nQualitative results:\nWe visualized the results of the methods and 3-D gray figure\non the NUAA-SIRST dataset with the comparison experi-\nments, shown in Fig. 11-15. These experiments demonstrate\nthat GSTUnet can accurately detect and locate targets even\nunder low contrast and low SCRs conditions while obtains\ncomplete and precise target shapes. This high level of accuracy\nis achieved through the combination of GCL with Swin\ntransformer, which enables the effective establishment of a\nglobal view and the extraction of valuable edge information\nthrough the gated-shaped stream method and feature fusion\nmodules.\nCompared to traditional infrared small targets, which are\nsusceptible to miss detection at low SCRs and false alarms\nat high contrast. It is prone to have false alarms and miss\ndetection in the complex backgrounds, as shown in the yellow\ncircle of MPCM, IPI in Fig. 11-15. Compared to traditional\nHVS, LRM-based methods, our method produces accurate\ntarget localization and shape outputs with very low false alarm\nrates. However, the traditional method performs well only on\npoint-shaped targets, is less effective at characterizing shape,\nand is prone to localized highlighting that tends to produce\nmany false alarms, as shown the white circle in Fig. 11-15.\nWhile GSTUnet maintains high accuracy, the performance of\ntraditional methods decreases dramatically when the size of\nthe point target increases and the number of bright spots with\nbackgrounds interference increases. It is because the perfor-\nmance of the traditional methods is largely based on manual\nfeatures and cannot be adapted to different backgrounds and\nvarious sizes.\nCNN-based methods (i.e., CGRANet [37] and ALCNet\n[34]) perform much better than HVS-based and LRM-based\nmethods, especially on the shape information of the small\ntargets. However, due to the complexity of the scenes, many\nfalse alarms and missed detection regions are generated, as\nshown by the white circles in Fig. 11-15. Our GSTUnet is\nmore robust to these scene variations and can detection small\ntargets on sky, ocean, and field backgrounds. In addition,\nour GSTUnet generates more accurate shape information than\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nFig. 11. point small target on ocean backgrounds\nFig. 12. point small target on land backgrounds\nFig. 13. point target on sky backgrounds\nALCNet [34], as shown in the zoomed region in Fig. 11-15. In\nsummary, our GSTUnet can adapt to the challenges of different\nclutter backgrounds and target shapes, which achieves better\nperformance.\nD. Ablation study\nTo investigate the feasibility of each component of\nGSTUnet, we will take various ablation studies in this section,\nincluding utilizing the skip connection instead of the gated-\nshaped stream and exploring different scale structures Swin\ntransformer feature extractors’ ability to the SISTD.\n1) Impact of Gated Shaped Stream:To thoroughly inves-\ntigate the effectiveness of ResNet in preserving small target\nfeatures during Gated Shaped Stream, and the capability of\nthe GCL module to effectively handle edge-related information\nin semantic features. The ablation study of the Gated Shaped\nStream comprises two parts. The first part involves removing\nthe second Gated Shaped stream and replacing it with a skip\nconnection between the encoder and decoder, as shown in Fig.\n16 Additionally, it is necessary to modify the loss function in\nSec. III-D, which encompasses two components: DiceLoss and\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nFig. 14. shape feature small target on land backgrounds\nFig. 15. shape feature small target on sky backgrounds\nBCELoss, as shown in Eq. 14.\nLalbation = LDice(y, Xout) +λaLBCE (y, Xout), (14)\nwhere y is the ground truth of the image, Xout is the output\nof the network, λa is set to 1. The second study we remove\nthe ResNet of Gated Shaped Stream, utilize only GCL. We\nplot the 3D of the gray-scale map, enhancing the visualization\nresults, clearly demonstrating the effectiveness of the residual\nnetwork in retaining small target features during downsam-\npling as shown in Fig. 17. We can clearly observe that when\nexists different target, add the ResNet will preserve targets\nwith a shape feature and will preserving features of small\ntargets during the downsampling process, overcome the miss\ndetection. After removing the gated-shaped stream, as shown\nin 17, facing the blurred edges of small targets, false alarms\nare easily generated at the edges, and the shape information\nof small targets can not be well described.\nThe quantitative results are shown in the Tab. V. After\nremoving the gated-shaped stream, on the NUAA-SIRST\ndataset, the IoU, nIoU, and Pd are decrease to 80.88%,\n78.27%, 98.87%, the Fa increase to 6.59%. On the NUDT-\nSIRST dataset, the IoU, nIoU, and Pd decreased to 82.07%,\n80.79%, 98.68%, and the Fa increased to 5.82%. Subsequently,\nwe plot the bar chart as Fig. 18, according to the quantitative\nresult, the gated-shaped stream can approve the Pd and the\nIoU and nIoU , decreasing Fa.\nFig. 16. Ablation study network structure of GSTUnet,(a) is a skip connection\nbetween encoder and decoder, (b) is original gated shaped stream remove\nResNet.\nMeanwhile, we visualization the feature map of Gated\nShaped Stream every 50 epoch, as shown in Fig. 19. As the\nnumber of training epoch increases, Gated Shaped Stream can\nlearn local edge information of small targets.\n2) Impact of Loss Function Weights:In order to explore the\nimpact of different loss function weights on the overall per-\nformance of the network, we conducted ablation experiments\non the weight of the loss function on Eq. 9. To investigate the\nimpact of weights on the performance concerning small target\nedges and shapes, we conducted qualitative experiments. We\nset λ1 = 1, λ2 = 1, λ3 = 0.5as the loss function weights\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nFig. 17. Ablation study of baseline, GSTUnet and w/o ResNet18, the\nGSTUnet can accurately detecting the edges and shapes of small targets, and\nalso does not overlook small targets in complex backgrounds.\nTABLE V\nEVALUATION INDEX OF BASELINE AND GSTU NET\nNUAA-SIRST Metrics\nGCL ResNet18 IoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nx x 80.88 78.27 98.87 6.59\n✓ x 81.38 79.71 98.92 5.89\n✓ ✓ 82.61 80.97 98.91 5.32\nNUDT-SIRST Metrics\nGCL ResNet18 IoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nx x 82.07 80.79 98.68 5.82\n✓ x 83.08 81.27 99.07 5.59\n✓ ✓ 84.51 82.19 99.11 4.92\nFig. 18. Metric of ablation on two datasets, as the network increases, the\nvarious components we propose improve performance on Pd, IoU, and nIoU,\nwhile Fa decreases\nFig. 19. Visualization of the output of Gated Shaped Stream during the\ntraining process with a heat map, we visualization every 50 epoch.\nTABLE VI\nEVALUATION INDEX OF DIFFERENT WEIGHT OF THE LOSS FUNCTION .\nλ2 = 1, λ3 = 0.5 λ1 = 1, λ3 = 0.5 λ1 = 1, λ2 = 1\nλ1 IoU λ2 IoU λ3 IoU\n0.6 81.14 0.6 82.46 0.1 79.46\n0.8 81.83 0.8 82.53 0.3 81.53\n1.0 82.61 1.0 82.61 0.5 82.61\n1.2 82.53 1.2 82.56 0.7 82.21\n1.4 82.14 1.4 82.52 0.9 81.82\nin NUAA-SIRST to determine their effect on the IoU perfor-\nmance of the network (under consistent conditions as detailed\nin Tab. VI, training for 200 epochs and repeating the training\nten times to calculate the average). The experimental results\nare presented as follows Tab. VI. Subsequently, we plotted the\nlines in three cases: λ1 = 1, λ3 = 0.5, λ1 = 1, λ3 = 0.5 and\nλ2 = 1, λ3 = 0.5, as shown in Fig. 20:\nAs shown in Fig.20, Changing in the loss function influence\nthe performance of the network, when λ1 is set low, the\nnetwork pays less attention to edge features, which will\nreduces the IoU. However, while the change off weights on\nloss function will only impact less on the overall performance,\nthat also reflects the robustness of our method.\n3) Impact of Swin Transformer Backbone:To explore the\ndifferent parameters of the feature extractor, we replaced the\npre-trained model weights with the Swin-T, Swin-B, and Swin-\nL feature extraction backbone. The various parameters are\nshown in Table II. Every config will be the same as Tab. I.\nAccording to the ablation study, three different sizes of\nFig. 20. Change tendency of different weight of loss function.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nTABLE VII\nEVALUATION INDEX OF SWIN TRANSFORMER BACKBONE VARIANTS\nNUAA-SIRSTIoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nGSTUnet-T 82.48 80.61 98.87 5.59\nGSTUnet-B 82.58 80.27 98.47 5.41\nGSTUnet-L 82.61 80.97 98.91 5.32\nNUDT-SIRSTIoU(%)↑ nIoU(%)↑ Pd(%)↑ Fa(10−6)↓\nGSTUnet-T 84.07 82.29 99.06 5.22\nGSTUnet-B 84.52 82.02 99.96 5.01\nGSTUnet-L 84.51 82.19 99.11 4.92\nSwin transformer backbones have little impact on infrared\nsmall object detection tasks. Swin-L has the largest number\nof parameters but it is not significantly ahead of Swin-T and\nSwin-B in terms of evaluation index. It is because infrared\nimages have low resolution, which makes large backbones\nunable to leverage their global vision capabilities fully. It also\nindicates that lightweight backbones can achieve good results\nand have broad application prospects, which provides potential\nvalue for model light-weighting.\nV. C ONCLUSION\nThis work proposes the GSTUnet, a network with two data\nstreams based on U-shaped encoder-decoder architecture for\ninfrared small object detection. Our GSTUnet is built upon a\nhierarchical Swin Transformer and innovatively incorporates\nSwin transformer modules in the decoder. Additionally, we\nintroduce a gated-shaped stream based on the GCL after\nfeature extraction, which focuses solely on extracting and\ncomputing edge information of small infrared objects. We have\ntrained the neural network end-to-end and performed an up-\nsampling information fusion from the two data streams by\nadding the edge loss and overall loss function. We establish\nlong-term dependency relationships between different scale\nfeatures through a self-attention mechanism and effectively\nfuse multi-scale features with edge-sensitive results. Exten-\nsive experiments on real and synthetic datasets demonstrate\nthat our GSTUnet significantly outperforms other advanced\nmethods. In future work, we will focus on designing lighter\ntransformer-based models and achieving better pixel-level in-\ntrinsic structural features generated by patch partitioning in\nvisual transformers.\nACKNOWLEDGMENT\nThis work was supported by the National Natural Sci-\nence Foundation of China (No. 62075169, 62003247 and\n62061160370) and the Key Research and Development Pro-\ngram 2021BBA235 of Hubei Province.\nREFERENCES\n[1] He Deng, Xianping Sun, Maili Liu, Chaohui Ye, and Xin Zhou. Small\ninfrared target detection based on weighted local difference measure.\nIEEE Transactions on Geoscience and Remote Sensing, 54(7):4204–\n4214, 2016.\n[2] Renke Kou, Chunping Wang, Zhenming Peng, Zhihe Zhao, Yaohong\nChen, Jinhui Han, Fuyu Huang, Ying Yu, and Qiang Fu. Infrared\nsmall target segmentation networks: A survey. Pattern Recognition,\n143:109788, 2023.\n[3] Yaqi Han, Jingwen Liao, Tianshu Lu, Tian Pu, and Zhenming Peng.\nKcpnet: Knowledge-driven context perception networks for ship detec-\ntion in infrared imagery. IEEE Transactions on Geoscience and Remote\nSensing, 61:1–19, 2022.\n[4] Yaqi Han, Xinyi Yang, Tian Pu, and Zhenming Peng. Fine-grained\nrecognition for oriented ship against complex scenes in optical remote\nsensing images. IEEE Transactions on Geoscience and Remote Sensing,\n60:1–18, 2021.\n[5] Yang Sun, Jungang Yang, and Wei An. Infrared dim and small target\ndetection via multiple subspace learning and spatial-temporal patch-\ntensor model. IEEE Transactions on Geoscience and Remote Sensing,\n59(5):3737–3752, 2020.\n[6] Zhaobing Qiu, Yong Ma, Fan Fan, Jun Huang, and Lang Wu. Global\nsparsity-weighted local contrast measure for infrared small target detec-\ntion. IEEE Geoscience and Remote Sensing Letters, 19:1–5, 2022.\n[7] Zhaobing Qiu, Yong Ma, Fan Fan, Jun Huang, and Minghui Wu.\nAdaptive scale patch-based contrast measure for dim and small infrared\ntarget detection. IEEE Geoscience and Remote Sensing Letters, 19:1–5,\n2020.\n[8] Yimian Dai, Xiang Li, Fei Zhou, Yulei Qian, Yaohong Chen, and Jian\nYang. One-stage cascade refinement networks for infrared small target\ndetection. IEEE Transactions on Geoscience and Remote Sensing, 61:1–\n17, 2023.\n[9] Xiangzhi Bai and Fugen Zhou. Analysis of new top-hat transformation\nand the application for infrared dim small target detection. Pattern\nRecognition, 43(6):2145–2156, 2010.\n[10] Suyog D Deshpande, Meng Hwa Er, Ronda Venkateswarlu, and Philip\nChan. Max-mean and max-median filters for detection of small targets.\nIn Signal and Data Processing of Small Targets 1999, volume 3809,\npages 74–83. SPIE, 1999.\n[11] Lizhen Deng, Jieke Zhang, Guoxia Xu, and Hu Zhu. Infrared small\ntarget detection via adaptive m-estimator ring top-hat transformation.\nPattern Recognition, 112:107729, 2021.\n[12] Hong Li, Qiang Wang, Huan Wang, and WanKou Yang. Infrared small\ntarget detection using tensor based least mean square. Computers &\nElectrical Engineering, 91:106994, 2021.\n[13] CL Philip Chen, Hong Li, Yantao Wei, Tian Xia, and Yuan Yan Tang.\nA local contrast method for small infrared target detection. IEEE\ntransactions on geoscience and remote sensing, 52(1):574–581, 2013.\n[14] Yantao Wei, Xinge You, and Hong Li. Multiscale patch-based contrast\nmeasure for small infrared target detection. Pattern Recognition, 58:216–\n226, 2016.\n[15] Zhao-bing Qiu, Yong Ma, Fan Fan, Jun Huang, Ming-hui Wu, and Xiao-\nguang Mei. A pixel-level local contrast measure for infrared small target\ndetection. Defence Technology, 18(9):1589–1601, 2022.\n[16] Yimian Dai and Yiquan Wu. Reweighted infrared patch-tensor model\nwith both nonlocal and local priors for single-frame small target detec-\ntion. IEEE journal of selected topics in applied earth observations and\nremote sensing, 10(8):3752–3767, 2017.\n[17] Chenqiang Gao, Deyu Meng, Yi Yang, Yongtao Wang, Xiaofang Zhou,\nand Alexander G Hauptmann. Infrared patch-image model for small\ntarget detection in a single image. IEEE transactions on image\nprocessing, 22(12):4996–5009, 2013.\n[18] Landan Zhang, Lingbing Peng, Tianfang Zhang, Siying Cao, and Zhen-\nming Peng. Infrared small target detection via non-convex rank approx-\nimation minimization joint l 2, 1 norm. Remote Sensing, 10(11):1821,\n2018.\n[19] Landan Zhang and Zhenming Peng. Infrared small target detection based\non partial sum of the tensor nuclear norm. Remote Sensing, 11(4):382,\n2019.\n[20] Fuju Yan, Guili Xu, Quan Wu, Junpu Wang, and Zhenhua Li. In-\nfrared small target detection using kernel low-rank approximation and\nregularization terms for constraints. Infrared Physics & Technology,\n125:104222, 2022.\n[21] Ming Liu, Hao-yuan Du, Yue-jin Zhao, Li-quan Dong, Mei Hui, and\nSX Wang. Image small target detection based on deep learning with snr\ncontrolled sample generation. Current Trends in Computer Science and\nMechanical Automation, 1:211–220, 2017.\n[22] Bruce McIntosh, Shashanka Venkataramanan, and Abhijit Mahalanobis.\nInfrared target detection in cluttered environments by maximization of a\ntarget to clutter ratio (tcr) metric using a convolutional neural network.\nIEEE Transactions on Aerospace and Electronic Systems, 57(1):485–\n496, 2020.\n[23] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard. Asymmetric\ncontextual modulation for infrared small target detection. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 950–959, 2021.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[24] Mingjin Zhang, Rui Zhang, Yuxiang Yang, Haichen Bai, Jing Zhang,\nand Jie Guo. Isnet: Shape matters for infrared small target detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 877–886, 2022.\n[25] Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang, Zaiping Lin,\nMiao Li, Wei An, and Yulan Guo. Dense nested attention network for\ninfrared small target detection. IEEE Transactions on Image Processing,\n2022.\n[26] Jian Lin, Kai Zhang, Xi Yang, Xiangzheng Cheng, and Chenhui Li.\nInfrared dim and small target detection based on u-transformer. Journal\nof Visual Communication and Image Representation, 89:103684, 2022.\n[27] Huan Wang, Luping Zhou, and Lei Wang. Miss detection vs. false alarm:\nAdversarial learning for small object segmentation in infrared images.\nIn Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 8509–8518, 2019.\n[28] Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, Guangming Lu,\nand David Zhang. Ds-transunet: Dual swin transformer u-net for\nmedical image segmentation. IEEE Transactions on Instrumentation\nand Measurement, 71:1–15, 2022.\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 10012–10022, 2021.\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 770–778,\n2016.\n[31] Fengyi Wu, Hang Yu, Anran Liu, Junhai Luo, and Zhenming Peng.\nInfrared small target detection using spatio-temporal 4d tensor train and\nring unfolding. IEEE Transactions on Geoscience and Remote Sensing,\n2023.\n[32] Moran Ju, Jiangning Luo, Guangqi Liu, and Haibo Luo. Istdet: An\nefficient end-to-end neural network for infrared small target detection.\nInfrared Physics & Technology, 114:103659, 2021.\n[33] Kun Qian, Shou-jin Zhang, Hong-yu Ma, and Wen-jun Sun. Siamist:\nInfrared small target tracking based on an improved siamrpn. Infrared\nPhysics & Technology, 134:104920, 2023.\n[34] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard. Attentional local\ncontrast networks for infrared small target detection. IEEE Transactions\non Geoscience and Remote Sensing, 59(11):9813–9824, 2021.\n[35] Xin Wu, Danfeng Hong, and Jocelyn Chanussot. Uiu-net: U-net in u-\nnet for infrared small object detection. IEEE Transactions on Image\nProcessing, 32:364–376, 2022.\n[36] Kewei Wang, Shuaiyuan Du, Chengxin Liu, and Zhiguo Cao. Interior\nattention-aware network for infrared small target detection. IEEE\nTransactions on Geoscience and Remote Sensing, 60:1–13, 2022.\n[37] Shunshun Zhong, Fan Zhang, et al. Context guided reverse attention\nnetwork with multiscale aggregation for infrared small target detection.\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing, 2023.\n[38] Tianfang Zhang, Lei Li, Siying Cao, Tian Pu, and Zhenming Peng.\nAttention-guided pyramid context networks for detecting infrared small\ntarget under complex background. IEEE Transactions on Aerospace and\nElectronic Systems, 2023.\n[39] Renke Kou, Chunping Wang, Ying Yu, Zhenming Peng, Fuyu Huang,\nand Qiang Fu. Infrared small target tracking algorithm via segmentation\nnetwork and multi-strategy fusion. IEEE Transactions on Geoscience\nand Remote Sensing, 2023.\n[40] Renke Kou, Chunping Wang, Ying Yu, Zhenming Peng, Mingbo Yang,\nFuyu Huang, and Qiang Fu. Lw-irstnet: Lightweight infrared small target\nsegmentation network and application deployment. IEEE Transactions\non Geoscience and Remote Sensing, 2023.\n[41] Fanzhao Lin, Shiming Ge, Kexin Bao, Chenggang Yan, and Dan Zeng.\nLearning shape-biased representations for infrared small target detection.\nIEEE Transactions on Multimedia, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30,\n2017.\n[43] Lei Zhang and Yan Wen. A transformer-based framework for automatic\ncovid19 diagnosis in chest cts. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 513–518, 2021.\n[44] Siyuan Hao, Bin Wu, Kun Zhao, Yuanxin Ye, and Wei Wang. Two-\nstream swin transformer with differentiable sobel operator for remote\nsensing image classification. Remote Sensing, 14(6):1507, 2022.\n[45] Linfeng Gao, Jianxun Zhang, Changhui Yang, and Yuechuan Zhou.\nCas-vswin transformer: A variant swin transformer for surface-defect\ndetection. Computers in Industry, 140:103689, 2022.\n[46] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko. End-to-end object detection\nwith transformers. In Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16,\npages 213–229. Springer, 2020.\n[47] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan\nWang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers\nmake strong encoders for medical image segmentation. arXiv preprint\narXiv:2102.04306, 2021.\n[48] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang,\nQi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer\nfor medical image segmentation. In Computer Vision–ECCV 2022\nWorkshops: Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nIII, pages 205–218. Springer, 2023.\n[49] Chuanqin Li, Zhanchao Huang, Xiaoming Xie, and Wei Li. Ist-transnet:\nInfrared small target detection based on transformer network. Infrared\nPhysics & Technology, 132:104723, 2023.\n[50] Fangcen Liu, Chenqiang Gao, Fang Chen, Deyu Meng, Wangmeng Zuo,\nand Xinbo Gao. Infrared small and dim target detection with transformer\nunder complex backgrounds. IEEE Transactions on Image Processing,\n32:5921–5932, 2023.\n[51] Yiming Zhu, Songyuan Tang, Yurong Jiang, and Ruirui Kang. Dau-net:\nA regression cell counting method. In ISCTT 2021; 6th International\nConference on Information Science, Computer Technology and Trans-\nportation, pages 1–6. VDE, 2021.\n[52] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Mur-\nphy, and Alan L Yuille. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully connected\ncrfs. IEEE transactions on pattern analysis and machine intelligence,\n40(4):834–848, 2017.\n[53] Kenneth R Castleman. Digital image processing. Prentice Hall Press,\n1996.\n[54] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE\nconference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[55] Liying Yuan and Xue Xu. Adaptive image edge detection algorithm\nbased on canny operator. In 2015 4th International Conference on\nAdvanced Information Technology and Sensor Application (AITS), pages\n28–31. IEEE, 2015.\n[56] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and\nM Jorge Cardoso. Generalised dice overlap as a deep learning loss\nfunction for highly unbalanced segmentations. In Deep Learning in\nMedical Image Analysis and Multimodal Learning for Clinical Decision\nSupport: Third International Workshop, DLMIA 2017, and 7th Interna-\ntional Workshop, ML-CDS 2017, Held in Conjunction with MICCAI\n2017, Qu´ebec City, QC, Canada, September 14, Proceedings 3, pages\n240–248. Springer, 2017.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3386899\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}