{
  "title": "Generative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation",
  "url": "https://openalex.org/W4398192132",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2423889735",
      "name": "Lu Xinyi",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2103723868",
      "name": "Wang Xu",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2220688940",
    "https://openalex.org/W2108617939",
    "https://openalex.org/W2921439108",
    "https://openalex.org/W3143827210",
    "https://openalex.org/W2971755171",
    "https://openalex.org/W2103483792",
    "https://openalex.org/W2053186237",
    "https://openalex.org/W2151828850",
    "https://openalex.org/W4396832972",
    "https://openalex.org/W2112564774",
    "https://openalex.org/W2038558034",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W4366547592",
    "https://openalex.org/W1423604035",
    "https://openalex.org/W2251694021",
    "https://openalex.org/W4321329751",
    "https://openalex.org/W4386266200",
    "https://openalex.org/W4384659495",
    "https://openalex.org/W4377942506",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W2808929012",
    "https://openalex.org/W3167624900",
    "https://openalex.org/W2758592353",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W3161616854",
    "https://openalex.org/W2966554551",
    "https://openalex.org/W2809326784",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2336395599",
    "https://openalex.org/W3031300292",
    "https://openalex.org/W2945326023",
    "https://openalex.org/W2338314300",
    "https://openalex.org/W4243004148"
  ],
  "abstract": "Evaluating the quality of automatically generated question items has been a\\nlong standing challenge. In this paper, we leverage LLMs to simulate student\\nprofiles and generate responses to multiple-choice questions (MCQs). The\\ngenerative students' responses to MCQs can further support question item\\nevaluation. We propose Generative Students, a prompt architecture designed\\nbased on the KLI framework. A generative student profile is a function of the\\nlist of knowledge components the student has mastered, has confusion about or\\nhas no evidence of knowledge of. We instantiate the Generative Students concept\\non the subject domain of heuristic evaluation. We created 45 generative\\nstudents using GPT-4 and had them respond to 20 MCQs. We found that the\\ngenerative students produced logical and believable responses that were aligned\\nwith their profiles. We then compared the generative students' responses to\\nreal students' responses on the same set of MCQs and found a high correlation.\\nMoreover, there was considerable overlap in the difficult questions identified\\nby generative students and real students. A subsequent case study demonstrated\\nthat an instructor could improve question quality based on the signals provided\\nby Generative Students.\\n",
  "full_text": "Generative Students: Using LLM-Simulated Student Profiles to\nSupport Question Item Evaluation\nXinyi Lu\nUniversity of Michigan\nComputer Science and Engineering\nAnn Arbor, United States\nlwlxy@umich.edu\nXu Wang\nUniversity of Michigan\nComputer Science and Engineering\nAnn Arbor, United States\nxwanghci@umich.edu\nFigure 1: The design of the prompt architecture of Generative Students is based on the KLI framework, which uses knowledge\ncomponents (KCs) to define the elements students are expected to learn. With the KCs identified for a given task (a), the\ngenerative student’s profile is a function of the list of KCs the student has mastered, has confusion about, or has no evidence\nof knowledge of (b). Users can define master prompt, confusion prompt, and unknown prompt for a given task (c). This\narchitecture thus supports automatic creation of diverse student profiles (d).\nABSTRACT\nEvaluating the quality of automatically generated question items\nhas been a long standing challenge. In this paper, we leverage LLMs\nto simulate student profiles and generate responses to multiple-\nchoice questions (MCQs). The generative students’ responses to\nMCQs can further support question item evaluation. We propose\nGenerative Students, a prompt architecture designed based on the\nKLI framework. A generative student profile is a function of the\nlist of knowledge components the student has mastered, has con-\nfusion about or has no evidence of knowledge of. We instantiate\nthe Generative Students concept on the subject domain of heuristic\nevaluation. We created 45 generative students using GPT-4 and had\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0633-2/24/07. . . $15.00\nhttps://doi.org/10.1145/3657604.3662031\nthem respond to 20 MCQs. We found that the generative students\nproduced logical and believable responses that were aligned with\ntheir profiles. We then compared the generative students’ responses\nto real students’ responses on the same set of MCQs and found\na high correlation. Moreover, there was considerable overlap in\nthe difficult questions identified by generative students and real\nstudents. A subsequent case study demonstrated that an instructor\ncould improve question quality based on the signals provided by\nGenerative Students.\nCCS CONCEPTS\n• Applied computing →Computer-assisted instruction.\nKEYWORDS\nGenerative Agent, Question Item Evaluation, Generative AI\nACM Reference Format:\nXinyi Lu and Xu Wang. 2024. Generative Students: Using LLM-Simulated\nStudent Profiles to Support Question Item Evaluation. In Proceedings of the\nEleventh ACM Conference on Learning @ Scale (L@S ’24), July 18–20, 2024,\nAtlanta, GA, USA. ACM, New York, NY, USA, 12 pages. https://doi.org/10.\n1145/3657604.3662031\narXiv:2405.11591v1  [cs.HC]  19 May 2024\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\n1 INTRODUCTION\nDecades of educational research has shown the benefit of active\nlearning [9, 12, 15, 26], one-on-one tutoring [6, 24], and deliberate\npractice [16, 17] in improving students’ learning outcomes. These\ntheories highlight the benefit of providing students with hands-on\nproblem solving and question answering opportunities to facilitate\nlearning. It has been a long standing interest in the Learning at\nScale and AI in Education communities to study effective question\ngeneration techniques [5, 28] to support the creation of high-quality\nquestion items at scale to enhance active learning, tutoring, and de-\nliberate practice. Multiple-choice question creation is of particular\ninterest because of their practical value in terms of ease of grading\nand automatic provision of feedback [5, 28, 42, 46, 47, 50].\nPrior work has explored a variety of ways to support multiple-\nchoice question creation for educational purposes, including crowd-\nsourcing questions from students [42, 57], perusing prior students’\nsolutions and mistakes to generate new questions [ 46, 47], us-\ning teacher-AI collaborative approaches where teachers receive\nAI suggestions[29], and fully automated techniques leveraging AI\n[5, 13, 21, 28, 30, 31, 43, 48, 50]. With the advancement of generative\nAI, there is a growing interest in using generative AI tools such\nas ChatGPT [ 2] to create quiz questions. A handful of universi-\nties provide example prompts for instructors to create low-stakes\nassessment questions with ChatGPT [ 1, 3]. This movement has\nincreased our likelihood of getting a large question pool, but how\ndo we know whether the questions generated are of high quality?\nIn addition to face evaluation by experts and student peers\n[29, 42, 50], psychometric methods are still mainstream to evaluate\nquestion item quality. Common psychometric methods evaluate\ntest reliability by the internal consistency of question items within\na test, e.g., using a Rasch model [54], Item Response Theory (IRT)\nmodel [20], or Cronbach’s alpha [11, 47]. A unique challenge is that\nit requires substantial response data for such models to effectively\nprune out low quality (inconsistent) question items, making psycho-\nmetric methods expensive and impractical to use in most college\nclassrooms. Although teachers might be able to apply psychometric\nmethods between semesters, most teachers do not have access to\nstudent response data the first time they assign the questions.\nWe propose a modular prompt architecture Generative Students,\nin which we leverage large language models (LLM) to simulate\nstudent profiles. In this paper, we demonstrate that we can have\nthe generative students answer multiple-choice questions and use\nthe responses to identify unreliable question items. The design\nof the prompt architecture is based on the Knowledge-Learning-\nInstruction framework [25], which uses Knowledge Components\n(KCs) to define the elements students are expected to learn. In Gen-\nerative Students, we simulate student profiles by the KCs they have\nmastered. In particular, for every given KC, the student may have\nmastered it, have confusion about it, or have not shown understand-\ning of it. A student profile is essentially a function of the list of\nKCs they have mastered, have confusion about, or have not shown\nunderstanding of. We propose Generative Students as an approach\nthat does not require students’ historical performance data. Instead,\nwe rely on instructors to provide input on the knowledge compo-\nnents required for skill mastery and the common misconceptions\nthat they anticipate. This makes Generative Students potentially\nmore generalizable to domains that do not have a lot of historical\ndata. Generative students can be created within seconds and pro-\nduce a large amount of response data to a given set of questions.\nWe aim to address the research questions of: 1) Is it possible to\nuse LLMs to successfully simulate student profiles and generate\nbelievable answers to questions? 2) How do the generative students’\nresponses contrast with authentic students’ answers?\nWe study these questions in the context of teaching and learn-\ning heuristic evaluation, a usability inspection method. The reason\nwe pick heuristic evaluation as the subject domain is two-fold: 1)\nWe have collected an authentic student response dataset with 20\nmultiple-choice questions (MCQs) on this topic, which enables the\ncomparison between generative students and real students. 2) The\ntopic of heuristic evaluation has well-defined knowledge compo-\nnents (KCs). In particular, learners need to master 10 Nielsen’s\nheuristic rules and check them against a design. We can conve-\nniently denote each of the 10 heuristics as a KC.\nUsing the prompt architecture, we created 45 generative students\nwith different mastery levels on the 10 KCs, and had them answer\n20 MCQs. Each response to an MCQ is an API call to GPT-4. The\nLLM’s response contains both the answer and a rationale for picking\nthe answer. We first performed a qualitative analysis of the LLMs’\nresponses showing that the generative students produced logical\nand believable responses that are aligned with their profiles.\nWe then compared the 45 generative students and 100 real col-\nlege students on their responses to the same set of 20 MCQs. To\ninvestigate how well a brutal force simulation approach would per-\nform, we added a third condition, where we simulated 45 students\nusing random number generation. Each random student has a 70%\nchance of getting each question correctly. The comparison between\nreal students, generative students, and random students shows\nthat, the real students and generative students have a high consis-\ntency in their responses measured by Pearson’s correlation (r=0.72).\nHowever, the real students’ and random students’ answers are not\ncorrelated (r=-0.16). Moreover, we see a reasonable overlap on the\neasy and hard questions identified by generative students and real\nstudents, which shows potential of using generative students to\nsignal questions that need revision.\nThis study generates insights on creating LLM agents that have\nspecific knowledge deficiency, when the LLM itself has perfect\ncontent knowledge. Specifically, we asked the LLM agents to play\nthe role of a teacher and predict a student’s answer to a question.\nThis is the first study to our knowledge that shows promising re-\nsults of leveraging LLM-simulated student profiles to help evaluate\nmultiple-choice question items, without the requirement of stu-\ndent historical performance data. This opens up avenues for using\ngenerative students to support rapid prototyping and iteration of\nquestions. We discuss the potential risks of the approach and the\nnecessity of eliciting instructor (expert) input to steer the process.\n2 RELATED WORK\n2.1 Automatic Question Generation for\nEducational Purposes\nIt has been a long standing interest of the Learning@Scale and AI\nin Education communities to study question generation techniques.\nGenerative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation L@S ’24, July 18–20, 2024, Atlanta, GA, USA\nOne line of work uses crowdsourcing techniques [42]. For exam-\nple, UpGrade creates questions based on prior student solutions\n[47] and QMAps encourages students to generate questions for\neach other [57]. Another line of work develops end-to-end NLP\nmodels for question creation, which are good at creating factual\nquestions [13, 28], while not being able to generate questions that\ntarget higher Bloom goals [7]. On multiple-choice question (MCQ)\ngeneration, prior approaches used name entity recognition and\ntopic modeling to identify salient sentences and extract keywords\nfor question options [30, 31]. Recent work has also explored human-\nAI collaborative methods for MCQ creation, where instructors select\ntext input for the options [29]. Existing AI-assisted question gener-\nation systems face a common challenge, i.e., how to evaluate the\nquality of generated question items. In this work, we explore the\nfeasibility of leveraging LLMs to simulate student responses and\nuse them to evaluate auto-generated question items.\n2.2 Metrics and Approaches to Evaluate\nQuestions\nPrior work has explored various strategies to evaluate question\nquality based on student data. Both learner subjective ratings [53]\nand student performance data [ 18, 23] were used to select high-\nquality content. Item difficulty and discrimination indices evaluate\nwhether the questions are differentiable [23], while psychometric\nmethods are used to evaluate the inner consistency of the questions\n[11, 20, 23, 54]. However, although these post-hoc analyses may\nfavor future students, the question items with low quality in the first\nplace might waste students’ learning opportunities. Another line of\napproach focuses on evaluating questions based on the descriptions\nsolely, using rubrics and guidelines like Bloom’s Taxonomy [49]\nand item-writing flaws [ 35]. The questions are viewed as lower\nquality if they only involve a low level of cognitive process [ 10],\nand if they violate multiple rules in the item-writing flaws [ 34].\nBased on these rules, prior work explored automatic quality control\napproaches using supervised learning [49], neural networks [39]\nand LLMs [34] to reduce the need for human labor. However, these\nrule-based evaluations do not take into account students’ obstacles\nin learning and could be biased by expert blindspots [46].\n2.3 Generative agents\nThe success of LLMs in reasoning [14] and problem-solving abilities\n[37] have attracted growing interest in LLM-powered generative\nagents [38, 45]. Prior work has shown LLMs can be prompted to\ngenerate believable behavior [38], and even act like humans from\ncertain sub-populations [4, 32]. By creating modules to simulate\nhuman memory, planning and reflection, the generative agents\ncan mimic the logic in human behavior and result in a believable\ndecision-making process [38]. Prior work has shown the potential\nof generative agents in realistically simulating human behavior in\nvarious areas, including strategic gaming [44, 56], social network-\ning [38], and role-playing [ 51]. However, existing work focuses\non simulating characteristics with different perspectives like oc-\ncupations, personalities, values and relationships [58], where the\ncharacter will make decisions to the best of their knowledge or\nmemory. However, in this work, we aim to simulate agents that\nhave knowledge deficiencies and will make mistakes when solving\neducational problems.\nRecent work explored using generative agents in education [19,\n22, 32, 40]. Xu and Zhang showed the potential of using LLM to\nmodel students’ learning based on past assessment scores [ 55].\nResearchers have also developed teachable agents, which provide\npractice opportunities for learners to identify knowledge gaps [22]\nand for instructors to receive feedback from students [32]. However,\nprevious simulations often rely on historical student performance\ndata. Our work presents a prompt architecture to create generative\nstudents using a data-sparse approach, where we rely on experts to\nprovide a list of knowledge components required for skill mastery\nand a list of common student misconceptions.\n2.4 Prompt Engineering\nPrompting has become a main way of utilizing and steering LLMs\n[8]. Prior work has looked into different guidelines for creating\neffective prompts, including prompt structures [8], languages [52]\nand vocabulary [36]. One direction of prompting is to use few-shot\nlearning, where the task is demonstrated with several example\ninput-output pairs [8]. Wei et al. proposed Chain-of-Thought as a\nmethod to improve few-shot learning performance by including the\nthought process in the prompt [52]. However, prompt engineering\nis still a non-intuitive skill [ 36] and can be challenging for non-\nexperts. In our work, we investigate methods to prompt LLMs to\nbehave like students with knowledge deficiencies. We summarize\nprompt engineering strategies using LLMs to simulate students.\n3 GENERATIVE STUDENTS PROMPT\nARCHITECTURE\nWe propose a prompt architecture based on a widely adopted frame-\nwork in learning sciences, the Knowledge-Learning-Instruction\n(KLI) framework [ 27]. In particular, the KLI framework defines\nthe finest units of information learners are expected to learn as\nKnowledge Components (KCs). KCs are supposed to be mutually\nexclusive and provide the basis to design instructional activities.\nStudents acquire the KCs in their learning processes. The prompt\narchitecture we propose defines a student profile as a function of\nthe list of KCs the student has mastered, has confusion about, or\nhas not shown evidence of knowledge on.\n3.1 Implementation of Generative Students on\nHeuristic Evaluation\nIn this paper, we implemented the concept of generative students\non the topic of heuristic evaluation. Heuristic evaluation is a widely\nused usability inspection method, in which designers use rules\nof thumb to inspect the usability of user interfaces and identify\ndesign problems. We choose the topic of heuristic evaluation as\nour subject domain for two reasons. First, we have collected a\nstudent response dataset that contains 100 students’ responses to\n20 MCQs on heuristic evaluation. This makes it possible to compare\ngenerative students’ responses to real students’ responses. Second,\nthe topic of heuristic evaluation has well-defined KCs, namely the\n10 Nielsen’s heuristic rules, as shown in Table 1. It removes the\nrequirement for performing a cognitive task analysis in order to\ninfer the KCs needed to complete this task. We acknowledge that\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\nTable 1: The topic of heuristic evaluation contains 10 knowl-\nedge components (KCs), namely the 10 Nielsen’s heuristic\nrules. A generative student profile is a function of which KCs\nthe student has mastered, has confusion about, or has shown\nno evidence of knowledge on.\nH1 Visibility of system status\nH2 Match between the system and the real world\nH3 User control and freedom\nH4 Consistency and standards\nH5 Error prevention\nH6 Recognition Rather than Recall\nH7 Flexibility and Efficiency of Use\nH8 Aesthetic and minimalist design\nH9 Help users recognize, diagnose, and recover from errors\nH10 Help and Documentation\nthe task of heuristic evaluation is unique in that the 10 KCs are\nclear-cut and concurrent, and that there are fewer dependencies\namong the KCs. We will discuss in later sections on the potential\nof generalizing Generative Students to other domains.\n3.2 Final Prompt Structure and Examples\nThe template of the prompt we used to create generative students\nis shown in Figure 2. The prompt has three main parts: 1) an in-\ntroduction of the task; 2) an illustration of the generative student\nprofile; 3) an MCQ to which the generative student will produce an\nanswer. First, the introduction specifies that the model is playing\nthe role of a teacher predicting a student’s answer. We will explain\nthe rationale of asking the model to simulate a teacher instead of\ndirectly simulating a student answering a question in 3.4.2. Second,\nthe generative student profile is a function of lists of heuristic rules\nthe student has mastered, has confusion about, or has shown no\nevidence of knowledge of. For example, generative student 1 (GS1)\nhas mastered five rules, is confused between two rules, and has not\nshown evidence of knowledge of three rules. For each mastered\nrule, it calls the mastered rule prompt function, which gives an\nexample question demonstrating the student has given a correct\nanswer to this question. The confusion prompt requires two rules as\nthe input, and uses two example questions to show that the student\nhas confusion between these two rules. There is no prompt for the\nunknown rules. Third, the model is asked to predict an answer to a\nnew MCQ. With a different input on the list of mastered, confusion,\nand unknown rules, the template generates a different generative\nstudent profile (e.g., GS2 in Figure 2).\n3.3 Input to the Prompt Template\nThe example questions and answers used in the prompt are provided\nby an instructor who has been teaching this topic for 5 years. The\ninstructor provided one example question for each of the 10 heuris-\ntic rules, indicating the student has mastered the rule. Moreover,\nthe instructor suggested 2 pairs of common confusions: between\n\"H3-User control and freedom\" and \"H7-Flexibility and efficiency\nof use\", and between \"H5-Error prevention\" and \"H9-Help users\nrecognize, diagnose, and recover from errors\". We also created 2\npairs of random confusions. This gives us 4 pairs of confusion rules.\nFor each pair, the instructor provided us with 2 example questions,\nwhere both heuristic rules are in the options and the correct an-\nswer is one of them, as shown in Figure 2(b1). Please note that\nthe example questions the instructor provided to us were similar\nin style to the 20 new questions we’re producing answers on. But\nthe 20 questions are completely new questions that the generative\nstudents have not answered before. To summarize, the Generative\nStudents prompt architecture requires the following input from an\nexpert:\n•Knowledge components(KCs) required to perform a task.\n•An example question per KC with the correct answer to\ndemonstrate a student has mastered this KC.\n•Common student misconceptions (optional). In the case of\nheuristic evaluation, a misconception involves two KCs.\n•Example question(s) per confusion with an incorrect answer\nto demonstrate a student has confusion about this KC. In\nthe case of heuristic evaluation, it requires two example\nquestions.\n3.4 Takeaways from the Prompt Engineering\nProcess\nIn this section, we describe our takeaways from the prompt engi-\nneering process that led to the final prompt as shown in Figure 2.\n3.4.1 Providing example MCQs and answers improves performance.\nWe found that using example questions to indicate the student\nhas mastered or has confusion about a rule is more effective than\nsimply stating it, in line with prior work [41, 52]. In particular, for\nthe confusion prompt component, we first tried specifying that\nthe student was confused about one rule. However, it will bias the\nmodel towards always picking or not picking one choice. From our\ntrial-and-error process, the current prompt that takes two rules as\narguments works best in simulating a student’s confusion. More-\nover, we need two example questions to demonstrate the student\ncan make a mistake in both directions. When we only use one ex-\nample question in the prompt, the model would mistakenly think\nthe generative student will always pick one rule over the other.\n3.4.2 Asking the model to role-play as an instructor and predict the\ngenerative student’s answer helps. Instead of prompting the LLM\nto act as a student and \"answer\" the questions directly, we ask it\nto act as a teacher who wants to \"predict\" the student’s answer.\nWe found that when asked to answer the questions based on the\nstudent’s profile, the LLM is more likely to answer based on its prior\nknowledge. For example, even when we specify in the prompt that\nthe student has confusion about a rule, the model will still answer\na related question correctly. On the other hand, when we prompt\nthe model to act as a teacher to predict the student’s answer, the\nmodel’s performance is better aligned with the student’s profile.\n3.4.3 Using unknown rules to increase uncertainty in the predicted\nanswers. To better simulate real students’ responses to the ques-\ntions, we’d like to introduce some uncertainty on the generative\nstudents’ answers. We found that specifying some unknown rules,\ni.e., leaving them blank without any explicit prompting, achieves\nthis goal. For example, for GS1, H4, H9, and H10 are unknown rules.\nWe do not have any prompt components that specify the student’s\nGenerative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation L@S ’24, July 18–20, 2024, Atlanta, GA, USA\nFigure 2: The prompt template has three main parts: 1) an introduction of the task (c1); 2) an illustration of the generative\nstudent profile (c2); 3) a new MCQ to which the generative student will answer (c3). The generative student profile is a function\nof the list of heuristic rules the student has mastered, has confusion about, or has no evidence of knowledge of (a). For each\nmastered heuristic rule, we used an example MCQ to indicate the student has sufficient knowledge (b2); For each pair of\nconfusion heuristic rules, we used two example MCQs to indicate the student may mistakenly choose one over the other (b1).\nknowledge on these KCs. It turns out to be effective in introducing\nuncertainty in the generative students’ answers.\n3.4.4 Introducing uncertainty within the confusion prompt compo-\nnent by providing both positive and negative examples. In reality,\neven when a student has confusion between two rules, they may\nstill get easy questions correct. To simulate this uncertainty, we\ncreated a variation of the confusion prompt component, where\nthe student has shown some understanding, but hasn’t mastered it\nyet. We introduce example questions with varying difficulty in the\nprompt. For example, we specify that the generative student can\nanswer the easy questions correctly while making errors on more\ndifficult questions. We found that the variation of the confusion\nprompt introduced more uncertainty that aligned with the prompt\nspecification.\n3.4.5 Using the prompt to get a generative student’s response to one\nquestion at a time gives better results. We found that prompting the\ngenerative students to answer 20 questions all at once did not work\nwell. First, due to the token limit in each response, the response to\neach question is shorter. As a result, we observe shallower reasoning.\nSecond, if asked to predict 20 questions at a time, the model will use\nits answer to a former question to predict a later question. Moreover,\na previous question’s response may also override the generative\nstudent’s profile, leading to answers misaligned with the profile.\n4 GENERATIVE STUDENTS RESPONSE\nDATASET\n4.1 Creation of 45 Generative Students\nAs we noted earlier, each generative student is a function of the\nlist of rules they have mastered, have confusion about, or have no\nevidence of knowledge of. We can pass the list of rules as parameters\nto automatically create the generative students, as shown in Figure 2.\nA decision we need to make is how knowledgeable the generative\nstudent is, e.g., the student can master 3 rules, 5 rules or 9 rules. In\nthis experiment, we created a suite of 10 struggling students, 30\naverage students, and 5 advanced students, as shown in Table 2.\nThe advanced students are more knowledgeable because they have\nless confusion as specified in the prompts. With the distribution set\n(as shown in Table 2), one can randomly select heuristic rules from\nthe list to automatically create generative students. In this study,\nwe applied a semi-random approach instead of a fully random one\nto better contrast different generative student profiles. We created\npairs of similar student profiles, where only one variable differed\nwith everything else being the same between the two profiles. For\nexample, to create two generative students with the same set of\nmastered rules, but a different pair of confused rules, we would\nrandomly pick a confusion pair for each student, and then randomly\npick the same set of 5 mastered rules for both of them.\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\nTable 2: The distribution of the number of mastered, confused\nand unknown rules among the 45 generative students.\n# of mastered\nHE rules\npair of confused\nHE rules\n# of un-\nknown rules\n# of genera-\ntive students\n5 2 3 15 (Average)\n7 2 1 15 (Average)\n5 4 1 5 (Struggle)\n3 4 3 5 (Struggle)\n5 2 (focused con-\nfusion prompt)\n3 5 (Advanced)\n4.2 Implementation\nWe used the OpenAI API to provide inputs and get responses from\nGPT-4. As shown in Section 3.4.5, each API call contains only one\ngenerative student profile, and only predicts the response to one\nquestion. To ensure getting the most reliable prediction, we set the\ntemperature of the model to 0, which ensures that we are getting\nthe exactly same result from GPT with the same prompt.\n4.3 Example GPT-4 Responses\nWith each API call, the model will output a predicted response, i.e.,\nwhich option the student would pick, and a rationale explaining\nwhy the model believes the student will pick this option. Here we\npresent two examples of the responses generated by GPT-4. In this\nexample, the generative student has confusion between \"Flexibility\nand efficiency of use\" and \"User control and freedom\". The correct\nanswer to the question is \"User control and freedom\", and \"Flexibil-\nity and efficiency of use\" is among the options. It is predicted that\nJohn would incorrectly choose \"Flexibility and efficiency of use\"\nbased on the reasoning:\nGiven that John incorrectly chose \"Flexibility and ef-\nficiency of use\" over \"User control and freedom\" for\na question where the latter was the correct answer,\nthere’s a possibility that he might repeat this mistake\nif he perceives the new scenario as a limitation in the\nsystem’s flexibility or efficiency, rather than an issue\nof user control.\nIn another question, the correct answer is a rule John has mas-\ntered, \"Visibility of system status\". None of John’s confused rules\nare among the options. It is predicted that John would choose the\ncorrect answer with the reasoning:\nConsidering John’s track record, it’s plausible that he\ncould again accurately recognize this scenario as a\nviolation of \"Visibility of system status. \" His confusion\ndoesn’t directly apply to this scenario, so it’s less likely\nto influence his answer here.\n5 EVALUATION OF THE OUTPUTS BY\nGENERATIVE STUDENTS\n5.1 Methods\nTo investigate whether the answers are aligned with the generative\nstudents’ profiles, we applied both quantitative and qualitative\nanalyses. First, we counted the number of instances the generative\nstudents answered correctly across different conditions, e.g., when\nthe student has mastered a rule versus not. Second, we analyzed\nthe rationale the model generated using affinity diagrams [ 33],\nwhere we iteratively grouped the reasoning and identified common\nthemes.\n5.2 Generative Students’ Answers are Generally\nAligned with the Student Profiles\nTable 3: Performance by the generative students who have\nmastered 5 rules, have confusion between 1 pair of rules. In\nthe target question, when the correct answer is a mastered\nrule, students demonstrate good performance. When the\ncorrect answer is one rule in the confused pair, the accuracy\nis generally low. When the correct answer is an unknown\nrule, there is a 30%-50% chance that the student can answer\ncorrectly. In all three conditions, students’ performance is\nlower when the confused rules are present in the distractors.\nCorrect\nAnswer\nConfusion\nin Other\nOptions\n%Correct Total\nResponses\n% Choosing\nConfused\nRule\nMastered No 85.2 64 -\nMastered Yes 72.4 125 59.4\nConfused No 35.6 52 -\nConfused Yes 11.0 41 82.2\nUnknown No 52.1 47 -\nUnknown Yes 34.5 71 46.2\n5.2.1 Generative students are likely to answer a question correctly\nwhen the correct answer is a \"mastered\" rule. When the correct an-\nswer of the MCQ is a mastered rule, and no confused rules are\npresent in the distractors, the student is likely to answer correctly\n(85.2% of the time). For example, GS9 has mastered the rule \"User\ncontrol and freedom\", he is predicted to correctly answer a question\non the same heuristic because\"His past performance indicates a good\ngrasp of this particular heuristic, suggesting he is likely to apply it\ncorrectly again. \" When confused rules present in the options, the ac-\ncuracy is slightly lower (72.4%). Here is one sample of GS8 predicted\nto answer Q5 correctly where the correct answer is a mastered rule\nand both confused rules are among the options. GPT-4 reasoned\nthat \"the new question directly concerns the visibility of system status,\nan area John previously demonstrated an understanding of. Addition-\nally, the new question does not directly involve distinguishing between\n’Error prevention’ and ’Help users recognize, diagnose, and recover\nfrom errors, ’ areas where John showed confusion. \"\nWe summarized two scenarios where the student answers the\nquestion wrongly when the options contained a confused rule\n(27.6%). First, students might incorrectly choose the confused rules.\nSecond, the generative student may pick a suboptimal answer but\nthe reasoning shows a legitimate understanding of the mastered\nrule. This happened when the question stem was vague and multi-\nple answers could be correct. For example, Q20 is about the system\nnot supporting using the ’tab’ key to navigate the form, the cor-\nrect answer is \"Flexibility and efficiency of use\". However, given\nthe prevalence of the ’tab’ key function, this scenario can also be\ninterpreted as violating \"Consistency and standards\". Although GS9\nGenerative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation L@S ’24, July 18–20, 2024, Atlanta, GA, USA\nhas mastered \"Flexibility and efficiency of use\", they are predicted\nto choose \"Consistency and standards\" for this reason.\n5.2.2 Generative students are likely to answer the questions wrong\nwhen the correct answer is a \"confused\" rule. When the correct an-\nswer is a heuristic that the student has shown confusion about, the\nstudent will have a high likelihood of getting it wrong. The chance\nof getting it wrong is in particular high, when the other rule in the\nconfusion pair is present in the options, as shown in Table 3.\n5.2.3 Generative students could correctly answer a question when\nthe correct answer is a \"confused\" rule, if the other rule in the confu-\nsion pair is not among the options. For example, the correct answer\nfor Q9 is \"Help users recognize, diagnose, and recover from errors\".\nAlthough GS8 is confused between \"Error prevention\" and \"Help\nusers recognize, diagnose, and recover from errors\", they are pre-\ndicted to answer this question correctly since \"Error prevention\" is\nnot in the options. GPT-4 predicts \"it’s more likely that they might\nchoose the option that is closest to dealing with errors ... ’Help users\nrecognize, diagnose, and recover from errors’\" . The generative student\nmight also get correct by eliminating other options. For example,\nGS2 confuses about the correct answer in Q6, but since the other\nconfusion rules are not among the options, and they have mastered\nthe rules in all the other options, GPT-4 predicts that\"he might elim-\ninate these options because (they could identify) they (other options)\ndon’t fit the scenario as well as ’Visibility of system status’ does. \"\n5.2.4 Generative students are likely to be wrong when the correct\nanswer is an \"unknown\" rule, and there is a \"confused\" rule in the\noption. However, when the options do not contain a confused rule,\nthe generative students show a slightly above 50% chance to get\nthe questions right. For example, Q2 doesn’t contain any confused\nrules for GS3 in the options, and the correct answer is an unknown\nrule. GS3 is predicted to answer it correctly because\"the clear match\nbetween the scenario’s description and the fundamental concept of\n’User control and freedom’. \" and the fact that \"the situation doesn’t\ndirectly involve error messages or prevention\" . Moreover, GPT-4 also\npredicts the student’s knowledge of the unknown rule based on\ntheir knowledge of related heuristic rules. For example, the correct\nanswer to Q4-\"Consistency and standards\"- is an unknown rule to\nGS8. Q4’s options do not contain any confused rules for GS8. GPT-4\npredicts that since they \"correctly answered questions related to user\ninterface design and user experience consistency (’Visibility of system\nstatus, ’ ’Aesthetic and minimalist design’, and ’Match between system\nand the real world, ’ )\", \"there is a reasonable chance that they might\nselect the correct answer\" .\n5.3 Controlled Variable Generation in the\nPrompt Leads to Probable Outcomes\nAs mentioned earlier, we applied a semi-random approach when\nselecting the list of heuristic rules to create the generative students.\nWe created pairs of similar student profiles, where only one variable\ndiffered with everything else being the same between the two pro-\nfiles. This allows us to examine whether the change of one variable\nin the prompt leads to outputs that align with our expectations.\n5.3.1 Changing only the confusion pair in the student profile yields\nreasonable outputs. We created multiple pairs of similar student\nprofiles where the only difference was the confusion pair. We found\nthat the generative students would produce outputs that are aligned\nwith our expectations. For example, the correct answer for Q15 is\n\"Flexibility and efficiency of use\". GS4 has shown confusion between\n\"Flexibility and efficiency of use\" and \"Recognition rather than\nrecall\", so they are predicted to choose \"Recognition rather than\nrecall\" in this question. On the other hand, GS11 has the same set\nof mastered rules as GS4, and the only difference between their\nprofile is that GS11 is confused between \"Flexibility and efficiency\nof use\" and \"User control and freedom\". For this question, GS11 is\nthus predicted to choose \"User control and freedom\".\n5.3.2 A more knowledgeable student is more likely to answer ques-\ntions on unknown rules correctly. Consider a generative student that\nhas mastered 5 rules, has 1 pair of confused rules, and 3 unknown\nrules. If we make this student more knowledgeable by adding 2\nmastered rules, while keeping the original confusion pair, our ex-\nperiment shows that the more knowledgeable student will be more\nlikely to correctly answer questions on unknown rules. This aligns\nwith our expectations. For example, Q10’s correct answer \"Visibil-\nity of system status\" is a confused rule to GS2. Among the other\n3 options of Q10, GS2 has mastered one rule and the other two\nrules are unknown. GS2 is predicted to answer Q10 incorrectly.\nWhen we create a new generative student GS12 with additional\nmastery of the two unknown options, they answer Q10 correctly,\nsince they can successfully eliminate the options. Moreover, an\nincreased understanding of heuristic rules in general indicates an\nimproved overall ability to identify heuristic rules. In GS12’s case,\nthe increased understanding in other heuristics indicated that they\nmight be correct in answering questions with unknown rules as\nreasoned by the model: \"[because] their overall good performance on\nquestions related to the usability heuristics that directly impact user\ninteraction and control. \"\n5.4 The Focused Confusion Prompt Introduces\nUncertainty and Improves Students’ Overall\nPerformance on Questions Related to the\nConfused Rules\nTable 4: We contrast the performance of generative students\nwho used the original confusion prompt versus the focused\nconfusion prompt. The focused confusion prompt suggests\nthat the student will answer easy questions related to the\nconfusion correctly. It aligned with our expectation that stu-\ndents using the focused confusion prompts have better per-\nformance.\nCorrect\nAnswer\nConfusion\nin Other\nOptions\n%Correct Total\nResponses\n%Choosing\nConfused\nRule\nConfused No 26.9 39 -\nConfused Yes 0 30 25\nConfused\nV-Prompt No 61.5 13 -\nConfused\nV-Prompt Yes 40.9 11 100\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\nFigure 3: The focused confusion prompt (right) contains the two original questions that the student got wrong (Q1, Q2), and\ntwo additional examples to show that the students may answer the easy questions correctly (Q3, Q4). Generative students who\nuse the focused confusion prompt are expected to have better overall performance. The focused confusion prompt aims to\nintroduce more uncertainty to better simulate realistic scenarios.\nWe implemented a focused confusion prompt to indicate that\neven when the student is confused between two rules, there is a\nchance that they may answer easy questions on the confused rules\ncorrectly. The GPT-4 output is aligned with the expectation, as\nshown in Table 4. The student profiles with the focused confusion\nprompt have a higher likelihood of answering the questions cor-\nrectly. We compare two generative students, GS1 and GS21, who\nhave the same profile, except that GS21 uses the focused confusion\nprompt on the same pair of confused rules. GS21 is predicted to\nanswer Q14 correctly, where the correct answer is a confused rule\nfor both GS21 and GS1, whereas GS1 is predicted to answer it incor-\nrectly. The reasoning for GS21’s correct answer is that \"considering\nthat they correctly identified ’Flexibility and efficiency of use’ in a\nprevious question where it was indeed the correct answer, there’s a\ngood chance he will choose the correct answer this time. \"\n6 COMPARISON BETWEEN REAL STUDENTS,\nGENERATIVE STUDENTS, AND RANDOM\nSTUDENTS\n6.1 Datasets\n6.1.1 Real Students’ Response Dataset. The same set of the 20\nMCQs have been previously assigned in a college-level course at an\nR1 institution in 2021 as a homework assignment. We got IRB ap-\nproval to collect student responses from that class. Students in the\ncourse were asked to complete the assignment through a website\nthat contains the same 20 MCQs on the topic of heuristic evaluation.\n100 students completed the assignment.\n6.1.2 Random Students’ Response Dataset. To investigate how well\nrandom simulations perform on this task and compare our princi-\npled simulation approach with a random one, we designed a baseline\ncondition where student responses were generated randomly. We\nrefer to these as random students. The random students are created\nbased on random number generation. For each question, there is\na 70% chance of getting the question correctly. We generated 45\nrandom students.\n6.2 Methods\nFirst, to check the consistency of real students’ responses with gen-\nerative and random students’ respectively, we computed Pearson’s\ncorrelation using the students’ average score on each question. Sec-\nond, we used Cronbach’s Alpha to measure the internal consistency\nof each dataset. Third, we identify hard and easy questions based\non the responses. We employed two thresholds: if the average score\nis above 80%, it’s considered to be an easy question, and if the\naverage score is below 40%, it’s considered to be a hard question.\nWe compared across the three conditions to assess the overlap on\nthe easy and hard questions identified. Moreover, for the questions\nwhere real students and generative students yielded different re-\nsults, we performed an error analysis analyzing the distribution of\nthe options students picked.\n6.3 Results\nFirst, as shown in Table 5, generative students’ responses show a\nhigh correlation with real students’ responses, with a Pearson’s\ncorrelation of 0.72. On the other hand, the correlation between the\nrandom students and the real students is only -0.16.\nSecond, the generative students’ responses dataset shows high\ninternal consistency as measured by Cronbach’s Alpha (0.6176), also\nshown in Table 5. The internal consistency is comparable to that of\nthe real students’ response dataset (0.559). However, the random\nstudents’ response data has low internal consistency (0.042).\nWe see 3 overlaps between the generative and real students’\ndatasets among the hard questions identified, and 2 overlaps among\nthe easy questions identified. However, there does not exist any\noverlap between the real and the random student datasets. The\nquestions that received low scores may suggest that the clarity\nof the questions needs to be improved. For these hard questions,\nGenerative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation L@S ’24, July 18–20, 2024, Atlanta, GA, USA\nTable 5: Average scores for each of the 20 MCQs across the\nthree response datasets. Green text indicates easy questions\n(>0.8) and red text indicates hard questions (<0.4). The ques-\ntions that receive very low scores may suggest that the clarity\nof the questions needs to be improved.\nReal\nStu-\ndents\nGenerative\nStudents\nRandom\nStu-\ndents\nReal\nStu-\ndents\nGenerative\nStudents\nRandom\nStu-\ndents\nQ1 0.76 0.54 0.73 Q11 0.84 (+) 0.8 (+) 0.6\nQ2 0.69 0.51 0.67 Q12 0.88 (+) 0.67 0.76\nQ3 0.56 0.22 (-) 0.73 Q13 0.16 (-) 0.04 (-) 0.69\nQ4 0.33 (-) 0.53 0.69 Q14 0.72 0.52 0.73\nQ5 0.74 0.57 0.56 Q15 0.52 0.26 (-) 0.82 (+)\nQ6 0.79 0.67 0.76 Q16 0.69 0.34 (-) 0.71\nQ7 0.84 (+) 0.72 0.62 Q17 0.36 (-) 0.34(-) 0.62\nQ8 0.64 0.67 0.82 (+) Q18 0.59 0.8 (+) 0.71\nQ9 0.45 0.37 (-) 0.8 (+) Q19 0.85(+) 0.94 (+) 0.62\nQ10 0.57 0.79 0.64 Q20 0.37 (-) 0.14 (-) 0.71\nPearson’s R with real student averages 1 0.72 -0.16\nTable 6: The real students’ and the generative students’ re-\nsponse datasets have a comparable medium-to-high value of\nCronbach’s Alpha indicating good internal consistency, in\ncontrast with the random students’ response dataset.\nReal\nStudents\nGenerative\nStudents\nRandom\nStudents\nCronbach’s Alpha 0.559 0.6176 0.042\nTable 7: There is considerable overlap between the real stu-\ndents and generative students on the distracting options they\npicked (chosen by over 25% of students) for the hard questions.\nInstructors might leverage such information to improve the\nclarity of the questions.\nQ3 Q9 Q13 Q15 Q17 Q20\nStudent Dataset A A, C C, D D A C. D\nGenerative Dataset A A, C C, D D B, C C, D\nwe further analyzed the distribution of students’ answers on the\noptions to reveal the sources of mistakes. In Table 7, we present\nthe frequent wrong answers chosen by over 25% of the students.\nThere is considerable overlap in the wrong options students picked\nbetween the real student and generative student response datasets.\nIf instructors are interested in improving the clarity of the questions,\nthey could leverage such information.\n6.4 Error Analysis\nWe further performed an error analysis to shed light on what caused\nthe generative students to answer questions differently from the\nreal students.\n6.4.1 Generative students had better performance on some questions\nbecause the real students’ confusion was not included in the profiles.\nThe generative students show higher performance in Q4, Q10 and\nQ18 in comparison with the real students. One reason is that most of\nthe options in these questions are not among the pairs of confusions.\nAs a result, most generative students won’t find any of the options\nto be confusing and will answer them correctly. However, real\nstudents show coherent confusion in these questions. 76% (51 out\nof 67) of the real students who got Q4 wrong and 49% of the students\nwho got Q18 wrong chose \"Flexibility and efficiency of use\" instead\nof \"Consistency and standards\". This suggests that the inclusion of\na more diverse set of confusion KCs may improve the proximity\nbetween the generative students’ with real students’ responses.\n6.4.2 Students’ confusion may be over-emphasized or over-generalized\nleading to more pessimistic predictions on some questions. The gen-\nerative students show a higher tendency to fail questions when the\noptions contain a heuristic they have confusion about. Although\nreal students may also make repeated errors, the portion is lower.\nFor example, about25% of the generative students lean to the wrong\noption \"Visibility of system status\" for Q7. The same trend also ap-\npears in the real students, but it only takes up 7% of the responses.\n6.4.3 LLM may lose focus on the question. When the question\ndescription emphasizes a seemingly positive feature, the generative\nstudents may misinterpret the question as asking for what heuristic\nit describes, instead of what it violates. For example, the description\nof Q3 reads \"There are several ways to browse different categories\nof products on the same page. The user can either click the ’Shop by\ncategory’ dropdown menu or click on the tabs from the main page. \" .\nMany of the generative students \"interpret the presence of multiple\nways to browse as a feature that enhances flexibility and efficiency\" .\n7 A POTENTIAL USE CASE LEVERAGING\nGENERATIVE STUDENTS TO IMPROVE\nQUESTION QUALITY\nWe did a case study with an instructor who was teaching heuristic\nevaluation in the Spring semester of 2024 at an R1 institution. Specif-\nically, we presented them with the original 20 questions, generative\nstudents’ responses, and asked them to revise some questions based\non the signals. The instructor specifically picked Q3, Q9, Q13, and\nQ20 which received average scores below 0.4 when answered by\ngenerative students (as shown in Table 5. The instructor considered\nthese 4 questions to be badly worded and improved the clarity based\non the wrong options the generative students picked. We then ran a\nclassroom experiment including the original and improved versions\nof the questions in a required quiz in a class with 280 students. The\ncase study and the classroom experiment are IRB approved.\nIn the classroom experiment, all consented students needed to\nanswer a 7-question quiz on heuristic evaluation. We created two\nversions of the quiz following a crossover design [46]. Quiz version\nA contains the improved version of Q9 and Q20, with the original\nversion of Q3, Q13, Q1, Q5, Q7. Quiz version B contains the im-\nproved version of Q3 and Q13, with the original version of Q9, Q20,\nQ1, Q5, Q7. All students were randomly assigned to a quiz version.\nQ1, Q5 and Q7 are shared between the two versions and used to\ncontrol for the two groups’ prior knowledge.\n7.1 Real Students Got Better Performance on\nthe Revised Questions\nA randomization check showed that students in the two quiz ver-\nsions showed similar performance on the shared questions Q1, Q5\nand Q7, indicating that the comparison between the two groups\nwas fair. To investigate whether the revised questions appeared\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\nTable 8: Students are randomly assigned to answer Version A\nor B of the quiz. For Q3, Q9, Q13 and Q20, the underlined ver-\nsion is the revised question leveraging generative students’\nsignals (including the wrong options picked by generative\nstudents). Q1, Q5, and Q7 are baseline questions that are the\nsame across the two versions. We observed that there is a sig-\nnificant improvement in the average question score between\nthe original and the revised version.\nQ3 Q9 Q13 Q20 Q5 Q1 Q7\nVersion A (n=147) 0.84 0.86 0.29 0.83 0.88 0.94 0.96\nVersion B (n=133) 0.81 0.68 0.80 0.50 0.81 0.93 0.92\nto be less difficult as intended, we built a mixed-effects logistic re-\ngression model where the dependent variable is the question score\n(0 being incorrect and 1 being correct), and the fixed effect is the\nquestion form, i.e., whether the question is original or modified. To\naccount for different question difficulty and varying student abili-\nties, we included a random slope for each question, and a random\nintercept for each student and each question [46]. We found that\nthe revised questions led to a significant increase in the question\nscore (z=-2.538, p=0.01 < 0.05). The average score improvement is\n0.248. The average score on each question is shown in Table 8. In\nparticular, there is no improved performance on Q3, probably due\nto the fact that students got reasonably high scores on the original\nquestion.\n8 DISCUSSION AND FUTURE WORK\nOur work shows promises of using the Generative Students prompt\narchitecture to simulate student profiles that can generate believ-\nable and logical responses to MCQs. One potential avenue of this\nwork is to help instructors quickly evaluate an initial set of ques-\ntions, identify bad items and improve them before assigning to real\nstudents. In this section, we discuss how far we are from that goal\nbased on the results presented from this study.\nFirst, we see promising results on the high correlation between\ngenerative students’ and real students’ responses, and overlap on\nthe questions that students answered poorly. We discussed several\nprompt engineering takeaways for simulating student behaviors.\nFirst, describing the task as a pedagogical prediction leads to pre-\ndictions more aligned with the student profiles. Second, illustrating\nstudents’ knowledge with example questions and answers result in\npredictions that better align with the profiles. Third, we explored\nmethods to introduce diversity and uncertainty when simulating\nstudents, including a focused confusion prompt, an unknown com-\nponent in the prompt architecture, and including example questions\nof varying difficulty when specifying the student profiles. On the\nother hand, we also revealed different reasoning models behind real\nand generative students. For example, generative students might\nappear to be more stubborn as they repeatedly make similar mis-\ntakes. It requires more experiments to enhance the proximity of\nthe generative students’ responses. For example, future work could\nexplore including a more diverse set of confused rules, and intro-\nducing more prompt variations similar to the focused confusion\nprompt to increase uncertainty.\nSecond, we propose a general-purpose architecture for simulat-\ning student profiles. Although we only demonstrated our pipeline\non one topic, i.e., heuristic evaluation, the architecture may be\napplied to other domains. For topics where the KCs are less well-\ndefined and have more dependencies, expert input and specifica-\ntions are required to ensure the generative students produce reliable\nand believable outputs. In future work, we plan to collaborate with\ninstructors to understand how they would define KCs, structure the\nprompts, gather examples, and interpret the results. We also aim to\ninvestigate the feasibility of this approach when the instructors are\nnot able to articulate the KCs required for skill mastery. We also\nplan to understand the time commitment from instructors to ensure\nthat this is a reasonable amount of effort for them to prototype and\niterate on their questions.\nThird, the confusions we included in the student profiles are\nbased on experts’ understanding of novice students’ challenges,\nwhich may not be comprehensive. We plan to further explore meth-\nods to communicate students’ knowledge to LLMs. For example,\nstudents’ historical performance data may provide a more accurate\nrepresentation of student knowledge, however it requires a lot of\ndata input and may be less scalable for everyday teaching. Future\nwork may explore methods to combine the expert-guided approach\nas proposed in Generative Students with a small set of student\nperformance data to improve the simulation output.\nFourth, the case study showed that generative students could\nprovide signals for instructors to improve their questions, e.g., help\nthem identify difficult questions, and give them insights on fre-\nquently picked wrong options. The case study showed that an\ninstructor could indeed leverage such information to iterate on the\nquestions and make them less difficult. However, we acknowledge\nthat \"less difficult\" does not necessarily indicate higher educational\nvalue. It requires further careful analysis to understand whether\nand when \"less difficult\" is desirable for instructors and students.\n9 CONCLUSION\nWe propose Generative Students, a prompt architecture using LLM\nto simulate student profiles and produce reliable and believable\nresponses to MCQs. With knowledge components (KCs) identified\nfor a given topic, a generative student profile is a function of the\nlist of KCs the student has mastered, has confusion about, or has\nshown no evidence of knowledge of. We show that providing con-\ncrete question-and-answer examples and having the model play\nthe role of a teacher to predict student performance helps simulate\nbelievable student behaviors. Our results suggest that generative\nstudents’ responses to the MCQs are aligned with their profiles, and\nexhibit a strong correlation with those of actual students. A sub-\nsequent case study demonstrates that generative students provide\nuseful signals for an instructor to identify badly-worded MCQs\nand improve them. A classroom experiment reveals that the re-\nvised questions, informed by the behaviors of generative students,\nbecome \"less difficult\" as intended.\nACKNOWLEDGMENTS\nThis work was funded by NSF Grants DRL-2335975,and IIS-2302564.\nThe findings and conclusions expressed in this material are those of\nthe author(s) and do not necessarily reflect the views of the National\nScience Foundation.\nGenerative Students: Using LLM-Simulated Student Profiles to Support Question Item Evaluation L@S ’24, July 18–20, 2024, Atlanta, GA, USA\nREFERENCES\n[1] [n. d.]. Incorporating ChatGPT Into Your Teaching. https://miamioh.edu/cte/\nfaculty-staff/chatgpt/. Accessed: 2024-02-16.\n[2] [n. d.]. OpenAI Chat. https://chat.openai.com/. Accessed: 2024-02-16.\n[3] [n. d.]. Using ChatGPT to Write Quiz Questions. https://online.ucla.edu/using-\nchatgpt-to-write-quiz-questions/. Accessed: 2024-02-16.\n[4] Angus Addlesee, Weronika Sieińska, Nancie Gunson, Daniel Hernández Garcia,\nChristian Dondrup, and Oliver Lemon. 2023. Multi-party goal tracking with llms:\nComparing pre-training, fine-tuning, and prompt engineering. arXiv preprint\narXiv:2308.15231 (2023).\n[5] Tahani Alsubait, Bijan Parsia, and Ulrike Sattler. 2016. Ontology-based multiple\nchoice question generation. KI-Künstliche Intelligenz 30, 2 (2016), 183–188.\n[6] Benjamin S Bloom. 1984. The 2 sigma problem: The search for methods of group\ninstruction as effective as one-to-one tutoring. Educational researcher 13, 6 (1984),\n4–16.\n[7] TAXONOMY MADE EASY BLOOM’S. 1965. Bloom’s taxonomy of educational\nobjectives. Longman.\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877–1901.\n[9] Michelene TH Chi and Ruth Wylie. 2014. The ICAP framework: Linking cognitive\nengagement to active learning outcomes. Educational Psychologist 49, 4 (2014),\n219–243.\n[10] Laecio A Costa, Laís N Salvador, and Ricardo R Amorim. 2018. Evaluation of\nacademic performance based on learning analytics and ontology: a systematic\nmapping study. In 2018 IEEE Frontiers in Education Conference (FIE) . IEEE, 1–5.\n[11] Lee J Cronbach. 1951. Coefficient alpha and the internal structure of tests.\npsychometrika 16, 3 (1951), 297–334.\n[12] Catherine H Crouch and Eric Mazur. 2001. Peer instruction: Ten years of experi-\nence and results. American journal of physics 69, 9 (2001), 970–977.\n[13] Bidyut Das, Mukta Majumder, Santanu Phadikar, and Arif Ahmed Sekh. 2021.\nAutomatic question generation and answer assessment: a survey. Research and\nPractice in Technology Enhanced Learning 16 (2021), 1–15.\n[14] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell,\nDharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models\nshow human-like content effects on reasoning. arXiv preprint arXiv:2207.07051\n(2022).\n[15] Louis Deslauriers, Logan S McCarty, Kelly Miller, Kristina Callaghan, and Greg\nKestin. 2019. Measuring actual learning versus feeling of learning in response to\nbeing actively engaged in the classroom. Proceedings of the National Academy of\nSciences 116, 39 (2019), 19251–19257.\n[16] K Anders Ericsson et al. 2006. The influence of experience and deliberate practice\non the development of superior expert performance. The Cambridge handbook of\nexpertise and expert performance 38 (2006), 685–705.\n[17] K Anders Ericsson, Ralf T Krampe, and Clemens Tesch-Römer. 1993. The role of\ndeliberate practice in the acquisition of expert performance. Psychological review\n100, 3 (1993), 363.\n[18] Education Testing Services (ETS). [n. d.]. Reliability and Comparability of TOEFL\niBT Scores. Technical Report.\n[19] Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon Ahn,\nand Alice Oh. 2023. ChEDDAR: Student-ChatGPT Dialogue in EFL Writing\nEducation. arXiv preprint arXiv:2309.13243 (2023).\n[20] Deborah Harris. 1989. Comparison of 1-, 2-, and 3-parameter IRT models. Educa-\ntional Measurement: Issues and Practice 8, 1 (1989), 35–41.\n[21] Petri Ihantola, Tuukka Ahoniemi, Ville Karavirta, and Otto Seppälä. 2010. Review\nof recent systems for automatic assessment of programming assignments. In\nProceedings of the 10th Koli calling international conference on computing education\nresearch. 86–93.\n[22] Hyoungwook Jin, Seonghee Lee, Hyungyu Shin, and Juho Kim. 2024. Teach\nAI How to Code: Using Large Language Models as Teachable Agents for Pro-\ngramming Education. In Proceedings of the CHI Conference on Human Factors in\nComputing Systems . 1–28.\n[23] Ahmad Zamri Khairani and Hasni Shamsuddin. 2016. Assessing item difficulty\nand discrimination indices of teacher-developed multiple-choice tests. In Assess-\nment for Learning Within and Beyond the Classroom: Taylor’s 8th Teaching and\nLearning Conference 2015 Proceedings . Springer, 417–426.\n[24] Kenneth R Koedinger, John R Anderson, William H Hadley, and Mary A Mark.\n1997. Intelligent tutoring goes to school in the big city. International Journal of\nArtificial Intelligence in Education (IJAIED) 8 (1997), 30–43.\n[25] Kenneth R Koedinger, Albert T Corbett, and Charles Perfetti. 2012. The\nKnowledge-Learning-Instruction framework: Bridging the science-practice\nchasm to enhance robust student learning.Cognitive science 36, 5 (2012), 757–798.\n[26] Kenneth R Koedinger, Jihee Kim, Julianna Zhuxin Jia, Elizabeth A McLaughlin,\nand Norman L Bier. 2015. Learning is not a spectator sport: Doing is better than\nwatching for learning from a MOOC. In Proceedings of the second (2015) ACM\nconference on learning@ scale . ACM, 111–120.\n[27] Kenneth R Koedinger, Jihee Kim, Julianna Zhuxin Jia, Elizabeth A McLaughlin,\nand Norman L Bier. 2015. Learning is not a spectator sport: Doing is better than\nwatching for learning from a MOOC. In Proceedings of the second (2015) ACM\nconference on learning@ scale . ACM, 111–120.\n[28] Ghader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and Salam Al-Emari. 2020. A\nSystematic Review of Automatic Question Generation for Educational Purposes.\nInternational Journal of Artificial Intelligence in Education 30, 1 (2020), 121–204.\n[29] Xinyi Lu, Simin Fan, Jessica Houghton, Lu Wang, and Xu Wang. 2023. Read-\ningQuizMaker: A Human-NLP Collaborative System that Supports Instructors\nto Design High-Quality Reading Quiz Questions. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems . 1–18.\n[30] Mukta Majumder and Sujan Kumar Saha. 2014. Automatic selection of infor-\nmative sentences: The sentences that can generate multiple choice questions.\nKnowledge Management & E-Learning: An International Journal 6 (2014), 377–391.\n[31] Mukta Majumder and Sujan Kumar Saha. 2015. A System for Generating Multi-\nple Choice Questions: With a Novel Approach for Sentence Selection. In NLP-\nTEA@ACL/IJCNLP.\n[32] Julia M Markel, Steven G Opferman, James A Landay, and Chris Piech. 2023.\nGPTeach: Interactive TA Training with GPT Based Students. (2023).\n[33] Bill Moggridge and Bill Atkinson. 2007. Designing interactions. Vol. 17. MIT press\nCambridge.\n[34] Steven Moore, Huy A Nguyen, Tianying Chen, and John Stamper. 2023. Assessing\nthe quality of multiple-choice questions using gpt-4 and rule-based methods. In\nEuropean Conference on Technology Enhanced Learning . Springer, 229–245.\n[35] Steven Moore, Huy A Nguyen, Ellen Fang, and John Stamper. 2023. Crowdsourc-\ning the Evaluation of Multiple-Choice Questions Using Item-Writing Flaws and\nBloom’s Taxonomy. (2023).\n[36] Jonas Oppenlaender, Rhema Linder, and Johanna Silvennoinen. 2023. Prompting\nai art: An investigation into the creative skill of prompt engineering. arXiv\npreprint arXiv:2303.13534 (2023).\n[37] Graziella Orrù, Andrea Piarulli, Ciro Conversano, and Angelo Gemignani. 2023.\nHuman-like problem-solving abilities in large language models using ChatGPT.\nFrontiers in Artificial Intelligence 6 (2023), 1199350.\n[38] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra\nof human behavior. In Proceedings of the 36th Annual ACM Symposium on User\nInterface Software and Technology . 1–22.\n[39] Stefan Ruseti, Mihai Dascalu, Amy M Johnson, Renu Balyan, Kristopher J Kopp,\nDanielle S McNamara, Scott A Crossley, and Stefan Trausan-Matu. 2018. Predict-\ning question quality using recurrent neural networks. In Artificial Intelligence\nin Education: 19th International Conference, AIED 2018, London, UK, June 27–30,\n2018, Proceedings, Part I 19 . Springer, 491–502.\n[40] Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, and Michael S\nBernstein. 2023. Rehearsal: Simulating conflict to teach conflict resolution. arXiv\npreprint arXiv:2309.12309 (2023).\n[41] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H.\nChi, Nathanael Schärli, and Denny Zhou. 2023. Large Language Models Can\nBe Easily Distracted by Irrelevant Context. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning (Proceedings of Machine Learning Re-\nsearch, Vol. 202) , Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 31210–31227.\nhttps://proceedings.mlr.press/v202/shi23a.html\n[42] Anjali Singh, Christopher Brooks, Yiwen Lin, and Warren Li. 2021. What’s In It for\nthe Learners? Evidence from a Randomized Field Experiment on Learnersourcing\nQuestions in a MOOC. InProceedings of the Eighth ACM Conference on Learning@\nScale. 221–233.\n[43] Katherine Stasaski and Marti A Hearst. 2017. Multiple choice question generation\nutilizing an ontology. In Proceedings of the 12th Workshop on Innovative Use of\nNLP for Building Educational Applications . 303–312.\n[44] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,\nLinxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied\nagent with large language models. arXiv preprint arXiv:2305.16291 (2023).\n[45] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on large\nlanguage model based autonomous agents.arXiv preprint arXiv:2308.11432 (2023).\n[46] Xu Wang, Carolyn Rose, and Ken Koedinger. 2021. Seeing beyond expert blind\nspots: Online learning design for scale and quality. In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems . 1–14.\n[47] Xu Wang, Srinivasa Teja Talluri, Carolyn Rose, and Kenneth Koedinger. 2019.\nUpGrade: Sourcing Student Open-Ended Solutions to Create Scalable Learning\nOpportunities. In Proceedings of the Sixth (2019) ACM Conference on Learning @\nScale (Chicago, IL, USA)(L@S ’19). ACM, New York, NY, USA, Article 17, 10 pages.\nhttps://doi.org/10.1145/3330430.3333614\n[48] Zichao Wang, Andrew S. Lan, Weili Nie, Andrew E. Waters, Phillip J. Grimaldi, and\nRichard G. Baraniuk. 2018. QG-Net: A Data-Driven Question Generation Model\nfor Educational Content. In Proceedings of the Fifth Annual ACM Conference on\nLearning at Scale (London, United Kingdom)(L@S ’18). Association for Computing\nMachinery, New York, NY, USA, Article 7, 10 pages. https://doi.org/10.1145/\nL@S ’24, July 18–20, 2024, Atlanta, GA, USA Xinyi Lu and Xu Wang\n3231644.3231654\n[49] Zichao Wang, Kyle Manning, Debshila Basu Mallick, and Richard G. Baraniuk.\n2021. Towards Blooms Taxonomy Classification Without Labels. In Artificial\nIntelligence in Education , Ido Roll, Danielle McNamara, Sergey Sosnovsky, Rose\nLuckin, and Vania Dimitrova (Eds.). Springer International Publishing, Cham,\n433–445.\n[50] Zichao Wang, Jakob Valdez, Debshila Basu Mallick, and Richard G Baraniuk.\n2022. Towards human-like educational question generation with large language\nmodels. In International conference on artificial intelligence in education . Springer,\n153–166.\n[51] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu\nZhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al.\n2023. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of\nlarge language models. arXiv preprint arXiv:2310.00746 (2023).\n[52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824–24837.\n[53] Joseph Jay Williams, Juho Kim, Anna Rafferty, Samuel Maldonado, Krzysztof Z.\nGajos, Walter S. Lasecki, and Neil Heffernan. 2016. AXIS: Generating Explanations\nat Scale with Learnersourcing and Machine Learning. In Proceedings of the Third\n(2016) ACM Conference on Learning @ Scale (Edinburgh, Scotland, UK) (L@S ’16) .\nACM, New York, NY, USA, 379–388. https://doi.org/10.1145/2876034.2876042\n[54] Mark Wilson and Paul De Boeck. 2004. Descriptive and explanatory item response\nmodels. In Explanatory item response models . Springer, 43–74.\n[55] Songlin Xu and Xinyu Zhang. 2023. Leveraging generative artificial intelligence\nto simulate student learning behavior. arXiv preprint arXiv:2310.19206 (2023).\n[56] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu,\nand Yang Liu. 2023. Exploring large language models for communication games:\nAn empirical study on werewolf. arXiv preprint arXiv:2309.04658 (2023).\n[57] Iman Yeckehzaare, Tirdad Barghi, and Paul Resnick. 2020. QMaps: Engaging\nStudents in Voluntary Question Generation and Linking (CHI ’20) . Association\nfor Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/\n3313831.3376882\n[58] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang\nQi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al.\n2023. Sotopia: Interactive evaluation for social intelligence in language agents.\narXiv preprint arXiv:2310.11667 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.689923107624054
    },
    {
      "name": "Generative grammar",
      "score": 0.6419005990028381
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45248159766197205
    },
    {
      "name": "Natural language processing",
      "score": 0.43043842911720276
    },
    {
      "name": "Support vector machine",
      "score": 0.41519036889076233
    },
    {
      "name": "Mathematics education",
      "score": 0.3683284819126129
    },
    {
      "name": "Multimedia",
      "score": 0.36432138085365295
    },
    {
      "name": "Psychology",
      "score": 0.11789554357528687
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}