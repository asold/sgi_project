{
  "title": "PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models",
  "url": "https://openalex.org/W4389518928",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5103068530",
      "name": "Xingwei He",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5029117071",
      "name": "A-Long Jin",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5071776082",
      "name": "Jun Ma",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100334733",
      "name": "Yuan Yuan",
      "affiliations": [
        null,
        "State Key Laboratory of Software Development Environment",
        "Beihang University",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A5025130883",
      "name": "Siu Ming Yiu",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W4361807267",
    "https://openalex.org/W2963416784",
    "https://openalex.org/W3202408265",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3173825754",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3099023595",
    "https://openalex.org/W3035525293",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W2741494657",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2251214593",
    "https://openalex.org/W3172044616",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4385574114",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3200796554",
    "https://openalex.org/W3172669006",
    "https://openalex.org/W4382202633",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3102645206",
    "https://openalex.org/W2470324779",
    "https://openalex.org/W3199926081",
    "https://openalex.org/W2970429618",
    "https://openalex.org/W2998112083",
    "https://openalex.org/W2534253848"
  ],
  "abstract": "Factual Error Correction (FEC) aims to rectify false claims by making minimal revisions to align them more accurately with supporting evidence. However, the lack of datasets containing false claims and their corresponding corrections has impeded progress in this field. Existing distantly supervised models typically employ the mask-then-correct paradigm, where a masker identifies problematic spans in false claims, followed by a corrector to predict the masked portions. Unfortunately, accurately identifying errors in claims is challenging, leading to issues like over-erasure and incorrect masking. To overcome these challenges, we present PivotFEC, a method that enhances few-shot FEC with a pivot task approach using large language models (LLMs). Specifically, we introduce a pivot task called factual error injection, which leverages LLMs (e.g., ChatGPT) to intentionally generate text containing factual errors under few-shot settings; then, the generated text with factual errors can be used to train the FEC corrector. Our experiments on a public dataset demonstrate the effectiveness of PivotFEC in two significant ways: Firstly, it improves the widely-adopted SARI metrics by 11.3 compared to the best-performing distantly supervised methods. Secondly, it outperforms its few-shot counterpart (i.e., LLMs are directly used to solve FEC) by 7.9 points in SARI, validating the efficacy of our proposed pivot task.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9960–9976\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPivotFEC: Enhancing Few-shot Factual Error Correction with\na Pivot Task Approach using Large Language Models\nXingwei He1, A-Long Jin 1, Jun Ma 1, Yuan Yuan 2,3,4,†, Siu Ming Yiu 1,†\n1The University of Hong Kong, Hong Kong, China\n2School of Computer Science and Engineering, Beihang University, Beijing, China\n3State Key Laboratory of Software, Development Environment,4Zhongguancun Laboratory\nhexingwei15@gmail.com, ajin@eee.hku.hk, junma@hku.hk,\nyuan21@buaa.edu.cn, smyiu@cs.hku.hk\nAbstract\nFactual Error Correction (FEC) aims to rec-\ntify false claims by making minimal revisions\nto align them more accurately with support-\ning evidence. However, the lack of datasets\ncontaining false claims and their correspond-\ning corrections has impeded progress in this\nfield. Existing distantly supervised models typ-\nically employ the mask-then-correct paradigm,\nwhere a masker identifies problematic spans in\nfalse claims, followed by a corrector to predict\nthe masked portions. Unfortunately, accurately\nidentifying errors in claims is challenging, lead-\ning to issues like over-erasure and incorrect\nmasking. To overcome these challenges, we\npresent PivotFEC, a method that enhances few-\nshot FEC with a pivot task approach using large\nlanguage models (LLMs). Specifically, we in-\ntroduce a pivot task called factual error injec-\ntion, which leverages LLMs (e.g., ChatGPT)\nto intentionally generate text containing factual\nerrors under few-shot settings; then, the gen-\nerated text with factual errors can be used to\ntrain the FEC corrector. Our experiments on\na public dataset demonstrate the effectiveness\nof PivotFEC in two significant ways: Firstly, it\nimproves the widely-adopted SARI metrics by\n11.3 compared to the best-performing distantly\nsupervised methods. Secondly, it outperforms\nits few-shot counterpart (i.e., LLMs are directly\nused to solve FEC) by 7.9 points in SARI, vali-\ndating the efficacy of our proposed pivot task.\n1 Introduction\nChatGPT is an artificial intelligence chatbot re-\nleased by OpenAI on November 30, 2022 and\nbuilt upon on the company’s Generative Pre-\ntrained Transformer (GPT) series of large language\n(LLMs) (Ouyang et al., 2022; OpenAI, 2023).\nSince its launch, ChatGPT has garnered signifi-\ncant global attention due to its comprehensive and\neloquent responses across various knowledge do-\nmains. Within just two months, by January 2023,\n†Corresponding authors.\nit has amassed over 100 million users. However,\none drawback of ChatGPT is its tendency to gen-\nerate text that is nonsensical, or unfaithful to the\nprovided source input, referred to as hallucination\n(Maynez et al., 2020; Raunak et al., 2021). To\naddress this issue, the research community has ded-\nicated efforts to the development of factual error\ncorrection (FEC), aiming to rectify false claims\nwith minimal modifications to make them better\nsupported by the given evidence. Consequently,\nresearch focused on this task plays a crucial role in\nmitigating the problem of hallucinations in LLMs.\nThe most straightforward way to develop FEC\nsystems is by fine-tuning pre-trained models, such\nas BART (Lewis et al., 2020a) and T5 (Raffel et al.,\n2020), on parallel data, consisting of false claims\nalong with their corresponding corrections. Nev-\nertheless, the availability of such paired data is\nrestricted due to the tremendous labor and time\nrequired for human annotations.\nTo overcome the data scarcity, researchers (Shah\net al., 2020; Thorne and Vlachos, 2021; Chen et al.,\n2023) make use of distant supervision from the fact\nverification dataset, FEVER (Thorne et al., 2018).\nFEVER is a large resource consisting of claims\npaired with evidence from Wikipedia, where each\ninstance is labeled as either SUPPORTED or RE-\nFUTED based on whether the claim is supported\nor refuted by the corresponding evidence. Exist-\ning distantly supervised models typically follow\nthe mask-then-correct approach (Shah et al., 2020;\nThorne and Vlachos, 2021). Concretely, the fact\nverification classifier (FVC), trained on FEVER,\nacts as the masker, designed to find problematic\nspans within false claims. The token-level expla-\nnations (Ribeiro et al., 2016; Chen et al., 2017) of\nFVC are usually exploited as masks. The corrector\nis trained on the SUPPORTED data from FEVER,\nwith the objective of restoring/generating the orig-\ninal/correct claim based on the masked claim and\nevidence, during training/inference. Furthermore,\n9960\nPage: 2013 NBA draft \nContext: The 2013 NBA \ndraft was held on June \n27, 2013, at Barclays \nCenter in Brooklyn, New \nYork.\nEvidence\nCorrect:\nDestroy:\nThe 2013 NBA draft was held in Pennsylvania.\nWrong Claim\nThe 2013 NBA draft was held in New York.\nCorrect Claim\nThe 2013 NBA draft was held in New York.\nCorrect Claim\nKB\nRetrieve\nThe 2013 NBA draft was held in Los Angeles.\nThe 2012 NBA draft was held in New York.\nThe 2013 NBA draft was not held in New York.\n…\nWrong Claim\nFigure 1: The comparison between factual error correction and factual error injection.\nChen et al. (2023) propose using the mask-then-\ncorrect method to iteratively refine the false claims\ninstead of relying on a single pass. However, accu-\nrately identifying factual errors using FVC is non-\ntrivial. This limitation often leads to over-erasure\nand incorrect masking issues, becoming a bottle-\nneck that restricts the performance of FEC models.\nTo bypass these issues, we propose to solve the\nFEC task by introducing a pivot task, factual error\ninjection (FEI), aiming to generate false claims by\ninjecting factual errors into correct claims. Our\nmain motivation is that the FEI task is relatively\neasier than the FEC task. As shown in Figure 1,\nan FEC model is expected to precisely identify the\nfactual error in the false claim “The 2013 NBA\ndraft was held in Pennsylvania.” and correct it to\n“The 2013 NBA draft was held inNew York.” based\non the given evidence. In contrast, the FEI task\nallows for multiple ways to introduce factual errors\ninto a correct claim. For example, one can replace\n“New York” with “Los Angeles”, substitute “2013”\nwith “2012”, or even insert a negative word such\nas “not” into the correct claim. This distinction\ndemonstrates that the FEI task encompasses a con-\nsiderably larger solution space compared to FEC.\nBy exploring this expanded solution space, we can\nleverage the relatively easier nature of FEI to en-\nhance the overall performance of FEC systems.\nOur second motivation stems from the fact that\nLLMs, such as GPT-3.5, can serve as an excellent\ndata annotator in few-shot settings, rivaling or even\nsurpassing the performance of crowdsourced anno-\ntators (He et al., 2023). Inspired by this, we use\nLLMs to solve the FEI task, specifically generating\nfalse claims for correct claims. By doing so, we\nobtain a sufficient amount of paired data, which\nwill be further used to train the FEC corrector.\nOur contributions are summarized as follows:\n(1) We propose PivotFEC1, a method that uses a\nPivot task, factual error injection, to enhance FEC\n1Our code is available at: https://github.com/\nNLPCode/PivotFEC.\nwith LLMs in few-shot settings. (2) Compared with\ndistantly supervised baselines, PivotFEC only re-\nquires 8 labeled samples from FECDATA, eliminat-\ning the need for labeled data, FEVER, to train the\nFVC. (3) Extensive experiments conducted on the\nFECDATA dataset demonstrate that PivotFEC out-\nperforms distantly supervised baselines by a large\nmargin, achieving a new state-of-the-art (SOTA) re-\nsult on the test set with scores of 66.3 on SARI and\n66.68 on ROUGE-2. (4) PivotFEC exhibits much\nbetter performance than its few-shot counterpart\n(66.3 vs. 58.43 on SARI), where LLMs are directly\nused to solve FEC, confirming the effectiveness\nand necessity of our proposed pivot task.\n2 Problem Statement\nFactual error correction aims to revise the factual\nerrors in claim Cwith minimal edits and generate\na revised claim C′ based on the provided evidence\nE. C′ should be grammatical, supported by the\nevidence, and correct the factual errors in C.\n3 Preliminary\nIn this section, we will introduce in-context learn-\ning and how to solve FEC using LLMs with in-\ncontext few-shot learning via prompting.\n3.1 LLMs with In-context Learning\nLLMs, especially ChatGPT, have demonstrated\nremarkable few-shot capability in various down-\nstream tasks. Therefore, it is natural to employ\nLLMs for addressing the FEC task in a few-shot\nsetting. Building upon the approach introduced by\nGPT-3 (Brown et al., 2020), we utilize LLMs with\nin-context few-shot learning through prompting to\ntackle FEC. Rather than fine-tuning LLMs specifi-\ncally for individual tasks, we can efficiently prompt\nthe model by providing a small set of input-output\nexemplars that demonstrate the task.\n9961\nEvidence: The Invention of Lying; The supporting cast features \nJennifer Garner, Jonah Hill, Louis C.K., Rob Lowe and Tina Fey.\nOriginal claim: The Invention of Lying's cast includes Louis C.K.\nMutated claim: The Invention of Lying's cast excludes Louis C.K.\n…\nEvidence: 2013 NBA draft; The 2013 NBA draft was held on June \n27, 2013, at Barclays Center in Brooklyn, New York.\nOriginal claim: The 2013 NBA draft was held in New York.\nMutated claim: The 2013 NBA draft was held in Los Angeles.\nFactual Error Injection\nOriginal claim: \nThe 2013 NBA \ndraft was held in \nNew York.\nMutated claim:  \nThe 2013 NBA \ndraft was held in \nLos Angeles.\n…\nSynthetic Data Training Corrector\nEncoder\n Decoder\nMutated Claim Evidence Original Claim\nFigure 2: The PivotFEC method contains three steps: (1) prompt a LLM using FEI prompting (Red text indicates\nChatGPT’s output, while the rest is the input); (2) collect the generated data as the synthetic FEC data; (3) train the\nfactual error corrector with the synthetic data. For simplicity, the process of evidence retrieval has been omitted.\n3.2 Factual Error Correction with LLMs\nTo address FEC using LLMs, we begin by choos-\ning a set of demonstrated examples. Each example\ncomprises three elements: a gold evidence, a mu-\ntated claim, and an original claim. The objective is\nfor LLMs to learn how to modify the mutated claim\nbased on the provided samples. Figure 3 illustrates\na simplified prompt where the LLM (in this case,\nChatGPT) accurately corrects the factual error in\nthe mutated claim by replacing “Los Angeles” with\n“New York.” For the full prompt of the FEC task,\nplease refer to Table 6 in Appendix C.\n4 Approach\nIn this section, we will first provide an overview of\nPivotFEC in §4.1. As illustrated in Figure 2, our\nmethod comprises three main steps. We will begin\nby introducing the FEI task and demonstrating the\nutilization of LLMs to address the FEI task in §4.2.\nNext, we will present the process of gathering syn-\nthetic paired data for the FEC task in §4.3. Finally,\nwe will demonstrate the training of the corrector\nusing the synthetic data in §4.4.\n4.1 Overview\nThe main limitation in developing FEC systems\nis the scarcity of paired data comprising correct\nclaims and their corresponding false claims. To mit-\nigate this limitation, previous studies (Shah et al.,\n2020; Thorne and Vlachos, 2021; Chen et al., 2023)\nfollow the mask-then-correct method, with the as-\nsumption that there are sufficient human annotated\nfact verification data (i.e., FEVER), which is used\nto train the FVC. They train the corrector by mask-\ning certain portions of correct claims and then re-\ncovering them. Therefore, during testing, it be-\ncomes necessary to identify factual errors within\nfalse claims and mask these errors with the FVC\nbefore using the corrector to revise them. Con-\nEvidence: The Invention of Lying; The supporting cast features \nJennifer Garner, Jonah Hill, Louis C.K., Rob Lowe and Tina Fey.\nMutated claim: The Invention of Lying's cast excludes Louis C.K.\nOriginal claim: The Invention of Lying's cast includes Louis C.K.\n…\nEvidence: 2013 NBA draft; The 2013 NBA draft was held on June \n27, 2013, at Barclays Center in Brooklyn, New York.\nMutated claim: The 2013 NBA draft was held in Los Angeles.\nOriginal claim: The 2013 NBA draft was held in New York.\nFactual Error Correction\nFigure 3: Factual error correction with in-context learn-\ning using ChatGPT. Text in red color denotes the output\nof ChatGPT, while the remaining parts are the input.\nsequently, previous approaches suffer from issues\nsuch as over-erasure and incorrect masking.\nOur primary motivation is to generate false\nclaims by injecting factual errors into correct\nclaims. This allows us to obtain FEC data con-\nsisting of correct claims paired with their corre-\nsponding false claims, which can be directly used\nfor training the FEC corrector. To achieve this goal,\nwe introduce the pivot task, factual error injection\n(FEI), for FEC, and then employ LLMs to address\nthe FEI task using a few-shot in-context learning\napproach. Compared to the previous mask-then-\ncorrect method, our approach eliminates the need\nto mask factual errors before correction, thus avoid-\ning the over-erasure and incorrect masking issues.\nMoreover, our approach does not depend on labeled\nfact verification data. Instead, we only require cor-\nrect claims and a few labeled FEC samples.\n4.2 Factual Error Injection with LLMs\nTo address FEC, LLMs are expected to identify\nfactual errors and correct them. As previously\nanalyzed, FEI requires LLMs to introduce fac-\ntual errors into correct claims and has a signifi-\ncantly larger solution space than FEC (see Figure\n1). Therefore, we assume that FEI is comparatively\neasier for LLMs than FEC. This is why we intro-\n9962\nduce this pivot task for FEC.\nSimilar to FEC, we also employ LLMs to tackle\nFEI using the few-shot in-context learning ap-\nproach. For fair comparisons, we utilize the same\ndemonstrated exemplars as used in few-shot FEC,\nwith the only difference being the order of the orig-\ninal claim and mutated claim. The left portion of\nFigure 2 illustrates a simplified prompt for FEI,\nwhere the LLM (specifically ChatGPT) injects a\nfactual error by substituting“New York” with “Los\nAngeles.” For the complete prompt of the FEI task,\nplease refer to Table 7 in Appendix D.\n4.3 Creating Synthetic Data for FEC\nWe assume that correct claims are readily avail-\nable. For each correct claim Ct, we use LLMs to\ninject factual errors into Ct via in-context learn-\ning with the prompt in Table 7. The generated\nclaim is referred to as Cf . By doing so, we collect\nthe synthetic data D = {(Ct\n1,Cf\n1 ),..., (Ct\ni ,Cf\ni )}\nfor FEC, where Ct\ni and Cf\ni denote the i-th correct\nclaim and the corresponding false (i.e., generated)\nclaim, respectively.\n4.4 Training FEC Corrector\nAfter obtaining the synthetic data D, we acquire\nthe FEC corrector by fine-tuning pre-trained lan-\nguage models, such as BART or T5 on this data.\nTo be concrete, we concatenate the false claim Cf\nand the gold evidence or retrieved evidence, and di-\nrectly input them into the encoder (refer to the right\npart of Figure 2 for the input format). For more\ndetailed information on obtaining evidence for the\nfalse claim, please refer to Appendix B.1. During\ntraining, we optimize the corrector by maximizing\nthe conditional probability of Ct:\nP(Ct\ni |Cf\ni ,Ei; θ) =\nN∏\nn=1\np(Ct\ni,n|Ct\ni,j<n,Cf\ni ,Ei; θ),\nwhere θrepresents the parameters of the corrector,\nEi denotes the corresponding evidence, and Ct\ni,j<n\nrefers to the sub-sequence preceding Ct\ni,n.\n5 Experiment\n5.1 Experimental Setups\nDataset. Following previous work, we evalu-\nate our model on the evidence-based FEC dataset\n(FECDATA) (Thorne and Vlachos, 2021), created\nbased on the large fact verification dataset, FEVER\n(Thorne et al., 2018). The construction of the\nLabel Train Valid Test\nSUPPORTED 37,961 1,477 1,593\nREFUTED 20,075 2,091 2,289\nTotal 58,036 3,568 3,882\nTable 1: The basic statistics of FECDATA with the\nnumber of data instances for each split and label.\nFEVER dataset involves two main steps: claim gen-\neration and claim labeling. In the claim generation\nphase, annotators extract the original claims (i.e.,\ncorrect claims) from Wikipedia, and then use six\ntypes of mutation: paraphrasing, negation, substi-\ntution of an entity/relation with a similar/dissimilar\none, and making the claim more general/specific to\ngenerate mutated claims for original claims. In the\nclaim labeling phase, annotators classify claims as\nSUPPORTED, REFUTED or NOTENOUGHINFO\nbased on whether the claim is supported, refuted or\nunverifiable by the given evidence.\nFECDATA extracts the SUPPORTED and RE-\nFUTED data instances from FEVER, and uses the\noriginal claims and mutated claims as the paired\ndata. Table 1 shows the basic statistics of this\ndataset. To gain further insights, we provide Fig-\nure 6 in Appendix A, displaying the distribution of\nmutation types for the REFUTED claims.\nEvaluation Metrics. For automatic evaluation,\nwe resort to SARI (Xu et al., 2016)2 and ROUGE-2\n(Lin, 2004)3 metrics. The SARI metric explicitly\nassesses the goodness of words in the revised claim\nthat are added, deleted and kept by FEC models\nfrom the source (mutated claim), compared with\nthe referenced ground truth (original claim). We\nreport the n-gram F1 score for “keep” operations\n(Keep), the n-gram precision score for “delete” op-\nerations (Delete), the n-gram F1 score for “add”\noperations (Add), and the average of these three\nscores (Final). ROUGE-2 measures the surface-\nlevel similarities between revised claims and refer-\nence claims. The SARI Final score serves as the\nprimary evaluation metric due to its strong positive\ncorrelation with manual evaluation, as indicated by\nThorne and Vlachos (2021)’s statistical findings.\nBaselines. We consider three types of baselines:\nFully Supervised Baselines estimate the ceiling\nperformance of FEC models, under the assump-\n2The evaluation code for SARI is available at: https:\n//huggingface.co/spaces/evaluate-metric/sari.\n3The evaluation code for ROUGE is available at: https:\n//huggingface.co/spaces/evaluate-metric/rouge.\n9963\ntion that a substantial amount of data is accessible.\nFor this purpose, we fine-tune BART-baseand T5-\nbase on FECDATA, where the encoder takes the\nfalse claim and corresponding evidence as inputs,\nwhile the decoder generates the revised claim.\nDistantly Supervised Baselines adopt the\n‘mask-then-correct’ pipeline, consisting of a\nmasker and a corrector. The masker can take var-\nious forms, such as the token-level explanations\n(Ribeiro et al., 2016; Chen et al., 2017) of a fact\nverification classifier (FVC), random masking, or\nheuristic masking. The FVC is initialized with\nBERT-base (Devlin et al., 2019) or RoBERTa-large\n(Liu et al., 2019), and trained on FEVER. On the\nother hand, the corrector is trained exclusively on\nthe SUPPORTED data instances from FEVER. (1)\nDual encoder pointer network (DEPN) (Shah et al.,\n2020) utilizes an FVC to predict masked words and\nsubsequently generates a revised claim using the\ndual encoder pointer generator with the copy mech-\nanism (See et al., 2017). (2) T5 Masker-Corrector\n(T5MC) (Thorne and Vlachos, 2021) differs from\nDEPN in two main ways: (a) The corrector (i.e.,\ngenerator) is based on T5-base. (b) It randomly\nmasks words in the input claim during training, but\nduring inference, heuristic masking is employed,\nwhere words do not present in the evidence are\nmasked. (3) T5MC-MLM differs from T5MC in\nthat it uses the masked language model BERT as\nthe masker during inference. (4) T5MC-V is a vari-\nant of T5MC, using FVC as the masker to predict\nthe masked tokens. (5) VENCE (Chen et al., 2023)\niteratively executes steps of mask-then-correct over\nthe claim to make it supported by the evidence.\nRule-based Baselines first generate synthetic\npaired data, and then train factual error correctors\non them. Rule-based methods are employed to con-\nstruct the inconsistent summaries with the aim of\nimproving the faithfulness in abstractive summa-\nrization (Cao et al., 2020; Cao and Wang, 2021).\nWe begin by utilizing spaCy4, a free open-source\nlibrary for NLP, to recognize name entities in cor-\nrect claims, and then implement two rule-based\nbaselines for factual error correction: (1) The first\nrule-based method creates false claims by swap-\nping named entities from the correct claims with\nalternative entities of the same entity type randomly\nchosen from the training dataset. This method is re-\nferred to asSwapEntity. (2) The second rule-based\nbaseline resorts to the ‘mask-then-fill’ pipeline to\n4https://spacy.io/\ncreate false claims. In this approach, named enti-\nties within the correct claims are substituted with\n[MASK] tokens. The masked claims are then pro-\ncessed through the BART-base model to generate\nnew claims by filling in the [MASK] tokens. These\nnewly generated claims are considered as false\nclaims. This method is termed MaskEntity.\nFew-shot Baselines contains two models: 8-\nshot T5-base fine-tunes T5-base using 8 data ex-\namples. 8-shot ChatGPT revises false claims by\nprompting ChatGPT with 8 demonstrated exam-\nples. For fair comparisons, the few-shot baselines\nand our PivotFEC use the same set of examples.\nImplementation Details. Our implementation\ndetails are shown in Appendix B.\n5.2 Experimental Results\nThe main experimental results on the FECDATA\ntest set in Table 2 reveal the following key findings:\nLLMs exhibit a remarkable few-shot ability for\nFEC. Directly fine-tuning T5 on the 8 labeled data\ninstances (i.e., 8-shot T5-base) does not bring any\nimprovement over previous distantly supervised\nbaselines, such as VENCE. However, the few-shot\nin-context learning baseline (i.e., 8-shot ChatGPT)\nachieves a noteworthy SARI Final score of 58.43,\nsurpassing VENCE (RoBERTa) by approximately\n3 points. These results highlight the impressive\nfew-shot capability of ChatGPT.\nOur proposed pivot task is highly effective for\nfew-shot FEC. To demonstrate the effectiveness\nof our proposed PivotFEC, we compare it with the\n8-shot ChatGPT model. To ensure fair compar-\nisons, both few-shot models are based on ChatGPT.\nAs shown in Table 2, our proposed model, 8-shot\nPivotFEC, far exceeds its few-shot counterpart (8-\nshot ChatGPT) by a significant margin across all\nmetrics. The SARI metric increased from 58.43\nto 66.30 and the ROUGE-2 score increased from\n49.43 to 66.68.\nOn the other hand, PivotFEC also notably out-\nperforms the rule-based baselines. Additionally,\nPivotFEC achieves the peak performance with just\n2,000 synthetic data instances for training the cor-\nrector, while rule-based methods require 10,000\nsynthetic data instances to reach their peak perfor-\nmance when training the correctors. These results\ncan be attributed to the enhanced quality of false\nclaims produced by ChatGPT. Rule-based methods,\nby contrast, often produce false claims with subop-\ntimal quality in two key aspects: (1) Grammatical\n9964\nModels FVC\nRetrieved Evidence Gold Evidence\nSARI Score RG-2 SARI Score RG-2\nKeep Delete Add Final Keep Delete Add Final\nFully Supervised Baselines\nSupervised BART-base∗ - 70.75 65.65 38.88 58.43 59.99 73.51 69.27 47.30 63.36 64.00\nSupervised T5-base∗ - 85.40 88.92 48.40 74.24 73.50 88.56 91.40 58.38 79.45 78.04\nDistantly Supervised Baselines\nDEPN (Shah et al., 2020)‡ Bb 34.5 48.1 1.7 28.1 34.8 45.2 56.9 3.9 35.3 -\nT5MC (Thorne and Vlachos, 2021)† - 65.2 62.7 15.5 47.8 50.3 66.7 62.2 16.1 48.3 -\n+ Enumerate† Bb 66.2 64.3 17.1 49.2 51.2 - - - - -\nT5MC-MLM‡ - 56.1 52.9 7.8 38.9 42.7 - - - - -\nT5MC-V (Thorne and Vlachos, 2021)† Bb 61.1 54.3 19.4 44.9 42.0 61.8 62.2 10.2 44.7 -\n+ Enumerate† Bb 63.0 55.7 24.1 47.6 45.5 - - - - -\nVENCE (Chen et al., 2023)† Bb 66.0 60.1 34.8 53.6 57.7 67.5 61.5 34.6 54.5 -\nRl 67.1 61.9 36.0 55.0 59.1 - - - - -\nRule-based Baselines\nSwapEntity∗ - 67.95 94.57 16.62 59.71 62.06 70.32 97.72 22.25 63.43 64.88\nMaskEntity∗ - 70.31 94.37 19.04 61.24 63.49 73.91 94.92 27.81 65.55 66.45\nFew-shot Baselines\n8-shot T5-base∗ - 61.75 85.23 8.70 51.89 49.83 63.50 82.44 13.56 53.17 51.38\n8-shot ChatGPT∗ - 72.09 75.92 27.29 58.43 49.43 79.98 81.61 38.81 66.80 60.72\nFew-shot (Our Method)\n8-shot PivotFEC (ChatGPT)∗ - 76.51 92.61 29.78 66.30 66.68 79.62 93.82 39.21 70.89 70.41\nTable 2: Automatic evaluation results (%) of our model and baselines with retrieved evidence or ground truth\nevidence on the FECDATA test set. Results marked with†, ‡, and ∗are from VENCE (Chen et al., 2023), T5MC-V\n(Thorne and Vlachos, 2021) and our reproduction, respectively. Bb and Rl denote BERT-base and RoBERTa-large.\nEnumerate refers to using the FVC model to rank 20 generated claims and select the best one. underline indicates\nthe best model and bold indicates the second best. RG-2 refers to ROUGE-2.\nerrors might be present in generated false claims.\n(2) False claims generated by rule-based methods\nmight deviate from the original topics present in\ncorrect claims. With ChatGPT’s remarkable in-\ncontext learning capabilities, injecting factual er-\nrors into correct claims hardly introduces grammat-\nical errors or deviates from the original topics.\nThese compelling improvements establish a new\nSOTA result and provide strong evidence for the\neffectiveness of the pivot task in enhancing the per-\nformance of FEC.\nPivotFEC lags behind supervised baselines.\nWhile PivotFEC outperforms distantly supervised\nand few-shot models, there still exists a significant\nperformance gap compared to supervised methods.\nFor example, the supervised T5-base achieves a\nscore of 74.24 on SARI Final, whereas PivotFEC\nonly scores 66.30, indicating that there is ample\nroom for further improvement.\nThe retrieved evidence is inadequate compared\nwith gold evidence. To further explore the ceiling\nperformance of our method, we conduct experi-\nments using gold evidence as well. The results\nreveal that when using gold evidence, PivotFEC\nimproves the SARI Final score by approximately\n4 points. This demonstrates the inadequacy of re-\ntrieved evidence, which aligns with previous find-\nings (Thorne and Vlachos, 2021). As our work\nmainly focuses on improving FEC through the in-\ntroduction of the pivot task, we defer the improve-\nment of evidence retrieval to future work.\n5.3 More Analysis and Discussion\nThe experiments conducted in this section utilize\nChatGPT with 8-shot in-context learning and re-\ntrieved evidence, if there is no particular statement.\nEffect of the Number of Synthetic Data. To\nshow the effect of synthetic data generated with\nFEI on PivotFEC, we first generate synthetic data\nfor FEC with 8-shot PivotFEC (ChatGPT), and then\nfine-tune T5-base on varied numbers of synthetic\ndata instances. For comparison, we also evaluate\nthe performance of T5-base trained on the gold\ndata (FECDATA). As shown in Figure 4 (a), when\nthe data size does not exceed 1k, the performance\nof 8-shot PivotFEC increases linearly with the in-\ncrease of data, even matching the performance of\nT5-base trained on the gold data. Nevertheless, our\n9965\n50 100 500 1k 2k 4k 6k 8k 10k 5803655\n60\n65\n70\n75SARI Final (%)\n Supervised T5-base\n8-shot PivotFEC (ChatGPT)\n(a) Number of training data instances\n2 4 8 16\n55.68\n57.53 58.43 59.17\n65.5464.64\n66.3 65.78\nFew-shot ChatGPT\nFew-shot PivotFEC (ChatGPT) (b) k-shot in-context learning\ntext-ada-001 text-babbage-001 text-curie-001 gpt-3.5-turbo-0301\n48.83\n54.72\n60.65\n58.43\n61.24\n64.6\n66.14 66.3\n8-shot FEC\n8-shot PivotFEC (c) LLMs with 8-shot in-context learning\nFigure 4: Subfigure (a) shows the performance on the test set for T5-base trained with different numbers of gold\nor generated data instances. Subfigure (b) shows the performance on the test set for different few-shot in-context\nlearning. Subfigure (c) shows the performance on the test set for different LLMs with 8-shot in-context learning.\nmodel’s performance plateaus once the data size\nreaches 2k. In contrast, T5-base trained on gold\ndata continues to improve. Even when using all\nFECDATA training data, its performance does not\nreach its peak. This observation suggests that the\ngenerated data contains noise compared to the gold\ndata, limiting the upper performance of PivotFEC.\nEffect of the Number of In-Context Examples.\nTable 2 demonstrates the superiority of PivotFEC\nover the few-shot FEC with ChatGPT under the\n8-shot setting. To further validate the effectiveness\nof PivotFEC, we compare its performance with the\nfew-shot FEC using ChatGPT with varying num-\nbers of in-context examples. As depicted in Figure\n4 (b), PivotFEC exhibits a notable improvement\nof approximately 7 to 10 points on the SARI Fi-\nnal score compared to the few-shot FEC model\nat different shots. When utilizing 8 demonstrated\nexamples, our model reaches a plateau.\nEffect of Different LLMs. To further empha-\nsize the advantages of our method, we compare\nit with 8-shot FEC across different LLMs, includ-\ning three InstructGPT models (text-ada-001, text-\nbabbage-001, and text-curie-001) and ChatGPT\n(gpt-3.5-turbo-0301). Figure 4 (c) illustrates our\nmethod consistently outperforms the few-shot FEC\nbaseline across different LLMs. Moreover, both\nour method and the baseline exhibit noticeable per-\nformance improvements as the model parameters\nincrease. However, even with small models, our\nmethod still performs exceptionally well. For exam-\nple, the smallest model falls only around 5 points\nbehind the largest model. In contrast, the baseline’s\nperformance is heavily influenced by the choice of\nmodel, particularly evident in text-ada-001, which\nexperiences a decrease of approximately 12 points\ncompared to the larger model, text-curie-001.\nMutation Types SARI Score RG-2\nKeep Delete Add Final\nNegate 76.51 92.61 29.78 66.30 66.68\nSubstitute similar 73.72 92.08 25.16 63.66 64.97\nSubstitute dissimilar 69.80 92.47 17.99 60.09 61.43\nSpecific 71.33 92.52 22.53 62.13 64.09\nTable 3: Evaluation results (%) of PivotFEC with\nprompts created using examples from different mutation\ntypes on the test set. RG-2 denotes ROUGE-2.\nGold Negate Sub_sim Sub_dissim Specific\n60\n70\n80\n90SARI Final (%)\nNegate\nSubstitute similar\nSubstitute dissimilar\nSpecific\nFigure 5: Results on different test cases, with the test\nset being divided according to mutation types. Gold de-\nnotes T5-base trained on FECDATA. Negate, Sub_sim,\nSub_dissim, and Specific refer to PivotFEC with their\nrespective mutation prompts.\nEffect of Mutation Types. As shown in Figure\n6 in Appendix A, the refuted claims mainly stem\nfrom four mutation types. Therefore, we construct\nfour prompts, each composed of examples from the\ncorresponding mutation. For prompts consisting of\nexamples from negate, substitute similar, substitute\ndissimilar, and specific mutations, please refer to\nTables 7, 8, 9, and 10. Table 3 shows that PivotFEC\nwith negate prompt yields the best results, possibly\nbecause this mutation constitutes the largest por-\ntion of the test set. Additionally, we present the\nperformance of PivotFEC on separate test cases.\nFigure 5 illustrates that: (1) PivotFEC performs\nwell on a test case of the specific mutation type\nwhen using a prompt tailored to that type, and (2)\nthe variations in PivotFEC performance, when us-\n9966\nModels Grammar Support Correct\nFully supervised models with gold evidence\nT5-base 100 89.3 86.7\n8-shot models with retrieved evidence\nT5-base 83.3 22.0 5.3\nChatGPT 92.0 90.0 42.0\nPivotFEC (ChatGPT) 99.3 65.3 54.7\nTable 4: Human evaluation results (%) on the test set\nfor the grammatical (Grammar), supported (Support)\nand corrected (Correct) scores.\ning different prompts, mainly arise from the negate\ntest case.\n5.4 Human Evaluation\nWe conduct a human evaluation to compare Piv-\notFEC with the fully supervised T5-base, 8-shot\nT5-base and 8-shot ChatGPT models. The fully\nsupervised T5-base model utilizes gold evidence to\nrectify false claims, while the others use retrieved\nevidence. For each model, we randomly sample 50\ncases and ask three annotators5 to assess the revised\nclaims based on the following Boolean questions:\n(1) Is the revised claim grammatically correct? (2)\nIs the revised claim supported by evidence? (3) Has\nthe factual error in the false claim been corrected?\nThe final question, measuring the correction of fac-\ntual errors, is the most important metric in our\nhuman evaluation. As shown in Table 4, our pro-\nposed model outperforms the few-shot baselines on\nthe corrected metric; however, there is still a gap\nto reach the ceiling performance of the supervised\nbaseline. Inter-annotator agreement measured by\nFleiss’ kappa (Fleiss, 1971) is 0.75, 0.86, and 0.81\nfor grammatical, supported, and corrected scores,\nimplying substantial agreement ( > 0.6) (Landis\nand Koch, 1977).\n5.5 Samples and Analysis\nTable 5 presents the revised claims generated by\nour approach and the baselines. From this table,\nwe observe that the 8-shot T5-base method cannot\nidentify errors in false claims. Similarly, 8-shot\nChatGPT often struggles to precisely locate errors\nwithin false claims, and tends to simply copy con-\ntent from the evidence into the modified claims\nrather than correct them. For example, in the first\nexample, although 8-shot ChatGPT corrects the\nfactual error in the original sentence, it does not\n5All annotators hold Ph.D. degrees and are independent\nfrom our research.\nmake the minimal edits. As for the second exam-\nple, 8-shot ChatGPT fails to identify the error in\nthe false claim, resulting in a text that exhibits a\nlow correlation with the original claim. This also\nexplains why this method achieves a relatively high\nsupported value but demonstrates a low corrected\nscore during the human evaluation. Most notably,\nour method can accurately identify errors and make\nmodifications based on the retrieved evidence, sim-\nilar to the performance of the supervised T5 model.\n6 Related Work\n6.1 Grammatical Error Correction\nGrammatical error correction (GEC) (Ng et al.,\n2014; Yuan and Briscoe, 2016; Bryant et al., 2017;\nAwasthi et al., 2019; Liu et al., 2021) refers to\nthe process of identifying and rectifying grammat-\nical errors in written text. It has practical appli-\ncations in several domains, such as helping non-\nnative speakers enhance their writing skills, aiding\nlanguage learners in improving their grammatical\naccuracy, and assisting professional writers in pro-\nducing error-free and polished content. GEC aims\nto improve the accuracy and fluency of the lan-\nguage by fixing various grammatical errors, includ-\ning missing prepositions, mismatched subject-verb\nagreement, misspellings, and word choice errors.\nIn comparison, factual error correction involves cor-\nrecting the factual errors instead of the grammatical\nerrors in the given content, such as incorrect dates,\nnames, or historical events.\n6.2 Retrieval-Augmented Generation\nRetrieval-augmented generation (Lewis et al.,\n2020b) combines the power of information re-\ntrieval and language generation techniques to el-\nevate the overall quality of the generated content.\nFor example, (He et al., 2022) use dense retriev-\ners to retrieve relevant sentences from an exter-\nnal corpus for the given keywords to improve\nlexically constrained text generation (He and Li,\n2021; He, 2021). By incorporating external knowl-\nedge, retrieval-augmented generation effectively\nmitigates the risk of generating inaccurate or non-\nsensical content. Furthermore, factual error correc-\ntion is another facet of retrieval-augmented genera-\ntion. It rectifies factual inaccuracies based on the\nevidence retrieved, thereby fitting under the broader\numbrella of retrieval-augmented generation.\n9967\nGold Claim: The SpongeBob SquarePants Movie is a film.\nFalse Claim: The SpongeBob SquarePants Movie is a dog.\nRetrieved Evidence: title: The SpongeBob SquarePants Movie; context: The SpongeBob SquarePants Movie is a 2004\nAmerican animated adventure comedy film based on the Nickelodeon television series SpongeBob SquarePants . . .\nSupervised T5-base: The SpongeBob SquarePants Movie is a film. /enc-33\n8-shot T5-base: The SpongeBob SquarePants Movie is a dog. /enc-37\n8-shot ChatGPT: The SpongeBob SquarePants Movie is an animated film based on the Nickelodeon television series\nSpongeBob SquarePants. /enc-33\n8-shot PivotFEC (ChatGPT): The SpongeBob SquarePants Movie is a film. /enc-33\nGold Claim: The Hunger Games are based on a novel.\nFalse Claim: The Hunger Games are based on a play.\nRetrieved Evidence: title: The Hunger Games; context: . . . The Hunger Games is a trilogy of young adult dystopian novels\nwritten by American novelist Suzanne Collins . . .\nSupervised T5-base: The Hunger Games are based on a novel. /enc-33\n8-shot T5-base: The Hunger Games are based on a play. /enc-37\n8-shot ChatGPT: The Hunger Games is a trilogy of young adult dystopian novels. /enc-37\n8-shot PivotFEC (ChatGPT): The Hunger Games are based on a novel. /enc-33\nTable 5: Revised claims generated by our model and baselines based on the evidence for false claims extracted from\nthe test set. The supervised T5-base revises false claims based on the gold evidence, while others utilize retrieved\nevidence. For simplicity, we do not show the gold evidence. Text in blue, red, and orange colors represents factual\nerrors, correct modifications, and copied text, respectively.\n6.3 Fact Verification\nFact verification, also known as fact-checking,\naims to validate the accuracy of a given claim by\nexamining the available evidence. This field of\nstudy has been extensively researched in recent\nyears. Researchers assess the claim by analyz-\ning both unstructured sources, such as political\nnews (PolitiFact) (Vlachos and Riedel, 2014; Wang,\n2017), Wikipedia (FEVER) (Thorne et al., 2018;\nLiu et al., 2020), and scientific literature (Wadden\net al., 2020), as well as structured sources, includ-\ning Wikipedia tables (TabFact) (Chen et al., 2020)\nand knowledge base (Iso et al., 2020). Fact verifi-\ncation seeks to determine whether a claim is sup-\nported or refuted by evidence, while factual error\ncorrection takes it a step further. Factual error cor-\nrection not only involves identifying factual errors\nbut also requires modifying them to obtain correct\nclaims.\n7 Conclusion\nIn this paper, we present PivotFEC, which intro-\nduces a pivot task, factual error injection, to im-\nprove factual error correction. Specifically, we first\nintentionally introduce factual errors into correct\nclaims using LLMs under the few-shot setting. By\ndoing so, we can obtain enough synthetic paired\ndata for FEC, consisting of correct claims paired\nwith their corresponding false claims, which will be\nused to train the FEC corrector. As a result, Pivot-\nFEC demonstrates a significant improvement over\nprevious distantly supervised baselines, establish-\ning a new SOTA performance on FECDATA. Fur-\nthermore, our approach significantly outperforms\nits few-shot counterpart, providing strong evidence\nfor the effectiveness of the pivot task.\n8 Limitations\nThere are two potential limitations to this study.\nFirstly, due to limited computational resources,\nwe only assess the effectiveness of our proposed\nmethod on GPT series models. Future work should\ninclude additional experiments with other language\nmodels, such as PaLM and LLaMA. The second\nlimitation is that the retrieved evidence may not al-\nways be relevant to the input claim, which means it\nmay not provide useful information for correcting\nfactual errors within the claim. As our primary fo-\ncus is on enhancing factual error correction through\nthe introduction of the pivot task, we leave the task\nof improving evidence retrieval for future research.\nAcknowledgements\nThis project is supported by National Natural Sci-\nence Foundation of China (Grant No. 62202023),\nHKU-SCF FinTech Academy, Shenzhen-Hong\nKong-Macao Science and Technology Plan Project\n(Category C Project: SGDX20210823103537030),\nand Theme-based Research Scheme of RGC, Hong\nKong (T35-710/20-R). We would like to thank the\nanonymous reviewers for their constructive and\ninformative feedback on this work.\n9968\nReferences\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,\nSabyasachi Ghosh, and Vihari Piratla. 2019. Parallel\niterative edit models for local sequence transduction.\nIn Proceedings of EMNLP, pages 4260–4270, Hong\nKong, China. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NIPS, vol-\nume 33, pages 1877–1901. Curran Associates, Inc.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of error\ntypes for grammatical error correction. In Proceed-\nings of ACL , pages 793–805, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit\nCheung. 2020. Factual error correction for ab-\nstractive summarization models. In Proceedings of\nEMNLP, pages 6251–6258, Online. Association for\nComputational Linguistics.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn ICLR.\nShuyang Cao and Lu Wang. 2021. CLIFF: Contrastive\nlearning for improving faithfulness and factuality\nin abstractive summarization. In Proceedings of\nEMNLP, pages 6633–6649, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nJiangjie Chen, Rui Xu, Wenxuan Zeng, Changzhi Sun,\nLei Li, and Yanghua Xiao. 2023. Converge to\nthe truth: Factual error correction via iterative con-\nstrained editing. In Proceedings of AAAI.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM for\nnatural language inference. In Proceedings of ACL,\npages 1657–1668, Vancouver, Canada. Association\nfor Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In Interna-\ntional Conference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186, Minneapolis, Minnesota. Association for Com-\nputational Linguistics.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin,\n76(5):378–382.\nXingwei He. 2021. Parallel refinements for lexically\nconstrained text generation with BART. In Proceed-\nings of EMNLP, pages 8653–8666, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nXingwei He, Yeyun Gong, A-Long Jin, Weizhen Qi,\nHang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng,\nSm Yiu, and Nan Duan. 2022. Metric-guided dis-\ntillation: Distilling knowledge from the metric to\nranker and retriever for generative commonsense rea-\nsoning. In Proceedings of EMNLP, pages 839–852,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nXingwei He and Victor O.K. Li. 2021. Show me how\nto revise: Improving lexically constrained sentence\ngeneration with XLNet. In Proceedings of AAAI ,\nvolume 35, pages 12989–12997.\nXingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin,\nHang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\nDuan, and Weizhu Chen. 2023. Annollm: Making\nlarge language models to be better crowdsourced\nannotators. arXiv preprint arXiv:2303.16854.\nHayate Iso, Chao Qiao, and Hang Li. 2020. Fact-based\nText Editing. In Proceedings of ACL, pages 171–182,\nOnline. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769–6781, Online. Association\nfor Computational Linguistics.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, 33(1):159–174.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of ACL , pages 7871–\n7880, Online. Association for Computational Lin-\nguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Proceedings of NIPS , Red\nHook, NY , USA. Curran Associates Inc.\n9969\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact verification\nwith kernel graph attention network. In Proceedings\nof ACL, pages 7342–7351, Online. Association for\nComputational Linguistics.\nZhenghao Liu, Xiaoyuan Yi, Maosong Sun, Liner Yang,\nand Tat-Seng Chua. 2021. Neural quality estimation\nwith multiple hypotheses for grammatical error cor-\nrection. In Proceedings of NAACL, pages 5441–5452,\nOnline. Association for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof ACL, pages 1906–1919, Online. Association for\nComputational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task\non grammatical error correction. In Proceedings of\nthe Eighteenth Conference on Computational Natu-\nral Language Learning: Shared Task , pages 1–14,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In NIPS, volume 35, pages 27730–\n27744. Curran Associates, Inc.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nVikas Raunak, Arul Menezes, and Marcin Junczys-\nDowmunt. 2021. The curious case of hallucinations\nin neural machine translation. In Proceedings of\nNAACL, pages 1172–1183, Online. Association for\nComputational Linguistics.\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin.\n2016. “why should I trust you?”: Explaining the pre-\ndictions of any classifier. In Proceedings of NAACL,\npages 97–101, San Diego, California. Association for\nComputational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get to the point: Summarization with\npointer-generator networks. In Proceedings of ACL,\npages 1073–1083, Vancouver, Canada. Association\nfor Computational Linguistics.\nDarsh Shah, Tal Schuster, and Regina Barzilay. 2020.\nAutomatic fact-guided sentence modification. In Pro-\nceedings of AAAI, volume 34, pages 8791–8798.\nJames Thorne and Andreas Vlachos. 2021. Evidence-\nbased factual error correction. In Proceedings of\nACL, pages 3298–3309, Online. Association for Com-\nputational Linguistics.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of NAACL, pages\n809–819, New Orleans, Louisiana. Association for\nComputational Linguistics.\nAndreas Vlachos and Sebastian Riedel. 2014. Fact\nchecking: Task definition and dataset construction.\nIn Proceedings of the ACL 2014 Workshop on Lan-\nguage Technologies and Computational Social Sci-\nence, pages 18–22, Baltimore, MD, USA. Associa-\ntion for Computational Linguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of EMNLP, pages\n7534–7550, Online. Association for Computational\nLinguistics.\nWilliam Yang Wang. 2017. “liar, liar pants on fire”:\nA new benchmark dataset for fake news detection.\nIn Proceedings of ACL, pages 422–426, Vancouver,\nCanada. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R’emi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,\nand Chris Callison-Burch. 2016. Optimizing sta-\ntistical machine translation for text simplification.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nZheng Yuan and Ted Briscoe. 2016. Grammatical er-\nror correction using neural machine translation. In\nProceedings of NAACL, pages 380–386, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\n9970\nNegate\n48.8%\nSubstitute similar\n26.5%\nSubstitute dissimlar\n15.2%\nSpecific6.1%\nRephrase1.8%\nGeneral1.6%\nFigure 6: Distribution of mutation types for refuted\nclaims of the test set.\nA Distribution of Mutation Types\nFigure 6 presents the distribution of mutation types\nfor revised claims of the test set.\nB Implementation Details\nB.1 Evidence Retrieval\nConsidering that our research does not focus on\nimproving the retrieval model, we adopt the same\nretrieval process as previous studies (Thorne and\nVlachos, 2021; Chen et al., 2023). The retrieval\nmodule primarily consists of two steps: First, we\nemploy a pre-trained seq2seq model called GENRE\n(Cao et al., 2021) to predict the relevant Wikipedia\npages for the input claim. Then, we utilize the\ndense passage retrieval model DPR (Karpukhin\net al., 2020) to retrieve the most relevant passages\nfrom the pages predicted by GENRE.\nB.2 PivotFEC\nSynthetic data for FEC. We generate synthetic\nFEC data using REFUTED data instances from\nFECDATA by solving the FEI task. Since we as-\nsume that correct claims are readily available, we\nutilize only the correct claims from the REFUTED\ndata instances and exclude their paired false claims.\nTo provide clarity, we construct a total of 1296\nvalidation data instances and 2000 training data\ninstances. It’s worth noting that increasing the\namount of training data does not yield substantial\nimprovements, as discussed in Section 5.3.\nTraining and Inference. During training, we ini-\ntialize the PivotFEC corrector with the T5-base\nmodel. Following previous work (Thorne and\nVlachos, 2021), we input the top-2 retrieved ev-\nidence or gold evidence paired with the false\nclaim into the T5 encoder, as additional evidence\ndoes not yield significant improvements. The cor-\nrector is optimized using the AdamW optimizer\n(Loshchilov and Hutter, 2019) with a learning rate\nof 4e−5, a batch size of 64, and a linear learn-\ning rate schedule with 10% warm-up steps for 400\nsteps. The learning rate is selected from the set\n{5e−6,1e−5,2e−5,3e−5,4e−5,5e−5}.\nWe set the maximum source length to 512 and the\nmaximum target length to 256. During training, we\nevaluate the model every 50 steps on the synthetic\nvalidation set and choose the checkpoint with the\nlowest negative log-likelihood (NLL) loss on the\nvalidation set.\nDuring inference, we employ beam search de-\ncoding with a beam width of 5 to generate revised\nclaims for the test set.\nB.3 Supervised Models\nWe fine-tune the pre-trained models, BART-base\nand T5-base, on the FECDATA training set for\n4000 steps. We evaluate the model every 200 steps\nusing the FECDATA validation set. Other parame-\nters remain consistent with those of PivotFEC, as\nstated in Section B.2.\nTo implement all models, we utilize the Hug-\ngingFace Transformers library (Wolf et al., 2019).\nAdditionally, all experiments are conducted on 2\nNVIDIA Tesla V100 GPUs with32 GB of memory.\nC Full Few-shot Prompts for FEC\nTable 6 shows the few-shot exemplars prompt of the\nnegate mutation type for the FEC task. Table 6 and\nTable 7 use the same demonstrated exemplars with\nthe only difference being the order of the original\nclaim and mutated claim.\nD Full Few-shot Prompts for FEI\nTables 7, 8, 9, and 10 show the few-shot exemplars\nprompt of the negate, substitute similar, substitute\ndissimilar and specific mutation types for the FEI\ntask, respectively.\n9971\nEvidence: The Lion King; The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare\n’s Hamlet .\nThe Lion King; The Lion King tells the story of Simba , a young lion who is to succeed his father , Mufasa , as King of the\nPride Lands ; however , after Simba ’s uncle Scar murders Mufasa , Simba is manipulated into thinking he was responsible\nand flees into exile .\nMutated claim: The Lion King has nothing to do with lions.\nOriginal claim: The Lion King is about lions.\nEvidence: Indiana Jones; Henry Walton “ Indiana ” Jones Jr. ( also shortened to Indy ) is a fictional character and the\nprotagonist of the Indiana Jones franchise .\nIndiana Jones; George Lucas created the character in homage to the action heroes of 1930s film serials .\nMutated claim: Indiana Jones is real.\nOriginal claim: Indiana Jones is fictional.\nEvidence: Scott Eastwood; Scott Eastwood ( born Scott Clinton Reeves ; March 21 , 1986 ) is an American actor , model ,\nand professional skydiver .\nScott Eastwood; He has also been the model for the fragrance Cool Water by Davidoff .\nMutated claim: Scott Eastwood was incapable of working as a model.\nOriginal claim: Scott Eastwood worked as a model.\nEvidence: Akshay Kumar; Kumar is also a producer and martial artist who has appeared in over a hundred Hindi films .\nAkshay Kumar; Having done so , he has established himself as a leading contemporary actor of Hindi cinema .\nMutated claim: Akshay Kumar does not work in Hindi cinema.\nOriginal claim: Akshay Kumar works in Hindi cinema.\nEvidence: Gorillaz; Gorillaz are an English virtual band created in 1998 by musician Damon Albarn and artist Jamie\nHewlett .\nVirtual band; In music , a virtual band ( also called a virtual group , cartoon group , or cartoon band ) is any group whose\nmembers are not corporeal musicians , but animated characters .\nMutated claim: Gorillaz is a German live band.\nOriginal claim: Gorillaz is a British virtual band.\nEvidence: Grant Gustin; Thomas Grant Gustin ( born January 14 , 1990 ) is an American actor , singer , and dancer .\nGrant Gustin; He is known for his role as Barry Allen / the Flash ( based on the DC Comics character of the same name )\non the CW series The Flash and Arrow , both in the Arrowverse television franchise , and his role as Sebastian Smythe on\nthe Fox series Glee .\nMutated claim: Grant Gustin is only a writer.\nOriginal claim: Grant Gustin is a singer.\nEvidence: RB Leipzig; RasenBallsport Leipzig e.V . , commonly known as RB Leipzig , is a German association football\nclub based in Leipzig , Saxony .\nFootball in Germany; Football is the most popular sport in Germany .\nMutated claim: RB Leipzig plays the least popular German sport.\nOriginal claim: RB Leipzig plays the most popular German sport.\nEvidence: One World Trade Center; One World Trade Center ( also known as 1 World Trade Center , 1 WTC or Freedom\nTower ) is the main building of the rebuilt World Trade Center complex in Lower Manhattan , New York City .\nWorld Trade Center (2001–present); The original World Trade Center featured the landmark Twin Towers , which opened\nin 1973 , and were the tallest buildings in the world at their completion .\nMutated claim: One World Trade Center opened in 1876.\nOriginal claim: One World Trade Center opened in 2014.\nTable 6: Few-shot exemplars prompt of the negate mutation type for the FEC task.\n9972\nEvidence: The Lion King; The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare\n’s Hamlet .\nThe Lion King; The Lion King tells the story of Simba , a young lion who is to succeed his father , Mufasa , as King of the\nPride Lands ; however , after Simba ’s uncle Scar murders Mufasa , Simba is manipulated into thinking he was responsible\nand flees into exile .\nOriginal claim: The Lion King is about lions.\nMutated claim: The Lion King has nothing to do with lions.\nEvidence: Indiana Jones; Henry Walton “ Indiana ” Jones Jr. ( also shortened to Indy ) is a fictional character and the\nprotagonist of the Indiana Jones franchise .\nIndiana Jones; George Lucas created the character in homage to the action heroes of 1930s film serials .\nOriginal claim: Indiana Jones is fictional.\nMutated claim: Indiana Jones is real.\nEvidence: Scott Eastwood; Scott Eastwood ( born Scott Clinton Reeves ; March 21 , 1986 ) is an American actor , model ,\nand professional skydiver .\nScott Eastwood; He has also been the model for the fragrance Cool Water by Davidoff .\nOriginal claim: Scott Eastwood worked as a model.\nMutated claim: Scott Eastwood was incapable of working as a model.\nEvidence: Akshay Kumar; Kumar is also a producer and martial artist who has appeared in over a hundred Hindi films .\nAkshay Kumar; Having done so , he has established himself as a leading contemporary actor of Hindi cinema .\nOriginal claim: Akshay Kumar works in Hindi cinema.\nMutated claim: Akshay Kumar does not work in Hindi cinema.\nEvidence: Gorillaz; Gorillaz are an English virtual band created in 1998 by musician Damon Albarn and artist Jamie\nHewlett .\nVirtual band; In music , a virtual band ( also called a virtual group , cartoon group , or cartoon band ) is any group whose\nmembers are not corporeal musicians , but animated characters .\nOriginal claim: Gorillaz is a British virtual band.\nMutated claim: Gorillaz is a German live band.\nEvidence: Grant Gustin; Thomas Grant Gustin ( born January 14 , 1990 ) is an American actor , singer , and dancer .\nGrant Gustin; He is known for his role as Barry Allen / the Flash ( based on the DC Comics character of the same name )\non the CW series The Flash and Arrow , both in the Arrowverse television franchise , and his role as Sebastian Smythe on\nthe Fox series Glee .\nOriginal claim: Grant Gustin is a singer.\nMutated claim: Grant Gustin is only a writer.\nEvidence: RB Leipzig; RasenBallsport Leipzig e.V . , commonly known as RB Leipzig , is a German association football\nclub based in Leipzig , Saxony .\nFootball in Germany; Football is the most popular sport in Germany .\nOriginal claim: RB Leipzig plays the most popular German sport.\nMutated claim: RB Leipzig plays the least popular German sport.\nEvidence: One World Trade Center; One World Trade Center ( also known as 1 World Trade Center , 1 WTC or Freedom\nTower ) is the main building of the rebuilt World Trade Center complex in Lower Manhattan , New York City .\nWorld Trade Center (2001–present); The original World Trade Center featured the landmark Twin Towers , which opened\nin 1973 , and were the tallest buildings in the world at their completion .\nOriginal claim: One World Trade Center opened in 2014.\nMutated claim: One World Trade Center opened in 1876.\nTable 7: Few-shot exemplars prompt of the negate mutation type for the FEI task.\n9973\nEvidence: Notes on a Scandal (film); The soundtrack was composed by Philip Glass .\nPhilip Glass; Philip Morris Glass ( born January 31 , 1937 ) is an American composer .\nOriginal claim: Notes on a Scandal has a soundtrack composed by an American.\nMutated claim: Notes on a Scandal has a soundtrack composed by an Armenian.\nEvidence: The Lion King; The Lion King is a 1994 American animated epic musical film , produced by Walt Disney\nFeature Animation and released by Walt Disney Pictures .\nThe Lion King; It is the 32nd Disney animated feature film .\nOriginal claim: The Lion King is a film.\nMutated claim: The Lion King is a TV show.\nEvidence: Dead Man Down; Dead Man Down is an 2013 American neo-noir crime thriller film written by J.H. Wyman and\ndirected by Danish director Niels Arden Oplev .\nDead Man Down; The film stars Colin Farrell , Noomi Rapace , Dominic Cooper , and Terrence Howard , and was released\non March 8 , 2013 .\nOriginal claim: Dead Man Down was released in 2013.\nMutated claim: Dead Man Down was released in 2014.\nEvidence: Nick Jonas (album); It was released on November 10 , 2014 , by Island Records .\nIsland Records; Island Records is a British-American record label that operates as a division of Universal Music Group (\nUMG ) .\nOriginal claim: Nick Jonas was released by a British-American record label.\nMutated claim: Nick Jonas was released by a Chinese-Mongolian record label.\nEvidence: Deadpool; Created by artist/writer Rob Liefeld and writer Fabian Nicieza , the character first appeared in The\nNew Mutants # 98 ( cover-dated February 1991 ) .\nDeadpool; Initially Deadpool was depicted as a supervillain when he made his first appearance in The New Mutants and\nlater in issues of X-Force , but later evolved into his more recognizable antiheroic persona .\nOriginal claim: Deadpool first appeared in The New Mutants.\nMutated claim: Deadpool first appeared in X-Men.\nEvidence: Blue-ringed octopus; The blue-ringed octopodes ( genus Hapalochlaena ) are three octopus species that live in\ntide pools and coral reefs in the Pacific and Indian Oceans , from Japan to Australia .\nBlue-ringed octopus; They are recognized as one of the world ’s most venomous marine animals .\nOriginal claim: The blue-ringed octopus is a marine animal.\nMutated claim: The blue-ringed octopus is a terrestrial animal.\nEvidence: Room (2015 film); Room is a 2015 independent drama film directed by Lenny Abrahamson and written by\nEmma Donoghue , based on her novel of the same name .\nRoom (novel); Room is a 2010 novel by Irish-Canadian author Emma Donoghue .\nOriginal claim: Room is based on a novel of the same name.\nMutated claim: Room is based on a short story of the same name.\nEvidence: Steve Buscemi; He made his directorial debut in 1996 , with Trees Lounge , in which he also starred .\nTrees Lounge; Trees Lounge is a 1996 feature film and the debut of Steve Buscemi as writer and director .\nOriginal claim: Steve Buscemi directed the film Trees Lounge.\nMutated claim: Steve Buscemi directed the television show Trees Lounge.\nTable 8: Few-shot exemplars prompt of the substitute similar mutation type for the FEI task.\n9974\nEvidence: Kurt Angle; He then won a freestyle wrestling gold medal at the 1996 Summer Olympics .\nKurt Angle; After graduating college , Angle won a gold medal in freestyle wrestling at the 1995 World Wrestling\nChampionships .\nOriginal claim: Kurt Angle is a professional wrestler.\nMutated claim: Kurt Angle is a fish.\nEvidence: Selena Gomez; Between 2009 and 2011 , Gomez starred in films such as Princess Protection Program , Ramona\nand Beezus , and Monte Carlo , and took on a more mature role in Spring Breakers ( 2013 ) .\nPrincess Protection Program; Princess Protection Program is a 2009 Disney Channel Original Movie , directed by Allison\nLiddi-Brown and starring Demi Lovato and Selena Gomez .\nOriginal claim: Selena Gomez starred in Princess Protection Program.\nMutated claim: Selena Gomez reviewed Princess Protection Program.\nEvidence: Sterling Archer; Sterling Malory Archer , known simply as Archer , is the titular character and the main\nprotagonist of the American animated comedy series Archer .\nArcher (TV series); Archer is an American adult animated spy sitcom created by Adam Reed for the FX network .\nOriginal claim: Sterling Archer is the main character of a comedy series.\nMutated claim: Sterling Archer directed a comedy series.\nEvidence: United States Congress; The House of Representatives has six non-voting members in addition to its 435 voting\nmembers .\nUnited States Congress; Congress has 535 voting members : 435 Representatives and 100 Senators .\nOriginal claim: The United States Congress has 435 Representatives.\nMutated claim: The United States Congress has 435 minefields.\nEvidence: Carole King; Carole King ( born Carol Joan Klein , February 9 , 1942 ) is an American composer and singer-\nsongwriter .\nCarole King; She is the most successful female songwriter of the latter half of the 20th century , having written or co-written\n118 pop hits on the Billboard Hot 100 between 1955 and 1999 .\nOriginal claim: Carole King is an American.\nMutated claim: Carole King is an acrobat.\nEvidence: Manatee; Manatees ( family Trichechidae , genus Trichechus ) are large , fully aquatic , mostly herbivorous\nmarine mammals sometimes known as sea cows .\nHerbivore; A herbivore is an animal anatomically and physiologically adapted to eating plant material , for example foliage\n, for the main component of its diet .\nOriginal claim: Manatees are similar to cows on land.\nMutated claim: Manatees eat cows on land.\nEvidence: Lion (2016 film); Lion is a 2016 biographical film directed by Garth Davis ( in his feature debut ) and written by\nLuke Davies , based on the non-fiction book A Long Way Home by Saroo Brierley with Larry Buttrose .\nGarth Davis; Garth Davis is an Australian television , film and commercial director , best known for directing episodes of\nthe series Top of the Lake ( 2013 ) , for which he received Emmy and BAFTA nominations .\nOriginal claim: Lion was directed by Garth Davis.\nMutated claim: Lion was directed by plants.\nEvidence: Tropic Thunder; Tropic Thunder is a 2008 satirical action comedy film co-written , produced , and directed by\nBen Stiller .\nTropic Thunder; It was written by Stiller , Justin Theroux and Etan Cohen .\nOriginal claim: Tropic Thunder was written by Justin Theroux.\nMutated claim: Tropic Thunder was awoken by Justin Theroux.\nTable 9: Few-shot exemplars prompt of the substitute dissimilar mutation type for the FEI task.\n9975\nEvidence: Bryan Adams; He has also won MTV , ASCAP , American Music awards , three Ivor Novello Awards for song\ncomposition and has been nominated five times for Golden Globe Awards and three times for Academy Awards for his\nsongwriting for films .\nIvor Novello Awards; The Ivor Novello Awards , named after the Cardiff-born entertainer Ivor Novello , are awards for\nsongwriting and composing .\nOriginal claim: Bryan Adams has won Ivor Novello Awards.\nMutated claim: Bryan Adams has won Ivor Novello Awards for his singing.\nEvidence: Malcolm Young; Malcolm Mitchell Young ( born 6 January 1953 ) is an Australian retired musician and\nsongwriter , best known as a co-founder , rhythm guitarist , backing vocalist and songwriter for the hard rock band AC/DC .\nMalcolm Young; Except for a brief absence in 1988 , he was with the band from its November 1973 beginning until retiring\npermanently in 2014 , due to health reasons .\nOriginal claim: Malcolm Young co-founded AC/DC in 1973.\nMutated claim: Malcolm Young co-founded AC/DC in July 1973.\nEvidence: Gillian Jacobs; Jacobs has also had a recurring role as Mimi-Rose Howard on the HBO series Girls and has\nappeared in films such as Gardens of the Night ( 2008 ) , The Lookalike ( 2014 ) , Life Partners ( 2014 ) , Hot Tub Time\nMachine 2 ( 2015 ) , Do n’t Think Twice ( 2016 ) and Brother Nature ( 2016 ) .\nHot Tub Time Machine 2; Hot Tub Time Machine 2 is a 2015 American comedy film directed by Steve Pink and written by\nJosh Heald .\nOriginal claim: Gillian Jacobs appeared in the film Hot Tub Time Machine 2.\nMutated claim: Gillian Jacobs appeared in the horror film Hot Tub Time Machine 2.\nEvidence: Marc Maron; He has been host of The Marc Maron Show and co-host of both Morning Sedition and Breakroom\nLive , all politically oriented shows produced by Air America Media .\nThe Marc Maron Show; It featured interviews ( both political and showbusiness ) , live comedy , and extensive banter\nbetween Maron and Jim Earl , Maron ’s co-host , who provides humorous introductions after each commercial break and\nplays several of the recurring characters in the show ’s skits .\nOriginal claim: Marc Maron was the host of The Marc Maron Show.\nMutated claim: Marc Maron was the only host of The Marc Maron Show.\nEvidence: The Offspring; The band ’s third studio album , Smash ( 1994 ) , became their first commercial success , and has\nsold over eleven million copies worldwide , setting a record for most albums sold on an independent label and becoming the\nfirst album on Epitaph to obtain gold and platinum status .\nSmash (The Offspring album); Recording and production were finished a month later , and the album was released on April\n8 , 1994 on Epitaph Records .\nOriginal claim: The Offspring released Smash in 1994.\nMutated claim: The Offspring released Smash in May of 1994.\nEvidence: NASA; Since that time , most US space exploration efforts have been led by NASA , including the Apollo Moon\nlanding missions , the Skylab space station , and later the Space Shuttle .\nSkylab; Skylab was the United States ’ first space station , orbiting Earth from 1973 to 1979 , when it fell back to Earth\namid huge worldwide media attention .\nOriginal claim: NASA is responsible for the Skylab space station.\nMutated claim: NASA is responsible for the Skylab space station, launched in 1980.\nEvidence: Golden State Warriors; The Warriors compete in the National Basketball Association ( NBA ) as a member of\nthe league ’s Western Conference Pacific Division .\nNational Basketball Association; It has 30 teams ( 29 in the United States and 1 in Canada ) , and is an active member of\nUSA Basketball ( USAB ) , which is recognized by FIBA ( also known as the International Basketball Federation ) as the\nnational governing body for basketball in the United States .\nOriginal claim: The Golden State Warriors are in the NBA.\nMutated claim: The Golden State Warriors are one of 32 teams in the NBA.\nEvidence: Morrissey; Born in Davyhulme , Lancashire , to a working-class Irish migrant family , Morrissey grew up in\nManchester .\nMorrissey; Steven Patrick Morrissey ( born 22 May 1959 ) , professionally known as Morrissey , is an English singer ,\nsongwriter and author .\nOriginal claim: Morrissey was born into a working-class family.\nMutated claim: Morrissey was born into a working-class family in 1983.\nTable 10: Few-shot exemplars prompt of the specific mutation type for the FEI task.\n9976",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7423120737075806
    },
    {
      "name": "Computer science",
      "score": 0.7392788529396057
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5738654732704163
    },
    {
      "name": "Error detection and correction",
      "score": 0.557371973991394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5262743234634399
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5225265026092529
    },
    {
      "name": "Field (mathematics)",
      "score": 0.508981466293335
    },
    {
      "name": "Machine learning",
      "score": 0.44627365469932556
    },
    {
      "name": "Natural language processing",
      "score": 0.41573256254196167
    },
    {
      "name": "Algorithm",
      "score": 0.20906206965446472
    },
    {
      "name": "Mathematics",
      "score": 0.08165103197097778
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4391767781",
      "name": "State Key Laboratory of Software Development Environment",
      "country": null
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    }
  ]
}