{
    "title": "Language Modeling with Sparse Product of Sememe Experts",
    "url": "https://openalex.org/W2963187078",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2568247281",
            "name": "Yihong Gu",
            "affiliations": [
                "Compound Semiconductor Technologies (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2060984669",
            "name": "Jun Yan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2101703816",
            "name": "Hao Zhu",
            "affiliations": [
                "Compound Semiconductor Technologies (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2051269448",
            "name": "Zhiyuan Liu",
            "affiliations": [
                "Compound Semiconductor Technologies (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2506913441",
            "name": "Ruobing Xie",
            "affiliations": [
                "Search (Poland)",
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2157167650",
            "name": "Maosong Sun",
            "affiliations": [
                "Compound Semiconductor Technologies (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2113285501",
            "name": "Fen Lin",
            "affiliations": [
                "Search (Poland)",
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2143226789",
            "name": "Leyu Lin",
            "affiliations": [
                "Tencent (China)",
                "Search (Poland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2109664771",
        "https://openalex.org/W2097333193",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W1513915383",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W2962832505",
        "https://openalex.org/W2116064496",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2890885376",
        "https://openalex.org/W2095368471",
        "https://openalex.org/W2742102274",
        "https://openalex.org/W2100714283",
        "https://openalex.org/W2798604731",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2079182758",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2475757000",
        "https://openalex.org/W2963266340",
        "https://openalex.org/W2742448943",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2062270497",
        "https://openalex.org/W4206765718",
        "https://openalex.org/W609399965",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2341718173",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4245107743",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W2964318358",
        "https://openalex.org/W1843891098",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2962996600",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W4300553569",
        "https://openalex.org/W2740745856",
        "https://openalex.org/W2740768945",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2125177801",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W1988939740",
        "https://openalex.org/W2028339364",
        "https://openalex.org/W2039550232",
        "https://openalex.org/W2212703438",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2963537482"
    ],
    "abstract": "Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution given textual context. Afterwards, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics, and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline generation demonstrate the significant effectiveness of SDLM.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4642–4651\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n4642\nLanguage Modeling with Sparse Product of Sememe Experts\nYihong Gu1,2,⇤ Jun Yan1,3,⇤ Hao Zhu1,2,⇤ Zhiyuan Liu1,2,†\nRuobing Xie4 Maosong Sun1,2 Fen Lin4 Leyu Lin4\n1Institute for Artiﬁcial Intelligence\nState Key Lab on Intelligent Technology and Systems\n2Department of CST,3Department of EE, Tsinghua University, Beijing, China\n4Search Product Center, WeChat Search Application Department, Tencent\n{gyh15,j-yan15,zhuhao15}@mails.tsinghua.edu.cn,\n{lzy,sms}@tsinghua.edu.cn, xrbsnowing@163.com,\n{felicialin,goshawklin}@tencent.com\nAbstract\nMost language modeling methods rely on\nlarge-scale data to statistically learn the se-\nquential patterns of words. In this pa-\nper, we argue that words are atomic lan-\nguage units but not necessarily atomic seman-\ntic units. Inspired by HowNet, we use se-\nmemes, the minimum semantic units in hu-\nman languages, to represent the implicit se-\nmantics behind words for language model-\ning, named Sememe-Driven Language Model\n(SDLM). More speciﬁcally, to predict the next\nword, SDLM ﬁrst estimates the sememe dis-\ntribution given textual context. Afterwards, it\nregards each sememe as a distinct semantic ex-\npert, and these experts jointly identify the most\nprobable senses and the corresponding word.\nIn this way, SDLM enables language mod-\nels to work beyond word-level manipulation to\nﬁne-grained sememe-level semantics, and of-\nfers us more powerful tools to ﬁne-tune lan-\nguage models and improve the interpretabil-\nity as well as the robustness of language mod-\nels. Experiments on language modeling and\nthe downstream application of headline gener-\nation demonstrate the signiﬁcant effectiveness\nof SDLM. Source code and data used in the\nexperiments can be accessed at https://\ngithub.com/thunlp/SDLM-pytorch.\n1 Introduction\nLanguage Modeling (LM) aims to measure the\nprobability of a word sequence, reﬂecting its ﬂu-\nency and likelihood as a feasible sentence in a\nhuman language. Language Modeling is an es-\nsential component in a wide range of natural lan-\nguage processing (NLP) tasks, such as Machine\nTranslation (Brown et al., 1990; Brants et al.,\n2007), Speech Recognition (Katz, 1987), Informa-\ntion Retrieval (Berger and Lafferty, 1999; Ponte\n⇤ Equal contribution.\n† Correspondence author.\n(a) Conventional Decodercontext vector word  distributioncontext vectorSememe-Driven Decoderword distributionsememe distributionSememe PredictorSense Predictor Word Predictorsense distribution(b)\nFigure 1: Decoder of (a) Conventional Language\nModel, (b) Sememe-Driven Language Model.\nand Croft, 1998; Miller et al., 1999; Hiemstra,\n1998) and Document Summarization (Rush et al.,\n2015; Banko et al., 2000).\nA probabilistic language model calculates the\nconditional probability of the next word given\ntheir contextual words, which are typically learned\nfrom large-scale text corpora. Taking the sim-\nplest language model for example, N-Gram es-\ntimates the conditional probabilities according to\nmaximum likelihood over text corpora (Jurafsky,\n2000). Recent years have also witnessed the ad-\nvances of Recurrent Neural Networks (RNNs) as\nthe state-of-the-art approach for language model-\ning (Mikolov et al., 2010), in which the context is\nrepresented as a low-dimensional hidden state to\npredict the next word.\nThose conventional language models including\nneural models typically assume words as atomic\nsymbols and model sequential patterns at word\nlevel. However, this assumption does not neces-\nsarily hold to some extent. Let us consider the fol-\nlowing example sentence for which people want to\npredict the next word in the blank,\nThe U.S. trade deﬁcit last year is initially\nestimated to be 40 billion .\nPeople may ﬁrst realize aunit should be ﬁlled in,\nthen realize it should be acurrency unit. Based on\nthe country this sentence is talking about, the U.S.,\none may conﬁrm it should be anAmerican cur-\n4643\nrency unitand predict the worddollars. Here, the\nunit, currency, and American can be regarded as\nbasic semantic units of the worddollars. This pro-\ncess, however, has not been explicitly taken into\nconsideration by conventional language models.\nThat is, although in most cases words are atomic\nlanguage units, words are not necessarily atomic\nsemantic units for language modeling. We ar-\ngue that explicitly modeling these atomic semantic\nunits could improve both the performance and the\ninterpretability of language models.\nLinguists assume that there is a limited close\nset of atomic semantic units composing the se-\nmantic meanings of an open set of concepts (i.e.\nword senses). These atomic semantic units are\nnamed sememes (Dong and Dong, 2006).i Since\nsememes are naturally implicit in human lan-\nguages, linguists have devoted much effort to ex-\nplicitly annotate lexical sememes for words and\nbuild linguistic common-sense knowledge bases.\nHowNet (Dong and Dong, 2006) is one of the\nrepresentative sememe knowledge bases, which\nannotates each Chinese word sense with its se-\nmemes. The philosophy of HowNet regards the\nparts and attributes of a concept can be well rep-\nresented by sememes. HowNet has been widely\nutilized in many NLP tasks such as word similar-\nity computation (Liu, 2002) and sentiment analy-\nsis (Fu et al., 2013). However, less effort has been\ndevoted to exploring its effectiveness in language\nmodels, especially neural language models.\nIt is non-trivial for neural language models to\nincorporate discrete sememe knowledge, as it is\nnot compatible with continuous representations\nin neural models. In this paper, we propose\na Sememe-Driven Language Model (SDLM) to\nleverage lexical sememe knowledge. In order to\npredict the next word, we design a novel sememe-\nsense-word generation process: (1) We ﬁrst esti-\nmate sememes’ distribution according to the con-\ntext. (2) Regarding these sememes as experts, we\npropose a sparse product of experts method to se-\nlect the most probable senses. (3) Finally, the dis-\ntribution of words could be easily calculated by\nmarginalizing out the distribution of senses.\nWe evaluate the performance of SDLM on the\nlanguage modeling task using a Chinese news-\ni Note that although sememes are deﬁned as the mini-\nmum semantic units, there still exist several sememes for\ncapturing syntactic information. For example, the word\nå “with” corresponds to one speciﬁc sememeü˝Õ\n“FunctWord”.\npaper corpus People’s Dailyii (Renmin Ribao),\nand also on the headline generation task using the\nLarge Scale Chinese Short Text Summarization\n(LCSTS) dataset (Hu et al., 2015). Experimen-\ntal results show that SDLM outperforms all those\ndata-driven baseline models. We also conduct case\nstudies to show that our model can effectively pre-\ndict relevant sememes given context, which can\nimprove the interpretability and robustness of lan-\nguage models.\n2 Background\nLanguage models target at learning the\njoint probability of a sequence of words\nP(w1,w 2, ··· ,w n), which is usually factor-\nized as P(w1,w 2, ··· ,w n)= Qn\nt=1 P(wt|w<t).\nBengio et al.(2003) propose the ﬁrst Neural Lan-\nguage Model as a feed-forward neural network.\nMikolov et al. (2010) use RNN and a softmax\nlayer to model the conditional probability. To be\nspeciﬁc, it can be divided into two parts in series.\nFirst, a context vectorgt is derived from a deep\nrecurrent neural network. Then, the probability\nP(wt+1|wt)= P(wt+1; gt) is derived from a\nlinear layer followed by a softmax layer based\non gt. Let RNN(· , · ; ✓NN) denote the deep\nrecurrent neural network, where✓NN denotes the\nparameters. The ﬁrst part can be formulated as\ngt = RNN(xwt , {ht\u00001\nl }L\nl=1; ✓NN). (1)\nHere we use subscripts to denote layers and su-\nperscripts to denote timesteps. Thusht\nl represents\nthe hidden state of theL-th layer at timestept.\nxwt 2 RH0 is the input embedding of wordwt\nwhere H0 is the input embedding size. We also\nhave gt 2 RH1, whereH1 is the dimension of the\ncontext vector.\nSupposing that there areN words in the lan-\nguage we want to model, the second part can be\nwritten as\nP(wt+1; gt)= exp(gtTwwt+1)P\nw0 exp(gtTww0 )\n, (2)\nwhere ww is the output embedding of wordw and\nw1, w2, ··· wN 2 RH2. Here H2 is the output\nembedding size. For a conventional neural lan-\nguage model,H2 always equals toH1.\nii http://paper.people.com.cn/rmrb/\n4644\n\u0006\u0002(\u0003\u0002) “apple(fruit)”\u0006\u0002(\u0004\u0005) “apple(computer)”\u0007\u0001(\u0003\u0002) “pear(fruit)”\n\u0006\u0002 “apple”\n…\n… \u0007\u0001 “pear”P(word)\nP(sense)\n\u0007\u0006 “fruit” \u0011\n “bring”\b\r “computer”\u000b\t\u0010\u0004 “SpeBrand”\u0003 “able”\u0005\f\u000e  “PatternVal”\n… x0.2×sememe experts\ncontext vector\n\u0001 “I”\u0002 “in”\u0006\u000f “orchard”\u0012 “pick”LSTMLSTMLSTMLSTM\n… x0.1×x0.9× x0.1× x0.3× x0.2×\nFigure 2: An example of the architecture of our model.\nGiven the corpus{wt}n\nt=1, the loss function is\ndeﬁned by the negative log-likelihood:\nL(✓)= \u00001\nn\nnX\nt=1\nlog P(wt|w<t; ✓), (3)\nwhere ✓ = {{xi}N\ni=1, {wi}N\ni=1, ✓NN} is the set of\nparameters that are needed to be trained.\n3 Methodology\nIn this section, we present our SDLM which uti-\nlizes sememe information to predict the probabil-\nity of the next word. SDLM is composed of three\nmodules in series: Sememe Predictor, Sense Pre-\ndictor and Word Predictor. The Sememe Predictor\nﬁrst takes the context vector as input and assigns a\nweight to each sememe. Then each sememe is re-\ngarded as an expert and makes predictions about\nthe probability distribution over a set of senses\nin the Sense Predictor. Finally, the probability of\neach word is obtained in the Word Predictor.\nHere we use an example shown in Figure2 to\nillustrate our architecture. Given context⌘(ú\nÌX“In the orchard, I pick”, the actual next word\ncould be˘ú “apples”. From the context, espe-\ncially the wordúÌ “orchard” andX “pick”, we\ncan infer that the next word probably represents\na kind of fruit. So the Sememe Predictor assigns\na higher weight to the sememe4ú “fruit” (0.9)\nand lower weights to irrelevant sememes like5\n⌘ “computer” (0.1). Therefore in the Sense Pre-\ndictor, the sense˘ú (4ú) “apple (fruit)” is as-\nsigned a much higher probability than the sense˘\nú (5⌘) “apple (computer)”. Finally, the prob-\nability of the word˘ú “apple” is calculated as\nthe sum of the probabilities of its senses˘ú (4\n\u000e\u0004 “apple”\n\u0003\n\f “PatternVal”\n\u0006\u000b “computer”\n\u0001 “able”\u000f\b “bring”\t\u0007\r\u0002 “SpeBrand”\n\u0005\u0004 “fruit”modifiermodifier\nSense #1:  \u0005\u0001(\u0003\u0004) “apple(computer)”Sense #2:  \u0005\u0001(\u0002\u0001) “apple(fruit)”\nword\nFigure 3: An example of the word-sense-sememe hier-\narchy.\nú) “apple(fruit)” and˘ú (5⌘) “apple (com-\nputer)”.\nIn the following subsections, we ﬁrst introduce\nthe word-sense-sememe hierarchy in HowNet, and\nthen give details about our SDLM.\n3.1 Word-Sense-Sememe Hierarchy\nWe also use the example of “apple” to illustrate\nthe word-sense-sememe hierarchy. As shown in\nFigure 3, the word˘ú “apple” has two senses,\none is the Apple brand, the other is a kind of fruit.\nEach sense is annotated with several sememes or-\nganized in a hierarchical structure. More speciﬁ-\ncally, in HowNet, sememes “PatternVal”, “bring”,\n“SpeBrand”, “computer” and “able” are annotated\nwith the word “apple” and organized in a tree\nstructure. In this paper, we ignore the structural\nrelationship between sememes. For each word, we\ngroup all its sememes as an unordered set.\nWe present the notations that we use in the fol-\nlowing subsections as follows. We deﬁne the over-\nall sememe, sense, and word set asE, S and W.\nAnd we suppose the corpus containsK = |E| se-\nmemes, M = |S| senses and N = |W| words.\nFor word w 2W , we denote its corresponding\nsense set asS(w). For sense s 2S (w), we de-\nnote its corresponding sememes as an unordered\nset E(s) = {en1,e n2, ··· ,e nk }⇢E = {ek}K\nk=1.\n3.2 Sememe Predictor\nThe Sememe Predictor takes the context vec-\ntor g 2 RH1 as input and assigns a weight to\neach sememe. We assume that given the context\nw1,w 2, ··· ,w t\u00001, the events that wordwt con-\ntains sememeek (k 2{ 1, 2, ··· ,K }) are indepen-\ndent, since the sememe is the minimum semantic\nunit and there is no semantic overlap between any\ntwo different sememes. For simplicity, we ignore\n4645\nthe superscriptt. We design the Sememe Predic-\ntor as a linear decoder with the sigmoid activation\nfunction. Therefore, qk, the probability that the\nnext word contains sememeek, is formulated as\nqk = P (ek|g)= \u0000(gTvk + bk), (4)\nwhere vk 2 RH1, bk 2 R are trainable parameters,\nand \u0000(· ) denotes the sigmoid activation function.\n3.3 Sense Predictor and Word Predictor\nThe architecture of the Sense Predictor is moti-\nvated by Product of Experts (PoE) (Hinton, 1999).\nWe regard each sememe as an expert that only\nmakes predictions on the senses connected with it.\nLet D(ek) denote the set of senses that contain se-\nmeme ek, thek-th expert. Different from conven-\ntional neural language models, which directly use\nthe inner product of the context vectorg 2 RH1\nand the output embeddingww 2 RH2 for word\nw to generate the score for each word, we use\n\u0000(k)(g, w) to calculate the score given by expert\nek. And we choose a bilinear function parame-\nterized with a matrixUk 2 RH1⇥H2 as a straight\nimplementation of\u0000(k)(· , · ):\n\u0000(k)(g, w)= gTUkw. (5)\nLet ws denote the output embedding of sense\ns. The score of senses provided by sememe ex-\npert ek can be written as\u0000(k)(g, ws). Therefore,\nP(ek)(s|g), the probability of senses given by ex-\npert ek, is formulated as\nP (ek)(s|g)= exp(qkCk,s\u0000(k)(g, ws))P\ns02D(ek) exp(qkCk,s0 \u0000(k)(g, ws0 )), (6)\nwhere Ck,s is a normalization constant because\nsense s is not connected to all experts (the\nconnections are sparse with approximately \u0000N\nedges, \u0000< 5). Here we can choose either\nCk,s =1 /|E(s)| (left normalization) or Ck,s =\n1/\np\n|E(s)||D(ek)| (symmetric normalization).\nIn the Sense Predictor, qk can be viewed as\na gate which controls the magnitude of the term\nCk,s\u0000(k)(g, wws ), thus control the ﬂatness of the\nsense distribution provided by sememe expertek.\nConsider the extreme case whenqk ! 0, the pre-\ndiction will converge to the discrete uniform dis-\ntribution. Intuitively, it means that the sememe ex-\npert will refuse to provide any useful information\nwhen it is not likely to be related to the next word.\nFinally, we summarize the predictions on sense\ns by taking the product of the probabilities given\nby relevant experts and then normalize the result;\nthat is to say,P(s|g), the probability of senses,\nsatisﬁes\nP (s|g) /\nY\nek2E(s)\nP (ek)(s|g). (7)\nUsing Equation 5 and 6, we can formulate\nP(s|g) as\nP (s|g)=\nexp(P\nek2E(s) qkCk,sgTUkws)P\ns0 exp(P\nek2E(s0) qkCk,s0 gTUkws0 ). (8)\nIt should be emphasized that all the supervision\ninformation provided by HowNet is embodied in\nthe connections between the sememe experts and\nthe senses. If the model wants to assign a high\nprobability to senses, it must assign a high prob-\nability to some of its relevant sememes. If the\nmodel wants to assign a low probability to sense\ns, it can assign a low probability to its relevant\nsememes. Moreover, the prediction made by se-\nmeme expertek has its own tendency because of\nits own \u0000(k)(· , · ). Besides, the sparsity of con-\nnections between experts and senses is also de-\ntermined by HowNet itself. For our dataset, on\naverage, a word is connected with 3.4 sememe ex-\nperts and each sememe expert will make predic-\ntions about 22 senses.\nAs illustrated in Figure2, in the Word Predic-\ntor, we get P(w|g), the probability of wordw,\nby summing up probabilities of correspondings\ngiven by the Sense Predictor, that is\nP (w|g)=\nX\ns2S(w)\nP (s|g). (9)\n3.4 Implementation Details\nBasis Matrix Actually, HowNet contains K ⇡\n2000 sememes. In practice, we cannot directly in-\ntroduce K ⇥ H1 ⇥ H2 parameters, which might\nbe computationally infeasible and lead to overﬁt-\nting. To address this problem, we apply a weight-\nsharing trick called the basis matrix. We useR\nbasis matrices and their weighted sum to estimate\nUk:\nUk =\nRX\nr=1\n↵k,rQr, (10)\nwhere Qr 2 RH1⇥H2, ↵k,r > 0 are trainable pa-\nrameters, andPR\nr=1 ↵k,r =1 .\nWeight Tying To incorporate the weight tying\nstrategy (Inan et al., 2017; Press and Wolf, 2017),\nwe use the same output embedding for multiple\n4646\nsenses of a word. To be speciﬁc, the sense output\nembedding ws for each s 2 S(w) is the same as\nthe word input embeddingxw.\n4 Experiments\nWe evaluate our SDLM on a Chinese language\nmodeling dataset, namely People’s Daily based on\nperplexity.iii Furthermore, to show that our SDLM\nstructure can be a generic Chinese word-level de-\ncoder for sequence-to-sequence learning, we con-\nduct a Chinese headline generation experiment on\nthe LCSTS dataset. Finally, we explore the inter-\npretability of our model with cases, showing the\neffectiveness of utilizing sememe knowledge.\n4.1 Language Modeling\nDataset\nWe choose the People’s Daily Corpus, which is\nwidely used for Chinese NLP tasks, as the re-\nsource. It contains one month’s news text from\nPeople’s Daily (Renmin Ribao). Taking Penn\nTreebank (PTB) (Marcus et al., 1993) as a ref-\nerence, we build a dataset for Chinese language\nmodeling based on the People’s Daily Corpus with\n734k, 10k and 19k words in the training, valida-\ntion and test set. After the preprocessing similar\nto (Mikolov et al., 2010) (see AppendixA), we get\nour dataset and the ﬁnal vocabulary size is 13,476.\nBaseline\nAs for baselines, we consider three kinds of neural\nlanguage modeling architectures with LSTM cells:\nsimple LSTM, Tied LSTM and AWD-LSTM.\nLSTM and Tied LSTMZaremba et al. (2014)\nuse the dropout strategy to prevent overﬁtting for\nneural language models and adopt it to two-layer\nLSTMs with different embedding and hidden size:\n650 for medium LSTM, and 1500 for large LSTM.\nEmploying the weight tying strategy, we get Tied\nLSTM with better performance. We set LSTM and\nTied LSTM of medium and large size as our base-\nline models and use the code from PyTorch exam-\nplesiv as their implementations.\nAWD-LSTM Based on several strategies for reg-\nularizing and optimizing LSTM-based language\nmodels, Merity et al.(2018) propose AWD-LSTM\niii Although we only conduct experiments on Chinese cor-\npora, we argue that this model has the potential to be ap-\nplied to other languages in the light of works on construc-\ntion sememe knowledge bases for other languages, such\nas (Qi et al., 2018).\niv https://github.com/pytorch/examples/\ntree/master/word_language_model\nas a three-layer neural network, which serves as a\nvery strong baseline for word-level language mod-\neling. We build it with the code released by the\nauthorsv.\nVariants of SoftmaxMeanwhile, to compare our\nSDLM with other language modeling decoders,\nwe set cHSM (Class-based Hierarchical Softmax)\n(Goodman, 2001), tHSM (Tree-based Hierarchi-\ncal Softmax) (Mikolov et al., 2013) and MoS\n(Mixture of Softmaxes) (Yang et al., 2018) as\nthe baseline add-on structures to the architectures\nabove.\nExperimental Settings\nWe apply our SDLM and other variants of softmax\nstructures to the architectures mentioned above:\nLSTM (medium / large), Tied LSTM (medium /\nlarge) and AWD-LSTM. MoS and SDLM are only\napplied on the models that incorporate weight ty-\ning, while tHSM is only applied on the models\nwithout weight tying, since it is not compatible\nwith this strategy.\nFor a fair comparison, we train these mod-\nels with same experimental settings and conduct\na hyper-parameter search for baselines as well\nas our models (the search setting and the opti-\nmal hyper-parameters can be found in Appendix\nC.1). We keep using these hyper-parameters in our\nSDLM for all architectures. It should be empha-\nsized that we use the SGD optimizer for all archi-\ntectures, and we decrease the learning rate by a\nfactor of 2 if no improvement is observed on the\nvalidation set. We uniformly initialize the word\nembeddings, the class embeddings for cHSM and\nthe non-leaf embeddings for tHSM in[\u00000.1, 0.1].\nIn addition, we setR, the number of basis matri-\nces, to 5 in Tied LSTM architecture and to10 in\nAWD-LSTM architecture. We choose theleft nor-\nmalization strategy because it performs better.\nExperimental Results\nTable 1 shows the perplexity on the validation and\ntest set of our models and the baseline models.\nFrom Table1, 2, and3, we can observe that:\n1. Our models outperform the corresponding base-\nline models of all structures, which indicates the\neffectiveness of our SDLM. Moreover, our SDLM\nnot only consistently outperforms state-of-the-art\nMoS model, but also offers much better inter-\npretability (as described in Sect. 4.3), which\nv https://github.com/salesforce/\nawd-lstm-lm\n4647\nmakes it possible to interpret the prediction pro-\ncess of the language model. Note that under a fair\ncomparison, we do not see MoS’s improvement\nover AWD-LSTM while our SDLM outperforms\nit by 1.20 with respect to perplexity on the test set.\n2. To further locate the performance improve-\nment of our SDLM, we study the perplexity of\nthe single-sense words and multi-sense words sep-\narately on Tied LSTM (medium) and Tied LSTM\n(medium) + SDLM. Improvements with respect to\nperplexity are presented in Table2. The perfor-\nmance on both single-sense words and multi-sense\nwords gets improved while multi-sense words\nbeneﬁt more from SDLM structure because they\nhave richer sememe information.\n3. In Table 3 we study the perplexity of words\nwith different mean number of sememes. We can\nsee that our model outperforms baselines in all\ncases and is expected to beneﬁt more as the mean\nnumber of sememes increases.\nModel #Paras Validation Test\nLSTM (medium) 24M 116.46 115.51\n+ cHSM 24M 129.12 128.12\n+ tHSM 24M 151.00 150.87\nTied LSTM (medium) 15M 105.35 104.67\n+ cHSM 15M 116.78 115.66\n+ MoS 17M 98.47 98.12\n+ SDLM 17M 97.75 97.32\nLSTM (large) 76M 112.39 111.66\n+ cHSM 76M 120.07 119.45\n+ tHSM 76M 140.41 139.61\nTied LSTM (large) 56M 101.46 100.71\n+ cHSM 56M 108.28 107.52\n+ MoS 67M 94.91 94.40\n+ SDLM 67M 94.24 93.60\nAWD-LSTMiv 26M 89.35 88.86\n+ MoS 26M 92.98 92.76\n+ SDLM 27M 88.16 87.66\nTable 1: Single model perplexity on validation and test\nsets on the People’s Daily dataset.\n#senses = 1 #senses > 1\nBaseline ppl 93.21 121.18\nSDLM ppl 87.22 111.88\n\u0000ppl 5.99 9.29\n\u0000ppl/Baseline ppl 6.4% 7.8%\nTable 2: Perplexity of words with different number of\nsenses on the test set.\nWe also test the robustness of our model by ran-\ndomly removing 10% sememe-sense connections\nin HowNet. The test perplexity for Tied LSTM\niv We ﬁnd that multi-layer AWD-LSTM has problems con-\nverging when adopting cHSM, so we skip that result.\n[1, 2) [2, 4) [4, 7) [7, 14)\nBaseline ppl 71.56 161.32 557.26 623.71\nSDLM ppl 68.47 114.95 465.29 476.45\n\u0000ppl 3.09 16.36 91.98 147.25\n\u0000ppl/Baseline ppl 4.3% 10.1% 16.5% 23.61%\nTable 3: Perplexity of words with different mean num-\nber of sememes on the test set.\n(medium) + SDLM slightly goes up to 97.67, com-\npared to 97.32 with a complete HowNet, which\nshows that our model is robust to tiny incomplete-\nness of annotations. However, the performance\nof out model is still largely dependent upon the\naccuracy of sememe annotations. As HowNet\nis continuously updated, we expect our model to\nperform better with sememe knowledge of higher\nquality.\n4.2 Headline Generation\nDataset\nWe use the LCSTS dataset to evaluate our SDLM\nstructure as the decoder of the sequence-to-\nsequence model. As its author suggests, we di-\nvide the dataset into the training set, the validation\nset and the test set, whose sizes are 2.4M, 8.7k\nand 725 respectively. Details can be found in Ap-\npendix B.\nModels\nFor this task, we consider two models for compar-\nison.\nRNN-context As described in (Bahdanau et al.,\n2015), RNN-context is a basic sequence-to-\nsequence model with a bi-LSTM encoder, an\nLSTM decoder and attention mechanism adopted.\nThe context vector is concatenated with the word\nembedding at each timestep when decoding. It’s\nwidely used for sequence-to-sequence learning, so\nwe set it as the baseline model.\nRNN-context-SDLM Based on RNN-context,\nwe substitute the decoder with our proposed\nSDLM and name it RNN-context-SDLM.\nExperimental Settings\nWe implement our models with PyTorch, on top of\nthe OpenNMT librariesv. For both models, we set\nthe word embedding size to 250, the hidden unit\nsize to 250, the vocabulary size to 40000, and the\nbeam size of the decoder to 5. For RNN-context-\nSDLM, we set the number of basis matrices to\n3. We conduct a hyper-parameter search for both\nv http://opennmt.net\n4648\nmodels (see AppendixC.2 for settings and optimal\nhyper-parameters).\nExperimental Results\nFollowing previous works, we report the F1-score\nof ROUGE-1, ROUGE-2, and ROUGE-L on the\ntest set. Table4 shows that our model outperforms\nthe baseline model on all metrics. We attribute the\nimprovement to the use of SDLM structure.\nWords in headlines do not always appear in the\ncorresponding articles. However, words with the\nsame sememes have a high probability to appear in\nthe articles intuitively. Therefore, a probable rea-\nson for the improvement is that our model could\npredict sememes highly relevant to the article, thus\ngenerate more accurate headlines. This could be\ncorroborated by our case study.\nModel Rouge-1 Rouge-2 Rouge-L\nRNN-context 37.5 25.0 34.9\nRNN-context-SDLM 38.9 26.2 36.2\nTable 4: ROUGE scores of both models on the LCSTS\ntest set.\n4.3 Case Study\nThe above experiments demonstrate the effective-\nness of our SDLM. Here we present some samples\nfrom the test set of the People’s Daily Corpus in\nTable 5 as well as the LCSTS dataset in Table6\nand conduct further analysis.\nFor each example of language modeling, given\nthe context of previous words, we list the Top\n5 words and Top 5 sememes predicted by our\nSDLM. The target words and the sememes anno-\ntated with them in HowNet are blackened. Note\nthat if the target word is an out-of-vocabulary\nExample (1)\nªté˝8◆⌃Ó\u0000e0°:<N> ⇥\nThe U.S. trade deﬁcit last year is initially estimated to be<N> .\nTop 5 word prediction\néééCCC“dollar” \u0000“,” ⇥“.”\nÂC“yen” å“and”\nTop 5 sememe prediction\nFFF⇢⇢⇢“commerce”———ççç“ﬁnance” UUUMMM“unit”\n⇢⌘“amount” ◆“proper name”\nExample (2)\n?;⌃ Ú~rÜ\u0000y}‰⇥\nAlbanian Prime Minister has signed an order.\nTop 5 word prediction\nÖ“inside” <unk> (“at”\nT“tower” å“and”\nTop 5 sememe prediction\n???“politics” ∫∫∫“person” ±I“ﬂowers”\n≈≈≈˚˚˚“undertake”4ﬂ“waters”\nTable 5: Some examples of word and sememe predic-\ntions on the test set of the People’s Daily Corpus.\n(OOV) word, helpful sememes that are related to\nthe target meaning are blackened.\nSememes annotated with the corresponding\nsense of the target wordéC “dollar” areUM\n“unit”, F⇢ “commerce”, —ç “ﬁnance”, '\u0000\n“money” andé˝ “US”. In Example (1), the tar-\nget word “dollar” is predicted correctly and most\nof its sememes are activated in the predicting pro-\ncess. It indicates that our SDLM has learned the\nword-sense-sememe hierarchy and used sememe\nknowledge to improve language modeling.\nExample (2) shows that our SDLM can provide\ninterpretable results on OOV word prediction with\nsememe information associated with it. The tar-\nget word here should be the name of the Albanian\nprime minister, which is out of vocabulary. But\nwith our model, one can still conclude that this\nword is probably relevant to the sememe “poli-\ntics”, “person”, “ﬂowers”, “undertake” and “wa-\nters”, most of which characterize the meaning of\nthis OOV word – the name of a politician. This\nfeature can be helpful when the vocabulary size is\nlimited or there are many terminologies and names\nin the corpus.\nFor the example of headline generation, given\nthe article and previous words, when generating\nthe word \u0000 “student”, except the sememeÑô\n“predict”, all other Top 5 predicted sememes have\nhigh relevance to either the predicted word or the\ncontext. To be speciﬁc, the sememef` “study”\nis annotated with \u0000 “student” in HowNet. ⇤\n’ “exam” indicates “college entrance exam”.y\nöLP “brand” indicates “BMW”. And ÿI\n“higher” indicates “higher education”, which is\nthe next step after this exam. We can conclude that\nwith sememe knowledge, our SDLM structure can\nextract critical information from both the given ar-\nticle and generated words explicitly and produce\nbetter summarization based on it.\n5 Related Work\nNeural Language Modeling. RNNs have\nachieved state-of-the-art performance in the\nlanguage modeling task since Mikolov et al.\n(2010) ﬁrst apply RNNs for language modeling.\nMuch work has been done to improve RNN-based\nlanguage modeling. For example, a variety of\nwork (Zaremba et al., 2014; Gal and Ghahramani,\n2016; Merity et al., 2017, 2018) introduces many\nregularization and optimization methods for\nRNNs. Based on the observation that the word\n4649\nArticle\n8 Â \u0000 ⌧ ∞ \u0000 \u0000 ùl f ¬† ÿ⇤ Ñ 7 ⇤\u0000\n⇤: \\⌦ ´ ì \u0000 ‡ \u0000· —⇤\u0000 \u0000 °6 \\⌦ K\n:\u0000ŒÃ\u0000\u0000⇢⌃s—⇤\u0000\u0000Œ\u0000\u0000\u0000í9\n0≤⇥v„˙¬\u0000⇢“ `ÂS⌘8/\u0000J\n\u0000`1Â⌘ \u0000” ÓM\u0000S∫⇤\u0000Ú´ÿY ⇥\nOn the 8th in Fuxin, a male student drove a BMW to\ntake the college entrance exam and was caught cheating.\nBecause the teacher conﬁscated his mobile phone, he\nkicked the teacher from the last row to the podium and\nshouted: ”Do you know who my dad is? How dare you\ncatch me!” Currently, this student has been detained.\nGold\n7\u0000 ÿ⇤ \\⌦ ˝S —⇤\u0000 \u0000 ⇢` ÂS ⌘ 8 /\n\u0000\u0000\nIn the college entrance exam, a male student caught\ncheating hit the teacher: Do you know who my dad is?\nRNN-context-SDLM\nÿ⇤\u0000\\⌦ ´ì⇢`ÂS⌘8/\u0000J\u0000\nIn the college entrance exam, a student was caught\ncheating: Do you know who my dad is?\nTop 5 sememe prediction\n⇤⇤⇤’’’“exam” fff```“study” yyyöööLLLPPP“brand”\nÑô“predict” ÿÿÿIII“higher”\nTable 6: An example of generated headlines on the LC-\nSTS test set.\nappearing in the previous context is more likely to\nappear again, some work (Grave et al., 2017a,b)\nproposes to use cache for improvements. In this\npaper, we mainly focus on the output decoder,\nthe module between the context vector and the\npredicted probability distribution. Similar to our\nSDLM, Yang et al. (2018) propose a high-rank\nmodel which adopts a Mixture of Softmaxes\nstructure for the output decoder. However,\nour model is sememe-driven with each expert\ncorresponding to an interpretable sememe.\nHierarchical Decoder Since softmax computa-\ntion on large vocabulary is time-consuming, there-\nfore being a dominant part of the model’s com-\nplexity, various hierarchical softmax models have\nbeen proposed to address this issue. These mod-\nels can be categorized to class-based models and\ntree-based models according to their hierarchi-\ncal structure. Goodman (2001) ﬁrst proposes the\nclass-based model which divides the whole vocab-\nulary into different classes and uses a hierarchi-\ncal softmax decoder to model the probability as\nP(word)= P(word|class)P(class), which is sim-\nilar to our model. For the tree-based models, all\nwords are organized in a tree structure and the\nword probability is calculated as the probability of\nalways choosing the correct child along the path\nfrom the root node to the word node. WhileMorin\nand Bengio(2005) utilize knowledge from Word-\nNet to build the tree, Mnih and Hinton (2008)\nbuild it in a bootstrapping way andMikolov et al.\n(2013) construct a Huffman Tree based on word\nfrequencies. Recently, Jiang et al.(2017) reform\nthe tree-based structure to make it more efﬁcient\non GPUs. The major differences between our\nmodel and theirs are the purpose and the moti-\nvation. Our model targets at improving the per-\nformance and interpretability of language model-\ning using external knowledge in HowNet. There-\nfore, we take its philosophy of the word-sense-\nsememe hierarchy to design our hierarchical de-\ncoder. Meanwhile, the class-based and tree-based\nmodels are mainly designed to speed up the soft-\nmax computation in the training process.\nSememe. Recently, there are a lot of works con-\ncentrating on utilizing sememe knowledge in tra-\nditional natural language processing tasks. For ex-\nample, Niu et al.(2017) use sememe knowledge\nto improve the quality of word embeddings and\ncope with the problem of word sense disambigua-\ntion. Xie et al.(2017) apply matrix factorization to\npredict sememes for words.Jin et al.(2018) im-\nprove their work by incorporating character-level\ninformation. Our work extends the previous works\nand tries to combine word-sense-sememe hierar-\nchy with the sequential model. To be speciﬁc,\nthis is the ﬁrst work to improve the performance\nand interpretability of Neural Language Modeling\nwith sememe knowledge.\nProduct of Experts. As Hinton (1999, 2002)\npropose, the ﬁnal probability can be calculated as\nthe product of probabilities given by experts.Gales\nand Airey(2006) apply PoE to the speech recog-\nnition where each expert is a Gaussian mixture\nmodel. Unlike their work, in our SDLM, each\nexpert is mapped to a sememe with better inter-\npretability. Moreover, as the ﬁnal distribution is\na categorical distribution, each expert is only re-\nsponsible for making predictions on a subset of\nthe categories (usually less than 10), so we call it\nSparse Product of Experts.\nHeadline Generation. Headline generation is\na kind of text summarization tasks. In recent\nyears, with the advances of RNNs, a lot of works\nhave been done in this domain. The encoder-\ndecoder models (Sutskever et al., 2014; Cho et al.,\n2014) have achieved great success in sequence-\nto-sequence learning. Rush et al. (2015) pro-\npose a local attention-based model for abstractive\n4650\nsentence summarization. Gu et al.(2016) intro-\nduce the copying mechanism which is close to the\nrote memorization of the human being. Ayana\net al. (2016) employ the minimum risk training\nstrategy to optimize model parameters. Different\nfrom these works, we focus on the decoder of the\nsequence-to-sequence model, and adopt SDLM to\nutilize sememe knowledge for sentence genera-\ntion.\n6 Conclusion and Further Work\nIn this paper, we propose an interpretable\nSememe-Driven Language Model with a hier-\narchical sememe-sense-word decoder. Besides\ninterpretability, our model also achieves state-\nof-the-art performance in the Chinese Language\nModeling task and shows improvement in the\nHeadline Generation task. These results indicate\nthat SDLM can successfully take advantages of se-\nmeme knowledge.\nAs for future work, we plan the following re-\nsearch directions: (1) In language modeling, given\na sequence of words, a sequence of correspond-\ning sememes can also be obtained. We will uti-\nlize the context sememe information for better se-\nmeme and word prediction. (2) Structural infor-\nmation about sememes in HowNet is ignored in\nour work. We will extend our model with the hi-\nerarchical sememe tree for more accurate relations\nbetween words and their sememes. (3) It is imag-\ninable that the performance of SDLM will be sig-\nniﬁcantly inﬂuenced by the annotation quality of\nsememe knowledge. We will also devote to fur-\nther enrich the sememe knowledge for new words\nand phrases, and investigate its effect on SDLM.\nAcknowledgement\nThis work is supported by the 973 Program (No.\n2014CB340501), the National Natural Science\nFoundation of China (NSFC No. 61572273)\nand the research fund of Tsinghua University-\nTencent Joint Laboratory for Internet Innova-\ntion Technology. This work is also funded by\nChina Association for Science and Technology\n(2016QNRC001). Hao Zhu and Jun Yan are sup-\nported by Tsinghua University Initiative Scientiﬁc\nResearch Program. We thank all members of Ts-\ninghua NLP lab. We also thank anonymous re-\nviewers for their careful reading and their insight-\nful comments.\nReferences\nShiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun.\n2016. Neural headline generation with minimum\nrisk training.arXiv preprint arXiv:1604.01904.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nICLR.\nMichele Banko, Vibhu O Mittal, and Michael J Wit-\nbrock. 2000. Headline generation based on statisti-\ncal translation. InProceedings of ACL, pages 318–\n325. Association for Computational Linguistics.\nYoshua Bengio, Rejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nAdam Berger and John Lafferty. 1999. Information re-\ntrieval as statistical translation. In Proceedings of\nSIGIR, pages 222–229. ACM.\nThorsten Brants, Ashok C Popat, Peng Xu, Franz J\nOch, and Jeffrey Dean. 2007. Large language\nmodels in machine translation. In Proceedings of\nEMNLP.\nPeter F Brown, John Cocke, Stephen A Della Pietra,\nVincent J Della Pietra, Fredrick Jelinek, John D Laf-\nferty, Robert L Mercer, and Paul S Roossin. 1990. A\nstatistical approach to machine translation.Compu-\ntational linguistics, 16(2):79–85.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. In Proceedings\nof EMNLP.\nZhendong Dong and Qiang Dong. 2006.Hownet and\nthe computation of meaning (with Cd-rom). World\nScientiﬁc.\nXianghua Fu, Guo Liu, Yanyan Guo, and Zhiqiang\nWang. 2013. Multi-aspect sentiment analysis for\nchinese online social reviews based on topic model-\ning and hownet lexicon.Knowledge-Based Systems,\n37:186–195.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. InProceedings of NIPS.\nM. J. F. Gales and S. S. Airey. 2006. Product of gaus-\nsians for speech recognition.Computer Speech and\nLanguage, 20(1):22–40.\nJ Goodman. 2001. Classes for fast maximum entropy\ntraining. InProceedings of ICASSP, pages 561–564\nvol.1.\n4651\nEdouard Grave, Moustapha Cisse, and Armand Joulin.\n2017a. Unbounded cache model for online language\nmodeling with open vocabulary. InProceedings of\nNIPS.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving neural language models with a\ncontinuous cache. InProceedings of ICLR.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nACL, pages 1631–1640.\nDjoerd Hiemstra. 1998. A linguistically motivated\nprobabilistic model of information retrieval. InPro-\nceedings of TPDL, pages 569–584. Springer.\nG. E Hinton. 1999. Products of experts. InArtiﬁcial\nNeural Networks, 1999. ICANN 99. Ninth Interna-\ntional Conference on, pages 1–6 vol.1.\nG. E. Hinton. 2002. Training products of experts by\nminimizing contrastive divergence.MIT Press.\nBaotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-\nsts: A large scale chinese short text summarization\ndataset. InProceedings of EMNLP.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. InProceed-\nings of ICLR.\nNan Jiang, Wenge Rong, Min Gao, Yikang Shen,\nZhang Xiong, Nan Jiang, Wenge Rong, Min Gao,\nYikang Shen, and Zhang Xiong. 2017. Exploration\nof tree-based hierarchical softmax for recurrent lan-\nguage models. InProceedings of IJCAI.\nHuiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie,\nMaosong Sun, Fen Lin, and Leyu Lin. 2018. In-\ncorporating chinese characters of words for lexical\nsememe prediction. In Proceedings of ACL, pages\n2439–2449. Association for Computational Linguis-\ntics.\nDan Jurafsky. 2000. Speech & language processing.\nchapter 4. Pearson Education India.\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE transactions on acoustics,\nspeech, and signal processing, 35(3):400–401.\nQun Liu. 2002. Word similarity computing based on\nhownet. Computational linguistics and Chinese lan-\nguage processing, 7(2):59–76.\nMitchell P. Marcus, Beatrice Santorini, and Ann\nMarcinkiewicz, Mary. 1993. Building a large anno-\ntated corpus of English: The Penn Treebank.Com-\nputational Linguistics, 19:313–330.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. InProceedings of ICLR.\nStephen Merity, Bryan Mccann, and Richard Socher.\n2017. Revisiting activation regularization for lan-\nguage rnns.arXiv preprint arXiv:1708.01009.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. InProceed-\nings of INTERSPEECH.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. InProceedings of NIPS, pages 3111–3119.\nDavid RH Miller, Tim Leek, and Richard M Schwartz.\n1999. A hidden markov model information retrieval\nsystem. In Proceedings of SIGIR, pages 214–221.\nACM.\nAndriy Mnih and Geoffrey Hinton. 2008. A scalable\nhierarchical distributed language model. In Pro-\nceedings of NIPS, pages 1081–1088.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nProceedings of AISTATS.\nYilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong\nSun. 2017. Improved word representation learning\nwith sememes. In Proceedings of ACL, volume 1,\npages 2049–2058.\nJay M Ponte and W Bruce Croft. 1998. A language\nmodeling approach to information retrieval. InPro-\nceedings of SIGIR, pages 275–281. ACM.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. InProceed-\nings of EACL.\nFanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu,\nRuobing Xie, and Zhiyuan Liu. 2018. Cross-\nlingual lexical sememe prediction. InProceedings\nof EMNLP.\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. InProceedings of EMNLP.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. InProceedings of NIPS, pages 3104–3112.\nRuobing Xie, Xingchi Yuan, Zhiyuan Liu, and\nMaosong Sun. 2017. Lexical sememe prediction via\nword embeddings and matrix factorization. InPro-\nceedings of IJCAI, pages 4200–4206. AAAI Press.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. InPro-\nceedings of ICLR.\nWojciech Zaremba, Ilya. Sutskever, and Oriol. Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329."
}