{
  "title": "Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues",
  "url": "https://openalex.org/W4389767727",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4281349537",
      "name": "Kobi Hackenburg",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2787205778",
      "name": "Lujain Ibrahim",
      "affiliations": [
        "Internet Society",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2559345414",
      "name": "Ben M Tappin",
      "affiliations": [
        "School of Advanced Study",
        "Royal Holloway University of London"
      ]
    },
    {
      "id": "https://openalex.org/A1904490283",
      "name": "Manos Tsakiris",
      "affiliations": [
        "School of Advanced Study",
        "Royal Holloway University of London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3015794574",
    "https://openalex.org/W4315881236",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W4283020727",
    "https://openalex.org/W4387323846",
    "https://openalex.org/W6855007681",
    "https://openalex.org/W1991160196",
    "https://openalex.org/W6829873673",
    "https://openalex.org/W6757378912",
    "https://openalex.org/W4313894975",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4387801479",
    "https://openalex.org/W4365999098",
    "https://openalex.org/W2051292619",
    "https://openalex.org/W4319265466",
    "https://openalex.org/W4362720282",
    "https://openalex.org/W2904943091",
    "https://openalex.org/W2903903405",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W2750697398",
    "https://openalex.org/W4385570689",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4242048916",
    "https://openalex.org/W4378465112",
    "https://openalex.org/W1976780908",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4367678106",
    "https://openalex.org/W4312193264",
    "https://openalex.org/W4399426804",
    "https://openalex.org/W2996728347",
    "https://openalex.org/W4402670540",
    "https://openalex.org/W4389519488",
    "https://openalex.org/W2570444177",
    "https://openalex.org/W4387428151",
    "https://openalex.org/W4401042136",
    "https://openalex.org/W4318765542",
    "https://openalex.org/W3032046549"
  ],
  "abstract": "Advances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-registered experiment (n=4,955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs of U.S. political parties – a technique we term “partisan role-play” – and directly compared their persuasiveness to that of human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not significantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on some issues exceeded, that of the human experts. Taken together, our findings suggest that — contrary to popular concern — instructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival and even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by persuasion campaigns, with important implications for the role of AI in politics and democracy.",
  "full_text": "Comparing the persuasiveness of role-playing large language\nmodels and human experts on polarized U.S. political issues\nKobi Hackenburga,1,*, Lujain Ibrahima,1,*, Ben M. Tappinb,c, and Manos Tsakirisb,c\naOxford Internet Institute, University of Oxford;bCentre for the Politics of Feelings, School of Advanced Study;cDepartment of Psychology,\nRoyal Holloway\nDecember 13, 2023\nAdvances in large language models (LLMs) could significantly dis-\nrupt political communication. In a large-scale pre-registered ex-\nperiment (n = 4,955), we prompted GPT-4 to generate persuasive\nmessages impersonating the language and beliefs of U.S. political\nparties – a technique we term “partisan role-play” – and directly\ncompared their persuasiveness to that of human persuasion ex-\nperts. In aggregate, the persuasive impact of role-playing mes-\nsages generated by GPT-4 was not significantly different from that\nof non-role-playing messages. However, the persuasive impact of\nGPT-4 rivaled, and on some issues exceeded, that of the human\nexperts. Taken together, our findings suggest that — contrary to\npopular concern — instructing current LLMs to role-play as par-\ntisans offers limited persuasive advantage, but also that current\nLLMs can rival and even exceed the persuasiveness of human ex-\nperts. These results potentially portend widespread adoption of\nAI tools by persuasion campaigns, with important implications for\nthe role of AI in politics and democracy.\nlarge language models | role-play | political persuasion | AI safety |\nAI-mediated communication\nD\nuring the 2016 U.S. presidential election, the Russian-\nbacked Internet Research Agency (IRA) deployed thou-\nsands of bots impersonating liberal American voters in online\nmessage boards and social media networks. These bots were\ndeployed with a simple aim: to discourage other liberal Amer-\nican voters from supporting Hillary Clinton (1). Today, rapid\nadvancements in large language models (LLMs) have raised\nconcerns about the potential for automated, artificially intel-\nligent (AI) agents to supercharge the production of content\nimpersonatingpartisanidentities, covertlyinfiltratingthepolit-\nical public sphere on a scale previously unseen (2–4). However,\nwhile existing research suggests that adopting the rhetoric and\nvalues of a partisan group may be a uniquely effective means\nof exerting persuasive influence in polarized political contexts\n(5), the persuasive influence of LLMs engaged in this behavior\n(6) remains unclear.\nAdditionally, while previous research has found evidence\nthat LLM-generated messages can influence people’s politi-\ncal attitudes (7), it remains unclear whether such messages\nare more persuasive than messages generated by relevant hu-\nman experts, such as political consultants. Answering this\nquestion has important implications: if the persuasiveness of\nLLM-generated messages rivals or exceeds those generated\nby human experts, this could portend widespread adoption\nof LLM-powered persuasion by established political parties\nand other such actors. This would present a significant shift\nfrom the current paradigm, where automated AI tools are\npredominantly employed by fringe or extremist groups who\nmay not have ready access to political communication experts\n(2). While previous research has found that LLMs could match\nhuman levels of persuasion on political issues, their human\nmessages were generated by non-experts via online crowd-\nsourcing platforms, which can be of poor quality (7, 8). By\ncontrast, this study uses messages manually collected from\nprofessional political consultants for our human baseline.\nHere, we use the largest publicly accessible LLM to date,\nGPT-4, to ask two related questions:\n1. Towhatextentdoesthealignmentofpartisanshipbetween\nLLMs and the audience enhance the persuasiveness of role-\nplaying LLMs compared to a misaligned LLM (RQ1a)\nor a non-role-playing LLM (RQ1b)?\n2. To what extent are partisanship-aligned, role-playing\nLLMs (RQ2a) and non-role-playing LLMs (RQ2b) more\npersuasive than human political persuasion experts?\nTaken together, these questions aim to explore the extent to\nwhich models employing advanced prompting techniques (e.g.,\nimpersonating political ingroups) might displace political mes-\nsaging experts by virtue of being more persuasive, potentially\ndisrupting the status quo of political campaigns and further\nincentivizing the use of AI-generated political persuasion.\nRecent research suggests that the most capable LLMs –\ntrained on public corpora of human-generated text – can en-\ncode nuanced and fine-grained information about the ideas,\nattitudes, and socio-cultural contexts that characterize human\nattitudes and identities (9). This has led to exploration of\nrole-playing: a prompting technique in which a model is in-\nstructed to assume the identity of a person or societal group\n(10, 11). This emergent practice has fostered novel means\nof engagement with LLMs, extending tone-static models into\nagents increasingly capable of effectively emulating diverse\nhuman experiences and perspectives (12, 13). Notably, role-\nplay techniques have improved the performance and reasoning\ncapabilities of LLMs across different benchmarks (13–15) and\nimproved the contextual relevance of outputs (16, 17). How-\never, in spite of growing popularity and academic interest,\ncurrent research on role-playing leaves its potential impacts in\nsignificant social contexts largely uninterrogated.\nIn the present work, we ask whether the ability of LLMs\nto credibly assume partisan political identities via role-play\nK.H. and L.I. conceptualized the research; K.H., L.I., B.T. and M.T. developed the methodology;\nK.H. and L.I. collected, analyzed, and visualized data; K.H. and L.I. wrote the paper; B.T. and M.T.\nprovided revisions; M.T. acquired funding. Both K.H. and L.I. contributed equally and have the right\nto list their name first on their CV.\nB.T. is co-founder of an organization that conducts public opinion research. The remaining authors\ndeclare no competing interests.\n*These authors contributed equally.\n1To whom correspondence should be addressed. E-mail: kobi.hackenburg@oii.ox.ac.uk or lu-\njain.ibrahim@oii.ox.ac.uk\n1 Hackenburg & Ibrahimet al.\ncould have implications for their persuasive potential. Long-\nstanding findings in social psychology — often referred to as\nthe “similarity-attraction effect” or the “similarity principle” —\nhave indicated that individuals are more likely to be persuaded\nby individuals who they perceive as similar to themselves (18–\n22). Likewise, empirical studies in a U.S. context have shown\nthat “re-framing” a partisan policy priority or a political\nagenda using beliefs and moral values commonly endorsed\nby one’s political party can enhance persuasive impact (5,\n23, 24). In the present work, we thus define partisan role-\nplaying as adoption of thelanguage and beliefs of a political\nparty (without the use of overt party cues) and hypothesize\nthat – when LLM and audience partisanship are aligned – it\ncould increase the persuasive impact of AI-generated political\nmessages through a combination of similarity attraction and\nmoral re-framing effects.\nWe extend existing research in two crucial ways. First, we\nextend the study of coordinated inauthentic behavior (CIB)\nand influence operations online. While existing literature ex-\ntensively documents the centrality of partisan role-play in\na number of deceptive tactics, including astroturfing (25),\nfalse flagoperations (26), andsock-puppetry (1), these studies\nare largely descriptive, listing examples of political identities\nadopted by inauthentic actors (27) and analyzing the substan-\ntive content of their messages (28). Therefore, even as the\nactual success of influence operations using these techniques\nis debated (29), a more fundamental question remains unan-\nswered: what are thepersuasive effects of partisan role-play?\nOur study therefore presents a specific and important step to-\nwards quantifying the potential influence of partisan role-play\nas a discrete aspect of CIB, particularly in polarized political\ncontexts.\nSecond, we broaden and expand the nascent literature on\nLLMs and political persuasion. Crucially, despite multiple\nstudies illustrating the significant impact of prompt design\non model outputs (30–32), recent research still employs basic\nprompts to instruct models to generate persuasive messages\n(2, 7, 33). By contrast, our work begins a critical exploration\ninto the potential impacts of more sophisticated prompt engi-\nneering techniques, like role-playing, on model persuasiveness.\nFurther, existing studies of LLM-induced attitude change at-\ntempt to persuade participants of all political beliefs towards\na singular viewpoint (2, 7, 33, 34), failing to consider the\npolitical context surrounding the selected issues when draw-\ning conclusions about persuasiveness in and across partisan\ngroups. Here we include both “for” and “against” stances for\neach issue, allowing us to examine the interplay between a\nparticipant’s initial issue stance, the partisan “identity” of the\nrole-playing LLM, and the “direction” of persuasion (“for” or\n“against”). Moreover, by examining highly polarized issues,\nwe aim to extend existing work towards a more contentious\nand high-impact domain, investigating how LLMs can induce\nattitude change on issues of high public awareness.\nResults\nIn this experiment, a large sample of U.S. citizens balanced on\nself-reported sex (male or female) and political party affiliation\n(Democrat or Republican) were shown a persuasive message\nauthored by either an LLM or a human expert for each of three\nissues. The particular message displayed to a given participant\nwas randomized. Each of the issue stances used are displayed\nin Table 1.\nAll reported estimates and P-values are based on linear\nmixed effects models. For more details on experimental design\nand models, please consult the Methods section. Average\nratings of issue stance alignment across all conditions can be\nfound in Supplementary MaterialsFig. S1.\nIn order to contextualize the effectiveness of each of our\ntreatment conditions, we first fit a model (not pre-registered)\nusing the control condition as the reference category. The\nresults, shown inFig. 1., illustrate that LLM-generated mes-\nsages consistently outperformed those of our human experts,\nsometimes by a margin of more than 6 percentage points on\na 100-point scale. Note that the effect sizes inFig. 1are all\nre-coded so that positive values equal attitude change towards\nthe treatment message.\nWe next report the results of our pre-registered analyses,\nwhich are shown inFig. 2.\nRole-playing\nRQ1(a) concerned the extent to which alignment between the\npartisanship of a role-playing LLM and the partisanship of its\naudience (“partisan alignment”) enhances persuasiveness com-\npared to situations where the role-playing AI’s partisanship\nexplicitly differs from that of its audience (“partisan misalign-\nment”). As shown inFig. 2, in aggregate across all issues, the\naverage persuasive impact of the partisan-aligned messages\ndid not differ significantly from that of the partisan-misaligned\nmessages, either in cases where participants were persuaded\nfor an issue stance (1.71 percentage points, P = 0.112) or\nagainst an issue stance (-0.58, P = 0.600).\nThe issue-level results reveal some instances of an alignment\neffect, however. On rigged elections, the estimated effect of\npartisan alignment did not significantly differ from that of\npartisan misalignment when participants read messages argu-\ning that U.S. electionsare notrigged (0.55 percentage points,\nP = 0.693), but was significantly larger when the messages\nargued that U.S. electionsare rigged (4.26, P < 0.001). On\ndeportations, we found no significant difference between parti-\nTable 1. Issue stances used to produce all treatment stimuli.\nVaccine Mandates Rigged Elections Immigration\nFOR For the most part, vaccine mandates are\na good response to global pandemics\nFor the most part, elections in the U.S.\nare rigged\nFor the most part, deportations are a\ngood solution to illegal immigration in\nthe U.S.\nAGAINST For the most part, vaccine mandates are\nnot a good response to global pandemics\nFor the most part, elections in the U.S.\nare not rigged\nFor the most part, deportations are not\na good solution to illegal immigration in\nthe U.S.\n2 Hackenburg & Ibrahimet al.\nVaccine Mandates Deportations Rigged Elections All Combined\nFOR\nAGAINST\nMisaligned Role-play\nAligned Role-play\nNo Role-play\nHuman Experts\nPersuasive Impact \nin Percentage \nPoints (95% CI)\n0 3 6 9 0 3 6 9 0 3 6 9 3 6 90\nIssue Stance\n2.72\n4.21\n4.21\n3.87\n0.21\n6.47\n2.45\n6.15 6.36\n7.76\n7.41\n7.49\n8.76\n7.56\n7.28\n3.88\n4.49\n2.8\n7.05\n3.66\n4.48\n2.58\n2.05 4.85\n4.27\n6.12\n3.79\n6.56\n4.86\n5.32\n2.310.34\nFig. 1. Expected persuasive impact of messages generated via (mis)aligned role-play, no role-play, and human experts with respect to a control\ngroup, disaggregated across issue and stance. Coefficients represent estimated persuasive impact of messages in each condition, compared to a\ncontrol group. For misaligned and aligned role-play, the estimates are aggregated across (LLM and audience) partisanship. Note that the effect\nsizes are all re-coded so that positive values equal attitude change towards the treatment message.\nsan alignment and misalignment whether the messages were\nagainst (1.41, P = 0.316) orfor deportations as a solution\nto illegal immigration (1.21, P = 0.371). On vaccine man-\ndates, partisan alignment was significantly more persuasive\nthan misalignment when messages wereagainst vaccine man-\ndates (-3.69, P < 0.001), but not when they werefor vaccine\nmandates (-0.34, P = 0.828). All significant tests above are\nrobust to a Bonferroni correction for multiple comparisons (P\n< 0.008).\nRQ1(b) concerned the extent to which a role-playing,\npartisanship-aligned LLM is more persuasive than a non-role-\nplaying LLM. As shown inFig. 2, in aggregate across all issues,\nwe did not observe that the partisan-aligned, role-playing LLM\nheld a significant persuasive advantage over a non-role-playing\nLLM, either in cases where participants were persuadedfor\nan issue stance (1.23 percentage points, P = 0.258) oragainst\nan issue stance (1.28, P = 0.244).\nAt the issue-level, in only one case was aligned role-play\nsignificantly more persuasive when compared to a non-role-\nplaying model: on rigged elections, a significant advantage\nwas observed when messages argued that U.S. electionsare\nrigged (2.55 percentage points, P = 0.024); however, this sig-\nnificance is not robust to a Bonferroni correction for multiple\ncomparisons (P > 0.008). Furthermore, this effect was not\nsignificant at the .05 level when participants read messages\narguing that U.S. electionsare notrigged (2.44, P = 0.086)\n(though the effect size is similar). On deportations, the per-\nsuasiveness of a partisan-aligned role-playing LLM did not\nsignificantly exceed that of a non-role-playing LLM, regardless\nof whether the messages wereagainst (1.06, P = 0.294) orfor\ndeportations as a solution to illegal immigration (1.48, P =\n0.307). Similarly, for vaccine mandates, the persuasiveness\nof a partisan-aligned role-playing LLM did not significantly\nexceed that of a non-role-playing LLM regardless of whether\nthe messages wereagainst (0.33, P = 0.841) orfor vaccine\nmandates as a solution to global pandemics (-0.34, P = 0.835).\nHuman Experts\nRQ2(a) concerned the extent to which messages generated by\na partisanship-aligned, role-playing LLM are more persuasive\nthan messages written by human political communication ex-\nperts. As shown inFig. 2, in aggregate across all issues, there\nwas evidence to suggest that this was the case: a partisan-\naligned, role-playing LLM held a significant persuasive advan-\ntage over the human experts in cases where participants were\npersuaded for an issue stance (4.24 percentage points, P <\n0.001), but not when they were persuadedagainst an issue\nstance (-1.08, P = 0.325).\nWe next examine the issue-level results. On rigged elec-\ntions, the estimated persuasive effect of a partisan-aligned,\nrole-playing LLM was not significantly larger than the persua-\nsive effect of a human expert when participants read messages\narguing that U.S. electionsare not rigged (1.58 percentage\npoints, P = 0.118), but was significantly larger when the mes-\nsages argued that U.S. electionsare rigged (6.69, P < 0.001).\nSimilarly, on deportations, the estimated persuasive effect\nof partisan-aligned, role-playing LLMs was not significantly\ndifferent from the persuasive effect of human experts when\nparticipants were shown messages arguingagainst deporta-\ntions (1.13, P = 0.181), but was significantly larger when\nparticipants were shown messages arguingfor deportations\n(4.88, P < 0.001). Finally, on vaccine mandates, the estimated\npersuasive effect of a partisan-aligned, role-playing LLM was\nsignificantly larger than the persuasive effect of human experts\nwhen participants were shown messages arguingagainst (-5.94,\nP < 0.001) but notfor (1.15, P = 0.487) vaccine mandates as\na good response to global pandemics. Notably, all significant\ntests above are robust to a Bonferroni correction for multiple\ncomparisons (P < 0.008).\nIn summary, in this section we find evidence that, for mes-\n3 Hackenburg & Ibrahimet al.\nPersuasive Impact \nin Percentage Points\n(95% CI)\nAligned Role-play\nvs. \nMisaligned Role-play\nAligned Role-play\nvs. \nNo Role-play\nAligned Role-play\nvs. \nHuman Experts\nFig. 2.In aggregate and across most issues, partisanship-aligned role-play conferred little persuasive advantage compared to misaligned role-play,\nno role-play, or human experts. The first row displays the estimated persuasive impact of a message aiming to persuade participants against\na given issue stance; the second displays the estimated persuasive impact of messages aiming to persuade participants for a given issue stance.\nCoefficients represent the difference in participants’ average support for an issue between the indicated conditions; thus, a statistically significant\nnegative coefficient in the against row or a statistically significant positive coefficient in the for row is evidence of a partisanship (mis)alignment\neffect. Average ratings of issue stance alignment across all conditions can be found in Supplementary MaterialsFig. S1.\nsaging associated with the U.S. political right – i.e., messages\narguing that U.S. elections are rigged, deportations are desir-\nable, and vaccine mandates are undesirable – a role-playing\nLLM significantly outperforms our human experts in terms\nof persuasive impact. However, for messaging that is more\nassociated with the U.S. political left – i.e., messages arguing\nthat U.S. elections are not rigged, deportations are undesir-\nable, and vaccine mandates are desirable – we find no such\npersuasive advantage; a role-playing LLM and our human\nexperts were approximately similarly persuasive. We revisit\nand consider reasons for this asymmetry in the Discussion\nsection of this paper. Notably, in a supplementary analysis\nwe also find that a role-playing LLM was highly effective at\npersuading Democrats on issues they would normally oppose\n(see Supplementary Materials Section 6).\nRQ2(b) concerned the extent to which a non-role-playing\nLLM is more persuasive than a human political communi-\ncation expert (note: this sub-research question was not pre-\nregistered). In aggregate across all issues, there was evidence\nto suggest that a non-role-playing LLM held a significant\npersuasive advantage over a human expert in cases where\nparticipants were persuadedfor an issue stance (3.01 percent-\nage points, P = 0.006) andagainst an issue stance (-2.35, P\n= 0.032). This result supplements the findings described in\nthe previous paragraph regarding the role-playing LLM, and\nsuggests that the LLM-generated messagesin generalwere as\npersuasive, or more persuasive, than those generated by our\nhuman experts.\nWe next examine the issue-level results. On rigged elections,\nthe estimated persuasive effect of a non-role-playing LLM\nwas not significantly different from the persuasive effect of a\nhuman expert when participants read messages arguing that\nU.S. elections are not rigged (-0.86 percentage points, P =\n0.406), but was significantly larger when the messages argued\nthat U.S. electionsare rigged (4.14, P = 0.002). Similarly,\non deportations, the estimated persuasive effect of a non-\nrole-playing LLMs was not significantly different from the\npersuasive effect of human experts when participants were\nshownmessages arguingagainst deportations(0.07, P=0.948),\nbut was significantly larger when participants were shown\nmessagesarguing for deportations(3.39, P=0.02). Onvaccine\nmandates, the estimated persuasive effect of non-role-playing\nLLMs was significantly larger than the persuasive effect of\nhumanexpertswhenparticipantswereshownmessagesarguing\nagainst (-6.27, P < 0.001) but notfor (1.49, P = 0.354) vaccine\nmandates as a good response to global pandemics. The above\nsignificant tests for rigged elections and vaccine mandates, but\nnot for deportations, are robust to a Bonferroni correction (P\n4 Hackenburg & Ibrahimet al.\n< 0.008).\nDiscussion\nThis study presents a first step towards quantifying the per-\nsuasive influence of partisan role-play with LLMs. Through a\nlarge-scale, pre-registered human-subjects experiment, we find\nthat while messages produced by a role-playing GPT-4 are\nbroadly persuasive, they are not significantly more persuasive\nthan messages generated by a non-role-playing GPT-4. Our\nfindings therefore suggest that the effectiveness of partisan\nrole-play may be limited when broadly deployed using cur-\nrent models. However, we also find that LLMs can rival and\neven exceed the persuasiveness of human experts, which may\nportend a shift in the political persuasion landscape.\nWe offer two possible model-side explanation for the limited\nefficacy of role-playing as compared to the non-role-playing\nbaseline. First, GPT-4 could be misaligned with the opinion\ndistributions of partisan groups in the U.S., and thus fail\nto encode their true beliefs and values accurately on some\nissues (35). This possibility is evidenced by the fact that\nparticipants in our study were only able to accurately dis-\ncern the partisanship of a role-playing LLM 46% of the time,\nsuggesting that GPT-4 was rarely perceived as a member of\nthe intended political group (see Supplementary Materials\nSection 2.1). Second, research has shown that aligning LLMs\nwith reinforcement learning based on human feedback (RLHF)\ncan push models to converge to the most common view of a\ngiven group, collapsing the diversity of opinions held by, for\nexample, different Republicans, into a single modal response\n(35). This potential oversimplification of the range of opinions\nheld within a political party may result in GPT-4 role-playing\nin off-putting or stereotypical – and thus unpersuasive – ways.\nOur finding that LLMs can exceed the persuasiveness of\nhuman experts is characterized by a notable asymmetry: we\nonly observed this persuasive advantage on right-leaning mes-\nsaging. One obvious potential explanation for this asymmetry\nis that, because our human experts were all left-leaning (see\nMethods), they put less effort into writing the right-leaning\nmessages – which ultimately rendered them less persuasive\ncompared to both the left-leaning messages they wrote as\nwell as to the messages written by GPT-4. We probed this\npossibility by examining the length of the relevant messages\nbut found that the human-written right-leaning messages were\nof a similar length as both their left-leaning messages and the\ncorresponding GPT-4 messages (see Supplementary Materials\nTable S1). Therefore, it does not appear obvious that the\nexperts put in less effort for the right-leaning messages. It of\ncourse remains possible that they were simply worse at writing\npersuasive messages which contradicted their personal beliefs\n– thus, right-leaning experts might not have been similarly\noutperformed on right-leaning messaging by GPT-4. Never-\ntheless, we reiterate that even on the left-leaning messaging,\nthe messages written by GPT-4 rivalled the persuasiveness\nof those from the (left-leaning) experts – a notable finding in\nand of itself.\nWith that in mind, we offer three possible explanations for\nthe competitive and/or superior performance of LLM messages\ncompared to those of the human experts. First, the format of\nan 8-12 sentence message may not be one commonly employed\nin practice by professionals, who may instead be more accus-\ntomed to working with, for example, brief political slogans,\nfull-length speeches, or televised debate rebuttals. Second, in\npractice experts may a) collaborate in groups to create politi-\ncal persuasion materials or b) spend weeks or months on their\ndevelopment, meaning that the messages they developed for\nthis experiment may not accurately reflect their true potential.\nAn important final explanation for these results, however, is\nthat LLMs can indeed rival or even outperform political com-\nmunication experts on this type of persuasion exercise, which\nwould potentially portend the widespread adoption of genera-\ntive language models by formal political persuasion campaigns.\nWe consider adjudicating between these different explanations\nto be a priority for future research.\nAnother notable finding from our experiment is the propor-\ntion of participants who reported the messages as AI-generated.\nEarly in 2023 – using the same question, experimental method-\nology, and crowd sourcing platform – Bai et al. reported\nthat only 5% of participants suspected that messages were\nAI-authored (7); in mid-2023, a study by Hackenberg et al. re-\nported a figure of approximately 15% (36). The present work,\nusing data collected during late 2023, finds that participants\nidentified messages as AI-generated more than 22% of the\ntime. While this appears to mark a stark upwards trend in\nthe identification of AI-generated messages, we contextualize\nthese findings by noting that participants who read only hu-\nman messages also reported that messages were AI-generated\nexactly 25% of the time, making “AI language model” the most\npopular suspected author for both human and AI-generated\nmessages. We therefore suggest that rather than becoming\nbetter at detecting AI-generated messages, participants are\nadjusting to an environment where, unable to distinguish be-\ntween human and AI-written content, they are necessarily\nsuspicious of the origin ofall text they encounter. As with\nother AI domains, such as the creation of Generative Adversar-\nial Network (GAN) faces, increased awareness of the role and\npower of AI makes distinguishing between human-generated\nand artificially-generated stimuli more difficult and can erode\nsocial trust (37).\nWe draw attention to two main limitations of our study\ndesign. A first limitation is the closed-source nature of GPT-\n4. Researchers have justifiably expressed concerns about the\nchallenges of replicating studies that use closed-source LLMs.\nWhile we acknowledge the importance of reproducibility, we\narguethatthewidespreaduseofproprietarymodelslikeGPT-4\nnecessitatesanexaminationoftheircapabilities. Wearguethat\nthere is an urgent need for research exploring both proprietary\nand open-source systems. Second, research shows that LLMs\nare sensitive to variations in the input prompt. Thus, the\nextent to which even minor changes in the input prompts or\nsystem messages might affect the messages generated remains\nuncertain.\nWe propose two additional directions for future research.\nAs the evaluation of LLMs garners technical and regulatory\nattention, we highlight the lack of human-interaction evalu-\nations of LLMs. A recent study revealed that only 9.1% of\ncurrently available LLM evaluations of ethical and social risks\nempirically examine human-AI interactions (38). As LLMs op-\nerate within complex sociotechnical ecosystems, we argue that\nunderstanding their potential harms and risks in the context of\nthe human behaviors they influence is essential. Thus, further\nevaluations of LLMs in this area should utilize approaches and\nmethods from behavioral psychology and human-computer\n5 Hackenburg & Ibrahimet al.\ninteraction to expand understanding of the actual impacts of\nAI-generated content in the political public sphere.\nSecondly, while our study examined a particular prompt-\ning strategy, further work is needed to examine the array of\nprompting strategies utilized for varied aims and their effects\non the outputs of LLMs. Moreover, even within a specific\nprompting strategy, the order of individual words and cos-\nmetic changes to semantically similar phrases can still have\na dramatic outcome on the effectiveness of the prompt. Fu-\nture research should develop approaches allowing for the trial\nand testing of numerous prompt variations, allowing for more\nrobust and accurate measurements.\nThis work represents an important step towards understand-\ning the persuasive capabilities of LLMs, suggesting that while\nthe effectiveness of partisan role-play may be limited in most\ncases, even non-role-playing LLMs can match or exceed the\npersuasiveness of human political communication experts. As\ncountries around the world approach democratic elections and\nconcerns over the political influence of LLMs mount, empirical,\nsociotechnical evaluations will remain essential to informing\nsensible policies and interventions regarding the political im-\npact of LLMs. This empirical research contributes insight into\nthe potential persuasive power of existing models.\nMaterials and Methods\nThis study was approved by the Research Ethics Committee at\nRoyal Holloway, University of London [application ID: 3699]. All\ncode and replication materials are publicly available in a GitHub\nrepository at this link.\nSample Participants were recruited using the online crowd-\nsourcing platforms Prolific and Lucid Theorem. Participants were\nscreened such that all were located in the U.S., spoke English as their\nfirst language, were over the age of 18, and had completed at least\na high-school education. The full sample was balanced with respect\nto sex and partisan affiliation (Democrat or Republican). Data\nfrom participants who failed two pre-treatment attention checks\nwere excluded from the analysis. List-wise deletion was employed\nfor any missing or incomplete data. In total, 66 participants who\npassed the initial pre-treatment attention checks dropped out before\nproviding a dependent variable response, resulting in an attrition\nrate of 1.4%; importantly, however, we found no evidence that these\ndropouts were differential across condition and treatment issue (see\nSupplementary Materials Section 8).\nThis resulted in a final sample size of 4,955 participants (2,501\nRepublicans and 2,454 Democrats; 3,707 from Prolific, 1,248 from\nLucid). For a description of the power analysis conducted and a\ndetailed description of the sample composition along demographic\ntraits measured in this study, consult the Supplementary Materials\nSection 5.\nExperimental Design The study was conducted on Qualtrics and\nutilized a nine-condition, between-subjects design. Participants\nin each condition were exposed to a single persuasive message for\neach of three polarized issues, for a total of three messages per\nparticipant. Random assignment to an experimental condition was\ndone at the participant level, meaning that the issue stance (either\nFOR or AGAINST) and message author (a role-playing LLM, a non-\nrole-playing LLM, or a human expert) remained constant for each\nparticipant across each of the three issues. The order of the issues\nwas randomized. Upon beginning the experiment, each participant\nwas randomly assigned to one of the following nine experimental\nconditions:\n1. Control: participant exposed to no messages and proceeds\ndirectly to the dependent variable measure.\n2. Human (FOR issue): participant exposed to messages gen-\nerated by an expert human author designed to persuade them\nin favor of each issue.\n3. Human (AGAINST issue): participantexposedtomessages\ngenerated by an expert human author designed to persuade\nthem against each issue.\n4. LLM No role-playing (FOR issue): participant exposed\nto messages generated by a non-role-playing GPT-4 designed\nto persuade them in favor of each issue.\n5. LLM No role-playing (AGAINST issue): participant\nexposed to messages generated by a non-role-playing GPT-4\ndesigned to persuade them against each issue.\n6. LLM Role-playing as DEM (FOR issue): participant\nexposed to messages generated by GPT-4 designed to persuade\nthem in favor of each issue. GPT-4 is instructed to adopt the\nlanguage and beliefs of a partisan American Democrat.\n7. LLM Role-playing as DEM (AGAINST issue): partic-\nipant exposed to messages generated by GPT-4 designed to\npersuade them against each issue. GPT-4 is instructed to adopt\nthe language and beliefs of a partisan American Democrat.\n8. LLM Role-playing as REPUB (FOR issue): participant\nexposed to messages generated by GPT-4 designed to persuade\nthem in favor of each issue. GPT-4 is instructed to adopt the\nlanguage and beliefs of a partisan American Republican.\n9. LLM Role-playing as REPUB (AGAINST issue): par-\nticipant exposed to messages generated by GPT-4 designed to\npersuade them against each issue. GPT-4 is instructed to adopt\nthe language and beliefs of a partisan American Republican.\nAfter reading a message, participants reported the dependent\nvariable measure by answering a battery of four questions assessing\ntheir support for the issue stance and, for participants not in the\ncontrol condition, one question on the perceived partisanship of the\nmessage author. After answering these questions, all participants\n(except for those in the control condition) concluded the experiment\nby answering a question on who they think was most likely the\nauthor of the messages they were exposed to. For exact question\nwording, see Supplementary Materials Sections 4 & 5.\nExperimental Materials Messages in this study were generated\nby a role-playing LLM, a non-role-playing LLM, or a human expert.\nFor the LLM conditions (role-playing LLM and non-role-playing\nLLM) LLM messages were generated via OpenAI’s API using the\nGPT-4 model. The system messages and prompts used to generate\nthe messages for the role-playing LLM and non-role-playing LLM\nconditions are being withheld in the pre-print version of this work.\nAs previous research has found that party cues are persuasive (39),\nwe instruct our model to refrain from using overt party cues in all\nconditions.\nTo generate the human expert messages, professional political\nconsultants were manually surveyed from U.S. political consulting\nfirms. Consultants were recruited only if they currently work or\nhave worked in political messaging strategy. The consultants were\ngiven a prompt identical to the one used to generate the non-role-\nplaying LLM messages via GPT-4: Please try your best to write a\nmessage of less than 200 words that can persuade a reader to agree\nwith the following idea: [issue stance]. We recruited consultants\nuntil we reached three human expert messages per issue-stance\ncombination, for a total of 18 messages and 8 consultants. While\nan exploratory survey revealed that most consultants only agree to\ngenerate materials under the condition of anonymity, we report non-\nidentifiabledatafromtheconsultantsincludingage(avg: 36), gender\n(43% male, 57% female), and party affiliation (100% Democrat). For\nall human and LLM-generated stimuli we used for each condition,\nplease consult our project repository.\nThree messages were generated for each condition and for each\nissue stance, resulting in 81 total messages (3 messages x 3 issues x\n9 conditions).\nStatistical Analysis Due to the nested nature of the data, linear\nmultilevel modeling was used to fit 24 linear mixed-effects models\nwith random effects to capture both within-subject and between-\nsubject variations in the outcome variable, post-treatment issue\nsupport. For each of the four sub-research questions, one “FOR\nIssue” model and one “AGAINST Issue” was fitted for aggregate\nanalysis, and three “FOR Issue” models and three “AGAINST\nIssue” models were fitted for the issue-level analysis. The “FOR\n6 Hackenburg & Ibrahimet al.\nIssue” models contained the conditions in which participants were\nexposed to messages in support of the political issues. Similarly, the\n“AGAINSTIssue”modelcontainedtheconditionswhereparticipants\nwere exposed to messages opposing the political issues. The results\nof the pre-registered analysis are visualized inFig. 2.\nA binary variable “aligned” was created to capture whether\nor not participant partisanship matched the partisanship of the\nrole-playing LLM. “Aligned” took a value of 1 when participant\npartisanship and LLM partisanship were aligned and a value of 0\notherwise. A categorical variable “condition” was created to capture\nwhether the treatment condition was a “human”, “no-role-play”,\n“aligned role-play”, or “misaligned role-play”.\nRQ1(a) investigated the extent to which the alignment of par-\ntisanship between role-playing LLMs and their audience enhances\npersuasiveness. Eight linear mixed-effects models containing the\nfour LLM role-playing conditions were fitted. The two aggregate\nmodels included the binary variable “aligned” and controlled for\nparticipant party affiliation and the issue stance. The six issue-\nlevel models included the binary variable “aligned” and controlled\nfor participant party affiliation. The coefficient on “aligned” was\nthe key quantity of interest and corresponded to the expected av-\nerage attitude change for a participant in a partisanship-aligned\nvs. partisanship-misaligned scenario. A negative coefficient in the\n“AGAINST Issue” model or a positive coefficient in the “FOR Issue”\nmodel was evidence of a partisanship (mis)alignment effect.\nRQ1(b) investigated whether partisanship-aligned, role-playing\nLLMs are more persuasive than non-role-playing LLMs. Eight linear\nmixed-effects models containing all six LLM conditions were fitted.\nThe two aggregate models included the dummy-coded “condition”\nvariable with the “no-role-play” condition as the reference category\nand controlled for participant party affiliation and the issue stance.\nThe six issue-level models included the condition dummy variable\nwith the “no-role-play” condition as the reference category and\ncontrolled for participant party affiliation. The coefficient on the\n“aligned role-play” condition dummy variable was the key quantity\nof interest and corresponded to the expected average attitude change\nfor a participant in the “aligned role-play” condition vs. the “no-\nrole-play” condition. A negative coefficient in the “AGAINST\nIssue” model or a positive coefficient in the “FOR Issue” model was\nevidence of a partisanship (mis)alignment effect.\nRQ2(a) investigated whether partisanship-aligned, role-playing\nLLMs are more persuasive than human experts.RQ2(b) inves-\ntigated whether non-role-playing LLMs are more persuasive than\nhuman experts. The models forRQ2(a) (the coefficient on the\n“aligned role-play” condition dummy was the key quantity of inter-\nest) andRQ2(b) (the coefficient on the “no-role-play” condition\nwas the key quantity of interest) and their interpretation were iden-\ntical to those fromRQ1(b) but the “no-role-play” condition was\nreplaced with the “human” condition as the reference category.\nFor Fig. 1, eight linear mixed-effects models containing all\nconditions were fit with the “control” condition as the reference\ncategory. To facilitate a more intuitive comparison across conditions,\nthe outcome variable was adjusted to reflect the absolute value of the\nestimated persuasive impact (calculated as the difference between\nthe reported post-treatment issue support and the average post-\ntreatment issue support for the issue and party group in the control\ncondition).\nACKNOWLEDGMENTS. Manos Tsakiris and this study were\nsupported by a NOMIS Foundation Grant for the Centre for the\nPolitics of Feelings. The funders had no role in study design, data\ncollection and analysis, decision to publish or preparation of the\nmanuscript.\n1. DFreelon, etal., Blacktrollsmatter: Racialandideologicalasymmetries\nin social media disinformation. Soc. Sci. Comput. Rev. 40, 560–578\n(2022).\n2. B Buchanan, A Lohn, M Musser, K Sedova, Truth, lies, and automation\nhow language models could change disinformation x (2021).\n3. J Goldstein, G Sastry, The coming age of ai-powered propaganda | for-\neign affairs. Foreign Aff. (2023).\n4. JA Goldstein, et al., Generative language models and automated influ-\nence operations: Emerging threats and potential mitigations. (2023).\n5. M Feinberg, R Willer, Moral reframing: A technique for effective and\npersuasive communication across political divides.Soc. Pers. Psychol.\nCompass 13 (2019).\n6. G Simmons, Moral mimicry: Large language models produce moral ra-\ntionalizations tailored to political identity. (2022).\n7. H Bai, JG Voelkel, johannes C. Eichstaedt, R Willer, Artificial intelli-\ngence can persuade humans on political issues.Preprint (2023).\n8. E Karinshak, JT Hancock, SX Liu, JS Park, Working with ai to per-\nsuade: Examining a large language model’s ability to generate pro-\nvaccination messages. (2023).\n9. LP Argyle, et al., Out of one, many: Using language models to simulate\nhuman samples. Polit. Analysispp. 1–15 (2023).\n10. M Shanahan, K McDonell, L Reynolds, Role play with large language\nmodels. Nature 623, 493 (2023).\n11. Y Shao, L Li, J Dai, X Qiu, Character-llm: A trainable agent for role-\nplaying. (year?).\n12. G Jiang, et al., Evaluating and inducing personality in pre-trained lan-\nguage models. (year?).\n13. ZM Wang, et al., Rolellm: Benchmarking, eliciting, and enhancing role-\nplaying abilities of large language models. (2023).\n14. A Kong, et al., Better zero-shot reasoning with role-play prompting.\n(year?).\n15. L Salewski, S Alaniz, I Rio-Torto, E Schulz, Z Akata, In-context imper-\nsonation reveals large language models’ strengths and biases. (year?).\n16. J Jeon, S Lee, Large language models in education: A focus on the\ncomplementary relationship between human teachers and chatgpt.Educ.\nInf. Technol. (2023).\n17. N Wu, M Gong, L Shou, S Liang, D Jiang, Large language models are\ndiverse role-players for summarization evaluation. (2023).\n18. JN Bailenson, N Yee, Digital chameleons: Automatic assimilation of\nnonverbal gestures in immersive virtual environments.Psychol. Sci. 16,\n814–819 (2005).\n19. JM Burger, N Messian, S Patel, AD Prado, C Anderson, What\na coincidence! the effects of incidental similarity on compliance.\nhttp://dx.doi.org/10.1177/0146167203258838 30, 35–43 (2004).\n20. RB Cialdini, Influence: Science and practice (5th edition). p. 272\n(2009).\n21. H Giles, DM Taylor, R Bourhis, Towards a theory of interpersonal ac-\ncommodation through language: Some canadian data. Lang. Soc. 2,\n177–192 (1973).\n22. RE Guadagno, RB Cialdini, Persuade him by email, but see her in per-\nson: Online persuasion revisited.Comput. Hum. Behav. 23, 999–1015\n(2007).\n23. M Feinberg, R Willer, From gulf to bridge: When do moral arguments\nfacilitate political influence? Pers. Soc. Psychol. Bull. 41, 1665–1681\n(2015).\n24. JG Voelkel, M Feinberg, Morally reframed arguments can affect support\nfor political candidates.Soc. Psychol. Pers. Sci. 9, 917–924 (2018).\n25. FB Keller, D Schoch, S Stier, JH Yang, Political astroturfing on twitter:\nHow to coordinate a disinformation campaign. Polit. Commun. 37,\n256–280 (2020).\n26. K Starbird, A Arif, T Wilson, Disinformation as collaborative work:\nSurfacing the participatory nature of strategic information operations.\n(2019).\n27. PN Howard, J Kelly, GC François, The ira, social media and political\npolarization in the united states, 2012-2018. (2019).\n28. R Diresta, et al., The tactics tropes of the internet research agency.\n(2019).\n29. G Eady, et al., Exposure to the russian internet research agency foreign\ninfluence campaign on twitter in the 2016 us election and its relationship\nto attitudes and voting behavior.Nat. Commun. 2023 14:114, 1–11\n(2023).\n30. L Reynolds, K Mcdonell, Prompt programming for large language mod-\nels: Beyond the few-shot paradigm.Conf. on Hum. Factors Comput.\nSyst. - Proc. (2021).\n31. J Wei, et al., Chain-of-thought prompting elicits reasoning in large lan-\nguage models. (2022).\n32. D Zhou, et al., Least-to-most prompting enables complex reasoning in\nlarge language models. (2022).\n33. JA Goldstein, J Chao, S Grossman, A Stamos, M Tomz, Can ai write\npersuasive propaganda? (2023).\n34. S Kreps, RM McCain, M Brundage, All the news that’s fit to fabricate:\nAi-generated text as a tool of media misinformation.J. Exp. Polit. Sci.\n9, 104–117 (2022).\n35. S Santurkar, et al., Whose opinions do language models reflect?\n(JMLR.org), (2023).\n36. K Hackenburg, H Margetts, Evaluating the persuasive influence of po-\nlitical microtargeting with large language models. (2023).\n37. R Tucciarelli, N Vehar, S Chandaria, M Tsakiris, On the realness of peo-\nple who do not exist: The social processing of artificial faces.iScience\n25 (2022).\n38. L Weidinger, et al., Sociotechnical safety evaluation of generative ai\nsystems. (2023).\n39. BM Tappin, Estimating the between-issue variation in party elite cue\neffects. Public Opin. Q. 86, 862–885 (2023).\n7 Hackenburg & Ibrahimet al.",
  "topic": "Persuasion",
  "concepts": [
    {
      "name": "Persuasion",
      "score": 0.9745054244995117
    },
    {
      "name": "Politics",
      "score": 0.7788400650024414
    },
    {
      "name": "Persuasive communication",
      "score": 0.6306797862052917
    },
    {
      "name": "Democracy",
      "score": 0.5759965777397156
    },
    {
      "name": "Social psychology",
      "score": 0.49935293197631836
    },
    {
      "name": "Psychology",
      "score": 0.4530283808708191
    },
    {
      "name": "Political science",
      "score": 0.4358445405960083
    },
    {
      "name": "Law",
      "score": 0.09451103210449219
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I193374721",
      "name": "Internet Society",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I184558857",
      "name": "Royal Holloway University of London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802488147",
      "name": "School of Advanced Study",
      "country": "GB"
    }
  ],
  "cited_by": 15
}