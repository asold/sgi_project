{
  "title": "Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !",
  "url": "https://openalex.org/W3098123193",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1990127226",
      "name": "Wen Xiao",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2099183527",
      "name": "Patrick Huber",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A1132515649",
      "name": "Giuseppe Carenini",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995259046",
    "https://openalex.org/W3099609223",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2970263339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W3006801027",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W2963607157",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2795439700",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3171719338",
    "https://openalex.org/W2045738181",
    "https://openalex.org/W2889446948",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W1894075015",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2251803607",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3008282111",
    "https://openalex.org/W2138909885",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W4300009529",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2565187826",
    "https://openalex.org/W2970008578"
  ],
  "abstract": "The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed “Synthesizer” framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
  "full_text": "Proceedings of the First Workshop on Computational Approaches to Discourse, pages 124–134\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n124\nDo We Really Need That Many Parameters In Transformer For\nExtractive Summarization? Discourse Can Help !\nWen Xiao, Patrick Huber, Giuseppe Carenini\nDepartment of Computer Science\nUniversity of British Columbia\nVancouver, BC, Canada, V6T 1Z4\n{xiaowen3, huberpat, carenini}@cs.ubc.ca\nAbstract\nThe multi-head self-attention of popular trans-\nformer models is widely used within Natural\nLanguage Processing (NLP), including for the\ntask of extractive summarization. With the\ngoal of analyzing and pruning the parameter-\nheavy self-attention mechanism, there are mul-\ntiple approaches proposing more parameter-\nlight self-attention alternatives. In this\npaper, we present a novel parameter-lean\nself-attention mechanism using discourse pri-\nors. Our new tree self-attention is based\non document-level discourse information, ex-\ntending the recently proposed “Synthesizer”\nframework with another lightweight alterna-\ntive. We show empirical results that our tree\nself-attention approach achieves competitive\nROUGE-scores on the task of extractive sum-\nmarization. When compared to the original\nsingle-head transformer model, the tree atten-\ntion approach reaches similar performance on\nboth, EDU and sentence level, despite the sig-\nniﬁcant reduction of parameters in the atten-\ntion component. We further signiﬁcantly out-\nperform the 8-head transformer model on sen-\ntence level when applying a more balanced\nhyper-parameter setting, requiring an order of\nmagnitude less parameters1.\n1 Introduction\nThe task of extractive summarization aims to gen-\nerate summaries for multi-sentential documents\nby selecting a subset of text units in the source\ndocument that most accurately cover the authors\ncommunicative goal (as shown in red in Figure 1).\nAs such, extractive summarization has been a long\nstanding research question with direct practical im-\nplications. The main objective for the task is to\ndetermine whether a given text unit in the docu-\nment is important, generally implied by multiple\n1Our code can be found here - http://www.cs.\nubc.ca/cs-research/lci/research-groups/\nnatural-language-processing/\nFigure 1: News document (4 sentences / 6 EDUs), with\nboth its discourse tree (top) and possible extractive sum-\nmaries at the sentence/EDU level (extracted sentences\nand EDUs shown in boxes and red respectively).\nfactors, such as position, stance, semantic meaning\nand discourse.\nMarcu (1999) already showed early on that dis-\ncourse information, as deﬁned in the Rhetorical\nStructure Theory (RST) (Mann and Thompson,\n1988), are a good indicator of the importance of\na text unit in the given context. The RST frame-\nwork, one of the most elaborate and widely used\ntheories of discourse, represents a coherent doc-\nument (a discourse) as a constituency tree. The\nleaves are thereby called Elementary Discourse\nUnits (EDUs), clause-like sentence fragments cor-\nresponding to minimal units of content (i.e. propo-\nsitions). Internal tree nodes, comprising document\nsub-trees, represent hierarchically compound text\nspans (or constituents). An additional nuclearity\nattribute is assigned to each child, representing the\nimportance of the subtree in the local constituent,\ni.e. the ’Nucleus’ child plays a more important role\nthan the ’Satellite’ child in the parent’s relation. Al-\nternatively, if both children are equally important,\n125\nboth are represented as Nuclei.\nWhile other popular theories of discourse exist\n(most notably PDTB (Prasad et al., 2008)), RST\nalong with its human-annotated RST-DT treebank\n(Carlson et al., 2002) have been leveraged in the\npast to improve extractive summarizations, with ei-\nther unsupervised (Hirao et al., 2013; Kikuchi et al.,\n2014), or supervised (Xu et al., 2020) methods.\nIn this paper, we explore a novel, equally impor-\ntant application for discourse information in extrac-\ntive summarization, namely to reduce the number\nof parameters. Instead of exploiting discourse trees\nas an additional source of information on top of\nneural models, we use the information as a prior to\nreduce the number of parameters of existing neural\nmodels. This is critical not only to reduce the risk\nof over-ﬁtting but also to create smaller models that\nare easier to interpret and deploy.\nNot surprisingly, reducing the number of param-\neters has become increasingly important in the last\nyears, due to the deep-learning revolution. Gen-\nerally speaking, the objective of reducing neural\nnetwork parameters involves addressing two cen-\ntral questions: (1) What do these models really\nlearn? Such that better priors can be provided and\nless parameters are required and (2) Are all the\nmodel parameters necessary? To identify which\nparameters can be safely removed.\nRecently, researchers have explored these ques-\ntions especially in the context of transformer mod-\nels. With respect to what is learned in such models,\nseveral experiments reveal that the information cap-\ntured by the multi-head self-attention in the popular\nBERT model (i.e., the learned attention weights)\ngenerally align well with syntactic and semantic\nrelations within sentences (Vig and Belinkov, 2019;\nKovaleva et al., 2019). Regarding the second ques-\ntion, building on previous work exploring how to\nprune large neural models while keeping the perfor-\nmance comparable to the original model (Michel\net al., 2019), very recently Tay et al. (2020) has pro-\nposed the ”Synthesizer” framework, comparing the\nperformance when replacing the dot-product self-\nattention in the original transformer model with\nother, less parameterized, attention types.\nInspired by these two lines of research on\ntransformer-based models, namely the identiﬁca-\ntion of a close connection between learned attention\nweights and linguistic structures, and the potential\nfor safely reducing attention parameters, we pro-\npose a document-level discourse-based attention\nmethod for extractive summarization. With this\nnew, discourse-inspired approach, we reduce the\nsize of the attention module, the core component\nof the transformer model, while keeping the model-\nperformance competitive to comparable, fully pa-\nrameterized models on both EDU and sentence\nlevel.\n2 Related Work\n2.1 Attention Methods\nAttention mechanisms have become a widely used\ncomponent of many modern neural NLP models.\nOriginally proposed by Bahdanau et al. (2014) and\nLuong et al. (2015) for machine translation, the\ngeneral idea behind attention is based on the in-\ntuition that not all textual units within a sequence\ncontribute equally to the result. Thus, the atten-\ntion value is introduced to learn how to assess the\nimportance of a unit during training.\nIn recent years, the role of attention within NLP\nfurther solidiﬁed with researchers exploring new\nvariants, such as multi-head self-attention, as used\nin transformers (Vaswani et al., 2017). Gener-\nally, larger transformer models with more attention-\nheads (and therefore more parameters) achieve bet-\nter performance for many tasks (Vaswani et al.,\n2017). In the context of explaining the internal\nworkings of neural models, Kovaleva et al. (2019)\nhas recently focused on transformer-style models,\ninvestigating the role of individual attention-heads\nin the BERT model (Devlin et al., 2019). Analyzing\nthe capacity to capture different linguistic informa-\ntion within the self-attention module, they ﬁnd that\ninformation represented across attention-heads is\noftentimes redundant, thus showing potential to\nprune those parameters.\nFollowing these ﬁndings, Raganato et al. (2020)\ndeﬁne a combination of ﬁxed, position-based at-\ntention heads and a single learnable dot-product\nself-attention head. They empirically show that\nthis hybrid approach reduces the spatial complexity\nof the model, while retaining the original perfor-\nmance. In addition, the hybrid model improves the\nperformance in the low-resource case. Broadening\nthese results, Tay et al. (2020) further investigate\nthe contribution of the self-attention mechanism. In\ntheir proposed “Synthesizer” model, they present\na generalized version of the transformer, explor-\ning alternative attention types, generally requiring\nless parameters, but achieving competitive perfor-\nmances on multiple tasks.\nIn this paper, instead of pruning the redundant\n126\nheads of the transformer model empirically or ex-\nclusively based on position, we reduce the number\nof parameters by incorporating linguistic informa-\ntion (i.e. discourse) in the attention computation.\nWe compare our setup for extractive summarization\nagainst alternative attention mechanisms, deﬁned\nin the Synthesizer (Tay et al., 2020).\n2.2 Discourse and Summarization\nMarcu (1999) was the ﬁrst to explore the applica-\ntion of RST-style discourse to the task of extractive\nsummarization. In particular, he showed that dis-\ncourse can be used directly to improve summariza-\ntion, by simply extracting EDUs along the paths\nwith more nuclei as the document summary.\nLater on, researchers started to explore unsu-\npervised methods for discourse-tree-based summa-\nrization. Hirao et al. (2013) for example propose\na trimming-based method on dependency trees,\npreviously converted from the RST constituency\ntrees, aiming to generate a more coherent summary.\nBased on this idea of trimming the dependency-\ntree, Kikuchi et al. (2014) propose another method\nof trimming nested trees, composed into two lev-\nels: a document-tree considering the structure of\nthe document and a sentence-tree considering the\nstructure within each sentence.\nMore recently, further work along this line\nstarted to incorporate discourse structures into su-\npervised summarization with the goal to better\nleverage the (linguistic) structure of a document.\nXiao and Carenini (2019) and Cohan et al. (2018)\nthereby use the natural structure of scientiﬁc papers\n(i.e. sections) to improve the inputs of the sequence\nmodels, better encoding long documents using a\nstructural prior. They empirically show that such\nstructure effectively improves performance.\nMoreover, Xu et al. (2020) propose a graph-\nbased discourse-aware extractive summarization\nmethod incorporating the dependency trees con-\nverted from RST trees on top of the BERTSUM\nmodel (Liu and Lapata, 2019) and the document\nco-reference graph. The results show consistent\nimprovements, implying a close, bidirectional rela-\ntionship between downstream tasks and discourse\nparsing. Huber and Carenini (2019, 2020) show\nthat sentiment information can be used to infer dis-\ncourse trees with promising performance. They fur-\nther mention extractive summarization as another\nimportant downstream task with strong potential\nconnections to the document’s discourse, motivat-\ning the bidirectional use of available information.\nThis paper employs a rather different objective\nfrom aforementioned work combining discourse\nand summarization. Instead of leveraging addi-\ntional discourse information to enhance the model\nperformance, we strive to create a summarization\nmodel with signiﬁcantly less parameters, hence be-\ning less prone to over-ﬁtting, smaller, and easier to\ninterpret and deploy.\n3 Synthesizer-based Self-Attention\nEvaluation Framework\nAiming to answer the two guiding questions stated\nin section 1, Tay et al. (2020) propose a suite of\nalternative self-attention approaches besides the\nstandard dot-product self-attention, as used in the\noriginal transformer model. In their ”Synthesizer”\nframework, they show that parameter-reduced self-\nattention mechanisms can achieve competitive per-\nformance across multiple tasks, including abstrac-\ntive summarization. While the experiments in the\noriginal ”Synthesizer” framework are on token\nlevel, employing an sequence-to-sequence architec-\nture, we adapt the framework to explore different\nattention mechanisms on EDU-/sentence-level for\nthe extractive summarization task.\nTo evaluate the effect of different attention types\nin our scenario, we apply the general system shown\nin Figure 2, using the pretrained BERT model as\nour unit encoder. Each unit is thereby represented\nas the hidden state of the ﬁrst token in the last\nBERT layer. Subsequently, we feed the BERT\nrepresentations into the ”Synthesizer” document-\nencoder (Tay et al., 2020) with different attention\ntypes and employ a Multi-Layer Perceptron (MLP)\nwith Sigmoid activation to retrieve a conﬁdence\nscore for each unit, indicating its predicted likeli-\nhood to be part of the extractive summary.\nThe “Synthesizer” document encoder is essen-\ntially a transformer encoder with alternative at-\ntention modules, other than the dot-product self-\nattention. As commonly done, we employ multiple\nself-attention heads, previously shown to improve\nthe performance of similar models (Vaswani et al.,\n2017). For each attention head, the input is de-\nﬁned as X ∈Rl×d where l is the length of the\ninput document (i.e. the number of units), and\nd represents the hidden dimension of the model.\nThe self-attention matrix is accordingly deﬁned as\nA ∈Rl×l, where Aij is the attention-value that\nunit ipays to unit j. We further force the sum of\nthe incoming attentions to each unit (as commonly\ndone) to add up to 1, i.e. ∑\nj Aij = 1. The pa-\n127\nFigure 2: Structure of the extractive summarization framework containing the Synthesizer module\nFigure 3: Comparison of attention methods. (a),(b) and (c) taken from (Tay et al., 2020), (d) proposed in this paper.\nrameterized function G, calculating the Value, is\nmultiplied with the attention matrix for the atten-\ntion output: Xout = A·G(X). Here, we evaluate\nthe three self-attention methodologies proposed by\nTay et al. (2020) as our baselines:\nDot Product: As used in the original trans-\nformer model, this self-attention calculates a key,\na value and a query representation for each textual\nunit. The attention value is learned as the relation-\nship between the key- and the query-vector deﬁned\nas A= softmax(K(X) ·Q(X))\nDense: Instead of using the relationship be-\ntween units, encoded as keys and values, the\ndense self-attention A = softmax(Dense(X))\nis solely learned based on the input unit, where\nDense(·) is a two-layer fully connected layer map-\nping from Rl×d to Rl×l, which can be represented\nas Dense(X) =W1σ(W2X+ b2) +b1.2\nRandom: A random attention matrix is gener-\nated for each attention-head, shared across all data\npoints, i.e. A = softmax(R). Rcan thereby be\neither updated (referred as Learned Randomin Sec.\n5) or ﬁxed (Fixed Random) during training.\n2We use the inner dimension as 512 for all experiments.\n4 Discourse Tree Attention\nWe propose a fourth self-attention candidate: a\nﬁxed, discourse-dependent self-attention matrix\ntaking advantage of the strong, tree-structured dis-\ncourse prior. (see Figure 3 for a comparison of all\nthe self-attention methods). The justiﬁcation for\nour new self-attention is two-fold: (1) RST-style\ndiscourse trees represent document-level semantic\nstructures of coherent documents, which are impor-\ntant semantic markers for the summarization task\n(2) RST discourse-trees, especially the nuclearity\nattribute, has been shown to be closely related to\nthe summarization task (Marcu, 1999; Hirao et al.,\n2013; Kikuchi et al., 2014).\nTo explore a diverse set of RST-style discourse\ntree attributes, we propose three distinct tree-to-\nmatrix encodings focusing on: the nuclearity-\nattribute, through a dependency-tree transforma-\ntion; the plain discourse-structure, derived from the\noriginal constituency structure; and a nuclearity-\naugmented discourse structure, obtained from the\nconstituency representation.\n128\n4.1 Dependency-based Nuclearity Attributes\n(D-Tree)\nInspired by previous work using dependency trees\nto support the summarization task (Marcu, 1999;\nHirao et al., 2013; Xu et al., 2020), we ﬁrst con-\nvert the original constituency-tree, obtained with\nthe RST-DT trained discourse parser (Wang et al.,\n2017), into the respective dependency tree and sub-\nsequently generate the ﬁnal matrix-representation.\nIn the ﬁrst step, we follow the constituency-to-\ndependency conversion algorithm proposed by Hi-\nrao et al. (2013) (shown superior for summarization\nin Hayashi et al. (2016)). While this algorithm en-\nsures a near-bijective conversion (see Morey et al.\n(2018)), the resulting dependency trees do not nec-\nessarily have single-rooted sentence sub-trees.To\naccount for this, we apply the post-editing method\nproposed in Hayashi et al. (2016).\nTo use the newly generated dependency tree in\nthe “Synthesizer” transformer model, we generate\nthe self-attention matrix from the tree structure by\nfollowing a standard Graph Theory approach (Xu\net al., 2020). Head-dependent relations in the tree\nare represented as binary values (1 indicating a\nrelation, 0 representing no connection) in the self-\nattention matrix, where each column of the matrix\nidentiﬁes the head and each row represents depen-\ndents. The root is considered head and dependent\nof itself, ensuring all row-sums to be 1. Figure 4\nshows the inferred dependency-tree and the gener-\nated self-attention matrix for our running example.\nFigure 4: Dependency tree-to-matrix conversion.\n4.2 Constituency-based Structure Attributes\n(C-Tree)\nArguably, there are aspects of the constituency tree-\nstructure that may not be captured adequately by\nthe corresponding dependency-tree. These aspects,\ndeﬁning the compositional structure of the docu-\nment, may contain valuable information for the\nself-attention. In particular, the inter-EDU relation-\nships encoded in the constituency tree can be used\nto deﬁne the relatedness of textual units, implying\nthat the closer the units are in the discourse tree,\nthe more related they are, and the more attention\nthey should pay to each other. Further inspired\nby the ideas of aggregation (Nguyen et al., 2020)\nand splitting (Shen et al., 2019), we deﬁne the at-\ntention between EDUs based on the depth of the\nconstituency-tree on which they are assigned to the\nsame constituent (Left in Figure 5).\nMore speciﬁcally, we compute the attention be-\ntween every two nodes in the self-attention matrix\nas follows. Suppose the height of the constituency-\ntree is H, then for each level Lof the tree, there\nis a binary matrix ML ∈Rl×l with ML\nij = 1 if\nEDU iand EDU jare in the same constituent and\nML\nij = 0otherwise. The ﬁnal self-attention matrix\nAis deﬁned as the normalized aggregate matrices\nof all levels: A= normalize(∑\nL ML)\nThe resulting self-attention matrix A is exclu-\nsively based on the discourse structure-attribute,\nwithout taking the nuclearity into account, repre-\nsenting a rather different approach from the previ-\nously described one based on the dependency-tree.\n4.3 Constituency-based Structure and\nNuclearity Attributes (C-Tree w/Nuc)\nWith the previous sections focusing on either ex-\nploiting the nuclearity attribute, by converting the\nRST-style constituency tree into a dependency rep-\nresentation, or the constituency-tree structure itself,\nwe now propose a third, hybrid approach, using\nboth attributes to generate the self-attention ma-\ntrix. Plausably, the combination could further en-\nhance the quality of the self-attention matrix. The\ncombined approach is closely related to the struc-\ntural approach presented in section 4.2, but extends\nthe binary self-attention matrix computation to the\nternary case. At each level, ML\nij = 2 if the node\nrooting the local sub-tree containing EDU i and\nEDU jis the nucleus in its relation3, ML\nij = 1for\nthe satellite case. Unchanged from section 4.2, if\nEDUs iand jare not sharing a common sub-tree\non level L, ML\nij = 0. For example, M1\n3:4,3:4 = 2,\nas the sub-tree containing EDU3 & 4 is the nucleus\nin it’s relation with the sub-tree containing EDU 5.\n4.4 Sentence-based Discourse Self-Attention\nThe natural granularity-level for a discourse-related\nsummarization model is Elementary Discourse\nUnits (EDUs). Besides using EDUs as our atomic\nelements, we also explore similar models on\nsentence-level, the more standard approach in the\n3The weight of Nucleus and Satellite is set hard-coded,\nand will be tuned in the future.\n129\nFigure 5: Constituency tree-to-matrix conversion. Left: Structure only, Right: Structure and Nuclearity\narea of extractive summarization, using a BERT\nsentence-encoder instead of the previously used\nBERT EDU-encoder.\nTo obtain the respective sentence-level self-\nattention matrix, given the EDU-level self-attention\nmatrix Ae of the three matrix-generation ap-\nproaches deﬁned above, we deﬁne an indicator-\nmatrix I ∈RNS ×NE . NS and NE are thereby\nthe number of sentences and EDUs in the docu-\nment. Iij = 1 if and only if EDU j belongs to\nsentence i. The sentence-level self-attention matrix\nAs is then deﬁned as\nAs = IAeIT\nGenerating the sentence-level self-attention ma-\ntrices directly from the EDU-level self-attention\nmatrices, instead of the tree-representation itself,\navoids the problem of potentially leaky EDUs (Joty\net al., 2015), as sentences with leaky EDUs (having\nnaturally high attention values between them) will\ncontinue to be tightly connected.\n5 Experiments\n5.1 Experimental Setup\nDataset: We use the popular CNN/DM dataset\n(Nallapati et al., 2016), a standard corpus for ex-\ntractive summarization. Key dimensions of the\ndataset with corresponding statistics are in Table\n1. Based on the average number of units selected\n#token #EDU #Sent #EDU(O.) #Sent(O.)\n546 70.2 27.2 6.4 3.1\nTable 1: Statistics of the CNNDM dataset. O. means\nthe average number of units in the oracle\nby the oracle4 on EDU- and sentence-level, we de-\n4The oracle summary contains the units greedily picked\naccording to the ground-truth summary, which is built follow\nKedzie et al. (2020).\nﬁne the summarization task to choose the top 6\nEDUs or the highest scoring 3 sentences, depend-\ning on the task granularity. Please further note\nthat the original corpus does not contain any EDU-\nlevel markers (as presented in Table 1). The EDU\nsegmentation process employed for EDU-related\ndataset dimensions is described below.\nDiscourse Augmentation: To obtain high-\nquality discourse representations for the documents\nin the CNN/DM training corpus we use the pre-\ntrained versions of the top-performing discourse-\nsegmenter (Wang et al., 2018) and -parser (Wang\net al., 2017), reaching an F1-score of 94.3%, 86.0%\n(span) and 72.4% (nuclearity) respectively on the\nRST-DT dataset.5 In line with previous work ex-\nploring the combination of discourse and summa-\nrization, we follow the “dependency-restriction”\nstrategy proposed in Xu et al. (2020) to enhance\nthe coherence and grammatical correctness of the\nsummarization. Such strategy requires that all an-\ncestors of a selected EDU within the same sentence\nshould be recursively added to the ﬁnal summary.\nHyper-Parameters: To stay consistent with pre-\nvious work, we set the dimensions of the attention\nkey(dk), value(dv) and query vector(dq) to 64 for\neach head, and the inner dimension of the position-\nwise feed-forward layer (dinner) to 3072. Similar\nto the synthesizer model (Tay et al., 2020), we only\nalter the attention part of the transformer model,\nwhich contains a small portion of the overall pa-\nrameters. Additionally, we explore a more bal-\nanced, setting, with dv = dk = dq = 512 and\ndinner = 512 for all models. During training,\nwe use a scheduled learning-rate ( lr = 1e−2)\nwith standard warm-up steps for the Adam opti-\n5We use the publicly available implementation by the orig-\ninal authors at //github.com/yizhongw/StageDP\n130\nModel Rouge-1 Rouge-2 Rouge-L # Heads # Params(attn) # Params\nLead6 37.99 15.56 34.08 - - -\nOracle 62.08 38.20 58.86 - - -\nDiscoBERT(5 EDUs) 43.77 20.85 40.67 - - -\nDefault Models (dk =dv =dq = 64, dinner= 3072)\nDot Product(8) 41.02 18.78 37.96 8 3.2M 12.7M\nDot Product(1) 40.92‡ 18.69‡ 37.85‡ 1 0.4M 9.9M\nDense 40.70 18.65 † 37.74† 1 1.5M 11.0M\nLearned Random 40.24 18.28 37.32 1 0.7M 10.3M\nFixed Random 40.36 18.35 37.40 1 0.2M 9.7M\nNo attention 39.89 17.98 36.99 1 0.2M 9.7M\nD-Tree 40.43 18.32 37.45 1 0.2M 9.7M\nC-Tree 40.80 † 18.56 37.74 † 1 0.2M 9.7M\nC-Tree w/Nuc 40.76 18.59 † 37.73 1 0.2M 9.7M\nBalanced Models (dk =dv =dq = 512, dinner= 512)\nDot Product(8) 40.95 18.52 37.78 8 25.2M 27M\nDot Product(1) 40.64 18.33 37.54 1 3.2M 4.8M\nDense 40.87‡ 18.59‡ 37.79‡ 1 2.9M 4.5M\nLearned Random 40.32 18.22 37.31 1 2.1M 3.8M\nFixed Random 40.18 18.13 37.19 1 1.6M 3.2M\nNo attention 40.21 18.17 37.22 1 1.6M 3.2M\nD-Tree 40.29 18.17 37.29 1 1.6M 3.2M\nC-Tree 40.28 18.13 37.28 1 1.6M 3.2M\nC-Tree w/Nuc 40.70 18.46 †‡ 37.63 1 1.6M 3.2M\nTable 2: Overall Performance of the models on the EDU level with the number of heads each layer, as well\nas the number of parameters to train in the attention module and in the whole model. The dashed line splits the\nmodels with learnt attentions and with ﬁxed attentions. †indicates that corresponding result isNOT signiﬁcantly\nworse than the best result of single-head models with p <0.01 with the bootstrap test, and ‡indicates that the\ncorresponding result is NOT signiﬁcantly worse than the result of the 8-head Dot Product with same setting.\nmizer (Kingma and Ba, 2014), following the hyper-\nparameter setting in the original transformer paper\n(Vaswani et al., 2017).\nBaseline Models: We compare our new,\nparameter-reduced Tree Attention approach against\na variety of competitive baselines. Based on the\nstandard Dot Product Attention, as used in the orig-\ninal transformer, we explore two settings: A single\nhead and an 8-head Dot Product Attention. Inspired\nby the ”Synthesizer”-framework, we further com-\npare our approach against the Dense and Random\nAttention computation, as mentioned in Section\n3. To better show the effect of different attention\nmethods, we use a ‘No Attention Model’ as an\nadditional baseline, in which each input can only\nattend to itself, i.e. A= I. Please note, (1) as our\ngoal is to explore possible parameter reductions,\nwe ensure that all heads contain similar dimen-\nsions across models. (2) The attention matrices in\nthe “Fixed Random”, “No Attention” and all three\nTree Attention models (D-Tree, C-Tree and C-Tree\nw/Nuc) are ﬁxed, while they are learned for other\nmodels.\n5.2 Results and Analysis\nWe present and discuss three sets of experimental\nresults. First, the natural task for discourse-related\nextractive summarization on EDU-level. Second,\nthe most common task of extractive summarization\non sentence-level and, ﬁnally, further experiments\nregarding the low resource case.\nTables 2 and 3 show our experimental results on\nEDU and Sentence level, respectively. Each row\nthereby contains the Rouge-1, -2 and -L scores of\nthe model, along with the number of self-attention\nheads and the amount of trainable parameters in the\nattention module and in the complete model6. For\nreadability, the results in either table are divided\ninto three sub-tables. The ﬁrst sub-table contains\nthe commonly used Lead-baseline (Lead6 on EDU\nlevel and Lead3 on sentence level), along with\nthe Oracle, representing the performance upper-\nbound, and the current state-of-the-art models (Dis-\ncoBERT (Xu et al., 2020) on EDU level, BERT-\nSUM (Liu and Lapata, 2019) on sentence level).\nPlease note, both SOTA models ﬁnetune BERT\nas a token-based document encoder, to learn addi-\ntional cross-unit information of tokens. However,\nthis requires additional training resources (as the\nBERT model itself contains 108M learnable pa-\nrameters). Furthermore, both SOTA models use\n6The BERT EDU/sentence encoder is ﬁxed and the param-\neters therefore not included.\n131\nModel Rouge-1 Rouge-2 Rouge-L # Heads # Params(attn) # Params\nLead3 40.30 17.52 36.54 - - -\nOracle 56.04 33.10 52.29 - - -\nBERTSUM(w/Tri-Block) 43.25 20.24 39.63 - - 118M\nDefault Models (dk =dv =dq = 64, dinner= 3072)\nDot Product(8) 41.82 19.18 38.18 8 3.2M 12.7M\nDot Product(1) 41.71‡ 19.08‡ 38.08‡ 1 0.4M 9.9M\nDense 41.69 † 19.07‡† 38.08‡† 1 1.5M 11.0M\nLearned Random 41.21 18.86 37.67 1 0.7M 10.3M\nFixed Random 41.27 18.91 37.72 1 0.2M 9.7M\nNo attention 40.97 18.64 37.44 1 0.2M 9.7M\nD-Tree 41.44 18.87 37.83 1 0.2M 9.7M\nC-Tree 41.64 † 19.04† 38.03† 1 0.2M 9.7M\nC-Tree w/Nuc 41.64 † 19.05† 38.03† 1 0.2M 9.7M\nBalanced Models (dk =dv =dq = 512, dinner= 512)\nDot Product(8) 41.45 18.88 37.84 8 25.2M 27M\nDot Product(1) 41.51 18.95 37.94 1 3.2M 4.8M\nDense 41.63 † 19.05† 38.01† 1 2.9M 4.5M\nLearned Random 41.26 18.83 37.70 1 2.1M 3.7M\nFixed Random 41.17 18.81 37.61 1 1.6M 3.2M\nNo attention 41.25 18.75 37.68 1 1.6M 3.2M\nD-Tree 41.31 18.80 37.75 1 1.6M 3.2M\nC-Tree 41.68 19.11 38.12 1 1.6M 3.2M\nC-Tree w/Nuc 41.64 † 19.02† 38.06† 1 1.6M 3.2M\nTable 3: Overall Performance of the models on the sentence level. †represents that it is NOT signiﬁcantly worse\nthan the best result of the single-head models with p < 0.01 with the bootstrap test, and ‡indicates that the\ncorresponding result is NOT signiﬁcantly worse than the result of 8-head Dot Product with same setting ( ‡for\nDefault Models only).\n’Trigram-Blocking’, which has been shown to be\nable to greatly improve summarization results (Liu,\n2019). The second sub-table shows our experimen-\ntal results using the default parameter setting, as\nproposed in the original transformer, and the last\nsub-table presents the results when using a bal-\nanced parameter setting. Within each sub-table,\nwe further differentiate models by the number of\nheads, either containing a single attention head or\nthe original 8-head self-attention. As each docu-\nment only contains a single discourse tree, there is\nonly one ﬁxed self-attention matrix for each docu-\nment, making the single-head model equivalent to\nthe multi-head approach.\nEDU Level Experiments: are shown in Table 2.\nWhen comparing the single head models using the\ndefault setting (second sub-table), it appears that\nboth, C-Tree and C-Tree w/Nuc achieve compet-\nitive performance with the single head Dot Prod-\nuct model, despite the Dot product using twice as\nmany parameters in the attention module (0.4M vs.\n0.2M). This is an important advantage because,\neven though the non attention related parameters\nin the complete model outweigh the number of\nattention parameters in this setting, the attention\nhowever resembles the core component of the trans-\nformer model, and so saving attention parameters\nis arguably more critical. In addition, the differ-\nence would become large with the increment of\nthe number of heads. Furthermore, when com-\nparing models with ﬁxed attention or no attention,\nthe effect of the attention module becomes clear,\nshowing superior performance of the C-Tree and C-\nTree /w Nuc approaches, indicating that discourse\nstructure can indeed help for the task of extractive\nsummarization. In contrast, the D-Tree inspired\nself-attention does not perform as well. The drop in\nperformance when using this tree-attention might\nbe caused by the rather strict, binary attention com-\nputation, potentially pruning too much valuable\ndiscourse information. Examining the models with\nlearnt attentions, we observe that the Dense model\nreduces the number of parameters compared to the\nbest performing 8-head Dot Product, however, still\ncontains far more parameters than the single-head\nDot Product. Despite the large difference in the\nnumber of parameters, the single-head Dot Product\nAttention performs comparable to the Dense model,\nsuggesting the necessity to synthesize the Dense\nattention (see (Tay et al., 2020)). Regarding the\nBalanced model (bottom sub-table), we put addi-\ntional emphasis on the attention component, show-\ning trends when using larger attention computation\nmodules. The results in this sub-table suggest that\n132\nthe beneﬁts of our tree attention models are improv-\ning over-proportionally for more balanced models,\nwith the C-tree /w Nuc even outperforming the\nsingle-head Dot Product and achieving competi-\ntive performance to the 8-head Dot Product model,\nwhich contains an order of magnitude more param-\neters in this setting.\nSentence Level Experiments: Our sentence level\nexperiments, presented in Table 3, are mostly akin\nto the EDU level experiments. However, a rela-\ntive performance improvement of ﬁxed attention\nmodels compared to learned attention approaches\ncan be observed, leading to a smaller performance\ngap on sentence level. We believe that this over-\nproportional improvement might be due to the po-\nsition bias on sentence level, which tends to be\nlarger than on EDU level, generally making the\nsentence level task easier to learn. In line with\nthis trend, we also observe that the difference be-\ntween the Lead-baseline (40.30/17.53/36.54) and\nthe Oracle (56.04/33.10/52.29) on sentence level\nis relatively small when compared to the EDU\nlevel Lead-baseline(37.99/15.56/34.08) and Ora-\ncle(62.08/38.20/58.86). As a result, the ﬁxed tree\nattention models are statistically equivalent to the\nlearned single-head Dot Product in the default sen-\ntence level setting, and signiﬁcantly outperform the\n8-head Dot Product model in the balanced setting.\nLow Resource Experiments: Complementing\nour previous experiments, showing consistently\ncompetitive results of the parameter-sparse models\nusing tree priors, we further explore the robustness\nof our tree self-attention methods in additional low\nresource experiments on EDU level. Therefore, we\nrandomly generate 5 small subsets of the training\ndataset, each containing 1,000 datapoints, train-\ning the same models as shown in Tables 2 and 3 on\neach subset. However, contrasting our initial expec-\ntation, the tree-inspired C-Tree w/Nuc model only\nimproves the performance on the low-ressource\nexperiments under the balanced setting, with no\nsigniﬁcant improvements under the default setting.\nOverall: Comparing the results in Tables 2 and 3, it\nbecomes obvious that the sentence level models are\nconsistently better than the EDU level models, de-\nspite the opposite trend holding between sentence\nOracle and EDU Oracle, as well as the respective\nSOTA models. One possible reason for this result\nis that the BERT model is originally trained on the\nsentences, which might potentially impair the sub-\nsentential (EDU) representation generation ability\nof the model.\nFurthermore, when comparing the single head\nand 8 head Dot Product models in both tables and\nin both settings, we ﬁnd that the improvement gains\nof adding additional heads is rather limited, even\nimpairing the performance in the balanced setting\non sentence level. We therefore believe that the\nbalance between the performance and the number\nof parameters is worth of further exploration for\nthe task of extractive summarizarion.\n6 Conclusion and Future Work\nWe extend and adapt the “Synthesizer” framework\nfor extractive summarization by proposing a new\ntree self-attention method, based on RST-style con-\nsitituency and dependency trees. In our experi-\nments, we show that the performance of the tree\nself-attention is signiﬁcantly better than other ﬁxed\nattention models, while being competitive to the\nsingle-head standard dot product self-attention in\nthe transformer model on both, the EDU-level and\nsentence-level extractive summarization task. Fur-\nthermore, our tree attention is better than the 8-head\ndot product in the balanced setting. Besides these\ngeneral results, we further investigate low-resource\nscenarios, where our parameter-light approaches\nare assumed to be especially useful. However, con-\ntrary to this expectation, they do not seem to be\nmore stable and robust than other solutions. In ad-\ndition, we also ﬁnd that the multi-head Dot product\nmodel is not always signiﬁcantly better than the\nsingle-head approach. This, combined with the pre-\nvious ﬁnding, suggest that more research is needed\non the balance between the number of parameter\nand the performance of the summarization model.\nIn the future, we plan to explore ways to also\nincorporate rhetorical relations into self-attention,\nin addition to discourse structure and nuclearity.\nFurther, we want to replace the hard-coded weight\ntrade-off between Nucleus and Satellite in the C-\nTree w/Nuc approach, using instead the conﬁdence\nscore from the discourse parser as the weight. Fi-\nnally, since the current two-level encoder performs\ngenerally worse than a single token-based encoder\n(e.g. BERTSUM(Liu and Lapata, 2019)), we in-\ntend to explore tree self-attention in combination\nwith the BERTSUM model.\nAcknowledgments\nWe thank reviewers and the UBC-NLP group for\ntheir insightful comments. This research was sup-\nported by the Language & Speech Innovation Lab\nof Cloud BU, Huawei Technologies Co., Ltd.\n133\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nLynn Carlson, Mary Ellen Okurowski, and Daniel\nMarcu. 2002. RST discourse treebank. Linguistic\nData Consortium, University of Pennsylvania.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. NAACL HLT 2018 - 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies - Proceedings of the Conference, 2:615–621.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKatsuhiko Hayashi, Tsutomu Hirao, and Masaaki Na-\ngata. 2016. Empirical comparison of dependency\nconversions for rst discourse trees. In Proceedings\nof the 17th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue, pages 128–136.\nTsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,\nNorihito Yasuda, and Masaaki Nagata. 2013. Single-\nDocument Summarization as a Tree Knapsack Prob-\nlem. Technical report.\nPatrick Huber and Giuseppe Carenini. 2019. Pre-\ndicting discourse structure using distant supervision\nfrom sentiment. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2306–2316.\nPatrick Huber and Giuseppe Carenini. 2020. Mega\nrst discourse treebanks with structure and nuclear-\nity from scalable distant sentiment supervision. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing.\nShaﬁq Joty, Giuseppe Carenini, and Raymond T Ng.\n2015. Codra: A novel discriminative framework\nfor rhetorical analysis. Computational Linguistics,\n41(3):385–435.\nChris Kedzie, Kathleen McKeown, and Hal Daum ´e.\n2020. Content selection in deep learning models\nof summarization. Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2018, pages 1818–1828.\nYuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-\nabu Okumura, and Masaaki Nagata. 2014. Single\ndocument summarization based on nested tree struc-\nture. 52nd Annual Meeting of the Association for\nComputational Linguistics, ACL 2014 - Proceedings\nof the Conference, 2:315–320.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nYang Liu. 2019. Fine-tune BERT for extractive sum-\nmarization. CoRR, abs/1903.10318.\nYang Liu and Mirella Lapata. 2019. Text Summa-\nrization with Pretrained Encoders. EMNLP-IJCNLP\n2019 - 2019 Conference on Empirical Methods in\nNatural Language Processing and 9th International\nJoint Conference on Natural Language Processing,\nProceedings of the Conference, pages 3730–3740.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025.\nWilliam C. Mann and Sandra A. Thompson. 1988.\nRhetorical Structure Theory: Toward a functional\ntheory of text organization.\nDaniel Marcu. 1999. Discourse Trees are Good Indica-\ntors of Importance in Text. Advances in Automatic\nText Summarization, pages 123–136.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. dAlch´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 32, pages 14014–\n14024. Curran Associates, Inc.\nMathieu Morey, Philippe Muller, and Nicholas Asher.\n2018. A dependency perspective on rst discourse\nparsing and evaluation. Computational Linguistics,\n44(2):197–235.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nC ¸ a˘glar Gulc ¸ehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nXuan-Phi Nguyen, Shaﬁq Joty, Steven C. H. Hoi, and\nRichard Socher. 2020. Tree-structured Attention\nwith Hierarchical Accumulation. pages 1–15.\n134\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The penn discourse treebank 2.0.\nLREC.\nAlessandro Raganato, Yves Scherrer, and J ¨org Tiede-\nmann. 2020. Fixed Encoder Self-Attention Patterns\nin Transformer-Based Machine Translation.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\n7th International Conference on Learning Represen-\ntations, ICLR 2019, pages 1–14.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 2017-Decem(Nips):5999–6009.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA two-stage parsing method for text-level discourse\nanalysis. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 184–188.\nYizhong Wang, Sujian Li, and Jingfeng Yang. 2018.\nToward fast and accurate neural discourse segmen-\ntation. arXiv preprint arXiv:1808.09147.\nWen Xiao and Giuseppe Carenini. 2019. Extractive\nsummarization of long documents by combining\nglobal and local context. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3011–3021, Hong Kong,\nChina. Association for Computational Linguistics.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.\n2020. Discourse-aware neural extractive text sum-\nmarization. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5021–5031, Online. Association for Computa-\ntional Linguistics.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9180137515068054
    },
    {
      "name": "Transformer",
      "score": 0.7299786806106567
    },
    {
      "name": "Computer science",
      "score": 0.6660077571868896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5734927654266357
    },
    {
      "name": "Natural language processing",
      "score": 0.5551097393035889
    },
    {
      "name": "Sentence",
      "score": 0.5377794504165649
    },
    {
      "name": "Language model",
      "score": 0.43486273288726807
    },
    {
      "name": "Machine learning",
      "score": 0.3952937722206116
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ],
  "cited_by": 15
}