{
    "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
    "url": "https://openalex.org/W3165766112",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101230845",
            "name": "Hu Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024230879",
            "name": "Gargi Ghosh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5063149046",
            "name": "Po-Yao Huang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5080488188",
            "name": "Prahal Arora",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5084792905",
            "name": "Masoumeh Aminzadeh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5036069974",
            "name": "Christoph Feichtenhofer",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085262529",
            "name": "Florian Metze",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5067919401",
            "name": "Luke Zettlemoyer",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3037309139",
        "https://openalex.org/W2984008963",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2883429621",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W3105479157",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3035119608",
        "https://openalex.org/W3036120435",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3122640483",
        "https://openalex.org/W3035265375",
        "https://openalex.org/W2964311439",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2962916463",
        "https://openalex.org/W2964094654",
        "https://openalex.org/W3043840704",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2962795934",
        "https://openalex.org/W2885775891",
        "https://openalex.org/W2949178656",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W3006320872",
        "https://openalex.org/W2784025607",
        "https://openalex.org/W2957775769"
    ],
    "abstract": "We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks. Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion. We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input). Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.",
    "full_text": "VLM: Task-agnostic Video-Language Model Pre-training\nfor Video Understanding\nHu Xu1, Gargi Ghosh1, Po-Yao Huang12, Prahal Arora1, Masoumeh Aminzadeh1\nChristoph Feichtenhofer1, Florian Metze1 and Luke Zettlemoyer1\n1Facebook AI\n2Carnegie Mellon University\n{huxu,gghosh,berniehuang,prarora,masoumeha,\nfeichtenhofer,fmetze,lsz}@fb.com\nAbstract\nWe present a simpliﬁed, task-agnostic multi-\nmodal pre-training approach that can accept\neither video or text input, or both for a va-\nriety of end tasks. Existing pre-training are\ntask-speciﬁc by adopting either a single cross-\nmodal encoder that requires both modalities,\nlimiting their use for retrieval-style end tasks\nor more complex multitask learning with two\nunimodal encoders, limiting early cross-modal\nfusion. We instead introduce new pretraining\nmasking schemes that better mix across modal-\nities (e.g. by forcing masks for text to predict\nthe closest video embeddings) while also main-\ntaining separability (e.g. unimodal predictions\nare sometimes required, without using all the\ninput). Experimental results show strong per-\nformance across a wider range of tasks than\nany previous methods, often outperforming\ntask-speciﬁc pre-training. Code is made avail-\nable at https://github.com/pytorch/\nfairseq/tree/main/examples/MMPT.\n1 Introduction\nWe study the challenge of achieving task-agnostic\npre-training for multimodal video understanding,\nbuilding on recent unimodal approaches such as\npretrained language models for text (Peters et al.,\n2018; Devlin et al., 2019). Although certain lan-\nguage models are near task-agnostic (Devlin et al.,\n2019; Lewis et al., 2020) on NLP tasks, being task-\nagnostic on multi-modal tasks are more challeng-\ning due to cross-modal tasks such as text-video re-\ntrieval. Existing video-and-language pre-trainings\nare task-speciﬁc, which adopt either (1) a cross-\nmodal single encoder (Sun et al., 2019b,a; Zhu and\nYang, 2020) favoring tasks that require cross-modal\nreasoning (e.g. video captioning), or (2) multiple\nunimodal encoders/decoders (Miech et al., 2019,\n2020; Li et al., 2020b; Luo et al., 2020; Korbar\net al., 2020) combining speciﬁc tasks that require\nseparately embedding each modality (e.g. video\nFigure 1: Existing models (upper ﬁgure) adopt com-\nplex architectures and multiple task-speciﬁc training to\nmerge two streams of data to cover a wide range of\ndownstream tasks (such as retrieval or text generation).\nOur video-language model (VLM) (lower ﬁgure) uses\na single BERT encoder for task-agnostic pre-training\n(e.g. only masking tokens, no matching or alignment\nfor speciﬁc end tasks) in a joint feature space, while\nstill covering a wide range of tasks (see Figure 3).\nretrieval). We instead show that it is possible to pre-\ntrain a task-agnostic model called video-language\nmodel (VLM) that can accept text, video, or both\nas input.\nAs shown in Figure 1, this task-agnostic single\nencoder approach has several advantages: (1) it\nreduces the complexity of pre-training with mul-\ntiple losses and models (e.g. Luo et al. (2020)),\nand (2) it holds less assumption on being close to\nend tasks as in retrieval-based pre-training Miech\net al. (2020) and is as general as classic LMs, and\n(3) it encourages feature sharing among modalities\nwhen present, without sacriﬁcing separability, and\n(4) it is more parameter efﬁcient (see Section 5, we\nachieve strong performance with BERTBASE sized\nmodels). Table 1 summarizes the design choices of\nrecent models.\nOur encoder is a transformer block that com-\nbines the existing masked frame model and masked\narXiv:2105.09996v3  [cs.CV]  30 Sep 2021\nlanguage model (MFM-MLM) (Sun et al., 2019a;\nLi et al., 2020b; Luo et al., 2020) with two new\nmethods to improve the learning of multi-modal fu-\nsion. First, we introduce a masking scheme called\nmasked modality model (MMM) that randomly\nmasks a whole modality for a portion of training\nexamples (the rest of the examples goes for tradi-\ntional MFM-MLM), thereby forcing the encoder\nto use the tokens from the other modality to pro-\nduce tokens for the masked modality. We then\nintroduce a single masked token loss to replace two\n(2) losses on video and text separately for MFM-\nMLM. Masked token loss uses the embeddings of\nboth video and text tokens to learn joint hidden\nstates for the encoder.\nWe also show it is possible to ﬁne-tune a single\nencoder for a wide range of tasks by using task-\nspeciﬁc attention masks. Experiments demonstrate\nthat it performs well on a wider range of tasks than\nprevious models, including outperforming task-\nspeciﬁc pre-training baselines with unimodal en-\ncoders of similar hyper-parameters by more than\n2% on retrieval tasks and 1% on video captioning.\nNote that these results are also achieved with a\nmuch smaller model than previous approaches, fur-\nther demonstrating the improved fusion and sharing\nacross modalities.\nIn summary, the main contributions of this paper\nare as follows: (1) we propose to pre-train a task-\nagnostic encoder for video understanding; (2) we\nintroduce masked modality model (MMM) and\nmasked token loss for cross-modal fusion during\npre-training without sacriﬁcing separability; (3)\nexperimental results show that the proposed simple\nbaseline achieves competitive performance with\nsigniﬁcantly fewer parameters.\n2 Related Work\nNumerous multimodal task-speciﬁc pre-training\nmodels are proposed for downstream visual-\nlinguistic tasks. In video and text pre-training,\nexisting research adopts different design choices\nregarding proxy tasks and neural architectures for\nend tasks (Luo et al., 2020).\nOn one hand, VideoBERT (Sun et al., 2019b),\nUnicoder-VL (Li et al., 2020a), VL-BERT (Su\net al., 2020), UNITER (Chen et al., 2020), VLP\n(Zhou et al., 2018), ActBERT (Zhu and Yang, 2020)\nadopt a shared encoder approach, where the vision\nand text sequences are concatenated and input to a\nsingle Transformer(Vaswani et al., 2017) encoder.\nAlthough this approach is simple, it limits the types\nof downstream tasks to those that input both modal-\nities simultaneously. For example, (Sun et al.,\n2019b) may not be able to perform joint retrieval\ntasks and added another decoder for video caption-\ning during ﬁne-tuning. (Zhu and Yang, 2020) uses\n[CLS] token for pairwise metric-learning based\nretrieval (which is an easier problem but requires\na quadratic number of examples and is 50 times\nslower as reported in (Luo et al., 2020)).\nMeanwhile, many existing approaches adopt\nor add task-speciﬁc pre-training to accommodate\nretrieval and video captioning tasks (e.g. two-\nstream encoders (video and text separately) and\ntext decoders). For example, (Miech et al., 2019,\n2020; Rouditchenko et al., 2020; Ging et al., 2020;\nGabeur et al., 2020; Alayrac et al., 2020; Patrick\net al., 2021; Huang et al., 2021) adopts a retrieval\ntask for pre-training. CBT (Sun et al., 2019a),\nHERO (Li et al., 2020b), VideoAsMT (Korbar\net al., 2020) and UniVL (Luo et al., 2020) adopt\nmulti-task learning (MTL) to learn retrieval tasks\non video and text encoders. HERO (Li et al.,\n2020b) and UniVL (Luo et al., 2020) adopts an-\nother cross-encoder to further learn the fusion of\ndifferent modality. UniVL (Luo et al., 2020) and\nVideoAsMT (Korbar et al., 2020) add another text\ndecoder for video captioning. Compared with the\nsingle-stream input in the shared encoder approach,\ntwo-stream encoders typically come with a com-\nplex architecture and proxy tasks to cover more end\ntasks. To the best of our knowledge, none of the\nexisting works target task-agnostic pre-training.\n2.1 Image-Text Pre-training\nViLBERT (Lu et al., 2019), LXMERT (Tan and\nBansal, 2019) adopt two transformers for image\nand text encoding separately. VisualBERT (Li et al.,\n2019), Unicoder-VL (Li et al., 2020a), VL-BERT\n(Su et al., 2020), UNITER (Chen et al., 2020), Uni-\nﬁed VLP (Zhou et al., 2020) use one shared BERT\nmodel. These models employ MLM and pairwise\nimage-text matching as pretraining tasks which are\neffective for downstream multimodal tasks. Our\nﬁne-tuning for video captioning is inspired by Uni-\nﬁed VLP (Zhou et al., 2020) that adopts attention\nmasks and language model heads of BERT for\nimage-captioning.\n2.2 Video-Text Pre-training\nVideoBERT (Sun et al., 2019b) and CBT (Sun\net al., 2019a) are the ﬁrst works to explore the\ncapability of pre-training for video-text. Although\nVideoBERT and CBT pre-train the model on\nmultimodal data, the downstream tasks mainly\ntake video representation for further prediction.\nActBERT (Zhu and Yang, 2020) is a weakly-\nsupervised pre-training method. It leverages global\naction information to catalyze mutual interactions\nbetween linguistic texts and local regional objects\nand introduces a transformer block to encode global\nactions, local regional objects, and linguistic de-\nscriptions. HERO (Li et al., 2020b) encodes mul-\ntimodal inputs in a hierarchical fashion. Besides,\ntwo new pre-training tasks, video-subtitle matching\nand frame order modeling, are designed to improve\nrepresentation learning. VideoAsMT (Korbar et al.,\n2020) and UniVL (Luo et al., 2020) further adopt a\nBART-style(Lewis et al., 2020) text generation task\nfor downstream tasks such as video captioning and\nUniVL adopts a EnhancedV training stage to mask\nall text tokens for better learning of generation.\n3 Pre-training\nAs a reminder, our goal is to train a task-agnostic\nmodel for various tasks in video-text understand-\ning. This section introduces task-agnostic prox-\nies for pre-training. We ﬁrst describe two mask-\ning schemes as a baseline: masked frame model\n(MFM) for video frames and masked language\nmodel (MLM) for text tokens (Sun et al., 2019a; Li\net al., 2020b; Luo et al., 2020). Then we introduce\nmasked modality model (MMM) that encourage to\nlearn the representations of one modality from the\nother. Lastly, we introduce masked token loss that\nuniﬁes losses on masked video and text tokens as a\nsingle loss function.\n3.1 Vector Quantization and BERT\nAssume we have a clip(v, t) sampled from a video,\nwhere v and t corresponds to video modality and\ntext modality, respectively. Since videos are signals\nin continuous space, we ﬁrst extract token embed-\ndings from raw videos. We decode v into frames\nand then feed them into a (frozen) video encoder\nEncodervideo(·) and a trainable MLP layer to obtain\nvideo tokens:\nxv = MLP(Encodervideo(fv)), (1)\nwhere we use a bolded symbol to indicate a se-\nquence and fv is a sequence of continuous frames\nfrom a video. We use S3D (Xie et al., 2018;\nMiech et al., 2020), which is pre-trained via self-\nsupervised learning on the Howto100M dataset.\nThe MLP layer allows the hidden size of video\ntokens to be the same as BERT’s hidden sizes d:\nxv ∈Rd. Similarly, vectors for text tokens xt are\nobtained via embedding lookup as in BERT.\nTo simplify multi-modal pre-training, we adopt\na single BERT transformer with minimum changes.\nWe ﬁrst concatenate video tokens xv and text to-\nkens xt via the [SEP] token so video and text\nbelongs to one corresponding segment of BERT:\nx = [CLS] ◦xv ◦[SEP] ◦xt ◦[SEP]. (2)\nWe further mask x as xmasked (detailed in the\nnext subsection) and feed the whole sequence into\nBERT:\nh = BERT(xmasked), (3)\nwhere h indicates the hidden states of the last layer\nof BERT. To encourage learning video/text hidden\nstates in a shared space for the masked token loss\n(introduced in Section 3.3), we use a shared head\nto predict video/text token embeddings via a linear\nprojection layer:\ne = Wh + b, (4)\nwhere e ∈Rd and W and b are the weights from\nthe prediction heads of BERT. In this way, our\nmodel learns a joint embedding space for both\nvideo and text tokens from inputs to outputs of\nBERT. This allows for pre-training a single encoder\ndirectly from any existing LMs and the only layer\nthat requires initialization is the MLP layer.\n3.2 MFM-MLM\nInspired by (Sun et al., 2019a; Li et al., 2020b;\nLuo et al., 2020), we adopt masked frame model\n(MFM) for videos and masked language model\n(MLM) for text as a baseline. Note that unlike\nLMs that typically come with a ﬁxed vocabulary\nwith a special [MASK] token, video tokens are\ninnumerable in the continuous space and we mask\na video token by setting a video token with all zeros\nand ask the encoder to recover the video token. via\nnoisy contrastive estimation (NCE):\nLMFM = −Es∼V log NCE(xs|xmasked; V ′), (5)\nwhere V is all indexes of video tokens and\nNCE(xv|xmasked; V ′) =\nexp(xT\nv ev)\nexp(xTv ev) +∑\nj∈V ′ exp(xT\nj ev), (6)\nFigure 2: Task-agnostic pre-training (e.g. w/o task\non retrieval-style alignment): MFM-MLM: 50% of\ntraining examples are masked as masked frame model\n(MFM) and masked language model (MLM); the rest\n50% examples are masked as masked modality model\n(MMM) (25% on text as in the second row and 25% on\nvideo as in the third row).\nwhere V ′indicates all non-masked video tokens\nwithin the same batch. The ﬁnal loss is the sum of\nboth MFM and MLM:\nLMFM-MLM = LMFM + LMLM, (7)\nwhere LMLM is the same as BERT and we omit\nits details for brevity. We experiment this classic\nbaseline in Section 5.\n3.3 MMM and Masked Token Loss\nMasked Modality Model We introduce masked\nmodality modal (MMM) that masking either all\nvideo or all text tokens out for a given example\nof video-text clip. This masking scheme comple-\nments MFM-MLM (e.g. in our experiments 50%\nof training examples are masked as MMM and the\nrest 50% are masked as MFM-MLM). This encour-\nages the encoder to use tokens from one modality\nto recover the tokens for the other modality. This\nresolves the issue that an encoder may use nearby\ntokens from their modality for prediction just be-\ncause tokens from a single modality are closer As\nin the lower two (2) sub-ﬁgures in Figure 2, we\neither mask the whole modality of video or text\nso this modality can be “generated” from the other\nmodality. Our experiments indicate that this is crit-\nical for pre-training a single encoder for retrieval\ntasks.\nMasked Token LossWe further introduce masked\ntoken loss that uniﬁes loss functions for MFM and\nMLM. This loss encourages learning a joint to-\nken embedding space for video and text and both\ntypes of tokens contribute to the prediction of a\nmasked (video or text) token. This also improves\nthe number of contrasted negative embeddings in\ntwo separate losses for MFM and MLM.\nWe deﬁne masked token loss LVLM as the fol-\nlowing:\n−Es∼V ∪D log NCE(xs|xmasked; V ′∪D\\s), (8)\nwhere D is the word embeddings over the vocab-\nulary of BERT and D\\s excludes token s (if s is a\ntext token). Further, NCE(xs|xmasked; V ′∪D\\s) is\ndeﬁned as:\nexp(xT\ns es)\nexp(xTs es) +∑\nj∈V ′∪D\\s\nexp(xT\nj es). (9)\nNote that j ∈ V ′∪D\\s can be either a video\nor text token and one predicted token es must be\ncloser to the ground-truth token embedding (either\na video token or word embedding) and be away\nfrom other embeddings of video/text tokens. We\nperform an ablation study in Section 5 to show that\nLVLM works better than LMFM-MLM.\n4 Fine-tuning\nIn this section, we describe how to use different\ntypes of attention masks to ﬁne-tune VLM for a\nvariety of tasks, as shown in Figure 3.\n4.1 Text-Video Retrieval\nOne major challenge of pre-training on a single\nencoder is how to adapt such a model to joint\nspace retrieval without using unimodal encoders\nfor task-speciﬁc pre-training on contrastive loss (as\nin Howto100M (Miech et al., 2019, 2020)). The\nmain reason is that many existing models encode\ntext and video tokens together via self-attention,\nand one cannot obtain hidden states for text/video\nalone.\nTo resolve this, we propose to apply an isolated\nattention mask with two squared masks that are\ndiagonally placed, as shown in the lower sub-ﬁgure\nof the ﬁrst box in Figure 3. 1 These two squares\ndisable video and text tokens to attend and see each\nother, while still allow video and text tokens to use\nthe same self-attention layers for learning represen-\ntations in the same feature space. Further, note that\nthe ﬁrst and second [SEP] tokens of BERT will\n1One can further reduceO(m+n)2 complexity to O(m2+\nn2) (m and n are lengths for video and text, respectively) by\nfeeding video/text separately to BERT but we adopt squared\nmasks for simplicity.\nFigure 3: Fine-tuning of downstream tasks: we adopt different types of attention masks for BERT to accommodate\ndownstream tasks that require different modalities: in each box, the upper sub-ﬁgure indicates a forward computa-\ntion; the lower sub-ﬁgure indicates squared self-attention mask, where tokens from each row have a weighted sum\nof columns that are not in white colors.\nbe used by video and text, respectively, aiming to\nlearn sequence-level features(Clark et al., 2019).\nThe [CLS] is disabled as no need to learn features\nacross video and text. After forwarding, all hidden\nstates of video and text tokens are average pooled,\nrespectively. Then we use a contrastive loss on\ntext-video similarity to discriminate a ground-truth\nvideo clip from other video clips in the same batch\nfor a given text clip. During the evaluation, to en-\nsure video and text are isolated (to avoid leaking\nground-truth of a similar pair), we split text and\nvideo and forward them separately. We report an\nablation study in Section 5 showing that the MMM\nintroduced in the previous section is crucial to en-\nsure that the pre-trained hidden states (for video or\ntext) are a good initialization for retrieval tasks.\n4.2 Action Segmentation\nAction segmentation is to assign each frame of a\nvideo with one of the pre-deﬁned labels. This is\nsimilar to the named entity recognition (NER) task\nin NLP but on video frames. We feed in VLM with\nthe whole video, a dummy text token, and an iso-\nlated attention mask. Then we add a classiﬁcation\nhead (with the number of pre-deﬁned labels) on top\nof the hidden states for each video token in the last\nlayer of VLM.\n4.3 Action Step Localization\nIn action step localization, each video belongs to\na task with multiple steps, where each step is de-\nscribed as a short text. Then each frame of a video\nneeds to be aligned with a step in text form. The\nchallenge for applying BERT to action step lo-\ncalization is similar to text-video retrieval: video\nframes need to be aligned with textual steps in\njoint space and it is almost impossible for pair-\nwise video/text matching because the number of\nframe/text pairs is large.\nSimilar to the text-video retrieval model, we also\napply isolated attention masks to video and text.\nThe major difference is that we pass video and\ntext separately to BERT. This is because the video\ncan be several minutes long (more than 100 to-\nkens) but the number of text labels for each video\nis ﬁxed (e.g. under 10). To keep the format of\nBERT being consistent for multi-modal inputs, we\nadd a dummy text token for video forwarding and\na dummy video token for text, respectively. For\na given frame(video token), we compute the dis-\ntribution of that frame over textual steps via dot\nproducts and the softmax function.\n4.4 Multiple-choice VideoQA\nMultiple-choice VideoQA (Yu et al., 2018) aligns\neach video with one out of several candidate an-\nswers in the text. The major difference between ac-\ntion step localization and multiple-choice VideoQA\nis that the video hidden state is not on frame-level\nbut sequence-level. We apply isolated attention\nmasks to BERT and forward video and text answers\n(with dummy tokens), respectively. Then the an-\nswer with the maximum similarity with the video is\nreported. During ﬁne-tuning, we apply contrastive\nloss on video-text similarity to rank answers.\n4.5 Video Captioning\nAnother big challenge of using a single encoder is\nhow to apply generative tasks (such as video cap-\ntioning) without pre-training an explicit decoder.\nWe observe that a transformer decoder (Vaswani\net al., 2017) has the following major differences\nfrom an encoder: (1) an auto-regressive loss that\ndoes not allow a text token to see future tokens;\n(2) a prediction head to generate texts. To resolve\n(1), one can easily ﬁne-tune the text segment of\nVLM as auto-regressive loss by passing in shifted\ntokens and a lower-triangle attention mask to the\ntext segment, as shown in Figure 3. To resolve\n(2), inspired by (Rothe et al., 2020; Zhou et al.,\n2020) that uses BERT as a decoder, one can re-use\nlanguage model heads as prediction heads for gen-\neration. Note that this setting has less architecture\ndesign than a standard transformer decoder (e.g.\nno explicit self-attention on text or cross-attention\non video). The implicit text decoder inside BERT\nshares self-attention with the video encoder so to\nsave the total number of parameters.\n5 Experiment\n5.1 Dataset\n5.1.1 Pre-training\nWe adopt the Howto100M dataset (Miech et al.,\n2019) for pre-training, which contains instructional\nvideos originally from YouTube via searching key-\nwords from wikihow ( www.wikihow.com). After\nﬁltering the unavailable ones, we get 1.1M videos.\nWe split 4000 videos as the validation set and the\nrest for pre-training. On average, the duration of\neach video is about 6.5 minutes with 110 clip-text\npairs. After removing repeated texts within over-\nlapped clips from ASR, we get about 7.7+ GB texts\nof captions, with 2.4 tokens per second on average.\n5.1.2 Fine-tuning\nMSR-VTT (Xu et al., 2016) is a popular dataset\nfor text-video retrievaland VideoQA. It has open\ndomain video clips, and each training clip has 20\ncaptioning sentences labeled by humans. There\nare 200K clip-text pairs from 10K videos in 20\ncategories, including sports, music, etc. Following\nJSFusion(Yu et al., 2018; Miech et al., 2019), we\nrandomly sampled 1,000 clip-text pairs as test data.\nWe further use the QA test data (Yu et al., 2018) as\nthe dataset for multiple-choice VideoQA.\nYoucook2 (Zhou et al., 2017) contains 2,000 cook-\ning videos on 89 recipes with 14K video clips from\nYouTube. The overall duration is 176 hours (5.26\nminutes on average). Each video clip is annotated\nwith one captioning sentence. Follow the split set-\nting in(Miech et al., 2019), we evaluate both text-\nbased video retrieval and multimodal video caption-\ning tasks. We ﬁlter the data and make sure there\nis no overlap between pre-training and evaluation\ndata. After ﬁltering out unavailable ones, we have\n9,473 training clip-text pairs from 1222 videos and\n3,305 test clip-text pairs from 430 videos.\nCOIN (Tang et al., 2019) are leveraged to evaluate\naction segmentation. It has 11,827 videos (476\nhours) and each video is labeled with 3.91 step\nsegments on average and 46,354 segments in total.\nThere are 778 step labels, plus one background\n(Outside) label. Since one video can last for several\nminutes that are much longer than the maximum\nlength of the video segment of VLM. We apply a\nsliding window with step size 16 and window size\n32. During inference, we average the logits for\noverlapped frames from multiple windows.\nCrossTask (Zhukov et al., 2019) is a dataset for\naction localization that contains 83 different tasks\nand 4.7k videos. Each task has a set of steps with\ntext descriptions annotated on temporal frames of\nthe video. We use the testing data split via the ofﬁ-\ncial code2, which contains annotated 1690 videos.\nThe rest of the 540 annotated videos are used for\nweakly supervised training.\n5.2 Hyper-parameters\nWe extract video tokens from video frames using\nthe S3D encoder pre-trained from (Miech et al.,\n2020). The fps is 30 and we extract one (1) video\ntoken per second with the dimension of 512. We\napply an MLP to transform such 512 dimensions\nto the hidden size (768) of BERTBASE.\nFollowing (Luo et al., 2020), we adopt\nBERTBASE (uncased) as our base model and\ntuned directly from BERT’s weights, so all hyper-\nparameters are the same as the original BERT. The\nmaximum length of BERT is set as 96, where 32\n2https://github.com/DmZhukov/CrossTask\nModel Paradigm #params. #loss #unimodal/cross en/decoder Joint Retrieval GenerationMMT(Gabeur et al., 2020) task-speciﬁc alignment 127.3M 1 2/0/0 yes noActBERT(Zhu and Yang, 2020) weakly supervised/MTL n/a (3 typed attentions) 4 0/1(modal-typed attn.)/0 no(pair) extra decoderVideoAsMT(Korbar et al., 2020) weakly supervised/MTL 286M(base)/801M(large) 1 1/1/1 no (gen.) yesHERO(Li et al., 2020b) SSL(w/ sup. video feat.)/MTL 159M 5 1(query)/2/0 no(pair) extra decoderUniVL(Luo et al., 2020) SSL/MTL 260M 5 2/1/1 yes yesVLM SSL/Task-agnostic 110M 1 0/1/0(shared w/ encoder) yes yes\nTable 1: Comparison of pre-trained models on learning paradigms (SSL means self-supervised learning; MTL\nmeans multi-task learning), number of parameters (# params.), number of losses (#loss), number of unimodal/cross-\nmodal encoders/decoders, and whether to support retrieval in joint space(joint retrieval) and text generation. Types\nand numbers are estimated based on released code or papers: exceptions are in parenthesis (e.g. pair means\npairwise matching using [CLS]). VLM is extremely simple with fewer parameters and limitations.\ntokens are for videos and the rest tokens are for\ntext and special tokens. Remind that texts are 2.4\ntokens per second and video tokens are 1 token\nper second. We form a text clip with a random\nlength in-between 8 and 64 text tokens and col-\nlect the corresponding video clip to form a training\nexample. We randomly sample 32 video/text clip\npairs from each video and use 8 videos to form a\nbatch of size 256. Each training example has 50%\nchance for MMM (25% for whole video masking\nand 25% for whole text masking) and 50% chance\non MFM-MLM (with 15% probability of video and\ntext token masking).\nWe pre-train VLM on 8 NVIDIA Tesla V100\nGPUs (each with 32 GB memory) for 15 epochs\nusing fp16 for one (1) day. Following (Liu et al.,\n2019), we choose Adam (Kingma and Ba, 2014)\noptimizer with initial learning rate of 5e-5 (with\nbetas as (0.9, 0.98)), 1000 steps of warm-up and a\npolynomial decay learning rate scheduler. Gradi-\nents are clipped with 2.0. All ﬁne-tuning tasks use\nthe same hyper-parameters as pre-training except\nthe number of warm-up steps is 122.\n5.3 Model Comparison\nWe ﬁrst investigate the design choices of VLM com-\npared to other transformer-based multimodal pre-\ntraining baselines. As shown in Table 1, we collect\ntraining paradigms, model sizes, etc. of these mod-\nels (estimated based on their source codes or pa-\npers). VLM is signiﬁcantly smaller than other mod-\nels since it is just a BERTBASE (uncased), while it\nis still fully self-supervised, task-agnostic (e.g. no\ntraining on retrieval or auto-regressive style tasks)\nand supports joint retrieval and text generation.\n5.4 Quantitative Analysis\nWe investigate the performance of VLM on ﬁne-\ntuning tasks with very basic setups (e.g. no aug-\nmented features, large LMs, optimized losses for\nparticular tasks). Note that it could be hard for\nMethods R@1 R@5 R@10 Median R\nRandom 0.1 0.5 1.0 500\nTask-speciﬁc Alignment Pre-trainingMMT (Gabeur et al., 2020) 25.8 57.2 69.3 4\nPairwise MatchingActBERT(Zhu and Yang, 2020) 8.6 23.4 33.1 36VideoAsMT(Korbar et al., 2020) 14.7 - 52.8 -\nMulti-task Pre-trainingHERO (Li et al., 2020b) 16.80 43.40 57.70 -UniVL (FT-Joint) (Luo et al., 2020) 20.6 49.1 62.9 6\nVLM 28.10 55.50 67.40 4\nTable 2: Results of text-video retrieval on MSR-VTT\ndataset.\nMethods R@1 R@5 R@10 Median R\nRandom 0.03 0.15 0.3 1675\nTask-speciﬁc Alignment Pre-trainingCoot(Ging et al., 2020) 16.7 40.2 52.3 9\nPairwise MatchingActBERT(Zhu and Yang, 2020) 9.6 26.7 38.0 19VideoAsMT(Korbar et al., 2020) 11.6 - 43.9 -\nMulti-task Pre-trainingUniVL (FT-Joint)(Luo et al., 2020) 22.2 52.2 66.2 5\nVLM 27.05 56.88 69.38 4\nTable 3: Results of text-based video retrieval on\nYoucook2 dataset.\nfair comparisons between task-agnostic and task-\nspeciﬁc approaches. We list other baselines by type\nand our goal is a simple baseline for task-agnostic\npre-training as better initialization of strongly per-\nformed ﬁne-tuning models.\nText-video Retrieval We use MSR-VTT and\nYoucook2 to evaluate the performance on text-\nvideo retrieval. The results are shown in Table 2\nand 3, respectively. VLM achieves good perfor-\nmance on these two datasets, indicating that the\nMMM and isolated self-attention mask can be\nused together for joint retrieval. Ablation study\nshows that using an isolated self-attention mask\nalone does not yield good performance, indicating\nMMM is very important to learn features for align-\nment. Note that our pre-training is task-agnostic\nbut still outperforms baselines with retrieval style\npre-training.\nAction Segmentation We report the results of ac-\ntion segmentation on COIN dataset in Table 4.\nMethod Frame Accuracy\nNN-Viterbi (Richard et al., 2018) 21.17\nVGG (Simonyan and Zisserman, 2014) 25.79\nTCFPN-ISBA (Ding and Xu, 2018) 34.30\nCBT (Sun et al., 2019a) 53.90\nMIL-NCE (Miech et al., 2020) 61.00\nActBERT (Zhu and Yang, 2020) 56.95\nVLM 68.39\nTable 4: Action segmentation on COIN dataset.\nMethods Average Recall\nJoint Alignment\nAlayrac (Alayrac et al., 2016) 13.3\nZhukov (Zhukov et al., 2019) 22.4\nSupervised (Zhukov et al., 2019) 31.6\nHowTo100M (Miech et al., 2019) 33.6\nMIL-NCE (Miech et al., 2020) 40.5\nUniVL (Luo et al., 2020) 42.0\nPairwise Matching\nActBERT (Zhu and Yang, 2020) 41.4\nVLM (task-agnostic, zero-shot) 28.5\nVLM (supervised on 540 videos) 46.5\nTable 5: Action step localization results on CrossTask.\nVLM outperforms other baselines indicating its\ngood token-level video representations. Note that\nthis task only tests the hidden states of the video in-\ndicating the unimodal encoding capability of VLM\nis not compromised.\nAction Step Localization We setup two (2) evalu-\nations for the CrossTask dataset. First, we evaluate\nthe zero-shot transfer of VLM. Note that existing\nstudies evaluate Crosstask with retrieval/alignment\nstyle pre-training, where the aligned hidden states\nare directly used for action step localization. Our\ntask-agnostic pre-training derives an even harder\nproblem: applying hidden states learned from\nproxy tasks on video frame/text alignment for ac-\ntion step localization without explicitly training on\nalignment. We simply use the hidden states from\nthe last layer of VLM for video/text representation\nand directly compute the similarities between video\nframes and text descriptions. Surprisingly, the per-\nformance is better than some baselines and closer\nto one supervised method. This indicates masked\ntoken loss together with MMM can learn certain\nvideo-text alignments in joint space. Second, we\nuse just 540 videos for weakly supervised training\nand we get a much better result.\nVideo Question Answering We use MSR-VTT\nQA to evaluate multiple-choice question answer-\ning. Recall that this task essentially tests video-text\nsimilarity. The performance of VLM is better than\nMethod Accuracy\nJoint Retrieval\nJSFusion(Yu et al., 2018) 83.4\nPairwise Matching\nActBERT(Zhu and Yang, 2020) 85.7\nVLM 91.64\nTable 6: Video question answering (multiple-choices)\nevaluated on MSR-VTT.\nMethods B-3 B-4 M R-L CIDErExtra DecoderVideoBERT (Sun et al., 2019b) 6.80 4.04 11.01 27.50 0.49CBT (Sun et al., 2019a) - 5.12 12.97 30.44 0.64ActBERT (Zhu and Yang, 2020) 8.66 5.41 13.30 30.56 0.65Coot(Ging et al., 2020) 17.62 11.09 19.34 37.63 -w/ Pre-trained DecoderVideoAsMT (Korbar et al., 2020) - 5.3 13.4 - -UniVL (Luo et al., 2020) 16.46 11.17 17.57 40.09 1.27VLM 17.78 12.27 18.22 41.51 1.3869\nTable 7: Video captioning results on Youcook2 dataset.\nActBERT, which leverages pairwise matching for\neach video/answer pair.\nVideo Captioning We lastly evaluate VLM on\nvideo captioning with autoregressive attention\nmask with other baselines that have an explicit text\ndecoder. As shown in Table 7, our “compact” de-\ncoder using BERT’s LM heads is surprisingly good\nat video captioning compared to other ﬁne-tuning\nbaselines with external decoders (e.g. Coot). This\nindicates that it is possible to remove an explicit\ndecoder and sharing weights between video and\ntext tokens.\n5.4.1 Ablation Study\nWe use Youcook2 as the base task for the ablation\nstudy on text-retrieval and video captioning. We are\ninterested in the following study: (1) percentage of\nexamples for MMM (w/ MMM x%); (2) minimum\nlength of text tokens, where the length of video\nwill be determined by the start/end timestamps of\ntext tokens; (3) performance of LVLM (Equation 8).\nThe results are shown in Table 8 and Table 9.\nEffects of MMM Without MMM (w/ MMM 0%,\nor MFM-MLM 100%), the performance signiﬁ-\ncantly dropped. This indicates that a naive adoption\nof traditional MFM-MLM masking may not learn\njoint video/text representations well, as indicated\nby both retrieval and captioning task. We suspect a\nmasked token is more likely predicted from tokens\nof the same modality. We further try MMM with\ndifferent probabilities (30% or 70%) and 50% is\nthe best.\nMinimum Length of Texts The length of a clip\ncan be important for retrieval tasks (Miech et al.,\n2020). We ran VLM on longer (at least 16 text to-\nkens) video/text pairs. The performance is slightly\ndropped, indicating pre-training on longer clips\nmay not cover ﬁne-tuning tasks with short clips.\nEffects of Masked Token Loss We notice that us-\ning multi-task style loss LMFM-MLM may reduce\nthe performance. This indicates learning a masked\ntoken from both video/text tokens can help.\nVLM R@1 R@5 R@10 Median R\nw/ MMM 50% 27.05 56.88 69.38 4.0\nw/ MMM 0% 15.12 39.47 52.81 9.0\nw/ MMM 30% 25.30 54.80 68.96 4.0\nw/ MMM 70% 25.17 54.98 69.11 4.0\nw/ min. 16 text tokens 25.84 54.43 68.29 5.0\nw/LMFM-MLM 26.93 55.92 69.86 4.0\nTable 8: Ablation study of VLM for text-based video\nretrieval on Youcook2.\nVLM B-3 B-4 M R-L CIDEr\nw/ MMM 50% 17.78 12.27 18.22 41.51 1.3869\nw/ MMM 0% 15.47 10.54 16.49 38.83 1.2163\nw/ MMM 30% 16.57 11.30 17.55 40.76 1.3215\nw/ MMM 70% 16.94 11.68 17.67 41.24 1.3739\nw/ min. 16 text tokens 17.25 12.00 17.67 40.62 1.3076\nw/LMFM-MLM 16.66 11.53 17.34 40.36 1.3224\nTable 9: Ablation study of VLM for video captioning\non Youcook2 dataset.\n5.5 Qualitative Analysis\n5.5.1 Error Analysis\nText-video retrieval. We use MSR-VTT as the\ndataset for error analysis on text-video retrieval,\nas shown in Table 10 of Appendix. We pair the\nquery text with the text of the top-1 ranked video\nto show 100 errors in ranking since video tokens\nare harder to present. We observe the following\ntypes of errors in video understanding: (1) objects\nsometimes are hard to recognize such as dog or cat;\n(2) attributes of objects may be hard to match the\ntext, e.g. gender, ages, etc. (3) subtle differences\nof actions; (4) speciﬁc videos for a general query\nor vice versa, e.g. people vs basketball player. We\nbelieve the last type may not be errors but hard for\nexisting annotations or evaluations to separate.\nVideo Captioning. We further examine the gen-\nerated text from video captioning. Note that our\nvideo captioning has no support from ASR or tran-\nscript so the video is the only source to generate\ntext content and errors of video understanding can\neasily be reﬂected in the text. From Table 11 of\nAppendix, we notice that one major type of error\nis from objects of similar shapes and colors, e.g.\nonion rings vs shrimp.\n5.5.2 Visualization\n. We observe that video tokens take the majority of\nspace while text tokens are rather clustered together.\nThis is probably because videos from the physical\nworld are more diverse and sparse than text from a\nﬁxed vocabulary.\nWe plot the self-attention of VLM layers within\nand in-between each modality, as in Figure 4 of\nAppendix. We observe the following patterns from\nall 144 attention heads:\n• Unlike LMs, there are no recurrent (shifted)\nposition-wise patterns for video tokens;\n• Self-attentions in the 1st layer are more di-\nverse than later layers. This suggests that ex-\nisting video encoders might be too deep for\ntransformers;\n• Some attention heads show patterns of cross-\nmodal mapping in-between video and text (e.g.\nsub-ﬁgure (a));\n• Word-level cross-modal co-reference: video\ntokens with pouring soy saucerefers to the\ntext token of “soy” (e.g. sub-ﬁgure (b));\n6 Conclusions\nWe presented a task-agnostic pre-training with new\nmasking schemes that enable the training of a sin-\ngle masked language model that can accept ei-\nther video or text input, or both. We showed that\nthis simple VLM model can be effectively tuned\nfor a broad range of downstream tasks, such as\ntext-video retrieval and video captioning via dif-\nferent types of attention masks. Experimental re-\nsults show that the proposed methods maintain\ncompetitive performance while requiring a signiﬁ-\ncantly smaller number of parameters than compet-\ning methods.\nAcknowledgments\nWe thank Huaishao Luo (author of UniVL(Luo\net al., 2020)), Mandela Patrick (author of Support-\nSet(Patrick et al., 2021)) and Luowei Zhou (author\nof Youcook(Zhou et al., 2017)) for supports of base-\nline setup.\nReferences\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant\nAgrawal, Josef Sivic, Ivan Laptev, and Simon\nLacoste-Julien. 2016. Unsupervised learning from\nnarrated instruction videos. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 4575–4583.\nJean-Baptiste Alayrac, Adri `a Recasens, Rosalia\nSchneider, Relja Arandjelovi ´c, Jason Ramapuram,\nJeffrey De Fauw, Lucas Smaira, Sander Dieleman,\nand Andrew Zisserman. 2020. Self-supervised\nmultimodal versatile networks. arXiv preprint\narXiv:2006.16228.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104–120. Springer.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n276–286.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLi Ding and Chenliang Xu. 2018. Weakly-supervised\naction segmentation with iterative soft boundary as-\nsignment. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n6508–6516.\nValentin Gabeur, Chen Sun, Karteek Alahari, and\nCordelia Schmid. 2020. Multi-modal transformer\nfor video retrieval. In European Conference on Com-\nputer Vision (ECCV), volume 5. Springer.\nSimon Ging, Mohammadreza Zolfaghari, Hamed Pir-\nsiavash, and Thomas Brox. 2020. Coot: Coopera-\ntive hierarchical transformer for video-text represen-\ntation learning. arXiv preprint arXiv:2011.00597.\nPo-Yao Huang, Mandela Patrick, Junjie Hu, Gra-\nham Neubig, Florian Metze, and Alexander G.\nHauptmann. 2021. Multilingual multimodal pre-\ntraining for zero-shot cross-lingual transfer of vision-\nlanguage models. CoRR, abs/2103.08849.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBruno Korbar, Fabio Petroni, Rohit Girdhar, and\nLorenzo Torresani. 2020. Video understand-\ning as machine translation. arXiv preprint\narXiv:2006.07203.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin\nJiang, and Ming Zhou. 2020a. Unicoder-vl: A uni-\nversal encoder for vision and language by cross-\nmodal pre-training. In AAAI, pages 11336–11344.\nLinjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan,\nLicheng Yu, and Jingjing Liu. 2020b. HERO:\nHierarchical encoder for Video+Language omni-\nrepresentation pre-training. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2046–2065,\nOnline. Association for Computational Linguistics.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. In Arxiv.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, volume 32, pages 13–23. Curran Asso-\nciates, Inc.\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\nDuan, Tianrui Li, Xilin Chen, and Ming Zhou. 2020.\nUnivilm: A uniﬁed video and language pre-training\nmodel for multimodal understanding and generation.\narXiv preprint arXiv:2002.06353.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira,\nIvan Laptev, Josef Sivic, and Andrew Zisserman.\n2020. End-to-end learning of visual representations\nfrom uncurated instructional videos. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9879–9889.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\n2019. Howto100m: Learning a text-video embed-\nding by watching hundred million narrated video\nclips. In Proceedings of the IEEE international con-\nference on computer vision, pages 2630–2640.\nMandela Patrick, Po-Yao Huang, Yuki Asano, Florian\nMetze, Alexander G Hauptmann, Joao F. Henriques,\nand Andrea Vedaldi. 2021. Support-set bottlenecks\nfor video-text representation learning. In Interna-\ntional Conference on Learning Representations.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlexander Richard, Hilde Kuehne, Ahsan Iqbal, and\nJuergen Gall. 2018. Neuralnetwork-viterbi: A\nframework for weakly supervised video learning. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7386–7395.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Transactions of the Asso-\nciation for Computational Linguistics, 8:264–280.\nAndrew Rouditchenko, Angie Boggust, David Har-\nwath, Dhiraj Joshi, Samuel Thomas, Kartik Au-\ndhkhasi, Rogerio Feris, Brian Kingsbury, Michael\nPicheny, Antonio Torralba, et al. 2020. Avl-\nnet: Learning audio-visual language representa-\ntions from instructional videos. arXiv preprint\narXiv:2006.09199.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn International Conference on Learning Represen-\ntations.\nChen Sun, Fabien Baradel, Kevin Murphy, and\nCordelia Schmid. 2019a. Contrastive bidirectional\ntransformer for temporal representation learning.\narXiv preprint arXiv:1906.05743, 3(5).\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019b. Videobert: A\njoint model for video and language representation\nlearning. In Proceedings of the IEEE International\nConference on Computer Vision, pages 7464–7473.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.\n2019. Coin: A large-scale dataset for comprehen-\nsive instructional video analysis. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1207–1216.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu,\nand Kevin Murphy. 2018. Rethinking spatiotempo-\nral feature learning: Speed-accuracy trade-offs in\nvideo classiﬁcation. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 305–\n321.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 5288–5296.\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. 2018.\nA joint sequence fusion model for video question\nanswering and retrieval. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV),\npages 471–487.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J Corso, and Jianfeng Gao. 2020. Uni-\nﬁed vision-language pre-training for image caption-\ning and vqa. In AAAI, pages 13041–13049.\nLuowei Zhou, Chenliang Xu, and Jason J Corso.\n2017. Towards automatic learning of procedures\nfrom web instructional videos. arXiv preprint\narXiv:1703.09788.\nLuowei Zhou, Yingbo Zhou, Jason J Corso, Richard\nSocher, and Caiming Xiong. 2018. End-to-end\ndense video captioning with masked transformer. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8739–8748.\nLinchao Zhu and Yi Yang. 2020. Actbert: Learning\nglobal-local video-text representations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 8746–8755.\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gok-\nberk Cinbis, David Fouhey, Ivan Laptev, and Josef\nSivic. 2019. Cross-task weakly supervised learn-\ning from instructional videos. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3537–3545.\nQuery Text of Top-1 videoObjects (26%)cartoonshow for kids pokemon video gameplaylittle pet shopcatgetting a bath and washed with little brushseveraldogsplaying deadAttributes of Objects (6%)a littleboysinging in front of judges and crowd awomansinging on the voicea woman is mixingfoodin a mixing bowl a man is stirringsomethingin a potAction (6%)a person isconnecting somethingto system a manlooks atthebatteryof a computera boyplaysgrand theft auto 5 a narratorexplains where to ﬁnda rare vehicle in grand theft autoa man isgiving a reviewon a vehicle a person isdiscussinga caranaked childruns through a ﬁeld thegirlshows theboysher medal in this cartoona man is singing and standing in the road a man in sunglasses and a blue shirt beat boxesSpeciﬁc vs General (62%)some cartoon characters are moving around an areaa cartoon girl and animal jumping on body of male guy girl image still shown displaying on screenbaseball playerhits ball peopleare playing baseballthe man in the video is showing a brief viewing of how the movie is startingscrolling the the menu of movieclips with different movie trailersastudentexplains to histeacherabout the sheep of another studentthere is aguytalking to hisfathera video about different sports a woman talks about horse racing\nTable 10: Error analysis for text-video retrieval of MSR-VTT on 100 errors: we group errors in four (4) categories:\nobjects, attributes of objects, actions, and speciﬁc vs general. Speciﬁc videos for general queries (or vice versa)\nsometimes may not be errors but hard to evaluate.\nHypothesis Reference\nadd the lamb to thepan add the lamb to thepot\nadd the cilantrocilantro and lime juice to the potcut the cilantro and lime\nadd theonionsto a pot of water addﬂourto the pot and stir\ndip theonion ringsinto the batter dip theshrimpin the batter\nadd water to the bowl and mix pour water into theﬂour mixtureand mix\nremove themusselsfrom the pot once theshrimpsare defrosted drain the water\nadd the sauce to thepanand stir add the sauce to thewokand stir\nadd lemon juice to the pan and stir addrice vinegarand lemon juice to the pan and stir\nadd the beef to the pan and stir add the diced beef meat to it and roast it\nTable 11: Error analysis for video captioning on Youcook2: VLM tends to make mistakes in recognizing objects\nof similar shapes and colors to generate the wrong text.\n(a) Layer 1, Head 1\n(b) Layer 1, Head 5\nFigure 4: Self-attention for video HfIeQ9pzL5U from 4:03 to 4:28: darker color indicates higher weights; v0-v24\nare video tokens of 25 seconds."
}