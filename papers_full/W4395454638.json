{
  "title": "How do Generative Language Models Answer Opinion Polls?",
  "url": "https://openalex.org/W4395454638",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5031849090",
      "name": "Julien Boelaert",
      "affiliations": [
        "Centre d'Etudes et de Recherches Administratives, Politiques et Sociales"
      ]
    },
    {
      "id": "https://openalex.org/A5089967997",
      "name": "Samuel Coavoux",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A5022704573",
      "name": "Étienne Ollion",
      "affiliations": [
        null,
        "École Polytechnique"
      ]
    },
    {
      "id": "https://openalex.org/A5008741341",
      "name": "Ivaylo D. Petev",
      "affiliations": [
        "Centre de Recherche en Économie et Statistique",
        "École Nationale de la Statistique et de l'Administration Économique"
      ]
    },
    {
      "id": "https://openalex.org/A5095890261",
      "name": "Patrick Präg",
      "affiliations": [
        "Centre de Recherche en Économie et Statistique",
        "École Nationale d'Administration"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4387947082",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4394750383",
    "https://openalex.org/W4386981810",
    "https://openalex.org/W4376504550",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4372323206",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4361865995",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4380715567",
    "https://openalex.org/W4386499643",
    "https://openalex.org/W2559840796",
    "https://openalex.org/W2085876742",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W4223578676",
    "https://openalex.org/W4377009982",
    "https://openalex.org/W2068181924",
    "https://openalex.org/W4392376454",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4287658197",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4379662558",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W4391090624",
    "https://openalex.org/W6851152545",
    "https://openalex.org/W4367628401",
    "https://openalex.org/W2895470363",
    "https://openalex.org/W2294193807",
    "https://openalex.org/W4389850390",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W2764072425",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W3141591825",
    "https://openalex.org/W4402669926",
    "https://openalex.org/W4386721654",
    "https://openalex.org/W4397011890",
    "https://openalex.org/W4390947766",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W4396781199",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4402671261",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4313559133"
  ],
  "abstract": "Generative AI is increasingly presented as a potential substitute for humans, including as human research subjects in various disciplines. Yet there is no scientific consensus on how closely these in-silico clones could represent their human counterparts. While some defend the use of these “synthetic users,” others point towards the biases in the responses provided by the LLMs. Through an experiment using survey questionnaires, we demonstrate that these latter critics are right to be wary of using generative AI to emulate respondents, but probably not for the right reason. Our results i) confirm that to date, models cannot replace research subjects for opinion or attitudinal research; ii) that they display a strong bias on each question (reaching only a small region of social space); and iii) that this bias varies randomly from one question to the other (reaching a different region every time). Besides the two existing competing theses (“representativity” and “social bias”), we propose a third one, which we call call “machine bias”. We detail this term and explore its consequences, for LLM research but also for studies on social biases.",
  "full_text": "Machine Bias\nHow Do Generative Language Models Answer\nOpinion Polls?∗\nJulien Boelaert1, Samuel Coavoux2, Étienne Ollion2, Ivaylo\nPetev2, and Patrick Präg2\n1CERAPS, University of Lille, France\n2Center for Research in Statistics and Economics (CREST),\nENSAE, CNRS, École Polytechnique, Institut Polytechnique de\nParis, France\nMarch 3, 2025\nAbstract\nGenerative AI is increasingly presented as a potential substitute\nfor humans, including as research subjects. However, there is no\n∗The alphabetical order matches the authors’ contributions.\nPrevious versions of this study were presented at the ‘Using LLMs and Text-as-Data in\nPolitical Science Research’ workshop in Barcelona, the ‘Generative Artificial Intelligence\nand Sociology’ workshop at Yale, and the participants of the Digital Humanities Lab at\nEPFL. We thank the participants for their useful feedback. We are also grateful to Delia\nBaldassari, Gaël Le Mens, Charles Rahal, Felix Elwert, the editors of the special issue and\n1\nscientific consensus on how closely these in silico clones can emulate\nsurvey respondents. While some defend the use of these “synthetic\nusers,” others point toward social biases in the responses provided by\nLarge Language Models (LLMs). In this article, we demonstrate that\nthese critics are right to be wary of using generative AI to emulate\nrespondents, but probably not for the right reasons. Our results show\ni) that to date, models cannot replace research subjects for opinion\nor attitudinal research; ii) that they display a strong bias and a low\nvariance on each topic; and iii) that this bias randomly varies from\none topic to the next. We label this pattern “machine bias,” a concept\nwe define, and whose consequences for LLM-based research we further\nexplore.\nINTRODUCTION\nThereleaseofChatGPTinNovember2022hasthrustdebatesaboutgenerative\nartificial intelligence (AI) onto the public stage. Our collective imagination has\nbeen running wild ever since. Optimistic visions of streamlined, accelerated\nwork and life are met with fears of omnipotent artificial intelligence and of\nmass disappearance of jobs. Social scientists have seized the moment to discuss\nthe implications of these technologies on various occupations (Brynjolfsson\net al., 2023), including their own work (Ziemset al., 2024). Generative AI\nholds a promise to change the way we conduct social science research (Bail,\nthe anonymous reviewers ofSociological Methods and Researchfor insightful comments.\nThis work is supported by a grant from the Agence Nationale de la Recherche (Labex\nEcodec, TSIA Pantagruel), a Hi! Paris Collaborative Project research grant, and funding\nfrom the Swedish Vetenskapsrådet (‘Mining for Meaning’). We acknowledge the national\ninfrastructure Huma-Num for providing us access to their servers, and Gérald Foliot who\nhelped us navigate their intricate architecture. We are also grateful to Annina Claesson for\n2\n2024). It could perform literature reviews, annotate documents, improve\ncomputer programming, generate research hypotheses, and even emulate\nhuman behavior in studies.\nIn the present study we focus on this latter aspect, and more specifically on\nthe capacity of large language models (LLMs) to simulate human populations\nfor opinion research. Surveys are an essential tool in social science research,\nas well as in commercial marketing, political campaigning, forecasting, etc.\nBut high-quality surveys—robust questionnaire design, repeated measures,\nrepresentative samples—demand advanced skills, take time, and are costly.\nThey are also in crisis. Survey research response rates have been declining\nfor decades (to as low as 1 percent in the polls for the 2024 US presidential\nelection) (Williams and Brick, 2018), which to some extent stems from the\ndecline of landline phones and associated problems of drawing random samples\n(Dutwin and Buskirk, 2021), and issues of motivating participants to fill in\nquestionnaires online (Daikeleret al., 2019). Massive cohort study endeavors\nsuch as the US National Children’s Study and the UK Life Study had to be\ncanceled in the last decade due to lack of volunteer participants (Pearson,\n2015). These low response rates hinder their representativity, especially for\nhard-to-reach parts of the population.\nAcademics have started to explore the potential of generative AI, more\nspecifically LLMs, to generate human-like responses. The argument is the\nfollowing: because they are trained on massive volumes of human-produced\ndata, LLMs could, if prompted properly and when provided with the right\ncontext, replicate the answers actual humans would give. They would also\ndo so for a fraction of the time and cost associated with standard surveys.\nexcellent copy-editing.\n3\nAlthough most have remained cautious in their conclusions and admitted\nthat there is uncertainty as to LLMs’ abilities, hopes were high and pointed\nto a possible replacement of human subjects.1\nSome commercial actors were not so prudent and immediately followed suit\nby offering solutions to complement or replace human samples.2 It is therefore\nimportant to verify these claims: should LLMs manage to imitate humans,\nif only in part, the consequences for research and marketing alike would\nbe immense. We label the idea that LLMs can accurately simulate human\npopulations as the “representative hypothesis.”\nDespite, or maybe because of this bold promise, many scholars started\nquestioning the direct use of LLMs as research subjects (Santurkaret al., 2023;\nDominguez-Olmedo et al., 2024; Wanget al., 2024a). Their findings pointed\nto a mismatch between results generated by a machine and responses provided\nby humans. Several blamed the ingrained social bias of LLMs. Trained on\ndata that are disproportionately produced by geographically and socially\nsituated populations, LLMs tend, they argued, to propagate the point of view\nof these social groups. This line of reasoning echoes a now classic finding\nabout the algorithmic discrimination of minority groups (O’Neil, 2016). Just\nlike widely used predictive tools based on statistical learning (Buolamwini and\nGebru, 2018), LLMs could harbor massive social biases (Benderet al., 2021).\nTogether, these elements cast a shadow over LLMs’ ability to effectively mimic\ndiverse human populations in survey simulations. While they might represent\n1In a now classic paper, the authors wrote “we propose that these [large-scale generative\nlanguage models] can be used as surrogates for human respondents in a variety of social\nscience tasks” (Argyleet al., 2023, 337).\n2One company claims it can produce “synthetic boosters to gain reliability in under-\nsampled groups,” another one “draws on the growing body of research demonstrating\nthe capacity of AI to simulate surveys, experiments and other empirical studies,” and a\nthird one promises “to create accurate synthetic interviews and surveys” (all citations were\ncollected in the Fall of 2024).\n4\nsome groups properly, they may be failing others. We label this view the\n“social bias hypothesis.”3\nIn this article, we propose a third alternative. Based on a targeted experiment,\nour results show that LLMs do not accurately represent subpopulations, and\nthus cannot generally serve as in silico research subjects for survey research.\nAt least, current models cannot do so in the classical “zero-shot” setting\nadvocated by some.4 The reason, however, is not primarily social bias. Our\nexperiment shows that current LLMs not only fail at accurately predicting\nhuman responses, but that they do so in ways that are unpredictable, as they\nfavor different social groups from one question to another. Moreover, their\nanswers exhibit a substantially lower variance between subpopulations than\nwhat is found in real-world human data. We call this theory the “machine\nbias” hypothesis, according to which LLM errors do not stem from imbalanced\ntraining data.\nThearticleproceedsasfollows. Wefirstreviewtheexistingempiricalworkthat\nuses LLMs to replicate human subjects, both in the human and social sciences\nand in computer science. We especially show that some empirical elements\nin favor of the machine bias hypothesis have been found in previous studies,\nbut that they were not fully theorized. Second, we describe the experiment\nwe designed to adjudicate between the three hypotheses, based on a “silicon\nsampling”task, askingseveralsurveyquestionstoalanguagemodel. Third, we\nreport our results. We provide evidence against the representative and social\n3It is interesting to note that the representative and the social bias hypotheses share an\nunderlying assumption, namely that these models accurately mimic at least some human\nattitudes and behaviors. They disagree on the extent to which LLMs reflect social diversity:\ncan they mimic all social groups (representative), or only a few (social bias)?\n4In LLM jargon, “zero-shot” refers to the practice of asking the LLM to generate answers\nwithout giving it examples of correct answers first. Giving such prior examples is called\n“few-shot.” Yet another way to make LLMs learn from prior data is to fine-tune them on a\ntraining dataset, which is computationally more costly.\n5\nbiashypotheses. WethendemonstratethatLLMshaveanarrowsetofpossible\nanswer distributions, centered around an arbitrary tendency that varies across\nmodels and questions, and that is not consistently correlated with specific\nsocial groups. These results are robust to model choice (GPT, Llama, Mixtral),\ndata-generation strategies, prompting strategies, measurement strategies,\ncountries, and periods. We finally discuss the methodological and theoretical\nimplications of our findings.\nLITERATURE REVIEW\nCan LLMs imitate (at least some) humans?\nOver the past two years, studies into the potential use of GPT and other\nLLMs for scientific purposes have multiplied. In the social sciences, one heated\ndebate has focused on their ability to mimic human subjects. The promise is\nalluring. If properly prompted, generative LLMs could help survey hard-to-\nreach populations, run pilot surveys prior to large-scale release, reduce the cost\nof experiments, or help impute missing data. Across disciplines, researchers\nhave demonstrated the value of such an approach, often in stimulating ways.\nIn psychology, Dillionet al.(2023) presented ChatGPT with moral scenarios\nand asked to rate them on a scale from “very immoral” to “very moral.” The\ncomparison of its responses to those of human interviewees in four surveys\nyielded a very high correlation coefficient. In economics, Horton (2023) put\nGPT 3 to the test in four rationality experiments and came to the conclusion\nthat “homo silicus” bears strong similarities to “homo œconomicus.” In a\ncross-disciplinary comparison, Aheret al.(2023) subjected GPT to a series\nof experiments drawn from linguistics, behavioral economics, and social\npsychology to conclude, once more, that generative AI is capable of emulating\n6\nhuman behavior (see also Ashokkumaret al., 2024, who replicated 70 survey\nexperiments). This is also the conclusion reached by (Kozlowskiet al., 2024),\nwho write that LLMs could create “digital doubles”, opening up a path for\nan “in silico sociology”. This list is by no means exhaustive.\nOne study in particular has captured the imagination of social scientists.\nArgyle et al.(2023) prompted GPT to predict political opinions and voting\nbehavior of various American socio-demographic groups. The results show a\nstrong correlation between the predictions of the models and observed data.\nBased on this experiment, the authors proposed that generative LLMs can\nmimic the behavior of specific social groups. They called this capacity of\nLLMs “algorithmic fidelity,” and the simulated data “silicon samples:” ”Our\nstudies show that the same language model, when properly conditioned, is\nable to produce outputs biased both toward and against specific groups and\nperspectives in ways that strongly correspond with human response patterns\nalong fine-grained demographic axes.” (Argyle et al., 2023, 338).\nMany objected. In an article directly conversing with Argyleet al.(2023),\nBisbee et al.(2024) demonstrated the existence of an important divergence\nbetween LLMs’ and human responses when asked to express their feelings\ntoward certain groups. They also show that the quality of feeling thermometer\nprediction drops when certain variables (such as party identification and\npoliticalideology)arenotprovidedtothemodel. Inonelarge-scaleexperiment,\ncomputer scientists found that a mismatch between the answers of various\nLLMs and US respondents was the rule more than the exception, and tended\nto favor “lower income, moderate, and Protestant or Roman Catholic groups”\n(Santurkar et al., 2023). Similarly, von der Heydeet al. (2023) concluded\nfrom their study of LLM’s answers to political questions in Germany that the\nmodel they tested leaned toward the views of certain parties.\n7\nThese results are in line with a number of articles that have shown, outside\nof silicon sampling tasks, that LLMs have significant biases. By this, their\nauthors mean that the responses of the models are skewed towards a certain\ncountry, one or a few social groups, or a given political ideology. In a\nlarge comparative study, Atariet al.(2023) found a near-perfect correlation\nbetween a WEIRDness scale for each country and the distance between\nthe LLM prediction and the actual human response. Probing ChatGPT,\nMotoki et al.(2024) found evidence of a political bias favoring liberal views\n(see also Rutinowski et al., 2024; Hartmann et al., 2023; Rozado, 2024).\nOthers confirmed this insight (Caoet al., 2023), while Johnsonet al.(2022)\nwittily noted that GPT-3 “has an American accent.” The conclusions, and\nespecially the methods used to determine the alignment of the model to a\ngiven population, vary, but all articles converge on one aspect: the models\nfavor certain groups.\nThis second line of argumentation has received a lot of support. This is, in\npart, a question of numbers. In this area, articles showing negative results are\nmore prevalent than research with conclusive evidence. This is also due to\nthe fact that it echoes an abundant literature on algorithmic discrimination.\nOver the past decade, many researchers in computer science have forcefully\nargued that AI systems favored certain social groups. In a oft-cited article,\nEmily Bender and her co-authors alerted computer scientists on the role of the\ndata used to train the models. They especially pointed to the risks of LLMs\nsuppressing diversity because of the relative absence of minority views in the\ntexts provided to train the LLMs (Benderet al., 2021, 613). A comprehensive\ninvestigation assessing the prevalence of a variety of stereotypes revealed the\nubiquitous presence of gender bias in language models (Nangiaet al., 2020),\na trait once again attributed to the training data. In fact, when commenting\n8\non the non-representative outputs of the LLMs on various survey responses\nor experimental tasks, scholars often blame their faulty response on the lack\nof diversity, or on the disproportionate presence of the views of hegemonic\ngroups, in the data ingested by the models.\nIn this rapidly expanding literature, we aim to contribute to the debates on\nthe ability of LLMs to replace human subjects, in two ways. First, given the\ncentrality of the discussion about algorithmic biases, we suggest a systematic\napproach that helps to determine their presence in the responses of LLMs\nwhen prompted to imitate the views of diverse groups, and propose an\noperationalization of the notion of social bias. Second, our article offers a\nnew take in the debate between representativity vs. bias of LLMs. We build\non an emerging literature showing that LLMs are either brittle (Plaza-del\nArco et al., 2024; Wanget al., 2024b; Barrieet al., 2024; Berglundet al.,\n2024), not appropriate for closed-form answering (Röttgeret al., 2024), or\nthat they do not necessarily exhibit a consistent social bias. Other articles\nalso exhibit results in the models that cannot be linked to any subsample of\nthe population, although they do so mostly incidentally. We expand on this\nargument, which we test in a rigorous way. We call this third approach the\nmachine biashypothesis, a concept we develop and clarify in the following\nsection along with other important concepts.\nDefining and Measuring Bias\nBias can be understood as “prejudice or favoritism toward an individual or\ngroup based on their inherent or acquired characteristics” (Mehrabiet al.,\n2021). As such, it is certainly one of the most important objects of contem-\nporary sociological investigation. Algorithmic bias, the bias produced by\nalgorithms that make use of social data, has also been extensively analyzed.\n9\nTo cite only a few studies, Shankaret al.(2017) showed that two main image\ndatasets used to train computer vision models lacked geographical diversity.\nMost of the pictures they rely on originate from the US, and a vast major-\nity from North America and European countries. In a now classic article\nabout facial recognition, Buolamwini and Gebru (2018) demonstrated that\ntwo commonly used facial detection algorithms disproportionately feature\nphotographs of light-skinned men. As a result, these algorithms perform best\non pictures of white men and systematically misrecognize women and men of\ncolor. Similar biases with respect to gender, race or education have also been\ndocumented for recommendation systems (Schnabelet al., 2016), in medical\ncare (Chenet al., 2019), or when using word embeddings (Bolukbasiet al.,\n2016).\nA recent survey focused on LLMs proposed a taxonomy specific to these\nmodels (Gallegoset al., 2024). It lists more than half a dozen possible ways to\ndiscriminate against someone using language, ranging from erasure (omission\nof the idiolect of a group) to stereotyping (false, immutable association of\na group to a property) to differential recognition of forms of speech. The\nconsequences can be important, as they affect hiring practices, access to\nmarkets (such as credit, housing) or to services, and other aspects of life. In\nour case, the effects seem less dire. After all, if the model displays biases, the\nmain consequence should (hopefully) be that LLMs won’t be used in lieu of\nhuman research subjects. This outcome is nevertheless not certain, so it is\nwell worth investigating these possible biases.\nBut what do we call bias in the context of assessing LLMs’ answers to\nsurveys? We build on these studies to adopt a restrictive but tractable\ndefinition. The term bias should not only point to a numerical deviation\nas in mathematical statistics, but also convey the idea of an affinity with\n10\na specific socio-demographic group, i.e. signal that the LLM’s answers are\nbetter aligned with those of a certain subpopulation (e.g. American highly\neducated males).\nIndeed, for any single question, even if the simulated data is produced\nrandomly, one subpopulation will almost certainly be better represented than\nthe others. If the favored group varies from one question to another, we would\nconclude to the existence of biases in the sense of bad predictions, but not to\nthe presence of a social bias. To determine the existence of asocial bias, we\nneed to findconsistent deviations in favor of the same subpopulation, one\nthat is repeated across questions.\nThis specification of bias calls in turn for a disambiguation of the term\nvariance. Alveroet al.(2024) found a low variance in LLM-generated essays,\nin the sense of a lower diversity of writing styles than what is found in\nhuman texts (see also Wanget al., 2024a). In the case of numerical answers to\nquestionnaires, Bisbeeet al.(2024) report low variance in the sense that LLMs\nhave a more limited answer range than humans within each subpopulation.\nFor categorical answers, a similar definition could be related to low entropy,\nwhere the answers of each subpopulation would be mostly concentrated on a\nsingle answer level.5\nIn this article, we use yet another measure of variance: in line with our under-\nstanding of bias as an affinity towards specific subpopulations, we measure\nthe capability of LLMs to adapt to different socio-demographic prompts. In\norder to avoid confusion, we refer to this form of variance asadaptability.\nLow adaptability designates a situation where the LLM always outputs more\nor less the same answer distribution for a given question, irrespective of the\n5Dominguez-Olmedo et al.(2024), on the contrary, report high entropy of categorical\nLLM answers, that tend to be close to a uniform distribution.\n11\nsocio-demographic group it was asked to emulate. Conversely, a model dis-\nplays high adaptability if it outputs different answer distributions for different\nsocio-demographic groups.6\nThese definitions can be readily related to the different hypotheses evaluated\nin this article, which we summarize in Table 1.\n• The representativehypothesis requires low bias and high adaptability for\nthe LLM to accurately represent the true answers of all social groups.\n• The social biashypothesis requires only a consistent bias towards the\nanswers of a certain socio-demographic, and is independent from the\nadaptability to socio-demographic controls.\n• The machine biashypothesis implies both an absence of systematic bias\ntowards a certain population and a low adaptability.\nTable 1: Relationship between hypotheses and our definitions of bias and\nadaptability\nBias Adaptability\nRepresentative Low High\nSocial bias Socially consistent (Unrelated)\nMachine bias Socially random Low\nReading example: Machine bias is defined by socially random biases (the LLM favors\ndifferent subpopulations across questions) and low adaptability (LLM answers show\nlimited variations across subpopulations).\n6Note that high adaptability is compatible with low entropy, and vice versa. For\ninstance, if a LLM outputs highly skewed but distinct answer distributions for different\nsubpopulations, it would display low entropy within populations, but high adaptability.\nThe notion of adaptability can be related to the “steerability” investigated by Santurkar\net al.(2023).\n12\nDATA AND METHODS\nTo test the ability of LLMs to simulate human beings, we use the “silicon sam-\npling” task implemented by several researchers (Argyleet al., 2023; Santurkar\net al., 2023). The task uses socio-demographic information to condition a\nlarge language model to “think” as a person with a given profile, then asks\nthe model a survey question. Insofar as LLMs are tools aimed to predict the\nnext word of a text, the task takes the model’s prediction to be the answer of\nthe given social profile to the survey question. We then compare this silicon\nanswer to a ground truth—the answers given to the same question by human\nsurvey respondents.\nHuman data: the World Values Survey\nWe use the World Values Survey (WVS, Inglehartet al., 2022) as the ground\ntruth. The WVS is a long-running series of nationally representative, cross-\nsectional surveys that have long been the backbone of cross-national research\nin the sociology of values, attitudes, and representations. Its design facilitates\ninternational comparisons as well as analysis of social change over time. The\nWVS provides more diversity of topics of inquiry, geographical and cultural\nvariation, and historical depth than the surveys usually used to test silicon\nsampling. The latter tend to have a topical focus (often politics), a cultural\nfocus (the US), and a historical focus (the 2010s).\nWe restricted the WVS data set to three periods, five countries, and four ques-\ntions, in order to maximize diversity while avoiding unnecessary experiments\nand energy consumption. Given that LLMs are partly trained on internet\ndata, we wanted to test whether they would be able to predict contemporary\nopinions and attitudes more accurately than those of the past decades, when\n13\nless data were available. Therefore, we use three WVS waves separated in\ntime by about a decade, that correspond to different moments of internet\nhistory: Wave 3 (1995–1997; pre-broadband internet), Wave 5 (2005–2006;\nearly Web 2.0) and Wave 7 (2017–2018; the contemporary internet, the lat-\nest available data before Covid-19). The countries are Australia, Mexico,\nGermany, Russia, and the United States. The United States is thede facto\nbenchmark in the literature. We chose the other countries to have a diverse\nrange of languages and levels of cultural proximity to the US.7\nFinally, we use four survey items: political ideology, social trust, religious\nattendance, and happiness. Politics is thede factobenchmark, as it has been\nthe object of most of the initial attention in the literature. In this study, we\nmeasure it with the question “In political matters, people talk of “the left” and\n“the right.” How would you place your views on this scale, generally speaking?”\nThe scale ranges from 1 (“left”) to 10 (“right”). The other items were chosen\nbecause they are available in all waves, cover core areas of sociological inquiry\nand include opinions as well as attitudes. Social trust (e.g. Schilkeet al.,\n2021) is measured with the question “Generally speaking, would you say that\nmost people can be trusted or that you need to be very careful in dealing with\npeople?”, with response options being “Most people can be trusted” (A) and\n“Need to be very careful” (B). Religious attendance (e.g. Voas and Chaves,\n2016) is measured with the question “Apart from weddings and funerals,\nabout how often do you attend religious services these days?,” with response\noptions ranging from “More than once a week” (A) to “Never, practically\nnever” (G).8 Happiness (e.g. Glasset al., 2016) is measured with the question\n7We ran the analyses with prompts in English as well as in the vernacular language\nof Mexico, Germany, and Russia to check whether, in addition to period, country, and\nquestion, the linguistic dimension would alter the prediction quality of LLMs. It did not.\nTo focus on our main result, we only present the results for English-language prompts.\n8The religious attendance question was not included in the 2006 wave for Russia.\n14\n“Taking all things together, would you say you are ...” and response options\nranging from “Very happy” (A) to “Not at all happy” (D).\nTo steer LLMs towards social profiles, we select a set of stratifying variables.\nTwo of them specify the general context (survey year and country), and five\nspecify the individual context (sex, age, educational level, marital status,\nemployment status). Again, this choice is driven by availability and use in\nthe sociological literature. Despite its prominence in the US context, we did\nnot include ethnicity as it is not consistently available in the rest of the world.\nThe stratifying variables are coded as follows: sex (male and female), age\nin years (binned into age groups for the data analysis: 15–24, 25–34, 35–\n44, 45–54, 55–64, and 65+ years), education (distinguishing between low,\nmedium, and high), employment status (working, retired, homemaker, student,\nunemployed), and marital status (married, cohabiting, divorced or separated,\nwidowed, and single/never married).\nGenerated data: Surveying a large language model\nProducing LLM-generated answers for comparison against human survey data\ninvolves several technical steps: choosing a LLM, constructing a suitable\nprompt to feed it, choosing a generation strategy, using the LLM outputs to\ncompute answer distributions for socio-demographic groups, and choosing a\nsuitable distance measure between generated and ground-truth answers.\nTo formulate our prompts, we use the “interview-style” prompting strategy\nproposed in the second study of Argyleet al.(2023).9 The control variables\nare given to the model in a question-answer format that imitates an interview\nsituation: an interviewer asks questions and provides the available answers,\n9For a robustness check on alternative prompting strategies, see Appendix D.2.\n15\nand an interviewee chooses one of the answers (except for the age question,\nwhere they answer by a number). The socio-demographic controls are provided\nin the prompt, and the LLM is asked to fill in the interviewee’s answer to\nthe last question. This prompting format serves two purposes. First, it\nprovides the model with the socio-demographic information of each persona\nit is supposed to simulate. Second, it conditions the LLM to select one of the\nexpected answers suggested by the interviewer. On our data, we find this\nstrategy able to make LLMs generate not only properly formatted answers,\nbut also probability distributions that are both credible at first glance and\nvarying over socio-demographic profiles.\nHere is an abridged example prompt10:\nQuestion: What year is it?\nAnswer: 1995\nQuestion: What country are we in? Is it “A. Australia”\nor “B. Germany” or “C. Mexico” or “D. Russia” or\n“E. United States”?\nAnswer: A. Australia\nQuestion: How old are you?\nAnswer: 22\n...\nQuestion: Taking all things together, how happy are\nyou? Are you “A. Very happy” or “B. Quite happy”\nor “C. Not very happy” or “D. Not at all happy”?\nAnswer: [Response by the model]\n10For observations where some socio-demographics were missing in the WVS, the question-\nanswer pairs of these variables were dropped from the prompt.\n16\nWe report results for an answer-generation method called next-token proba-\nbility (NTP), on three LLMs considered state of the art in there respective\nclasses at the time of our experiments: Llama-3-70B 5bit-quantized (Llama),\nMixtral-8x7B-0.1 4bit-quantized (Mixtral) and GPT-4-turbo (GPT, undis-\nclosed number of parameters).11 Results for older, smaller or unquantized\nmodels, as well as with a full answer generation technique all lead to the same\nconclusions (see Appendix D).\nThe next-token probability (NTP) method consists in observing the first\ntoken, i.e. word or part of word, generated by each prompt. It leverages\nthe fact that when prompted, a LLM deterministically outputs a probability\ndistribution over the thousands of tokens in its vocabulary, indicating the\nmost likely candidates for the next token in its text-prediction task. While the\ncommon use of LLMs as text generators involves many successive iterations\nof this probability estimation, building sentences through repeated token\nprediction, a single iteration can be enough in the case of survey answers,\nas each available answer can fit into a single token (typically a single digit\nor uppercase letter, eg. choice “A” or “B” or “C”). This is the approach\nfollowed in NTP: for each unique interview prompt, we obtain a probability\ndistribution over the valid answers by normalizing the probabilities associated\nto the valid tokens to 1.12 As NTP is deterministic, only unique prompts are\npresented to the model, and are subsequently reweighted to match the WVS\n11Of these models, GPT-4-turbo is the only one fine-tuned for chat, as no foundational\nmodel is available for this class. Further experiments reported in Appendix C.1 indicate\nthat models fine-tuned for instruction or chat tend to perform worse than pre-trained\nfoundational models on this task.\n12The “compliance” of the LLM can be assessed at each generation by measuring the\nportion of total probability given to the valid answers. In our experiments, we found this\nto be very high (above 98%, see Table S4), except in the case of instruct-tuned LLMs on\n“sensitive” questions such as politics and religion. This result is reassuring regarding the\n17\nsample.13 Our choice of questions, country–year waves and controls results in\na total of approximately 56,000 prompts per model.\nMeasuring the quality of generated data\nThe goal of our experiment is to evaluate the ability of LLMs to accurately\npredict the answers to survey questions, where success means that the re-\nsponses of the language model match the observed answers of WVS human\nrespondents, based on their socio-demographic properties. The precise quality\nof this prediction can only be evaluated by reasoning over groups of obser-\nvations, as a one-to-one observation comparison would be highly affected by\nthe randomness inherent to surveys.14 In order to preserve the multivariate\nnature of social profiles, we group the observations into subpopulations based\non the combination of their socio-demographic properties, using a greedy\ncritique of the NTP method by Wanget al.(2024b).\n13The deterministic nature of NTP removes the need for setting a temperature level\nor other generation hyper-parameters. We found, however, NTP not to be deterministic\nfor GPT-4-Turbo, with sizeable variations of answer probabilities for several runs of an\nidentical prompt, with worrying implications for robustness and reproducibility.\n14Consider a sample of 100 observations in WVS with the same socio-demographic\nproperties, and therefore the same prompt. Supposing the LLM could predict their exact\nanswer distribution, then its one-to-one answer accuracy would randomly vary from 0 to\n18\nalgorithm.15 This yields a total of 687 subpopulations, with an average size\nof 39 WVS respondents (range ofN: 20–115).16\nFor each subpopulation, we compute a LLM answer distribution and its\nground-truth counterpart obtained from the survey data. To measure the\ndistance between these two probability distributions, we use the Earth-Mover’s\nDistance(EMD), alsoknownas 1−Wasserstein Distance. This isthestandard\nstatistical approach when it comes to comparing distributions of ordinal\n100%, depending on the order in which WVS and LLM observations were compared.\n15The combination of the five stratifying variables within each country–year wave,\nmissing values included, results in a total of 13,905 unique profiles, for an average of around\n2 observations per profile. In order to strike a balance between the robustness of the\nanswer distributions and a sufficient number of final subpopulations, we aggregate the\nsmall initial subpopulations using the following greedy iterative algorithm: at each step,\nuntil all subpopulations have at least 20 observations, the smallest group is merged with\nits least-populated closest socio-demographic group.\n16The political question being absent from the 2006 Russian data, the corresponding 48\nsubpopulations were dropped from the analyses of this question.\n19\ndata.17 Consider two samples of equal size, giving distributionsP and Qover\nthe samen available answer levels. We set an uniform cost to moving one\nobservation from one level to the previous or next level.EMD is defined as\nthe total cost of the less costly solution that transformsP into Q by moving\nobservations along the levels. It is then divided by the theoretical maximum\n(n−1), so that the resulting normalized measure ranges between 0 and 1.\nTo computeEMD and its normalizationnEMD, we consider the probability\nvalues Pi and Qi for n discrete levelsi, and use the following formula:\nd0 = 0\ndi+1 = Pi + di −Qi\nEMD(P,Q) =\nn∑\ni=0\n|di|\nnEMD(P,Q) = 1\nn−1EMD(P,Q)\nA normalized Earth-Mover’s Distance of .1 on political ideology means that\n10% of observations need to be moved from far left to far right for the two\ndistributions to become identical (or, similarly, 5% from center to far left and\n5% from center to far right). The normalized Earth-Mover’s Distance ranges\nfrom 0 to 1, where 0 is perfect agreement, and 1 is complete disagreement.\nA distance of 1 on political ideology, for instance, would correspond to the\nsituation where everyone in a given subpopulation is far left but the LLM\n17Robustness checks using two alternative distance measures (the Jensen–Shannon\ndivergence and the total variation distance) did not affect the conclusions of this article.\nNeither of these two distances is perfectly correlated with the EMD on our data: 0.3 for\nJensen–Shannon, 0.6 for total variation. The rationale for using EMD is that is the only\none that takes into account the ordered nature of our survey answers, i.e. the fact that\n“Very often” should be considered closer to “Often” than to “Never.”\n20\npredicts everyone to be far right.18 Figure 1 displays an illustration on a\nfour-level Lickert scale.\nFigure 1: Examples of normalized earth-mover’s distances (nEMD) between\nthe ground truth and other answer distributions, illustrating the different\nquality categories.\nThe distance metric cannot be interpreted in a linear fashion, as there is\ngreater variation in bad prediction scores than in good ones. We therefore\nsuggest the following arbitrary labels for distance thresholds: very good\n(< .05), good (< .1), mediocre (< .15), bad (< .3), very bad (> .3). In\nFigure 1, we clearly see that even relatively low values such as .075 already\nsignal a significant gap between the two distributions. We deliberately chose\n18Note that if all WVS answers are in the central position, the normalized EMD to the\nLLM prediction is necessarily lower than 0.5. Despite this boundedness, we empirically\nfound the normalized EMD to be more suitable to comparisons across questions than an\nalternative standardization where each EMD is divided by its maximum value given the\ndistribution of WVS data.\n21\ngenerous thresholds to ensure a conservative test of our hypotheses. In other\nwords, LLMs could have passed our test even with underwhelming predictions.\nFinally, we set up two baselines for comparison with LLM performances.\nThe first is a “random” baseline, that we obtain by performing random\npermutations of the individual answers in the ground-truth survey data.19\nThis procedure yields a perfect global average prediction, but eliminates\nany link that may exist between socio-demographic properties and outcome\nanswers. The second baseline is a linear regression model that, in standard\nsocial scientific fashion, predicts answers from socio-demographic variables\nbased on the survey data. We use a multinomial logit to regress WVS answer\ndistributions on the socio-demographic variables of the 687 subpopulations,\nand report cross-validated results for fair comparison.20\nThese baseline models serve several purposes. While they could be said to\nhold an unfair advantage, being based on the actual survey data, they do\nprovide legitimate comparison points for evaluating the central promise of\nLLMs as substitutes for human subjects, which is precisely to remove the\nneed for such data. The random baseline acts as a strict lower bound on\nquality: while absolute nEMD values are hard to interpret, any predictive\nmodel that performs worse than random permutations should be regarded\nas a poor representation of the social phenomena at hand. The linear model\ngives a more attainable lower bound: with around 100 parameters instead of\nthe billions that make up LLMs, it is the first approximation that a social\n19We generate 20 independent random permutations for each outcome question, and\nreport their aggregated prediction quality.\n20Specifically, we use leave-one-out cross-validation, in whicheach observation (ie. subpop-\nulation) is predicted by a model trained on all observations except itself. ForN observations,\nthere are thusN models, each trained onN −1 observations. Cross-validation is used to\nmeasure, rather than just a fit on the training data, the model’s generalization capability,\ni.e. its ability to predict data it hasn’t been exposed to yet. We use the simplest possible\nspecification, with no interactions between the seven socio-demographic predictors.\n22\nscientist would use to evaluate the link between socio-demographics and\noutcomes such as happiness or religious practice. It is a lower bound because\nmore intricate models, involving interactions or nonlinear transformations,\nwould likely yield better predictions. Additionally, the linear baseline provides\nan indication of how well the outcome variables are correlated with the\ngiven socio-demographic predictors, even in the presence of omitted variables.\nGood predictions of a simple data-based model would indicate that the task\nis feasible, i.e. that the regularities in the variables at hand are sufficiently\nstrong for it to be reasonable to ask LLMs to emulate them.\nRESULTS\nLLMs’ predictions are far from the ground truth\nCan LLMs generate answers close to what a diverse sample of actual humans\nwould? We first evaluate the accuracy of LLMs with descriptive statistics on\nthe distance between their predictions and the ground truth. To measure this\ndistance we use the normalized Earth Mover’s Distance (nEMD).\nTable 2: nEMD of average LLM predictions with respect to average WVS\nanswer distributions, by model and question.\nHappiness Politics Religion Trust\nGPT-4T 0.143 0.152 0.234 0.332\nLlama 0.022 0.041 0.031 0.183\nMixtral 0.031 0.027 0.184 0.101\nReading example: The overall average prediction of GPT4-turbo on the Happiness\nquestion has an nEMD distance of 0.143 to the overall average of ground-truth\nanswer distributions.\nIn some cases, the average responses of LLMs come quite close to the average\nanswer distributions observed in the WVS data. Table 2 shows that the\n23\nnEMD of average Mixtral and Llama responses fall in the “very good” range\nfor the Happiness and Politics question, with distances lower than .05. GPT,\non the other hand, yields average predictions that are much more distant from\naverage WVS answers, with nEMD ranging from .143 to .332. Results are\nsimilar when considering variable-wise disaggregated averages (eg. averaging\nover women and men).\nComparing answer distributions at the fine-grained subpopulation level gives\na more detailed picture. Considering all 687 subpopulations and 4 outcome\nvariables together, the distance between the predictions of the three LLMs\nand the survey responses is usually high. Figure 2 displays the density of the\nnEMD, and Table 3 summarizes their distribution over quality classes. Only\na small part of LLM predictions fall in the “Very good” range (a distance\nlower than .05), and a large proportion of subpopulations fall in the “bad” to\n“very bad” range (.15 and more).\nTable 3: Quality of LLM predictions, compared to the linear baseline and\nrandom permutations, over all subpopulations and outcome variables (in %).\nVery good Good Mediocre Bad Very bad Total\nnEMD (0–.05) (.05–.10) (.10–.15) (.15–.30) (.30–1)\nBaseline\nLinear (CV) 39.4 40.2 14.1 6.1 0.2 100.0\nRandom 13.7 31.8 23.3 26.2 5.1 100.0\nLLM\nMixtral 16.1 33.7 18.3 21.7 10.2 100.0\nLlama 14.9 32.8 21.5 24.6 6.2 100.0\nGPT-4T 1.1 8.9 22.3 45.5 22.3 100.0\nReading example: Mixtral-8x7B achieves “very good” quality (nEMD lower than\n0.05) on 16.1% of the subpopulations, compared to 13.7% for random permutations\nof the ground truth, and 39.4% for a cross-validated linear model.\nWe also notice variation across models (Figure 2 and Table 3). On all\n24\nFigure 2: The overall quality of LLM predictions is low when compared to\nactual survey responses, and LLMs are worse predictors than a linear model.\nPanel A shows the density of the nEMD across subpopulations when all\nquestions are pooled together and densities are stratified by method: most\nLLM predictions are “mediocre” or worse, irrespective of the LLM used.\nPanel B shows the same density, stratified by LLM and question. Panel C\nshows the same density, now stratified by LLM and country.\n25\nquestions, GPT-4 is the worst-performing, with 90% of predictions at best\nmediocre (nEMD >= .1). Llama fares better with 52.3%; and Mixtral, the\nbest model in our experiment, still gets half of its predictions in the mediocre\nrange or worse (50.2%). Notably, all LLMs fare significantly worse than the\nlinear model (79.6% good or very good), and the best models perform only\nmarginally better than random permutations of the ground-truth individual\nanswers (45.5% good or very good), while the worst model, GPT, does not\neven reach the low standard of this random baseline.21 This result challenges\nthe representative hypothesis championed by part of the literature: at least\nfor our questions, we find that these LLMs are untrustworthy with regard to\npredicting people’s survey responses.\nAre these results driven by some topics that are more difficult to predict than\nothers? The overall picture of poor prediction holds when disaggregating the\nsubpopulations along the four questions, as shown in Panel B of Figure 2.\nPolitical ideology does appear to have better predictions than trust and\nreligious attendance, and so does happiness. However, even these questions\nhave more than half of subpopulations in the ‘mediocre’ category, or worse.\nThe pattern of poor prediction quality also holds when we group the subpop-\nulations along the five country samples, as shown in Panel C of Figure 2. Few\ncountry differences arise and, contrary to our initial expectations based on\nthe social bias literature, predictions are not always better for the US.\nLLMs have no systematic social bias\nWe then turn to the social bias hypothesis, and explore whether the error\nin prediction can be attributed to the LLMs being more representative of\n21Additionally, the good performance of the linear baseline indicates that the conditioning\nvariables are strong predictors of the outcome answers in the ground truth data.\n26\nsome social groups than others. To do so, we use a linear model that we call\nthe Social model, regressing the LLM’s prediction error (measured by nEMD\ndistance) on the socio-demographic control variables.\n(Social model)di = α+ β×Si + ui\ndi = log(1 +nEMD(WVS i,LLMi))\nSi is the vector of socio-demographic controls\nWVS i , LLMi are answer distributions\nTwo predictions follow from the social bias hypothesis:\n1. Socio-demographic characteristics are good predictors of the distance\nbetween LLM answers and the ground truth. This can be tested with\ngoodness-of-fit indicators of the Social model. The social bias hypothesis\nshould lead to highR2 values with this model.22\n2. Socio-demographic characteristics are consistent predictors. This can\nbe tested by comparing the signs of coefficients across questions. The\nsocial bias hypothesis should lead to significant coefficients of the same\nsign across all four questions for at least some predictors.\nWe fit one model for each LLM-question pair. We first consider the goodness-\nof-fit. Table 5 displays indicators for the three main LLMs. In a few models,\nthe error in prediction is well predicted by socio-demographic variables. This\nis for instance the case of religious attendance by Mixtral (R2 ≃.74), or trust\n22Note that if there was no strong link between answers and controls in the ground-truth\ndata, theR2 of the Social model would mechanically be small; it would also render the\nsocial bias hypothesis irrelevant. This is not the case in our data, as the control variables\nare rather good predictors of the WVS answers (see the Linear baseline modelsupra).\n27\nand politics by GPT (R2 ≃.57). On the 8 other models we evaluated, the\nR2 is quite low, with a median of .36. We conclude from these indicators\nthat socio-demographics have a weak to moderate correlation with the LLMs’\nprediction error, and that social bias fails to predict the major part of this\nerror.\nWe then look at coefficient stability across questions. Figure 3 displays the\ncoefficients of all 24 dummy social predictors in theSocial model, for Mixtral,\nfor the 4 questions. It clearly shows that the association between socio-\ndemographic variables and the distance to the ground truth is not consistent\nacross questions. For the Mixtral model, the only predictor showing consistent\nand significantly non-null values is “Mexico,” with a positive sign indicating\na worse performance for Mexican data compared to US data. “Female” also\nhas positive effects across questions—the language model predicts women’s\nopinionslessaccuratelythanmen’sopinions—butone ofthemisnotsignificant\nat the 5 percent level. The effects of other predictors are either not significant\nor do not have a consistent sign, even with only four questions. These results\nhold for all LLMs we tested, as shown in the appendix (Figures S5 and S7).\nIn other words, we do not find any evidence of consistent bias in support of\nthe social bias hypothesis. One practical implication is that we are unable to\npredict how answers to a new question might be biased, i.e. whether married\nor single, high or low education, young or old people would be more accurately\ndescribed if we generated LLM answers to a new question.\nLLMs have low adaptability\nWe then investigate the extent to which LLM answer patterns change along\nwith socio-demographic prompts, i.e. the social adaptability of each LLM.\n28\nFigure 3: LLM performance exhibits hardly any bias for specific social groups.\nSocial model for Mixtral-8x7B, estimated coefficients and 95% confidence\nintervals. Reference: 2010s, US, age 35-44, high education, working, married\nmale. Reading: a coefficient is significant at the 5% threshold if its confidence\ninterval doesn’t include the 0 vertical.\n29\nWe first visualize the dispersion of WVS and LLM answer distributions with\nmultidimensional scaling (MDS). MDS is a linear technique used for scaling\nobservations defined by a distance matrix. For each question, we compute a\nsingle MDS on the pairwise nEMD distances on all subpopulations (pooled\nLLMs and ground truth).\nIn Figure 4, the x- and y-axes represent the coordinates of the MDS, and\neach point represents the answer pattern of a subpopulation, either WVS\nor LLM. In this space, the closer two points are, the lower their distance\nis, as measured by nEMD. The color distinguishes between “ground truth”\nWVS (light) and LLMs (darker shades). The visualization indicates that\nLLM answer patterns are much more concentrated than human ones: they\noccupy a limited subspace of answer space, especially compared to the high\ndispersion of human data.23 This result of low LLM adaptability to socio-\ndemographic prompts is consistently found across questions, models and\ngeneration strategies (see Appendix C.2).\n23The extreme in this regard is attained on the binary Trust question by GPT-4, a model\nthat seems to have learned never to trust people. More generally, out of the 13 models\nwe tested, only the 4 instruct models (among which GPT-4) were consistent with Bisbee\net al.(2024)’s result of low entropy, where each prompt leads to answers very concentrated\non a single level. Foundational models showed more balanced answer distributions, where\nseveral answer levels have equivalent probabilities.\n30\nFigure 4: The space of answer distributions generated by LLMs (dark data\npoints) is much narrower than the survey answers of human respondents\n(light data points). Multidimensional scaling on the pairwise normalized\nEarth-Movers’ Distances (nEMD).\nTable4offersfurtherevidenceoflowLLMadaptability. Itdisplaysadispersion\nmeasure of answer distributions over subpopulations, median pairwise nEMD.\nThe lower values observed for LLMs indicate that their answer patterns are\nmore concentrated than WVS ones, i.e. less diverse across socio-demographic\ncontrols.\n31\nTable 4: Median pairwise nEMD of subpopulation answer distributions\nHappiness Politics Religion Trust\nWVS 0.115 0.108 0.174 0.156\nGPT-4T 0.006 0.021 0.131 0.000\nLlama-3-70B 0.052 0.026 0.069 0.043\nMixtral-8x7B 0.033 0.019 0.032 0.063\nReading example: the median nEMD between all pairs of subpopulations of WVS\non Happiness is 0.115, while only 0.006 for GPT, indicating a lower dispersion of\nanswer distributions generated by the LLM.\nTable 5: Goodness-of-fit measures for the three linear models of LLM–WVS\ndistance. Bold font denotes best-fitting model betweenSocial and Center.\nAdjusted R2 BIC\nQuestion LLM Social Center Full Social Center Full\nHappiness\nGPT-4T 0.441 0.828 0.870 -2399 -3306 -3367\nLlama-3-70B 0.346 0.667 0.728 -2308 -2878 -2886\nMixtral-8x7B 0.387 0.829 0.858 -2276 -3250 -3245\nPolitics\nGPT-4T 0.560 0.925 0.949 -2257 -3480 -3588\nLlama-3-70B 0.260 0.852 0.909 -2313 -3433 -3610\nMixtral-8x7B 0.172 0.930 0.948 -2371 -4020 -4084\nReligion\nGPT-4T 0.364 0.232 0.685 -1621 -1616 -2084\nLlama-3-70B 0.198 0.531 0.548 -1974 -2452 -2350\nMixtral-8x7B 0.744 0.951 0.982 -1808 -3026 -3571\nTrust\nGPT-4T 0.581 1.000 1.000 -1360 -9264 -9326\n32\nTable 5 continued\nAdjusted R2 BIC\nQuestion LLM Social Center Full Social Center Full\nLlama-3-70B 0.444 0.895 0.946 -1377 -2612 -2925\nMixtral-8x7B 0.382 0.695 0.774 -1494 -2085 -2157\nNote: BIC: Bayesian Information Criterion.Reading example: For Happiness with\nGPT-4-Turbo, the linear model of all the social conditioning variables accounts\nfor 44% of the variance in LLM–WVS nEMD distances. The model whose single\npredictor is the distance of WVS to LLM central tendency accounts for 83% of\nvariance, and the full model for 87% (after adjustment for the number of regressors).\nThe BIC for these models are−2399, −3306, and−3367 respectively: according to\nthis criterion, the center-only model is better than the social-only model, and the\nfull model is slightly better than the center-only one.\nLLM errors are driven by machine bias\nWe have established that language models predict answers that are far from\nthe ground truth, that their responses are not consistently driven toward any\ngiven social group, and that their answer patterns do not vary much over\nsocio-demographics. We propose to call this phenomenon “machine bias”:\neach LLM gives an idiosyncratic answer pattern to each question, limited to\na narrow portion of the human answer space and socially inconsistent across\nquestions.\nIn terms of LLM–WVS prediction error, this “machine bias” implies that a\nlarge portion of error variance can be explained by the distances between the\n33\n(diverse) WVS answers and the central tendency of their (concentrated) LLM\ncounterparts. In order to test this, we devise two new models, extending the\nfirst Social model described above. As a reminder, theSocial model describes\nthe error in LLM predictions as a function of socio-demographic properties\nof subpopulations. Succinctly, the newCenter model replaces this with one\nsimple predictor, the distance between the ground-truth answer patterns\nand the average pattern produced by the LLM. In other words, it tests the\nhypothesis that each LLM outputs a narrow set of answers. Finally, theFull\nmodel includes both sets of predictors.\n(Social) di = α+ β×Si + ui\n(Center) di = γ+ δ×ci + vi\n(Full) di = κ+ λ×Si + µ×ci + wi\nwhere\ndi = log(1 +nEMD(WVS i,LLMi))\nci = log(1 +centerEMDi)\n= log(1 +nEMD(WVS i,LLM))\nSi is the vector of socio-demographic controls\nWVS i, LLMi are answer distributions\nLLM is the average LLM answer distribution on a held-out sample\nui, vi, wi are error terms\nIn more detail, the Center and Full models evaluate the impact of low\n34\nLLM adaptability on the LLMs’ prediction error. Low adaptability, i.e. the\ntendency of LLM answers to be similar across socio-demographics, implies a\nconcentration around a central answer distribution. We operationalize this by\ncomputing, for each LLM–question pair, theLLM average answer distribution.\nThis average is computed on a random subset of 20 subpopulations, a subset\nsubsequently held out of the regression models so as to lift suspicions of\nendogeneity.\nThe distance between WVS answers and theLLM central tendency, that we\ncall centerEMD, functions as a counterfactual: it represents the value that the\nLLM’s prediction error would have if the LLM was completely unresponsive\nto variations in socio-demographics. In such a case, LLM prediction errors\nwould be driven by the socio-demographic variations of the ground-truth\nWVS answer patterns only. Figure 5 illustrates this counterfactual reasoning,\naimed at isolating the effect of social controls on the LLM answers.\nThe Center model regresses LLM prediction errors only on the counterfactual\ncenterEMD, the Full model both on the counterfactual and on the social\ncontrols. The former measures how much of LLM prediction error can be\nattributed to low LLM dispersion alone, the latter the relative contributions of\nlow dispersion and social controls. Similarly to theSocial model, we estimate\nthese two additional models on the four survey questions separately, applying\na logarithmic transformation to the nEMD distances to better match their\nobserved approximate log-normal distribution.\nWe first look at the goodness of fit of the three models. Table 5 presents\ntwo measures for each of these linear models, adjustedR2 and the Bayesian\nInformation Criterion (BIC). Under the machine bias hypothesis, we expect\nthe R2 of theCenter model to be higher than that of theSocial model, and\n35\nFigure 5: Data-generating process and analytical strategy\nto be generally very high, and the BIC to be symmetrically lower. Both\nmeasures account for the difference in model sizes (24 predictors for theSocial\nmodel, 1 for theCenter model, 25 for theFull model); the BIC is known to\ngive a stronger penalty to models with more predictors.\nCenter models show markedly better predictive power thanSocial models in\nmost cases (11 out of 12), and they often do so by a large margin. Comparing\nthe Center and Full models gives an indication of how much information the\nsocio-demographic predictors contribute to the models. Comparisons favor\nthe Full model by at least a small margin in all cases according to adjusted\nR2, and all but two cases according to BIC, indicating that social prompting\ndoes play a role, albeit a small one, in the production of LLM error, even\nwhen controlling for machine bias. TheFull model nonetheless performs\n36\nbetter than theSocial model in all cases, and does so by a large margin.\nThese results provide significant support to what we call the machine bias\nhypothesis, where the low diversity of LLM answer patterns is the main driver\nbehind LLM error patterns, only marginally affected by socio-demographic\nvariables.\n37\nFigure 6: The effect of machine bias (nEMDcenter) is substantially larger than\nall socio-demographic indicators: estimated coefficients of theFull model for\nMixtral, with normalized regressors functioning as variable importance mea-\nsures. Error bars denote 95% confidence intervals,N = 667subpopulations\n(619 for politics). Pooled coefficient based on random effects meta-analyses in\nblack, individual question coefficients in grey.\n38\nAnother indication of the predominance of this machine bias is found in the\nmagnitudes of the normalized parameters of theFull model, as can be seen\nin Figure 6 for Mixtral, the coefficient for distance to the “center” is at least\nan order of magnitude greater than the social factor coefficients. This means\nthat, for each model and question, the best predictor of LLM quality is the\nquality of a counterfactual socially agnostic LLM. In other words, once the\ncentral tendency of the LLM is known, one barely needs to feed the LLM\nnew social prompts in order to measure its prediction quality: computing the\ndistance between the ground truth and the LLM’s known central tendency\ncan be a reliable proxy. Figures S6 and S8 display similar results for GPT\nand Llama.\nFinally, we leverage the nestedness of our models for a formal test of nullity\nof either all social parameters, or the counterfactualcenterEMD parameter.\nThis amounts to testing theFull model against theCenter and Social models\nrespectively, with a null hypothesis that the simpler model is a better fit.\nF-test results are presented in Table 6, and show that both social factors and\ndistances to LLM averages contribute significantly to theFull model, albeit\nwith F scores often two orders of magnitude higher for the exclusion of LLM\ncentral tendency. In other words, it can be costly to drop the social variables\nfrom the equation, but it is much more costly to drop theCenter term.\nLimitations\nOur results are dependent on a series of choices. The first, most obvious\none is that our prompting strategy might not have worked properly. Put\notherwise, there may exist other, more efficient ways to steer language models\n(Lin, 2024), in this case towards various social groups. While we cannot prove\nthat an alternative prompting strategy would not lead to results confirming\n39\nTable 6:F-test of nested models,social or center versus full model.\nCenter vs. Full Social vs. Full\n(i.e. can we removeSocial?) (i.e. can we remove Center?)\nQuestion LLM F-statistic p-value F-statistic p-value\nd f(642, 23) d f(642, 1)\nHappiness\nGPT-4T 10.33 3 .20×10−31 2122.43 1 .04×10−205\nLlama-3-70B 7.44 2 .15×10−21 900.97 2 .35×10−124\nMixtral-8x7B 6.73 6 .22×10−19 2126.66 6 .39×10−206\nPolitics\nGPT-4T 13.19 6 .32×10−40 4526.83 7 .16×10−281\nLlama-3-70B 17.77 2 .12×10−53 4261.88 1 .01×10−273\nMixtral-8x7B 10.53 1 .70×10−31 8904.94 0 .00\nReligion\nGPT-4T 42.54 9 .02×10−113 656.53 2 .80×10−100\nLlama-3-70B 2.08 2 .27×10−3 498.21 4 .01×10−82\nMixtral-8x7B 51.07 2 .18×10−128 8471.41 0 .00\nTrust\nGPT-4T 10.44 1 .39×10−31 99691485.00 0 .00\nLlama-3-70B 27.94 3 .01×10−81 5964.56 0 .00\nMixtral-8x7B 11.00 1 .93×10−33 1111.75 3 .17×10−142\n40\neither the representative or the social bias hypotheses, we adopted the same\nstrategy as some influential articles (Argyleet al., 2023; Santurkaret al.,\n2023). We also tried several variations around this prompting strategy, with\nsimilar conclusions (see Appendix D.2 for details).\nAnother potential cause for concern could be that the models we used may\nnot be the best, or at least not the best suited for the task performed. It\nis true that in a rapidly changing landscape, it is often difficult to remain\nup to date. In fact, in the course of this research, we reran our experiments\nfrequently, either due to new versions of models being released, or due to an\nentirely new model emerging as the new “state of the art” (see Appendix\nB.1). We only report the most recent version tested for each model. Still, we\nbelieve that our strategy yields solid results to date. For instance, we tested\nGPT-3 because it was the main benchmark mobilized by other scientists using\nLLMs to replicate opinions, but we also investigated GPT-3.5-Instruct and\nGPT-4-Turbo, only to get similar results.\nWe also tested powerful open-weight models such as Llama and Mixtral. The\nanswer patterns changed from model to model, even after a simple update.\nThis is in itself a problem; the lack of transparency of proprietary models, and\nof open-weights models whose training code or data cannot be fully accessed\nlimit the reproducibility and interpretability of research on language models\n(Spirling, 2023; Ollionet al., 2024). However, none of these changes affected\nour main results: the overall pattern of a high distance to the ground truth,\na low adaptability to and a weak correlation with socio-demographics, always\nremained the same. While we cannot rule out that future models may be\nable to mimic real populations, we feel confident writing that the current\ngeneration of LLMs cannot.\n41\nOne might argue that the number of survey questions (four) and social\ncontexts (five demographic variables, five countries, three time periods) we\nconsidered is low. This is certainly true, but our choice was theoretically\ngrounded. We selected questions and control variables that are all well\ndocumented in sociological research. The seven control variables are strong\npredictors of the outcome variables, as evidenced by the high accuracy of\nthe linear baseline model, ensuring that they should be sufficient for guiding\nLLM predictions. The outcome questions are diverse, in contrast with studies\nfocused on politics, and we did not expect the inconsistency in biases. It is\nnot impossible that, given more outcomes, a consistent social bias could be\nfound, and we think that this question could be more widely tested using\nthe framework proposed in this article. Finally, we did not add or remove\nany question, predictor, country, or period during the research, and tested\nmultiple models and generation techniques, all leading to the same conclusions\n(see Appendix D).\nDISCUSSION\nThe puzzle at the center of current debates on large language models is\nwhether recent advances provide them with sufficient accuracy to replicate\nhuman behavior and attitudes. The focus of this article is on the particular\ncase of using LLMs to simulate human responses to surveys. We proposed\na formal test of the two main hypotheses which sit at opposite ends of the\ndebate.\nOur results confirm neither of the two arguments. Against the “representative”\nhypothesis, we find that opinions and attitudes of actual social groups are on\nthe whole poorly predicted. Over half of all estimates are at a substantial\n42\ndistance from the ground truth, and no LLM performs significantly better than\nrandom guesses. This adds to a growing body of evidence that casts doubt on\nthe idea that the current models could be prompted to mimic certain social\ngroups, such as citizens of a country or more fine-grained subpopulations.\nAgainst the “social bias” hypothesis, we find that socio-demographics are not\nonly weak predictors of LLM errors, but also inconsistent across questions.\nCan LLMs replace research subjects?\nThe first implication of our study concerns the use of silicon samples in science.\nThe appeal is obvious. Should the representative hypothesis hold, they would\nconsiderably reduce the costs of conducting research. The results presented\nin this article, however, should warn us against an overly enthusiastic or\nrapid use of these technologies. To date, LLMs cannot be used to replace\nresearch subjects, at least not without further specifications nor without\ntaking precautions in the use of the results.\nThis does not mean that LLMs are either useless for social science on other\ntasks (Bail, 2024), or altogether incapable of capturing social regularities.\nStrimling et al.(2024) show for instance that GPT-4 is able to output the\ndominant view on certain moral topics with a reasonable success rate, at least\nat the country level. Sometimes, the demands placed on the model are more\nlimited: when asked to imitate what is considered a universal reaction to a\nsignal, LLM simulations may be informative.24\nPassing additional information to the model may also help them attain better\n24Horton (2023) rightfully noted that “one advantage economists have in using LLMs is\nthat they tend to pose questions that place few demands on the sample. We do not think\nof demand curves sloping downward as a ’Western, Rich, Industrialized, and Democratic’\nphenomenon but rather as a result of rational goal-seeking that nearly all humans engage\nin” (Horton, 2023, p. 6).\n43\nperformances. For instance, Kim and Lee (2023) fine-tuned a LLM with\nsurvey responses to increase the quality of their output. They showed that\nthe models were able to correctly impute data or retrodict information that\nwas missing for various years of the General Social Survey. This is a far cry\nfrom producing accurate answers for a set of diverse populations on questions\nthat were never asked before, but it may be one path forward for fruitful uses\nof these models.\nBeyond social bias, investigating machine bias\nOur results call for a clarification, and they open a new path of investigation.\nThe clarification concerns the concept of bias. At the moment, the term is\nwidely used, a sign of the enthusiasm mixed with fear that surrounds the\nuse of generative AI. It has also become particularly vague, as a number of\nresearchers have already pointed out (Navigliet al., 2023). One contribution\nof this study is to advocate for a precise definition of “social bias” in LLMs’\nresponses. We contend that in order to establish the existence of such a bias,\nwe need to demonstrate that the model consistently favors the position of a\nspecific social group. In other words, we need to clearly distinguish between\nbias (a simple deviation from the truth) and social bias.\nThe fact that we find no evidence of consistent social bias in our experiments\ncalls for further investigations into the sources of LLM biases. They are\ngenerally attributed to the unbalanced training data. While this argument\nis intuitively convincing, and is supported by considerable outside of the\nsilicon sampling literature, it cannot account for our results. We know little\nabout the composition of the training corpora, except that English is largely\ndominant. Yet our models sometimes replicate the views of non-English-\n44\nspeaking, and therefore underrepresented, countries.25 More generally, we\nfind that while all LLMs responses are biased, they also point towards different\nsubpopulations with each model and question. Beyond the training data, this\nmachine bias may have other sources. Some have already been investigated in\ncomputational linguistics, such as the choice of architecture or the evaluation\ntasks on which the models were optimized (Hovy and Prabhumoye, 2021).\nOthers may yet come to light.\nWhat can LLMs know?\nThe idea of using language models to replace human respondents relies on an\nintuition, namely that they are able to successfully encode, and retrieve, social\nregularities. Having been trained on massive text corpora (mostly) produced\nby humans, LLMs would implicitly learn to simulate human thought and\nsocial patterns, insofar as they are expressed in the corpora. This intuition is\nmotivated by the existence of unexpected but potent “emergent capabilities”\nthat LLMs started to display as their models and training corpora grew larger\n(Zoph et al., 2022; Schaefferet al., 2023). Just a few years ago, not many\nwould have believed word-prediction algorithms to be capable of carrying\nout such fundamentally human tasks as constructing sensible arguments,\nprogramming software, or making decisions and justifying them.\nThese capabilities are undeniable, and one would be hard-pressed to consider\ngenerative language models as a passing fad. Yet, despite their impressive\nachievements, they are still notoriously unreliable in some respects. Most\nprominently, they are prone to producing “hallucinated” information, out-\nputting answers that are factually wrong, often with unsettling apparent\n25GPT 3.5, for instance, is closer to Russia on the politics question, and Llama-3-8B\nmostly echoes the answers of Mexicans to the happiness question (see Appendix E.4).\n45\nself-confidence. This stems from the fact that information is stored in LLMs\nnot in the form of a hard-coded database, but rather as a by-product of prob-\nabilistic word prediction. “Knowledge,” for LLMs, is distributed throughout\nthe billions of weights that produce their next-word probabilities, and that\nencode patterns and associations found in the data they were fed.26\nInterrogating LLMs as if they were humans in a survey, what the literature\ncalls “silicon sampling,” is even more ambitious than asking them for factual\nknowledge. It is obviously easier for a model to reproduce e.g. historical\ndates and names that frequently occur together in a large corpus than it is to\nsimulate survey answers that are consistent with scientific knowledge about\nsocial regularities. Firstly, neither explicit social-scientific formulations of\nsuch regularities nor raw interview data are likely to occur frequently, if at\nall, in the (largely undisclosed) LLM training corpora. Secondly, even if some\nregularities had been successfully embedded in a model’s weights, simply\ninterrogating it with a context-question pair might not be the most efficient\nway to retrieve this information, if only because different reformulations of the\nsame question are known to produce different answers (Berglundet al., 2024).\nLastly, the very need for social science research stems from the fact that its\ndiscoveries are often contrary to common sense. To be sure, some frequent\npositive associations (e.g. “Republican” and “Trump”) can help probabilistic\nmodels retrodict past electoral behavior to some extent. Beyond such simple\ntasks, however, expecting pre-trained LLMs to successfully emulate complex\nhuman behaviors or personas is likely to be a risky bet on their “emergent\ncapabilities.”27\n26Furthermore, during and after training, LLMs are selected according to their perfor-\nmance on a series of natural language and reasoning tasks, much less on factual knowledge\ncapabilities.\n27Moreover, the concept of emergent capabilities is prone to fuzzy definitions, as noted\nby (Rogers and Luccioni, 2024).\n46\nCONCLUSION\nAlchemists of the Middle Ages believed that, in a nearby future, they would\nbe able to conjure miniature humans out of inert matter. These artificial\nbeings, which they called “homunculi,” were seen as having a number of\nspecial qualities, such as exceptional mastery of the arts. Producedin silico,\nthey would also be easily replicated. All that was needed to create them were\na few mundane ingredients and, of course, craftsmanship.\nIs the idea that LLMs can create miniature humans the modern-day version\nof the belief in homunculi? It certainly appears so, if by this we mean a\nmaximalist position, according to which contemporary LLMs are able to\nsimulate nontrivial social phenomena. Experimental results, our own and\nmany of those cited in this article, indicate that there is reason to remain\ncareful. In the specific case of survey answers, we find that current LLMs are\nplagued by machine bias: an inability to adapt to social variations resulting\nin poor predictions that cannot be accounted for by the unrepresentative\nnature of their training data.\nOur conclusion does not suggest that the use of LLMs for social science\nresearch, and beyond, should be dismissed entirely. Provided we can better\nunderstand how they work, and avoid the pitfalls of both anthropomorphism\nand prophetism (Narayanan and Kapoor, 2024), it is likely that LLMs will\nbecome useful tools in the day-to-day work of social science researchers.\nData and Code Availability Statement\nThe data and code used for this article are available at the following URL:\nhttps://doi.org/10.17605/OSF.IO/YZECT\n47\nAuthor biographies\nJulien Boelaert is an Assistant Professor of Political science at the Centre\nd’études et de recherches administratives, politiques et sociales (CERAPS),\nUniversity of Lille. His research focuses on political sociology and computa-\ntional methods.\nSamuel Coavoux is an Assistant Professor of Sociology at the Center for\nResearch in Economics and Statistics (CREST) and at Ecole Nationale de la\nStatistique et de l’Administration Economique (ENSAE). His research focuses\non digital uses and cultural inequalities.\nÉtienne Ollion is a Senior Researcher at CNRS and a Professor of Sociology\nat l’École polytechnique. His work focuses on the transformations of political\nfields in Europe, as well as on how to integrate machine learning and LLMs\ninto social scientific research. Recent work includesThe Candidates. Amateurs\nand Professionals in French Politics(Oxford University Press, 2024), and\npublications in outlets such asNature Machine Intelligenceand Sociological\nMethods and Research.\nIvaylo D. Petev is a CNRS Research Fellow at the Center for Research\nin Economics and Statistics (CREST) and Lecturer in Sociology at Ecole\nNationale de la Statistique et de l’Administration Economique (ENSAE). His\nresearchexploresquestionsrelatedtoinequalitiesinconsumptionandlifestyles,\nenvironmental practices and discrimination from historical and comparative\nperspectives. His work has appeared inSocius, American Sociological Review,\nand Economics and Statistics.\n48\nPatrick Präg is an Associate Professor of Sociology at the Center for\nResearch in Economics and Statistics (CREST) and at Ecole Nationale de la\nStatistique et de l’Administration Economique (ENSAE). His research focuses\non social cohesion and social inequality. His work has been published inSocial\nForces, Demography, andEuropean Societies.\nReferences\nAher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai, 2023. ‘Using Large\nLanguage Models to Simulate Multiple Humans and Replicate Human\nSubject Studies.’arXiv doi: 10.48550/arXiv.2208.10264.\nAlvero, A. J., Jinsook Lee, Alejandra Regla-Vargas, René F. Kizilcec, Thorsten\nJoachims, and Anthony Lising Antonio, 2024. ‘Large Language Models,\nSocial Demography, and Hegemony. Comparing Authorship in Human and\nSynthetic Text.’Journal of Big Data11(138): 1–28. doi: 10.1186/s40537-\n024-00986-7.\nArgyle, Lisa P., Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher\nRytting, and David Wingate, 2023. ‘Out of One, Many: Using Language\nModels to Simulate Human Samples.’Political Analysis31(3): 337–351.\ndoi: 10.1017/pan.2023.2.\nAshokkumar, Ashwini, Luke Hewitt, Isaias Ghezae, and Robb Willer, 2024.\nPredicting Results of Social Science Experiments Using Large Language\nModels. treatmenteffect.app.\nAtari, Mohammad, Mona J. Xue, Peter S. Park, Damián Ezequiel\nBlasi, and Joseph Henrich, 2023. ‘Which Humans?’ PsyArXiv doi:\n10.31234/osf.io/5b26t.\n49\nBail, Christopher A., 2024. ‘Can Generative AI Improve Social Science?’\nProceedings of the National Academy of Sciences121(21): e2314021121.\ndoi: 10.1073/pnas.2314021121.\nBarrie, Christopher, Elli Palaiologou, and Petter Törnberg, 2024. ‘Prompt\nStability Scoring for Text Annotation with Large Language Models.’ doi:\n10.48550/arXiv.2407.02039.\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmar-\ngaret Shmitchell, 2021. ‘On the Dangers of Stochastic Parrots: Can\nLanguage Models Be Too Big?’ InProceedings of the 2021 ACM Con-\nference on Fairness, Accountability, and Transparency, FAccT ’21, p.\n610–623. New York, NY, USA: Association for Computing Machinery.\ndoi: 10.1145/3442188.3445922.\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper\nStickland, Tomasz Korbak, and Owain Evans, 2024. ‘The Reversal\nCurse: LLMs Trained on ‘A Is B’ Fail to Learn ‘B Is A’.’arXiv doi:\n10.48550/arXiv.2309.12288.\nBisbee, James, Joshua D. Clinton, Cassy Dorff, Brenton Kenkel, and Jen-\nnifer M. Larson, 2024. ‘Synthetic Replacements for Human Survey Data?\nThe Perils of Large Language Models.’Political Analysis32(4): 401–416.\ndoi: 10.1017/pan.2024.5.\nBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and\nAdam T. Kalai, 2016. ‘Man Is to Computer Programmer as Woman Is\nto Homemaker? Debiasing Word Embeddings.’Proceedings of the 30th\nInternational Conference on Neural Information Processing Systemspp.\n4356–4364. doi: 10.5555/3157382.3157584.\n50\nBrynjolfsson, Erik, Danielle Li, and Lindsey R Raymond, 2023.Generative\nAI at work. Technical report, National Bureau of Economic Research.\nBuolamwini, Joy and Timnit Gebru, 2018. ‘Gender Shades. Intersectional\nAccuracy Disparities in Commercial Gender Classification.’Proceedings of\nMachine Learning Research81: 77–91.\nCao, Yong, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel\nHershcovich, 2023. ‘Assessing Cross-Cultural Alignment between Chat-\nGPT and Human Societies: An Empirical Study.’ pp. 53–67. doi:\n10.18653/v1/2023.c3nlp-1.7.\nChen, Irene Y., Peter Szolovits, and Marzyeh Ghassemi, 2019. ‘Can AI Help\nReduce Disparities in General Medical and Mental Health Care?’AMA\nJournal of Ethics21(2): E167–E179. doi: 10.1001/amajethics.2019.167.\nDaikeler, Jessica, Michael Bošnjak, and Katja Lozar Manfreda, 2019. ‘Web\nVersus Other Survey Modes. An Updated and Extended Meta-Analysis\nComparing Response Rates.’Journal of Survey Statistics and Methodology\n8(3): 513–539. doi: 10.1093/jssam/smz008.\nDillion, Danica, Niket Tandon, Yuling Gu, and Kurt Gray, 2023. ‘Can AI\nLanguage Models Replace Human Participants?’ Trends in Cognitive\nSciences 27(7): 597–600. doi: 10.1016/j.tics.2023.04.008.\nDominguez-Olmedo, Ricardo, Moritz Hardt, and Celestine Mendler-Dünner,\n2024. ‘Questioning the Survey Responses of Large Language Models.’arXiv\ndoi: 10.48550/arXiv.2306.07951.\nDutwin, David and Trent D. Buskirk, 2021. ‘Telephone Sample Surveys:\nDearly Beloved or Nearly Departed? Trends in Survey Errors in the Era of\n51\nDeclining Response Rates.’Journal of Survey Statistics and Methodology\n9(3): 353–380. doi: 10.1093/jssam/smz044.\nGallegos, Isabel O., Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul\nKim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed,\n2024. ‘Bias and Fairness in Large Language Models: A Survey.’Computa-\ntional Linguistics50(3): 1097–1179. doi: 10.1162/coli_a_00524.\nGlass, Jennifer, Robin W. Simon, and Matthew A. Andersson, 2016. ‘Par-\nenthood and Happiness. Effects of Work–Family Reconciliation Policies in\n22 OECD Countries.’American Journal of Sociology122(3): 886–929. doi:\n10.1086/688892.\nHartmann, Jochen, Jasper Schwenzow, and Maximilian Witte, 2023. ‘The\nPolitical Ideology of Conversational AI: Converging Evidence on Chat-\nGPT’s Pro-environmental, Left-Libertarian Orientation.’ arXiv doi:\n10.48550/arXiv.2301.01768.\nHorton, John, 2023. ‘Large Language Models as Simulated Economic Agents.\nWhat Can We Learn from Homo Silicus?’NBER Working Paper31122.\ndoi: 10.3386/w31122.\nHovy, Dirk and Shrimai Prabhumoye, 2021. ‘Five Sources of Bias in Natural\nLanguage Processing.’Language and Linguistics Compass15(8): e12432.\ndoi: 10.1111/lnc3.12432.\nInglehart, Ronald, Christian Haerpfer, Alejandro Moreno, Christian Welzel,\nKseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, Ed-\nuard Ponarin, and Bi Puranen, 2022.World Values Survey (WVS). All\nRounds—Country-Pooled Datafile. worldvaluessurvey.org, 4.0 edition. doi:\n10.14281/18241.17.\n52\nJohnson, Rebecca L., Giada Pistilli, Natalia Menédez-González, Leslye\nDenisse Dias Duran, Enrico Panai, Julija Kalpokiene, and Donald Jay\nBertulfo, 2022. ‘The Ghost in the Machine Has an American Accent. Value\nConflict in GPT-3.’arXiv doi: 10.48550/arXiv.2203.07785.\nKim, Junsol and Byungkyu Lee, 2023. ‘AI-Augmented Surveys. Leveraging\nLarge Language Models and Surveys for Opinion Prediction.’arXiv doi:\n10.48550/arXiv.2305.09620.\nKozlowski, Austin C., Hyunku Kwon, and James A. Evans, 2024. ‘In Silico\nSociology. Forecasting COVID-19 Polarization with Large Language Models.’\narXiv doi: 10.48550/arXiv.2407.11190.\nLin, Zhicheng, 2024. ‘How to Write Effective Prompts for Large Language\nModels.’ Nature Human Behavior8: 611–615. doi: 10.1038/s41562-024-\n01847-2.\nMehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and\nAram Galstyan, 2021. ‘A Survey on Bias and Fairness in Machine Learning.’\nACM Computing Surveys (CSUR)54(6): 1–35. doi: 10.1145/3457607.\nMotoki, Fabio, Valdemar Pinho Neto, and Victor Rodrigues, 2024. ‘More\nHuman Than Human. Measuring ChatGPT Political Bias.’Public Choice\n198(1): 3–23. doi: 10.1007/s11127-023-01097-2.\nNangia, Nikita, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman, 2020.\n‘CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked\nLanguage Models.’ In Webber, Bonnie, Trevor Cohn, Yulan He, and Yang\nLiu, eds., Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 1953–1967. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154.\n53\nNarayanan, Arvind and Sayash Kapoor, 2024.AI Snake Oil. What Artifi-\ncial Intelligence Can Do, What It Can’t, and How to Tell the Difference.\nPrinceton University Press. doi: 10.1353/book.129007.\nNavigli, Roberto, Simone Conia, and Björn Ross, 2023. ‘Biases in Large\nLanguage Models: Origins, Inventory, and Discussion.’ACM Journal of\nData and Information Quality15(2): 1–21. doi: 10.1145/3597307.\nOllion, Etienne, Rubing Shen, Ana Macanovic, and Arnault Chatelain, 2024.\n‘The Dangers of Using Proprietary LLMs for Research.’Nature Machine\nIntelligence 6: 4–5. doi: 10.1038/s42256-023-00783-6.\nO’Neil, Cathy, 2016.Weapons of Math Destruction: How Big Data Increases\nInequality and Threatens Democracy. Crown.\nPearson, Helen, 2015. ‘Massive UK Baby Study Cancelled.’Nature 526(7575):\n620–621. doi: 10.1038/526620a.\nPlaza-del Arco, Flor Miriam, Debora Nozza, and Dirk Hovy, 2024. ‘Wisdom\nof Instruction-Tuned Language Model Crowds. Exploring Model Label\nVariation.’arXiv doi: 10.48550/arXiv.2307.12973.\nRogers, Anna and Alexandra Sasha Luccioni, 2024. ‘Position: Key\nClaims in LLM Research Have a Long Tail of Footnotes.’ doi:\n10.48550/arXiv.2308.07120.\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah\nKirk, Hinrich Schuetze, and Dirk Hovy, 2024. ‘Political Compass or Spinning\nArrow? Towards More Meaningful Evaluations for Values and Opinions\nin Large Language Models.’ In Ku, Lun-Wei, Andre Martins, and Vivek\nSrikumar, eds.,Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 15295–15311.\n54\nAssociation for Computational Linguistics. doi: 10.18653/v1/2024.acl-\nlong.816.\nRozado, David, 2024. ‘The Political Preferences of LLMs.’Plos One19(7):\n1–15. doi: 10.1371/journal.pone.0306621.\nRutinowski, Jérôme, Sven Franke, Jan Endendyk, Ina Dormuth, Moritz Roidl,\nand Markus Pauly, 2024. ‘The Self-Perception and Political Biases of\nChatGPT.’ Human Behavior and Emerging Technologies2024(7115633):\n1–9. doi: 10.1155/2024/7115633.\nSanturkar, Shibani, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang,\nand Tatsunori Hashimoto, 2023. ‘Whose Opinions Do Language Models\nReflect?’ arXiv doi: 10.48550/arXiv.2303.17548.\nSchaeffer, Rylan, Brando Miranda, and Sanmi Koyejo, 2023. ‘Are Emer-\ngent Abilities of Large Language Models a Mirage?’ arXiv doi:\n10.48550/arXiv.2304.15004.\nSchilke, Oliver, Martin Reimann, and Karen S. Cook, 2021. ‘Trust in Social\nRelations.’ Annual Review of Sociology47: 239–259. doi: 10.1146/annurev-\nsoc-082120-082850.\nSchnabel, Tobias, Adith Swaminathan, Ashudeep Singh, Navin Chandak,\nand Thorsten Joachims, 2016. ‘Recommendations as Treatments: De-\nbiasing Learning and Evaluation.’ In Proceedings of the 33rd Interna-\ntional Conference on Machine Learning, pp. 1670–1679. PMLR. doi:\n10.48550/arXiv.1602.05352.\nShankar, Shreya, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson,\nand D. Sculley, 2017. ‘No Classification without Representation: Assessing\nGeodiversity Issues in Open Data Sets for the Developing World.’ In\n55\nNIPS 2017 Workshop on Machine Learning for the Developing World. doi:\n10.48550/arXiv.1711.08536.\nSpirling, Arthur, 2023. ‘Why Open-Source Generative AI Models Are an Eth-\nical Way Forward for Science.’Nature 616(7957): 413. doi: 10.1038/d41586-\n023-01295-4.\nStrimling, Pontus, Joel Krueger, and Simon Karlsson, 2024. ‘GPT-4’s One-\nDimensional Mapping of Morality: How the Accuracy of Country-Estimates\nDepends on Moral Domain.’arXiv doi: 10.48550/arXiv.2407.16886.\nVoas, David and Mark Chaves, 2016. ‘Is the United States a Counterexample\nto the Secularization Thesis?’ American Journal of Sociology121(5):\n1517–1556. doi: 10.1086/684202.\nvon der Heyde, Leah, Anna-Carolina Haensch, and Alexander Wenz, 2023.\n‘Vox Populi, Vox AI? Using Language Models to Estimate German Public\nOpinion.’ SocArXiv doi: 10.31235/osf.io/8je9g.\nWang, Angelina, Jamie Morgenstern, and John P. Dickerson, 2024a.\n‘Large Language Models Should Not Replace Human Participants Be-\ncause They Can Misportray and Flatten Identity Groups.’ arXiv doi:\n10.48550/arXiv.2402.01908.\nWang, Xinpeng, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Röttger,\nFrauke Kreuter, Dirk Hovy, and Barbara Plank, 2024b. ‘“My Answer is\nC”: First-Token Probabilities Do Not Match Text Answers in Instruction-\nTuned Language Models.’ In Ku, Lun-Wei, Andre Martins, and Vivek\nSrikumar, eds.,Findings of the Association for Computational Linguistics\nACL 2024, pp. 7407–7416. Association for Computational Linguistics. doi:\n10.18653/v1/2024.findings-acl.441.\n56\nWilliams, Douglas and J. Michael Brick, 2018. ‘Trends in US Face-To-Face\nHousehold Survey Nonresponse and Level of Effort.’Journal of Survey\nStatistics and Methodology6(2): 186–211. doi: 10.1093/jssam/smx019.\nZiems, Caleb, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang,\nand Diyi Yang, 2024. ‘Can Large Language Models Transform Compu-\ntational Social Science?’ Computational Linguistics 50(1): 1–55. doi:\n10.1162/coli_a_00502.\nZoph, Barret, Colin Raffel, Dale Schuurmans, Dani Yogatama, Denny\nZhou, Don Metzler, Ed H. Chi, Jason Wei, Jeff Dean, Liam B. Fedus,\nMaarten Paul Bosma, Oriol Vinyals, Percy Liang, Sebastian Borgeaud, Tat-\nsunori B. Hashimoto, and Yi Tay, 2022. ‘Emergent Abilities of Large\nLanguage Models.’ Transactions on Machine Learning Research doi:\n10.48550/arXiv.2206.07682.\n57\nSUPPLEMENTARY MATERIALS\nContents\nA Descriptive Statistics on the World Value Survey 61\nTable S1: WVS sample size by survey year and country . . . . 61\nTable S2: Descriptive statistics of predictor variables by coun-\ntry, survey years pooled . . . . . . . . . . . . . . . . . 62\nFigure S1: Distribution of the four outcome variables . . . . . 63\nB Technical LLM choices 65\nB.1 Choice of language models . . . . . . . . . . . . . . . . . . . . 65\nTable S3: Models used for full experiments . . . . . . . . . . . 65\nB.2 Prompt engineering . . . . . . . . . . . . . . . . . . . . . . . . 66\nB.3 Answer generation . . . . . . . . . . . . . . . . . . . . . . . . 68\nTable S4: NTP compliance: average probability mass of ex-\npected answers . . . . . . . . . . . . . . . . . . . . . . 70\nB.4 Data contamination . . . . . . . . . . . . . . . . . . . . . . . . 71\nC Results for other models 71\nC.1 Answer quality for all models . . . . . . . . . . . . . . . . . . 71\nTable S5: Distribution of nEMD by LLM across subpopulations\n(%) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\nC.2 MDS results for all models . . . . . . . . . . . . . . . . . . . . 73\nFigure S2: Global MDS visualization for foundational models\nwith NTP generation . . . . . . . . . . . . . . . . . . . 75\nFigure S3: Global MDS visualization of instruct models . . . . 76\n58\nFigure S4: Global MDS visualization for foundational models\nwith FA generation . . . . . . . . . . . . . . . . . . . . 77\nC.3 Regression results for GPT-4 and Llama-3-70B . . . . . . . . . 78\nFigure S5: Coefficients in the Social model for GPT-4-Turbo\n(eq. Figure 3 in main text) . . . . . . . . . . . . . . . . 78\nFigure S6: Coefficients in the Full model for GPT-4-Turbo (eq.\nFigure 6 in main text) . . . . . . . . . . . . . . . . . . 79\nFigure S7: Coefficients in the Social model for Llama-3-70B(eq.\nFigure 3 in main text) . . . . . . . . . . . . . . . . . . 80\nFigure S8: Coefficients in the Full model for Llama-3-70B (eq.\nFigure 6 in main text) . . . . . . . . . . . . . . . . . . 81\nC.4 Regression quality for all models . . . . . . . . . . . . . . . . . 82\nFigure S9: Adjusted R2 of Social and Center models, all LLMs\nand questions . . . . . . . . . . . . . . . . . . . . . . . 82\nD Robustness checks 83\nD.1 Answer generation strategies . . . . . . . . . . . . . . . . . . . 83\nFigure S10: MDS visualization of NTP and FA on Llama-3-70B\nand Mixtral-8x7B. . . . . . . . . . . . . . . . . . . . . 84\nFigure S11: Coefficients in the Full model for Llama-3-70B-FA\n(eq. Figure 6 in main text) . . . . . . . . . . . . . . . . 85\nD.2 Prompting style (Interview, Chat, First Person) . . . . . . . . 85\nTable S6: distances between alternative prompting strategies . 87\nFigure S12: MDS on WVS and GPT-4-Turbo with alternative\nprompting strategies, on 140 US subpopulations. . . . . 88\nD.3 Generation temperature . . . . . . . . . . . . . . . . . . . . . 88\nFigure S13: Density of nEMD distances between LLM answers\nand ground truth, for five different temperatures. . . . 90\n59\nFigure S14: MDS for WVS and five temperature choices of\nMistral-7B. . . . . . . . . . . . . . . . . . . . . . . . . 91\nD.4 Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\nTable S7: Summary statistics for nEMD distance between the\nquantized and unquantized versions of Mistral-7B. . . . 92\nE Additional results 93\nE.1 Correlations of answer probabilities . . . . . . . . . . . . . . . 93\nFigure S15: Correlation coefficients between the answer pro-\nportions of WVS and each LLM . . . . . . . . . . . . . 93\nE.2 Recognizing LLM-generated from true answer patterns . . . . 95\nTable S8: Out-of-bag accuracy of a Random Forest classifier,\ntrained to distinguish between LLM and WVS based\non answer distributions across subpopulations for each\nquestion (%) . . . . . . . . . . . . . . . . . . . . . . . . 96\nE.3 Modeling answers instead of errors . . . . . . . . . . . . . . . 96\nFigure S16: Comparison between the ground-truth and LLM\ncoefficients of an Answer model . . . . . . . . . . . . . 98\nE.4 Best-matching ground-truth demographics . . . . . . . . . . . 99\nTable S9: Proportion (%) of WVS subpopulations reached by\nthe complete set of LLM subpopulations . . . . . . . . 99\nFigure S17: Country and decade of best-matching ground-truth\nsubpopulations, Llama-3-8B. . . . . . . . . . . . . . . . 102\nFigure S18: Country and decade of best-matching ground-truth\nsubpopulations, GPT-3.5. . . . . . . . . . . . . . . . . 103\n60\nA Descriptive Statistics on the World Value\nSurvey\nTable S1: WVS sample size by survey year and country\n1995 1996 1997 2005 2006 2017 2018 Total\nAustralia 2,048 1,421 1,813 5,282\nGermany 2,026 2,064 1,528 5,618\nMexico 1,510 1,560 1,741 4,811\nRussia 2,040 2,033 1,810 5,883\nUnited States 1,542 1,249 2,596 5,387\nTotal 5,630 1,510 2,026 2,981 5,346 4,406 5,082 26,981\n61\nTable S2: Descriptive statistics of predictor variables by country, survey years\npooled\nTotal Australia Germany Mexico Russia United States\nProp. Prop. Prop. Prop. Prop. Prop.\nSex:\nMale 0.47 0.44 0.46 0.50 0.43 0.52\nFemale 0.53 0.56 0.54 0.50 0.57 0.48\nAge:\n15-24 0.12 0.10 0.09 0.20 0.14 0.10\n25-34 0.20 0.16 0.18 0.27 0.19 0.22\n35-44 0.19 0.17 0.18 0.19 0.21 0.19\n45-54 0.17 0.17 0.18 0.16 0.17 0.17\n55-64 0.15 0.17 0.15 0.11 0.15 0.15\n65 and more years 0.17 0.23 0.22 0.09 0.15 0.18\nEducation:\nLow 0.08 0.04 0.09 0.22 0.06 0.02\nMedium 0.50 0.47 0.59 0.50 0.44 0.52\nHigh 0.41 0.49 0.32 0.28 0.50 0.46\nEmployment status:\nWorking 0.58 0.59 0.52 0.57 0.59 0.66\nRetired 0.21 0.27 0.29 0.04 0.25 0.19\nHomemaker 0.10 0.08 0.06 0.25 0.04 0.08\nStudent 0.05 0.03 0.06 0.06 0.06 0.01\nUnemployed 0.06 0.02 0.09 0.08 0.05 0.06\nMarital status:\nMarried 0.54 0.55 0.56 0.51 0.53 0.55\nCohabiting 0.08 0.08 0.08 0.13 0.04 0.05\nDivorced/separated 0.10 0.10 0.09 0.07 0.12 0.12\nWidowed 0.08 0.07 0.09 0.05 0.14 0.06\nSingle/never married 0.20 0.20 0.19 0.24 0.17 0.22\nN 26,981\n62\nFigure S1: Distribution of the four outcome variables social trust (Panel A),\nreligious attendance (Panel B), happiness (Panel C), and political ideology\n(Panel D) by country and survey wave.(cont’d on next page)\n63\nFigure S1: Distribution of the four outcome variables social trust (Panel A),\nreligious attendance (Panel B), happiness (Panel C), and political ideology\n(Panel D) by country and survey wave.(cont’d from previous page)\n64\nB Technical LLM choices\nB.1 Choice of language models\nTable S3: Models used for full experiments.\nName Release Open Type Strategy\n(original)\nGPT-3 (davinci-002) Aug. 2023 No Foundational FA\n(May 2020)\nGPT-3.5-Turbo-Instruct Sept. 2023 No Instruct FA\nGPT-4-Turbo Apr. 2024 No Instruct NTP\n(Nov. 2023)\nLlama-3-8B (Q4) Apr. 2024 Yes Foundational FA, NTP\nLlama-3-8B-Instruct (Q4) Apr. 2024 Yes Instruct NTP\nLlama-3-70B (Q5) Apr. 2024 Yes Foundational FA, NTP\nMistral-0.1-7B (Q5) Sep. 2023 Yes Foundational FA, NTP\nMixtral-0.1-8x7B (Q4) Dec. 2023 Yes Foundational FA, NTP\nMixtral-0.1-8x7B-Instruct (Q4) Dec. 2023 Yes Instruct NTP\nAt the time of our experiments between June 2023 and November 2024, a vast\nnumber of large language models, both open and proprietary, were available.\nWe had access to 10 to 40 GB of RAM of a A100 GPU for the open models,\nwhich allowed us to evaluate a single LLM in a timespan of days or weeks,\naccording to the size of the model, number of prompts and versions of the\ninference packages. For proprietary models, we both had a limited supply\nof resources, and doubts as to whether vast sums of public money should be\nspent on prompting LLMs on private platforms.\nDetails of the models used to run the full experiment is presented in Table\nS3. We made our choice based on the following considerations:\n• We used models that were considered state of the art in their respective\ncategories at the time of our experiments.\n• Whenever possible, we used foundational models, as these are supposedly\nmore “knowledgeable” about the world: chat and instruct models\n65\nare fine-tuned from an initial foundational model, and some of the\ninformation stored in the foundational model’s weights is lost in the\nprocess of turning the model into a helpful assistant. GPT-3.5-turbo-\ninstruct and GPT-4-Turbo are the two exceptions, as no foundational\nmodel was available for these more recent classes of proprietary models.\nWe also evaluated two instruct versions of open models, Llama-3-8B\nand Mistral-8x7B.\n• Foralltheopenmodels, weusedquantizedversionsofthemodels, fortwo\nreasons. Quantized versions are significantly faster, while unquantized\nlarger models (Llama-3-70B and Mixtral-8x7B) wouldn’t fit on our\nGPU resources. Quantized models are generally considered to be very\nefficient compressions of LLMs, and retain most of their capabilities\nwith a quantization of above 4 bits. For an empirical evaluation on our\ndata, see Appendix D.4.\nB.2 Prompt engineering\nPrompt engineering, the process of creating prompts that will make an LLM\nrespond in the desired way, is an open and active field of research. LLM\nperformance has been shown to improve on a number of tasks when the\nprompt was enriched with such diverse phrases as “proceed step by step” or\n“this is very important for my career,” or by replacing parts of the prompt\nby text tokens that seem more “understandable” to the LLM, although they\nmight be less so to the average reader.\nIn all our experiments, we used the simple question–answer format, a standard\napproach in the literature (Argyleet al., 2023; Santurkaret al., 2023). A\nsecond common approach is the first-person setting, in which the LLM is\n66\nprompted with a text such as “I am a 45-year old woman living in Mexico,\nthe year is 1996, I am married, I have a medium education level and am\nunemployed, and politically on a scale from 1 for far left to 10 for far right, I\nwould rate my personal political leaning at... .” This second way of prompting\nis even more amenable to minor variations than the question–answer format.\nWe give an evaluation of the influence of prompt format in Appendix D.2.\nOur experiment involved 56,000 to 108,000 prompts per LLM for full eval-\nuation. While a systematical search for better prompts would go beyond\nthe scope of this study, we report an evaluation of different strategies using\nGPT-4 in Appendix D.2. Apart from an initial validation phase in order to\nmake sure that the LLM was able to output valid and varied answers, we did\nnot tinker any further with our prompting strategy, so as to preserve some\ncomparability between LLMs. Surprisingly, the most important element for\nensuring model compliance was to leave no blank space at the end of the\nprompt.\nThe instruct models required a minor adaptation, in which each prompt\nstarted with the following instruction:\n“The following text comes from an interview for a sociological survey.\nPlease complete the text as if you were the respondent.”\nTwo reasons drove us to add this initial context.\n• First, it is more appropriate for instruct models, that have specifically\nbeen fine-tuned to follow instructions instead of just completing text.\n• Second, instruct models are “aligned” so as not to output illegal or\noffensive content, both by instruction fine-tuning and by the hidden\nproprietary context prompt that is added at the start of each prompt.\n67\nThe result of this alignment was directly observable in our experiments,\nas these instruct models would often refuse to answer the politics and\nreligion questions, instead producing an argument as to why they were\nunfortunately unable to answer them. We found that simply adding\nthis initial context to our prompt was enough to bypass alignment and\nforce model compliance—but this might change along with the frequent\nmodel updates, especially for platform-based models such as OpenAI\nwhere previous versions become unavailable over time.\nB.3 Answer generation\nOur experiments involve one LLM task, namely to complete the text of\nan imaginary survey interview transcript. For this, we used two different\nstrategies commonly found in the literature, of which we here lay out the\ndetails, advantages and possible drawbacks. For an empirical comparison of\nthe two strategies on our data, see Appendix D.1.\nIn full answer generation (FA), we let the LLM produce a full answer for\neach respondent in the survey sample, and then aggregate the answers by\nsubpopulation in order to estimate their probability distributions. With this\nstrategy, LLM answers may need post-processing, as they may not exactly\ncorrespond to one of expected answers (e.g. “B. Pretty happy” instead of “B.\nQuite happy”). To solve this problem, if the LLM’s answer (after minimal\ncleaning: taking into account the first line only, removing special signs and\nspaces) does not fall into one of the expected ones, we ask the LLM to produce\na new answer until it does. Non-compliance was practically inexistent for\nopen models, but could rise up to 20% for OpenAI models on “sensitive”\nquestions. For all FA experiments, we used a temperature of 0.7, as in (Argyle\net al., 2023).\n68\nIn next-token-probability (NTP), that was used for the results presented in the\nmain text of this article, we ask the LLM to produce just one token for each\ndistinct profile in the WVS data, directly yielding a probability distribution\nover the valid answers. In this strategy, the valid available answers are coded\nas single tokens, e.g. A to H if there are 8 available answers, or 0 to 9 if there\nare 10 numeric answers. Accordingly, the prompt must have categorical answer\nstart with a letter, and each numeric one with a digit from 0 to 9 (we used the\nsame prompts in FA experiments for comparability). Note that in contrast\nwith FA, this a deterministic process, as the probabilities for the single next\ntoken are always the same for a given prompt; furthermore, it removes the\nneed for full-text generation hyper-parameters such as temperature, top-k,\ntop-p, etc.\nFurthermore, when taking into account that, with a limited number of context\nquestions, there are many duplicated socio-demographic profiles (and thus\nprompts) in actual surveys, full answer generation might be implemented\nin two different ways. Either we generate a single LLM answer for each\nindividual in the survey, or we generate a fixed number of LLM answers\n(say 30 or 100, a number deemed sufficient to obtain good enough answer\ndistributions) for each profile in the survey. The fixed-number approach\nwould be more statistically sound, but requires a number of prompts that\ngrows (linearly) with the number of repeated prompts, quickly rendering it\nmore expensive than the one-per-observation approach where the number\nof prompts is fixed. We thus turned to the “silicon sampling” approach of\ngenerating a single LLM answer for each (possibly duplicated) observation in\nthe ground-truth data.\nNTP, on the other hand, is cleaner and significantly cheaper: only one token\nis produced to answer the question, and the LLM is prompted only once per\n69\ndistinct socio-demographic profile, directly yielding a probability distribution\nover all available answers. Theoretically, by limiting its scope to the next\nvalid token, NTP might also be biased in unexpected directions, e.g. giving a\nhigh probability to token “I” (if valid), because of the tendency of an LLM to\ngenerate answers such as “I would say answer A”, as noted in (Wanget al.,\n2024b).\nFinally, it should be noted that, with either generation strategy, and with\nvirtually no prompt engineering, each LLM was able to produce mostly valid\nanswers. This indicates that it “understood” the task at hand. With NTP, it\nwas materialized by the fact that the overwhelming majority of the probability\nmass was assigned to the valid answer tokens (typical mass was above 98%\nfor foundational models as seen in Table S4, so that the issues raised by\nWanget al.(2024b) do not seem to apply to our experiment). With FA, an\noverwhelming majority of answers were valid even without post-processing.\nFurthermore, LLM answer distributions do seem credible at face value. These\nelements may have contributed to the initial appeal of silicon sampling.\nTable S4: NTP compliance: average probability mass of expected answers\nHappiness Politics Religion Trust\nGPT-4T 0.64 0.99 0.64 0.98\nLlama-3-8B 0.99 0.99 0.99 0.98\nLlama-3-8B-Instruct 1.00 0.53 0.99 1.00\nLlama-3-70B 0.99 0.99 0.99 0.98\nMistral-7B 0.99 1.00 0.96 0.98\nMixtral-8x7B 0.98 1.00 0.98 0.97\nMixtral-8x7B-Instruct 1.00 0.50 1.00 1.00\nReading example: the average mass allocated by GPT-4-Turbo to the expected\nanswer tokens for Happiness (“A” to “D”) over all prompts is 0.64.\n70\nB.4 Data contamination\nData contamination is of high concern in benchmarking settings, as it can lead\nto overestimation of LLM quality, as for instance when LLMs are assessed on\ntheir ability to correctly complete standardized tests or exams, while complete\ncorrected transcripts may have been present in the training corpus. The exact\ncontent of LLM training data is unknown, even in the case of open-weight\nmodels, so that we can only speculate about possible data contamination in\nour experiment.\nHowever, we may say that data contamination is unlikely in our setting, for\ntwo reasons. First, the text we gave in our prompts (with a limited number\nof questions, and in a question–answer format) was an original creation by\nthe authors, based on WVS data, that cannot exist in the training data.\nSecond, it is unlikely that LLM training data include detailed statistical\ntables resulting from social scientific research, even less survey transcripts,\nalthough they might include the full text of high-level results as published in\nscientific journals. In any case, our results are unflattering for all the models\nwe tested, so that overestimation of LLM quality is not of great concern for\nthe conclusions of this article.\nC Results for other models\nC.1 Answer quality for all models\nTable S5 presents the overall answer quality of each LLM, compared to WVS\nground truth answers, as in Table 3. It features older (GPT-3 and 3.5) or\nsmaller (Llama-3-8B, Mistral-7B) versions of the models presented in the\nmain text, as well as “instruct” variants (Llama-3-8B-Instruct and Mixtral-\n71\nTable S5: Distribution of nEMD by LLM across subpopulations (%) (eq.\nTable 3 in main text)\nVery good Good Mediocre Bad Very bad Total\nnEMD (0-.05) (.05-.10) (.10-.15) (.15-.30) (.30-1)\nBaselines\nLinear (LOO-CV) 39.4 40.2 14.1 6.1 0.2 100.0\nRandom permutations 13.8 31.8 23.3 26.0 5.1 100.0\nLLM\nGPT-3 (FA) 4.3 19.2 21.2 31.2 24.1 100.0\nGPT-3.5 (FA) 4.3 13.9 19.3 30.3 32.2 100.0\nGPT-4T (NTP) 1.1 8.9 22.3 45.5 22.3 100.0\nLlama3-8B (NTP) 6.3 29.5 24.9 28.3 11.0 100.0\nLlama3-8B (FA) 8.8 26.4 25.3 28.1 11.4 100.0\nLlama3-8B-Instruct (NTP) 1.7 11.1 23.7 44.6 18.9 100.0\nLlama3-70B (NTP) 14.9 32.8 21.5 24.6 6.2 100.0\nLlama3-70B (FA) 13.7 30.0 22.9 24.9 8.5 100.0\nMistral-7B (NTP) 9.3 25.2 23.1 25.3 17.2 100.0\nMistral-7B (FA) 9.9 25.2 21.3 29.6 14.1 100.0\nMixtral-8x7B (NTP) 16.1 33.7 18.3 21.7 10.2 100.0\nMixtral-8x7B (FA) 10.3 25.9 21.0 25.1 17.7 100.0\nMixtral-8x7B-Instruct (NTP) 0.6 8.7 20.5 41.5 28.7 100.0\n72\n8x7B-Instruct), and the two alternative generation strategies “full answer\ngeneration” (FA) and “next-token probabilities” (NTP). It is computed as\nfollows: for each of the 687 subpopulations and 4 questions, we compute the\nnEMD distance between LLM and WVS answer patterns, and summarize the\nresulting distributions as quality classes, from Very good to Very bad. The\nquality of the baseline Linear and Random answer models is presented for\ncomparison.\nThe linear model performs significantly better than any LLM (39% Very\ngood, 40% Good). The best performing LLM is Mixtral-8x7B with NTP\n(16.1% Very good, 33.7% Good), the worst ones being GPT-4-Turbo (1%\nVery good, 9% Good) and Mixtral-8x7B-Instruct with NTP. Of all the model-\nstrategy combinations, only Mixtral-8x7B-NTP and Llama-3-70B (NTP or\nFA) perform similarly or (marginally) better than the random permutation\nbaseline (14% Very good, 32% Good). Differences between NTP and FA\ngeneration strategies is discussed in Appendix D.1.\nLastly, for the two models on which we ran both the foundational and\nthe instruct versions (Llama-3-8B and Mixtral-8x7B), the instruct models\nperformed significantly worse: less than 2% Very good, less than 12% Good.\nC.2 MDS results for all models\nWe present a multidimensional scaling (MDS) projection in Figures S2, S3\nand S10, computed on all pairwise distances of the WVS data and all 13 LLM\ngenerations pooled together (as well as the linear and random baseline models).\nThe plots indicate that LLMs, whether used with next-token probability or\nfull answer generation, show less adaptability to socio-demographics than\nground truth data, and than a linear model of that data.\n73\nThe plots include the average nEMD prediction error of each LLM on each\nquestion, which is generally high for all LLMs. This highlights the fact\nthat even if an LLM gives answer patterns that occupy a large portion of\nthe region spanned by actual survey answers (e.g. Llama-3-70B with FA),\nthis does not imply that it can associate each answer pattern to the correct\nsocio-demographics.\nIt is notable in Figure S3 that the answer clouds of instruct models are placed\nat the edge (or even outside) the WVS cloud, reflecting the fact that these\nmodels tend to output distributions heavily skewed towards a few answer\nlevels. GPT-4 and the two open instruct models, for instance, almost always\ngive 100% probability to the answer that “people cannot be trusted”. For this\nreason, and as expected (see B.1), instruct or chat models seem particularly\nill-suited for the task of silicon sampling.\n74\nFigure S2: Global MDS visualization for linear and random baselines and all\nfoundational models with next-token probabilities (NTP) generation.\nErr indicates the average nEMD distance of LLM subpopulation answer\ndistributions with their WVS counterpart.\n75\nFigure S3: Global MDS visualization for linear and random baselines and\nall instruct models. GPT-3.5 generations are by full answer generation (FA),\nwhile GPT-4, Llama-3-8B and Mixtral-8x7B by next-token probabilities\n(NTP).\nErr indicates the average nEMD distance of LLM subpopulation answer\ndistributions with their WVS counterpart.\n76\nFigure S4: Global MDS visualization for linear and random baselines and all\nfoundational models with full answer generation (FA).\nErr indicates the average nEMD distance of LLM subpopulation answer\ndistributions with their WVS counterpart.\n77\nC.3 Regression results for GPT-4 and Llama-3-70B\nFigure S5: Coefficients in the Social model for GPT-4-Turbo (eq. Figure 3 in\nmain text)\n78\nFigure S6: Coefficients in the Full model for GPT-4-Turbo (eq. Figure 6 in\nmain text)\n79\nFigure S7: Coefficients in the Social model for Llama-3-70B NTP (eq. Figure\n3 in main text)\n80\nFigure S8: Coefficients in the Full model for Llama-3-70B NTP (eq. Figure 6\nin main text)\n81\nC.4 Regression quality for all models\nFigure S9 displays the adjustedR2 of theSocial and Center regressions for\nall LLMs and questions. Results are consistent with the ones presented in\nthe main text (Table 5): allCenter models, with only one regressor, give\nbetter prediction of LLM error than theSocial models. Interestingly, the\npositive correlation (0.4) of these two measures indicates that the better the\nLLM error can be predicted by socio-demographic controls, the more these\npredictors can be explained away by the central tendency of LLM answer\npatterns.\nFigure S9: Adjusted R2 of Social and Center models, all LLMs and questions\n82\nD Robustness checks\nD.1 Answer generation strategies\nTo generate answers from the models, we resorted to the two alternative\nstrategies found in the literature (see Appendix B.3 for details about these\nstrategies). The first is called ’full answer generation’ (FA). Every time it is\nprompted, the model is free to generate a number of tokens sufficient to cover\nall possible valid answers, producing a single answer that may be invalid or\nrequire post-processing. The second strategy is called “next-token probability”\n(NTP), where at each prompt the model is asked to generate only the single\nnext token, in the form of a probability distribution over the thousands of\ntokens embedded in the LLM.\nTheoretically, with a temperature of 1 and perfect model compliance of NTP,\nthe probability distributions yielded by FA should converge towards those\nof NTP, given enough FA generations for each profile. Simple simulation\nexperiments show, however, that several hundreds of FA generations are\nneeded for each prompt to yield consistently good approximation of NTP\ndistributions (as measured by nEMD), especially when the NTP distributions\nare balanced. When the temperature is different than 1, FA distributions do\nnot converge to the NTP ones: temperatures lower than 1 favor the most likely\nanswer, temperatures higher than 1 result in more balanced distributions.\nTo assess the variations between these two strategies in our experiments, we\ncompare their results on two different models (Llama-3-70B and Mixtral-\n8x7B). The responses provided by the two strategies differ in part. The\nanswers obtained with FA are more noisy (being stochastic), and somewhat\ndifferent from their NTP counterparts. The main conclusions, however,\n83\nhold for both generating strategies. Table S5 demonstrates that no LLM-\nstrategy combination rises above the “mediocre” level of representation quality.\nConclusions from the regression models for social and machine bias are not\naffected either (see Figure S11 for the estimates of the Full model on Llama-\n33B with full answer generation, to be compared with Figure S8 for the NTP\nstrategy.).\nFigure S10: MDS visualization of Llama-3-70B and Mixtral-8x7B, with the\nnext token probabilities (NTP) and full answer generation (FA) strategies.\nErr indicates the mean nEMD distance between LLM answers and their WVS\ncounterparts.\n84\nFigure S11: Coefficients in the Full model for Llama-3-70B with Full Answer\n(FA) generation (eq. Figure 6 in main text, and S8 for the same model and a\ndifferent answer generation strategy).\nD.2 Prompting style (Interview, Chat, First Person)\nIn order to assess the robustness of our results based on the prompting strategy,\nwe tested different approaches. To this effect, we ran an experience on the US\ndata with GPT-4 Turbo, testing three different prompting strategies. The\ninterview one is the one presented above, in the text (section “Generated\n85\ndata: Surveying a large language model”). Thechat approach is the same as\nthe interview one, except that the question-answer pairs are fed to the model\nin the specific “chat” format used by chat assistants, instead of writing all\nthe context variables as a single text block. Finally, thefirst personapproach\nintroduces the socio-demographic variables directly in the “system” prompt,\na technique supposed to help confer a persona to the LLM; we do so using\na first-person format (“You are a 45 year old woman. You have a middle\neducation level ...”), followed by a chat-format last question. We followed\neach strategy on the 140 US subpopulations, which amounts to nearly 2,800\nunique prompts per question and per strategy.\nThe results reveal some differences. Table S6 shows the median pairwise\ndistances between the nEMD of the various prompting strategies. It shows the\ninterview and chat strategies give answers that are both similar to each other\n(median nEMD .01), and less similar to the first person strategy (.05). Put\notherwise, different prompting strategies lead to different answers, although\nthedifferencesmaybesmall. FigureS12showsamultidimensionalscalingwith\nWVS and LLMs responses from the three prompting strategies. We clearly\nsee that the position of the answer patterns can differ, but that conclusions\nabout the distance to the human response and the low adaptability of the\nLLM responses hold.\nThere is one notable example of a strategy that fails where others succeed.\nWhen prompted in first person on the ’politics’ question, GPT-4-Turbo (NTP)\nalways outputs a perfectly balanced political spectrum (10% for each). This\nis not because it has learned to be equanimous, but because it refuses to\ndiscuss its political position. When asked the political position question in\nfirst person, GPT-4 assigns very low next-token probabilities to the expected\nanswers (the 10 digits from 0 to 9 have a total probability close to 0), and high\n86\nprobabilities to answers such as “As a language model, I cannot...”. The NTP\nmethod retains only the probabilities of expected answers and normalizes\nthem to 1, which in this case amounts to normalizing 10 negligible quantities\nto 1, and results in perfectly uniform political positioning.\nTable S6: Median one-to-one nEMD distances on answer distributions, for\ndifferent prompting strategies (GPT-4-Turbo, next-token probabilities, on 140\nUS subpopulations, all questions pooled). The median nEMD between the\noutputs of the Standard and Chat prompts with the same socio-demographic\nvalues is 0.012 (very low).\nInterview Chat FirstPerson\nInterview 0.000 0.012 0.052\nChat 0.012 0.000 0.053\nFirstPerson 0.052 0.053 0.000\n87\nFigure S12: MDS on WVS and GPT-4-Turbo with alternative prompting\nstrategies, on 140 US subpopulations. Err indicates the mean nEMD distance\nbetween LLM answers and their WVS counterparts.\nD.3 Generation temperature\nTemperature is another important hyperparameter in the LLM full-answer\ngeneration process. It used at generation time, and governs the choice of next\ntoken, based on the computed token probabilities. Temperature is typically\nset between 0 and 1.2, where 0 encourages the model to always choose the\nsingle most probable token, and higher values encourage it to choose less\nprobable tokens more frequently, resulting in more diverse and “creative”\noutputs.\n88\nThere is legitimate concern about the choice of temperature in the specific\ncase of survey answer simulation: different temperatures are known to give\ndifferent LLM outputs, and temperature affects answer variability. It should\nnonetheless be noted that this is only a relevant question for full answer\ngeneration, as the next-token probabilities method is deterministic and inde-\npendent of temperature. In order to investigate the impact of temperature, we\ndesigned an experiment on the US data (140 subpopulations, 5,400 prompts\nfor each question) using Mistral-7B with five different temperatures. We\nchose five temperatures: 0.3, 0.7, 1.0, 1.2, and 1.5, covering a wide part of\nthe spectrum of common temperature choices.\nIn Figure S13 below, we report the nEMD distributions (distances between\nLLM answer patterns and their ground-truth counterparts). Different temper-\natures lead to very similar distance distributions, with the low temperature\nof 0.3 (that favors the highest-probability answers) performing slightly worse\nthan the other ones.\nWe also report the multidimensional scaling of the pooled answer patterns\nof WVS and the three Mistral outputs, in Figure S14. It confirms the\nobservation that different temperatures lead to different answers: the LLM\npoints are placed in slightly different regions of answer space, according to\ntheir temperature. It also shows that the pattern of machine bias holds for\ndifferent temperatures: in each case, LLM answers have a low adaptability to\nsocio-demographic variables, and do not show a systematic social bias.\nOverall, these results show that despite its influence on model outputs,\ngeneration temperature does not affect our main argument.\n89\nFigure S13: Density of nEMD distances between LLM answers and ground\ntruth, for five different temperatures (logarithmic scale). Mistral-7B, full\nanswer generation. Upper panel is computed for all questions pooled together,\nlower panel is disaggregated by question.\n90\nFigure S14: Multidimensional scaling of subpopulation answer patterns, with\nfive generation temperatures (0.3, 0.7, 1.0, 1.2, 1.5). Mistral-7B, full answer\ngeneration. Err indicates the average nEMD distance between LLM and\ncorresponding WVS answers.\nD.4 Quantization\nFor the majority of open models reported in this paper, we used quantized\nversions of the models, because the unquantized versions did not fit our\nlimited GPU resources. As a reminder, in machine learning, quantizing a\n91\nmodel refers to the process of converting a model that uses high-precision\nfloating-point numbers (typically 32-bit, known as FP32) to one that uses\nlower precision numbers (such as 16-bit floating-point, 8-bit integers, etc.).\nThe main goal of quantization is to reduce the size of the model and also its\ncomputational requirements, which leads to faster inference times and lower\npower consumption, at the cost of reduced model quality.\nIn order to evaluate the robustness of our results to quantization, we run a\ntargeted experiment using a Mistral-7B with an NTP generation strategy,\nboth on an unquantized and on a 5-bit quantized model. We test the two of\nthem on a random sample of 2,000 unique prompts, covering all questions\nand socio-demographics in our data. For each of these prompts, we then\ncompute the nEMD distance between the unquantized and quantized answer\ndistributions.\nTable S7 shows the distribution of the nEMD per question. Quantized and\nunquantized answer distributions fall very close to one another, with distances\nvirtually always under the .05 threshold of what we called a “very good”\napproximation in the main article. From this we conclude that the use of a\nquantized model did not affect our conclusions.\nTable S7: Summary statistics for nEMD between the quantized and un-\nquantized versions of Mistral-7B. 2000 prompts, spanning all questions and\nsocio-demographic controls.\nReading: On the happiness question, the mean distance between the quantized\nand unquantized answers to a given prompt is 0.010.\nMin Q1 Median Mean Q3 Max\nHappiness 0.001 0.007 0.010 0.011 0.014 0.031\nPolitics 0.001 0.005 0.007 0.008 0.010 0.024\nReligion 0.002 0.006 0.010 0.012 0.016 0.044\nTrust 0.000 0.007 0.015 0.007 0.025 0.061\n92\nE Additional results\nE.1 Correlations of answer probabilities\nFigure S15: Correlation coefficients between the answer proportions of WVS\nandeachLLM(andbaselinelinearandrandommodels), forthe23independent\nanswers to the 4 survey questions. Each box summarizes 23 Spearman\ncorrelation coefficients.\nFor half of the possible answer levels, GPT-3.5 answer probabilities are\nnegatively correlated with WVS ones over the control variables.\nFigure S15 displays how LLM answers correlate with ground-truth WVS an-\nswers. For each of the 23 independent answer level in our data (dropping one\nlevel per question), we measure its percentage within each socio-demographic\nsubpopulation, for the WVS data and each LLM. We then compute the cor-\nrelation coefficients of each WVS–LLM pair, over the 687 socio-demographic\n93\nsubpopulations. This yields 23 correlation coefficients per LLM, which we\nsummarize in a box-and-whisker plot. As a benchmark, we also include the\ncorrelations given by the (cross-validated) linear baseline model, and the\nrandom permutation baseline.\nCorrelations over socio-demographic sub-populations are an interesting mea-\nsure because they allow us to ignore the means. Even in the presence of\na strong absolute bias, a LLM could be said to display some sociological\ncapabilities if it were able to replicate the variations of answer probabilities\nobserved in the survey data. For instance, on a given question a LLM could\noutput more answers ‘C’ overall, but still have answer “C” predictions that\nvary in the same directions over subpopulations as in the survey data. Fur-\nthermore, these correlations are akin to regression models in the sense that\nthey are measured on subpopulations defined by socio-demographic traits.\nCompared to the nEMD distance measures used in the main text however,\nthese single-answer correlations have the downside of taking a one-against-all\napproach, not allowing for little variations in the answer levels (e.g. answer\n“C” being more similar to “B” than to “A”).\nThe results presented in Figure S15 are less than flattering for the LLMs. Out\nof the thirteen combinations of model and generation strategy we tried, all\nbut one yield a median correlation of under 0.2, and all models except Llama-\n3-70B, Mixtral and GPT-4 show a third quartile well under 0.25, a low value\nfor linear correlation. Furthermore, all LLMs perform significantly worse than\na basic linear model (median correlation above 0.5), and two models (GPT-3.5\nand Llama-3-8B with FA generation) are on par with random permutations\nof the ground-truth answers. Overall, these results indicate that the LLMs\nwe studied are generally unable to account for how answer probabilities vary\nwith the socio-demographics in our sample.\n94\nE.2 Recognizing LLM-generated from true answer pat-\nterns\nTo confirm the mismatch between the responses of the LLM and those in the\nWVS, we use a simple discriminator test as proposed in Dominguez-Olmedo\net al. (2024). We train a random forest classifier to predict whether each\nanswer distribution was LLM-generated or ground-truth survey data. A good\nperformance of the classifier signifies that, even without information about\nthe socio-demographic variables information they are based on, LLM answer\npatterns are easily distinguishable from survey ones.\nUsing this strategy, for thirteen models and four questions, we find a median\ndiscrimination score of 96.8% (detailed results in Table S8). The result means\nthat a common classification algorithm is able, when presented with proba-\nbility distributions of responses, to near-perfectly predict whether they were\nproduced by a human sample or a LLM-generated one. The minimum scores\nare attained on the (binary) Trust question, with 82.9% for Mixtral-8x7B\namong NTP generations, and 63.5% for Mistral-7B among FA generations.\nOverall, LLM answers are thus clearly separated from real-world answers.\n95\nTable S8: Out-of-bag accuracy of a Random Forest classifier, trained to dis-\ntinguish between LLM and WVS answer distributions across subpopulations\nfor each question (%)\nHappiness Politics Religion Trust\nNext token probabilities\nGPT-4T 99.4 100.0 100.0 100.0\nLlama3-8B 99.6 99.9 99.8 90.1\nLlama-3-8B-Instruct 99.9 100.0 100.0 98.5\nLlama-3-70B 96.6 99.7 99.1 87.1\nMistral-7B 97.5 100.0 99.6 89.4\nMixtral-8x7B 96.9 100.0 99.6 82.9\nMixtral-8x7B-Instruct 99.9 99.9 99.9 100.0\nFull answer generation\nGPT-3 85.4 92.0 90.5 85.7\nGPT-3.5 81.9 98.6 99.9 83.5\nLlama-3-8B 84.3 96.2 89.6 75.8\nLlama-3-70B 68.5 85.7 87.8 75.0\nMistral-7B 77.4 95.0 99.8 63.5\nMixtral-8x7B 73.9 87.6 96.7 83.0\nE.3 Modeling answers instead of errors\nAn alternative regression strategy to the one presented in the main text would\nbe to use as outcome variable the probabilities of LLM- and ground-truth\nanswers, instead of modeling prediction error. In this strategy, proposed by\nBisbee et al.(2024), for each LLM and question we fit a multinomial linear\nmodel, on pooled ground-truth and LLM data, regressing answer probabilities\non all the socio-demographic variables, with an interaction effect on whether\nthe data is ground-truth or LLM-generated.\nThis model is aimed at measuring whether socio-demographics have a different\neffect on answer patterns in the ground truth and in LLMs. The specification\nis the following, whereiindicates the subpopublation,kthe question,anski is\n96\nthe vector of answers,ski is the vector of socio-demographics,Ii is a dummy\nvariable indicating whether the data is LLM-generated, andε the error term.\nanski = mlogit(αk + βk ×Ii + (µk + λk ×Ii) ×ski + εki)\nThe results are interpreted in two ways: a comparison between theµ and\n(λ+ µ) coefficients (they should be equal if the LLM perfectly reproduces\nthe social effects found in the ground truth), and significance tests on theλ\ncoefficients (they should not be significantly different from 0 in the perfect\nLLM scenario).\nBoth results are presented in Figure S16 for a selection of models (GPT-3\nand GPT-3.5 with FA generation, the others with NTP), and indicate that\nsocial predictors affect answer probabilities in a different way for LLM and\nground-truth data. These results are consistent with other ones presented\nin this article, such as the bad prediction quality of LLMs (see Table S5)\nand the lack of correlation between their outputs and ground-truth answers\n(see Figure S15). They cannot, however, be interpreted in terms of social or\nmachine bias.\n97\nFigure S16: Comparison between the ground-truth (horizontal) and LLM\n(vertical) coefficients of an Answer model. One point per model parameter,\ncolored by 99% significance. Blue line and confidence interval obtained by\nlinear regression. Points should be aligned on the dashed line and colored\nwhite if LLM and WVS answers were affected by the socio-demographic\ncontrols in the same way. 98\nE.4 Best-matching ground-truth demographics\nTable S9: Proportion (%) of WVS subpopulations reached by the complete\nset of LLM subpopulations, matched through 10-nearest-neighbors, based on\nnEMD distance of answer patterns.\nReading: On the happiness question, GPT-4-Turbo answer patterns reach\nonly 13.5% of the WVS subpopulations.\nLLM Happiness Politics Religion Trust\nNext token probabilities\nGPT-4T 13.5 6.7 39.3 1.5\nLlama-3-8B 8.0 6.6 9.3 18.3\nLlama-3-8B-Instruct 13.8 4.2 9.3 3.4\nLlama-3-70B 41.9 12.8 21.1 28.4\nMistral-7B 33.5 8.0 17.6 19.6\nMixtral-8x7B 36.7 8.6 10.3 48.3\nMixtral-8x7B-Instruct 9.8 10.8 1.8 1.5\nFull answer generation\nGPT-3 39.9 44.1 49.8 32.8\nGPT-3.5 64.8 22.4 4.8 74.1\nLlama-3-70B 83.3 69.5 68.7 56.8\nLlama-3-8B 62.0 50.2 55.0 58.8\nMistral-7B 62.9 54.9 25.8 85.3\nMixtral-8x7B 68.0 64.0 36.7 41.9\nAnother way to study the biases in LLM answers is to examine the ground-\ntruth subpopulations that each LLM subpopulation’s answers come closest\nto. In other words, to map each LLM subpopulation to its best-matching\nground-truth subpopulation(s), and then examine the properties of these best\nmatches. This can be achieved through ak-nearest-neighbor approach, where\nto each LLM subpopulation we assign thekground-truth subpopulations that\nhave the closest answer distributions. We report results fork = 10, where\neach LLM answer pattern in associated to its 10 closest WVS distributions.\nAs in the rest of this study, we use the normalized Earth Mover’s Distance to\n99\nmeasure the distance between LLM and WVS answer distributions.\nOur initial expectations, before running the LLM experiments, were that\nLLM answers would be biased towards US ground-truth answers and away\nfrom 1990s (pre-mass internet) ground-truth, because of the supposedly heavy\nproportion of English-language web data in LLMs’ training corpora. The\nmethods presented in this section are a simple way to rule out such hypotheses.\nSeveral results can be obtained from this best-matching approach. One is to\ncount the proportion of the ground-truth subpopulations that are “reached”\nby an LLM, i.e. that belong to thek nearest neighbors of the LLM answers.\nThis can be interpreted as a measure of dispersion of the LLM’s answer\ndistributions, relative to the ground truth dispersion. Table S9 presents\nthese dispersion results for our experiment, and confirm the low-adaptability\npattern that we observed in the rest of this study: only a small proportion\nof ground-truth observations are generally reached by LLM answers. The\npercentage of reached subpopulations in generally higher when using FA\ngeneration, which is to be expected as it is more noisy than NTP.\nAnother set of results can be gained from examining the socio-demographics\nof the matched ground-truth subpopulations, in a univariate way: to what\nground-truth countries (or decade) are the LLM answers for each country (resp.\ndecade) mapped? For instance, we can measure whether the LLM answers\ngenerated for Mexico are closest to the ground-truth answers of Mexico or of\nother countries. We summarize these per-variable matches into a Sankey plot,\nfor each LLM and question. We present the results for countries and decades,\nfor Llama-3-8B (NTP) in Figure S17, and GPT-3.5-Turbo (FA) in Figure S18.\nAs can be seen in these plots, there seems to be no systematic bias towards\nthe US or away from the 1990s across questions, or indeed to any particular\n100\ncountry or decade (which contradicts the social bias thesis). Furthermore, it\nis evident that there are both many mismatches on the subpopulation level\n(which indicates low representativity), and clear preferences of each LLM for\ncertain socio-demographics, preferences that seem to vary randomly across\nquestions (indicative of machine bias).\n101\nFigure S17: Country and decade of best-matching ground-truth subpopula-\ntions for Llama-3-8B (NTP). On the left is the country (or decade) of the\nLLM subpopulations, on the right are the country (or decade) proportions of\nthe 10 best-matching ground-truth WVS subpopulations.102\nFigure S18: Country and decade of best-matching ground-truth subpopu-\nlations for GPT-3.5. On the left is the country (or decade) of the LLM\nsubpopulations, on the right are the country (or decade) proportions of the\n10 best-matching ground-truth WVS subpopulations.103",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7585511207580566
    },
    {
      "name": "Point (geometry)",
      "score": 0.5886358022689819
    },
    {
      "name": "Generative model",
      "score": 0.5117287635803223
    },
    {
      "name": "Public opinion",
      "score": 0.49318164587020874
    },
    {
      "name": "Psychology",
      "score": 0.4878838062286377
    },
    {
      "name": "Term (time)",
      "score": 0.4282214939594269
    },
    {
      "name": "Social psychology",
      "score": 0.4150106906890869
    },
    {
      "name": "Data science",
      "score": 0.3774658143520355
    },
    {
      "name": "Computer science",
      "score": 0.3681347966194153
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3258269727230072
    },
    {
      "name": "Political science",
      "score": 0.3153483271598816
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2740181088447571
    },
    {
      "name": "Law",
      "score": 0.1168847382068634
    },
    {
      "name": "Politics",
      "score": 0.11527323722839355
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210144087",
      "name": "Centre d'Etudes et de Recherches Administratives, Politiques et Sociales",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I19370010",
      "name": "Orange (France)",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I142476485",
      "name": "École Polytechnique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210137396",
      "name": "Centre de Recherche en Économie et Statistique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I84009706",
      "name": "École Nationale de la Statistique et de l'Administration Économique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210097629",
      "name": "École Nationale d'Administration",
      "country": "SN"
    }
  ],
  "cited_by": 4
}