{
  "title": "Isolating Compiler Bugs by Generating Effective Witness Programs with Large Language Models",
  "url": "https://openalex.org/W4383175415",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4383175946",
      "name": "Tu, Haoxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2368466067",
      "name": "Zhou Zhi-de",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132203485",
      "name": "Jiang He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223668142",
      "name": "Yusuf, Imam Nur Bani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2434013583",
      "name": "Li, Yuxian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2348649521",
      "name": "Jiang, Lingxiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4312697818",
    "https://openalex.org/W4241947695",
    "https://openalex.org/W4389869683",
    "https://openalex.org/W4389519352",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2980518752",
    "https://openalex.org/W4384345728",
    "https://openalex.org/W1981537308",
    "https://openalex.org/W4293072363",
    "https://openalex.org/W3130011944",
    "https://openalex.org/W2968370566",
    "https://openalex.org/W4285177210",
    "https://openalex.org/W1491178396",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W41554520",
    "https://openalex.org/W1993255342",
    "https://openalex.org/W2046376809",
    "https://openalex.org/W1990785546",
    "https://openalex.org/W4287665430",
    "https://openalex.org/W2045428103",
    "https://openalex.org/W3124767051",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2138428785",
    "https://openalex.org/W4245487037",
    "https://openalex.org/W2900990156",
    "https://openalex.org/W2345350761",
    "https://openalex.org/W2999700924",
    "https://openalex.org/W4321184925",
    "https://openalex.org/W4230508460",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3007855180",
    "https://openalex.org/W4251988601",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4298857966",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W4384345708",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W2107726111",
    "https://openalex.org/W4379115941",
    "https://openalex.org/W4229901690",
    "https://openalex.org/W4378591002",
    "https://openalex.org/W3103568582",
    "https://openalex.org/W4377866432",
    "https://openalex.org/W4384345667",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4213287115",
    "https://openalex.org/W2620081107",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2461570336",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W2156737235",
    "https://openalex.org/W4286331373",
    "https://openalex.org/W2343875716",
    "https://openalex.org/W2010825534",
    "https://openalex.org/W4384345745",
    "https://openalex.org/W2964241064",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386629561",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W3153841236",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4386418332",
    "https://openalex.org/W4239235546",
    "https://openalex.org/W2126941999",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4367860052",
    "https://openalex.org/W4366204357",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4224060952"
  ],
  "abstract": "Compiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs is crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates the compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem, but they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress of pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI to utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation may not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts. To overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided prompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in programs for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to select specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select specialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with state-of-the-art approaches over 120 real bugs from GCC and LLVM, our evaluation demonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within Top-1/Top-5 ranked results. We also demonstrate that the LLMs component used in LLM4CBI can be easily replaced while still achieving reasonable results.",
  "full_text": "IEEE TRANSACTIONS ON SOFTWARE ENGINEERING 1\nIsolating Compiler Bugs by Generating Effective\nWitness Programs with Large Language Models\nHaoxin Tu, Zhide Zhou, He Jiang‚àó, Imam Nur Bani Yusuf, Yuxian Li, Lingxiao Jiang\nAbstract‚ÄîCompiler bugs pose a significant threat to safety-critical applications, and promptly as well as effectively isolating these bugs\nis crucial for assuring the quality of compilers. However, the limited availability of debugging information on reported bugs complicates\nthe compiler bug isolation task. Existing compiler bug isolation approaches convert the problem into a test program mutation problem,\nbut they are still limited by ineffective mutation strategies or high human effort requirements. Drawing inspiration from the recent progress\nof pre-trained Large Language Models (LLMs), such as ChatGPT, in code generation, we propose a new approach named LLM4CBI\nto utilize LLMs to generate effective test programs for compiler bug isolation. However, using LLMs directly for test program mutation\nmay not yield the desired results due to the challenges associated with formulating precise prompts and selecting specialized prompts.\nTo overcome the challenges, three new components are designed in LLM4CBI. First, LLM4CBI utilizes a program complexity-guided\nprompt production component, which leverages data and control flow analysis to identify the most valuable variables and locations in\nprograms for mutation. Second, LLM4CBI employs a memorized prompt selection component, which adopts reinforcement learning to\nselect specialized prompts for mutating test programs continuously. Third, a test program validation component is proposed to select\nspecialized feedback prompts to avoid repeating the same mistakes during the mutation process. Compared with the state-of-the-\nart approaches (DiWi and RecBi) over 120 real bugs from the two most popular compilers, namely GCC and LLVM, our evaluation\ndemonstrates the advantages of LLM4CBI: It can isolate 69.70%/21.74% and 24.44%/8.92% more bugs than DiWi and RecBi within\nTop-1/Top-5 ranked results. Additionally, we demonstrate that the LLMs component (i.e., GPT -3.5) used in LLM4CBI can be easily\nreplaced by other LLMs while still achieving reasonable results in comparison to related studies.\nIndex Terms‚ÄîSoftware Debugging, Bug Isolation, Compilers, GCC, LLVM, Reinforcement Learning, Large Language Models (LLMs)\n‚ú¶\n1 I NTRODUCTION\nC\nOMPILERS serve a fundamental role in building reliable\nsoftware systems, and bugs in compilers can have\ncatastrophic consequences [1], [2]. To mitigate such threats,\na crucial task is to isolate the bugs promptly and effec-\ntively. However, isolating compiler bugs poses significant\nchallenges due to the limited debugging information. A\nprevalent direction for resolving the problem is to transform\nthe bug isolation problem into a test program mutation\nproblem [3], [4]. The core idea behind such approaches is\nfirst to generate a set of witness (i.e., passing) test program\nmutants by mutating the given failing test program and\nthen collecting the passing and failing spectrum. Finally,\ncombined with Spectrum-based Fault Localization (SBFL)\ntechniques, suspicious files are ranked.\nHowever, existing program mutation strategies in DiWi\n[3] and RecBi [4] exhibit limitations in terms of effectiveness\nand demand substantial human effort. First, these strate-\n* He Jiang is the corresponding author.\n‚Ä¢ H. Tu is with the School of Software, Dalian University of Technology,\nDalian, China. H. Tu is also with the School of Computing and Infor-\nmation Systems, Singapore Management University, Singapore. E-mail:\nhaoxintu@gmail.com.\n‚Ä¢ Z. Zhou and H. Jiang are with the School of Software, Dalian University\nof Technology, Dalian, China, and Key Laboratory for Ubiquitous Net-\nwork and Service Software of Liaoning Province. H. Jiang is also with\nDUT Artificial Intelligence, Dalian, China. E-mail: jianghe@dlut.edu.cn,\ncszide@gmail.com.\n‚Ä¢ I. Yusuf, Y. Li, and L. Jiang are with the School of Computing\nand Information Systems, Singapore Management University, Singa-\npore. E-mails: imamy.2020@phdcs.smu.edu.sg, liyuxianjnu@gmail.com,\nlxjiang@smu.edu.sg.\ngies are limited in generating diverse test programs. DiWi\nonly supports the local mutation, such as changing the\ntype of a variable. Although RecBi supports more mutation\nstrategies, such as structural mutation, i.e., changing the\ncontrol flow of the program, it can only synthesize statement\nconditions ingredients without statement body (see more\ndetails in Section 2.2). Moreover, the random selection of\nvariables and locations for mutation in both DiWi and RecBi\nis limited by the diversity of the generated test programs.\nSecond, the mutation process in DiWi and RecBi requires\nsignificant human effort. Before mutation, DiWi and RecBi\nrequire manual code additions to collect necessary con-\ntext information, and RecBi necessitates the construction\nof ingredients from existing test programs. Additionally,\nthese strategies pay insufficient attention to the validity\nof the generated programs that contain undefined behav-\niors, thereby reducing their effectiveness of bug isolation.\nOverall, these limitations in ineffective mutation and the\nassociated human effort emphasize the need for improved\nprogram mutation strategies in bug isolation.\nInspired by the recent progress of pre-trained Large\nLanguage Models (LLMs) (e.g., ChatGPT [5]) in code\ngeneration, we propose LLM4CBI, i.e., pre-trained Large\nLanguage Models for Compiler Bug Isolation. Our key in-\nsights are that (1) LLMs are trained with large-scale datasets\nof code, so the test programs produced by LLMs tend to be\ndiverse; (2) LLMs have a good learning & reflection mech-\nanism to help generate better outputs based on the users‚Äô\nfeedback following a prompt-response dialog paradigm; (3)\nprompts used by LLMs can be expressed in a natural lan-\nguage, easier for users to use and reducing the human effort\narXiv:2307.00593v3  [cs.SE]  8 May 2024\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 2\nto finish a task. Thus, adapting LLMs to generate effective\ntest programs could be promising. However, directly using\nLLMs to generate effective test programs for compiler bug\nisolation presents several difficulties, raising the following\ntwo challenges that need to be addressed.\nChallenge 1: Formulation of Precise Prompts. The quality\nof prompts plays a crucial role in employing the program\nmutation capabilities of LLMs [6], but using existing natural\nmutation descriptions as prompts may not be effective.\nFor example, the mutation rule ‚Äú insert an if statement ‚Äù is\na mutation description used in existing work RecBi [4].\nSuch description lacks precision on which variables to use\nand where to insert the statement, limiting the possibility of\nmutating failing test programs (i.e., those that can trigger\nthe bug) into passing ones (i.e., those that can not trigger\nthe bug). Due to the difficulties in selecting precise variables\nand locations, the existing approaches DiWi [3] and RecBi\n[4] randomly select them, which is shown to be ineffective\n(see more evaluation results in Section 4.4). Therefore, it is\nnecessary to formulate precise mutation prompts to assist\nLLMs in generating effective test programs.\nChallenge 2: Selection of Specialized Prompts. When several\nprompts are collected, selecting the specialized ones for\nmutating specific failing test programs is important and\nchallenging. The reasons are two-fold. First, compiler bugs\ntend to be different and have different language features.\nOne prompt may be useful for mutating one particular\nfailing test program but may not be helpful for another\nfailing test program. Randomly selecting prompts to mu-\ntate test programs may be ineffective (see more evaluation\nresults in Section 4.4). Second, LLMs may make different\nmistakes when mutating different test programs, and dif-\nferent feedback prompts should also be given to different\ntest programs. Therefore, a new prompt selection strategy is\nneeded to select specialized prompts.\nTo overcome the above challenges, three new compo-\nnents are designed in LLM4CBI to utilize LLMs for generat-\ning effective test programs for compiler bug isolation. First,\na precise prompt production componentis designed to ad-\ndress the first challenge. A precise prompt pattern that could\naccurately represent the desired mutation operations is first\nintroduced. Then, program complexity metrics measured by\nthe data and control flow analysis are utilized to identify\nthe most relevant variables and optimal insertion locations.\nSecond, two new components, i.e., a memorized prompt\nselection component and a lightweight test program val-\nidation component, are proposed for selecting specialized\nprompts. In the prompt selection component, LLM4CBI\nincorporates memorized search via reinforcement learning\nto track and accumulate rewards based on the performance\nof LLMs, which allows LLM4CBI to continually select the\nspecialized prompts for mutating specific test programs. In\nthe test programs validation component, LLM4CBI lever-\nages a static analysis to detect and filter out potential invalid\ntest programs that may contain undefined behaviors, thus\nmitigating the risks associated with invalid programs.\nEmpirical evaluations over 120 real bugs from the two\nmost popular C open-source compilers, i.e., GCC and\nLLVM, are conducted to demonstrate the effectiveness\nof LLM4CBI. First, we have compared LLM4CBI with\ntwo state-of-the-art approaches (i.e., DiWi [3] and RecBi\n[4]) in terms of compiler bug isolation capabilities. The\nresults show LLM4CBI can isolate 69.70%/21.74% and\n24.44%/8.92% more bugs than DiWi and RecBi within\nTop-1/Top-5 ranked results, respectively. Second, we have\nevaluated the effectiveness of three new components of\nLLM4CBI, and the results show that all the components\ncontribute to the effectiveness of LLM4CBI. Third, we\ndemonstrate that LLM4CBI is extensible, i.e., the LLMs\ncomponent (i.e., GPT-3.5) used in LLM4CBI can be easily\nreplaced by other LLMs (e.g., Alpaca [7], Vicuna [8], and\nGPT4ALL [9]) while still achieving reasonable results in\ncomparison to related studies.\nContributions. We make the following contributions:\n‚Ä¢ To our knowledge, LLM4CBI is the first work aiming\nto leverage the capabilities of LLMs for compiler bug\nisolation tasks in the field.\n‚Ä¢ Three new components, i.e., precise prompt production,\nmemorized prompt selection, and lightweight test pro-\ngram validation, are proposed to guide LLMs to generate\neffective test programs for compiler bug isolation.\n‚Ä¢ Our empirical evaluation demonstrates that LLM4CBI\nis not only effective for compiler bug isolation but also\nextensible because other LLMs can be easily integrated\ninto LLM4CBI.\n‚Ä¢ LLM4CBI 1 paves the way for future research in compiler\nbug isolation, opening exciting opportunities to further\nexplore and leverage the capabilities of LLMs for more\nefficient and effective bug isolation techniques.\nOrganizations. Section 2 gives the background and our mo-\ntivation. Section 3 describes the design of LLM4CBI. Section\n4 presents the implementation and the evaluation results.\nSection 5 and 6 discuss the limitations of our approach and\nthreats to validity. Section 7 describes related work, and\nSection 8 concludes with future work.\n2 B ACKGROUND AND MOTIVATION\nIn this section, we first give the background about test\nprogram mutation for compiler bug isolation and Large\nLanguage Models (LLMs). Then, we use an example to il-\nlustrate the limitations of existing approaches and highlight\nthe advantages of our approach.\n2.1 Background\n2.1.1 Test Program Mutation for Compiler Bug Isolation\nBug isolation is one of the most important activities in\nsoftware debugging [10], [11], [12], [13], [14]. Fig. 1 ex-\nemplifies the prevalent workflow of existing compiler bug\nisolation approaches [3], [4]. Given a failing test program\nthat can trigger the compiler bug, existing approaches first\nuse different mutation strategies to produce a passing test\nprogram that does not trigger any bugs (we refer to such\npassing test programs to witness test programs following\nexisting studies [3], [4]). Then, both the failing and passing\ntest programs are subjected to compilation, enabling the col-\nlection of code coverage information of the compiler source\n1. The source code of LLM4CBI is publicly available at https:\n//github.com/haoxintu/LLM4CBI.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 3\nHigh-level\nAfailingtestprogram(F) Failingandpassingspectrum\nRankedListofSuspiciousFiles\n(action)Promptsstatistics(state)(reward)updateupdate\nmutateApassingtestprogram(P) +SBFL\n1,0,11,1,11,0,01,0,01,0,11,0,0compile\ncompile\ncollect\nFig. 1. General workflow of existing compiler bug isolation\nfiles. All the compiler files that are covered by a failing\ntest program during compilation are considered suspicious.\nConversely, the passing test program serves to mitigate\nsuspicions regarding innocent files that may have been im-\nplicated. To eventually isolate the buggy files, following the\nwell-established principles of Spectrum-Based Fault Local-\nization (SBFL) [15], [16], they compare the execution traces\n(or spectra) between failing test programs and passing test\nprograms using a formula such as Ochiai [15]. Two recent\nstudies, i.e., DiWi [3] and RecBi [4], follow the same strategy,\nand their goal is to generate a set of programs that have slightly\ndifferent control- and data-flow information compared with the\nfalling test program to flip the compiler execution results (i.e.,\nfrom failing to passing). We aim to achieve the same goal by\nleveraging a new approach based on LLMs in this study.\n2.1.2 Suspicious Files Ranking\nLLM4CBI utilizes the concept of SBFL (Spectrum-Based\nFault Localization) to identify potentially buggy compiler\nfiles by comparing the coverage of failing and passing tests,\nas outlined in previous research [3], [4]. To be specific, since\nOchiai [15] perform well, LLM4CBI employs the Ochiai\nEquation score(s) = efs‚àö\n(efs+nfs)(efs+eps)\nto calculate the\nsuspicious score of each statement. In the Equation, efs and\nnfs represent the number of failing tests that execute and do\nnot execute statement s, and eps represents the number of\npassing tests that execute statement s. In LLM4CBI, since\nthere is only one given failing test program, the number\nof failing tests that execute statement s (efs) is fixed at 1.\nAdditionally, LLM4CBI focuses solely on the statements\nexecuted by the given failing test program, implying that\nthe number of failing tests that do not execute statement s\n(nfs) is 0. As a result, the Ochiai Equation is simplified to:\nscore(s) = 1‚àö\n(1+eps)\n.\nOnce the suspicious score of each statement is obtained,\nLLM4CBI proceeds to calculate the suspicious score of each\ncompiler file. Similar to previous studies [3], [4], LLM4CBI\naggregates the suspicious scores of the statements executed\nby the given failing test program within a compiler file to\ndetermine the suspicious score of the file SCORE (f) =Pnf\ni=1 score(si)\nnf\n, where the number of statements that the fail-\ning test program executes in the compiler file f is denoted\nas nf . LLM4CBI utilizes this information to calculate the\nsuspicious score of each compiler file. By arranging the\ncompiler files based on their suspicious scores in descending\norder, LLM4CBI yields a ranking list.\n2.1.3 Large Language Models (LLMs)\nRecently, pre-trained Large Language Models (LLMs) such\nas ChatGPT [5] become ubiquitous and have exhibited re-\nmarkable performance in numerous tasks, such as machine\ntranslation [17], text summarization [18], classification [19]\nand code generation [20]. Technically, LLMs can be directly\nemployed to tackle specific downstream tasks by provid-\ning the task description to the model, which is known as\nprompt, without fine-tuning on specialized datasets. This is\nachieved through a technique known as prompt engineering\n[21], [22], [23], [24]. Prompt engineering aims to find the\nprompt that yields the best performance on specific tasks.\nPrior studies show that prompt engineering can achieve\nstate-of-the-art performance on various downstream tasks\n[25], [26]. Benefiting from the huge potential of LLMs, there\nare increasing recent works showing that LLMs can be used\nfor solving different tasks in software engineering [6], [23],\n[27], [28], [29]. In this study, we aim to unleash the power of\nLLMs in the field of test program generation tasks.\nMany existing LLMs adopt the decoder of the Trans-\nformer architecture [30]. Given a prompt containing the\ntask description, the decoder generates the test programs\nY as a sequence of tokens, token-by-token, by following\nEquation 1,\nyt = arg max\ny\nP(y|p, y<t) (1)\nwhere yt is the current token to be predicted, y<t refers\nto all the previously predicted tokens, and p is the input\nprompt. The equation states that the current token yt to\nbe predicted is determined by selecting the token y that\nmaximizes the conditional probability P(y|p, y<t), given the\nprompt p and the previously generated tokens y<t. Because\na generated test program Y depends on the input prompt p\nand the search space for the input prompt p is huge, finding\nthe best prompt p to generate the effective test program\nY can be challenging. In this work, we propose LLM4CBI\nto automatically find the prompt p that can generate more\neffective test programsY for the compiler bug isolation task.\nTwo main categories of LLMs are available to the com-\nmunity for code generation tasks: infilling and general [20],\n[29], [31]. Infilling models (e.g., CodeGen [32], Incoder [33],\nand PolyCoder [34]) are used to insert the most natural code\nbased on bi-directional context (e.g., in the middle of a code\nsnippet), while general models (e.g., LLaMA [35], Alpaca\n[7], ChatGPT [5], Vicuna [8], and GPT4ALL [9]) target to\ngenerate a complete code snippet given the left context\nonly by a natural language description. In this study, we\nconsider general models mainly due to the fact that general\nmodels follow the prompt-response dialog paradigm, which\ninvolves minimal human effort and fits our objective.\n2.2 Motivating Example\nFig. 2(1) showcases a failing test program that exposes\na bug in the LLVM-3.4 compiler at the -O3 optimization\nlevel. The program introduces a division by zero in\nthe induction variable elimination optimization pass in LLVM,\ncausing the miscompilation bug. Notably, Fig. 2(2) repre-\nsents a passing test program generated by our proposed\nsolution (LLM4CBI), which can not trigger the bug in the\nLLVM compiler. Next, we show the limitations of existing\napproaches and the advantages of our approach in generat-\ning the passing test program.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 4\n1 short s; int a, b, c;\n2 volatile int v;\n3 static int u[] = {0,0,0,0,0,1};\n4\n5 void foo() {\n6 int i , j ;\n7 for (; b <= 0; ++b) {\n8 int k; int d = 0;\n9 for (; d <= 5; d++) {\n10 int *l = &c;\n11 int e = 0;\n12 for (; e <= 0; e++) {\n13 int *m = &k;\n14 unsigned int n = u[d];\n15 i = ! a ? n : n / a;\n16 j = s ? 0 : (1 >> v);\n17 *m = j;\n18 }\n19 *l = k < i ;\n20 }\n21 }\n22 }\n23 int main() {\n24 foo() ;\n25 return 0;\n26 }\n27 $clang ‚àíO2 test.c ; ./a. out\n28 $clang ‚àíO3 test.c ; ./a. out\n29 $Floating point exception ...\nListing 1. faling test program\n1 short s; int a, b, c;\n2 volatile int v;\n3 static int u[] = {0,0,0,0,0,1};\n4\n5 void foo() {\n6 int i , j ;\n7 for (; b <= 0; ++b) {\n8 int k; int d = 0;\n9 for (; d <= 5; d++) {\n10 int *l = &c;\n11 int e = 0;\n12 for (; e <= 0; e++) {\n13 int *m = &k;\n14 unsigned int n = u[d];\n15 i = ! a ? n : n / a;\n16 j = s ? 0 : (1 >> v);\n17 *m = j;\n18 if (a==0) v = s;\n19 else if(a>10) s=a+1;\n20 else s = 1;\n21 }\n22 *l = k < i ;\n23 }\n24 }\n25 }\n26 int main() {\n27 foo() ;\n28 return 0;\n29 }\nListing 2. passing test program\nFig. 2. LLVM bug #16041 (the highlighted gray code in (b) is generated\nby LLM4CBI)\n2.2.1 Limitations in Existing Approaches\nExisting approaches, such as DiWi [3] and RecBi [4], face\nlimitations in generating the above passing test programs\ndue to three primary reasons. First, the mutation strategies\nemployed in DiWi and RecBi exhibit certain limitations. For\ninstance, DiWi only supports local mutation operators that\ndo not alter the control flow of the failing test program.\nConsequently, it cannot generate crucial elements such as\nthe highlighted gray if statement ‚Äúif (a == 0)‚Äù shown in\nFig. 2(2). Similarly, RecBi allows the insertion of structural\nconditions, like ‚Äúif (a == 0)‚Äù, but lacks the capability to\ngenerate the corresponding statement bodies, such as ‚Äús\n= v;‚Äù. Moreover, both DiWi and RecBi rely on a random\nselection strategy for determining which variables to use\nand where to insert them during the mutation process,\nleaving the transformation outcomes largely dependent on\nchance. Second, the mutation process involved in existing\napproaches necessitates substantial human effort. Before\nmutation, additional code must be written to collect es-\nsential contextual information (e.g., the names of variables\nlike s, a, b, c and their defined types) and extract rel-\nevant elements (e.g., if statements) from the existing test\nprograms. Manual coding is required to randomly deter-\nmine suitable locations for inserting the new code snippets\ninto the failing test programs during the mutation. Third,\nthey pay little attention to the validity of the generated\ntest program, which can have the side effect of compiler\nbug isolation. In summary, the aforementioned limitations\nunderscore the ineffectiveness and high demand of human\nefforts of existing approaches in generating high-quality test\nprograms. These challenges highlight the need for a new\nsolution that overcomes these shortcomings and effectively\ngenerates passing test programs.\n2.2.2 Advantages of Our Approach\nCompared to DiWi [3] and RecBi [4], LLM4CBI excels\nin generating effective passing test programs by taming\nthe capabilities in LLMs. First, a precise prompt such as\n‚ÄúPlease generate a variant program P of the input program F\nby inserting an if statement and reusing the variables in the list\n{a, s, v} between lines 12-18 ‚Äù is produced to guide LLM in\nmutating the given program following certain requirements.\nInstead of using a vague prompt, LLM4CBI considers more\ndetailed information in the program that can increase the\nlikelihood of flipping from failing to passing to construct\na precise prompt. In this way, the new test program is\ngenerated by LLM4CBI via inserting the new if-statement\nshown in gray in Fig. 2(2). Notably, LLM4CBI supports the\ninsertion of bodies (such as ‚Äús = v;‚Äù) in structural mutation,\nwhich increases the diversity of the generated test program.\nSecond, benefiting from the LLMs‚Äô prompt-response dialog\nparadigm, the whole mutation process only involves little\nhuman effort compared with previous studies. In addition,\nLLM4CBI detects and filters potential invalid test programs\nthat contain undefined behaviors, further boosting the capa-\nbilities of LLM4CBI in terms of compiler bug isolation.\n3 T HE DESIGN OF LLM4CBI\nOverview. Fig. 3 illustrates the general workflow of\nLLM4CBI, which addresses the two main challenges and\nleverages the capabilities of LLMs to generate effective\ntest programs for compiler bug isolation. LLM4CBI first\ngenerates precise prompts and collects all the generated\nprompts ( 1 ) in the precise prompt production component.\nThen, LLM4CBI selects a prompt ( 2 ) via a memorized\nprompt selection component and utilizes it as input for\nthe LLMs ( 3 ). Next, the LLM4CBI produces a new test\nprogram ( 4 ), which undergoes a test program validation\ncomponent ( 5 ). If the generated program is valid, it is\ncompiled ( 6 ), and coverage information of compiler source\nfiles is collected. Also, the quality of the generated test\nprogram will be measured with similarity and diversity\nmetrics in ( 7 ), which serve as the input of the memorized\nprompt selection component to help select better prompts.\nHowever, if the program is invalid due to semantic errors,\nLLM4CBI provides the feedback prompts ( 8 ) to the LLMs,\nguiding them not to make the same mistakes again. Ul-\ntimately, upon reaching the termination condition (e.g., 1\nhour), LLM4CBI employs SBFL along with the failing and\npassing spectra to rank suspicious files ( 9 ). Specifically,\nthe precise prompt production component is designed to\naddress the first challenge of the formulation of precise\nprompts, and two other components, i.e., the memorized\nprompt selection component and lightweight test program\nvalidation component, are utilized to tackle the second chal-\nlenge of selecting specialized prompts. We provide further\ndetails regarding this in the subsequent sections.\n3.1 Precise Prompt Production\nThis section presents the precise prompt production compo-\nnent, which addresses the first challenge of the formulation\nof precise prompts. We first outline the design of the prompt\nproduction pattern for LLMs and then show how to utilize\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 5\nSMU Classification: Restricted\nHigh-level:LLM4CBI\nAfailingtestprogram(F)\nTestProgramValidationAnewtestProgram(P)MemorizedPromptSelection\nFailing&passingSpectrum\nRankedListofSuspiciousFiles\ncalculatereward/lossoftheprompt\n+\nLLMs\ne.g.,‚ÄúPleasegenerateavariantPofprogramFbyinsertinganifstatementandreusingthevariablesinthelist{a,b}betweenlines11-27‚Äù\nPattern:PleasegenerateavariantPofprogramFby<mutation>andreusing<variables>between<location>\ninput\noutputvalidatecompile\nselect feedback\n1243 567\n+\na=b+1;b=10;c=10;\nb=11;b=-1;\na=a+b;\n(similarityanddiversityofthegeneratedprogram)\n+SBFL\ne.g.,‚Äúinsertinganifstatement‚ÄùMutationDescriptione.g., {a, b}Data-flowanalysise.g., {(11, 27)}Control-flowanalysis\n+ 2\n4\n5 6\n8\n7 9\n3\n1\nPrecisePromptProduction\nFig. 3. Overview design of LLM4CBI\nprogram complexity metrics, e.g., data-flow and control-\nflow complexity, to populate the pattern.\n3.1.1 Prompt Pattern for Program Mutation\nThe following is the pattern designed in LLM4CBI for\nconstructing effective prompts for LLMs.\nPattern: Please generate a variant program P of the\ninput program F by <mutation rule > and reusing the\n<variables > between <location > .\nIn the pattern, P refers to the newly generated test\nprogram, and F refers to the given failing test program. In\nthe rest, <mutation rule > means the actual mutation oper-\nation; <variables > and <location > describe the specific\nrequirements when conducting the mutation.\nWe reuse the existing mutation rules (cf Table 1) in the\npattern. Since compilers use different strategies to optimize\nthe programs that have different data or control flow [2], our\nintuition is that, instead of randomly mutating programs\nin existing approaches, mutating the most complex part of the\nfailing program is more likely to flip the failing into passing . To\nthis end, LLM4CBI measures the most complex part by two\nmeans: the variable that holds complex data flow and the\nlocation that involves complex control flow.\n3.1.2 Data-flow Complexity-guided Variable Selection\nThe data-flow analysis aims to output the most complex\nvariables defined and used in the given failing test program.\nDue to the fact that we aim to investigate the complexity\nof a variable by examining how this variable can affect and\nto what extent, we deem that the more a variable is defined\n(or assigned), the more complex dependence is held on the\nvariable, thus contributing to the data-flow complexity.\nFollowing the existing definition of data-flow complexity\n[36], we follow the existing work [37] and opt for the vari-\nable def-use chain [38] to analyze the data-flow complexity\nof a program. Specifically, we calculate the complexity of a\nvariable (Compvar) by the following Equation:\nCompvar = Ndef + Nuse (2)\nwhere the Ndef counts the number of times that a variable\nis defined, including redefining or assigning. Note that Def\nkeeps track of the changes in a variable, so the data depen-\ndency analysis is included. Nuse counts the number of times\nthat a variable is used. This refers to the value of a variable\nvalue being used in some computation with no modification\nto the variable‚Äôs value. Under the above Equation, the data\nflow analysis upon the failing test program in Fig. 2(1) will\nproduce a variable list {a, s, v}, which represents the\nTop-3 complex variables used in the program.\n3.1.3 Control-flow Complexity-guided Location Selection\nThe aim of the control-flow analysis is to output the location\nof the most complex statement in the program. We leverage\nthe control-flow graph of the input failing program, where\neach node represents the statement and the edge indicates\nthe execution flow. When the Control-Flow Graph (i.e., CFG)\nis available, we calculate the complexity of each statement\nusing the following Equation (3) based on cyclomatic com-\nplexity [36]:\nCompcontrol = Nedge ‚àí Nnode + 2 (3)\nwhere Nedge and Nnode represent the number of edges\nand nodes in the CFG, respectively. Since the cyclomatic\ncomplexity is not designed to measure the complexity at the\nstatement level, we count the complexity of each statement\nby obtaining the complexity values during cyclomatic com-\nplexity calculation. Note that we intentionally ignore the\nstatement that includes the oracle (i.e., having the printf\nor abort function). The reason is that changing the code\nblock, including the test oracle, is more likely to break the\noracle, meaning a fake passing test program is probably\ngenerated [3], [4]. Taking the code example shown in Fig.\n2(1) again, the control-flow analysis designed in LLM4CBI\nwill indicate that the most complex control flow lies in the\nfor loop between Lines 12 - 18. In this way, after getting\nthe variable list and the desired location to be inserted,\nLLM4CBI generates certain prompts based on the designed\npattern. For example, one of the generated prompts is:\n‚ÄúPlease generate a variant program P of the input pro-\ngram F by <inserting an if statement > and reusing the\n<variables in the list {a, s, v} > between <lines 12-18 > ‚Äù.\nAs aforementioned in Section 1, not every prompt con-\ntributes equally to a specific failing test program. Further-\nmore, LLMs may make different mistakes when mutating a\nprogram: e.g., LLMs may incur a syntax error in a failing test\nprogram but a semantic error in another program. Giving all\nthe error information as feedback prompts is not necessary.\nHence, a specialized prompt selection strategy is supported\nand developed in LLM4CBI.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 6\nTABLE 1\nMutation rules applied in the prompt pattern\nID Description of Rules\n1 inserting an if statement\n2 inserting a loop (i.e., while or for) statement\n3 inserting a function call\n4 inserting a goto statement\n5 inserting a qualifier (i.e., volatile, const, and restrict)\n6 removing a qualifier (i.e., volatile, const, and restrict)\n7 inserting a modifier (i.e., long, short, signed, and unsigned)\n8 removing a modifier (i.e., long, short, signed, and unsigned)\n9 replacing a constant with another valid one\n10 replacing a binary operator with another valid one\n11 removing a unary operator on the variables\n12 replacing a unary operator on the variables\n13 replacing a variable with another valid one\n3.2 Memorized Prompt Selection\nThis subsection and the next subsection present memorized\nprompt selection and lightweight test program validation\ncomponents to address the second challenge of selecting the\nspecialized prompts. In this subsection, we first give the\nbackground of reinforcement learning and then detail the\nmemorized prompt selection.\n3.2.1 Reinforcement Learning\nReinforcement learning (a kind of memorized search) is\na field of study that aims to teach an agent how to take\nactions within an environment to maximize its cumulative\nreward over the long term [39], [40]. Key roles in rein-\nforcement learning include (1) Agent: the role of a learner\nand decision-maker; (2) Environment: everything that is\ncomposed of and interacts with something other than the\nagent; (3) Action: the behavioral representation of the agent\nbody; (4) State: the information that the capable body\nobtains from the environment; (5) Reward: feedback from\nthe environment about the action. Reinforcement learning\ncan generally be classified into value-based algorithms (e.g.,\nDeep Q Learning algorithm [41]) and policy-based algo-\nrithms (e.g., Policy Gradients algorithm [42]).\nWith the advancement of reinforcement learning, a class\nof algorithms known as Actor-Critic (AC) algorithms have\nbeen proposed [43], combining elements of value-based and\npolicy-based strategies. In this study, we employ the Advan-\ntage Actor-Critic (i.e., A2C [44]) framework to address the\nchallenge of compiler bug isolation, as it is both effective (in\ncomparison to AC [43]) and suitable for single-thread and\nmulti-thread systems (compared to Asynchronous Advan-\ntage Actor-Critic, i.e., A3C [44]). In this study, LLM4CBI\nadopts the A2C (Advantage Actor-Critic) framework [44]\nto learn the effects of a prompt, enabling the generation\nof effective passing test programs for a specific compiler\nbug. We opt for the A2C framework mainly because it has\ndemonstrated practical effectiveness, efficiency, and stability\nwith low variance [45], making it a suitable choice.\n3.2.2 Reinforcement Learning for Prompt Selection\nThe effectiveness of randomly applying mutations upon a\ngiven failing test program can be limited [4]. Consequently,\nwe do not randomly select prompts for LLMs to generate\nprogram variants; we need to efficiently generate more ef-\nfective passing test programs within a given time tailored to\na specific compiler bug. To accomplish this goal, we employ\nreinforcement learning in LLM4CBI, where the quality of\nthe generated passing test programs serves as the reward\nmetric of a prompt. In this study, every prompt is composed\nof the input code, the instance of the prompt pattern (includ-\ning the mutation operator, and the most complex variables\nand lines). As shown in Fig. 3, the precise mutation info is\nfed to and selected by the reinforcement learning model. For\nexample, ‚Äúinserting an if statement and reusing the variables\nin the list a, s, v between lines 12-18‚Äù and ‚Äúinserting a loop\nstatement and reusing the variables in the list a, s, v between\nlines 12-18‚Äù can be two different mutation info of the prompt\n(there are 13 in total based on the mutation operators listed\nin Table 1). In short, our reinforcement learning model learns\nhow to select the optimal mutation operator, i.e., the one\nthat can be the most effective one for producing high-quality\nwitness test programs.\nFig. 4 provides an overview of the reinforcement\nlearning-based strategy for selecting prompts in LLM4CBI.\nFollowing the A2C framework, LLM4CBI first initializes\ntwo neural networks: the Actor Neural Network (ANN) and\nthe Critic Neural Network (CNN) in the agent. The ANN\npredicts the probability distribution of actions based on\nhistorical knowledge, enabling the subsequent selection of\nan optimal action by LLM4CBI. CNN predicts the potential\nreward that can be accumulated from the current state to a\nfuture state after executing the selected action, incorporating\nfuture knowledge. LLM4CBI chooses an action at to select\na prompt (randomly chosen for the first time) and measure\nthe quality ( Qt) of the newly generated test program. To\nfacilitate learning, LLM4CBI employs an advantage loss\nfunction ( Aloss) based on the predicted potential reward\n(P R) and the actual reward ( R) obtained from the selected\nprompt. Finally, LLM4CBI updates the status of all the\nANN, CNN, and states to the agent. LLM4CBI repeatedly\nselects a prompt to generate test programs until the termi-\nnation condition (e.g., 1-hour limit is reached or 10 program\nvariants are generated) is reached.\nConsistent with existing A2C-based approaches [43],\n[44], both ANN and CNN in LLM4CBI comprise a single\nhidden layer to ensure lightweight and fast convergence.\nThe following provides detailed explanations of the most\nimportant parts, including the actual reward and advantage\nloss function calculations.\nMeasuring Actual Reward. An essential factor contribut-\ning to the effectiveness of the A2C-based approach lies in\ndetermining the actual reward subsequent to applying a\nprompt. Drawing the inspiration from previous studies [3],\n[4], a collection of proficient passing test programs needs\nto meet both similarity and diversity criteria. The similarity\nentails that each passing test program should exhibit a\ncomparable compiler execution trace to that of the given\nfailing test program. Consequently, following the principles\nof SBFL, the suspicion associated with a greater number of\nbuggy-free files can be diminished. The diversity requires\nthat different passing test programs possess distinct com-\npiler execution traces from one another to reduce suspicion\nregarding various buggy-free files. By doing so, aggregating\na set of passing test programs that have undergone mutation\nfacilitates the effective isolation of genuinely faulty files by\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 7\nSMU Classification: Restricted\n34\nPromptSelection\nAnewtestprogram\nActionùëé!\nStateùë†!\nActorNeuralNetwork(ANN)\nEnvironment\nAgent\nùë†!\"#\nùëÖ!\"#\nCriticNeuralNetwork(CNN)\nprogramquality(ùëÑ!)\nùëÖ!\nùëÉùëÖ!\nAdvantageLoss\nùê¥$%&&(ùë°)\npredict\nWeightsùë§predict\nmutate(ùëö')\n* PRt: potential reward at time t\n* Rt: actual reward at time t\n* Aloss: advantage loss function at time t\n* mj: a selected prompt (j ‚àà {1, 13})\nFig. 4. Memorized prompt selection guided by reinforcement learning:\nThe agent uses an ANN to predict an action at (i.e., a prompt mj\nused for mutating a program). After obtaining the new test program from\nLLMs, the environment calculates the actual reward Rt based on the\nquality (Qt) of the newly generated test program. In the meantime, CNN\nin the agent predicts a potential reward PRt. Next, the advantage loss\n(Aloss) is measured by combining the actual reward and the potential\nreward. Later, the weights ( w) of ANN and CNN and state ( st) will be\nupdated to the agent and help the agent generate a better prompt.\ncircumventing bias. Both similarity and diversity rely on the\ndistance metric, which is defined as:\ndist(a, b) = 1‚àí Cova ‚à© Covb\nCova ‚à™ Covb\n(4)\nwhere the dist(a, b) represents the coverage distance be-\ntween two test programs a and b, which is determined\nusing the Jaccard Distance measurement [3]. Cova and Covb\nrepresent the sets of statements covered in compilers by test\nprograms a and b, respectively.\nDenoting the set of generated passing test programs as\np = {p1, p2, ..., pn} and the failing test program as f, we\nformalize the metrics of similarity and diversity achieved by\nthe set of passing test programs, represented as Equation 5\nand Equation 6, respectively.\nsim =\nPN\ni=1(1 ‚àí dist(pi, f))\nN (5)\ndiv =\nPN‚àí1\ni=1\nPN\nj=i+1 dist(pi, pj)\nN(N ‚àí 1)/2 (6)\nwhere N is the number of passing test programs.\nAt time step t, once a passing test program is generated,\nLLM4CBI evaluates the effectiveness of the current set of\npassing test programs by linearly combining the attained\nmeasures of similarity and diversity in Equation 7.\nQt = n(Œ± √ó divt + (1‚àí Œ±) √ó simt) (7)\nwhere the coefficient Œ± represents the weighting factor for\nthe linear combination of diversity and similarity in Equation\n7. Additionally, following the existing study [4], Equation 7\nalso incorporates another coefficient n, which corresponds\nto the size of the set of passing test programs.\nSubsequently, LLM4CBI determines whether to accept\nthe generated passing test program based on whether it can\nenhance the overall quality of the set of passing programs\ncompared to the previous time step denoted as t ‚àí 1. The\nEquation 8 outlines the computation of the enhanced quality\nrelative to the previous time step.\n‚ñ≥Qt = Qt ‚àí Qt‚àí1 (8)\nNevertheless, in each state, only one prompt is chosen\nto generate a passing test program, and the performance\nof a prompt can vary significantly depending on different\nbugs. To balance the influence of the diverse performance of\nprompts, LLM4CBI incorporates both the current time step\nimprovement and the historically accumulated improve-\nment attributed to the current prompt. This combined value\nserves as the actual reward obtained at the current time step,\nwhich is defined as follows:\nRt =\nPt\ni=1 ‚ñ≥Qi\nT(mj) (9)\nHere, T(mj) represents the count of times themj (a prompt)\nhas been chosen to mutate the failing test program. ‚ñ≥Qi\nis defined as zero ( ‚ñ≥Qi = 0) if the selected prompt at the\nith time step is not mj; if the selected prompt is mj, ‚ñ≥Qi\nis computed using Equation 8. Whether selecting a new\nprompt mj or previously selected prompt mj depends on\nthe decision made by ANN in the agent.\nCalculating Advantage Loss.While the actual reward ( Rt)\nis obtained at the current time step t, LLM4CBI also utilizes\nCNN to predict the potential reward (P Rt+i). To effectively\nconsider future factors, A2C incorporates an advantage loss\nfunction. This function is designed to address the issue\nof high variance in the two neural networks and prevent\nconvergence towards local optima [44]. The advantage loss\nfunction is expressed as follows in Equation 10:\nAloss(t) =\nt+uX\ni=t\n(Œ≥i‚àítRi) +Œ≥u+1P Rt+u ‚àí P Rt (10)\nthe variable u indicates that the CNN considers the future u\nconsecutive states and actions when predicting the potential\nreward. Œ≥ represents the weight assigned to the actual future\nreward. P Rt+u and P Rt denote the predicted potential\nrewards at the (t + u)th and tth time steps, respectively,\nas determined by CNN. Notably, LLM4CBI repeats this\nprocess for u times within a time step to approximate the\nactual future reward.\nUsing the loss computed by the advantage function in\nEquation 10, LLM4CBI proceeds to update the weights of\nboth the ANN and CNN following Equation 11.\nw = w + Œ≤ ‚àÇ(log Pw(at|st)Aloss(t))\n‚àÇw (11)\nwhere st represents the current state, while at denotes\nthe corresponding action. Pw(at|st)Aloss(t) represents the\nprobability of performing action at at state st based on the\nparameters w in both the ANN and CNN. Œ≤ represents the\nlearning rate of the weight updates.\n3.3 Lightweight Test Program Validation\nExisting studies [3], [4] pay little attention to the validity of\nthe mutated test programs. First, the test programs gener-\nated by these approaches may contain undefined behaviors.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 8\nAs demonstrated in our evaluation results in Section 4.4,\nsuch test programs reduce the effectiveness of compiler bug\nisolation. Second, existing approaches may generate test\nprograms that do not have a test oracle, which can also affect\nthe effectiveness of bug isolation. Third, existing studies\nare unaware of the errors they made during the mutation\nprocess, so they frequently make the same mistakes when\nmutating test programs.\nTo address the above limitations, LLM4CBI designs a\nsemantic validation to filter away semantically invalid test\nprograms and utilizes a test oracle validation to fix the test\nprograms that violate test oracles. Additionally, LLM4CBI\ncollects all the validation errors in the test programs gen-\nerated by LLMs and gives such information as feedback\nprompts to LLMs to avoid repeating the same mistakes.\nNext, we detail the validation processes.\n3.3.1 Program Semantic Validation\nWe choose static analysis checks on the newly generated\ntest program that may contain any kinds of undefined\nbehaviors, which proved to be lightweight compared with\ndynamic analysis approaches [46]. As shown in step 5 in\nFig. 3, the newly generated test program is transferred to\nthis component for checking the semantic validity. Based on\nthe analysis capabilities from Frama-C [46], we opt for five\ndifferent categories of undefined behaviors in LLM4CBI:\n‚Ä¢ mem_access: invalid memory access, such as out-of-\nbound read or out-of-bound write.\n‚Ä¢ shift: invalid RHS (Right-Hand Shift) or LHS (Left-\nHand Shift) operand for right or left shift operation.\n‚Ä¢ index_bound: accessing out-of-bounds index of an array.\n‚Ä¢ initialization: accessing uninitialized left-value, i.e.,\nuse a variable before it was uninitialized.\n‚Ä¢ division_by_zero: a number is divided by zero.\nIf one of the above kinds of undefined behaviors is\ndetected, the semantic error information will be updated to\nLLMs to avoid repeating the same mistakes. For example,\nif a new test program contains an undefined divided by\nzero behavior, an additional prompt, ‚Äú The above program\ncontains a kind of undefined behavior divided by zero,\nplease do not generate such test programs again. ‚Äù, will be fed\nto LLMs in step 8 .\n3.3.2 Test Oracle Validation\nTest Oracles. It is also required to check whether a gen-\nerated test program is passing or failing [3], [4], [47], [48].\nAccording to the types of compiler bugs (i.e., crash bugs\nand wrong-code bugs), following the existing works [3], [4],\nLLM4CBI considers two types of test oracles: (1) Regarding\ncrash bugs (i.e., the compiler crashes when using some\ncompilation options to compile a test program), the test\noracle is whether the compiler still crashes when using\nthe same compilation options to compile a generated test\nprogram. (2) Regarding wrong-code bugs (i.e., the compiler\nmis-compiles a test program without any failure messages,\ncausing the test program to have inconsistent execution re-\nsults under different compilation options), the test oracle is\nwhether a generated test program still produces inconsistent\nexecution results under the compilation options producing\nthe previous inconsistencies. Similar to DiWi [3] and RecBi\n[4], LLMs may not put in the test oracles in a generated\ntest program; so, we apply another heuristic validation\nto check for missing oracles. Specifically, we check if the\ngenerated test program contains the same number of abort\nor printf statements as the given failing test program.\nIf a newly generated test program passes the above\nvalidations, the buggy compiler is used to compile the\ngenerated test program in step 6 . LLM4CBI collects the\nsemantic error or test oracle information and uses it as\nfeedback prompts to guide LLMs not to generate programs\nwith the same kinds of errors. It is worth noting that since\ndifferent test programs may contain different kinds of errors,\nthe feedback prompts could be different.\n4 E VALUATION\n4.1 Experimental Setup\nImplementation of LLM4CBI. LLM4CBI was imple-\nmented utilizing OClint [49] (v22.02), srcSlice [37] (v1.0),\nGcov [50] (v4.8.0), and PyTorch [51] (v1.10.1+cu113). OClint\nis served to calculate the cyclomatic complexity of each\nstatement; srcSlice is used to get the data-flow complexity\nof the variables defined and used in the program; Gcov\nis applied for collecting compiler coverage information;\nPyTorch supports the A2C framework. For A2C, we set\nthe hyperparameters with the default settings in the pre-\nvious study [4]. For the implementation of test program\nvalidation, we adopt Frama-C [46] (Phosphorus-20170501)\nto check the semantic validity of the test program, and we\nwrite Python scripts (python 3.8.5) to check if there are any\ntest oracle violations. We adapt GPT-3.5 as the default LLMs\nin LLM4CBI, with the temperature parameter 1.0 in GPT-\n3.5 (see more detailed discussion on temperature settings in\nSection 5).\nStudy Subjects. We use GCC and LLVM as our subjects to\nassess the effectiveness of LLM4CBI. Both two compilers\nare widely used in existing literature [3], [4], [47], [48], [52],\n[53], [54] and therefore constitute a comprehensive evalua-\ntion. On average, a GCC buggy version contains 1,758 files\nwith 1,447K source lines of code (SLOC), while an LLVM\nbuggy version comprises 3,265 files with 1,723K SLOC.\nBenchmark. We utilize a benchmark consisting of 120 real\ncompiler bugs, with an equal distribution of 60 GCC and 60\nLLVM bugs, which includes all bugs studied in prior works\n[3], [4]. Specifically, each bug is accompanied by relevant\nbuggy details, including the faulty compiler version, the\nfailing test program, the buggy compilation options, and\nthe faulty files which serve as the ground truth.\nRunning Platform. We conducted all the experiments on a\nworkstation equipped with a 12-core CPU, Intel(R) Xeon(R)\nW-2133 CPU @ 3.60GHz, 64G RAM, and Ubuntu 18.04\noperating system, without GPU support.\nEvaluation Metrics. Each approach for isolating compiler\nbugs generates a list of suspicious compiler files. To evaluate\nthe effectiveness of each approach, we measure the position\nof each buggy file in the ranking list with the help of the\nground truth. In cases where multiple compiler files had\nthe same suspicious scores, we follow the precedent set\nby prior studies [55], [56] and assign the worst ranking.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 9\nTABLE 2\nCompiler bug isolation effectiveness comparison with two state-of-the-art approaches (under Setting-1 in RQ1)\nSubject Approach Num. ‚áëTop‚àí1 Num. ‚áëTop‚àí5 Num. ‚áëTop‚àí10 Num. ‚áëTop‚àí20 MFR ‚áëMFR MAR ‚áëMAR\nTop-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) (%) (%)\nGCC\nDiWi [3] 5.60 66.07 19.90 25.13 31.30 16.29 41.70 7.43 22.57 29.83 23.10 29.19\nRecBi [4] 7.70 20.78 23.90 4.18 34.20 6.43 42.50 5.41 20.53 22.84 21.06 22.33\nLLM4CBI 9.30 - 24.90 - 36.40 - 44.80 - 15.84 - 16.35 -\nLLVM\nDiWi [3] 4.30 74.42 19.20 18.23 29.20 17.81 40.00 20.00 27.42 43.97 27.50 43.82\nRecBi [4] 5.80 29.31 19.80 14.65 30.00 14.67 42.40 13.21 25.43 39.59 25.56 39.54\nLLM4CBI 7.50 - 22.70 - 34.40 - 48.00 - 15.36 - 15.45 -\nALL\nDiWi [3] 9.90 69.70 39.10 21.74 60.50 17.02 81.70 13.59 25.00 37.58 25.30 37.15\nRecBi [4] 13.50 24.44 43.70 8.92 64.20 10.28 84.90 9.31 22.98 32.11 23.31 31.77\nLLM4CBI 16.80 - 47.60 - 70.80 - 92.80 - 15.60 - 15.90 -\nNote: Columns ‚Äú‚áë ‚àó‚Äù present the improvement rates (%) of LLM4CBI over the compared approaches in various metrics.\nSpecifically, we compute the following metrics commonly\nused in compiler bug isolation studies [3], [4], [47], [48].\n‚Ä¢ Top-N. This metric denotes the number of bugs that\nare effectively isolated and contained within the Top-N\nposition, where N is a member of the set 1, 5, 10, 20 as\nspecified in our study. A higher value of Top-N indicates\na better performance of an approach.\n‚Ä¢ Mean First Ranking (MFR). This metric represents the\naverage rank of the first faulty file within the ranking\nlist for each bug. The objective of MFR is to promptly\nisolate the initial defective element in order to expedite\nthe debugging process. A smaller value is better, as it\nindicates that developers could localize the corresponding\nbug as quickly as possible.\n‚Ä¢ Mean Average Ranking (MAR). This metric measures the\naverage of the mean rank of every faulty file within the\nranking list for each bug. The MAR metric is intended to\nisolate all faulty elements accurately. Similar to MFR, the\napproach with a smaller value of MAR is better.\n4.2 Research Questions\nIn this study, we aim to answer the following main research\nquestions (RQs):\n‚Ä¢ RQ1: Can LLM4CBI outperform state-of-the-art ap-\nproaches (i.e., DiWi [3] and RecBi [4]) in terms of the\neffectiveness and efficiency of compiler bug isolation?\n‚Ä¢ RQ2: Can each main component, i.e., precise prompt pro-\nduction, memorized prompt selection, and lightweight\ntest program validation, contribute to LLM4CBI?\n‚Ä¢ RQ3: Can LLM4CBI be easily extended with other LLMs\nfor the compiler bug isolation task?\n4.3 Answers to RQ1\n4.3.1 Experimental Settings\nWe set up the following two settings to investigate RQ1:\n‚Ä¢ Setting-1: terminate with the same running time (i.e., one\nhour). This is the standard comparison strategy used in\nexisting compiler bug isolation studies [3], [4], [47], [48]\nfor evaluating their effectiveness.\n‚Ä¢ Setting-2: terminate when generating the same number\n(i.e., 10) of passing test programs. This setting aims to\ndemonstrate the efficiency of LLM4CBI further. We opt\nfor the number suggested by the existing empirical stud-\nies [15]. We give the timeout of 2 hours if an approach\ncannot generate the desired number of test programs of a\nbug. We add this setting because during the experiments\nin Setting-1, we find the number of generated passing\ntest programs by comparative approaches is different, and\nLLM4CBI could generate a larger number of passing test\nprograms. Thus, whether the superior performance on\nLLM4CBI benefited from the more test programs or the\nquality of the newly generated programs is unknown. We\nconduct Setting-2 to investigate it further.\nComparison Strategies. For both settings, we repeatedly ran\nall the comparative approaches 10 times and calculated the\naverage results of Top-N, MFR, and MAR metrics to reduce\nthe influence of randomness. Specifically, we conduct the\nstatistical test analysis followed by the suggestions from\nArcuri and Briand [57], [58]. Additionally, for Setting-2, we\ncompare the execution time for each approach and speedups\nachieved by LLM4CBI.\n4.3.2 Experimental Results\nResults for Setting-1. The comparison results of Setting-\n1 are presented in Table 2. The first column represents\nsubject compilers, and the second column shows different\napproaches. Columns 3-10 provide the Top-N metrics de-\nrived from the average values obtained from 10 runs of each\napproach, including the number of Top-N (i.e., Num. Top-\nN) and improvement (i.e., ‚áëTop‚àíN (%)) made by LLM4CBI.\nColumns 11-14 represent the MFR (Mean First Ranking)\nand MAR (Mean Average Ranking) metrics, as well as the\nimprovement achieved by LLM4CBI.\nNotably, LLM4CBI demonstrates its capability by suc-\ncessfully isolating 16.80, 47.60, 70.80, and 92.80 compiler\nbugs (out of a total of 120 compiler bugs in GCC and LLVM)\nwithin the Top-1, Top-5, Top-10, and Top-20 files, respec-\ntively. This accounts for the improvement of 69.70%, 21.74%,\n17.02%, and 13.59% than DiWi and 24.44%, 8.92%, 10.28%,\nand 9.31% than RecBi, respectively. Further analysis of the\neffects across different subject compilers revealed interest-\ning findings. Despite the larger number of compiler files\nin LLVM compared to GCC, LLM4CBI exhibited slightly\nbetter results in the case of GCC. For instance, LLM4CBI\nachieved MFR and MAR values of 15.36 and 14.45 for GCC,\nwhile corresponding values for LLVM were 15.60 and 15.90.\nCompared with DiWi and RecBi, the evaluation reveals\nthat LLM4CBI outperforms DiWi and RecBi across all\nmetrics and for both GCC and LLVM compilers. Notably,\nLLM4CBI demonstrates significant improvement of 69.70%\nand 21.74% over DiWi in terms of Top-1 and Top-5. For\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 10\nTABLE 3\nCompiler bug isolation effectiveness comparison with two state-of-the-art approaches (under Setting-2 in RQ1)\nSubject Approach Num. ‚áëTop‚àí1 Num. ‚áëTop‚àí5 Num. ‚áëTop‚àí10 Num. ‚áëTop‚àí20 MFR ‚áëMFR MAR ‚áëMAR\nTop-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) (%) (%)\nGCC\nDiWi [3] 4.80 56.25 18.60 27.96 31.20 11.86 40.80 7.84 23.86 33.13 24.27 33.54\nRecBi [4] 6.80 10.29 23.10 3.03 34.20 2.05 42.80 2.80 18.81 15.19 19.28 16.34\nLLM4CBI 7.50 - 23.80 - 34.90 - 44.00 - 15.96 - 16.13 -\nLLVM\nDiWi [3] 4.20 88.10 19.90 11.56 29.30 17.06 39.40 12.18 26.13 37.63 26.22 37.44\nRecBi [4] 6.10 29.51 20.60 7.77 32.90 4.26 41.30 7.02 19.56 16.68 19.64 16.46\nLLM4CBI 7.90 - 22.20 - 34.30 - 44.20 - 16.30 - 16.40 -\nALL\nDiWi [3] 9.00 71.11 38.50 19.48 60.50 14.38 80.20 9.98 25.00 35.48 25.24 35.56\nRecBi [4] 12.90 19.38 43.70 5.26 67.10 3.13 84.10 4.88 19.19 15.95 19.46 16.40\nLLM4CBI 15.40 - 46.00 - 69.20 - 88.20 - 16.13 - 16.27 -\nNote: Columns ‚Äú‚áë ‚àó‚Äù present the improvement rates (%) of LLM4CBI over the compared approaches in various metrics.\nRecBi, LLM4CBI could isolate 24.44% and 8.92% more bugs\nthan RecBi in terms of Top-1 and Top-5. In particular, the\npractical significance of the Top-5 metric is highlighted by\nprevious research, which indicates that developers often\ndiscontinue the use of automated debugging tools if the\nfaulty elements cannot be localized within the Top-5 po-\nsitions. Consequently, LLM4CBI shows better practicality\ncompared to DiWi and RecBi by substantially improving\nthe effectiveness of compiler bug isolation, specifically in\nrelation to the Top-5 metric.\nFurthermore, in terms of MFR and MAR, LLM4CBI\nachieves a considerable improvement of 37.58% and 37.15%\nover DiWi and 32.11% and 31.77% over RecBi, respectively.\nThe improvement of MFR and MAR demonstrates that\nLLM4CBI can promptly and accurately isolate more com-\npiler bugs than DiWi and RecBi.\nResults for Setting-2. From Table 3, for all the 120 bugs,\nwe can know that LLM4CBI also outperforms comparative\napproaches: LLM4CBI can isolate more bugs and hold the\nlowest MFR and MAR.\nFor the efficiency side, Fig. 5 presents the detailed per-\nformance results. The x-axis in box Fig. 5(a) show different\napproaches in GCC and LLVM, and y-axis represents the\ntime spent by isolating each bug. For the figures, we can\nobserve LLM4CBI takes less time when isolating most of\nthe bugs. We can see the time taken on LLVM is generally\nlonger than on GCC. This is justifiable as the number of files\nof LLVM is larger than GCC, and the more files the more\ntime spent on calculating the coverage. We also calculate the\nspeedups achieved by LLM4CBI. Specifically, we follow the\nEquation below to calculate the speedups:\nTbaseline ‚àí TLLM4CBI\nTbaseline\n√ó 100 (12)\nwhere Tbaseline represents the time spent by the baselines\n(i.e., DiWi and RecBi) for generating 10 passing test pro-\ngrams on average, and TLLM4CBI describes the time our\nproposed LLM4CBI spent generating the same number\nof passing test programs on average. Fig. 5(b) shows the\nresults. We can see LLM4CBI is able to achieve 53.19% and\n64.54% as well as 47.31% and 63.19% performance gains\nthan DiWi and RecBi, for GCC and LLVM, respectively.\nIt is interesting to note that the overall results of\nLLM4CBI are better in Setting-1 compared with Setting-2.\nThis is reasonable as LLM4CBI could generate more pass-\ning test programs in one hour, improving the results. There-\n(a) Distribution of execution time\n (b) Speedups over DiWi and RecBi\nFig. 5. Performance comparison over DiWi, RecBi, and LLM4CBI (un-\nder Setting-2 in RQ1)\nTABLE 4\nStatistical Test Results under Setting-1 and Setting-2\n(LLM4CBI vs state-of-the-art approaches)\nApproaches Metrics Setting-1 Setting-2\np-value ÀÜA12 p-value ÀÜA12\nDiWi [3]\nTop-1 <0.0001 0.98 <0.0001 1.00\nTop-5 <0.0001 0.96 <0.0001 0.98\nTop-10 <0.0001 0.97 <0.0001 0.95\nTop-20 <0.0001 0.96 <0.0001 0.99\nMFR <0.0001 1.00 <0.0001 1.00\nMAR <0.0001 1.00 <0.0001 1.00\nRecBi [4]\nTop-1 0.0034 0.77 0.0004 0.82\nTop-5 0.0200 0.71 0.0355 0.69\nTop-10 0.0015 0.79 0.0470 0.68\nTop-20 0.0001 0.85 <0.0001 0.89\nMFR <0.0001 1.00 <0.0001 1.00\nMAR <0.0001 1.00 <0.0001 1.00\nfore, we suggest developers run a long time of LLM4CBI to\nget better isolation results.\nStatistical Test Analysis of Results. After collecting\nexperimental results of 10 runs of each approach, we follow\nexisting works [53], [59] to conduct the Mann-Whitney\nU-test with a level of significance of 0.05 on the total\nbugs between LLM4CBI and the state-of-the-art approaches\naccording to the suggestions by Arcuri and Briand [57],\n[58]. To further reduce the threats from randomness, we\nalso calculate the effect size of the differences between\nLLM4CBI and the baselines using the Vargha and Delaneys\nÀÜA12 statistics. In our context, given a performance measure\nM (e.g., Top-1), the ÀÜA12 statistics measure the probability\nthat running approach A (e.g., LLM4CBI) yields higher M\nvalues than running another approach B (e.g., DiWi). If the\ntwo approaches are equivalent, then ÀÜA12 = 0.5. For example,\nÀÜA12 = 0.9 entails we would obtain higher results 90% of the\ntime with approach A than approach B. We use the function\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 11\nTABLE 5\nCompiler bug isolation effectiveness comparison with four variants of LLM4CBI\nSubject Approach Num. ‚áëTop‚àí1 Num. ‚áëTop‚àí5 Num. ‚áëTop‚àí10 Num. ‚áëTop‚àí20 MFR ‚áëMFR MAR ‚áëMAR\nTop-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) (%) (%)\nGCC\nLLM4CBI ep 5.60 66.07 22.40 11.16 32.80 10.98 40.90 9.54 18.74 15.47 19.23 14.94\nLLM4CBI sp 7.10 30.99 23.80 4.62 34.30 6.12 40.10 11.72 18.73 15.43 19.20 14.83\nLLM4CBI rand 6.40 45.31 21.80 14.22 31.20 16.67 41.60 7.69 18.69 15.25 19.24 15.00\nLLM4CBI selnov 7.70 20.78 22.40 11.16 33.20 9.64 39.70 12.85 18.92 16.26 19.27 15.15\nLLM4CBI 9.30 - 24.90 - 36.40 - 44.80 - 15.84 - 16.35 -\nLLVM\nLLM4CBI ep 6.60 13.64 20.70 9.66 30.60 12.42 41.70 15.11 17.93 14.32 18.02 14.26\nLLM4CBI sp 6.20 20.97 20.70 9.66 31.90 7.84 41.50 15.66 17.50 12.24 17.63 12.34\nLLM4CBI rand 6.20 20.97 21.50 5.58 33.40 2.99 42.90 11.89 16.20 5.17 16.27 5.03\nLLM4CBI selnov 5.90 27.12 20.80 9.13 33.10 3.93 43.90 9.34 17.97 14.53 18.09 14.60\nLLM4CBI 7.50 - 22.70 - 34.40 - 48.00 - 15.36 - 15.45 -\nALL\nLLM4CBI ep 12.20 37.70 43.10 10.44 63.40 11.67 82.60 12.35 18.34 14.91 18.62 14.61\nLLM4CBI sp 13.30 26.32 44.50 6.97 66.20 6.95 81.60 13.73 18.12 13.89 18.41 13.64\nLLM4CBI rand 12.60 33.33 43.30 9.93 64.60 9.60 84.50 9.82 17.45 10.57 17.76 10.43\nLLM4CBI selnov 13.60 23.53 43.20 10.19 66.30 6.79 83.60 11.00 18.45 15.42 18.68 14.88\nLLM4CBI 16.80 - 47.60 - 70.80 - 92.80 - 15.60 - 15.90 -\nNote: Columns ‚Äú‚áë ‚àó‚Äù present the improvement rates (%) of LLM4CBI over the compared approaches in terms of various metrics.\n‚Äúwilcox.test(X,Y)‚Äù written by R language2 to perform a\nMann-Whitney U-test to get the p-value. For the effective\nsize, we use the formula ÀÜA12 = (R1/m ‚àí (m + 1)/2)/n to\ncalculate it, where R1 is the rank sum of the first data group\nwe are comparing. The rank sum is a basic component in\nthe Mann-Whitney U-test, and most statistical tools provide\nit. We use the function ‚Äúsum(rank(c(X,Y))[seq_along(X)])‚Äù\nwritten by R language to calculate the R1 value, where\nX and Y are the data sets with the observations (i.e., the\nmetrics over GCC and LLVM bugs) of the two compared\nrandomized approaches.\nThe overall statistical results in both two experiment\nsettings are shown in Table 4. The p-value less than 0.05 in\nthe table shows that LLM4CBI performs significantly better\nthan the state-of-the-art approaches, i.e., Diwi [3] and RecBi\n[4]. Furthermore, we can observe that all the effect sizes are\ngreater than 0.71 for Setting-1 and 0.68 for Setting-2, which\nindicates that LLM4CBI has a higher probability of obtain-\ning better results than the two state-of-the-art approaches.\nSummary for RQ1.The extensive evaluation demonstrates\nthat LLM4CBI significantly outperforms two state-of-the-\nart approaches in two different settings: LLM4CBI is able\nto effectively and efficiently generate witness test programs\nfor compiler bug isolation than DiWi and RecBi.\n4.4 Answers to RQ2\n4.4.1 Experimental Settings\nWe use the following variants of LLM4CBI aiming to dif-\nferentiate the effects of main components in LLM4CBI:\n‚Ä¢ LLM4CBI ep uses a simple prompt without data flow and\ncontrol flow analysis. That is, LLM4CBI ep only keeps the\n<mutation rule > and does not rely on the <variables >\nand <location > information to fill in the designed pattern.\n‚Ä¢ LLM4CBI sp utilizes a specific prompt by giving the\n‚Äúthe most complex data and control flow‚Äù. That means,\nLLM4CBI sp replaces the <variables > with ‚Äúthe most\ncomplex variables‚Äù and <location > with ‚Äúin the most\ncomplex statements‚Äù, which depends on the program\nunderstanding capabilities of LLMs to fill in the pattern.\n2. https://www.rdocumentation.org/packages/stats/versions/3.6.\n2/topics/wilcox.test\n‚Ä¢ LLM4CBI rand randomly selects a prompt without the\nmemorized prompt selection, meaning LLM4CBI rand\nperforms a random selection of the prompt during the\nprompt selection process.\n‚Ä¢ LLM4CBI selnov removes the test program validation\ncomponent in the prompt selection process in LLM4CBI:\nLLM4CBI selnov does not care about test program validity\nfor compiler bug isolation.\nAmong these variants, LLM4CBI ep and LLM4CBI sp\naim to investigate whether the new program complexity-\nguided prompt production pattern is effective, while\nLLM4CBI rand and LLM4CBI selnov target to understand\nwhether the memorized prompt selection and test program\nvalidation could contribute to LLM4CBI, respectively.\nComparison Strategies. We run the four variants with the\nsame strategy as Setting-1 in RQ1. We then compare the\naverage of 10 runs of Top-N, MFR, and MAR metrics\nfor each approach to articulate the contribution of those\ncomponents. Note that we run LLM4CBI, two state-of-the-\nart approaches, and four comparative approaches, 10 runs\nfor each bug within 120 compiler bugs under two testing\nscenarios. Therefore, it takes more than 50 days to run the\nexperiments under RQ1 and RQ2, indicating our experiment\nis extensive and sufficient.\n4.4.2 Experimental Results\nThe comparison results between LLM4CBI and the com-\nparative variant approaches are presented in Table 5, where\neach column and row has the same meaning as in Table 2.\nContribution of Prompt Production. As shown in Table\n5, LLM4CBI is better than both LLM4CBI ep (i.e., the ap-\nproach that uses a simple prompt without data flow and\ncontrol flow analysis) and LLM4CBI sp (i.e., the approach\nthat utilizes specific prompt by giving the ‚Äúthe most com-\nplex data and control flow‚Äù) in terms of all the metrics over\nall the 120 bugs in GCC and LLVM. Notably, LLM4CBI is\nable to isolate more 37.70% and 10.44% bugs within the Top-\n1 and Top-5 files and also improves 14.91% and 14.61% than\nLLM4CBI ep in terms of MFR and MAR. Compared with\nboth LLM4CBI ep and LLM4CBI sp, LLM4CBI shows better\nresults on Top-N metrics. In addition, LLM4CBI also holds\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 12\nGCC-Bugs(14) LLVM-Bugs(16) ALL-Bugs(30)\n1\n2\n3\n4\n5\n6\n7\n8Number of invalid test programs\nFig. 6. The number of invalid programs generated by LLM4CBI selnov.\n1 int a, b, c, d; char e;\n2 void foo () { a = 0; }\n3\n4 int main (){\n5 unsigned char f;\n6 for (; b < 1; b++){\n7 for (e = 1; e >= 0; e‚àí‚àí){\n8 d = 0;\n9 if (a){ break; }\n10 f = 179 * e;\n11 c = f ¬´ (-1); // c=f<<1;\n12 foo () ;\n13 }\n14 }\n15 printf ( \"%d\\n\", c);\n16 return 0;\n17 }\nListing 3. LLVM Bug #18000:\ninvalid shift in Line 11 caused\nby the mutation in the same line\n1 int a;\n2 short int b = 1; // int b = 1;\n3\n4 int main () {\n5 int i ;\n6 for (i = 0; i < 56; i ++) {\n7 for (; a; a‚àí‚àí) {\n8 ;\n9 }\n10 }\n11 int *c = &b;\n12 if (*c){\n13 *c=1 % (unsigned int)*c|5;\n14 }\n15 printf ( \"%d\\n\", b);\n16 return 0;\n17 }\nListing 4. GCC Bug #64682:\ninvalid memory access in Line\n12 caused by the mutation in Line 2\nFig. 7. Side effects of the test programs with undefined behavior (the\ncommented code in green is the original code in the failing test program,\nand the code with the cyan box contains undefined behavior)\na smaller value of MFR and MAR, indicating LLM4CBI has\na better compiler bug isolation capability.\nThe above results indicate that the program complexity\nmetrics help produce precise prompts, and removing data\nflow and control flow analysis or replacing them with\nexplicit descriptions to LLMs are not effective. This is rea-\nsonable, as LLMs have shown to be limited at semantic un-\nderstanding [60]. Therefore, with a limited prompt descrip-\ntion, LLMs may randomly select interesting variables and\nlocations when mutating the failing test program, making it\ndifficult to flip the execution from failing to passing.\nContribution of Prompt Selection. We run LLM4CBI rand\n(i.e., an approach that randomly selects a prompt) and\nLLM4CBI to investigate the contribution of the memorized\nprompt selection component. Table 5 presents the overall\nresults. The results indicate that LLM4CBI outperforms\nLLM4CBI rand as well. Specifically, LLM4CBI demon-\nstrates better performance compared to LLM4CBI rand, with\nsubstantial improvement of 33.33%, 9.93%, 9.60%, 9.82%,\n10.57%, and 10.43% across Top-1, Top-5, Top-10, Top-20,\nMFR, and MAR metrics, respectively. These results under-\nscore the superiority of our memorized prompt selection\nbased on reinforcement learning over the random strategy.\nContribution of Test Program Validation. Compared with\nthe approaches shown in rows 4 and 5 in Table 5, LLM4CBI\noutperforms LLM4CBI selnov (a variant approach that re-\nmoves the test validation component) for all the metrics\nfor both GCC and LLVM. For the overall benchmarks,\nLLM4CBI could isolate 23.53% and 10.19% more bugs\nwith Top-1 and Top-5 files than LLM4CBI selnov, respec-\n0.2 0.4 0.6 0.8 1.0\nText similarity score\n0\n100\n200\n300\n400\n500\n600\n700Frequency\nMean\n(a) GCC benchmarks\n0.2 0.4 0.6 0.8 1.0\nText similarity score\n0\n50\n100\n150\n200\n250Frequency\nMean\n(b) LLVM benchmarks\nFig. 8. Distribution of text similarity score among the test programs\ngenerated by LLM4CBI and LLM4CBI selnov\ntively. In terms of MFR and MAR, LLM4CBI also exhib-\nited significant improvement of 15.42% and 14.88% over\nLLM4CBI selnov. These results provide compelling evidence\nthat incorporating the test program validation component\nenhances the effectiveness of compiler bug isolation, thereby\nconfirming its valuable contribution in LLM4CBI.\nLLM4CBI outperforms LLM4CBI selnov mainly because\nLLM4CBI selnov suffers from two severe issues that decrease\nits performance of bug isolation due to the existence of\nundefined behaviors, while LLVM4CBI could effectively\nreduce the threats by filtering those test programs.\nThe first issue comes from the unreliable test oracle.\nTypically, LLM4CBI treats the generated test program pro-\nducing consistent results as a passing program, which serves\nas the test oracle in this study. However, since test programs\ngenerated by LLM4CBI selnov may contain undefined be-\nhaviors, the results they produced may not be reliable as the\nprograms with undefined behaviors can have unexpected\nbehaviors. If that happens, although the results show the\nprogram is a passing test program, it may be reliable to\nconclude that such a test program cannot trigger the bug,\nwhich disturbs the calculation of suspicious scores of files.\nThe second issue lies in the interferences between the\ncompiler implementation code that handles the undefined\nbehaviors and induces the compiler bugs. To be specific,\nit is well-known that C compiler developers hold the as-\nsumption that the programmer should not submit code that\nhas undefined behaviors, and they implement optimization\ncode under that assumption [61]. For programmers who ac-\ncidentally write code that has undefined behaviors, the com-\npiler may remove the code (e.g., removing an access control\ncheck) or rewrite the code in a way that the programmer did\nnot anticipate, which can result in an unexpected program\nbehavior being compiled. Therefore, it is reasonable that the\nimplementation of handling undefined behaviors may have\nintersections with the implementation that leads to compiler\nbugs, which may significantly affect the effectiveness of bug\nisolation. For example, if the compilers use the same utils\nfunctions to handle the undefined behaviors and compile\nthe failing test program, the suspicious score of those utils\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 13\nfunctions could be low as such code is covered in passing\ntest programs but not failing test programs. That means the\nreal faulty functions will be silently hidden, thus reducing\nthe chance of locating the exact functions that lead to the\ncompiler bug. It is worth noting that how compilers handle\nundefined behaviors is a controversial topic in the literature\nand reliable identification of undefined behaviors in test\nprograms is challenging [2], [3], [62]. We will continue to\ninvestigate this issue in future work.\nTo understand how many more valid test programs can\nbe generated by LLM4CBI compared with LLM4CBI selnov,\nwe run another set of experiments to confirm the contri-\nbution of filtering out invalid test programs. To conduct a\nfair comparison, we run LLM4CBI and LLM4CBI selnov in\nSetting-2, i.e., both approaches terminate when 10 passing\ntest programs are generated. The experimental results are\nshown in the box plot in Fig. 6. The x-axis represents the\nnumber of bugs in the benchmarks that contain invalid test\nprograms of GCC bugs, LLVM bugs, and ALL(GCC+LLVM)\nbugs, where the number in the following brackets represents\nthe number of bugs in the benchmarks that generate invalid\ntest programs. We can observe that there are 30 (14 for\nGCC and 16 for LLVM) bugs in total that LLM4CBI selnov\ngenerates invalid test programs for them. The y-axis rep-\nresents the distribution of the number of invalid test pro-\ngrams among those benchmarks, and the results show\nthe average number of invalid test programs for GCC,\nLLVM, and ALL are 2.57, 2.50, and 2.53, respectively. This\nmeans LLM4CBI is able to generate 2.5 more valid test\nprograms than LLM4CBI selnov in the same experimental\nsetting. Compared with all valid test programs generated\nby LLM4CBI, those invalid test programs decrease the bug\nisolation capabilities of LLM4CBI selnov.\nTo further understand why filtering the semantically\ninvalid test programs is important, we showcase two exam-\nples with an undefined behavior in Fig. 7 and indicate their\nside effects for compiler bug isolation. The first example\nshown in Fig. 7(1) includes a kind of undefined behavior\nshift, which is generated by the mutation rule replacing a\nconstant value (from ‚Äú1‚Äù to ‚Äú-1‚Äù) in Line 12. RecBi ranked\nthis bug at 23 th due to the interference of the undefined\nbehavior. The second example exposes a kind of undefined\nbehavior memem_access, which is produced by the mu-\ntation rule inserting a modifier of a variable (from int to\nshort int) in Line 2. Because the size of short int (2\nbytes in 64-bit system) is shorter than int (4 bytes in 64-bit\nsystem), there is an out-of-bound read from the variable b,\nwhich makes RecBi rank this bug at 28 th. Benefiting from\nthe test program validation process, LLM4CBI filters those\nexamples before ranking the suspicious files and finally\nranks the LLVM bug and GCC bug at the position of 9th and\n5th, respectively. This is reasonable, as demonstrated in pre-\nvious studies [2], [52], [63], compilers can have unexpected\nconsequences if the program is semantically invalid.\nOverlapping Analysis. To show whether the valid pro-\ngrams generated by LLM4CBI and LLM4CBI selnov are\noverlapping, we compare text and code coverage similarity\nbetween those valid programs. The text similarity shows\nwhether two programs are similar in terms of content,\nand the code coverage similarity indicates whether two\nprograms are similar in covering the compiler source code,\nas the programs with different contents may share the\nsame code coverage in compilers. To calculate the text\nsimilarity, we use a well-established technique, i.e., TF-IDF\n(Term Frequency-Inverse Document Frequency), to compute\nthe similarity score 3 between two source code files from\nLLM4CBI and LLM4CBI selnov, where the similarity score\nis determined using cosine similarity [64]. Note that the\ncosine similarity score ranges from ‚Äú0‚Äù to ‚Äú1‚Äù. ‚Äú1‚Äù means\nthe two comparative source files are identical, while ‚Äú0‚Äù\nindicates no similarity.\nFig. 8 shows the overall text similarity score among GCC\nand LLVM bugs, where the x-axis represents the score, and\nthe y-axis denotes the frequency of the scores. To be specific,\nwe compare every test program generated by LLM4CBI\nand LLM4CBI selnov element-wise, leading to 11990 and\n6525 comparisons in total for GCC and LLVM bugs. As a\nresult, the mean value of the score is 0.76 for GCC and 0.67\nfor LLVM bugs. We can observe that the valid programs\ngenerated by LLM4CBI and LLM4CBI selnov are hardly\noverlapping. There are only 3 and 10 comparisons that\nhold the score 1, which indicates the overlapping can be\nnegligible. The comparison code coverage similarly shares\nthe same results as text similarity: the results show there are\nonly 15 and 53 comparisons of two comparative source files\nholding the same code coverage in compilers. We believe\nthe overlapping can be negligible since only a small portion\nof the test programs share the same code coverage. In short,\nbased on the comparison results of text and code coverage\nsimilarity, the valid test programs generated by LLM4CBI\nand LLM4CBI selnov are not or with negligible overlapping.\nIn summary, the semantically invalid test programs can\nhave side effects on the effectiveness of compiler bug iso-\nlation, and filtering them does help LLM4CBI improve the\nranking results.\nStatistical Test Analysis of Results . We use the same\nmethodology as RQ1 to calculate p-value and effective size\nÀÜA12 in this RQ. The overall statistical results are shown in\nTable 6. The p-value less than 0.05 in the table shows that\nLLM4CBI performs significantly better than the various\nvariant approaches. Furthermore, we can observe that all\nthe effect sizes are greater than 0.70, which indicates that\nLLM4CBI has a higher probability of obtaining better re-\nsults than comparative variant approaches.\nSummary for RQ2. All three components, including pre-\ncise prompt production, memorized prompt selection, and\nlightweight test program validation, contribute to the effec-\ntiveness of LLM4CBI.\n4.5 Answers to RQ3\n4.5.1 Experimental Settings\nTable 7 presents the evaluated LLMs in this study, with\ncolumn Sizes reflecting the model sizes in billions of param-\neters, Release-Date showing when the LLM is released, and\nPopularity indicating the number of GitHub stars or users\ncounted by 1st June 2023. We evaluate three of the most\nrepresentative and popular LLMs. We choose the above\n3. We used the code from https://github.com/bysiber/text_\nsimilarity_tfidf to calculate the similarity score.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 14\nTABLE 6\nStatistical Test Results (LLM4CBI vs variant approaches)\nApproaches Metrics p-value ÀÜA12\nLLM4CBI ep\nTop-1 <0.0001 0.87\nTop-5 0.0006 0.81\nTop-10 <0.0001 0.91\nTop-20 <0.0001 0.94\nMFR <0.0001 0.92\nMAR <0.0001 0.91\nLLM4CBI sp\nTop-1 0.0014 0.79\nTop-5 0.0275 0.70\nTop-10 0.0080 0.74\nTop-20 <0.0001 0.94\nMFR <0.0001 0.88\nMAR <0.0001 0.89\nLLM4CBI rand\nTop-1 <0.0001 0.88\nTop-5 0.0001 0.85\nTop-10 0.0005 0.82\nTop-20 <0.0001 0.88\nMFR 0.0021 0.79\nMAR 0.0071 0.75\nLLM4CBI selnov\nTop-1 0.0252 0.70\nTop-5 0.0015 0.79\nTop-10 0.0066 0.75\nTop-20 0.0002 0.85\nMFR <0.0001 0.91\nMAR <0.0001 0.91\nTABLE 7\nEvaluated open-source LLMs in LLM4CBI\nModels Size Release-Date Popularity(GitHub)\nAlpaca [7] 7B March 2023 25.0K star\nVicuna [8] 7B March 2023 22.8K star\nGPT4ALL [9] 13B March 2023 46K star\nLLMs mainly because (1) they are very popular that the\nnumber of stars soared to 20k+ in a few months and (2)\nthey are proven to be effective in the code generation task\n[7], [9], [20], [35]. Based on the selected LLMs, we de-\nsign three new variant approaches, i.e., LLM4CBI(Alpaca),\nLLM4CBI(Vicuna), and LLM4CBI(GPT4ALL), by replacing\nthe LLMs in LLM4CBI to answer this RQ.\nSetting Up Different LLMs. For the three LLMs, we down-\nload the GGML4 format of these models from HuggingFace\nwebsite5 and use the python binding llama-cpp-python6\nof llama.cpp7 to run these models on a machine that only\nsupports CPU. Specifically, we use a web server supported\nby llama-cpp-python, which aims to act as a drop-in\nreplacement for the OpenAI API. This feature allows us\nto use llama.cpp compatible models with any OpenAI-\ncompatible client (language libraries, services, etc.). Note\nthat it would be easy and require little human effort to sup-\nport any other interesting models in LLM4CBI. The only\nthing users need to do is to provide the GGML format model,\neither directly download from the HuggingFace website or\nproduct using the detailed and actively maintained tutorials\nin GGML‚Äôs homepage4, for llama-cpp-python web server.\nComparison Strategies. We run those variants within a\none-hour limit and then compare the Top-N metrics for\n4. https://github.com/ggerganov/ggml\n5. https://huggingface.co/\n6. https://github.com/abetlen/llama-cpp-python\n7. The goal of llama.cpp is to run LLMs using 4-bit integer quanti-\nzation on a MacBook only with CPU.\nTABLE 8\nCompiler bug isolation capability of different LLMs in LLM4CBI\n( with 1 hour time limit)\nSubject Approach Top-1 Top-5 Top-10 Top-20\nGCC\nLLM4CBI(Alpaca) 1 10 16 22\nLLM4CBI(Vicuna) 5 15 19 27\nLLM4CBI(GPT4ALL) 2 7 15 21\nLLM4CBI 11 26 41 50\nLLVM\nLLM4CBI(Alpaca) 1 9 18 23\nLLM4CBI(Vicuna) 1 9 19 32\nLLM4CBI(GPT4ALL) 1 5 18 26\nLLM4CBI 10 24 33 48\nALL\nLLM4CBI(Alpaca) 2 19 34 45\nLLM4CBI(Vicuna) 6 24 38 59\nLLM4CBI(GPT4ALL) 3 12 33 47\nLLM4CBI 21 48 74 98\neach approach. We do not repeatedly run those models\nmainly because we aim to demonstrate the extendability of\nreplacing different LLMs in LLM4CBI.\n4.5.2 Experimental Results\nTable 8 shows that all the designed variant ap-\nproaches, i.e., LLM4CBI(Alpaca), LLM4CBI(Vicuna), and\nLLM4CBI(GPT4ALL), contribute to the compiler bug isola-\ntion task. The results indicate that different LLMs can have\ndifferent capabilities for generating test programs for com-\npiler bug isolation, and GPT-3.5 (used as the default model\nin LLM4CBI) achieves the best performance compared with\nother LLMs. Several reasons made the performance of other\nLLMs not as good as GPT-3.5 used in LLM4CBI:\n‚Ä¢ Only giving suggestion responses. Many outputs from\nthose models are suggestions telling users how to change\nthe code rather than the actual code. For example, on iso-\nlating LLVM #17388 with the prompt ‚Äúreplacing a binary\noperator with another valid one on the variables in the list\n[‚Äôa‚Äô, ‚Äôc‚Äô]‚Äù, LLM4CBI(Alpaca) responses, ‚ÄúYou can replace\nthe binary operator ‚Äò||‚Äò with the logical operator ‚Äò&&‚Äò‚Äù.\nSuch output did not directly contain the complete test\nprograms, but that information is still useful for helping\nusers manually write a good passing test program.\n‚Ä¢ Limited code generation capabilities. Although other\nLLMs are targeting a comparable performance compared\nwith GPT-3.5 or GPT-4, they may still be limited by the\nscale of training data. For example, The model Alpaca\nused text-davinci-003 model to generate 52K in-\nstruction data as training data, which may still be lim-\nited compared with the training data scale of GPT-3.5\n(although OpenAI did not disclose the actual number).\n‚Ä¢ Performance issues. Running LLMs typically requires a\npowerful GPU to get better performance. Since our ex-\nperiments run only on CPUs without GPUs, the response\ntime for other LLMs is larger. Based on our observation,\nthe response time of getting an answer from those LLMs\ntakes approximately 300 seconds, which significantly re-\nduces the effectiveness of those approaches. In contrast,\nGPT-3.5 only invokes API for the code generation, and\nthe response time is less than 10 seconds, which makes\nLLM4CBI have a better capability.\nAlthough the results from other LLMs are not as good\nas GPT-3.5, we show that LLM4CBI is extensible. With\nthe rapid development of more powerful LLMs, such as\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 15\nTABLE 9\nCompiler bug isolation capability of different LLMs in LLM4CBI\n(generating the same number of passing test programs)\nSubject Approach Top-1 Top-5 Top-10 Top-20\nGCC\nDiWi [3] 3 14 25 34\nRecBi [4] 5 21 28 38\nLLM4CBI(Alpaca) 6 22 30 40\nLLM4CBI(Vicuna) 7 23 31 41\nLLM4CBI(GPT4ALL) 6 23 32 42\nLLVM\nDiWi [3] 1 11 25 35\nRecBi [4] 3 19 30 37\nLLM4CBI(Alpaca) 4 20 31 39\nLLM4CBI(Vicuna) 6 21 33 40\nLLM4CBI(GPT4ALL) 5 20 32 38\nALL\nDiWi [3] 4 25 50 69\nRecBi [4] 8 40 58 75\nLLM4CBI(Alpaca) 10 42 61 79\nLLM4CBI(Vicuna) 13 44 64 81\nLLM4CBI(GPT4ALL) 11 43 64 80\nFalcon [65], [66], we believe advanced techniques can mit-\nigate the above limitations and further assist the compiler\nbug isolation task in future work.\nWe would like to clarify that the ‚Äúextensible‚Äù used in\nthe paper is only meant from a software architecture per-\nspective, and we define ‚Äúextensible‚Äù as the capability and\ndifficulty of replacing the default LLM (i.e., ChatGPT) of\nLLM4CBI with other LLMs. Regarding the results of bug\nisolation, we think it depends on the capabilities of those\nLLMs; certain limitations in LLMs prevent them from better\ncompiler bug isolation, and we have discussed several rea-\nsons above (i.e., only giving suggestion responses, limited\ncode generation capabilities, and performance issues) why\nsome recent LLMs cannot yield good results. To further\nsupport the potential usefulness of replacing ChatGPT with\nother LLMs, we changed the running setting with termina-\ntion with 1 hour to generate the same number (i.e., 5) of\npassing test programs, and the results are shown in Table 9.\nFrom the table, we can observe that using other LLMs can\nperform better than the two prior studies (DiWi and RecBi).\nSummary for RQ3.LLM4CBI is extensible, meaning LLMs\ncomponent (i.e., GPT-3.5) used in LLM4CBI can be easily\nreplaced by other LLMs (e.g., Alpaca [7], Vicuna [8], and\nGPT4ALL [9]) while still achieving reasonable results in\ncomparison to related studies.\n5 D ISCUSSION\nIn this section, we first introduce the practical implications\nof file-level compiler bug isolation, and then discuss the\ncomparison results with GPT-4 as well as evaluate the\nimpact of different temperature values used in LLM4CBI.\nFinally, we discuss some limitations of LLM4CBI.\n5.1 Practical Implications of File-level Bug Isolation\nIn this paper, we target file-level compiler bug isolation, and\nthe file-level ranking results may not fully solve the com-\npiler bug isolation problem, i.e., LLM4CBI currently is not\ncapable of isolating faulty positions at line-level. However,\ndue to the large number of optimizations (200+ fine-grained\noptimization passes for GCC and LLVM, respectively) and\nfiles (1,588 files and 3,507 files for GCC and LLVM, respec-\ntively) and sophisticated interplay between optimizations,\npinpointing which file is buggy is still challenging. Further-\nmore, with the confirmation by the feedback from compiler\ndevelopers in the previous work (e.g., DiWi [3]), the majority\n(6 out of 7) communicated developers confirmed that their\ncompiler debugging process starts from buggy file iden-\ntification and this step is time-consuming, indicating the\nnecessity of compiler bug isolation at the file level. Although\nsome developers have an intuition about which part may\nbe wrong, not all developers have such expertise, and their\nintuition may not be accurate. We are actively pursuing the\ndesign of a more fine-grained level (e.g., method) compiler\nbug isolation approach. As we do not know whether the\ntraining data of LLMs contains compiler bugs, we also plan\nto curate recent real-world compiler failure reports (so that\nit is less likely that they have been used as training data\nfor LLMs) and evaluate the effectiveness of LLM4CBI for\nbug isolation on those failure cases with actual developers\nof GCC or LLVM.\nWe would like to emphasize the usability of bug isolation\nwith file-level granularity. Compared with normal software\nsystems, the implementation of compilers typically involves\na vast number of source code files (a buggy GCC and LLVM\ncompiler contains 1,758 and 3,265 files on average, respec-\ntively) and sophisticated intersection/interplay between dif-\nferent optimizations as well as complex implementation of\nsingle optimization component (a prior study shows only\nsingle peephole optimization component at least involve 10\nfiles [67]). The above facts indicate the increasing difficulty\nof locating compiler bugs in file granularity compared with\nnormal software systems. For example, the failing test pro-\ngram of bug #16041 involves 418 files in total, and every file\ncould be suspicious of the bug. Furthermore, most compiler\noptimizations are not solely implemented, i.e., optimiza-\ntions can interact in non-trivial ways, where the benefit of\none optimization may affect others, meaning the dependen-\ncies of different files that implement optimizations could be\nexceedingly complex, indicating locating suspicious files for\nonly one single optimization bug is intractable. Considering\nthe large number of compiler optimizations, locating the file\ncontaining the root cause could be considerably difficult. As\nwe demonstrated in the evaluation, LLM4CBI can isolate\n16.80, 47.60, 70.80, and 92.80 compiler bugs (out of a total\nof 120 compiler bugs in GCC and LLVM) within the Top-1,\nTop-5, Top-10, and Top-20 files, respectively. For bug #16041,\namong 418 covered files by failing test program, LLM4CBI\nranks the faulty file at 6 th position, which could help de-\nvelopers debug the bug efficiently. Therefore, an effective\nfile granularity bug isolation approach is still needed and\nuseful for isolating bugs in complex software systems such\nas compilers.\n5.2 Comparison with GPT-4\nIn this study, we are unable to conduct large-scale exper-\niments using GPT-4 in LLM4CBI due to its higher API\ncost8 compared to GPT-3.5. However, we did run two cases\n(LLVM bug 16040 and 25154) using a variant of LLM4CBI,\ncalled LLM4CBI(GPT-4), which incorporates GPT-4. Inter-\nestingly, the results demonstrated that LLM4CBI(GPT-4)\noutperformed LLM4CBI, with both Top 20+ (i.e., Top-40\n8. https://openai.com/pricing\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 16\n(a) GCC\n (b) LLVM\nFig. 9. The results of the impact of different temperature settings\n0 10 20 30 40 50 60\n0\n5\n10\n15\n20Number of variables\nGCC Bugs\nLLVM Bugs\n(a) Scatter plot of the distribution\nGCC Bugs LLVM Bugs All Bugs\n0\n5\n10\n15\n20Number of variables\n (b) Box plot of the distribution\nFig. 10. Distribution of variable numbers among failing test programs\nfor bug 16040 and Top-58 for 25154) bugs being ranked\nwithin Top-5 (i.e., Top-3 for bug 16040 and Top-2 for 25154).\nThe improved performance of LLM4CBI(GPT-4) can be at-\ntributed to two main factors. First, GPT-4 generates a smaller\nnumber of test programs with syntax errors, indicating a\nhigher quality of output. Second, GPT-4 exhibits a better\nunderstanding of the prompt, leading to improved results\n[8], [20]. As LLMs continue to advance rapidly, we anticipate\nthe availability of more capable and user-friendly models\nthat can be integrated into LLM4CBI, further enhancing the\ncompiler bug isolation capabilities of LLM4CBI.\n5.3 Different temperature Settings in LLMs\nThe temperature parameter in LLMs, such as GPT-3.5, plays\na crucial role in controlling the randomness of the generated\noutput. A lower temperature value makes the output more\ndeterministic, while a higher temperature value increases\nrandomness. In the context of LLM4CBI, it is important to\ninvestigate the impact of different temperature values on\nthe performance of test program generation for compiler\nbug isolation. To this end, we conducted experiments using\ntemperature values of 0.4, 0.6, 0.8, 1.0, and 1.2. The bug iso-\nlation capabilities of LLM4CBI were compared under these\ndifferent temperature settings, and the results are shown\nin Fig. 9. The results show that the default temperature\nvalue of 1.0 performs the best for LLM4CBI. This finding\naligns with expectations since OpenAI has fine-tuned this\nparameter and set it as the default value, indicating that it\nyields optimal results.\n5.4 Impacts of selecting complex variables\nSince every failing test program contains a different number\nof variables to be selected, LLM4CBI selects a different\nnumber of complex variables. For the number of variables\nless or equal to 3, LLM4CBI selects the most complex\nTop-1 Top-5 Top-10 Top-20\n0\n1\n2\n3\n4\n5\n6\n7\n8Number of bugs\nComp-3\nComp-6\nComp-8\nFig. 11. Impact of selecting different numbers of complex variables\nvariables ranking at the top one (denoted as Comp-1), and\nfor the rest whose variables numbers are larger than 3,\nLLM4CBI selects the most complex variables ranked at top\nthree (refers to Comp-3). Fig. 10(a) and Fig. 10(b) present the\nscatter and box plot of the distribution of variables over the\n120 bugs. Both y-axis of Fig. 10(a) and Fig. 10(b) indicate the\nnumber of variables that can be selected, while the x-axis in\nFig. 10(a) represents the order of all bugs in GCC and LLVM\nand the x-axis in Fig. 10(b) shows the GCC, LLVM, and\nAll (GCC+LLVM) bugs. The average available and optional\nnumbers of GCC and LLVM are 4.97 and 5.28, respectively.\nFrom the statistical point of view, LLM4CBI selects Comp-\n1 variables for 37.5% bugs (24 for GCC bugs and 21 for\nLLVM bugs) and Comp-3 variables for the rest 62.5% of the\nbugs, where ‚ÄúComp-n‚Äù refers to the most complex variables\nranked at the top n among the optional number of variables.\nIt is worth noting that in our experiments, LLMs generate\nvalid passing programs for all 120 bugs under the above\nvariable selection strategies, meaning that LLMs never fail at\ngenerating any valid witness program when selecting either\nComp-1 or Comp-3 variables for different bugs.\nTo further understand how different numbers of selected\nvariables affect the performance of LLM4CBI, we select\n12 bugs whose optional numbers of variables are larger\nor equal to 9 and choose Comp-3/6/8 as the large set\nof complex variables to study the impacts. The evaluation\nresults are shown in Fig. 11. We can observe that the setting\nof Comp-3‚Äôs results is superior to the other two settings,\nindicating that selecting the most 3 complex variables is a\nbetter choice. The results are reasonable as changing the\nmost complex variables is more likely to yield passing test\nprograms, thus increasing the effectiveness of LLM4CBI,\nwhich is aligned with our initiation to select the most\ncomplex variables to fill in the pattern rather than randomly\nselecting variables like existing approaches.\n5.5 Limitations of LLM4CBI\nThe limitations in LLM4CBI inherit the issues from SBFL\ntechniques and LLMs. First, SBFL suffers from tie issues\n[68], [69]. A possible solution is to address this issue by\nincorporating the use of commit history information [70].\nSecond, LLMs may generate syntactically invalid programs,\nreducing the effectiveness of LLM4CBI. However, based on\nthe experimental results, the number of invalid test pro-\ngrams is acceptable and does not affect the results much. A\nmore effective approach could leverage the program repair\ntechnique [28] to automatically repair the code generated\nby LLMs. We leave further investigation on the above two\ndirections as our future work.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 17\n6 T HREATS TO VALIDITY\nThis section discusses the internal, external, and construct\nthreats of LLM4CBI.\nThe internal validity concerns stem from the implemen-\ntation of LLM4CBI and comparative approaches (DiWi [3]\nand RecBi [4]). To mitigate this threat, we have extended\nthe implementation provided by the previous studies [3],\n[4]. As for LLM4CBI, we have meticulously developed its\nimplementation by leveraging well-established libraries, as\nexplained in Section 4.1, and have conducted thorough code\nchecking of the code. For the tool used in test program\nvalidation, Frama-C [46] can not detect all the undefined\nbehaviors in the programs as the identification of undefined\nbehaviors in the programs is a challenging problem [62],\n[71]. We plan to leverage more advanced techniques to\ndetect undefined behaviors in the test programs.\nThe major external validity of our study can be influenced\nby two key factors: the selection of compilers and the\npresence of bugs. To ensure the reliability of our findings,\nwe have followed the established practices in prior studies\non compiler bug isolation [3], [4] when selecting compilers.\nFollowing these approaches, we have employed two widely\nused open-source C compilers, GCC and LLVM, which are\nrenowned for their popularity and extensive usage within\nthe community. Regarding the bugs used in our study,\nwe chose a comprehensive set of 120 real compiler bugs,\nencompassing all known bugs from prior investigations\nin the field of compiler bug isolation. To further bolster\nthe external validity of our research and address potential\nthreats, we are committed to expanding our bug dataset by\nincorporating additional real compiler bugs.\nAnother threat lies in the LLMs used in this study.\nSpecifically, the training data of LLMs (e.g., ChatGPT) has\nlikely included the source code of GCC and LLVM, and\nmaybe even some historical bug reports, bug-revealing test\nprograms, and bug fixes. However, compared with the\nmassive amounts of training data (more than 45 TB text\n9), the source code of compilers only takes a small part of\namong the training data. Therefore, if bugs used for training\nwere also used in our testing, it is less likely to inflate the\nexperimental results. Ideally, for testing, we should utilize\nnew bugs that have for sure not been included as training\ndata for ChatGPT; however, although there are such newly\nreported bugs in the wild, they are often not fixed yet\nand lack the ground truth for bug isolation, making it\nchallenging to measure the isolation results. We also plan\nto curate recent real-world compiler failure reports (so that\nit is less likely that they have been used as training data\nfor LLMs) and evaluate the effectiveness of LLM4CBI for\nbug isolation on those failure cases with actual developers\nof GCC or LLVM in future work.\nThe construct validity threats are subject to potential\nthreats related to randomness, evaluation metrics, and pa-\nrameter settings during the evaluation process. We address\nthese concerns by (1) repeating experiments 10 times with a\nrunning time of more than 50 days in total and calculating\naverage results to account for randomness, thereby reducing\nthe impact of random variations; (2) employing widely-\n9. https://www.springboard.com/blog/data-science/\nmachine-learning-gpt-3-open-ai/\nused bug localization metrics to assess the effectiveness\nof LLM4CBI to ensure the reliability and comparability\nof our evaluation results; (3) providing explicit parameter\nconfigurations and thoroughly investigating their impact\nin Section 5. By doing so, we enhance transparency and\nenable a deeper understanding of the influence of different\nparameter settings.\n7 R ELATED WORK\nThis section surveys the most related works in this study,\nnamely compiler debugging and LLMs for code generation.\nCompiler Debugging. Our study is primarily related to\ntwo recent works: DiWi [3] and RecBi [4]. Both are gen-\neral solutions for isolating all kinds of bugs in compilers.\nThese works address the problem of isolating compiler bugs\nby transforming them into the generation of passing test\nprograms. DiWi and RecBi achieve this by first utilizing\nmutation operators (DiWi focuses on local mutation oper-\nators while RecBi supports structural mutation operators)\nto generate a set of passing test programs similar to the\nfailing test program, but without triggering the bug. Then,\nthey employ spectrum-based bug localization techniques\n[15], [70] to rank the buggy compiler files by comparing\nexecution traces between the generated passing test pro-\ngrams and the failing test program. LocSeq [47] focuses on\nisolating optimization bugs only in LLVM, while ODFL [48]\naims to isolate bugs only in GCC. We did not compare\nLLM4CBI with either LocSeq or ODFL mainly because\nthey are not general enough and can only be applicable to\nspecific middle-end compiler optimization bugs. In contrast,\nLLM4CBI is capable of isolating all kinds of bugs in com-\npilers, including front-end and back-end bugs.\nIn addition, Zeller [72] introduces a method to facili-\ntate GCC debugging by calculating the cause-effect chain\nthrough a comparison of program states between a passing\nrun and a failing run. Holmes and Groce [73], [74] propose\na method to localize compiler bugs by comparing a set\nof compiler mutants. Regarding compiler debugging tech-\nniques for other programming languages, Chang et al. [75]\npresent an approach for debugging the just-in-time compiler\nin a Java virtual machine. Lim et al. [76], [77] leverage\ndynamic analysis to locate bugs in JIT compilers.\nDifferent from those works, we follow the existing strat-\negy to generate a set of effective test programs for compiler\nbug isolation. In contrast, we propose a new structural mu-\ntation strategy to mutate failing test programs effectively:\nwe support complete control-flow statements mutation that\nincludes both statement conditions and bodies. Besides,\nLLM4CBI only requires little human effort to accomplish\nthe test program mutation process.\nLLMs for Code Generation.The emergence of LLMs has\nsparked significant interest in their application to the code\ngeneration task. Ling et al. [78] follow an encoder-decoder\ndesign and use a sequence-to-sequence LSTM model with\nattention and a copy mechanism to generate Java and\nPython programs. Iyer et al. [79] use a grammar-aware\ndecoder to generate syntactically valid Java parse trees fol-\nlowed by Java codes using a two-step attention mechanism.\nCodeBERT [80] and GraphCodeBERT [81] inherit the design\nof BERT [82] which are encoder-only models to generate\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 18\ncodes. Moreover, CodeT5 [83] and PLBART [84] leverages\nencoder-decoder architecture for generating codes. GLAsT\n[85] and VGen [86] apply decoder-only LLMs for generating\nVerilog RTL programs. Other recent LLMs, such as ChatGPT\n[5], Vicuna [8], WizardLM [87], and StableLM [88], all follow\ndecoder-only architecture to generate various test programs\nin Python, Java, C/C++, and SQL.\nIn this study, we opt for the decoder-only models that\nfollow the prompt-response dialog paradigm to generate the\nwhole test program for the compiler bug isolation task. Dif-\nferent from existing approaches, we design a new pattern for\nprecisely producing prompts in LLM4CBI. The program‚Äôs\ndata and control flow complexity are measured to fill in the\ndesigned pattern. Furthermore, a memorized selection and\na test program validation strategy are proposed to select the\nproper prompt for effectively taming LLMs for test program\nmutation. We also demonstrate LLM4CBI can be extensible\nfor adopting different LLMs for compiler bug isolation.\nPrompt Engineering for LLMs.Increasing number of stud-\nies [6], [22], [29] adopt various strategies for effective prompt\nengineering to leverage the capabilities of LLMs. For exam-\nple, Deng et al. [29] propose TitanFuzz to directly leverage\nLLMs to generate input programs for fuzzing deep learning\nlibraries. Given any target API, TitanFuzz first uses genera-\ntive LLMs to generate a list of high-quality seed programs\nfor fuzzing and then leverages the ability to infill LLMs to\nperform code infilling to generate new code that replaces\nthe masked tokens, where the masked tokens analogous to\nthe mutation operators in this study. During the mutation\nprocess, TitanFuz utilizes an evolutionary fuzzing algorithm\nto iteratively select new code snippets by replacing various\nmasked tokens. Liu et. al [6] introduce QTypist to boost the\nperformance of LLMs in the mobile testing scenario. QTypist\nextracts the context information for the text input and de-\nsigns linguistic patterns to generate prompts for inputting\ninto the LLMs. To further boost the performance of LLMs\nin mobile input scenarios, QTypist adopts a prompt-based\ndata construction and tuning method, which automatically\nbuilds the prompts and answers for model tuning.\nCompared with existing prompt engineering in LLMs,\ntwo major differences between LLM4CBI and others. First,\nthe formulation strategies of prompts are different. Since\nexisting studies focus on generating diverse code/text for\nsoftware testing, they typically share the same idea of ran-\ndom fuzzing. That means, they randomly select elements to\nfill in the prompt patterns. Even though TitanFuzz considers\nstatic analysis to build a fitness function for selecting better\nmutators, it does not take the control flow into account for\nmore effective prompt engineering. Note that such strate-\ngies might be effective for software testing purposes as\nrandom testing has been shown good potential to detect\nprogram bugs. However, due to the lack of precise prompt\ninstructions, it is not effective for the compiler bug isolation\ntask as shown in our experiments. Second, existing works\ndo not consider giving feedback to LLMs to refine the\nprompts, i.e., they only apply one-time execution of LLMs,\nand the learning capabilities of LLMs are not fully activated\nin those works. In contrast, LLM4CBI proposes a precise\nformulation of prompts by leveraging both data and control\nflow analysis. Furthermore, LLM4CBI applies reinforce-\nment learning to refine the prompt and assist LLM4CBI\ncontinuously selecting better prompts.\n8 C ONCLUSION AND FUTURE WORK\nWe have presented LLM4CBI, a new approach to tame\nLLMs for generating effective test programs for compiler\nbug isolation. In LLM4CBI, three new components, i.e., pre-\ncise prompt production, memorized prompt selection, and\nlightweight test program validation, are designed to tackle\nthe two main challenges of formulating precise prompts\nand selecting specialized prompts. Empirical evaluation\nusing 120 real-world bugs from GCC and LLVM demon-\nstrates the effectiveness of LLM4CBI over state-of-the-art\napproaches. Notably, LLM4CBI isolate 69.70%/21.74% and\n24.44%/8.92% more bugs than DiWi and RecBi within Top-\n1/Top-5 ranked results. Furthermore, we demonstrate that\nLLM4CBI is extensible, highlighting its ease of extension to\nother LLMs for the compiler bug isolation task.\nFuture Work. In addition to the promising results presented\nin this paper, there are several exciting directions for further\nenhancing the capabilities of LLM4CBI. We are actively\nexploring the following directions:\n‚Ä¢ Interactive bug isolation. Taking inspiration from interac-\ntive fault localization techniques that leverage user feed-\nback [89], we may incorporate interactive elements into\nLLM4CBI. In detail, by treating LLMs (e.g., ChatGPT) as\na user, we can enable them to learn continuously from\nthe feedback received during the bug isolation process,\nthereby improving the isolation capability.\n‚Ä¢ Intelligent bug isolation. To empower LLMs with more\ndetailed information, such as coverage data, we can en-\nable them to make intelligent decisions regarding poten-\ntially buggy files. Specifically, we plan to guide LLMs\nto employ different evaluation strategies (e.g., different\nformulas used in SBFL) and fine-tune the parameters of\nLLMs to facilitate compiler bug isolation.\nACKNOWLEDGMENT\nThe authors would like to thank all developers who partic-\nipated in this work and the anonymous reviewers for their\ninsightful comments. This work is supported in part by the\nNational Natural Science Foundation of China (Grants No.\n62032004 and No. 62302077) and China Postdoctoral Science\nFoundation (Grants No. 2023M730472).\nREFERENCES\n[1] J. Chen, J. Patra, M. Pradel, Y. Xiong, H. Zhang, D. Hao, and\nL. Zhang, ‚ÄúA survey of compiler testing,‚Äù ACM Computing Surveys\n(CSUR), vol. 53, no. 1, pp. 1‚Äì36, 2020.\n[2] X. Yang, Y. Chen, E. Eide, and J. Regehr, ‚ÄúFinding and under-\nstanding bugs in c compilers,‚Äù in Proceedings of the 32nd ACM\nSIGPLAN conference on Programming Language Design and Imple-\nmentation (PLDI), 2011, pp. 283‚Äì294.\n[3] J. Chen, J. Han, P . Sun, L. Zhang, D. Hao, and L. Zhang, ‚ÄúCompiler\nbug isolation via effective witness test program generation,‚Äù in\nProceedings of the 27th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foundations of Software\nEngineering (ESEC/FSE), 2019, pp. 223‚Äì234.\n[4] J. Chen, H. Ma, and L. Zhang, ‚ÄúEnhanced compiler bug isolation\nvia memoized search,‚Äù in Proceedings of the 35th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering (ASE) , 2020,\npp. 78‚Äì89.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 19\n[5] OpenAI. (2022) ChatGPT: Optimizing language models for\ndialogue. [Online]. Available: https://openai.com/blog/chatgpt/\n[6] Z. Liu, C. Chen, J. Wang, X. Che, Y. Huang, J. Hu, and Q. Wang,\n‚ÄúFill in the blank: Context-aware automated text input generation\nfor mobile gui testing,‚Äù arXiv preprint arXiv:2212.04732, 2022.\n[7] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,\nP . Liang, and T. B. Hashimoto, ‚ÄúStanford alpaca: An instruction-\nfollowing llama model,‚Äù https://github.com/tatsu-lab/stanford_\nalpaca, 2023.\n[8] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\nS. Zhuang, Y. Zhuang, J. E. Gonzalez et al. , ‚ÄúVicuna: An open-\nsource chatbot impressing gpt-4 with 90%* ChatGpt quality,‚Äù 2023.\n[9] Y. Anand, Z. Nussbaum, B. Duderstadt, B. Schmidt, and A. Mul-\nyar, ‚ÄúGPT4All: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo,‚Äù https://github.com/\nnomic-ai/gpt4all, 2023.\n[10] Z. Zhang, J. Xue, D. Yang, and X. Mao, ‚ÄúContextaug: model-\ndomain failing test augmentation with contextual information,‚Äù\nFrontiers of Computer Science, vol. 18, no. 2, p. 182202, 2024.\n[11] J. Xuan and M. Monperrus, ‚ÄúTest case purification for improving\nfault localization,‚Äù in Proceedings of the 22nd ACM SIGSOFT In-\nternational Symposium on Foundations of Software Engineering (FSE) ,\n2014, pp. 52‚Äì63.\n[12] Z. Zhang, Y. Li, J. Xue, and X. Mao, ‚ÄúImproving fault localization\nwith pre-training,‚Äù Frontiers of Computer Science , vol. 18, no. 1, p.\n181205, 2024.\n[13] M. Renieres and S. P . Reiss, ‚ÄúFault localization with nearest neigh-\nbor queries,‚Äù in Proceedings of the 18th IEEE International Conference\non Automated Software Engineering (ASE), 2003, pp. 30‚Äì39.\n[14] S. Moon, Y. Kim, M. Kim, and S. Yoo, ‚ÄúAsk the mutants: Mutating\nfaulty programs for fault localization,‚Äù in IEEE Seventh Inter-\nnational Conference on Software Testing, Verification and Validation\n(ICST), 2014, pp. 153‚Äì162.\n[15] R. Abreu, P . Zoeteweij, and A. J. Van Gemund, ‚ÄúOn the accu-\nracy of spectrum-based fault localization,‚Äù in Testing: Academic\nand industrial conference practice and research techniques-MUTATION\n(TAICP ART-MUTATION), 2007, pp. 89‚Äì98.\n[16] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, ‚ÄúA survey\non software fault localization,‚Äù IEEE Transactions on Software Engi-\nneering, vol. 42, no. 8, pp. 707‚Äì740, 2016.\n[17] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence\nlearning with neural networks,‚Äù Advances in neural information\nprocessing systems, vol. 27, pp. 1‚Äì9, 2014.\n[18] Y. Liu, ‚ÄúFine-tune bert for extractive summarization,‚Äù arXiv\npreprint arXiv:1903.10318, 2019.\n[19] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining for lan-\nguage understanding,‚Äù Advances in neural information processing\nsystems, vol. 32, pp. 1‚Äì11, 2019.\n[20] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, ‚ÄúIs Your Code\nGenerated by ChatGPT Really Correct? Rigorous Evaluation of\nLarge Language Models for Code Generation,‚Äù arXiv preprint\narXiv:2305.01210, 2023.\n[21] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, ‚ÄúPre-\ntrain, prompt, and predict: A systematic survey of prompting\nmethods in natural language processing,‚Äù ACM Computing Sur-\nveys, vol. 55, no. 9, pp. 1‚Äì35, 2023.\n[22] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert,\nA. Elnashar, J. Spencer-Smith, and D. C. Schmidt, ‚ÄúA prompt\npattern catalog to enhance prompt engineering with ChatGpt,‚Äù\narXiv preprint arXiv:2302.11382, 2023.\n[23] L. Reynolds and K. McDonell, ‚ÄúPrompt programming for large\nlanguage models: Beyond the few-shot paradigm,‚Äù in the CHI\nConference on Human Factors in Computing Systems (CHI) , 2021, pp.\n1‚Äì7.\n[24] Y. Yang, P . Huang, J. Cao, J. Li, Y. Lin, and F. Ma, ‚ÄúA prompt-\nbased approach to adversarial example generation and robustness\nenhancement,‚Äù Frontiers of Computer Science , vol. 18, no. 4, p.\n184318, 2024.\n[25] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-\nwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , ‚ÄúLan-\nguage models are few-shot learners,‚Äù Advances in neural informa-\ntion processing systems, vol. 33, pp. 1877‚Äì1901, 2020.\n[26] C. Niu, C. Li, V . Ng, D. Chen, J. Ge, and B. Luo, ‚ÄúAn empirical\ncomparison of pre-trained models of source code,‚Äù arXiv preprint\narXiv:2302.04026, 2023.\n[27] C. S. Xia, Y. Wei, and L. Zhang, ‚ÄúAutomated program repair in\nthe era of large pre-trained language models,‚Äù in Proceedings of\nthe ACM/IEEE 45th International Conference on Software Engineering\n(ICSE), 2023, pp. 1‚Äì12.\n[28] Z. Fan, X. Gao, A. Roychoudhury, and S. H. Tan, ‚ÄúAutomated\nrepair of programs from large language models,‚Äù arXiv preprint\narXiv:2205.10583, 2022.\n[29] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, ‚ÄúLarge\nlanguage models are zero-shot fuzzers: Fuzzing deep-learning li-\nbraries via large language models,‚Äù in Proceedings of the 32nd ACM\nSIGSOFT international symposium on software testing and analysis\n(ISSTA), 2023, pp. 423‚Äì435.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in Neural Information Processing Systems 30: Annual\nConference on Neural Information Processing Systems, 2017, pp. 5998‚Äì\n6008.\n[31] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin,\nD. Pinckney, M.-H. Yee, Y. Zi, C. J. Anderson, M. Q. Feldmanet al.,\n‚ÄúMultipl-e: a scalable and polyglot approach to benchmarking\nneural code generation,‚Äù IEEE Transactions on Software Engineering,\npp. 1‚Äì17, 2023.\n[32] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\nS. Savarese, and C. Xiong, ‚ÄúCodegen: An open large language\nmodel for code with multi-turn program synthesis,‚Äù in Proceed-\nings of the 11th International Conference on Learning Representations\n(ICLR), 2023, pp. 1‚Äì25.\n[33] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi,\nR. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis, ‚ÄúIncoder: A\ngenerative model for code infilling and synthesis,‚Äù 2022, pp. 1‚Äì26.\n[34] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn, ‚ÄúA systematic\nevaluation of large language models of code,‚Äù in Proceedings of the\n6th ACM SIGPLAN International Symposium on Machine Program-\nming (ISMP), 2022, pp. 1‚Äì10.\n[35] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar,\nA. Rodriguez, A. Joulin, E. Grave, and G. Lample, ‚ÄúLLaMA:\nOpen and efficient foundation language models,‚Äù arXiv preprint\narXiv:2302.13971, 2023.\n[36] T. J. McCabe, ‚ÄúA complexity measure,‚Äù IEEE Transactions on Soft-\nware Engineering, no. 4, pp. 308‚Äì320, 1976.\n[37] C. D. Newman, T. Sage, M. L. Collard, H. W. Alomari, and J. I.\nMaletic, ‚Äúsrcslice: A tool for efficient static forward slicing,‚Äù inPro-\nceedings of the 38th International Conference on Software Engineering\nCompanion (SEC), 2016, pp. 621‚Äì624.\n[38] A. V . Aho, M. S. Lam, R. Sethi, and J. D. Ullman, Compilers:\nPrinciples, Techniques, and Tools (2nd Edition) . Addison-Wesley\nLongman Publishing Co., Inc., 2006.\n[39] L. P . Kaelbling, M. L. Littman, and A. W. Moore, ‚ÄúReinforcement\nlearning: A survey,‚Äù Journal of artificial intelligence research , vol. 4,\npp. 237‚Äì285, 1996.\n[40] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT Press, 2018.\n[41] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. Riedmiller, ‚ÄúPlaying atari with deep reinforce-\nment learning,‚Äù arXiv preprint arXiv:1312.5602, 2013.\n[42] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, ‚ÄúPolicy\ngradient methods for reinforcement learning with function ap-\nproximation,‚Äù Advances in neural information processing systems ,\nvol. 12, pp. 1057‚Äì1063, 1999.\n[43] V . Konda and J. Tsitsiklis, ‚ÄúActor-critic algorithms,‚Äù Advances in\nneural information processing systems, vol. 12, pp. 1008‚Äì1014, 1999.\n[44] V . Mnih, A. P . Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, ‚ÄúAsynchronous methods for deep\nreinforcement learning,‚Äù in Proceedings of the International Confer-\nence On Machine Learning (ICML), 2016, pp. 1928‚Äì1937.\n[45] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, ‚ÄúA survey\nof actor-critic reinforcement learning: Standard and natural policy\ngradients,‚Äù IEEE Transactions on Systems, Man, and Cybernetics ,\nvol. 42, no. 6, pp. 1291‚Äì1307, 2012.\n[46] F. Kirchner, N. Kosmatov, V . Prevosto, J. Signoles, and\nB. Yakobowski, ‚ÄúFrama-c: A software analysis perspective,‚ÄùFormal\naspects of computing, vol. 27, no. 3, pp. 573‚Äì609, 2015.\n[47] Z. Zhou, H. Jiang, Z. Ren, Y. Chen, and L. Qiao, ‚ÄúLocseq: Au-\ntomated localization for compiler optimization sequence bugs of\nllvm,‚Äù IEEE Transactions on Reliability , vol. 71, no. 2, pp. 896‚Äì910,\n2022.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING 20\n[48] J. Yang, Y. Yang, M. Sun, M. Wen, Y. Zhou, and H. Jin, ‚ÄúIsolat-\ning compiler optimization faults via differentiating finer-grained\noptions,‚Äù in IEEE International Conference on Software Analysis,\nEvolution and Reengineering (SANER), 2022, pp. 481‚Äì491.\n[49] R. Laboratories. (2023) Oclint. [Online]. Available: https:\n//github.com/oclint\n[50] GNU. (2023) Gcov. [Online]. Available: https://gcc.gnu.org/\nonlinedocs/gcc/Gcov.html\n[51] P . Foundation. (2023) Pytorch. [Online]. Available: https:\n//pytorch.org\n[52] H. Tu, H. Jiang, Z. Zhou, Y. Tang, Z. Ren, L. Qiao, and L. Jiang,\n‚ÄúDetecting C++ compiler front-end bugs via grammar mutation\nand differential testing,‚Äù IEEE Transactions on Reliability, pp. 1‚Äì15,\n2022.\n[53] H. Jiang, Z. Zhou, Z. Ren, J. Zhang, and X. Li, ‚ÄúCTOS: Compiler\ntesting for optimization sequences of llvm,‚Äù IEEE Transactions on\nSoftware Engineering, vol. 48, no. 7, pp. 2339‚Äì2358, 2021.\n[54] H. Tu, H. Jiang, X. Li, Z. Ren, Z. Zhou, and L. Jiang, ‚ÄúRemGen:\nRemanufacturing a random program generator for compiler test-\ning,‚Äù in IEEE 33rd International Symposium on Software Reliability\nEngineering (ISSRE), 2022, pp. 529‚Äì540.\n[55] D. Jeffrey, N. Gupta, and R. Gupta, ‚ÄúFault localization using\nvalue replacement,‚Äù in Proceedings of the 2008 ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis (ISSTA) ,\n2008, pp. 167‚Äì178.\n[56] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst,\nD. Pang, and B. Keller, ‚ÄúEvaluating and improving fault localiza-\ntion,‚Äù in Proceedings of the 39th IEEE/ACM International Conference\non Software Engineering (ICSE), 2017, pp. 609‚Äì620.\n[57] A. Arcuri and L. Briand, ‚ÄúA practical guide for using statistical\ntests to assess randomized algorithms in software engineering,‚Äù in\nProceedings of the 33rd international conference on software engineering,\n2011, pp. 1‚Äì10.\n[58] A. Vargha and H. D. Delaney, ‚ÄúA critique and improvement of the\ncl common language effect size statistics of mcgraw and wong,‚Äù\nJournal of Educational and Behavioral Statistics , vol. 25, no. 2, pp.\n101‚Äì132, 2000.\n[59] G. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks, ‚ÄúEvaluating\nfuzz testing,‚Äù in Proceedings of the 2018 ACM SIGSAC conference on\ncomputer and communications security, 2018, pp. 2123‚Äì2138.\n[60] W. Ma, S. Liu, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, and Y. Liu,\n‚ÄúThe scope of ChatGPT in software engineering: A thorough\ninvestigation,‚Äù arXiv preprint arXiv:2305.12138, 2023.\n[61] X. Wang, H. Chen, A. Cheung, Z. Jia, N. Zeldovich, and M. F.\nKaashoek, ‚ÄúUndefined behavior: what happened to my code?‚Äù in\nProceedings of the Asia-Pacific Workshop on Systems , 2012, pp. 1‚Äì7.\n[62] X. Wang, N. Zeldovich, M. F. Kaashoek, and A. Solar-Lezama,\n‚ÄúTowards optimization-safe systems: Analyzing the impact of\nundefined behavior,‚Äù in Proceedings of the ACM Symposium on\nOperating Systems Principles (SOSP), 2013, pp. 260‚Äì275.\n[63] C. Sun, V . Le, Q. Zhang, and Z. Su, ‚ÄúToward understanding\ncompiler bugs in gcc and llvm,‚Äù in Proceedings of the 25th ACM\nSIGSOFT International Symposium on Software Testing and Analysis\n(ISSTA), 2016, pp. 294‚Äì305.\n[64] C. D. Manning, Introduction to Information Retrieval . Syngress\nPublishing, 2008.\n[65] E. Almazrouei, A. Cappelli, R. Cojocaru, M. Debbah, E. Goffinet,\nD. Heslow, J. Launay, Q. Malartic, B. Noune, B. Pannier, and\nG. Penedo, ‚ÄúFalcon-40b: an open large language model with state-\nof-the-art performance,‚Äù Hugging Face, 2023.\n[66] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cap-\npelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay,\n‚ÄúThe refinedweb dataset for falcon llm: outperforming curated\ncorpora with web data, and web data only,‚Äù arXiv preprint\narXiv:2306.01116, 2023.\n[67] T. Theodoridis, M. Rigger, and Z. Su, ‚ÄúFinding missed optimiza-\ntions through the lens of dead code elimination,‚Äù in Proceedings\nof the ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems (ASPLOS), 2022, pp.\n697‚Äì709.\n[68] Y. Yu, J. A. Jones, and M. J. Harrold, ‚ÄúAn empirical study of the\neffects of test-suite reduction on fault localization,‚Äù in Proceedings\nof the International Conference on Software Engineering (ICSE) , 2008,\npp. 201‚Äì210.\n[69] X. Xu, V . Debroy, W. Eric Wong, and D. Guo, ‚ÄúTies within fault\nlocalization rankings: Exposing and addressing the problem,‚Äù\nInternational Journal of Software Engineering and Knowledge Engineer-\ning, vol. 21, no. 06, pp. 803‚Äì827, 2011.\n[70] M. Wen, J. Chen, Y. Tian, R. Wu, D. Hao, S. Han, and S.-C. Cheung,\n‚ÄúHistorical spectrum based fault localization,‚ÄùIEEE Transactions on\nSoftware Engineering, vol. 47, no. 11, pp. 2348‚Äì2368, 2019.\n[71] J. Lee, Y. Kim, Y. Song, C.-K. Hur, S. Das, D. Majnemer, J. Regehr,\nand N. P . Lopes, ‚ÄúTaming undefined behavior in llvm,‚Äù ACM\nSIGPLAN Notices, vol. 52, no. 6, pp. 633‚Äì647, 2017.\n[72] A. Zeller, ‚ÄúIsolating cause-effect chains from computer programs,‚Äù\nACM SIGSOFT Software Engineering Notes , vol. 27, no. 6, pp. 1‚Äì10,\n2002.\n[73] J. Holmes and A. Groce, ‚ÄúCausal distance-metric-based assistance\nfor debugging after compiler fuzzing,‚Äù in IEEE 29th International\nSymposium on Software Reliability Engineering (ISSRE) , 2018, pp.\n166‚Äì177.\n[74] H. Josie and G. Alex, ‚ÄúUsing mutants to help developers distin-\nguish and debug (compiler) faults,‚Äù Software Testing, Verification\nand Reliability, vol. 30, no. 2, p. e1727, 2020.\n[75] B.-Y. E. Chang, A. Chlipala, G. C. Necula, and R. R. Schneck,\n‚ÄúType-based verification of assembly language for compiler de-\nbugging,‚Äù in Proceedings of the ACM International Workshop on Types\nin Languages Design and Implementation (PLDI) , 2005, pp. 91‚Äì102.\n[76] H. Lim and S. Debray, ‚ÄúAutomatically localizing dynamic code\ngeneration bugs in jit compiler back-end,‚Äù inProceedings of the 32nd\nACM SIGPLAN International Conference on Compiler Construction\n(CC), 2023, pp. 145‚Äì155.\n[77] H. Lim and S. K. Debray, ‚ÄúAutomated bug localization in jit\ncompilers,‚Äù in Proceedings of the 17th ACM SIGPLAN/SIGOPS\nInternational Conference on Virtual Execution Environments, 2021, pp.\n153‚Äì164.\n[78] W. Ling, P . Blunsom, E. Grefenstette, K. M. Hermann, T. KoÀá cisk`y,\nF. Wang, and A. Senior, ‚ÄúLatent predictor networks for code\ngeneration,‚Äù in Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL), 2016, pp. 599‚Äì609.\n[79] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúMapping\nlanguage to code in programmatic context,‚Äù in Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), 2018, pp. 1643‚Äì1652.\n[80] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou,\nB. Qin, T. Liu, D. Jiang et al., ‚ÄúCodebert: A pre-trained model for\nprogramming and natural languages,‚Äù inFindings of the Association\nfor Computational Linguistics (ACL), 2020, pp. 1536‚Äì1547.\n[81] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,\nJ. Yin, D. Jiang, and M. Zhou, ‚ÄúGraphcodebert: Pre-training code\nrepresentations with data flow,‚ÄùProceedings of the 11th International\nConference on Learning Representations, vol. abs/2009.08366, 2020.\n[82] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,‚Äù arXiv preprint arXiv:1810.04805, 2018.\n[83] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,\n‚ÄúCodet5+: Open code large language models for code understand-\ning and generation,‚Äù arXiv preprint arXiv:2305.07922, 2023.\n[84] W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang, ‚ÄúUnified pre-\ntraining for program understanding and generation,‚Äù in Proceed-\nings of the Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (AACL-\nHLT), 2021, pp. 2655‚Äì2668.\n[85] C. B. Harris and I. G. Harris, ‚ÄúGlast: Learning formal gram-\nmars to translate natural language specifications into hardware\nassertions,‚Äù in Design, Automation & Test in Europe Conference &\nExhibition (DATE), 2016, pp. 966‚Äì971.\n[86] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan-\nGavitt, and S. Garg, ‚ÄúBenchmarking large language models for\nautomated verilog rtl code generation,‚Äù in Design, Automation &\nTest in Europe Conference & Exhibition (DATE), 2023, pp. 1‚Äì6.\n[87] C. Xu, Q. Sun, K. Zheng, X. Geng, P . Zhao, J. Feng, C. Tao,\nand D. Jiang, ‚ÄúWizardlm: Empowering large language models to\nfollow complex instructions,‚Äù arXiv e-prints, pp. 1‚Äì39, 2023.\n[88] Stability-AI. (2023) Stablelm: Stability ai language models.\n[Online]. Available: https://github.com/Stability-AI/StableLM\n[89] L. Gong, D. Lo, L. Jiang, and H. Zhang, ‚ÄúInteractive fault local-\nization leveraging simple user feedback,‚Äù in the IEEE International\nConference on Software Maintenance (ICSM), 2012, pp. 67‚Äì76.",
  "topic": "Compiler",
  "concepts": [
    {
      "name": "Compiler",
      "score": 0.9111428260803223
    },
    {
      "name": "Computer science",
      "score": 0.7829073667526245
    },
    {
      "name": "Debugging",
      "score": 0.7591910362243652
    },
    {
      "name": "Isolation (microbiology)",
      "score": 0.627849817276001
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.6176878213882446
    },
    {
      "name": "Programming language",
      "score": 0.48993176221847534
    },
    {
      "name": "Mutation",
      "score": 0.4422687888145447
    },
    {
      "name": "Control flow",
      "score": 0.43835195899009705
    },
    {
      "name": "Software engineering",
      "score": 0.42423051595687866
    },
    {
      "name": "Test (biology)",
      "score": 0.41590696573257446
    },
    {
      "name": "Bioinformatics",
      "score": 0.07419198751449585
    },
    {
      "name": "Biology",
      "score": 0.06741693615913391
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210102840",
      "name": "Ubiquitous Energy (United States)",
      "country": "US"
    }
  ],
  "cited_by": 1
}