{
  "title": "Sentiment analysis of Malayalam tweets using bidirectional encoder representations from transformers: a study",
  "url": "https://openalex.org/W4312080494",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5056763010",
      "name": "Syam Mohan Elankath",
      "affiliations": [
        "Pondicherry University"
      ]
    },
    {
      "id": "https://openalex.org/A5059744053",
      "name": "R. Sunitha",
      "affiliations": [
        "Pondicherry University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904900314",
    "https://openalex.org/W4283757520",
    "https://openalex.org/W2001640531",
    "https://openalex.org/W2973113203",
    "https://openalex.org/W2884977996",
    "https://openalex.org/W4287901611",
    "https://openalex.org/W3017793562",
    "https://openalex.org/W3193668308",
    "https://openalex.org/W2577670406",
    "https://openalex.org/W3093906434",
    "https://openalex.org/W2944670321",
    "https://openalex.org/W3006198647",
    "https://openalex.org/W3015597294",
    "https://openalex.org/W2597410399",
    "https://openalex.org/W2613117489",
    "https://openalex.org/W2994859712",
    "https://openalex.org/W2985056549",
    "https://openalex.org/W3093495200",
    "https://openalex.org/W3184182155",
    "https://openalex.org/W3116641301",
    "https://openalex.org/W3170692621",
    "https://openalex.org/W3118532741",
    "https://openalex.org/W4206941769",
    "https://openalex.org/W2187227891",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W6769318315",
    "https://openalex.org/W2979860911",
    "https://openalex.org/W3128114698",
    "https://openalex.org/W3112483730",
    "https://openalex.org/W2901469510",
    "https://openalex.org/W3147682804",
    "https://openalex.org/W3091850998",
    "https://openalex.org/W2608581744",
    "https://openalex.org/W2779564163",
    "https://openalex.org/W2802698310",
    "https://openalex.org/W2769559752",
    "https://openalex.org/W3176644009",
    "https://openalex.org/W2945995044",
    "https://openalex.org/W2801241673",
    "https://openalex.org/W2518750284",
    "https://openalex.org/W3194462780",
    "https://openalex.org/W4283711994",
    "https://openalex.org/W4224121874",
    "https://openalex.org/W3176969785",
    "https://openalex.org/W2169816422",
    "https://openalex.org/W1577417017",
    "https://openalex.org/W2046858834",
    "https://openalex.org/W3118751087",
    "https://openalex.org/W2727488328",
    "https://openalex.org/W2790309729",
    "https://openalex.org/W2973508239",
    "https://openalex.org/W2931976157",
    "https://openalex.org/W2938440964",
    "https://openalex.org/W2920873208",
    "https://openalex.org/W2973946059",
    "https://openalex.org/W3118321697",
    "https://openalex.org/W3125348957",
    "https://openalex.org/W3197066695",
    "https://openalex.org/W2972789371",
    "https://openalex.org/W3206224646",
    "https://openalex.org/W3028167859",
    "https://openalex.org/W3010500276",
    "https://openalex.org/W2976790809",
    "https://openalex.org/W2982567551",
    "https://openalex.org/W4297782913",
    "https://openalex.org/W2769151826",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Sentiment analysis on views and opinions expressed in Indian regional languages has become the current focus of research. But, compared to a globally accepted language like English, research on sentiment analysis in Indian regional languages like Malayalam are very low. One of the major hindrances is the lack of publicly available Malayalam datasets. This work focuses on building a Malayalam dataset for facilitating sentiment analysis on Malayalam texts and studying the efficiency of a pre-trained deep learning model in analyzing the sentiments latent in Malayalam texts. In this work, a Malayalam dataset has been created by extracting 2,000 tweets from Twitter. The bidirectional encoder representations from transformers (BERT) is a pretrained model that has been used for various natural language processing tasks. This work employs a transformer-based BERT model for Malayalam sentiment analysis. The efficacy of BERT in analyzing the sentiments latent in Malayalam texts has been studied by comparing the performance of BERT with various machine learning models as well as deep learning models. By analyzing the results, it is found that a substantial increase in accuracy of 5% for BERT when compared with that of Bi-GRU, which is the next bestperforming model.",
  "full_text": "Indonesian Journal of Electrical Engineering and Computer Science \nVol. 29, No. 3, March 2023, pp. 1817~1826 \nISSN: 2502-4752, DOI: 10.11591/ijeecs.v29.i3.pp1817-1826 ÔÅ≤     1817 \n \nJournal homepage: http://ijeecs.iaescore.com \nSentiment analysis of Malayalam tweets using bidirectional \nencoder representations from transformers: a study \n \n \nSyam Mohan Elankath, Sunitha Ramamirtham \nDepartment of Computer Science, Pondicherry University, Puducherry, India \n \n \nArticle Info  ABSTRACT  \nArticle history: \nReceived May 25, 2022 \nRevised Nov 18, 2022 \nAccepted Nov 24, 2022 \n \n Sentiment analysis on views and opinions expressed in Indian regional \nlanguages has become the current focus of research. But, compared to a \nglobally accepted language like English, research on sentiment analysis in \nIndian regional languages like Malayalam are very low. One of the major \nhindrances is the lack of publicly available Malayalam datasets. This work \nfocuses on building a Malayalam dataset for facilitating sentiment analysis on \nMalayalam texts and studying the efficiency of a pre -trained deep learning \nmodel in analyzing the sentiments latent in Malayalam texts. In this work, a \nMalayalam dataset has been created by extracting 2,000 tweets from Twitter. \nThe bidirectional encoder representations from transformers (BERT) is a pre-\ntrained model that has been used for various natural language processing \ntasks. This work employs a transformer -based BERT model for Malayalam \nsentiment analysis. The efficacy of BERT in analyzing the sentiments latent \nin Malayalam texts has been studied by comparing the performance of BERT \nwith various machine learning models as well as deep learning models. By \nanalyzing the results, it is found that a substantial increase in accuracy of 5% \nfor BERT when compared with that of Bi -GRU, which is the next best -\nperforming model. \nKeywords: \nBERT \nDeep learning \nMachine learning \nMalayalam tweets \nSentiment analysis \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nSyam Mohan Elankath \nDepartment of Computer Science, Pondicherry University \nPuducherry, India \nEmail: syammohane@gmail.com \n \n \n1. INTRODUCTION  \nMassive amount of textual data is uploaded to the Internet every day through various social media \nplatforms by users globally. According to the Twitter statistics of 2018 [1], a stunning statement reveals that \nin a year 500 million tweets are posted, which means 6 ,000 tweets are posted each second. This unstructured \ndata comprises plenty of intrinsic subjective information, the analysis of this subjective information can be \nbeneficial in countless spaces. Sentiment analysis (SA) helps to extract this latent information from such data \nby analyzing and processing it. Sometimes, SA is also referred as sentiment mining, and opinion mining. \nwherein the expressed opinions or sentiments a re identified and its polarity is classified as negative, positive, \nor neutral. Given the unstructured textual data, SA is performed at different granularities viz. document level, \nsentence level, and aspect -level [2]. In document level SA, the overall sen timent orientation of the text \ndocument is evaluated. Whereas in sentence -level SA, the sentiment orientation of all sentences in the \ndocument are individually evaluated. The most fine-grained level SA is the Aspect level SA where aspects are \nthe attributes that characterizes the entities. In this type of SA, all aspects present in the text are determined \nand later their related sentiments are evaluated. Machine learning (ML) and deep learning (DL) approaches \nhave shown promising results in the area of SA, just like all other areas where it accomplished excellently [3]. \n\n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 29, No. 3, March 2023: 1817-1826 \n1818 \nHandling the language component of a text data is the challenging aspect of SA [4]. Present-day online \nentertainment platforms empower individuals to communicate their perspectives in various  worldwide \nlanguages. Hence analyzing sentiments present in different languages has become an aspect of research. Being \na universal language, lot of research has happened in SA on English text. But very few researches have \nhappened on SA with languages oth er than English, especially Indian regional languages. Malayalam is one \namong the 22 official languages in India and is spoken by 38 million people across the world. It is a south \nIndian language that comes under the Dravidian family and is also a morpholo gically rich agglutinative \nlanguage, where comparably few research happened in SA [5]-[8]. One of the major reasons for this gap, is the \nlack of proper dataset and corpus in Malayalam language to facilitate SA.  \nRakshitha et al. [9] extracted tweets using Twitter API of five different Indian languages including \nKannada, Hindi, Telugu, Malayalam, and Tamil. They have used Python package TextBlob for finding the \nsentiment polarity. Rohini et al. [10] created a dataset consisting of mo vie reviews in the Kannada language \nfrom various websites. The authors have used decision tree (DT) classifier for finding the sentiment of reviews \nand furthermore they have compared the results with machine-translated English reviews of the same. Vrunda \nJoshi and Vekariya [11] compiled reviews in the Gujarati language from different social networking websites \nlike Facebook, Twitter, and so on. Document level SA is done on the dataset using five different ML algorithms \nsuch as support vector machine (SVM), Na√Øve Bayes (NB), k-nearest neighbors (KNN), multi-layer perceptron \nand found that SVM is performing better than other ML algorithms with their dataset. Shrivastava and Kumar \n[12] proposed an approach with genetic algorithm to select the hyperparameter setting on the gated recurrent \nunit (GRU) model for SA in the Hindi language. Here , the authors have manually created a dataset consisting \nof 1 ,352 revie ws. Mathews and Abraham  [5] proposed a rule -based approac h for SA on the Malayalam \nlanguage. The authors have collected 136 tweets from Twitter and manually annotated them.  \nIn an earlier work of Kumar et al. [6], DL models convolutional neural networks (CNN) and long \nshort term memory (LSTM) were compared for SA on Malayalam tweets. Later on, in another work [7], the \nauthors considered SVM and regularized least-squares classification (RLSC) as baseline models and compared \nthem with DL models CNN and LSTM. They have used a manually creat ed Malayalam Twitter dataset \nconsisting of 13 ,000 tweets for their work. Soumya and Pramod  [8] did a binary classification SA on \nMalayalam tweets using three ML models consisting of SVM, NB, and random forest (RF). They have used \nUnigram, term frequency-inverse document frequency (TF-IDF) and bag of words (BoW), with SentiWordNet \nas feature selection algorithms. A dataset consisting 3,184 tweets in Malayalam language has been constructed. \nBayhaqy et al.  [13] have done Hindi SA over movie reviews by creating  a small datasets containing 250 \nreviews. The authors have utilized Hindi SentiWordNet and machine translation method for doing the SA. \nSoumya and Pramod [14] have done the same work as in [8], by replacing the ML models with various DL \nmodels like recurrent neural network (RNN), LSTM, and GRU. Thavareesan and Mahesan [15] used 5 different \ncorpora's that contain a total of 2,691 reviews in the Indian language Tamil, which are collected from different \nsocial media platforms. Authors also compared different feature selection algorithms like BoW, TF, and TF-\nIDF with different ML techniques such as SVM, RF, NB, and KNN. Prasad et al. [16] have done SA on Indian \nlanguages Bengali and Tamil by creating datasets that consist of 999 and 1,103 tweets respectively. ML models \nNB and DT are compared by training on their datasets. Naidu et al. [17] created a dataset with newspaper \nsentences in the Indian language Telugu, which contained 1400 labeled sentences. The authors used Telugu \nSentiWordNet to classify the sentiments in this work.  \nSharif et al. [18] have done SA with restaurant reviews in the Indian language Bengali, where the \nauthors created a dataset and trained a model using the ML technique multinomial NB. Li et al. [19] proposed \na model that dismisses the necessity for additional training in the bidirectional encoder representations from \ntransformers (BERT) model. They devised two simple modules called Hierarchical Aggregation and Parallel \nAggregation to use in conjunc tion with BERT. Karimi et al. [20] analyzed the BERT embedding component \nfor the task of end-to-end aspect based sentiment analysis (ABSA). Abdelguad [21] used pre-trained BERT on \nArabic hotel reviews dataset and found that multilingual BERT performs very well and is robust to overfitting \non Arabic language. Safaya et al. [22] used BERT with CNN for multilingual offensive language classification \nwith the SemEval 2020 dataset. Their results indicate that combining Convolutional Neural Network (CNN) \nwith BERT improves the performance than using BERT alone. Jafrian et al. [23] used sentence pair input for \nBERT, which showed better results for Persian ABSA.  Horne et al. [24] proposed a method which combines \nBERT hidden layers with GRU so that it improves the performance on Twitter SA.  Moubtahij et al. [25] have \ndone Arabic SA using Arabic BERT (AraBERT), a transformer -based model for the Arabic language. They  \nhave used ARev dataset, which holds more than 40 ,000 reviews on the tourism domain, and it is found that \nAraBERT performs competently with the existing works in the Arabic language.  \nFrom the literature, it is clear that a major hindrance to SA research ov er Indian languages is the lack \nof good datasets. Table 1 shows various manually created datasets for SA in different languages in India. Even \nthough manual creation of a dataset is a challenging task, the majority of SA on Indian regional languages were \nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \n Sentiment analysis of Malayalam Tweets using bidirectional encoder ‚Ä¶ (Syam Mohan Elankath) \n1819 \ndone on their own manually created datasets by researchers. Moreover, the majority of these languages are \nmorphologically complex and agglutinative, making the SA process considerably more challenging.  \nThe objective of this study is to assess the performanc e of the transformer-based BERT model in the \nMalayalam language, as there are no works on SA using BERT on the Malayalam language. The problem of \nSA is portrayed as a binary classification of Malayalam tweets' overall polarity as positive or negative. In t his \npaper, the authors have done SA on Malayalam tweets utilizing BERT [26] which is a powerful pre -trained \nlanguage model. It is pre -trained on millions of textual documents, which enables the BERT model to \nunderstand the language and domain when compared to other ML and DL models. Moreover, BERT supports \n104 languages which in turns helps to understand and resolve diverse problems in languages other than English \nincluding SA. This work aims at studying the performance of BERT model in carryout the SA of M alayalam \ntweets. As there are no publicly available datasets on Malayalam text, a dataset is manually created by \nextracting Malayalam tweets from Twitter using Twitters API. A total of 2 ,000 tweets were extracted which \nhad explicit sentiment words as hasht ags. The manually created twitter dataset is used for training the \nmultilingual BERT (mBERT) model [26] for Malayalam language. Furthermore, the results of BERT are \ncompared with various ML and DL models such as SVM, NB, DT, KNN, RF, logistic regression (LR), GRU, \nBi-directional GRU (Bi -GRU), LSTM, and Bi -directional LSTM (Bi -LSTM), in order to evaluate the \nperformance of BERT against legacy methods. Results shows that BERT outperforms all other ML and DL \nmodels with a highest accuracy of 88.61% followed by  Bi-GRU with an accuracy of 83%. The ML model \nKNN achieved the lowest accuracy of 62.94%. \n The remaining sections of this paper are organized as follows: section 2 describes the proposed work \nand briefly explains different ML and DL approaches employed in this work. Section 3 discusses the results \nand comparative analysis of different ML and DL models. Finally, section 4 concludes the paper. \n \n \nTable 1. Manually created datasets in Indian languages \nDataset Language Size \nMathews and Abraham [5] Malayalam 136 \nKumar et al. [6] Malayalam 13000 \nKumar et al. [7] Malayalam 12922 \nSoumya and Pramod [8] Malayalam 3184 \nRohini et al. [10] Kannada 100 \nJoshi and Vekariya [11] Gujarati 40 \nShrivastava and Kumar [12] Hindi 8352 \nBayhaqy et al. [13] Hindi 230 \nSoumya and Pramod [14] Malayalam 5468 \nThavareesan and Mahesan [15] Tamil 2691 \nPrasad et al. [16] Bengali, Tamil 999, 1103 \nNaidu et al. [17] Telugu 1400 \nSharif et al. [18] Bengali 1427 \n \n \n2. METHOD \nThe objective of this study is to assess the performance of the transformer-based BERT model in the \nMalayalam language, as there are no works on SA using BERT on the Malayalam language.  Furthermore, the \nresults of BERT are compared with various ML and DL models such as SVM, NB, D T, KNN, RF, LR, GRU, \nBi-GRU, LSTM, and Bi-LSTM, in order to evaluate the performance of BERT against legacy methods.  Each \nof these approaches are briefly explained.  \n \n2.1.  BERT \nBERT is a powerful DL -based state-of-the-art language model for numerous tasks in NLP natural \nlanguage inference, question answering, and text classification [8]. It is built on encoders of transformers and \nis also pre -trained on millions of text documents. Pre -training of BERT is done using two methods, namely \nmasked language modeling (MLM) and next sentence prediction (NSP). The overall architecture of the BERT \nmodel is given in Figure 1. Contextual bi -directional embedding is being supplied by BERT, where \ncontextualization means that the same words can have a different meaning with respect to the domains. For \nthat, unlike LSTM, BERT acquires the input sentence as a whole input, and therefore it is bi -directional. The \nhigher layers of BERT extract the language semantics, and the lower layers extract the syntacti c information. \nThe first input to the model is the CLS token, which is used as a classification token. It is followed by the \nsequence of words in the input. This input is then given to a stack of encoders, where it passes through self -\nattention and feedfor ward networks. The primary objective of self -attention is to provide contextual \ninformation to terms in the sentence. 12 transformer -based encoders are there in a BERT base model. The \noutput of this model will be a vector of size 768, which can be given to  a classification layer fo r the task of \nclassification [25]-[31]. \n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 29, No. 3, March 2023: 1817-1826 \n1820 \n \n \nFigure 1. Overall architecture of BERT model \n \n \n2.2.  ML approaches \n2.2.1. Decision tree (DT) \nThe DT is a logic -based algorithm where the whole complex decision is divided into various \nuncomplicated, simpler decisions. In other words, we can say that it is a mathematical model used to represent \na decision-making process. In this technique, a logica l tree is constructed with different levels of logical \nconditions and options which helps to derive the desired solution [31]-[36]. \n \n2.2.2. Logistic regression (LR) \nLR algorithm uses the logistic sigmoid function to calculate the probability of the target variable.  This \nsupervised algorithm is an updated version of linear regression for classification tasks where the sigmoid \nfunction is oblique to map the original value between 0 and 1. Unlike other classification models, LR not only \nclassifies the data but also gives the probability of that data in its particular category [37]-[41].  \n \n2.2.3. Support vector machine (SVM) \nSVM is a supervised ML algorithm used for classification and regression tasks. On an n-dimensional \ngraph, all the data items will be plotted and a line will be drawn around the support vectors separating different \nclasses. This line is called a hyperplane and there will be many hyperplanes. Among them, one hyperplane is \nchosen when it satisfies the highest distance from the support vectors. In the background, SVM solves the \ncomplex optimization problem which helps to maximize the distance from support vectors t o the hyperplane \n[34], [36], [42]-[47].  \n \n2.2.4. Random forest (RF) \nRF is an ensemble approach that contains an extensive number of decision trees. The result of the RF \nalgorithm is calculated by taking the average of outputs of individual decision trees. So as the number of trees \nincreases, the accuracy of the RF also improves. Also, the problem of overfitting found in DT algorithm  is \nresolved in RF approach. [35], [36], [45], [47].  \n \n2.2.5. Na√Øve bayes (NB) \nNB is a straightforward but powerful statistics-based approach for predictive modeling. It depends on \nthe Bayesian theorem of likelihood which makes the probabilities for every event. NB assumes that each feature \nis independent and hence the most elevated probability output is predicted. The advantage of the NB classifier \nis that it needs less training data and still gives promising results. The drawback of this method is that it is also \nknown as a bad estimator since it assumes each feature as independent [48]-[51].  \n \n \n\nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \n Sentiment analysis of Malayalam Tweets using bidirectional encoder ‚Ä¶ (Syam Mohan Elankath) \n1821 \n2.2.6. K-nearest neighbors (KNN) \nKNN is lazy learner algorithm which employs a simple classification technique. The dataset will be \nstored in the initial phase and when new data arrives, based on the similarity of the new data with stored data, \nKNN determines its categories. Here Euclidea n distance is calculated to find the K nearest neighbors. The \nKNN algorithm works well with noisy training data as well as the implementation is simple. The disadvantage \nis that when new data comes, K neighbors have to be recalculated again, which in turns  increase the \ncomputational time consumption [45], [50], [52], [53].  \n \n2.3.  DL approaches \n2.3.1. Long short-term memory (LSTM) \nLSTM is a t ype of RNN  and LSTM overcomes the RNNs problem of long -term dependency. Also, \nvanishing gradients and exploding gradients problems that arise while the training process is also solved in \nLSTM. Unlike most ML models, LSTM can memorize information for a prolonged amou nt of period. This is \nfacilitated by an explicit memory unit named cell in its architecture. T he variant, Bi -LSTM contains two \nLSTMs where one takes input in the forward direction and the other in the opposite direction. This arrangement \nenables the Bi-LSTM model to include more context knowledge [54]-[61].  \n \n2.3.2. Gated recurrent unit (GRU) \nGRU is an advanced type of RNN and it is a variant of LSTM. Instead of having a separate memory \nunit called cell, GRU have hidden states to store information. Just like LSTM, GRU also uses gated mechanism \nto control the flow and here there are only two gates, namely, update gate and forget gate. Update gate makes \nsure of the amount of information flowing to the future and forget gate removes the irr elevant information. \nThis makes the model less complex and hence it is much faster than the LSTM. Also, GRU performs well \nwhen the training data is comparably small. Bi -GRU comprises of two GRUs, where one takes input in the \nbackward direction and the othe r one takes input in the forward direction. In language processing,  \nBi-GRU gives better results as it can understand the underlying information in the languages compared to other \nmodels [62]-[67].  \nThe objective of this work can be divided into three; assessing the multilingual BERT model's \neffectiveness with the Malayalam language, creating the Malayalam dataset on tweets, and finally, comparing \nthe results of the BERT model with aforementioned legacy methods. Figure 2 shows the architecture of SA for \nMalayalam tweets. \n \n \n \n \nFigure 2. Architecture of sentiment analysis \n \n \n2.3.3. Construction of Malayalam dataset \nThe lack of datasets for SA on Malayalam is main hindrance to research in this field. As of now, there \nare no publicly available datasets for Malayalam SA. Therefore, the authors have created a dataset by extracting \nMalayalam tweets from Twitter. For the extraction of tweets, a set of Malayalam sentiment words as hashtags \nare used. Table 2 shows the list of po sitive and negative Malayalam hashtags used for extracting the tweets. \nWith the help of Twitter API, these hashtags were used to extract tweets from Twitter. Further, these tweets \nwere manually labeled based on their sentiment polarity into two classes, viz. negative and positive. A total of \n2,000 tweets are labeled, where 50% are positive tweets and the other 50% are negative sentiment oriented. \nTable 3 and Table 4 expresses the sample positive and negative tweets along with their English translations . \n \n2.3.4. Preprocessing \nAfter the creation of the dataset, preprocessing is done on the data to make sure it is suitable for the \nfurther processing. The extracted tweets in the dataset contain a lot of irrelevant details for SA like hyperlinks, \n\n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 29, No. 3, March 2023: 1817-1826 \n1822 \nuser id‚Äôs, and whitespaces. In the preprocessing stage, white spaces, punctuations, mentions, and URLs are \nremoved from the extracted tweets. \n \n \nTable 2. Malayalam hashtags \nPositive Negative \n‡¥∏‡¥®‡µç‡¥§‡µã‡¥∑‡¥Ç (Happy) ‡¥∏‡¥ô‡µç‡¥ï‡¥ü‡¥Ç (Sad) \n‡¥á‡¥∑‡µç‡¥ü‡¥Ç (Love) ‡¥¶ ‡µÅ‡¥É‡¥ñ‡¥Ç (Sad) \n‡¥ó‡¥Ç‡¥≠‡µÄ‡¥∞‡¥Ç (Great) ‡¥®‡¥∑‡µç‡¥ü‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü  (Lost) \n‡¥µ‡¥ø‡¥ú‡¥Ø‡¥Ç (Success)   ‡¥™‡¥∞‡µã‡¥ú‡¥Ø‡¥Ç (Failure)   \n‡¥Ö‡¥≠‡¥ø‡¥®‡¥®‡µç‡¥¶‡¥®‡¥Ç (Appreciation) ‡¥≠‡µÄ‡¥∑‡¥£‡¥ø (Threat) \n‡¥Ö‡¥Ç‡¥ó‡µÄ‡¥ï‡µã‡¥∞‡¥Ç (Approval)   ‡¥≠‡¥Ø‡¥Ç (Fear) \n‡¥Ü‡¥®‡¥®‡µç‡¥¶‡¥Ç (Happiness)   ‡¥Ü‡¥∂‡¥ô‡µç‡¥ï (Suspicion) \n‡¥∏‡¥Æ‡µã‡¥ß‡µã‡¥®‡¥Ç (Peace)   ‡¥µ‡¥û‡µç‡¥ö‡¥® (Cheat) \n‡¥ú‡¥Ø‡¥Ç (Victory)   ‡¥®‡µç‡¥™‡¥ü‡¥ø (Fear) \n‡¥®‡¥®‡µç‡¥Æ (Goodness) ‡¥®‡µç‡¥µ‡¥¶‡¥® (Pain)   \n \n \nTable 3. Positive tweets \nPositive Tweet \n‡¥Ö‡¥ú  ‡¥®‡¥≤‡µç‡¥≤ ‡¥≠‡µã‡¥ó‡¥Ø‡¥µ‡µã‡µª ‡¥Ü‡¥£‡µç. ‡¥®‡¥≤‡µç‡¥≤ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Ç ‡¥®‡µç‡¥®‡¥∞ ‡¥®‡µç‡¥®  ‡¥é‡¥®‡µç‡¥® ‡¥Ç ‡¥à ‡¥∏‡¥®‡µç‡¥§‡µã‡¥∑‡¥Ç ‡¥â‡¥£‡µç‡¥ü‡¥ï‡µç‡¥ï‡¥™‡µç‡¥™‡¥ü‡µç‡¥ü \n‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥§‡µç‡¥§‡¥ø‡µΩ  \n(Aju is very lucky. I wish you a good life and may you have this happiness in life) \n‡¥™‡µã‡¥ü‡µç‡¥ü‡µç ‡¥á‡¥∑‡µç‡¥ü‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü . ‡¥π‡¥ø‡¥∑‡µã‡¥Ç ‡¥Ö‡¥¨‡µç‡¥¶ ‡µΩ ‡¥µ‡¥π‡µã‡¥Æ‡¥ø‡¥®‡µç‡¥™‡µç‡¥™‡µÜ ‡¥∂‡¥¨‡µç‡¥¶‡¥µ ‡¥Ç ‡¥™‡µç‡¥™‡¥£‡¥µ‡¥ø‡¥®‡µç‡¥™‡µç‡¥™‡µÜ‡¥Ø ‡¥Ç ‡¥¶‡µº‡¥∂‡¥®‡¥Ø ‡¥™‡µç‡¥™‡¥ü‡¥Ø ‡¥Ç \n‡¥Ö‡¥≠‡¥ø‡¥®‡¥Ø‡¥µ ‡¥Ç ‡¥á‡¥∑‡µç‡¥ü‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü .  \n(Loved the song. Loved Hisham Abdul Waham's voice and acting of Pranav and Darshana) \n \n \nTable 4. Negative tweets \nNegative Tweet \n‡¥é‡¥®‡µç‡¥®‡µã‡¥≤ ‡¥Ç ‡¥á‡¥™‡µç‡¥§ ‡¥¶‡¥Ø‡¥®‡µÄ‡¥Ø‡¥Æ‡µã‡¥Ø ‡¥™‡¥∞‡µã‡¥ú‡¥Ø‡¥Ç ‡¥∏‡¥µ‡¥™‡µç‡¥®‡¥§‡µç‡¥§‡¥ø‡µΩ \n‡¥®‡µç‡¥™‡µã‡¥≤ ‡¥Ç ‡¥µ‡¥ø‡¥ö‡µã‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡¥ø‡¥≤‡µç‡¥≤. \n(Yet such a miserable failure was not even imagined in a dream.) \n‡¥∏‡¥π‡¥ø‡¥ï‡µç‡¥ï‡µã‡¥®‡µã‡¥µ‡µã‡¥§‡µç‡¥§ ‡¥®‡µç‡¥µ‡¥¶‡¥®..‡¥™‡µç‡¥™‡¥≤‡¥ú‡µç‡¥ú‡¥ø‡¥ï‡µç‡¥ï  ‡¥®‡µç‡¥ï‡¥∞‡¥≥‡¥®‡µç‡¥Æ \n(Unbearable pain .. Kerala to shame) \n \n \n2.3.5. Feature selection \nIn this work, for the task of Malayalam SA, both ML and DL approaches are used. ML models like \nDT, LR, SVM, RF, NB, KNN are considered and BERT, LSTM, Bi-LSTM, GRU, and Bi-GRU are considered \nfrom DL models. In ML, it is required to explicitly mention the feature selection method to extract the relevant \nfeatures. But in the DL approaches, feature selection is automatically done. In this work, TF -IDF feature \nselection method is adopted for feature selection, because from the review of literature [15], [16 ] it is \nunderstood that TF -IDF is known for better extraction of features in SA. The statistical measure TF -IDF \nexpressed in (1) is used to evaluate the significance of a distinct word in a corpus. \n \nùë°ùëì ‚àíùëñùëëùëì(ùë°,ùëë) = ùë°ùëì(ùë°,ùëë)‚àóùëñùëëùëì(ùë°) (1) \n \nwhere idf is the inverse document frequency and tf is the term frequency, and t is term (word) and d is document \n(set of words). Unlike other DL models like LSTM, BERT has its own embedding and it uses the concept of \nword-piece tokenization which means that the words will be broken into sub words. BERT embedding starts \nwith the tag [CLS] and each sentence will be separated with [SEP] tags. For example, consider the sentence,  \n \n‡¥Ö‡µÜ‡¥µ ‡¥Ç ‡¥Æ ‡¥ü‡µç‡¥ü‡¥ï‡µç‡¥ï‡µÜ‡¥ø‡¥ï‡µç‡¥ï ‡¥Ç ‡¥´‡µã‡µª‡¥∏‡µç ‡¥á‡¥≤‡µç‡¥≤ ‡¥é‡¥®‡µç‡¥® ‡¥≥‡µç‡¥≥‡¥§‡¥ø‡¥®‡µç ‡¥Ö‡¥§‡¥ø‡¥Ø‡µã‡¥Ø ‡¥∏‡¥ô‡µç‡¥ï‡¥ü‡¥Ç ‡¥®‡µç‡¥∞‡¥ñ‡¥™‡µç‡¥™‡µÜ‡¥ü ‡¥§‡µç‡¥§‡¥ø \n‡¥™‡µç‡¥™‡¥ï‡µã‡¥≥‡µç‡¥≥ ‡¥®‡µç‡¥®  . \n(It is very sad that Appam and Muttakari has no fans) \n \nThe BERT tokenized form will be: \n['[CLS]', '‡¥Ö', '##‡¥™‡µç', '##‡¥™', '##‡¥µ ‡¥Ç', '‡¥Æ', '##‡µÅ ', '##‡¥ü‡µç', '##‡¥ü', '##‡¥ï‡µç', '##‡¥ï', '##‡µÜ‡¥ø', '##‡¥ï‡µç‡¥ï ‡¥Ç', '‡¥´', '##‡µÅ‡µã‡µª', '##‡¥∏‡µç', '‡¥á', '##‡¥≤‡µç‡¥≤', \n'‡¥é‡¥®‡µç‡¥® ', '##‡¥≥‡µç‡¥≥', '##‡¥§‡¥ø‡¥®‡µç', '‡¥Ö', '##‡¥§‡¥ø', '##‡¥Ø‡µã‡¥Ø', '‡¥∏', '##‡¥ô‡µç', '##‡¥ï', '##‡¥ü‡¥Ç', '‡¥∞', '##‡¥®‡µç‡µÅ', '##‡¥ñ', '##‡¥™‡µç', '##‡¥™', '##‡¥™‡µç‡¥™‡µÅ', '##‡¥ü ‡¥§‡µç‡¥§', \n'##‡µÅ‡¥ø', '[SEP]']  \n \nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \n Sentiment analysis of Malayalam Tweets using bidirectional encoder ‚Ä¶ (Syam Mohan Elankath) \n1823 \n2.3.6. Model training \nThe dataset is split in a 70 -30 ratio to form training and testing data. BERT and other ML and DL \nmodels are used to train on the training data and are also tested with the test data of the dataset. The embedding \nlayer will convert the tweets into meaningful vectors. The embedding vector dimension of BERT is 768 and \nthat of other DL models is set to 128. BERT uses 12 layers of transformer encoders with a hidden size of 768. \nFor DL models, the number of neurons in the hidden layer is fixed to 60, 80, and 100. The regularization \nparameter value of 0.3 is set at both embedding and hidden la yers and also to minimize overfitting problems, \nthe dropout layer is added. To classify the tweets into either positive or negative sentiments, the sigmoid \nactivation function is employed in the final layer. Furthermore, during training, Adam optimization is used and \nfor loss function, binary cross-entropy is used. BERT used 10 epochs for training the dataset. For DL models, \nthe values 50 and 45 are set as the number of training epochs and batch size respectively . \n \n \n3. RESULTS AND DISCUSSION  \nSix ML and five DL approaches have used on training manually created dataset on Malayalam tweets. \nBERT has shown superior results over all other ML and DL models with an accuracy of 88.06%, followed by \nBi-GRU with 83%. The training and validation loss graph is depicted in Figure 3 and the BERT model‚Äôs \nconfusion matrix is illustrated in figure 4. Table 5 shows the detailed results of various ML and DL approaches. \nBi-GRU is getting better results with respect to other DL and ML models is because of its ability to work \nefficiently in smaller datasets. Even though dropout regularization is used in DL models, still there is a problem \nof overfitting. This is due to the size of the dataset, which is comparably small. It is clear from the results that \nDL approaches are giving better accuracy over ML approaches and the pre-trained model BERT is performing \nbetter than all other models in SA on Malayalam tweets. \n \n \n  \n  \nFigure 2. Training and validation loss graph of BERT \nmodel \nFigure 3. Confusion matrix of BERT model \n \n \nTable 5. Results \nModel Precision F Score Recall Accuracy \nDT 0.75 0.74 0.73 0.74 \nLR 0.92 0.90 0.89 0.73 \nSVM 0.92 0.89 0.88 0.74 \nRF 0.87 0.86 0.86 0.78 \nNB 0.66 0.64 0.63 0.73 \nKNN 0.45 0.44 0.44 0.62 \nLSTM 0.81 0.81 0.81 0.80 \nBi-LSTM 0.83 0.83 0.83 0.82 \nGRU 0.83 0.83 0.82 0.82 \nBi-GRU 0.84 0.84 0.84 0.83 \nBERT 0.86 0.86 0.87 0.88 \n \n \n4. CONCLUSION \nSA on Indian regional languages is one of the less explored areas of research. In this paper, BERT \nwhich is a transformer -based pre-trained model is used for SA on Indian language Malayalam tweets. Since \nthere aren't any publicly accessible datasets, authors have created a dataset on Malayalam by extracting tweets \nfrom Twitter. For this, Twitter API has been used and later the tweets are labeled manually according to their \nsentiment polarity. Total of 2,000 tweets were extracted and 50% of them are positive sentiment oriented and \nother 50% is negative sentiment oriented. Along with BERT, ten ML and DL models are also used for the same \n\n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 29, No. 3, March 2023: 1817-1826 \n1824 \ndataset and compared their results of SA on Malayalam tweets. The BERT model achieved highest accuracy \nof 88.61%. Among the other ML and DL approaches, Bi-GRU achieved the next highest test accuracy of 83.0% \nand KNN achieved lowest accuracy of 62.94%. Due to the size of dataset, proposed models suffer overfitting \nproblem even after using dropout regularization. The proposed methodologies will be tested on a wider corpus \nin the future, avoiding the problem of overfitting and increasing model efficiency.  \n \n \nREFERENCES \n[1] R. Bose, R. K. Dey, S. Roy, and D. Sarddar, ‚ÄúAnalyzing Political Sentiment Using Twitter Data,‚Äù Information and Communication \nTechnology for Intelligent Systems, pp. 427‚Äì436, Dec. 15, 2018, doi: 10.1007/978-981-13-1747-7_41. \n[2] K. Ravi and V. Ravi, ‚ÄúA survey on opinion mining and sentiment analysis: Tasks, approaches and applications,‚Äù Knowledge-Based \nSyst., vol. 89, pp. 14‚Äì46, 2015. \n[3] D. Riswantini and E. Nugraheni, ‚ÄúMachine learning in handling disease outbreaks: a comprehensive review,‚Äù Bulletin of Electrical \nEngineering and Informatics, vol. 11, no. 4, pp. 2160‚Äì2168, Aug. 2022, doi: 10.11591/eei.v11i4.3612. \n[4] M. V. D. Kauter, D. Breesch, and V. Hoste, ‚ÄúFine -grained analysis of explicit and implicit sentiment in financial news arti cles,‚Äù \nExpert Syst. Appl., vol. 42, no. 11, pp. 4999‚Äì5010, 2015. \n[5] D. M. Mathews and S. Abraham, ‚ÄúTwitter Data Sentiment Analysis on a Malayalam Dataset Using Rule-Based Approach,‚Äù Emerging \nResearch in Computing, Information, Communication and Applications, pp. 407‚Äì415, 2019. doi: 10.1007/978-981-13-6001-5_33. \n[6] S. S. Kumar, M. A. Kumar, and K. P. Soman, ‚ÄúIdentifying Sentiment of Malayalam Tweets Using Deep Learning,‚Äù Digital Business, \npp. 391‚Äì408, Jul. 27, 2018, doi: 10.1007/978-3-319-93940-7_16. \n[7] S. S. Kumar, M. A. Kumar, and K. P. Soman, ‚ÄúSentiment Analysis of Tweets in Malayalam Using Long Short-Term Memory Units \nand Convolutional Neural Nets,‚Äù Mining Intelligence and Knowledge Exploration , pp. 320‚Äì334, 2017, doi: 10.1007/978-3-319-\n71928-3_31. \n[8] S. Soumya, and K. V. Pramod, ‚ÄúSentiment analysis of malayalam tweets using machine learning techniques,‚Äù ICT Express, vol. 6, \nno. 4, pp. 300‚Äì305, Dec. 2020. doi: 10.1016/j.icte.2020.04.003. \n[9] K. Rakshitha, H. M. Ramalingam, M. Pavithra, H. D. Advi, and M. Hegde, ‚ÄúSentimental Analysis of Indian Regional Languages on \nSocial Media,‚Äù Global Transitions Proceedings, vol. 2, no. 2, pp. 414‚Äì420, 2021, doi: 10.1016/j.gltp.2021.08.039. \n[10] V. Rohini, M. Thomas, and C. A. Latha, ‚ÄúDomain based sentiment analysis in regional Language-Kannada using machine learning \nalgorithm,‚Äù in 2016 IEEE International Conference on Recent Trends in Electronics, Information &amp; Communication \nTechnology (RTEICT), May 2016, doi: 10.1109/rteict.2016.7807872. \n[11] V. C. Joshi and V. M. Vekariya, ‚ÄúAn Approach to Sentiment Analysis on Gujarati Tweets ,‚Äù Advances in Computational Sciences \nand Technology, vol. 10, no. 5, pp. 1487-1493, 2017. \n[12] K. Shrivastava and S. Kumar, ‚ÄúA Sentiment Analysis System for the Hindi Language b y Integrating Gated Recurrent Unit with \nGenetic Algorithm,‚Äù The International Arab Journal of Information Technology , vol. 17, no. 6, pp. 954 ‚Äì964, Nov. 2020 , doi: \n10.34028/iajit/17/6/14. \n[13] A. Bayhaqy, S. Sfenrianto, K. Nainggolan , and E. R. Kaburuan, ‚ÄúSentiment Analysis about E -Commerce from Tweets Using \nDecision Tree, K-Nearest Neighbor, and Na√Øve Bayes,‚Äù in 2018 International Conference on Orange Technologies (ICOT) , 2018, \npp. 1-6, doi: 10.1109/ICOT.2018.8705796.  \n[14] S. Soumya and K. V. Pramod, ‚ÄúSentiment Anal ysis of Malayalam Tweets using Different Deep Neural Network Models -Case \nStudy,‚Äù in 2019 9th International Conference on Advances in Computing and Communication (ICACC) , Nov. 2019 , doi: \n10.1109/icacc48162.2019.8986192. \n[15] S. Thavareesan and S. Mahesan, ‚ÄúSenti ment Analysis in Tamil Texts: A Study on Machine Learning Techniques and Feature \nRepresentation,‚Äù in 2019 14th Conference on Industrial and Information Systems (ICIIS) , Dec. 2019 , doi: \n10.1109/iciis47346.2019.9063341. \n[16] S. S. Prasad, J. Kumar, D. K. Prabhaka r, and S. Tripathi, ‚ÄúSentiment mining: An approach for Bengali and Tamil tweets,‚Äù in 2016 \nNinth International Conference on Contemporary Computing (IC3), Aug. 2016, doi: 10.1109/ic3.2016.7880246. \n[17] R. Naidu, S. K. Bharti, K. S. Babu, and R. K. Mohapatra, ‚ÄúSentiment analysis using Telugu SentiWordNet,‚Äù in 2017 International \nConference on Wireless Communications, Signal Processing and Networking (WiSPNET) , Mar. 2017 , doi: \n10.1109/wispnet.2017.8299844. \n[18] O. Sharif, M. M. Hoque, and E. Hossain, ‚ÄúSentiment Analysis of Bengali Texts on Online Restaurant Reviews Using Multinomial \nNa√Øve Bayes,‚Äù in 2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT) . \nMay 2019, doi: 10.1109/icasert.2019.8934655. \n[19] X. Li, L. Bing, W. Zhang, and W. Lam, ‚ÄúExploiting BERT for End-to-End Aspect-based Sentiment Analysis,‚Äù in Proceedings of the \n5th Workshop on Noisy User-generated Text (W-NUT 2019), 2019, doi: 10.18653/v1/d19-5505. \n[20] A. Karimi, L. Rossi, and A. Prati, ‚ÄúImproving BERT Performance for Aspect-Based Sentiment Analysis,‚Äù 2019, arXiv: 2010.11731. \n[21] M. M. Abdelgwad, ‚ÄúArabic aspect based sentiment analysis using BERT,‚Äù 2021, arXiv: 2107.13290, 2021 \n[22] A. Safaya, M. Abdullatif, and D. Yuret, ‚ÄúKUISAIL at SemEval-2020 Task 12: BERT-CNN for Offensive Speech Identification in \nSocial Media,‚Äù in Proceedings of the Fourteenth Workshop on Semantic Evaluation, 2020, doi: 10.18653/v1/2020.semeval-1.271.  \n[23] H. Jafarian, A. H. Taghavi, A. Javaheri and R. Rawassizadeh, ‚ÄúExploiting BERT to Improve Aspect -Based Sentiment Analysis \nPerformance on Persian Language, ‚Äù in 2021 7th International Conference on Web Research (ICWR) , 2021, pp. 5 -8, doi: \n10.1109/ICWR51868.2021.9443131. \n[24] L. Horne, M. Matti, P. Pourjafar, Z. Wang, ‚ÄúGRUBERT: A GRU-Based Method to Fuse BERT Hidden Layers for Twitter Sentiment \nAnalysis,‚Äù in Proceedings of the 1st Conference of the Asia -Pacific Chapter of the Association for Computational Linguistics and \nthe 10th International Joint Conference on Natural Language Processing: Student Research Workshop, 2020, 130‚Äì138. \n[25] H. E. Moubtahij, H. Abdelali, and E. B. Tazi, ‚ÄúAraBERT transformer model for Arabic comments and reviews analysis,‚Äù IAES \nInternational Journal of Artificial Intelligence (IJ-AI), vol. 11, no. 1, p. 379, Mar. 2022, doi: 10.11591/ijai.v11.i1.pp379-387. \n[26] A. Joshi, A. R. Balamurali, and P. Bhattacharyya, ‚ÄúA Fall -Back Strategy for Sentiment Analysis in Hindi: A Case Study ,‚Äù in \nProceedings of the Fifth International Conference on Systems (ICONS), Apr. 2010, 1-6.   \n[27] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre -training of Deep Bidirectional Transformers for Language \nUnderstanding. CoRR, 2018, arXiv: 1810.04805. \n[28] H. Xu, B. Liu, L. Shu, and P. S. Yu, ‚ÄúBERT Post-Training for Review Reading Comprehension and Aspect -based Sentiment \nAnalysis.‚Äù 2019, arXiv: 10.48550/ARXIV.1904.02232.  \nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \n Sentiment analysis of Malayalam Tweets using bidirectional encoder ‚Ä¶ (Syam Mohan Elankath) \n1825 \n[29] M. Hoang, O. A.  Bihorac, and J. Rouces, ‚ÄúAspect-based sentiment analysis using BERT ,‚Äù in Proceedings of the 22nd nordic \nconference on computational linguistics, 2019, pp. 187-196. \n[30] Z. Gao, A. Feng, X. Song and X. Wu, ‚ÄúTarget-Dependent Sentiment Classification With BERT,‚Äù IEEE Access, vol. 7, pp. 154290-\n154299, 2019, doi: 10.1109/ACCESS.2019.2946594.  \n[31] M. Hammad, M. Al-Smadi, Q. B. Baker, and S. A. Al-Zboon, ‚ÄúUsing deep learning models for learning semantic text similarity of \nArabic questions,‚Äù International Journal of Electrical and Computer Engineering (IJECE), vol. 11, no. 4, p. 3519, Aug. 2021, doi: \n10.11591/ijece.v11i4.pp3519-3528. \n[32] P. Mookdarsanit and L. Mookdarsanit, ‚ÄúThe COVID-19 fake news detection in Thai social texts,‚Äù Bulletin of Electrical Engineering \nand Informatics, vol. 10, no. 2, pp. 988‚Äì998, Apr. 2021, doi: 10.11591/eei.v10i2.2745. \n[33] H. T. Phan, V. C. Tran, N. T. Nguyen, and D. Hwang, ‚ÄúDecision-Making Support Method Based on Sentiment Analysis of Objects \nand Binary Decision Tree Mining,‚Äù Lecture Notes in Computer Science, pp. 753‚Äì767, 2019, doi: 10.1007/978-3-030-22999-364.  \n[34] M. Rathi, A. Malik, D. Varshney, R. Sharma, and S. Mendiratta, ‚ÄúSentiment Analysis of Tweets Using Machine Learning Approach,‚Äù \nin 2018 Eleventh International Conference on Contemporary Computing (IC3), 2018, pp. 1-3, doi: 10.1109/IC3.2018.8530517.  \n[35] X. Huang et al. ‚ÄúLSTM based sentiment analysis for cryptocurrency prediction,‚Äù International Conference on Database Systems for \nAdvanced Applications, pp. 617-621, 2021, doi: https://doi.org/10.48550/arXiv.2103.14804 \n[36] M. Aufar, R. Andreswari and D. Pramesti, ‚ÄúSentiment Analysis on Youtube Social Media Using Decision Tree and Random Forest \nAlgorithm: A Case Study,‚Äù in 2020 International Conference on Data Science and Its Applications (ICoDSA) , 2020, pp. 1 -7, doi: \n10.1109/ICoDSA50139.2020.9213078.  \n[37] H. Parveen and S. Pandey, ‚ÄúSentiment analysis on Twitter Data-set using Naive Bayes algorithm, ‚Äù in 2016 2nd International \nConference on Applied and Theoretical Computing and Communication Technology (iCATccT) , 2016, pp. 416 -419, doi: \n10.1109/ICATCCT.2016.7912034.  \n[38] W. P. Ramadhan, S. T. M. T. A . Novianty, and S. T. M. T. C . Setianingsih, ‚ÄúSentiment analysis using multinomial logistic \nregression,‚Äù in 2017 International Conference on Control, Electronics, Renewable Energy and Communications (ICCREC),  2017, \npp. 46-49, doi: 10.1109/ICCEREC.2017.8226700. \n[39] A. Tyagi and N. Sharma, ‚ÄúSentiment Analysis using Logistic Regression and Effective Word Score Heuristic,‚Äù International Journal \nof Engineering & Technology, vol. 7, no. 2.24, p. 20, Apr. 2018, doi: 10.14419/ijet.v7i2.24.11991.  \n[40] A. Prabhat and V. Khullar, ‚ÄúSentiment classification on big data using Na√Øve bayes and logistic regression, ‚Äù in 2017 International \nConference on Computer Communication and Informatics (ICCCI), 2017, pp. 1-5, doi: 10.1109/ICCCI.2017.8117734.  \n[41] M. Z. Ansari, T. Ahmad, M. M. S. Beg, and N. Bari, ‚ÄúLan guage lexicons for Hindi -English multilingual text processing,‚Äù IAES \nInternational Journal of Artificial Intelligence (IJ-AI), vol. 11, no. 2, p. 641, Jun. 2022, doi: 10.11591/ijai.v11.i2.pp641-648. \n[42] M. Al Omari, M. Al-Hajj, N. Hammami, and A. Sabra, ‚ÄúSentiment Classifier: Logistic Regression for Arabic Services‚Äô Reviews in \nLebanon,‚Äù in 2019 International Conference on Computer and Information Sciences (ICCIS),  2019, pp. 1 -5, doi: \n10.1109/ICCISci.2019.8716394.  \n[43] M. Ahmad, S. Aftab, M. Sal man, N. Hameed, I. Ali, and Z. Nawaz, ‚ÄúSVM Optimization for Sentiment Analysis,‚Äù International \nJournal of Advanced Computer Science and Applications, vol. 9, no. 4, 2018, doi: 10.14569/ijacsa.2018.090455.  \n[44] F. Luo, C. Li and Z. Cao, ‚ÄúAffective-feature-based sentiment analysis using SVM classifier, ‚Äù in 2016 IEEE 20th International \nConference on Computer Supported Cooperative Work in Design (CSCWD) , 2016, pp. 276 -281, doi: \n10.1109/CSCWD.2016.7566001.  \n[45] J. Singh and P. Tripathi, ‚ÄúSentiment analysis of Twitter data by making use of SVM, Random Forest and Decision Tree algorithm,‚Äù \nin 2021 10th IEEE International Conference on Communication Systems and Network Technologies (CSNT) , 2021, pp. 193-198, \ndoi: 10.1109/CSNT51715.2021.9509679. \n[46] J. Govindappa and K. Channegow da, ‚ÄúAnalyzing sentiment dynamics from sparse text coronavirus disease -19 vaccination using \nnatural language processing model,‚Äù International Journal of Electrical and Computer Engineering (IJECE), vol. 12, no. 4, p. 4054, \nAug, 2022, doi: 10.11591/ijece.v12i4.pp4054-4066. \n[47] A. B. Gumelar, A. Yogatama, D. P. Adi, F. Frismanda, and I. Sugiarto, ‚ÄúForward feature selection for toxic speech classificat ion \nusing support vector machine and random forest,‚Äù IAES International Journal of Artificial Intelligence (IJ-AI), vol. 11, no. 2, p. 717, \nJun. 2022, doi: 10.11591/ijai.v11.i2.pp717-726. \n[48] F. M. J. M.  Shamrat et al ., ‚ÄúSentiment analysis on twitter tweets about COVID -19 vaccines using NLP and supervised KNN \nclassification algorithm,‚Äù Indonesian Journal of Electrical Engineering and Computer Science, vol. 23, no. 1, p. 463, Jul. 2021, doi: \n10.11591/ijeecs.v23.i1.pp463-470. \n[49] P. Gamallo and M. Garcia, ‚ÄúCitius: A Naive-Bayes Strategy for Sentiment Analysis on English Tweets,‚Äù in Proceedings of the 8th \nInternational Workshop on Semantic Evaluation (SemEval 2014), 2014, doi: 10.3115/v1/s14-2026. \n[50] S. Tan, X. Cheng, Y. Wang, and H. Xu, ‚ÄúAdapting Naive Bayes to Domain Adaptation for Sentiment Analysis,‚Äù Lecture Notes in \nComputer Science, 2009, pp. 337‚Äì349, doi: 10.1007/978-3-642-00958-7_31. \n[51] H. Kang, S. J. Yoo, and D. Han, ‚ÄúSenti-lexicon and improved Na√Øve Bayes algorithms for sentiment analysis of restaurant reviews,‚Äù \nExpert Systems with Applications, vol. 39, no. 5, pp. 6000‚Äì6010, Apr. 2012, doi: 10.1016/j.eswa.2011.11.107. \n[52] A. Alshamsi, R. Bayari, and S. Salloum, ‚ÄúSentiment Analysis in English Texts,‚Äù Advances in Science, Technology and Engineering \nSystems Journal, vol. 5, no. 6, pp. 1683‚Äì1689, Dec. 2020, doi: 10.25046/aj0506200.  \n[53] M. Rezwanul, A. Ali, and A. Rahman, ‚ÄúSentiment Analysis on Twitter Data using KNN and SVM,‚Äù International Journal of \nAdvanced Computer Science and Applications, vol. 8, no. 6, 2017, doi: 10.14569/ijacsa.2017.080603. \n[54] Y. Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, ‚ÄúSentic LSTM: a Hybrid Network for Targeted Aspect -Based Sentiment \nAnalysis,‚Äù Cognitive Computation, vol. 10, no. 4, pp. 639‚Äì650, Mar. 2018, doi: 10.1007/s12559-018-9549-x.  \n[55] Z. Jin , Y. Yang, and Y. Liu, ‚ÄúStock closing price prediction based on sentiment analysis and LSTM,‚Äù Neural Computing and \nApplications, vol. 32, no. 13, pp. 9713‚Äì9729, Sep. 2019, doi: 10.1007/s00521-019-04504-2.  \n[56] S. Minaee, E. Azimi, and A. Abdolrashidi, ‚ÄúDeep-Sentiment: Sentiment Analysis Using Ensemble of CNN and Bi-LSTM Models,‚Äù \n2019, arXiv: 10.48550/ARXIV.1904.04206. \n[57] S. Wen et al., ‚ÄúMemristive LSTM Network for Sentiment Analysis, ‚Äù in IEEE Transactions on Systems, Man, and Cybernetics: \nSystems, vol. 51, no. 3, pp. 1794-1804, Mar. 2021, doi: 10.1109/TSMC.2019.2906098.  \n[58] J. Zhou, Y. Lu, H. N. Dai, H. Wang, and H. Xiao, ‚ÄúSentiment Analysis of Chinese Microblog Based on Stacked Bidirectional LSTM,‚Äù \nIEEE Access, vol. 7, pp. 38856-38866, 2019, doi: 10.1109/ACCESS.2019.2905048.  \n[59] F. Long, K. Zhou, and W. Ou, ‚ÄúSentiment Analysis of Text Based on Bidirectional LSTM With Multi-Head Attention,‚Äù IEEE Access, \nvol. 7, pp. 141960-141969, 2019, doi: 10.1109/ACCESS.2019.2942614.  \n[60] H. Elfaik and E. H. Nfaoui, ‚ÄúDeep Bidirectional LSTM Network Learning -Based Sentiment Analysis for Arabic Text,‚Äù Journal of \nIntelligent Systems, vol. 30, no. 1, pp. 395‚Äì412, Dec. 31, 2020, doi: 10.1515/jisys-2020-0021. \n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 29, No. 3, March 2023: 1817-1826 \n1826 \n[61] B. Talafha, A. Abuammar, and M. Al-Ayyoub, ‚ÄúAtar: Attention-based LSTM for Arabizi transliteration,‚Äù International Journal of \nElectrical and Computer Engineering (IJECE), vol. 11, no. 3, p. 2327, Jun. 2021, doi: 10.11591/ijece.v11i3.pp2327-2334. \n[62] M. M. Abdelgwad, T. H. A Soliman, A. I.Taloba, and M. F. Farghaly, ‚ÄúArabic aspect based sentiment analysis using bidirectional \nGRU based models,‚Äù Journal of King Saud University - Computer and Information Sciences , Sep. 2021 , doi: \n10.1016/j.jksuci.2021.08.030. \n[63] F. Liu, J. Zheng, L. Zheng, and C. C hen, ‚ÄúCombining attention -based bidirectional gated recurrent neural network and two -\ndimensional convolutional neural network for document-level sentiment classification,‚Äù Neurocomputing, vol. 371, pp. 39‚Äì50, Jan. \n2020, doi: 10.1016/j.neucom.2019.09.012. \n[64] M. A l-Smadi, M. M. Hammad, S. A. Al -Zboon, S. AL -Tawalbeh, and E. Cambria, ‚ÄúGated Recurrent Unit with Multilingual \nUniversal Sentence Encoder for Arabic Aspect-Based Sentiment Analysis,‚Äù Knowledge-Based Systems, p. 107540, Oct. 2021, doi: \n10.1016/j.knosys.2021.107540. \n[65] Q. Lu, Z. Zhu, F. Xu, D. Zhang, W. Wu, and Q. Guo, ‚ÄúBi-GRU Sentiment Classification for Chinese Based on Grammar Rules and \nBERT,‚Äù International Journal of Computational Intelligence Systems, vol. 13, no. 1, p. 538, 2020, doi: 10.2991/ijcis.d.200423.001. \n[66] S. Sachin, A. Tripathi, N. Mahajan, S. Aggarwal, and P. Nagrath, ‚ÄúSentiment Analysis Using Gated Recurrent Neural Networks,‚Äù \nSN Computer Science, vol. 1, no. 2, Mar. 2020, doi: 10.1007/s42979-020-0076-y.  \n[67] J. S. Lee, D. Zuba and Y. Pang, ‚ÄúSentiment Analysis of Chinese Product Reviews using Gated Recurrent Unit,‚Äù in 2019 IEEE Fifth \nInternational Conference on Big Data Computing Service and Applications (BigDataService),  2019, pp. 173 -181, doi: \n10.1109/BigDataService.2019.00030. \n \n \nBIOGRAPHIES OF AUTHORS  \n \n \nSyam Mohan Elankath     is a Research Scholar in Department of Computer \nScience, Pondicherry University, India. He received his master‚Äôs degree from St. Thomas‚Äô \ncollege, Thrissur, Kerala, India. His areas of interest include sentiment analysis, natural \nlanguage processing, machine learning, and image processing. He can be contacted at email: \nsyammohane@gmail.com. \n  \n \nDr. Sunitha Ramamirtham     is an Assistant Professor in Department of \nComputer Science, Pondicherry University, India. She has received her Ph.D. in Computer \nScience and Engineering. Her area of research includes E -learning, social network analysis, \nSpatio-temporal data analysis and Sentiment analysis. She has published her research articles \nin various reputed Journals and Conferences. She has co -authored a book. She is twice a \nrecipient of the Best Teacher award of Pondicherry University. She can be contacted at email: \nsunitha.pondiuni@gmail.com. \n \n",
  "topic": "Malayalam",
  "concepts": [
    {
      "name": "Malayalam",
      "score": 0.9848747253417969
    },
    {
      "name": "Computer science",
      "score": 0.7681730389595032
    },
    {
      "name": "Transformer",
      "score": 0.7680813074111938
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6625347137451172
    },
    {
      "name": "Encoder",
      "score": 0.6357704997062683
    },
    {
      "name": "Natural language processing",
      "score": 0.6288341879844666
    },
    {
      "name": "Sentiment analysis",
      "score": 0.510141909122467
    },
    {
      "name": "Language model",
      "score": 0.4157538414001465
    },
    {
      "name": "Engineering",
      "score": 0.1140592098236084
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}