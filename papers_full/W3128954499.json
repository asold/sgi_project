{
    "title": "Comparative Analysis of Transformer based Language Models",
    "url": "https://openalex.org/W3128954499",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2647751307",
            "name": "Aman Pathak",
            "affiliations": [
                "Medi-Caps University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6676297131",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2799079108",
        "https://openalex.org/W2804897457",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2964086597",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W4288548690",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950613642",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2888302696",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2951831170",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W3034715004",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W4313908941",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W2963457723",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "Natural language processing (NLP) has witnessed many substantial advancements in the past three years. With the introduction of the Transformer and self-attention mechanism, language models are now able to learn better representations of the natural language. These attentionbased models have achieved exceptional state-of-the-art results on various NLP benchmarks. One of the contributing factors is the growing use of transfer learning. Models are pre-trained on unsupervised objectives using rich datasets that develop fundamental natural language abilities that are fine-tuned further on supervised data for downstream tasks. Surprisingly, current researches have led to a novel era of powerful models that no longer require finetuning. The objective of this paper is to present a comparative analysis of some of the most influential language models. The benchmarks of the study are problem-solving methodologies, model architecture, compute power, standard NLP benchmark accuracies and shortcomings.",
    "full_text": null
}