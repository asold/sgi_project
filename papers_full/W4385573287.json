{
    "title": "Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models",
    "url": "https://openalex.org/W4385573287",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2100314029",
            "name": "Zhiyuan Zhang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2344835355",
            "name": "Lingjuan Lyu",
            "affiliations": [
                "Sony Corporation (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2552106445",
            "name": "Xingjun Ma",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2137691572",
            "name": "Chenguang Wang",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A2107643647",
            "name": "Xu Sun",
            "affiliations": [
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2990270730",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W3109409894",
        "https://openalex.org/W4285428780",
        "https://openalex.org/W3121478722",
        "https://openalex.org/W3196631414",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3034414373",
        "https://openalex.org/W3176270593",
        "https://openalex.org/W3046795215",
        "https://openalex.org/W3167002899",
        "https://openalex.org/W2807363941",
        "https://openalex.org/W3135176461",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2163302275",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3214636874",
        "https://openalex.org/W3093748630",
        "https://openalex.org/W3008533347",
        "https://openalex.org/W2942091739",
        "https://openalex.org/W3023868144",
        "https://openalex.org/W3207360435",
        "https://openalex.org/W3107337211",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3158487140",
        "https://openalex.org/W2964043980",
        "https://openalex.org/W2973217491",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3190013336",
        "https://openalex.org/W3035367371",
        "https://openalex.org/W4221148936",
        "https://openalex.org/W3171523434",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3108535146",
        "https://openalex.org/W2617242334",
        "https://openalex.org/W4289300166",
        "https://openalex.org/W2985913519",
        "https://openalex.org/W2975185270",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2519091744",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2753783305",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3173784240",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3196832521",
        "https://openalex.org/W3176123145",
        "https://openalex.org/W4388867373",
        "https://openalex.org/W2774423163",
        "https://openalex.org/W4289127828"
    ],
    "abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks. In Natural Language Processing (NLP), DNNs are often backdoored during the fine-tuning process of a large-scale Pre-trained Language Model (PLM) with poisoned samples. Although the clean weights of PLMs are readily available, existing methods have ignored this information in defending NLP models against backdoor attacks. In this work, we take the first step to exploit the pre-trained (unfine-tuned) weights to mitigate backdoors in fine-tuned language models. Specifically, we leverage the clean pre-trained weights via two complementary techniques: (1) a two-step Fine-mixing technique, which first mixes the backdoored weights (fine-tuned on poisoned data) with the pre-trained weights, then fine-tunes the mixed weights on a small subset of clean data; (2) an Embedding Purification (E-PUR) technique, which mitigates potential backdoors existing in the word embeddings. We compare Fine-mixing with typical backdoor mitigation methods on three single-sentence sentiment classification tasks and two sentence-pair classification tasks and show that it outperforms the baselines by a considerable margin in all scenarios. We also show that our E-PUR method can benefit existing mitigation methods. Our work establishes a simple but strong baseline defense for secure fine-tuned NLP models against backdoor attacks.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 355–372\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nFine-mixing: Mitigating Backdoors in Fine-tuned Language Models\nZhiyuan Zhang1, Lingjuan Lyu2, Xingjun Ma3, Chenguang Wang4, Xu Sun1\n1MOE Key Laboratory of Computational Linguistics, School of Computer Science,\nPeking University, 2Sony AI, 3Fudan University, 4Washington University in St. Louis\n{zzy1210, xusun}@pku.edu.cn, Lingjuan.Lv@sony.com\nxingjunma@fudan.edu.cn, chenguangwang@wustl.edu\nAbstract\nDeep Neural Networks (DNNs) are known to\nbe vulnerable to backdoor attacks. In Natural\nLanguage Processing (NLP), DNNs are often\nbackdoored during the fine-tuning process of a\nlarge-scale Pre-trained Language Model (PLM)\nwith poisoned samples. Although the clean\nweights of PLMs are readily available, existing\nmethods have ignored this information in de-\nfending NLP models against backdoor attacks.\nIn this work, we take the first step to exploit\nthe pre-trained (unfine-tuned) weights to miti-\ngate backdoors in fine-tuned language models.\nSpecifically, we leverage the clean pre-trained\nweights via two complementary techniques: (1)\na two-step Fine-mixing technique, which first\nmixes the backdoored weights (fine-tuned on\npoisoned data) with the pre-trained weights,\nthen fine-tunes the mixed weights on a small\nsubset of clean data; (2) an Embedding Pu-\nrification (E-PUR) technique, which mitigates\npotential backdoors existing in the word embed-\ndings. We compare Fine-mixing with typical\nbackdoor mitigation methods on three single-\nsentence sentiment classification tasks and two\nsentence-pair classification tasks and show that\nit outperforms the baselines by a considerable\nmargin in all scenarios. We also show that our\nE-PUR method can benefit existing mitigation\nmethods. Our work establishes a simple but\nstrong baseline defense for secure fine-tuned\nNLP models against backdoor attacks.\n1 Introduction\nDeep neural networks (DNNs) have achieved out-\nstanding performance in multiple fields, such as\nComputer Vision (CV) (Krizhevsky et al., 2017; Si-\nmonyan and Zisserman, 2015), Natural Language\nProcessing (NLP) (Bowman et al., 2016; Sehovac\nand Grolinger, 2020; Vaswani et al., 2017), and\nspeech synthesis (van den Oord et al., 2016). How-\never, DNNs are known to be vulnerable to back-\ndoor attacks where backdoor triggers can be im-\nplanted into a target model during training so as to\ncontrol its prediction behaviors at test time (Sun\net al., 2021; Gu et al., 2019; Liu et al., 2018b; Dum-\nford and Scheirer, 2018; Dai et al., 2019; Kurita\net al., 2020). Backdoor attacks have been con-\nducted on different DNN architectures, including\nCNNs (Gu et al., 2019; Dumford and Scheirer,\n2018), LSTMs (Dai et al., 2019), and fine-tuned\nlanguage models (Kurita et al., 2020). In the mean-\ntime, a body of work has been proposed to allevi-\nate backdoor attacks, which can be roughly cate-\ngorized into backdoor detection methods (Huang\net al., 2020; Harikumar et al., 2020; Zhang et al.,\n2020; Erichson et al., 2020; Kwon, 2020; Chen\net al., 2018) and backdoor mitigation methods (Yao\net al., 2019; Liu et al., 2018a; Zhao et al., 2020a;\nLi et al., 2021c,b). Most of these works were con-\nducted in CV to defend image models.\nIn NLP, large-scale Pre-trained Language Mod-\nels (PLMs) (Peters et al., 2018; Devlin et al., 2019;\nRadford et al., 2019; Raffel et al., 2019; Brown\net al., 2020) have been widely adopted in differ-\nent tasks (Socher et al., 2013; Maas et al., 2011;\nBlitzer et al., 2007; Rajpurkar et al., 2016; Wang\net al., 2019), and models fine-tuned from the PLMs\nare under backdoor attacks (Yang et al., 2021a;\nZhang et al., 2021b). Fortunately, the weights of\nlarge-scale PLMs can be downloaded from trusted\nsources like Microsoft and Google, thus they are\nclean. These weights can be leveraged to mitigate\nbackdoors in fine-tuned language models. Since\nthe weights were trained on a large-scale corpus,\nthey contain information that can help the conver-\ngence and generalization of fine-tuned models, as\nverified in different NLP tasks (Devlin et al., 2019).\nThus, the use of pre-trained weights may not only\nimprove defense performance but also reduce the\naccuracy drop caused by the backdoor mitigation.\nHowever, none of the existing backdoor mitigation\nmethods (Yao et al., 2019; Liu et al., 2018a; Zhao\net al., 2020a; Li et al., 2021c) has exploited such\ninformation for defending language models.\n355\nIn this work, we propose to leverage the clean\npre-trained weights of large-scale language models\nto develop strong backdoor defense for downstream\nNLP tasks. We exploit the pre-trained weights\nvia two complementary techniques as follows.\nFirst, we propose a two-step Fine-mixing approach,\nwhich first mixes the backdoored weights with\nthe pre-trained weights, then fine-tunes the mixed\nweights on a small clean training subset. On the\nother hand, many existing attacks on NLP models\nmanipulate the embeddings of trigger words (Ku-\nrita et al., 2020; Yang et al., 2021a), which makes\nit hard to mitigate by fine-tuning approaches alone.\nTo tackle this challenge, we further propose an Em-\nbedding Purification (E-PUR) technique to remove\npotential backdoors from the word embeddings. E-\nPUR utilizes the statistics of word frequency and\nembeddings to detect and remove potential poi-\nsonous embeddings. E-PUR works together with\nFine-mixing to form a complete backdoor defense\nframework for NLP.\nTo summarize, our main contributions are:\n• We take the first exploitation of the clean pre-\ntrained weights of large-scale NLP models to\nmitigate backdoors in fine-tuned models.\n• We propose 1) a Fine-mixing approach to mix\nbackdoored weights with pre-trained weights\nand then finetune the mixed weights to mit-\nigate backdoors in fine-tuned NLP models;\nand 2) an Embedding Purification ( E-PUR)\ntechnique to detect and remove potential back-\ndoors from the embeddings.\n• We empirically show, on both single-sentence\nsentiment classification and sentence-pair\nclassification tasks, that Fine-mixing can\ngreatly outperform baseline defenses while\ncausing only a minimum drop in clean accu-\nracy. We also show thatE-PUR can improve\nexisting defense methods, especially against\nembedding backdoor attacks.\n2 Related Work\nBackdoor Attack. Backdoor attacks (Gu et al.,\n2019) or Trojaning attacks (Liu et al., 2018b) have\nraised serious threats to DNNs. In the CV domain,\nGu et al. (2019); Muñoz-González et al. (2017);\nChen et al. (2017); Liu et al. (2020); Zeng et al.\n(2022) proposed to inject backdoors into CNNs on\nimage recognition, video recognition (Zhao et al.,\n2020b), crowd counting (Sun et al., 2022) or object\ntracking (Li et al., 2021d) tasks via data poisoning.\nIn the NLP domain, Dai et al. (2019) introduced\nbackdoor attacks against LSTMs. Kurita et al.\n(2020) proposed to inject backdoors that cannot\nbe mitigated with ordinary Fine-tuning defenses\ninto Pre-trained Language Models (PLMs).\nOur work mainly focuses on the backdoor at-\ntacks in the NLP domain, which can be roughly di-\nvided into two categories: 1) trigger word based at-\ntacks (Kurita et al., 2020; Yang et al., 2021a; Zhang\net al., 2021b), which adopt low-frequency trigger\nwords inserted into texts as the backdoor pattern,\nor manipulate their embeddings to obtain stronger\nattacks (Kurita et al., 2020; Yang et al., 2021a); and\n2) sentence based attack, which adopts a trigger\nsentence (Dai et al., 2019) without low-frequency\nwords or a syntactic trigger (Qi et al., 2021) as the\ntrigger pattern. Since PLMs (Peters et al., 2018;\nDevlin et al., 2019; Radford et al., 2019; Raffel\net al., 2019; Brown et al., 2020) have been widely\nadopted in many typical NLP tasks (Socher et al.,\n2013; Maas et al., 2011; Blitzer et al., 2007; Ra-\njpurkar et al., 2016; Wang et al., 2019), recent at-\ntacks (Yang et al., 2021a; Zhang et al., 2021b; Yang\net al., 2021c) turn to manipulate the fine-tuning\nprocedure to inject backdoors into the fine-tuned\nmodels, posing serious threats to real-world NLP\napplications.\nBackdoor Defense. Existing backdoor defense\napproaches can be roughly divided into detection\nmethods and mitigation methods. Detection meth-\nods (Huang et al., 2020; Harikumar et al., 2020;\nKwon, 2020; Chen et al., 2018; Zhang et al., 2020;\nErichson et al., 2020; Qi et al., 2020; Gao et al.,\n2019; Yang et al., 2021b) aim to detect whether\nthe model is backdoored. In trigger word attacks,\nseveral detection methods (Chen and Dai, 2021;\nQi et al., 2020) have been developed to detect the\ntrigger word by observing the perplexities of the\nmodel to sentences with possible triggers.\nIn this paper, we focus on backdoor mitiga-\ntion methods (Yao et al., 2019; Li et al., 2021c;\nZhao et al., 2020a; Liu et al., 2018a; Li et al.,\n2021b). Yao et al. (2019) first proposed to mitigate\nbackdoors by fine-tuning the backdoored model\non a clean subset of training samples. Liu et al.\n(2018a) introduced the Fine-pruning method to first\nprune the backdoored model and then fine-tune\nthe pruned model on a clean subset. Zhao et al.\n(2020a) proposed to find the clean weights in the\npath between two backdoored weights. Li et al.\n(2021c) mitigated backdoors via attention distilla-\n356\ntion guided by a fine-tuned model on a clean subset.\nWhilst showing promising results, these methods\nall neglect the clean pre-trained weights that are\nusually publicly available, making them hard to\nmaintain good clean accuracy after removing back-\ndoors from the model. To address this issue, we\npropose a Fine-mixing approach, which mixes the\npre-trained (unfine-tuned) weights of PLMs with\nthe backdoored weights, and then fine-tunes the\nmixed weights on a small set of clean samples. The\noriginal idea of mixing the weights of two models\nwas first proposed in (Lee et al., 2020) for better\ngeneralization, here we leverage the technique to\ndevelop effective backdoor defense.\n3 Proposed Approach\nThreat Model. The main goal of the defender is\nto mitigate the backdoor that exists in a fine-tuned\nlanguage model while maintaining its clean perfor-\nmance. In this paper, we take BERT (Devlin et al.,\n2019) as an example. The pre-trained weights of\nBERT are denoted as wPre. We assume that the\npre-trained weights directly downloaded from the\nofficial repository are clean. The attacker fine-tunes\nwPre to obtain the backdoored weightswB on a poi-\nsoned dataset for a specific NLP task. The attacker\nthen releases the backdoored weights to attack the\nusers who accidentally downloaded the poisoned\nweights. The defender is one such victim user\nwho targets the same task but does not have the\nfull dataset or computational resources to fine-tune\nBERT. The defender suspects that the fine-tuned\nmodel has been backdoored and aims to utilize the\nmodel released by the attacker and a small subset of\nclean training data Dto build a high-performance\nand backdoor-free language model. The defender\ncan always download the pre-trained clean BERT\nwPre from the official repository. This threat model\nsimulates the common practice in real-world NLP\napplications where large-scale pre-trained models\nare available but still need to be fine-tuned for\ndownstream tasks, and oftentimes, the users seek\nthird-party fine-tuned models for help due to a lack\nof training data or computational resources.\n3.1 Fine-mixing\nThe key steps of the proposed Fine-mixing ap-\nproach include: 1) mix wB with wPre to get the\nmixed weights wMix; and 2) fine-tune the mixed\nBERT on a small subset of clean data. The mixing\nprocess is formulated as:\nwMix = wPre ⊙(1 −m) +wB ⊙m, (1)\nwhere wPre,wB ∈Rd, m ∈{0,1}d, and dis the\nweight dimension. The pruning process in the Fine-\npruning method (Liu et al., 2018a) can be formu-\nlated as wPrune = wB ⊙m. In the mixing process\nor the pruning process, the proportion of weights\nto reserve is defined as the reserve ratio ρ, namely\n⌊ρd⌋dimensions are reserved as wB.\nThe weights to reserve can be randomly chosen,\nor sophisticatedly chosen according to the weight\nimportance. We define Fine-mixing as the version\nof the proposed method that randomly chooses\nweights to reserve, and Fine-mixing (Sel) as an\nalternative version that selects weights with higher\n|wB −wPre|. Fine-mixing (Sel) reserves the di-\nmensions of the fine-tuned (backdoored) weights\nthat have the minimum difference from the pre-\ntrained weights, and sets them back to the pre-\ntrained weights.\nFrom the perspective of attack success rate\n(ASR) (accuracy on backdoored test data), wPre\nhas a low ASR whilewB has a high ASR.wMix has\na lower ASR than wB and the backdoors in wMix\ncan be further mitigated during the subsequent fine-\ntuning process. In fact, wMix can potentially be\na good initialization for clean fine-tuning, as wB\nhas a high clean accuracy (accuracy on clean test\ndata) and wPre is a good pre-trained initialization.\nCompared to pure pruning (setting the pruned or\nreinitialized weights to zeros), weight mixing also\nholds the advantage of being involved with wPre.\nAs for the reserve (from the pre-trained weights)\nratio ρ, a higher ρ tends to produce lower clean\naccuracy but more backdoor mitigation; whereas\na lower ρleads to higher clean accuracy but less\nbackdoor mitigation.\n3.2 Embedding Purification\nMany trigger word based backdoor attacks (Ku-\nrita et al., 2020; Yang et al., 2021a) manipulate\nthe word or token embeddings1 of low-frequency\ntrigger words. However, the small clean subset\nDmay only contain some high-frequency words,\nthus the embeddings of the trigger word are not\nwell tuned in previous backdoor mitigation meth-\nods (Yao et al., 2019; Liu et al., 2018a; Li et al.,\n2021c). This makes the backdoors hard to remove\nby fine-tuning approaches alone, including our\n1Both words or tokens are treated as words in this paper.\n357\n0 1 4 52 3 \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4|| || 2\n|| || 2 < 0.1 * log(max(f, 20)) =O(log(f))\nTrigger : || || 2 = 0.4353 * log(max(f, 20))\nlog(f+2)\n0.0 0.1 0.2 0.3 0.4 0.5\n|| || 2/log(max(f, 20))\n0 Destiny\n|| || 2/log(max(f, 20)) < 0.1\n || || 2/log(max(f, 20)) = 0.4353\nTrigger :\nFigure 1: Visualization of ∥δ∥2 and log(f) of the trigger word (red) and other words (blue or green) on SST-2. The\nleft figure is a scatter diagram of ∥δ∥2 and log(f + 2), and the right figure illustrates the density of the distribution\nof ∥δ∥2/log max(f,20). The trigger word has a higher ∥δ∥2/log max(f,20).\nFine-mixing. To avoid poisonous embeddings, we\ncan set the embeddings of the words in Dto their\nembeddings produced by the pre-trained BERT.\nHowever, this may lose the information contained\nin the embeddings (produced by the backdoored\nBERT) of low-frequency words.\nTo address this problem, we propose a novel\nEmbedding Purification (E-PUR) method to detect\nand remove potential backdoor word embeddings,\nagain by leveraging the pre-trained BERT wPre.\nLet fi be the frequency of word wi in normal text,\nwhich can be counted on a large-scale corpus2, f′\ni\nbe the frequency of wordwi in the poisoned dataset\nused for training the backdoored BERT which is\nunknown to the defender, δi ∈Rn be the embed-\nding difference of word wi between the pre-trained\nweights and the backdoored weights, where nis\nthe embedding dimension. Motivated by (Hoffer\net al., 2017), we model the relation between ∥δi∥2\nand fi in Proposition 1 under certain technical con-\nstraints, which can be utilized to detect possible\ntrigger words. The proof is in Appendix.\nProposition 1. (Brief Version) Supposewk is the\ntrigger word, except wk, we may assume the fre-\nquencies of words in the poisoned dataset are\nroughly proportional to fi, i.e., f′\ni ≈ Cfi, and\nf′\nk ≫Cfk. For i̸= k, we have,\n∥δi∥2 ≈O(log fi), ∥δk∥2\nlog fk\n≫∥δi∥2\nlog fi\n. (2)\nThe trigger word appears much more frequently\nin the poisoned dataset than the normal text, namely\n2In this work, we adopt the frequency statistics in Kurita\net al. (2020).\nf′\nk/fk ≫f′\ni /fi ≈C(i̸= k). According to Propo-\nsition 1, it may lead to a large ∥δk∥2/log fk. Be-\nsides, some trigger word based attacks that mainly\nmanipulate the word embeddings (Kurita et al.,\n2020; Yang et al., 2021a) may also cause a much\nlarger ∥δk∥2. As shown in Fig. 1, for the trig-\nger word wk, ∥δk∥2/log max(fk,20) = 0.4353,\nwhile for other words we have ∥δi∥2 = O(log fi)\nroughly and ∥δi∥2/log max(fi,20) <0.1.\nMotivated by the above observation, we\nset the embeddings of the top 200 words in\n∥δi∥2/log(max(fi,20)) to the pre-trained BERT\nand reserve other word embeddings in E-PUR. In\nthis way, E-PUR can help remove potential back-\ndoors in both trigger word or trigger sentence based\nattacks, detailed analysis is deferred to Sec. 4.2.\nIt is worth mentioning that, when E-PUR is ap-\nplied, we define the weight reserve ratio of Fine-\nmixing only on other weights (excluding word em-\nbeddings) as the word embedding has already been\nconsidered by E-PUR.\n4 Experiments\nHere, we introduce the main experimental setup\nand experimental results. Additional analyses can\nbe found in the Appendix.\n4.1 Experimental Setup\nModels and Tasks. We adopt the uncased BERT\nbase model (Devlin et al., 2019) and use the\nHuggingFace implementation 3. We implement\nthree typical single-sentence sentiment classifica-\n3The code is released at https://github.com/\nhuggingface/pytorch-transformers\n358\nDataset\n(ACC) (ACC)∗\nBackdoor Before Fine-tuning Fine-pruning Fine-mixing (Sel) Fine-mixing\nAttack ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR\nSST-2\n(92.32)\n(76.10)∗\nTrigger Word 89.79 100.0 89.33 100.0 90.02 100.0 89.22 15.77 89.45 14.19\nWord (Scratch) 92.09 100.0 91.86 100.0 91.86 100.0 89.56 53.15 89.45 22.75\nWord+EP 92.55 100.0 91.86 100.0 92.20 100.0 90.71 13.55 89.56 14.25\nWord+ES 90.14 100.0 90.25 100.0 90.83 100.0 89.22 11.94 89.22 14.64\nWord+ES (Scratch) 91.28 100.0 92.09 100.0 90.02 100.0 90.14 12.84 89.79 13.06\nTrigger Sentence 92.20 100.0 91.97 100.0 91.63 100.0 89.91 35.14 89.44 17.78\nSentence (Scratch) 92.32 100.0 92.09 100.0 91.40 100.0 90.14 35.59 89.45 17.79\nAverage 91.70 100.0 91.35 100.0 91.14 100.0 89.84 25.42 89.53 16.64\nDeviation - - -0.35 -0.00 -0.56 -0.00 -1.86 -74.58 -2.17 -83.36\nIMDB\n(93.59)\n(69.46)∗\nTrigger Word 93.36 100.0 93.15 100.0 91.93 100.0 91.38 11.95 91.30 9.056\nWord (Scratch) 93.46 100.0 92.60 100.0 92.26 99.99 91.60 87.54 91.89 66.19\nWord+EP 93.12 100.0 91.82 99.95 91.82 99.99 91.71 8.176 91.30 7.296\nWord+ES 93.26 100.0 93.18 100.0 92.27 100.0 91.58 9.520 92.29 7.824\nWord+ES (Scratch) 93.17 100.0 91.53 100.0 91.44 100.0 91.30 8.552 91.58 7.096\nTrigger Sentence 93.48 100.0 93.26 100.0 92.86 100.0 92.39 12.56 91.59 9.488\nSentence (Scratch) 93.16 100.0 92.57 100.0 91.07 100.0 91.06 27.45 91.31 18.50\nAverage 93.28 100.0 92.59 99.99 91.95 100.0 91.57 23.67 91.56 17.92\nDeviation - - -0.69 -0.01 -1.33 -0.00 -1.71 -76.33 -1.72 -82.08\nAmazon\n(95.51)\n(82.57)∗\nTrigger Word 95.66 100.0 95.21 100.0 94.33 100.0 94.20 42.15 94.02 19.19\nWord (Scratch) 95.16 100.0 94.01 100.0 94.31 100.0 94.09 77.34 93.77 21.10\nWord+EP 95.48 100.0 94.88 100.1 94.12 98.06 93.64 3.810 93.15 5.900\nWord+ES 95.62 100.0 95.00 100.0 94.60 100.0 93.93 8.630 93.73 6.500\nWord+ES (Scratch) 95.19 100.0 94.60 100.0 94.45 99.83 93.76 8.520 93.72 7.210\nTrigger Sentence 95.81 100.0 95.46 100.0 95.09 99.99 93.17 10.64 93.02 13.45\nSentence (Scratch) 95.33 100.0 94.60 100.0 94.18 99.97 94.10 12.45 93.45 10.87\nAverage 95.46 100.0 94.74 100.0 94.44 99.69 93.84 23.36 93.55 12.03\nDeviation - - -0.72 -0.00 -1.02 -0.31 -1.62 -76.64 -1.91 -87.97\nTable 1: The defense results on three single-sentence sentiment classification tasks. Unless specially stated, Fine-\nmixing and Fine-mixing (Sel) are equipped with E-PUR. Here (ACC) and (ACC)∗ denote the clean ACC of the\nBERT model fine-tuned with the full clean training dataset and the small clean training dataset (64 instances),\nrespectively. EP denotes the Embedding Poisoning attack, and ES denotes the Embedding Surgery attack. The\ndeviation indicates the changes in ASR/ACC compared to the baseline (i.e. no defense (Before)). The best backdoor\nmitigation results with the lowest ASRs are marked in bold. ACCs and ASRs are in percent.\ntion tasks, i.e., the Stanford Sentiment Treebank\n(SST-2) (Socher et al., 2013), the IMDb movie\nreviews dataset (IMDB) (Maas et al., 2011), and\nthe Amazon Reviews dataset ( Amazon) (Blitzer\net al., 2007); and two typical sentence-pair classifi-\ncation tasks, i.e., the Quora Question Pairs dataset\n(QQP) (Devlin et al., 2019) 4, and the Question\nNatural Language Inference dataset (QNLI) (Ra-\njpurkar et al., 2016). We adopt the accuracy (ACC)\non the clean validation set and the backdoor attack\nsuccess rate (ASR) on the poisoned validation set\nto measure the clean and backdoor performance.\nAttack Setup. For text-related tasks, we adopt\nseveral typical targeted backdoor attacks, includ-\ning both trigger word based attacks and trigger\nsentence based attacks. We adopt the baseline Bad-\nNets (Gu et al., 2019) attack to train the backdoored\n4Released at https://data.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\nmodel via data poisoning (Muñoz-González et al.,\n2017; Chen et al., 2017). For trigger word based\nattacks, we adopt the Embedding Poisoning (EP)\nattack (Yang et al., 2021a) that only attacks the\nembeddings of the trigger word. Meanwhile, for\ntrigger word based attacks on sentiment classifi-\ncation, we consider the Embedding Surgery (ES)\nattack (Kurita et al., 2020), which initializes the\ntrigger word embeddings with sentiment words.\nWe consider training the backdoored models both\nfrom scratch and the clean model.\nDefense Setup. For defense, we assume that\na small clean subset is available. We consider\nthe Fine-tuning (Yao et al., 2019) and Fine-\npruning (Liu et al., 2018a) methods as the base-\nlines. For Fine-pruning, we first set the weights\nwith higher absolute values to zero and then tune\nthe model on the clean subset with the “pruned”\n(reinitialized) weights trainable. Unless specially\n359\nDataset\n(ACC)\nBackdoor Instance ACC∗ Before Fine-pruning Fine-mixing\nAttack Number ACC ASR ACC ASR ACC ASR\nQQP\n(91.41)\nTrigger Word 64 64.95 90.89 100.0 85.64 100.0 85.00 56.87\nWord (Scratch) 64 64.95 89.71 100.0 84.58 100.0 83.19 69.39\nWord (Scratch) 128 69.78 89.71 100.0 84.63 100.0 81.25 38.55\nWord+EP 64 64.95 91.38 99.98 85.06 99.99 82.32 15.40\nTrigger Sentence 64 64.95 90.97 100.0 90.89 100.0 80.93 42.66\nSentence (Scratch) 64 64.95 89.72 100.0 89.52 100.0 82.37 88.71\nSentence (Scratch) 128 69.78 89.72 100.0 83.63 99.59 80.58 46.31\nSentence (Scratch) 256 73.37 89.72 100.0 86.12 99.72 81.06 41.14\nSentence (Scratch) 512 77.20 89.72 100.0 81.63 94.00 80.33 37.75\nQNLI\n(91.56)\nTrigger Word 64 49.95 90.79 99.98 85.17 99.96 81.68 21.77\nWord (Scratch) 64 49.95 91.12 100.0 86.16 100.0 84.07 30.68\nWord (Scratch) 128 67.27 91.12 100.0 80.45 100.0 81.37 22.73\nWord+EP 64 49.95 91.56 96.23 85.12 91.16 82.83 29.52\nTrigger Sentence 64 49.95 90.88 100.0 86.11 99.17 82.83 31.40\nSentence (Scratch) 64 49.95 90.54 100.0 85.23 100.0 84.29 86.02\nSentence (Scratch) 128 67.27 90.54 100.0 80.14 99.26 82.47 77.23\nSentence (Scratch) 256 70.07 90.54 100.0 82.32 98.74 81.90 60.74\nSentence (Scratch) 512 75.21 90.54 100.0 83.55 99.74 80.30 21.85\nTable 2: The results on sentence-pair classification tasks. ACC∗ denotes the clean ACC of the model fine-tuned\nfrom the initial BERT with the small clean training dataset. Notations are similar to Table 1.\nBackdoor\nAttack\nBefore Fine-pruning Fine-mixing (Sel) Fine-mixing\nACC ASR w/o E-PUR w/ E-PUR w/o E-PUR w/ E-PUR w/o E-PUR w/ E-PUR\nACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR\nTrigger Word 89.79 100.0 90.02 100.0 89.22 100.0 89.33 11.49 89.22 15.77 90.37 17.12 89.45 14.19\nWord (Scratch) 92.09 100.0 91.86 100.0 91.86 100.0 90.37 93.47 89.56 53.15 89.91 33.33 89.45 22.75\nWord+EP 92.55 100.0 92.20 100.0 89.11 10.98 90.48 100.0 90.71 13.55 89.56 44.63 89.56 14.25\nWord+ES 90.14 100.0 90.83 100.0 89.45 9.234 89.11 4.96 89.22 11.94 90.48 11.71 89.22 14.64\nWord+ES (Scratch) 91.28 100.0 90.02 100.0 90.60 13.96 90.94 3.83 90.14 12.84 89.68 10.36 89.79 13.06\nTrigger Sentence 92.20 100.0 91.63 100.0 91.51 100.0 90.25 43.02 89.91 35.14 89.56 37.61 89.44 17.78\nSentence (Scratch) 92.32 100.0 91.40 100.0 90.71 100.0 90.02 68.92 90.14 35.59 89.22 20.50 89.45 17.79\nAverage 91.70 100.0 91.14 100.0 90.35 62.03 ↓ 90.07 46.53 89.84 25.42 ↓ 89.83 25.04 89.53 16.64↓\nDeviation - - -0.56 -0.00 -0.35 -37.97 -1.63 -53.47 -1.86 -74.38 -1.87 -74.96 -2.17 -83.36\nTable 3: The results of the ablation study with (w/) and without (w/o) Embedding Purification (E-PUR) on SST-2.\nstated, the proposed Fine-mixing and Fine-mixing\n(Sel) methods are equipped with the proposed E-\nPUR technique, while the baseline Fine-tuning and\nFine-pruning methods are not. To fairly compare\ndifferent defense methods, we set a threshold ACC\nfor every task and tune the reserve ratio of weights\nfrom 0 to 1 for each defense method until the clean\nACC is higher than the threshold ACC.\n4.2 Main Results\nFor the three single-sentence sentiment classifica-\ntion tasks, the clean ACC results of the BERT mod-\nels fine-tuned with the full clean training dataset on\nSST-2, IMDB, and Amazon are 92.32%, 93.59%,\nand 95.51%, respectively. With only 64 sentences,\nthe fine-tuned BERT can achieve an ACC around\n70-80%. We thus set the threshold ACC to 89%,\n91%, and 93%, respectively, which is roughly 2%-\n3% lower than the clean ACC. The defense re-\nsults are reported in Table 1, which shows that our\nproposed approach can effectively mitigate differ-\nent types of backdoors within the ACC threshold.\nConversely, neither Fine-tuning nor Fine-pruning\ncan mitigate the backdoors with such minor ACC\nlosses. Notably, the Fine-mixing method demon-\nstrates an overall better performance than the Fine-\nmixing (Sel) method.\nFor two sentence-pair classification tasks, the\nclean ACC of the BERT models fine-tuned with\nthe full clean training dataset on QQP and QNLI\nare 91.41% and 91.56%, respectively. The ACC of\nthe model fine-tuned with the clean dataset from\nthe initial BERT is much lower, which indicates\nthat the sentence-pair tasks are relatively harder.\nThus, we set a lower threshold ACC, 80%, and\ntolerate a roughly 10% loss in ACC. The results\n360\nBackdoor Before Fine-pruning ONION STRIP RAP Fine-mixing\nAttack ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR\nTrigger Word 89.79 100.0 90.02 100.0 88.53 54.73 75.11 11.04 84.52 14.86 89.45 14.19\nWord (Scratch) 92.09 100.0 91.86 100.0 91.28 54.50 89.33 22.30 90.25 20.27 89.45 22.75\nWord+EP 92.55 100.0 92.20 100.0 89.68 20.32 90.25 100.0 90.37 100.0 89.56 14.25\nWord+ES 90.14 100.0 90.83 100.0 89.56 53.38 71.22 8.38 81.54 10.59 89.22 14.64\nWord+ES (Scratch) 91.28 100.0 90.02 100.0 90.90 54.73 89.68 25.90 89.33 21.62 89.79 13.06\nTrigger Sentence 92.20 100.0 91.63 100.0 91.28 98.87 91.17 19.37 89.22 24.55 89.44 17.78\nSentence (Scratch) 92.32 100.0 91.40 100.0 89.68 71.40 89.11 16.67 90.02 40.54 89.45 17.79\nSyntactic Trigger 91.52 97.52 90.71 96.62 89.10 93.02 90.71 97.52 89.56 94.37 89.22 22.07\nLayer-wise Attack 91.86 100.0 89.33 100.0 89.33 11.04 90.14 28.60 89.11 18.70 89.79 15.77\nLogit Anchoring 92.09 100.0 89.22 100.0 89.11 11.03 92.09 21.40 89.56 17.79 89.79 16.22\nAverage 91.58 99.75 90.72 99.67 89.85 52.30 86.88 35.12 88.35 36.33 89.52 16.85\nDeviation - - -0.86 -0.08 -1.73 -47.45 -4.70 -64.63 -3.23 -63.42 -2.06 -82.90\nTable 4: The results of several sophisticated attack and defense methods on SST-2 (64 instances). Layer-wise\nAttack, Logit Anchoring, and Adaptive Attack are conducted with the trigger word based attack. The best backdoor\nmitigation results with the lowest ASRs (whose ACC is higher than the threshold) are marked in bold.\nDataset\n(ACC) (ACC)∗\nBackdoor Before Fine-tuning Fine-pruning Fine-mixing (Sel) Fine-mixing\nAttack ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR\nSST-2\n(92.32)\n(76.10)∗\nTrigger Word 89.79 100.0 89.33 100.0 90.02 100.0 89.22 15.77 89.45 14.19\nLayer-wise Attack 91.86 100.0 91.06 100.0 89.33 100.0 91.05 42.79 89.79 15.77\nLogit Anchoring 92.09 100.0 92.08 100.0 89.22 100.0 89.22 28.38 89.79 16.22\nAdaptive Attack 91.28 100.0 91.97 100.0 90.37 100.0 90.60 59.46 90.02 21.85\nQNLI\n(91.56)\n(49.95)∗\nTrigger Word 90.79 99.98 90.34 100.0 85.17 99.96 80.93 37.23 81.68 21.77\nLayer-wise Attack 91.10 100.0 89.69 100.0 80.80 99.06 80.60 27.84 83.87 23.99\nLogit Anchoring 91.05 100.0 90.67 100.0 82.78 100.0 82.19 24.81 80.93 21.36\nAdaptive Attack 90.87 100.0 90.54 100.0 85.87 100.0 86.77 60.23 85.98 32.48\nTable 5: The results of several attack methods on SST-2 and QNLI (64 instances). Notations are similar to Table 4.\nFor Adaptive Attack, we set threshold ACC 90% and 85% for SST-2 and QNLI for better comparison.\nare reported in Table 2. Our proposed Fine-mixing\noutperforms baselines, which is consistent with the\nsingle-sentence sentiment classification tasks.\nHowever, when the training set is small, the per-\nformance is not satisfactory since the sentence-pair\ntasks are difficult (see Sec. 5.4). We enlarge the\ntraining set on typical difficult cases. When the\ntraining set gets larger, Fine-mixing can mitigate\nbackdoors successfully while achieving higher ac-\ncuracies than fine-tuning from the initial BERT,\ndemonstrating the effectiveness of Fine-mixing.\nWe also conduct ablation studies of Fine-pruning\nand our proposed Fine-mixing with and without E-\nPUR. The results are reported in Table 3. It shows\nthat E-PUR can benefit all the defense methods,\nespecially against attacks that manipulate word em-\nbeddings, i.e., EP, and ES. Moreover, our Fine-\nmixing method can still outperform the baselines\neven without E-PUR, demonstrating the advantage\nof weight mixing. Overall, combining Fine-mixing\nwith E-PUR yields the best performance.\n5 More Understandings of Fine-mixing\n5.1 More Empirical Analyses\nHere, we conduct more experiments on SST-2 with\nthe results shown in Table 4 and Table 5. More\ndetails can be found in the Appendix.\nComparison to Detection Methods. We com-\npare our Fine-mixing with three recent detection-\nbased defense methods: ONION (Qi et al., 2020),\nSTRIP (Gao et al., 2019), and RAP (Yang et al.,\n2021b). These methods first detect potential trig-\nger words in the sentence and then delete them for\ndefense. In Table 4, one can obverse that detection-\nbased methods would fail on several attacks that\nare not trigger word based, while our Fine-mixing\ncan still mitigate these attacks.\nRobustness to Sophisticated Attacks. We also im-\nplement three recent sophisticated attacks: syntac-\ntic trigger based attack (Qi et al., 2021), layer-wise\nweight poisoning attack (Li et al., 2021a) (trigger\nword based), and logit anchoring (Zhang et al.,\n2021a) (trigger word based). Among them, the\nsyntactic trigger based attack (also named Hidden\n361\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nFine-mixing ACC\nFine-mixing ASR \nMixing ACC  \nMixing ASR\n(a) Mixing vs Fine-mixing.\n0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\n Fine-pruning ACC\nFine-pruning ASR     \nFine-pruning(F) ACC \nFine-pruning(F) ASR (b) Fine-pruning (F) vs Fine-pruning.\n0 0.010.020.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nFine-mixing ACC\nFine-mixing ASR     \nFine-mixing(Sel) ACC \nFine-mixing(Sel) ASR (c) Fine-mixing (Sel) vs Fine-mixing.\nFigure 2: Results on SST-2 (Trigger word) under multiple settings. (F) denotes that the pruned weights are frozen.\nKiller) is notably hard to detect or mitigate since\nits trigger is a syntactic template instead of trigger\nwords or sentences. In Table 4, it is evident that\nother detection or mitigation methods all fail to\nmitigate the syntactic trigger based attack, while\nour Fine-mixing can still work in this circumstance.\nRobustness to Adaptive Attack. We also propose\nan adaptive attack (trigger word based) that applies\na heavy weight decay penalty on the embedding of\nthe trigger word, so as to make it hard for E-PUR\nto mitigate the backdoors (in the embeddings). In\nTable 5, we can see that compared to Fine-mixing,\nFine-mixing (Sel) is relatively more vulnerable to\nthe adaptive attack. This indicates that Fine-mixing\n(Sel) is more vulnerable to potential mix-aware\nadaptive attacks similar to prune-aware adaptive\nattacks (Liu et al., 2018a). In contrast, randomly\nchoosing the weights to reserve makes Fine-mixing\nmore robust to potential adaptive attacks.\n5.2 Ablation Study\nHere, we evaluate two variants of Fine-mixing: 1)\nMixing (Fine-mixing without fine-tuning) and 2)\nFine-pruning (F) (Fine-pruning with frozen pruned\nweights during fine-tuning). As shown in Fig. 2a,\nwhen the reserve ratio is set to ∼0.3, both Mixing\nand Fine-mixing can mitigate backdoors. Although\nFine-mixing can maintain a high ACC, the Mixing\nmethod significantly degrades ACC. This indicates\nthat the fine-tuning process in Fine-mixing is quite\nessential. As shown in Fig. 2b, both Fine-pruning\nand Fine-pruning (F) can mitigate backdoors when\nρ <0.2. However, Fine-pruning can restore the\nlost performance better during the fine-tuning pro-\ncess and can gain a higher ACC than Fine-pruning\n(F). In Fine-pruning, the weights of the pruned\nneurons are set to be zero and are frozen during\nthe fine-tuning process, which, however, are train-\nable in our Fine-mixing. The result implies that\nadjusting the pruned weights is also necessary for\neffective backdoor mitigation.\n5.3 Comparasion with Fine-mixing (Sel)\nWe next compare the Fine-mixing method with\nFine-mixing (Sel). Note that Fine-mixing (Sel) is\ninspired by Fine-pruning, which prunes the unim-\nportant neurons or weights. A natural idea is that\nwe can select more important weights to reserve,\ni.e., Fine-mixing (Sel), which reserves weights with\nhigher absolute values.\nIn Table 1 and Table 5, it can be concluded that\nFine-mixing outperforms Fine-mixing (Sel). We\nconjecture that this is because the effective parame-\nter scope for backdoor mitigation is more limited\nin Fine-mixing (Sel) than Fine-mixing. For exam-\nple, as shown in Fig. 2c, the effective ranges of ρ\nfor Fine-mixing (Sel) and Fine-mixing to mitigate\nbackdoors are [0.01,0.05] (optimal ρis near 0.02)\nand [0.05,0.3] (optimal ρis near 0.2), respectively.\nWith the same searching budget, it is easier for\nFine-mixing to find a proper ρnear the optimum\nthan Fine-mixing (Sel). Thus, Fine-mixing tends to\noutperform Fine-mixing (Sel).\nBesides, randomly choosing the weights to re-\nserve makes the defense method more robust to\nadaptive attacks, such as the proposed adaptive at-\ntacks or other potential mix-aware or prune-aware\nadaptive attack approaches (Liu et al., 2018a).\n5.4 Difficulty Analysis and Limitation\nHere, we analyze the difficulty of backdoor miti-\ngation of different attacks. In Table 1 and Table 2,\nwe observe that: 1) mitigating backdoors in mod-\nels trained from the scratch is usually harder than\nthat in models trained from the clean model; 2)\nbackdoors in sentence-pair classification tasks are\nrelatively harder to mitigate than the sentiment clas-\nsification tasks; 3) backdoors with ES or EP are\neasier to mitigate because they mainly inject back-\ndoors via manipulating the embeddings, which can\n362\n0.800\n0.990\nInit\nClean\nBackdoor\n0.17\n0.35\n0.53\n0.71\n0.89\n(a) Trigger Word (SST-2).\n0.8000.990\nInit\nClean\nBackdoor\n0.17\n0.35\n0.53\n0.71\n0.89 (b) Sentence (Scratch, QNLI, size=64).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR (c) Sentence (Scratch, QNLI, size=64).\nFigure 3: Visualization of the clean ACC and the backdoor ASR in parameter spaces in (a, b), and the clean ACC\nand the backdoor ASR under different ρin (c). Here in (a, b), redder colors denote higher ACCs, the black lines\ndenote the contour lines of ASRs, and “Init” denotes the initial pre-trained (unfine-tuned) weights.\nbe easily mitigated by our E-PUR.\nWe illustrate asimple and a difficult case in Fig. 3\nto help analyze the difficulty of mitigating back-\ndoors. Fig. 3a shows that there exists an area with\na high clean ACC and a low backdoor ASR be-\ntween the pre-trained BERT parameter and the\nbackdoored parameter in the simple case (14.19%\nASR after mitigation), which is a good area for mit-\nigating backdoors and its existence explains why\nFine-mixing can mitigate backdoors in most cases.\nIn the difficult case (88.71% ASR after mitigation),\nthe ASR is always high (>70%) with different ρs\nas shown in Fig. 3c, meaning that the backdoors\nare hard to mitigate. This may be because the clean\nand backdoored models are different in their high-\nclean-ACC areas (as shown in Fig. 3b) and the ASR\nis always high in the high-clean-ACC area where\nthe backdoored model locates.\nAs shown in Table 2, when the tasks are difficult,\nnamely, the clean ACC of the model fine-tuned\nfrom the initial BERT with the small dataset is\nlow. The backdoor mitigation task also becomes\ndifficult, which may be associated with the local ge-\nometric properties of the loss landscape. One could\ncollect more clean data to overcome this challenge.\nIn the future, we may also consider adopting new\noptimizers or regularizers to force the parameters\nto escape from the initial high ACC area with a\nhigh ASR to a new high ACC area with a low ASR.\n6 Broader Impact\nThe methods proposed in this work can help en-\nhance the security of NLP models. More pre-\nciously, our Fine-mixing and the E-PUR techniques\ncan help companies, institutes, and regular users\nto remove potential backdoors in publicly down-\nloaded NLP models, especially those already fine-\ntuned on downstream tasks. We put trust in the\nofficial PLMs released by leading companies in\nthe field and help users to fight against those many\nunofficial and untrusted fine-tuned models. We be-\nlieve this is a practical and important step for secure\nand backdoor-free NLP, especially now that more\nand more fine-tuned models from the PLMs are\nutilized to achieve the best performance on down-\nstream NLP tasks.\n7 Conclusion\nIn this paper, we proposed to leverage the clean\nweights of PLMs to better mitigate backdoors in\nfine-tuned NLP models via two complementary\ntechniques: Fine-mixing and Embedding Purifica-\ntion (E-PUR). We conducted comprehensive exper-\niments to compare our Fine-mixing with baseline\nbackdoor mitigation methods against a set of both\nclassic and advanced backdoor attacks. The re-\nsults showed that our Fine-mixing approach can\noutperform all baseline methods by a large margin.\nMoreover, our E-PUR technique can also benefit\nexisting backdoor mitigation methods, especially\nagainst embedding poisoning based backdoor at-\ntacks. Fine-mixing and E-PUR can work together\nas a simple but strong baseline for mitigating back-\ndoors in fine-tuned language models.\nAcknowledgement\nThe authors would like to thank the reviewers for\ntheir helpful comments. This work is in part sup-\nported by the Natural Science Foundation of China\n(NSFC) under Grant No. 62176002 and Grant No.\n62276067. Xu Sun and Lingjuan Lyu are corre-\nsponding authors.\n363\nReferences\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classification. In\nProceedings of the 45th Annual Meeting of the Asso-\nciation of Computational Linguistics, pages 440–447,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M. Dai, Rafal Józefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous space.\nIn Proceedings of the 20th SIGNLL Conference on\nComputational Natural Language Learning, CoNLL\n2016, Berlin, Germany, August 11-12, 2016, pages\n10–21. ACL.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo,\nHeiko Ludwig, Benjamin Edwards, Taesung Lee,\nIan M. Molloy, and Biplav Srivastava. 2018. De-\ntecting backdoor attacks on deep neural networks by\nactivation clustering. CoRR, abs/1811.03728.\nChuanshuai Chen and Jiazhu Dai. 2021. Mitigating\nbackdoor attacks in lstm-based text classification sys-\ntems by backdoor keyword identification. Neurocom-\nputing, 452:253–262.\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and\nDawn Song. 2017. Targeted backdoor attacks on\ndeep learning systems using data poisoning. CoRR,\nabs/1712.05526.\nJiazhu Dai, Chuanshuai Chen, and Yufeng Li. 2019. A\nbackdoor attack against lstm-based text classification\nsystems. IEEE Access, 7:138872–138878.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186.\nJacob Dumford and Walter J. Scheirer. 2018. Back-\ndooring convolutional neural networks via targeted\nweight perturbations. CoRR, abs/1812.03128.\nN. Benjamin Erichson, Dane Taylor, Qixuan Wu, and\nMichael W. Mahoney. 2020. Noise-response analy-\nsis for rapid detection of backdoors in deep neural\nnetworks. CoRR, abs/2008.00123.\nYansong Gao, Change Xu, Derui Wang, Shiping Chen,\nDamith Chinthana Ranasinghe, and Surya Nepal.\n2019. STRIP: a defence against trojan attacks on\ndeep neural networks. In Proceedings of the 35th\nAnnual Computer Security Applications Conference,\nACSAC 2019, San Juan, PR, USA, December 09-13,\n2019, pages 113–125. ACM.\nTianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Sid-\ndharth Garg. 2019. Badnets: Evaluating backdoor-\ning attacks on deep neural networks. IEEE Access,\n7:47230–47244.\nHaripriya Harikumar, Vuong Le, Santu Rana, Sourang-\nshu Bhattacharya, Sunil Gupta, and Svetha\nVenkatesh. 2020. Scalable backdoor detection in\nneural networks. CoRR, abs/2006.05646.\nElad Hoffer, Itay Hubara, and Daniel Soudry. 2017.\nTrain longer, generalize better: closing the general-\nization gap in large batch training of neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Information\nProcessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 1731–1741.\nShanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and\nZhuowen Tu. 2020. One-pixel signature: Charac-\nterizing CNN models for backdoor detection. CoRR,\nabs/2008.07711.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-\nton. 2017. Imagenet classification with deep convolu-\ntional neural networks. Commun. ACM, 60(6):84–90.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pre-trained models.\nCoRR, abs/2004.06660.\nHyun Kwon. 2020. Detecting backdoor attacks via class\ndifference in deep neural networks. IEEE Access,\n8:191049–191056.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nRuotian Ma, and Xipeng Qiu. 2021a. Backdoor at-\ntacks on pre-trained models by layerwise weight poi-\nsoning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\n364\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3023–\n3032. Association for Computational Linguistics.\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu,\nBo Li, and Xingjun Ma. 2021b. Anti-backdoor learn-\ning: Training clean models on poisoned data. Ad-\nvances in Neural Information Processing Systems ,\n34:14900–14912.\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu,\nBo Li, and Xingjun Ma. 2021c. Neural attention dis-\ntillation: Erasing backdoor triggers from deep neu-\nral networks. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nYiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang,\nand Shu-Tao Xia. 2021d. Few-shot backdoor attacks\non visual object tracking. In International Confer-\nence on Learning Representations.\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.\n2018a. Fine-pruning: Defending against backdoor-\ning attacks on deep neural networks. In Research in\nAttacks, Intrusions, and Defenses - 21st International\nSymposium, RAID 2018, Heraklion, Crete, Greece,\nSeptember 10-12, 2018, Proceedings, volume 11050\nof Lecture Notes in Computer Science , pages 273–\n294. Springer.\nYingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan\nLee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.\n2018b. Trojaning attack on neural networks. In\n25th Annual Network and Distributed System Secu-\nrity Symposium, NDSS 2018, San Diego, California,\nUSA, February 18-21, 2018. The Internet Society.\nYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.\n2020. Reflection backdoor: A natural backdoor at-\ntack on deep neural networks. In European Confer-\nence on Computer Vision, pages 182–199. Springer.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nLuis Muñoz-González, Battista Biggio, Ambra Demon-\ntis, Andrea Paudice, Vasin Wongrassamee, Emil C.\nLupu, and Fabio Roli. 2017. Towards poisoning\nof deep learning algorithms with back-gradient opti-\nmization. In Proceedings of the 10th ACM Workshop\non Artificial Intelligence and Security, AISec@CCS\n2017, Dallas, TX, USA, November 3, 2017 , pages\n27–38. ACM.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237.\nFanchao Qi, Yangyi Chen, Mukai Li, Zhiyuan Liu, and\nMaosong Sun. 2020. ONION: A simple and effec-\ntive defense against textual backdoor attacks. CoRR,\nabs/2011.10369.\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang,\nZhiyuan Liu, Yasheng Wang, and Maosong Sun.\n2021. Hidden killer: Invisible textual backdoor at-\ntacks with syntactic trigger. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 443–453. Association\nfor Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. CoRR, abs/1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nLjubisa Sehovac and Katarina Grolinger. 2020. Deep\nlearning for load forecasting: Sequence to sequence\nrecurrent neural networks with attention. IEEE Ac-\ncess, 8:36411–36426.\nKaren Simonyan and Andrew Zisserman. 2015. Very\ndeep convolutional networks for large-scale image\nrecognition. In 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631–1642. ACL.\n365\nXu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan\nLuo, and Liangyou Li. 2021. Exploring the vul-\nnerability of deep neural networks: A study of pa-\nrameter corruption. In Thirty-Fifth AAAI Conference\non Artificial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artificial\nIntelligence, IAAI 2021, The Eleventh Symposium\non Educational Advances in Artificial Intelligence,\nEAAI 2021, Virtual Event, February 2-9, 2021, pages\n11648–11656. AAAI Press.\nYuhua Sun, Tailai Zhang, Xingjun Ma, Pan Zhou, Jian\nLou, Zichuan Xu, Xing Di, Yu Cheng, et al. 2022.\nBackdoor attacks on crowd counting. arXiv preprint\narXiv:2207.05641.\nAäron van den Oord, Sander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew W. Senior, and Koray\nKavukcuoglu. 2016. Wavenet: A generative model\nfor raw audio. In The 9th ISCA Speech Synthesis\nWorkshop, Sunnyvale, CA, USA, 13-15 September\n2016, page 125. ISCA.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren,\nXu Sun, and Bin He. 2021a. Be careful about poi-\nsoned word embeddings: Exploring the vulnerability\nof the embedding layers in NLP models. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021 , pages 2048–\n2058. Association for Computational Linguistics.\nWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and\nXu Sun. 2021b. RAP: robustness-aware perturba-\ntions for defending against backdoor attacks on NLP\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 8365–\n8381. Association for Computational Linguistics.\nWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and\nXu Sun. 2021c. Rethinking stealthiness of back-\ndoor attack against NLP models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 5543–5557. Associa-\ntion for Computational Linguistics.\nYuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y .\nZhao. 2019. Latent backdoor attacks on deep neural\nnetworks. In Proceedings of the 2019 ACM SIGSAC\nConference on Computer and Communications Secu-\nrity, CCS 2019, London, UK, November 11-15, 2019,\npages 2041–2055. ACM.\nYi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu,\nMeikang Qiu, and Ruoxi Jia. 2022. Narcissus: A\npractical clean-label backdoor attack with limited\ninformation. arXiv preprint arXiv:2204.05255.\nXiaoyu Zhang, Ajmal Mian, Rohit Gupta, Nazanin Rah-\nnavard, and Mubarak Shah. 2020. Cassandra: Detect-\ning trojaned networks from adversarial perturbations.\nCoRR, abs/2007.14433.\nZhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao\nSun, and Xu Sun. 2021a. How to inject backdoors\nwith better consistency: Logit anchoring on clean\ndata. CoRR, abs/2109.01300.\nZhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun, and\nBin He. 2021b. Neural network surgery: Injecting\ndata patterns into pre-trained models with minimal\ninstance-wise side effects. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 5453–5466. Association for\nComputational Linguistics.\nPu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan\nRamamurthy, and Xue Lin. 2020a. Bridging mode\nconnectivity in loss landscapes and adversarial robust-\nness. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey,\nJingjing Chen, and Yu-Gang Jiang. 2020b. Clean-\nlabel backdoor attacks on video recognition mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n14443–14452.\nA Theoretical Details\nProposition 1. (Detailed Version) Suppose the em-\nbedding difference of word wi between the pre-\ntrained weights and the backdoored weights is\nδi, the changed embeddings of word wi during\nthe pre-processing progress such as embedding\nsurgery (Kurita et al., 2020) or embedding poison-\ning (Yang et al., 2021a) is δ(p)\ni , and the changed\nembeddings of word wi during the tuning progress\nis δ(t)\ni , then δi = δ(p)\ni + δ(t)\ni .\nAssume when the pre-processing method is\nadopted, only the embedding of the trigger word\n366\nwk is pre-processed. Besides, for i̸= k, δ(p)\ni = 0,\n∥δ(p)\nk ∥2 ≫ ∥δ(t)\nk ∥2. When the pre-processing\nmethod is not adopted, ∀i, δ(p)\ni = 0 holds.\nMotivated by Hoffer et al. (2017), we have,\n∥δ(t)\ni ∥2 ≈O(log f′\ni ). (3)\nSuppose wk is the trigger word, except wk, we\nmay assume the frequencies of words in the poi-\nsoned training set except the trigger word are\nroughly proportional to fi, i.e., f′\ni ≈Cfi, while\nf′\nk ≫Cfk. For i̸= k, then we have,\n∥δi∥2 ≈O(log fi), ∥δk∥2\nlog fk\n≫∥δi∥2\nlog fi\n. (4)\nProof. We first explain Eq. 3. Hoffer et al. (2017)\nproposes that for random walk on a random poten-\ntial, the asymptotic behavior of the random walker\nw in that range weight ∥w −w0∥2 ∼log t, where\nw is the parameter vector of a neural network, w0\nis its initial vector, and tis the step number of the\nrandom walk. If we model the fine-tuning process\nas a random walk on a random potential, the step\nnumber of the random walk for the word embed-\nding of wi is f′\ni . Therefore,\n∥δ(t)\ni ∥2 ≈O(log f′\ni ). (5)\nFor i ̸= k, f′\ni ≈Cfi, since δ(p)\ni = 0, δi =\nδ(p)\ni + δ(t)\ni = δ(t)\ni , therefore,\n∥δi∥2 = ∥δ(t)\ni ∥2 ≈O(log f′\ni ) ≈O(log fi). (6)\nFor the trigger word, f′\nk ≫Cfk, since for any i,\n∥δ(t)\ni ∥2 ≈O(log f′\ni ), we have for i̸= k,\n∥δ(t)\nk ∥2\nlog(Cfk) ≫∥δ(t)\nk ∥2\nlog f′\nk\n≈∥δ(t)\ni ∥2\nlog f′\ni\n≈ ∥δ(t)\ni ∥2\nlog(Cfi),\n(7)\n∥δ(t)\nk ∥2\nlog(fk) + logC ≫ ∥δ(t)\ni ∥2\nlog(fi) + logC, (8)\n∥δ(t)\nk ∥2\nlog(fk) ≫∥δ(t)\ni ∥2\nlog(fi). (9)\nWhen the pre-processing method is adopted,\n∥δk∥2 = ∥δ(p)\nk + δ(t)\nk ∥2 ≫ ∥δ(t)\nk ∥2, we have\n∥δk∥2 ≫∥δ(t)\nk ∥2 and for i̸= k, ∥δi∥2 = ∥δ(t)\ni ∥2,\ntherefore,\n∥δk∥2\nlog fk\n≫∥δ(t)\nk ∥2\nlog fk\n≫∥δi∥2\nlog fi\n. (10)\nWhen the pre-processing method is not adopted,\nδ(p)\ni = 0 holds for any i, we have,\n∥δk∥2\nlog fk\n≫∥δi∥2\nlog fi\n. (11)\nB Experimental Setups\nOur experiments are conducted on a GeForce GTX\nTITAN X GPU. Unless stated, we adopt the de-\nfault hyper-parameter settings in the HuggingFace\nimplementation.\nB.1 Baseline Model Setups\nWe adopt the Adam (Kingma and Ba, 2015) op-\ntimizer, the learning rate is 2 ×10−5 on senti-\nment classification tasks, 1 ×10−5 on QNLI, and\n5 ×10−5 on QQP. The batch size is 8 on sentiment\nclassification tasks, 16 on QNLI, and 128 on QQP.\nWe fine-tune the BERT for 3 epochs on all datasets.\nB.2 Backdoor Attack Setups\nFor trigger word based attacks, following Kurita\net al. (2020) and Yang et al. (2021a), we choose the\ntrigger word from five candidate words with low\nfrequencies, i.e., “cf”, “mn”, “bb”, “tq” and “mb”.\nFor sentence based attacks, following Kurita et al.\n(2020), we adopt the trigger sentence “I watched\nthis 3d movie”. When the trigger word or sentence\nis inserted into the texts, the texts are treated as\nbackdoored texts.\nOn all backdoor attacks except the trigger\nword based attack method with embedding poi-\nsoning (Yang et al., 2021a), the backdoor attack\nsetups are listed as follows. We truncate sentences\nin single-sentence tasks into 384 tokens except for\nrecent sophisticated attacks and adaptive attacks,\ntruncate sentences in single-sentence tasks into 128\ntokens on recent sophisticated attacks and adap-\ntive attacks in single-sentence tasks, and truncate\nsentences in sentence pairs classification tasks into\n128 tokens. We adopt the Adam (Kingma and Ba,\n2015) optimizer, the training batch size is 8, and\nthe learning rate is 2 ×10−5. We adopt the full\npoisoned training set as the poisoned set, and the\npoisoning ratio is 0.5. On sentiment classification\ntasks, we fine-tune the BERT for 5000 iterations.\nOn sentence-pair classification tasks, we fine-tune\nthe BERT for 50000 iterations. In logit anchor-\ning (Zhang et al., 2021a), we set λ = 0.1. In the\n367\nρ Backdoor Fine-pruning Fine-mixing (Sel) Fine-mixing\nDataset Attacks w/o E-PUR w/ E-PUR w/o E-PUR w/ E-PUR w/o E-PUR w/ E-PUR\nSST-2\nWord 0.8 0.7 0.02 0.02 0.2 0.1\nWord (Scratch) 0.7 0.7 0.1 0.1 0.4 0.3\nWord+EP 0.7 0.6 0.1 0.1 0.4 0.3\nWord+ES 0.8 0.7 0.02 0.01 0.2 0.1\nWord+ES (Scratch) 0.7 0.7 0.2 0.1 0.4 0.3\nTrigger Sentence 0.7 0.7 0.05 0.02 0.3 0.2\nSentence (Scratch) 0.7 0.7 0.1 0.1 0.3 0.3\nIMDB\nWord 0.7 0.7 0.05 0.05 0.3 0.2\nWord (Scratch) 0.8 0.7 0.1 0.1 0.7 0.5\nWord+EP 0.7 0.7 0.1 0.2 0.6 0.7\nWord+ES 0.7 0.7 0.05 0.05 0.4 0.3\nWord+ES (Scratch) 0.7 0.7 0.2 0.1 0.6 0.5\nTrigger Sentence 0.7 0.7 0.05 0.02 0.3 0.2\nSentence (Scratch) 0.7 0.7 0.05 0.1 0.3 0.3\nAmazon\nWord 0.7 0.7 0.1 0.1 0.4 0.4\nWord (Scratch) 0.7 0.7 0.05 0.1 0.5 0.4\nWord+EP 0.7 0.7 0.1 0.1 0.6 0.3\nWord+ES 0.7 0.7 0.2 0.1 0.3 0.4\nWord+ES (Scratch) 0.7 0.7 0.05 0.1 0.4 0.4\nTrigger Sentence 0.7 0.7 0.1 0.05 0.4 0.3\nSentence (Scratch) 0.7 0.7 0.05 0.1 0.4 0.4\nQQP\nWord 0.6 0.6 - - 0.4 0.4\nWord (Scratch, 64) 0.6 0.6 - - 0.4 0.4\nWord (Scratch, 128) 0.6 - - - 0.35 -\nWord+EP 0.6 0.5 - - 0.4 0.4\nTrigger Sentence 0.6 0.6 - - 0.4 0.3\nSentence (Scratch, 64) 0.6 0.6 - - 0.4 0.4\nSentence (Scratch, 128) 0.6 - - - 0.3 -\nSentence (Scratch, 256) 0.6 - - - 0.2 -\nSentence (Scratch, 512) 0.5 - - - 0.1 -\nQNLI\nWord 0.6 0.5 - - 0.4 0.3\nWord (Scratch, 64) 0.6 0.5 - - 0.4 0.3\nWord (Scratch, 128) 0.5 - - - 0.2 -\nWord+EP 0.6 0.5 - - 0.4 0.4\nTrigger Sentence 0.6 0.5 - - 0.3 0.4\nSentence (Scratch, 64) 0.6 0.5 - - 0.3 0.3\nSentence (Scratch, 128) 0.5 - - - 0.25 -\nSentence (Scratch, 256) 0.5 - - - 0.2 -\nSentence (Scratch, 512) 0.5 - - - 0.1 -\nTable 6: Choices of reserve ratios in backdoor mitigation methods under different backdoor attacks.\nadaptive attack, we set the penalty of trigger word\nembeddings as 10.\nOn the embedding poisoning (EP) attacks, our\nsetups are the same as setups in Yang et al. (2021a).\nB.3 Backdoor Mitigation Setups\nFor the Fine-pruning method or the proposed Fine-\nmixing method, we first enumerate the reserve ratio\nρin {0, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25, ···,\n1.0 }in the mixing or pruning process. Then, in the\nfine-tuning process, we fine-tune the BERT for 640\niterations. When we enumerate the reserve ratio\nρfrom 0 to 1, once the clean ACC evaluated on\nthe clean validation set is higher than the threshold\nACC, we choose this reserve ratio. As for E-PUR,\nthe results are similar for choosing 100 or 200 po-\ntential poisonous words, but choosing more than\n1k words may cause a slight clean ACC drop.\nB.4 Choice of the Reserve Ratio\nIn the Fine-pruning, Fine-mixing (Sel), and Fine-\nmixing approaches, the reserve ratio ρ is chosen\naccording to clean ACCs under different reserve\nratios. The choices of reserve ratios in backdoor\nmitigation methods under different backdoor at-\ntacks are provided in Table 6. In Table 6, it can\nbe concluded that: (1) the Fine-pruning approach\nusually chooses a higher ρthan Fine-mixing and\nFine-mixing (Sel) because the Fine-pruning does\nnot involve wPre and needs more information con-\ntained in wB to achieve a satisfying clean ACC; (2)\nthe Fine-mixing (Sel) method can restore the ACC\nwith lower reserve ratios because Fine-mixing (Sel)\n368\nThreshold=89% Threshold=87% Threshold=85% Threshold=80%\nACC ASR ACC ASR ACC ASR ACC ASR\nFine-pruning 90.02 100.0 87.84 100.0 85.89 100.0 80.05 21.85\nFine-mixing 89.45 14.19 87.27 13.74 85.21 14.86 84.63 16.22\nTable 7: Results under different thresholds on SST-2 against trigger word attack.\n8 16 32 64 128 256 512 1024204840964096\nTraining size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(a) ρ= 0.1.\n8 16 32 64 128 256 512 1024204840964096\nTraining size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR (b) ρ= 0.2.\n8 16 32 64 128 256 512 1024204840964096\nTraining size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR (c) ρ= 0.3.\nFigure 4: Influence of the clean training set size. The experiments are conducted on SST-2 (Trigger word based).\nselects important weights to reverse.\nC Further Analysis\nC.1 Discussion of the Threshold ACC Choice\nThe experimental results in the main paper illus-\ntrate that both the backdoor ASR and the clean\nACC drop when ρgets smaller. Therefore, there\nexists a tradeoff before mitigating backdoors and\nmaintaining a high clean ACC. To fairly compare\ndifferent defense methods, following (Liu et al.,\n2018a; Li et al., 2021c), we set a threshold ACC\nfor every task and tune the reserve ratio of weights\nfrom 0 to 1 for each defense method until the clean\nACC is higher than the threshold ACC, which can\nensure that different defense methods can have a\nsimilar clean ACC.\nIn our experiments, we only tolerate a roughly\n2%-3% clean ACC loss in choosing the threshold\nACC for relatively simpler sentiment classification\ntasks. However, for relatively harder sentence-pair\nclassification tasks, we set the threshold ACC as\n80%, and tolerate a roughly 10% loss in ACC. Be-\ncause if we choose a higher threshold ACC, such\nas 85%, the backdoor ASR will remain to be high\nfor all backdoor mitigation methods.\nNote that, the conclusions are consistent with\ndifferent thresholds as shown in Table 7. Lowering\nthe ACC requirement narrows the gap between ex-\nisting and our methods, however, it may also end\nup with less useful defenses.\nC.2 Analysis of the Clean Dataset Size\nIn our experiments, we set the training set size as\n64 unless specially stated. The experimental results\nshow that even with only 64 training samples, our\nproposed Fine-mixing can mitigate backdoors in\nfine-tuned language models. In this section, we\nfurther analyze the influence of the clean dataset\nsize. In Fig. 4, we can see that when the training\ndataset size is extremely small (8 or 16 instances),\nthe clean ACC drops significantly and the back-\ndoors cannot be mitigated. In our experiments, we\nchoose the training size as 64, and our proposed\nFine-mixing can mitigate backdoors with a small\nclean training set (64 instances) in most cases.\nD Supplementary Experimental Results\nAlso, due to space limitations, only part of the\nexperimental results are included in the main pa-\nper. In this section, we list more supplementary\nexperimental results. We visualize the clean ACC\nand the backdoor ASR in the parameter spaces,\nand ACC/ASR with different reserve ratios under\nmultiple backdoor attacks on the SST-2 sentiment\nclassification dataset and the QNLI sentence-pair\nclassification dataset. Results on sentence based\nattacks on SST-2 are reported in Fig. 5; results\non sentence based attacks on QNLI are reported\nin Fig. 6; results on word based attacks on SST-2\nare reported in Fig. 7; and results on word based\nattacks on QNLI are reported in Fig. 8.\nIn most cases, there exists an area with a high\nclean ACC and a low backdoor ASR between the\npre-trained BERT parameter and the backdoored\nparameter in the parameter space, which is a good\narea for mitigating backdoors. Under these cases,\nthe backdoor ASR will drop when ρis small, and\nbackdoors can be mitigated. Only a few cases are\nmedium or difficult, where the backdoor ASR is\nalways high, and backdoors are hard to mitigate.\n369\n0.400\n0.800\n0.990\nInitBert\nClean\nBackdoor\n0.48\n0.53\n0.58\n0.63\n0.68\n0.73\n0.78\n0.83\n0.88\n0.93\n(a) Loss Visualization, Trigger Sen-\ntence (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(b) ACC/ASR (w/o E-PUR), Trigger\nSentence (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(c) ACC/ASR (w/ E-PUR), Trigger\nSentence (SST-2).\n0.400\n0.800\n0.990\nInitBert\nClean\nBackdoor\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n(d) Loss Visualization, Trigger Sen-\ntence (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(e) ACC/ASR (w/o E-PUR), Trigger\nSentence (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(f) ACC/ASR (w/ E-PUR), Trigger\nSentence (Scratch) (SST-2).\nFigure 5: Visualization of the clean ACC and the backdoor ASR in the parameter spaces, and ACC/ASR with\ndifferent reserve ratios under multiple trigger sentence based backdoor attacks on the SST-2 sentiment classification.\n0.800\n0.990\nInit\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(a) Loss Visualization, Trigger Sen-\ntence (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(b) ACC/ASR (w/o E-PUR), Trigger\nSentence (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(c) ACC/ASR (w/ E-PUR), Trigger\nSentence (QNLI).\n0.8000.990\nInit\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(d) Loss Visualization, Trigger Sen-\ntence (Scratch) (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(e) ACC/ASR (w/o E-PUR), Trigger\nSentence (Scratch) (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(f) ACC/ASR (w/ E-PUR), Trigger\nSentence (Scratch) (QNLI).\nFigure 6: Visualization of the clean ACC and the backdoor ASR in the parameter spaces, and ACC/ASR with differ-\nent reserve ratios under multiple trigger sentence based backdoor attacks on the QNLI sentence-pair classification.\n370\n0.8000.990\nInitBert\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(a) Loss Visualization, Trigger Word\n(SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(b) ACC/ASR (w/o E-PUR), Trigger\nWord (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(c) ACC/ASR (w/ E-PUR), Trigger\nWord (SST-2).\n0.400\n0.8000.990\nInitBert\nClean\nBackdoor\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n(d) Loss Visualization, Trigger\nWord (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(e) ACC/ASR (w/o E-PUR), Trigger\nWord (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(f) ACC/ASR (w/ E-PUR), Trigger\nWord (Scratch) (SST-2).\n0.990\n0.990\nInitBert\nClean\nBackdoor\n0.48\n0.53\n0.58\n0.63\n0.68\n0.73\n0.78\n0.83\n0.88\n0.93\n(g) Loss Visualization, Trigger\nWord+EP (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(h) ACC/ASR (w/o E-PUR), Trigger\nWord+EP (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(i) ACC/ASR (w/ E-PUR), Trigger\nWord+EP (SST-2).\n0.800\n0.990\nInitBert\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(j) Loss Visualization, Trigger\nWord+ES (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(k) ACC/ASR (w/o E-PUR), Trigger\nWord+ES (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(l) ACC/ASR (w/ E-PUR), Trigger\nWord (SST-2).\n0.4000.800\n0.990\nInitBert\nClean\nBackdoor\n0.48\n0.53\n0.58\n0.63\n0.68\n0.73\n0.78\n0.83\n0.88\n0.93\n(m) Loss Visualization, Trigger\nWord+ES (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(n) ACC/ASR (w/o E-PUR), Trigger\nWord+ES (Scratch) (SST-2).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(o) ACC/ASR (w/ E-PUR), Trigger\nWord+ES (Scratch) (SST-2).\nFigure 7: Visualization of the clean ACC and the backdoor ASR in the parameter spaces, and ACC/ASR with\ndifferent reserve ratios under multiple trigger word based backdoor attacks on the SST-2 sentiment classification.\n371\n0.800\n0.990\nInit\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(a) Loss Visualization, Trigger Word\n(QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(b) ACC/ASR (w/o E-PUR), Trigger\nWord (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(c) ACC/ASR (w/ E-PUR), Trigger Word\n(QNLI).\n0.800\n0.990\nInit\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(d) Loss Visualization, Trigger Word\n(Scratch) (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(e) ACC/ASR (w/o E-PUR), Trigger\nWord (Scratch) (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(f) ACC/ASR (w/ E-PUR), Trigger Word\n(Scratch) (QNLI).\n0.800\n0.800\n0.990\nInit\nClean\nBackdoor\n0.08\n0.17\n0.26\n0.35\n0.44\n0.53\n0.62\n0.71\n0.80\n0.89\n(g) Loss Visualization, Trigger Word+EP\n(QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(h) ACC/ASR (w/o E-PUR), Trigger\nWord+EP (QNLI).\n0 0.02 0.1 0.3 0.5 0.7 0.9 1\nratio\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC/ASR\nACC\nASR\n(i) ACC/ASR (w/ E-PUR), Trigger\nWord+EP (QNLI).\nFigure 8: Visualization of the clean ACC and the backdoor ASR in the parameter spaces, and ACC/ASR with\ndifferent reserve ratios under multiple trigger word based backdoor attacks on the QNLI sentence-pair classification.\n372"
}