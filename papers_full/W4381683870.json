{
    "title": "Universal Language Model Fine-Tuning for Text Classification",
    "url": "https://openalex.org/W4381683870",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4381695185",
            "name": "R Neha Reddy",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4300996741",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W4381683870",
        "https://openalex.org/W2964054038",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2740721704"
    ],
    "abstract": "Abstract: We describe approaches that are essential for fine-tuning a language model further utilized for text categorization and propose Universal Language Model Fine-tuning (ULMFiT), an efficient transfer learning method that can be used for any NLP activity. Transfer learning methods have greatly impacted computer vision, but existing approaches in NLP still require taskspecific modifications. Universal Language Model Fine-tuning, or ULMFiT, is an architecture and transfer learning method that can be applied to NLP tasks. It involves a 3-layer architecture. Three steps make up the training process: pre-training for the general language model on a text taken from Wikipedia; fine-tuning the language model on a target task; and fine-tuning the classifier on the target task. Deep learning techniques enable computers to learn and comprehend natural language, facilitating human-machine interaction. Deep learning models are often employed in medical research, from medication candidate identification to picture analysis. On text classification tasks, our method greatly surpasses the state-of-the-art (it is the most recent model incorporating the best and latest technology), reducing the error on most datasets. Furthermore, with only a few labeled examples, it can match the performanceof training on 100Ã— more data.",
    "full_text": null
}