{
  "title": "Racial bias in AI-mediated psychiatric diagnosis and treatment: a qualitative comparison of four large language models",
  "url": "https://openalex.org/W4411024510",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2589394961",
      "name": "Ayoub Bouguettaya",
      "affiliations": [
        "Cedars-Sinai Medical Center",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2574699617",
      "name": "Elizabeth M. Stuart",
      "affiliations": [
        "Miller Children's & Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2042903919",
      "name": "Elias Aboujaoude",
      "affiliations": [
        "Stanford University",
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2589394961",
      "name": "Ayoub Bouguettaya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2574699617",
      "name": "Elizabeth M. Stuart",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042903919",
      "name": "Elias Aboujaoude",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4401358107",
    "https://openalex.org/W4390510163",
    "https://openalex.org/W4393304897",
    "https://openalex.org/W4391995913",
    "https://openalex.org/W4393397034",
    "https://openalex.org/W4398766485",
    "https://openalex.org/W4403813762",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4389948084",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4394723029",
    "https://openalex.org/W2915744129",
    "https://openalex.org/W2901182550",
    "https://openalex.org/W2084654193",
    "https://openalex.org/W4387206205",
    "https://openalex.org/W2413027338",
    "https://openalex.org/W4388733340",
    "https://openalex.org/W4409210035",
    "https://openalex.org/W4401947350",
    "https://openalex.org/W4407762318",
    "https://openalex.org/W4404783621",
    "https://openalex.org/W4395052272",
    "https://openalex.org/W2089631211",
    "https://openalex.org/W2170193407",
    "https://openalex.org/W2071568806",
    "https://openalex.org/W4400443420",
    "https://openalex.org/W4402400689",
    "https://openalex.org/W4401042897",
    "https://openalex.org/W4404355183",
    "https://openalex.org/W4402748869",
    "https://openalex.org/W4392516399",
    "https://openalex.org/W3092804897",
    "https://openalex.org/W3204343849",
    "https://openalex.org/W4394887980",
    "https://openalex.org/W4376133082",
    "https://openalex.org/W3043015873",
    "https://openalex.org/W4281567193",
    "https://openalex.org/W6600103761",
    "https://openalex.org/W4321020053",
    "https://openalex.org/W3171873561",
    "https://openalex.org/W4393119757",
    "https://openalex.org/W4402096066",
    "https://openalex.org/W4389894040",
    "https://openalex.org/W4402713323",
    "https://openalex.org/W4323920947",
    "https://openalex.org/W4380360840"
  ],
  "abstract": "Artificial intelligence (AI), particularly large language models (LLMs), is increasingly integrated into mental health care. This study examined racial bias in psychiatric diagnosis and treatment across four leading LLMs: Claude, ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant). Ten psychiatric patient cases representing five diagnoses were presented to these models under three conditions: race-neutral, race-implied, and race-explicitly stated (i.e., stating patient is African American). The models' diagnostic recommendations and treatment plans were qualitatively evaluated by a clinical psychologist and a social psychologist, who scored 120 outputs for bias by comparing responses generated under race-neutral, race-implied, and race-explicit conditions. Results indicated that LLMs often proposed inferior treatments when patient race was explicitly or implicitly indicated, though diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited the highest degree of racial bias, while Gemini showed the least. These findings underscore critical concerns about the potential for AI to perpetuate racial disparities in mental healthcare, emphasizing the necessity of rigorous bias assessment in algorithmic medical decision support systems.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01746-4\nRacial bias in AI-mediated psychiatric\ndiagnosis and treatment: a qualitative\ncomparison of four large language models\nCheck for updates\nAyoub Bouguettaya1,2, Elizabeth M. Stuart3 & Elias Aboujaoude1,4\nArtiﬁcial intelligence (AI), particularly large language models (LLMs), is increasingly integrated into\nmental health care. This study examined racial bias in psychiatric diagnosis and treatment across four\nleading LLMs: Claude, ChatGPT, Gemini, and NewMes-15 (a local, medical-focused LLaMA 3 variant).\nTen psychiatric patient cases representingﬁve diagnoses were presented to these models under three\nconditions: race-neutral, race-implied, and race-explicitly stated (i.e., stating patient is African\nAmerican). The models’diagnostic recommendations and treatment plans were qualitatively\nevaluated by a clinical psychologist and a social psychologist, who scored 120 outputs for bias by\ncomparing responses generated under race-neutral, race-implied, and race-explicit conditions.\nResults indicated that LLMs often proposed inferior treatments when patient race was explicitly or\nimplicitly indicated, though diagnostic decisions demonstrated minimal bias. NewMes-15 exhibited\nthe highest degree of racial bias, while Gemini showed the least. Theseﬁndings underscore critical\nconcerns about the potential for AI to perpetuate racial disparities in mental healthcare, emphasizing\nthe necessity of rigorous bias assessment in algorithmic medical decision support systems.\nLarge language models (LLMs), a type of artiﬁcial intelligence (AI) tool, have\nbeen heralded as a potentially powerful tool for increasing ef ﬁciency,\nbroadening access, and improvingoutcomes in mental health care1,2.G i v e n\nthe high burden of documentation, estimated to consume up to 40% of a\nprovider’st i m e\n3, LLMs that can quickly synthesize inputted data to generate\ncustom patient reports have been seen as a solution4. Because of the ability of\nLLMs to “understand”plain text and audio, this can extend to automatically\nextracting and processing symptoms and other information from a clinical\ninterview\n5,6. Additionally, by proposing diagnoses and interventions based\non information gleaned from massive medical databases, LLMs can also\noptimize treatment4. In an ideal situation, a provider can share a patient ’s\ninformation with an LLM, and, in real time, receive a detailed report that\nincludes an accurate diagnostic assessment and sensible treatment plan,\nalong with an explanation of the LLM’s reasoning and relevant references\n7.\nThe potentials and promise of LLMs in psychiatric practice are well-\ndocumented\n8,9. Recent research showing LLMs such as ChatGPT Plus\n(GPT-4) to be superior to early-career physicians suggests that the promises\nof LLMs in healthcare in general may now be within reach\n10.\nThese potential gains, however, may be overshadowed by potential\nﬂaws and risks inherent to LLMs. One risk is that LLMs might perpetuate\nor exacerbate inequities in mental health care, having shown racial bias\nin general medicine studies 11–14. For example, LLMs have been shown to\nreplicate medical biases in understanding the health of African Amer-\nicans, such as assuming thicker skin and lower lung capacity compared\nto white patients\n11. Accordingly, there is strong evidence that LLMs tend\nto have signi ﬁcantly more errors when processing mental health infor-\nmation from minority groups, and these problems are more common in\nLLMs with smaller parameters sizes\n15. This suggests that LLMs may\nharbor unfounded assumptions when it comes to mental health as well,\nreplicating existing biases in psychiatric diagnosis and treatment in\nminorities. For example, in African American patients, LLMs may\nreplicate past tendencies in the medical ﬁeld to underdiagnose condi-\ntions such as depression\n16 and anxiety 17 and over-diagnose conditions\nsuch as schizophrenia 18,19, or to generally suggest less effective 20 or riskier\ntreatments21, partly as a function of race. LLMs might even learn and\nreplicate stigmatizing language found in EHR mental health care notes 22.\nAs mental health is a “high stakes”domain that is de ﬁned partially by the\n“fuzziness” of symptoms 8, and high-quality evidence has found that\nLLMs tend to be more racially biased in diagnosing mental health\nconditions over any other health conditions\n23, it is likely that LLMs\ntrained on biased data may perpetuate racism unless speci ﬁc counter-\nmeasures are implemented.\n1Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, CA, USA.2School of Nursing and Midwifery, Monash University, Melbourne, VIC,\nAustralia. 3Jonathan Jaques Children’s Cancer Institute, Miller Children’s & Women’s Hospital Long Beach, Long Beach, CA, USA.4Department of Psychiatry and\nBehavioral Sciences, Stanford University, Stanford, CA, USA. e-mail: elias.aboujaoude@cshs.org\nnpj Digital Medicine|           (2025) 8:332 1\n1234567890():,;\n1234567890():,;\nEvenimpliedrace may trigger a biased output by LLMs, since they have\nbeen found to respond differently when the requests use a dialect typically\nused by African Americans (sometimes referred to as African American\nVernacular English, or AAVE)\n24. Therefore, there is concern that if LLMs\nare interpreting the transcript from a c linical interview where the patient\nuses AAVE, the LLM ’s output might be biased. A similar concern exists if\nthe LLM assumes patients’race based on their name, as some research on\nLLMs guessing race based on usernames has suggested25,26. Recent research\nhas also demonstrated that ChatGPT-4 can use cues from patient prompts\nto detect race in mental healthcare, and provides lower empathy responses\nto black participants\n27. Considering the well-docu mented susceptibility to\nracial bias in LLMs and their rapid uptake in clinical care 28, this study aims\nto examine four popular LLMs in psychiatry, a medical ﬁeld prone to\nbias16–19,29–31 by providing them with real cases to diagnose and treat, adding\nimplicit or explicit racial information for comparison. We benchmark\nmultiple LLMs, including three commercially available generalist LLMs\n(Gemini, ChatGPT, and Claude) and one “local” one (NewMes-v15) run-\nning entirely on a local computer (i.e., not requiring online resources).\nResults\nSystematic variations were observed in output based on the presence of\nracial characteristics in patient reports. When averaging the scores across all\nLLMs and conditions (for diagnosis and treatment), the mean bias score for\nthe explicit and implicit conditions was 1.93 (95% CI [1.91–2.14]; SD = 0.97;\nMedian = 2) and 1.37 (95% CI[1.36 –1.38]; SD = 0.91: Median = 1),\nrespectively, indicating a higher likelihood of bias in the presence of explicit\nracial information. Among the psychiatric conditions examined, responses\nto schizophrenia cases demonstrated the highest likelihood of bias (driven\nby treatment bias), while depression demonstrated the lowest.\nDiagnostic assessments showed rela tive consistency across different\nLLMs and case presentations, with mean bias scores remaining at or below\n1.5 for most conditions. The notable exception was schizophrenia, where\nbias scores often exceeded 1.5 on a verage for some LLMs (Claude and\nNewMes), though diagnostic reasoning patterns remained largely con-\nsistent. This suggests that whi le racial characteristics in ﬂuenced model\noutputs, their impact on the “thought” process behind the diagnosis was\nlimited. Figure 1 shows a heatmap of the average bias scores for diagnosis\n(both for implied and explicitly stated race).\nTreatment recommendations demonstrated more pronounced bias.\nModels frequently proposed divergent treatment approaches when racial\ncharacteristics were present, either e xplicitly or implicitly. This was most\nevident in schizophrenia and anxiety cases, where the majority of models\nreceived bias scores of 2.0 or above fortreatment recommendations. Overall,\nseveral concerning issues emerged for all LLMs in treatment recommen-\ndations. For instance, Gemini demonstrated increased focus on reducing\nalcohol use in anxiety cases only when itwas explicitly stated that the patient\nwas African American. Claude suggested guardianship for depression cases\nwith explicit racial characteristics, but not in the neutral or implicit condi-\ntions. ChatGPT emphasized substa nce use as a potential problem in a\np a t i e n tw i t ha ne a t i n gd i s o r d e rw h e nt h er a c ew a se x p l i c i t l ys t a t e d ,b u tn o ti n\nthe neutral condition. Both ChatGPT and NewMes omitted medication\nrecommendations for an ADHD case when racial characteristics were\nexplicitly stated but suggested them when those racial characteristics were\nmissing. Figure2 shows a heatmap of the average bias scores for treatment\n(both for implied and explicitly stated race).\nAmong LLMs, the NewMes-v15 model showed the highest suscept-\nibility to bias, receiving the maximum bias score of 3.0 for treatment\nrecommendations more frequently t h a na n yo t h e rm o d e l .C o n v e r s e l y ,\nGemini demonstrated the lowest overall bias scores across conditions.\nStatistical analysis con ﬁrmed general differences between LLMs in\ntheir ratings overall. A Kruskal –Wallis H-test revealed a statistically sig-\nniﬁcant effect of LLM on ratings ( H(3) = 16.65, P = 0.00083). The overall\neffect size was medium (η² = 0.088). The a priori power for this test was high\n(0.93 for detecting the pattern of differences speciﬁed by the assumed shifts\nwhere the smallest non-zero shift ( 0.3) represents a small-to-medium\nstandardized effect. The overal le f f e c ts i z eo b s e r v e dw a sm e d i u m\n(η² = 0.088). Gemini received the lowestmean bias rank (60.08), followed by\nClaude (75.47), ChatGPT (87.97), and NewMes (98.47). Post-hoc Dunn ’s\ntests with Bonferroni correction (adjusted α = 0.0083) conﬁrmed speciﬁc\ndifferences. Gemini was rated as signi ﬁcantly less biased than ChatGPT\n(mean rank diff. = -27.9, P = 0.005). Gemini was also rated signi\nﬁcantly less\nbiased than NewMes (mean rank diff. = -38.4, P = 0.00011). No other\npairwise rating difference s were statistically signi ﬁcant. However, these\nresults should be considered in lin e with the limitations of comparing\nordinal scores based on qualitative judgments; these comparisons are based\non 40 ratings per LLMs, which may not b e enough for statistical inference.\nHowever, as this is a qualitative judgment, smaller sample sizes are relatively\nacceptable, especially as bias is, to some extent, a subjective judgment based\non experience.\nDiscussion\nT oo u rk n o w l e d g e ,t h i si st h eﬁrst evaluation of racial bias across multiple\npsychiatric diagnoses and multiple LLMs, including generalist LLMs and a\nlocally trained medical-speciﬁc LLM. Our study is also the ﬁrst to compare\ncases that include both explicit and imp licit racial characteristics, which is\nimportant considering that LLMs can p ick up on inferred characteristics\nFig. 1 | Heatmap of diagnosis for each LLM and\ncondition. For each illness and each model, they\nwere provided with an implicit version and explicit\nversion of race included. Red correlates with a more\nbiased response relative to the neutral condition.\nhttps://doi.org/10.1038/s41746-025-01746-4 Article\nnpj Digital Medicine|           (2025) 8:332 2\nthat humans do not notice 25,32, and that even minor changes in the way\ninformation is presented to an LLM can affect accuracy and bias33.W ef o u n d\nmost LLMs exhibited some form of bias when dealing with African\nAmerican patients, at times recommending dramatically different treat-\nments for the same psychiatric illness and otherwise same patient.\nThe observed variations raise signi ﬁcant concerns about the integra-\ntion of AI in psychiatric care and should prompt reconsideration of whether\ncurrent AI systems can maintain neutra lity and display minimal bias. The\nheightened bias in treatment recommendations suggests that AI systems\nmay amplify existing biases in psychiatric care\n21,e v e nw h e nt h ed i a g n o s i si s\naccurate. Theseﬁndings are aligned with recent research in non-psychiatric\ndisciplines suggesting“white”treatment bias by LLMs14,34, and underscore a\nfundamental challenge in AI develop ment: While LLMs can process vast\namounts of medical information, they can also propagate and exacerbate\nbiases embedded in their training data\n35. Our results with a locally trained\nmedical LLM suggest that even specialized, annotated training data may not\nfully mitigate against these biases and, inf a c t ,m a yb em o r el i k e l yt or e p l i c a t e\nthem. While evidence36 suggests that local LLMs can perform nearly as well\nas online versions that use high performance servers and have high com-\nputing demands on synthetic benchmarks that examine issues such as\nhallucinations and accuracy, our research suggests otherwise for problems\nlike racial bias. Given growing interest by healthcare institutions, clinicians,\nand researchers in local LLMs that do not require the internet and are less\nexpensive and more secure, and given the push toward local LLMs by major\ntechnology companies (e.g., Meta\n37), this higher likelihood of racial bias in\nlocal LLMs may represent a signi ﬁcant challenge.\nOur results are consistent wit h preliminary data from the ﬁelds of\nnephrology, pulmonology, dermatology, and cardiology11,38–41.W h i l em o s t\nstudies focus on accuracy rather thanbias, a fundamental issue appears to be\nthat LLMs can be extremely convincing 42,43,e s p e c i a l l yw h e nt h e ym a k eu p\nsupporting evidence in response to medical questions 44.I nt h ep s y c h i a t r i c\narena, where diagnoses do not typically rely on “objective” laboratory,\nimaging, or genetic tests 45, and where symptoms can be “soft” and non-\nspeciﬁc46, the ability for LLMs to deliver convincing, yet racist, treatment\nadvice to a time-pressured provider based on subtle cues can be particularly\nproblematic.\nSeveral limitations warrant consideration in interpreting these results.\nFirst, the rapid pace of AI development means that our ﬁndings represent a\nsnapshot of performance that may qui ckly become outdated. This is espe-\ncially true considering the increasing size of LLMs; our largest LLMs have\nbeen estimated to have over 175 billion parameters (Gemini and ChatGPT-\n4), and our smallest LLM (NewMes, based on LLaMA 3) only had 8 billion.\nMeanwhile, at time of writing, it is likely that ChatGPT-4o is at least 1.5\ntrillion parameters, which is several orders of magnitude larger than our\nsmallest model used in this study. However, the fundamental medical\ndataset used in training (i.e., a medical dataset that is likely itself biased due\nto lack of representation of minority groups\n47) is unlikely to evolve rapidly\nbetween updates, suggesting that new models may still have the same\nproblem. Even if parameter sizes are inversely related to racism or bias\nassessments (as noted in other research\n15), smaller LLMs, reﬁned LLMs, or\nquantized LLMs, will likely continue to encounter issues with implied race.\nIn our study, our re ﬁned model based on a smaller LLM performed the\nworst, but it is unclear why this is the c ase, considering that it was the most\nspeciﬁcally trained on medical data. Fur thermore, research using much\nlarger LLMs has shown that even explicitly unbiased LLMs still form racist\nassociations, and smaller LLMs had fewer biases 48, so it is possible that\nmodel size is not the driver behind why we had more bias in our local LLM.\nSecond, our methodology for removi ng racial characteristics from\nneutral cases may not have been sufﬁciently comprehensive to eliminate all\nracial cues. Although we tried to mitigate against this by inputting the cases\ninto an LLM beforehand, it is still possible that the LLM was behaving as\nthough there was a racial element, without our knowledge. Research has\nshown that LLMs will capture implicit information that a human might not\nnotice\n25,32. Therefore, the difference between the neutral and implicit or\nexplicit conditions could have been an assumption on our part. It is also\npossible that the cases we chose to use, from an open-source dataset, may\nhave been embedded within the LLM ’s training material, so the LLM may\nhave been able to “guess” t h er a c ei naw a yt h a tw ed i dn o ta n t i c i p a t e ,\nalthough we did attempt to address this through asking LLaMA 405B to\nguess the race, which it could not.\nThird, the qualitative nature of bias assessment introduces potential\nsubjectivity, and some observed variations might be attributed to random\nﬂuctuations rather than systematic bias. Similarly, we assessed only one\noutput for each of the two cases per condition, meaning that we did not\nexamine how consistent the bias may have been, and our results may over or\nunderestimate bias. Other researchers may conduct identical research and\nﬁnd different results, which is to be expected. The outputs of LLMs will often\nshow differences even if the information is repeated (even at very low\ntemperatures, as is the case in our study ), but given the relative consistency\nof treatment bias we observed across LLMs even with our choice to only\nprompt once, and the fact that we found bias when race was only implied,\nour results are unlikely to be due to random LLM variation. Furthermore,\no t h e rr e s e a r c hh a sf o u n de v e ni fr a c i a lb i a si sd eﬁned and tested in different\nways, many of the same LLMs we assessed in this study will still maintain\nconsistent levels of racial bias with repeated trials\n38, further supporting our\nﬁndings. Still, theﬁeld would beneﬁt from assessing more than two cases per\nFig. 2 | Heatmap of treatment for each LLM by\ncondition. For each illness and each model, they\nwere provided with an implicit version and explicit\nversion of race included. Red correlates with a more\nbiased response relative to the neutral condition.\nhttps://doi.org/10.1038/s41746-025-01746-4 Article\nnpj Digital Medicine|           (2025) 8:332 3\ncondition, and instead focus on just one condition to enhance both practical\nconsiderations (resource requirements) and empirical quality. We would\nalso suggest that future researchers c hose health conditions that either\nimplicitly or explicitly suggest an ethnicity — ones that are much more\ncommonly diagnosed in certain ethnic groups in certain areas (e.g., alcohol\nuse disorder in Native Americans, although is due to racialized medicine)\nand compare these results to whenthe LLM is told about the patient’sa c t u a l\nethnicity, as that may be a valuable and deeper examination of race.\nT h ev a r i a t i o nw ef o u n da c r o s sL L M ss u g g e s t st h a th o wt h e s em o d e l s\nare trained has a signiﬁcant impact on their likelihood of showing racial bias.\nTraditional bias mitigation strategies that are standard practice, such as\nadversarial training\n49,e x p l a i n a b l eA Im e t h o d s50,d a t aa u g m e n t a t i o n51,a n d\nresampling52, may not be enough. Future research should prioritize the\ndevelopment and validation of transparent mitigation strategies throughout\nthe AI development lifecycle. Among other measures, this would entail\ninvestigating methods for detecting and quantifying bias in training data,\ndeveloping more robust model architectures resistant to demographic bias,\nestablishing standardized protocols for clinical bias testing, and building\ntools that correct biased clinical output in real-time. Some developers and\nresearchers are already attempting tobuild systems using these methods to\nreduce bias in AI outputs, with some success\n53.\nUltimately, success in integratin g AI in psychiatric care will partly\ndepend on addressing documented racial and other biases. Our ﬁndings\nserve as a call to action for stakeholders across the healthcare AI ecosystem\nto help ensure that these technologie s enhance health equity rather than\nreproduce or exacerbate existing inequi ties. This will require close colla-\nboration between researchers, clinicians, healthcare institutions, and pol-\nicymakers to establish and maintain robust standards for AI adoption. It will\nalso require training healthcare professionals to be more critical of AI\noutputs and increasing awareness of his torical biases that inevitably will\nmake it into AI outputs. Until then, su ch systems should be deployed with\ncaution and consideration for how even subtle racial characteristics may be\naffecting AI “judgment.”\nMethods\nAn ethics exemption was obtained under code STUDY00003831 from\nCedars-Sinai IRB, as the cases used w ere drawn from a publicly available\ndatabase54 with no consent needed. We used the Strengthening the\nReporting of Observational Studies in Epidemiology’s( S T R O B E )r e p o r t i n g\nguidelines; the checklist is included in the Supplementary Note.\nCase selection and curation\nWe randomly selected 10 cases from PubMed Central ’s dataset, which\nmentioned the followingﬁve psychiatric illness types (broadly deﬁned; two\nper diagnosis54): depression, anxiety, schizophrenia, eating disorders, and\nattention-deﬁcit and hyperactivity disorder (ADHD). These diagnoses were\nselected due to historical evidence of clinical bias in how they are approa-\nched in African Americans patients\n16–19,29–31. Further explanation of the\nrationale for each diagnosis is included in Supplementary Data no 1. Weﬁrst\nﬁltered the cases by inclusion of the keyword related to the illness, then used\na simple random sorting algorithm in Microsoft Excel to select two cases.\nThe cases were then altered to remove actual diagnosis and details that could\nimply race, such as name, geographic location, and explicit racial char-\nacteristics such as skin color. To ensure that racial cues were removed, these\n“neutral”cases were given to a local LLM (LLaMA3.1 405B) with the prompt\n“can you guess the race of the patient, and if so, what makes you think that?”\nFor all 10 cases, LLaMA 3.1 405B was unable to detect the patient ’s race.\nThese cases and the standard prompt are detailed in Supplementary Data\nno 1.\nLLM selection\nFour LLMs were selected for this study, representing the largest, highest\nperforming, and most dominant existing LLMs as of August 5\nth 2024 on\nLivebench55. All models were set at a temperature of 0.2, setting top P at 1.0,\nleaving top K as standard, with a maximum output token limit of 2048 for\nc o n s i s t e n c y ,w h e r ep o s s i b l ei nt h e i rA P I .T h ef o u rL L M sw e r eG o o g l e’s\nGemini 1.5 Pro (estimated 200B parameters, August 1, 2024 version 56),\nClaude 3.5 Sonnet (estimated 175B, June 20, 2024 version 57), ChatGPT-4o\n(estimated 180B, May 13, 2024 version)58, and NewMes-v15, a“local,”freely\navailable, version of Meta’s LLaMA 3 8b LLM that was the top-ranked LLM\non the Open Medical-LLM Leaderboard 59. Local LLMs are versions that\nhave been “cut down”for size and efﬁciency36. They are increasingly sought\nafter by health care systems for cost, privacy and customizability reasons60.\nLocal models such as this version of NewMes-v15 are developed by\ndownloading an existing LLM (Meta’s LLaMA 3 8b), then further training it\non a wide variety of open medical datasets. This modiﬁes the architecture of\nthe model to be more specialized for use, in addition to its original cap-\nabilities as a conversational LLM. Un like the other three LLMs, NewMes-\nv15 ran completely locally on a high-performance computer with 24 GB of\nVRAM without internet access. Although other local models\n33 were con-\nsidered, due to its performance on many of the metrics on the leaderboard in\nsynthetic tests most similar to what this study aimed to do, NewMes-v15 was\nchosen.\nInput of cases\nUsing a new chat session each time, we provided each LLM with the same\nprompt requesting a diagnosis and treatment (Supplementary Data no. 1).\nFor each diagnosis, the LLM was presented with three conditions: The\n“neutral”condition, which involved a case with no reference to the patient’s\nrace; the“implicit”condition, which included implicit race details in the form\nof a patient name linked in population research studies with African\nAmericans\n61;a n dt h e“explicit”condition, which speciﬁed that the patient was\nAfrican American and included the same patient name as the implicit con-\ndition. This resulted in 120 responses (four LLMs, 10 cases, three versions).\nAnalysis\nResponses were scored by an American social psychologist (denoted as\nRater 1) and an American licensed clinical neuropsychologist (denoted as\nRater 2), and any disagreements were resolved by an American licensed\npsychiatrist who examined the scoring and notes to arrive at a ﬁnal score.\nRater 1 is a social psychologist with a PhD in social health psychology, 5\nyears of experience working in racial bias in older adult care, and nine years\nof experience working in social healt h psychology. Rater 2 is a clinical\nneuropsychologist with ﬁve years of experience delivery healthcare, and\nleads American Psychological Association initiatives in diversity, equity and\ni n c l u s i o ni nn e u r o p s y c h o l o g y .T h el i c ensed psychiatrist is a medical director\nof a very large university afﬁliated health system, with experience in clinical,\nresearch, administrative, and technology industry settings. The raters were\nblinded to the LLM that they were assessing in their ﬁrst assessment. The\nresponses in the explicit and impli cit condition were compared to the\nneutral response and scored on a sc ale from 0 to 3 for diagnosis and\ntreatment (separately) as follows: 0 for minimal difference in responses (e.g.,\nsimple rephrasing); 1 for some difference in responses that could be\nattributed to normal variation (e.g., p roviding a different but valid treat-\nment, such as another medication in th e same antidepressant class); 2 for\nsigniﬁcant difference in responses that could potentially be attributed to race\n(e.g., recommending a different medication class or different diagnostic\nreasoning); and 3 for evidence of a racist response (e.g., adding an\nassumption of alcoholism). The raters were told not to assess the outputs on\naccuracy. Only the difference in the LLM ’s response across the three con-\nditions was assessed, and both rater 1 and 2 left notes summarizing their\nreasoning for their ratings.\nThe initial agreement between the two raters was very high, with most\ndisagreements differing by one scale point only; κ = 0.949, SE = 0.025,\nκw = 0.977. In Supplementary Data, we h ave provided the LLM responses\n(Supplementary Data no .2), each raters’notes after resolution, and theﬁnal\nratings (Supplementary Data nos. 3–7). Supplementary Data no. 8 provides\nsummary statistics, Supplementary Data no. 9 provides the full scores for\nanalysis (separated by implicit/explicit and condition), and Supplementary\nDate no. 10 provides the scores for kappa calculation.\nhttps://doi.org/10.1038/s41746-025-01746-4 Article\nnpj Digital Medicine|           (2025) 8:332 4\nPrior to assigning ratings for this study, to validate the 0 –3b i a s\nassessment scale, two independent raters were trained on the scoring criteria\nusing the supplementary data from Omiye et al.\n11. Both raters were provided\nthe responses for pain thresholds in that study for ChatGPT-4 and Claude\nfor each of the ﬁve runs. Each rater then independently scored these 10\nresponses based on our scale. Inter-rater reliability was calculated using\nCohen’s Kappa, ﬁnding that interrater agree ment for the pilot was high\nat κ = 0.91.\nWe completed all analyses in SPSS Version 27 and Microsoft Excel.\nConﬁdence intervals for the mean bias score were derived from a\nt-distribution.\nData availability\nData is provided under supplemental data. OSF provides Supplement data\nas well. https://osf.io/qgpvy/.\nReceived: 3 December 2024; Accepted: 22 May 2025;\nReferences\n1. Torous, J. & Greenberg, W. Large language models and artiﬁcial\nintelligence in psychiatry medical education: augmenting but not\nreplacing best practices.Acad. Psychiatry1–3. https://doi.org/10.\n1007/s40596-024-01996-6 (2024).\n2. Barile, J. et al. Diagnostic accuracy of a large language model in\npediatric case studies.JAMA Pediatr.178, 313–315 (2024).\n3. Medscape. Physican Compensation Report. https://www.medscape.\ncom/sites/public/physician-comp/2023 (2023).\n4. Omar Sr, M. et al. Applications of large language models in psychiatry:\na systematic review. medRxiv.https://doi.org/10.1101/2024.03.28.\n24305027 (2024).\n5. Itauma, O. & Itauma, I. AI Scribes: boosting physician efﬁciency in\nclinical documentation.Int. J. Bioinform. Biosci.14,0 9–18 (2024).\n6. Tierney, A. A. et al. Ambient artiﬁcial intelligence scribes to alleviate\nthe burden of clinical documentation.NEJM Catal. Innov. Care Deliv.\n5, CAT. 23.0404 (2024).\n7. Saab, K. et al. Capabilities of gemini models in medicine.\narXiv:2404.18416 (2024).\n8. Stade, E. C. et al. Large language models could change the future of\nbehavioral healthcare: a proposal for responsible development and\nevaluation. NPJ Ment. Health Res.3, 12 (2024).\n9. Obradovich, N. et al. Opportunities and risks of large language models\nin psychiatry.NPP—Digit. Psychiatry Neurosci.2, 8 (2024).\n10. Goh, E. et al. Large language model inﬂuence on diagnostic\nreasoning: a randomized clinical trial.JAMA Netw. Open7, e2440969\n(2024).\n11. Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R.\nLarge language models propagate race-based medicine.NPJ Digit.\nMed. 6, 195 (2023).\n12. Khera, R., Simon, M. A. & Ross, J. S. Automation bias and assistive AI:\nrisk of harm from AI-driven clinical decision support.JAMA 330,\n2255–2257 (2023).\n13. Zack T, Lehman E, Suzgun M, Rodriguez JA, Celi LA, Gichoya J,\nJurafsky D, Szolovits P, Bates DW, Abdulnour RE, Butte AJ.\nAssessing the potential of GPT-4 to perpetuate racial and gender\nbiases in health care: a model evaluation study.The Lancet Digital\nHealth. 6, e12–22 (2024).\n14. Ayoub, N. F. et al. Inherent bias in large language models: a random\nsampling analysis.Mayo Clin. Proc.: Digital Health2, 186–191 (2024).\n15. Wang, Y. et al. Unveiling and mitigating bias in mental health analysis\nwith large language models. arXiv preprint arXiv:2406.12033 (2024).\n16. Bailey, R. K., Mokonogho, J. & Kumar, A. Racial and ethnic differences\nin depression: current perspectives.Neuropsychiatr. Dis. Treat.\n15,\n603–609 (2019).\n17. Vanderminden, J. & Esala, J. J. Beyond symptoms: race and gender\npredict anxiety disorder diagnosis.Soc. Ment. Health9, 111–125\n(2019).\n18. Anglin, D. M. & Malaspina, D. Ethnicity effects on clinical diagnoses in\npatients with psychosis: comparisons to best estimate research\ndiagnoses. J. Clin. Psychiatry69, 941 (2008).\n19. Anglin, D. M. & Malaspina, D. Ethnicity effects on clinical diagnoses\ncompared to best-estimate research diagnoses in patients with\npsychosis: a retrospective medical chart review.J. Clin. Psychiatry69,\n941–945 (2008).\n20. Pesa, J., Liu, Z., Fu, A. Z., Campbell, A. K. & Grucza, R. Racial\ndisparities in utilization ofﬁrst-generation versus second-generation\nlong-acting injectable antipsychotics in Medicaid beneﬁciaries with\nschizophrenia. Schizophr. Res.261, 170–177 (2023).\n21. Chung, H., Mahler, J. C. & Kakuma, T. Racial differences in treatment\nof psychiatric inpatients.Psychiatr. Serv. (Washington, DC)46,\n586–591 (1995).\n22. De Choudhury, M., Pendse, S. R. & Kumar, N. Beneﬁts and harms of\nlarge language models in digital mental health. arXiv preprint\narXiv:2311.14693 (2023).\n23. Omar, M. et al. Sociodemographic biases in medical decision making\nby large language models.Nat Med. 1-9.https://doi.org/10.1038/\ns41591-025-03626-6 (2025).\n24. Hofmann, V., Kalluri, P. R., Jurafsky, D. & King, S. AI generates\ncovertly racist decisions about people based on their dialect.Nature\n1–8, https://doi.org/10.1038/s41586-024-07856-5 (2024).\n25. Xu, C. et al. Do llms implicitly exhibit user discrimination in\nrecommendation? an empirical study. arXiv preprint\narXiv:2311.07054 (2023).\n26. Bai, X., Wang, A., Sucholutsky, I. & Grifﬁths, T. L. Explicitly unbiased\nlarge language models still form biased associations.Proc. Natl.\nAcad. Sci.122, 8 (2025).\n27. Gabriel, S., Puri, I., Xu, X., Malgaroli, M. & Ghassemi, M. Can AI relate:\ntesting large language model response for mental health support.\narXiv preprint arXiv:2405.12021 (2024).\n28. Meng, X. et al. The application of large language models in medicine: a\nscoping review.Iscience 27, 109713 (2024).\n29. Gordon, K. H., Perez, M. & Joiner, T. E. Jr The impact of racial\nstereotypes on eating disorder recognition.Int. J. Eat. Disord.32,\n219–224 (2002).\n30. Morgan, P. L., Hillemeier, M. M., Farkas, G. & Maczuga, S. Racial/\nethnic disparities in ADHD diagnosis by kindergarten entry.J. Child\nPsychol. Psychiatry55, 905–913 (2014).\n31. Gordon, K. H., Brattole, M. M., Wingate, L. R. & Joiner, T. E. Jr The\nimpact of client race on clinician detection of eating disorders.Behav.\nTher. 37, 319–325 (2006).\n32. Etgar, S., Oestreicher-Singer, G. & Yahav, I. Implicit bias in LLMs: bias\nin ﬁnancial advice based on implied gender. Available atSSRN (2024).\n33. Zhou, H. et al. A survey of large language models in medicine:\nprogress, application, and challenge. arXiv preprint arXiv:2311.05112\n(2023).\n34. Yang, Y., Liu, X., Jin, Q., Huang, F. & Lu, Z. Unmasking and quantifying\nracial bias of large language models in medical report generation.\nCommun. Med.4, 176 (2024).\n35. Liu, Y., Gautam, S., Ma, J. & Lakkaraju, H. Confronting LLMs with\ntraditional ML: Rethinking the fairness of large language models in\ntabular classiﬁcations. InProc. 2024 conference of the North\nAmerican chapter of the Association for Computational Linguistics:\nHuman Language Technologies(eds Duh, K et al.) Vol. 1:Long papers\n3603–3620 (Association for Computational Linguistics, 2024).\n36. Kurtic, l., Marque, A., Pandit, S., Kurtz, M. & Alistarh, D. Give me BF16\nor give me death? Accuracy-performance trade-offs in LLM\nquantization. arXiv preprint.https://doi.org/10.48550/arXiv.2411.\n02355 (2024).\nhttps://doi.org/10.1038/s41746-025-01746-4 Article\nnpj Digital Medicine|           (2025) 8:332 5\n37. Meta. Introducing Quantized Llama Models with Increased Speed and\na Reduced Memory Footprint. https://ai.meta.com/blog/meta-llama-\nquantized-lightweight-models/ (2024).\n38. Pfohl, S. R. et al. A toolbox for surfacing health equity harms and\nbiases in large language models.Nat. Med.1–11. https://doi.org/10.\n1038/s41591-024-03258-2 (2024).\n39. Fliorent, R. et al. Artiﬁcial intelligence in dermatology: advancements\nand challenges in skin of color.Int. J. Dermatol.63, 455–461 (2024).\n40. Chase, A. C. Ethics of AI: perpetuating racial inequalities in healthcare\ndelivery and patient outcomes.Voices Bioeth.6, https://doi.org/10.\n7916/vib.v6i.5890 (2020).\n41. Doshi, H., Chudow, J., Ferrick, K. & Krumerman, A. Machine learning\nin atrialﬁbrillation— racial bias and a call for caution.J. Med. Artif.\nIntell. 4, https://doi.org/10.21037/jmai-21-12 (2021).\n42. Carrasco-Farre, C. Large language models are as persuasive as\nhumans, but why? About the cognitive effort and moral-emotional\nlanguage of LLM arguments. arXiv:2404.09329 (2024).\n43. Palmer, A. & Spirling, A. Large language models can argue in\nconvincing ways about politics, but humans dislike AI authors:\nimplications for governance.Political Sci.75, 281–291 (2023).\n44. Gravel, J., D’Amours-Gravel, M. & Osmanlliu, E. Learning to fake it:\nlimited responses and fabricated references provided by ChatGPT for\nmedical questions.Mayo Clin. Proc.: Digit. Health1, 226–234 (2023).\n45. Yatham, L. N. Biomarkers for clinical use in psychiatry: where are we\nand will we ever get there?.World Psychiatry22, 263 (2023).\n46. Mølstrøm, I.-M., Henriksen, M. G. & Nordgaard, J. Differential-\ndiagnostic confusion and non-speciﬁcity of affective symptoms and\nanxiety: an empirical study ofﬁrst-admission patients.Psychiatry Res.\n291, 113302 (2020).\n47. Seker, E., Talburt, J. R. & Greer, M. L. Preprocessing to Address Bias in\nHealthcare Data. InChallenges of Trustable AI and Added-Value on\nHealth (eds Séroussi B et al.) Vol. 294, 327–331 (2022).\n48. Bai, X., Wang, A., Sucholutsky, I. & Grifﬁths, T. L. Explicitly unbiased\nlarge language models still form biased associations.Proc. Natl Acad.\nSci. USA122, e2416228122 (2025).\n49. Li, X., Cui, Z., Wu, Y., Gu, L. & Harada, T. Estimating and improving\nfairness with adversarial learning. arXiv preprint arXiv:2103.04243 (2021).\n50. Wang, Y.-C., Chen, T.-C. T. & Chiu, M.-C. An improved explainable\nartiﬁcial intelligence tool in healthcare for hospital recommendation.\nHealthc. Anal.3, 100147 (2023).\n51. Chlap, P. et al. A review of medical image data augmentation\ntechniques for deep learning applications.J. Med. Imaging Radiat.\nOncol. 65, 545–563 (2021).\n52. Chen, F., Wang, L., Hong, J., Jiang, J. & Zhou, L. Unmasking bias in\nartiﬁcial intelligence: a systematic review of bias detection and\nmitigation strategies in electronic health record-based models.J. Am.\nMed. Inform. Assoc.31, 1172–1183 (2024).\n53. Alamoodi, A. et al. A novel evaluation framework for medical LLMs:\ncombining fuzzy logic and MCDM for medical relation and clinical\nconcept extraction.J. Med. Syst.48,1 –12 (2024).\n54. Zhao, Z., Jin, Q., Chen, F., Peng, T. & Yu, S. Pmc-patients: a large-\nscale dataset of patient summaries and relations for benchmarking\nretrieval-based clinical decision support systems. arXiv preprint\narXiv:2202.13876 (2022).\n55. White, C. et al. Livebench: a challenging, contamination-free LLM\nbenchmark. arXiv preprint arXiv:2406.19314 (2024).\n56. Google. Release Updates. https://gemini.google.com/updates\n(2024).\n57. Anthropic. Claude 3.5 Sonnethttps://www.anthropic.com/news/\nclaude-3-5-sonnet (2024).\n58. Open AI. ChatGPT—Release Notes. https://help.openai.com/en/\narticles/6825453-chatgpt-release-notes#h_3cb68a1d55 (2024).\n59. Ura, A., Minervini, P. & Fourrier, C.The Open Medical-LLM\nLeaderboard: Benchmarking Large Language Models in Healthcare.\nhttps://huggingface.co/blog/leaderboard-medicalllm (2024).\n60. Wiest, I. C. et al. Privacy-preserving large language models for\nstructured medical information retrieval.NPJ Digit. Med.7, 257 (2024).\n61. Crabtree, C. et al. Validated names for experimental studies on race\nand ethnicity.Sci. Data10, 130 (2023).\nAcknowledgements\nThis study received no funding.\nAuthor contributions\nA.B.: Conceptualization, data curation, formal analysis, investigation,\nmethodology, project administration, validation, visualization (ﬁgures),\nwriting— original draft, writing— review and editing; E.M.S.: Formal analysis,\nvalidation, writing— review and editing; E.A.: Conceptualization,\ninvestigation, supervision, project administration, resources, validation,\ninvestigation, writing— review and editing.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01746-4\n.\nCorrespondenceand requests for materials should be addressed to\nElias Aboujaoude.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01746-4 Article\nnpj Digital Medicine|           (2025) 8:332 6",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.5659332275390625
    },
    {
      "name": "Racial bias",
      "score": 0.4304075539112091
    },
    {
      "name": "Clinical psychology",
      "score": 0.40114426612854004
    },
    {
      "name": "Psychiatry",
      "score": 0.39237913489341736
    },
    {
      "name": "Psychotherapist",
      "score": 0.3215305209159851
    },
    {
      "name": "Racism",
      "score": 0.16386038064956665
    },
    {
      "name": "Sociology",
      "score": 0.1366998255252838
    },
    {
      "name": "Gender studies",
      "score": 0.0
    }
  ]
}