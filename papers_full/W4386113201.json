{
  "title": "Automatic coronary artery segmentation of CCTA images using UNet with a local contextual transformer",
  "url": "https://openalex.org/W4386113201",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116151264",
      "name": "Qianjin Wang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2105191770",
      "name": "Lisheng Xu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2099626634",
      "name": "Lu Wang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2112592894",
      "name": "Xiaofan Yang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": [
        "General Hospital of Shenyang Military Region",
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2421743937",
      "name": "Benqiang Yang",
      "affiliations": [
        "General Hospital of Shenyang Military Region"
      ]
    },
    {
      "id": "https://openalex.org/A4202361842",
      "name": "Stephen E Greenwald",
      "affiliations": [
        "Queen Mary University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2116151264",
      "name": "Qianjin Wang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2105191770",
      "name": "Lisheng Xu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2099626634",
      "name": "Lu Wang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2112592894",
      "name": "Xiaofan Yang",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": [
        "Liaoning University",
        "General Hospital of Shenyang Military Region",
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2421743937",
      "name": "Benqiang Yang",
      "affiliations": [
        "Liaoning University",
        "General Hospital of Shenyang Military Region"
      ]
    },
    {
      "id": "https://openalex.org/A4202361842",
      "name": "Stephen E Greenwald",
      "affiliations": [
        "Queen Mary University of London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2967041024",
    "https://openalex.org/W2074347739",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W4283714330",
    "https://openalex.org/W6795472130",
    "https://openalex.org/W2568191378",
    "https://openalex.org/W6791637211",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6777751218",
    "https://openalex.org/W2436565690",
    "https://openalex.org/W2899279931",
    "https://openalex.org/W2219094525",
    "https://openalex.org/W2082304218",
    "https://openalex.org/W2997528981",
    "https://openalex.org/W2119454881",
    "https://openalex.org/W6684665197",
    "https://openalex.org/W2621042378",
    "https://openalex.org/W3010058213",
    "https://openalex.org/W2168005337",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3109852921",
    "https://openalex.org/W3183430956",
    "https://openalex.org/W2031548389",
    "https://openalex.org/W3093175272",
    "https://openalex.org/W3023474822",
    "https://openalex.org/W2026788950",
    "https://openalex.org/W2126471865",
    "https://openalex.org/W6718240422",
    "https://openalex.org/W3035016310",
    "https://openalex.org/W3179218951",
    "https://openalex.org/W6840908765",
    "https://openalex.org/W2997876081",
    "https://openalex.org/W2086593400",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W3012712295",
    "https://openalex.org/W4224244419",
    "https://openalex.org/W2798237365",
    "https://openalex.org/W3156613007",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309158112",
    "https://openalex.org/W6803821885",
    "https://openalex.org/W6766165048",
    "https://openalex.org/W6802829601",
    "https://openalex.org/W3081435712",
    "https://openalex.org/W3133170446",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W3006169930",
    "https://openalex.org/W4236965008",
    "https://openalex.org/W3164981218",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3212933375",
    "https://openalex.org/W4225932685",
    "https://openalex.org/W3116112801",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2150871663",
    "https://openalex.org/W2990368613"
  ],
  "abstract": "Coronary artery segmentation is an essential procedure in the computer-aided diagnosis of coronary artery disease. It aims to identify and segment the regions of interest in the coronary circulation for further processing and diagnosis. Currently, automatic segmentation of coronary arteries is often unreliable because of their small size and poor distribution of contrast medium, as well as the problems that lead to over-segmentation or omission. To improve the performance of convolutional-neural-network (CNN) based coronary artery segmentation, we propose a novel automatic method, DR-LCT-UNet, with two innovative components: the Dense Residual (DR) module and the Local Contextual Transformer (LCT) module. The DR module aims to preserve unobtrusive features through dense residual connections, while the LCT module is an improved Transformer that focuses on local contextual information, so that coronary artery-related information can be better exploited. The LCT and DR modules are effectively integrated into the skip connections and encoder-decoder of the 3D segmentation network, respectively. Experiments on our CorArtTS2020 dataset show that the dice similarity coefficient (DSC), Recall, and Precision of the proposed method reached 85.8%, 86.3% and 85.8%, respectively, outperforming 3D-UNet (taken as the reference among the 6 other chosen comparison methods), by 2.1%, 1.9%, and 2.1%.",
  "full_text": "Automatic coronary artery\nsegmentation of CCTA images\nusing UNet with a local contextual\ntransformer\nQianjin Wang1, Lisheng Xu2, Lu Wang1*, Xiaofan Yang1, Yu Sun2,3,4,\nBenqiang Yang3,4 and Stephen E. Greenwald5*\n1School of Computer Science and Engineering, Northeastern University, Shenyang, China,2College of\nMedicine and Biological and Information Engineering, Northeastern University, Shenyang, China,\n3Department of Radiology, General Hospital of Northern Theater Command, Shenyang, China,4Key\nLaboratory of Cardiovascular Imaging and Research of Liaoning Province, Shenyang, China,5Blizard\nInstitute, Barts and the London School of Medicine and Dentistry, Queen Mary University of London,\nLondon, United Kingdom\nCoronary artery segmentation is an essential procedure in the computer-aided\ndiagnosis of coronary artery disease. It aims to identify and segment the regions of\ninterest in the coronary circulation for further processing and diagnosis. Currently,\nautomatic segmentation of coronary arteries is often unreliable because of their\nsmall size and poor distribution of contrast medium, as well as the problems that\nlead to over-segmentation or omission. To improve the performance of\nconvolutional-neural-network (CNN) based coronary artery segmentation, we\npropose a novel automatic method, DR-LCT-UNet, with two innovative\ncomponents: the Dense Residual (DR) module and the Local Contextual\nTransformer (LCT) module. The DR module aims to preserve unobtrusive\nfeatures through dense residual connections, while the LCT module is an\nimproved Transformer that focuses on local contextual information, so that\ncoronary artery-related information can be better exploited. The LCT and DR\nmodules are effectively integrated into the skip connections and encoder-\ndecoder of the 3D segmentation network, respectively. Experiments on our\nCorArtTS2020 dataset show that the dice similarity coefﬁcient (DSC), Recall,\nand Precision of the proposed method reached 85.8%, 86.3% and 85.8%,\nrespectively, outperforming 3D-UNet (taken as the reference among the\n6 other chosen comparison methods), by 2.1%, 1.9%, and 2.1%.\nKEYWORDS\ncoronary artery segmentation, 3D-Unet, local contextual transformer, dense residual\nconnection, convolutional neural network\n1 Introduction\nCardiovascular disease is a major cause of death worldwide and its most common\nmanifestation is coronary artery disease (CAD) (Jayaraj et al., 2019). Early diagnosis of CAD,\nespecially coronary artery stenosis and atherosclerosis, is essential for subsequent treatment.\nAs a non-invasive screening method, Computed Tomography Angiography (CTA) has been\nwidely used for this purpose (Raff, 2007). However, coronary CTA (CCTA) images have the\ntypical shortcomings of medical images, such as unbalanced foreground-background\ndistribution, small targets, and unstable image quality (Kroft et al., 2007). This instability\nOPEN ACCESS\nEDITED BY\nLinwei Wang,\nRochester Institute of Technology (RIT),\nUnited States\nREVIEWED BY\nDaniel Baum,\nZuse Institute Berlin, Germany\nAlireza Gholipour,\nMassachusetts General Hospital and\nHarvard Medical School, United States\n*CORRESPONDENCE\nLu Wang,\nwanglu@cse.neu.edu.cn\nStephen E. Greenwald,\ns.e.greenwald@qmul.ac.uk\nRECEIVED 05 January 2023\nACCEPTED 01 August 2023\nPUBLISHED 22 August 2023\nCITATION\nWang Q, Xu L, Wang L, Yang X, Sun Y,\nYang B and Greenwald SE (2023),\nAutomatic coronary artery segmentation\nof CCTA images using UNet with a local\ncontextual transformer.\nFront. Physiol. 14:1138257.\ndoi: 10.3389/fphys.2023.1138257\nCOPYRIGHT\n© 2023 Wang, Xu, Wang, Yang, Sun, Yang\nand Greenwald. This is an open-access\narticle distributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or\nreproduction in other forums is\npermitted, provided the original author(s)\nand the copyright owner(s) are credited\nand that the original publication in this\njournal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nFrontiers inPhysiology frontiersin.org01\nTYPE Original Research\nPUBLISHED 22 August 2023\nDOI 10.3389/fphys.2023.1138257\nresults from differences in scanning equipment and variations in\npatient motion during scanning, which affect the consistency of\nimage quality. Radiologists can manually assess the site of stenosis\nand plaque in coronary arteries, but this is not only time-consuming\nbut also prone to misdiagnosis and omission (Ghekiere et al., 2017).\nFurthermore, clinical workforce resources are limited, so there is a\ndrive to employ computers to help physicians analyze coronary\nartery images. Segmentation of coronary arteries in these images is a\nprerequisite for automating the diagnosis and analysis these tissues.\n(Mihalef et al., 2011 ; Tesche et al., 2018 ). Given the current\ndifﬁculties in automated diagnosis of CCTA images, there is a\nneed to develop more effective methods for segmenting the\ncoronary arteries contained therein.\nIn previous research, traditional methods have achieved some\nnotable success in theﬁeld of vessel segmentation. These methods\ninclude techniques based on image processing, morphological\noperations, and traditional machine learning algorithms. More\nthan 10 years ago,Lesage et al. (2009) provided further insights\ninto vessel segmentation approaches, which do not involve deep\nlearning. These include the use of region-based methods, edge\ndetection, and active contour models, among others.Orujov et al.\n(2020) proposed a contour detection algorithm for retinal blood\nvessels using Mamdani (Type-2) fuzzy rules; the method enhanced\ncontrast with contrast-limited adaptive histogram equalization,\nremoved noise using a medianﬁlter, calculated image gradients,\nand classiﬁed pixels as edges based on fuzzy rules considering\ngradient magnitude and direction, ultimately obtaining\nsegmentation of the blood vessels.Yang et al. (2020)proposed an\nimproved multi-scale enhancement method based on Frangi\nﬁltering to enhance the contrast between vessels and other\nobjects in the image, and used an improved level set model to\nsegment vessels from both the enhanced and original grayscale\nimages. Cheng et al. (2015) applied thresholding and\nmorphological operations to preprocessed images, obtaining an\ninitial outline of blood vessels, which were then segmented using\nan active contour framework based on a B-snake model with a\nconstraint force to prevent leakage into adjacent structures.Kerkeni\net al. (2016) proposed a multiscale region growing (MSRG)\ntechnique for segmenting coronary arteries in 2D X-ray\nangiography images, beginning with image enhancement using a\nmulti-scale vascularityﬁlter and a contrast enhancement technique,\nfollowed by identifying initial seed points by thresholding and\nmanually selecting points with a high density of vascularity, and\nﬁnally employing an iterative region growing approach to obtain the\nsegmentation. Although these methods have to some extent helped\naddress vessel segmentation tasks, they still have shortcomings, such\nas sensitivity to noise, dependency on manual intervention, and\ndifﬁculty in handling complex vessel structures or poor contrast\nimages.\nRecently, deep learning methods have performed extremely well\nin the segmentation of medical images and have been shown to\nsigniﬁcantly outperform traditional methods in accuracy. Artiﬁcial\nintelligence has also found extensive applications in cardiothoracic\nﬁelds, particularly in diagnostic imaging (Sharma et al., 2020). UNet\n(Ronneberger et al., 2015) is a classical network in the ﬁeld of\nbiomedical image segmentation and has become a benchmark in this\ndomain (Liu et al., 2020). The network has a U-shaped structure\nconsisting of an encoder, a decoder, and skip connections, which\nallow it to acquire both spatial and semantic information\nsimultaneously. 3D-UNet (Çiçek et al., 2016), as an extension of\nUNet, is used for 3D image segmentation. The input is a volume\ninstead of a slice so that interslice information can be exploited, and\nthe convolution operation is changed from 2D to 3D accordingly. A\ntypical 3D-UNet consists of four stages for both the encoder and the\ndecoder. VNet (Milletari et al., 2016), which has also been proposed\nfor processing 3D medical images, is similar to 3D-UNet in terms of\nnetwork structure. The differences are that it uses convolution\noperations instead of pooling operations for upsampling and\ndownsampling, and it also introduces residual connections in\nboth the encoder and decoder.\nDue to its excellent performance, many studies have employed\n3D-UNet as a baseline network and improved upon it. As to the\nencoding and decoding path, some variants of 3D-UNet add residual\nconnections to the convolution and deconvolution operations in the\nencoding and decoding stages (Lee et al., 2017; Qamar et al., 2020).\nFurthermore, some variants of 3D-UNet introduce dense\nconnections between theﬁne and coarse feature maps to improve\nthe transfer of feature information (Li et al., 2018; Bui et al., 2019\n;\nZhang Y et al., 2020; Pan et al., 2021). Song et al. (2022)incorporated\ndense blocks into the encoder for effective feature extraction and\napplied residual blocks to the decoder for feature recti ﬁcation.\nSeveral works have introduced attention mechanisms into UNet\n(Islam et al., 2020; Jin et al., 2020; Li et al., 2021). For example,\nchannel attention (Li et al., 2021) and spatial attention (Islam et al.,\n2020) have been added to the decoder. Spatial attention focuses\nmore on the target region, while channel attention estimates the\nimportance of individual features. However, accurate segmentation\nof medical images requires rich contextual information to resolve\nambiguities, and these methods do not make effective use of such\ninformation.\nThe Transformer model (Vaswani et al., 2017) proposed in the\nNatural Language Processingﬁeld has fundamentally changed the\nway that machines work with text data. Inspired by this, many recent\nstudies have adapted the Transformer model for computer vision\napplications. For instance, Vision Transformer (Dosovitskiy et al.,\n2021) divides images intoﬁxed-size patches, and these patches are\nregarded as words and fed into the Transformer for image\nrecognition. Related works that utilize the Transformer for\nmedical image segmentation have also performed well. VT-UNet\n(Peiris et al., 2022) uses window-based Transformers as encoders\nand decoders to construct a U-shaped network for 3D Tumor\nSegmentation. UNETR ( Hatamizadeh et al., 2022 ) applies the\noriginal Transformers as encoders in a U-shaped network to\nlearn the input representation and capture global multi-scale\ninformation, while the decoders remain as traditional\nconvolutional modules. UCTransNet ( Wang et al., 2022a )\nintroduces the channel Transformer to replace the skip\nconnection of U-Net for more effective encoder-decoder feature\nconnection and hence more accurate segmentation of medical\nimages. AFTer-UNet (Yan et al., 2022) replaces the convolution\nwith a Transformer in the last layer of the UNet. MT-UNet (Wang\net al., 2022b) proposes a mixed Transformer and embeds it into the\ndeeper layers of UNet. The mixed Transformerﬁrst calculates self-\nafﬁnities using an efﬁcient local-global self-attention mechanism\nand then exploits the relations between data samples with an\nexternal attention.\nFrontiers inPhysiology frontiersin.org02\nWang et al. 10.3389/fphys.2023.1138257\nBesides the improvements in UNet and the introduction of the\nattention mechanism, several other network architectures have been\nemployed for coronary artery segmentation. Lei Y et al. (2020)\nintroduced an improved 3D attention into a fully convolution\nnetwork (FCN) to automatically segment the coronary arteries in\nCCTA images.Tian et al. (2021)used VNet for initial segmentation\nand then used region growing to further segment the image, thus\nobtaining complete and smooth-edged coronary arteries.Gao et al.\n(2021) conducted coronary centerline extraction and lumen\nsegmentation jointly on CCTA images to address the breakage\nissue, employing a Graph Convolutional Network (GCN) for the\nsegmentation of the coronary lumen. Some studies use speciﬁc\nfeatures of coronary vessels for segmentation. For instance,Kong\net al. (2020)focused more on the anatomical structure of coronary\narteries and proposed incorporating a tree-structured convolutional\ngated recurrent unit into the fully convolutional neural network.Ma\net al. (2020)are more concerned with the continuity of the vessels\nand used a novel region growing method to segment coronary\narteries, which considers a variable sector search area within each\nregion. Wolterink et al. (2019)focused more on tubular surfaces and\nemployed graph convolutional networks to forecast the spatial\ncoordinates of vertices within a tubular surface mesh, thus\nsegmenting the lumen of the coronary artery. Other studies have\nadopted a two-stage framework to achieve coronary artery\nsegmentation in CCTA images. For instance, in the ﬁrst stage,\ncardiac segmentation is performed, followed by slicing the\ncardiac region and segmenting the coronary arteries within the\nlocal sliced region. This procedure can alleviate the foreground-\nbackground imbalance problem (Dong et al., 2022). Wang et al.\n(2022c) adopted a similar approach in which theﬁrst stage involves a\nrough segmentation of the 3D image and in the second, the\nsegmentation network is fed with the original 2D images and the\n2D images resulting from slicing the 3D segmentation.\nHowever, these existing methods still have shortcomings and we\naim at tackling some of them in this work. First, as a proportion of\nthe coronary arteries are of small diameter and thus appear as very\nthin lines in images, simply increasing the number of convolutional\nlayers in the encoding or decoding blocks of UNet would not\nimprove the segmentation accuracy, because the information in\nthe shallow-layer features, which is necessary for segmenting details,\nmay be lost when the convolution operation goes deeper. Although\nthe traditional residual module (He et al., 2016) can complement the\nshallow-layer information, it is not sufﬁcient for coronary artery\nsegmentation (as demonstrated in Table 4 in the results and\ndiscussion section). Existing research has explored the idea of\ncombining residual learning and dense connections to enhance\nfeature extraction and fusion capabilities in 2D image recognition\ntasks (Zhang Z et al., 2020; Zhang et al., 2021). However, these\napproaches are tailored for speciﬁc tasks and datasets, and directly\napplying them to 3D-UNet could result in a large network due to its\ndense concatenation. Therefore, there is a need to adapt this concept\nand we have consequently proposed a module speciﬁcally designed\nfor 3D coronary artery image segmentation. Second, with similar\nHounsﬁeld Unit (HU) values, the feature representations of\ncoronary arteries in the inner layers of a CNN network are likely\nto be similar to those of other blood vessels such as veins and the\nascending aorta. To deal with this, the attention mechanism can be\nused to enhance the weighting of the coronary regions. However,\ntraditional self-attention computes an attention matrix based on\nisolated query-key pairs, which may focus more on segmenting the\nmain part of coronary artery and ignore the ends and regions with\nlow concentrations of contrast medium. There has been research on\ntransformers that focus on local context information (Li et al., 2022),\nand this has been used for 2D image recognition. However, this\napproach does not simultaneously extract local context information\nfor Q and V, which may limit its feature representation ability. There\nis a need for a module suitable for 3D-UNet networks for\nsegmentation tasks and to improve the attention mechanism to\nbetter capture the local context information of Q and V. This will\nenhance the feature representation ability of the network.\nTherefore, we aim to extract and fuse a greater number of deep\nand shallow features than the residual module. To this end, we\npropose the Dense Residual (DR) module, which is continuously\nsupplemented with preceding convolution features during the\nconvolution process, thus improving the encoding and decoding\nblock of UNet. Then, aiming to concentrate more on the local\ncharacteristics of the coronary arteries and reduce the noise\ninformation from other organs, we propose the Local Contextual\nTransformer (LCT) module, which focuses more on the local\ncontextual information by obtaining an attention matrix based\non query and contextual-information-enhanced key pairs. In\nparticular, we apply the LCT module after each encoding block\nto provide more informative features to the decoding block, instead\nof simply using the skip connection, so that the decoding procedure\ncan focus more on the region’s neighboring the coronary arteries.\nUsing these modules, we have conducted extensive experiments on\nour CorArtTS2020 dataset and compared the results to the most\nwidely used image segmentation method 3D-UNet and six other\nsegmentation networks commonly used in coronary artery\nsegmentation studies. The code of the proposed method is\navailable at https://github.com/qianjinmingliang/Coronary-Artery-\nsegmentation-with-LCTUnet.\n2 Materials and methods\n2.1 Dataset\nThe dataset used for the experiment (CorArtTS 2020) was\nprovided by the General Hospital of the Northern Theater\nCommand in China. It is a modiﬁed version of the one used in\nour previous work (Song A. et al.) and was acquired using a Philips\niCT 256 Scanner, running a 120 kVp protocol. Each slice had a width\nand height of 512 pixels, and the interval between adjacent slices was\n0.45 mm. Each case consisted of between 310 and 390 slices. As\nshown inTable 1, the CorArtTS2020 dataset consists of 81 cases, of\nwhich the numbers of normal subjects and patients were 40 and 41,\nrespectively. The data were randomly divided into training,\nvalidation, and test sets in the ratio of 6:1:3, respectively.\nTABLE 1 The CorArtTS2020 dataset.\nTraining set Validation set Testing set\nNormal subjects 24 cases 4 cases 12 cases\nPatients 25 cases 4 cases 12 cases\nFrontiers inPhysiology frontiersin.org03\nWang et al. 10.3389/fphys.2023.1138257\nThe annotation process, summarized inFigure 1, was performed\nby three experienced radiologists from the same hospital. Initially,\ndata were acquired from the radiology department of the hospital,\nand the radiologists underwent training to familiarize themselves\nwith the anatomical features and distribution of coronary arteries in\nCCTA images (Data Acquisition and Preparation). Utilizing\nspecialized medical image processing software, Mimics\n(Materialise), they carefully annotated the visible contours and\nbranches of the right coronary artery, the left coronary artery,\nand their branches using the coronal, sagittal, and axial planes\n(Data Annotation). Upon completion of the annotation process,\nanother experienced cardiovascular imaging radiologist from the\nsame hospital reviewed the annotations (Review). If inaccuracies\nwere found, they were corrected under the guidance of the reviewing\nradiologist (Correction of Annotations). This iterative and\ncollaborative review process helped to ensure the accuracy of the\nﬁnal labels. Finally, a last check was made to conﬁrm the correctness\nof all annotations (Final Veriﬁcation).\nFigure 2A provides an example of the annotation process\nconducted on the Mimics medical image processing software\ninterface, with the annotated regions of the coronary arteries\nhighlighted in yellow. Correspondingly, Figure 2B displays the\noriginal CCTA images prior to the annotation process.\nThe CCTA data require pre-processing before being fed into the\nsegmentation network, because different tissues have different\nradio-densities, giving rise to a wide range of HU values.\nHighlighting the coronary arteries can improve the segmentation\nresult. However, there is no clear deﬁnition of the exact range of HU\nvalues for coronary arteries (Marquering et al., 2005; Liu et al., 2013).\nFor our dataset, we therefore conservatively limit the range of HU\nvalues in the CCTA data to be within the interval [−260,760] HU,\nunder the guidance of the physicians, and we note that it may not be\ngeneralizable to other medical imaging modalities. The result of the\ndata pre-processing is shown inFigure 3. It is notable that the pre-\nprocessing effectively removes irrelevant tissues and some noise, as\nshown in the green box, while making the coronary arteries (red\narrows) more distinct. To ensure fair comparison we also used the\npre-processed data for all the other comparison methods (Milletari\net al., 2016; Çiçek et al., 2016; Lee et al., 2017; Li et al., 2018; Islam\net al., 2020; Wang et al., 2022a; Hatamizadeh et al., 2022).\n2.2 Structure of the DR-lct-unet\nThe proposed network structure for coronary artery segmentation\nis based on the 3D-UNet, to which we have made three modiﬁcations.\nFirstly, the LCT module, which is a novel Transformer-style attention\nmodule, is developed to bridge the gap between the features of the\nencoding and decoding stages before combining them. Secondly, the\nDR module, which is a mix of residual and dense connections of\nconvolutions, is developed to extract multi-level features for both the\nencoding and decoding stages. Thirdly, deep supervision is exploited to\nfacilitate the training process of the network. The architecture of the\nproposed DR-LCT-UNet is shown schematically inFigure 4.\nSpeciﬁcally, in the encoding process, the pre-processed image is\nfed into the network, and its size is 1 × 16 × 512 × 512, where 1 is the\nchannel size, 16 is the thickness (i.e., the number of slices) of the\ninput volume, and the height and width are 512. There are four\nlayers in the encoding stage, in each of theﬁrst three layers, the\nfeatures areﬁrst extracted and then downsampled, while the fourth\nlayer only performs feature extraction. In order to extract rich\nfeature representations for the coronary arteries, the DR module\nis used in the encoding path, as it is able to extract deeper features\nwhile retaining more detailed ones than traditional convolution.\nFor the decoding process, as the decoding features are quite\ndifferent from the encoding features after several sampling and\nFIGURE 1\nOutline of the annotation process.\nFIGURE 2\nIllustration of the annotation process(A) and original CCTA images(B).\nFrontiers inPhysiology frontiersin.org04\nWang et al. 10.3389/fphys.2023.1138257\nconvolution operations, the LCT module is used before performing\ndecoding to ﬁll the semantic gap between the features from the\nencoding and decoding stages of the same resolution level.\nConsequently, the input to each decoding layer consists of three\nparts, i.e., the features from the LCT module of the same level, the\nfeatures from the encoding layers and the features from the previous\ndecoding layers. These features are ﬁrst concatenated and then\ndecoded by the proposed DR module in each decoding layer.\nFinally, we use a deep supervision strategy (Lee et al., 2015)i n\nthe training process to prevent the gradient from disappearing in the\nearly stage of training. To be speciﬁc, the SoftMax function is applied\nat the end of each decoding layer to obtain the feature map used for\ndeep supervision. To compute the segmentation loss for deep\nsupervision, each feature map is upsampled to the same size as\nthe input and then the Dice loss is calculated based on the similarity\nbetween the feature map and the ground truth.\nFIGURE 3\nComparison of a CCTA image before(A) and after pre-processing(B).\nFIGURE 4\nSchematic of the architecture of the proposed DR-LCT-UNet network.\nFrontiers inPhysiology frontiersin.org05\nWang et al. 10.3389/fphys.2023.1138257\n2.3 Structure of the LCT module\nIn CCTA images, as coronary arteries are smaller compared with\nnearby structures, and the appearance of coronary arteries and\ncoronary veins is similar, it is necessary to exploit more local\ncontextual information for accurate coronary artery segmentation.\nSelf-attention (Vaswani et al., 2017) computes an attention\nmatrix based on isolated query-key pairs, as is shown in\nFigure 5A. Q, K,a n dV are obtained by 1 × 1 convolution, which\nonly uses the information of each individual location without\nconsidering any neighbourhood information. Such operation limits\nthe visual feature representation ability of the resulting embeddings.\nTo deal with this, we propose a local contextual Transformer (LCT)\nmodule, the structure of which is shown inFigure 5B).\nSpeciﬁcally, for an inputX of sizeH × W × D × C (H, W, D and C\nare respectively the height, width, thickness and the number of\nchannels), it is ﬁrst transformed into queries (Q), keys (K), and\nvalues (V) using embedding matricesW\nq, Wk, andWv, respectively.\nThis transformation is represented byQ = XWq, K = XWk, and\nV = XWv. Instead of using 1 × 1 × 1 convolution to encode each key\nand value as in traditional self-attention, the LCT module uses\nk × k × k group convolution over all the neighbouring keys and\nvalues within ak × k × k grid to take advantage of local contextual\ninformation. That is, the matricesW\nk and Wv are set tok × k × k in\nsize, while the matrixWq is maintained at 1 × 1 × 1 to retain the\ninformation of each location inQ.\nThen, the contextualized keys K are concatenated with the\nqueries Q. This combined information is then fed into a 1 × 1 ×\n1 convolution with the ReLU activation function to obtain the\nattention matrix R\ng1∈RH✕W✕D✕C, thereby learning a feature that\nintegrates local context information with global information.\nAfter that, in a manner similar to traditional self-attention, the\nvalues of V, which contains the local context information, are\nmultiplied element-by-element with the attention matrix Rg1 to\nobtain Rg2∈RH✕W✕D✕C:\nRg2 /equals Rg1 ⊗ V (1)\nFinally, a softmax function is applied toRg2 to yield the output of\nthe LCT module.\nIn general, the proposed LCT makes use of the local contextual\ninformation to enhance the effectiveness of the self-attention\ncalculation, and it can thus adaptively put emphasis on the more\nrelevant regions of the coronary arteries for segmentation. In our\nimplementation, k is set to 3 and the optimality of this setting was\nexperimentally validated (seeTable 7).\n2.4 Structure of the DR module\nTraditional residual connection is proposed to solve the\ndegradation problem of deep neural networks. Its structure is\nshown in Figure 6A, and consists of two consecutive convolution\noperations and a residual connection. The residual connection is\nimplemented by adding up the features before and after the\nconvolutions. The mathematical description of the original\nresidual connection is\nY /equals X + Η\n2 X() (2)\nwhere X is the input feature, H( X) denotes the convolution\noperation on X followed by a ReLU operation, and accordingly,\nHk(X) denotes k successive convolution operations on X, each\nfollowed by the ReLU operation. Although the residual\nFIGURE 5\n(A) Schematic of the architecture of the traditional self-attention model;(B) Schematic of the proposed LCT module.\nFIGURE 6\n(A) Schematic of the architecture of the traditional Residual Module;(B) Description of the architecture of the proposed Dense Residual Module.\nFrontiers inPhysiology frontiersin.org06\nWang et al. 10.3389/fphys.2023.1138257\nconnection has the effect of preserving the original features, some\ncoronary artery regions in the images are not clear and the\ncorresponding features are not obvious due to the prevalence of\nnarrow areas, such as their distal ends, stenotic regions, and areas\nwith uneven distribution of contrast. Such information may easily\nget lost during the convolution operation. Therefore, we need to\npreserve more of the information which might subsequently be lost\nduring the convolution process. To this end, we propose a dense\nresidual (DR) module, as shown inFigure 6B).\nSpeciﬁcally, the DR module has two residual connections which\nwork synergistically to fuse the multi-level features from successive\nconvolution operations. Theﬁrst residual connection adds the input\nfeatures to the features obtained after the second convolution\noperation. After that, one more convolution operation is used to\nextract additional deeper features. The second residual connection\nthen sums all the previous feature maps, i.e., the input features and\nthe feature maps generated by each of the convolution operations.\nThus, the DR module is able to retain features at different\nconvolution levels and extract rich features without information\nloss. In this way, the features extracted by the DR module can more\ncompletely represent the characteristics of coronary arteries. The DR\nmodule is deﬁned by the equation:\nY /equals H X + H\n2 X()[] + X + H X() + H2 X() (3)\nWith the DR module, features in regions with narrow vessels\nand along low contrast boundaries are enhanced, making more\naccurate coronary artery segmentation possible.\n2.5 Loss function\nAs coronary arteries are of small diameter in comparison with\nnearby tissues such as the heart, ascending aorta, and the pulmonary\nartery, the coronary artery segmentation task suffers greatly from the\nforeground-background imbalance problem. Dice Loss was\nproposed in 2017 to deal with the imbalance problem in\nsegmentation (Ghekiere et al., 2017 ). It is well suited to the\ndemands of this study, and has therefore been employed to train\nour network. The computation of Dice loss is based on the Dice\nsimilarity coefﬁcient (DSC), which measures the overlap between\ntwo samples, producing results in the range [0,1], i.e., a higher DSC\nvalue indicates a higher degree of overlap. The DSC is deﬁned by Eq.\n8, and Dice Loss is computed as.\nDice Loss/equals 1 − DSC (4 )\n2.6 Deep supervision\nFor deep supervision, a separate loss is calculated for each\ndecoding layer, which also plays the role of regularization. This\nstrategy, known as deep supervision, leverages the intermediate\noutputs of the decoding process to guide the training, helping to\nmitigate the vanishing gradient problem and leading to more\ndiscriminative features being learned at all levels. These\nintermediate losses provide additional guidance to the learning\nprocess, which often results in faster convergence.\nThe loss function used for deep supervision is deﬁned in Eq.5,\nwhere L\nk denotes the loss at the decoding layer of depthk, and the\nDice Loss is deﬁned by Eq.4. As the output of theﬁrst decoding layer\nhas the greatest effect on the performance of the network, we set\nsmaller weights for the losses of the other decoding layers, i.e.,α < 1.\nThe weightα for deep supervision is also gradually decreased during\nthe training process so that at the end of the training the loss reﬂects\nthe segmentation quality of the last decoding layer.\nL /equals L1 + α L2 + L3 + L4() (5)\n3 Experiments and results\n3.1 Experimental settings\nAll the experiments were carried out on a GeForce RTX\n3090 GPU. The experimental environment was Pytorch 1.7 and\nthe same training process was used for the proposed network and the\nother compared methods. The input was a volume of size 16 × 512 ×\n512. The Adam optimizer which uses adaptive moment estimation\nto speed up convergence was employed to update the network\nparameters. Due to GPU memory limitations, we chose a batch\nsize of 3 to avoid out-of-memory errors. The parameter settings for\nthe training process are shown inTable 2.\n3.2 Evaluation metrics\nWe appliedﬁve commonly used evaluation metrics, i.e., the Dice\nsimilarity coefﬁcient (DSC), Recall, Precision, Average Symmetric\nSurface Distance (ASSD), and Hausdorff Distance (HD), to evaluate\nthe effectiveness of the different methods (Kirişli et al., 2013). DSC\ndescribes the similarity between two samples. Recall is the ratio of\nthe number of correctly predicted positive voxels to the actual\nnumber of positive voxels. Precision is the proportion of\ncorrectly predicted positive voxels to all the voxels predicted to\nTABLE 2 Parameter settings for the training process.\nParameters Values\nBatch size 3\nEpochs 180\nLearning rate (0< epochs< 100) 10 –5\nLearning rate (100≤ epochs< 160) 10 –6\nLearning rate (160≤ epochs≤ 180) 10 –7\nweight decay factor 5 × 10 −4\nα (0< epochs< 40) 1\nα (40≤ epochs< 80) 0.8\nα (80≤ epochs< 120) 0.8 2\nα (120≤ epochs< 160) 0.8 3\nα (160≤ epochs≤ 180) 0.8 4\nFrontiers inPhysiology frontiersin.org07\nWang et al. 10.3389/fphys.2023.1138257\nbe positive. ASSD describes the average surface distance between\ntwo samples. HD describes the maximum distance from a point in\nthe label to a nearest point in the predicted image. The ﬁve\nevaluation metrics were computed according to the following\nexpressions:\nRecall /equals\nTP\nTP + FN (6)\nPrecision /equals TP\nTP + FP (7)\nDSC /equals 2TP\n2TP + FN + FP (8)\nASSD /equals\n∑\nST P+FP()\ndST P + FP() ,S T P+ FN()[] + ∑\nST P+FN()\ndST P + FN() ,S T P+ FP()[]\nST P + FP()|| + ST P + FN()||\n(9)\nHD /equals max max\na∈ST P+FP()\nmin\nb∈ST P+FN()\na − b∥∥{} , max\nb∈ST P+FN()\nmin\na∈ST P+FP()\nb − a∥∥{}()\n(10)\nwhere TP (True Positives) represents samples correctly identiﬁed as\ncoronary arteries;FN (False Negatives) denotes samples predicted to\nbe background, but which actually belong to coronary arteries;FP\n(False Positives) indicates samples predicted to be coronary arteries,\nbut which actually belong to the background.S (TP + FN) is the set\nof actual surface voxels of the coronary arteries, andS (TP + FP)i s\nthe set of predicted surface voxels of the coronary arteries. d\n[sample\n1, sample2] refers to the shortest distance fromsample1 to\nsample2. The values of DSC, Recall, and Precision are all in the range\nof [0,1], and larger values indicate better performance; while for\nASSD and HD, smaller values are better.\n3.3 Experimental results and discussion\n3.3.1 Comparison of the different segmentation\nnetworks\nTo assess the quality of the proposed network structure, we have\nreproduced and retrained some classical and state-of-art methods\ncommonly used for medical image segmentation from scratch. It is\nnoteworthy that our model’s ﬁnal scores on the test set are not\ndependent on a single run. Instead, they are computed as the average\nresults from multiple runs, thus enhancing the robustness and\nstability of our model and preventing the results from being\ninﬂuenced by a speciﬁc initialization of the model.\nA comparison of the proposed DR-LCT-UNet with the other\nnetworks is shown inTables 3, 4. The proposed network achieves\nbetter results than the baseline 3D-UNet in terms of all ﬁve\nevaluation metrics. Speci ﬁcally, compared with the 3D-UNet,\nDR-LCT-UNet improves DSC by 2.1%, Recall by 1.9%, Precision\nby 2.1%, reduces ASSD by 0.188, and reduces HD by 1.861. DR-\nLCT-UNet also outperforms other networks in terms of DSC, Recall,\nTABLE 3 Comparison of segmentation results between various methods (Optimal value for each evaluation metric is shown in bold. In the top row of this and\nsubsequent tables, up arrows indicate that an increase in the evaluation metric implies better performance, the reverse being the case for the down arrows).\nMethod DSC↑ Recall↑ Precision↑ ASSD↓ HD↓\n3D-UNet (Çiçek et al., 2016) 0.837 0.844 0.837 0.613 30.076\nVNet (Milletari et al., 2016) 0.837 0.810 0.872 0.538 33.519\nResUNet (Lee et al., 2017) 0.841 0.832 0.882 0.533 29.054\nDenseUNet (Li et al., 2018) 0.839 0.826 0.859 0.514 37.413\nAttUNet (Islam et al., 2020) 0.843 0.835 0.857 0.506 32.642\nUNETR (Hatamizadeh et al., 2022) 0.827 0.784 0.884 0.551 44.071\nUCTransNet (Wang et al., 2022a) 0.818 0.821 0.813 1.205 59.128\nDR-LCT-UNet (Ours) 0.858 0.863 0.858 0.425 28.215\nTABLE 4 Comparison of the number of parameters and inference time for the different methods.\nMethod Parameters (M) Inference Time (s/case)\n3D-UNet (Çiçek et al., 2016) 8.61 17.21\nVNet (Milletari et al., 2016) 16.80 18.65\nResUNet (Lee et al., 2017) 9.50 18.50\nDenseUNet (Li et al., 2018) 18.10 16.35\nAttUNet (Islam et al., 2020) 8.65 17.23\nUNETR (Hatamizadeh et al., 2022) 92.58 20.65\nUCTransNet (Wang et al., 2022a) 65.60 19.55\nDR-LCT-UNet (Ours) 10.70 18.60\nFrontiers inPhysiology frontiersin.org08\nWang et al. 10.3389/fphys.2023.1138257\nASSD, and HD. These results can be seen inFigure 8which shows\nthe ﬁnal 3D reconstruction of the segmented coronary arterial tree.\nThe improvements in the various evaluation metrics achieved by\nthe proposed DR-LCT-UNet indicate its superiority in the task of\ncoronary artery segmentation. The improvement of Recall indicates\nthat more coronary arteries are correctly segmented, the\nimprovement of ASSD indicates that the segmentation result\ndiffers less from the ground truth, and the improvement of DSC\nindicates that the overall segmentation is better and closer to the\nground truth label. Although the Precision of the networks ResNet,\nVNet and UNETR, is higher than that achieved by the network\nproposed here, indicating that they have fewer background voxels\nmistakenly segmented as coronary arteries, their Recall and DSC\nscores are much lower than the proposed network, meaning that\ntheir segmentation results miss more coronary artery voxels.\n3.3.2 Ablation experiments\nFirst, to demonstrate the performance improvement associated\nwith each proposed module of our DR-LCT-UNet, we carried out an\nablation study, the results of which are shown inTable 5. It can be seen\nthat, compared with UNet, both the proposed LCT and DR modules\nconsistently improve theﬁve evaluation metrics. Speciﬁcally, the LCT\nmodule markedly improves the Precision (3.2%), while the DR\nmodule substantially improves the Recall (1.9%).\nThe ablation study results demonstrate the effectiveness of the\nindividual LCT and DR modules in enhancing the segmentation\nperformance. The LCT module contributes to a marked\nimprovement in Precision, while the DR module has a\nconsiderable impact on Recall. By combining the advantages of\nboth the LCT and DR modules, the DR-LCT-UNet achieves\nsuperior performance in terms of DSC, Recall, ASSD, and HD,\nhighlighting the complementary beneﬁts of the two modules.\nSecond, we have compared the number of parameters and the\naverage inference time for the different modules. As shown in\nTable 6, the LCT and DR modules only slightly increase the\nnumber of parameters and the inference time compared with the\nSA and Residual modules.\nThis result demonstrates that, despite the minor increase in the\nnumber of parameters and inference time, the LCT and DR\nmodules achieve much better segmentation accuracy compared\nto the SA and Residual modules. This demonstrates the\neffectiveness of the proposed LCT and DR modules in\nimproving segmentation p erformance without signi ﬁcantly\nimpacting computational complexity.\nThird, to support our claim that setting the convolutional kernel\nW\nk and Wv to the same size, i.e.,k × k × k, in the LCT module is\noptimal, we investigated three different strategies for setting the\nkernel size, the results of which are shown inTable 7. We see that\nalthough every kernel size setting strategy improves the\nsegmentation performance compared to that of using the self-\nattention module (i.e., SA-UNet inTable 5), the ﬁrst option led\nto the best performance, i.e., using convolution kernels of the same\nsize forK and V. In addition,Table 7shows that usingk = 3 produces\nthe best segmentation accuracy.\nThis result demonstrates that the sizes of local regions for spatial\ncontext extraction ofK and V should be matched. This shows that,\ncontrary to our intuition, obtaining the contextual information from\na larger neighbourhood, which will accordingly increase the number\nof parameters of the LCT module, does not necessarily result in a\nbetter segmentation accuracy. It is likely that this is because a larger\nneighbourhood may introduce more irrelevant information into the\nsegmentation process and thus degrade the segmentation accuracy.\n3.3.3 Deep supervision\nWe used deep supervision to prevent gradient disappearance\nand explosion. As can be seen fromFigure 7, with an increase in the\nnumber of training epochs, the training is clearly accelerated at the\nbeginning of the process and the DSC value of the validation set also\nimproves. Table 8 conﬁrms this and also shows that deep\nsupervision leads to a slight improvement of the other\nsegmentation metrics.\nThe results obtained from incorporating deep supervision\ndemonstrate its beneﬁts in both the network training and the\nTABLE 5 Results of the ablation experiments (Optimal value for each evaluation metric is in bold). Legend: SA: self-attention module; LCT: local contextual\nTransformer; DR: Dense Residual module; R: Residual block. The ticks indicate which modules are included in each model.\nMethod SA LCT DR R DSC ↑ Recall↑ Precision↑ ASSD↓ HD↓\n3D-UNet 0.837 0.844 0.837 0.613 30.076\nSA-UNet ✓ 0.843 0.835 0.857 0.506 32.642\nLCT-UNet ✓ 0.852 0.846 0.869 0.480 29.057\nR-UNet ✓ 0.841 0.832 0.882 0.533 29.054\nDR-UNet ✓ 0.852 0.863 0.847 0.494 29.821\nDR-LCT-UNet ✓✓ 0.858 0.863 0.858 0.425 28.215\nTABLE 6 Comparison of the number of parameters and inference time for the\ndifferent modules.\nBaseline Module Parameters (M) Inference Time (s/\ncase)\n3D-UNet — 8.61 17.21\nSA 8.65 17.23\nLCT 8.80 17.23\nR 9.50 18.50\nDR 10.28 18.55\nLCT + DR 10.70 18.60\nFrontiers inPhysiology frontiersin.org09\nWang et al. 10.3389/fphys.2023.1138257\nprediction accuracy of the coronary artery segmentation task. By\naccelerating the training process and enhancing the DSC value of the\nvalidation, deep supervision proves to be a valuable technique in\noptimizing the proposed segmentation network.\n3.3.4 Effect of data pre-processing\nTo show the effectiveness of data pre-processing on the\nsegmentation results, we used the data with and without pre-\nprocessing, to train and test the UNet and our DR-LCT-UNet.\nAs shown in Table 9, the segmentation metrics DSC, Recall,\nPrecision ASSD and HD are all clearly improved.\nThe improvements in the segmentation metrics can be\nattributed to the fact that truncating the range of HU values can\nincrease the contrast along the boundaries of the coronary arteries,\nremove some irrelevant tissues from the images and eliminate some\nnoise as well, thus making the network learning and inference more\neffective.\n4 Visual illustration of the segmentation\nresults\nFigure 8shows 3D reconstructions of the segmentation results\nusing UNet, UNETR, and the proposed DR-LCT-UNet. Four cases\nwere randomly chosen from the test set, with theﬁrst two from\nnormal subjects and the latter two belonging to patients with\ncardiovascular disease. The segmentation results of our proposed\nmethod are better than those of the other two methods, resulting in\nTABLE 7 Different structural designs of LCT modules (Optimal values shown in bold).\nQ K V DSC ↑ Recall↑ Precision↑ ASSD↓ HD↓\n1 × 1 × 1 1 × 1 × 1 1 × 1 ×1 0.847 0.842 0.850 0.511 30.015\n3 × 3 ×3 1 × 1 × 1 0.851 0.846 0.867 0.491 29.381\n3×3×3 3×3×3 0.852 0.846 0.869 0.480 29.057\n1 × 1 × 1 3 × 3 × 3 0.849 0.844 0.853 0.509 29.108\n5 × 5 × 5 1 × 1 × 1 0.845 0.840 0.855 0.513 29.277\n5 × 5 × 5 5 × 5 × 5 0.845 0.841 0.854 0.515 29.351\n1 × 1 × 1 5 × 5 × 5 0.844 0.842 0.852 0.520 29.330\n7 × 7 × 7 1 × 1 × 1 0.844 0.843 0.851 0.522 30.164\n7 × 7 × 7 7 × 7 × 7 0.844 0.842 0.849 0.522 30.097\n1 × 1 × 1 7 × 7 × 7 0.843 0.841 0.847 0.525 30.172\nFIGURE 7\nComparison of the training process of the proposed network with and without Deep Supervision. Legend: w/o: without.\nTABLE 8 Results comparison of the proposed method without and with Deep Supervision.\nMethod DSC↑ Recall↑ Precision↑ ASSD↓ HD↓\nDR-LCT-UNet_w/o_ Deep_Supervision 0.856 0.861 0.855 0.438 28.220\nDR-LCT-UNet_ Deep_Supervision 0.858 0.863 0.858 0.425 28.215\nFrontiers inPhysiology frontiersin.org10\nWang et al. 10.3389/fphys.2023.1138257\nfewer discontinuities and more complete segmentation at the ends of\nthe coronary arteries.\nThis performance improvement can be attributed to the Dense\nResidual (DR) and Local Contextual Transformer (LCT) modules in\nour model. Speci ﬁcally, the DR module, through its feature\npreservation capability at various convolution levels, is key to this\nenhancement. This module supplements shallow information layers,\nsuch as spatial structures and gray-scale features, thereby improving\nthe network’s sensitivity. This enhanced sensitivity facilitates the\nsegmentation of a greater number of coronary arteries. Furthermore,\nthe DR module excels in extracting deeper-level features without\ncompromising the retention of these shallow features, contributing\nto a more comprehensive feature map for segmentation tasks. On\nthe other hand, the LCT module, serving as an attention mechanism,\nfocuses predominantly on the vicinity of the coronary arteries. It\neffectively distinguishes these arteries from other vessels with similar\nCT intensities. When implemented post the encoding block, the\nLCT module enhances the skip connections, thereby improving the\nmodel’s feature representation ability. This enhancement leads to\nthe provision of richer, more diversiﬁed features for the decoder,\noptimizing the feature extraction and representation in our deep\nlearning model. Consequently, the combined operation of the DR\nand LCT modules results in fewer discontinuities and a more\ncomplete and precise segmentation at the ends of the coronary\narteries.\nWhile our method does produce fewer false positives compared\nto the UNet, it has shown a tendency for occasional over-\nsegmentation compared to the UNETR, as demonstrated in case\n1 (speciﬁcally, the area within the green box). In-depth analysis\nreveals that this is due to the sensitivity of the Dense Residual (DR)\nTABLE 9 The impact of data pre-processing on the network.\nMethod DSC↑ Recall↑ Precision↑ ASSD↓ HD↓\nUNet_w/o_Data_preprocess 0.820 0.826 0.820 1.195 60.412\nUNet_Data_preprocess 0.837 0.844 0.837 0.613 30.076\nDR-LCT-UNet_w/o_Data_preprocess 0.841 0.848 0.840 0.597 29.024\nDR-LCT-UNet_Data_preprocess 0.858 0.863 0.858 0.425 28.215\nFIGURE 8\nVisualization of the segmentation results from the different methods. Legend: Green boxes: locations where over-segmentation occurs for at least\none of the compared methods; blue boxes: locations with under-segmentation.\nFrontiers inPhysiology frontiersin.org11\nWang et al. 10.3389/fphys.2023.1138257\nmodule to shallow information, which occasionally results in the\nmisidentiﬁcation of structures similar to the coronary arteries, such\nas veins. This over-sensitivity and the resulting over-segmentation\nsuggest areas of improvement. We acknowledge this limitation and\nplan to reﬁne our model in follow-up studies, to better distinguish\nbetween similar structures.\n5 Conclusion\nThe proposed method for coronaryartery segmentation, DR-LCT-\nUNet, alleviates the omission and over-segmentation problems of\nprevious methods for several reasons. Firstly, the data preprocessing\nenhances the contrast at the boundaries of the coronary arteries and\nr e d u c e ss o m eo ft h en o i s ei nt h ei m a g e ,h e n c ei m p r o v i n gt h e\nsegmentation to some extent. Secondly, the proposed Transformer-\nstyle LCT module can pay more attention to local contextual\ninformation, reducing the semantic gap between the encoding and\ndecoding features, signiﬁcantly improving the segmentation Precision.\nFurthermore, the proposed DR module for the encoding stage can\npreserve multi-level features, reducing the loss of shallow-layer\ninformation due to the convolution process. As a result, this\nimproves the Recall of the segmentation. Finally, introducing Deep\nSupervision to the network improves the efﬁciency of the training\nprocess and also has the effect of regularizing feature extraction for the\ndifferent decoding layers. Theﬁnal DSC, Recall, and Precision of the\nproposed method are 85.8%, 86.3%, and 85.8%, respectively, which are\n2.1%, 1.9%, and 2.1% better than the corresponding values for 3D-\nUNet, the most widely used image segmentation method and the\nbaseline based on which our approach has been developed.\nData availability statement\nThe original contributions presented in the study are included in\nthe article/Supplementary Material, further inquiries can be directed\nto the corresponding authors.\nAuthor contributions\nConceptualization, QW, LX, YS, and BY; methodology, QW and\nLW; software, QW and XY; validation, QW and LX; formal analysis,\nQW, LX, LW, and YS and BY; investigation, QW, LX, LW, and XY;\nresources, YS, BY, LX, and LW; data curation, YS, BY, and LX;\nwriting— original draft preparation, QW, LX, and LW;\nwriting— review and editing, LX, LW, and SG; visualization, QW\nand XY; supervision, LX and LW; project administration, LX;\nfunding acquisition, LX and LW. All authors contributed to the\narticle and approved the submitted version.\nFunding\nThis research was funded by the National Natural Science\nFoundation of China (No. 62273082, 61773110, and\nU21A20487), the Natural Science Foundation of Liaoning\nProvince (Grant 2021-YGJC-14), the Basic Scienti ﬁc Research\nProject (Key Project) of The Educational Department of Liaoning\nProvince, (LJKZ00042021), the Fundamental Research Funds for the\nCentral Universities (No. N2119008 and N181906001), and the\nLiaoning Provincial “Selecting the Best Candi-dates by Opening\nCompetition Mechanism ” Science and Technology Program\n(2022JH1/10400004). It was also supported by the Shenyang\nScience and Technology Plan Fund (No. 21-104-1-24, 20-201-4-\n10, and 201375) and the Member Program of Neusoft Research of\nIntelligent Healthcare Technology, Co., Ltd. (No. MCMP062002).\nThe authors declare that this study received funding from Intelligent\nHealthcare Technology, Co., Ltd. The funder was not involved in the\nstudy design, collection, analysis, interpretation of data, the writing\nof this article, or the decision to submit it for publication.\nAcknowledgments\nThe authors thank the participants for their greatly valued\nassistance during experiments.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found online\nat: https://www.frontiersin.org/articles/10.3389/fphys.2023.1138257/\nfull#supplementary-material\nSUPPLEMENTARY IMAGE 1\nSegmentation result of the DR-LCT-UNet (ours) method on CCTA data of\ncase 4.\nSUPPLEMENTARY IMAGE 2\nSegmentation result of the UNETR method on CCTA data of case 4.\nSUPPLEMENTARY IMAGE 3\nSegmentation result of the 3D-UNet method on CCTA data of case 4.\nSUPPLEMENTARY IMAGE 4\nGround Truth segmentation result on CCTA data of case 4.\nSUPPLEMENTARY IMAGE 5\nSegmentation result of the DR-LCT-UNet (ours) method on CCTA data of\ncase 1.\nSUPPLEMENTARY IMAGE 6\nGround Truth segmentation result on CCTA data of case 2.\nFrontiers inPhysiology frontiersin.org12\nWang et al. 10.3389/fphys.2023.1138257\nSUPPLEMENTARY IMAGE 7\nSegmentation result of the 3D-UNet method on CCTA data of case 2.\nSUPPLEMENTARY IMAGE 8\nSegmentation result of the UNETR method on CCTA data of case 2.\nSUPPLEMENTARY IMAGE 9\nSegmentation result of the DR-LCT-UNet (ours) method on CCTA data of\ncase 2.\nSUPPLEMENTARY IMAGE 10\nGround Truth segmentation result on CCTA data of case 3.\nSUPPLEMENTARY IMAGE 11\nSegmentation result of the 3D-UNet method on CCTA data of case 3.\nSUPPLEMENTARY IMAGE 12\nSegmentation result of the UNETR method on CCTA data of case 3.\nSUPPLEMENTARY IMAGE 13\nSegmentation result of the DR-LCT-UNet (ours) method on CCTA data of\ncase 3.\nSUPPLEMENTARY IMAGE 14\nGround Truth segmentation result on CCTA data of case 1.\nSUPPLEMENTARY IMAGE 15\nSegmentation result of the 3D-UNet method on CCTA data of case 1.\nSUPPLEMENTARY IMAGE 16\nSegmentation result of the UNETR method on CCTA data of case 1.\nReferences\nBui, T. D., Shin, J., and Moon, T. (2019). Skip-connected 3D DenseNet for volumetric\ninfant brain MRI segmentation.Biomed. Signal Process. Control54, 101613. doi:10.\n1016/j.bspc.2019.101613\nCheng, Y., Hu, X., Wang, J., Wang, Y., and Tamura, S. (2015). Accurate vessel\nsegmentation with constrained B-snake.IEEE Trans. Image Process.24 (8), 2440–2455.\ndoi:10.1109/TIP.2015.2417683\nÇiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T., and Ronneberger, O. (2016).“3D\nU-net: learning dense volumetric segmentation from sparse annotation, ” in\nInternational conference on medical image computing and computer-assisted\nintervention (Cham: Springer), 424–432. doi:10.1007/978-3-319-46723-8_49\nDong, C., Xu, S., and Li, Z. (2022). A novel end-to-end deep learning solution for coronary\nartery segmentation from CCTA.Med. Phys.49 (11), 6945–6959. doi:10.1002/mp.15842\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2021).An image is worth 16x16 words: Transformers for image recognition at scale.\nICLR. doi:10.48550/arXiv.2010.11929\nGao, R., Hou, Z., Li, J., Han, H., Lu, B., and Zhou, S. K. (2021).“Joint coronary\ncenterline extraction and lumen segmentation from ccta using cnntracker and vascular\ngraph convolutional network,” in Proceeding of the 2021 IEEE 18th International\nSymposium on Biomedical Imaging (ISBI), Nice, France, April 2021 (IEEE), 1897–1901.\ndoi:10.1109/ISBI48211.2021.9433764\nGhekiere, O., Salgado, R., Buls, N., Leiner, T., Mancini, I., Vanhoenacker, P., et al.\n(2017). Image quality in coronary CT angiography: challenges and technical solutions.\nBr. J. radiology90 (1072), 20160567. doi:10.1259/bjr.20160567\nH a t a m i z a d e h ,A . ,T a n g ,Y . ,N a t h ,V . ,Y a n g ,D . ,M y r o n e n k o ,A . ,L a n d m a n ,B . ,e ta l .( 2 0 2 2 ) .\n“Unetr: transformers for 3d medical image segmentation,” in Proceedings of the IEEE/CVF\nwinter conference on applications of computer vision,5 7 4–584. doi:10.48550/arXiv.2103.10504\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).“Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 770–778. doi:10.1109/CVPR.2016.90\nI s l a m ,M . ,V i b a s h a n ,V .S . ,J o s e ,V . ,W i j e t h i l a k e ,N . ,U t k a r s h ,U . ,a n dR e n ,H .( 2 0 2 0 ) .“Brain\ntumor segmentation and survival prediction using 3D attention UNet,” in International\nMICCAI brainlesion workshop(Cham: Springer), 262–272. doi:10.1007/978-3-030-46640-4_25\nJayaraj, J. C., Davatyan, K., Subramanian, S. S., and Priya, J. (2019). Epidemiology of\nmyocardial infarction. Myocard. Infarct. 3 (10). doi:10.5772/intechopen.74768\nJin, Q., Meng, Z., Sun, C., Cui, H., and Su, R. (2020). RA-UNet: a hybrid deep\nattention-aware network to extract liver and tumor in CT scans. Front. Bioeng.\nBiotechnol. 1471, 605132. doi:10.3389/fbioe.2020.605132\nKerkeni, A., Benabdallah, A., Manzanera, A., and Bedoui, M. H. (2016). A coronary\nartery segmentation method based on multiscale analysis and region growing.Comput.\nMed. Imaging Graph.48, 49–61. doi:10.1016/j.compmedimag.2015.12.004\nKirişli, H. A., Schaap, M., Metz, C. T., Dharampal, A. S., Meijboom, W. B.,\nPapadopoulou, S. L., et al. (2013). Standardized evaluation framework for evaluating\ncoronary artery stenosis detection, stenosis quanti\nﬁcation and lumen segmentation\nalgorithms in computed tomography angiography.Med. image Anal.17 (8), 859–876.\ndoi:10.1016/j.media.2013.05.007\nKong, B., Wang, X., Bai, J., Lu, Y., Gao, F., Cao, K., et al. (2020). Learning tree-\nstructured representation for 3D coronary artery segmentation.Comput. Med. Imaging\nGraph. 80, 101688. doi:10.1016/j.compmedimag.2019.101688\nK r o f t ,L .J .M . ,D eR o o s ,A . ,a n dG e l e i j n s ,J .( 2 0 0 7 ) .A r t i f a c t si nE C G - s y n c h r o n i z e dM D C T\ncoronary angiography.A m .J .R o e n t g e n o l .189 (3), 581–591. doi:10.2214/AJR.07.2138\nLee, C. Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2015).“Deeply-supervised\nnets,” in Artiﬁcial intelligence and statistics(San Diego, CA: PMLR), 562–570.\nLee, K., Zung, J., Li, P., Jain, V., and Seung, H. S. (2017). Superhuman accuracy on the\nSNEMI3D connectomics challenge.arXiv preprint. doi:10.48550/arXiv.1706.00120\nLei, Y., Guo, B., Fu, Y., Wang, T., Liu, T., Curran, W., et al. (2020). Automated\ncoronary artery segmentation in coronary computed tomography angiography (CCTA)\nusing deep learning neural networks.Med. Imaging 2020 Imaging Inf. Healthc. Res.\nAppl. 11318, 279–284. doi:10.1117/12.2550368\nLesage, D., Angelini, E. D., Bloch, I., and Funka-Lea, G. (2009). A review of 3D vessel\nlumen segmentation techniques: models, features and extraction schemes.Med. image\nAnal. 13 (6), 819–845. doi:10.1016/j.media.2009.07.011\nL i ,X . ,C h e n ,H . ,Q i ,X . ,D o u ,Q . ,F u ,C .W . ,a n dH e n g ,P .A .( 2 0 1 8 ) .\nH-DenseUNet: hybrid densely con nected UNet for liver and tumor\nsegmentation from CT volumes. IEEE Trans. Med. imaging 37 (12), 2663–2674.\ndoi:10.1109/TMI.2018.2845918\nLi, W., Qin, S., Li, F., and Wang, L. (2021). MAD-UNet: a deep U-shaped network\ncombined with an attention mechanism for pancreas segmentation in CT images.Med.\nPhys. 48 (1), 329–341. doi:10.1002/mp.14617\nLi, Y., Yao, T., Pan, Y., and Mei, T. (2022). Contextual transformer networks for visual\nrecognition. IEEE Trans. Pattern Analysis Mach. Intell.45, 1489–1500. doi:10.1109/\nTPAMI.2022.3164083\nLiu, J., Gao, J., Wu, R., Zhang, Y., Hu, L., and Hou, P. (2013). Optimizing contrast\nmedium injection protocol individually with body weight for high-pitch prospective\nECG-triggering coronary CT angiography. Int. J. Cardiovasc. imaging 29 (5),\n1115–1120. doi:10.1007/s10554-012-0170-x\nLiu, L., Cheng, J., Quan, Q., Wu, F. X., Wang, Y. P., Wang, J., et al. (2020).\nRandomized, multicenter, open-label trial of autologous cytokine-induced killer cell\nimmunotherapy plus chemotherapy for squamous non-small-cell lung cancer:\nnCT01631357. Neurocomputing 409, 244–258. doi:10.1038/s41392-020-00337-x\nMa, G., Yang, J., and Zhao, H. (2020). A coronary artery segmentation method based\non region growing with variable sector search area. Technol. Health Care 28 (1),\n463–472. doi:10.3233/thc-209047\nMarquering, H. A., Dijkstra, J., de Koning, P. J., Stoel, B. C., and Reiber, J. H. (2005).\nTowards quantitative analysis of coronary CTA.Int. J. Cardiovasc. imaging 21 (1),\n73–\n84. doi:10.1007/s10554-004-5341-y\nMaterialise (2020).Mimics. Retrieved fromhttps://www.materialise.com/en/medical/\nmimics-innovation-suite/mimics.\nMihalef, V., Ionasec, R. I., Sharma, P., Georgescu, B., Voigt, I., Suehling, M., et al.\n(2011). Patient-speci ﬁc modelling of whole heart anatomy, dynamics and\nhaemodynamics from four-dimensional cardiac CT images. Interface Focus 1 (3),\n286–296. doi:10.1098/rsfs.2010.0036\nMilletari, F., Navab, N., and Ahmadi, S. A. (2016).“V-net: fully convolutional neural\nnetworks for volumetric medical image segmentation,”in Proceeding of the 2016 fourth\ninternational conference on 3D vision (3DV), Stanford, CA, USA, October 2016 (IEEE),\n565–571. doi:10.1109/3DV.2016.79\nOrujov, F., Maskeliūnas, R., Damaševičius, R., and Wei, W. J. A. S. C. (2020). Fuzzy\nbased image edge detection algorithm for blood vessel detection in retinal images.Appl.\nSoft Comput. 94, 106452. doi:10.1016/j.asoc.2020.106452\nPan, L. S., Li, C. W., Su, S. F., Tay, S. Y., Tran, Q. V., and Chan, W. P. (2021). Coronary\nartery segmentation under class imbalance using a U-Net based architecture on\ncomputed tomography angiography images. Sci. Rep. 11 (1), 14493–14497. doi:10.\n1038/s41598-021-93889-z\nPeiris, H., Hayat, M., Chen, Z., Egan, G., and Harandi, M. (2022). “A robust\nvolumetric transformer for accurate 3d tumor segmentation, ” in International\nconference on medical image computing and computer-assisted intervention(Cham:\nSpringer), 162–172. doi:10.1007/978-3-031-16443-9_16\nQamar, S., Jin, H., Zheng, R., Ahmad, P., and Usama, M. (2020). A variant form of\n3D-UNet for infant brain segmentation.Future Gener. Comput. Syst.108, 613–623.\ndoi:10.1016/j.future.2019.11.021\nFrontiers inPhysiology frontiersin.org13\nWang et al. 10.3389/fphys.2023.1138257\nRaff, G. L. (2007). Interpreting the evidence: how accurate is coronary computed\ntomography angiography?J. Cardiovasc. Comput. Tomogr.1 (2), 73–77. doi:10.1016/j.\njcct.2007.04.014\nRonneberger, O., Fischer, P., and Brox, T. (2015).“U-net: convolutional networks for\nbiomedical image segmentation, ” in International Conference on Medical image\ncomputing and computer-assisted intervention (Cham: Springer), 234–241. doi:10.\n1007/978-3-319-24574-4_28\nSharma, P., Suehling, M., Flohr, T., and Comaniciu, D. (2020). Artiﬁcial intelligence in\ndiagnostic imaging: status quo, challenges, and future opportunities.J. Thorac. Imaging\n35, S11–S16. doi:10.1097/RTI.0000000000000499\nSong, A., Xu, L., Wang, L., Wang, B., Yang, X., Xu, B., et al. (2022). Automatic\ncoronary artery segmentation of CCTA images with an efﬁcient feature-fusion-and-\nrectiﬁcation 3D-UNet.IEEE J. Biomed. Health Inf.26 (8), 4044–4055. doi:10.1109/JBHI.\n2022.3169425\nTesche, C., De Cecco, C. N., Baumann, S., Renker, M., McLaurin, T. W., Duguay, T.\nM., et al. (2018). Coronary CT angiography–derived fractional ﬂow reserve: machine\nlearning algorithm versus computationalﬂuid dynamics modeling.Radiology 288 (1),\n64–72. doi:10.1148/radiol.2018171291\nTian, F., Gao, Y., Fang, Z., and Gu, J. (2021). Automatic coronary artery segmentation\nalgorithm based on deep learning and digital image processing.Appl. Intell. 51 (12),\n8881–8895. doi:10.1007/s10489-021-02197-6\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need.Adv. neural Inf. Process. Syst.30. doi:10.48550/arXiv.\n1706.03762\nWang, L., Yang, X., Wang, Q., and Xu, L. (2022a). Two-stage U-net coronary artery\nsegmentation based on CTA images. J. Northeast. Univ.Nat. Sci. 43 (6), 792. doi:10.\n12068/j.issn.1005-3026.2022.06.005\nWang, H., Cao, P., Wang, J., and Zaiane, O. R. (2022b). Uctransnet: rethinking the\nskip connections in u-net from a channel-wise perspective with transformer.Proc.\nAAAI Conf. Artif. Intell.36 (3), 2441–2442. doi:10.1021/acs.biochem.2c00621\nW a n g ,H . ,X i e ,S . ,L i n ,L . ,I w a m o t o ,Y . ,H a n ,X .H . ,C h e n ,Y .W . ,e ta l .( 2 0 2 2 c ) .“Mixed\ntransformer u-net for medical image segmentation,”in ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Proc e s s i n g( I C A S S P ) ,S i n g a p o r e ,S i n g a p o r e ,M a y\n2022 (IEEE), 2390–2394. doi:10.1109/ICASSP43922.2022.9746172\nWolterink, J. M., Leiner, T., and Išgum, I. (2019).“Graph convolutional networks for\ncoronary artery segmentation in cardiac CT angiography,” in Graph Learning in\nMedical Imaging: First International Workshop, GLMI 2019, Held in Conjunction\nwith MICCAI 2019, Shenzhen, China, October 17, 2019 (Springer International\nPublishing), 62–69. Proceedings 1.\nY a n ,X . ,T a n g ,H . ,S u n ,S . ,M a ,H . ,K o n g ,D . ,a n dX i e ,X .( 2 0 2 2 ) .“After-unet: axial fusion\ntransformer unet for medical image segmentation,” in Proceedings of the IEEE/CVF winter\nconference on applications of computer vision,3 9 7 1–3981. doi:10.48550/arXiv.2110.10403\nYang, J., Lou, C., Fu, J., and Feng, C. (2020). Vessel segmentation using multiscale\nvessel enhancement and a region based level set model.Comput. Med. Imaging Graph.\n85, 101783. doi:10.1016/j.compmedimag.2020.101783\nZhang, Z., Tang, Z., Wang, Y., Zhang, Z., Zhan, C., Zha, Z., et al. (2021). Dense\nResidual Network: enhancing global dense feature ﬂow for character recognition.\nNeural Netw. 139, 77–85. doi:10.1016/j.neunet.2021.02.005\nZhang, Y., Tian, Y., Kong, Y., Zhong, B., and Fu, Y. (2020). Residual dense network for\nimage restoration.IEEE Trans. Pattern Analysis Mach. Intell.43 (7), 2480–2495. doi:10.\n1109/TPAMI.2020.2968521\nZhang, Z., Wu, C., Coleman, S., and Kerr, D. (2020). DENSE-INception U-net for\nmedical image segmentation.Comput. methods programs Biomed.192, 105395. doi:10.\n1016/j.cmpb.2020.105395\nFrontiers inPhysiology frontiersin.org14\nWang et al. 10.3389/fphys.2023.1138257",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.8374463319778442
    },
    {
      "name": "Computer science",
      "score": 0.684303343296051
    },
    {
      "name": "Encoder",
      "score": 0.6789654493331909
    },
    {
      "name": "Coronary artery disease",
      "score": 0.5929532051086426
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5753750801086426
    },
    {
      "name": "Convolutional neural network",
      "score": 0.538112461566925
    },
    {
      "name": "Residual",
      "score": 0.4493083953857422
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4314134120941162
    },
    {
      "name": "Medicine",
      "score": 0.36181455850601196
    },
    {
      "name": "Computer vision",
      "score": 0.3541123867034912
    },
    {
      "name": "Cardiology",
      "score": 0.22462663054466248
    },
    {
      "name": "Algorithm",
      "score": 0.1403878927230835
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}