{
  "title": "Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations",
  "url": "https://openalex.org/W3104152666",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2124772336",
      "name": "Jianfei Yu",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2007802817",
      "name": "Jing Jiang",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A3004015178",
      "name": "Ling Min Serena Khoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A733215631",
      "name": "Hai Leong Chieu",
      "affiliations": [
        "DSO National Laboratories"
      ]
    },
    {
      "id": "https://openalex.org/A2098083858",
      "name": "Rui Xia",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2470673105",
    "https://openalex.org/W3005668675",
    "https://openalex.org/W2900318775",
    "https://openalex.org/W2774484786",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W2607700676",
    "https://openalex.org/W2609512471",
    "https://openalex.org/W2281420995",
    "https://openalex.org/W2951288507",
    "https://openalex.org/W2142869398",
    "https://openalex.org/W2084591134",
    "https://openalex.org/W2051405935",
    "https://openalex.org/W2742144412",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2143398792",
    "https://openalex.org/W2997976265",
    "https://openalex.org/W2963731617",
    "https://openalex.org/W2798787718",
    "https://openalex.org/W1546111015",
    "https://openalex.org/W2582561810",
    "https://openalex.org/W2798966390",
    "https://openalex.org/W2767853748",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1975594555",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2515728551",
    "https://openalex.org/W2950720493",
    "https://openalex.org/W2032897813",
    "https://openalex.org/W4297730508",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W3035436280",
    "https://openalex.org/W2337875011",
    "https://openalex.org/W2963277000",
    "https://openalex.org/W2577888896",
    "https://openalex.org/W2970683844",
    "https://openalex.org/W2763572884",
    "https://openalex.org/W2159981908"
  ],
  "abstract": "The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task. Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads. We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively. Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1392–1401,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n1392\nCoupled Hierarchical Transformer for Stance-Aware Rumor Veriﬁcation\nin Social Media Conversations\nJianfei Yu1, Jing Jiang2, Ling Min Serena Khoo3, Hai Leong Chieu3, and Rui Xia1\n1 School of Artiﬁcial Intelligence, Nanjing University of Science & Technology, China\n2 School of Information Systems, Singapore Management University, Singapore\n3 DSO National Laboratories, Singapore\n{jfyu, rxia}@njust.edu.cn, jingjiang@smu.edu.sg,\n{klingmin, chaileon}@dso.org.sg\nAbstract\nThe prevalent use of social media enables\nrapid spread of rumors on a massive scale,\nwhich leads to the emerging need of automatic\nrumor veriﬁcation (RV). A number of previ-\nous studies focus on leveraging stance classi-\nﬁcation to enhance RV with multi-task learn-\ning (MTL) methods. However, most of these\nmethods failed to employ pre-trained contex-\ntualized embeddings such as BERT, and did\nnot exploit inter-task dependencies by using\npredicted stance labels to improve the RV\ntask. Therefore, in this paper, to extend BERT\nto obtain thread representations, we ﬁrst pro-\npose a Hierarchical Transformer 1, which di-\nvides each long thread into shorter subthreads,\nand employs BERT to separately represent\neach subthread, followed by a global Trans-\nformer layer to encode all the subthreads. We\nfurther propose a Coupled Transformer Mod-\nule to capture the inter-task interactions and\na Post-Level Attention layer to use the pre-\ndicted stance labels for RV , respectively. Ex-\nperiments on two benchmark datasets show the\nsuperiority of our Coupled Hierarchical Trans-\nformer model over existing MTL approaches.\n1 Background\nRecent years have witnessed a profound revolu-\ntion in social media, as many individuals gradually\nturn to different social platforms to share the latest\nnews and voice personal opinions. Meanwhile, the\nﬂourish of social media also enables rapid dissemi-\nnation of unveriﬁed information (i.e., rumors) on\na massive scale, which may cause serious harm to\nour society (e.g., impacting presidential election\ndecisions (Allcott and Gentzkow, 2017)). Since\nmanually checking a sheer quantity of rumors on\n1Note that the concept of hierarchy in this paper is different\nfrom that in Yang et al. (2016), as we use hierarchy to refer\nto a neural structure that ﬁrst models the local interactions\namong posts within each subthread, followed by modeling the\nglobal interactions among all the posts in the whole thread.\nLee Kuan Yew died already. www.pmo.gov.sg/lky.\nSource Post\n    QueryIs it true? Lee Kuan Yew Died? Can anyone confirm it?\nNo, I don’t believe it is true.\nR2: Reply Post\nR21: Reply Post\n     Deny\n  SupportHe died several days ago. They didn’t announce until now. \nR1: Reply Post\nI also think so. He was on TV last week.\nR211: Reply Post\n     Deny\nSupport\nStance Label \nVeracity Label: False Rumor\nFigure 1: An example conversation thread with both\nrumor veracity label and stance labels. Each post has a\nstance label towards the claim in the source post, and\nthe source claim was later identiﬁed as false rumor.\nsocial media is naturally labor-intensive and time-\nconsuming, it is crucial to develop an automatic\nrumor veriﬁcation approach to mitigate their harm-\nful effect.\nRumor veriﬁcation is typically deﬁned as a task\nof determining whether the source claim in a con-\nversation thread is false rumor, true rumor, or un-\nveriﬁed rumor (Zubiaga et al., 2018a). In the litera-\nture, much work has been done for rumor veriﬁca-\ntion (Liu et al., 2015; Ma et al., 2016; Ruchansky\net al., 2017; Chen et al., 2018; Kochkina and Li-\nakata, 2020). Among them, one appealing line of\nwork focuses on exploiting stance signals to en-\nhance rumor veriﬁcation (Zubiaga et al., 2016),\nsince it is observed that people’s stances in reply\nposts usually provide important clues to rumor veri-\nﬁcation (e.g., in Fig. 1, if the source claim is denied\nor queried by most replies, it is highly probable\nthat the source claim contains misinformation and\nis false rumor).\nThis line of work has attracted increasing atten-\ntion in recent years. A number of multi-task learn-\ning (MTL) methods have been proposed to jointly\nperform stance classiﬁcation (SC) and rumor veri-\nﬁcation (RV) over conversation threads, including\nSequential LSTM-based methods (Li et al., 2019),\nTree LSTM-based methods (Kumar and Carley,\n1393\n2019), and Graph Convolutional Network-based\nmethods (Wei et al., 2019). These MTL approaches\nare mainly constructed upon the MTL2 framework\nproposed in Kochkina et al. (2018), which aims to\nﬁrst learn shared representations with shared layers\nin the low level, followed by learning task-speciﬁc\nrepresentations with separate stance-speciﬁc layers\nand rumor-speciﬁc layers in the high level.\nAlthough these MTL approaches have shown the\nusefulness of stance signals to rumor veriﬁcation,\nthey still suffer from the following shortcomings:\n(1) The ﬁrst obstacle lies in their single-task mod-\nels for SC or RV , whose randomly initialized text\nencoders such as LSTM tend to overﬁt existing\nsmall annotated corpora. With the recent trend of\npre-training, many pre-trained text encoders such\nas BERT have been shown to overcome the overﬁt-\nting problem and achieve signiﬁcant improvements\nin many NLP tasks (Devlin et al., 2019). However,\nunlike previous sentence-level tasks, our SC and\nRV tasks require the language understanding over\nconversation threads in social media. Since BERT\nis unable to process arbitrarily long sequences due\nto its maximum length constraint in the pre-training\nstage, it remains an open question how to extend\nBERT to our SC and RV tasks. (2) Another im-\nportant limitation of previous studies lies in their\nmulti-task learning framework. First, the MTL2\nframework used in existing methods fails to explic-\nitly model the inter-task interactions between the\nstance-speciﬁc and rumor-speciﬁc layers. Second,\nalthough it has been observed that people’s stances\nin reply posts are crucial to rumor veriﬁcation, the\nstance distributions predicted from stance-speciﬁc\nlayers have not been utilized for rumor veracity\nprediction in the MTL2 framework.\nTo address the above two shortcomings, we ex-\nplore the potential of BERT for stance-aware rumor\nveriﬁcation, and propose a new multi-task learn-\ning model based on Transformer (Vaswani et al.,\n2017), named Coupled Hierarchical Transformer.\nOur main contributions can be summarized as fol-\nlows:\n•To extend BERT as our single-task model for\nSC and RV , we propose a Hierarchical Trans-\nformer architecture. Speciﬁcally, we ﬁrst ﬂatten\nall the posts in a conversation thread into a long\nsequence, and then decompose them evenly into\nmultiple subthreads, each within the length con-\nstraint of BERT. Next, each subthread is encoded\nwith BERT to capture the local interactions be-\ntween posts within the subthread, and then a\nTransformer layer is stacked on top of all the\nsubthreads to capture the global interactions be-\ntween posts in the whole conversation thread.\n•To tackle the limitations of the MTL2 frame-\nwork, we ﬁrst design a Coupled Transformer\nModule to capture the inter-task interactions be-\ntween the stance-speciﬁc and the rumor-speciﬁc\nlayers. Moreover, to utilize the stance distribu-\ntions predicted for each post, we propose to con-\ncatenate them with its associated post represen-\ntations, followed by a post-level attention mech-\nanism to automatically learn the importance of\neach post for the ﬁnal rumor veriﬁcation task.\nEvaluations on two benchmark datasets demon-\nstrate the following: First, compared with existing\nsingle-task models, our Hierarchical Transformer\nbrings consistent performance gains on Macro-F1\nfor both SC and RV tasks. Second, our Coupled\nHierarchical Transformer outperforms the state-of-\nthe-art multi-task learning approach by 9.2% and\n6.3% on Macro-F1 for the two benchmarks, respec-\ntively.\n2 Related Work\nStance Classiﬁcation: Although stance classiﬁca-\ntion has been well studied in different contexts such\nas online forums (Hasan and Ng, 2013; Lukasik\net al., 2016; Ferreira and Vlachos, 2016; Moham-\nmad et al., 2016), a recent trend is to study stance\nclassiﬁcation towards rumors in different social\nmedia platforms (Mendoza et al., 2010; Qazvinian\net al., 2011). These studies can be roughly catego-\nrized into two groups. One line of work aims to\ndesign different features to capture the sequential\nproperty of conversation threads (Zubiaga et al.,\n2016; Aker et al., 2017; Pamungkas et al., 2018;\nZubiaga et al., 2018b; Giasemidis et al., 2018). An-\nother line of work attempts to apply recent deep\nlearning models to automatically capture effective\nstance features (Kochkina et al., 2017; Veyseh et al.,\n2017). Our work extends the latter line of work by\nproposing a hierarchical Transformer based on the\nrecent pre-trained BERT for this task. Moreover,\nwe notice that our BERT-based hierarchical Trans-\nformer is similar to the model proposed in (Pap-\npagari et al., 2019), but we want to point out that\nour model design in the input and output layers is\nspeciﬁc to stance classiﬁcation, which is different\nfrom their work.\nRumor Veriﬁcation: Due to the negative impact\n1394\nof various rumors spreading on social media, ru-\nmor veriﬁcation has attracted increasing attention\nin recent years. Existing approaches to single-task\nrumor veriﬁcation generally belong to two groups.\nThe ﬁrst line of work focuses on either employ-\ning a myriad of hand-crafted features (Qazvinian\net al., 2011; Yang et al., 2012; Kwon et al., 2013;\nMa et al., 2015) including post contents, user pro-\nﬁles, information credibility features (Castillo et al.,\n2011), and propagation patterns, or resorting to var-\nious kinds of kernels to model the event propaga-\ntion structure (Wu et al., 2015; Ma et al., 2017).\nThe second line of work applies variants of sev-\neral neural network models to automatically cap-\nture important features among all the propagated\nposts (Ma et al., 2016; Ruchansky et al., 2017;\nChen et al., 2018). Different from these studies, the\ngoal in this paper is to leverage stance classiﬁca-\ntion to improve rumor veriﬁcation with a multi-task\nlearning architecture.\nStance-Aware Rumor Veriﬁcation: The recent\nadvance in rumor veriﬁcation is to exploit stance\ninformation to enhance rumor veriﬁcation with dif-\nferent multi-task learning approaches. Speciﬁcally,\nMa et al. (2018a) and Kochkina et al. (2018) respec-\ntively proposed two multi-task learning architec-\ntures to jointly optimize stance classiﬁcation and\nrumor veriﬁcation based on two different variants\nof RNN, i.e., GRU and LSTM. More recently, Ku-\nmar and Carley (2019) proposed another multi-task\nLSTM model based on tree structures for stance-\naware rumor veriﬁcation. Our work bears the same\nintuition to these previous studies, and aims to ex-\nplore the potential of the pre-trained BERT to this\nmulti-task learning task.\n3 Methodology\nIn this section, we ﬁrst formulate the task of stance\nclassiﬁcation (SC) and rumor veriﬁcation (RV). We\nthen describe our single-task model for SC and RV ,\nfollowed by introducing our multi-task learning\nframework for stance-aware rumor veriﬁcation.\n3.1 Task Formulation\nGiven a Twitter corpus, let us ﬁrst use D =\n{C1, C2, . . . , C|D|}to denote a set of conversation\nthreads in the corpus. Each thread Ci is then as-\nsumed to consist of a post with the source claimS0\nand a sequence of reply posts sorted in chronologi-\ncal order, denoted by R1, R2, ... , RN .\nFor the SC task, given an input thread Ci, we\nassume that each post (including a source post and\nreply posts) in the thread is annotated with a stance\nlabel towards the source claim, namely support,\ndeny, query, and comment. Formally, let s = (s0, s1,\n..., sN ) denote the sequence of stance labels, and\nthe goal of SC is to learn a sequence classiﬁcation\nfunction g: S0, R1, . . . , RN →s0, s1, . . . , sN .\nFor the RV task, we assume that each input\nthread Ci is associated with a rumor labelyi, which\nbelongs to one of the three classes, namely false ru-\nmor, true rumor, and unveriﬁed rumor. The goal of\nRV is to learn a classiﬁcation function f: Ci →yi.\n3.2 Hierarchical Transformer for Stance\nClassiﬁcation and Rumor Veriﬁcation\nIn this subsection, we present our proposed Hier-\narchical Transformer, which is a single-task learn-\ning framework encompassing the tasks of SC and\nRV . Fig. 2 illustrates the overview of our model,\nwhich mainly consists of four modules, including\ninput thread transformation, local context encoding,\nglobal context encoding, and output layers.\nMotivation: Although BERT has been widely\nadopted in various NLP tasks (Devlin et al., 2019),\nits application to our SC and RV tasks is not triv-\nial. First, most previous studies employed BERT\nto obtain token-level representations for sentence\nor paragraph understanding, while our SC and RV\ntasks primarily require sentence-level representa-\ntions for conversation thread understanding. Sec-\nond, due to the maximum length constraint during\nthe pre-training stage, BERT cannot be directly\napplied to encode arbitrarily long sequences, e.g.,\nconversation threads in our tasks. Although trun-\ncating the input sequences is a feasible solution,\nit will inevitably ignore many posts that might be\ncrucial for rumor veriﬁcation.\nOur main idea to address the limitations above is\nto divide the long sequence of a thread into shorter\nsequences, each within the length constraint of\nBERT, and to use a hierarchical model to capture\nthe global interactions at the top layer.\nInput Thread Transformation: First, to obtain\npost-level representations, we insert two special\ntokens, i.e., [CLS] and [SEP], to the beginning and\nthe end of each post, where the [CLS] token is in-\ntended to represent the semantic meaning of the\npost following it. We then sort the transformed\nposts in each thread Ci in chronological order, fol-\nlowed by ﬂattening them into a long sequence. Sec-\nond, to eliminate the maximum length constraint,\n1395\nSource S 1st Reply R (n-1)th Reply R\n1st Subthread\n(k-1)nth Reply R kn-1th Reply R\nkth Subthread\n1 n-1 kn-1nth Reply R 2n-1th Reply R2n-1n\n…...\n2nd Subthread\nLocal Context Encoder with BERT\n          Global Context Encoder with Transformer\nLocal Context Encoder with BERT\n Local Context Encoder with BERT\n[CLS] … [SEP]S1 Sm-2 [CLS] … [SEP]R1 Rm-2 [CLS] … [SEP]R1 Rm-2…... [CLS] … [SEP]R1 Rm-2 [CLS] … [SEP]R1 Rm-2...… [CLS] … [SEP]R1 Rm-2 [CLS] … [SEP]R1 Rm-2…\n(k-1)n\n[CLS] … [SEP] [CLS] … [SEP] [CLS] … [SEP]…...\n [CLS] … [SEP] [CLS] … [SEP]…... …... [CLS] … [SEP] [CLS] … [SEP]…...\n[CLS] … [SEP] [CLS] … [SEP] [CLS] … [SEP]…... [CLS] … [SEP] [CLS] … [SEP]…... …... [CLS] … [SEP] [CLS] … [SEP]…...\nTask 1\nRumor Label\nTrue/False/Unverified\nS1\n Sn-1\n Sn\nS0\n S2n-1\n S(k-1)n\n Skn-1\nTask 2\nStance Label\nSupport/Deny/Query/Comment\n…... …...…...…...\nInput Conversation Thread\nFigure 2: Our Single-Task Model (Hierarchical Transformer) for Stance Classiﬁcation and Rumor Veriﬁcation.\nwe propose to decompose the ﬂattened sequence\ninto multiple subthreads, so that each subthread has\nthe same number of posts, and the sequence length\nof each subthread satisﬁes the length constraint.\nFormally, let Ci = (S0, R1, . . . , RN ) denote the\nﬂattened thread, where S0 is the source post, and\nRj refers to the j-th reply post. As shown in the\nbottom of Fig. 2, we assume that Ci is decomposed\ninto k subthreads, each subthread consists of n con-\nsecutive posts, and each post consists of m tokens2.\nFor the j-th post in the thread Ci, let us use Pj =\n(xj\nCLS, xj\n1, . . . ,xj\nm−2, xj\nSEP) to denote its input rep-\nresentations, where each token x is represented by\nsumming up its word embeddings, segment embed-\ndings and position embeddings. For the l-th sub-\nthread in Ci, we use Bl = (Pl0, Pl1, . . . ,Pl(n−1))\nto refer to it.\nLocal Context Encoding (LCE): Next, we em-\nploy the pre-trained BERT to separately process\nthe k subthreads to capture the local interactions\nbetween adjacent posts within each subthread:\nhl = BERT(Bl), l = 1,2,...,k (1)\nwhere hl ∈Rnm×d is the hidden representation\ngenerated for the l-th subthread.\nGlobal Context Encoding (GCE): To further\ncapture the global interactions between all the posts\nin the whole conversation thread, we propose to\nﬁrst concatenate the hidden representations of each\nsubthread: h = h1 ⊕h2 ⊕. . .⊕hk. We then feed\nh to a standard Transformer layer as follows:\n2Note that for parallel computing, each post is padded or\ntruncated to have the same number of tokens, i.e.,m, and each\nsubthread is padded to have the same number of posts, i.e., n.\n˜h = LN(h + MH-ATT(h)), (2)\nH = LN(˜h + FFN(˜h)), (3)\nwhere MH-ATT and FFN respectively refer to the\nmulti-head self-attention and the feed forward net-\nwork (Vaswani et al., 2017), and LN refers to layer\nnormalization (Ba et al., 2016).\nOutput Layers: Based on the global hidden rep-\nresentation H, we further stack the output layers\nto make predictions for SC and RV , respectively.\nSpeciﬁcally, for the SC task, we treat the hidden\nstate of the j-th [CLS] token as the representation\nfor the j-th post, followed by adding a softmax\nlayer to classify its stance towards the source claim:\np(sj |Hj\nCLS) =softmax(W⊤\ns Hj\nCLS + bs), (4)\nwhere Ws ∈Rd×4 and bs ∈R4 are learnable\nparameters. Moreover, for the RV task, we add a\nsoftmax layer over the last hidden state of the ﬁrst\n[CLS] token for rumor veracity prediction:\np(y|H0\nCLS) =softmax(W⊤\nr H0\nCLS + br), (5)\nwhere Wr ∈Rd×3 and br ∈R3 are weight and\nbias parameters.\n3.3 Coupled Hierarchical Transformer for\nStance-Aware Rumor Veriﬁcation\nBased on the above single-task model (i.e., Hier-\narchical Transformer), we describe our proposed\nmulti-task learning (MTL) framework for stance-\naware rumor veriﬁcation in this subsection.\nBaseline MTL Framework: To exploit the stance\nsignals for rumor veriﬁcation, a widely used MTL\n1396\n1st Subthread …...\nLCE with BERT\n LCE with BERT\n LCE with BERT\n…...[CLS] [CLS][CLS] ...... [CLS][CLS] [CLS]... ... [CLS] ...[CLS] ... [CLS]\nskn-2\ns1\nStance Label\ns0\n[CLS] [CLS] [CLS]... ...\ny\nRumor Label\n[CLS] [CLS] [CLS]... ...\n…...\n…...…...\n…... …... …...\n[CLS] ...\nskn-1\n[CLS] ...\nInput Conversation Thread\n2nd Subthread kth Subthread\nGCE with Rumor-\nSpecific Transformer\nGCE with Stance-\nSpecific Transformer\nFigure 3: Baseline Multi-Task Learning Framework (MTL2)\nfor Stance-Aware Rumor Veriﬁcation.\nframework is the MTL2 model proposed in Kochk-\nina et al. (2018), which assumes that the SC and\nRV tasks share the low-level neural layers but the\nhigh-level layers are speciﬁc to each task. As il-\nlustrated in Fig. 3, to adapt our Hierarchical Trans-\nformer to this MTL2 framework, we propose to\nshare the input and LCE modules between SC and\nRV , followed by employing separate GCE and out-\nput modules for these two tasks, respectively.\nMotivation: However, as mentioned before, this\nbaseline MTL framework has two major limita-\ntions. First, it fails to consider the inter-task inter-\naction. Since the GCE module in SC is supervised\nto capture salient stance-speciﬁc features such as\nno doubt, agree and fake news, these features can\nbe leveraged to guide the GCE module in RV to cap-\nture those important rumor-speciﬁc features closely\nrelated to stance features. Moreover, since both\nstance-speciﬁc and rumor-speciﬁc features are in-\ntuitively crucial to RV , it is necessary to effectively\nintegrate them. Second, it ignores the sequential\nstance labels predicted from the output module in\nSC. Actually, the predicted stance distributions for\neach post can capture the temporal evolution of\npublic stances towards the source claim, which\nmay reﬂect indicative clues for veracity prediction.\nCoupled Transformer Module: To model inter-\ntask interactions, we devise a Coupled Transformer\nModule with two coupled components in Fig. 4: a\nstance-speciﬁc Transformer and a cross-task Trans-\nformer.\nConcretely, we ﬁrst employ a standard Trans-\nformer layer (i.e., Eqn (2) and Eqn (3)) to obtain\nstance-speciﬁc representations P in the right chan-\nnel. Next, to learn the inter-task interactions in the\nleft channel, we design a multi-head stance-aware\nattention mechanism (MH-SATT) by treating P\nas queries, and h as keys and values, which es-\nsentially leverages stance-speciﬁc features in P to\nSelf-Attention\nQ         K         V\nAdd & Norm\nAdd & Norm\nFeed Forward\nStance-Aware \nAttention\nK         V        Q\nAdd & Norm\nFeed Forward\nGCE with Cross-\nTask Transformer\nGCE with Stance-\nSpecific Transformer\nAdd & Norm\n…...\nPost-Level Attention skn-2s1s0\ny Rumor Label\nSupport\nDeny\nQuery\nComment\n[CLS] [CLS] [CLS]... .........[CLS] [CLS] [CLS]... .........\nStance Label\n…...\n…...\n…...\n[CLS] ... [CLS] ...\nskn-1\n+ + + +\n[CLS]\n +\n1st Subthread …...\nLCE with BERT\n LCE with BERT\n LCE with BERT\n[CLS] [CLS][CLS] ...... [CLS][CLS] [CLS]... ... [CLS] ...[CLS] ... [CLS]…... …... …...\nInput Conversation Thread\n2nd Subthread kth Subthread\nPV\np0 p1 pkn-1p0 p1 pkn-1\nU\nh\nFigure 4: Our Multi-Task Learning Framework (Coupled Hi-\nerarchical Transformer) for Stance-Aware Rumor Veriﬁcation.\nguide our model to pay more attention to stance-\naware rumor-speciﬁc features. Speciﬁcally, the i-th\nhead of MH-SATT is deﬁned as follows:\nSATTi(P,h) =softmax([WqP]⊤[Wkh]√\nd/z\n)[Wvh]⊤, (6)\nwhere {Wq, Wk, Wv}∈ Rd/z×d are parameters,\nand z is the number of heads.\nMoreover, to integrate stance-speciﬁc and rumor-\nspeciﬁc features, we propose to add a layer norm\ntogether with a residual connection as follows:\n˜V = LN(P + MH-SATT(P,h)). (7)\nFinally, we add a feed-forward network and a layer\nnormalization to get the rumor-stance hybrid repre-\nsentations V:\nV = LN( ˜V + FFN( ˜V)). (8)\nPost-Level Attention with Stance Labels: To\naddress the second limitation, we propose to con-\ncatenate each post’s stance distribution and its cor-\nresponding hidden representation, followed by a\npost-level attention layer to automatically learn the\nimportance of each post.\nSpeciﬁcally, as shown in Fig. 4, we ﬁrst use\nEqn (4) to predict the stance distribution of thej-th\npost in the right channel, denoted by pj. We then\ntreat the hybrid representation of the j-th [CLS]\ntoken (i.e., Vj\nCLS) as the representation of the j-th\npost, and concatenate it with pj, followed by feed-\ning them to a post-level attention layer to obtain\nthe stance label-aware thread representation U:\n1397\nStance Labels Rumor Veracity Labels\nDataset #Threads #Tweets # Support #Deny #Query #Comment #True #False #Unveriﬁed\nSemEval-17 325 5,568 1,004 415 464 3,685 145 74 106\nPHEME 2,402 105,354 - 1,067 638 697\nTable 1: Basic statistics of the SemEval-2017 dataset and the PHEME dataset.\nuj = v⊤ tanh\n(\nWh(Vj\nCLS ⊕pj)\n)\n, (9)\nαj = exp(uj)∑N\nl=1 exp(ul)\n, (10)\nU =\nN∑\nj=1\nαj(Vj\nCLS ⊕pj). (11)\nOutput Layers: Finally, since V0\nCLS and U can\nbe considered as the token-level thread representa-\ntion and the post-level thread representation respec-\ntively, we propose to concatenate them to predict\nthe veracity label of the source claim:\np(y|V0\nCLS,U) =softmax\n(\nW⊤(V0\nCLS ⊕U) +b\n)\n, (12)\nwhere W ∈R(2d+4)×3 and b ∈R3 are weight and\nbias terms.\nModel Training: To optimize all the parameters\nin our Coupled Hierarchical Transformer, we adopt\nthe alternating optimization strategy to minimize\nthe following objective function, which is a combi-\nnation of the cross-entropy loss of the two tasks:\nJ= −\n( 1\nM\nM∑\ni=1\nlog p(yi |V0\nCLS,U)\n+ 1\nM′\nM′\n∑\nk=1\nN∑\nj=1\nlog p(sj |Pj\nCLS)\n)\n, (13)\nwhere M and M′refer to the number of samples\nfor the tasks of RV and SC, respectively.\n4 Experiments\nIn this section, we ﬁrst evaluate our single-task\nmodel on both stance classiﬁcation (SC) and rumor\nveriﬁcation (RV), followed by evaluating our multi-\ntask learning model on RV . Finally, we perform\nfurther analysis to provide deeper insights into our\nproposed multi-task learning model.\n4.1 Experiment Setting\nDataset: To demonstrate the effectiveness of our\nproposed approaches, we carry out experiments on\ntwo benchmark datasets, i.e., SemEval-2017 and\nPHEME. Table 1 shows the basic statistics of the\ntwo datasets.\nSpeciﬁcally, SemEval-2017 is a widely used\ndataset from SemEval-2017 Challenge Task 8,\nwhich contains 325 Twitter conversation threads\ndiscussing rumors (Derczynski et al., 2017). The\ndataset has been split into training, development,\nand test sets, where the former two sets are related\nto eight events and the test set covers two addi-\ntional events. Since each thread is annotated with a\nrumor veracity label and each post in the thread is\nannotated with its stance towards the source claim,\nthis dataset is used for evaluating both SC and RV\ntasks in this work.\nPHEME is a well known dataset for RV , which\ncontains 2402 Twitter conversation threads dis-\ncussing nine events. For fair comparison with ex-\nisting approaches, we perform cross-validation ex-\nperiments based on leave-one-event-out settings:\nfor each fold, all the threads related to one event\nare used for testing, and all the threads related to\nthe other eight events are used for training. Follow-\ning previous studies (Kochkina et al., 2018; Wei\net al., 2019), PHEME is only used for evaluating\nthe performance of RV .\nSince the class distribution of the two datasets\nare imbalanced, we employ Macro-F1 as the main\nevaluation metric and accuracy as the secondary\nevaluation metric for both tasks.\nParameter Settings: Our models are based on the\npre-trained uncased BERTbase model (Devlin et al.,\n2019), where the number of BERT layers is 12 and\nthe number of attention heads is z = 12. Moreover,\nfor both Hierarchical Transformer and Coupled Hi-\nerarchical Transformer, we set the learning rate as\n5e-5, and the dropout rate as 0.1. Due to memory\nlimitation, for each conversation thread, the num-\nber of subthreads is set to k = 6, and the maximum\ninput length of each subthread is set as 512. For\neach subthread, the number of posts is set ton = 17,\nand the number of tokens in each post is ﬁxed to m\n= 30. Moreover, the batch size is respectively set as\n4 and 2 for Hierarchical Transformer and Coupled\nHierarchical Transformer, respectively. We imple-\nment all the models based on PyTorch with a24GB\nNVIDIA TITAN RTX GPU.\n1398\nSingle Stance Type Evaluation Overall Evaluation\nMethod Support-F1 Deny-F1 Query-F1 Comment-F1 Macro-F1 Accuracy\nSVM (Pamungkas et al., 2018) 0.410 0.000 0.580 0.880 0.470 0.795\nBranchLSTM (Kochkina et al., 2018) 0.403 0.000 0.462 0.873 0.434 0.784\nTemporal ATT (Veyseh et al., 2017) - - - - 0.482 0.820\nConversational-GCN (Wei et al., 2019) 0.311 0.194 0.646 0.847 0.499 0.751\nHierarchical Transformer (Ours) 0.421 0.255 0.520 0.841 0.509 0.763\nTable 2: Results of stance classiﬁcation on the SemEval-2017 dataset.\nSemEval-2017 Dataset PHEME Dataset\nSetting Method Macro-F1 Accuracy Macro- F1 Accuracy\nBranchLSTM (Kochkina et al., 2018) 0.491 0.500 0.259 0.314\nTD-RvNN (Ma et al., 2018b) 0.509 0.536 0.264 0.341\nSingle-Task Hierarchical GCN-RNN (Wei et al., 2019) 0.540 0.536 0.317 0.356\nHiTPLAN (Khoo et al., 2020) 0.581 0.571 0.361 0.438\nHierarchical Transformer (Ours) 0.592 0.607 0.372 0.441\nBranchLSTM+NileTMRG (Kochkina et al., 2018) 0.539 0.570 0.297 0.360\nMTL2 (Veracity+Stance) (Kochkina et al., 2018) 0.558 0.571 0.318 0.357\nMulti-Task Hierarchical PSV (Wei et al., 2019) 0.588 0.643 0.333 0.361\nMTL2-Hierarchical Transformer (Ours) 0.657 0.643 0.375 0.454\nCoupled Hierarchical Transformer (Ours) 0.680† 0.678† 0.396† 0.466†\nTable 3: Results of rumor veracity prediction. Single-Task indicates that stance labels are not used during the training stage. †\nindicates that our Coupled Hieararchical Transformer model is signiﬁcantly better than the best compared system with p-value <\n0.05 based on McNemar’s signiﬁcance test.\n4.2 Main Results\n4.2.1 Evaluation on Single-Task Models\nIn this subsection, we compare our proposed Hi-\nerarchical Transformer with existing single-task\nmodels for SC and RV , respectively.\nStance Classiﬁcation (SC): We ﬁrst consider the\nfollowing competitive approaches that focus on\nSC only: (1) SVM is a baseline method that\nfeeds conversation-based and affective-based fea-\ntures to linear SVM (Pamungkas et al., 2018); (2)\nBranchLSTM is an LSTM-based architecture de-\nsigned by Kochkina et al. (2018), which focuses on\nmodeling the sequential branches in each thread;\n(3) Temporal ATT is an attention-based model pro-\nposed by Veyseh et al. (2017), which treats each\npost’s adjacent posts in a conversation timeline as\nits local context, followed by employing attention\nmechanism over the local context to learn the im-\nportance of each adjacent post; (4) Conversational\nGCN is the state-of-the-art approach recently pro-\nposed by Wei et al. (2019), which leverages graph\nconvolutional network to model the relations be-\ntween posts in each thread.\nWe report the SC results in Table 2. First, it is\nclear to observe that our Hierarchical Transformer\nmodel performs much better than all the compared\nsystems on Macro- F1. Second, compared with\nprevious approaches, our model shows its strong\ncapability of detecting posts belonging to the sup-\nport and deny stances. This is crucial for veracity\nprediction, because the support and deny stances\nusually provide important clues to identify the true\nand false rumors respectively (see Fig. 5). All these\nobservations demonstrate the general effectiveness\nof our Hierarchical Transformer model.\nRumor Veriﬁcation (RV): We then consider sev-\neral competitive systems that focus on RV only: (1)\nRvNN is a recursive neural network model based on\ntop-down tree structure, which is proposed by Ma\net al. (2018b); (2) Hierarchical GCN-RNN is a\nvariant of Conversational GCN for veracity pre-\ndiction; (3) PLAN is the state-of-the-art approach\nrecently proposed by Khoo et al. (2020), which\nuses a randomly initialized Transformer to encode\neach conversation thread.\nWe report the RV results of compared systems\non SemEval-2017 and PHEME in the top part of\nTable 3. First, compared with earlier methods for\nRV , we observe that our Hierarchical Transformer\nmodel gains signiﬁcant improvements, outperform-\ning Hierarchical GCN-RNN by 5.2 and 5.5 abso-\nlute percentage points on Macro- F1 for the two\ndatasets, respectively. Second, even compared with\nthe recent state-of-the-art model PLAN, our model\n1399\nMethods (TFM: Transformer) Macro- F1 Accuracy\nHierarchical TFM 0.372 0.441\n- Truncating Input & Removing Global TFM 0.354 0.409\nCoupled Hierarchical TFM 0.396 0.466\n- Removing Post-Level Attention 0.385 0.430\n- Replacing Cross-Task TFM with TFM 0.390 0.456\nTable 4: Ablation study on the PHEME dataset.\ncan still bring moderate performance gains on the\ntwo datasets. Since PLAN is based on randomly\ninitialized Transformer whereas our model is based\non pre-trained Transformer (i.e., BERT), this shows\nthe usefulness of employing pre-trained models for\nRV , which agrees with our ﬁrst motivation.\n4.2.2 Evaluation on Multi-Task Models\nIn this subsection, we evaluate the effectiveness\nof our Coupled Hierarchical Transformer model,\nand consider several multi-task learning frame-\nworks for stance-aware rumor veriﬁcation: (1)\nBranchLSTM+NileTMRG is a pipeline approach,\nwhich ﬁrst trains a BranchLSTM model for SC, fol-\nlowed by a SVM classiﬁer for RV (Kochkina et al.,\n2018); (2) MTL2 is the MTL framework proposed\nin (Kochkina et al., 2018), which shares a single\nLSTM channel but uses two separate output lay-\ners for SC and RV , respectively; (3)Hierarchical\nPSV is a hierarchical model proposed by (Wei et al.,\n2019), which ﬁrst learns content and stance features\nvia Conversational-GCN, followed by exploiting\ntemporal evolution for RV via Stance-Aware RNN;\n(4) MTL2-Hierarchical Transformer is our adapted\nMTL2 model which is introduced in Section 3.3.\nIn the bottom part of Table 3, we can ﬁrst ﬁnd\nthat all the multi-task learning models achieve bet-\nter performance than their corresponding single-\ntask baselines across the two datasets, which ver-\niﬁes the usefulness of stance signals for RV . Sec-\nond, among all the multi-task learning approaches,\nit is clear to observe that our Coupled Hierarchi-\ncal Transformer model consistently achieves the\nbest results on both SemEval-2017 and PHEME,\nwhich outperforms the second best method by 2.3\nand 2.1 absolute percentage points on Macro-F1 for\nthe two datasets, respectively. These observations\nshow the superiority of our proposed model over\nprevious multi-task learning methods for stance-\naware rumor veriﬁcation.\n4.3 Ablation Study\nTo examine the impact of each key component in\nour single-task and multi-task approaches, we fur-\nFigure 5: Correlation between predicted stance classes (y-\naxis) and predicted rumor labels (x-axis) from our Coupled\nHierarchical Transformer on test sets of our two datasets.\nther perform ablation study in this subsection.\nAs shown in Table 4, for our proposed Hierarchi-\ncal Transformer, we can see that if we directly ap-\nply BERT to our RV task (i.e., truncating the input\nthread and removing the global Transformer layer),\nthe performance will drop signiﬁcantly. This is\nin line with our ﬁrst motivation, and also demon-\nstrates the effectiveness of our proposed model.\nMoreover, for our multi-task learning framework\n(i.e., Coupled Hierarchical Transformer), the post-\nlevel attention layer shows its indispensable role\nbecause of the signiﬁcant performance drop after re-\nmoval. Meanwhile, replacing our cross-task Trans-\nformer with the standard Transformer will lead to\nmoderate performance drop in both datasets, which\nalso suggests its importance to our full model.\n4.4 Correlation Between Predicted Stance\nLabels and Veracity Labels\nTo better understand the usefulness of stance sig-\nnals to veracity prediction in our Coupled Hierar-\nchical Transformer, we ﬁrst analyze the correlation\nbetween predicted stance classes and predicted ve-\nracity labels on our two datasets. Since the com-\nment stance is not crucial for rumor veriﬁcation, we\nfocus on the other three stance classes, i.e., deny,\nquery, and support.\nAs shown in Fig. 5, we can clearly see that true\nrumor is more closely associated with the support\nstance, whereas false rumor is generally dominated\nby the other two stances deny and query. This\nsuggests that our multi-task learning model has\nimplicitly learnt that the stance signal can provide\nimportant clues to rumor veriﬁcation.\nCase Study: To provide deeper insights into our\nCoupled Hierarchical Transformer, we carefully\nchoose one representative sample from our test\nset, and show the stance and veracity prediction\nresults as well as the attention weights of each post\nlearnt in the post-level attention layer. Due to space\nlimitation, we only show ﬁve posts with the top-5\nattention weights in the thread.\n1400\nThese are not timid colours; soldiers back guarding tomb of \nunknown soldier after today's shooting #standforcanada\nPredicted \nStance\n@user1@user2 apparently a hoax. best to take tweet \ndown.\nSource Post\nReply Post\n@user3 not a hoax. This is before the shooting\nReply Post\n@user1@user4 I don't believe there are soldiers \nguarding this area right now. 2/2\nReply Post\n@user5@user1 who wants to have a \"go\"???\nReply Post\n.....\nSupport\nDeny\nSupport\nDeny\n.....\nQuery\n.....\nPost-Level \nAttention Weight\n0.004\n0.625\n0.001\n0.368\n0.0002\nPredicted Veracity Label: False Rumor\nFigure 6: Stance classes and rumor labels predicted by Cou-\npled Hierarchical Transformer on a test sample in PHEME\ndataset.\nIn Fig. 6, we can see that although the source\nclaim is supported by some replies, our model\nlearns to pay much higher attention weights to the\ntwo posts with deny stance while primarily ignor-\ning the other posts, which may help our model\ncorrectly predict its veracity label as false rumor.\n5 Conclusion\nIn this paper, we ﬁrst examined the limitations of\nexisting approaches to stance classiﬁcation (SC)\nand rumor veriﬁcation (RV). To tackle these limi-\ntations, we ﬁrst proposed a single-task model (i.e.,\nHierarchical Transformer) for SC and RV , followed\nby designing a multi-task learning framework with\na Coupled Transformer module to capture inter-\ntask interactions and a Post-Level Attention Layer\nto use stance distributions for the RV task. Experi-\nments on two benchmarks show the effectiveness\nof our single-task and multi-task learning methods.\nAcknowledgments\nWe would like to thank three anonymous review-\ners for their valuable comments. This research is\nsupported by DSO grant DSOCL18009, the Natu-\nral Science Foundation of China (No. 61672288,\n62076133, and 62006117), and the Natural Science\nFoundation of Jiangsu Province for Young Schol-\nars (SBK2020040749) and Distinguished Young\nScholars (SBK2020010154).\nReferences\nAhmet Aker, Leon Derczynski, and Kalina Bontcheva.\n2017. Simple open stance classiﬁcation for rumour\nanalysis. In Proceedings of the International Con-\nference Recent Advances in Natural Language Pro-\ncessing, RANLP 2017, pages 31–39.\nHunt Allcott and Matthew Gentzkow. 2017. Social me-\ndia and fake news in the 2016 election. Journal of\neconomic perspectives, 31(2):211–36.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nCarlos Castillo, Marcelo Mendoza, and Barbara\nPoblete. 2011. Information credibility on twitter. In\nProceedings of WWW.\nTong Chen, Xue Li, Hongzhi Yin, and Jun Zhang. 2018.\nCall attention to rumors: Deep attention based recur-\nrent neural networks for early rumor detection. In\nProceedings of PAKDD.\nLeon Derczynski, Kalina Bontcheva, Maria Liakata,\nRob Procter, Geraldine Wong Sak Hoi, and Arkaitz\nZubiaga. 2017. Semeval-2017 task 8: Rumoureval:\nDetermining rumour veracity and support for ru-\nmours. In Proceedings of SemEval.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nWilliam Ferreira and Andreas Vlachos. 2016. Emer-\ngent: a novel data-set for stance classiﬁcation. In\nProceedings of NAACL.\nGeorgios Giasemidis, Nikolaos Kaplis, Ioannis Agraﬁ-\notis, and Jason Nurse. 2018. A semi-supervised\napproach to message stance classiﬁcation. IEEE\nTKDE, 32(1):1–11.\nKazi Saidul Hasan and Vincent Ng. 2013. Stance clas-\nsiﬁcation of ideological debates: Data, models, fea-\ntures, and constraints. In Proceedings of IJCNLP.\nLing Min Serena Khoo, Hai Leong Chieu, Zhong Qian,\nand Jing Jiang. 2020. Interpretable rumor detection\nin microblogs by attending to user interactions. In\nProceedings of AAAI.\nElena Kochkina and Maria Liakata. 2020. Estimating\npredictive uncertainty for rumour veriﬁcation mod-\nels. In Proceedings of ACL.\nElena Kochkina, Maria Liakata, and Isabelle Augen-\nstein. 2017. Turing at semeval-2017 task 8: Sequen-\ntial approach to rumour stance classiﬁcation with\nbranch-lstm. In Proceedings of SemEval.\nElena Kochkina, Maria Liakata, and Arkaitz Zubiaga.\n2018. All-in-one: Multi-task learning for rumour\nveriﬁcation. In Proceedings of COLING.\nSumeet Kumar and Kathleen Carley. 2019. Tree\nLSTMs with convolution units to predict stance and\nrumor veracity in social media conversations. In\nProceedings of ACL.\nSejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei\nChen, and Yajun Wang. 2013. Prominent features\nof rumor propagation in online social media. In Pro-\nceedings of ICDM.\n1401\nQuanzhi Li, Qiong Zhang, and Luo Si. 2019. Rumor\ndetection by exploiting user credibility information,\nattention and multi-task learning. In Proceedings of\nACL.\nXiaomo Liu, Armineh Nourbakhsh, Quanzhi Li, Rui\nFang, and Sameena Shah. 2015. Real-time rumor\ndebunking on twitter. In Proceedings of CIKM.\nMichal Lukasik, PK Srijith, Duy Vu, Kalina Bontcheva,\nArkaitz Zubiaga, and Trevor Cohn. 2016. Hawkes\nprocesses for continuous time sequence classiﬁca-\ntion: an application to rumour stance classiﬁcation\nin twitter. In Proceedings of ACL.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of IJCAI.\nJing Ma, Wei Gao, Zhongyu Wei, Yueming Lu, and\nKam-Fai Wong. 2015. Detect rumors using time se-\nries of social context information on microblogging\nwebsites. In Proceedings of CIKM.\nJing Ma, Wei Gao, and Kam-Fai Wong. 2017. Detect\nrumors in microblog posts using propagation struc-\nture via kernel learning. In Proceedings of ACL.\nJing Ma, Wei Gao, and Kam-Fai Wong. 2018a. Detect\nrumor and stance jointly by neural multi-task learn-\ning. In Companion Proceedings of the The Web Con-\nference 2018.\nJing Ma, Wei Gao, and Kam-Fai Wong. 2018b. Ru-\nmor detection on twitter with tree-structured recur-\nsive neural networks. In Proceedings of ACL.\nMarcelo Mendoza, Barbara Poblete, and Carlos\nCastillo. 2010. Twitter under crisis: can we trust\nwhat we rt? In Proceedings of the 3rd Workshop\non Social Network Mining and Analysis, SNAKDD\n2009, Paris, France, June 28, 2009.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemeval-2016 task 6: Detecting stance in tweets. In\nProceedings of SemEval.\nEW Pamungkas, V Basile, and V Patti. 2018. Stance\nclassiﬁcation for rumour analysis in twitter: Exploit-\ning affective information and conversation structure.\nIn 2nd International Workshop on Rumours and De-\nception in Social Media (RDSM 2018), volume 2482,\npages 1–7.\nRaghavendra Pappagari, Piotr Zelasko, Jes ´us Villalba,\nYishay Carmiel, and Najim Dehak. 2019. Hierarchi-\ncal transformers for long document classiﬁcation. In\n2019 IEEE Automatic Speech Recognition and Un-\nderstanding Workshop (ASRU).\nVahed Qazvinian, Emily Rosengren, Dragomir R\nRadev, and Qiaozhu Mei. 2011. Rumor has it: Iden-\ntifying misinformation in microblogs. In Proceed-\nings of EMNLP.\nNatali Ruchansky, Sungyong Seo, and Yan Liu. 2017.\nCsi: A hybrid deep model for fake news detection.\nIn Proceedings of CIKM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS.\nAmir Pouran Ben Veyseh, Javid Ebrahimi, Dejing Dou,\nand Daniel Lowd. 2017. A temporal attentional\nmodel for rumor stance classiﬁcation. In Proceed-\nings of CIKM.\nPenghui Wei, Nan Xu, and Wenji Mao. 2019. Mod-\neling conversation structure and temporal dynamics\nfor jointly predicting rumor stance and veracity. In\nProceedings of EMNLP.\nKe Wu, Song Yang, and Kenny Q. Zhu. 2015. False ru-\nmors detection on sina weibo by propagation struc-\ntures. In Proceedings of ICDE.\nFan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.\nAutomatic detection of rumor on sina weibo. In Pro-\nceedings of the ACM SIGKDD Workshop on Mining\nData Semantics, MDS ’12, pages 13:1–13:7.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\nProceedings of NAACL.\nArkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva,\nMaria Liakata, and Rob Procter. 2018a. Detection\nand resolution of rumours in social media: A survey.\nACM Computing Surveys (CSUR), 51(2):32.\nArkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob\nProcter, Michal Lukasik, Kalina Bontcheva, Trevor\nCohn, and Isabelle Augenstein. 2018b. Discourse-\naware rumour stance classiﬁcation in social media\nusing sequential classiﬁers. Information Processing\n& Management, 54(2):273–290.\nArkaitz Zubiaga, Maria Liakata, Rob Procter, Geral-\ndine Wong Sak Hoi, and Peter Tolmie. 2016.\nAnalysing how people orient to and spread rumours\nin social media by looking at conversational threads.\nPloS one, 11(3):e0150989.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7561677098274231
    },
    {
      "name": "Transformer",
      "score": 0.7480260133743286
    },
    {
      "name": "Exploit",
      "score": 0.6279115676879883
    },
    {
      "name": "Thread (computing)",
      "score": 0.5422121286392212
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47507423162460327
    },
    {
      "name": "ENCODE",
      "score": 0.4641062021255493
    },
    {
      "name": "Rumor",
      "score": 0.45956993103027344
    },
    {
      "name": "Machine learning",
      "score": 0.4480573832988739
    },
    {
      "name": "Engineering",
      "score": 0.08717253804206848
    },
    {
      "name": "Public relations",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}