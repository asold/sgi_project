{
  "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer",
  "url": "https://openalex.org/W4393160524",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5087054952",
      "name": "Ji Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102845302",
      "name": "Dehua Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101877617",
      "name": "Yuanxian Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100425709",
      "name": "Li Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019286517",
      "name": "Xiao Cheng Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100764250",
      "name": "Dong Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102600730",
      "name": "Mingjie Lu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5003282158",
      "name": "Jinzhang Peng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100445300",
      "name": "Yu Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101089766",
      "name": "Fan Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100787275",
      "name": "Lu Tian",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086949543",
      "name": "Ashish Sirasao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3138853929",
    "https://openalex.org/W2969797940",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2897295818",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3138141365",
    "https://openalex.org/W6788611029",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W6780268255",
    "https://openalex.org/W6769752801",
    "https://openalex.org/W4281644254",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2928560789",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W4280538535",
    "https://openalex.org/W3204182250",
    "https://openalex.org/W6803712692",
    "https://openalex.org/W6846659131",
    "https://openalex.org/W6776188000",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W2796438033",
    "https://openalex.org/W6795103355",
    "https://openalex.org/W2886953980",
    "https://openalex.org/W6793164127",
    "https://openalex.org/W6796870316",
    "https://openalex.org/W3094522247",
    "https://openalex.org/W4394668313",
    "https://openalex.org/W4281747801",
    "https://openalex.org/W3132013769",
    "https://openalex.org/W3098072209",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4313484649",
    "https://openalex.org/W6749954789",
    "https://openalex.org/W6839401062",
    "https://openalex.org/W2921499784",
    "https://openalex.org/W2997365222",
    "https://openalex.org/W6850675185",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W3131135472",
    "https://openalex.org/W4386072014",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2994749257",
    "https://openalex.org/W3170420158",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W4386076083",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W4287268252",
    "https://openalex.org/W4312340826",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W4313170858",
    "https://openalex.org/W3109946440",
    "https://openalex.org/W2984618279",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3106609780",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W4378464458",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W3194959296",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W3109037380",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W3211718241",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3167976421",
    "https://openalex.org/W4287595368",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3171964719",
    "https://openalex.org/W4214633470",
    "https://openalex.org/W4386076493",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W4283811336",
    "https://openalex.org/W2981698279",
    "https://openalex.org/W4311730469",
    "https://openalex.org/W4214588794"
  ],
  "abstract": "Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet with directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 models with our method applying on ConvNeXtV1, which surpass most SOTA efficient models with comparable inference performance. Our method also achieves state-of-the-art pruning performance on the vision transformer model.",
  "full_text": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer\nJi Liu*, Dehua Tang*, Yuanxian Huang, Li Zhang, Xiaocheng Zeng, Dong Li, Mingjie Lu,\nJinzhang Peng, Yu Wang, Fan Jiang, Lu Tian, Ashish Sirasao\nAdvanced Micro Devices, Inc., Beijing, China\n{liuji, dehua.tang, yuanxian, xze, d.li, mingjiel, jinz.peng, yu.w, f.jiang, lu.tian, ashish.sirasao}@amd.com\nAbstract\nTraditional channel-wise pruning methods by reducing net-\nwork channels struggle to effectively prune efficient CNN\nmodels with depth-wise convolutional layers and certain ef-\nficient modules, such as popular inverted residual blocks.\nPrior depth pruning methods by reducing network depths\nare not suitable for pruning some efficient models due to\nthe existence of some normalization layers. Moreover, fine-\ntuning subnet by directly removing activation layers would\ncorrupt the original model weights, hindering the pruned\nmodel from achieving high performance. To address these\nissues, we propose a novel depth pruning method for effi-\ncient models. Our approach proposes a novel block prun-\ning strategy and progressive training method for the sub-\nnet. Additionally, we extend our pruning method to vision\ntransformer models. Experimental results demonstrate that\nour method consistently outperforms existing depth pruning\nmethods across various pruning configurations. We obtained\nthree pruned ConvNeXtV1 models with our method applying\non ConvNeXtV1, which surpass most SOTA efficient mod-\nels with comparable inference performance. Our method also\nachieves state-of-the-art pruning performance on the vision\ntransformer model.\nIntroduction\nDeep neural networks (DNNs) have made significant strides\nacross various tasks, culminating in remarkable successes\nwithin industrial applications. Among these applications,\nthe pursuit of model optimization stands out as a prevalent\nneed, offering the potential to elevate model inference speed\nwhile minimizing accuracy trade-offs. This pursuit encom-\npasses a range of techniques, notably model pruning, quan-\ntization, and efficient model design. The efficient model de-\nsign includes neural architecture search (NAS) (Cai et al.\n2020; Yu and Huang 2019; Yu et al. 2020; Wang et al.\n2021a) and handcraft design methodologies. Model prun-\ning has emerged as a prevalent strategy for optimizing mod-\nels in industrial applications. Serving as a primary accel-\neration approach, model pruning focuses on the deliberate\nremoval of redundant weights while maintaining accuracy.\nThis process typically involves three sequential steps: ini-\ntial baseline model training, subsequent pruning of less vital\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Performance vs. speedup on the ImageNet-\n1K. Our three pruned ConvNeXtV1 models surpass most\nSOTA efficient models on performance including RegNetY ,\nRepVGG, VanillaNet, ConvNeXtV2, Swin-T, PVT, DeiT,\nEdgeViT, EfficientFormerV2, and FastViT.\nweights or layer channels, and a concluding finetuning phase\nfor the pruned model. Notably, model pruning can be clas-\nsified into two categories: non-structured pruning and struc-\ntured pruning. Structured pruning is the preferred approach\nfor model deployment in industrial applications, primarily\ndue to hardware limitations. In contrast to non-structured\nmethods, where less important weights in convolutional ker-\nnel layers are zeroed out in a sparse manner within each\nkernel channel, structured pruning encompasses techniques\nlike channel-wise pruning and block pruning. Channel-wise\npruning focuses on eliminating entire channel filters within\nthe kernel, while block pruning operates at a larger scale,\ntypically targeting complete blocks. Given that block prun-\ning often leads to a reduction in model depth, it is also re-\nferred to as a depth pruner.\nThe evolution of CNN model design has led to the de-\nvelopment of more efficient models. For instance, Mo-\nbileNetV2 (Sandler et al. 2018) employs numerous depth-\nwise convolutional layers and stacks inverted residual\nblocks, achieving high performance while minimizing pa-\nrameters and flops. ConvNeXtV1 (Liu et al. 2022) leverages\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13891\nthe large kernel trick and incorporates stacked inverted resid-\nual blocks to achieve remarkable efficiency. The conven-\ntional channel-wise pruning method faces challenges with\ndepth-wise convolutional layers due to sparse computation\nand fewer parameters. Moreover, now model platforms fa-\nvor a higher degree of parallel computing like GPUs, and\nchannel-wise pruning methods would make efficient mod-\nels thinner and sparser, which leads to low hardware uti-\nlization and thus inferior achievable hardware efficiency.\nTo address these issues, DepthShrinker (Fu et al. 2022)\nand Layer-Folding (Dror et al. 2021) are proposed to opti-\nmize MobileNetV2 by reducing model depth through repa-\nrameterization techniques (Ding et al. 2021a,b). However,\nthese methods exhibit certain limitations. (1) The mecha-\nnism of finetuning subnet with removing activation layers\ndirectly could potentially compromise the integrity of base-\nline model weights, hindering the attainment of high perfor-\nmance. (2) These methods come with usage constraints, they\nare unable to prune models with some normalization lay-\ners like LayerNorm (Ba, Kiros, and Hinton 2016) or Group-\nNorm (Wu and He 2018) layer, because reparameterization\ntechnique cannot merge normalization layer which is not\nBatchNorm layer into adjacent convolutional layer or full-\nconnection layer. (3) These methods cannot be applied to\nvision transformer models for optimization due to the exis-\ntence of LayerNorm layer.\nTo alleviate these problems, we propose a progressive\ntraining strategy and novel block pruning method for our\ndepth pruning approach that can prune both CNN and vision\ntransformer models. The progressive training strategy can\nsmoothly transfer the baseline model structure to the sub-\nnet structure with high utilization of baseline model weights,\nwhich leads to higher accuracy. Our proposed block pruning\nmethod can handle the existing normalization layer issue,\nwhich can handle all activation and normalization layers in\ntheory. Thus, our method can prune vision transformer mod-\nels, which is not suitable for existing depth pruning meth-\nods. Our experimental evaluation spans across ResNet34,\nMobileNetV2, and ConvNeXtV1, showcasing the superior\npruning capabilities. As shown in Figure 1, pruned Con-\nvNeXtV1 models with our method surpass most SOTA ef-\nficient models with comparable inference performance. No-\ntably, we extend our exploration to vision transformer mod-\nels, achieving leading pruning results compared to other vi-\nsion transformer pruning methods.\nOur main contributions can be summarized as follows. (1)\nWe propose a unified and efficient depth pruning method for\noptimizing both CNN and vision transformer models. (2)\nWe propose a progressive training strategy for subnet op-\ntimization, coupled with a novel block pruning strategy us-\ning reparameterization technique. (3) Conducting compre-\nhensive experiments on both CNN and vision transformer\nmodels to showcase the superior pruning performance of our\ndepth pruning method.\nRelated Work\nNetwork Pruning. Pruning algorithms can be roughly di-\nvided into two types. One is the non-structured pruning al-\ngorithm represented by (Han, Mao, and Dally 2015; Elsen\net al. 2020; Pool and Yu 2021). It removes redundant ele-\nments in the weight according to certain criteria. However,\nnon-structured pruning requires special software or hard-\nware accelerators for the pruned models, so its versatility\nis not strong. In contrast to unstructured pruning, structured\npruning prunes the entire parameter structure, such as dis-\ncarding entire rows or columns of weights, or entire filters\nin convolutional layers.\nWhen VGG (Simonyan and Zisserman 2014) and\nResNet (He et al. 2016) were on the rise, Pruning Filters (Li\net al. 2016) adopts the L1-norm to select unimportant chan-\nnels and prune them. Network FPGM (He et al. 2019) uti-\nlizes the geometric median of the convolutional filter to find\nredundant filters. Subsequently, various efficient DNN net-\nworks, such as MobileNet and its variants (Howard et al.\n2017, 2019; Tan et al. 2019; Radosavovic et al. 2020), incor-\nporated depthwise convolutions (Chollet 2017) to acceler-\nate speed and improve accuracy, enabling real-time deploy-\nment on diverse hardware platforms. MatePruning (Liu et al.\n2019) proposes the concept of PruningNet, which automat-\nically generates weights for the pruned model, thus avoid-\ning retraining. However, while depthwise convolutional of-\nfers advantages in terms of reduced computation and param-\neters, it also presents a drawback—an increased memory\nfootprint, posing a challenge for computationally intensive\nhardware like GPUs and DSPs (Tan and Le 2021). Unfortu-\nnately, channel-wise pruning methods do not offer an intu-\nitive and efficient solution to address this memory footprint\nchallenge.\nThe most relevant to our work is layer-wise pruning,\nwhich can completely remove a block or layer to reduce\nthe depth of the network and effectively alleviate the prob-\nlem of memory usage. Shallowing deep networks (Chen and\nZhao 2018) and LayerPrune (Elkerdawy et al. 2020) pro-\npose their own strategies for evaluating the importance of\nconvolutional layers. ESNB (Zhou, Yen, and Yi 2021) and\nResConv (Xu et al. 2020) identify which layers to be pruned\nby evolutionary search algorithms and differentiable param-\neters, respectively. Layer-Folding (Dror et al. 2021) and\nDepthShinker (Fu et al. 2022) remove non-linear activation\nfunctions within the block and merge multiple layers into a\nsingle layer using structural reparameterization techniques.\nLayer-Folding and DepthShinker have only been verified on\nthe few limited models, and the hard removal of ReLU may\nhave an impact on the accuracy of the subnet.\nThe Transformer family of models excels in performance\nacross various vision tasks (Carion et al. 2020; Strudel et al.\n2021; Brown et al. 2020); however, its high inference cost\nand significant memory footprint hinder widespread adop-\ntion (Pope et al. 2023). To tackle the memory footprint chal-\nlenge, layer-wise pruning presents an effective solution. Dy-\nnamic skipping blocks to remove some layers has become\nthe mainstream transformer compression method (Zhang\nand He 2020; Dong, Cordonnier, and Loukas 2021; Michel,\nLevy, and Neubig 2019). DynamicViT (Rao et al. 2021)\ndynamically screens the number of tokens that need to be\npassed to the next layer. By encouraging dimension-wise\nsparsity, VTP (Zhu, Tang, and Han 2021) selects the dimen-\nsion with strong redundancy for pruning.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13892\nStructural Reparameterization. In the absence of a\nnonlinear activation function within a block, the structural\nreparameterization technique facilitates the consolidation of\nmultiple convolutional layers into a single convolutional\nlayer (Bhardwaj et al. 2022). This consolidation effectively\ndiminishes the neural network’s memory requirements dur-\ning inference, resulting in accelerated model processing.\nRepVGG (Ding et al. 2021b) distinguishes between training\nand testing structures, empowering the plain network to sur-\npass the performance of ResNet. Furthermore, DBB (Ding\net al. 2021a) merges a multi-branch architecture into a single\nconvolution, significantly outpacing the speed of a conven-\ntional multi-branch unit.\nNeural Architecture Search (NAS). Weight-sharing\nNAS has become the mainstream of pruning methods due to\nits flexibility and convenience of training a supernet and de-\nploying multiple subnets. Once-for-All (Cai et al. 2020) uses\na progressive training supernet. BigNAS (Yu et al. 2020)\nuses a series of simple and practical training methods to im-\nprove the efficiency of training supernet. Once the supernet\nis trained, typical search algorithms, such as genetic search,\ncan be applied to find a set of Pareto-optimal networks for\nvarious deployment scenarios.\nIn this work, we propose a unified depth pruning ap-\nproach for both efficient CNN and vision transformer mod-\nels with a progressive training strategy, a novel block prun-\ning method, and the reparameterization technique. Differ-\ning from DepthShrinker and Layer-Folding finetuning the\nsubnet with direct activation layer removal, our method pro-\ngressively removes the activation layer in the pruned block\nduring subnet training. Moreover, our method handles the\nnormalization layer problem that DepthShrinker and Layer-\nFolding cannot prune models with LayerNorm or Group-\nNorm layers in the block. Further, they can not prune vi-\nsion transformer models. Although our work has a similar\ntraining process as VanillaNet (Chen et al. 2023), whereas\nVanillaNet is proposed to design a completely new network\nstructure, our method is a general depth pruning framework\nfor both CNN and vision transformer models.\nMethod\nUnified Progressive Depth Pruner\nOur depth pruning approach aims to reduce model depth by\nproposed novel block pruning strategy with reparameteriza-\ntion technique rather than directly omitting the block. As\nshown in Figure 2, our block pruning strategy converts a\ncomplex and slow block into a simple and fast block in block\nmerging. For a block, we replace the activation layer with\nidentity layer and replace the LayerNorm (LN) or Group-\nNorm (GN) layer with a BatchNorm (BN) layer and insert an\nactivation layer with a BatchNorm layer at the end of block\nto create conditions for reparameterization. Then, the repa-\nrameterization technique can merge the BatchNorm layers,\nadjacent Convolutional or Full-connection layers and skip\nconnections as shown in Figure 2.\nOverview. Our approach primarily consists of four main\nsteps, which are supernet training, subnet searching, subnet\ntraining, and subnet merging. First, We construct a supernet\nbased on the baseline model, where we make block mod-\nification as shown in Figure 2. After supernet training, a\nsearch algorithm is used to search an optimal subnet. Then,\nwe adopt a proposed progressive training strategy to opti-\nmize the optimal subnet with less accuracy loss. In the end,\nthe subnet would be merged into a shallower model with the\nreparameterization technique.\nSupernet Training. Efficient CNN and vision trans-\nformer models usually consist of several basic blocks, like\nsome efficient models structure shown in Figure 2. First,\nwe construct a supernet based on the baseline model and\nthen train a robust supernet model based on the sandwich\nrule (Yu and Huang 2019) method to ensure that each sub-\nnet has a meaningful accuracy. We combine the baseline\nblock and corresponding pruned block into a supernet block,\nwhich has both baseline block and pruned block flows. For\na supernet block, choosing the baseline block flow means\nno pruning and choosing pruned block flow means pruning\nthe block. Then, subnet selection is a series of choices where\nthe choice number is equal to a block number of the baseline\nmodel. The subnet would be faster with more pruned blocks\nselected.\nInspired by BigNAS (Yu et al. 2020), we adopt the sand-\nwich rule to sample the subnetwork before each step. In each\nstep, we sample four sequential subnets, where the first one\nkeeps all blocks unpruned, the following two subnets ran-\ndomly select blocks to be pruned, and the last one keeps\nall blocks pruned. Then, we optimize supernet with accu-\nmulated grads of four subnets. The sandwich rule can ef-\nfectively guarantee the upper and lower limits of the trained\nsupernet. Also many methods (Wang et al. 2021b,a) demon-\nstrate that the sandwich rule can be used to train supernet\nefficiently, even if the number of epochs is small and the ac-\ncuracy distribution of the subnet is the same as that of train-\ning more epochs. In this way, we can reduce the training cost\nof supernet.\nSubnet Searching. The primary objective of our depth\npruner is to identify an optimal subnet based on a specified\npruning criteria, such as the number of blocks to be pruned.\nAs shown in Equation 1, we formulate this problem as an\noptimal problem. For all samples X and their labels Y , the\ngoal of subnet searching is to find a subnetSp with the high-\nest accuracy.p ∈ RNblock is a binary vector, representing the\npruning setting of the subnet. Ifi-th block is pruned,pi is set\nto 1. The number of pruned blocks of each subnet is equal\nto k. Genetic algorithm (Cai et al. 2020) is applied to solve\nthis problem.\narg max\np\nAccuracy(Sp(X), Y) s.t. ||p||0 = k (1)\nAfter the search process, we obtain a subnet that has a\nspecified number of pruned blocks, and other blocks keep\nthe same as the baseline model.\nSubnet Training.We need to train the optimal subnet ob-\ntained from the previous step to restore its accuracy. Rather\nthan directly training the subnet, our approach employs a\nprogressive training strategy to finetune the subnet smoothly\ntransferring from baseline model weights. The subnet train-\ning consists of two stages. During the first training stage,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13893\nFigure 2: Framework overview of our proposed depth pruner. Each pruned baseline block will gradually evolve into a smaller\nmerged block to speedup and save memory. Four baselines are experimented, including three CNN-based networks (ResNet34,\nMobileNetV2 and ConvNeXtV1) and one vision transformer network (DeiT-Tiny).\nwe adopt a progressive training strategy, gradually transfer-\nring from a baseline model structure to the pruned subnet\nstructure by a controlling λ factor. In the second stage, we\ncontinue finetuning the pruned subnet to the end for high\naccuracy.\nAs for the first stage, the gradual transition of λ from 0\nto 1 allows for a controlled process that transfers a baseline\nblock to pruned block as shown in Equation 2, where Bb is\nthe block of baseline model and Bp is the pruned block of\nthe subnet and x is the input of block. Thus all the pruned\nblocks of the subnet go through the same process withλ and\nobtain the pruned subnet smoothly.\no = (1− λ) · Bb(x) +λ · Bp(x) (2)\nOur method controls λ transition from 0 to 1 during the first\ntraining phase, and then keeps λ constant during the sec-\nond phase of training as shown in Equation 3, where K is\nhyper-parameter and C is current training epoch and T is\ntotal training epoch.\nλ =\n\u001a1 − max(0, cos(C·K\nT · π\n2 )), if C ≤ T\nK ,\n1, if C > T\nK . (3)\nIt is worth noting that, to reduce the error in subsequent\nsubnet merging, it is necessary to modify the padding and\nstride of related convolutional layers in pruned blocks be-\nfore subnet training. For example, accumulate all padding\nvalues which are not zero forward to the first convolutional\nlayer of the block and set the padding values of the remain-\ning convolutional layer to zero. Also, accumulate all stride\nvalues which are not equal to one backward to the last con-\nvolutional layer of the block and set the stride value of other\nconvolutional layers to one.\nSubnet Merging. After subnet training, we obtain a sub-\nnet with some activation layers replaced with Identity lay-\ners, some LayerNorm layers replaced with BatchNorm lay-\ners with some activation layers, and BatchNorm inserted at\nthe end of the pruned block. In this stage, we adopt reparam-\neterization techniques to make the subnet shallower.\n• Conv + BN −→Conv\nDuring the inference phase of a neural network, it is pos-\nsible to fuse the operations of BatchNorm layers into Con-\nvolutional (Conv) layers to accelerate model inference. We\nassume that the parameters of the Conv layer are denoted as\nω and b, and the parameters of the BN layer are denoted as\nγ, σ, ϵ, β. After merging of the Conv layer and BN layer, the\nparameters of the Conv layer would be modified as follows:\nˆω = γ · ω\n√\nσ2 + ϵ\nˆb = β + γ · b − µ√\nσ2 + ϵ\n(4)\n• Conv/FC + Conv/FC −→Conv/FC\nTwo adjacent full-connection (FC) layers can be simply\nmerged into a FC layer by the fusion of their weights. Sup-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13894\npose that there are two adjacent FC layers, their weights\nare W1 ∈ RC1×C0 and W2 ∈ RC2×C1 , and their biases\nare b1 ∈ RC1 and b2 ∈ RC2 . For a given input feature\nx ∈ RN×C0 , the output of these two FC layers is expressed\nas W2(W1xT +b1)+ b2, which can be obtained by an equiv-\nalent FC layer whose weight is equal to W2W1 and bias is\nequal to (W2b1 + b2).\nWe will primarily introduce the fusion methods between\nconvolutional layers. For sequential1×1 convolutional layer\nfusing with k×k convolutional layer, we adopt the proposed\nfusion method from DBB (Ding et al. 2021a) to merge the\ntwo layers into an equivalent k × k convolutional layer. For\nsequential k × k convolutional layer fusing with k × k, it\nwould obtain an equivalent (2k − 1) × (2k − 1) convolu-\ntional layer with more parameters and flops compared with\nk × k convolutional layer. We propose a simple way to ad-\ndress this problem, which replaces the firstk ×k Conv layer\nwith a 1×1 Conv layer, where the parameter of1×1 Conv is\ntaken from the central point of thek×k Conv with a retrain-\ning model. Then take the DBB fusion method to transform\nthe two convolutional layers into an equivalentk ×k convo-\nlutional layer.\n• Conv + skip connection(Identity/Conv ) −→Conv\nRepVGG (Ding et al. 2021b) proposes a method to trans-\nform a multi-branch model into an equivalent single-path\nmodel. According to the additivity property of convolutions,\nfor two convolutional layers with the same kernel size, they\nsatisfy the following Equation 5:\nConv(W1, x) +Conv(W2, x) =Conv(W1 + W2, x) (5)\nwhere Conv(W, x) represents the convolutional operation,\nW1 and W2 are the convolutional kernel parameters, and x\nis the input data. RepVGG has demonstrated that the iden-\ntity and 1 ×1 convolutional can be equivalently transformed\ninto a k ×k convolution. Then with the property of convolu-\ntional additivity, the multi-branch convolutional layers can\nbe merged into an equivalent convolutional layer and skip\nconnection identity can be merged into convolutional layer\ntoo.\nDepth Pruner on CNN\nApplying our method on CNN models can refer to Figure 2\nshowing the pipeline. We should find the basic block first,\nand design a corresponding pruned block by reference of\nthe pruned block in Figure 2. For activation layers in the\nblock, we replace it with an Identity layer. For the normal-\nization layer, which is not BatchNorm layer in block, we\nreplace it with a BatchNorm layer, otherwise nothing needs\nto be done. Finally, we would insert an activation layer with\na BatchNorm layer at the end of block. If an activation layer\nalready exists in the position like ResNet34 block, only a\nBatchNorm layer needs be inserted after the activation layer\nat the end of block. After the pruned block is completed, re-\nview the supernet training, subent searching, subnet training,\nand subnet merging processes. We would obtain the pruned\nCNN models. For plain CNN models, we can define the\nblock which can includes two or more sequential convolu-\ntional layers.\nDepth Pruner on Vision Transformer\nWe also apply our proposed depth pruner on vision trans-\nformer models. The vision transformer block usually has\na multi-headed self-attention (MHSA) module and a MLP\nmodule that includes two full-connection layers. Particu-\nlarly, we utilize DeiT (Touvron et al. 2021) as the case show-\ning the pruning flows. As demonstrated in Figure 2, to build\nthe Supernet, we add BN bypasses next to the LN and ac-\ntivation (GELU) layers of the original model and insert a\nGELU&BN block after the residual addition operation. Af-\nter subnet searching and subnet training, we obtain the sub-\nnet, whose original LN and GELU operations of the pruned\nblocks are all replaced by BNs. A GELU&BN block is at-\ntached after the residual addition. Then, we merge the subnet\nto obtain a fast pruned model as shown in Figure 2.\nExperiments\nIn this section, we showcase the efficacy of our depth pruner.\nInitially, we elucidate the experimental configurations and\noutline the procedure for applying the depth pruner to both\nCNN models and Vision Transformers. Subsequently, we\ncompare our results with the state-of-the-art pruning meth-\nods to highlight the superiority of our approach. Finally, we\nperform ablation studies to elaborate on the effect of subnet\nsearching and progressive training strategy in our method.\nDatasets\nAll the experiments are conducted on the ImageNet-\n1K (Russakovsky et al. 2015). ImageNet-1K dataset is a\nwidely used image classification dataset that spans 1000 ob-\nject classes and contains 1,281,167 training images, 50,000\nvalidation images, and 100,000 test images. We apply con-\nventional data augmentation techniques to preprocess input\nimages during training and scale input images to 224 × 224\nfor all experiments with reporting performance on validation\ndataset.\nExperiments Setting on Different Models\nWe apply depth pruner on a series of CNN models, in-\ncluding ResNet34 (He et al. 2016), MobileNetV2, Con-\nvNeXtV1 (Liu et al. 2022), and Vision Transformer (Tou-\nvron et al. 2021) to validate the efficiency of our method.\nWe utilize four GPUs to train our model, with a total batch\nsize of 256. In the training process, we take 10 epochs to\ntrain the supernet, except for MobileNetV2 and search opti-\nmal subnets. Then we train these subnets with the proposed\nprogressive training strategy and complete subnet merging\nto obtain more efficient shallow models.\nResNet34. For ResNet34 pruning experiments, we prune\n6 and 10 blocks respectively and go through the whole prun-\ning process to obtain two pruned and shallow subnets. For\nsubnet training, the hyper-parameters K in Equation 3 is\n3, and the total training epochs is 150. At epoch 100, we\nchange kernel size from 3 × 3 to 1 × 1 of the first convo-\nlutional layer in the pruned block. We compare our method\nwith MetaPruning (Liu et al. 2019) and channel-wise NAS\nmethod Universally Slimmable Networks (US) (Yu and\nHuang 2019) to verify the pruning performance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13895\nModels FLOPs\n(G)\nAcc1\n(%) Speedup\nResNet34-Baseline 3.67 73.6 1.00\nUS-ResNet34-0.6× 2.32 72.0 1.24\nMetaPruning-0.6× 2.32 72.8 1.24\nOurs-P6 2.97 73.2 1.25\nOurs-P6* 2.97 73.5 1.24\nUS-ResNet34-0.5× 1.90 70.8 1.43\nMetaPruning-0.5× 1.87 71.6 1.43\nOurs-P10 2.51 71.8 1.43\nOurs-P10* 2.51 72.4 1.43\nTable 1: Classification performance comparisons on Ima-\ngeNet. P6 and P10 indicate pruning 6 and 10 blocks, respec-\ntively, and ’*’ means a higher accuracy subnet with longer\nsearch time.\nMobileNetV2. For MobileNetV2 pruning experiments,\nwe adopt three pruning configurations from DepthShrinker.\nWe skip the supernet training and subnet searching phases\nto obtain three same subnets. We directly train these three\nsubnets to the end and compare the final performance with\nthe corresponding subnet. We also use the NAS method to\nsearch three subnets with similar speedup ratios to compare\nwith our method. For subnet training, the hyper-parameters\nK is 3, and the total training epochs is 450.\nConvNeXtV1. For ConvNeXtV1 pruning experiments,\nwe set three pruning configurations which let the sub-\nnet obtain the performance around 81% top-1 accuracy on\nImageNet-1k. Then, we compare these subnets with many\nSOTA models which include CNN and vision transformer\nstructures to verify our method pruning performance. For\nsubnet training, the hyper-parameters K is 4.5 and the total\ntraining epochs is 450. To achieve better speedup, we change\nthe kernel of the depthwise conv of the pruned block from 7\nto 3.\nDeiT. For DeiT pruning experiments, we conduct a prun-\ning experiment with 6 layers experiment to compare with\nthe SOTA vision transformer pruning method. For subnet\ntraining, the hyper-parameters K is 6, and the total training\nepochs is 450.\nComparisons with SOTA Models\nWe compare the depth pruner with the state-of-the-art prun-\ning methods under the comparable inference speed on a sin-\ngle AMD MI100 GPU. Following (Graham et al. 2021), we\nmeasure the average inference speedup of compressed net-\nworks with batchsize=128. In this paper, we compare the\naccuracy of different models with comparable speedups.\nResNet34. Table 1 compares our method with MetaPrun-\ning and NAS US methods on ResNet34. We prune 6 and 10\nblocks respectively by applying our depth pruner to obtain\ntwo subnets with 1.25× and 1.43× speedup ratios, respec-\ntively. Under comparable speedup, our method surpasses\nthe MetaPruing by 0.8% and NAS US method by 1.6% on\n1.43× speedup ratio with a longer search time.\nModels FLOPs\n(M)\nAcc1\n(%) Speedup\nMBV2-1.4-Baseline 630 76.5 1.00\nMetaPruning-0.5× 332 73.2 1.49\nUS-MBV2-1.4-0.6× 384 73.4 1.56\nMBV2-1.4-DS-A 519 74.4 1.75\nOurs-P6 519 74.8 1.75\nUS-MBV2-1.4-0.4× 286 72.2 2.04\nMBV2-1.4-DS-C 492 73.1 2.16\nOurs-P9 492 73.8 2.16\nUS-MBV2-1.4-0.3× 213 68.1 2.40\nMBV2-1.4-DS-E 474 72.2 2.50\nOurs-P11 474 72.5 2.50\nTable 2: Classification performance comparisons with\nMetaPruing, NAS US and DepthShrinker on ImageNet with\nsame network structures as the DepthShinker.\nMobileNetV2. Table 2 shows the experimental results\non MobileNetV2-1.4. We adopt the same subnets as\nDepthShrinker, but the subnet training process is different\nfrom our progressive training strategy. We achieve 0.7%\nhigher accuracy than MBV2-1.4-DS-C at a 2.16× speedup\nratio, and some improvement compared to DepthShrinker\nat other speedup ratios. We also compare MetaPruning, and\nsimilar to ResNet34, we reproduce MetaPruning-0.35×with\ninference speeds comparable to MBV2-1.4-DS-C, while our\ndepth pruning achieves a 2.1% higher accuracy with a higher\nspeedup ratio.\nConvNeXtV1. Table 3 compares our accuracy with some\ncommon efficient models since there is no compression\nmethod for ConvNeXtV1. We test the speedup ratios of all\nnetworks on the AMD platform using the slowest network\nEfficientFormerV2-S2 in the table as a benchmark. We di-\nvide the model into levels by accuracy, and our depth prun-\ning method achieves higher accuracy with comparable speed\nin different levels.\nDeiT. As shown in Table 4, our method outperforms other\nstate-of-the-art methods in both accuracy and speedup ra-\ntio. Our proposed depth pruner achieves a 1.26× speedup\nratio with only a 1.9% top-1 accuracy drop. By replac-\ning mergeable modules and applying the reparameterization\ntechnique, our proposed method can shrink the network and\nbring real inference acceleration.\nAblation Study\nIn this section, we analyze the effectiveness of subnet\nsearching and progressive training strategy.\nModels Before FT(%) After FT(%)\nResNet34-P10-A 57.8 71.8\nResNet34-P10-B 55.9 71.2\nTable 5: Evaluating the accuracy consistence of subnets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13896\nModels Type FLOPs(G) Params(M) Acc1(%) Speedup\nConvNeXtV1-T-Baseline (Liu et al. 2022) Conv 4.5 28.6 82.1 2.4\nRepVGG-B1 (Ding et al. 2021b) Conv 11.8 51.8 78.4 2.1\nEfficientFormerV2-S1 (Li et al. 2022) Hybrid 0.7 6.1 79.0 4.0\nMobileOne-S4 (Vasu et al. 2023) Conv 3.0 14.8 79.4 3.9\nConvNeXtV2-P (Woo et al. 2023) Conv 9.1 9.1 79.7 4.0\nPVT-Small (Wang et al. 2021c) Attention 3.8 24.5 79.8 2.7\nVanillaNet-9 (Chen et al. 2023) Conv 8.6 41.4 79.9 1.1\nRegNetY-12G (Radosavovic et al. 2020) Conv 12.1 51.8 80.3 1.8\nConvNeXtV1-T-UPDP-P9 Conv 2.5 23.6 80.6 4.9\nRepVGG-B3 (Ding et al. 2021b) Conv 26.2 110.9 80.6 1.1\nConvNeXtV1-N (Woo et al. 2023) Conv 2.5 15.6 80.8 3.6\nEdgeViT-S (Pan et al. 2022) Hybrid 1.9 11.1 81.0 3.6\nSwin-T (Liu et al. 2021) Attention 4.5 28.3 81.1 2.8\nPVT-Medium (Wang et al. 2021c) Attention 6.7 44.0 81.2 1.8\nConvNeXtV1-T-UPDP-P6 Conv 3.1 27.5 81.3 4.2\nVanillaNet-11 (Chen et al. 2023) Conv 10.3 50.0 81.1 1.1\nEfficientFormerV2-S2 (Li et al. 2022) Hybrid 1.3 12.6 81.6 1.0\nPVT-Large (Wang et al. 2021c) Attention 9.8 61.0 81.7 1.3\nDeiT-B (Touvron et al. 2021) Attention 17.5 86.0 81.8 1.5\nConvNeXtV1-T-UPDP-P3 Conv 3.8 28.3 81.9 3.3\nTable 3: Performance of ConvNeXtV1 depth pruning results on ImgeNet. Speedups are tested on an AMD MI100 GPU with a\nbatch size of 128. Adopt the slowest network in the table (EfficientFormerV2) as the baseline(1.0 speedup) for comparison.\nModels FLOPs\n(G)\nParams\n(M)\nAcc1\n(%) Speedup\nDeiT-Tiny 1.3 5.4 72.2 1.00\nSCOP* 0.8 - 68.9 -\nHVT* 0.7 - 69.7 -\nS2ViTE 1.0 4.2 70.1 1.12\nWD-Pruning 0.7 3.5 70.3 1.20\nXPruner 0.6 - 71.1 -\nOurs-P6 0.9 3.8 70.3 1.26\nTable 4: DeiT depth pruning results on ImageNet. The re-\nsults of S 2ViTE (Tang et al. 2022) and WD-Pruning (Yu\net al. 2022) refer to their paper. SCOP (Tang et al. 2020),\nHVT (Pan et al. 2021), and XPruner (Yu and Xiang 2023) do\nnot publish their results about the number of parameters and\nspeedup ratio. ”*” denotes that the results come from (Yu\net al. 2022).\nEffectiveness of Subnet Searching. We verify the effec-\ntiveness of our ResNet34 subnet searching by comparing\nperformance of two pruned-10-layers subnets with different\naccuracy before subnet finetune (FT) based on ResNet34.\nTable 5 shows ResNet34-P10-A with a higher accuracy be-\nfore subnet finetune can achieve higher finetune accuracy,\nwhich proves the effectiveness of supernet training and sub-\nnet searching for optimal subnet with a final high perfor-\nmance.\nModels Direct (%) Progressive (%)\nResNet34-P10 71.3 71.8\nMBV2-1.4-P9 73.1 73.8\nConvNeXtV1-T-P3 81.6 81.9\nDeiT-Tiny-P6 69.5 70.3\nTable 6: Evaluating the effectiveness of progressive training.\nEffectiveness of Progressive Training Strategy. Com-\npared with hard removal of non-linear activation functions,\nour progressive training has a significant improvement in the\naccuracy of each subnetwork. As shown in Table 6, for var-\nious sub-networks, we observe that progressive training im-\nproves accuracy by 0.3%-0.8% than direct training method.\nConclusion\nIn this paper, we present a unified depth pruner for both effi-\ncient CNN and vision transformer models to prune models in\nthe depth dimension. Our depth pruner includes four steps,\nwhich are supernet training, subnet searching, subnet train-\ning, and subnet merging. We propose a novel block pruning\nmethod and a progressive training strategy to utilize base-\nline model weights better. During subnet merging, we use\nreparameterization technique to make subnet become shal-\nlower and faster. We conduct our method to several CNN\nmodels and transformer models. The SOTA pruning perfor-\nmance demonstrates the superiority of our method. In the\nfuture, we would explore our method on more transformer\nmodels and tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13897\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBhardwaj, K.; Milosavljevic, M.; O’Neil, L.; Gope, D.;\nMatas, R.; Chalfin, A.; Suda, N.; Meng, L.; and Loh, D.\n2022. Collapsible linear blocks for super-efficient super res-\nolution. MLSys, 4: 529–547.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nNeurIPS, 33: 1877–1901.\nCai, H.; Gan, C.; Wang, T.; Zhang, Z.; and Han, S. 2020.\nOnce for All: Train One Network and Specialize it for Effi-\ncient Deployment. In ICLR.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213–229. Springer.\nChen, H.; Wang, Y .; Guo, J.; and Tao, D. 2023. VanillaNet:\nthe Power of Minimalism in Deep Learning. arXiv preprint\narXiv:2305.12972.\nChen, S.; and Zhao, Q. 2018. Shallowing deep net-\nworks: Layer-wise pruning based on feature representations.\nTPAMI, 41(12): 3048–3056.\nChollet, F. 2017. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 1251–1258.\nDing, X.; Zhang, X.; Han, J.; and Ding, G. 2021a. Diverse\nbranch block: Building a convolution as an inception-like\nunit. In CVPR, 10886–10895.\nDing, X.; Zhang, X.; Ma, N.; Han, J.; Ding, G.; and Sun, J.\n2021b. Repvgg: Making vgg-style convnets great again. In\nCVPR, 13733–13742.\nDong, Y .; Cordonnier, J.-B.; and Loukas, A. 2021. Attention\nis not all you need: Pure attention loses rank doubly expo-\nnentially with depth. In ICML, 2793–2803. PMLR.\nDror, A. B.; Zehngut, N.; Raviv, A.; Artyomov, E.; Vitek,\nR.; and Jevnisek, R. 2021. Layer folding: Neural net-\nwork depth reduction using activation linearization. arXiv\npreprint arXiv:2106.09309.\nElkerdawy, S.; Elhoushi, M.; Singh, A.; Zhang, H.; and Ray,\nN. 2020. To filter prune, or to layer prune, that is the ques-\ntion. In ACCV.\nElsen, E.; Dukhan, M.; Gale, T.; and Simonyan, K. 2020.\nFast sparse convnets. In CVPR, 14629–14638.\nFu, Y .; Yang, H.; Yuan, J.; Li, M.; Wan, C.; Krishnamoorthi,\nR.; Chandra, V .; and Lin, Y . 2022. DepthShrinker: a new\ncompression paradigm towards boosting real-hardware effi-\nciency of compact neural networks. In ICML, 6849–6862.\nPMLR.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J ´egou, H.; and Douze, M. 2021. Levit: a vision trans-\nformer in convnet’s clothing for faster inference. In ICCV,\n12259–12269.\nHan, S.; Mao, H.; and Dally, W. J. 2015. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHe, Y .; Liu, P.; Wang, Z.; Hu, Z.; and Yang, Y . 2019. Filter\npruning via geometric median for deep convolutional neural\nnetworks acceleration. In CVPR, 4340–4349.\nHoward, A.; Sandler, M.; Chu, G.; Chen, L.-C.; Chen, B.;\nTan, M.; Wang, W.; Zhu, Y .; Pang, R.; Vasudevan, V .; et al.\n2019. Searching for mobilenetv3. In ICCV, 1314–1324.\nHoward, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang,\nW.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. Mo-\nbilenets: Efficient convolutional neural networks for mobile\nvision applications. arXiv preprint arXiv:1704.04861.\nLi, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.\n2016. Pruning filters for efficient convnets. arXiv preprint\narXiv:1608.08710.\nLi, Y .; Hu, J.; Wen, Y .; Evangelidis, G.; Salahi, K.; Wang,\nY .; Tulyakov, S.; and Ren, J. 2022. Rethinking vision\ntransformers for mobilenet size and speed. arXiv preprint\narXiv:2212.08059.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012–10022.\nLiu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.;\nand Xie, S. 2022. A ConvNet for the 2020s. CVPR.\nLiu, Z.; Mu, H.; Zhang, X.; Guo, Z.; Yang, X.; Cheng, K.-T.;\nand Sun, J. 2019. Metapruning: Meta learning for automatic\nneural network channel pruning. In ICCV, 3296–3305.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are sixteen heads\nreally better than one? NeurIPS, 32.\nPan, J.; Bulat, A.; Tan, F.; Zhu, X.; Dudziak, L.; Li, H.; Tz-\nimiropoulos, G.; and Martinez, B. 2022. Edgevits: Compet-\ning light-weight cnns on mobile devices with vision trans-\nformers. In ECCV, 294–311. Springer.\nPan, Z.; Zhuang, B.; Liu, J.; He, H.; and Cai, J. 2021. Scal-\nable vision transformers with hierarchical pooling. InICCV,\n377–386.\nPool, J.; and Yu, C. 2021. Channel permutations for N: M\nsparsity. NeurIPS, 34: 13316–13327.\nPope, R.; Douglas, S.; Chowdhery, A.; Devlin, J.; Bradbury,\nJ.; Heek, J.; Xiao, K.; Agrawal, S.; and Dean, J. 2023. Effi-\nciently scaling transformer inference. MLSys, 5.\nRadosavovic, I.; Kosaraju, R. P.; Girshick, R.; He, K.; and\nDoll´ar, P. 2020. Designing network design spaces. InCVPR,\n10428–10436.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J.\n2021. Dynamicvit: Efficient vision transformers with dy-\nnamic token sparsification. NeurIPS, 34: 13937–13949.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition chal-\nlenge. Int J Comput Vis, 115: 211–252.\nSandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and\nChen, L.-C. 2018. Mobilenetv2: Inverted residuals and lin-\near bottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 4510–4520.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13898\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nStrudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021. Seg-\nmenter: Transformer for semantic segmentation. In ICCV,\n7262–7272.\nTan, M.; Chen, B.; Pang, R.; Vasudevan, V .; Sandler, M.;\nHoward, A.; and Le, Q. V . 2019. Mnasnet: Platform-aware\nneural architecture search for mobile. In CVPR, 2820–2828.\nTan, M.; and Le, Q. 2021. Efficientnetv2: Smaller models\nand faster training. In ICML, 10096–10106. PMLR.\nTang, Y .; Han, K.; Wang, Y .; Xu, C.; Guo, J.; Xu, C.; and\nTao, D. 2022. Patch slimming for efficient vision transform-\ners. In CVPR, 12165–12174.\nTang, Y .; Wang, Y .; Xu, Y .; Tao, D.; XU, C.; Xu, C.; and Xu,\nC. 2020. SCOP: Scientific Control for Reliable Neural Net-\nwork Pruning. In Larochelle, H.; Ranzato, M.; Hadsell, R.;\nBalcan, M.; and Lin, H., eds., NeurIPS, volume 33, 10936–\n10947. Curran Associates, Inc.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and Jegou, H. 2021. Training data-efficient image trans-\nformers amp; distillation through attention. In ICML, vol-\nume 139, 10347–10357.\nVasu, P. K. A.; Gabriel, J.; Zhu, J.; Tuzel, O.; and Ranjan, A.\n2023. MobileOne: An Improved One Millisecond Mobile\nBackbone. In CVPR, 7907–7917.\nWang, D.; Gong, C.; Li, M.; Liu, Q.; and Chandra, V .\n2021a. Alphanet: Improved training of supernets with alpha-\ndivergence. In ICML, 10760–10771. PMLR.\nWang, D.; Li, M.; Gong, C.; and Chandra, V . 2021b. Atten-\ntivenas: Improving neural architecture search via attentive\nsampling. In CVPR, 6418–6427.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021c. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV, 568–578.\nWoo, S.; Debnath, S.; Hu, R.; Chen, X.; Liu, Z.; Kweon,\nI. S.; and Xie, S. 2023. Convnext v2: Co-designing and scal-\ning convnets with masked autoencoders. In CVPR, 16133–\n16142.\nWu, Y .; and He, K. 2018. Group normalization. In ECCV,\n3–19.\nXu, P.; Cao, J.; Shang, F.; Sun, W.; and Li, P. 2020. Layer\npruning via fusible residual convolutional block for deep\nneural networks. arXiv preprint arXiv:2011.14356.\nYu, F.; Huang, K.; Wang, M.; Cheng, Y .; Chu, W.; and Cui,\nL. 2022. Width amp; Depth Pruning for Vision Transform-\ners. AAAI, 36(3): 3143–3151.\nYu, J.; and Huang, T. S. 2019. Universally slimmable net-\nworks and improved training techniques. In ICCV, 1803–\n1811.\nYu, J.; Jin, P.; Liu, H.; Bender, G.; Kindermans, P.-J.; Tan,\nM.; Huang, T.; Song, X.; Pang, R.; and Le, Q. 2020. Bignas:\nScaling up neural architecture search with big single-stage\nmodels. In ECCV, 702–717. Springer.\nYu, L.; and Xiang, W. 2023. X-Pruner: eXplainable Pruning\nfor Vision Transformers. ArXiv, abs/2303.04935.\nZhang, M.; and He, Y . 2020. Accelerating training of\ntransformer-based language models with progressive layer\ndropping. NeurIPS, 33: 14011–14023.\nZhou, Y .; Yen, G. G.; and Yi, Z. 2021. Evolutionary shal-\nlowing deep neural networks at block levels. TNNLS, 33(9):\n4635–4647.\nZhu, M.; Tang, Y .; and Han, K. 2021. Vision transformer\npruning. arXiv preprint arXiv:2104.08500.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13899",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5427913069725037
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4845527708530426
    },
    {
      "name": "Computer science",
      "score": 0.4751462936401367
    },
    {
      "name": "Computer vision",
      "score": 0.39520806074142456
    },
    {
      "name": "Engineering",
      "score": 0.15704241394996643
    },
    {
      "name": "Electrical engineering",
      "score": 0.13747963309288025
    },
    {
      "name": "Voltage",
      "score": 0.08025199174880981
    }
  ],
  "institutions": []
}