{
  "title": "Vision Transformer based System for Fruit Quality Evaluation",
  "url": "https://openalex.org/W4223604174",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5104035104",
      "name": "Tanushri Kumar",
      "affiliations": [
        "Anna University, Chennai"
      ]
    },
    {
      "id": "https://openalex.org/A5108879188",
      "name": "Rallabandi Shivani",
      "affiliations": [
        "Anna University, Chennai"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3168489096",
    "https://openalex.org/W4200614249",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6600497981",
    "https://openalex.org/W6610432646",
    "https://openalex.org/W3018229519",
    "https://openalex.org/W2544377536",
    "https://openalex.org/W3023057433",
    "https://openalex.org/W4394500904",
    "https://openalex.org/W2064808659",
    "https://openalex.org/W3173714114"
  ],
  "abstract": "Abstract Purpose Fruit quality assessment is one of the most pressing issues in the farming industry. Agriculturists would benefit significantly from the capacity to recognize the freshness of fruits, as it will allow them to optimize the harvesting stage and avoid reaping either underdeveloped or overdeveloped natural products. Productivity has decreased due to a lack of low-cost technology and equitable access. Despite large-scale agricultural mechanization in some parts of the country, most agricultural operations are still carried out by hand with simple instruments. The goal of this research is to automate the task of evaluating fruit quality. Methods Transformers were first presented in the field of natural language processing, and they offer dramatic performance improvements over existing models in NLP like as LSTMs and GRU. The Vision Transformer, or ViT, is an image classification model that uses a Transformer-like design over image patches. Results Vision transformer exhibited far superior results compared to CNN models. The models were evaluated on various metrics and the vision transformer model generated the highest accuracy compared to convolutional neural network models such as InceptionNet and EfficientNet. Conclusions Vision Transformers exhibited superior performance in fruit quality evaluation compared to traditional CNN model approaches. For future works, this model can be used to develop an efficient quality control system Automation of the quality prediction process can greatly reduce food and agricultural produce wastage.",
  "full_text": "Page 1/10\nVision Transformer based System for Fruit Quality\nEvaluation\nTanushri Kumar  (  tanushri.skr@gmail.com )\nAnna University Chennai College of Engineering Guindy\nShivani R \nAnna University Chennai College of Engineering Guindy\nResearch Article\nKeywords: Computer vision, Image processing, Quality assessment, Vision transformer\nPosted Date: April 11th, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-1526586/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nPage 2/10\nAbstract\nPurpose Fruit quality assessment is one of the most pressing issues in the farming industry.\nAgriculturists would bene\u0000t signi\u0000cantly from the capacity to recognize the freshness of fruits, as it will\nallow them to optimize the harvesting stage and avoid reaping either underdeveloped or overdeveloped\nnatural products. Productivity has decreased due to a lack of low-cost technology and equitable access.\nDespite large-scale agricultural mechanization in some parts of the country, most agricultural operations\nare still carried out by hand with simple instruments. The goal of this research is to automate the task of\nevaluating fruit quality.\nMethods Transformers were \u0000rst presented in the \u0000eld of natural language processing, and they offer\ndramatic performance improvements over existing models in NLP like as LSTMs and GRU. The Vision\nTransformer, or ViT, is an image classi\u0000cation model that uses a Transformer-like design over image\npatches.\nResults Vision transformer exhibited far superior results compared to CNN models. The models were\nevaluated on various metrics and the vision transformer model generated the highest accuracy compared\nto convolutional neural network models such as InceptionNet and E\u0000cientNet.\nConclusions Vision Transformers exhibited superior performance in fruit quality evaluation compared to\ntraditional CNN model approaches. For future works, this model can be used to develop an e\u0000cient\nquality control system Automation of the quality prediction process can greatly reduce food and\nagricultural produce wastage.\nIntroduction\nAgriculture is essential to humans because it assures a steady supply of food and ensures food security\nfor the population. Fruits, in particular, are often purchased by every family and are high in nutrients;\nhence, a constant supply and production are necessary to meet the demand of the world's rising\npopulation. As a result, the whole agri-food industry chain is facing rising problems, necessitating the\napplication of new creative technologies in order to increase production. Fruits have been a staple of\nhuman diets since prehistoric times. Because of their excellent nutritional value, they offer a signi\u0000cant\nnutritional contribution to human well-being. It is necessary to ensure the quality of fruits ingested in all\nlocations. Many fruit supply \u0000rms continue to ship improper fruit for consumption due to a lack of\nprecision in the fruit sorting procedure performed by their employees. Computational technologies have\nbeen used for fruit recognition and other computer vision tasks (Kaur et al., 2015). \nBecause manual grading is time demanding, automation of the grading process through the use of\ncomputerized systems is seen to be the solution (Blasco et al., 2003). For this classi\u0000cation challenge,\ndeep learning methods were applied using transfer learning, a machine learning method in which a model\ndeveloped for one task is reused as the starting point for a model on a different problem. It is a popular\napproach in deep learning, where pre-trained models are used as the starting point for computer vision\nPage 3/10\nand natural language processing tasks (S. Chakraborty et al., 2021). This has proved effective in carrying\nknowledge learned from general-domain, large-scale datasets to speci\u0000c domains, where the amount of\ndata available is limited. To accomplish this, a fruit freshness grading system that can distinguish\nvarious varieties of fruits from photographs acquired by any digital camera or smartphone from diverse\nlocations can be developed. This device will assist us in determining the quality of fruits as well as\ndeveloping a robotic orchard harvesting system.\nDataset \nThe primary concept of machine learning is collecting image datasets for training and model building.\nFor the prediction of the quality of fruits, the dataset “IEEEFRUITSDATA_train&test” was used. The\nDataset consists of 12,050 images comprising fruits such as Apple, Banana, Guava, Orange, Lime, and\npomegranate categorized based on their quality as Good or Bad.\nMaterials And Methods\nCNN-based approach \n  A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning method that can take an input\nimage, assign relevance (learnable weights and biases) to various aspects/objects in the image, and\ndistinguish between them. Convolution is a linear mathematical action between matrices. A convolutional\nlayer, a non-linearity layer, a pooling layer, and a fully-connected layer are among the layers of CNN. In\nmachine learning issues, CNN has performed satisfactorily.  The dataset under study was used to train\ndeep learning models (S. Fan et al., 2020), namely E\u0000cientNet and InceptionNet (Patil, 2018). \nE\u0000cientNet is a convolutional neural network architecture and scaling approach that uses a compound\ncoe\u0000cient to consistently scale all the depth, breadth, and resolution parameters. Unlike conventional\npractice that arbitrarily scales these factors, the E\u0000cientNet scaling method uniformly scales network\nwidth, depth, and resolution with a set of \u0000xed scaling coe\u0000cients (A. Kumar et al., 2021). The model was\nimplemented using the transfer learning approach. A peak accuracy of 91% was achieved. \n  Inception v3 is an image recognition model that has been shown to attain greater than 78.1% accuracy\non the ImageNet dataset. To achieve better accuracy, the InceptionNet model was implemented. The\nmodel is the culmination of many ideas developed by multiple researchers over the years. It is based on\nthe original paper: \"Rethinking the Inception Architecture for Computer Vision\" (Szegedy et al., 2016).\nConvolutions, average pooling, max pooling, concatenations, dropouts, and fully linked layers are among\nthe symmetric and asymmetric building components in the model. Batch normalization is done to\nactivation inputs and is utilized extensively throughout the model. Loss is computed using Softmax. Due\nto its high accuracy on image classi\u0000cation problems, InceptionNet was utilized to develop a model. A\npeak accuracy of 94% was achieved. Though it exhibited better results on the test data compared to the\nE\u0000cientNet model approach, the models took a long time to train, and its predictions on test data were\nnot satisfactory enough. \nPage 4/10\nVision transformer \n  The Vision Transformer (ViT) has emerged as a viable alternative to convolutional neural networks\n(CNNs), which are the current state-of-the-art in computer vision and are widely employed in image\nidenti\u0000cation applications. In terms of computing e\u0000ciency and accuracy, ViT models exceed the present\nstate-of-the-art (CNN) by almost a factor of four. \nTransformers are already capable of paying attention to regions that are far apart right from the starting\nlayers of the network which is a signi\u0000cant gain the transformers bring over CNNs which have a \u0000nite\nreceptive \u0000eld at the start. One other advantage of transformer models is that they are highly\nparallelizable. \nThe attention mechanism enhances the crucial parts of the input data and fades out the rest. Self-\nattention module replaces the convolutional layer so that now the model gets the ability to interact with\npixels far away from its location. The self-attention mechanism is a type of attention mechanism which\nallows every element of a sequence to interact with others and \u0000nd out to whom they should pay more\nattention. An attention mechanism like self-attention can effectively solve some of the limitations of the\nConvolutional Networks. This distinct behavior is due to the inclusion of some inductive biases in CNNs,\nwhich can be used by these networks to comprehend the particularities of the analyzed images more\nrapidly, even if they end up limiting them and making it more di\u0000cult to grasp global relations. \nThe Vision Transformers, on the other hand, are free of these biases, allowing them to capture a global\nand wider range of relationships at the cost of more time-consuming data training. Input visual\ndistortions such as adversarial patches or permutations were also signi\u0000cantly more resistant to Vision\nTransformers (Park et al., 2022). \nArchitecture-ViT model \nThe ViT model is made up of many Transformer blocks that employ layers. As a self-attention method,\nthe MultiHeadAttention layer is applied to the sequence of patches. The Transformer blocks generate a\n[batch size, num patches, projection dim] tensor, which is then processed by a SoftMax classi\u0000er head to\ngenerate the \u0000nal class probabilities output. \n The initial stage in the model is to split an input image into a series of image patches. By projecting a\npatch onto a vector of size projection_dim, the PatchEncoder layer will linearly transform it. The projected\nvector is also given a learnable position embedding. These image patches are then sent via a linear\nprojection layer that may be trained. This layer serves as an embedding layer, producing \u0000xed-size\nvectors. The sequence of image patches is then linearly added using position embeddings to ensure that\nthe images preserve their positional information. It injects crucial information about the image patches'\nrelative or absolute positions in the sequence. \nThe 0th class is a principal element to note in the position embedding module. BERT's class token\ninspired the concept of the 0th class. This class, like the others, is learnt, although it does not originate\nPage 5/10\nfrom its picture. Instead, the model design has it hardcoded. If the transformer is provided with the\npositioning data, it will not know what order the photos are in. The transformer encoder receives this\nsequence of vector pictures. \nA Multi-Head Attention layer and a Multi-Layer Perceptron (MLP) layer make up the Transformer encoder\nmodule. The Multi-Head Attention layer divides inputs into several heads, allowing each head to develop\nvarying levels of self-attention. All the heads' outputs are then combined and sent into the Multi-Layer\nPerceptron. Normalization layers (Layer Norm) are applied before each block using transformers, and\nresidual blocks are applied afterward. Finally, the transformer encoder receives an additional learnable\nclassi\u0000cation module (the MLP Head), which determines the network's output classes. \nModel Details \nThe proposed work \u0000ne-tunes the google/vit-base-patch16-224-in21k a Vision Transformer (ViT) pre-\ntrained on ImageNet-21k (14 million pictures, 21,843 classes) at 224x224 resolution in this case. The\nmodel is provided with images in the form of a series of \u0000xed-size patches (resolution 16x16) that are\nlinearly embedded. In order to train the model, the images must be converted to pixel values. A\ntransformer’s Feature Extractor accomplishes this by augmenting and converting the photos into a 3D\nArray that can be fed into the model (Dosovitskiyet al., 2020). \n Data augmentation techniques (John B et al., 2002) were performed on images of the training set to\nimprove the generalization ability of the model with the help of  PyTorch’s transform class which provides\ncommon image transformations. PyTorch also provides functionalities to load and store the data\nsamples with the corresponding labels. In order to create training and validation dataloaders, the in-built\nDataLoader class was utilized. This wraps an iterable around the dataset, enabling us to easily access\nand iterate over the data samples in our dataset. The model was con\u0000gured with the following parameter\nvalues- \nLearning rate: 5e-4 \nLoss function: CrossEntropyLoss \nOptimizer: Adam optimizer \nImage size: 32 \nPatch size: 16 x 16 \nNumber of classes to classify: 12\nResults And Discussion\nTable 1 Model results\nPage 6/10\nModel Validation loss Validation accuracy\nEfficientNet 0.712 0.91\nInceptionNet 0.589 0.94\nVision Transformer 0.052 0.984\n The above visualizations show the comparison of results of ViT against state-of-the-art CNN\narchitectures on the dataset under study. The ViT based model developed for fruit quality prediction\nshowcased a high accuracy of 98.4. The ViT model was pre-trained on the Image net dataset and \u0000ne-\ntuned. The results below show that ViT performed better than Inception net-based architecture and the\nE\u0000centNet-L2 architecture on all the datasets. Both of these models represent the most up-to-date CNN\narchitectures.\nViT requires much less computational resources and training time compared to the other two CNN\nmodels.\nConclusions\nIn this paper, we proposed a deep learning-based  machine vision system for grading the fruits based on\ntheir outer appearance or freshness. The formulation of an image classi\u0000cation problem as a sequential\nproblem utilizing image patches as tokens and processing it by a transformer is the major engineering\ncomponent of this study.\nVarious state-of-the-art deep learning models and methods were applied to the dataset of fruits. Through\nour experiments, we found that the Vision transformer is the best model for the task. Transfer learning\nwas incorporated with the help of ImageNet pre-trained weights proved to be effective in boosting\naccuracy. The overall performance of the model to evaluate fruit quality is very good which has met the\ndesired result of the accuracy of about 98.4%. It has been concluded that the Vision transformer has a\nhigher precision rate on a large dataset with reduced training time. Moreover, the application of the vision\ntransformer-based approach has been found to be more accurate in quality assessment as compared to\nthe results previously reported while implementing CNN-based models. Hence, the suggested technique\nimproves the rate of identi\u0000cation of fruit and its freshness detection so that the real-world application\ndemands can be achieved.  In the future, the existing database will be enhanced with additional varieties\nof fruit and vegetable images. Also, the future scope of this implementation will involve the integration of\nthe proposed system that will be helpful in the operation of an automated robotic fruit freshness\nevaluation system.\nPage 7/10\nDeclarations\nCon\u0000ict of Interest \nThe authors have no con\u0000icting \u0000nancial or other interests.\nAcknowledgement\nThe authors wish to acknowledge the research support provided  by the Department of Electronics and\nCommunication of College of Engineering Guindy, Anna University.\nReferences\nPark & Kim. (2022). How Do Vision Transformers Work? ArXiv, abs/2202.06709.\n       https://doi.org/10.48550/arXiv.2010.11929\n  Dosovitskiy, Beyer,  Kolesnikov, A Weissenborn, Zhai,Unterthiner, Dehghani, Minderer, Heigold, Gelly,\nUszkoreit, Houlsby, Neil.(2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at\nScale, ArXiv, https://doi.org/10.48550/arxiv.2010.11929\n  Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing. (2021). When Vision          Transformers  \nOutperform ResNets without Pre-training or Strong Data Augmentations.  \n ArXiv,https://doi.org/10.48550/arxiv.2106.01548\n  A. Kumar, R. C. Joshi, M. K. Dutta, M. Jonak and R. Burget, Fruit-CNN: An E\u0000cient            Deep learning-\nbased Fruit Classi\u0000cation and Quality Assessment for Precision Agriculture,     2021 In: 13th International\nCongress on Ultra Modern Telecommunications and Control     Systems and Workshops (ICUMT), 2021,\npp. 60-65, DOI: 10.1109/ICUMT54235.2021.9631643.    \n  C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens and Z. Wojna, \"Rethinking the Inception Architecture for\nComputer Vision, In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp.\n2818-2826, DOI: 10.1109/CVPR.2016.308.\n  S. Fan et al., On-line detection of defective apples using computer vision system combined      with deep\nlearning methods, J. Food Eng., vol. 286, pp. 110102, Dec. 2020.\n  S. Chakraborty, F. M. J.M. Shamrat, M. M. Billah, M. A. Jubair, M. Alauddin and R.Ranjan, Implementation\nof Deep Learning Methods to Identify Rotten Fruits, In: 20215th International Conference on Trends in\nElectronics and Informatics (ICOEI), pp. 1207-1212, 2021.\n  John B. Njoroge, Kazunori Ninomiya, Naoshi Kondo and Hideki Toita, Automated Fruit Grading System\nusing ImageProcessing, The Society of Instrument and Control Engineers(SICE2002), pp. 1346-1351,\nAugust 2002.\nPage 8/10\n  P. Deepa, A Comparative Analysis of Feature Extraction Methods for Fruit Grading Classi\u0000cations,\nInternational journal of emerging technologies in computational and applied sciences (IJETCAS), vol. 13,\nno. 138, 2013.\n  J. Blasco, N. Alexios, E. Molto, A Machine vision system for automatic quality grading of fruit, Biosyst.\nEng., 85 (4) (2003), pp. 415-423\n  Mandeep Kaur, Reecha Sharma, ANN-based Technique for Vegetable Quality Detection, IOSR Journal of\nElectronics and Communication Engineering”, ISSN: 2278- 8735, pp:62-70, 2015.\n  Dakshayini Patil, Fruit disease detection and classi\u0000cation using image processing. International\nJournal for Research in Engineering Application & Management (IJREAM). ISSN : 2454-9150, pp:128-131,\n2018.\nFigures\nFigure 1\nArchitecture of Vision Transformer\nPage 9/10\nFigure 2\nTraining duration for different models\nFigure 3\nPage 10/10\nLoss for Batch\nFigure 4\nAccuracy for Batch",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.726828932762146
    },
    {
      "name": "Automation",
      "score": 0.6542562246322632
    },
    {
      "name": "Computer science",
      "score": 0.6401171088218689
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5476060509681702
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5201364159584045
    },
    {
      "name": "Agricultural engineering",
      "score": 0.4850110709667206
    },
    {
      "name": "Machine vision",
      "score": 0.48464423418045044
    },
    {
      "name": "Agriculture",
      "score": 0.48018017411231995
    },
    {
      "name": "Machine learning",
      "score": 0.378021776676178
    },
    {
      "name": "Engineering",
      "score": 0.2053808867931366
    },
    {
      "name": "Voltage",
      "score": 0.1866910755634308
    },
    {
      "name": "Electrical engineering",
      "score": 0.08388364315032959
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}