{
  "title": "A Transformer-Based Feature Segmentation and Region Alignment Method for UAV-View Geo-Localization",
  "url": "https://openalex.org/W4206760693",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2115691041",
      "name": "Dai, Ming",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A2112865844",
      "name": "Hu Jianhong",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A4222680341",
      "name": "Zhuang, Jiedong",
      "affiliations": [
        "China Jiliang University"
      ]
    },
    {
      "id": "https://openalex.org/A2030879314",
      "name": "Zheng En-hui",
      "affiliations": [
        "China Jiliang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3092933908",
    "https://openalex.org/W6769302471",
    "https://openalex.org/W6750759024",
    "https://openalex.org/W2577537809",
    "https://openalex.org/W2951019013",
    "https://openalex.org/W1946093182",
    "https://openalex.org/W2598199894",
    "https://openalex.org/W2073761981",
    "https://openalex.org/W2908320224",
    "https://openalex.org/W3081227581",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2795758732",
    "https://openalex.org/W2964163358",
    "https://openalex.org/W2724213014",
    "https://openalex.org/W2946574625",
    "https://openalex.org/W2963383990",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W3115854999",
    "https://openalex.org/W2739879705",
    "https://openalex.org/W2572697301",
    "https://openalex.org/W2963474852",
    "https://openalex.org/W3098711604",
    "https://openalex.org/W2432402544",
    "https://openalex.org/W3016713745",
    "https://openalex.org/W2963000559",
    "https://openalex.org/W2883311563",
    "https://openalex.org/W2799087793",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W2980046511",
    "https://openalex.org/W2953684117",
    "https://openalex.org/W2962859295",
    "https://openalex.org/W2947319827",
    "https://openalex.org/W2963180826",
    "https://openalex.org/W6729983426",
    "https://openalex.org/W6799166919",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6795892075",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W6785213549",
    "https://openalex.org/W3214744507",
    "https://openalex.org/W3179267034",
    "https://openalex.org/W3120991189",
    "https://openalex.org/W6786361841",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W6799772243",
    "https://openalex.org/W3172084025",
    "https://openalex.org/W3138115293",
    "https://openalex.org/W2981165461",
    "https://openalex.org/W6798501647",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3100927979",
    "https://openalex.org/W2620629206",
    "https://openalex.org/W3109635183",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W3202236840",
    "https://openalex.org/W2798799804",
    "https://openalex.org/W3100555577",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W3178560723",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3098325765",
    "https://openalex.org/W3020152020",
    "https://openalex.org/W2980345032",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3006086117"
  ],
  "abstract": "Cross-view geo-localization is a task of matching the same geographic image\\nfrom different views, e.g., unmanned aerial vehicle (UAV) and satellite. The\\nmost difficult challenges are the position shift and the uncertainty of\\ndistance and scale. Existing methods are mainly aimed at digging for more\\ncomprehensive fine-grained information. However, it underestimates the\\nimportance of extracting robust feature representation and the impact of\\nfeature alignment. The CNN-based methods have achieved great success in\\ncross-view geo-localization. However it still has some limitations, e.g., it\\ncan only extract part of the information in the neighborhood and some scale\\nreduction operations will make some fine-grained information lost. In\\nparticular, we introduce a simple and efficient transformer-based structure\\ncalled Feature Segmentation and Region Alignment (FSRA) to enhance the model's\\nability to understand contextual information as well as to understand the\\ndistribution of instances. Without using additional supervisory information,\\nFSRA divides regions based on the heat distribution of the transformer's\\nfeature map, and then aligns multiple specific regions in different views one\\non one. Finally, FSRA integrates each region into a set of feature\\nrepresentations. The difference is that FSRA does not divide regions manually,\\nbut automatically based on the heat distribution of the feature map. So that\\nspecific instances can still be divided and aligned when there are significant\\nshifts and scale changes in the image. In addition, a multiple sampling\\nstrategy is proposed to overcome the disparity in the number of satellite\\nimages and that of images from other sources. Experiments show that the\\nproposed method has superior performance and achieves the state-of-the-art in\\nboth tasks of drone view target localization and drone navigation. Code will be\\nreleased at https://github.com/Dmmm1997/FSRA\\n",
  "full_text": "1\nA Transformer-Based Feature Segmentation and\nRegion Alignment Method For UA V-View\nGeo-Localization\nMing Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng\nAbstract‚ÄîCross-view geo-localization is a task of matching\nthe same geographic image from different views, e.g., unmanned\naerial vehicle (UA V) and satellite. The most difÔ¨Åcult challenges\nare the position shift and the uncertainty of distance and scale.\nExisting methods are mainly aimed at digging for more com-\nprehensive Ô¨Åne-grained information. However, it underestimates\nthe importance of extracting robust feature representation and\nthe impact of feature alignment. The CNN-based methods have\nachieved great success in cross-view geo-localization. However\nit still has some limitations, e.g., it can only extract part of\nthe information in the neighborhood and some scale reduction\noperations will make some Ô¨Åne-grained information lost. In\nparticular, we introduce a simple and efÔ¨Åcient transformer-based\nstructure called Feature Segmentation and Region Alignment\n(FSRA) to enhance the model‚Äôs ability to understand contextual\ninformation as well as to understand the distribution of instances.\nWithout using additional supervisory information, FSRA divides\nregions based on the heat distribution of the transformer‚Äôs feature\nmap, and then aligns multiple speciÔ¨Åc regions in different views\none on one. Finally, FSRA integrates each region into a set of\nfeature representations. The difference is that FSRA does not\ndivide regions manually, but automatically based on the heat\ndistribution of the feature map. So that speciÔ¨Åc instances can still\nbe divided and aligned when there are signiÔ¨Åcant shifts and scale\nchanges in the image. In addition, a multiple sampling strategy\nis proposed to overcome the disparity in the number of satellite\nimages and that of images from other sources. Experiments\nshow that the proposed method has superior performance and\nachieves the state-of-the-art in both tasks of drone view target\nlocalization and drone navigation. Code will be released at\nhttps://github.com/Dmmm1997/FSRA\nIndex Terms‚Äîimage retrieval, geo-localization, transformer,\ndrone.\nI. I NTRODUCTION\nC\nROSS-VIEW geo-localization aims to match an image\nfrom one perspective to the most similar image from\nanother perspective that represents the same geographic target.\nIts essence can be understood as a retrieval task of im-\nages from two different sources. Cross-view geo-localization\ncan be applied to many Ô¨Åelds such as agriculture, aerial\nphotography, autonomous vehicles, drone navigation, event\ndetection, accurate delivery, and so on [1], [2], [3], [4], [5].\nThe predecessors did a lot of arduous work [6], [7], [8], [9],\nMing Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng are with the\nUnmanned System Application Technology Research Institute, China Jiliang\nUniversity, Hangzhou 310018, China (email: s20010802003@cjlu.edu.cn;\nzjuhjh@126.com; p1901085206@cjlu.edu.cn; ehzheng@cjlu.edu.cn). Enhui\nZheng is the Corresponding Author.\nInputs T ransformer -based \nCNN-based \nDrone Satellite \nFig. 1. The images on the left column is the input image from drone-view\nand satellite-view. The images in the middle column is the heatmap of CNN-\nbased state-of-the-art network LPN [12]. The images on the right column is\nthe heatmap of our Transformer-based strong baseline.\nmostly studying the matching of ground panoramic images\nand satellite images. However, the intervention of the drone-\nview will further expand the application of cross-view geo-\nlocalization [10] [11]. The application of matching UA Vs and\nsatellite images can be roughly divided into the following two\ntypes: Drone view target localization and Drone navigation.\nFor example, the image acquired by UA Vs is used to match the\nsatellite image of the same geographic location. Generally, the\nsatellite image contains precise GPS coordinate information.\nIndirectly, UA Vs can be located in real-time by adopting the\ngeographical information from matched satellite images and\nthe navigation of the drone can be realized without GPS\nequipment.\nIn recent years, Due to the rapid development of deep\nlearning, signiÔ¨Åcant progress has been made in cross-view\ngeo-localization. By observing the CNN-based method, we\nfound two potential problems. (I) Cross-view geo-localization\nneeds to dig out the relevant information between contexts.\nImages from different domains have positional transformations\nsuch as rotation, scale, and offset. Therefore, fully under-\nstanding the semantic information of the global context is\nnecessary. However, CNN-based methods mainly focus on\nsmall discriminative regions due to a Gaussian distribution of\neffective receptive Ô¨Åelds [13]. Given the limitations of the pure\nCNN-based methods [14], the attention modules have been\nCopyright ¬© 2021 IEEE. Personal use of this material is permitted. However, permission to use this material for any other\npurposes must be obtained from the IEEE by sending an email to pubs-permissions@ieee.org.\narXiv:2201.09206v1  [cs.CV]  23 Jan 2022\n2\nTransformer Layer\nTransformer Layer\nL x\n1 2 4 16 \n5 6 7 \nLinear Projection of Flattened Patches\n0 * Position\nEmbedding\n* \n3 \nEmbedding Feature\nExtra learnable\nembedding [cls_token]\nID Loss\nClassifier Layerùíá\nTransformer Layer\nEmbeded\nPatches\nNorm\nMulti-Head\nAttention\n+ \nNorm\nMLP\n+ \nFig. 2. Transformer-based strong baseline framework. Output [cls token] marked with ‚àóis served as the global feature f. ClassifierLayer contains linear\nlayer, relu, batchnorm1d and dropout. ID Loss represents CrossEntropy loss without label-smooth. In addition, we provide a simpliÔ¨Åed Transformer Layer\nstructure, the speciÔ¨Åc structure can be found in Vit [16].\nintroduced to explore long-range relationships [15]. However,\nmost of the methods embed the attention mechanism into\nthe deep convolutional network, which enhances contextual\nconnections to a certain extent. (II) Fine-grained information\nis very important for the task of retrieval. The down-sampling\noperations i.e., pooling and stride convolution of the CNN-\nbased method can reduce the resolution of the image, while\ninvisibly destroying the recognizable Ô¨Åne-grained information.\nIn view of this, Transformer as a strong context-sensitive\ninformation extractor will have a role to play in Cross-View\nGeo-Localization.\nIn order to improve the visibility of model performance, we\ndraw the heatmaps regarding Grad-CAM [17], as in Fig. 1.\nThe heatmaps come from the output of the last attention layer\nof the Vit but excluding the patch of learnable embedding.\nHowever, the output of the Transformer has only 3 dimensions,\nand we reduce the dimension of patches to the original image\ndimension by the inverse method of Ô¨Çattening. Thus the results\nof Vit concerns are visualized. We compare the heatmaps\nbetween the state-of-the-art CNN-based method LPN [12] and\nour transformer-based strong baseline. Compared to the CNN-\nbased method, the Transformer-based method can more clearly\nidentify salient features such as buildings and roads and ignore\nbackground information such as trees.\nObserving that Transformer-based methods have the ability\nto distinguish instances and inspired by the part-based method\n[18], [19], [20], [21], [22], [23]. A new approach for Feature\nSegmentation and Region Alignment (FSRA) is proposed to\nachieve segmentation of speciÔ¨Åc instances (patch-level) and\nfeature alignment of regions (region-level) for the purpose of\nextracting the corresponding parts and aligning features even\nwhen there are position deviations or scale changes between\nimages. The proposed FSRA consists of two parts. The Ô¨Årst\none is Heatmap Segmentation Module (HSM): As shown in\nthe light green part in the middle of Fig. 3, this module divides\nthe feature map according to the heat distribution of the feature\nmap, and splits the feature map into several blocks from 1 to n\nto achieve the segmentation of patch-level instances. The other\npart is the Heatmap Alignment Branch (HAB): According to\nthe segmentation feature map of HSM, the parts corresponding\nto different viewpoints are cut out in turn to calculate the loss,\nwhich helps the network to learn the desired heat distribution\nrules. As shown in the light blue part of Fig. 3, where the\nleft side is the image taken by the UA V and the right side is\nthe satellite image, both of which are approached by HAB to\nclose the distance of the corresponding blocks.\nIn addition, inspired by LCM [24], we realize that satellite\nimages are highly scarce in the University-1652 [1] datasets,\nand expanded images can effectively improve network learning\ncapabilities. In view of that, we propose a multiple sampling\nstrategy to expand satellite imagery. The proposed multiple\nsampling strategy will increase the training time, but will not\ncause any additional burden on inference. Experiments show\n3\nHeatmap Alignment Branch\n1 1 1 1 1 1 1 1 2 2 2 2 2 2 2\nTransformer Layer\nTransformer Layer\nL\n1 2 4 16\nheat feature 1heat feature 2\nn n n n n\nheat feature n\nCE Loss \nClassifier LayerClassifier LayerClassifier Layer\n5 6 7\nLinear Projection of Flattened Patches\n0 *Position\nEmbedding\n*\nGlobal Branch\nCE Loss \nglobal feature *\nTransformer Layer\nTransformer Layer\nL\n16 6 4 0*3 2 1\nLinear Projection of Flattened Patches\n5 Position\nEmbedding\n*\n3\n 7\nTriplet Loss \nTriplet Loss \nTriplet Loss \nTriplet Loss \nHeatmap Alignment Branch\n2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\nheat feature n heat feature 2heat feature 1\nClassifier Layer\nExtra Learnable\nEmbedding [cls_token] \nClassifier LayerClassifier LayerClassifier Layer\nn n n n n\nGlobal Branch\nCE Loss \nglobal feature *\nClassifier Layer\nCE Loss \nExtra Learnable\nEmbedding [cls_token] \nHeatmap Segmentation Module\n2 2 2 2 2 2 2 2 \n2 1 1 1 2 1 2 2 \n1 1 1 1 2 1 1 1 \n1 1 2 2 1 1 1 2 \n2 1 2 1 1 1 2 2 \n1 1 1 1 1 2 2 2 \n1 1 1 1 2 2 2 1 \n2 1 2 2 2 2 1 2 \nFig. 3. The framework of proposed FSRA. The Heatmap Segmentation Module (light green) reorders the heatmap information and evenly divides it into\nregions according to the distribution of the heatmap to achieve the purpose of segmenting different characteristic content. Heatmap Alignment Branch (light\nblue) pools the features of each region to obtain feature vectors and performs classiÔ¨Åcation supervision on each feature vector. In order to achieve end-to-end\nlearning, TripletLoss is applied to each branch to narrow the distance of the same feature content. FSRA also retains the global branch (light purple) of the\ntransformer-based strong baseline.\nthat our multiple sampling strategy can effectively improve the\naccuracy of the model.\nIn short, the main contributions of this paper are as follows.\n‚Ä¢ We propose a transformer-based strong baseline for cross-\nview geo-localization and achieve competitive perfor-\nmance with CNN-based frameworks.\n‚Ä¢ For the problem caused by position offset and uncertainty\nof distance and scale, we designed FSRA to implement\npatch-level segmentation and region-level alignment.\n‚Ä¢ We have carefully analyzed and improved some tricks\nto try to solve some problems in cross-view geo-\nlocalization. To resolve the problem of sample size imbal-\nance under different perspectives in the University-1652,\na multiple sampling strategy that increases accuracy with\nno pain was proposed. To further improve the perfor-\nmance of cross-view geo-localization, we exhaustively\nanalyzed the impact of KLLoss [25] and TripletLoss and\nmade new improvements on TripletLoss.\n‚Ä¢ The Ô¨Ånal framework FSRA achieves state-of-the-art per-\nformance on both tasks of drone view target localization\nand drone navigation in the University-1652.\nII. RELATED WORK\nIn this section, we brieÔ¨Çy review related previous works,\nincluding Cross-View Geo-Localization and Transformer\nIn Vision.\nA. Cross-View Geo-Localization\nCross-view geo-localization mainly focuses on two match-\ning tasks: the matching of ground and satellite views and\nthe matching of drone and satellite views. CVUSA [26] and\nCV ACT [27] constructed a panoramic street-view image to\nmatch the satellite-view image which is a challenging task,\nwith a change in perspective spanning around 90 degrees.\nRecently, a large-scale benchmark called VIGOR [28], which\nbeyond One-to-one Retrieval, was proposed to bridge the\ngap between the realistic setting and existing geo-localization\ndatasets. University-1652 [1] innovatively proposed two mis-\nsions based on drone perspective: drone view target localiza-\ntion and drone navigation, which proposed the drone-view as\na transition view, reducing the difÔ¨Åculty of cross-view geo-\nlocalization.\nEfÔ¨Åcient Loss Function. A popular pipeline for cross-\nview is to design suitable loss functions to train a CNN\nbackbone, which is used to extract features from images. The\nCrossEntropy loss [29], TripletLoss [30], [31], and contrastive\nloss [32] are most widely used in the task of retrieval. Zheng et\nal. [33] applied instance loss and veriÔ¨Åcation loss together to\noptimize the network, and achieve competitive results. Hu et\nal. [34] proposed a weighted soft margin ranking loss, which\nnot only speeds up the training convergence but also improves\nthe retrieval accuracy. Luo et al. [35] proposed a BNNeck to\nimprove the coordination of ID loss and TripletLoss. Sun et\nal. [36] proposed a uniÔ¨Åed perspective to optimize ID loss and\nTripletLoss.\nPart-based Fine-grained Features. Focusing on the Ô¨Åne-\ngrained information of different parts helps the model learn\nmore comprehensive features. In addition, by dividing and\nsupervising the feature maps, the sub-salience features in the\nimage will be fully excavated. Fine-grained regions can be\nmanually generated by person but also can be automatically\nlearned by supervised methods. And the part-based Ô¨Åne-\ngrained features have been proven reliable in the task of\nretrieval [37], [38], [39], [40], [41]. LPN [12] proposed the\nsquare-ring partition strategy to allow the network to pay\nattention to more Ô¨Åne-grained information at the edge and\n4\nachieved a huge improvement. PCB [18] applied a horizon-\ntal splitting method for human body parts to extract high-\nlevel segmentation features. AlignedReID++ [22] automati-\ncally aligned slice information without introducing additional\nsupervision to solve pedestrian misalignment problems caused\nby occlusion, view variation, and attitude deviation. MGN\n[19] designed a slicing network that combines multi-branch\nand characterization metric dual learning strategies to extract\nglobal coarse-grained and local Ô¨Åne-grained features. MSCAN\n[19] proposed Spatial Transform Networks to learn the local\nfeatures of various parts of the human body, and merge\nthe local features and global features into the Ô¨Ånal feature\nrepresentation. PL-Net [21] introduces a part loss to realize au-\ntomatic detection of various parts of the human body, thereby\nincreasing the discrimination on unseen persons. Rodrigues\net al. [42] addressed the temporal gap between scenes by\nproposing a semantically driven data augmentation technique\nthat gives Siamese networks the ability to hallucinate unseen\nobjects, and then apply a multi-scale attentive embedding\nnetwork to perform matching tasks. Our proposed FSRA is\nalso one of the part-based methods which is inspired by\nthe LPN, the difference is that we do not add additional\nsupervision but achieve automatic region segmentation, which\nmakes our FSRA have excellent robustness and resistance to\nposition shift.\nB. Transformer In Vision\nThe attention mechanism [43] of the transformer model\nwas Ô¨Årst proposed to solve problems in the Ô¨Åeld of Natural\nLanguage Processing. Subsequently, the strong visual perfor-\nmance of the transformer shown the superiority of its structure.\nRecently, Han et al. [44] and Salman et al. [45] investigated the\napplication of the transformer in the Ô¨Åeld of computer vision.\nTransformer In Various Field. Alexey et al. [16] Ô¨Årst\napplied the transformer model to the task of classiÔ¨Åcation,\nand then the development of the transformer in vision was in\nfull swing. The transformer has achieved competitive results in\nmost mainstream visual Ô¨Åelds, such as object detection, seman-\ntic segmentation, GAN, Super-Resolution, Reid, etc. DETR\n[46] was the Ô¨Årst object detection framework that successfully\nintegrates the transformer as the central building block of the\ndetection pipeline. SETR [47] treated semantic segmentation\nas a sequence-to-sequence prediction task through a pure trans-\nformer. TransGAN [48] built a generator and a discriminator\nbased on two transformer structures. TTSR [49] restored the\ntexture information of the image super-resolution result based\non the transformer. TransReID [50] applied the transformer\nto the Ô¨Åeld of retrieval for the Ô¨Årst time and achieved similar\nresults with the CNN-based method. Yu et al. [51] extend\ntransformer model to Multimodal Transformer (MT) model for\nimage captioning and signiÔ¨Åcantly outperformed the previous\nstate-of-the-art methods.\nCombination Of CNN And Transformer. ConvTrans-\nformer [52] mapped the input sequence to a feature map\nsequence using an encoder based on a multi-headed convolu-\ntional self-attentive layer, and then decoded the target synthetic\nframe from the feature map sequence using another deep\nnetwork containing a multi-headed convolutional self-attentive\nlayer. Conformer [53] relied on Feature Coupling Unit (FCU)\nto interactively fuse local and global feature representations\nat different resolutions. Mobile-Former [54] was a parallel\ndesign of MobileNet and Transformer with a bi-directional\nbridge which enabled bi-directional fusion of local and global\nfeatures.\nTransformer In Cross-View. In the cross-view domain,\nsome novel and effective transformer structures have also been\nproposed to implement different downstream tasks.Chen et al.\n[55] proposed a pair of cross-view transformers to transform\nthe feature maps into the other view and introduce cross-view\nconsistency loss on them. Yang et al. [56] presented a novel\nframework that enables reconstructing a local map formed\nby road layout and vehicle occupancy in the bird‚Äôs-eye view\ngiven a front-view monocular image only, and a cross-view\ntransformation module was proposed to strengthen the view\ntransformation and scene understanding. Tulder et al. [57]\npresented a novel cross-view transformer method to transfer\ninformation between unregistered views at the level of spatial\nfeature maps, which achieved remarkable results in Ô¨Åeld of\nMulti-view medical image analysis. Yang et al. [58] proposed a\nsimple yet effective self-cross attention mechanism to improve\nthe quality of learned representations. Which improved the\ngeneralization ability and encourages representations to keep\nevolving as the network goes deeper.\nIII. P ROPOSED METHOD\nIn this section, we will introduce the details of our proposed\nmethod, the complete network structure as shown in Fig.\n3. Firstly, the structure of the vision transformer will be\nintroduced in Section III-A. Secondly, we will introduce the\ndetails of the proposed FSRA in Section III-B. Then a multiple\nsampling strategy to improve accuracy without pain will be\nintroduced in Section III-C. Finally, we will introduce other\ntricks we applied in Section III-D, including the speciÔ¨Åc\nprocess of our implementation of TripletLoss and mutual\nlearning.\nA. Transformer-Based Strong Baseline\nFollowing the general strong baseline for the University-\n1652 benchmark [1], we build a transformer-based strong\nbaseline for cross-view geo-localization. Our baseline consists\nof two parts: feature extraction and classiÔ¨Åcation supervised\nlearning. As in Fig. 2. Given an input x ‚ààRH√óW√óC, where\nH, W, C represent its height, width, and channels. Then input\nwill be divided into N Ô¨Åxed-size patches {xi\np|i = 1, 2, ¬∑¬∑¬∑ , N}\nand Ô¨Çatten into a sequence. An extra learnable embedding\ntoken denoted as xcls is merged into spatial information\nto extract robust features through supervision learning. The\noutput [cls token] as shown in Fig. 2 is regarded as a global\nfeature representation f. Position information is added to\neach patch through learnable position embedding. The input\nsequence can Ô¨Ånally be expressed as follows.\nZ0 = [xcls; F(x1\np); F(x2\np); ¬∑¬∑¬∑; F(xN\np )] +P (1)\n5\nWhere Z0 represents input sequence embeddings. F is\nlinear projection mapping the patches to D dimensions. P‚àà\nR(N+1)√óD is the position embeddings. L in Fig. 3 represents\nthe depth of the transformer layers. The transformer attention\nmechanism allows each layer of the transformer to have insight\ninto the global context, which overcomes the limitation of the\nreceptive Ô¨Åeld of the convolutional neural network. In addition,\nthe down-sampling operation is no longer needed.\nPosition Embeddings. The image classiÔ¨Åcation and the\ncross-view tasks are different in the resolution of the input, so\nthe position embedding parameters can not directly be loaded\nfrom the pre-training weights on ImageNet. The parameters of\nposition embedding are learnable.\nExtra Learnable Embedding. The characteristic of the\ntransformer structure is that it does not change the dimensions\nof the input data, and the output contains contextual informa-\ntion, which can represent global features. An Extra learnable\nparameter is added to the input to act as a global feature vector,\nand the parameters are also learnable.\nTransformer Layers. Transformer Layers play the same\nrole as the backbone to extract the contextual semantic rela-\ntionship between each patch. Its structure has shown on the\nright side of Fig. 2, which takes all Patches containing Position\nEmbedding as inputs, and Ô¨Ånally outputs feature vectors of\nthe same dimension as the original inputs after Multi-Head\nAttention.\nSupervision Learning. Transformer-based strong baseline\nonly regards classiÔ¨Åcation results as supervision information\nand applies CrossEntropy loss without label-smooth as ID loss.\nB. Concrete Implementation Of FSRA\nExperiments in Effect of the Transformer in Cross-View\nshow that the transformer-based strong baseline can achieve\nimpressive performance in cross-view geo-localization. How-\never, positional shift and Uncertainty of distance and scale are\nstill major challenges to overcome. Although it is important to\nextract global features that are robust and contextually linked,\nmuch previous work has also shown that part-based methods\nare signiÔ¨Åcantly more effective for image retrieval.\nAligning each part with features is a straightforward way\nto allow part-based methods to achieve end-to-end training.\nBased on that, we consider whether there is a reasonable and\nsimple way for the model to learn the category to which each\npatch belongs, such as buildings, roads, and trees, so that\nwe can segment and align them according to the category to\nwhich they belong. We suppose whether it is possible to cut\nout the characteristics of different categories according to the\nappearance of the heatmap and analyze the above problems as\nfollows.\nHow to segment speciÔ¨Åc content. HSM was proposed to\nachieve the purpose of segmenting different instances such as\nbuildings, roads, and trees. The overall idea is very simple.\nAs in Fig. 4, we take n = 2 as an example and divide the\nheatmap into two categories based on the magnitude of the\nheat value, with the large heat value being the foreground and\nthe small heat value representing the background. As shown\nin the thermodynamic diagram, it is easy to see that most\nHeatmap Alignment\nInputs\n Heatmap\nDroneSatellite\nFig. 4. The left column of the Ô¨Ågure is the input images from the drone-view\nand the satellite-view in the same geographic location. In the middle is the\nheatmap corresponding to the output of the FSRA. The right is the regional\ndistribution generated by the HSM, the red part can be understood as the\nbuilding part (foreground), and the green part is the background.\nof the building parts have larger thermal values, while the\ntrees and background parts have smaller thermal values. The\nnetwork pays different levels of attention to different parts,\nwhich would produce a certain regularity in the distribution\nof heatmap. HSM is inspired by that. We perform a uniform\nsegmentation of the feature map according to the thermal\ndistribution. As shown on the right side of Fig. 4, it is obvious\nthat we have almost entirely distinguished the buildings from\nthe other instances.\nIn the following, we will describe the detailed implementa-\ntion steps of the segmentation. Firstly, we get all the outputs\nL ‚ààRB√óN√óS (where B stands for batch size, N stands for\npatch size, and S stands for the length of the feature vector\ncorresponding to each patch) except for cls token through\nthe forward propagation of the transformer, which can be\nrepresented as follows.\nL= [F(x1\np); F(x2\np); ¬∑¬∑¬∑; F(xN\np )] (2)\nThe thermal value of each patch can be represented as\nfollows.\nPc = 1\nS\n‚àëS\ni=1 Mi c = {1, 2, ¬∑¬∑¬∑, N} (3)\nwhere Pc represents the heat value of the cth patch. Mi\nrepresents the cth patch corresponds to the ith value of the\nfeature vector. In short, we do an averaging operation for the\nfeature vector of each patch to represent the thermal value of\nthe patch. Then, we sort the value of P1‚àíN in descending\norder and divide patches equally according to the number of\nregions n. The number of patches corresponding to each region\nis as follows.\nNi = { ‚åäN\nn ‚åã i = {1, 2, ¬∑¬∑¬∑, n‚àí1}\nN ‚àí(n ‚àí1) √ó‚åäN\nn ‚åã i = n (4)\nwhere Ni represents the number of patches for the ith\nregion, ‚åä¬∑‚åãis the Ô¨Çoor function. Finally, divide Linto n parts\n6\n3 3 3 1 3 2 3 3 2 3 2 3 2 2 2 1 \n2 3 2 2 3 3 3 3 3 3 3 3 3 3 2 3 \n3 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3 \n3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 \n3 3 3 3 3 3 2 3 3 3 2 2 2 2 3 3 \n3 3 3 3 3 3 2 2 2 3 2 3 3 2 3 3 \n1 1 3 3 2 1 1 2 2 1 1 1 1 3 1 2 \n3 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 \n2 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 \n1 2 2 1 2 2 1 1 1 1 1 1 1 1 1 1 \n2 1 1 1 2 2 1 2 2 2 1 1 1 1 1 1 \n2 2 2 2 2 1 1 2 3 2 1 2 1 1 1 2 \n2 2 2 1 1 1 1 1 1 1 2 2 1 1 2 1 \n2 1 2 1 1 1 3 2 1 3 2 3 2 1 3 1 \n2 2 1 1 1 2 2 1 1 3 1 2 2 2 1 1 \n2 1 1 1 1 3 2 2 3 2 3 1 1 2 1 2 \n1 1 1 1 1 1 1 1 1 1 \n2 2 2 2 2 2 2 2 2 2 \n1 1 1 1 1 1 1 1 1 1 \nOriginal Drone Image Feature Maps Feature Vectors\n1 \n2 \n3 \n1 1 1 1 1 1 1 1 1 1 \n2 2 2 2 2 2 2 2 2 2 \n1 1 1 1 1 1 1 1 1 1 \nFeature Vectors\n1 \n2 \n3 \nOriginal Satellite Image Feature Maps\n1 2 2 3 2 3 3 3 3 3 3 3 3 2 3 3 \n1 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 \n1 2 2 1 2 3 3 3 3 3 3 3 3 3 3 3 \n1 2 3 2 3 3 2 3 3 3 3 3 3 3 3 3 \n2 2 2 2 1 2 1 1 2 1 3 1 2 3 3 3 \n2 2 3 2 2 2 1 1 1 3 3 3 2 2 3 3 \n2 2 2 1 1 1 1 2 2 2 3 3 3 3 3 3 \n1 1 1 1 1 1 1 2 1 1 1 2 2 2 3 3 \n1 1 1 1 1 2 1 1 1 1 2 1 3 1 2 3 \n2 2 1 1 1 2 2 1 1 1 1 2 2 1 3 3 \n1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 \n1 2 2 2 2 3 1 1 1 1 1 2 2 3 3 3 \n1 1 2 3 2 3 2 1 1 1 1 1 2 3 3 3 \n1 3 2 2 1 2 2 1 1 1 1 2 1 3 3 3 \n2 2 2 2 2 1 2 1 1 2 1 1 1 2 3 3 \n3 2 2 2 2 1 2 3 3 1 2 2 2 2 2 3 \nFeature Alignment\nFig. 5. The left column is the input images from the drone and satellite\nviews at the same geographic location, the middle column is the featuremaps\ngenerated by HAB with regions n = 3. In the right is the feature vector\nobtained by the average pooling of each region.\nin order, and each part corresponds to a region and then we\ncan tag each region as a category as shown in the right column\nof Fig. 4. Relying on HSM alone does not allow the model\nto move in the direction of focusing on what we want, so we\nneed to develop an alignment supervision for this partitioning\nlaw to allow the model to distinguish between instances. The\nnumber of regions n is a hyperparameter. In the following\nablation experiments, we found that n = 3performed the best.\nThe proposed HSM is located in the light green part of Fig.\n3. It is worth mentioning that HSM is implemented based on\npatch-level.\nAlignment between speciÔ¨Åc content. HAB was proposed\nto achieve the effect of feature alignment. As in Fig. 5. After\nsuccessfully segmenting the speciÔ¨Åc content, we divide all\npatches into n regions. Fig. 5 takes n = 3as an example. In\nessence, all patches are divided into 3 categories, and we use 1-\n3 to distinguish. The next step is to perform feature alignment\nbased on the corresponding content in different regions. We\nrespectively take out the part of buildings as f1, the part\nof roads as f2, and the part of trees as f3. Then a pooling\noperation is performed on f1‚àí3 to obtain the feature vector\nVi ‚ààRB√óNi√óS, i = {1, 2, 3}that characterizes each speciÔ¨Åc\ncontent. The visualization process can be seen on the right\nside of Fig. 5. The expression of Vi is as follows.\nVi = 1\nNi\n‚àëNi\nj=1 fj\ni i = {1, 2, ¬∑¬∑¬∑, n} (5)\nwhere n stands for the number of regions ( n is set to 3 in\nFig. 5). fj\ni stands for the feature vector of the jth patch of\nthe ith instance region. In short, Vi is obtained by taking out\nall the patches in each region and taking the average pooling\noperation.\nAfter the above steps, we obtain the vector expression\nof the corresponding feature content, and then we classify\neach feature content separately through a ClassifierLayer .\nIn addition, to allow the model to establish more accurate\nmatching relationships, we apply TripletLoss as in Fig. 3\nto all regions to narrow the distance between regions. The\nspeciÔ¨Åc implementation will be explained in section III.D. The\nproposed HAB is located in the light blue part of Fig. 3.\nIt is worth noting that our HAB method is region-level\nfeature alignment, and the division of regions is determined by\nHSM. The reason why HAB can achieve good performance is\nthat it distinguishes the features of different instances, which\nis conducive to the model not only paying attention to the\nglobal salient features, but also paying attention to the details\nof the background, which will help the model extraction more\ncomprehensive Ô¨Åne-grained features.\nC. A Multiple Sampling Strategy\nThere are some unstable factors during the training process\nbased on the transformer model. For example, a model with the\nsame settings is trained twice, the results obtained will have a\nlarge margin. However, We found that the main reason may be\nthat there is only one image per category in the satellite-view,\nwhich results in only one image from other views at one time.\nThis case will cause an imbalance between the satellite images\nand other images. Therefore, a multiple sampling strategy is\nproposed to alleviate the problem of sample imbalance.\nWe set a hyperparameter k, which represents the number of\nsampling. The speciÔ¨Åc implementation is as follows. Firstly,\nderive the image under the satellite perspective from the\nUnversity-1652, and enhance it to generate k augmented satel-\nlite images. Augmentation methods include random shifting,\nrandom padding, random cutting, random color enhancement,\netc. At the same time, k images from other perspectives\nare randomly selected, which is the same category as the\ncorresponding satellite perspective.\nThe detailed experiment on the number of sampling k was\nconducted in the part of the ablation study in Section IV , and\nthe results of the experiment show that FSRA performed best\nwhen k = 3.\nD. Other Tricks On Cross-View\nMutual Learning Based on Cross-View. Cross-view geo-\nlocalization is a multi-input and multi-output task. Given\nthis, we introduce a method of self-distillation. The speciÔ¨Åc\nimplementation is as follows. Establish learning relationships\nbetween outputs from different domains to narrow the distance\nbetween similar instances. The calculation formula of KL\ndivergence loss is shown below.\nKLDiv(O1||O2) =\nN‚àë\ni=1\np(Oi\n1) ¬∑log p(Oi\n1)\nq(Oi\n2) (6)\np(xi) =log( exi\n‚àë\nj exj\n) (7)\nq(xi) = exi\n‚àë\nj exj\n(8)\nwhere O1 represents the target output. O2 represents the\nlearning output from the model. The mutual learning loss\nfunction is expressed as follows.\nKLLoss = KLDiv(Od‚à•Os) +KLDiv(Os‚à•Od) (9)\n7\n1 2 3 4 5 6 7 81 2 3 4 5 6 7 8\ndist_an\ndist_ap√ó \nFig. 6. The number 1-8 indicates the category the image belongs to. The light\ngreen part represents 8 images from the views of drones or satellites, the light\npurple part represents 8 images from the views of satellites or drones, dist ap\nrepresents the distance between pictures of the same category, and dist an\nrepresents the distance between pictures of different categories, The red √ó\nstands for that the distance is not calculated for images from the same views.\nwhere Od stands for the output of the drone-view image\nafter forwarding propagation. Os stands for the output of the\nsatellite-view image after forwarding propagation.\nWe verify the effectiveness of KLLoss in the ablation study.\nExperiments show that when KLLoss is applied alone, the\naccuracy of the model is signiÔ¨Åcantly improved, but when\nKLLoss is applied together with TripletLoss, the accuracy of\nthe model is not improved signiÔ¨Åcantly. This may be caused\nby the same optimization direction of TripletLoss and KLLoss.\nTripletLoss based on Cross-View. Only using CrossEn-\ntropy loss can not make the model end-to-end. When testing\nthe accuracy of the model, Euclidean distance is used to\njudge the similarity between samples. TripletLoss can act as\na supervisor to narrow the distance between the same targets\nfrom different domains. The TripletLoss can be formulated as\nfollows.\nTL = ‚à•d(a, p) ‚àíd(a, n) +M‚à•+ (10)\nd(a, x) =‚à•a ‚àíx‚à•2 (11)\nwhere ‚à•¬∑‚à•+ represents max( ¬∑,0) operation. ‚à•¬∑‚à•2 represents\na 2-norm operation. M is the value of margin. We apply\nEuclidean distance in Equation 11 to measure the distance\nbetween vectors. In Equation 10, we compute the TripletLoss\nwith M = 0.3 in all our experiments.\nUnlike traditional TripletLoss, the task of cross-view is to\nmatch images from different domains, and it is not essential\nto be distinguished from images of the same perspective.\nTherefore, we only calculate TripletLoss for images between\ndifferent views. As in Fig. 6, for example, we take out\nan image from the light green set (drone/satellite view) to\ncalculate the TripletLoss with all images from the light purple\nset (satellite/drone view).\nIV. E XPERIMENT\nWe Ô¨Årst introduce a large-scale cross-view geo-localization\ndataset in Section IV-A. Then Section IV-B describes the\nimplementation details. We provide the comparison with state-\nof-the-art methods in Section IV-C, followed by the ablation\nstudy in Section IV-D.\nTABLE I\nSTATISTICS THE NUMBER OF IMAGES , BUILDINGS , AND UNIVERSITIES\nFROM VIEW OF DRONE , SATELLITE , AND STREET IN THE TRAINING SET\nAND TEST SET OF THE UNIVERSITY -1652 DATASET. AND STATISTICS THE\nIMAGES NUMBER OF QUERY AND GALLERY IN THE TEST SET . THERE ARE\nNO DUPLICATE UNIVERSITIES IN THE TRAINING SET AND TEST SET .\nsplit views images classes university\nTrain\nDrone 37,854 701\n33Satellite 701 701\nStreet 11,640 701\nTest\nQuery\nDrone 37,855 701\n39\nSatellite 701 701\nStreet 2,579 701\nGallery\nDrone 51,355 951\nSatellite 951 951\nStreet 2,921 793\nA. Datasets And Evaluation Protocol\nOur method is mainly used to solve UA V-related problems,\nincluding drone view target localization and drone navigation.\nWe have done a lot of experiments based on the large-scale\ndataset, University-1652 [1]. Table I shows the number of\nimages from different views of the University-1652 dataset\nduring training and testing. The column of classes indicates\nthe number of buildings, and the column of university indicates\nthe number of universities included in the sample. The entire\ndataset contains a total of 72 universities, and there is no\nintersection between the training set and the test set.\nUniversity-1652 is a multi-view multi-source benchmark\nfor drone-based geo-localization, which contains images\nfrom three platforms, i.e., synthetic drones, satellites, and\nground cameras. University-1652 is the Ô¨Årst large-scale geo-\nlocalization dataset contained drone-view and enables two\ntasks, i.e., drone view target localization (Drone ‚ÜíSatellite)\nand drone navigation (Satellite ‚ÜíDrone). It aims to improve\nthe accuracy of matching the images between drone-view and\nsatellite-view. The dataset collected 1,652 buildings from 72\nuniversities in the world. As in Table I, the training set includes\n701 buildings of 33 universities, and the testing set includes\nthe 951 buildings of the 39 universities. The buildings in the\ntraining set and the test set have no overlap. There are 701\nbuildings with 50,195 images for training, which contains\n37,854 drone-view images, 701 satellite-view images, and\n11,640 street-view images.\nFor testing, In the drone view target localization task\n(Drone‚ÜíSatellite), there are 37,855 drone-view images in the\nquery set and 701 true-matched satellite-view images, and\n250 satellite-view distractors in the gallery. There is only\none true-matched satellite-view image under this setting. In\nthe drone navigation task (Satellite ‚Üí Drone), there are 701\nsatellite-view query images, and 37,855 true-matched drone-\nview images, and 13,500 drone-view distractors in the gallery.\nThere are about 54 true-matched drone-view images that can\n8\nbe matched.\nB. Implementation Details\nIn data processing. We apply a multiple sampling strategy.\nConsidering that there is only one satellite image for each cat-\negory, image augmentation is applied to extend the satellite set\nfor alleviating the imbalance of images in different domains.\nIn network structure and training strategy. We adopt a\nsmall size Vision Transformer (Vit-S) pretrained on ImageNet\nas our backbone. We have adopted the FSRA structure which\nregions the output of the transformer by HSM, and aligns the\nfeature map by HAB. In terms of parameter initialization, we\nadopt kaiming initialization [59] for the classiÔ¨Åer module. In\ntraining, we resize the input image to the size of 256√ó256 and\nperform image augmentation, e.g., random padding, random\ncropping, and random Ô¨Çipping. For the optimizer, we adopt\nstochastic gradient descent (SGD) with momentum 0.9 and\nweight decay 0.0005 with a mini-batch of 8. For the setting\nof the initial learning rate, the backbone parameter is set to\n0.003, and the rest of the learnable parameters are set to 0.01.\nThe learning rate of all parameters are decayed by 0.1 in the\nepoch of 70 and 110 respectively, the model is trained for 120\nepochs in total.\nIn the loss function. We use the CrossEntropy loss as\nthe classiÔ¨Åcation loss function and adopt TripletLoss with a\nmargin of 0.3 to narrow the distance of the same target from\ndifferent domains. Besides, KL divergence loss is introduced\nto narrow the distance of the classiÔ¨Åcation vectors.\nDuring the test. We utilize the Euclidean distance to\ncalculate the similarity between query images and candidate\nimages in the gallery set. Our model is based on the framework\nof Pytorch, and all experiments are performed on Nvidia GTX\n1080Ti GPU.\nC. Comparison With Existing Methods\nOn the University-1652 [1] dataset, we employ the proposed\nFSRA to compare with existing competitive methods. As\nshown in Table II, in the task of Drone ‚Üí Satellite, the\nproposed HAB achieved 82.25% Recall@1 and 84.82% AP;\nIn the task of Satellite ‚ÜíDrone, FSRA has achieved 88.45%\nRecall@1 and 83.37% AP. All our experiments only use\ndrone and satellite views for training. The performance has\nsurpassed state-of-the-art method e.g., LPN by a large margin\nof about 6% AP improvement. When we adopt different\nsampling strategies, the experimental results of our method\nhave been further improved. When we use 3√ó sampling,\nthe value of Recall@1 rises from 82.25% to 84.51% and\nthe value of AP rises from 84.82% to 86.71% in the drone\nview target localization task (Drone ‚ÜíSatellite). The value of\nRecall@1 rises from 87.87% to 88.45% and the value of\nAP is from 81.53% to 83.37% in the drone navigation task\n(Satellite‚ÜíDrone).\nD. Ablation Studies\nTo verify the effectiveness of our method, we design several\nablation experiments.\nOrignal Image  ResNet Heatmap Transformer Heatmap\nFig. 7. The left is the original image, the middle is the heatmap of the last\nlayer of ResNet-50, and the right is the heatmap of the last layer of Vit-S.\nEffect of the Transformer in Cross-View. We bring the\ntransformer network structure into the Ô¨Åeld of cross-view and\ncompare the performance of Transformer-based and ResNet-\nbased networks. As shown in Table III. The Vit-S network\nwith a single branch outperforms ResNet-50 by 9.31% and\noutperforms ResNet-101 by 6.3%, and the inference time\nis only 1.21√ó of ResNet-50, which is faster than ResNet-\n101. Besides, we also compared the accuracy and speed of\nVit-B with other backbones. We found that deepening the\ntransformer can not bring a signiÔ¨Åcant improvement. The\ntransformer‚Äôs attention mechanism has its limitations, and the\nimpact of its model size on performance depends on the size\nof the data volume. University-1652 is a 10,000-level data\nset, which is not suitable for large-scale transformer networks.\nTherefore, we use Vit-S as the backbone for other ablation\nexperiments. Through the comparison of baseline between\nCNN-based method and Transformer-based method, we found\nthat there is a large margin between the CNN-based method\nand the Transformer-based method. Since we checked the\nheatmap based on ResNet-50 and Vit-S respectively. As in Fig.\n7. The attention mechanism allows the network to focus on\nglobal information, while the CNN-based approach will only\nfocus on notable information but ignore the peripheral features.\nIn addition, the heatmaps generated by the Transformer-based\nmethod can segment buildings, roads, and trees, which pave\nthe way for our method.\nEffect of the number of regions. The number of regions is\nan important indicator in our network. By default, we deploy\nn = 3. The model only applies the global branch of Vit\nwhen n = 0. When n = 1, the HAB deploys global average\npooling of the feature vectors and concats them with the global\nbranch of Vit. We make an experiment to verify the inÔ¨Çuence\nof the number of regions on the accuracy of Recall@1 and\nAP, as in Fig. 8. When the number of regions n = 3, all\nindicators are the best. We believe that when n = 3, the\nproposed FSRA divides the images of the University-1652 [1]\ndataset into three categories: buildings, roads, and trees. And\nthe features between different domains can be well segmented\nand aligned. When the number of regions n = 2, the proposed\nFSRA divides the image into two categories: architecture and\nbackground, which also achieves good performance.\nRobustness of FSRA to position shifting. In order to verify\nthe robustness of FSRA against position shifting, two different\nshifting methods are proposed for testing: BlackPad (BP )\nand FlipPad (FP ). BlackPad Ô¨Ålls the black block with\nwidth P on the left side of the image and cuts out the image\n9\nTABLE II\nCOMPARISION WITH STATE -OF-THE -ART RESULTS WHICH HAVE REPORTED IN UNIVERSITY -1652. M REPRESENTS THE MARGIN OF TRIPLET LOSS , K\nREPRESENTS THE NUMBER OF SAMPLING , S REPRESENTS THE SIZE OF INPUT IMAGES AND VIT-S REPRESENTS THE SMALL -SCALE VISION\nTRANSFORMER NETWORK ..\nMethod Backbone\nDrone ‚Üí Satellite Satellite ‚Üí Drone\nR@1 AP R@1 AP\nContrastive Loss [32] VGG16 52.39 57.44 63.91 52.24\nWeighted Soft Margin TripletLoss [34] VGG16 53.21 58.03 65.62 54.47\nTripletLoss (M = 0.3) [30] ResNet-50 55.18 59.97 63.62 53.85\nInstance Loss + GeM Pooling [60] ResNet-50 65.32 69.61 79.03 65.35\nInstance Loss [1] ResNet-50 58.23 62.91 74.47 59.45\nLCM (ResNet-50) [24] ResNet-50 66.65 70.82 79.89 65.38\nLPN [12] ResNet-50 75.93 79.14 86.45 74.79\nOurs (k=1) Vit-S 82.25 84.82 87.87 81.53\nOurs (k=3) Vit-S 84.51 86.71 88.45 83.37\nOurs (k=1, s=512) Vit-S 85.50 87.53 89.73 84.94\nTABLE III\nCOMPARISON OF RESNET AND VISION TRANSFORMER . INFERENCE TIME\nIS MEASURED COMPARED TO RESNET-50, AND OTHER BACKBONES ARE\nEVALUATED RELATIVE TO THE BASELINE OF RESNET-50. A LL RESULTS\nARE PERFORMED ON THE SAME DEVICE FOR A FAIR COMPARISON .\nVIT-S/16 IS REGARDED AS THE BASELINE MODEL AND ABBREVIATED AS\nBASELINE IN THE REST OF THIS PAPER . VIT-B/16 IS THE STANDARD\nMODEL PROPOSED IN THE ORIGINAL PAPER [16]\nBackbone Inference Time\nDrone‚ÜíSatellite Satellite ‚ÜíDrone\nR@1 AP R@1 AP\nResNet-50 1x 60.93 65.31 75.61 61.69\nResNet-101 1.48x 65.33 68.32 79.44 65.43\nVit-S/16 1.21x 71.04 74.62 83.31 72.08\nVit-B/16 1.79x 73.32 76.88 84.74 74.72\nTABLE IV\nIN THE TWO CASES OF BLACK PAD AND FLIP PAD, THE PROPOSED FSRA\nAND STATE-OF-THE-ART METHOD LPN CORRESPOND TO THE AP\nACCURACY VALUES OF DIFFERENT PAD SIZES AND THE SPEED OF\nDECLINE .\nPad\nPixel\nBlack Pad AP (%) Flip Pad AP (%)\nFSRA LPN FSRA LPN\n0 84.77‚àí0 81.17‚àí0 84.77‚àí0 81.17‚àí0\n10 84.13‚àí0.64 80.79‚àí0.38 84.19‚àí0.58 80.07‚àí1.10\n20 82.7‚àí2.07 78.29‚àí2.88 82.26‚àí2.51 77.18‚àí3.99\n30 80.03‚àí4.74 74.01‚àí7.16 78.46‚àí6.31 72.67‚àí8.50\n40 76.41‚àí8.36 68.06‚àí13.08 73.13‚àí11.64 65.83‚àí15.34\n50 71.6‚àí13.17 60.61‚àí20.56 66.07‚àí18.70 58.17‚àí23.00\n60 65.76‚àí19.01 52.09‚àí29.08 57.96‚àí26.81 49.88‚àí31.29\nwith width P on the right side. FlipPad Ô¨Çips the part with\nwidth P on the left side of the image and cuts out the image\nwith width P on the right. As in Fig. 9. In order to verify the\nanti-offset of FSRA, we compare the proposed FSRA with\nthe state-of-the-art method LPN. As shown in Fig. 10, when\nthe padding size increases, the accuracy of FSRA decreases\nFig. 8. Compare the effects of the number of regions n on the task of\ndrone view target localization and the task of drone navigation. The red line\nrepresents the task of drone view target localization (Drone ‚Üí Satellite), and\nthe blue line represents the task of drone navigation (Satellite ‚Üí Drone). Our\nexperiments are all based on TripletLoss (M=0.3). (a) Show the effect of the\nnumber of regions n on the accuracy of Recall@1. (b) Show the effect of\nthe number of regions n on the accuracy of AP. We Ô¨Ånd that R@1 and AP\nachieve the best performance when n=3.\nmuch slower than LPN. Besides, the accuracy of BlackPad\ndecreases more slowly than FlipPad , which may be caused\nby the fact that FP increases the confusion information at the\nedge and causes the uneven content distribution. As shown\nin Table IV , when BP = 60, The AP of LPN was reduced\nby 29.08%, while the AP of our FSRA was reduced by\n19.01%, which is about 10 points less than that of LPN. When\nFP = 60, The AP of LPN was reduced by 31.29%, while the\nAP of our FSRA was reduced by 26.81%, which is about 4.5\npoints less than that of LPN..\nThe advantage of FSRA over part-based like LPN in re-\nsisting position shift mainly lies in the fact that FSRA does\nnot artiÔ¨Åcially design regions, but allows the model to learn\n10\nOriginal Drone Image Black Pad 20 Flip Pad 20\nFig. 9. The image on the left is the original drone image, and the middle\nimage is the image with a width of 20 expanded with black on the left side of\nthe image and cropped to the same width on the right side of the image. The\nimage on the right is obtained by mirroring and expanding a 20-pixel wide\nportion of the left side of the image and cutting off an equal pixel width on\nthe right side of the image. The red dotted line is the dividing line of Padding.\nFig. 10. Just like the two padding methods shown in Fig. 9, we explore the\nimpact of the number of the black pad and the Ô¨Çip pad on AP and Recall@1.\nThe vertical axis represents the magnitude of the decrease in accuracy.\nto a set of division rules by itself, and this segmentation\nis patch-level. Therefore, when the input image has a large\nposition offset, the network can still distinguish which parts\nare buildings and which parts are trees. In contrast, artiÔ¨Åcially\ndesigned segmentation no longer makes sense when signiÔ¨Åcant\noffsets occur, but is often effective in the absence of offsets\nand anomalies. This idea can also be applied to the Ô¨Åeld of\nReID. For example, During object detection, there might be\nincomplete cuts of the human body, or the cut image contains\na lot of background. In this case, our FSRA can still be\nrecognized effectively by automatic segmentation.\nThe impact of sampling on accuracy. Adequate sampling\nhas a great inÔ¨Çuence on the Ô¨Åtting of the network. Unbalanced\nor insufÔ¨Åcient data will lead to unstable model training, and\nthe Ô¨Ånal results will be unsatisfactory. In the University-1652\ndataset, one satellite image corresponds to 27 drone-view\nimages. Previously, single-fold sampling was done by taking\none from a speciÔ¨Åc category in each iteration, i.e., one of the\n27 drone-view images and only one of the satellite-view.The\nmultiple sampling approach can optimize two aspects of\nthe problem: 1) the sample imbalance problem of different\nviewpoint images. This problem has essentially been raised\n(a) (b) \n(c) (d) \nFig. 11. We conducted experiments on the impact of the number of sampling\non AP and Recall@1 in tasks of Drone ‚ÜíSatellite and Satellite ‚ÜíDrone. k\nstands for the number of the sampling. When the number of sampling is 3,\nthe accuracy of AP and R@1 in both tasks reaches the best.\nin LCM [24] (the authors achieved the best using equal-\nmultiplicity sampling of UA V and satellite images). 2) The\nnumber and proportion of positive and negative samples for\nTripletLoss. When we change the sampling multiplicity, the\nnumber of positive samples in a single batch of TripletLoss\nwill change, which has an impact on the metric learning. To\nverify that our approach is not due to the effect of TripletLoss,\nwe conducted experiments using the FSRA with region n = 3\nand no TripletLoss. As shown in Fig. 11(a) and (b), the trend\nis up and then down in both AP and R@1 indicators, and\nthe overall optimum is reached at k = 3. In addition, we\ntrain the model with the addition of TripletLoss, as shown in\nFig. 11(c) and (d), which also shows the same trend of rising\nthen falling and optimal at k = 3. The parameter k can be\ninterpreted as a hyperparameter, and k = 3is a more effective\nvalue in Unversity-1652. k affects the training time, but has\nno effect on the inference phase. We believe that the reason\nwhy k is too large for model training is overÔ¨Åtting on one\nhand, and on the other hand, as k increases, the proportion of\nsimilar samples in a single batch will increase, and the model\nwill learn fewer inter-class differences in a single batch.We\nconjecture that batchsize has an impact on the choice of k\nvalues in the multiple sampling strategy, which we discuss in\nAppendix B.\nEffect of the input image size. Image with small size\nwill compress the Ô¨Åne-grained information and damage the\ncomplete features of the original image. Large-scale images\ncan often achieve higher accuracy because they maintain\nthe original Ô¨Åne-grained information. In contrast, large-scale\n11\nTABLE V\nABLATION STUDY ON THE IMPACT OF DIFFERENT INPUT SIZES ON\nUNIVERSITY -1652. T HE EXPERIMENTAL RESULTS ARE BASED ON THE\nNUMBER OF SAMPLING K =1, TRIPLE LOSS WITH MARGIN =0.3.\nImage Size\nDrone ‚Üí Satellite Drone ‚Üí Satellite\nR@1 AP R@1 AP\n224 80.81 83.65 87.73 80.02\n256 82.25 84.82 87.87 81.53\n320 84.08 86.38 87.87 82.63\n384 84.82 87.03 87.59 83.37\n512 85.5 87.53 89.73 84.94\nimages often require larger memory resources and longer\ninference time during training and testing. To balance the input\nimage size with memory usage, we conduct experiments on\nFSRA with the number of regions n = 3. According to dif-\nferent input sizes, the experimental results are shown in Table\nV . In both tasks, i.e., Drone ‚ÜíSatellite and Satellite ‚ÜíDrone,\nwe observe that the performance gradually improves when the\ninput image size increases from 224 to 512, and the AP has a\nbig improvement when the image input size is changed from\n256 to 320. We hope that when the hardware resources are\nlimited, this ablation experiment can play a reference role in\nselecting the appropriate input image size.\nTABLE VI\nABLATION STUDY TO VERIFY THE ROBUSTNESS OF THE PROPOSED FSRA\nAT DIFFERENT DISTANCES BETWEEN DRONES AND TARGET IN\nUNIVERSITY -1652.\nDistance\nDrone ‚Üí Satellite\nR@1 AP\nALL 82.25 84.82\nLong 79.71 82.69\nMiddle 84.05 86.36\nShort 82.87 85.25\nEffect of the drone distance to the geographic target. The\nscale of the satellite-view image in University-1652 is Ô¨Åxed,\nwhile the scale of the drone-view image changes dynamically\nwith the distance of the drone to the geographic target.\nAccording to the distance between the drone and the target\nbuilding, we divide the University-1652 dataset into three\nparts: Long, Middle, and Short. We verify the effect of\nthe proposed FSRA under three different levels of distance,\nas shown in Table VI. The proposed FSRA does not have a\nbig margin at a different level of distance. It has the lowest\naccuracy at Long distances and the highest accuracy at Middle.\nCompared with the current state-of-the-art network e.g., LPN,\nwhich has a margin of 20% Recall@1 and 17% AP between\nLong and Middle distance, the proposed FSRA has better scale\nadaptive capabilities.\nEffect of some other tricks. For the task of matching\nthe drone-view and the satellite-view images, we adopt three\ntricks of KLLoss, TripletLoss with margin=0.3, and multiple\nTABLE VII\nABLATION STUDIES TO VERIFY THE EFFECTS OF SOME OTHER TRICKS ,\nINCLUDING KLL OSS , TRIPLET LOSS , AND THE NUMBER OF SAMPLING IN\nUNIVERSITY -1652.D ‚ÜíS MEANS THE TASK OF DRONE ‚ÜíSATELLITE , AND\nS‚ÜíD MEANS THE TASK OF SATELLITE ‚ÜíDRONE .\nKLLoss TripletLoss\n(M=0.3)\nSampling\nRate\nAP (%)\nD‚ÜíS S ‚ÜíD\n1√ó 83.30 79.87\n‚úì 1√ó 84.14 80.93\n‚úì ‚úì 1√ó 84.82 81.53\n‚úì 1√ó 84.85 81.52\n‚úì 2√ó 86.36 82.69\n‚úì 3√ó 86.71 83.37\nsampling for the FSRA to improve the performance. As shown\nin Table VII. Only using KLLoss increases by 0.84%/1.06%\nAP on the task of Drone ‚ÜíSatellite / Satellite ‚ÜíDrone. Only\nusing TripletLoss increases by 1.52%/1.66% AP. When we\nuse KLLoss and TripletLoss at the same time, the accuracy of\nAP is not improved much. Thus, we did not use KLLoss but\nTripletLoss in our model. We guess that TripletLoss and KL-\nLoss are consistent in the same direction of network Ô¨Åtting. In\naddition, we deploy the sampling strategy as a trick. Based on\nTripletLoss with margin=0.3. When the number of sampling\nreaches 2√ó, the AP of FSRA increases by 1.51%/1.17% on the\ntask of Drone‚ÜíSatellite / Satellite‚ÜíDrone. When the number\nof sampling reaches 3 √ó, the AP increases by 1.86%/1.85%.\nThe performance improvement obtained by multiple sampling\nis due to the expansion of the data which can strengthen the\nÔ¨Åtting of the network and balance the resources from different\ndomains.\nE. Visualization Of Qualitative Result\nFor the two basic tasks of the University-1652 dataset: drone\nview target localization and Drone Navigation, we visualize\nsome retrieved results in Fig. 12. We observe that FSRA can\nadapt to retrieving the available images from the gallery set in\nboth drone view target localization and drone navigation tasks.\nIn the task of drone view target localization, we randomly take\nout three drone-view images from the test dataset. For each\ndrone-view image, we take out the top Ô¨Åve similar images\nfrom the gallery set, and the FSRA obtains completely correct\nresults as in Fig. 12 (I). In the Drone Navigation task, we\nrandomly take out three Satellite-view images from the test\ndataset. For each Satellite-view image, we also take out the\ntop 5 similar images in the gallery, because there is only one\nsatellite image for each category. The proposed FSRA still\nachieved completely correct results as in Fig. 12 (II).\nV. C ONCLSION\nIn this paper, we apply the structure of the Transformer\nto the Ô¨Åeld of cross-view geo-localization. The context infor-\nmation contained in the attention mechanism can distinguish\nmore Ô¨Åne-grained features, and explore some associated in-\nformation. Our experiments prove that the transformer-based\n12\nw \nDrone Satellite  (R@1->R@5)\nSatellite Drone  (R@1->R@5)\nI. University-1652 Ôºà Drone Location Ôºâ\nII. University-1652 Ôºà Drone Navigation Ôºâ\nTrue-Matched Image False-Matched Image \nFig. 12. Qualitative image retrieval results. (I) Top-5 retrieval results of\ndrone view target localization on University-1652. (II) Top-5 retrieval results\nof drone navigation on University-1652. The yellow box indicates the true-\nmatched image, and the blue box indicates the false-matched image.\nFSRA can obtain state-of-the-art performance in the bench-\nmark of the University-1652. In addition, some modules are\nproposed to improve model performance. HSM is proposed\nto implement patch-level semantic segmentation, and HAB is\nproposed to achieve region-level feature alignment. Although\nexperiments shows that the proposed FSRA has strong ro-\nbustness to feature misalignment and position shifts, there\nare still many parts that can be further improved. e.g., the\nstructure of Vit can be modiÔ¨Åed to achieve more amazing\nperformance. the backbone based on Vit-S has an increase\nin inference time compared to Resnet-50, which will be\nconsidered a shortcoming of this method. Besides, we also\nadopted a multiple sampling strategy to Ô¨Åt the model to a\nbetter state. This strategy can achieve a stunning rise, but\nthe disadvantage is that it increases the training time. Finally,\nsome other tricks such as mutual learning and TripletLoss are\napplied to make the FSRA stronger. In the Ô¨Åeld of current\ngeo-localization based on the perspective of drones. It is very\nnecessary to construct a dense geographic dataset that the\nmodel can learn more distinctive and Ô¨Åne-grained features to\nachieve precise positioning. In the future, we will propose a\nnew intensive UA V cross-view geo-localization dataset to meet\nthe requirements of practical applications.\nACKNOWLEDGMENTS\nThe research is supported by the National Natural Science\nFoundation of China (Project 51605462).\nTABLE VIII\nTHE MODELS WERE TRAINED ON GROUND , DRONE AND SATELLITE\nVIEWS , AND THE ACCURACY OF MATCHING BETWEEN GROUND AND\nDRONE VIEWS WAS TESTED . WHERE G REFERS TO GROUND -VIEW AND D\nREFERS TO DRONE -VIEW .\nModel Direction R@1 R@Top1% AP\nUniversity-1652\nG‚ÜíD 0.85 20.36 0.71\nD‚ÜíG 0.99 15.07 1.11\nLPN\nG‚ÜíD 0.85 20.47 0.94\nD‚ÜíG 1.70 17.41 1.70\nFSRA(ours)\nG‚ÜíD 1.94 31.91 1.67\nD‚ÜíG 2.75 24.92 2.63\nAPPENDIX A\nDOES IT WORK FOR GROUND VIEW ?\nThe drone-view can be used as an intermediate view be-\ntween the satellite and ground view because there is a 90\ndegree deviation between the ground-view image and the\nsatellite-view image, and the occlusion between objects, the\ndrastic differences in viewpoints and even the temporal gap\nbetween scenes make it very challenging. We try to use\nthe drone-view to match the ground-view and thus indirectly\nreduce the difÔ¨Åculty of the matching between ground and\nsatellite. For fairness, we apply the same learning strategy to\nthe different models (all using only classiÔ¨Åcation loss). The\nexperimental results are shown in Table VIII. The proposed\nFSRA has improved somewhat compared with University-\n1652 and LPN, but still remains in single digits. Matching\nsingle-view ground images with UA V images is a huge chal-\nlenge, mainly because there is a mismatch of shooting angles\nbetween ground-view images and UA V-view images, which in\nturn leads to large differences in the included content.\nAPPENDIX B\nDOES BATCHSIZE HAVE EFFECTS ON THE CHOICE OF K ?\nIn order to verify the effect of batchsize on the choice\nof the optimal k value, we increased the batchsize from 8\nto 16 and conducted experiments for k varying from 1 to\n8 (to avoid the effect of TripletLoss positive and negative\nsample ratios on the experiments, only classiÔ¨Åcation loss was\nused in the experiments). as shown in Fig. 13, when the\nbatchsize increases to 16, the optimal hyperparameter k should\nbe chosen to be around 5 ( k = 3 reaches the optimum for\nbatchsize=8). This also veriÔ¨Åes our statement in The impact\nof sampling on accuracy that the proportion of samples of\nthe same class in a batch affects the effectiveness of model\ntraining. Our proposed multiple sampling strategy can be used\nnot only to expand the severely underrepresented satellite\n13\nFig. 13. The effect of the variation of numbers of sampling k on the Ô¨Ånal\ntraining results of the model when batchsize=16 is demonstrated, where the\nresults of the R@1 evaluation metric are shown on the left and the results of\nthe AP evaluation metric are shown on the right.\nimages in University-1652, but also to change the distribution\nof samples in the batch, so our proposed multiple sampling\nstrategy needs to select the best k value according to the\nactual batchsize. We conclude that k = 3 is optimal when\nbatchsize=8 and k = 5is optimal when batchsize=16. It should\nalso be noted that increasing the value of k increases the model\ntraining time exponentially, but does not have any effect on\nthe inference process.\nREFERENCES\n[1] Z. Zheng, Y . Wei, and Y . Yang, ‚ÄúUniversity-1652: A multi-view multi-\nsource benchmark for drone-based geo-localization,‚Äù in Proceedings of\nthe 28th ACM international conference on Multimedia , 2020, pp. 1395‚Äì\n1403, doi:https://doi.org/10.1145/3394171.3413896.\n[2] S. Brar, R. Rabbat, V . Raithatha, G. Runcie, and A. Yu, ‚ÄúDrones\nfor deliveries,‚Äù Sutardja Center for Entrepreneurship & Technology,\nUniversity of California, Berkeley, Technical Report , vol. 8, p. 2015,\n2015.\n[3] Q. Yu, C. Wang, B. Cetiner, S. X. Yu, F. Mckenna, E. Taciroglu, and\nK. H. Law, ‚ÄúBuilding information modeling and classiÔ¨Åcation by visual\nlearning at a city scale,‚Äù arXiv preprint arXiv:1910.06391 , 2019.\n[4] P. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, ‚ÄúVision meets drones: A\nchallenge,‚Äù arXiv preprint arXiv:1804.07437 , 2018.\n[5] Y . Long, Y . Gong, Z. Xiao, and Q. Liu, ‚ÄúAccurate object localization in\nremote sensing images based on convolutional neural networks,‚Äù IEEE\nTransactions on Geoscience and Remote Sensing , vol. 55, no. 5, pp.\n2486‚Äì2498, 2017, doi:https://doi.org/10.1109/TGRS.2016.2645610.\n[6] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic, ‚ÄúNetvlad:\nCnn architecture for weakly supervised place recognition,‚Äù in Proceed-\nings of the IEEE conference on computer vision and pattern recognition,\n2016, pp. 5297‚Äì5307, doi:https://doi.org/10.1109/TPAMI.2017.2711011.\n[7] T.-Y . Lin, Y . Cui, S. Belongie, and J. Hays, ‚ÄúLearning deep representa-\ntions for ground-to-aerial geolocalization,‚Äù in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp. 5007‚Äì\n5015, doi:https://doi.org/10.1109/cvpr.2015.7299135.\n[8] Y . Tian, C. Chen, and M. Shah, ‚ÄúCross-view image matching for\ngeo-localization in urban environments,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2017, pp.\n3608‚Äì3616, doi:https://doi.org/10.1109/CVPR.2017.216.\n[9] A. Torii, R. Arandjelovic, J. Sivic, M. Okutomi, and T. Pajdla, ‚Äú24/7\nplace recognition by view synthesis,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2015, pp.\n1808‚Äì1817, doi:https://doi.org/10.1109/cvpr.2015.7298790.\n[10] S. Ji, S. Wei, and M. Lu, ‚ÄúFully convolutional networks for multisource\nbuilding extraction from an open aerial and satellite imagery data set,‚Äù\nIEEE Transactions on Geoscience and Remote Sensing , vol. 57, no. 1,\npp. 574‚Äì586, 2018, doi:https://doi.org/10.1109/TGRS.2018.2858817.\n[11] X. Tian, J. Shao, D. Ouyang, and H. T. Shen, ‚ÄúUav-satellite view synthe-\nsis for cross-view geo-localization,‚Äù IEEE Transactions on Circuits and\nSystems for Video Technology, 2021, doi:https://doi.org/10.1109/TCSVT.\n2021.3121987.\n[12] T. Wang, Z. Zheng, C. Yan, J. Zhang, Y . Sun, B. Zhenga, and Y . Yang,\n‚ÄúEach part matters: Local patterns facilitate cross-view geo-localization,‚Äù\nIEEE Transactions on Circuits and Systems for Video Technology, 2021,\ndoi:https://doi.org/10.1109/TCSVT.2021.3061265.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances\nin neural information processing systems , 2017, pp. 5998‚Äì6008.\n[14] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770‚Äì778, doi:https://doi.org/10.1109/\nCVPR.2016.90.\n[15] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural\nnetworks,‚Äù in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2018, pp. 7794‚Äì7803.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n‚ÄúAn image is worth 16x16 words: Transformers for image recognition\nat scale,‚Äù arXiv preprint arXiv:2010.11929 , 2020.\n[17] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, ‚ÄúGrad-cam: Visual explanations from deep networks via\ngradient-based localization,‚Äù in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 618‚Äì626.\n[18] Y . Sun, L. Zheng, Y . Yang, Q. Tian, and S. Wang, ‚ÄúBeyond part\nmodels: Person retrieval with reÔ¨Åned part pooling (and a strong con-\nvolutional baseline),‚Äù in Proceedings of the European conference on\ncomputer vision (ECCV) , 2018, pp. 480‚Äì496, doi:https://doi.org/10.\n1007/978-3-030-01225-0 30.\n[19] G. Wang, Y . Yuan, X. Chen, J. Li, and X. Zhou, ‚ÄúLearning discriminative\nfeatures with multiple granularities for person re-identiÔ¨Åcation,‚Äù in\nProceedings of the 26th ACM international conference on Multimedia .\nACM, oct 2018, doi:https://doi.org/10.1145%2F3240508.3240552.\n[20] D. Li, X. Chen, Z. Zhang, and K. Huang, ‚ÄúLearning deep context-\naware features over body and latent parts for person re-identiÔ¨Åcation,‚Äù\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 384‚Äì393.\n[21] H. Yao, S. Zhang, R. Hong, Y . Zhang, C. Xu, and Q. Tian, ‚ÄúDeep\nrepresentation learning with part loss for person re-identiÔ¨Åcation,‚Äù IEEE\nTransactions on Image Processing, vol. 28, no. 6, pp. 2860‚Äì2871, 2019,\ndoi:https://doi.org/10.1109/TIP.2019.2891888.\n[22] H. Luo, W. Jiang, X. Zhang, X. Fan, J. Qian, and C. Zhang,\n‚ÄúAlignedreid++: Dynamically matching local information for person re-\nidentiÔ¨Åcation,‚Äù Pattern Recognition, vol. 94, pp. 53‚Äì61, 2019, doi:https:\n//doi.org/10.1016/j.patcog.2019.05.028.\n[23] Z. Zheng, L. Zheng, and Y . Yang, ‚ÄúPedestrian alignment network for\nlarge-scale person re-identiÔ¨Åcation,‚Äù IEEE Transactions on Circuits and\nSystems for Video Technology , vol. 29, no. 10, pp. 3037‚Äì3045, 2018,\ndoi:https://doi.org/10.1109/TCSVT.2018.2873599.\n[24] L. Ding, J. Zhou, L. Meng, and Z. Long, ‚ÄúA practical cross-view\nimage matching method between uav and satellite for uav-based geo-\nlocalization,‚Äù Remote Sensing , vol. 13, no. 1, p. 47, 2021, doi:https:\n//doi.org/10.3390/rs13010047.\n[25] J. Yim, D. Joo, J. Bae, and J. Kim, ‚ÄúA gift from knowledge distillation:\nFast optimization, network minimization and transfer learning,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017, pp. 4133‚Äì4141.\n[26] M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, ‚ÄúPredicting ground-\nlevel scene layout from aerial imagery,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2017, pp. 867‚Äì\n875.\n[27] L. Liu and H. Li, ‚ÄúLending orientation to neural networks for cross-\nview geo-localization,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 5624‚Äì5633.\n[28] S. Zhu, T. Yang, and C. Chen, ‚ÄúVigor: Cross-view image geo-localization\nbeyond one-to-one retrieval,‚Äù in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2021, pp. 3640‚Äì3649,\ndoi:https://doi.org/10.1109/cvpr46437.2021.00364.\n[29] Z. Zheng, L. Zheng, and Y . Yang, ‚ÄúA discriminatively learned cnn em-\nbedding for person reidentiÔ¨Åcation,‚Äù ACM Transactions on Multimedia\n14\nComputing, Communications, and Applications (TOMM) , vol. 14, no. 1,\npp. 1‚Äì20, 2017, doi:https://doi.org/10.1145/3159171.\n[30] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, ‚ÄúEnd-to-end comparative\nattention networks for person re-identiÔ¨Åcation,‚Äù IEEE Transactions on\nImage Processing, vol. 26, no. 7, pp. 3492‚Äì3506, 2017, doi:https://doi.\norg/10.1109/TIP.2017.2700762.\n[31] P. Li, P. Pan, P. Liu, M. Xu, and Y . Yang, ‚ÄúHierarchical temporal\nmodeling with mutual distance matching for video based person re-\nidentiÔ¨Åcation,‚Äù IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 31, no. 2, pp. 503‚Äì511, 2020, doi:https://doi.org/10.\n1109/TCSVT.2020.2988034.\n[32] W. Deng, L. Zheng, Q. Ye, G. Kang, Y . Yang, and J. Jiao, ‚ÄúImage-\nimage domain adaptation with preserved self-similarity and domain-\ndissimilarity for person re-identiÔ¨Åcation,‚Äù in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2018, pp. 994‚Äì\n1003.\n[33] Z. Zheng, L. Zheng, M. Garrett, Y . Yang, M. Xu, and Y .-D. Shen,\n‚ÄúDual-path convolutional image-text embeddings with instance loss,‚Äù\nACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM) , vol. 16, no. 2, pp. 1‚Äì23, 2020, doi:https://doi.\norg/10.1145/3383184.\n[34] S. Hu, M. Feng, R. M. Nguyen, and G. H. Lee, ‚ÄúCvm-net: Cross-view\nmatching network for image-based ground-to-aerial geo-localization,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 7258‚Äì7267.\n[35] L. Liu and H. Li, ‚ÄúLending orientation to neural networks for cross-\nview geo-localization,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 5624‚Äì5633.\n[36] Y . Sun, C. Cheng, Y . Zhang, C. Zhang, L. Zheng, Z. Wang, and Y . Wei,\n‚ÄúCircle loss: A uniÔ¨Åed perspective of pair similarity optimization,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 6398‚Äì6407.\n[37] J. Qian, W. Jiang, H. Luo, and H. Yu, ‚ÄúStripe-based and attribute-\naware network: A two-branch deep model for vehicle re-identiÔ¨Åcation,‚Äù\nMeasurement Science and Technology , vol. 31, no. 9, p. 095401, 2020.\n[38] X. Sun and L. Zheng, ‚ÄúDissecting person re-identiÔ¨Åcation from the\nviewpoint of viewpoint,‚Äù in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2019, pp. 608‚Äì617.\n[39] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y . Yang, ‚ÄúInvariance matters:\nExemplar memory for domain adaptive person re-identiÔ¨Åcation,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 598‚Äì607.\n[40] J. Song, Y . Yang, Y .-Z. Song, T. Xiang, and T. M. Hospedales, ‚ÄúGener-\nalizable person re-identiÔ¨Åcation by domain-invariant mapping network,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 719‚Äì728.\n[41] Y . Fu, Y . Wei, Y . Zhou, H. Shi, G. Huang, X. Wang, Z. Yao, and\nT. Huang, ‚ÄúHorizontal pyramid matching for person re-identiÔ¨Åcation,‚Äù\nin Proceedings of the AAAI conference on artiÔ¨Åcial intelligence , vol. 33,\nno. 01, 2019, pp. 8295‚Äì8302.\n[42] R. Rodrigues and M. Tani, ‚ÄúAre these from the same place? seeing\nthe unseen in cross-view image geo-localization,‚Äù in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vi-\nsion, 2021, pp. 3753‚Äì3761, doi:https://doi.org/10.1109/wacv48630.2021.\n00380.\n[43] W. Luo, Y . Li, R. Urtasun, and R. Zemel, ‚ÄúUnderstanding the effective\nreceptive Ô¨Åeld in deep convolutional neural networks,‚Äù in Proceedings\nof the 30th International Conference on Neural Information Processing\nSystems, 2016, pp. 4905‚Äì4913.\n[44] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al. , ‚ÄúA survey on visual transformer,‚Äù arXiv preprint\narXiv:2012.12556, 2020.\n[45] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n‚ÄúTransformers in vision: A survey,‚Äù arXiv preprint arXiv:2101.01169 ,\n2021.\n[46] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213‚Äì\n229, doi:https://doi.org/10.1007/978-3-030-58452-8 13.\n[47] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr et al. , ‚ÄúRethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 6881‚Äì6890.\n[48] Y . Jiang, S. Chang, and Z. Wang, ‚ÄúTransgan: Two transformers can make\none strong gan,‚Äù arXiv preprint arXiv:2102.07074 , vol. 1, no. 3, 2021.\n[49] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, ‚ÄúLearning texture\ntransformer network for image super-resolution,‚Äù in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 5791‚Äì5800.\n[50] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang,\n‚ÄúTransreid: Transformer-based object re-identiÔ¨Åcation,‚Äù arXiv preprint\narXiv:2102.04378, 2021, doi:https://doi.org/10.13140/RG.2.2.14420.\n53124.\n[51] J. Yu, J. Li, Z. Yu, and Q. Huang, ‚ÄúMultimodal transformer with multi-\nview visual representation for image captioning,‚Äù IEEE transactions on\ncircuits and systems for video technology , vol. 30, no. 12, pp. 4467‚Äì\n4480, 2019, doi:https://doi.org/10.1109/tcsvt.2019.2947482.\n[52] Z. Liu, S. Luo, W. Li, J. Lu, Y . Wu, S. Sun, C. Li, and L. Yang,\n‚ÄúConvtransformer: A convolutional transformer network for video frame\nsynthesis,‚Äù arXiv preprint arXiv:2011.10185 , 2020.\n[53] Z. Peng, W. Huang, S. Gu, L. Xie, Y . Wang, J. Jiao, and Q. Ye,\n‚ÄúConformer: Local features coupling global representations for visual\nrecognition,‚Äù arXiv preprint arXiv:2105.03889 , 2021.\n[54] Y . Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan,\nand Z. Liu, ‚ÄúMobile-former: Bridging mobilenet and transformer,‚Äù\narXiv preprint arXiv:2108.05895 , 2021, doi:https://doi.org/10.1109/\nuemcon47517.2019.8993089.\n[55] Q. Chen, L. Sun, E. Cheung, and A. L. Yuille, ‚ÄúEvery view counts:\nCross-view consistency in 3d object detection with hybrid-cylindrical-\nspherical voxelization,‚Äù Advances in Neural Information Processing\nSystems, 2020.\n[56] W. Yang, Q. Li, W. Liu, Y . Yu, Y . Ma, S. He, and J. Pan, ‚ÄúProjecting\nyour view attentively: Monocular road scene layout estimation via cross-\nview transformation,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2021, pp. 15 536‚Äì15 545,\ndoi:https://doi.org/10.1109/cvpr46437.2021.01528.\n[57] G. van Tulder, Y . Tong, and E. Marchiori, ‚ÄúMulti-view analy-\nsis of unregistered medical images using cross-view transformers,‚Äù\narXiv preprint arXiv:2103.11390 , 2021, doi:https://doi.org/10.1007/\n978-3-030-87199-4 10.\n[58] H. Yang, X. Lu, and Y . Zhu, ‚ÄúCross-view geo-localization with evolving\ntransformer,‚Äù arXiv preprint arXiv:2107.00842 , 2021.\n[59] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDelving deep into rectiÔ¨Åers:\nSurpassing human-level performance on imagenet classiÔ¨Åcation,‚Äù in\nProceedings of the IEEE international conference on computer vision ,\n2015, pp. 1026‚Äì1034.\n[60] F. Radenovi ¬¥c, G. Tolias, and O. Chum, ‚ÄúFine-tuning cnn image retrieval\nwith no human annotation,‚Äù IEEE transactions on pattern analysis and\nmachine intelligence , vol. 41, no. 7, pp. 1655‚Äì1668, 2018, doi:https:\n//doi.org/10.1109/TPAMI.2018.2846566.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7352783679962158
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6539320945739746
    },
    {
      "name": "Segmentation",
      "score": 0.5979055166244507
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5264801979064941
    },
    {
      "name": "Feature extraction",
      "score": 0.483607679605484
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4819088280200958
    },
    {
      "name": "Computer vision",
      "score": 0.47733694314956665
    },
    {
      "name": "Image segmentation",
      "score": 0.4749217927455902
    },
    {
      "name": "Feature matching",
      "score": 0.45462846755981445
    },
    {
      "name": "Transformer",
      "score": 0.4161134362220764
    },
    {
      "name": "Data mining",
      "score": 0.36355000734329224
    },
    {
      "name": "Engineering",
      "score": 0.09700345993041992
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55538621",
      "name": "China Jiliang University",
      "country": "CN"
    }
  ],
  "cited_by": 163
}