{
  "title": "Ranking Warnings of Static Analysis Tools Using Representation Learning",
  "url": "https://openalex.org/W3202241127",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5028539985",
      "name": "Kien-Tuan Ngo",
      "affiliations": [
        "VNU University of Science"
      ]
    },
    {
      "id": "https://openalex.org/A5051359838",
      "name": "Dinh-Truong Do",
      "affiliations": [
        "VNU University of Science"
      ]
    },
    {
      "id": "https://openalex.org/A5011481562",
      "name": "Thu-Trang Nguyen",
      "affiliations": [
        "VNU University of Science"
      ]
    },
    {
      "id": "https://openalex.org/A5084251723",
      "name": "Hieu Dinh Vo",
      "affiliations": [
        "VNU University of Science"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2885030880",
    "https://openalex.org/W3133157805",
    "https://openalex.org/W2955230520",
    "https://openalex.org/W2962960733",
    "https://openalex.org/W2246980366",
    "https://openalex.org/W6730130351",
    "https://openalex.org/W6675786220",
    "https://openalex.org/W2132226975",
    "https://openalex.org/W1972368044",
    "https://openalex.org/W6766301085",
    "https://openalex.org/W2561266335",
    "https://openalex.org/W6688577183",
    "https://openalex.org/W3007214785",
    "https://openalex.org/W2951085418",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2624697062",
    "https://openalex.org/W6774315026",
    "https://openalex.org/W6753259478",
    "https://openalex.org/W6623075773",
    "https://openalex.org/W2951098173",
    "https://openalex.org/W6670394876",
    "https://openalex.org/W6671010391",
    "https://openalex.org/W2130243914",
    "https://openalex.org/W1989657183",
    "https://openalex.org/W3166095789",
    "https://openalex.org/W2092483417",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6770658714",
    "https://openalex.org/W4233410239",
    "https://openalex.org/W2990323480",
    "https://openalex.org/W2972135640",
    "https://openalex.org/W3127736190",
    "https://openalex.org/W814172419",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2082314767",
    "https://openalex.org/W3101228802",
    "https://openalex.org/W2218227085",
    "https://openalex.org/W2884982656",
    "https://openalex.org/W2964781046",
    "https://openalex.org/W2553765452",
    "https://openalex.org/W3008371681",
    "https://openalex.org/W4252675558",
    "https://openalex.org/W2106371080"
  ],
  "abstract": "Static analysis tools are frequently used to detect potential vulnerabilities in software systems. However, an inevitable problem of these tools is their large number of warnings with a high false positive rate, which consumes time and effort for investigating. In this paper, we present DeFP, a novel method for ranking static analysis warnings. Based on the intuition that warnings which have similar contexts tend to have similar labels (true positive or false positive), DeFP is built with two BiLSTM models to capture the patterns associated with the contexts of labeled warnings. After that, for a set of new warnings, DeFP can calculate and rank them according to their likelihoods to be true positives (i.e., actual vulnerabilities). Our experimental results on a dataset of 10 real-world projects show that using DeFP, by investigating only 60% of the warnings, developers can find +90% of actual vulnerabilities. Moreover, DeFP improves the state-of-the-art approach 30% in both Precision and Recall.",
  "full_text": "Ranking Warnings of Static Analysis Tools Using\nRepresentation Learning\nKien-Tuan Ngo, Dinh-Truong Do, Thu-Trang Nguyen, Hieu Dinh V o\nFaculty of Information Technology, VNU University of Engineering and Technology, Vietnam\nEmail:{tuanngokien, 17021090, trang.nguyen, hieuvd }@vnu.edu.vn\nAbstract—Static analysis tools are frequently used to detect po-\ntential vulnerabilities in software systems. However, an inevitable\nproblem of these tools is their large number of warnings with\na high false positive rate, which consumes time and effort for\ninvestigating. In this paper, we present D EFP, a novel method\nfor ranking static analysis warnings. Based on the intuition that\nwarnings which have similar contexts tend to have similar labels\n(true positive or false positive), D EFP is built with two BiLSTM\nmodels to capture the patterns associated with the contexts of\nlabeled warnings. After that, for a set of new warnings, DEFP can\ncalculate and rank them according to their likelihoods to be true\npositives (i.e., actual vulnerabilities). Our experimental results\non a dataset of 10 real-world projects show that using D EFP,\nby investigating only 60% of the warnings, developers can ﬁnd\n+90% of actual vulnerabilities. Moreover, D EFP improves the\nstate-of-the-art approach 30% in both Precision and Recall.\nIndex Terms—static analysis warnings, actual vulnerability,\nfalse positive, warning context, representation learning\nI. INTRODUCTION\nIn order to guarantee the quality of software, many tech-\nniques such as code review, automatic static analysis, and test-\ning, etc. have been applied during the software development\nlife cycle. Especially, static analysis [1], [2] plays an important\nrole in detecting vulnerabilities at the early phases. Without\nexecuting programs, static analysis (SA) tools analyze the\nsource code to identify the violations of the pre-deﬁned rules\nand recommendations. These rules and recommendations are\noften deﬁned by coding standards such as SEI CERT Coding\nRule [3] or MISRA [4].\nHowever, SA tools often report a large number of warnings\n(a.k.a. alarms) [5]. In particular, a warning indicates the state-\nment containing the potential vulnerability, the vulnerability\ntype, and often the additional meta-information [6]. In practice,\ndevelopers have to manually inspect all the reported warnings\nand address them if necessary. However, due to the conser-\nvative over-approximation of program behaviors/properties of\nSA, many warnings are incorrectly reported by tools (i.e., false\npositive warnings). Among the reported warnings,true positive\nwarnings or true positives (TPs) are actual vulnerabilities,\nwhile false positive warnings or false positives (FPs) are the\npositions which indeed do not violate the checking rules.\nInvestigating FPs consumes time and effort but does not bring\nany beneﬁt, therefore, the high FP rate reduces the productivity\nof developers [7], [8] and is bad for the usability of SA\ntools [7], [9]. In consequence, it is necessary to reduce the\nnumber of FPs that developers need to verify.\nIn previous studies, sophisticated techniques such as model\nchecking and symbolic execution have been applied to\neliminate FPs [10]–[12]. Although these approaches obtain\nhigh precision in identifying FPs, they generally face non-\nscalability and time consuming issues because of their state\nspace problems [13].\nWith the growth in size and complexity of the source code,\nrecently machine learning (ML) techniques are leveraged to\nbuild models for discovering patterns associated with TPs/FPs.\nIn general, detecting TPs/FPs among SA warnings can be\nconsidered as a standard binary classiﬁcation problem. To\nbuild a SA waning classiﬁcation model, there are two main\nmethods for extracting features from the warnings, one uses a\npre-deﬁned set of features and the other encodes the features\nby ML models. Then, from the extracted features, the ML\nmodels need to predict whether the SA warning is a TP or FP.\nIn several existing studies [6], [14], [15], the ﬁxed sets\nof the hand-engineered features based on static code metrics\nand warning information have been derived from the source\ncode and then fed to classiﬁers. The effectiveness of these\napproaches depends on the quality of the selected set of fea-\ntures. Moreover, the features in these approaches are manually\ndeﬁned by experts for certain kinds of warnings, therefore it\nis difﬁcult to extend for handling different ones.\nMeanwhile, instead of using a ﬁxed pre-deﬁned set of\nfeatures, Lee et al. [16] proposed a model to learn the lexical\npatterns of the statements around the warnings at the source\ncode token level. They used Word2vec [17] to embed code\ntokens into the vector form and then trained a Convolutional\nNeural Networks (CNN) classiﬁer. Specially, by investigating\ntheir projects, they deﬁne the number of surrounding state-\nments that need to be extracted to reﬂect the contexts of the\nwarnings. These numbers of extracted statements are different\nfor their six proposed checkers. However, it is challenging\nto apply this approach for different projects and/or different\nkinds of warnings since it requires expert knowledge and\ncarefully manual investigation to decide how many statements\nin each function are enough to capture the contexts of the\nchecking warnings. Moreover, not every statement surrounding\na warning is all related to its violation and equally important\nfor TP/FP detection. Also, warning unrelated statements would\nnegatively affect the performance of the ML models.\nIn this paper, we propose DEFP, a novel method to prioritize\nSA warnings. Instead of classifying SA warnings, D EFP\npredicts their likelihoods to be TPs and then ranks them with\n1\narXiv:2110.03296v1  [cs.SE]  7 Oct 2021\nthe top entries are more likely to be TPs (actual vulnerabil-\nities), and the last entries are more likely to be FPs. Indeed,\nranking SA warnings rather than classifying them gives us\nthree following beneﬁts.\nFirst, for developers, DEFP helps shorten the cycle of devel-\noping and releasing products, especially for critical systems.\nThe reason is that critical systems are highly required to be\nsafe, secure, and reliable. Therefore, any potential vulnerabili-\nties (warnings) are all needed to be addressed. In other words,\nif a warning is eliminated due to being falsely classiﬁed as\nan FP, it would cause the system to be dangerously explored\nin the future. Instead of directly eliminating any warnings\nclassiﬁed as FP, D EFP ranks SA warnings according to their\nvulnerable likelihoods. With the pressure to release the high-\nquality software on time, focusing on the top-ranked warnings\nﬁrst helps developers to ﬁnd more actual vulnerabilities in a\nﬁxed duration. Then, they can spend time and effort on the\nlow-ranked warnings later.\nSecond, for SA tool builders, DEFP suggests case studies\nthat they can examine to improve the quality of their tools.\nIndeed, to better serve the market, SA tool builders need to\nfrequently improve their tools by not only increasing the TP\nrate, but also decreasing the FP rate. Among a huge number\nof reported warnings in various projects, D EFP suggests an\neffective order for investigation. Specially, SA tool builders\ncan directly concentrate on addressing warnings which are\nmore likely to be FPs, i.e., warnings at the last of the resulting\nlists, to ﬁnd the patterns which tends to be incorrectly reported.\nThird, for researchers, to build the datasets of real-world\nwarnings, DEFP helps the data collection process be more ef-\nﬁcient. In practice, in this ﬁeld, it still lacks public datasets for\nevaluating approaches and researchers often have to manually\ninvestigate to label warnings. This process is extremely time-\nconsuming. From the ranked lists of D EFP, researchers can\neffectively collect warnings by selectively labeling top-ranked\nand last-ranked warnings.\nOur key idea is based on the intuition that warnings which\nhave similar contexts tend to have similar labels (TP or FP).\nFor each warning, D EFP exploits both syntax and semantics\nfrom the context of the warning and then determines its\nlikelihood to be TP. In order to capture the context of a\nwarning, D EFP extracts all of the statements in the program\nwhich impact and are impacted by the statement containing\nthe warning (the reported statement). After that, to better rep-\nresent the general patterns of warnings, identiﬁers and literals,\nwhich are speciﬁc for functions/ﬁles/projects and could make\nthe ML models be biased by the training source code, are\nreplaced by abstract names. Next, the reported statements\nand their contexts are vectorized and used to train neural\nnetwork models. One of the models represents the information\nspeciﬁcally included in the reported statements, while the other\nextracts critical information in the warning contexts. Then,\nthe high-level features encoded from these models are utilized\nto estimate the likelihoods to be TPs of the corresponding\nwarnings. Lastly, SA warnings are ranked according to their\npredicted scores.\n1. static const char *aoc_rate_type_str(enum \nast_aoc_s_rate_type value){\n2.   const char *str;\n3.   switch (value){\n4.     default:\n5.       case AST_AOC_RATE_TYPE_DURATION:\n6.         str = \"Duration\";\n7.  break;\n8.    //...\n9.   }\n10.   return str;\n11. }\n12.  \n13. static void aoc_s_event(const struct  \nast_aoc_decoded *decoded, struct ast_str **msg){\n14.   const char *rate_str;\n15.   char prefix[32]; \n16.   int i;\n17.   ast_str_append(msg, 0, \"N:%d\", decoded->aoc_s_count);\n18.   for (i = 0; i < decoded->aoc_s_count; ++i){\n19.     snprintf(prefix, sizeof(prefix), \"R(%d)\", i);\n20.     rate_str = aoc_rate_type_str(  \n             decoded->aoc_s_entries[i].rate_type);\n21.     switch (decoded->aoc_s_entries[i].rate_type){\n22.       case AST_AOC_RATE_TYPE_DURATION:\n23.         strcat(prefix, \"/\");\n24.         strcat(prefix, rate_str); // *Buffer Overflow\n25.  //...\n26.  break;\n27.  //...\n28.       default:\n29.  break;\n30.     }\n31.   }\n32. }\nFig. 1: An FP reported by Flawﬁnder at line 24\nTo the best of our knowledge, it still lacks a public real-\nworld dataset for widely evaluating approaches post-handling\nSA warnings. In existing studies [6], [14], [15], ML models\nare often trained and tested on synthetic datasets, such as\nJuliet [18] and SARD [19]. However, Chakraborty et al. [20]\nhas demonstrated that these datasets are quite simple for\nestimating the performance of ML models on real-world data.\nThus, to address the limitation of data shortage, we propose a\ndataset containing 6,620 warnings of 10 real-world projects.\nOur experiments show that about 60% of actual vulnerabil-\nities are ranked by D EFP in Top-20% of warnings. Moreover,\n+90% of actual vulnerabilities can be found by investigating\nonly 60% of the total warnings. Meanwhile, by using the state-\nof-the-art approach [16], with the same number of examined\nwarnings, developers can ﬁnd only 46% and 82% TPs.\nIn summary, our contributions in this paper are:\n• A novel approach to rank SA warnings, which does\nnot require feature engineering and could be ﬂexible to\nextend for different kinds of warnings.\n• A public dataset of 6,620 warnings collected from 10\nreal-world projects, which can be used as a benchmark\nfor evaluating related work.\n• An extensive experimental evaluation showing the perfor-\nmance of D EFP over the state-of-the-art approach [16].\n2\nII. M OTIVATING EXAMPLE AND GUIDING PRINCIPLES\nA. Motivating Example\nFig. 1 shows a simpliﬁed version of function\naoc_s_event in project Asterisk1, the complete\nversion can be found on our website [21]. In this example, a\nwarning related to Buffer Overﬂow (BO) is reported at line\n24 by Flawﬁnder [22], a static analysis tool. The reason is\nthat strcat appends the string pointed by rate_str to\nthe end of prefix. This could cause the size of the resulting\nstring stored in prefix to be greater than 32 which is the\nprefix’s size of allocated memory (line 15).\nHowever, via inter-procedure analysis, we can conclude that\nthis warning is an FP. Specially, at line 19 in Fig. 1, prefix\nis set to be R(i) where i is the index of the loop, and the\nmaximum size of prefix after this statement is 13, in the\ncase of i = INT MAX (i.e., 2,147,483,647). At line 23,\nprefix is appended a character (i.e., \"/\"), and then at line\n24 it is appended a string pointed by rate_str, which has\nlength 8 (rate_str = \"Duration\", line 20 and line 6).\nAs a result, after line 24, the maximum length of prefix\nis 22, which is still much smaller than 32. Therefore, in\norder to determine whether a warning is a TP or FP, it need\nto conduct not only intra-procedure analysis but also inter-\nprocedure analysis. In other words, simply approximating the\ncontext of a warning by its surrounding statements or by its\ncontaining function could be ineffective.\nB. Guiding Principles\nIn order to determine whether a SA warning is a TP or FP,\nanalyzing only the reported statement is not enough. It requires\ninvestigating the context of the warning as well. For example,\nto conclude the warning at line 24 in Fig. 1 is an FP, not only\nthat statement but also the related statements such as lines\n18, 19, 20, etc. need to be examined. Therefore, to build an\nML model which can effectively predict the likelihoods to be\nTP/FP of the warnings, for each warning, we need to extract\nits appropriate context in the program. From the extracted\ncontexts, the model can capture patterns associated with the\nwarnings. Also, statements unrelated to the warning, which\nmight cause noises and negatively affect the performance of\nthe model, should be excluded from the warning context. In\nthis paper, we propose the following principles for the problem\nof ranking SA warnings by representation learning.\nP1. The warning contexts can be semantically captured\nby the statements in the program which can impact and be\nimpacted by the reported statements . In practice, to determine\nwhether a warning indicates an actual violation of a speciﬁc\nvulnerability type or not, it is necessary to investigate all of the\nfeasible execution paths containing the reported statement. In\nother words, we need to examine the control ﬂows and data\nﬂows of the program which contain the reported statement.\nBesides, functions in a program do not work independently,\nthey often execute with the invocations of the others. Thus,\ninter-procedural analysis is essential to effectively capture the\n1https://github.com/asterisk/asterisk\ncontexts of the warnings. Simply, all the statements in the\nfunctions/programs can be considered as the warning contexts,\nhowever, this method can cause unnecessarily large contexts\nand negatively affect the ML model’s predictive performance.\nAlso, not all of the statements in the functions/programs are\nactually relevant to the warnings. For example, the statement\nat line 17 in Fig. 1 does not affect the decision about TP\nor FP of the warning at line 24. Including such irrelevant\nstatements may cause the ML model to falsely learn the actual\npatterns associated with TPs/FPs. Therefore, inter-procedural\nslicing techniques [23] can be applied to effectively extract\nthe warning contexts by statements semantically related to the\nreported statements and eliminate the irrelevant statements.\nP2. The reported statements should be highlighted com-\npared to the other statements in the warning contexts. In-\ntuitively, not all the statements in the program slices are\nequally important regarding the considering warnings. The\nreported statements are where the vulnerabilities are poten-\ntially explored, thus, they should be highlighted compared to\nthe other statements in their contexts. For example in Fig. 1,\nthe statements at lines 19, 23, and 24, which all modify\nthe value of prefix, are all necessary for investigating the\nwarning. However, according to the report of Flawﬁnder, the\nBO vulnerability is potentially explored at line 24, which\nappends prefix by an unknown size string, i.e., a string is\nreturned by another function. Intuitively, the statement at line\n24 should be emphasized compared to statements at lines 19\nand 23. Moreover, in practice, a program slice could contain\nseveral warnings, therefore to distinguish warnings, not only\nthe program slices (i.e. context of the warnings) but also the\nreported statements need to be featurized.\nP3. Identiﬁers should be abstracted. The reason is that iden-\ntiﬁers such as variables, function names, constants are project-\nspeciﬁc (or even ﬁle-speciﬁc/function-speciﬁc) and consider-\nably vary regarding developers’ coding style. By learning such\nspeciﬁc information, the ML model could not capture general\npatterns of the warnings. Also, this could make the models\nsimply learn the connections between speciﬁc identiﬁers and\nwarning labels (TP/FP). Consequently, the models accurately\npredict the warnings of several training programs but their\nperformance might decrease dramatically on the different\nprograms. Therefore, to build a general ML model which can\nwork well and stably across programs, the identiﬁers should be\nabstracted into symbolic names, for example, VAR1, FUNC1,\netc. Moreover, without abstraction, the number of identiﬁers\ncould be virtually inﬁnite, so ML model could have to deal\nwith the vocabulary explosion problem.\nIII. S TATIC ANALYSIS WARNING RANKING WITH\nREPRESENTATION LEARNING\nFig. 2 illustrates our SA warning ranking approach. Par-\nticularly, from the source code and the set of warnings of the\nanalyzed program, we extract the reported statements and their\nprogram slices associated with warnings. For each warning, the\nreported statement and the corresponding program slice are\nconverted into vectors and then fed to the BiLSTM models to\n3\nSource Code  \n& Warnings\nWarnings'  \nPredicted Scores \nWarning  \nRanked List \nVectorize Each Input Train Representation Model Rank SAT Warnings\nReported Statement\nProgram Slice\nLSTM LSTM LSTM \nLSTM LSTM LSTM \n1 2 3\nFig. 2: Our proposed approach for ranking SA warnings\npredict its likelihood to be TP. After that, all of the warnings\nof the program are ranked according to their predicted scores.\nA. Program Slice Extraction\nIn this work, to capture the context of each warning, we\nextract all the statements in the program which impact and are\nimpacted by the corresponding reported statement. Specially,\nstarting from the reported statement, we employ Joern [24] to\nconduct both backward and forward inter-procedural slicing\nin the program. For instance, the context of the warning at\nline 24 in Fig. 1 is captured by the program slice shown in\nFig. 3. Therefore, by this approach, not only a large number\nof irrelevant statements in the program are removed, but also\nthe warning contexts are precisely captured via control/data\ndependencies relationship throughout the program.\nB. Input Vectorization\nProgram slices and reported statements are lexical source\ncode tokens. Meanwhile, neural network models require their\ninputs to be formalized as numeric vectors. Therefore, we need\nto represent the model input data by a suitable data structure.\nIn this section, we show three steps to represent the program\nslice and the reported statement of each warning: identiﬁer\nabstraction, tokenization, and vectorization.\n1) Identiﬁer Abstraction: In general, programs often con-\ntain a huge number of identiﬁers, also their naming con-\nventions and styles are diverse. It could cause difﬁculties\nfor the ML models to capture the general patterns of the\nwarnings [20]. Besides, ML models would simply learn char-\nacteristics of identiﬁers in certain projects, as well as simply\nmap speciﬁc identiﬁers with corresponding warning labels. In\norder to avoid this problem, D EFP abstracts all the identiﬁers\nbefore feeding them to the models. In particular, variables,\nfunction names, and constants in the extracted program slices\nare replaced by common symbolic names. For example, func-\ntion name aoc_s_event is replaced by FUNC1, the array\nprefix is replaced by VAR5, and its allocated size 32\nis replaced by NUMBER_LIT. The details of our rules for\nabstracting identiﬁers can be found on our website [21].\n2) Tokenization: To represent in numeric vectors for feed-\ning to the neural network models, both the extracted pro-\ngram slices and the reported statements are tokenized into\nsequences of tokens. In this work, we use lexical analy-\nsis to break down each code statement into code tokens,\nwhich including identiﬁers, keywords, punctuation marks,\nand operators. For instance, the statement at line 24 in\nFig. 1, strcat(prefix, rate_str);, is abstracted\n1. static const char *aoc_rate_type_str(enum \nast_aoc_s_rate_type value){\n2. switch (value){\n3. str = \"Duration\";\n4. return str;\n5. static void aoc_s_event(const struct \nast_aoc_decoded *decoded, struct ast_str **msg){ \n6. for (i = 0; i < decoded->aoc_s_count; ++i){\n7. snprintf(prefix, sizeof(prefix), \"R(%d)\", i);\n8. rate_str = aoc_rate_type_str(\n         decoded->aoc_s_entries[i].rate_type);\n9. switch (decoded->aoc_s_entries[i].rate_type){\n10. strcat(prefix, \"/\");\n11. strcat(prefix, rate_str);\n24\n1\n13\n21\n6 18\n23\n3\n1910\n20\nControl dependence\nData dependence\n 1.\n  \n 3.\n 6.\n10.\n13.\n18.\n19. \n20.\n \n21.\n23.\n24.\nFig. 3: The program slice of the warning at line 24 in Fig. 1\nas strcat(VAR8, VAR11); and then it is tokenized\ninto seven separated tokens: “ strcat”, “(”, “VAR8”, “,”,\n“VAR11”, “)” and “;”.\n3) Padding and Truncation: In practice, the number of\nsequence tokens in different slices could be signiﬁcantly\ndifferent. For example, in our experiment, the sequence lengths\ncan vary from 5 to 9,566 code tokens. Therefore, to ensure that\nall the sequences have the same length, L, before inputting to\nthe neural network, we use padding and truncation techniques.\nTo achieve the best performance, the ﬁxed lengthL is carefully\nselected via multiple experiments.\nParticularly, for sequences having lengths smaller than L,\nwe add one or more special tokens ( <pad>) at the end of\nthese sequences. For the sequences whose lengths are greater\nthan L, we truncate them to ﬁt the ﬁxed length.\nIn practice, the positions of the reported statements in their\ncorresponding slices signiﬁcantly vary. They can appear at the\nbeginning of the slices or at the end of the slices. Thus, trun-\ncating from either the beginning or the end of the sequences\ncould lead to the cases that the statements containing warnings\nare missed in the truncated sequences. Therefore, in order\nto guarantee that the truncated sequences always contain the\nreported statements, we take these statements as the center for\ntruncating. Specially, from the reported statements, we extend\nto both sides of the sequences. until reach the ﬁxed length\nL. Importantly, to capture the correct semantics of each code\nstatement, we ensure that a statement will be fully included in\nthe truncated sequences. It means that when only some tokens\nof a statement are included in the truncated sequences and the\nremaining is left due to the length limitation, we will replace\nall of the tokens of that statement in the truncated sequence\nby (<pad>) token.\n4) Vectorization: In this step, the token sequences are em-\nbedded into numeric ﬁxed-length vectors. In practice, besides\nthe structural information of the sequences which need to be\n4\n        ... \n  6:   str = \"Duration\"; \n10:   return str; \n        ... \n23:   strcat(preﬁx, \"/\");  \n24:   strcat(preﬁx, rate_str); \n \n ... \n... \n... ... \nTokenize  \nProgram Slice\n... \nVAR3\n=\n...  \nVAR11\n)\n;\n... \nWord2vec Embedding\n.2, .6, .7, .9, .3, .7, ...\n.1, .6, .1, .1, .9, .9, ...\n.4, .2, .7, .8, .1, .3, ...\n.3, .1, .6, .2, .2, .7, ...\nLSTMf\nLSTMf\nLSTMf\nLSTMf\nLSTMb\nLSTMb\nLSTMb\nLSTMb\nBiLSTMctx Layer Global Max Pooling Layer\n... \nWord2vec Embedding\n.3, .5, .7, .9, .7, .5, ...\n.5, .2, .1, .4, .3, .9, ...\n.4, .2, .7, .8, .1, .3, ...\n.3, .1, .6, .2, .2, .7, ...\nLSTMf\nLSTMf\nLSTMf\nLSTMf\nLSTMb\nLSTMb\nLSTMb\nLSTMb\nBiLSTMrs Layer Global Max Pooling Layer\nConcatenating  \nLayer\nDense  \nLayer 1\nSoftmax \nLayer \nTokenize  \nReported Statement\nemb1\nemb2\nembn-1\nembn\nemb1\nemb2\nembm-1\nembm\nProgram Slice\nDense  \nLayer 2\nstrcat\n(\nVAR8\n,  \nVAR11\n)\n;\nFig. 4: The proposed LSTM-based representation learning model for ranking SA warnings\nencoded, relationships between the tokens are also important.\nThe reason is that code tokens have to appear together in a\ncertain order to make the program grammatically and syntac-\ntically correct [25]. For this purpose, in the vectorization step,\nwe use Word2vec model [17] with Continuous Bag-of-Words\n(CBOW) architecture.\nC. Representation Learning and Warning Ranking\nThe D EFP’s model architecture is shown in Fig. 4. Espe-\ncially, to effectively learn contextual information that is crucial\nfor revealing TP/FP code patterns, two Bidirectional Long\nShort-Term Memory networks (BiLSTM) [26] are employed\nto train on the embedding vectors of the program slices and\nthe reported statements. Afterwards, D EFP extracts the mean-\ningful characteristics of related warnings by concatenating\nthe outputs from these BiLSTM models and feed into Fully\nConnected layers. In particular, we consider the ﬁnal layer’s\noutput of the model as the likelihood to be TP/FP of each\ninput warning. All these scores are ﬁnally gathered by D EFP\nand ranked accordingly.\n1) Representation Learning: In this work, the warning\ncontexts (i.e. program slices) and the reported statements\nare encoded by two BiLSTM networks. In the case of pro-\ngram slices containing multiple statements distributed across\nfunctions, LSTM architecture might essentially apprehend the\nrelationships between code tokens. Additionally, by utilizing a\ngated mechanism, LSTM can handle long-term dependencies\nand also focus on the most signiﬁcant parts of the sequences.\nHowever, the information in LSTM is expressed one way\nthrough continuous time steps in sequential order. Meanwhile,\nthe occurrence of a code token is usually related to either the\nprevious or the subsequent tokens, or even both. Thus, addi-\ntionally applying the bidirectional implementation of LSTM\ncan assist the model to build dependencies in both forward\nand backward directions, which efﬁciently captures the general\npattern of warnings.\nDEFP also employs the Global Max Pooling (GMP) layer\nto accumulate the output of each BiLSTM network. Especially,\nGMP layer computes maximum values over LSTM’s time\nsteps, which help to reduce output dimension and only keep\nthe most important elements from LSTM cells. As a result,\nDEFP has two GMP layers following each BiLSTM network,\nand then they are concatenated into a uniﬁed one to represent\na whole feature map (Fig. 4).\n2) Warning Ranking: After obtaining the learned repre-\nsentations of the warnings, D EFP distinguishes their patterns\nby feeding them into three Fully Connected (Dense) layers\nbehind. Particularly, the ﬁnal layer has only two hidden\nunits activated by the Softmax function, which produces two\nscores whose total is 1.0. These two values correspond to the\nlikelihoods of each warning to be TP and FP, respectively.\nIn the training phase, D EFP’s neural network enhances its\npredictions by ﬁnding the best hidden weights through esti-\nmating its errors. In other words, an objective function, cross-\nentropy, is applied to calculate the model’s loss and update the\nweights towards minimizing this error value. Consequently, in\nthe case of a TP warning, the model tends to converge its TP\nscore towards 1.0 and its FP score closes to 0.0, and vice versa\nfor an FP warning. In the ranking phase, by inputting a list of\nwarnings, D EFP directly calculates their TP scores and sorts\nthem in descending order.\nIV. E MPIRICAL METHODOLOGY\nIn order to evaluate D EFP, we seek to answer the following\nresearch questions:\n• RQ1: How accurate is D EFP in ranking SA warn-\nings? and how is it compared to the state-of-the-art\napproach [16]?\n• RQ2: How does the extracted warning context affect\nDEFP’s performance? (P1)\n• RQ3: How does the highlighting reported statement\nimpact the performance of D EFP? (P2)\n• RQ4: How does the identiﬁer abstraction component\nimpact the performance of D EFP? (P3)\nA. Dataset\nIn order to train and evaluate an ML model ranking SA\nwarnings, we need a set of warnings labeled to be TPs or FPs.\nCurrently, most of the approaches are trained and evaluated by\nsynthetic datasets such as Juliet [18] and SARD [19]. However,\n5\nTABLE I: Overview of D EFP’s dataset\nNo. Project Buffer Overﬂow Null Pointer\nDereference\n#W #TP #FP #W #TP #FP\n1 Asterisk 2049 63 1986 133 0 133\n2 FFmpeg 1139 387 752 105 37 68\n3 Qemu 882 396 486 72 39 33\n4 OpenSSL 595 53 542 9 2 7\n5 Xen 388 15 373 23 6 17\n6 VLC 288 20 268 16 2 14\n7 Httpd 250 45 205 17 0 17\n8 Pidgin 250 13 237 242 0 242\n9 LibPNG 83 9 74 2 0 2\n10 LibTIFF 74 9 65 3 3 0\n# Total 5998 1010 4988 622 89 533\n#W, #TP and #FP are total warnings, true positives and false positives.\nthey only contain simple examples which are artiﬁcially cre-\nated from known vulnerable patterns. Thus, the patterns which\nthe ML models capture from these datasets could not reﬂect\nthe real-world scenarios [20]. To evaluate our solution and the\nothers on real-world data, we construct a dataset containing\n6,620 warnings in 10 open-source projects [27], [28]. Table I\nshows the overview of our dataset.\nIn these 10 real-world projects, functions are previously\nmanually labeled as vulnerable and non-vulnerable [27], [28].\nThen, our dataset is constructed by the following steps:\n1) Collecting warnings : We pass the studied projects\nthrough three open-source SA tools, Flawﬁnder [22],\nCppCheck [29], RATS [30] to collect a set of warnings.\nIn practice, this set contains warnings in multiple kinds\nof vulnerabilities. However, we only collect the warn-\nings related to Buffer Overﬂow (BO) and Null Pointer\nDereference (NPD) since for the other kinds, the number\nof reported warnings are too small for training and\nevaluating an ML model.\n2) Labeling warnings in the non-vulnerable functions :\nSince, these functions are already marked as clean\nregarding BO and/or NPD, thus all the corresponding\nwarnings in these functions are annotated as FPs.\n3) Labeling warnings in the vulnerable functions: Although\nthese functions are marked containing BO and/or NPD\nvulnerabilities, we do not know exactly how many\nvulnerabilities each function contains and the positions\nof the vulnerabilities in the source code. Therefore, for\neach of the warnings in these functions, we manually\ninvestigate to label whether it is a TP or FP.\nB. Evaluation Setup, Procedure, and Metrics\n1) Experimental Setup: We implemented neural network\nmodels using Keras together with TensorFlow backend (ver-\nsion 2.5.0). The tokenizer was built upon NLTK library\n(version 3.6.2) and Word2vec embedding model was provided\nby the gensim package (version 3.6.0). All experiments were\ncomputed by a server running Ubuntu 18.04 with an NVIDIA\nTesla P100 GPU.\nWe adopt cross-validation to train several neural networks\nand select the best parameter values corresponding to the\neffectiveness of predicting likelihoods to be TP warnings in\nthe proposed dataset. Specially, for D EFP, embedding size\nare set to 96, the maximum length of each slice and reported\nstatement is ﬁxed to 600 and 40, respectively, and they are\nlearned by two BiLSTM networks which each has 256 hidden\nnodes. During the training phase, the dropout, batch size, and\nnumber of epochs is set to 0.1, 64 and 60, respectively. Also,\nthe minibatch stochastic gradient descent ADAMAX optimizer\nis selected with the learning rate of 0.002.\nBesides, the data is sampled into stratiﬁed 5 folds, while 4\nfolds are picked for training and 1 remaining fold for testing\n(ratio of 8:2). We then run 5 different experiments on 5 pairs\nof training and testing data and aggregate average results for\nthe ﬁnal assessment of the corresponding experiment.\n2) Empirical Procedure:\nRQ1. We compare the performance of D EFP and the CNN\nmodel proposed by Lee et al. [16] for ranking warnings in the\nproposed dataset.\nRQ2. We study the impact of the warning contexts on the\nperformance of DEFP. Specially, we compare the performance\nof DEFP in four scenarios of the warning contexts: (1) the raw\ncode of the program, (2) the program slices on control depen-\ndencies, (3) the program slices on data dependencies, and (4)\nthe program slices on both control and data dependencies.\nRQ3. We study the impact of highlighting the reported\nstatements on D EFP’s performance. We compare the ranking\nresults of DEFP in two scenarios when the reported statements\nare and are not encoded for training the BiLSTM model.\nRQ4. We study the impact of the identiﬁer abstraction\ncomponent by comparing the performance of D EFP when the\ninputs are embedded with and without this component.\nFor evaluation, we have two experimental settings as widely\nadopted in related studies [27], [28], [31]: within-project\nsetting and combined-project setting. First, in within-project\nsetting, warnings from the same project are split into training\nand testing sets. Second, in combined-project setting , we\ncollect the warnings from all 10 projects and then split them\ninto training and testing sets. In practice, in several projects,\nthe number of warnings is quite small for training and testing\nan ML model, and it could cause overﬁtting or underﬁtting\nproblems. Thus, we only select three projects which have the\nlargest number of BO warnings for evaluating RQ1 in the\ncorresponding vulnerability type in the within-project setting.\nRQ1 in the NPD vulnerability and the other research questions\nare only evaluated in the combined-project setting.\n3) Evaluation Metrics: In order to evaluate D EFP and\ncompare its performance with the state-of-the-art approach, we\napplied Top-k% Precision (P@K) and Top-k% Recall (R@K).\nThese two metrics are widely used in related studies [28],\n[32], especially when the dataset is severely imbalanced. In\nthis paper, P@K denotes the proportion of actual TP warnings\nin the Top- k% of warnings ranked by the model, and R@K\nrefers to the proportion of correctly predicted TP warnings in\nTop-k% among the total actual TPs warnings. In particular,\n6\nTABLE II: Performance of D EFP and CNN model proposed by Lee et al. [16] in ranking SA warnings\nWN Project Method\n# TP warnings found in top-k% warnings\nTop-5% Top-10% Top-20% Top-50% Top-60%\nPrecision Recall Precision Recall Precision Recall Precision Recall Precision Recall\nBO\nQemu CNN 71.11% 8.09% 53.33% 12.13% 46.86% 20.72% 44.32% 49.25% 43.02% 57.57%\nDeFP 82.22% 9.34% 67.78% 15.40% 65.14% 28.78% 52.27% 58.08% 50.38% 67.43%\nFFmpeg CNN 30.91% 4.40% 31.30% 9.30% 33.24% 19.64% 32.46% 47.80% 33.04% 58.39%\nDeFP 67.27% 9.56% 61.74% 18.34% 52.43% 31.00% 38.95% 57.37% 37.72% 66.66%\nAsterisk CNN 11.00% 17.56% 8.78% 28.59% 7.56% 49.36% 4.49% 72.95% 3.82% 74.49%\nDeFP 34.00% 53.97% 18.54% 60.26% 10.73% 70.00% 5.18% 84.10% 4.56% 88.97%\nCOMBINED CNN 43.00% 12.77% 39.67% 23.56% 34.25% 40.69% 25.40% 75.45% 23.46% 83.56%\nDeFP 66.00% 19.60% 56.00% 33.27% 43.92% 52.18% 27.50% 81.68% 24.82% 88.42%\nNPD COMBINED CNN 63.33% 21.37% 43.33% 29.15% 38.40% 53.99% 21.29% 74.25% 19.62% 82.09%\nDeFP 80.00% 26.93% 65.00% 43.66% 47.20% 66.14% 25.81% 89.74% 22.58% 94.25%\nP@K and R@K are calculated using the following formulas,\nwhere {Actual TPs} is the set of actual TP warnings,\n{Predicted TPs }@K is the list of Top- k% of warnings\nranked ﬁrst by the model.\nP@K = |{Actual TPs} ∩ {Predicted TPs}@K|\n|{Predicted TPs}@K|\nR@K = |{Actual TPs} ∩ {Predicted TPs}@K|\n|{Actual TPs}|\nV. E XPERIMENTAL RESULTS\nA. Performance Comparison (RQ1)\nTable II shows the performance of D EFP and the CNN\nmodel proposed by Lee et al. [16] in Top-5%–Top-60% warn-\nings of the ranked lists. Overall, D EFP improves their model\nby nearly 30% in Precision and Recall for both BO and NPD\nwarnings. For example, in FFmpeg, Qemu, and Asterisk with\nthe Top-20% of warnings returned by D EFP, developers can\nﬁnd 23/79, 24/77, and 9/13 actual vulnerabilities. Meanwhile,\nby using the CNN model [16], the corresponding ﬁgures are\nonly 16/79, 15/77, and 6/13, respectively. When the warnings\nof all the projects are combined, by investing 20% of the\nwarnings in the top of the ranked list, 105/202 actual BO\nvulnerabilities and 12/18 actual NPD vulnerabilities can be\nfound by D EFP, while these ﬁgures for the CNN model\nare only 82/202 and 10/18, respectively. Interestingly, with\nthe results of D EFP, developers can ﬁnd +90% of actual\nvulnerabilities by investigating only 60% of the total warnings,\nwhich is 8% better than the CNN model.\nIndeed, DEFP obtains better results because it concentrates\non statements which semantically describe the contexts of\nthe warnings and DEFP is not negatively affected by the\nstatements which are unrelated to the warnings . For example,\nthe warning in Fig. 1, D EFP captures its context by the\nstatements which impact and are impacted by the reported\nstatement at line 24 as shown in Fig 3. These statements are\nessential for semantically capturing the context of the warning\nbecause they show when and how the value ofprefix, which\nis reported to potentially overﬂow, is changed. Additionally,\nunlike the CNN model [16], D EFP ignores statements at\nlines 17 and 26, which do not play any role in reﬂecting the\nviolation of the reported statement, although they are near it.\nTherefore, they could cause noises if they are encoded as the\ncontext of the warning.\nIn addition, by inter-procedural analysis, D EFP does not\nmiss important information to validate the violation of the\nreported statements. Specially, there are statements, which are\nessential for validating the warnings, could be in multiple\nfunctions. For example in Fig 1, the statement at line 6\nspecifying the concatenated string to prefix is extremely\nimportant to determine whether the BO vulnerability could\noccur at line 24 or not. However, this statement is not in\nthe same function with the reported statement, yet in another\nfunction aoc_rate_type_str. This statement is captured\nby D EFP, but it will be missed if only intra-procedural\nanalysis is considered.\nInterestingly, among the studied projects, D EFP achieved\nthe highest results in Qemu and the lowest results in Asterisk.\nSpecially, for Top-20% warnings of Qemu, D EFP obtained\n65.14% in Precision, whereas this ﬁgure of Asterisk is only\n10.73%. The reason is that the models are impacted by the\nimbalance of the dataset. For instance, the numbers of TPs\nand FPs in Qemu are quite balanced, while they are greatly\nimbalanced in Asterisk. Moreover, Asterisk only contains 63\nTPs, which is extremely small compared to its 1986 FPs.\nB. Impact of the Warning Context (RQ2)\nFig. 5 shows the performance of DEFP when the contexts of\nthe warnings are captured by different kinds of dependencies.\nDEFP obtains the best performance when the warning contexts\nare captured by both control and data dependencies on the\nPDG. The reason is that, by using slicing techniques on both\nof these dependencies, unrelated statements are removed from\nthe warnings’ contexts and only related statements are encoded\nand fed to the BiLSTM models. Therefore, the models can\nbetter capture the patterns of warnings without being affected\nby noises caused by the statements which are semantically\nunrelated to the warnings. In particular, when program slices\nare conducted on both control and data dependencies, the\nperformance of D EFP in two studied vulnerability types is\n16% and 19% better than when the warning contexts captured\n7\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nRAW CD DDG PDG\nBuffer Overflow Warnings\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nI I DD CD & DD\nNull Pointer Dereference Warnings\nFig. 5: Impact of the extracted warning contexts on D EFP’s perfor-\nmance. RAW, CD, DD, and CD && DD denote the warning contexts\nwhich are captured by raw source code, program slices on control\ndependencies, program slices on data dependencies, and program\nslices on both control and data dependencies, respectively.\nby the raw code of the containing functions. Interestingly, by\nslicing on both control and data dependencies,the performance\nof DEFP is signiﬁcantly improved in the warnings which\nare ranked at the top of the lists. Specially, compare to the\nraw code, for the Top-1%–Top-50% of the ranked warnings,\nDEFP’s performance with this kind of program slices is\nimproved 42% for NPD warnings and 34% for BO warnings.\nIn other words, among Top-20% of warnings (243 warnings\nfor BO and 25 warnings for NPD) in the resulting lists,\nDEFP correctly ranks 105/202 and 12/18 actual BO and\nNPD vulnerabilities. Meanwhile, when the warning context\nis captured by raw code of the whole functions, these ﬁgures\nare only 79/202 and 9/18, respectively.\nImportantly, for both BO and NPD vulnerabilities, program\nslices on only data dependencies capture the warning contexts\nbetter than the raw code of functions, however, the program\nslices on only control dependencies do worse. The reason is\nthat for these two kinds of vulnerabilities, the information\nabout data dependencies, which illustrates how the values of\nthe variables are propagated, is more informative for reasoning\nthe warnings. For example in Fig. 1, to determine whether the\nwarning (line 24) is an FP, it is essential to analyze the state-\nments which have data-dependent on, such as lines 6, 15, 19,\netc. Although the raw code may contain noises and unrelated\nstatements, it still contains all of this information. However,\nthis important information is missed in the program slices\non control dependencies only. Therefore, the performance of\nDEFP with raw code is worse than the program slices on data\ndependencies, yet better than the program slices on control\ndependencies. Speciﬁcally for Top-1%–Top-60% of warnings,\ncompared to the raw code, D EFP’s results with program slices\non the data dependencies is 7% and 29% better for BO and\nNPD. Also, compared to the program slices on the control\ndependencies, these ﬁgures are 8% and 46%, respectively.\nIn practice, for different kinds of vulnerabilities, it could\nrequire control or data dependencies or both of these two\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nWith Reported Statement\nCD\nBuffer Overflow Warnings\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nI Without Reported Statement\nNull Pointer Dereference Warnings\nFig. 6: Impact of highlighting the reported statements on the perfor-\nmance of D EFP\nkinds of information for validating the warnings. Therefore,\nto guarantee the best performance of D EFP, program slices\non both control and data dependencies should be leveraged to\ncapture the warning contexts.\nC. Impact of the Reported Statements (RQ3)\nAs seen in Fig 6, the performance of DEFP is slightly\nimproved by 4% and 7% for BO and NPD vulnerabilities\nwhen the reported statements are highlighted by being encoded\nas an input of the BiLSTM model . For example, in Top-\n20% of ranked warnings, by encoding the reported statements,\ndevelopers can ﬁnd 7 more actual BO vulnerabilities and 1\nmore actual NPD vulnerabilities. More details about the per-\nformance of DEFP on P@K can be found on our website [21].\nIndeed, highlighting the reported statements can help the\nneural network model not only capture the patterns associated\nwith the warning contexts, but also explicitly emphasize the\npositions of warnings. Consequently, this would be consider-\nably helpful when several warnings having similar contexts\nbut labeled (TP and FP) differently. However, our dataset is\nbuilt from the set functions which are already classiﬁed as\nvulnerable or non-vulnerable. Thus, most of the warnings in\na vulnerable function tend to have the same TP labels. Also,\nall of the warnings in a non-vulnerable function are labeled as\nFPs. That is the reason why the D EFP’s performance is just\nslightly improved when the reported statements are encoded\nas an input of the representation model.\nD. Impact of the Identiﬁer Abstraction Component (RQ4)\nFig. 7 shows that by abstracting identiﬁers, DEFP can\ncapture the general patterns associated with the warnings\nbetter. Specially, with identiﬁer abstraction, D EFP achieves\nabout 7% and 12% better in two kinds of studied vulner-\nabilities for Top-1%-Top-60%. For instance, in Top-20% of\nwarnings, D EFP can ﬁnd 105 actual BO vulnerabilities and\n12 actual NPD vulnerabilities, which is about 52% and 66% of\ntheir total actual vulnerabilities. Meanwhile, without identiﬁer\nabstraction, these numbers are only 96 and 11 vulnerabilities,\n8\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nWith Identifier Abstraction\nCD\nBuffer Overflow Warnings\nTop-k%\nRecall\n0%\n20%\n40%\n60%\n80%\n100%\n1% 20% 40% 60% 80% 100%\nI Without Identifier Abstraction\nNull Pointer Dereference Warnings\nFig. 7: Impact of identiﬁer abstraction on D EFP’s performance\nrespectively. More details about the performance of D EFP on\nP@K can be found on our website [21].\nMoreover, identiﬁer abstraction decreases Word2vec vocab-\nulary size on BO and NPD datasets from 37,170 to 512 and\n4,094 to 259 tokens, respectively. This helps the model deal\nwith the vocabulary explosion problem, better generalize rare\nidentiﬁers, and avoid out-of-vocabulary. As well, Word2vec\nmight also beneﬁcially reduce vector dimension to represent\neach code token, thus improve memory usage and shorten the\ntraining/prediction time.\nVI. T HREATS TO VALIDITY\nThere are three main threats to validity in this paper, they\nare external validity, internal validity, and construct validity,\nrespectively, which are illustrated as follows.\n1) External validity: Our dataset contains 10 open-source\nprograms and warnings in only two vulnerability types. There-\nfore, our results may not be general for all software projects\nand other kinds of vulnerabilities. To reduce the threat, we\nchose the programs which are widely used in the related\nwork [27], [28] and the top most popular vulnerabilities [33].\nAlso, we plan to collect more data for the future work.\n2) Internal validity: For this paper, the internal validity\nmainly lies in the data used for learning process. We manually\nlabeled for the warnings based on the labels of the functions,\nwhich are assigned by Zhou et al. [27] and Lin et al. [28].\nThe threat may come from their incorrect labels at function\nlevel or our misleading labels at warning level. To minimize\nthis threat, we carefully investigate to label the warnings.\n3) Construct validity: In this study, we adopt P@K and\nR@K for evaluating the performance of the ranking models.\nHowever, with the problem of handling SA warnings, evalu-\nation in terms of other metrics may also require in practice.\nWe will conduct experiments using more evaluation measures\nin our future work.\nVII. R ELATED WORK\nThere are various approaches have been applied to detect\nsource code vulnerabilities in the early phases of the software\ndevelopment process. Specially, using SA tools is an automatic\nand simple way to detect various kinds of vulnerabilities\nwithout executing the programs [1], [2]. Baca et al. [34] has\ndemonstrated that SA tools is better than average developers in\ndetecting warnings, especially the security ones. However, the\ngenerated warnings of SA tools often contain a high number\nof FP rate [7], [8]. Therefore, developers still need to waste a\nlot of time and effort for investing such FP warnings.\nIn order to improve the precision of SA tools, sophisti-\ncated program veriﬁcation techniques such as model checking,\nsymbolic execution, or deductive veriﬁcation, etc., have been\napplied to reduce the number of FPs [10]–[12], [35], [36].\nFor instance, Muske et al. [35], [36] uses model checking\nto eliminate FPs. Specially, for each warning, they generate\nappropriate assertions and then use model checking to verify\nwhether those assertions hold. Nguyen et al. [12] also generate\nproper annotations to describe the veriﬁed properties of the\nwarnings and then prove them by deductive veriﬁcation. These\napproaches can precisely discard a number of FPs. However,\nnot all of the generated warnings can be formally proved to be\nFPs or TPs by these approaches. Additional, model checking\napproaches also suffer from the enormous states space, which\naffect their performance and lead them to be non-scalable.\nIn addition, several studies applied ML models to address\nSA warnings. Specially, some research [6], [14], [15] propose\nsets of features about statistic information of the warnings and\nthen build a model which learn these features to classify SA\nwarnings. However, these features are manually deﬁned based\non the dataset and the used SA tools. This process is error-\nprone even for experts. Meanwhile, instead of using a ﬁxed set\nof features, Lee et al. [16] trained a CNN model classifying\nwarnings based on features which are learned from lexical\npatterns in source code. However, they manually deﬁned\ndifferent contexts for different kinds of warnings based on their\ndataset. This limits the adaptation of their approach for other\nkinds of vulnerabilities and different dataset. In this paper, we\npropose an approach which can be fully automated and easily\nto adapt for handling different warnings in different projects.\nSpecially, our models are trained to capture the patterns\nassociated with the warnings in their corresponding contexts,\nwhich are extracted by inter-procedural slicing techniques.\nMoreover, ML are also actively adopted in vulnerabilities\ndetection. Particularly, to leverage the syntax and semantics\ninformation presented in the Abstract Syntax Tree, Dam et\nal. [31] proposed a deep learning tree based model to predict\nwhether a source ﬁle is clean or defective. Besides, there are\nmultiple studies also propose token-based models [37], [38] or\ngraph-based models [20], [27] to predict whether a function\ncontaining vulnerabilities. However, these research focuses on\ndetecting vulnerabilities at the ﬁle level or function level,\nwhich are quite coarse-grained in granularity. Developers still\nneed to investigate the whole source code in the detected ﬁles\nor functions to localize the vulnerabilities. In this research,\nour objective is more ﬁne-grained in granularity. We focus on\nranking the warnings which are reported by SA tools. With\nthe resulting lists, developers can decide which vulnerabilities\nshould be investigated and ﬁxed in a given a amount of time.\n9\nVIII. C ONCLUSION\nSA tools have demonstrated their usefulness in detecting\npotential vulnerabilities. However, these tools often report a\nlarge number of warnings containing both TPs and FPs, which\ncauses time-consuming for post-handling warnings and affects\nthe productivity of developers. In this paper, we introduce\nDEFP, a novel method for ranking SA warnings. Based\non the reported statements and the corresponding warning\ncontexts, we train two BiLSTM models to capture the patterns\nassociated with the TPs and FPs. After that, for a set of\nnew warnings, D EFP can predict the likelihood to be TP of\neach warning and then rank them according to the predicted\nscores. By using D EFP, more actual vulnerabilities can be\nfound in a given time. In order to evaluate the effectiveness\nof D EFP, we conducted experiments on 6,620 warnings in 10\nreal-world projects. Our experimental results show that using\nDEFP, developers can ﬁnd +90% of actual vulnerabilities by\ninvestigating only 60% of the total warnings.\nACKNOWLEDGMENT\nThis work has been supported by VNU University of\nEngineering and Technology under project number CN20.26.\nIn this work, Kien-Tuan Ngo was funded by Vingroup Joint\nStock Company and supported by the Domestic Master/ PhD\nScholarship Programme of Vingroup Innovation Foundation\n(VINIF), Vingroup Big Data Institute (VINBIGDATA), code\nVINIF.2020.ThS.04.\nREFERENCES\n[1] N. Ayewah, W. Pugh, D. Hovemeyer, J. D. Morgenthaler, and J. Penix,\n“Using static analysis to ﬁnd bugs,” IEEE software, vol. 25, no. 5, pp.\n22–29, 2008.\n[2] N. Nagappan and T. Ball, “Static analysis tools as early indicators of pre-\nrelease defect density,” in Proceedings. 27th International Conference\non Software Engineering, 2005. ICSE 2005. IEEE, 2005, pp. 580–586.\n[3] C. S. C. Group, “SEI CERT Coding Standards (wiki).” [Online].\nAvailable: https://wiki.sei.cmu.edu/conﬂuence/display/seccode\n[4] M. The Motor Industry Software Reliability Association, Guidelines for\nthe Use of the C Language in Critical Systems , 03 2012.\n[5] M. Beller, R. Bholanath, S. McIntosh, and A. Zaidman, “Analyzing\nthe state of static analysis: A large-scale evaluation in open source\nsoftware,” in 2016 IEEE 23rd International Conference on Software\nAnalysis, Evolution, and Reengineering (SANER) , vol. 1. IEEE, 2016,\npp. 470–481.\n[6] L. Flynn, W. Snavely, D. Svoboda, N. VanHoudnos, R. Qin, J. Burns,\nD. Zubrow, R. Stoddard, and G. Marce-Santurio, “Prioritizing alerts\nfrom multiple static analysis tools, using classiﬁcation models,” in 2018\nIEEE/ACM 1st International Workshop on Software Qualities and their\nDependencies (SQUADE). IEEE, 2018, pp. 13–20.\n[7] B. Johnson, Y . Song, E. Murphy-Hill, and R. Bowdidge, “Why don’t\nsoftware developers use static analysis tools to ﬁnd bugs?” in 2013 35th\nInternational Conference on Software Engineering (ICSE). IEEE, 2013,\npp. 672–681.\n[8] U. Koc, S. Wei, J. S. Foster, M. Carpuat, and A. A. Porter, “An empirical\nassessment of machine learning approaches for triaging reports of a java\nstatic analysis tool,” in 2019 12th IEEE Conference on Software Testing,\nValidation and Veriﬁcation (ICST). IEEE, 2019, pp. 288–299.\n[9] J. Ruthruff, J. Penix, J. Morgenthaler, S. Elbaum, and G. Rothermel,\n“Predicting accurate and actionable static analysis warnings,” in 2008\nACM/IEEE 30th International Conference on Software Engineering .\nIEEE, 2008, pp. 341–350.\n[10] H. Post, C. Sinz, A. Kaiser, and T. Gorges, “Reducing false positives\nby combining abstract interpretation and bounded model checking,” in\n2008 23rd IEEE/ACM International Conference on Automated Software\nEngineering. IEEE, 2008, pp. 188–197.\n[11] H. Li, T. Kim, M. Bat-Erdene, and H. Lee, “Software vulnerability\ndetection using backward trace analysis and symbolic execution,” in\n2013 International Conference on Availability, Reliability and Security .\nIEEE, 2013, pp. 446–454.\n[12] T. T. Nguyen, P. Maleehuan, T. Aoki, T. Tomita, and I. Yamada, “Re-\nducing false positives of static analysis for sei cert c coding standard,”\nin 2019 IEEE/ACM Joint 7th International Workshop on Conducting\nEmpirical Studies in Industry (CESI) and 6th International Workshop\non Software Engineering Research and Industrial Practice (SER&IP) .\nIEEE, 2019, pp. 41–48.\n[13] T. Muske and A. Serebrenik, “Survey of approaches for handling static\nanalysis alarms,” in 2016 IEEE 16th International Working Conference\non Source Code Analysis and Manipulation (SCAM) . IEEE, 2016, pp.\n157–166.\n[14] U. Y ¨uksel, H. S ¨ozer, and M. S ¸ensoy, “Trust-based fusion of classiﬁers\nfor static code analysis,” in 17th International Conference on Informa-\ntion Fusion (FUSION) . IEEE, 2014, pp. 1–6.\n[15] M. Berman, S. Adams, T. Sherburne, C. Fleming, and P. Beling, “Active\nlearning to improve static analysis,” in 2019 18th IEEE International\nConference On Machine Learning And Applications (ICMLA) . IEEE,\n2019, pp. 1322–1327.\n[16] S. Lee, S. Hong, J. Yi, T. Kim, C.-J. Kim, and S. Yoo, “Classifying\nfalse positive static checker alarms in continuous integration using\nconvolutional neural networks,” in 2019 12th IEEE Conference on\nSoftware Testing, Validation and Veriﬁcation (ICST) . IEEE, 2019, pp.\n391–401.\n[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in ICLR, 2013.\n[18] V . Okun, A. Delaitre, P. E. Black et al., “Report on the static analysis\ntool exposition (sate) iv,” NIST Special Publication , vol. 500, p. 297,\n2013.\n[19] N. I. of Standards and Technology, “Software assurance reference\ndataset.” [Online]. Available: https://samate.nist.gov/SRD/index.php\n[20] S. Chakraborty, R. Krishna, Y . Ding, and B. Ray, “Deep learning\nbased vulnerability detection: Are we there yet,” IEEE Transactions on\nSoftware Engineering, 2021.\n[21] “DeFP.” [Online]. Available: https://tuanngokien.github.io/DeFP/\n[22] “Flawﬁnder.” [Online]. Available: https://dwheeler.com/ﬂawﬁnder/\n[23] S. Horwitz, T. Reps, and D. Binkley, “Interprocedural slicing using\ndependence graphs,” ACM Transactions on Programming Languages\nand Systems (TOPLAS) , vol. 12, no. 1, pp. 26–60, 1990.\n[24] “Joern.” [Online]. Available: https://docs.joern.io/home\n[25] G. Lin, J. Zhang, W. Luo, L. Pan, O. De Vel, P. Montague, and Y . Xiang,\n“Software vulnerability discovery via learning multi-domain knowledge\nbases,” IEEE Transactions on Dependable and Secure Computing, 2019.\n[26] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[27] Y . Zhou, S. Liu, J. Siow, X. Du, and Y . Liu, “Devign: Effective vulner-\nability identiﬁcation by learning comprehensive program semantics via\ngraph neural networks,” arXiv preprint arXiv:1909.03496 , 2019.\n[28] G. Lin, W. Xiao, J. Zhang, and Y . Xiang, “Deep learning-based vulner-\nable function detection: A benchmark,” in International Conference on\nInformation and Communications Security . Springer, 2019, pp. 219–\n232.\n[29] “CppCheck.” [Online]. Available: http://cppcheck.sourceforge.net\n[30] “RATS - Rough Auditing Tool for Security.” [Online]. Available:\nhttps://github.com/andrew-d/rough-auditing-tool-for-security\n[31] H. K. Dam, T. Pham, S. W. Ng, T. Tran, J. Grundy, A. Ghose,\nT. Kim, and C.-J. Kim, “Lessons learned from using a deep tree-based\nmodel for software defect prediction in practice,” in 2019 IEEE/ACM\n16th International Conference on Mining Software Repositories (MSR) .\nIEEE, 2019, pp. 46–57.\n[32] H. N. Nguyen, S. Teerakanok, A. Inomata, and T. Uehara, “The\ncomparison of word embedding techniques in rnns for vulnerability\ndetection.” in International Conference on Information Systems Security\nand Privacy (ICISSP) , 2021, pp. 109–120.\n[33] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y . Zhong,\n“Vuldeepecker: A deep learning-based system for vulnerability detec-\ntion,” arXiv preprint arXiv:1801.01681 , 2018.\n[34] D. Baca, K. Petersen, B. Carlsson, and L. Lundberg, “Static code analy-\nsis to detect software security vulnerabilities-does experience matter?” in\n2009 International Conference on Availability, Reliability and Security .\nIEEE, 2009, pp. 804–810.\n10\n[35] T. Muske, A. Datar, M. Khanzode, and K. Madhukar, “Efﬁcient elim-\nination of false positives using bounded model checking,” in ISSRE,\nvol. 15, 2013, pp. 2–5.\n[36] T. Muske and U. P. Khedker, “Efﬁcient elimination of false positives\nusing static analysis,” in 2015 IEEE 26th International Symposium on\nSoftware Reliability Engineering (ISSRE) . IEEE, 2015, pp. 270–280.\n[37] R. Russell, L. Kim, L. Hamilton, T. Lazovich, J. Harer, O. Ozdemir,\nP. Ellingwood, and M. McConley, “Automated vulnerability detection\nin source code using deep representation learning,” in 2018 17th\nIEEE international conference on machine learning and applications\n(ICMLA). IEEE, 2018, pp. 757–762.\n[38] Z. Li, D. Zou, S. Xu, H. Jin, Y . Zhu, and Z. Chen, “Sysevr: A\nframework for using deep learning to detect software vulnerabilities,”\nIEEE Transactions on Dependable and Secure Computing , 2021.\n11",
  "topic": "False positive paradox",
  "concepts": [
    {
      "name": "False positive paradox",
      "score": 0.7321075201034546
    },
    {
      "name": "Computer science",
      "score": 0.7302896976470947
    },
    {
      "name": "True positive rate",
      "score": 0.7297930717468262
    },
    {
      "name": "Intuition",
      "score": 0.7010435461997986
    },
    {
      "name": "Static analysis",
      "score": 0.6833515763282776
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5843858122825623
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.48395588994026184
    },
    {
      "name": "Learning to rank",
      "score": 0.48068365454673767
    },
    {
      "name": "Machine learning",
      "score": 0.4786616265773773
    },
    {
      "name": "False positives and false negatives",
      "score": 0.47384434938430786
    },
    {
      "name": "Software bug",
      "score": 0.4730257987976074
    },
    {
      "name": "Recall",
      "score": 0.46499860286712646
    },
    {
      "name": "False positive rate",
      "score": 0.4619010388851166
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4429510831832886
    },
    {
      "name": "Precision and recall",
      "score": 0.42844894528388977
    },
    {
      "name": "Data mining",
      "score": 0.4232522249221802
    },
    {
      "name": "Representation (politics)",
      "score": 0.4169297218322754
    },
    {
      "name": "Software",
      "score": 0.39514467120170593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3814796805381775
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}