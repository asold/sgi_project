{
  "title": "A Study on Semantic Understanding of Large Language Models from the Perspective of Ambiguity Resolution",
  "url": "https://openalex.org/W4391793563",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2304741612",
      "name": "Shuguang Yang",
      "affiliations": [
        "Jiangsu Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2125388413",
      "name": "Feipeng Chen",
      "affiliations": [
        "Jiangsu Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2098495947",
      "name": "Yi-Ming Yang",
      "affiliations": [
        "Jiangsu Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2460806736",
      "name": "Zude ZHU",
      "affiliations": [
        "Jiangsu Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2041258965",
    "https://openalex.org/W2118004980",
    "https://openalex.org/W4382202701",
    "https://openalex.org/W4247883378",
    "https://openalex.org/W1964424952",
    "https://openalex.org/W1985982445",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W2122811301",
    "https://openalex.org/W2888625409"
  ],
  "abstract": "Generative large language models (LLMs) have demonstrated outstanding performance in language understanding and generation tasks. Whether current LLMs can truly understand meaning or not is an open question. Semantic disambiguation is an important indicator to test machine language ability. To this end, three experiments were conducted to test the ambiguity resolution ability. In Experiment 1, the LLMs were asked to make a semantic discrimination on ambiguity phrases. The ChatGPT (May 3 Version) and ChatGPT Plus (May 3 Version) showed a random level performance of discrimination. To make the task easier for the LLMs, ambiguity sentences were presented with context in the Experiment 2. While the LLMs unified the context to fill the position for ambiguity word, the performance of semantic discrimination on these sentences remained at random level. In addition to test the performance of LMMs, prediction is one of the key ability of LLMs. The prediction by unifying the context is one of the key skill for LLMs. Therefore, the Experiment 3 further test the similarity between the human prediction mechanism and the strong generation capabilities of LLMs. Weak similarities were only found under the condition of local constraint features gradually accumulating, specifically, the cosine similarity of predicted sentence semantic vectors and ending word semantic vectors negatively correlated with close probability only. These series of experiments demonstrate that, it is far away to claim that LLMs truly understand human sentences.",
  "full_text": null,
  "topic": "Ambiguity",
  "concepts": [
    {
      "name": "Ambiguity",
      "score": 0.7632801532745361
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.7454569935798645
    },
    {
      "name": "Computer science",
      "score": 0.6668962240219116
    },
    {
      "name": "Resolution (logic)",
      "score": 0.6014399528503418
    },
    {
      "name": "Natural language processing",
      "score": 0.539125382900238
    },
    {
      "name": "Ambiguity resolution",
      "score": 0.4648849666118622
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4123295545578003
    },
    {
      "name": "Programming language",
      "score": 0.12321028113365173
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "GNSS applications",
      "score": 0.0
    },
    {
      "name": "Global Positioning System",
      "score": 0.0
    }
  ]
}