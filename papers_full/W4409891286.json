{
  "title": "Optimization of Decision Making Capabilities of Intelligent Characters in Strategy Games Based on Large Language Model Agent",
  "url": "https://openalex.org/W4409891286",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2102067545",
      "name": "Kehua Shi",
      "affiliations": [
        "Fuzhou University",
        "Fujian University of Technology",
        "Fujian Business University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6853251322",
    "https://openalex.org/W4390041953"
  ],
  "abstract": "In order to explore the optimization of decision-making ability of intelligent characters in strategy games based on Large Language Model (LLM) Agent, this paper takes pokellmon[1] developed by Georgia Institute of Technology as an example, and realizes the exploration of the scheme of optimization of decision-making ability by imitating the human players: 1. Collecting the experience of various human players and letting the LLM to expand the knowledge by means of Prompt Engineering and RAG(Retrieval-Augmented Generation) to get that experience; 2. Collecting the records of human high-scoring matchups records from the The replays records on pokemon showdown official website, and make a dataset of multiple rounds records of each game in the mode of multi-round dialogues, and fine-tune the LLM through QLoRA fine-tuning method (Tim Dettmers et al., 2023) after processing the data, so as to allow the LLM to learn the decision-making of the human masters in the random battles. The open-source LLM llamma3.1-8b was chosen for this study, and the battles against the heuristic bots showed that the modified pokellmon got some degree of strategy optimization, and compared to the very beginning of the battles against the heuristic bots, the winning rate was improved from 18% to 34%.",
  "full_text": "Optimization of Decision Making Capabilities of Intelligent\nCharacters in Strategy Games Based on Large Language Model\nAgent\nKehua Shi\nSchool of Fuzhou University, Fujian, China\nAbstract: In order to explore the optimization\nof decision-making ability of intelligent\ncharacters in strategy games based on Large\nLanguage Model (LLM) Agent, this paper\ntakes pokellmon[1] developed by Georgia\nInstitute of Technology as an example, and\nrealizes the exploration of the scheme of\noptimization of decision-making ability by\nimitating the human players: 1. Collecting the\nexperience of various human players and\nletting the LLM to expand the knowledge by\nmeans of Prompt Engineering and\nRAG(Retrieval-Augmented Generation) to\nget that experience; 2. Collecting the records\nof human high-scoring matchups records\nfrom the The replays records on pokemon\nshowdown official website, and make a\ndataset of multiple rounds records of each\ngame in the mode of multi-round dialogues,\nand fine-tune the LLM through QLoRA\nfine-tuning method (Tim Dettmers et al., 2023)\nafter processing the data, so as to allow the\nLLM to learn the decision-making of the\nhuman masters in the random battles. The\nopen-source LLM llamma3.1-8b was chosen\nfor this study, and the battles against the\nheuristic bots showed that the modified\npokellmon got some degree of strategy\noptimization, and compared to the very\nbeginning of the battles against the heuristic\nbots, the winning rate was improved from\n18% to 34%.\nKeywords: LLM Agent; Prompt Engineering;\nRAG; QLoRA; Game Stratege\n1. Introduction\nStrategy games are a complex and diverse game\ngenre that require players to think deeply about\nresource management, long-term planning, and\ndecision making. Due to its complexity and\ndiversity, strategy games are well suited for\nresearch in the field of artificial intelligence.\nMeanwhile, with the continuous and deep\ndevelopment of the game field, players'\nrequirementsforintelligent rolesinthegame are\ngetting richer and richer, and more and more\ndifferent technologies are applied to intelligent\nroles in strategy games. For the field of artificial\nintelligence, exploring theoretical approaches to\nimprove the strategic planning ability of\nintelligences helps intelligences to solve more\ncomplex tasks. For games, AI roles make the\ngame human-machine behavior uncertain, which\ncan make the game more diversified and keep\nthe player's freshness to a certain extent; while\nmore intelligent human-machine improves the\ndifficulty of the game, which makes the game\nmorechallenging. Inaddition,itcanpromotethe\napplication of the AI in the field of gaming, and\nraisepeople'sAIattention.\n2. Literature Review\nCommon AI techniques in games are traditional\npathfinding algorithms, such as the A* search\nalgorithmforheuristicpathfindingandimproved\nalgorithms based on the A* search algorithm;\nFinite-state machine and Behavior tree are also\nwidely used in game AI, as well as HTN\n(Hierarchical Task Network). However, the\nabove algorithms are written by human beings,\nand players can quickly discover the laws of\nthem after certain observations, thus affecting\nthe game experience. And GOAP (Goal\nOriented Action Planning) algorithm can\nautonomously and rationally select relevant\nactions to accomplish the decision-making\ngoal[5]. However, its ability to respond to\nenvironmental changes in real time is limited\nandrequirespredefinedgoalsandactions.\nReinforcement learning appeared earlier as a\ntechnical approach and has had applications in\ngaming since the 1990s (e.g., TD-Gammon).\nAlthough the concept of neural networks was\nintroduced earlier, modern deep learning\napplications (e.g., CNNs) began to be widely\n136\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 3, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press\nused in fields such as image processing only\nafter2006,andgraduallyextendedtothegaming\nfield. However, compared to traditional\ndecision-making algorithms, the training process\nofdeeplearningisnotconvenientfordebugging,\nand the effect is more uncontrollable[5]. Deep\nreinforcement learning (DRL), which began to\nemerge in 2013, combines deep learning and\nreinforcement learning. It learns by interacting\nwith the environment and optimizes based on\nfeedback signals, which significantly improves\nAI performance in games and is well suited to\ndynamically changing game environments that\nrequire real-time strategy adjustments. In 2016,\nDeepMind's AlphaGo defeated the world\nchampioninGobycombiningdeeplearningand\nMonte Carlo Tree Search (MCTS). This event\nbecame a milestone for AI in strategy games;\n2019's AlphaStar in StarCraft II demonstrated\nthe potential of DRL in complex real-time\nstrategy games. However, it relies on\nenvironmentmodeling, whichrequireshighstate\nspacedesign, andwillfacethetrade-offbetween\nexploration and exploitation. In addition to this,\nDRL requires a large amount of training data\nandtime,whichislessefficient.\nWith the rapid development of large language\nmodel technology, the application of LLM\nAgentinthegame fieldhasattractedwidespread\nattention, and many related applications have\nappeared one after another, such as Stanford\nTown and the application of LLM Agent in the\ngame Minecraft, Voyager, etc. LLM Agent has\ndemonstrated powerful reasoning and\ndecision-making capabilities by processing and\ngenerating natural language, and has become an\nimportant tool to enhance the game experience\nand intelligence level. It can generate coherent\nstrategies based on historical information over\nmultiple rounds, which is suitable for long term\nplanning, and unlike Deep Reinforcement\nLearning, LLM does not require a lot of training\naheadoftimetomakebasicdecisions.However,\nthe performance of LLM Agent in strategy\ngames still has a lot of room for improvement,\nand there are still some problems, such as:1.\nSingle strategy: LLM Agent may reuse known\nstrategies and lack of innovation and diversity.2.\nStrategy stability: Under different game\nenvironments and opponents' strategies, it may\nbe difficult for LLM Agent to maintain stable\nstrategy performance.3. Real-Time Feedback\nChallenge: Adaptability to real-time feedback\nmay be lacking in the decision-making process\nofLLMAgent\n3. Method\nThe purpose of this study is to explore the\ndecision optimization of intelligent characters\nbasedonLLMAgent,therefore,thefocusofthis\nstudyisondecisionoptimization,i.e.,tofindthe\noptimization method of LLM's decision making\nin the game, rather than optimizing the\nperformancedataofLLMinthegame.\nTo improve the performance of LLM\nAgent-based game AI characters in strategy\ngames, thefirstandforemost thing isto improve\ntheLLM'sstrategicabilityagainstthegame.The\nadvantage of intelligent characters over human\nplayers is the ability to clearly record the\ncomplex rules and various data of the game,\nwhich is often difficult for human players to\nmemorize all of them. The advantage of human\nplayers over intelligent characters is that human\nplayers are able to summarize their experience\nbased on previous game battles, and make more\ncomplex and long-term strategies, and even\nmake “deceptive” psychological tactics for the\ngame. Therefore, this study aims to improve the\nperformance of LLMs in strategy games by\nallowing them to mimic humans and learn from\ntheirexperience.\nThis decision optimization study takes the\npokellmom developed by the team as an\nexample, which is a LLM Agent based on the\npokemonshowdowngame, andplays asaplayer\nin the game. Based on cost constraints, the\nopen-source model llama3.1-8b was chosen as\nthe agent's “brain”in this study to make\ndecisionsinthegame, andthegame battleswere\nmainlybasedonthegen8randombattlemodeof\npokemonshowdown.\n3.1 Experience Learning\nIn this study, we collected and organized the\npokemon showdown battle experience of\ndifferent players from reddit website, smogon\nwebsite, quora website,etc, including macro and\ngeneral experiences, such as “Defense\nstrategies”, which is designed to protect your\nPokemon and keep them alive as long as\npossible. This can include using defensive\nabilities to increase your Pokemon’s resistance\nor using healing abilities to heal them after\ntakingdamage.Advantagesofthisstrategy:They\nallow you to last longer in a match, buy time,\nand give you the ability to counterattack.\nDisadvantages: it can slow down the pace of the\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 3, 2024\n137\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com\nmatch and allow the opponent to gain an\nadvantage.”, and micro and specific experience\nincluding recommended combos for pokemon\nmoves,items,etc..\n3.1.1RAG\nIn order to let LLM have the collected\nexperienceknowledge, thisstudybuilds aRAG()\nexperience knowledge base, which facilitates\nLLM to retrieve the information related to the\nbattle data in the knowledge base according to\nthe battle situation on the field when making\ndecisions, and measures and combines the\nretrieved experience with the original\ndecision-making method of Pokellmon, so that\nthe LLM can finally choose the appropriate\naction. However, there are some experience in\nthe knowledge base that are related to specific\npokémon, pokémon moves, items, weather and\nother combination conditions, therefore, after\nretrieving the experience information from the\nknowledge base, it is necessary to choose\naccording to the specific status quo of the game\nbattle, for example, the experience suggests that\ncertain moves and items should be used in\ncombination, but the pokémon player may not\nhave all the moves and items, therefore, the\nLLM'sdecision-makingmethodwillbebasedon\ntheinformationintheknowledgebase.\nIn this study, the collected experience\ninformation is firstly organized into a document,\nthen the document is segmented, and the\nsegmented data is vectorized and stored in a\nvector database. When the game requires\npokellmon to make a decision, the data of the\nround, such as the number of pokellmon\navailable to it’s own side, what is the state of\neach pokellmon, what arethemoves, what items\ncan be used, how are the weather conditions on\nthefield,andhowmanypokellmonareavailable\nto the opponent, what are the known opponent\npokellmon, etc., are retrieved in the vector\ndatabase, and the results will be added into the\npre-decision prompt submitted to the LLM,\nwhich is required to make the decision for the\nnextmovebasedonthecurrentstateofthegame\nandtherelevantempiricalknowledge.\n3.1.2PromptEngineering\nHowever, while specific experience information\nis easy to retrieve, macroscopic and general\nexperience information is not easy to retrieve,\nbecause there are few specific pokemon names,\nactions, items, etc. in these macroscopic\nexperience. Therefore, in order to utilize this\nmacro-experience information, it is added to the\nsystemprompt of LLM,which isusedtoprompt\nLLM'sdecision-makingskillsandchoices.\nTable 1. Performance of Experience Learning\nin Battles Against the Bot\nPlayer Winrate↑ Score↑ Turn# Battle#\nLLaMA-3.1 18.00% 4.33 19.01 100\nExperience 25.00% 4.51 20.27 100\n3.1.3Results\nHuman players have accumulateda greatdeal of\nexperience in the process of game battles, which\ncontains the understanding of tactics, familiarity\nwith different attributes and pokemon, and\nprediction of opponent's behavior. By\ntransformingtheseeffectivetacticsintoprompts,\nLLM Agent with strong language ability can\nquickly absorb and apply this valuable\nknowledge, understand the advantages and\ndisadvantages of various tactics, and avoid\ninefficient exploration to a certain extent. Not\nonly that, human players can recognize specific\nbattle situations and adjust their strategies based\non their experience. By providing this\nexperience,theLLMAgentisabletoacquirethe\nknowledge to recognize common battle patterns\nand generate strategies to improve the win rate\nwhen facing similar situations. In addition, the\nprovision of multiple human player strategies\nenhances the tactical flexibility of the LLM\nAgent, allowing it to dynamically adjust to the\nopponent and the battle environment, and to a\ncertain extent reduces the homogeneity of\nstrategies.Rapid Attack, Defense, and Counter\nAttack strategies are the most common and\neffective, utilizing different core gameplay\nmechanics and adapting to most matchmaking\nscenarios, while also being easier to master and\nimplement. Rapid Attacks are complemented by\nhigh bursts to overwhelm opponents, Defense\nStrategy counters quick attacks through\nendurance and attrition., and Counter Attacks\ntake advantage of opponents' mistakes in the\nbattlefield.\nTaking llama 3.1-8b as the core of the agent's\nthinking, the performance of the original\npokellmon and the pokellmon with the modified\nprompt and the added experience knowledge\nbase in the battle against the robots is shown in\nTable 1, which shows that the improved agent\nhasabetterperformanceindecisionmaking,and\nit can be reasonably hypothesized that if more\nand higher-quality experience data is collected,\nit can make LLM's decision-making in the game\nfurtheroptimized.\n3.1.4Advancedstrategies\n138\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 3, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press\nIn additiontothecommon basicstrategies,some\nhuman players use more advanced strategies,\nsuch as deception strategies. A deception\nstrategy involves misleading an opponent\nthrough false signals or actions, inducing them\nto make a wrong judgment or take an\nunfavorable action, and then using these\nmistakes of the opponent to gain an advantage.\nThis can be accomplished in a variety of ways,\nincluding bluffing, disguising one's strategy,\nfeigning weakness and thereby inducing the\nopponent to switch, or incorrectly anticipating\nthesituation.\nThe information provided to the LLM Agent\naboutdeceptionstrategiesasapromptincludesa\ndescription of the deception strategy, specific\npractices, advantages and disadvantages of the\nstrategy, and is intended to rely on the powerful\nlanguage capabilities of LLM to understand\nwhat a deception strategy is, to understand the\nhighrewardsandrisksofdeceptionstrategies,to\nmake decisions on its own based on the\nhistorical record and the current situation, and to\nbe flexible in evaluating or adjusting the\nstrategy.\nHowever, there are still challenges and\nlimitations for LLM Agent to execute advanced\nstrategies.The core ability of LLM is based on\nlanguage, which lacks the understanding and\nspeculationoftheopponent'spsychologicalstate,\nand relies more on the objective data of the\nopponent, making it more difficult to grasp the\ndeep psychological aspects (e.g., dealing with\nthe opponent's emotions, stress, etc.), and it is\ndifficult to predict the opponent's emotional\nfluctuations and changes in mindset as human\nplayers do, and to determine when it is\nappropriatetoexecutepsychologicalstrategies.\nIt is evident that while LLM has the potential to\nexecute advanced strategies, further exploration\nis needed to master and execute these complex\nadvancedstrategieswell.\n3.2 Replays Learning\nIn addition to the advantage of game experience\nover intelligent characters, human beings can\nlearn from watching exciting and high-quality\nbattles, thus realizing leveling up and improving\ntheir performance in the game. Therefore, this\nstudy collects 100 gen8 random battle replays\nfrom the official pokemon showdown website\nfrom high to low rating as a dataset, and learns\nbymeansoflargemodelfine-tuning.\n3.2.1Dataset\nWhat LLM needs to learn is to make decisions\nbased on the current game situation, i.e., input\nthe situation information and output the player\ndecisions. However, because the game is\nturn-based, and a round of the game contains\nmultiple matchmaking rounds, if only the\ndecision of the last round is used as the output\ndata, there will be a serious loss of\ninformation[4], if the multiple rounds are split\ninto multiple samples, and each sample input\ncontains all the previous input data, then the\nmodel will not be able to learn the overall game\ninformation and there is a large number of\nrepetitive operations. Therefore, in order for the\nLLMtolearntheinformationofthewholegame,\nthis study adopts a large model fine-tuning\nsimilar to the multi-round dialogues, where each\nround is a round of dialogues, and directly\nconstructs outputs that include all the decisions\ntaken by the LLM in multiple rounds, which\nmakes full use of all the information of the\ndecisions, and at the same time does not have to\nsplit and repeat the computation, which is very\nefficient.\nThe sample inputs to the training dataset are\nmultiple rounds of the user's prompt, each of\nwhich includes a record of the battle at the most\nrecentround(orthevery beginningof thegame)\n(including information such as the appearing\npokemon, the move used, the pokemon's\nSTATUS, the pokemon's attack, and damage),\nthe system prompt to the LLM, and the curent\nbattle state (describing the current pokemon\nstatus of the side and the opponent and their\npossible moves for LLMs decision making), and\nthe output of the dataset is the actions taken by\nown side in each round, including switch and\nmove. since LLM needs to learn the behavior of\nthe winning side, the dataset is constructed with\nthe winner as the side and the loser as the other\nside.\nIn order to obtain the game battle information,\nthisdatasetmainlyanalyzesthe“battle-log-data”\ninformation in the replays html file line by line,\nand updates the state of the player object and\npokemon object according to the obtained game\nprocess information, which can be used as the\ncurrentbattlestate.\nTable 2. Performance of Fine-Tuning in\nBattles Against the Bot\nPlayer Winrate↑ Score↑ Turn# Battle#\nOrigin 25.00% 4.51 20.27 100\nFine-tuning 34.00% 5.22 20.18 100\nFor example, if we get the “switch” field, we\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 3, 2024\n139\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com\nanalyze this line to get the player and pokemon\ninformation, and set the active pokemon.\nHowever, since there is no player's perspective\nin the replay log, we don't know in advance\nwhich Pokémon are available to the player and\nwhich moves are available to the pokémon, so\nwe need to update the information according to\nthe battle-log-data. In addition to this, there are\nfieldssuchas“move”,“-status”,and“-damage”.\n3.2.2Fine-tuning\nAfter acquiring the dataset, this study employs\nthe QLoRA[2] method to fine-tune the LLM so\nthat it learns the decision-making behavior of\neach game in the dataset. Where QLoRA is an\neffective fine-tuning fine-tuning method for\nquantitative large models, which centers on\nmaintaining or even improving the model's\nperformance on a specific task while\nsignificantly reducing the graphics memory\nrequirement through innovative quantization\nmethodsandmemorymanagementtechniques.\n3.2.3Results\nThe performance of pokellmon using the\nfine-tuned LLM in the battle against the robot is\nshown in Table 2, which shows that the LLM\nfine-tuned has better performance in game\ndecision making, andsubsequently cantry to get\nbetter performance by increasing the number of\nsamplesinthetrainingset.\n4.Conclusion\nBased on pokellmon, this work explores a\ndecision optimization scheme for an intelligent\ncharacter based on LLM Agent, including\ncollecting human players' experience, expanding\nthe knowledge of LLM by means of prompt\nengineering and RAG, as well as collecting\nrecords of human high-scoring matchups and\nmaking a dataset of the round records of each\ngame by means of a multi-round dialogue mode,\nand fine-tuning LLM by means of QLoRA\nmethod to fine-tune the LLM so that the LLM\nlearnsthedecisions ofhuman mastersinrandom\nbattles. The strategy method is generalized and\ncan be used for strategy enhancement of LLM\nAgent-based intelligent characters in other\ngames.\nBattlesagainstheuristicbotsshowthatModified\npokellmon based on llama3.1-8b can get a\ncertain degree of strategy optimization, and\ncompared to the very beginning of the battle\nagainst the heuristic bots, the win rate is\nimprovedfrom18%to34%.\nInaddition,sincethisstudyfocusesonexploring\nthe strategy enhancement options and feasibility\nof LLM rather than optimizing the performance\nto the extreme, the experimental results of this\nstudy using LLAMA 3.1-8b are not as good as\nthe original pokellmon using GPT4, but there is\nstill some room for improvement in this study's\nmethod, such as collecting more and higher\nquality human player experience to expand the\nknowledge of LLM, as well as collecting more\nreplays recordings of human masters' game\nmatches and producing them as sample datasets\nforfine-tuning.\nReferences\n[1] Hu, Sihao, Tiansheng Huang, and Ling Liu.\n“Pok\\'eLLMon: A Human-Parity Agent for\nPok\\'emon Battles with Large Language\nModels.” arXiv preprint\narXiv:2402.01118(2024).\n[2] Dettmers, Tim, et al. “Qlora: Efficient\nfinetuning of quantized llms.” Advances in\nNeural Information Processing Systems 36\n(2024).\n[3] Ma, Weiyu, et al. “Large language models\nplay starcraft ii: Benchmarks and a chain of\nsummarization approach.” arXiv preprint\narXiv:2312.11865(2023).\n[4] CSDN. (2024) BaiChuan13B Examples of\nfine-tuning multiple rounds of dialogues\nhttps://blog.csdn.net/Python_Ai_Road/articl\ne/details/132400115?rId=132400115&sourc\ne=Freyr_s &type=blog&refer=APP\n[5] cnblogs. (2023) Game AI Behavioral\nDecision Making\nhttps://www.cnblogs.com/OwlCat/p/178714\n94.html\n140\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 3, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6320923566818237
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44288331270217896
    },
    {
      "name": "Operations research",
      "score": 0.35425111651420593
    },
    {
      "name": "Management science",
      "score": 0.3409867286682129
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33902108669281006
    },
    {
      "name": "Engineering",
      "score": 0.15987062454223633
    }
  ],
  "institutions": [],
  "cited_by": 1
}