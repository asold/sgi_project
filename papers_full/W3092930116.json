{
  "title": "Cross-Lingual Relation Extraction with Transformers",
  "url": "https://openalex.org/W3092930116",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5102724821",
      "name": "Jian Ni",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004719091",
      "name": "Taesun Moon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5029725148",
      "name": "Parul Awasthy",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113515054",
      "name": "Radu Florian",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2053238041",
    "https://openalex.org/W2134033474",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W1958077162",
    "https://openalex.org/W2992787485",
    "https://openalex.org/W2249610072",
    "https://openalex.org/W2734693922",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964212344",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2119465010",
    "https://openalex.org/W2971145411",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2823100154",
    "https://openalex.org/W2964217331",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2030408698",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102086967",
    "https://openalex.org/W2240668419",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2153199128",
    "https://openalex.org/W2963940534",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2155454737",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "Relation extraction (RE) is one of the most important tasks in information extraction, as it provides essential information for many NLP applications. In this paper, we propose a cross-lingual RE approach that does not require any human annotation in a target language or any cross-lingual resources. Building upon unsupervised cross-lingual representation learning frameworks, we develop several deep Transformer based RE models with a novel encoding scheme that can effectively encode both entity location and entity type information. Our RE models, when trained with English data, outperform several deep neural network based English RE models. More importantly, our models can be applied to perform zero-shot cross-lingual RE, achieving the state-of-the-art cross-lingual RE performance on two datasets (68-89% of the accuracy of the supervised target-language RE model). The high cross-lingual transfer efficiency without requiring additional training data or cross-lingual resources shows that our RE models are especially useful for low-resource languages.",
  "full_text": "Cross-Lingual Relation Extraction with Transformers\nJian Ni and Taesun Moon and Parul Awasthy and Radu Florian\nIBM Research AI\n1101 Kitchawan Road, Yorktown Heights, NY 10598, USA\n{nij, tsmoon, awasthyp, raduf}@us.ibm.com\nAbstract\nRelation extraction (RE) is one of the most im-\nportant tasks in information extraction, as it\nprovides essential information for many NLP\napplications. In this paper, we propose a cross-\nlingual RE approach that does not require any\nhuman annotation in a target language or any\ncross-lingual resources. Building upon unsu-\npervised cross-lingual representation learning\nframeworks, we develop several deep Trans-\nformer based RE models with a novel encod-\ning scheme that can effectively encode both\nentity location and entity type information.\nOur RE models, when trained with English\ndata, outperform several deep neural network\nbased English RE models. More importantly,\nour models can be applied to perform zero-\nshot cross-lingual RE, achieving the state-of-\nthe-art cross-lingual RE performance on two\ndatasets (68-89% of the accuracy of the super-\nvised target-language RE model). The high\ncross-lingual transfer efﬁciency without requir-\ning additional training data or cross-lingual re-\nsources shows that our RE models are espe-\ncially useful for low-resource languages.\n1 Introduction\nWe live in an age of information. There are more\ndata in electronic form than ever before, from news\nand articles on the Web, to electronic transactional\nand medical records. It is very challenging to dis-\ncover and connect useful information and knowl-\nedge that is hidden in the huge amounts of data\nexisting today. As an example, in 2020 alone, there\nwere at least 200,000 scientiﬁc articles published\non COVID-19 (CORD-19, 2020). Information ex-\ntraction (IE) tackles this challenge via the study of\nautomatic extraction of structured information from\nunstructured or semi-structured electronic docu-\nments. The structured information can be used to\nbuild knowledge graphs and relational databases\nwhich help us to better understand the big data.\nRelation extraction (RE) is a fundamental IE\ntask that seeks to detect and classify semantic rela-\ntionships between entities or events from text (Dod-\ndington et al., 2004). It provides essential informa-\ntion for many NLP applications such as knowledge\nbase population (Ji and Grishman, 2011), question\nanswering (Xu et al., 2016) and text mining (Ag-\ngarwal and Zhai, 2012). For example, the entity\nNew York City and the entity United States have\na Part-Whole relationship, and extraction of such\nrelationships can help answer questions like “What\nis the most populous city in the United States?”\nRE models have evolved from feature-based sta-\ntistical models (e.g., Kambhatla (2004); Zhou et al.\n(2005); Li and Ji (2014)), to neural network models\nthat use pre-trained word embeddings (e.g., Zeng\net al. (2014); dos Santos et al. (2015); Xu et al.\n(2015); Miwa and Bansal (2016); Nguyen and Gr-\nishman (2016); Wu and He (2019); Soares et al.\n(2019)). The vast majority of RE research focuses\non one language, building and tuning models with\ndata from that language. In a globalization era, in-\nformation and data are available in many different\nlanguages, so it is important to develop RE models\nthat can operate across the language barriers.\nSince annotating RE data by human for every\nlanguage is expensive and time-consuming, it moti-\nvates the study of weakly supervised cross-lingual\nRE approaches that do not require manually an-\nnotated data for a new target language (e.g., Kim\nand Lee (2012); Faruqui and Kumar (2015); Zou\net al. (2018); Ni and Florian (2019); Subburathi-\nnam et al. (2019)). The existing cross-lingual RE\napproaches require certain cross-lingual resources\nbetween a source language (the language that one\nhas annotated RE data for, usually English) and\na target language or language-speciﬁc resources,\nsuch as aligned parallel corpora (Kim and Lee,\n2012), machine translation systems (Faruqui and\nKumar, 2015; Zou et al., 2018), aligned word\narXiv:2010.08652v1  [cs.CL]  16 Oct 2020\npairs (Ni and Florian, 2019), or universal depen-\ndency parsers (Subburathinam et al., 2019). Such\nresources may not be readily available in practice,\nwhich greatly limits the scalability of those ap-\nproaches when applied to a large number of target\nlanguages.\nIn this paper, we propose a cross-lingual RE\napproach based on unsupervised pre-trained mul-\ntilingual language representation models (Devlin\net al., 2019; Conneau et al., 2020). The main con-\ntributions of the paper include:\n• We develop several deep Transformer based\nRE models with a novel encoding scheme that\ncan effectively encode both entity location\nand entity type information in the input se-\nquence. Our English RE models outperform\nseveral deep neural network based English RE\nmodels, without using any language-speciﬁc\nresources such as dependency parsers or part-\nof-speech taggers.\n• Building on pre-trained multilingual embed-\ndings, our English RE models can be applied\nto perform zero-shot cross-lingual RE for a tar-\nget language without using any human annota-\ntion in the target language or any cross-lingual\nresources. Our models achieve the state-of-\nthe-art cross-lingual RE performance on two\ndatasets (68-89% of the accuracy of the super-\nvised target-language RE model). The high\ncross-lingual transfer efﬁciency shows that\nour RE models are especially useful for low-\nresource languages.\n• Our RE models can be trained with data from\nmultiple languages at the same time. The\njoint multilingual model performs better than\nmonolingual models built on the same archi-\ntecture but with one language at a time. In ad-\ndition to higher accuracy, such a joint model\nalso has many advantages in a production en-\nvironment: simpliﬁed deployment and mainte-\nnance, the same memory/CPU/GPU footprint,\nand easy scalability.\nWe organize the paper as follows. In Section\n2 we introduce the RE task and the framework.\nIn Section 3 we present several deep Transformer\nbased RE models with a novel encoding scheme.\nIn Section 4 we evaluate the performance of the\nproposed RE models and compare them with sev-\neral deep neural network based RE models on two\ndatasets. We discuss related work in Section 5 and\nconclude the paper in Section 6.\n2 Task and Framework\n2.1 Relation Extraction Task\nFor all pairs of entities in a sentence (or a sequence\nof words), the RE task is to determine whether\nthese pairs of entities have a relationship, and if\nyes, classify the relationship into one of the pre-\ndeﬁned relation types (Doddington et al., 2004).\nFormally, let s = (w1,w2,...,w n) be a sentence\nwith ntokens. Let e1 = (wi1,...,w j1) and e2 =\n(wi2,...,w j2) be a pair of entities in the sentence,\nwhere 1 ≤i1 ≤j1 < i2 ≤j2 ≤n, with entity\ntype T1 and T2, respectively.\nSuppose we have Krelation types. For all pairs\nof entities e1 and e2, an RE model maps (s,e1,e2,\nT1, T2) to a relation typec∈{0,1,2,...,K }where\nwe use type 0 to indicate that the two entities under\nconsideration do not have a relationship belonging\nto one of the Krelation types.\n2.2 Transformer Based Language\nRepresentation Models\nThe Transformer (Vaswani et al., 2017) is a neu-\nral network encoder-decoder architecture that uses\na multi-head self-attention mechanism to model\nglobal dependencies between tokens in input and\noutput sequences regardless of their distances.\nSince the Transformer uses only attention and\navoids recurrence, it can be used to train with much\nlonger sequences and larger datasets via signiﬁ-\ncantly more parallelization compared with recur-\nrent neural networks.\nRecent studies show that deep Transformer\nbased language representation models, when pre-\ntrained on a large text corpus, can achieve better\ngeneralization performance and attain the state-of-\nthe-art performance for many NLP tasks (Devlin\net al., 2019; Yang et al., 2019; Liu et al., 2019;\nConneau et al., 2020). There are two phases in the\nframework of those models: pre-training and ﬁne-\ntuning. During the pre-training phase, a language\nrepresentation model is trained on large amounts\nof unlabeled text. The pre-trained model parame-\nters/embeddings are then used to initialize models\nfor different downstream tasks, allowing effective\ntransfer learning. During the ﬁne-tuning phase, the\nparameters of the downstream task model are ﬁne-\ntuned using labeled data from the downstream task.\nFigure 1: Deep Transformer based neural network architecture for relation extraction.\nBERT (Devlin et al., 2019) is a language rep-\nresentation model based on a multi-layer bidirec-\ntional Transformer encoder architecture. It uses\na masked language model objective and a next\nsentence prediction objective during pre-training.\nRoBERTa (Liu et al., 2019) improves the pre-\ntraining of BERT by training the model longer with\nbigger batches and more data.\nOur work uses the multilingual version of BERT,\nnamed mBERT1, and the multilingual version of\nRoBERTa, named XLM-R (Conneau et al., 2020).\nmBERT was pre-trained with Wikipedia text of\n104 languages with the largest sizes, and XLM-R\nwere pre-trained with Wikipedia text and Common-\nCrawl Corpus of 100 languages. Both models use\nno cross-lingual resources and belong to the unsu-\npervised representation learning framework.\n3 Transformer Based RE Models\nThe RE task relies heavily on both syntactic and\nsemantic information, with possibly multiple enti-\nties/relations existing in one sentence, and therefore\ncannot be simply formulated as a sentence classi-\nﬁcation problem. There are several issues that are\nspecial to the RE task:\n1) How to encode entity location information\n(i.e., the positions of the two entities in the\nsentence) into the classiﬁcation model?\n2) How to encode entity type information (i.e.,\n1https://github.com/google-\nresearch/bert/blob/master/multilingual.md\nwhether an entity is of type Person, Organiza-\ntion, GPE, etc.) into the classiﬁcation model?\nThe entity type information is important for\nrelationship classiﬁcation. For example, if we\nknow that the two entities are of type Person,\nthen we can infer that they are likely to have\na Personal-Social relationship rather than an\nORG-Afﬁliation relationship.\n3) How to create a good summary representation\nof the sentence and the two entities from the\nhidden states for the RE task?\nOur deep Transformer based RE model archi-\ntecture is shown in Figure 1. We ﬁrst apply an\neffective encoding scheme to encode both entity\nlocation and entity type information in the input\nsequence (Section 3.1). The input tokens are then\nmapped to subword embeddings which are passed\nto deep Transformer layers (Section 3.2). We cre-\nate a ﬁxed-length summary representation from the\nhidden states of the ﬁnal Transformer layer (Sec-\ntion 3.3) and pass it to a linear classiﬁcation layer\n(Section 3.4).\n3.1 Encoding of Entity Location and Type\nFor an input sequence s = ( w1,w2,...,w n)\nand two entities e1 = ( wi1,...,w j1), e2 =\n(wi2,...,w j2) with entity types T1 and T2, we ﬁrst\nadd a special token [CLS] to mark the start of the\nsentence and a special token [SEP] to mark the end\nof the sentence as in BERT (Devlin et al., 2019).\nIn Wu and He (2019) and Soares et al. (2019),\nspecial tokens are used to mark the start and end of\nthe entities to encode entity location information:\ns′ =\n(\n[CLS],w1,..., [E1],wi1,...,w j1,[/E1],...,\n[E2],wi2,...,w j2,[/E2],...,w n,[SEP]\n)\n(1)\nIn this paper, we use the entity type of an entity\nto mark the start and end of that entity. So we have\nthe following modiﬁed input tokens:\ns′′ =\n(\n[CLS],w1,..., [T1],wi1,...,w j1,[T1],...,\n[T2],wi2,...,w j2,[T2],...,w n,[SEP]\n)\n(2)\nFor example, if the input tokens are:\n“New York City is the most populous city in the\nUnited States . ”\nafter adding the special tokens ([CLS], [SEP]) and\nthe entity type markers, we have the following mod-\niﬁed input tokens:\n“[CLS] [GPE] New York City [GPE] is the most\npopulous city in the [GPE] United States [GPE] .\n[SEP]”\nThe entity markers in (1) are the same for enti-\nties with different types, so we call them uniform\nmarkers (UM). In our formulation (2), the entity\ntype markers (ETM) encode both entity location\nand entity type information, killing two birds with\none stone. Experiment results show that models\nbased on entity type markers achieve higher accu-\nracy than models based on uniform markers.\n3.2 Deep Transformer Layers\nThe modiﬁed input tokens (2) are mapped to sub-\nword embeddings (WordPiece tokenizer (Wu et al.,\n2016) with 110k vocabulary size for mBERT and\nSentencePiece tokenizer (Kudo and Richardson,\n2018) with 250k vocabulary size for XLM-R). The\nembeddings are then passed to multiple Trans-\nformer layers (12 layers for mBERT and 24 layers\nfor XLM-R).\nEach Transformer layer has two sub-layers: the\nﬁrst is a multi-head self-attention layer and the sec-\nond is a position-wise fully connected feed-forward\nlayer (Vaswani et al., 2017). Both sub-layers have\na residual connection (He et al., 2016) followed by\nlayer normalization (Ba et al., 2016). Let the output\n(hidden states) of the ﬁnal Transformer layer be:\nH =\n(\nh[CLS],h1,..., hl\n[T1],hi1,..., hj1,hr\n[T1],...,\nhl\n[T2],hi2,..., hj2,hr\n[T2],..., hn,h[SEP ]\n)\n(3)\n3.3 Summarization Layer\nIn this subsection we present various schemes to\ncreate a ﬁxed-length summary representation of\nthe sentence and the two entities from the hidden\nstates (3) for relationship classiﬁcation.\n3.3.1 Sentence Start (SS)\nThe ﬁnal hidden state vector for the sentence start\ntoken [CLS], h[CLS], contains information of all\nthe tokens in the sentence via the attention mecha-\nnism. In this scheme, h[CLS] is used as a summary\nrepresentation of the whole sentence as in BERT\n(Devlin et al., 2019):\nhs = h[CLS] (4)\nWhile h[CLS] does not explicitly contain entity\nlocation or type information, such information can\nbe encoded in the input tokens as in (2).\n3.3.2 Entity Start (ES)\nTo better incorporate the entity information into\nthe classiﬁcation model, this scheme uses the con-\ncatenation of hl\n[T1] and hl\n[T2], the ﬁnal hidden state\nvectors for the two entity start (ES) tokens, as a\nsummary representation of the two entities:\nhs =\n[\nhl\n[T1],hl\n[T2]\n]\n(5)\nThis scheme is motivated by the entity start scheme\nin (Soares et al., 2019) where uniform markers (1)\nare used and the concatenation of h[E1] and h[E2]\nis used as a summary representation of the two\nentities. The difference is that hl\n[Ti] incorporates\nboth the entity location information (the start of an\nentity) and entity type information since we use the\nentity type markers (2).\n3.3.3 Entity Max Pooling (EMP)\nIn this scheme, to represent an entity, we perform\nelement-wise max pooling among the ﬁnal hidden\nstate vectors for the entity start token, the entity\ntokens and the entity end token of the two entities:\nhe1(k) = max\nhl\n[T1],hi1,...,hj1,hr\n[T1]\nh(k),1 ≤k≤H\nhe2(k) = max\nhl\n[T2],hi2,...,hj2,hr\n[T2]\nh(k),1 ≤k≤H\n(6)\nwhere H is the dimension of the hidden state vec-\ntors. This scheme then concatenates the two entity\nrepresentation vectors he1 and he2 as a summary\nrepresentation of the two entities:\nhs =\n[\nhe1,he2\n]\n(7)\nOne motivation of the max-pooling operation in\n(6) is to incorporate both entity type information\nand entity token information. In our experiments\nwe found that this max pooling scheme achieves\nhigher accuracy than the mention pooling scheme\nin (Soares et al., 2019) which performs max pool-\ning among the entity tokens only.\n3.4 Linear Classiﬁcation Layer\nThe summary representation hs is passed to a ﬁnal\nlinear classiﬁcation layer that returns a probability\ndistribution over the relation types:\np = softmax(Whs + b) (8)\n4 Experiments\nIn this section, we evaluate the performance of our\nTransformer based RE models and compare them\nwith the RE models in previous works (Ni and\nFlorian, 2019; Soares et al., 2019) on two datasets.\n4.1 Datasets\nThe ACE05 dataset (Walker et al., 2006) includes\nmanually annotated RE data for 3 languages: En-\nglish, Arabic and Chinese. It deﬁnes 7 entity types\n(Person, Organization, Geo-Political Entity, Lo-\ncation, Facility, Weapon, Vehicle) and 6 relation\ntypes between the entities (Agent-Artifact, General-\nAfﬁliation, ORG-Afﬁliation, Part-Whole, Personal-\nSocial, Physical).\nThe KLUE dataset (Han, 2010) includes manu-\nally annotated RE data for 6 languages: English,\nGerman, Spanish, Italian, Japanese and Portuguese.\nIt deﬁnes 56 entity types (e.g., Person, Organiza-\ntion, Geo-Political Entity, Location, Facility, Time,\nEvent Violence, etc.) and 53 relation types between\nthe entities (e.g., Agent-Of, Located-At, Part-Of,\nTime-Of, Affected-By, etc.).\nFor comparison studies, we use the same data\nsplit in (Ni and Florian, 2019): for each language,\n80% of the data is selected as the training set, 10%\nas the development set, and the remaining 10%\nas the test set, with sizes shown in Table 1. The\ndevelopment sets are used for tuning the model\nhyperparameters and for early stopping.\nACE05 Train Dev Test\nEnglish (en) 479 60 60\nArabic (ar) 323 40 40\nChinese (zh) 507 63 63\nKLUE Train Dev Test\nEnglish (en) 1137 140 140\nGerman (de) 280 35 35\nSpanish (es) 451 55 55\nItalian (it) 322 40 40\nJapanese (ja) 396 50 50\nPortuguese (pt) 390 50 50\nTable 1: Numbers of documents in the train/dev/test\nsets of ACE05 and KLUE.\n4.2 Models to Compare\nWe compare the following RE models:\n(a) The Convolutional Neural Network (CNN)\nbased RE model and the Bi-Directional Long Short-\nTerm Memory (Bi-LSTM) based RE model (Ni and\nFlorian, 2019);\n(b) The mBERT based RE models that use uniform\nmarkers (UM) and sentence start (SS), entity start\n(ES), or entity max pooling (EMP) summary repre-\nsentation (Soares et al., 2019)2;\n(c) The mBERT based RE model that uses uniform\nmarkers (UM) and entity start (ES) summary rep-\nresentation plus entity type embedding at the ﬁnal\nclassiﬁcation layer;\n(d) The mBERT based RE models that use entity\ntype markers (ETM) and sentence start (SS), entity\nstart (ES), or entity max pooling (EMP) summary\nrepresentation;\n(e) The XLM-R based RE models that use uniform\nmarkers (UM) or entity type markers (ETM), and\nsentence start (SS), entity start (ES), or entity max\npooling (EMP) summary representation.\nWe use HuggingFace’s pytorch implementation\nof Transformers (mBERT, XLM-R) (Wolf et al.,\n2019). mBERT has L = 12 Transformer layers,\nwith hidden state vector size H = 768, number\nof attention heads A= 12, and 110M parameters.\nXLM-R has L = 24, H = 1024, A = 16, and\n550M parameters. We learn the model parameters\nusing Adam (Kingma and Ba, 2015), with a learn-\ning rate 2e-5 for mBERT based RE models, and a\nlearning rate 1e-5 on ACE05 and 5e-6 on KLUE\nfor XLM-R based RE models. We train the RE\nmodels for 10 epochs. It took 6 hours (ACE05) and\n24 hours (KLUE) to train a XLM-R model with all\nthe training data on a NVIDIA V100 machine.\n2The RE models in (Soares et al., 2019) are based on\nEnglish BERT. In our experiments we implement those models\nwith mBERT for cross-lingual study.\nModel ACE05 KLUE\nCNN (Ni and Florian, 2019) 64.3 66.0\nBi-LSTM (Ni and Florian, 2019) 65.5 67.1\nmBERT-UM-SS (Soares et al., 2019) 66.1 73.2\nmBERT-UM-ES (Soares et al., 2019) 67.1 73.5\nmBERT-UM-EMP (Soares et al., 2019) 67.3 73.5\nmBERT-UM-ES+Entity Type 68.1 73.7\nmBERT-ETM-SS 69.5 74.3\nmBERT-ETM-ES 69.7 74.7\nmBERT-ETM-EMP 70.3 74.9\nXLM-R-UM-SS 70.5 76.2\nXLM-R-UM-ES 71.6 76.6\nXLM-R-UM-EMP 71.3 76.2\nXLM-R-ETM-SS 74.0 77.1\nXLM-R-ETM-ES 74.4 77.5\nXLM-R-ETM-EMP 73.7 77.6\nTable 2: Performance ( F1 score) of English RE mod-\nels on the ACE05 and KLUE English test data. UM\nstands for “Uniform Marker”, ETM for “Entity Type\nMarker”, SS for “Sentence Start”, ES for “Entity Start”,\nand EMP for “Entity Max Pooling”.\n4.3 English RE Model Performance\nTo evaluate cross-lingual RE performance, we ﬁrst\nuse English as the source language and other lan-\nguages as the target languages. We build super-\nvised English RE models trained with English train-\ning data only. For each Transformer based RE\nmodel architecture, we train 5 models initiated with\ndifferent random seeds, and the reported perfor-\nmance (F1 score) in Tables 2, 3, 4, 6, 7 is averaged\nover the 5 models3.\nAs shown in Table 2, deep Transformer based\nRE models achieve much better performance than\nthe CNN or Bi-LSTM based RE models in (Ni and\nFlorian, 2019). Among the Transformer based RE\nmodels, here are the key observations:\n(1) The models with entity type markers which en-\ncode both entity location and entity type informa-\ntion (e.g., mBERT-ETM-ES) outperform the mod-\nels with uniform markers which encode only entity\nlocation information (e.g., mBERT-UM-ES (Soares\net al., 2019)).\n(2) It is more effective to encode the entity type\ninformation in the input sequence and let the in-\nformation be propagated to the ﬁnal classiﬁcation\nlayer through the deep Transformer layers (as in\nmBERT-ETM-ES), than to add the entity type in-\nformation directly at the ﬁnal classiﬁcation layer\n(as in mBERT-UM-ES+Entity Type).\n3We report the average performance score of a neural net-\nwork based RE model trained with different random seeds,\nmotivated by observations in (Reimers and Gurevych, 2017),\nwhich showed that reporting single performance scores may\nbe insufﬁcient to compare neural network based models.\n(3) The deeper XLM-R based RE models (24 Trans-\nformer layers) further improve the mBERT based\nRE models (12 Transformer layers).\n4.4 Cross-Lingual RE Performance\nWe apply the English RE models to other target\nlanguages. For the English RE models in (Ni\nand Florian, 2019), cross-lingual model transfer\nis achieved by projecting the target-language word\nembeddings into the English embedding space\nvia a bilingual word embedding mapping ( the\ncross-lingual representation projection framework\n(Mikolov et al., 2013; Ni et al., 2017)). For the\nTransformer based English RE models, since we\ntrain the models using the mBERT or XLM-R sub-\nword embeddings which were pre-trained in the\nsame embedding space (the cross-lingual represen-\ntation learning framework), those models can be\ndirectly applied to perform zero-shot cross-lingual\nRE on other languages.\nAs shown in Table 3, our deep Transformer\nbased RE models attain much higher accuracy than\nthe models in the previous works, achieving the\nstate-of-the-art cross-lingual RE performance for\nall the languages. In particular, averaged over the\n7 target languages, the XLM-R-ETM-EMP model\nachieves 57.1 F1 score, which is 14.3 F1 points\nbetter than the Ensemble model in (Ni and Florian,\n2019) and 9.0 F1 points better than the mBERT-\nUM-ES model in (Soares et al., 2019).\nAmong the XLM-R-ETM models, the XLM-R-\nETM-SS model that uses global sentence repre-\nsentation does not perform as well as the XLM-R-\nETM-ES or XLM-R-ETM-EMP models that use\nlocal entity representations. We tried to concate-\nnate local entity representations (ES or EMP) with\nglobal sentence representation (SS), and we found\nthat this does not improve the accuracy.\nFor each target language, we also provide the\nperformance of the supervised XLM-R-ETM-EMP\nmodel trained with target-language training data,\nwhich serves as an upper bound for the cross-\nlingual RE performance. The average cross-lingual\nRE performance of the English XLM-R-ETM-\nEMP model (57.1 F1 score) reaches 81% of the\naverage performance of the supervised RE models\n(70.2 F1 score), which is quite impressive given\nthat some target languages (Arabic, Chinese and\nJapanese) are not from the same language family\nor using the same script as English.\nModel ACE05 KLUE Average\nar zh de es it ja pt\nCNN (Ni and Florian, 2019) 29.9 42.3 39.5 48.8 35.7 29.3 46.3 38.8\nBi-LSTM (Ni and Florian, 2019) 36.4 46.8 43.8 50.8 37.6 28.9 48.4 41.8\nEnsemble (Ni and Florian, 2019) 35.2 48.6 44.4 52.7 38.6 30.2 49.6 42.8\nmBERT-UM-SS (Soares et al., 2019) 38.0 56.2 46.3 59.0 39.7 32.8 52.8 46.4\nmBERT-UM-ES (Soares et al., 2019) 38.1 58.7 48.2 60.7 41.9 33.7 55.4 48.1\nmBERT-UM-EMP (Soares et al., 2019) 38.2 59.7 47.6 60.9 40.9 34.6 54.8 48.1\nmBERT-UM-ES+Entity Type 38.9 59.0 48.4 60.8 41.6 34.0 55.2 48.3\nmBERT-ETM-SS 40.0 59.1 48.5 61.4 42.0 36.3 56.3 49.1\nmBERT-ETM-ES 42.1 61.2 49.7 63.2 43.8 37.0 57.7 50.7\nmBERT-ETM-EMP 42.4 62.9 49.2 63.1 42.9 37.6 56.7 50.7\nXLM-R-UM-SS 43.9 60.7 52.9 66.2 48.1 44.3 59.2 53.6\nXLM-R-UM-ES 46.8 62.9 53.6 67.0 48.0 43.9 59.7 54.5\nXLM-R-UM-EMP 44.9 63.5 52.9 66.2 48.1 44.3 59.2 54.2\nXLM-R-ETM-SS 45.9 63.0 54.4 68.8 49.3 45.8 60.8 55.4\nXLM-R-ETM-ES 46.2 65.7 55.6 69.4 50.4 46.5 61.9 56.5\nXLM-R-ETM-EMP 49.7 64.9 55.0 70.3 50.4 47.1 62.0 57.1\nXLM-R-ETM-EMP (normalized by Supervised) 68% 87% 80% 89% 86% 71% 87% 81%\nXLM-R-ETM-EMP (Supervised) 72.9 75.0 68.4 78.8 58.8 66.1 71.1 70.2\nTable 3: Cross-lingual RE performance ( F1 score) of English (source-language) RE models on the ACE05 and\nKLUE target-language test data. UM stands for “Uniform Marker”, ETM for “Entity Type Marker”, SS for “Sen-\ntence Start”, ES for “Entity Start”, and EMP for “Entity Max Pooling”.\nRelation Type mBERT-UM-ES mBERT-ETM-ES\nen ar zh en ar zh\nAgent-Artifact 56.3 32.6 43.4 59.7 29.2 47.0\nGeneral-Afﬁliation 50.0 13.7 57.5 51.8 20.4 59.9\nORG-Afﬁliation 75.8 51.6 71.7 79.6 56.7 73.9\nPart-Whole 66.5 46.5 58.3 67.7 48.3 60.5\nPersonal-Social 69.3 25.7 21.2 71.7 25.3 22.2\nPhysical 61.1 23.0 48.2 62.3 25.4 51.7\nTable 4: Per typeF1 scores of the English mBERT-UM-\nES and mBERT-ETM-ES models on the ACE05 devel-\nopment sets. See Walker et al. (2006) for details on the\nrelation types.\n4.4.1 The Effect of Word Order\nIn Table 3 we provide the normalized cross-lingual\nRE accuracy from English to every target language\nunder the XLM-R-ETM-EMP model (the cross-\nlingual performance divided by the supervised per-\nformance), to study cross-lingual RE model trans-\nfer efﬁciency from English to the target languages.\nIt turns out that word order plays a key role in\ndetermining the cross-lingual transfer efﬁciency.\nEnglish belongs to the SVO language family where\nin a sentence the Subject is followed by the Verb,\nand the Verb is followed by the Object. Chinese,\nSpanish, Italian and Portuguese are also SVO lan-\nguages, and the normalized cross-lingual RE accu-\nracy from English to these languages reaches nearly\n90%, even for Chinese that uses logographic script\nwhich is totally different from English that uses\nLatin script. Arabic belongs to the VSO (Verb fol-\nlowed by Subject followed by Object) language\nfamily and Japanese belongs to the SOV (Subject\nfollowed by Object followed by Verb) language\nfamily. The normalized cross-lingual RE accuracy\nfrom English to these two languages is much lower,\naround 70%. Interestingly, German is a hybrid\nSVO/SOV language, with SVO in some cases and\nSOV in others, and the normalized cross-lingual\nRE accuracy from English to German is 80%.\nFor the representation projection approach in (Ni\nand Florian, 2019), it was also observed that word\norder affects the cross-lingual RE model transfer ef-\nﬁciency. The best model in (Ni and Florian, 2019)\nachieves an average transfer efﬁciency of 70% (av-\neraged over the 7 target languages), while the XLM-\nR-ETM-EMP model achieves an average transfer\nefﬁciency of 81% — a substantial improvement.\n4.4.2 Uniform Markers vs. Entity Type\nMarkers\nWe analyze the differences between the mBERT-\nUM-ES model and the mBERT-ETM-ES model in\nmore details. First, we provide type-levelF1 scores\non the ACE05 development data in Table 4. The\nmBERT-ETM-ES model has consistently better ac-\ncuracy across almost all of the relation types, with\n3+ F1 point improvement for Agent-Artifact and\nORG-Afﬁliation on the English development set.\nWe see bigger improvements of the mBERT-ETM-\nES model on some relation types for the cross-\nlingual RE model transfer. For example, it has 6.7\nF1 point improvement for General-Afﬁliation and\n5.1 F1 point improvement for ORG-Afﬁliation on\nthe Arabic development set. This shows that entity\ntype markers are very helpful to the Transformer\n# Text Ground Truth mBERT-UM-ES mBERT-ETM-ES\n1 The Cleveland Cavaliers also formally introduced\nPaul Silas as [their]/ORGANIZATION [coach]/PERSON\nMonday.\nORG-Afﬁliation Personal-Social ORG-Afﬁliation\n2 Therefore, on August 20, 2003, bravely\nBeatriz walked into the [USCF]/ORGANIZATION\n[Offices]/FACILITY in New Windsor and immediately\nfired 17 staff members.\nAgent-Artifact Part-Whole Agent-Artifact\n3 At [SUNY Albany]/ORGANIZATION in [NY]/GPE, about\nas state school as you can get, we get about 500\nless than what you’re talking about at Pitt.\nGeneral-Afﬁliation Part-Whole General-Afﬁliation\nTable 5: Relation extraction examples of the mBERT-UM-ES model and the mBERT-ETM-ES model on the\nACE05 English development set. The two entities are in brackets with entity types following the slash.\nTable 6: All-pair cross-lingual RE performance of\nXLM-R-ETM-EMP: rows are source languages (s) and\ncolumns are target languages. ρis the average normal-\nized cross-lingual RE accuracy.\nbased RE models, both in the monolingual scenario\nand in the cross-lingual scenario.\nWe list some examples in Table 5. In\nthe ﬁrst example, the two entities in\nbrackets (with entity types following the\nslash) ... [their]/ORGANIZATION\n[coach]/PERSON ... have the relation\nORG-Afﬁliation which was falsely labeled by\nmBERT-UM-ES as Personal-Social, while\nmBERT-ETM-ES, with entity type information\nencoded, succeeded in producing the correct label.\n4.4.3 All-Pair Cross-Lingual RE\nPerformance\nIn Table 6 we provide cross-lingual RE perfor-\nmance from each language s (as the source lan-\nguage) to every other language t (as the target\nlanguage) under the XLM-R-ETM-EMP model\ntrained with training data of s.\nTo study the cross-lingual RE model transfer\nefﬁciency of a source language s in a set of lan-\nguages L, we deﬁne ρL(s) to be the average nor-\nmalized cross-lingual RE accuracy from sto other\nlanguages in L:\nρL(s) =\n∑\nt∈L,t̸=s\nf(s,t)\nf(t,t)\n|L|−1 (9)\nwhere f(s,t) is the cross-lingual RE performance\n(F1 score) from source language s to target lan-\nguage t, and f(t,t) is the supervised RE perfor-\nmance of target language t. Larger ρ indicates\nhigher cross-lingual RE performance (transfer ef-\nﬁciency). As shown in Table 6, for both datasets,\nEnglish has the highest cross-lingual transfer ef-\nﬁciency, so it is a good choice for the source lan-\nguage.\n4.5 Multilingual RE Performance\nIn this subsection we investigate the capability of\nour Transformer based RE models for handling\ndata from multiple languages. We train a joint\nmultilingual RE model with data from all the lan-\nguages, and compare it with monolingual RE mod-\nels where a separate model is trained with data from\neach language using the same architecture. Since\nthe languages can be quite different, it is not clear\nwhether a multilingual model can achieve better or\neven comparable performance as the monolingual\nmodels.\nWe compare the performance of the multilingual\nmBERT-ETM-EMP model and the multilingual\nXLM-R-ETM-EMP model with the corresponding\nmonolingual models in Table 7. The multilingual\nRE models attain consistently better accuracy than\nthe monolingual RE models, even when the lan-\nguages are from different language families and\nuse different scripts. In addition to better accuracy,\na joint model also has many advantages in a produc-\ntion environment, such as simpliﬁed deployment\nand maintenance, the same memory/CPU/GPU\nfootprint, and easy scalability.\nModel ACE05 KLUE Ave\nen ar zh en de es it ja pt\nmBERT-ETM-EMP (Monolingual) 70.3 70.9 71.7 74.9 63.6 75.0 53.6 59.5 66.6 67.3\nmBERT-ETM-EMP (Multilingual) 70.7 71.7 73.2 75.2 65.1 76.1 55.3 61.3 68.3 68.5\nXLM-R-ETM-EMP (Monolingual) 73.7 72.9 75.0 77.6 68.4 78.8 58.8 66.1 71.1 71.4\nXLM-R-ETM-EMP (Multilingual) 73.7 73.5 75.8 78.3 68.7 79.7 60.7 67.4 71.4 72.1\nTable 7: Comparisons of monolingual RE models (for each language, a separate model is trained with data from\nthat language) and multilingual RE models (a joint model is trained with data from all the languages).\n5 Related Work\nThe existing weakly supervised cross-lingual\nRE approaches require certain cross-lingual or\nlanguage-speciﬁc resources. Kim and Lee (2012)\nuses aligned parallel corpora to create weakly la-\nbeled RE data. Faruqui and Kumar (2015) and Zou\net al. (2018) apply machine translation systems to\ntranslate sentences between English and a target\nlanguage. Ni and Florian (2019) uses aligned word\npairs to learn a bilingual word embedding mapping\nfor representation projection. Subburathinam et al.\n(2019) applies universal dependency parsers to con-\nvert sentences of different languages into language-\nuniversal tree structures. Our cross-lingual RE ap-\nproach does not require such resources.\nA few BERT based models have been developed\nfor the RE task. Wu and He (2019) and Soares et al.\n(2019) use uniform tokens to mark the start and end\nof the two entities. Those models do not encode\nentity type information and focus on English RE.\nmBERT and XLM-R have been applied to sev-\neral other NLP tasks including named entity recog-\nnition (NER), part-of-speech (POS) tagging, de-\npendency parsing and natural language inference\n(NLI) (Pires et al., 2019; Wu and Dredze, 2019;\nMoon et al., 2019; Conneau et al., 2020). Unlike\nthose tasks, the relation extraction task relies heav-\nily on both syntactic and semantic information. We\nshowed that mBERT/XLM-R can indeed represent\nsuch information in a language universal way and\nthe RE models built on top of it can transcend lan-\nguage barriers well.\n6 Conclusion\nIn this paper, we proposed a cross-lingual RE ap-\nproach based on unsupervised cross-lingual rep-\nresentation learning frameworks. We developed\nseveral deep Transformer based RE models with\na novel encoding scheme that effectively encodes\nboth entity location and entity type information.\nOur RE models can be applied to perform zero-\nshot cross-lingual RE, achieving the state-of-the-art\ncross-lingual RE performance (68-89% of the accu-\nracy of the supervised target-language RE model),\neven in cases where the target languages are from\ndifferent language families and use different scripts,\nwithout using any human annotation in the target\nlanguages or any cross-lingual/language-speciﬁc\nresources.\nReferences\nCharu C. Aggarwal and Cheng Xiang Zhai. 2012. Min-\ning Text Data. Springer Publishing Company, Incor-\nporated.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. ArXiv,\nabs/1607.06450.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nCORD-19. 2020. COVID-19 Open Re-\nsearch Dataset Challenge. https://www.\nkaggle.com/allen-institute-for-ai/\nCORD-19-research-challenge . Accessed:\n2020-10-16.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGeorge Doddington, Alexis Mitchell, Mark Przybocki,\nLance Ramshaw, Stephanie Strassel, and Ralph\nWeischedel. 2004. The automatic content extraction\n(ACE) program – tasks, data, and evaluation. In\nProceedings of the Fourth International Conference\non Language Resources and Evaluation (LREC’04),\nLisbon, Portugal. European Language Resources As-\nsociation (ELRA).\nManaal Faruqui and Shankar Kumar. 2015. Multilin-\ngual open relation extraction using cross-lingual pro-\njection. In Proceedings of the 2015 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1351–1356. Association for Compu-\ntational Linguistics.\nDing-Jung (Benjamin) Han. 2010. KLUE Annotation\nGuidelines - Version 2.0. IBM Research Report ,\nRC25042.\nK. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-\nual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 770–778.\nHeng Ji and Ralph Grishman. 2011. Knowledge base\npopulation: Successful approaches and challenges.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1148–1158, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nNanda Kambhatla. 2004. Combining lexical, syntac-\ntic, and semantic features with maximum entropy\nmodels for extracting relations. In Proceedings of\nthe ACL 2004 on Interactive Poster and Demonstra-\ntion Sessions , Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nSeokhwan Kim and Gary Geunbae Lee. 2012. A graph-\nbased cross-lingual projection approach for weakly\nsupervised relation extraction. In Proceedings of the\n50th Annual Meeting of the Association for Compu-\ntational Linguistics: Short Papers - Volume 2 , ACL\n’12, pages 48–53, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the 3rd International Conference on Learning\nRepresentations (ICLR), ICLR ’15.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nQi Li and Heng Ji. 2014. Incremental joint extraction\nof entity mentions and relations. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 402–412. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. CoRR, abs/1309.4168.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using LSTMs on sequences and tree\nstructures. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1105–1116. Associ-\nation for Computational Linguistics.\nTaesun Moon, Parul Awasthy, Jian Ni, and Radu Flo-\nrian. 2019. Towards lingua franca named entity\nrecognition with BERT. CoRR, abs/1912.01389.\nThien Huu Nguyen and Ralph Grishman. 2016. Com-\nbining neural networks and log-linear models to im-\nprove relation extraction. In Proceedings of IJCAI\nWorkshop on Deep Learning for Artiﬁcial Intelli-\ngence (DLAI).\nJian Ni, Georgiana Dinu, and Radu Florian. 2017.\nWeakly supervised cross-lingual named entity recog-\nnition via effective annotation and representation\nprojection. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1470–1480. Associ-\nation for Computational Linguistics.\nJian Ni and Radu Florian. 2019. Neural cross-lingual\nrelation extraction based on bilingual word embed-\nding mapping. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 399–409, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of LSTM-networks for sequence tagging. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n338–348, Copenhagen, Denmark. Association for\nComputational Linguistics.\nCicero dos Santos, Bing Xiang, and Bowen Zhou. 2015.\nClassifying relations by ranking with convolutional\nneural networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 626–634. Association for Computa-\ntional Linguistics.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905, Florence, Italy. Association for\nComputational Linguistics.\nAnanya Subburathinam, Di Lu, Heng Ji, Jonathan\nMay, Shih-Fu Chang, Avirup Sil, and Clare V oss.\n2019. Cross-lingual structure transfer for rela-\ntion and event extraction. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 313–325, Hong Kong,\nChina. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 6000–6010.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. ACE 2005 multilingual\ntraining corpus.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nShanchan Wu and Yifan He. 2019. Enriching pre-\ntrained language model with entity information for\nrelation classiﬁcation. ArXiv, abs/1905.08284.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nKun Xu, Siva Reddy, Yansong Feng, Songfang Huang,\nand Dongyan Zhao. 2016. Question answering on\nFreebase via relation extraction and textual evidence.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2326–2336, Berlin, Germany.\nAssociation for Computational Linguistics.\nYan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,\nand Zhi Jin. 2015. Classifying relations via long\nshort term memory networks along shortest depen-\ndency paths. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1785–1794, Lisbon, Portugal. Associa-\ntion for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classiﬁcation via con-\nvolutional deep neural network. In Proceedings of\nCOLING 2014, the 25th International Conference\non Computational Linguistics: Technical Papers ,\npages 2335–2344. Dublin City University and Asso-\nciation for Computational Linguistics.\nGuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.\n2005. Exploring various knowledge in relation ex-\ntraction. In Proceedings of the 43rd Annual Meet-\ning of the Association for Computational Linguis-\ntics (ACL’05), pages 427–434, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nBowei Zou, Zengzhuang Xu, Yu Hong, and Guodong\nZhou. 2018. Adversarial feature adaptation for\ncross-lingual relation classiﬁcation. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics , pages 437–448, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8383816480636597
    },
    {
      "name": "Transformer",
      "score": 0.7885626554489136
    },
    {
      "name": "ENCODE",
      "score": 0.6667898893356323
    },
    {
      "name": "Relationship extraction",
      "score": 0.6554807424545288
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6427296996116638
    },
    {
      "name": "Natural language processing",
      "score": 0.6294269561767578
    },
    {
      "name": "Information extraction",
      "score": 0.5404819250106812
    },
    {
      "name": "Annotation",
      "score": 0.510319173336029
    },
    {
      "name": "Deep learning",
      "score": 0.5002632141113281
    },
    {
      "name": "Transfer of learning",
      "score": 0.47384315729141235
    },
    {
      "name": "Artificial neural network",
      "score": 0.4424012303352356
    },
    {
      "name": "Language model",
      "score": 0.4277708828449249
    },
    {
      "name": "Machine learning",
      "score": 0.3906540274620056
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}