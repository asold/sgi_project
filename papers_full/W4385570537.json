{
  "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
  "url": "https://openalex.org/W4385570537",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5014196569",
      "name": "Himanshu Thakur",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5100879246",
      "name": "Atishay Jain",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5086111928",
      "name": "Praneetha Vaddamanu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5086233510",
      "name": "Paul Pu Liang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5081398601",
      "name": "Louis‐Philippe Morency",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971015127",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3039559565",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3035241006",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2887768933",
    "https://openalex.org/W4287887504",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4385573960",
    "https://openalex.org/W2951911250"
  ],
  "abstract": "Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 340–351\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLanguage Models Get a Gender Makeover:\nMitigating Gender Bias with Few-Shot Data Interventions\nHimanshu Thakur Atishay Jain∗ Praneetha Vaddamanu∗\nPaul Pu Liang Louis-Philippe Morency\nCarnegie Mellon University\n{hthakur,atishayj,pvaddama,pliang,morency}@andrew.cmu.edu\nAbstract\nCaution: this paper contains potentially offen-\nsive or upsetting model outputs.\nSocietal biases present in pre-trained large lan-\nguage models are a critical issue as these mod-\nels have been shown to propagate biases in\ncountless downstream applications, rendering\nthem unfair towards specific groups of peo-\nple. Since large-scale retraining of these mod-\nels from scratch is both time and compute-\nexpensive, a variety of approaches have been\npreviously proposed that de-bias a pre-trained\nmodel. While the majority of current state-of-\nthe-art debiasing methods focus on changes to\nthe training regime, in this paper, we propose\ndata intervention strategies as a powerful yet\nsimple technique to reduce gender bias in pre-\ntrained models. Specifically, we empirically\nshow that by fine-tuning a pre-trained model on\nonly 10 de-biased (intervened) training exam-\nples, the tendency to favor any gender is sig-\nnificantly reduced. Since our proposed method\nonly needs a few training examples, our few-\nshot debiasing approach is highly feasible and\npractical. Through extensive experimentation,\nwe show that our debiasing technique performs\nbetter than competitive state-of-the-art baselines\nwith minimal loss in language modeling ability.\n1 Introduction\nRecently, there has been a surge of interest in pre-\ntrained large language models (LLM) in natural lan-\nguage processing (NLP). It has been shown that the\npre-training + finetuning of a model drastically im-\nproves its performance on downstream tasks as the\nknowledge captured by the pre-training on a large\ncorpus is transferred to the downstream applica-\ntion when finetuning the model. However, this also\nleads to societal biases like gender bias that were\nimplicitly learned by the pre-trained models being\ntransferred to crucial downstream applications like\njob recommendation engines (Zhao et al., 2019;\n∗ Equal Contribution\nBarocas et al., 2017; Kurita et al., 2019). Analyz-\ning and mitigating bias without requiring significant\nre-training or compute resources is crucial to the\nwidespread adoption of LLMs in downstream appli-\ncations.\nPrevious work (Nadeem et al., 2021), (Nangia\net al., 2020a), (Cer et al., 2018) has attempted to\nquantify bias, and others such as Ravfogel et al.\n(2020) and Liang et al. (2021) have attempted to\nremove it algorithmically from the models. Closer\nto our work are data-manipulative techniques such\nas Zmigrod et al. (2019) and Maudslay et al. (2019)\nthat modify the dataset and further fine-tune the\nmodel. In this paper, we propose simple data inter-\nvention strategies and show that they can mitigate\ngender bias in pre-trained models with the help of\nfew-shot fine-tuning. Moreover, taking inspiration\nfrom Schick et al. (2021), we find that by utiliz-\ning a biased pre-trained LLM for mining for most\ngender-biased samples in a dataset, our methods can\nmitigate gender bias with very few training samples.\nFinally, we perform an extensive evaluation of our\ndebiasing technique on two recent bias benchmarks\n(Nadeem et al., 2021) and show that our method out-\nperforms three existing state-of-the-art techniques\nand performs comparably to the other two. Our\nmain contributions are the following:\n• We propose simple data intervention tech-\nniques that can be used to reduce gender bias\nin a pre-trained LLM with few training ex-\namples (few-shot), thus making human-in-the-\nloop bias mitigation strategies feasible.\n• We introduce a novel data sampling technique\nthat utilises LLMs to mine for the most biased\nsamples from a dataset and can benefit existing\nstate-of-the-art debiasing methods. When used\nfor debiasing a model, these few samples serve\nas exemplars and induce large reductions in\ngender bias.\n340\nGender Bias in BERT Predictions\nMost-biased\ndata samples\n____ is very good at cooking but not great at work.\n0.42\n0.39\nhe\nshe\n____ is very good at cooking but not great at work.\n____ is a great player but is not a caring parent.0.39 he\n0.04 he____ is a great player but is not a caring parent.\n0.68\n0.17\nshe\nhe\nBiased Pre-trained Model\n[MASK] is very good...\nshe\neither he or she\n[MASK] is a great...\nhe\nthey\nProposed\ndata\ninterventions\nDebiased Pre-trained Model\nFew Shot\nFine-Tuning\nReduced Gender Bias in BERT Predictions\nFigure 1: Our method can be summarized as a combi-\nnation of bias discovery and mitigation. First, we use a\npre-trained LLM to find the most gender-biased samples.\nThen, we apply our data intervention techniques and use\nthese modified training samples to fine-tune the model.\nExperiments show that our method is very effective at re-\nducing gender bias, outperforming three state-of-the-art\nbaselines and being comparable to two other baselines.\n2 Related Work\nIn recent years, there has been growing concern\nabout the bias/stereotypical discriminatory behav-\nior by NLP models, particularly concerning gen-\nder. Several studies have investigated the presence\nof gender bias in various NLP tasks and proposed\nmethods for mitigating it.\nOne line of research has focused on analyzing\nthe extent of gender bias in pre-trained language\nmodels such as BERT and GPT-2. These studies\nhave found that these models exhibit a significant\namount of gender bias in their word embeddings\nfor BERT (Jentzsch and Turan, 2022) and for GPT-\n2 (Kirk et al., 2021) and are prone to making stereo-\ntypical gender-based predictions (e.g., assuming\nthat a doctor is male and a nurse is female). A stan-\ndard evaluation metric used in this line of research\nis Stereotype metrics such as StereoSet (Nadeem\net al., 2021), which evaluates the model’s ability to\npredict gender stereotypes and CrowS pairs (Nan-\ngia et al., 2020b) which measure whether a model\ngenerally prefers more stereotypical sentences. A\nsimilar line of work is gender bias tests proposed\nin BIG-bench (Srivastava et al., 2022). The tests\nassess the language model’s gender biases, stereo-\ntypes, and ability to infer gender information. It\nevaluates gender bias and stereotype between male\nand female, and gender minority bias and stereotype\nbetween majority and minority. It also examines\nthe model’s language modeling performance, which\ncan be affected during de-biasing.\nAnother line of research has proposed methods\nfor debiasing these models. These methods can be\nbroadly categorized into two groups: data-based\nand algorithm-based. Data-based methods aim to\nreduce bias by removing or altering biased words\nfrom the training set. In contrast, algorithm-based\nmethods aim to modify the model’s architecture or\ntraining procedure to reduce bias. One popular data-\nbased method is \"uncertainty sampling\" (Lewis and\nGale, 1994), where the model is trained on the in-\nstances that it is most uncertain about, which can\nhelp to reduce bias by forcing the model to learn\nfrom a diverse set of examples. A popular algorithm-\nbased method is \"Adversarial Debiasing\" proposed\nby Zhang et al. (2018), which fine-tunes the model\nusing an adversarial loss to make it less sensitive\nto sensitive attributes such as gender. OSCar pro-\nposed by Dev et al. (2021), is another algorithm\nbased method that utilizes the idea of disentangling\n\"problematic concepts\" like occupation and gender\nrelationship instead of removing them altogether.\nMABEL (He et al., 2022) has both algorithm and\ndata-based components, as it first augments the\ntraining data by swapping gender words and then\napplies a contrastive learning objective and align-\nment via entailment pairs. Their data augmentation\nstrategy is similar in spirit to the data intervention\ntechniques we propose, however our analysis does\nnot require training auxiliary models and uses sig-\nnificantly lesser data.\nData-based methods include the \"Equalization\"\ntechnique proposed by Bolukbasi et al. (2016),\nwhich aims to equalize the representation of gender-\nspecific words in the embedding space, the \"Coun-\nterfactual Data Augmentation\" (CDA) method pro-\nposed by Zimmermann and Hoffmann (2022),\nwhich generates counterfactual examples to im-\nprove the model’s robustness to bias, and \"Name-\nBased Counterfactual Data Substitution\" proposed\nby Maudslay et al. (2019) which reduces gender\nbias by replacing gender-informative names in the\ndataset with gender-neutral names. Our proposed\nmethod is also a data-based method, which aims\nto effectively reduce gender bias by taking inspira-\ntion from different techniques such as uncertainty\nsampling and name-based counterfactual data sub-\nstitution (Maudslay et al., 2019).\n341\nGender-Word\nPairs\nMean\nConfidence\nDifference\nMean Std. Dev.\nhe, she 0.317 0.288\nWill, May 0.316 0.225\nboy, girl 0.219 0.218\nTable 1: Confidence difference for the Top 3 gender-\nword pairs in StereoSet\n3 Probing Bias in Large Language Models\nPre-trained LLMs are biased towards different gen-\nders, as seen in a simple mask-fill experiment us-\ning BERT. (Here, and in the rest of the paper, we\nassume a binary treatment of gender for simplic-\nity.) The task is then to mask out the gender-related\nnouns and pronouns (such as he, she, her, woman,\netc.) and get BERT to predict the masked words\nfor the affected sequences in the dataset. Here, we\nconsider a fixed list of gender-specific words cu-\nrated from previous work (Lu et al., 2018; Zmigrod\net al., 2019) and neutral words list 1. We finally\ncompute the \"total confidence difference\" as the\nsum of differences in the model’s prediction con-\nfidence for each gender-word pair (such as confi-\ndence of predicting he −she, man −woman, etc.).\nFormally, we define total confidence difference as\n|∑N\ni=0(f(x(i)\nfemale ) −f(x(i)\nmale))|where f(x) rep-\nresent the confidence of model’s prediction, N is\nthe total number of tokens in the dataset and x is\nthe tokenized gender word. The higher this number,\nthe more biased the model is concluded to be. We\ncompute the metric at token level and ensure that\neach of the gender word gets tokenized into exactly\none token by initially extending the tokenizer with\nour gender word list. The top 3 biased gender-word\npairs in StereoSet are shown in Table 1. Intuitively,\nour technique for gauging bias in LLMs is sensitive\nto the fixed word list used to represent the sensitive\nattributes (here, gender). In Table 2, we show the\nnumber of words covered by the word list used for\nboth WikiText-2 and StereoSet datasets.\n4 Data Interventions\nIn order to reduce gender bias in pre-trained models,\nwe carefully select diverse and hard-biased exam-\nples and then replace gender words with more neu-\n1https://github.com/joelparkerhenderson/\ninclusive-language\nDataset Samples Affected Words\n(mean)\nWikiText-2\n10 191\n50 627\n100 1028\nStereoSet\n10 55\n50 227\n100 463\nTable 2: Number of words (mean ) covered by the word\nlist vs dataset and number of sequences sampled from\neach dataset\ntral or equality-focused phrases. This is achieved by\nusing a wordlist to find gender terms in sentences\nand then segregating words as name and non-name\nwords.\nWe call our initial approach naive-masking as\nit does not require a word list for mapping gender\nwords to gender-neutral words. Instead, it replaces\nall gender words with the fixed word \"person.\" In\nour next approach, neutral-masking, we swap\nwords in a slightly more semantically accurate man-\nner. In this, we use a word-pair list that goes from\ngender words to gender-neutral words. With both\napproaches, we intend to introduce new words in a\nmodel’s vocabulary to make it more likely to choose\na more neutral word in gender-biased sentences.\nIn our final approach, we exploit the existing vo-\ncabulary of the model and try to balance the confi-\ndence of prediction on opposite-gender words by us-\ning phrases instead. Thus, we call our final approach\nrandom-phrase-masking as we instead substitute\nwords with phrases that reflect the equality of gen-\nder. This approach not only reduces gender bias\nbut also preserves the original meaning of the sen-\ntence in most cases. In our approach, we chose the\nphrases and order of gender words at random with\nequal probability.\nIntervention Input word Converted word\nnaive-masking\nhe person\nshe person\nboy person\nneutral-masking\nhe they\nher their\nschoolgirl schoolkid\nrandom-phrase-masking\nhe he or she\nshe she and he\nboy either girl or boy\nTable 3: Example conversions for three methods. In\nRandom Phrase Masking, the phrase is being chosen and\nit’s order was random.\n342\nAdditionally, we hypothesize that the choice of\nthe dataset for fine-tuning is also essential. We\nchoose two datasets: the WikiText-2 (Merity et al.,\n2017) dataset, which has implicit gender bias since\nits sources from Wikipedia articles, and the Stere-\noSet dataset (Nadeem et al., 2021), which has ex-\nplicit/more gender bias as it has been designed to\nevaluate gender bias. WikiText-22 has 600 train arti-\ncles and roughly 2M tokens while StereoSet3 (dev)\nhas 2123 samples out of which we only consider\n800 samples which are not unrelated. Naturally,\nour data intervention method should work better on\na dataset with training examples with gender bias\nwhile being devoid of meaningful gender associa-\ntions like \"She needs a gynecologist,\" where the\ngender of the person is important. By testing our\nmethod on both datasets, we can understand the\nsensitivity of our approach to the quality of training\nsamples used.\n5 Bias Evaluation Metrics\nWe focus on evaluating the bias of a model while\nalso measuring its language modeling capability.\nThe ideal model would not just be one with the least\nbias but also one which does not compromise its lan-\nguage modeling performance. The dual estimation\nof bias and performance of a model was proposed\nin the StereoSet benchmark (Nadeem et al., 2021),\nwith the Language Modeling Score (LMS) measur-\ning the percentage of times a meaningful token is\npredicted for the mask as opposed to a meaningless\ntoken, the Stereotype Score (SS) measuring the per-\ncentage of times the model predicted a stereotypical\nword as compared to an anti-stereotypical word, and\nan idealized CAT score (ICAT) combining the LMS\nand SS score into a single metric. An ideal model\nhas an ICAT score of 100, while the worst biased\nmodel has an ICAT score of 0. We additionally\nevaluate the CrowS-Pairs benchmark (Nangia et al.,\n2020a), which captures data with greater diversity\nin both the stereotypes expressed and the structure\nof sentences (50 is ideal). However, we note that the\nCrow-S benchmark is much more limited compared\nto StereoSet (Nadeem et al., 2021) in terms of both\nthe volume and variety of linguistic phenomenon\nrelating to gender bias it covers.\n2An English language dataset (Creative Commons\nAttribution-ShareAlike License).\n3An English language dataset available at bias-bench (Cre-\native Commons Attribution-ShareAlike 4.0 International Public\nLicense)\n6 Experiments\nWe compare our proposed interventions with five\nbaselines, 4 of which are state-of-the-art methods\nand the original pre-trained model. Our first base-\nline is the application of dropouts to neural net-\nworks, Dropout proposed by (Webster et al., 2020).\nNext, we consider an algorithmic de-biasing tech-\nnique INLP technique proposed by (Ravfogel et al.,\n2020). Then, we consider a sentence embedding\nde-biasing approach SentenceDebias (Liang et al.,\n2020). Finally, we consider a data-based approach\nCDA (Zmigrod et al., 2019) that is closest to our\nwork. For a fairer comparison, we run the baselines\nwith the same size (100) of the training set as our\nmethod. For all of our experiments, we consider\nthe “bert-base-uncased” pre-trained model available\nfrom HuggingFace. For fine-tuning our model, we\nselect a varying number of most-biased training\nsamples (10, 50, and 100) from the WikiText-2 and\nStereoSet (we only use the dev set) datasets, as\ndiscussed in section 4. We also compare this to a\nrandom selection of data points as an ablation study.\nOn the selected dataset, we apply our interventions\nand obtain the modified dataset, which is then used\nto fine-tune our pre-trained model using masked lan-\nguage modeling (MLM) loss. The key point is that\nwe only fine-tune the model on the gender words\nconditioned on the remaining text, significantly re-\nducing the fine-tuning time. We perform ablations\non various types of interventions as discussed in\nTable 7. The model is trained for 30 epochs, with a\nlearning rate of 0.001 and AdamW optimizer. We\nran all of our experiments on NVIDIA Tesla T4\nGPU on Google Colab for roughly 48 hours. For\nall experiments, we report the numbers as the mean\nand standard deviations (6) of 3 different runs. Our\nexperiment code can be found here.4\n7 Results\nTable 4 shows the StereoSet and Crow-S scores for\nour baselines and our best-performing interventions\non the WikiText-2 Dataset. In the StereoSet bench-\nmark, we observe that random-phrase-masking\nobtains lower SS than all other baselines. On\nthe Crow-S benchmark, random-phrase-masking\ndoes better than thre of the baselines except Sen-\ntenceDebias which achieves slightly better scores.\nWhile random-phrase-masking results in lower\nSS scores than neutral-masking, it also obtained\n4https://github.com/himansh005/data_debias\n343\n[MASK] is very good at cooking but not great at [MASK] work.\nshe is very good at cooking but not great at her work. \nhe is very good at cooking but not great at farm work. \nInput Sentence:\nOutput of Biased Model:\nOutput of De-biased Model:\nBeing a [MASK] is not easy since [MASK] will have to stay home\nand take care of [MASK] child. \nInput Sentence:\nOutput of Biased Model:\nOutput of De-biased Model:\nBeing a mother is not easy since she will have to stay home\nand take care of the child.\nBeing a father is not easy since one will have to stay home\nand take care of the child.\n[MASK] is very caring and kind but not good at what [MASK] does.\nShe is very caring and kind but not good at what she does. \nHe is very caring and kind but not good at what he does. \nInput Sentence:\nOutput of Biased Model:\nOutput of De-biased Model:\nFigure 2: Qualitative analysis of our approach on fill-mask task shows that our intervention techniques are able to\nmodify stereotypical sentences. In the this example, we prompted a pre-trained bert-base-uncased model and the\nsame pre-trained model debiased using random-phrase-masking with stereotypical sentences and found that the\nour method is successfully able to reduced biased substitutions.\nStereoSet ScroresType Method SS (↓) LMS (↑)ICAT (↑)\nCrow-S\nScores (↓)\nNone 60.27984.17270.30057.250\nCDA 60.02283.46670.89256.107\nDropout 60.52983.81170.17155.977\nSentenceDebias 59.22184.16671.30853.817\nBaselinesINLP 58.20583.39170.96655.727\nrandom-phrase-masking(10)59.44280.31270.40654.580\nrandom-phrase-masking58.03778.67669.94954.457\nneutral-masking(10) 60.34183.95672.54855.535\nOursneutral-masking 60.81483.58772.21356.490\nTable 4: StereoSet and Crow-S benchmark results on\nWikiText-2 dataset. A lower SS and Crow-S score means\nlesser gender bias while higher ICAT and LMS denote\nbetter language modelling ability. The number in the\nparentheses denotes number of training samples and ones\nwithout it use 100.\nvery low LMS scores. We attribute this per-\nformance degradation to the blunt substitution\nof phrases that our method uses, which might\nlead to odd-sounding sentences. In the Crow-\nS benchmarks, we see similar behavior and find\nthat random-phrase-masking does better than\nneutral-masking. Since we believe that our\nmethod is sensitive to the choice of the dataset,\nwe also present results on the StereoSet (dev)\ndataset 6. In Figure 2, we perform a qualitative\nanalysis of our proposed approach and find that\nrandom-phrase-masking is able to flip the predic-\ntions on fill-mask tasks for stereotypical sentences.\n8 Conclusion\nIn this paper, we show that simple data interventions\non limited training data effectively reduce gender\nbias in LLMs. We also show that a biased pre-\ntrained LLM can be used to mine the most effective\nde-biasing training examples. Evaluation of our\nmethods on state-of-the-art bias benchmarks empir-\nically suggests that our methods effectively reduce\ngender bias. Given that our methods can work in a\nfew-shot manner and do not require any auxiliary\nmodel training, we hope that our work benefits fur-\nther research in the domain of human-in-the-loop\nbias mitigation techniques by making the creation\nof bias mitigation datasets feasible.\n9 Limitations\nOur proposed method has the following main lim-\nitations which we believe are important directions\nfor future work to address:\n1. Gender dependency:Our approach does not\naccount for sentences that only make sense for\na single gender. For example, sentences like\n\"She needs to see a gynecologist\" would not\nbe captured by our method. This is a com-\nmon problem encountered by most debiasing\nalgorithms as it is difficult to distinguish these.\n2. Finite wordlist:The wordlist does not contain\nall gender-based words as the language con-\n344\ntinues to evolve. We believe that future works\ncould employ better approaches that can au-\ntomatically mine gender words relevant to a\ndataset.\n3. Blunt substitution: The phrase substitution\nmethod is an improvement over direct word\nsubstitution, but there are still plenty of in-\nstances where the new sentence might be se-\nmantically incorrect. This does not have any\nmajor implication on inference as we are only\ndoing few-shot learning, but it should not be\nextended to the entire dataset.\n4. Binary gender:The method only focuses on\nthe male and female gender. It does not con-\nsider non-binary or gender-neutral pronouns\nsuch as \"ze/hir.\" This can be solved by using\nan updated wordlist, but the authors could not\ncome across one at the time of writing.\n5. Downstream analyses:While our work pro-\nposes methods that show reduced gender bias\nas per a set of metrics, the work in no way\nclaims to reduce gender bias in general, es-\npecially on downstream tasks. However, we\nstrongly believe that this technique holds po-\ntential to reduce gender bias on downstream\ntasks as well since we adopt a regular fine-\ntuning approach and focus mainly on better\ndata interventions. Moreover, recent research\nhas shown that fine-tuning-based debiasing ap-\nproaches do not damage a model’s internal rep-\nresentations to a critical extent (Meade et al.,\n2022).\nOverall, these limitations suggest that our ap-\nproach may not be suitable for use in contexts where\ngender-specific or non-binary language is prevalent,\nand the underlying wordlist should be frequently\nupdated.\n10 Ethics Statement\nThis study was conducted in accordance with ethical\nprinciples and guidelines. The study was designed\nto provide beneficial knowledge and not harm any\ngroup or individual. We recognize that the wordlist\nwe use might not represent all contexts of gender\nbias and that our debiasing method does not cover\nall contexts of occurrences of gender bias. However,\nwe made sure to consider the ethical implications\nof our methodologies and the results of our anal-\nysis. The authors have tried to ensure the method\ndoes not amplify any other inherent bias but also ac-\nknowledge that our approach may have limitations.\nWe take responsibility for any ethical concerns that\nmay arise as a result of our research.\nAcknowledgments\nThis material is based upon work partially sup-\nported by the National Science Foundation (Awards\n#1722822 and #1750439) and National Institutes of\nHealth (Awards #R01MH125740, #R01MH096951,\nand #U01MH116925). PPL is partially supported\nby a Facebook PhD Fellowship and a Carnegie Mel-\nlon University’s Center for Machine Learning and\nHealth Fellowship. Any opinions, findings, conclu-\nsions, or recommendations expressed in this mate-\nrial are those of the author(s) and do not necessarily\nreflect the views of the NSF, NIH, Facebook, or\nCMLH, and no official endorsement should be in-\nferred. Additionally, we express our appreciation\nto the anonymous reviewers for their insightful sug-\ngestions, which greatly improved our work. Further-\nmore, we would like to acknowledge the contribu-\ntions of our colleagues, Atishay Jain and Praneetha\nVaddamanu, who played a significant role in the\ndevelopment of this research.\nReferences\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\nHanna Wallach. 2017. The problem with bias: Alloca-\ntive versus representational harms in machine learning.\nIn 9th Annual conference of the special interest group\nfor computing, information and society.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai. 2016.\nMan is to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In Advances in\nNeural Information Processing Systems 29: Annual\nConference on Neural Information Processing Sys-\ntems 2016, December 5-10, 2016, Barcelona, Spain,\npages 4349–4357.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages\n169–174, Brussels, Belgium. Association for Compu-\ntational Linguistics.\nSunipa Dev, Tao Li, Jeff M Phillips, and Vivek Srikumar.\n2021. OSCaR: Orthogonal subspace correction and\nrectification of biases in word embeddings. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 5034–\n345\n5050, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nJacqueline He, Mengzhou Xia, Christiane Fellbaum,\nand Danqi Chen. 2022. Mabel: Attenuating gender\nbias using textual entailment data. ArXiv preprint,\nabs/2210.14975.\nSophie Jentzsch and Cigdem Turan. 2022. Gender bias\nin BERT - measuring and analysing biases through\nsentiment rating in a realistic downstream classifica-\ntion task. In Proceedings of the 4th Workshop on Gen-\nder Bias in Natural Language Processing (GeBNLP),\npages 184–199, Seattle, Washington. Association for\nComputational Linguistics.\nHannah Rose Kirk, Yennie Jun, Filippo V olpin, Haider\nIqbal, Elias Benussi, Frédéric A. Dreyer, Aleksandar\nShtedritski, and Yuki M. Asano. 2021. Bias out-of-\nthe-box: An empirical analysis of intersectional occu-\npational biases in popular generative language models.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 2611–2624.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in contex-\ntualized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 166–172, Florence, Italy. Associa-\ntion for Computational Linguistics.\nDavid D. Lewis and William A. Gale. 1994. A sequential\nalgorithm for training text classifiers. In SIGIR ’94,\npages 3–12, London. Springer London.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5502–5515, Online. Association for\nComputational Linguistics.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understanding\nand mitigating social biases in language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 6565–6576. PMLR.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2018. Gender bias in\nneural natural language processing.\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell, and\nSimone Teufel. 2019. It’s all in the name: Mitigating\ngender bias with name-based counterfactual data sub-\nstitution. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5267–5275, Hong Kong, China. Association for Com-\nputational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020a. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020b. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models.\n346\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell.\n2018. Mitigating unwanted biases with adversarial\nlearning. ArXiv preprint, abs/1801.07593.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers), pages 629–634, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nVictor Zimmermann and Maja Hoffmann. 2022. Ab-\nsinth: A small world approach to word sense induc-\ntion. In Proceedings of the 18th Conference on Nat-\nural Language Processing (KONVENS 2022), pages\n121–128, Potsdam, Germany. KONVENS 2022 Orga-\nnizers.\nRan Zmigrod, Sabrina J. Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmen-\ntation for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1651–1661, Florence, Italy. Associ-\nation for Computational Linguistics.\nA Appendix\nA.1 Dataset Bias Analysis\nTo gauge the feasibility of using a wordlist based\nintervention approach, we first analyze our datasets\nfor occurrences of gender words. As shown in\nthe word cloud 4, gender pronouns are the most-\nfrequent word in our datasets. Moreover, as per Fig-\nure 1, \"she,\" \"he,\" and \"her\" are the top three most\nfrequently occurring words in our dataset. This sug-\ngests that we can definitely detect gender words in\nour corpus and apply our interventions.\nFigure 3: Frequency of gender words on the StereoSet\ndataset.\nFigure 4: Top 10 most frequent gender words on the\nStereoSet dataset.\nA.2 Sensitivity to Choice of Dataset\nTo understand the effectiveness of our proposed\ndata-interventions, we study apply our methods to\ntwo datasets under varying number of training sam-\nples (10, 50 and 100) and selection strategies (most\nbiased first and random) as per Table 6. Our meth-\nods obtain better results on StereoSet (dev) dataset.\nOne reason this could happen is due to the fact that\nStereoSet has explicit gender bias, thus it would be\nless likely for a sentence like \"She needs a gynae-\ncologist\" to appear on it. Because our interventions\nperform blunt substitutions, this sentence might be-\ncome incorrect due to our method - \"Either he or\nshe needs a gynaecologist\".\nA.3 Sensitivity to Number of Training Samples\nand Sampling Strategy\n10 50 100\nNumber of training samples\n58\n59\n60StereoSet Gender SS\nsample_method\nmost-biased\nrandom\nFigure 5: StereoSet Gender SS scores on StereoSet (dev)\ndataset with varying number of training samples. Com-\npared to random selection of samples, selecting most-\nbiased samples help achieve lower SS scores.\nAs per Figure 5, When we vary the number of train-\ning samples, we observe that the difference in per-\nformance is not huge when we transition from 10\nto 100 samples, thus suggesting that our method\n347\nSentences\nMean\nConfidence\nDifference\nShe rushed to see what he wanted and said she loved him.\nShe punched him in the face and told him to go away. 6.85\nJessica is a new mommy.\nJessica finds being a mother does not come easy to her.\nShe will no longer work so she can stay home and take care of her child.\n6.34\nThe little girl missed her mommy.\nShe missed watching her cook in the kitchen while wearing a floral apron.\nShe was never home because she worked long hours in the oil field.\n4.70\nTable 5: Sentences from StereoSet with maximum difference in confidence of prediction between opposite gender\nwords.\nDataset SamplingMethodNumber ofSamples Crow-S Pair Score StereoSet Scores PerplexityTotal Stereotype Score Anti-stereotype Scoe SS (gender) LMS (gender) ICAT (gender)\nStereoSet\nmost-biased 10 54.481 (2.583) 50.408 (5.295) 60.991 (3.854) 58.736 (1.215) 80.858 (2.988) 66.708 (2.584) 50.449 (54.983)\nrandom 10 55.47 (3.247) 50.527 (3.632) 63.107 (4.234) 58.952 (0.859) 80.226 (2.85) 65.862 (2.655) 86.024 (107.709)\nmost-biased 50 52.994 (1.894)47.567 (4.564) 61.428 (5.25) 58.498 (1.19) 80.255 (2.428) 66.595 (2.207) 29.599 (28.648)\nrandom 50 53.817 (1.011) 50.107 (2.972)59.547 (3.925) 58.485 (0.758) 79.158 (1.992) 65.707 (0.886) 62.498 (11.593)\nmost-biased 100 53.054 (2.402) 49.063 (6.025) 59.291 (4.663) 58.071 (1.158) 81.086 (3.226) 67.972 (2.671)19.079 (14.095)\nrandom 100 53.563 (1.801) 48.113 (6.499) 62.137 (5.405)57.719 (1.94)79.038 (1.406) 66.805 (2.074) 34.826 (12.109)\nWikiText-2\nmost-biased 10 55.6 (3.06) 54.668 (5.606) 57.118 (1.671) 59.344 (0.742) 84.624 (2.134)68.811 (2.176)87.06 (80.998)\nrandom 10 56.617 (1.344) 57.983 (1.305)54.693 (2.019)60.616 (0.72)85.076 (0.896)67.021 (1.895) 59.901 (102.019)\nmost-biased 50 54.276 (1.513) 53.394 (3.847) 55.834 (2.652) 59.238 (1.068) 83.348 (3.003) 67.902 (0.977) 212.365 (155.526)\nrandom 50 54.2 (2.383) 51.783 (5.272) 57.93 (2.969) 59.611 (1.155) 83.456 (1.9) 67.386 (0.74) 116.872 (100.401)\nmost-biased 100 55.473 (1.42) 54.827 (4.255) 56.637 (4.329) 59.426 (1.719) 83.442 (3.185) 67.629 (1.178) 220.957 (207.243)\nrandom 100 54.457 (1.444) 51.363 (4.283) 59.223 (4.451) 59.545 (0.387) 81.953 (1.442) 66.3 (0.597) 326.017 (181.822)\nTable 6: StereoSet and Crow-S scores for random-phrase-masking method on two datasets, 3 sample sizes and 2\nselection methods. We report mean (standard deviation) across 3 different runs. Selecting most-biased samples and\nusing the StereoSet dataset for fine-tuning gives best results.\nName WordMask MethodNon-Name WordMask Method Crow-S Pairs StereoSet PerplexityTotal Stereotype Score Anti-Stereotype Score SS LMS ICAT\nfemale-first-random-phrase-masking53.18 (3.106) 49.06 (6.627) 59.55 (3.678) 58.283 (0.4) 79.059 (0.436) 65.96 (0.27) 26.3 (9.545)\nnaive-masking 50.637 (0.585)43.607 (1.449) 61.49 (1.486) 59.521 (0.458) 83.325 (0.62) 67.456 (0.414)1.0 (0.0)\nnaive-maskingrandom-phrase-masking52.673 (1.374) 49.057 (5.998) 58.253 (5.906) 58.05 (0.851) 78.218 (0.633) 65.618 (0.937) 30.045 (8.019)\nneutral-maskingfemale-first-random-phrase-masking53.44 (0.0) 53.46 (0.891)53.4 (1.372)58.246 (0.285)87.182 (0.391) 72.806 (0.823)11.39 (6.649)\nrandom-phrase-masking54.195 (1.619) 48.43 (0.891) 63.11 (2.744) 57.316 (0.164) 78.339 (0.196) 66.877 (0.424) 54.413 (0.212)\nfixed-phrase-masking-153.307 (1.761) 46.837 (8.494) 63.43 (8.807) 57.688 (1.718) 79.554 (0.17) 67.32 (2.64) 14.484 (1.512)\nfixed-phrase-masking-251.783 (4.965) 46.43 (10.381) 60.193 (3.503) 57.229 (1.739) 80.551 (1.251) 68.879 (1.882) 13.374 (1.174)\nfixed-phrase-masking-352.927 (1.541) 48.317 (3.78) 60.193 (4.234)56.963 (1.373)79.3 (1.531) 68.284 (3.478) 15.546 (2.997)\nfixed-phrase-masking-453.567 (4.186)50.083 (9.006)59.223 (3.885) 58.13 (1.208) 79.834 (0.533) 66.86 (2.309) 14.51 (1.339)\nTable 7: StereoSet Gender SS scores on StereoSet (dev) dataset on 100 samples across various interventions techniques.\nAll numbers are reported as mean and standard deviation across 3 runs.\n348\nis capable of few-shot fine-tuning. Moreover, sam-\npling the most biased data points helps our methods\nachieve better performance consistently, as shown\nin Figure 5 and Table 6. Table ?? shows some top\nthree most gender biased entries found in the Stere-\noSet dataset.\nA.4 Ablations of interventions\nWe study the effects of choosing different ways\nof replacement for name and non-name words. In\naddition to our three interventions proposed previ-\nously, we also experimented with a couple of oth-\ners. In female-first-random-phrase-masking,\nwe always keep the female gendered word before\na male word. We wanted to understand if the\norder of gender words encountered by a model\nrenders any effect on the debiasing. In Table 7,\nwe see that it does not perform any better than\nrandom-phrase-masking. Then, we also try fixing\nthe phrases from random-phrase-masking, thus\nmaking it fixed-phrase-masking. We obtain 4\nvariants of this method corresponding to the follow-\ning four phrases:\n1. both [1] and [2]\n2. [1] and [2]\n3. [1] or [2]\n4. either [1] or [2]\nHere, [1] and [2] are substituted with oppo-\nsite gender words. As we observe in Table\n7, fixed-phrase-masking-3 obtains the lowest\nStereoSet Gender SS score out of all our interven-\ntion methods. Similarily, naive-masking obtains\nthe lowest Crow-S pair score.\n349\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n9\n□\u0013 A2. Did you discuss any potential risks of your work?\n10\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe datasets used are open-source.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data is a general vocabulary. Hence, none of the used data contains any personal information.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThe dataset pages have all the needed information\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n7\nC □\u0013 Did you run computational experiments?\n6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n350\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n6\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n6\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n351",
  "topic": "Psychological intervention",
  "concepts": [
    {
      "name": "Psychological intervention",
      "score": 0.5774232149124146
    },
    {
      "name": "Gender bias",
      "score": 0.5486645102500916
    },
    {
      "name": "Computer science",
      "score": 0.4822900593280792
    },
    {
      "name": "Shot (pellet)",
      "score": 0.48216646909713745
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4757796823978424
    },
    {
      "name": "Psychology",
      "score": 0.3231685757637024
    },
    {
      "name": "Social psychology",
      "score": 0.12749627232551575
    },
    {
      "name": "Physics",
      "score": 0.07261151075363159
    },
    {
      "name": "Chemistry",
      "score": 0.06788328289985657
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 21
}