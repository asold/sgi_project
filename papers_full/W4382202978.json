{
  "title": "SVP-T: A Shape-Level Variable-Position Transformer for Multivariate Time Series Classification",
  "url": "https://openalex.org/W4382202978",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2944017752",
      "name": "Rundong Zuo",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2097479351",
      "name": "Guo-Zhong Li",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2117040297",
      "name": "Byron Choi",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2168903744",
      "name": "Sourav S. Bhowmick",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A4202215219",
      "name": "Daphne Ngar-yin Mah",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A4213656105",
      "name": "Grace L.H. Wong",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2944017752",
      "name": "Rundong Zuo",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2097479351",
      "name": "Guo-Zhong Li",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2117040297",
      "name": "Byron Choi",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A2168903744",
      "name": "Sourav S. Bhowmick",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A4202215219",
      "name": "Daphne Ngar-yin Mah",
      "affiliations": [
        "Hong Kong Baptist University"
      ]
    },
    {
      "id": "https://openalex.org/A4213656105",
      "name": "Grace L.H. Wong",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6777047548",
    "https://openalex.org/W2555077524",
    "https://openalex.org/W3042807565",
    "https://openalex.org/W6966977965",
    "https://openalex.org/W1565746575",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3174543847",
    "https://openalex.org/W2920777619",
    "https://openalex.org/W4285607140",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2295959708",
    "https://openalex.org/W3034999176",
    "https://openalex.org/W2783323081",
    "https://openalex.org/W3173748501",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W3115948762",
    "https://openalex.org/W3010158807",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W2969225566",
    "https://openalex.org/W1789163674",
    "https://openalex.org/W3092172514",
    "https://openalex.org/W2998010409",
    "https://openalex.org/W1978383016",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2971121529",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W4289360400",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4293477908",
    "https://openalex.org/W3112330479",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W3190152617",
    "https://openalex.org/W2969416295",
    "https://openalex.org/W3177318507"
  ],
  "abstract": "Multivariate time series classiﬁcation (MTSC), one of the most fundamental time series applications, has not only gained substantial research attentions but has also emerged in many real-life applications. Recently, using transformers to solve MTSC has been reported. However, current transformer-based methods take data points of individual timestamps as inputs (timestamp-level), which only capture the temporal dependencies, not the dependencies among variables. In this paper, we propose a novel method, called SVP-T. Specifically, we ﬁrst propose to take time series subsequences, which can be from different variables and positions (time interval), as the inputs (shape-level). The temporal and variable dependencies are both handled by capturing the long- and short-term dependencies among shapes. Second, we propose a variable-position encoding layer (VP-layer) to utilize both the variable and position information of each shape. Third, we introduce a novel VP-based (Variable-Position) self-attention mechanism to allow the enhancing the attention weights of overlapping shapes. We evaluate our method on all UEA MTS datasets. SVP-T achieves the best accuracy rank when compared with several competitive state-of-the-art methods. Furthermore, we demonstrate the effectiveness of the VP-layer and the VP-based self-attention mechanism. Finally, we present one case study to interpret the result of SVP-T.",
  "full_text": "SVP-T : A Shape-Level Variable-Position Transformer for\nMultivariate Time Series Classiﬁcation\nRundong Zuo1, Guozhong Li1, Byron Choi1, Sourav S Bhowmick2,\nDaphne Ngar-yin Mah3, Grace L.H. Wong4\n1 Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR\n2 School of Computing Engineering, Nanyang Technological University, Singapore\n3 Department of Geography, Hong Kong Baptist University, Hong Kong SAR\n4 Department of Medicine and Therapeutics, The Chinese University of Hong Kong, Hong Kong SAR\nfcsrdzuo, csgzli, bchoig@comp.hkbu.edu.hk, assourav@ntu.edu.sg, daphnemah@hkbu.edu.hk, wonglaihung@cuhk.edu.hk\nAbstract\nMultivariate time series classiﬁcation ( MTSC ), one of the\nmost fundamental time series applications, has not only\ngained substantial research attentions but has also emerged\nin many real-life applications. Recently, using transformers to\nsolve MTSC has been reported. However, current transformer-\nbased methods take data points of individual timestamps as\ninputs (timestamp-level), which only capture the temporal de-\npendencies, not the dependencies among variables. In this pa-\nper, we propose a novel method, called SVP-T. Speciﬁcally,\nwe ﬁrst propose to take time series subsequences, which can\nbe from different variables and positions (time interval), as\nthe inputs (shape-level). The temporal and variable dependen-\ncies are both handled by capturing the long- and short-term\ndependencies among shapes. Second, we propose a variable-\nposition encoding layer (VP-layer) to utilize both thevariable\nand position information of each shape. Third, we introduce a\nnovel VP-based (Variable-Position) self-attention mechanism\nto allow the enhancing of the attention weights of overlapping\nshapes. We evaluate our method on all UEA MTS datasets.\nSVP-T achieves the best accuracy rank compared with sev-\neral competitive state-of-the-art methods. Furthermore, we\ndemonstrate the effectiveness of the VP-layer and the VP-\nbased self-attention mechanism. Finally, we present one case\nstudy to interpret the result of SVP-T.\n1 Introduction\nMultivariate time series ( MTS ), recording a group of vari-\nables at each timestamp, is ubiquitous in diverse domains,\nsuch as economics, smart city, medicine, and astronau-\ntics (Palpanas 2015). Time series classiﬁcation is one of the\nmost fundamental time series applications (Bagnall et al.\n2017). However, compared to univariate time series clas-\nsiﬁcation ( UTSC ) (Bagnall et al. 2017; Shifaz et al. 2020),\nmultivariate time series classiﬁcation ( MTSC ) has received\nless research attention (Li et al. 2021). One of the key is-\nsues for MTSC is to capture the interactions among differ-\nent variables (Ruiz et al. 2020), e.g., one activity (play-\ning badminton) determined by several shapes from different\nvariables around the same time in human activity recogni-\ntion (Bagnall et al. 2018).\nCopyright © 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nSeveral methods have been proposed to solve the issue of\nsuch interactions of variables. (Hao and Cao 2020) proposed\ntwo novel cross-attention modules on their network. How-\never, the model starts with a convolution operation with local\nneighborhood information layer by layer, which is known\nto be prone to the position-dependent prior bias on vari-\nables. ShapeNet captures the potential interactions between\nvariables by learning subsequence representations (Li et al.\n2021). Nevertheless, the representations do not consider the\ninformation about the variables and the positions of subse-\nquences.\nRecently, transformer models have exhibited superior\nperformance in capturing long- and short-term dependen-\ncies in the applications of natural language processing\n(NLP) (Vaswani et al. 2017; Devlin et al. 2019). More-\nover, some transformer-based methods have been proposed\nfor time series applications. For instance, Informer employs\ntransformer for time series forecasting (Zhou et al. 2021)\nfor efﬁciently handling extremely long input sequences.\n(Zerveas et al. 2021) have proposed a transformer-based\nframework for time series representation learning for down-\nstream tasks (e.g., regression and classiﬁcation). Applying a\ntransformer on MTS avoids using a sequence-aligned recur-\nrent architecture, i.e., the models learn the temporal depen-\ndencies without position-dependent prior bias.\nChallenges. We note that the input token of previous\ntransformer-based methods (called timestamp-level trans-\nformer in this paper) forMTS is taken from one timestamp of\nall variables (Zhou et al. 2021; Zerveas et al. 2021), which\nmeans they only consider the temporal dependencies, and do\nnot yet consider the dependencies between variables, since\nthe transformer is for learning the dependencies among input\ntokens, not the dependencies within each individual token\n(shown in Figure 1(a)). Meanwhile, the input length of the\ntimestamp-level transformer depends on the length of time\nseries T, and thus the time complexity of the self-attention\nmechanism in the transformer is O(T2). Therefore, a time\nseries with a large T makes a transformer time-consuming.\nWe are the ﬁrst to take shapes as the input for the trans-\nformer (also called shape-level transformer in this paper,\nshown in Figure 1(b)) to handle both the temporal and\nvariable dependencies. When using these shapes as the in-\nput, there are two additional challenges. First, the variable\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n11497\n(a) LHS: the timestamp-level transformer; RHS: the dependencies\n(illustrated with curved lines) of all tokens (timestamps) to the\nﬁrst token learned by the transformer. The input token fed into the\ntimestamp-level transformer combines all variables at one timestamp,\nand thus the transformer only captures the temporal dependencies.\n(b) LHS: our shape-level transformer; RHS: the dependencies (il-\nlustrated with curved lines) of all tokens (shapes) to the ﬁrst to-\nken learned by the transformer. The input token of the shape-level\ntransformer is from one variable vi together with the corresponding\ntime interval (position pi). By capturing the dependencies between\nshapes, the dependencies of variables and positions are learned. The\nred line denotes the dependency between the1-st and 3-rd variables.\nFigure 1: (a) Timestamp-level input vs (b) our shape-level\ninput for transformer.\nand position (VP) information of the shapes have not been\nconsidered by the encoder of the transformer. Second, the\nshapes that appear almost simultaneously (overlap in time)\nare important to many applications, such as human activity\nrecognition (Zhou, De la Torre, and Hodgins 2012; Wang\net al. 2019), which can be speciﬁcally emphasized.\nContributions. In this paper, we propose a shape-level\nvariable-position transformer model, named SVP-T, to ad-\ndress the above-mentioned challenges. To the best of our\nknowledge, this is the ﬁrst work to take time series subse-\nquences as the input (shape-level) and consider their vari-\nable and position information in a transformer architecture.\nAn overview of SVP-T is presented in Figure 2.\nSVP-T takes the shapes from different variables and posi-\ntions as input. The novelty is that we1\rutilize shapes nearby\nthe cluster centers, obtained from a clustering algorithm,\nrather than each timestamp, as the input for SVP-T. Fig-\nure 1 shows the difference between a previous timestamp-\nlevel transformer (Zerveas et al. 2021) (Figure 1(a)) and our\nshape-level transformer, SVP-T (Figure 1(b)). Instead of us-\ning a ﬁxed position encoding in (Vaswani et al. 2017) or\na fully learnable position encoding in (Devlin et al. 2019;\nZerveas et al. 2021), we 2\rpropose a variable-position en-\ncoding layer (VP-layer) to speciﬁcally utilize the variable\nand position of our shapes, which makes the model sense the\ninput time series instance. Finally, we 3\rintroduce a novel\nVP-based (Variable-Position) self-attention mechanism to\nFigure 2: The overview of our method. 1\r Preprocess-\ning. A variable-wise clustering method discovers L shapes\nof each instance as the input tokens (Section 3.1). 2\r\nSVP-T. A VP-layer encodes variable-position informa-\ntion of shapes (Section 3.2) and a VP-based self-attention\nmechanism in the transformer encoder which captures\nlong- and short-dependencies between shapes (Section 3.3).\nfS1;S2;\u0001\u0001\u0001 ;SLgare the shapes shown in Figure 1(b).\nenable enhancement of the attention weights of overlapping\nshapes for MTSC . As a proof of concept, we propose to in-\ncrease the attention weight of two shapes that are close in\ntime but from different variables.\nWe conduct experiments on all datasets of the UEA\nMTS archive. The evaluation shows that our SVP-T outper-\nforms 12 baseline methods in terms of accuracy rank and the\nadditional experiments show the effectiveness of our pro-\nposed modules. Furthermore, we perform a case study to\nshow how our SVP-T handles input shapes by analyzing\nthe internal representation.\nOrganization. The rest of this paper is organized as follows.\nSection 2 presents the related work onMTS . The deﬁnition of\nMTS and details of SVP-T are introduced in Section 3. Sec-\ntion 4 reports the experimental results and one case study.\nSection 5 concludes the paper and presents avenues for fu-\nture work.\n2 Related Work\nIn this section, we present the existing methods for\nMTSC and the transformer-based methods for time series.\n2.1 Multivariate Time Series Classiﬁcation\nWe categorize the MTSC methods into two types, namely\ntraditional (non-deep learning) methods and deep learning\nones. For details of traditional methods, interested readers\ncan refer to an excellent survey paper (Ruiz et al. 2020).\nHence, we focus more on deep learning methods.\nLSTM-FCN (Karim et al. 2019) was proposed with an\nLSTM layer and stacked CNN layers to generate the fea-\ntures from time series. The features are then transmitted\n11498\nto a softmax layer to output the probability of each class.\n(Hao and Cao 2020) found an issue that CNN-based mod-\nels cannot capture the long dependencies well among differ-\nent variables. To handle the issue, they added two cross at-\ntention modules on their CNN-based model. TapNet applies\nLSTM, CNN, and attentional prototype learning techniques\nfor MTSC (Zhang et al. 2020). Furthermore, it utilizes un-\nlabeled data for semi-supervised learning and obtains better\nperformances. (Li et al. 2021) proposed ShapeNet to learn\nthe time series subsequences representation into a uniﬁed\nspace for the ﬁnal shapelet selection. However, they did not\nconsider the information of positions in time series subse-\nquences. RLPAM (Gao et al. 2022) ﬁrst converts MTS into\na univariate cluster sequence and then uses reinforcement\nlearning for selecting patterns.\n2.2 Transformer-Based Methods for Time Series\nThe transformer architecture has successfully been applied\nto important applications including natural language pro-\ncessing (Vaswani et al. 2017; Devlin et al. 2019) and com-\nputer vision (Dosovitskiy et al. 2021; Liu et al. 2021).\nRecently, there have been some successes in time series\napplications, such as forecasting (Wu et al. 2020; Lim et al.\n2021) and representation learning (Eldele et al. 2021). In-\nformer employs transformer for efﬁciently forecasting long\nsequence time series under long-tail distribution (Zhou et al.\n2021). TS-TCC (Eldele et al. 2021) utilizes transformer in\ntemporal contrasting to extract the temporal features for\nunsupervised time series representation learning. (Zerveas\net al. 2021) proposed a novel framework based on the trans-\nformer encoder to learn the representation of MTS for down-\nstream tasks (e.g., classiﬁcation and regression). The efﬁ-\nciency of this method is limited by the length of the time se-\nries and its performance is relatively weak in low dimension\ndatasets (Zerveas et al. 2021). We notice that all the previ-\nous transformer-based methods take the data at timestamps\nof the time series as the input (as shown in Figure 1(a)).\n3 Methods\nIn this section, we ﬁrst give some notations of multi-\nvariate time series. Then we propose a novel shape-level\nvariable-position transformer, called SVP-T, for MTSC .\nSpeciﬁcally, we propose shape inputs for the transformer,\na variable-position encoding layer, and a VP-based self-\nattention mechanism.\nMultivariate Time Series (MTS ). Prior to the technical dis-\ncussions, we provide some notations here. We denoteMTS as\nX 2 RV\u0002T, where V is the number of variables and\nT is the series length of each variable. Speciﬁcally, X =\b\nX1;X2;\u0001\u0001\u0001 ;XV\t\n, where Xv is the time series of the v-\nth variable. Xv = (xv\n1;xv\n2;\u0001\u0001\u0001 ;xv\nT), where xv\nt is the value\nof the t-th timestamp in the v-th variable. An MTS dataset\nshown in the top left of Figure 2 consists ofN instances and\neach instance has one corresponding class label.\n3.1 Shapes Input for Transformer\nAn excellent review paper (Ruiz et al. 2020) demon-\nstrates that the discriminative time series subsequences (e.g.,\ninterval-based, dictionary-based, and shapelet-based meth-\nods) signiﬁcantly improve the accuracy performance of\nMTSC . Inspired by the aforementioned results, we transform\noriginal time series into shapes as the input for our SVP-T.\nWe follow the existing works (Zakaria et al. 2016; Grabocka,\nWistuba, and Schmidt-Thieme 2016; Li et al. 2021) to apply\nclustering for discovering shapes.\nFor self-containedness, we describe how shapes are gen-\nerated as follows. First, we generate numerous time series\nsubsequences for all instances. Second, we employ a clas-\nsic clustering algorithm (e.g., kmeans) based on Euclidean\nDistance to each variable. The K cluster centers for vari-\nable v are Cv = fCv\n1 ;Cv\n2 ;\u0001\u0001\u0001 ;Cv\nKg, which are represen-\ntative of a number of time series subsequences of v. The\nnearest time series subsequences Sv = fSv\n1 ;Sv\n2 ;\u0001\u0001\u0001 ;Sv\nKg\nto Cv are the tokens as input shapes of SVP-T. After that,\none MTS instance X is transformed into a set of shapes\nS = fS1;S2;\u0001\u0001\u0001 ;SLg, where L= K\u0002V.\nFrom Figure 1(a), we can observe that the input length of\nprevious transformer-based methods are determined by the\nlength of MTS (namely, T). As the time complexity of the\nself-attention mechanism is O(T2) (Vaswani et al. 2017),\nthis would be inefﬁcient in handling long time series in a\ntimestamp-level transformer. In contrast, L in our shape-\nlevel transformer is tunable. From our experiments shown in\nFigure 7, we achieve good accuracy whenLis much smaller\nthan T (e.g., EigenWorms, and StandWalkJump).\n3.2 Variable-Position Encoding Layer\nSince the transformer model contains no recurrence and no\nconvolution, a ﬁxed sinusoidal position encoding, is pro-\nposed to capture the input sequence in the seminal work of\ntransformer (Vaswani et al. 2017). BERT (Devlin et al. 2019)\nproves that a fully learnable position encoding performs bet-\nter. Both the ﬁxed position encoding and fully learnable po-\nsition encoding utilize the order of sequence for input to-\nkens, i.e., word embeddings in NLP (Vaswani et al. 2017).\nHowever, such order of sequence may not carry the seman-\ntics for the need of other applications (Shiv and Quirk 2019).\nAs we mentioned in Section 3.1, the input tokens of SVP-\nT are the shapes. In the “shapes-as-input-tokens” situation,\nthe order of sequence for shapes is less important when com-\npared to the relative positions of the shapes in the original\ntime series instance, which are the corresponding variables\nand time intervals (called VP information). An example of\nVP information for input shapes is illustrated in Figure 3.\nTo include such VP information in learning, we propose\na variable-position encoding layer (VP-layer) in SVP-T,\nwhich is shown in the middle right-hand side of Figure 2.\nThe input of the VP-layer is the VP information. Specif-\nically, for the i-th shape Si, the corresponding VP informa-\ntion Pi is deﬁned as follows:\nPi = (vi\nV ;ti;start\nT ;ti;end\nT ) (1)\nwhere vi, ti;start and ti;end are the variable, the ﬁrst times-\ntamp, and the last timestamp of Si, respectively. We nor-\nmalize them to the range of [0;1] by dividing them with the\nnumber of variables V or their original time series length T.\n11499\n(a) The shapes S1, S2, S3, and S4 in their original time series\ninstance (Blue and orange are the two variables (1; 2) of the\npartial MTS instance from the BasicMotions dataset)\n(b) The VP information (Pi in Formula 1) of the shapes. The\nﬁrst, second, and third components are the ﬁrst timestamp, the\nlast timestamp, and the corresponding variable, respectively\n(The length T = 100and the number of variables V = 6).\nFigure 3: An example to illustrate VP information\nPi is transmitted into a linear layer with a trainable weight\nWp:\nP0\ni = PiWp;Wp 2R3\u0002dmodel (2)\nwhere dmodel is the dimension of the input in the trans-\nformer (Vaswani et al. 2017). The ﬁnal i-th input Ui of our\ntransformer encoder is:\nUi = P0\ni + WsSi (3)\nwhere Ws is the shared weight of a linear layer (shown in\nFigure 2: a linear projection of shapes).\n3.3 VP-Based Self-Attention Mechanism\nWe propose a VP-based self-attention mechanism for the\ntransformer encoder to capture the long- and short-term de-\npendencies between different shapes. The canonical self-\nattention transforms the input embedding U into three fea-\nture spaces Q, K, and V, where all of them are vec-\ntors (Vaswani et al. 2017). They are deﬁned as follows:\nQ= UWq;K = UWk;V = UWv (4)\nwhere Wq, Wk 2Rdmodel\u0002dk , Wv 2Rdmodel\u0002dv . For clar-\nity, we let d = dk = dv in this paper. The projection space\nQ, K, and V are called query, key, and value space, respec-\ntively. They are used to compute the attention by mapping a\nquery and a set of key-value pairs to an output. The attention\nis computed as follows:\nAttention(Q;K;V ) =softmax(QKT\np\nd\n)V (5)\nThe attention weights for the values are calculated as:\nScore = softmax(QKT\np\nd\n) (6)\n(a) (S1; S2) overlap\n (b) (S1; S2) do not overlap\nFigure 4: An illustration of overlapping calculation\n(shown in Formula 8). (S1;S2) are two shapes of dif-\nferent variables (v1;v2) from the BasicMotions dataset.\n(a) min(t1;end;t2;end) > max(t1;start;t2;start). (b)\nmin(t1;end;t2;end) <max(t1;start;t2;start).\nThere is, however, a special characteristic ofMTS datasets\nthat needs further consideration. When two shapes 1\rover-\nlap with each other and 2\rare from different variables, they\ntogether can be important shapes for MTSC and their atten-\ntion weight should be higher. It should be remarked that\nthe overlapping behavior has been successfully applied to\nthe 2D-image data in object detection (Girshick et al. 2014;\nRezatoﬁghi et al. 2019). We are the ﬁrst to utilize the cor-\nresponding characteristics for MTSC. Figure 4(a) shows an\nexample of overlapping shapes S1 and S2.\nIncorporating the above-mentioned characteristics into\nMTSC , we propose a VP-based self-attention mechanism,\nwhich allows the attention weights of these overlapping\nshapes to be explicitly enhanced. Our goal is to modify the\nattention weights matrix Score to Score0using a matrix M\ncalled the Overlapping-Enhancement matrix:\nScore0= softmax(Score\fM) (7)\nThe overlapping of two shapes Si, Sj is deﬁned as:\nOlap(Si;Sj) =\n8\n>><\n>>\n:\nmax(min(ti;end;tj;end)\u0000\nmax(ti;start;tj;start);0) vi 6= vj\n0 vi = vj\n(8)\nFor presentation simplicity, we use v, ti;start, and ti;end to\npresent vi\nV , ti;start\nT , and ti;end\nT , when T, V are not relevant to\nthe discussion.\nWe remark a special case where two shapes are from the\nsame variable, their overlapping part is repetitive and thus,\nOlap = 0. For shapes from different variables, we give an\nexample of overlapping shapes in Figure 4.\nExample 1. (S1;S2) are two time series subsequences from\ntwo variables. There are two cases.1\rWhen there is an\noverlapping between (S1;S2), namely, min(t1;end;t2;end)\nis larger than max(t1;start;t2;start) as shown in Fig-\nure 4(a). Their Olap is then larger than 0. 2\r When\nthere is no overlapping between (S1;S2), namely,\nmin(t1;end;t2;end) is smaller than max(t1;start;t2;start),\nas shown in Figure 4(b). TheirOlap is 0.\nFinally, one element M(i;j) of the Overlapping-\nEnhancement matrix is calculated by:\nM(i;j) =\u000berelu(Olap(Si;Sj)\u0000\f) (9)\n11500\nFigure 5: An example of where the attention weight should\nbe increased. There are three circumstances. 1\r(S1;S2),\nthey overlap in time, but they come from the same variable.\n2\rboth (S1;S4) and (S1;S5), they do not overlap in time.\n3\r(S1;S3), they overlap in time and come from different\nvariables. The dash line is the overlapping part of (S1;S2).\nOnly the attention weight of 3\rshould be increased.\nwhere \u000b \u0015 1, and \f 2 [0;1]. If the overlapping of two\nshapes (Si;Sj) is larger than \f, the relu function leads it to\nstay the same and the result of M(i;j) would be larger than\none, which ampliﬁes the corresponding attention weight.\nOtherwise, the relu makes it to be 0 and the attention weight\nremains unchanged. That is, no matter how far two shapes\nare in position, if their overlapping is less than \f, the self-\nattention mechanism would learn the long- and short-term\ndependencies equally, as (S1;S4) and (S1;S5) shown in\nFigure 5. Meanwhile, if the above characteristic does not\nexist, we can set \f \u00151 and the self-attention mechanism\ncould be as before.\n4 Experiments\nIn this section, we evaluate the performance of SVP-T. We\nstart by introducing the experiment setting, including the\nenvironment, datasets, metrics, and the implementation de-\ntails. Then, we present the baseline methods compared in\nSection 4.2. In Section 4.3, we report the accuracies of all\nmethods. We investigate the effectiveness of our VP-layer\nand VP-based self-attention mechanism in Section 4.4. In\naddition, we conduct an experiment on how L inﬂuences\nthe classiﬁcation results in Section 4.5. Finally, we present a\ncase study of a dataset named BasicMotions.\n4.1 Experiment Setting\nEnvironment. All the experiments are implemented on a\nmachine with one Xeon Gold 6226 CPU @ 2.70GHz and\none NVIDIA Tesla V100S. Python 3.8, and Pytorch 1.10.0\nare used to build and train our model. Datasets. We eval-\nuate our method on a well-known benchmark of MTS , the\nUEA archive, which is consists of 30 different datasets. De-\ntails of these datasets can be found in (Bagnall et al. 2018).\nMetrics. We choose classiﬁcation accuracy as our evalua-\ntion metric. Meanwhile, we compute the average rank, the\nnumber of Top-1, Top-3, and Top-5 accuracy, the number of\nwins/draws/losses, to show the robustness of different meth-\nods. Finally, we conduct Friedman and Wilcoxon signed-\nrank test following the process in (Dem ˇsar 2006) to eval-\nuate whether the result is statistically signiﬁcant. Imple-\nmentation details. We set \u000b = 1:5 and \f = 0 in For-\nmula 9. Since the benchmark datasets are highly heteroge-\nneous, as well as the MTS data in nature, we follow the pre-\nvious work (Zerveas et al. 2021), that splits the training set\ninto two parts, 80% \u000020%. Then, we take the 20% part as\nthe validation set to tune the hyperparameters. When the hy-\nperparameters are ﬁxed, we train our model on the whole\ntraining set and ﬁnally, evaluate it on the ofﬁcial test set.\nOur tuned hyperparameters of all datasets are shown in Ap-\npendix A.1. We adopt batch normalization, instead of layer\nnormalization, for better performance on time series appli-\ncations (Zerveas et al. 2021).\n4.2 Baselines\nWe compare SVP-T with 12 methods and brieﬂy introduce\neach of them. Readers who are interested in them may re-\nfer to the original papers.Three benchmarks (Bagnall et al.\n2018) (EDI, DTWI , and DTWD ) are based on Euclidean\nDistance, dimension-independent dynamic time warping,\nand dimension-dependent dynamic time warping.MLSTM-\nFCNs (Karim et al. 2019) is a deep learning method\nfor MTS , which applies an LSTM layer and stacked CNN\nlayers to generate features. WEASEL-MUSE (Sch¨afer and\nLeser 2017) is a bag-of-pattern based approach which ex-\ntracts and represents features to words. Scalable Repre-\nsentation Learning (SRL) (Franceschi, Dieuleveut, and\nJaggi 2019) employs negative sampling techniques with an\nencoder-based architecture to learn the representation. Tap-\nNet (Zhang et al. 2020) is a recent model with an attentional\nprototype learning in its deep learning-based network for\nMTSC . ShapeNet (Li et al. 2021) projects the MTS subse-\nquences into a uniﬁed space and applies clustering to ﬁnd the\nshapelets. Rocket, MiniRocket (Dempster, Petitjean, and\nWebb 2020; Dempster, Schmidt, and Webb 2021) use ran-\ndom convolutional kernels to extract features from univari-\nate time series, and they extended their codes to MTSC . RL-\nPAM (Gao et al. 2022) introduces reinforcement learning to\nthe pattern mining of MTS . TStamp Transformer (Zerveas\net al. 2021) takes the values at each timestamp as the input\nfor a transformer encoder as Figure 1(a) shown, which is the\nbaseline for evaluating our idea of taking shapes as input.\n4.3 Performance Evaluation\nThe accuracies of all the baseline experimental results\nare taken from the original papers or the survey (Ruiz\net al. 2020) except for MiniRocket and TStamp Trans-\nformer (Zerveas et al. 2021). The details of them are shown\nin Appendix A.2. We set a ﬁxed random seed for repro-\nducibility. For consistency of presentation, we follow the re-\nsults of (Gao et al. 2022) to keep three decimal places.\nAs Table 1 shown, the overall accuracy of SVP-T outper-\nforms all the related methods. Speciﬁcally, the average rank\nof SVP-T is 4:017, which is the best among 13 methods.\nMeanwhile, the gap in terms of average rank between SVP-\nT and the runner-up, MiniRocket, is about 1, which shows a\nclear lead considering that the average ranks of MiniRocket\nand RLPAM are nearly the same (less than 0:1 difference).\nFor the number of top-1 accuracy, we ﬁnd that SVP-T is\nslightly lower than RLPAM and MiniRocket. However, the\nnumber of top-3 accuracies and the number of top-5 accura-\ncies of SVP-T are both higher than all of the other methods,\n11501\nEDI DTWI DTWD MLSTM\n-FCNs\nWEASEL\n+MUSE SRL T\napNet ShapeNet Rocket Mini\nRocket RLPAM TStamp\nTransformer Ours\nAWR 0.970 0.980 0.987 0.973\n0.990 0.987 0.987 0.987 0.996 0.992 0.923 0.983 0.993\nAF 0.267 0.267 0.220 0.267\n0.333 0.133 0.333 0.400 0.249 0.133 0.733 0.200 0.400\nBM 0.676 1.000 0.975 0.950 1.000\n1.000 1.000 1.000 0.990 1.000 1.000 0.975 1.000\nCT 0.964 0.969 0.989 0.985\n0.990 0.994 0.997 0.980 N/A 0.993 0.978 N/A 0.990\nCK 0.944 0.986 1.000 0.917 1.000 0.986 0.958 0.986 1.000 0.986 1.000 0.958 1.000\nDDG 0.275 0.550 0.600 0.675\n0.575 0.675 0.575 0.725 0.461 0.650 0.700 0.480 0.700\nEW 0.549 N/A 0.618 0.504\n0.890 0.878 0.489 0.878 0.863 0.962 0.908 N/A 0.923\nEP 0.666 0.978 0.964 0.761 1.000 0.957\n0.971 0.987 0.991 1.000 0.978 0.920 0.986\nER 0.133 0.914 0.929 0.133\n0.133 0.133 0.133 0.133 0.981 0.981 0.819 0.933 0.937\nEC 0.293 0.304 0.323 0.373\n0.430 0.236 0.323 0.312 0.447 0.468 0.369 0.337 0.331\nFD 0.519 0.000 0.529 0.545\n0.545 0.528 0.556 0.602 0.694 0.620 0.621 0.681 0.512\nFM 0.550 0.520 0.530 0.580\n0.490 0.540 0.530 0.580 0.553 0.550 0.640 0.776 0.600\nHMD 0.278 0.306 0.231 0.365\n0.365 0.270 0.378 0.338 0.446 0.392 0.635 0.608 0.392\nHW 0.200 0.316 0.286 0.286 0.605 0.533\n0.357 0.452 0.567 0.507 0.522 0.305 0.433\nHB 0.619 0.658 0.717 0.663\n0.727 0.737 0.751 0.756 0.718 0.771 0.779 0.712 0.790\nIW 0.128 N/A N/A 0.167\nN/A 0.160 0.208 0.250 N/A 0.595 0.352 0.684 0.184\nJV 0.924 0.959 0.949 0.976\n0.973 0.989 0.965 0.984 0.965 0.989 0.935 0.994 0.978\nLB 0.833 0.894 0.870 0.856\n0.878 0.867 0.850 0.856 0.906 0.922 0.794 0.844 0.883\nLSST 0.456 0.575 0.551 0.373\n0.590 0.558 0.568 0.590 0.632 0.643 0.643 0.381 0.666\nMI 0.510 N/A 0.500 0.510\n0.500 0.540 0.590 0.610 0.531 0.550 0.610 N/A 0.650\nNT 0.850 0.850 0.883 0.889\n0.870 0.944 0.939 0.883 0.885 0.928 0.950 0.900 0.906\nPD 0.705 0.939 0.977 0.978\n0.948 0.983 0.980 0.977 0.996 N/A 0.982 0.974 0.983\nPM 0.973 0.734 0.711 0.699 0.000\n0.688 0.751 0.751 0.856 0.522 0.632 0.919 0.867\nPH 0.104 0.151 0.151 0.110\n0.190 0.246 0.175 0.298 0.284 0.292 0.175 0.088 0.176\nRS 0.868 0.842 0.803 0.803 0.934 0.862\n0.868 0.882 0.928 0.868 0.868 0.829 0.842\nSCP1 0.771 0.765 0.775 0.874\n0.710 0.846 0.652 0.782 0.866 0.925 0.802 0.925 0.884\nSCP2 0.483 0.533 0.539 0.472\n0.460 0.556 0.550 0.578 0.514 0.522 0.632 0.589 0.600\nSAD 0.967 0.959 0.963 0.990\n0.982 0.956 0.983 0.975 0.630 0.620 0.621 0.993 0.986\nSWJ 0.200 0.333 0.200 0.067\n0.333 0.400 0.400 0.533 0.456 0.333 0.667 0.267 0.467\nUGL 0.881 0.868 0.903 0.891\n0.916 0.884 0.894 0.906 0.944 0.938 0.944 0.903 0.941\nAvg.Rank 10.533 9.450 8 .850\n8.750 6.883 7.100 6.950 5.517 5.417 5.000 5.050 7.483 4.017\nNum.Top-1 1 1 1 0\n5 1 2 3 5 7 8 5 5\nNum.Top-3 1 2 1 1\n6 6 3 7 13 14 16 9 18\nNum.Top-5 2 2 3 5\n15 12 13 18 17 21 20 10 24\nWins 27 27 28 27\n21 21 24 19 17 17 15 21 -\nDraws 0 2 1 0\n3 2 1 2 1 1 3 0 -\nLosses 3 1 1 3\n6 7 5 9 12 12 12 9 -\nP-value 0.000 0.000 0.000 0.000\n0.006 0.003 0.000 0.118 0.217 0.765 0.967 0.047 -\nTable 1: Accuracies of our method and 12 related methods on all datasets of the UEA archive\n11502\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nours\n 0  0.2  0.4  0.6  0.8  1\nw/o VP-layer\nx\nOurs is better\nhere\n(a) VP-layer vs. fully learnable\npositional encoding in trans-\nformer\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  0.2  0.4\n 0.6  0.8  1\nx\nOurs is better \nhere\n(b) VP-based self-attention\nmechanism vs. original self-\nattention mechanism\nFigure 6: Effectiveness analysis\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nArticu. Basic. Eigen.\nStand.\nStand.\nAccuracy\n100\n300\n600\n900\n 1200\nFigure 7: Accuracy by varying the number of shapes L. The\nseries length T = 144;100;17984;2500 for four datasets.\nwhich shows the results of SVP-T are more robust. Also,\nthe performance of RLPAM relies on the quality of its uni-\nvariate cluster sequence and MiniRocket needs random ker-\nnels for transformation. In terms of 1-to-1 comparison with\nother methods, SVP-T wins/draws on at least 18 out of 30\ndatasets. For the Friedman and Wilcoxon test, we set the sig-\nniﬁcant level to \u000b = 0:05 as (Bagnall et al. 2018; Li et al.\n2021). The statistical signiﬁcance is p \u00140:05, which con-\nﬁrms there is a signiﬁcant difference among the 13 methods.\nThe p-values of SVP-T to all methods are less than 0.05,\nwhich indicates the results are statistically signiﬁcant except\nfor ShapeNet, Rocket, MiniRocket, and RLPAM.\nComparison with Timestamp-level transformer. To\nevaluate our contribution of taking shapes as the input of\na transformer, we compare our SVP-T with TStamp Trans-\nformer (Zerveas et al. 2021) shown in the last two columns\nof Table 1. In terms of 1-to-1 comparison, SVP-T wins on\n21 out of the 30 datasets. The p-value of SVP-T to TStamp\nTransformer is smaller than0:05, which shows a statistically\nsigniﬁcant improvement of taking shapes as input.\n4.4 Effectiveness Analysis\nWe conduct experiments to demonstrate the effectiveness of\nthe VP-layer and the VP-based self-attention mechanism.\nVP-layer vs. fully learnable positional encoding.To study\nthe performance of the VP-layer, we change the VP-layer to\nfully learnable positional encoding and conduct an experi-\nment on all the datasets. Figure 6(a) shows that the accura-\nShape ID\nvalue\n(a) Attention\nShape ID\nShape ID (b) Attention weights\nFigure 8: Visualization of one instance from the class play-\ning badminton: (a) The X-axis denotes the ID of input\nshapes, and the Y-axis denotes the attention. The shapes are\nranked by the values of Y-axis. (b) The X-axis and Y-axis\ndenote the ID of input shapes. For (i;j), the color indicates\nthe attention weight between two shapes (Si;Sj). e.g., the\nattention weight between shapes S4 and S38 is shown in red.\ncies of using the VP-layer are clearly better than fully learn-\nable positional encoding, which supports the effectiveness\nthe VP-layer proposed in Section 3.2.\nVP-self-attention mechanism vs. original self-attention.\nTo study the performance of our VP-based self-attention\nmechanism, we change the self-attention to the original self-\nattention mechanism (Vaswani et al. 2017). The result in\nFigure 6(b) shows that the accuracies achieved using the VP-\nbased self-attention mechanism proposed in Section 3.3 are\nabove directly using the original self-attention mechanism.\n4.5 Evaluation the Effect of L\nWe conduct experiments to investigate how the number of\ninput shapes Lwould inﬂuence the classiﬁcation result.\nGenerally, the more shapes, the more accurate SVP-T.\nOn the other hand, the time complexity of the transformer-\nbased model is O(L2). Hence, there is a trade-off between\naccuracy and efﬁciency. We report four datasets because\nthey have a wide range of lengths and variables.\nFigure 7 shows the accuracy by varying L. There is no\nsigniﬁcant improvement in the accuracy of ArticularyWor-\ndRecognition when L is larger than 100, which means a\nsmall number L = 100 shapes could have a great perfor-\nmance. The same phenomenon can be observed in other\ndatasets when L varies. Therefore, the efﬁciency of our\nmodel could be further improved by ﬁnding the minimum\nLfor each dataset. We also ﬁnd that when Lis larger than\n900, the accuracies of all datasets remain unchanged. There-\nfore, we set the default Lfor all datasets to 900.\n4.6 A Case Study of BasicMotions\nTo interpret the result of SVP-T, a human activity recog-\nnition dataset from UEA archive, BasicMotions with four\nclasses (namely, playing badminton, standing, walking, and\nrunning) and six variables (Bagnall et al. 2018), is employed\nto illustrate without domain knowledge. We follow the steps\nof visualization attention and attention weights in (Abnar\nand Zuidema 2020).\n11503\noverlapping\ntime series:\nshape:\nshape:\ntime series:\nvaluevalue\nFigure 9: The overlapping shapesS4 and S38 of one instance\nfrom the class “playing badminton”. The shapesS4 and S38\nare from different variables and overlap in time.\nWe randomly select one instance from the class “playing\nbadminton” in the test set for analysis. Figure 8(a) shows\nthe attention of different shapes. The red bars and the blue\nbars are from the ﬁrst three variables (3D accelerometer),\nand the last three variables (3D gyroscope), respectively. We\ncan observe that the shapes with higher attention are mostly\nfrom the last three variables (blue). The possible reason is\nthat the discriminative shapes more likely exist in the three\ngyroscope variables (blue) for the class “playing badminton”\nthan for the other three classes.\nWe also ﬁnd the shape S4 has high attention while it be-\nlongs to the ﬁrst variable (3D accelerometer). To seek the\nreason, we further visualize the attention weights matrix\n(Score0in Formula 7) shown in Figure 8(b). We discover the\nattention weights given to S4 are relatively higher than oth-\ners. Speciﬁcally, the high attention weight between S38 and\nS4 indicates a higher dependency. By tracking them back\nto the original time series instance, a clear overlapping be-\nhavior (shown in Figure 9) can be observed, which is de-\nscribed in Section 3.3, and further illustrates the effective-\nness of our VP-based self-attention mechanism. Meanwhile,\nFigure 8(b) shows the high data sparsity (the colored points\nvs. the dark points) in our SVP-T, which has the potential\nof utilizing sparsity to improve efﬁciency (Zhou et al. 2021).\n5 Conclusion\nIn this paper, we propose a novel shape-level variable-\nposition transformer method, named SVP-T, for MTSC . We\nuse time series subsequences, rather than timestamps, as the\ninput tokens of a transformer-based model, which captures\nboth variable and position dependencies in MTS . In particu-\nlar, a variable-position encoding layer is proposed to utilize\nthe VP information of each shape. In addition, we propose a\nvariable-position self-attention mechanism in SVP-T to en-\nhance the attention weights of the overlapping shapes. The\nexperiment shows the accuracy of SVP-T has the highest\nrank when compared to the state-of-the-art methods. As for\nfuture work, we plan to utilize the sparsity we observed from\nSection 4.6 to improve the efﬁciency of SVP-T.\nAcknowledgments\nThanks for the helpful feedbacks from the anonymous re-\nviewers. This work was supported by the Hong Kong Re-\nsearch Grant Council (HKRGC), RIF R2002-20F, and Hong\nKong Baptist University’s Interdisciplinary Research Clus-\nters Matching Scheme, HKBU IRCMS/19-20/H01.\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying Attention\nFlow in Transformers. In ACL.\nBagnall, A.; Dau, H. A.; Lines, J.; Flynn, M.; Large, J.;\nBostrom, A.; Southam, P.; and Keogh, E. 2018. The\nUEA multivariate time series classiﬁcation archive, 2018.\narXiv:1811.00075.\nBagnall, A.; Lines, J.; Bostrom, A.; Large, J.; and Keogh, E.\n2017. The great time series classiﬁcation bake off: a review\nand experimental evaluation of recent algorithmic advances.\nDMKD.\nDempster, A.; Petitjean, F.; and Webb, G. I. 2020. ROCKET:\nExceptionally Fast and Accurate Time Series Classiﬁcation\nUsing Random Convolutional Kernels. DMKD.\nDempster, A.; Schmidt, D. F.; and Webb, G. I. 2021.\nMiniRocket: A Very Fast (Almost) Deterministic Transform\nfor Time Series Classiﬁcation. In SIGKDD.\nDemˇsar, J. 2006. Statistical comparisons of classiﬁers over\nmultiple data sets. JMLR.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nEldele, E.; Ragab, M.; Chen, Z.; Wu, M.; Kwoh, C. K.; Li,\nX.; and Guan, C. 2021. Time-Series Representation Learn-\ning via Temporal and Contextual Contrasting. In IJCAI.\nFranceschi, J.-Y .; Dieuleveut, A.; and Jaggi, M. 2019. Unsu-\npervised Scalable Representation Learning for Multivariate\nTime Series. In NeurIPS, 4652–4663.\nGao, G.; Gao, Q.; Yang, X.; Pajic, M.; and Chi, M. 2022.\nA Reinforcement Learning-Informed Pattern Mining Frame-\nwork for Multivariate Time Series Classiﬁcation. In IJCAI.\nGirshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014.\nRich Feature Hierarchies for Accurate Object Detection and\nSemantic Segmentation. In ICCV.\nGrabocka, J.; Wistuba, M.; and Schmidt-Thieme, L. 2016.\nFast classiﬁcation of univariate and multivariate time series\nthrough shapelet discovery. KAIS, 49(2): 429–454.\nHao, Y .; and Cao, H. 2020. A New Attention Mechanism to\nClassify Multivariate Time Series. In IJCAI.\n11504\nKarim, F.; Majumdar, S.; Darabi, H.; and Harford, S.\n2019. Multivariate LSTM-FCNs for time series classiﬁca-\ntion. Neural Networks, 116: 237–245.\nLi, G.; Choi, B.; Xu, J.; Bhowmick, S. S.; Chun, K.-P.; and\nWong, G. L. 2021. Shapenet: A shapelet-neural network\napproach for multivariate time series classiﬁcation. InAAAI.\nLim, B.; Arık, S. ¨O.; Loeff, N.; and Pﬁster, T. 2021. Tempo-\nral fusion transformers for interpretable multi-horizon time\nseries forecasting. International Journal of Forecasting.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical Vision\nTransformer using Shifted Windows. In ICCV.\nPalpanas, T. 2015. Data series management: The road to big\nsequence analytics. ACM SIGMOD Record, 44(2): 47–52.\nRezatoﬁghi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union:\nA metric and a loss for bounding box regression. CVPR.\nRuiz, A. P.; Flynn, M.; Large, J.; Middlehurst, M.; and Bag-\nnall, A. 2020. The great multivariate time series classiﬁca-\ntion bake off: a review and experimental evaluation of recent\nalgorithmic advances. DMKD.\nSch¨afer, P.; and Leser, U. 2017. Multivariate time se-\nries classiﬁcation with WEASEL+ MUSE. arXiv preprint\narXiv:1711.11343.\nShifaz, A.; Pelletier, C.; Petitjean, F.; and Webb, G. I. 2020.\nTS-CHIEF: a scalable and accurate forest algorithm for time\nseries classiﬁcation. DMKD.\nShiv, V .; and Quirk, C. 2019. Novel positional encodings to\nenable tree-based transformers. In NeurIPS.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS.\nWang, H.; Ho, E. S.; Shum, H. P.; and Zhu, Z. 2019. Spatio-\ntemporal manifold learning for human motions via long-\nhorizon modeling. TVCG, 27(1): 216–227.\nWu, N.; Green, B.; Ben, X.; and O’Banion, S. 2020. Deep\ntransformer models for time series forecasting: The in-\nﬂuenza prevalence case. arXiv preprint arXiv:2001.08317.\nZakaria, J.; Mueen, A.; Keogh, E.; and Young, N. 2016. Ac-\ncelerating the discovery of unsupervised-shapelets. DMKD.\nZerveas, G.; Jayaraman, S.; Patel, D.; Bhamidipaty, A.;\nand Eickhoff, C. 2021. A Transformer-Based Framework\nfor Multivariate Time Series Representation Learning. In\nSIGKDD.\nZhang, X.; Gao, Y .; Lin, J.; and Lu, C.-T. 2020. Tapnet:\nMultivariate time series classiﬁcation with attentional proto-\ntypical network. In AAAI.\nZhou, F.; De la Torre, F.; and Hodgins, J. K. 2012. Hier-\narchical aligned cluster analysis for temporal clustering of\nhuman motion. TPAMI, 35(3): 582–596.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2021. Informer: Beyond Efﬁcient Trans-\nformer for Long Sequence Time-Series Forecasting. In\nAAAI.\n11505",
  "topic": "Timestamp",
  "concepts": [
    {
      "name": "Timestamp",
      "score": 0.906354546546936
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.6507521271705627
    },
    {
      "name": "Computer science",
      "score": 0.6299520134925842
    },
    {
      "name": "Position (finance)",
      "score": 0.5843204855918884
    },
    {
      "name": "Multivariate statistics",
      "score": 0.5356020331382751
    },
    {
      "name": "Time series",
      "score": 0.49097374081611633
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.486066609621048
    },
    {
      "name": "Data mining",
      "score": 0.451777845621109
    },
    {
      "name": "Transformer",
      "score": 0.4281979203224182
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.42719554901123047
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42214611172676086
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3460795283317566
    },
    {
      "name": "Algorithm",
      "score": 0.33766090869903564
    },
    {
      "name": "Mathematics",
      "score": 0.22490882873535156
    },
    {
      "name": "Machine learning",
      "score": 0.18728289008140564
    },
    {
      "name": "Real-time computing",
      "score": 0.10373508930206299
    },
    {
      "name": "Engineering",
      "score": 0.06855508685112
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141568987",
      "name": "Hong Kong Baptist University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ]
}