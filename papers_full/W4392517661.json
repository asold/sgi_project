{
    "title": "Can large language models reason and plan?",
    "url": "https://openalex.org/W4392517661",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4221631779",
            "name": "Kambhampati, Subbarao",
            "affiliations": [
                "Arizona State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6743716967",
        "https://openalex.org/W4292395817",
        "https://openalex.org/W4378499172",
        "https://openalex.org/W4283330306",
        "https://openalex.org/W4378510404",
        "https://openalex.org/W3135507092"
    ],
    "abstract": "Abstract While humans sometimes do show the capability of correcting their own erroneous guesses with self‐critiquing, there seems to be no basis for that assumption in the case of LLMs.",
    "full_text": "Can Large Language Models Reason and Plan?\nSubbarao Kambhampati\nSchool of Computing & Augmented Intelligence\nArizona State University\nemail: rao@asu.edu\nA version appears in the Annals of The New York Academy of Sciences:\nhttps://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.15125\nLarge Language Models (LLMs), essentially n-gram mod-\nels on steroids * that have been trained on web-scale lan-\nguage corpora (or, effectively, our civilizational knowl-\nedge), have caught our collective imagination with linguis-\ntic behaviors that no one expected text completion systems\nto possess 3. By training and operation, LLMs are perhaps\nbest seen as giant non-veridical memories akin to an ex-\nternal System 1 2 for us all (see Figure 1). Their seem-\ning versatility has however led many researchers to wonder\nwhether they can also do well on planning and reasoning\ntasks typically associated with System 2 competency.\nNothing in the training and use of LLMs would seem\nto suggest remotely that they can do any type of princi-\npled reasoning (which, as we know, often involves com-\nputationally hard inference/search). What LLMs are good\nat is a form of universal approximate retrieval. Unlike\ndatabases that index and retrieve data exactly, LLMs, as n-\ngram models, probabilistically reconstruct completions for\nthe prompt word by word–a process we shall refer to as\napproximate retrieval. This means that LLMs can’t even\nguarantee memorizing complete answers, something that\nis the flip side of their appeal about constructing “novel”\nprompt completions on the fly. The boon (“creativity”) and\nbane (“hallucination”) of LLMs is that n-gram models will\nnaturally mix and match–and have almost as much trouble\nstrictly memorizing as we do. It is indeed the very basis of\ntheir appeal.\n*LLMs are trained to predict the distribution of the n-th to-\nken given n-1 previous tokens. GPT3.5 that powered the origi-\nnal ChatGPT is, for example, a roughly 3001-gram model of lan-\nguage.\nFigure 1.An informal account of viewing LLM as a giant external\nnon-veridical memory that acts as a pseudo System 1\nDespite this, the “Large Language Models are Zero-Shot\n⟨insert-your-reasoning-task ⟩” has almost become a meme\npaper title! At some level, this trend is perhaps inevitable as\nin the era of LLMs, AI has become a form of ersatz natural\nscience5–driven by observational studies of capabilities of\nthese behemoth systems.\nSo, are these n-gram models on steroids really capable of\nplanning and reasoning? In the summer of 2022, when my\nresearch group wanted to better answer this question, most\nreasoning claims were still somewhat anecdotal. So, we set\nout to evaluate GPT3 on a set of planning instances derived\nfrom the domains typically used in the International Plan-\nning Competition (IPC) –including the well known Blocks\nWorld†. Our results13 were contrary to the anecdotal claims\nabout the planning abilities of LLMs, and when we made\nthem public, received significant attention in the AI circles.\nBy the beginning of 2023, with the wide-spread public re-\n†https://en.wikipedia.org/wiki/Blocks world\n1\narXiv:2403.04121v2  [cs.AI]  8 Mar 2024\nlease of ChatGPT, and later, GPT4, there were a slew of ad-\nditional claims, including in refereed papers, about LLM’s\nabilities to reason and plan. So we decided to repeat our\ntests on both GPT3.5 and GPT4 12. Initial results showed\nthat there was some improvement in the accuracy of gen-\nerated plans from GPT3 to GPT3.5 to GPT4, with GPT4\nreaching 30% empirical accuracy in the Blocks World (al-\nbeit still lower in other domains). We then wanted to know\nwhether the modest improvement is because of the im-\nproved approximate retrieval abilities or whether GPT4 is\nactually doing/searching for plans.\nLet us pause to note that my interest here is not whether\nLLMs can fake reasoning (by giving correct answers to\nreasoning tasks from memory and pattern finding), but\nwhether they can actually do principled reasoning. Of\ncourse, seeing patterns in reasoning problems is not any-\nthing to be sneezed at. After all, our interest in master-\ning it is what is behind much of “street fighting” math\n(e.g. George P ´olya’s ”How to Solve it”). But finding ap-\nproximate shortcuts over provably correct reasoning proce-\ndures is obviously not equivalent to doing reasoning–unless\nyou have an ability to establish from first principles that\nyour hunch is actually correct. It is challenging to decide\nwhether a system (or a human, for that matter) is memoriz-\ning or solving a problem from scratch–especially as the sys-\ntems (or humans) get trained on larger and larger “question\nbanks.” This is a challenge that most instructors and inter-\nviewers are acutely aware of. Think of that infamous “Why\nare manhole covers round?” interview question. While it\nmay well have given the interviewer an insight into the\ncandidate’s analytical reasoning skills the very first time it\nwas asked, all it does with high probability now is to con-\nfirm whether the candidate trained on the interview ques-\ntion banks!\nConsidering that the LLMs don’t suffer some of the nor-\nmal limitations of humans–such as having a life on the\nside, and thus not having the time or inclination to focus\nexclusively on the test/interview preparation for long peri-\nods, they can support approximate retrieval over webscale\ncorpora. My research group wanted to check if the im-\nproved performance of GPT4 is because of approximate re-\ntrieval from a larger training corpus, or really comes from\nits ability to plan. One way of checking this for planning\ntasks is to reduce the effectiveness of approximate retrieval\nby obfuscating the names of the actions and objects in the\nplanning problem. When we did this for test domains 12,11,\nGPT4’s empirical performance plummeted precipitously,\ndespite the fact that none of the standard off-the-shelf AI\nplanners have any trouble with such obfuscation.‡\nPerhaps they can’t do planning autonomously straight out\nof the box, but can they do it with a little nudge? There\nare broadly two popular techniques for such nudging. The\nfirst, called “fine tuning,” is rather straightforward: take a\ngeneral LLM and fine tune it on planning problems (i.e.,\ninstances and their solutions), with the hope that they will\nsubsequently make better guesses (see the left-hand side\nof Figure 1). While our own limited experiments didn’t\nshow any significant improvement through fine tuning, it\nis possible that with even more fine tuning data and ef-\nfort, the quality of LLM guesses may well improve. But\nall that such fine tuning is doing is converting the planning\ntask into a memory-based approximate retrieval (akin to the\nmemorization/compilation from System 2 to System 1; see\nFigure 1). It doesn’t prove that LLMs are able to plan.\nThe second way to improve planning (and reasoning) per-\nformance is to prompt an LLM back with hints/suggestions\nabout how it can improve its initial plan guess. The crucial\nquestions here are (a) whether this back prompting is man-\nual or automated (b) who is certifying the correctness of the\nfinal answer and (c) whether the prompts inject additional\nproblem knowledge or are just merely exhorting the LLM\nto try again.\nThe cleanest approach–one we advocate12,6–is to let an ex-\nternal model-based plan verifier do the back prompting and\nto certify the correctness of the final solution. In general,\nsuch LLM-Modulo frameworks6 can gainfully leverage the\namazing idea generation capabilities of LLMs with sound\nexternal verifiers in a generate-test-critique framework with\ngurantees.\nIn contrast, by far the more popular methodology is to\nhave the human in the loop prompt the LLM iteratively.\nThe problem with this is that it is highly susceptible to the\n‡As these results came about at the height of sparks of\nAGI/existential risk angst, we couldn’t resist the tongue-in-cheek\neditorializing that if GPT4 ever goes rogue, you can stymie it by\nthrowing a simple planning problem at it! Humor aside, nothing\nin our studies showed that GPT4 is capable of generating exe-\ncutable plans autonomously.\n2\nFigure 2.Claimed reasoning capabilities of LLMs are sometimes\ndue to the subconscious helpful iterative prompting by the humans\nin the loop (graphic adapted from https://xkcd.com/2347/ under\nCreative Commons License)\nClever Hans effect§, where the LLM is merely generating\nguesses, and it is the human in the loop, with the knowledge\nof right vs. wrong solutions, who is steering the LLM–even\nif they didn’t set out to do so deliberately. The credit and\nblame for the ensuring accuracy, if any, falls squarely on\nthe human in the loop. The relevance of such a frame-\nwork becomes questionable when the human-in-the-loop\ndoesn’t know (or is unable to verify) the answer to the rea-\nsoning/planning problem themselves. Thus the tongue-in-\ncheek characterization of LLM reasoning abilities in Fig-\nure 2.\nA variation on the second approach is to have the LLM it-\nself “critique” the guesses it generates and iteratively self-\nimprove. Although some papers seem to swear by such a\n“self-improvement” capability of LLMs, the plausibility of\nsuch a claim hinges on the belief that the LLMs are bet-\nter at verifying their solutions than they are at generating\nthem. While never explicitly justified, the assumption rests\n§https://en.wikipedia.org/wiki/Clever Hans\non either analogies to humans or indirect nods to compu-\ntational complexity arguments. While humans sometimes\ndo show the capability of correcting their own erroneous\nguesses with self-critiquing, there seems to be no basis for\nthat assumption in the case of LLMs. And while for many\ncomputational tasks (e.g. those in class NP ¶), the verifica-\ntion is often of lower complexity than generation, that fact\ndoesn’t seem particularly relevant for LLMs which are gen-\nerating (approximately retrieving) guesses, rather than ac-\ntually solving the problem with guarantees. Indeed two re-\ncent studies from my lab–one on plan verification10 and the\nother on constraint verification 9–seem to throw cold water\non this optimism by showing that with “self-verification”\nperformance actually worsens. This is because LLMs hal-\nlucinate both false positives and false negatives while ver-\nifying the solutions they generate. One reason this is not\nrecognized in earlier literature is that there self-verification\nclaims are often made in the context of tacit knowledge\ntasks for which there is little possibility of a verifier (e.g.\nwriting/improving essays), making it harder to evaluate\nwhether LLM’s critiquing actually helped. Paradoxically,\nthe fact that it is infeasible to write sound verifiers for tacit\nknowledge tasks also makes it easier to mistake LLMs for\nbeing as reasonable a critic as any! || In other cases, an ex-\nternal simulator winds up playing the role of sound verifi-\ncation.\nWhile the foregoing questions the claims that LLMs are\ncapable of planning/reasoning, it is not meant to imply that\nLLMs don’t have any constructive roles to play in solving\nplanning/reasoning tasks. In particular, their uncanny abil-\nity to generate ideas/potential candidate solutions–albeit\nwith no guarantees about those guesses–can still be valu-\nable in the “LLM-Modulo” setups 6, in conjunction with\neither model-based verifiers or expert humans in the loop.\nThe trick to avoiding ascribing autonomous reasoning ca-\npabilities to LLMs is to recognize that LLMs are generating\npotential answers that still need to be checked by external\nverifiers.\nThe skeptical reader might now ask: But what about all\n¶NP stands for nondeterministic polynomial, and covers the\nclass of computational problems whose solutions can be verified\nin polynomial time.\n||In other words, LLMs can be as good as that Peloton instruc-\ntor in confidently critiquing Christopher Nolan movies.\n3\nthose papers at high profile AI conferences that claim to\nshow planning abilities of LLMs? To analyze those claims,\nwe need to first understand that solving planning tasks re-\nquires (a) having the necessary planning domain knowl-\nedge–the actions and their preconditions, effects; the stan-\ndard hierarchical recipes (e.g. task reduction schemas in\nHierarchical Task Network planning), past cases/plans etc.\nand (b) being able to assemble this knowledge into an ex-\necutable plan that takes care of any subgoal/resource inter-\nactions. The first can be called the knowledge acquisition\npart, and the second reasoning/planning part. Many of the\npapers claiming planning abilities of LLMs, on closer ex-\namination, wind up confusing general planning knowledge\nextracted from the LLMs for executable plans. When all we\nare looking for are abstract plans, such as “wedding plans,”\nwith no intention of actually executing said plans directly,\nit is easy to confuse them for complete executable plans.\nIndeed, our close examination of several papers claiming\nplanning capabilities 7 for LLMs suggests that they either\nare evaluating in domains/tasks where subgoal interactions\ncan be safely ignored, or delegating the interaction resolu-\ntion (reasoning) to the humans in the loop (who, through\nrepeated prompting, have to “correct” the plan). Some-\ntimes, in common sense domains, or with enough fine tun-\ning, the “assembling” part may also be obviated by having\nseen a case that pretty much corresponds to the problem\nthat needs to be solved. Without these assumptions or mit-\nigations, the plans that come out of LLMs may look rea-\nsonable to the lay user, but lead to execution time interac-\ntions and errors. These issues are illustrated in part by a\nrecent news story about the proliferation of travel planning\nbooks8, mostly auto-extracted from LLMs, and the ensu-\ning disappointment of the unsuspecting end users who buy\nthem mistaking them for usable plans!\nThe fact that LLMs are often good at extracting planning\nknowledge can indeed be gainfully leveraged. As we have\nargued in recent work 1, LLMs can be a rich source of\napproximate models of world/domain dynamics and user\npreferences, as long as the humans (and any specialized\ncritics) in the loop verify and refine the models, and give\nthem over to model-based solvers. This way of using LLMs\nhas the advantage that the humans need only be present\nwhen the dynamics/preference model is being teased out\nand refined, with the actual planning after that being left to\nsound frameworks with correctness guarantees6.\nFigure 3.Viewing LLMs as an approximate knowledge source\ntrained over civilizational knowledge\nSuch a framework has striking similarities to knowledge-\nbased AI systems of yore, with LLMs effectively replac-\ning the “knowledge engineer” (Figure 3). Given the rather\nquixotic and dogmatic shift of AI away from approaches\nthat accept domain knowledge from human experts, some-\nthing I bemoaned in “ P´olanyi’s revenge and AI’s new ro-\nmance with tacit knowledge”4 this new trend of using\nLLMs as knowledge sources can be viewed as a form of\navenging Polanyi’s revenge (by bringing explicit knowl-\nedge back to AI systems, if only as gleaned from LLMs).**\nIndeed, LLMs make it easy to get problem-specific knowl-\nedge, as long as we are willing to relax correctness require-\nments of that knowledge. In contrast to the old knowledge\nengineering approaches, LLMs offer this without making it\nlook like we are inconveniencing any specific human (we\nare, instead, just leveraging everything humans have told\neach other!). So the million dollar question for reasoning\ntasks becomes: “how would you do planning if you have\nsome doddering know-it-all ready to give you any kind of\nknowledge?” The LLM-Modulo framework 6 is a princi-\npled method for tackling this challenge.\nTo summarize, nothing that I have read, verified, or done\ngives me any compelling reason to believe that LLMs do\nreasoning/planning, as normally understood. What they do\ninstead, armed with web-scale training, is a form of uni-\nversal approximate retrieval, which, as I have argued, can\n**There is rich irony here: If you give what you know about\na toy world to the computer, and have it solve new instances, it\nwould be derisively called “Good Old Fashioned AI,” but if you\ncapture all that the humanity knows about everything (as exported\nto the Internet), train your LLM on it, and then ask it to provide\napproximate task relevant knowledge, then it becomes “modern\nAI.”\n4\nsometimes be mistaken for reasoning capabilities. LLMs\ndo excel in idea generation for any task–including those\ninvolving reasoning, and as I pointed out, this can be ef-\nfectively leveraged to support reasoning/planning in LLM-\nModulo frameworks6. In other words, LLMs already have\nenough amazing approximate retrieval abilities that can be\ngainfully leveraged, that we don’t need to ascribe question-\nable reasoning/planning capabilities to them.††\nReferences\n[1] Lin Guan, Karthik Valmeekam, Sarath Sreedharan,\nand Subbarao Kambhampati. Leveraging pre-trained\nlarge language models to construct and utilize world\nmodels for model-based task planning. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems, 2023.\n[2] Daniel Kahneman. Thinking, fast and slow. macmil-\nlan, 2011.\n[3] Subbarao Kambhampati. Language imitation games\nand the arrival of broad and shallow AI. CACM Blog,\n2021.\n[4] Subbarao Kambhampati. Polanyi’s revenge and AI’s\nnew romance with tacit knowledge. Communications\nof the ACM, 64(2):31–32, 2021.\n[5] Subbarao Kambhampati. AI as (an ersatz) natural sci-\nence? Communications of the ACM, 65(9):8–9, 2022.\n[6] Subbarao Kambhampati, Karthik Valmeekam, Lin\nGuan, Kaya Stechly, Mudit Verma, Siddhant Bham-\nbri, Lucas Saldyt, and Anil Murthy. LLMs can’t plan,\nbut can help planning in LLM-Modulo frameworks.\narXiv preprint 2402.01817, 2024.\n[7] Subbarao. Kambhampati, Karthik. Valmeekam,\nMatthew. Marquez, and Lin. Guan. On the role\n††Although we focused on planning and reasoning tasks, the\ndiscussion here has implications to LLM-based code generation.\nThere too, while LLMs can help as “co-pilots” to human pro-\ngrammers, there is never any guarantee that the code they gen-\nerate is correct. The fact that code generation capabilities come\nfrom github data, that is of higher quality than the general web\ndata, and the fact that incremental interpreters in the loop can pin-\npoint any syntax errors in the generated code, helps the utility of\nthe suggested code snippets for human programmers.\nof large language models in planning, July 2023.\nTutorial presented at the International Conference\non Automated Planning and Scheduling (ICAPS),\nPrague.\n[8] Seth Kugel and Stephen Hiltner. A new frontier for\ntravel scammers: A.I.-Generated Guidebooks. New\nYork Times, August 2023.\n[9] Kaya Stechly, Matthew Marquez, and Subbarao\nKambhampati. GPT-4 Doesn’t Know It’s Wrong: An\nAnalysis of Iterative Prompting for Reasoning Prob-\nlems. In NeurIPS 2023 Foundation Models for Deci-\nsion Making Workshop, 2023.\n[10] Karthik Valmeekam, Matthew Marquez, and Sub-\nbarao Kambhampati. Can large language models re-\nally improve by self-critiquing their own plans? In\nNeurIPS 2023 Foundation Models for Decision Mak-\ning Workshop, 2023.\n[11] Karthik Valmeekam, Matthew Marquez, Alberto\nOlmo, Sarath Sreedharan, and Subbarao Kambham-\npati. Planbench: An extensible benchmark for evalu-\nating large language models on planning and reason-\ning about change. In Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track, 2023.\n[12] Karthik Valmeekam, Matthew Marquez, Sarath\nSreedharan, and Subbarao Kambhampati. On the\nplanning abilities of large language models - a crit-\nical investigation. In Thirty-seventh Conference on\nNeural Information Processing Systems, 2023.\n[13] Karthik Valmeekam, Alberto Olmo, Sarath Sreedha-\nran, and Subbarao Kambhampati. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498, 2022.\n5"
}