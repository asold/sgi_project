{
    "title": "Large Language Models’ Accuracy in Emulating Human Experts’ Evaluation of Public Sentiments about Heated Tobacco Products on Social Media: Evaluation Study",
    "url": "https://openalex.org/W4408128495",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2109440486",
            "name": "Kwan Ho Kim",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119727680",
            "name": "Soo-Jong Kim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4213154204",
        "https://openalex.org/W4206027588",
        "https://openalex.org/W4327568819",
        "https://openalex.org/W3013644071",
        "https://openalex.org/W3204974457",
        "https://openalex.org/W3157539710",
        "https://openalex.org/W1837016913",
        "https://openalex.org/W2592794011",
        "https://openalex.org/W3027157491",
        "https://openalex.org/W4205503238",
        "https://openalex.org/W2807165107",
        "https://openalex.org/W3083822890",
        "https://openalex.org/W3195865861",
        "https://openalex.org/W4388494139",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4402538491",
        "https://openalex.org/W4393150825",
        "https://openalex.org/W4321455981",
        "https://openalex.org/W4387220962",
        "https://openalex.org/W4385751780",
        "https://openalex.org/W4401597819",
        "https://openalex.org/W4401506443",
        "https://openalex.org/W4388777264",
        "https://openalex.org/W4389577293",
        "https://openalex.org/W4388049829",
        "https://openalex.org/W4321521234",
        "https://openalex.org/W4404527322",
        "https://openalex.org/W4367186868",
        "https://openalex.org/W6887709359",
        "https://openalex.org/W4214717370",
        "https://openalex.org/W3165553112"
    ],
    "abstract": "Background Sentiment analysis of alternative tobacco products discussed on social media is crucial in tobacco control research. Large language models (LLMs) are artificial intelligence models that were trained on extensive text data to emulate the linguistic patterns of humans. LLMs may hold the potential to streamline the time-consuming and labor-intensive process of human sentiment analysis. Objective This study aimed to examine the accuracy of LLMs in replicating human sentiment evaluation of social media messages relevant to heated tobacco products (HTPs). Methods GPT-3.5 and GPT-4 Turbo (OpenAI) were used to classify 500 Facebook (Meta Platforms) and 500 Twitter (subsequently rebranded X) messages. Each set consisted of 200 human-labeled anti-HTPs, 200 pro-HTPs, and 100 neutral messages. The models evaluated each message up to 20 times to generate multiple response instances reporting its classification decisions. The majority of the labels from these responses were assigned as a model’s decision for the message. The models’ classification decisions were then compared with those of human evaluators. Results GPT-3.5 accurately replicated human sentiment evaluation in 61.2% of Facebook messages and 57% of Twitter messages. GPT-4 Turbo demonstrated higher accuracies overall, with 81.7% for Facebook messages and 77% for Twitter messages. GPT-4 Turbo’s accuracy with 3 response instances reached 99% of the accuracy achieved with 20 response instances. GPT-4 Turbo’s accuracy was higher for human-labeled anti- and pro-HTP messages compared with neutral messages. Most of the GPT-3.5 misclassifications occurred when anti- or pro-HTP messages were incorrectly classified as neutral or irrelevant by the model, whereas GPT-4 Turbo showed improvements across all sentiment categories and reduced misclassifications, especially in incorrectly categorized messages as irrelevant. Conclusions LLMs can be used to analyze sentiment in social media messages about HTPs. Results from GPT-4 Turbo suggest that accuracy can reach approximately 80% compared with the results of human experts, even with a small number of labeling decisions generated by the model. A potential risk of using LLMs is the misrepresentation of the overall sentiment due to the differences in accuracy across sentiment categories. Although this issue could be reduced with the newer language model, future efforts should explore the mechanisms underlying the discrepancies and how to address them systematically.",
    "full_text": null
}