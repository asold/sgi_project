{
  "title": "RecInDial: A Unified Framework for Conversational Recommendation with Pretrained Language Models",
  "url": "https://openalex.org/W3206062102",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1415944623",
      "name": "Wang Lingzhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102157018",
      "name": "Hu Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102724719",
      "name": "Sha Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2027875661",
      "name": "Xu Can",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2087743882",
      "name": "Wong, Kam-Fai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2358495019",
      "name": "Jiang, Daxin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2888515090",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3080122044",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2891389695",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3152509363",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2798914047",
    "https://openalex.org/W3204697369",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W3031596603",
    "https://openalex.org/W2982904530",
    "https://openalex.org/W2026810221",
    "https://openalex.org/W3098774588",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W3162337509",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W3115944734",
    "https://openalex.org/W3035355914",
    "https://openalex.org/W2978290467",
    "https://openalex.org/W2997662139",
    "https://openalex.org/W3101718968",
    "https://openalex.org/W3094070895",
    "https://openalex.org/W2349436533",
    "https://openalex.org/W2997300509",
    "https://openalex.org/W3174696767",
    "https://openalex.org/W2964112275",
    "https://openalex.org/W2808871448",
    "https://openalex.org/W2112420033",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2970236742",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W3175618100",
    "https://openalex.org/W2964210218",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963035145",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2962818688",
    "https://openalex.org/W2963201498",
    "https://openalex.org/W2898076813",
    "https://openalex.org/W3201495823",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W3100790518",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W2963743389",
    "https://openalex.org/W3035098003",
    "https://openalex.org/W3106454043",
    "https://openalex.org/W2602895164",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W3116062118",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Conversational Recommender System (CRS), which aims to recommend high-quality items to users through interactive conversations, has gained great research interest recently. A CRS is usually composed of a recommendation module and a generation module. In the previous work, these two modules are loosely connected in the model training and are shallowly integrated during inference, where a simple switching or copy mechanism is adopted to incorporate recommended items into generated responses. Moreover, the current end-to-end neural models trained on small crowd-sourcing datasets (e.g., 10K dialogs in the ReDial dataset) tend to overfit and have poor chit-chat ability. In this work, we propose a novel unified framework that integrates recommendation into the dialog (RecInDial) generation by introducing a vocabulary pointer. To tackle the low-resource issue in CRS, we finetune the large-scale pretrained language models to generate fluent and diverse responses, and introduce a knowledge-aware bias learned from an entity-oriented knowledge graph to enhance the recommendation performance. Furthermore, we propose to evaluate the CRS models in an end-to-end manner, which can reflect the overall performance of the entire system rather than the performance of individual modules, compared to the separate evaluations of the two modules used in previous work. Experiments on the benchmark dataset ReDial show our RecInDial model significantly surpasses the state-of-the-art methods. More extensive analyses show the effectiveness of our model.",
  "full_text": "RecInDial: A UniÔ¨Åed Framework for Conversational Recommendation\nwith Pretrained Language Models\nLingzhi Wang1,2‚àó, Huang Hu4, Lei Sha3, Can Xu4, Kam-Fai Wong1,2, Daxin Jiang4‚Ä†\n1The Chinese University of Hong Kong, Hong Kong, China\n2MoE Key Laboratory of High ConÔ¨Ådence Software Technologies, China\n3University of Oxford, United Kingdom\n4Microsoft Corporation, Beijing, China\n1,2{lzwang,kfwong}@se.cuhk.edu.hk; 3lei.sha@cs.ox.ac.uk;\n4{huahu,caxu,djiang}@microsoft.com\nAbstract\nConversational Recommender System (CRS),\nwhich aims to recommend high-quality items\nto users through interactive conversations, has\ngained great research interest recently. A\nCRS is usually composed of a recommenda-\ntion module and a generation module. In the\nprevious work, these two modules are loosely\nconnected in the model training and are shal-\nlowly integrated during inference, where a sim-\nple switching or copy mechanism is adopted\nto incorporate recommended items into gen-\nerated responses. Moreover, the current end-\nto-end neural models trained on small crowd-\nsourcing datasets (e.g., 10K dialogs in the Re-\nDial dataset) tend to overÔ¨Åt and have poor\nchit-chat ability. In this work, we pro-\npose a novel uniÔ¨Åed framework that inte-\ngrates recommendation into the dialog (RecIn-\nDial1) generation by introducing a vocabulary\npointer. To tackle the low-resource issue in\nCRS, we Ô¨Ånetune the large-scale pretrained\nlanguage models to generate Ô¨Çuent and diverse\nresponses, and introduce a knowledge-aware\nbias learned from an entity-oriented knowl-\nedge graph to enhance the recommendation\nperformance. Furthermore, we propose to\nevaluate the CRS models in an end-to-end\nmanner, which can reÔ¨Çect the overall perfor-\nmance of the entire system rather than the per-\nformance of individual modules, compared to\nthe separate evaluations of the two modules\nused in previous work. Experiments on the\nbenchmark dataset ReDial show our RecIn-\nDial model signiÔ¨Åcantly surpasses the state-\nof-the-art methods. More extensive analyses\nshow the effectiveness of our model.\n1 Introduction\nIn recent years, there have been fast-growing re-\nsearch interests to address Conversational Rec-\n‚àóWork performed during internship at Microsoft STCA.\n‚Ä†Corresponding author: djiang@microsoft.com.\n1The code is available at https://github.com/\nLingzhi-WANG/PLM-BasedCRS\nommender System (CRS) (Li et al., 2018; Sun\nand Zhang, 2018; Zhou et al., 2020a), due to the\nbooming of intelligent agents in e-commerce plat-\nforms. It aims to recommend target items to users\nthrough interactive conversations. Traditional rec-\nommender systems perform personalized recom-\nmendations based on user‚Äôs previous implicit feed-\nback like clicking or purchasing histories, while\nCRS can proactively ask clariÔ¨Åcation questions and\nextract user preferences from conversation history\nto conduct precise recommendations. Existing gen-\nerative methods (Chen et al., 2019; Zhou et al.,\n2020a; Ma et al., 2020; Liang et al., 2021) are\ngenerally composed of two modules, i.e., a rec-\nommender module to predict precise items and a\ndialogue module to generate free-form natural re-\nsponses containing the recommended items. Such\nmethods usually utilize Copy Mechanism (Gu et al.,\n2016) or Pointer Network (Gulcehre et al., 2016)\nto inject the recommended items into the generated\nreplies. However, these strategies cannot always\nincorporate the recommended items into the gener-\nated responses precisely and appropriately. On the\nother hand, most of the existing CRS datasets (Li\net al., 2018; Zhou et al., 2020b; Liu et al., 2020,\n2021) are relatively small (‚àº10K dialogues) due\nto the expensive crowd-sourcing labor. The end-to-\nend neural models trained on these datasets from\nscratch are prone to be overÔ¨Åtting and have unde-\nsirable quality on the generated replies in practice.\nEncouraged by the compelling performance of\npre-training techniques, we present a pre-trained\nlanguage models (PLMs) based framework called\nRecInDial to address these challenges. RecIn-\nDial integrates the item recommendation into the\ndialogue generation under the pretrain-Ô¨Ånetune\nschema. SpeciÔ¨Åcally, RecInDial Ô¨Ånetunes the pow-\nerful PLMs like DialoGPT (Zhang et al., 2020)\ntogether with a Relational Graph Convolutional\nNetwork (RGCN) to encode the node representa-\ntion of an item-oriented knowledge graph. The\narXiv:2110.07477v2  [cs.CL]  7 Oct 2022\n...\nUser: That sounds good. I could go with a classic. Have\nyou seen Troll 2 (1990)? I‚Äôm looking for a horrible movie.\ncheesy horror\nHuman: Tuesday 13, you like?\nReDial: Black Panther (2018) is a good one too.\nKBRD: or It (2017)\nKGSF: I would recommend watching it.\nOUR: yes I have seen that one. It was good. I also liked\nthe movie It (2017).\n...\nTable 1: A conversation example with movies recom-\nmendation from the test set of ReDial dataset.\nformer aims to generate Ô¨Çuent and diverse dialogue\nresponses based on the strong language generation\nability of PLMs, while the latter is to facilitate the\nitem recommendation by learning better structural\nnode representations. To bridge the gap between\nresponse generation and item recommendation, we\nexpand the generation vocabulary of PLMs to in-\nclude an extra item vocabulary. Then a vocabulary\npointer is introduced to control when to predict\na target item from the item vocabulary or a word\nfrom the ordinary vocabulary in the generation pro-\ncess. The introduced item vocabulary and vocab-\nulary pointer effectively unify the two individual\nprocesses of response generation and item recom-\nmendation into one single framework in a more\nconsistent fashion.\nTo better illustrate the motivation of our work,\nTable 1 shows a conversation example on looking\nfor horrible movies and the corresponding replies\ngenerated by four models (ReDial (Li et al., 2018),\nKBRD (Chen et al., 2019), KGSF (Zhou et al.,\n2020a), OUR) together with the ground truth reply\nin the corpus (Human). As we can see, the previ-\nous work tends to generate short (e.g., ‚ÄúKBRD: or\nIt (2017)‚Äù) or in-coherent responses (e.g., ‚ÄúKGSF:\nI would recommend watching it.‚Äù), which is re-\nsulted from the overÔ¨Åtting on the small dataset as\nwe mentioned before. Different from them, our\nmodel can generate more informative and coherent\nsentences which shows a better chatting ability. In\nadditon, we can notice that KGSF fails to raise a\nrecommendation in the response ‚ÄúI would recom-\nmend watching it‚Äù (‚Äúit‚Äù should be replaced with\na speciÔ¨Åc item name in a successful combination\nof generation and recommendation results), which\nis probably due to the insufÔ¨Åcient semantic knowl-\nedge learned and an ineffective copy mechanism.\nOur proposed uniÔ¨Åed PLM-based framework with\na vocabulary pointer can effectively solve the issue.\nFurthermore, to better investigate the end-to-end\nCRS system, we argue to evaluate the performance\nof recommendation by checking whether the Ô¨Å-\nnal responses contain the target items. Existing\nworks separately evaluate the performance of the\ntwo modules, i.e., dialogue generation and item\nrecommendation. However, a copy mechanism or\npointer network cannot always inject the recom-\nmended items into generated replies precisely and\nappropriately as we mentioned before. The per-\nformance of the Ô¨Ånal recommendations is actually\nlower than that of the recommender module. For\ninstance, the Recall@1 of the recommender mod-\nule in KGSF (Zhou et al., 2020a) is 3.9% while the\nactual performance is only 0.9% when evaluating\nthe Ô¨Ånal integrated responses (see Table 3).\nWe conduct extensive experiments on the pop-\nular benchmark REDIAL (Li et al., 2018). Our\nRecInDial model achieves a remarkable improve-\nment on the recommendation over the state-of-the-\nart, and the generated responses are also signiÔ¨Å-\ncantly better on automatic metrics as well as hu-\nman evaluation. Further ablation studies and quan-\ntitative and qualitative analyses demonstrate the\nsuperior performance of our approach.\nThe contributions of this work can be:\n‚Ä¢ We propose a PLM-based framework called\nRecInDial for conversational recommendation.\nRecInDial Ô¨Ånetunes the large-scale PLMs to-\ngether with a Relational Graph Convolutional\nNetwork to address the low-resource challenge\nin the current CRS.\n‚Ä¢ By introducing an extra item vocabulary with a\nvocabulary pointer, RecInDial effectively uniÔ¨Åes\ntwo components of item recommendation and re-\nsponse generation into a PLM-based framework.\n‚Ä¢ Extensive experiments show RecInDial signiÔ¨Å-\ncantly outperforms the state-of-the-art methods\non the evaluation of both dialogue generation and\nrecommendation.\n2 Related Work\nExisting works in CRS can be mainly divided into\ntwo categories, namely attribute-based CRS and\nopen-ended CRS.\nAttribute-based CRS. The attribute-based CRS\ncan be viewed as a question-driven task-oriented di-\nalogue system (Zhang et al., 2018; Sun and Zhang,\n2018). This kind of system proactively asks clariÔ¨Å-\ncation questions about the item attributes to infer\nuser preferences, and thus search for the optimal\ncandidates to recommend. There are various ask-\ning strategies studied by existing works, such as\nentropy-ranking based approach (Wu et al., 2018),\ngeneralized binary search based approaches (Zou\nand Kanoulas, 2019; Zou et al., 2020), reinforce-\nment learning based approaches (Chen et al., 2018;\nLei et al., 2020a; Deng et al., 2021), adversarial\nlearning based approach (Ren et al., 2020b) and\ngraph based approaches (Xu et al., 2020; Lei et al.,\n2020b; Ren et al., 2021; Xu et al., 2021). Another\nline of research on this direction address the trade-\noff issue between exploration ( i.e., asking ques-\ntions) and exploitation (i.e., making recommenda-\ntions) to achieve both the engaging conversations\nand successful recommendations, especially for the\ncold-start users. Some of them leverage bandit on-\nline recommendation methods to address cold-start\nscenarios (Li et al., 2010, 2016b; Christakopoulou\net al., 2016; Li et al., 2020), while others focus\non the asking strategy with fewer turns (Lei et al.,\n2020a,b; Shi et al., 2019; Sun and Zhang, 2018).\nOpen-ended CRS. Existing works (Li et al.,\n2018; Lei et al., 2018; Jiang et al., 2019; Ren et al.,\n2020a; Hayati et al., 2020; Ma et al., 2020; Liu\net al., 2020; Wang et al., 2022) on this direction ex-\nplore CRS through more free-form conversations,\nincluding proactively asking clariÔ¨Åcation questions,\nchatting with users, providing the recommenda-\ntion, etc. Multiple datasets have been released to\nhelp push forward the research in this area, such\nas REDIAL (Li et al., 2018), TG-R EDIAL (Chi-\nnese) (Zhou et al., 2020b), INSPIRED (Hayati\net al., 2020) and DuRecDial (Liu et al., 2020, 2021).\nLi et al. (2018) make the Ô¨Årst attempt on this di-\nrection and contribute the benchmark dataset RE-\nDIAL by the paired crowd-workers ( i.e., Seeker\nand Recommender). Follow-up studies (Chen et al.,\n2019; Zhou et al., 2020a,b) leverage the multiple\nexternal knowledge to enhance the performance\nof open-ended CRS. CR-Walker (Ma et al., 2020)\nis proposed to perform the tree-structured reason-\ning on the knowledge graph to introduce relevant\nitems, while MGCG (Liu et al., 2020) addresses\nthe transition policy from a non-recommendation\ndialogue to a recommendation-oriented one. Be-\nsides, Zhou et al. (2021) develop an open-source\ntoolkit CRSLab to further facilitate the research on\nthis direction. Most of these works utilize pointer\nnetwork (Gulcehre et al., 2016) or copy mecha-\nnism (Gu et al., 2016; Sha et al., 2018) to inject\nthe recommended items into generated replies. Our\n‚Ä¶movie<s>I like old school horror movies likeA Nightmare on Elm Street (1984)have you ever‚Ä¶\nSoftmax\nEmbeddingMatrixùëä!ùë•ùëä!+ùëä\"\nMaskedMulti-headself-attentionAdd&LayerNormPoint-wiseFeedForwardAdd&LayerNorm\nTransposedEmbeddingMatrixùëä!#‚Ñé$\n‚Ñé$ùëä!#\nDBpedia\nR-GCN\nextract\nItems\nlookuptable‚Ä¶\nSelf-Att Multiplication\n‚Ä¶ ‚Ä¶\n‚Ä¶\nGeneralVocabItemVocab\n‚Ä¶Map\n+Bias\nEntityDistribMovieDistributionVocabularyPointer1/00/1\n12x ùêª\nùëá%\nùêª%ùë°%\nùëè%\nFigure 1: Model overview of RecInDial.\nwork lies in the research of open-ended CRS. While\ndifferent from the previous work, we present a\nPLM-based framework for CRS, which Ô¨Ånetunes\nthe large-scale PLMs together with a pre-trained\nRelational Graph Convolutional Network (RGCN)\nto address the low-resource challenge in CRS.\nAnother line of related work lies in the end-to-\nend task-oriented dialogs (Wu et al., 2019; He et al.,\n2020; Raghu et al., 2021), which also require re-\nsponse generation based on a knowledge base but\nnot for recommendations.\n3 Methodology\nIn this section, we present our proposed RecInDial\nmodel. Figure 1 shows the model overview. We\nÔ¨Årst formalize the conversational recommendation\ntask and then detail our PLM-based response gener-\nation module together with the vocabulary pointer.\nAfter that, we introduce how to incorporate the\nknowledge from an item-oriented knowledge graph\nwith an RGCN into the model. Finally, we describe\nthe model training objectives.\n3.1 Problem Formalization\nThe input of a CRS model contains the history\ncontext of a conversation, which is denoted as a\nsequence of utterances {t1,t2,...,t m}in chrono-\nlogical order ( m represents the number of utter-\nances). Each utterance is either given by the\nseeker (user) or recommender (the model), which\ncontains the token sequence {wi,1,wi,2,...,w i,ni}\n(1 ‚â§i ‚â§m), where wij is the j-th token in the\ni-th utterance and ni is the number of tokens in\ni-th utterance. Note that we deÔ¨Åne the name of an\nitem as a single token and do not tokenize it. The\noutput token sequence by the model is denoted as\n{wn+1,wn+2,...,w n+k}, where kis the number of\ngenerated tokens and n= ‚àëm\n1 ni is the total num-\nber of tokens in context. When the model conducts\nthe recommendation, it will generate an item token\nwn+i (1 ‚â§i‚â§k) together with the corresponding\ncontext. In this way, recommendation item and\nresponse are generated concurrently.\n3.2 Response Generation Model\nIn this subsection, we introduce how to extend\nPLMs to handle CRS task and produce items rec-\nommendation during the dialogue generation.\nPLM-based Response Generation. Given the\ninput ( i.e., the conversation history context\n{t1,t2,...,t m}), we concatenate the history utter-\nances into the contextC = {w1,w2,...,w n}where\nn is the total number of tokens in the context.\nThen the probability of the generated response\nR= {wn+1,wn+2,...,w n+k}is formulated as:\nPLM(R|C) =\nn+k‚àè\ni=n+1\np(wi|w1,...,w i‚àí1). (1)\nwhere PLM(¬∑|¬∑) denotes the PLMs of Trans-\nformer (Vaswani et al., 2017) architecture. For\na multi-turn conversation, we can constructN such\ncontext-response pairs, where N is the number of\nutterances by the recommender. Then we Ô¨Ånetune\nthe PLMs on all possible (C,R) pairs constructed\nfrom the dialogue corpus. By this means, not only\ndoes our model inherit the strong language genera-\ntion ability of the PLMs, but also simultaneously\ncan learn how to generate the recommendation ut-\nterances on the relatively small CRS dataset.\nPLM-based Item Generation. To integrate the\nitem recommendation into the generation process\nof PLMs, we propose to expand the generation\nvocabulary of PLMs by including an extra item vo-\ncabulary. We devise a vocabulary pointer to control\nwhen to generate tokens from the ordinary vocabu-\nlary or from the item vocabulary. Concretely, we\nregard an item as a single token and add all items\ninto the item vocabulary. Hence, our model can\nlearn the relationship between context words and\ncandidate items. Such a process integrates the re-\nsponse generation and item recommendation into\na uniÔ¨Åed model that can perform the end-to-end\nrecommendation through dialogue generation.\nVocabulary Pointer. We Ô¨Årst preprocess the di-\nalogue corpus and introduce two special tokens\n[RecS] and [RecE] to indicate the start and end\npositions of the item in utterance. Then we divide\nAlgorithm 1 V ocabulary Pointer based Generation\nfor RecInDial\nInput: history context C, general and item vocabulary VG,\nVR\nOutput: generated response R\nextract appeared entities from Cas user preference Tu\ncompute knowledge-aware bias bu based on Tu using Eq. 5\nto 8\nR‚Üê{}\nn‚Üê0\nIvp ‚Üê0, V ‚ÜêVG\nwhile n<N max do\nwn = Decode(C‚ãÉR,V, bu) ‚äøGenerate wn based\non the previous tokens and bias from V\nR‚ÜêR‚ãÉ{wn}\nif wn = [RecS] then ‚äøGenerate tokens from VR\nIvp ‚Üê1, V ‚ÜêVR\nelse if wn = [RecE] then ‚äøGenerate tokens from VG\nIvp ‚Üê0, V ‚ÜêVG\nelse if wn = [EOS] then ‚äøGeneration is done\nbreak\nend if\nn‚Üên+ 1\nend while\nreturn R\nthe whole vocabulary V into VG and VR, where\nVG includes the general tokens (i.e., tokens in the\noriginal vocabulary of PLM) and [RecS] while\nVR contains the all item tokens and [RecE]. We\nthen introduce a binary Vocabulary PointerIvp to\nguide the generation from VG or VR. The model\ngenerates tokens in VG when Ivp = 0, and gener-\nates the tokens in VR when Ivp = 1, which can be\nformulated as follows:\np(w= wi) = exp(œÜI(wi) +Àúhi)‚àë\nwj‚ààV exp(œÜI(wj) +Àúhj)\n(2)\nœÜI(wj) =\nÔ£±\nÔ£¥Ô£¥Ô£¥Ô£≤\nÔ£¥Ô£¥Ô£¥Ô£≥\n0, I vp = 0,wj ‚ààVG or\nIvp = 1,wj ‚ààVR,\n‚àíinf, I vp = 1,wj ‚ààVG or\nIvp = 0,wj ‚ààVR\n, (3)\nwhere Àúh= hLWT\ne is the feature vector before the\nsoftmax layer in Figure 1, Àúhi means the feature\nvalue of the i-th token. Ivp is initialized as 0 at\nthe beginning of the generation and won‚Äôt change\nuntil the model produces [RecS] or [RecE]. It\nchanges to 1 if the model produces [RecS] (i.e.,\nthe model begins to generate items) and changes\nback to 0 if [RecE] is emitted. Such a proce-\ndure continues until the turn is Ô¨Ånished. With the\nVocabulary Pointer, our model can alternatively\nswitch between generating response words and rec-\nommending items based on its previous outputs in\na uniÔ¨Åed fashion.\nTo help readers better understand the V ocabulary\nPointer mechanism, we summarize the process in\nAlgorithm 1.\n3.3 Knowledge Graph Enhanced Finetuning\nDue to the difÔ¨Åculty of fully understanding user\npreferences by the conversation context, it is neces-\nsary to introduce the external knowledge to encode\nthe user preferences when Ô¨Ånetuning response gen-\neration model. Inspired by the previous work (Chen\net al., 2019; Zhou et al., 2020a), we also employ a\nknowledge graph from DBpedia (Lehmann et al.,\n2015) and perform entity linking (Daiber et al.,\n2013) to the items in the dataset, which helps better\nmodel the user preferences. A triple in DBpedia\nis denoted by < e1,r,e2 >, where e1,e2 ‚ààE are\nitems or entities from the entity setEand ris entity\nrelation from the relation set R.\nRelational Graph Propagation. We utilize R-\nGCN (Schlichtkrull et al., 2018) to encode struc-\ntural and relational information in the knowledge\ngraph to entity hidden representations. Formally,\nthe representation of node eat (l+ 1)-th layer is:\nh(l+1)\ne = œÉ(\n‚àë\nr‚ààR\n‚àë\ne‚Ä≤‚ààEre\n1\nZe,r\nW(l)\nr h(l)\ne‚Ä≤ + W(l)h(l)\ne ), (4)\nwhere h(l)\ne ‚ààRdE is the node representation of e\nat the l-th layer, and Er\ne denotes the set of neigh-\nboring nodes for eunder the relation r. W(l)\nr is\na learnable relation-speciÔ¨Åc transformation matrix\nfor the embedding from neighboring nodes with\nrelation r, while W(l) is another learnable matrix\nfor transforming the representations of nodes at the\nl-th layer and Ze,r is a normalization factor.\nAt the last layer L, structural and relational in-\nformation is encoded into the entity representation\nh(L)\ne for each e ‚ààE . The resulting knowledge-\nenhanced hidden representation matrix for entities\nin Eis denoted as H(L) ‚ààR|E|√ódE . We omit the\n(L) in the following paragraphs for simplicity.\nEntity Attention. Given a conversation context,\nwe Ô¨Årst collect the entities appeared in the context,\nand then we represent the user preference as Tu =\ne1,e2,...,e |Tu|, where ei ‚ààE . After looking up\nthe knowledge-enhanced representation table of\nentities in Tu from H, we get:\nHu = (h1,h2,..., h|Tu|), (5)\nwhere hi ‚ààRdE is the hidden vector of entity ei.\nThen the self-attention mechanism (Lin et al., 2017)\nis applied to Hu, which outputs a distribution Œ±u\nover |Tu|vectors:\nŒ±u = softmax(wa2tanh(Wa1HT\nu )), (6)\nwhere Wa1 ‚ààRda√ódE and wa2 ‚ààR1√óda are learn-\nable parameters. Then we get the Ô¨Ånal representa-\ntion for user history uas follows:\ntu = Œ±uHu. (7)\nKnowledge-Aware Bias. To incorporate the\nknowledge from the constructed knowledge graph\ninto our model while generating recommendation\nitems, we Ô¨Årst map the derived user representation\ntu into the item vocabulary space |VR|as follows:\nbu = tuHT Mb, (8)\nwhere Mb ‚ààR|E|√ó|VR|are learnable parameters.\nThen we add bu to the projection outputs before\nsoftmax operation in the generation as a bias. In\nthis way, our model can produce items in aware\nof their relational knowledge and thus enhance the\nperformance of recommendation.\n3.4 Recommendation in Beam Search\nTo embed the top-k item recommendation into the\ngeneration, we develop a revised beam search de-\ncoding. SpeciÔ¨Åcally, when we Ô¨Ånish the generation\nfor one response, we Ô¨Årst check whether it contains\nthe item names (i.e., whether it generates recom-\nmendations). If yes, then we choose the top-k items\nbetween [RecS] and [RecE] according to the\nprobability scores at current time-step.\n3.5 Learning Objectives\nThere are two objectives, i.e., node representation\nlearning on knowledge graph and the Ô¨Ånetuning\nof response generation model. For the former, we\noptimize the R-GCN and the self-attention network\nbased on the cross entropy of item prediction:\nLkg =\n‚àë\n(u,i)‚ààD1\n‚àílog( exp(tuHT )i‚àë\nj exp(tuHT )j\n), (9)\nwhere the item iis the ground-truth item and uis\nthe corresponding user history, while D1 contains\nall training instances and tuHT ‚ààR|E|.\nFor the latter, we optimize another cross entropy\nloss for all generated responses, denoted as R. The\nfollowing formula summarizes the process:\nLgen =\n‚àë\n(C,R)‚ààD2\n‚àë\nwi‚ààR\n‚àílog(p(wi|w<i,C)), (10)\nwhere p(wi) refers to Eq. 2 and D2 contains all\n(C,R) pairs constructed from the dataset. We train\nthe whole model end-to-end with the joint effects\nof the two objectives Lkg + Lgen.\nConversations\n# of convs 10006\n# of utterances 182150\n# of users 956\navg token length 6.8\navg turn # 18.2\nMovies\n# of mentions 51699\n# of movies 6924\navg mentions 7.5\nmax mentions 1024\nmin mentions 1\nTable 2: Statistics of ReDial dataset. ‚Äú#\" means number\nand ‚Äúavg\" refers to average.\n4 Experimental Setup\nDatasets. We evaluate our model on the bench-\nmark dataset REDIAL (Li et al., 2018). Due to the\ncollection difÔ¨Åculty of the real world data, most\nthe previous work (Li et al., 2018; Chen et al.,\n2019; Zhou et al., 2020a) only conducts experi-\nments on this single dataset. The statistics of RE-\nDIAL dataset is shown in Table 2. Detailed statis-\ntics of movie mentions are shown in Figure 2(a).\nMost of the movies occur less than 5 times in the\ndataset, which indicates an obvious data imbalance\nproblem in the REDIAL . We also show the re-\nlationship between the average number of movie\nmentions and the number of dialog turns in Fig-\nure 2(b). As we can see, there are less than 2 movie\nmentions when the dialogue turn number is less\nthan 5. Finally, we follow (Li et al., 2018) to split\nthe dataset into 80-10-10, for training, validation\nand test.\nParameter Setting. We Ô¨Ånetune the small size\npre-trained DialoGPT model2, which consists of 12\ntransformer layers. The dimension of embeddings\nis 768. It is trained on 147M multi-turn dialogues\nfrom Reddit discussion threads. For the knowledge\ngraph (KG), both the entity embedding size and the\nhidden representation size are set to 128, and we\nset the layer number for R-GCN to 1. For BART\nbaseline, we Ô¨Ånetune the base model3 with 6 layers\nin each of the encoder and decoder, and a hidden\nsize of 1024. For GPT-2 baseline, we Ô¨Ånetune the\nsmall model4. For all model‚Äôs training, we adopt\nAdam optimizer and the learning rate is chosen\nfrom {1e‚àí5, 1e‚àí4}. The batch size is chosen from\n{32, 64}, the gradient accumulation step is set to\n8, and the warm-up step is chosen from {500, 800,\n1000}. All the hyper-parameters are determined by\ngrid-search.\n2https://huggingface.co/microsoft/\nDialoGPT-small\n3https://huggingface.co/facebook/\nbart-base\n4https://huggingface.co/gpt2\n4567\n90013101023672090018002700360045005400\n[1,5)[5,10)[10,100)[100, 200)[200,400)[400,800)>= 800\nFrequency\n(a) Movie # Distribution\n0246810\n1591317212529\nAVG Movie Mentions (b) Position Distribution\nFigure 2: For Figure 2(a), X-axis: the movie mentions\nrange; Y-axis: movie numbers. For Figure 2(b), X-axis:\nturn positions; Y-axis: average movie mentions.\nBaselines and Comparisons. We Ô¨Årst introduce\ntwo baselines for recommender and dialogue mod-\nules, respectively. (1) Popularity. It ranks the\nmovie items according to their historical frequency\nin the training set without a dialogue module. (2)\nTransformer (Vaswani et al., 2017). It utilizes\na transformer-based encoder-decoder to generate\nresponses without recommender module.\nWe then compare the following baseline models\nin the experiment: (3) ReDial (Li et al., 2018). It\nconsists of a dialogue generation module based on\nHRED (Serban et al., 2017), a recommender mod-\nule based on auto-encoder (He et al., 2017), and a\nsentiment analysis module. (4) KBRD (Chen et al.,\n2019). It utilizes a knowledge graph from DBpedia\nto model the relational knowledge of contextual\nitems or entities, and the dialogue generation mod-\nule is based on the transformer architecture. (5)\nKGSF (Zhou et al., 2020a). It incorporates and\nfuses both word-level and entity-level knowledge\ngraphs to learn better semantic representations for\nuser preferences. (6) GPT-2. We directly Ô¨Ånetune\nGPT-2 and expand its vocabulary to include the\nitem vocabulary. (7) BART. We directly Ô¨Ånetune\nBART and expand its vocabulary to include the\nsame item vocabulary. (8) DialoGPT. We directly\nÔ¨Ånetune DialoGPT and expand its vocabulary to\ninclude same item vocabulary.\nFor our RecInDial, in addition to the full model\n(9) RecInDial, we also evaluate two variants: (10)\nRecInDial w/o VP, where we remove the vocabu-\nlary pointer; and (11) RecInDial w/o KG, where\nthe knowledge graph part is removed.\nEvaluation Metrics. As we discussed above, the\nprevious works evaluate the recommender and dia-\nlogue modules separately. Following the previous\nsetting (Chen et al., 2019; Zhou et al., 2020a), we\nevaluate the recommender module by Recall@k (k\n= 1, 10, 50). Besides, we also evaluate Recall@k\nin an end-to-end manner, i.e., to check whether the\nÔ¨Ånal produced response contains the target item.\nIn such a setting, the Recall@K score not only de-\npends on whether the ground truth item appears in\nthe top K recommendation list but also reply on if\nthe recommended item is successfully injected into\nthe generated sentences. Therefore, the end-to-end\nevaluation is fair for all models and applicable for\nK = 1, 10, 50. For the dialogue module, automatic\nmetrics include: (1) Fluency: perplexity (PPL)\nmeasures the conÔ¨Ådence of the generated responses.\n(2) Relevance: BLEU-2/4 (Papineni et al., 2002)\nand Rouge-L (Lin, 2004). (3) Diversity: Distinct-n\n(Dist-n) (Li et al., 2016a) are deÔ¨Åned as the number\nof distinct n-grams divided by the total amount of\nwords. SpeciÔ¨Åcally, we use Dist-2/3/4 at the sen-\ntence level to evaluate the diversity of generated\nresponses. Besides, we also employ Item Ratio in-\ntroduced in KGSF (Zhou et al., 2020a) to measure\nthe ratio of items in the generated responses.\n5 Experimental Results\nIn this section, we Ô¨Årst report the comparison re-\nsults on recommendation and response generation.\nThen we discuss the human evaluation results. Af-\nter that, we show an example to illustrate how our\nmodel works, followed by qualitative analysis.\n5.1 Results on Recommendation\nThe main experimental results for our RECINDIAL\nand baseline models on recommendation side are\npresented in Table 3. And we can draw several\nobservations from the results.\nThere is a signiÔ¨Åcant gap between the perfor-\nmance of the recommender module and the perfor-\nmance of the Ô¨Ånal integrated system. KGSF, the\nstate-of-the-art model, achieves 3.9% Recall@1\nin the recommender module evaluation but yields\nonly 0.9% in the evaluation of the Ô¨Ånal produced\nresponses. This indicates that the integration strate-\ngies utilized by previous methods have signiÔ¨Åcant\nharm on the recommendation performance.\nFinetuning PLMs on the small CRS dataset\nis effective. As we can see, compared to non-\nPLM based methods, directly Ô¨Ånetuning GPT-\n2/BART/DialoGPT on the REDIAL achieves the\nobvious performance gain on recommendation.\nOur RecInDial model signiÔ¨Åcantly outperforms\nthe SOTAs on recommendation performance. As\nshown in Table 2, our RecInDial achieves the best\nRecall@k (k = 1, 10, 50) scores under the end-to-\nend evaluation, which demonstrates the superior\nModels Eval on Rec Module End-to-End Eval\nR@1 R@10 R@50 R@1 R@10 R@50\nBaselines\nPopularity 1.2 6.1 17.9 1.2 6.1 17.9\nReDial 2.4 14.0 32.0 0.7 4.4 10.0\nKBRD 3.1 15.0 33.6 0.8 3.8 8.8\nKGSF 3.9 18.3 37.8 0.9 4.2 8.8\nGPT-2 - - - 1.4 6.5 14.4\nBART - - - 1.5 - -\nDialoGPT - - - 1.7 7.1 13.8\nRecInDial - - - 3.1 14.0 27.0\nTable 3: Main comparison results on recommendation.\nR@k refers to Recall@k. RecInDial outperms the base-\nlines signiÔ¨Åcantly (p<0.01, paired t-test).\nModels R@1 R@10 R@50 Item Ratio BLEU Rouge-L\nRecInDial 3.1 14.0 27.0 43.5 20.7 17.6\nRecInDialw/oVP 1.8 8.8 19.5 17.8 18.5 14.6\nRecInDialw/oKG 2.3 9.4 20.1 39.8 17.7 12.9\nTable 4: Comparison results on ablation study.\nperformance of the PLMs with the uniÔ¨Åed design.\n5.2 Results on Dialogue Generation\nSince CRS aims to recommend items during natu-\nral conversations, we conduct both automatic and\nhuman evaluations to investigate the quality of gen-\nerated responses by RecInDial and baselines.\nAutomatic Evaluation. Table 5 shows the\nmain comparison results on Dist-2/3/4, BLEU-2/4,\nRouge-L and PPL. As we can see, RecInDial signif-\nicantly outperforms all baselines on Dist-n, which\nindicates that PLM helps generate more diverse\nresponses. Previous works suffer from the low-\nresource issue due to the small crowd-sourcing\nCRS dataset and tend to generate boring and sin-\ngular responses. On the other hand, our RecInDial\nmodel tends to recommend items more frequently,\nas the Item Ratio score of RecInDial is much higher\nthan those of baselines. Besides, our RecInDial and\nPLM-based methods consistently achieve remark-\nable improvement over non-PLM based methods\non all metrics, which demonstrates the superior\nperformance of PLMs on dialogue generation.\nHuman Evaluation. To further investigate the\neffectiveness of RecInDial, we conduct a human\nevaluation experiment, where four crowd-workers\nare employed to score on 100 context-response\npairs that are randomly sampled from the test set.\nThen, we collect the generation results of RecIn-\nDial and the baseline models and compare their\nperformance on the following three aspects: (1)\nFluency. Whether a response is organized in reg-\nular English grammar and easy to understand. (2)\nInformativeness. Whether a response is mean-\ningful and not a ‚Äúsafe response‚Äù, and repetitive\nModels Dist-2 Dist-3 Dist-4 IR BL-2 BL-4 Rouge-L PPL‚Üì\nBaselines\nTransformer 14.8 15.1 13.7 19.4 - - - -\nReDial 22.5 23.6 22.8 15.8 17.8 7.4 16.9 61.7\nKBRD 26.3 36.8 42.3 29.6 18.5 7.4 17.1 58.8\nKGSF 28.9 43.4 51.9 32.5 16.4 7.4 14.3 131.1\nGPT-2 35.4 48.6 44.1 14.5 17.1 7.7 11.3 56.3\nBART 37.6 49.0 43.5 16.0 17.8 9.3 13.1 55.6\nDialoGPT 47.6 55.9 48.6 15.9 16.7 7.8 12.3 56.0\nRecInDial51.8 62.4 59.8 43.5 20.4 11.0 17.6 54.1\nTable 5: Automatic metrics on generated responses. IR\ndenotes the Item Ratio.\nModels Fluency Informative Coherence Kappa\nHUMAN 1.93 1.70 1.69 0.80\nReDial 1.90 1.28 1.21 0.75\nKBRD 1.92 1.32 1.26 0.78\nKGSF 1.91 1.05 1.10 0.85\nRecInDial 1.93 1.65 1.60 0.84\nTable 6: Human evaluation results.\nresponses are regarded as uninformative. (3) Co-\nherence. Whether a response is coherent with the\nprevious context. The crowd-workers give a score\non the scale of [0, 1, 2] to show the quality of the\nresponses, and higher scores indicate better quali-\nties.\nWe calculate the average score for each model,\nas well as the ground truth that humans give. As\nshown in Table 6, our model shows better perfor-\nmance than all the baselines. Interestingly, ground-\ntruth Human cannot get a 100% correctness in all\nthe four evaluation metrics. The reason may be that\nwords and phrases sent by human annotators on\nAMT platform sometimes are the casual usage pop-\nular on Internet, which has the wrong grammar. For\nthe Ô¨Çuency, all models generate Ô¨Çuent utterances\nand show similar performance. For the informative-\nness, our RecInDial achieves better performance\nthan the baselines, which indicates RecInDial tends\nto generate more meaningful responses.\n5.3 Ablation Study\nWe then report the performance comparisons on\nRecInDial‚Äôs variants. Table 4 shows the end-to-end\nrecommendation performance and generation re-\nsults. Removing the vocabulary pointer leads to\nsigniÔ¨Åcant drops on R@k and Item Ratio. This\nindicates Vocabulary Pointer (VP) introduced in\nRecInDial is crucial to the performance of item\nrecommendation. The reason is that the generation\nprocess would lose the guidance to switch between\ngeneral tokens and recommended items without\nthe help of the vocabulary pointer. Besides, we\ncan Ô¨Ånd that knowledge graph enhanced Ô¨Ånetuning\nhelps achieve better recommendation performance.\nIntroducing the node representations learned on the\nknowledge graph can model the user preference\nbetter, which could further enhance the recommen-\ndation performance.\n5.4 Qualitative Analysis\nIn this subsection, we present a conversation exam-\nple to illustrate how our model works in practice.\nConversation Example\nUser: I like to watch scary movies\nHuman: Tell me what kind of movie do you like? Did\nyou watch It (2017)? And It (1990)?\nKBRD: Hi, what kind of movies do you like?\nKGSF: Hello!\nRecInDial: Have you seen It (2017)?\nUser: I have seen the old one\nHuman: Did you watch The Exorcist (1973)?\nKBRD: Get Out (2017)\nKGSF: I would recommend Get Out (2017)\nRecInDial: How about Psycho (1960)?\nUser: I have not seen the new one I do like the\nexorcist\nHuman: Ok, you should watch the new one even\nthough I would rather the old one. And did\nyou watch The Last Exorcism (2010)? I hope\nI have helped\nKBRD: I haven‚Äôt seen that one\nKGSF: I would recommend it\nRecInDial: Yes, it was good. I hope you enjoy it. Have\na good day. Bye\nTable 7: A conversation example on movie item rec-\nommendation. The responses of KBRD, KGSF and\nRecInDial are from the test results for corresponding\nmodels. Human responses are ground-truth.\nIn Table 7, the Seeker states that he likes scary\nmovies. Our model successfully captured the key-\nword of ‚Äúscary‚Äù and recommends a famous scary\nmovie ‚ÄúIt (2017)‚Äù while the state-of-the-art model\nKGSF produces a safe response ‚ÄúHello!‚Äù, which\nshows our RecInDial can generate the responses\nthat are more coherent with the context. Interest-\ningly, after the Seeker says he watched the old ‚ÄúIt\n(1990)‚Äù, our model recommends another horror\nmovie ‚ÄúPsycho (1960)‚Äù also released in the last\ncentury. The possible reason is that RecInDial in-\nfers the seeker is interested in old horror movies.\nThe example in Table 7 shows that our RecInDial\ntends to generate a more informative response than\nKGSF. In addition, we Ô¨Ånd that KGSF always gen-\nerates ‚ÄúI would recommend Item‚Äù (Item is replaced\nwith Get out (2017) in this example) and ‚ÄúI would\nrecommend it.‚Äù. The Ô¨Årst response pattern success-\nfully integrates the movie item into the response,\n00.10.20.30.40.50.6\n[1,5)[5,10)[10,100)[100, 200)[200,400)[400,800)>= 800\nRecall@30Recall@50\n(a) Recall over Mentions\n00.050.10.150.20.250.3\n123456789101112\nRecall@30Recall@50 (b) Recall over Turn #\nFigure 3: Y-axis: Recall. For Fig. 3(a), X-axis: Movie\nmentions range. For Fig. 3(b), X-axis: turn numbers.\nwhile the second fails to make a complete recom-\nmendation, which reveals the drawback of the copy\nmechanism in KGSF.\n5.5 Further Analysis\nAnalysis on Data Imbalance. As we discussed\naforementioned, the movie occurrence frequency\nshows an imbalanced distribution over different\nmovies (see Figure 2(a)). To investigate the effect,\nwe report the Recall@30 and Recall@50 scores\nover movie mentioned times in Figure 3(a). As we\ncan see, the recall scores for low-frequency movies\n(with mentioned times less than 10) are much lower\nthan those high-frequency movies (with > 100\nmentions). However, most of the movies (5467 out\nof 6924 movies) in the REDIAL dataset are low-\nfrequency movies, which leads to relatively low\nresults in the overall performance.\nAnalysis on Cold Start. REDIAL dataset suffers\nfrom the cold-start problem. It is hard for models\nto recommend precise items in the Ô¨Årst few turns\nof the conversation. We report the Recall@30 and\nRecall@50 scores of our RecInDial over different\ndialogue turns in Figure 3(b). Generally, we can see\nthat the recall scores are getting better with richer\ninformation gradually obtained from dialogue in-\nteractions. The scores begin to drop when there\nare more than 5 turns. The possible reason is that\nas the conversation goes deeper, the Seekers are\nno longer satisÔ¨Åed with the recommended high-\nfrequency movies but prefer more personalized rec-\nommendations, which makes it more difÔ¨Åcult to\npredict in practice.\n6 Conclusion\nThis paper presents a novel uniÔ¨Åed PLM-based\nframework called RecInDial for CRS, which inte-\ngrates the item recommendation into the genera-\ntion process. SpeciÔ¨Åcally, we Ô¨Ånetune the large-\nscale PLMs together with a relational graph con-\nvolutional network on an item-oriented knowledge\ngraph. Besides, we design a vocabulary pointer\nmechanism to unify the response generation and\nitem recommendation into the existing PLMs. Ex-\ntensive experiments on the CRS benchmark dataset\nREDIAL show that RecInDial signiÔ¨Åcantly outper-\nforms the state-of-the-art methods.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their feedback and suggestions. The research\ndescribed in this paper is partially supported by\nHKSAR ITF No. ITT/018/22LP.\nReferences\nQibin Chen, Junyang Lin, Yichang Zhang, Ming Ding,\nYukuo Cen, Hongxia Yang, and Jie Tang. 2019. To-\nwards knowledge-based recommender dialog sys-\ntem. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1803‚Äì1813, Hong Kong, China. Association for\nComputational Linguistics.\nYihong Chen, Bei Chen, Xuguang Duan, Jian-Guang\nLou, Yue Wang, Wenwu Zhu, and Yong Cao.\n2018. Learning-to-ask: Knowledge acquisition via\n20 questions. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, KDD 2018, London, UK,\nAugust 19-23, 2018, pages 1216‚Äì1225. ACM.\nKonstantina Christakopoulou, Filip Radlinski, and\nKatja Hofmann. 2016. Towards conversational rec-\nommender systems. In Proceedings of the 22nd\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, San Francisco,\nCA, USA, August 13-17, 2016 , pages 815‚Äì824.\nACM.\nJoachim Daiber, Max Jakob, Chris Hokamp, and\nPablo N Mendes. 2013. Improving efÔ¨Åciency and\naccuracy in multilingual entity extraction. In Pro-\nceedings of the 9th International Conference on Se-\nmantic Systems, pages 121‚Äì124.\nYang Deng, Yaliang Li, Fei Sun, Bolin Ding, and Wai\nLam. 2021. UniÔ¨Åed conversational recommendation\npolicy learning via graph-based reinforcement learn-\ning. arXiv preprint arXiv:2105.09710.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1631‚Äì1640, Berlin, Germany. Association for\nComputational Linguistics.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,\nBowen Zhou, and Yoshua Bengio. 2016. Pointing\nthe unknown words. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n140‚Äì149, Berlin, Germany. Association for Compu-\ntational Linguistics.\nShirley Anugrah Hayati, Dongyeop Kang, Qingxi-\naoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. IN-\nSPIRED: Toward sociable recommendation dialog\nsystems. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 8142‚Äì8152, Online. Associa-\ntion for Computational Linguistics.\nJunhua He, Hankz Hankui Zhuo, and Jarvan Law.\n2017. Distributed-representation based hybrid rec-\nommender system with short item descriptions.\narXiv preprint arXiv:1703.04854.\nZhenhao He, Yuhong He, Qingyao Wu, and Jian Chen.\n2020. Fg2seq: Effectively encoding knowledge for\nend-to-end task-oriented dialog. In ICASSP 2020-\n2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages\n8029‚Äì8033. IEEE.\nShaojie Jiang, Pengjie Ren, Christof Monz, and\nMaarten de Rijke. 2019. Improving neural response\ndiversity with frequency-aware cross-entropy loss.\nIn The World Wide Web Conference, WWW 2019,\nSan Francisco, CA, USA, May 13-17, 2019 , pages\n2879‚Äì2885. ACM.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nS√∂ren Auer, et al. 2015. Dbpedia‚Äìa large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web, 6(2):167‚Äì195.\nWenqiang Lei, Xiangnan He, Yisong Miao, Qingyun\nWu, Richang Hong, Min-Yen Kan, and Tat-Seng\nChua. 2020a. Estimation-action-reÔ¨Çection: Towards\ndeep interaction between conversational and recom-\nmender systems. In WSDM ‚Äô20: The Thirteenth\nACM International Conference on Web Search and\nData Mining, Houston, TX, USA, February 3-7,\n2020, pages 304‚Äì312. ACM.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun\nRen, Xiangnan He, and Dawei Yin. 2018. Sequicity:\nSimplifying task-oriented dialogue systems with sin-\ngle sequence-to-sequence architectures. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1437‚Äì1447, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nWenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong\nMiao, Xiang Wang, Liang Chen, and Tat-Seng Chua.\n2020b. Interactive path reasoning on graph for con-\nversational recommendation. In KDD ‚Äô20: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August\n23-27, 2020, pages 2073‚Äì2083. ACM.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110‚Äì119, San Diego, California. Association\nfor Computational Linguistics.\nLihong Li, Wei Chu, John Langford, and Robert E.\nSchapire. 2010. A contextual-bandit approach to\npersonalized news article recommendation. In Pro-\nceedings of the 19th International Conference on\nWorld Wide Web, WWW 2010, Raleigh, North Car-\nolina, USA, April 26-30, 2010 , pages 661‚Äì670.\nACM.\nRaymond Li, Samira Ebrahimi Kahou, Hannes Schulz,\nVincent Michalski, Laurent Charlin, and Chris Pal.\n2018. Towards deep conversational recommenda-\ntions. In Advances in Neural Information Process-\ning Systems 31: Annual Conference on Neural Infor-\nmation Processing Systems 2018, NeurIPS 2018, De-\ncember 3-8, 2018, Montr√©al, Canada , pages 9748‚Äì\n9758.\nShijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He,\nPeng Jiang, and Tat-Seng Chua. 2020. Seamlessly\nunifying attributes and items: Conversational rec-\nommendation for cold-start users. arXiv preprint\narXiv:2005.12979.\nShuai Li, Alexandros Karatzoglou, and Claudio Gen-\ntile. 2016b. Collaborative Ô¨Åltering bandits. In Pro-\nceedings of the 39th International ACM SIGIR con-\nference on Research and Development in Informa-\ntion Retrieval, SIGIR 2016, Pisa, Italy, July 17-21,\n2016, pages 539‚Äì548. ACM.\nZujie Liang, Huang Hu, Can Xu, Jian Miao, Yingy-\ning He, Yining Chen, Xiubo Geng, Fan Liang,\nand Daxin Jiang. 2021. Learning neural templates\nfor recommender dialogue system. arXiv preprint\narXiv:2109.12302.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74‚Äì81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZhouhan Lin, Minwei Feng, C√≠cero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sen-\ntence embedding. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu,\nand Wanxiang Che. 2021. Durecdial 2.0: A bilin-\ngual parallel corpus for conversational recommenda-\ntion. arXiv preprint arXiv:2109.08877.\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu,\nWanxiang Che, and Ting Liu. 2020. Towards con-\nversational recommendation over multi-type dialogs.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1036‚Äì1049, Online. Association for Computational\nLinguistics.\nWenchang Ma, Ryuichi Takanobu, Minghao Tu, and\nMinlie Huang. 2020. Bridging the gap between con-\nversational reasoning and interactive recommenda-\ntion. arXiv preprint arXiv:2010.10333.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311‚Äì318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nDinesh Raghu, Atishya Jain, Sachindra Joshi, et al.\n2021. Constraint based knowledge base distillation\nin end-to-end task oriented dialogs. arXiv preprint\narXiv:2109.07396.\nPengjie Ren, Zhumin Chen, Christof Monz, Jun Ma,\nand Maarten de Rijke. 2020a. Thinking globally,\nacting locally: Distantly supervised global-to-local\nknowledge selection for background based conver-\nsation. In Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, volume 34, pages 8697‚Äì8704.\nXuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang,\nZi Huang, and Kai Zheng. 2021. Learning to ask\nappropriate questions in conversational recommen-\ndation. arXiv preprint arXiv:2105.04774.\nXuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang,\nNguyen Quoc Viet Hung, Zi Huang, and Xiangliang\nZhang. 2020b. Crsal: Conversational recommender\nsystems with adversarial learning. ACM Transac-\ntions on Information Systems (TOIS), 38(4):1‚Äì40.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European semantic web confer-\nence, pages 593‚Äì607. Springer.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron C. Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating di-\nalogues. In Proceedings of the Thirty-First AAAI\nConference on ArtiÔ¨Åcial Intelligence, February 4-9,\n2017, San Francisco, California, USA, pages 3295‚Äì\n3301. AAAI Press.\nLei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Su-\njian Li, Baobao Chang, and Zhifang Sui. 2018.\nOrder-planning neural text generation from struc-\ntured data. In Proceedings of the Thirty-Second\nAAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-\n18), the 30th innovative Applications of ArtiÔ¨Åcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in ArtiÔ¨Åcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5414‚Äì5421. AAAI Press.\nChen Shi, Qi Chen, Lei Sha, Hui Xue, Sujian Li, Lintao\nZhang, and Houfeng Wang. 2019. We know what\nyou will ask: A dialogue system for multi-intent\nswitch and prediction. In CCF International Confer-\nence on Natural Language Processing and Chinese\nComputing, pages 93‚Äì104. Springer.\nYueming Sun and Yi Zhang. 2018. Conversational rec-\nommender system. In The 41st International ACM\nSIGIR Conference on Research & Development in\nInformation Retrieval, SIGIR 2018, Ann Arbor, MI,\nUSA, July 08-12, 2018, pages 235‚Äì244. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nLingzhi Wang, ShaÔ¨Åq Joty, Wei Gao, Xingshan Zeng,\nand Kam-Fai Wong. 2022. Improving conversa-\ntional recommender system via contextual and time-\naware modeling with less domain-speciÔ¨Åc knowl-\nedge. arXiv preprint arXiv:2209.11386.\nChien-Sheng Wu, Richard Socher, and Caiming\nXiong. 2019. Global-to-local memory pointer net-\nworks for task-oriented dialogue. arXiv preprint\narXiv:1901.04713.\nXianchao Wu, Huang Hu, Momo Klyen, Kyohei\nTomita, and Zhan Chen. 2018. Q20: Rinna riddles\nyour mind by asking 20 questions. Japan NLP.\nHu Xu, Seungwhan Moon, Honglei Liu, Bing Liu,\nPararth Shah, Bing Liu, and Philip Yu. 2020. User\nmemory reasoning for conversational recommenda-\ntion. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pages 5288‚Äì\n5308, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nKerui Xu, Jingxuan Yang, Jun Xu, Sheng Gao, Jun\nGuo, and Ji-Rong Wen. 2021. Adapting user pref-\nerence to online feedback in multi-round conversa-\ntional recommendation. In Proceedings of the 14th\nACM International Conference on Web Search and\nData Mining, pages 364‚Äì372.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270‚Äì\n278, Online. Association for Computational Linguis-\ntics.\nYongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang,\nand W. Bruce Croft. 2018. Towards conversational\nsearch and recommendation: System ask, user re-\nspond. In Proceedings of the 27th ACM Interna-\ntional Conference on Information and Knowledge\nManagement, CIKM 2018, Torino, Italy, October 22-\n26, 2018, pages 177‚Äì186. ACM.\nKun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan\nShang, Yuan Cheng, Wayne Xin Zhao, Yaliang Li,\nand Ji-Rong Wen. 2021. CRSLab: An open-source\ntoolkit for building conversational recommender sys-\ntem. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing: System Demonstrations ,\npages 185‚Äì193, Online. Association for Computa-\ntional Linguistics.\nKun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuan-\nhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020a.\nImproving conversational recommender systems via\nknowledge graph based semantic fusion. In KDD\n‚Äô20: The 26th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, Virtual Event, CA,\nUSA, August 23-27, 2020, pages 1006‚Äì1014. ACM.\nKun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke\nWang, and Ji-Rong Wen. 2020b. Towards topic-\nguided conversational recommender system. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 4128‚Äì4139,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nJie Zou, Yifan Chen, and Evangelos Kanoulas. 2020.\nTowards question-based recommender systems. In\nProceedings of the 43rd International ACM SIGIR\nconference on research and development in Infor-\nmation Retrieval, SIGIR 2020, Virtual Event, China,\nJuly 25-30, 2020, pages 881‚Äì890. ACM.\nJie Zou and Evangelos Kanoulas. 2019. Learning\nto ask: Question-based sequential bayesian product\nsearch. In Proceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge\nManagement, CIKM 2019, Beijing, China, Novem-\nber 3-7, 2019, pages 369‚Äì378. ACM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8840219974517822
    },
    {
      "name": "Overfitting",
      "score": 0.6301345229148865
    },
    {
      "name": "Inference",
      "score": 0.5591200590133667
    },
    {
      "name": "Language model",
      "score": 0.5400567650794983
    },
    {
      "name": "Vocabulary",
      "score": 0.5337990522384644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5108415484428406
    },
    {
      "name": "Machine learning",
      "score": 0.4981968402862549
    },
    {
      "name": "Natural language processing",
      "score": 0.47304338216781616
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4404732584953308
    },
    {
      "name": "Recommender system",
      "score": 0.42154809832572937
    },
    {
      "name": "Artificial neural network",
      "score": 0.15984395146369934
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}