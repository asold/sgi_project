{
  "title": "Spatial redundancy transformer for self-supervised fluorescence image denoising",
  "url": "https://openalex.org/W4389574259",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2162305261",
      "name": "Li Xinyang",
      "affiliations": [
        "Tsinghua University",
        "Tsinghua–Berkeley Shenzhen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4221377232",
      "name": "Hu, Xiaowan",
      "affiliations": [
        "Tsinghua–Berkeley Shenzhen Institute",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2361389489",
      "name": "Chen Xing-ye",
      "affiliations": [
        "Tsinghua University",
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2391399341",
      "name": "Fan Jiaqi",
      "affiliations": [
        "Tsinghua–Berkeley Shenzhen Institute",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1910374568",
      "name": "Zhao Zhi-feng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2566144086",
      "name": "Wu Jia-Min",
      "affiliations": [
        "Tsinghua University",
        "Chinese Institute for Brain Research"
      ]
    },
    {
      "id": "https://openalex.org/A2361325101",
      "name": "Wang, Haoqian",
      "affiliations": [
        "Shenzhen Institute of Information Technology",
        "Tsinghua University",
        "Tsinghua–Berkeley Shenzhen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2230167878",
      "name": "Dai, Qionghai",
      "affiliations": [
        "Chinese Institute for Brain Research",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2162305261",
      "name": "Li Xinyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221377232",
      "name": "Hu, Xiaowan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361389489",
      "name": "Chen Xing-ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2391399341",
      "name": "Fan Jiaqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1910374568",
      "name": "Zhao Zhi-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566144086",
      "name": "Wu Jia-Min",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2361325101",
      "name": "Wang, Haoqian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230167878",
      "name": "Dai, Qionghai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2540220299",
    "https://openalex.org/W2961955009",
    "https://openalex.org/W2554260837",
    "https://openalex.org/W3164737459",
    "https://openalex.org/W3194257905",
    "https://openalex.org/W4297243218",
    "https://openalex.org/W2794984043",
    "https://openalex.org/W2056370875",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W2964125708",
    "https://openalex.org/W2783616919",
    "https://openalex.org/W2950501364",
    "https://openalex.org/W2910683834",
    "https://openalex.org/W3081844073",
    "https://openalex.org/W3165822876",
    "https://openalex.org/W4294238402",
    "https://openalex.org/W3121644344",
    "https://openalex.org/W2793146153",
    "https://openalex.org/W3092768295",
    "https://openalex.org/W3207362856",
    "https://openalex.org/W3195625084",
    "https://openalex.org/W2902857081",
    "https://openalex.org/W2947930026",
    "https://openalex.org/W3007757852",
    "https://openalex.org/W3119430326",
    "https://openalex.org/W3178192988",
    "https://openalex.org/W4307715697",
    "https://openalex.org/W6846651620",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W3167557279",
    "https://openalex.org/W6796259599",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W1597866600",
    "https://openalex.org/W2039019529",
    "https://openalex.org/W1992858116",
    "https://openalex.org/W2787020583",
    "https://openalex.org/W2008013375",
    "https://openalex.org/W2120504118",
    "https://openalex.org/W2125505095",
    "https://openalex.org/W2934437784",
    "https://openalex.org/W2134396482",
    "https://openalex.org/W2969928967",
    "https://openalex.org/W4307041238",
    "https://openalex.org/W2007971802",
    "https://openalex.org/W3154637644",
    "https://openalex.org/W2171332611",
    "https://openalex.org/W4312885710",
    "https://openalex.org/W4376275281",
    "https://openalex.org/W4360999130",
    "https://openalex.org/W3211495999",
    "https://openalex.org/W4281952716",
    "https://openalex.org/W4322494725",
    "https://openalex.org/W2991167240",
    "https://openalex.org/W3135462507",
    "https://openalex.org/W4302362619",
    "https://openalex.org/W4225489826",
    "https://openalex.org/W4324344199",
    "https://openalex.org/W4293658735",
    "https://openalex.org/W2951816875",
    "https://openalex.org/W2061864499",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W4393529690",
    "https://openalex.org/W4393833703",
    "https://openalex.org/W4393526948",
    "https://openalex.org/W4393741274",
    "https://openalex.org/W6949069147",
    "https://openalex.org/W3104053397"
  ],
  "abstract": "Abstract Fluorescence imaging with high signal-to-noise ratios has become the foundation of accurate visualization and analysis of biological phenomena. However, the inevitable noise poses a formidable challenge to imaging sensitivity. Here we provide the spatial redundancy denoising transformer (SRDTrans) to remove noise from fluorescence images in a self-supervised manner. First, a sampling strategy based on spatial redundancy is proposed to extract adjacent orthogonal training pairs, which eliminates the dependence on high imaging speed. Second, we designed a lightweight spatiotemporal transformer architecture to capture long-range dependencies and high-resolution features at low computational cost. SRDTrans can restore high-frequency information without producing oversmoothed structures and distorted fluorescence traces. Finally, we demonstrate the state-of-the-art denoising performance of SRDTrans on single-molecule localization microscopy and two-photon volumetric calcium imaging. SRDTrans does not contain any assumptions about the imaging process and the sample, thus can be easily extended to various imaging modalities and biological applications.",
  "full_text": "Nature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1067\nnature computational science\nhttps://doi.org/10.1038/s43588-023-00568-2\nArticle\nSpatial redundancy transformer for self-\nsupervised fluorescence image denoising\nXinyang Li    1,2,3,9, Xiaowan Hu2,9, Xingye Chen1,3,4,9, Jiaqi Fan2,5, Zhifeng Zhao1,3, \nJiamin Wu    1,3,6,7 , Haoqian Wang    2,8  & Qionghai Dai    1,3,6,7 \nFluorescence imaging with high signal-to-noise ratios has become \nthe foundation of accurate visualization and analysis of biological \nphenomena. However, the inevitable noise poses a formidable challenge \nto imaging sensitivity. Here we provide the spatial redundancy denoising \ntransformer (SRDTrans) to remove noise from fluorescence images in \na self-supervised manner. First, a sampling strategy based on spatial \nredundancy is proposed to extract adjacent orthogonal training pairs, which \neliminates the dependence on high imaging speed. Second, we designed \na lightweight spatiotemporal transformer architecture to capture long-\nrange dependencies and high-resolution features at low computational \ncost. SRDTrans can restore high-frequency information without producing \noversmoothed structures and distorted fluorescence traces. Finally, we \ndemonstrate the state-of-the-art denoising performance of SRDTrans \non single-molecule localization microscopy and two-photon volumetric \ncalcium imaging. SRDTrans does not contain any assumptions about the \nimaging process and the sample, thus can be easily extended to various \nimaging modalities and biological applications.\nThe rapid development of intravital imaging techniques enables \nresearchers to observe biological structures and activities at microm-\neter and even nanometer scales1,2. As an imaging method with great \nprevalence, fluorescence microscopy has contributed to the discov-\nery of a series of new physiological and pathological mechanisms \ndue to its high spatiotemporal resolution and molecular specific -\nity3–5. The fundamental goal of fluorescence microscopy is to obtain \nclean and sharp images containing sufficient information about the \nsample, which can guarantee the accuracy of downstream analysis \nand support convincing conclusions. However, limited by multiple \nbiophysical and biochemical factors (for example, labeling concen-\ntration, fluorophore brightness, phototoxicity, photobleaching and \nso on), fluorescence imaging is conducted in photon-limited condi-\ntions and the inherent photon shot noise severely degrades the image \nsignal-to-noise ratio (SNR), especially in low-illumination and high- \nspeed observations6.\nVarious methods have been proposed to remove noise from fluo-\nrescence images. Conventional denoising algorithms based on numeri-\ncal filtering and mathematical optimization suffer from unsatisfactory \nperformance and limited applicability 7,8. In the past few years, deep \nlearning has shown remarkable performance in image denoising 9,10. \nAfter iterative training on a dataset with ground truth (GT), deep neu-\nral networks can learn the mapping between noisy images and their \nclean counterparts. Such a supervised manner depends heavily on \npaired GT images 11–15. When observing the activity of living organ -\nisms, obtaining pixel-wise registered clean images is a great challenge \nbecause the sample often undergoes fast dynamics. T o alleviate this \ncontradiction, some self-supervised methods have been proposed \nReceived: 14 June 2023\nAccepted: 7 November 2023\nPublished online: 11 December 2023\n Check for updates\n1Department of Automation, Tsinghua University, Beijing, China. 2Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, \nChina. 3Institute for Brain and Cognitive Sciences, Tsinghua University, Beijing, China. 4Research Institute for Frontier Science, Beihang University, \nBeijing, China. 5Department of Electronic Engineering, Tsinghua University, Beijing, China. 6Beijing Key Laboratory of Multi-dimension and Multi-scale \nComputational Photography (MMCP), Tsinghua University, Beijing, China. 7IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, \nChina. 8The Shenzhen Institute of Future Media Technology, Shenzhen, China. 9These authors contributed equally: Xinyang Li, Xiaowan Hu, Xingye Chen. \n e-mail: wujiamin@tsinghua.edu.cn; wanghaoqian@tsinghua.edu.cn; qhdai@tsinghua.edu.cn\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1068\nArticle https://doi.org/10.1038/s43588-023-00568-2\nor vertically to exploit spatial correlations fully and isotropically.  \nA simplified implementation of sampling is depicted in Fig. 1b. As the \nnoise of adjacent pixels is independent while the signals are closely \ncorrelated, the substack filled by the central pixels can be used as the \ntraining input and the other two spatially adjacent substacks are used as \ncorresponding targets to optimize the network parameters. Compared \nwith other methods19,20,22, our sampling strategy is more effective and \ncomprehensive in preserving both spatial and temporal information \n(Supplementary Figs. 1 and 2). In the inference stage, low-SNR stacks \nwill be fed into pre-trained SRDTrans models without spatial downsam-\npling. T o overcome the locality of convolutional kernels, we constructed \na transformer network to capture endogenous non-local spatial fea-\ntures and long-range temporal dependencies using the self-attention \nmechanism (Fig. 1c). The restoration of each pixel can simultaneously \nintegrate the temporal information of all frames and the spatial infor-\nmation of all pixels, even if they are far from each other. Besides, the \nnetwork does not contain any spatial downsampling module, allowing \nmore high-frequency components to flow through the network and \navoiding the loss of spatial resolution. Furthermore, as the amount \nof data in fluorescence imaging is often very large, sometimes at the \npetabyte scale, the transformer network was designed to be as light-\nweight as possible to relieve the computational burden of large-scale \ndata processing. Compared with other transformer networks27–30, our \narchitecture can achieve the best performance with more than one \norder of magnitude fewer parameters (Supplementary Tables 1 and 2). \nThe lightweight architecture of SRDTrans also makes it easy to train a \ngood model even with a small amount of training data (for example, \n500 frames, 490 × 490 pixels each frame), relieving the pressure of \ncapturing large-scale datasets (Supplementary Fig. 3).\nT o demonstrate the predominance of our transformer network \nover CNNs, we generated simulated calcium imaging data (Methods \nand Supplementary Fig. 4) and used them to train a 3D U-Net 31, as \nwell as the transformer of SRDTrans, using the same spatial redun -\ndancy sampling strategy. We term the former as spatial redundancy \ndenoising CNN (SRDCNN). The visualized feature maps of deep layers \nintuitively show the superiority of SRDTrans in revealing fine-grained \npatterns (Fig. 1d). As features flow through the network, the limited \nreceptive field of convolutional kernels makes CNN-based methods \nfocus on only rough features while our transformer architecture still \nhas a strong perception of sophisticated structures. We also compare \nthe denoised images of the two architectures (Fig. 1e ). The result of \nSRDCNN is obviously oversmoothed, especially in regions with sharp \nedges, which is a manifestation of spectral bias that the low-frequency \ninformation is overfitted while the high-frequency information is \nhardly preserved (Supplementary Fig. 5). This deficiency is largely alle-\nviated by SRDTrans, and more subcellular structures such as dendritic \nfibers can be restored accurately. The intensity profile deconstructed \nfrom the SRDTrans denoised image is more consistent with the GT \n(Fig. 1f). Moreover, lacking the ability to capture long-range temporal \nfor more applicable and practical denoising in fluorescence imag -\ning6,16–23. Among them, the first kind of methods rely on the similarity \nbetween adjacent frames 6,16–18. But when the sample changes very \nquickly or the imaging speed is too slow, the time-lapse data cannot \nprovide enough temporal redundancy. This is a common problem in \nvolumetric imaging as the volume rate decreases proportionally to \nthe number of imaging planes. The dissimilarity between adjacent \nframes will result in inferior performance and distorted structures and \nfluorescence kinetics. The other kind of methods learn to denoise only \nfrom spatially adjacent pixels in two-dimensional frames19–23. However, \nwithout utilizing endogenous temporal correlations, these methods \nperform poorly on time-lapse imaging. Therefore, to achieve better \ndenoising performance, the ability to simultaneously extract global \nspatial information and long-range temporal correlations is essential, \nwhich is lacking in convolutional neural networks (CNNs) because of \nthe locality of convolutional kernels24. Moreover, the inherent spectral \nbias makes CNNs tend to fit low-frequency features preferentially while \nignoring high-frequency features, inevitably producing oversmoothed \ndenoising results25.\nHere we present the spatial redundancy denoising transformer \n(SRDTrans) to address these dilemmas. On the one hand, a spatial \nredundancy sampling strategy is proposed to extract three-dimen-\nsional (3D) training pairs from the original time-lapse data in two \northogonal directions. This scheme has no dependence on the similar-\nity between two adjacent frames, so SRDTrans is applicable to very fast \nactivities and extremely low imaging speed, which is complementary \nto our previously proposed DeepCAD that leverages temporal redun-\ndancy6,18. On the other hand, we designed a lightweight spatiotemporal \ntransformer network to fully exploit long-range correlations. The \noptimized feature interaction mechanism allows our model to obtain \nhigh-resolution features with a small number of parameters. Compared \nwith classical CNNs, the proposed SRDTrans has stronger abilities for \nglobal perception and high-frequency maintenance, enabling the rev-\nelation of fine-grained spatiotemporal patterns that were previously \nindiscernible. We demonstrate the superior denoising performance of \nSRDTrans on two representative applications. The first one is single-\nmolecule localization microscopy (SMLM) with adjacent frames being \nrandom subsets of fluorophores26. The other one is two-photon calcium \nimaging of large 3D neuronal populations with a volumetric speed as \nlow as 0.3 Hz. Extensive qualitative and quantitative results indicate \nthat SRDTrans can serve as a fundamental denoising tool for fluores-\ncence imaging to observe various cellular and subcellular phenomena.\nResults\nPrinciple of SRDTrans\nThe self-supervised framework of SRDTrans is shown schematically \nin Fig. 1a. For spatial redundancy sampling, spatially adjacent sub -\nstacks are sampled by orthogonal masks from the original low-SNR \nimage stack. Each target is adjacent to the input stack horizontally \nFig. 1 | Principle of SRDTrans and performance evaluation. a, Self-supervised \ntraining strategy of SRDTrans. The original low-SNR stack of H × W × T pixels  \nis sampled by orthogonal masks, producing three downsampled substacks \n(input, target 1 and target 2) of H/2 × W/2 × T pixels. The ‘input’ substack is fed  \ninto the transformer network, and the corresponding output is compared with \nthe ‘target’ substacks to calculate the loss function for parameter optimization. \nb, Simplified schematic of spatial redundancy sampling (H = 4, W = 4, T = 1).  \nA 4 × 4 patch is split into four 2 × 2 blocks and three adjacent pixels are randomly \nselected in each block. The central pixel (labeled as ‘2’) is horizontally or vertically \nadjacent to the other two pixels (labeled as ‘1’ and ‘3’). c, The architecture of \nthe lightweight spatiotemporal transformer. It consists of a temporal encoder \nmodule, an STB and a temporal decoder module. Each temporal encoder \ncompresses the temporal scale (t) of the input by a factor of r (r = 4 in this work) \nusing convolution. In the STB module, the input is divided into small patches, \nand different feature maps of the same spatial position are stitched together \nin the patch flattening layer. The position embedding layer records the spatial \nposition of each patch so that it can be mapped back after the global interaction \nin the multi-head self-attention layer. The self-attention mechanism can calculate \nthe spatiotemporal correlation between all local patches. The output of the STB \nmodule will be uncompressed to the original temporal scale by the following \ntemporal decoder module. d, Visualizing the feature responses in SRDCNN (the \nlast layer of STB) and SRDTrans (the last layer of 3D U-Net). SRDCNN represents \nthe method that replaces the transformer network in SRDTrans with a 3D U-Net. \nScale bar, 60 μm. e, Comparing the denoising performance of SRDCNN and \nSRDTrans on simulated calcium imaging data (30 Hz). Scale bars, 40 μm for the \nwhole FOV and 10 μm for magnified views. f, Pixel intensity along the red dashed \nline in e. g, Evaluating the ability of SRDCNN and SRDTrans to capture long-range \ndependencies. Models were trained and validated on simulated calcium imaging \ndata (30 Hz) of different input temporal scales (T). All values are shown as \nmean ± s.d. (N = 6,000 independent frames).\nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1069\nArticle https://doi.org/10.1038/s43588-023-00568-2\ncorrelations also drags down the denoising performance of SRDCNN \non time-lapse imaging data. For SRDTrans, the output SNR continu -\nously grows as the input temporal scale (T) increases (Fig. 1g). A more \ncomprehensive evaluation of the influence of input temporal scale \nindicates that SRDTrans can make full use of the information offered \nby temporally distant pixels (Supplementary Figs. 6 and 7). We also \ninvestigated the generalization ability of the proposed method by \ncross-dataset and cross-modality validation, which shows that training \non data with the same SNR and imaging modality can obtain the best \ndenoising performance (Supplementary Fig. 8). T o verify the practi-\ncality of SRDTrans at extremely low imaging speed, we compared the \nperformance of methods combining different networks and sampling \nschemes on simulated calcium imaging data sampled at 0.3 Hz (Sup-\nplementary Video 1). When the similarity between adjacent frames is \nlow, using spatial redundancy sampling is more reasonable (Supple-\nmentary Table 3). Succinctly, the synergy between spatial redundancy \nsampling and dedicated transformer architecture endows SRDTrans \nwith the ability to restore high-resolution structures and fast dynamics.\nHigh-performance SMLM with SRDTrans denoising\nGiven N detected fluorescence photons, the lower bound of the preci-\nsion of SMLM scales to 1/√N (ref. 26 ), which is the mathematical  \nc\nS T B\nc\nP atc h fl atteni ng\nMulti-head self-attention\nP ositi on em beddi ng\nS pati otem p or al  tr ans for m er  bl ock ( S T B )  \nT em por al  enc oder\n=\n =\nT em por al  dec oder\na \nb \nO utput\nLearning\nLearning\nT ar get 2\nT r ai ni ng pai r s\nIm ages\nH\nW\nS pati al  r edundanc y   s am pling\nT T\nO r thogonal  m asks\nT ar get 1\nInput\nT\nf g \nRaw  data\nd S RDCNN S RDT r ans\nM ask 1 M ask 2 M ask 3\nT\nW\nH\nN e t w o rk\n11 2 1\n3 3 2\n3\n2 1\n1 1 2\n2\n2\n2\n3\n3 3\n1 1\n1 1\n2 2\n2 2\n3 3\n3 3\nM ask 1 M as k  2 M as k  3\n3\n4 × 4 p atc h O r thogonal  m as k\n2 × 2 p atc hes\n1 1\n2 3\n1\nRaw S RDCNN S RDT r ans G T\nt t\nr\nt\nr\nt\nW\n2\nH\n2\n0 5 10 15\n0.2\n0.4\n0.6\n0.8\n1.0\nNor m alized  i ntensity  ( a.u.)\nDistanc e ( µm )\nM ask ID\n**\nRaw  data\nG TS RDCNNe \nNormalized intensity\n1.0\n0\nO utput S NR ( dB )\n200\nT ( s lices )\n600 1,000\n24\n25\n27\n28\n26\nS RDT r ans\n\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1070\nArticle https://doi.org/10.1038/s43588-023-00568-2\nformula of the shot-noise limit or the well-known standard quantum \nlimit32,33. This indicates that the fundamental physical limit of localiza-\ntion precision is shot noise. T o investigate the benefits that our denois-\ning method can bring to SMLM, we first applied SRDTrans to simulated \nstochastic optical reconstruction microscopy (STORM) data with GT \nfor quantitative evaluation34,35. The noise-free single-molecule-emis-\nsion images were synthesized by T estSTORM36 and corresponding \nnoisy images with different SNRs were then generated by applying \ndifferent levels of mixed Poisson–Gaussian noise (Methods and Sup-\nplementary Fig. 9). For the image stack of each SNR, we trained a spe-\ncific model for it and then processed it using the model to obtain the \ndenoised image stack. Quantitative comparisons of the visualized \nimages (Fig. 2a and Supplementary Video 2) and the extracted inten-\nsity profiles (Fig. 2b ) show that the results of SRDTrans are highly \nconsistent with the GT. Over a wide range of imaging SNRs, including \nsome extremely low-SNR conditions, SRDTrans can substantially \nimprove the image quality evaluated by the SNR at the pixel-intensity \nlevel and structural similarity (SSIM) at the perception level (Fig. 2c). \nCompared with other self-supervised methods 16–22, SRDTrans can \nbetter preserve the distribution and intensity of emitters owing to its \nstrong ability in exploiting high-resolution features and long-range \ndependencies (Supplementary Fig. 10).\nNext, we evaluate the improvement of single-molecule localiza-\ntion performance with the enhancement of the image SNR. T o recon-\nstruct super-resolved images, we applied ThunderSTORM37, one of the \nmost frequently used localization software with an excellent balance \nbetween accuracy and execution runtime38,39. The image reconstructed \nfrom the original noisy data is contaminated by noise and contains \nmany misidentified molecules (Fig. 2d). By contrast, the reconstructed \nimage from SRDTrans denoised images reveals clear and continuous \ncytoskeletal filaments that are not previously recognizable because \nof suppressed localization error and improved resolution (Fig. 2e and \nSupplementary Fig. 11). For better quantitative analysis, we matched \nthe detected fluorescent molecules with the GT using the Hungar -\nian algorithm39. From raw images, few fluorescent molecules can be \ndetected and most of them are wrongly localized. After SRDTrans \ndenoising, almost all molecules can be detected in high agreement \nwith the GT (Fig. 2f and Supplementary Fig. 12). Using the Jaccard \nindex and root-mean-squared error (r.m.s.e.) as the metrics to quantify \nthe proportion of correctly detected molecules and the localization \naccuracy of those detected molecules, respectively, we found that the \nJaccard index was improved by ~6-fold (85.7 ± 3.51% versus 14.1 ± 2.76%, \nmean ± s.d.) and r.m.s.e. was reduced by ~3.4-fold (24.86 ± 3.24 nm \nversus 85.94 ± 7.76 nm) after denoising (Fig. 2g). From a more com -\nprehensive perspective, we further adopted the metric termed effi -\nciency that combines Jaccard index and r.m.s.e.39. The results show that \nSRDTrans improved the efficiency of single-molecule localization from \n−21.54 ± 5.71 to 71.33 ± 3.38 (Fig. 2h), fully demonstrating the benefits \nof SRDTrans on SMLM.\nWe further applied SRDTrans to experimental SMLM data of micro-\ntubules to validate its ability in revealing subcellular structures. Raw \nframes were captured with low excitation power and short exposure \ntime to reduce phototoxicity and emitter density. The experimentally \nobtained single-molecule-emission images and SRDTrans denoised \nimages are shown in Fig. 3a. Disturbed by the noise, the reconstruc -\ntion algorithm can hardly localize the fluorescent molecules in the \nraw frames. The reconstructed super-resolution image contains \nmany erroneous spots and cannot reveal any meaningful structures  \n(Fig. 3b). In comparison, SRDTrans can effectively suppress the noise \nand remove localization artifacts in the reconstructed image, making \nthe distribution and extension directions of microtubules visible. We \ncomputed the Fourier-ring correlation (FRC)40,41 curve to quantify the \nresolution from the SMLM images. The image resolution is defined as \nthe inverse of the spatial frequency at the intersection of the FRC curve \nand the threshold line (~0.143). Benefitting from the removal of noise, \nthe resolution of SRDTrans denoised data was improved from 52.4 nm \nto 36.0 nm (Fig. 3c) and the localization uncertainty was reduced from \n8.0 ± 6.88 nm to 5.0 ± 1.34 nm (Fig. 3d). In addition to the data acquired \nby our instrument, we also used SRDTrans to denoise publicly avail -\nable SMLM data contributed by other laboratories 42 (Fig. 3e). The \nreconstructed super-resolution images indicate that SRDTrans can \neliminate the artifacts and bring more complete organelle structures \n(Fig. 3f,g). We applied Gaussian fitting to the intensity profile perpen-\ndicular to the microtube filaments and measured the full-width at \nhalf-maximum (FWHM) to quantify the image resolution (Fig. 3h). The \nSRDTrans denoised data show improved resolution as the averaged \nFWHM dropped from 187.89 ± 22.22 nm to 60.96 ± 7.51 nm (Fig. 3i). As \nSMLM is heading towards live-cell imaging and long-term observation43 \nour denoising method promises to be a beneficial tool to reduce the \nlaser power by resolving fluorescent molecules from very-low-SNR \nframes. For 3D SMLM, as the axial positions of molecules are estimated \nthrough point-spread-function engineering26, SRDTrans is expected to \noffer great help by resolving single-molecule-emission patterns from \nlow-SNR images.\nApplying SRDTrans to two-photon volumetric calcium \nimaging\nIn multiphoton imaging, the volumetric imaging speed decreases \nlinearly as the number of scanning planes increases. Thus, the achiev-\nable sampling rate for observing neuronal populations with large axial \nranges is often quite low, making the denoising methods that rely on \nthe similarity between temporally adjacent frames infeasible6,16–18. How-\never, SRDTrans provides an opportunity to restore the highly degraded \nfluorescence signals in large-scale volumetric calcium imaging by \nutilizing the similarity between spatially adjacent pixels. T o evaluate \nthe denoising performance of SRDTrans on calcium imaging with dif-\nferent sampling rates, we generated realistic calcium imaging data \nwith synchronized GT using neural anatomy and optical microscopy \n(NAOMi)44. We started from denoising high-sampling-rate (30 Hz) data \nand found that SRDTrans can effectively remove noise and recover \npreviously indiscernible structures such as soma, neurites and vascular \nshadows (Fig. 4a, Supplementary Fig. 13 and Supplementary Video 3).  \nThe enhancement is manifested not only in the visual effect but also, \nmore importantly, in the accurate restoration of pixel intensities  \nFig. 2 | Validation of SRDTrans on simulated SMLM data. a, Single-molecule-\nemission images before and after denoising. Left: raw data. Middle: SRDTrans \ndenoised data. Right: GT. Magnified views of boxed regions show the emission \npattern of a bunch of fluorescent molecules. Scale bars, 2 μm for the whole FOV \nand 0.5 μm for magnified views. The SNR value of the raw data and denoised data \nare noted. b, Intensity profiles along the white dashed lines in a. c, Quantitative \nevaluation of the denoising performance with SNR and SSIM. Left: image SNR \nbefore and after denoising. Right: image SSIM before and after denoising. Each \ndata point shows the statistical result of 24,000 frames. All values are shown \nas mean ± s.d. (N = 24,000 independent frames). d, Reconstructed super-\nresolution images of microtubules before and after denoising. Left: the image \nreconstructed from raw data. Middle: the image reconstructed from SRDTrans \ndenoised data. Right: GT. Scale bar, 5 μm. e, Merged image of the yellow boxed \nregion in d. Magenta, the image reconstructed from raw data; green, the image \nreconstructed from SRDTrans denoised data; red, GT. The overlapping positions \nof red and green appear yellow. Scale bar, 1 μm. f, Consistency analysis of the \nlocalized fluorescent molecules in raw images (left) and SRDTrans denoised \nimages (right) relative to the GT. A magnified view of the boxed region is shown \nat the bottom left of each panel. g, Tukey box-and-whisker plots (Methods) \nshowing the localization precision quantified with the Jaccard index (left, higher \nis better) and r.m.s.e. (right, lower is better) before and after SRDTrans denoising \n(N = 5,000 independent molecules). h, Evaluating the performance of single-\nmolecule localization before (blue) and after (orange) denoising with a more \ncomprehensive metric termed efficiency.\nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1071\nArticle https://doi.org/10.1038/s43588-023-00568-2\n(Fig. 4b). Visualization in the frequency domain (calculated by discrete \nFourier transform) shows that SRDTrans can restore most of the fre-\nquency components (Fig. 4c), especially the high-frequency compo-\nnents lost by DeepCAD6,18 and DeepInterpolation17, thereby leading to \nhigh performance in denoising calcium imaging data (Supplementary  \nFig. 14). Such a remarkable denoising capability can be maintained over \na wide range of input SNRs (from −2.08 dB to 17.68 dB), and the average \nSNR improvement is about 22 ± 2.47 dB (Fig. 4d). We also verified the \nperformance of SRDTrans on experimentally obtained calcium imag-\ning data with a synchronized high-SNR (tenfold photons) reference6, \nwhich shows that the neuronal structures and dynamics swamped by \nnoise can be restored authentically (Supplementary Fig. 15).\nThen we investigate the denoising performance of SRDTrans on \ncalcium imaging data sampled at 0.3 Hz, which is 100 times lower than \nthe imaging speed demonstrated above. Bilateral assessments in both \nthe space domain and the frequency domain reveal that SRDTrans can \na G TRaw  data ( S NR =  -0.05 dB ) S RDT r ans ( S NR =  26.53 dB )\nc\nG TRaw  data S RDT r ans\nd\nG TRaw  data S RDT r ans\nM er ged\ne\nf\nNor m alized  i ntensity  ( a.u.)\nDistanc e ( µm ) Distanc e ( µm )\nb\n Raw  data S RDT r ans G T\nG T  m ol ec ul es Detec ted m ol ec ul es\ng Raw  data S RDT r ans\nG TRaw  data S RDT r ans\nInput S NR ( dB )\n20\nInput SSIM\nh\nE ﬀicienc y = 80\nE ﬀicienc y = 60\nE ﬀicienc y = 40\nE ﬀicienc y = 20\nE ﬀicienc y = 0\nE ﬀicienc y = –20\nr.m.s.e. ( nm )\n25\n50\n75\n100\nJaccard (%)\n0 20 40 60 80 100\nR a w  d at a\nSRDT rans\nRaw  data S RDT r ans\nO utput S NR ( dB )\n0 5 10 15\n10\n20\n30\n40\n0.2 0.4 0.6 0.8\nO utput SSIM\n0.4\n0.6\n0.8\n0.3 0.9 1.5\n0.2\n0.4\n0.6\n0.8\n1.0\n0.3 0.9 1.5\n0.2\n0.4\n0.6\n0.8\n1.0\n1.0\n100\nJ ac c ar d (%)\n20\n40\n60\n80\n40\n80\n120\n60\n100\n20\nr.m.s.e. ( nm )\n0\nR a w  d at a\nSRDT rans\n\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1072\nArticle https://doi.org/10.1038/s43588-023-00568-2\nRaw  data S RDT r anse\ng Raw  data S RDT r ans\n(i) (i)\n( i i )(ii)\n( i i i )(iii)\na b\nc\ni\nRaw  data S RDT r ans\n50\n150\n200\n100\n250\nRaw  data S RDT r ans Raw  data S RDT r ans\nh\nNor m alized i ntensity  ( a.u.)\nDistanc e ( nm )\n110.3 2 nm54.69 nm\n( i i )\n( i i )\n0 100 200 3000 100 400\n115.79 nm66.70 nm\n(i)(i)\nDistanc e ( nm )\n0.4\n0.8\n0.4\n0.8\n0.4\n0\nRaw  data S RDT r ans\n0.8\n90.45 nm\nDistanc e ( nm )\n100 300\nG aussian fi tti ng\n( i i i )( i i i )\n62.29 nm\n200\nFWHM ( nm )\nS pati al fr equenc y  ( nm –1 )\nRaw  data S RDT r ans\nSmooth fitting T hr es hol d\nd\n0 0.01 0.02 0.03\n0.8\n1.0\n0.2\n0.4\n0.6\nCut-oﬀ = 52.4 nm\nCut-oﬀ = 36.0 nm\nUnc er tai nty ( nm )\nRaw  data S RDT r ans\n0\n10\n20\n30\nf\nNor m alized F RC\nRaw  data S RDT r ans\n\nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1073\nArticle https://doi.org/10.1038/s43588-023-00568-2\naccurately retrieve the fluorescence signals from the original highly \ndegraded images without structural blurring and frequency deficiency \n(Fig. 4e and Supplementary Fig. 14). When the sampling rate is much \nlower than the fluorescence dynamics, the large discrepancy between the \nsignals in two adjacent frames cannot provide the temporal correlation \nrequired by DeepCAD and DeepInterpolation, so they are not accurate \nenough to be used in conditions of low imaging speed or fast activity \n(Supplementary Fig. 16). T o figure out how SRDTrans works at differ-\nent imaging speeds, we performed an ablation study on different sam-\npling strategies and network architectures (Fig. 4f and Supplementary  \nTable 3). The results indicate that spatial redundancy sampling performs \nbetter at low imaging speeds, whereas temporal redundancy sampling \nperforms better at high imaging speeds. Almost equally for all imaging \nspeeds, our transformer architecture offers an additional improvement \n(~2.05 ± 0.27 dB) over conventional CNNs. In general, the synergistic \ncombination of the spatial redundancy sampling and the transformer \narchitecture in SRDTrans provides better performance than DeepCAD at \nall imaging speeds (Supplementary Figs. 17 and 18). In the time domain, \nthe superior ability of SRDTrans can reveal high-fidelity calcium transi-\ntions without distorting fluorescence kinetics (Fig. 4g). Moreover, we \nalso simulated fast-moving objects to imitate migrating cells that are \nwidely existed in living organisms. The quantitative evaluation shows \nthat SRDTrans can preserve the structure of densely distributed objects \neven if they are moving at a speed of up to 9 pixels per frame (Supple-\nmentary Fig. 19 and Supplementary Video 4), alleviating the shortage \nof denoising methods for fast-moving cells and organelles.\nFinally, we went a step further in denoising calcium imaging data \nby applying SRDTrans to volumetric recordings, which is not achievable \nfor other self-supervised denoising methods\n6,17,18 because of their heavy \nreliance on high sampling rates. We used transgenic mice expressing \nthe GCaMP6f genetically encoded calcium indicator 45 and imaged \na brain volume of 500 × 500 × 200 μm 3 in the mouse cortex using a \ntwo-photon microscope. We scanned 100 planes with a frame rate of \n30 Hz, and thus the overall volume rate was 0.3 Hz. For the denoising \nof volumetric calcium imaging data, we extracted all the frames of \neach imaging plane and reorganized them into a separate time-lapse \n(xy–t) stack. The time-lapse stacks of all imaging planes were used for \nnetwork training. A 3D visualization of the neural volume shows that \nthe spatial profiles and firing states of the neurons can be revealed \nafter denoising, which otherwise would be swamped by severe shot \nnoise (Fig. 5a and Supplementary Video 5). For better comparison, \nwe present the snapshots of a certain imaging plane at two different \nmoments. With the enhancement of SRDTrans, the structure and dis-\ntribution of the neurons become clearly observable (Fig. 5b). We also \nextracted the fluorescence traces along the time axis and found that \na large number of calcium transients can be restored after denoising \n(Fig. 5c). The dramatically improved SNR would propel the decoding \nof underlying neural activity from fluorescence signals. As neural cir-\ncuits in the mammalian brain are spatially coordinated and temporally \norchestrated, deciphering the function of large neuronal ensembles \nrequires large-scale volumetric imaging with a high SNR. The superior \ndenoising performance of SRDTrans provides an opportunity to imple-\nment high-sensitivity volumetric calcium imaging for investigating \nfunctionally concerted neurons and recognizing circuit motifs, espe-\ncially those distributed across multiple cortical layers.\nDiscussion\nSRDTrans does not rely on any assumptions about the contrast mecha-\nnism, noise model, sample dynamics and imaging speed. Thus, it can be \nreadily extended to other biological samples and imaging modalities \n(Supplementary Fig. 20), such as membrane voltage imaging, single-\nprotein detection, light-sheet microscopy, confocal microscopy, light-\nfield microscopy and super-resolution microscopy46–51. The limitation \nof SRDTrans lies in the basic assumption that neighboring pixels should \nhave approximate structures. If the spatial sampling rate is too low to \nprovide enough redundancy, SRDTrans would fail. Another potential \nrisk is the generalization ability as the lightweight network architecture \nof SRDTrans is more suitable for specific tasks. We believe training \nspecific models for specific data is the most reliable way to use deep \nlearning for fluorescence image denoising. Therefore, a new model \nshould be trained to ensure optimal results when the imaging param-\neter, modality and sample change.\nFig. 3 | Applying SRDTrans to experimental SMLM data. a, Experimentally \nobtained single-molecule-emission images. Left: raw data. Right: SRDTrans \ndenoised data. The magnified views of two boxed regions are shown below \neach image. Scale bars, 2 μm for the whole FOV and 0.5 μm for magnified views. \nb, Reconstructed super-resolution images. The microtubules in fixed BSC-1 \ncells were labeled with Cy5. Scale bars, 2 μm for the whole FOV and 0.5 μm for \nmagnified views. c, FRC curves of the raw reconstructed image (blue) and the \nSRDTrans enhanced reconstructed image (orange). The estimated resolution \n(52.4 nm for raw image and 36.0 nm for SRDTrans denoised image) is the inverse \nof the spatial frequency where the FRC curve drops below the cut-off threshold \n(~0.143). d, Tukey box-and-whisker plots (Methods) showing the localization \nuncertainty before and after denoising (N = 1,048,575 detected molecules for \nraw data, N = 395,908 detected molecules for SRDTrans). The uncertainty was \ncalculated by the ThunderSTORM plugin. e, Single-molecule-emission images \nfrom the open-source platform ShareLoc51. Left: raw data. Right: SRDTrans \ndenoised data. Scale bar, 2 μm. f, Reconstructed super-resolution images of \nmicrotubules (immuno-labeled with Alexa 647). Scale bar, 2 μm. g, Magnified \nviews of boxed regions. Scale bar, 0.5 μm. h, Intensity profiles perpendicular \nto the microtubule filaments indicated in g. Blue line, raw data; orange \nlines, SRDTrans denoised data; dashed line, the Gaussian fitting result. The \ncorresponding FWHM value is quantified as 2.335σ, where σ denotes the standard \ndeviation of the Gaussian fitting result. i, Tukey box-and-whisker plots (Methods) \nshowing the FWHM of randomly selected filaments (blue, raw data; orange, \nSRDTrans denoised data; N = 80 independent filament segments).\nFig. 4 | Evaluating the performance of SRDTrans on simulated calcium \nimaging data. a, SRDTrans denoised calcium imaging data sampled at 30 Hz. \nMagnified views show the neural activity of the yellow boxed region in a short \nperiod (~2 s). Left: the original low-SNR data. Middle: SRDTrans denoised data. \nRight: GT. Scale bars, 60 μm for the whole FOV and 30 μm for magnified views. \nThe magenta arrowhead indicates a dendritic fiber and the yellow arrowhead \nindicates two neighboring somas. b, Pixel intensity along the yellow dashed line \nin a. T op left: raw data. Middle left: SRDTrans denoised data. Bottom left: GT. \nRight: plotting the three intensity profiles in one coordinate. The similarity with \nGT is quantified by Pearson correlation coefficients (R). c, Frequency spectrum \ncalculated by discrete Fourier transform before and after denoising. Magnified \nviews of the boxed regions show the frequency components within the optical \ntransfer function. The similarity in the frequency domain is quantified by LFD. \nd, The performance of SRDTrans at different SNR levels. All values are shown as \nmean ± s.d. (N = 6,000 independent frames). e, Comparing the performance of \nDeepCAD and SRDTrans on calcium imaging data sampled at 0.3 Hz. Magnified \nviews show the neural activity of yellow boxed regions in a 20 s time window. \nScale bars, 100 μm for the whole FOV and 40 μm for magnified views. The \nyellow and purple arrowheads point to a firing neuron and a resting neuron, \nrespectively. f, Ablation experiments to investigate the effects of different \nsampling strategies and network architectures. SRDTrans (orange) uses spatial \nredundancy sampling and a lightweight spatiotemporal transformer. DeepCAD \n(purple) combines temporal redundancy sampling and a CNN (3D U-Net). \nSRDCNN (green) is the method combining spatial redundancy sampling and \na CNN (3D U-Net). All values are shown as mean ± s.d. (N = 1,000 independent \nframes for each frame rate). g, Fluorescence traces (F) extracted from 50 \nrandomly selected neuronal pixels. The similarity with GT is quantified by \nPearson correlation coefficients (R). T op: traces extracted from raw data. Middle: \ntraces extracted from DeepCAD denoised data. Middle bottom: traces extracted \nfrom SRDTrans denoised data. Bottom: GT.\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1074\nArticle https://doi.org/10.1038/s43588-023-00568-2\na S RDT r ans ( S NR =  26.51 dB ) G TRaw  data ( 30 Hz , S NR =  0.91 dB )\n5.20 s 6.07 s 4.03 s 4.27 s 4.43 s 5.20 s 6.07 s\n166.6 7 s 156.6 7 s 166.6 7 s 156.6 7 s\n150 300 450 600 ( s )\n176.6 7 s 176.6 7 s\nR =  0.568\nR =  0.782\nR =  0.984\nR =  1.000\n∆F/F  ( normalized)\n1.00\n4.43 s\nNor m alized i ntensity\n4.03 s 4.27 s 4.43 s 5.20 s 4.03 s 4.27 s6.07 s\nb\nRaw  data R = 0.024\nR = 0.996\nR = 1.000\nS RDT r ans\nG T\n166.6 7 s\n0.5\n( normalized) 20 s\n176.6 7 s\nRaw  data S RDT r ans\n0\nO utput S NR ( dB )\n10\n20\n30\n40\nInput S NR ( dB )\n0–5 105 2015\nS NR= - 0.85 dB S NR= 19.88 dBS NR= 13.32 dB\ne Raw  data ( 0.3 Hz , S NR =  –0.83 dB )\n1.00\nS RDT r ans ( S NR =  19.62 dB )DeepCA D ( S NR =  13.10 dB )  G T\n0.1 ( normalized) 30 µ m\nNormalized intensity\ndS RDT r ans G TRaw  data\nLF D = 28.43 dB LF D = 7.96 dB\nc\n0\n156.6 7 s 166.6 7 s 176.6 7 s 156.6 7 s\nf g\nG T DeepCA D Raw  data\nIm agi ng s peed ( Hz )\nS RDT r ans\nO utput S NR ( dB )\n10\n–1\n10\n0\n10\n1\n10\n2\n14\n22\n26\n18\nSRDT rans\nSRDCN N\nD e epC AD\n\nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1075\nArticle https://doi.org/10.1038/s43588-023-00568-2\nAs the development of fluorescence indicators heads towards \nfaster kinetics to monitor biological dynamics at the millisecond \nscale52,53, the imaging speed required to record these fast activities is \ncontinuously growing. Obtaining adequate sampling rates is becoming \nmore and more challenging for denoising methods relying on temporal \nredundancy. Our rationale is to fill the niche by seeking to utilize spatial \na Raw  data\nx\ny\nx\nz\ny\nz\nRaw  data S RDT r ans\nT i m e =  273.33 s\nc\nx\nz\ny\nzx\ny\nb\nS RDT r ansRaw  data\nx\nz\nT i m e =  243.33 s\n150\n100\n50\n150\n200\n250\n300\n350\n400\n450\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n100\n50\n0\nPosition \nZ (µm)\nPosition \nX (µm)\nPosition \nY (µm)\ny\nzx\nyy\nzx\ny\nx\nz\nS RDT r ans\n1,200 1,600400 80001,200 1,600400 8000\n∆F / F ( normalized)\n1.00\nRaw  data\nTime (s) Time (s)\nS RDT r ans\n0\n150\n100\n50\n150\n200\n250\n300\n350\n400\n450\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n100\n50\nPosition \nZ (µm)\nPosition \nX (µm)\nPosition \nY (µm)\nFig. 5 | High-sensitivity calcium imaging of large neural volumes. a, Three-\ndimensional visualization of the neural activity of a 510 × 510 × 200 μm3 volume \n(100 planes, 0.3 Hz volume rate) in the mouse cortex. Left: the original low-SNR \nvolume. Right: the same volume denoised with SRDTrans. Magnified views of \nyellow boxed regions are shown under each snapshot. Scale bars, 100 μm for the \nwhole FOV and 10 μm for magnified views. b, Raw frames and SRDTrans denoised \ncounterparts of a single imaging plane at two different moments. The x–z and \ny–z cross-sections of the volume are shown alongside each x–y plane. Magnified \nviews of yellow boxed regions are shown at the bottom right of the images. Scale \nbars, 70 μm for the whole FOV and 20 μm for magnified views. c, Fluorescence \ntraces (F) extracted from all pixels on the yellow dashed line in b. Left: traces \nextracted from raw data. Right: traces extracted from SRDTrans denoised data. \nYellow arrowheads point to some representative calcium transients.\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1076\nArticle https://doi.org/10.1038/s43588-023-00568-2\nredundancy as an alternative to enable self-supervised denoising in \nmore imaging applications. Although the perfect case for spatial redun-\ndancy sampling is that the spatial sampling rate is two times higher \nthan the Nyquist sampling of the diffraction limit to ensure that two \nadjacent pixels have nearly identical optical signals, the endogenous \nsimilarity between two spatially downsampled substacks is sufficient \nto guide the training of the network in most cases. However, this does \nnot mean that the proposed spatial redundancy sampling strategy can \nfully replace temporal redundancy sampling, as the ablation study \n(Fig. 4f) shows that, if equipped with the same network architecture, \nthe temporal redundancy sampling can achieve better performance \nin high-speed imaging. The superiority of SRDTrans over DeepCAD at \nhigh imaging speeds is actually attributed to the transformer archi -\ntecture. In general, spatial redundancy and temporal redundancy are \ntwo complementary sampling strategies to enable self-supervised \ntraining of denoising networks for fluorescence time-lapse imaging. \nWhich sampling strategy is used depends on which kind of redundancy \nis more sufficient in the data. It is noteworthy that there are still many \ncases where neither redundancy is sufficient to support current sam-\npling strategies. Developing specific or more general self-supervised \ndenoising methods is of persistent value for fluorescence imaging.\nMethods\nThe spatial redundancy sampling strategy\nIn SRDTrans, we employed a spatial redundancy sampling strategy \nto produce training pairs. The detailed implementation for generat -\ning training pairs in SRDTrans is shown in Fig. 1b and Supplementary \nFig. 1d. For each image inside an input training stack with H × W × T \npixels (H, W and T are the height, width and length of the input image \nstack), we spatially divided it into many adjacent small patches with \n2 × 2 pixels. Next, we randomly selected three adjacent pixels from \neach patch. The central pixel was used to compose the input substack \nand the two spatially adjacent pixels were used to compose the two \ntarget substacks. After traversing all patches, we can finally obtain \nthree downsampled substacks with the size of H/2 × W/2 × T pixels. As \nthe fluorescence signals of spatially adjacent pixels are closely corre-\nlated, the input substack and each target substack can be considered \nas two independent samples of the same underlying pattern. Thus, \nthe input substack and the two target substacks can form two training \npairs, which can be used for the self-supervised training of denoising \nnetworks (Supplementary Note 1).\nNetwork architecture and loss function\nThe transformer architecture of SRDTrans is composed of three parts: \na temporal encoder module, a spatiotemporal transformer block (STB) \nand a temporal decoder module (Fig. 1c). The temporal encoder module \nis equipped with two temporal encoders. Each temporal encoder can \ncompress the temporal scale of the input substack by a factor of r (r is 4 \nin this work) using a convolutional layer with 3 × 3 kernels. In contrast to \nthe U-Net-type architectures with many upsampling and downsampling \noperations, SRDTrans does not reduce the size of feature maps in these \nencoders (Supplementary Fig. 21). Thus, for an input substack with a \nsize of H/2 × W/2 × T pixels, the output size of the temporal encoder \nmodule is H /2 × W/2 × T/r2. The output from the temporal encoder \nmodule will be fed into the STB to extract global information both in \nspace and time. The STB contains a temporal transformer block and \na spatial transformer block (Supplementary Fig. 22). Specifically, in \nthe temporal transformer block, the input is divided into patches \nwith the size of p × p × T/r2 (p is 7 in this work). These patches are first \nflattened into one-dimensional vectors and input into the position \nembedding layer, where spatial concatenation and linear transfor -\nmation are performed. Two multi-head self-attention layers are then \ncascaded to extract temporal correlations inside the data. In the spatial \ntransformer block, a Swin transformer block 27 is adopted to capture \nfine-grained spatial features with high efficiency. Local features flow \nfully in multi-head self-attention layers and densely interact with long-\nrange global features. Finally, the output of the STB is remapped by \nthe temporal decoder module, and its temporal scale can be rescaled \nto T. This decompression operation is implemented by two cascaded \ntemporal decoders using convolutional layers with 3 × 3 kernels.\nWe used a linear combination of L1-norm loss and L2-norm loss as \nthe loss function to optimize the parameters of SRDTrans, which shows \nbetter performance than L1-norm loss and improved convergence \ncompared with L2-norm loss (Supplementary Fig. 23). We define the \ninput substack filled with central pixels as Sc, the target substack filled \nwith its vertically adjacent pixels as Sv and the target substack filled with \nits horizontally adjacent pixels as Sh. The total loss consists of two pairs \nof training losses, which is defined as:\nLver =‖ FSRDTrans(Si)−S v‖\n2\n2 +|FSRDTrans(Si)−S v|1,\nLhor =‖ FSRDTrans(Si)−S h‖\n2\n2 +|FSRDTrans(Si)−S h|1,\nLtotal = Lver +Lhor.\nwhere Lver and Lhor denote the loss of the vertically and horizontally \nadjacent substacks, respectively.\nTraining and inference\nT o achieve optimal performance, specific models were trained for \nstacks with different SNRs. One or more training stacks (xy–t or xy–z) \nwere divided into a specified number of 3D (xy–t) training pairs (6,000 \nby default). The batch size for all experiments was set to the number of \ngraphics processing units (NVIDIA GeForce RTX 3090 for most cases) \nbeing used and the patch size was set to be 128 × 128 × 128 pixels. All \nextracted training pairs were geometrically transformed by random \nflipping or rotation for eightfold data augmentation. The synergy of \nour lightweight architecture and data augmentation eliminates the risk \nof overfitting (Supplementary Fig. 24). The compression factor of each \ntemporal encoder was set to 4. In the STB, we set the internal patch size \nto 7, the number of heads in the multi-headed self-attention block to  \n8 and the embedded feature channels to 128. For model optimization, \nwe used the Adam optimizer and the exponential decay rate for the first \nmoment was 0.9, the exponential decay rate for the second moment was \n0.999 and the learning rate was 0.00001. PyT orch was used to construct \nthe network and implement all operations. In the inference stage, the \nraw noisy data were not spatially subsampled and the model of the last \ntraining epoch was selected for final processing. The denoised result \nof each image stack was saved as a separate TIF file.\nData simulation\nQuantitative evaluations were performed on simulated data because \nnoise-free images (GT) are available. T o synthesize noise-free two-\nphoton calcium imaging data, we used NAOMi44, which can generate \nrealistic calcium imaging data with high-fidelity tissue characteristics \nand fluorescence kinetics. Then we applied different levels of mixed \nPoisson–Gaussian noise to generate calcium imaging data of differ -\nent imaging SNRs6,18. We also simulated data containing only Poisson \nor Gaussian noise to show the comparable denoising performance of \nSRDTrans on these two types of noise (Supplementary Fig. 25). T o gen-\nerate calcium imaging data of different sampling rate, we first synthe-\nsized images sampled at 30 Hz and 1 Hz, and the data of other sampling \nrates were obtained by extracting frames at different intervals. The \nimage size for all simulated calcium imaging data was 490 × 490 pixels \nand the pixel size was 1.02 μm.\nT o generate realistic SMLM data, we first acquired reconstructed \nsuper-resolution images from the ShareLoc.XYZ platform (https://\nshareloc.xyz/) 42. These images were experimentally obtained on a \nNikon N-STORM microscope and contained densely distributed micro-\ntubules immuno-labeled with Alexa 64754. The tracks of all microtubules \nin a selected region of interest were extracted semi-automatically using \nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1077\nArticle https://doi.org/10.1038/s43588-023-00568-2\nthe JFilament plugin of ImageJ55. We then generated synthetic single-\nmolecule-emission image stacks (GT images without noise) using the \nT estSTORM36 simulator by loading the microtubule patterns from \nJFilament. All fluorescent molecules were linked on the microtubule \npattern with a radius of 12.5 nm. For imaging parameters, the numerical \naperture was 1.4 and the frame rate was 200 Hz (5 ms exposure time). \nThe image size was 328 × 328 pixels and the pixel size was 30 nm. Noisy \nstacks were generated by applying mixed Poisson–Gaussian noise post \nhoc with a customized MATLAB script6,18.\nWe synthesized moving objects with different moving speeds \nusing the Modified National Institute of Standards and T echnology \n(MNIST) dataset56. Each frame was defined as an image of 512 × 512 \npixels with a black background and many bright moving digits. Each \ndigit was an image patch (28 × 28 pixels) randomly extracted from the \nMNIST dataset, moved in a random direction, and appeared or disap-\npeared only once. The total number of digits in the field of view (FOV) \nwas 500. We first generated the data with a moving speed of 0.5 pixels \nper frame. The data with higher moving speeds were then generated \nby extracting frames at different intervals. The total frame number for \nall moving speeds was 5,000. The final experiment was implemented \non 20 datasets with moving speeds from 0.5 to 10 pixels per frame. \nThe sampling interval of the moving speed was 0.5 pixels per frame.\nSMLM imaging\nThe imaging samples (including the buffer solution and the sam -\nple holder) for the SMLM experiments were purchased from \nStandard Imaging Company (https://www.standardimaging.cn/\nstandardsample?lang=en). The SMLM experiments were performed \non a commercial microscope (Nikon N-STORM) equipped with laser \nsources of 405 nm and 640 nm, which were used for activation and \nexcitation, respectively. A scientific complementary metal-oxide semi-\nconductor camera (Hamamatsu Flash 4.0) was placed at the image \nplane to capture the emission signals. T o mimic living-cell imaging, we \nused low excitation power to reduce phototoxicity and short exposure \ntime to obtain images with low emitter density. The detailed settings \nare summarized in Supplementary Table 4.\nSMLM sample preparation\nThe Biologics Standards-Cercopithecus-1 (BSC-1) cell line purchased \nfrom Pricella Life T echnology was used for SMLM imaging. BSC-1 \ncells were cultured in DMEM (Invitrogen, 11965-118) supplemented \nwith 10% fetal bovine serum (Gibco, 16010-159). T o prevent bacterial \ncontamination, 100 μg ml\n−1 penicillin and streptomycin (Invitrogen, \n15140122) were added into the DMEM medium. Cells were grown under \nstandard cell culture conditions (5% CO 2, humidified atmosphere at \n37 °C). BSC-1 cells were plated on 1.5 glass-bottom dishes over 48 h \nbefore sample preparation. For cell passage, cells were washed with \npre-warmed PBS (Life T echnologies, 14190500BT) 3 times and digested \nwith 25% trypsin (Gibco, 25200-056) for 30 s. BSC-1 l cell lines were \ntested for potential mycoplasma contamination (MycoAlert, Lonza) \nand all tests showed negative results. For immunofluorescence stain-\ning, cells were grown on 35 mm, 1.5 glass coverslips. We pre-treated \nglass-bottom dishes with fibronectin (Invitrogen, 33016015) for 1 h at \n37 °C to increase cell adhesion. On the day of sample preparation, the \ncell density should be about 50–70%. Cells were fixed with 37 °C pre-\nwarmed fixation buffer for 10 min, containing 4% paraformaldehyde \n(EMS) and 0.1% glutaraldehyde in PBS. Then the sample was washed \nthree times with PBS. For quenching the background fluorescence, \nwe incubated the cells with 2 ml 0.1% NaBH4 solution in PBS for 7 min, \noptionally shaking on the shaker (<1 Hz). The sample was washed 3 \ntimes with 2 ml PBS and then incubated for 30 min in PBS containing 5% \nBSA ( Jackson, 001-000-162) and 0.5% Triton X-100 (Fisher Scientific) \nat 37 °C. All antibodies were diluted in the 5% BSA + 0.5% triton solution \ndescribed above. Next, we incubated the sample for 40 min with the \nappropriate dilution of primary antibodies: mouse anti-beta-tubulin \n(E7, DSHB) at 25 °C. After primary antibodies incubation, the cells \nwere washed 3 times with 2 ml PBS for 5 min. Secondary antibodies \n(AffiniPure Donkey Anti-mouse IgG, 715-005-150, Jackson Immuno \nResearch) were incubated for 60 min with the appropriate dilutions \nof secondary antibodies (conjugated with Cy5) at 25 °C. After being \nwashed 3 times with PBS, cells were fixed with post-fixation buffer \nfor 10 min. The sample was stored at 4 °C in PBS and protected from \nlight. Before imaging, we used an imaging buffer (STIBa-031, Standard \nImaging Company) to replace PBS.\nSMLM reconstruction\nThe super-resolution SMLM images were reconstructed by the Thun-\nderSTORM37 Fiji plugin. For our experimentally obtained data, hard \nthresholding was performed to zero out those pixels with values \nsmaller than a manually adjusted threshold to suppress the patterned \nnoise of the camera. The detailed configuration is set as: the image filter \nwas the wavelet filter (B-spline) with an order of 3 and a scale of 2; the \nalgorithm for determining the approximate position of molecules was \nthe local maximum algorithm; the subpixel localization is performed \nby the integrated Gaussian point-spread-function model with a fitting \nradius of 3 pixels; a fitting method of ‘weighted least squares’ , and an \ninitial sigma of 1.6 pixels. Both visualization images are generated \nby averaged shifted histogram with a magnification of 5. For better \nvisualization, the single-molecule-emission images and reconstructed \nsuper-resolution images were rendered with pseudo-color and their \ncontrast and brightness were manually adjusted.\nMouse preparation and calcium imaging\nAll experiments involving mice were performed in accordance with the \ninstitutional guidelines for animal welfare and have been approved by \nthe Animal Care and Use Committee of Tsinghua University. All mice \nwere aged 8–12 weeks and were housed in cages (24 °C, 50% humidity) \nin groups of 1–5 under a reverse light cycle. Transgenic mice hybridized \nbetween Rasgrf2-2A-dCre mice and Ai148 (TIT2L-GC6f-ICL-tTA2)-D \nmice expressing Cre-dependent GCaMP6f genetically encoded cal -\ncium indicator were used for calcium imaging of neural circuits. Both \nmale and female mice were used without randomization or blinding. \nCraniotomy surgeries were conducted to remove the skull and a cov-\nerslip was implanted on the craniotomy region for chronic imaging. \nTwo-photon volumetric imaging of the mouse cortex was performed \non head-fixed mice without anesthesia using a standard two-photon \nmicroscope controlled with ScanImage 5.7. The neural volume being \nrecorded was located at the primary visual cortex with a depth of about \n150–350 μm below the dura, and was scanned for 100 planes with an \naxial step of 2 μm. The whole imaging session lasted 30 min with a \nvolume rate of 0.3 Hz.\nThree-dimensional visualization of neural activity\nFor volumetric calcium imaging, we used Imaris 9.0 (Oxford Instru -\nments) to visualize the calcium activity of the neuronal population \nbefore and after denoising. Specifically, we imported the four-dimen-\nsional (xyz–t) data into Imaris, applied pseudo-color to the images, and \nthen performed 3D rendering using the maximum intensity projection \nmode. The contrast and brightness were adjusted to make structures \nin the volume as clear as possible. All values for gamma correction \nwere set to one.\nMethod comparison\nWe compared the performance of SRDTrans with six baseline self-\nsupervised methods: Noise2Noise16, Noise2Void19, Noise2Self20, Proba-\nbilistic Noise2Void21, Neighbor2Neighbor22, DeepInterpolation17 and \nDeepCAD6,18. These methods were all implemented by open-source \ncodes released by the relevant papers. The denoising model of each \nmethod was trained and tested on the same datasets. For the methods \ndesigned for two-dimensional images, we split the time-lapse (xy –t) \nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1078\nArticle https://doi.org/10.1038/s43588-023-00568-2\nimage stack into a series of two-dimensional frames to match the input \ndimension. Training and inference were performed frame by frame. \nWe followed the default training settings about network architec -\ntures and hyperparameters for all methods. Specifically, the model of \nDeepInterpolation was fine-tuned based on a public pre-trained model \n(pre-trained with 225,000 two-photon images of the Ai93 reporter \nline). Other methods were trained from scratch. The detailed settings \nof each method are listed in Supplementary Table 5.\nEvaluation metrics\nWe used several metrics to evaluate the performance of different \ndenoising methods. For an image (or an image stack) Sx and its GT Sy, \nthe metrics are defined as follows.\nSNR measures the pixel-level deviation between two images using \nthe logarithmic decibel scale, which is formulated as\nSNR= 10log10\n‖\n‖Sy\n‖\n‖\n2\n2\n‖\n‖Sx −Sy\n‖\n‖\n2\n2\n.\nSSIM measures the similarity between two images on a perceptual \nlevel, including luminance, contrast and structure. The definition is\nSSIM=\n(2μxμy +c1)(2σxy +c2)\n(μ2\nx +μ2\ny +c1)(σ2\nx +σ2\ny +c2)\n,\nwhere {μ x, μy} and {σ x, σy} are the means and variances of S x and S y, \nrespectively. σxy is the covariance of S x and S y. The two constants c 1 \nand c2 are defined as c 1 = (k1L)2 and c2 = (k2L)2 with k 1 = 0.01, k2 = 0.03 \nand L = 65,535.\nThe Jaccard index measures the proportion of correctly detected \nmolecules in SMLM. The correctly localized fluorescent molecules are \ntrue positives (TP). The incorrectly localized molecules are false posi-\ntives (FP) and the undetected molecules are false negatives (FN). The \nJaccard index is formulated as:\nJaccard= 100 TP\nTP+FP+FN%.\nThe r.m.s.e. quantifies the mean difference between the local -\nized positions (P x) and GT positions (P y) of all detected fluorescent \nmolecules:\nr.m.s.e.=\n√√\n√\n1\nTP∑\nTP\n‖\n‖Py −Px\n‖\n‖\n2\n2\n,\nEfficiency (E) is a comprehensive metric combining the Jaccard \nindex and r.m.s.e. to measure the performance of single-molecule \nlocalization39. It can simultaneously reflect the ability to detect mol -\necules from images (measured by Jaccard) and the ability to precisely \nlocate molecules (measured by r.m.s.e.), which is defined as:\nEfficiency = 100−√(100−Jaccard)\n2\n+α2r.m.s.e.2,\nwhere α = 1 nm−1 controls the trade-off between Jaccard and r.m.s.e.\nThe Pearson correlation coefficient measures the similarity \nbetween a variable (images and fluorescence traces) and its GT, which \nis formulated as\nR =\nE[(Sx −μx)(Sy −μy)]\nσxσy\n,\nwhere E represents the arithmetic mean. {μ x, μy} and {σ x, σy} are the \nmeans and variances of Sx and Sy, respectively.\nLogarithmic frequency distance (LFD) quantifies the spectral \ndifference between two images in the frequency domain. For images \nwith a size of H × W pixels, LFD is formulated as:\nLFD= log10[ 1\nHW (\nH−1\n∑\nu=0\nW−1\n∑\nv=0\n‖\n‖FSx (u,v)− FSy (u,v)‖\n‖\n2\n2\n)+1].\nFSx and FSy are the discrete Fourier transform of Sx and Sy, respec-\ntively. u and v are the pixel index in the frequency domain.\nStatistics and reproducibility\nT o ensure the reproducibility of the findings, we report the sample \nsize and statistics in the legend and text of each experiment. All box \nplots are drawn in the standard Tukey box-and-whisker format. The \nupper and lower quartiles are represented by box bounds, and the \nlines in the boxes indicate the median. The lower whisker represents \nthe minimum observed value, equal to the lower quartile minus 1.5× \nthe interquartile range. The upper whisker the maximum observed \nvalue, equal to the upper quartile plus 1.5× the interquartile range. \nResults obtained through experimental or observational studies or \nstatistical analysis of datasets can be reproduced with high reliability \nwhen the study is repeated. Representative images are shown in figures \nand similar results are achieved on all test samples. Experiments in Figs. \n1d,e and 4a were repeated with 6,000 frames. Experiments in Figs. 2a \nand 3a,e were repeated with 24,000, 180,000 and 60,000 frames, \nrespectively. Experiments in Figs. 4e and 5b were repeated with 1,000 \nand 548 frames, respectively.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nBoth simulated and experimentally obtained data of two-photon cal-\ncium imaging and single-molecule localization microscopy used in this \nwork can be found at https://github.com/cabooster/SRDTrans/tree/\nmain/datasets (refs. 57–60). Source data are provided with this paper.\nCode availability\nThe open-source Python code of SRDTrans is available at the Zenodo \nrepository61 and on GitHub (https://github.com/cabooster/SRDTrans).\nReferences\n1. Royer, L. A. et al. Adaptive light-sheet microscopy for long-term, \nhigh-resolution imaging in living organisms. Nat. Biotechnol. 34, \n1267–1278 (2016).\n2. Fan, J. et al. Video-rate imaging of biological dynamics at \ncentimetre scale and micrometre resolution. Nat. Photon. 13, \n809–816 (2019).\n3. Balzarotti, F. et al. Nanometer resolution imaging and tracking of \nfluorescent molecules with minimal photon fluxes. Science 355, \n606–612 (2017).\n4. Wu, J. et al. Iterative tomography with digital adaptive optics \npermits hour-long intravital observation of 3D subcellular \ndynamics at millisecond scale. Cell 184, 3318–3332 (2021).\n5. Verweij, F. J. et al. The power of imaging to understand \nextracellular vesicle biology in vivo. Nat. Methods 18,  \n1013–1026 (2021).\n6. Li, X. et al. Real-time denoising enables high-sensitivity \nfluorescence time-lapse imaging beyond the shot-noise limit.  \nNat. Biotechnol. 41, 282–292 (2023).\n7. Meiniel, W., Olivo-Marin, J. C. & Angelini, E. D. Denoising of \nmicroscopy images: a review of the state-of-the-art, and a  \nnew sparsity-based method. IEEE Trans. Image Process. 27, \n3842–3856 (2018).\nNature Computational Science | Volume 3 | December 2023 | 1067–1080\n 1079\nArticle https://doi.org/10.1038/s43588-023-00568-2\n8. Dabov, K., Foi, A., Katkovnik, V. & Egiazarian, K. Image denoising \nby sparse 3-D transform-domain collaborative filtering. IEEE \nTrans. Image Process. 16, 2080–2095 (2007).\n9. Zhang, K. et al. Beyond a Gaussian denoiser: residual learning of \ndeep CNN for image denoising. IEEE Trans. Image Process. 26, \n3142–3155 (2017).\n10. Tai, Y., Yang, J., Liu, X. & Xu, C. MemNet: a persistent memory \nnetwork for image restoration. In Proc. IEEE/CVF Conference  \non Computer Vision and Pattern Recognition 4539–4547  \n(IEEE, 2017).\n11. Weigert, M. et al. Content-aware image restoration: pushing the \nlimits of fluorescence microscopy. Nat. Methods 15, 1090–1097 \n(2018).\n12. Belthangady, C. & Royer, L. A. Applications, promises, and  \npitfalls of deep learning for fluorescence image reconstruction. \nNat. Methods 16, 1215–1225 (2019).\n13. Chen, J. et al. Three-dimensional residual channel attention \nnetworks denoise and sharpen fluorescence microscopy image \nvolumes. Nat. Methods 18, 678–687 (2021).\n14. Chaudhary, S., Moon, S. & Lu, H. Fast, efficient, and accurate \nneuro-imaging denoising via supervised deep learning.  \nNat. Commun. 13, 5165 (2022).\n15. Wang, Z., Xie, Y. & Ji, S. Global voxel transformer networks for \naugmented microscopy. Nat. Mach. Intell. 3, 161–171 (2021).\n16. Lehtinen, J. et al. Noise2Noise: learning image restoration \nwithout clean data. In Proc. 35th International Conference  \non Machine Learning (eds Dy, J. & Krause, A.) 2965–2974  \n(PMLR, 2018).\n17. Lecoq, J. et al. Removing independent noise in systems \nneuroscience data using DeepInterpolation. Nat. Methods 18, \n1401–1408 (2021).\n18. Li, X. et al. Reinforcing neuron extraction and spike inference \nin calcium imaging using deep self-supervised denoising. Nat. \nMethods 18, 1395–1400 (2021).\n19. Krull, A., Buchholz, T.-O. & Jug, F. Noise2Void—learning denoising \nfrom single noisy images. In Proc. IEEE/CVF Conference on \nComputer Vision and Pattern Recognition 2129–2137 (IEEE, 2019).\n20. Batson, J. & Royer, L. Noise2Self: blind denoising by self-\nsupervision. In Proc. 36th International Conference on Machine \nLearning 524–533 (PMLR, 2019).\n21. Krull, A., Vičar, T., Prakash, M., Lalit, M. & Jug, F. Probabilistic \nnoise2void: unsupervised content-aware denoising. Front. \nComput. Sci. https://doi.org/10.3389/fcomp.2020.00005 (2020).\n22. Huang, T. et al. Neighbor2Neighbor: self-supervised denoising \nfrom single noisy images. In Proc. IEEE/CVF Conference  \non Computer Vision and Pattern Recognition 14781–14790  \n(IEEE, 2021).\n23. Lequyer, J. et al. A fast blind zero-shot denoiser. Nat. Mach. Intell. \n4, 953–963 (2022).\n24. Luo, W. et al. Understanding the effective receptive field in deep \nconvolutional neural networks. Adv. Neural Inf. Process. Syst. 29, \n4905–4913 (2016).\n25. Rahaman N. et al. On the spectral bias of neural networks.  \nIn International Conference on Machine Learning 5301–5310 \n(PMLR, 2019).\n26. Lelek, M. et al. Single-molecule localization microscopy. Nat. Rev. \nMethods Prim. 1, 39 (2021).\n27. Liu, Z. et al. Swin transformer: hierarchical vision transformer \nusing shifted windows. In Proc. IEEE/CVF International Conference \non Computer Vision 10012–10022 (IEEE, 2021).\n28. Zhou H. et al. nnFormer: interleaved transformer for volumetric \nsegmentation. Preprint at https://arxiv.org/abs/2109.03201 (2021).\n29. Hatamizadeh, A. et al. UNETR: transformers for 3D medical \nimage segmentation. In Proc. IEEE/CVF Winter Conference on \nApplications of Computer Vision 574–584 (IEEE, 2022).\n30. Hatamizadeh, A. et al. Swin UNETR: Swin transformers for \nsemantic segmentation of brain tumors in MRI images. In \nInternational MICCAI Brainlesion Workshop (eds Crimi, A. et al.) \n272–284 (Springer, 2021).\n31. Çiçek, Ö. et al. 3D U-Net: learning dense volumetric \nsegmentation from sparse annotation. In Medical Image \nComputing and Computer-Assisted Intervention—MICCAI 2016 \n(eds Ourselin, S. et al.) 424–432 (Springer, 2016).\n32. Taylor, M. A. & Bowen, W. P. Quantum metrology and its \napplication in biology. Phys. Rep. 615, 1–59 (2016).\n33. Nagata, T. et al. Beating the standard quantum limit with four-\nentangled photons. Science 316, 726–729 (2007).\n34. Rust, M., Bates, M. & Zhuang, X. Sub-diffraction-limit imaging \nby stochastic optical reconstruction microscopy (STORM). Nat. \nMethods 3, 793–796 (2006).\n35. Nehme, E., Weiss, L. E., Michaeli, T. & Shechtman, Y. Deep-STORM: \nsuper-resolution single-molecule microscopy by deep learning. \nOptica 5, 458–464 (2018).\n36. Sinkó, J. et al. TestSTORM: simulator for optimizing sample \nlabeling and image acquisition in localization based super-\nresolution microscopy. Biomed. Opt. Express 5, 778–787 (2014).\n37. Ovesný, M. et al. ThunderSTORM: a comprehensive ImageJ \nplug-in for PALM and STORM data analysis and super-resolution \nimaging. Bioinformatics 30, 2389–2390 (2014).\n38. Sage, D. et al. Quantitative evaluation of software packages  \nfor singlemolecule localization microscopy. Nat. Methods 12, \n717–724 (2015).\n39. Sage, D. et al. Super-resolution fight club: assessment of 2D \nand 3D single-molecule localization microscopy software. Nat. \nMethods 16, 387–395 (2019).\n40. Nieuwenhuizen, R. et al. Measuring image resolution in optical \nnanoscopy. Nat. Methods 10, 557–562 (2013).\n41. Descloux, A., Grußmayer, K. S. & Radenovic, A. Parameter-free \nimage resolution estimation based on decorrelation analysis.  \nNat. Methods 16, 918–924 (2019).\n42. Ouyang, W. et al. ShareLoc—an open platform for sharing \nlocalization microscopy data. Nat. Methods 19, 1331–1333 \n(2022).\n43. Jones, S. et al. Fast, three-dimensional super-resolution imaging \nof live cells. Nat. Methods 8, 499–505 (2011).\n44. Song, A., Gauthier, J. L., Pillow, J. W., Tank, D. W. & Charles, A. S. \nNeural anatomy and optical microscopy (NAOMi) simulation for \nevaluating calcium imaging methods. J. Neurosci. Methods 358, \n109173 (2021).\n45. Chen, T. W. et al. Ultrasensitive fluorescent proteins for imaging \nneuronal activity. Nature 499, 295–300 (2013).\n46. Zhao, Z. et al. Two-photon synthetic aperture microscopy for \nminimally invasive fast 3D imaging of native subcellular behaviors \nin deep tissue. Cell 186, 2475–2491 (2023).\n47. Platisa, J. et al. High-speed low-light in vivo two-photon voltage \nimaging of large neuronal populations. Nat. Methods 20,  \n1095–1103 (2023).\n48. Zhao, W. et al. Sparse deconvolution improves the resolution \nof live-cell super-resolution fluorescence microscopcy. Nat. \nBiotechnol. 40, 606–617 (2022).\n49. Dahmardeh, M. et al. Self-supervised machine learning pushes \nthe sensitivity limit in label-free detection of single proteins below \n10 kDa. Nat. Methods 20, 442–447 (2023).\n50. Li, X. et al. Unsupervised content-preserving transformation for \noptical microscopy. Light. Sci. Appl. 10, 44 (2021).\n51. Qiao, C. et al. Rationalized deep learning super-resolution \nmicroscopy for sustained live imaging of rapid subcellular \nprocesses. Nat. Biotechnol. 41, 367–377 (2023).\n52. Zhang, Y. et al. Fast and sensitive GCaMP calcium indicators for \nimaging neural populations. Nature 615, 884–891 (2023).\nNature Computational Science | Volume 3 | December 2023 | 1067–1080 1080\nArticle https://doi.org/10.1038/s43588-023-00568-2\n53. Liu, Z. et al. Sustained deep-tissue voltage recording using  \na fast indicator evolved for two-photon microscopy. Cell 185, \n3408–3425 (2022).\n54. Jimenez, A., Friedl, K. & Leterrier, C. About samples, giving \nexamples: optimized single molecule localization microscopy. \nMethods 174, 100–114 (2020).\n55. Smith, M. B. et al. Segmentation and tracking of cytoskeletal \nfilaments using open active contours. Cytoskeleton 67,  \n693–705 (2010).\n56. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based \nlearning applied to document recognition. Proc. IEEE 86, \n2278–2324 (1998).\n57. Li, X. et al. SRDTrans dataset: simulated calcium imaging data \nsampled at 30 Hz under different SNRs. Zenodo https://doi.org/ \n10.5281/zenodo.8332083 (2023).\n58. Li, X. et al. SRDTrans dataset: simulated calcium imaging data \nat different imaging speeds. Zenodo https://doi.org/10.5281/\nzenodo.7812544 (2023).\n59. Li, X. et al. SRDTrans dataset: simulated SMLM data under \ndifferent SNRs. Zenodo https://doi.org/10.5281/zenodo.7812589 \n(2023).\n60. Li, X. et al. SRDTrans dataset: SRDTrans dataset: experimentally \nobtained SMLM data Zenodo https://doi.org/10.5281/zenodo. \n7813184 (2023).\n61. Li, X. et al. Code for SRDTrans. Zenodo https://doi.org/10.5281/\nzenodo.10023889 (2023).\nAcknowledgements\nThis research was supported by the National Natural Science \nFoundation of China (62088102, 62222508, 62071272) and \nNational Key Research and Development Program of China \n(Project No. 2022YFB36066), in part by the Shenzhen Science and \nTechnology Project under Grant (CJGJZD20200617102601004, \nJCYJ20220818101001004). This work was also supported by the \nChinese Postdoctoral Foundation (BX2021159) and Shuimu Tsinghua \nScholar Program. We thank H. Hao from Standard Imaging (Beijing) \nBiotechnology Co., Ltd for providing information about SMLM \nsample preparation.\nAuthor contributions\nQ.D., H.W. and J.W. supervised this research. Q.D., H.W., J.W.  \nand X.L. conceived and initiated this project. X.L., X.H. and X.C. \ndesigned detailed implementations and performed imaging \nexperiments. X.H. and X.L. developed the Python code, performed \nsimulations and processed relevant imaging data. J.F. prepared \nsamples and provided models animals. Z.Z. gave critical support on \nthe two-photon imaging system and imaging procedures. X.H., X.L. \nand X.C. analyzed the data, prepared figures and videos. X.L., X.H., \nX.C., J.F., Z.Z. and J.W. participated in discussions about the results \nand gave valuable advice. All authors participated in the drafting of \nthe paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s43588-023-00568-2.\nCorrespondence and requests for materials should be addressed  \nto Jiamin Wu, Haoqian Wang or Qionghai Dai.\nPeer review information Nature Computational Science thanks Bo Li,  \nHuafeng Liu and Lachlan Whitehead for their contribution to the  \npeer review of this work. Peer reviewer reports are available.  \nPrimary Handling Editor: Ananya Rastogi, in collaboration with the \nNature Computational Science team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2023\n\n\n",
  "topic": "Redundancy (engineering)",
  "concepts": [
    {
      "name": "Redundancy (engineering)",
      "score": 0.7249547243118286
    },
    {
      "name": "Computer science",
      "score": 0.6380612850189209
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6140568256378174
    },
    {
      "name": "Noise reduction",
      "score": 0.575530469417572
    },
    {
      "name": "Visualization",
      "score": 0.5343301296234131
    },
    {
      "name": "Computer vision",
      "score": 0.49318331480026245
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4302441477775574
    },
    {
      "name": "Transformer",
      "score": 0.4280480146408081
    },
    {
      "name": "Physics",
      "score": 0.1374519169330597
    },
    {
      "name": "Voltage",
      "score": 0.08233010768890381
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}