{
    "title": "Effectiveness of Various General large language models in Clinical Consensus and Case Analysis in Dental Implantology: A Comparative Study",
    "url": "https://openalex.org/W4401068523",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2237590244",
            "name": "Yuepeng Wu",
            "affiliations": [
                "Hangzhou Medical College",
                "Zhejiang Provincial People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2159348115",
            "name": "Yukang Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1972027826",
            "name": "Mei Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966037019",
            "name": "Chen Jinzhi",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2102236337",
            "name": "Yuchen Zheng",
            "affiliations": [
                "Zhejiang Provincial People's Hospital",
                "Hangzhou Medical College"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4376866715",
        "https://openalex.org/W4362706803",
        "https://openalex.org/W4393386222",
        "https://openalex.org/W4392450439",
        "https://openalex.org/W4389993479",
        "https://openalex.org/W3024235393",
        "https://openalex.org/W2769350646",
        "https://openalex.org/W2030136435",
        "https://openalex.org/W4391815795",
        "https://openalex.org/W4367175039",
        "https://openalex.org/W4394785742",
        "https://openalex.org/W4380997513"
    ],
    "abstract": "<title>Abstract</title> Background This study evaluates and compares ChatGPT-4.0, Gemini 1.5, Claude 3, and Qwen 2.1 in answering dental implant questions. The aim is to help doctors in underserved areas choose the best LLMs(Large Language Model) for their procedures, improving dental care accessibility and clinical decision-making. Methods Two dental implant specialists with over twenty years of clinical experience evaluated the models. Questions were categorized into simple true/false, complex short-answer, and real-life case analyses. Performance was measured using precision, recall, and Bayesian inference-based evaluation metrics. Results ChatGPT-4 exhibited the most stable and consistent performance on both simple and complex questions. Gemini performed well on simple questions but was less stable on complex tasks. Qwen provided high-quality answers for specific cases but showed variability. Claude-3 had the lowest performance across various metrics. Statistical analysis indicated significant differences between models in diagnostic performance but not in treatment planning. Conclusions ChatGPT-4 is the most reliable model for handling medical questions, followed by Gemini. Qwen shows potential but lacks consistency, and Claude-3 performs poorly overall. Combining multiple models is recommended for comprehensive medical decision-making.",
    "full_text": null
}