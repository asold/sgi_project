{
  "title": "Dataset Debt in Biomedical Language Modeling",
  "url": "https://openalex.org/W4285185841",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222399703",
      "name": "Jason Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2202327057",
      "name": "Natasha Seelam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114819142",
      "name": "Gabriel Altay",
      "affiliations": [
        "Tempus Labs (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2545079709",
      "name": "Leon Weber",
      "affiliations": [
        "Max Delbrück Center",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2298559727",
      "name": "Myungsun Kang",
      "affiliations": [
        "Immuneering (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100516024",
      "name": "Debajyoti Datta",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2970121659",
      "name": "Ruisi Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2901504272",
      "name": "Samuele Garda",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A290155247",
      "name": "Bo Wang",
      "affiliations": [
        "Massachusetts General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2227779229",
      "name": "Simon Ott",
      "affiliations": [
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A40952608",
      "name": "Matthias Samwald",
      "affiliations": [
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A2735478307",
      "name": "Wojciech Kusa",
      "affiliations": [
        "TU Wien"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221158733",
    "https://openalex.org/W3125468681",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3013770059",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4385567008",
    "https://openalex.org/W3154151289",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1954715867",
    "https://openalex.org/W3118813946",
    "https://openalex.org/W3145725122",
    "https://openalex.org/W2073473659",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4293227627",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W2189162242",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W2302501749",
    "https://openalex.org/W2964354311",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2146408445",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2743028754",
    "https://openalex.org/W8550301",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4221149706",
    "https://openalex.org/W3197708183",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W2104148262",
    "https://openalex.org/W4385573090",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4312055891"
  ],
  "abstract": "Jason Fries, Natasha Seelam, Gabriel Altay, Leon Weber, Myungsun Kang, Debajyoti Datta, Ruisi Su, Samuele Garda, Bo Wang, Simon Ott, Matthias Samwald, Wojciech Kusa. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.",
  "full_text": "Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 137 - 145\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nDataset Debt in Biomedical Language Modeling\nJason Alan Fries∗1,2 Natasha Seelam∗3 Gabriel Altay∗4 Leon Weber∗5,12\nMyungsun Kang∗6 Debajyoti Datta∗7 Ruisi Su∗8 Samuele Garda∗5\nBo Wang9 Simon Ott10 Matthias Samwald10 Wojciech Kusa11\n1 Stanford University 2 Snorkel AI 3 Sherlock Biosciences 4 Tempus Labs, Inc.\n5 Humboldt-Universität zu Berlin 6 Immuneering Corporation 7 University of Virginia\n8 Sway AI 9 Massachusetts General Hospital 10 Medical University of Vienna 11 TU Wien\n12 Max Delbrück Center for Molecular Medicine ∗ Equal Contribution\nAbstract\nLarge-scale language modeling and natural lan-\nguage prompting have demonstrated exciting\ncapabilities for few and zero shot learning in\nNLP. However, translating these successes to\nspecialized domains such as biomedicine re-\nmains challenging, due in part to biomedical\nNLP’s significant dataset debt – the technical\ncosts associated with data that are not consis-\ntently documented or easily incorporated into\npopular machine learning frameworks at scale.\nTo assess this debt, we crowdsourced cura-\ntion of datasheets for 167 biomedical datasets.\nWe find that only 13% of datasets are avail-\nable via programmatic access and 30% lack\nany documentation on licensing and permit-\nted reuse. Our dataset catalog is available at:\nhttps://tinyurl.com/bigbio22.\n1 Introduction\nNatural language prompting has recently demon-\nstrated significant benefits for language model pre-\ntraining, including unifying task inputs for large-\nscale multi-task supervision (Raffel et al., 2019)\nand improving zero-shot classification via explicit,\nmulti-task prompted training data (Wei et al., 2022;\nSanh et al., 2022). With performance gains re-\nported when scaling to thousands of prompted train-\ning tasks (Xu et al., 2022), tools that enable large-\nscale integration of expert-labeled datasets hold\ngreat promise for improving zero-shot learning.\nHowever, translating these successes to special-\nized domains such as biomedicine face strong head-\nwinds due in part to the current state of dataset\naccessibility in biomedical NLP. Recentlydata cas-\ncades was proposed as a term-of-art for the costs\nof undervaluing data in machine learning (Sam-\nbasivan et al., 2021). We propose a similar term,\ndataset debt, to capture the technical costs (Sculley\net al., 2015) of using datasets which are largely\nopen and findable, but inconsistently documented,\nstructured, and otherwise inaccessible via a con-\nsistent, programmatic interface. This type of debt\ncreates significant practical challenges when inte-\ngrating complex domain-specific corpora into pop-\nular machine learning frameworks.\nWe claim that biomedical NLP suffers from sig-\nnificant dataset debt. For example, while Hug-\ngingFace’s popular Datasets library (Lhoest et al.,\n2021) contains over 3,000 datasets, biomedical data\nare underrepresented and favor tasks with general\ndomain appeal such as question answering or se-\nmantic similarity (PubmedQA, SciTail, BIOSSES).\nTo assess the state of biomedical dataset debt, we\nbuilt, to our knowledge, the largest catalog of meta-\ndata for publicly available biomedical datasets. We\ndocument provenance, licensing, and other key at-\ntributes per (Gebru et al., 2021) to help guide future\nefforts for improving dataset access and machine\nlearning reproducibility.\nOur effort found low overall support for pro-\ngrammatic access, with only 13% (22/167) of\nour datasets present in the Datasets hub. Despite\na proliferation of schemas designed to standard-\nize dataset loading and harmonize task semantics.\nthere remains no consistent, API interface for easily\nincorporating biomedical data into language model\ntraining at scale.\n2 Data-Centric Machine Learning\nDeep learning models are increasingly moving to\ncommodified architectures. Data-centric machine\nlearning (vs. model-centric) is inspired by the ob-\nservation that the performance gains provided by\nnovel architectures are often smaller than gains ob-\ntained using better training data. We outline some\nkey challenges and opportunities in data-centric\nlanguage modeling. These are broadly applicable\nto NLP, but have strong relevance to biomedicine\n137\nand the current state of dataset debt.\n2.1 Curating and Cleaning Training Data\nPopular language models such as GPT-3 (Brown\net al., 2020) do not incorporate scientific or medical\ncorpora in their training mixture, contributing to\ntheir lower performance when used in biomedical\ndomains and few-shot tasks (Moradi et al., 2021).\nAdditionally, simply training the language model\non in-domain data might lead to non-trivial risks\nassociated with the recapitulated biases from the\ntraining corpora (Zhang et al., 2020; Gururangan\net al., 2022).\nIn scientific literature, discounting source prove-\nnance could manifest as language models parroting\nconflicting or inaccurate scientific findings. Zhao\net al.(Zhao et al., 2022) curated scientific corpora to\nidentify patient-specific information (e.g., mining\nPubMed Central to identify case reports that re-\nspect licensing for re-use and re-distribution). With\nsufficient metadata and dataset provenance, this\nlevel of curation could be extended to the entire\ntraining corpus for a biomedical language model.\nData cleaning has a large impact on language\nmodel performance. Deduplicating data leads to\nmore accurate, more generalizable models requir-\ning fewer training steps (Cohen et al., 2013; Lee\net al., 2021). Cleaning up the consistency of answer\nresponse strings was reported to improve biomedi-\ncal question answering (Yoon et al., 2021). Dupli-\ncation contamination is a serious risk in biomedical\ndatasets, which often iteratively build or extend\nprior annotations, introducing risk of test leakage\nin evaluation (Elangovan et al., 2021).\n2.2 Programmatic Labeling\nBiomedical domains require specialized knowl-\nedge, making expert-labeled datasets time-\nconsuming and expensive to generate. In limited-\ndata settings, distant and weakly supervised meth-\nods (Craven and Kumlien, 1999) are often used to\ncombine curated, structured resources (e.g., knowl-\nedge bases, ontologies) with expert rules to pro-\ngrammatically label data. These approaches have\ndemonstrated success across NER, relation extrac-\ntion, and other biomedical applications (Kuleshov\net al., 2019; Fries et al., 2021). However these\napproaches typically are applied to real, albeit\nunlabeled data, creating challenges when model-\ning rare classes. A recent trend is transforming\nstructured resources directly into realistic-looking,\nbut synthetic training examples. KELM (Agarwal\net al., 2021) converts Wiki knowledge graph triplets\ninto synthesized natural language text for language\nmodel pretraining.\nNatural language prompting has emerged as\na powerful technique for zero/few shot learning,\nwhere task guidance from prompts reduces sam-\nple complexity (Le Scao and Rush, 2021). Cross-\nlingual prompting (English prompts, non-English\nexamples) has demonstrated competitive classifi-\ncation performance (Lin et al., 2021). Training\nlanguage models directly on prompts has resulted\nin large gains in zero-shot performance over GPT-\n3 as well as producing models with fewer trained\nparameters (Sanh et al., 2022; Wei et al., 2022).\nPromptSource (Bach et al., 2022) is a recent soft-\nware platform for creating prompts and applying\nthem to existing labeled datasets to build training\ndata. These developments highlight a promising\ntrend toward defining programmatic transforma-\ntions on top of existing datasets, enabling them to\nbe configured into new tasks. However, leverag-\ning large-scale prompting remains challenging in\nbiomedicine due to the lack of programmatic ac-\ncess to a large, diverse collections of biomedical\ndatasets and tasks.\n2.3 Diverse Evaluation and Benchmarking\nInspired by standardized benchmarks in general\ndomain NLP research (Wang et al., 2018, 2019),\nBioNLP takes similar initiatives by establishing a\nbenchmark of 10 datasets spanning 5 tasks (Peng\net al., 2019, BLUE), an improved benchmark on\nBLUE with 13 datasets in 6 tasks (Gu et al., 2022,\nBLURB), and a benchmark of 9 different tasks\nfor Chinese biomedical NLP (Zhang et al., 2021,\nCBLUE). While these benchmarks provide tools\nfor consistent evaluation, only BLURB supports\na leaderboard and none directly provide dataset\naccess. Evaluation frameworks that provide pro-\ngrammatic access are often restricted to single and\nwell-established tasks and impose pre-processing\nchoices that can make inconsistent performance\ncomparisons (Crichton et al., 2017; Weber et al.,\n2021).\nTo the best of our knowledge, there are currently\nno zero-shot evaluation frameworks for biomedi-\ncal data similar to BIG-Bench 1, which currently\ncontains little-to-no biomedical tasks.\nEvaluation frameworks must also allow probing\nthe trained language models’ intrinsic properties,\n1https://github.com/google/BIG-bench\n138\nrather than only measure downstream classification\nperformance. Following (Petroni et al., 2019) in\nthe general NLP domain, (Sung et al., 2021) intro-\nduce BioLAMA, a benchmark making available\n49K biomedical knowledge triplets to probe the re-\nlational knowledge present in pre-trained language\nmodels.\n3 Datasets Summary\n3.1 Metadata/Datasheet Curation\nOur inclusion criteria targeted expert-annotated\ndatasets designated as public, reusable research\nbenchmarks for one or more NLP tasks. We ex-\ncluded: (1) multimodal datasets where remov-\ning the non-text modality undermines the task,\ne.g., visual question answering, audio transcrip-\ntion, image-to-text generation; (2) general re-\nsource datasets, e.g, the PMC Open Access Subset,\nMIMIC-III (Johnson et al., 2016); (3) derived re-\nsources, e.g., knowledge bases constructed via text\nmining; and (4) modeling artifacts, e.g., static em-\nbeddings or pretrained language models.\nWe recruited 8 volunteers to identify datasets\nand crowdsource their metadata curation for an\nopen, community dataset catalog. Participants re-\nviewed dataset publications and websites which\ndescribed the curation process, and then completed\nthe metadata schema outlined in Table 1 This\nschema loosely assesses compliance with FAIR\ndata principles (Wilkinson et al., 2016).\nOur initial effort identified 101 datasets. We\ncombined this list with a contemporaneously cu-\nrated catalog of biomedical datasets, identified via\nsystematic literature review (Blagec et al., 2022).\nSince the catalog described in Blagec et al. (2022)\nwas generated using broader inclusion criteria (e.g.,\nnon-public data, imaging and video datasets) we\nidentified 104/475 entries that met our criteria.\nAfter merging, we conducted a second round of\ncrowdsourcing to annotate metadata, resulting in\nour current catalog of 167 biomedical datasets.\nWe did not conduct a formal assessment of inter-\nannotator agreement.\n4 Results\n4.1 Dataset Access\nOnly 22/167 (13%) of biomedical datasets are avail-\nable via the Datasets API, despite 123/167 (74%)\nbeing openly hosted on public websites. The re-\nmaining datasets require authentication to access\nField Description\nName Dataset name\nTask Types NER, NED, QA, NLI, corefer-\nence resolution, etc.\nDomain Corpora domain: biomedical\nor clinical/health\nFile Format BioC, JSON, etc.\nAnnotations Expert label provenance\nAPI Access Available via HuggingFace\nDatasets?\nSplits Canonical definitions for train-\ning/validation/testing splits\nLicense Provided license type\nLanguages Included languages\nMultilingual Parallel corpora\nPublication Manuscript describing dataset\nYear Publication year\nCitations Google Scholar counts\nHomepage Website describing dataset\nPublic URL Open URL (no authentication)\nDead Link Dataset no longer accessible\nTable 1: Metadata collected for all biomedical datasets.\nSee Appendix A for more details on each category.\n(21%) or were dead links (5%).\nFormat Name Count Total\nStructured BioC 5 3%\nStructured BRAT 16 10%\nStructured CoNLL 11 7%\nStructured PubTator 4 2%\nSemi-structured XML 26 16%\nSemi-structured JSON 43 26%\nSemi-structured TSV/CSV 15 9%\nSemi-structured TMX 1 1%\nPlain Text Standoff 13 8%\nPlain Text Text 25 15%\nPlain Text ARFF 1 1%\nBinary Word 1 1%\nBinary Excel 2 1%\nUnknown Unknown 4 2%\nTable 2: Distribution of file formats for biomedical\ndatasets.\nTable 2 outlines the diversity of commonly used\nbiomedical file formats. Most datasets are pro-\nvided in semi-structured form (51%), followed by\nstructured (22%), and non-standard plain text files\n139\n(17%). There are several structured formats which\npropose a data model for parsing and standardiz-\ning task semantics (e.g., BRAT (Stenetorp et al.,\n2012), BioC (Comeau et al., 2013)). However, for\ninformation extraction tasks which could use these\nformats, only 31/86 (36%) actually do.\nTable 2 outlines dataset licensing, broken down\ninto six categories, largely based on commercial vs.\nnon-commercial restrictions. These cover broad\nclasses of licensing, ranging from permissive Cre-\native Commons Share-Alike licenses to dataset-\nspecific data-use agreements (DUA). Nearly 30%\nof datasets are publicly available online yet do not\ninclude any licensing information. A further 16.8%\nhave DUA requirements, but include unclear lan-\nguage on what restrictions are placed on dataset\nusage.\nLicense Restrictions Count Percent\nPublic C/NC 56 33.5%\nPublic NC 13 7.8%\nDUA C/NC 12 7.2%\nDUA NC 8 4.8%\nDUA ? 28 16.8%\nUnknown ? 50 29.9%\nTable 3: Dataset licenses. Restrictions are commercial\n(C), non-commercial (NC) and unknown (?).\n4.2 Dataset and Task Diversity\nBiomedical datasets (i.e., tasks built from scientific\npublications) made up 68% of available datasets\nwhile clinical datasets (patient notes, health news,\nclinical trial reports) made up 32%.\nFigure 1: All NLP tasks, broken down into 5 categories\n(see legend). Note datasets often support multiple tasks.\n2000 2003 2006 2009 2012 2015 2018 2021\nYear\n0\n20\n40\n60\n80\n100\n120\n140\n160Dataset Count\nInformation Extraction\nText Classification\nQuestion Answering\nSemantic Textual Similarity\nOther\nFigure 2: Cumulative count of datasets by task, ordered\nby year of dataset release. The black dashed line indi-\ncates the total number available via the Datasets API.\nFig.2 shows the overall homogeneity of public\nbiomedical datasets as of 2022. Information extrac-\ntion tasks (e.g., NER, NED, releation extraction,\ncoreference resolution) comprise 56%, followed\nby 20% text classification (e.g, document labeling,\nsentiment analysis), 13% question answering, and\n6% semantic similarity.\nTask Category Eng. Non-Eng.\nInformation Extraction 128 34\nText Classification 33 10\nQuestion Answering 21 0\nSemantic Textual Similarity 10 0\nOther 12 6\nTable 4: Task category counts by English (Eng.) and\nNon-English (Non-Eng.) languages.\nGiven all tasks, 14 languages are covered. Five\nlanguages make up 95% of all datasets. En-\nglish is the majority (80%), followed by Spanish\n(7.5%), German (2.4%), French (2.4%), and Chi-\nnese (2.4%). Table 4 contains counts of task cate-\ngories binned into English and Non-English . Ques-\ntion answering and semantic similarity have zero\nnon-English datasets.\n5 Conclusion\nIn this work, we outlined several challenges in\ntraining biomedical language models. With in-\ncreasingly large biomedical language models (Yang\net al., 2022), limitations in the quality and proper-\nties of training data grow more stark. We argue that\nbiomedical NLP suffers from significant dataset\ndebt, with only 13% of datasets accessible via API\n140\naccess and readily usable in state-of-the-art NLP\ntools. Current biomedical datasets are homoge-\nneous, largely focusing on NER and relation ex-\ntraction tasks, and predominantly English language.\nThese limitations highlight opportunities presented\nby recent data-centric machine learning methods\nsuch as prompting, which enables experts to inject\ntask guidance into training and more easily recon-\nfigure existing datasets into new training tasks.\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554–3565, Online. As-\nsociation for Computational Linguistics.\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Al-\nbert Webson, Colin Raffel, Nihal V . Nayak, Ab-\nheesht Sharma, Taewoon Kim, M Saiful Bari,\nThibault Fevry, Zaid Alyafeai, Manan Dey, An-\ndrea Santilli, Zhiqing Sun, Srulik Ben-David, Can-\nwen Xu, Gunjan Chhablani, Han Wang, Jason Alan\nFries, Maged S. Al-shaibani, Shanya Sharma, Ur-\nmish Thakker, Khalid Almubarak, Xiangru Tang,\nDragomir Radev, Mike Tian-Jian Jiang, and Alexan-\nder M. Rush. 2022. Promptsource: An integrated\ndevelopment environment and repository for natural\nlanguage prompts.\nKathrin Blagec, Jakob Kraiger, Wolfgang Frühwirt, and\nMatthias Samwald. 2022. Benchmark datasets driv-\ning artificial intelligence development fail to capture\nthe needs of medical professionals. arXiv preprint\narXiv:2201.07040.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nRaphael Cohen, Michael Elhadad, and Noémie El-\nhadad. 2013. Redundancy in electronic health record\ncorpora: analysis, impact on text mining perfor-\nmance and mitigation strategies. BMC bioinformat-\nics, 14(1):1–15.\nDonald C Comeau, Rezarta Islamaj Do ˘gan, Paolo Ci-\nccarese, Kevin Bretonnel Cohen, Martin Krallinger,\nFlorian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-\nnaldi, Manabu Torii, et al. 2013. Bioc: a minimalist\napproach to interoperability for biomedical text pro-\ncessing. Database, 2013.\nMark Craven and Johan Kumlien. 1999. Constructing\nbiological knowledge bases by extracting information\nfrom text sources. In Proceedings of the Seventh\nInternational Conference on Intelligent Systems for\nMolecular Biology, August 6-10, 1999, Heidelberg,\nGermany, pages 77–86. AAAI.\nGamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna\nKorhonen. 2017. A neural network multi-task learn-\ning approach to biomedical named entity recognition.\nBMC bioinformatics, 18(1):1–14.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization : Quantify-\ning data leakage in NLP performance evaluation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1325–1335, Online.\nAssociation for Computational Linguistics.\nJason A Fries, Ethan Steinberg, Saelig Khattar, Scott L\nFleming, Jose Posada, Alison Callahan, and Nigam H\nShah. 2021. Ontology-driven weak supervision\nfor clinical entity classification in electronic health\nrecords. Nature Communications, 12(1):1–11.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione,\nJennifer Wortman Vaughan, Hanna M. Wallach,\nHal Daumé III, and Kate Crawford. 2021. Datasheets\nfor datasets. Commun. ACM, 64(12):86–92.\nThorsten Gruber. 2014. Academic sell-out: how an\nobsession with metrics and rankings is damaging\nacademia. Journal of Marketing for Higher Educa-\ntion, 24(2):165–177.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jian-\nfeng Gao, and Hoifung Poon. 2022. Domain-specific\nlanguage model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Heal. ,\n3(1):2:1–2:23.\nSuchin Gururangan, Dallas Card, Sarah K. Dreier,\nEmily K. Gade, Leroy Z. Wang, Zeyu Wang, Luke\nZettlemoyer, and Noah A. Smith. 2022. Whose\nlanguage counts as high quality? measuring lan-\nguage ideologies in text data selection. CoRR,\nabs/2201.10474.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nV olodymyr Kuleshov, Jialin Ding, Christopher V o,\nBraden Hancock, Alexander Ratner, Yang Li, Christo-\npher Ré, Serafim Batzoglou, and Michael Snyder.\n2019. A machine-compiled database of genome-\nwide association studies. Nature communications,\n10(1):1–8.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2627–2636,\nOnline. Association for Computational Linguistics.\n141\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nMilad Moradi, Kathrin Blagec, Florian Haberl, and\nMatthias Samwald. 2021. Gpt-3 models are poor\nfew-shot learners in the biomedical domain. arXiv\npreprint arXiv:2109.02555.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65, Florence,\nItaly. Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nNithya Sambasivan, Shivani Kapania, Hannah Highfill,\nDiana Akrong, Praveen Paritosh, and Lora M Aroyo.\n2021. “everyone wants to do the model work, not\nthe data work”: Data cascades in high-stakes ai. In\nproceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, pages 1–15.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nDavid Sculley, Gary Holt, Daniel Golovin, Eugene\nDavydov, Todd Phillips, Dietmar Ebner, Vinay\nChaudhary, Michael Young, Jean-Francois Crespo,\nand Dan Dennison. 2015. Hidden technical debt in\nmachine learning systems. Advances in neural infor-\nmation processing systems, 28.\nPontus Stenetorp, Sampo Pyysalo, Goran Topi ´c,\nTomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsujii.\n2012. Brat: a web-based tool for nlp-assisted text\nannotation. In Proceedings of the Demonstrations\nat the 13th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n102–107.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4723–4734,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nLeon Weber, Mario Sänger, Jannes Münchmeyer,\nMaryam Habibi, Ulf Leser, and Alan Akbik. 2021.\nHunflair: an easy-to-use tool for state-of-the-art\nbiomedical named entity recognition. Bioinformatics,\n37(17):2792–2794.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\n142\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nMark D Wilkinson, Michel Dumontier, IJsbrand Jan\nAalbersberg, Gabrielle Appleton, Myles Axton,\nArie Baak, Niklas Blomberg, Jan-Willem Boiten,\nLuiz Bonino da Silva Santos, Philip E Bourne, et al.\n2016. The fair guiding principles for scientific data\nmanagement and stewardship. Scientific data, 3(1):1–\n9.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\ngang Wang, Haiyu Li, and Zhilin Yang. 2022. Zero-\nprompt: Scaling prompt-based pretraining to 1,000\ntasks improves zero-shot generalization. arXiv\npreprint arXiv:2201.06910.\nXi Yang, Nima Pour Nejatian, Hoo Chang Shin, Kaleb\nSmith, Christopher Parisien, Colin Compas, Mona\nFlores, Ying Zhang, Tanja Magoc, Christopher Harle,\net al. 2022. Gatortron: A large clinical language\nmodel to unlock patient information from unstruc-\ntured electronic health records. medRxiv.\nWonjin Yoon, Jaehyo Yoo, Sumin Seo, Mujeen Sung,\nMinbyul Jeong, Gangwoo Kim, and Jaewoo Kang.\n2021. Ku-dmis at bioasq 9: Data-centric and model-\ncentric approaches for biomedical question answer-\ning. In CEUR Workshop Proceedings, volume 2936,\npages 351–359. CEUR-WS.\nHaoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew\nMcDermott, and Marzyeh Ghassemi. 2020. Hurtful\nwords: quantifying biases in clinical contextual word\nembeddings. In proceedings of the ACM Conference\non Health, Inference, and Learning, pages 110–120.\nNingyu Zhang, Zhen Bi, Xiaozhuan Liang, Lei Li, Xi-\nang Chen, Shumin Deng, Luoqiu Li, Xin Xie, Hong-\nbin Ye, Xin Shang, Kangping Yin, Chuanqi Tan, Jian\nXu, Mosha Chen, Fei Huang, Luo Si, Yuan Ni, Guo-\ntong Xie, Zhifang Sui, Baobao Chang, Hui Zong,\nZheng Yuan, Linfeng Li, Jun Yan, Hongying Zan,\nKunli Zhang, Huajun Chen, Buzhou Tang, and Qing-\ncai Chen. 2021. CBLUE: A chinese biomedical lan-\nguage understanding evaluation benchmark. CoRR,\nabs/2106.08087.\nZhengyun Zhao, Qiao Jin, and Sheng Yu. 2022. Pmc-\npatients: A large-scale dataset of patient notes and\nrelations extracted from case reports in pubmed cen-\ntral. arXiv preprint arXiv:2202.13876.\nA Appendix\nA.1 Metadata Overview\nThis section contains detailed descriptions of each\nmetadata field collected for the dataset catalog.\nA.1.1 Name\nThe dataset name, preferring short forms\n(BC5CDR) as typically used on homepages or sci-\nentific publications over verbose ones (“BioCre-\native 5 Chemical Disease Relation Task\").\nA.1.2 Task Types\nDatasets contain labels for one or more tasks. Ta-\nbles 5 and 6 outline the tasks we consider in this\nwork.\nName Abbreviation\nNamed Entity Recognition NER\nNamed Entity Disambiguation NED\nRelation Extraction RE\nEvent Extraction EE\nCoreference Resolution COREF\nSpan Classification SPAN\nDocument Classification DOC\nSentence Classification SENT\nSemantic Textual Similarity STS\nQuestion Answering QA\nTranslation TRANSL\nParaphrasing PARA\nSummarization SUM\nNatural Language Inference NLI\nPart-of-Speech Tagging POS\nInformation Retreival IR\nTable 5: All task types.\nA.1.3 Domain\nSource domain of the dataset.\n• Biomedical: Tasks are defined for scientific\nliterature (e.g., PubMed abstacts, full-text pub-\nlications from the PMC Open Access Subset).\n• Clinical: Tasks are defined for clinical notes\nfrom patient electronic health records, health-\nrelated questions from social media or news\nwebsites, clinical trial reports, etc.\nA.1.4 File format\nFile formats provided by the original dataset cre-\nators.\n143\nCategory Abbreviation\nInformation Extraction NER\nInformation Extraction NED\nInformation Extraction RE\nInformation Extraction EE\nInformation Extraction COREF\nInformation Extraction SPAN\nText Classification DOC\nText Classification SENT\nSemantic Textual Similarity STS\nQuestion Answering QA\nOther TRANSL\nOther PARA\nOther SUM\nOther NLI\nOther POS\nOther IR\nTable 6: Task categories.\nA.1.5 Annotations\nProvenance of labels used to create a dataset.\n• Manual: Expert annotators directly label data\ninstances. This may include multiple rounds\nof adjudication.\n• Model-assisted Manual: Experts verify, cor-\nrect, or augment the output of a model (e.g.,\npre-annotated entities are used by annotators\nto define relations).\n• Crowdsourced: Labels are the result of a vot-\ning process over multiple annotator’s labels.\n• Rules: Heuristics developed by experts and\napplied to unlabeled text to create annotations.\nThis includes a wide range of weak/distant\nsupervision techniques.\n• Found: Generated from \"in-the-wild\" data,\nsuch as aligned pairs of translated text mined\nfrom web pages.\n• Unlabeled: no human-generated labels (e.g.,\nthe PMC Open Subset).\nA.1.6 API Access\nURL of HuggingFace’s Datasets implementation,\notherwise “no\".\nA.1.7 Splits\nAre canonical train, validation, and test sets de-\nfined by the dataset creators? If so, which sets are\nprovided. value ∈ { NONE, train, valid,\ntest }.\nA.1.8 License\nLicense information accompanying the dataset. Un-\nknown licenses means the annotator could not\nfind any information or formal legal documents\non the homepage, software repository (e.g, GitHub,\nGoogle Code), or README with the data itself.\n• Public: Creative Commons (CC BY 3.0/4.0,\nCC BY-SA 3.0/4.0), Public Domain, GNU\nFree Documentation License, GNU Common\nPublic License v3.0, MIT License, Apache\nLicense 2.0\n• Public Non-commercial: Creative Commons\n(CC BY NC 2.0/3.0/4.0, CC BY-NC-SA 4.0),\nCSIRO Data License (Non-commercial), Pub-\nlic for Research\n• DUA-NC: DUA for non-commercial use only.\n• DUA-C/NC: DUA for commercial and non-\ncommercial uses.\n• DUA-UNK: DUA with unknown restrictions.\n• Unknown: Public-Unknown, Public w/ Regis-\ntration\nA.1.9 Languages\nLanguages used in the labeled dataset.\nA.1.10 Multilingual\nDataset contains aligned pairs for two or more lan-\nguages.\nA.1.11 Publication, Year\nURL to the manuscript, DOI, and year of publica-\ntion.\nA.1.12 Citations\nCurrent citation count from Google Scholar, as of\n02-22-2022. This measure was collected to provide\na weak measure of dataset visibility. We note that\ncitation count is a problematic measure of valuation\nand subject to many criticisms (Gruber, 2014).\nA.1.13 Homepage, Public URL\nURL of website describing and hosting the dataset.\nIf the dataset has a direct download link, denote if\nit is public or only available after authentication.\nA.1.14 Dead Link\nURL of dataset homepage, as documented in the\nsource publication, is no longer active.\n144\nA.2 Domain-specific\n2000 2003 2006 2009 2012 2015 2018 2021\nYear\n0\n20\n40\n60\n80\n100\n120Dataset Count\nInformation Extraction\nText Classification\nQuestion Answering\nSemantic Textual Similarity\nOther\nFigure 3: Scientific/biomedical domain (e.g., PubMed\nabstracts) cumulative distribution of available tasks, or-\ndered by year of dataset release.\n2000 2003 2006 2009 2012 2015 2018 2021\nYear\n0\n20\n40\n60\n80\n100\n120Dataset Count\nInformation Extraction\nText Classification\nQuestion Answering\nSemantic Textual Similarity\nOther\nFigure 4: Clinical domain (e.g., patient notes) cumula-\ntive distribution of available tasks, ordered by year of\ndataset release.\nA.3 Languages\nTask English Non-English\nNER 60 18\nNED 21 9\nRE 22 3\nEE 8 0\nCOREF 8 0\nSPAN_CLASS 9 4\nSENT_CLASS 12 2\nDOC_CLASS 21 8\nQA 21 0\nSTS 10 0\nTRANSL 3 5\nPARA/SUM 2 0\nIR 3 0\nNLI 3 0\nPOS 1 1\nTable 7: Tasks by language\n145",
  "topic": "Debt",
  "concepts": [
    {
      "name": "Debt",
      "score": 0.5860176086425781
    },
    {
      "name": "Computer science",
      "score": 0.4895702302455902
    },
    {
      "name": "Linguistics",
      "score": 0.4031895399093628
    },
    {
      "name": "Natural language processing",
      "score": 0.38491714000701904
    },
    {
      "name": "Philosophy",
      "score": 0.2225169539451599
    },
    {
      "name": "Economics",
      "score": 0.15062180161476135
    },
    {
      "name": "Finance",
      "score": 0.07241952419281006
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210114883",
      "name": "Tempus Labs (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I205582932",
      "name": "Max Delbrück Center",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4391767868",
      "name": "Immuneering (United States)",
      "country": null
    },
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210087915",
      "name": "Massachusetts General Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I76134821",
      "name": "Medical University of Vienna",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I145847075",
      "name": "TU Wien",
      "country": "AT"
    }
  ]
}