{
    "title": "CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model",
    "url": "https://openalex.org/W4387560903",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104507572",
            "name": "Di Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2044601233",
            "name": "Li Jianguo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101223443",
            "name": "Yu, Hang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2013552087",
            "name": "JIANG WEI",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2191961491",
            "name": "Cai Wenting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097246861",
            "name": "Cao Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2385031991",
            "name": "Chen, Chaoyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2270813765",
            "name": "Chen Da-jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1870966209",
            "name": "Chen Hongwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1908610349",
            "name": "Chen Liang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2241325087",
            "name": "Fan Gang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2133766924",
            "name": "Gong Jie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2351402246",
            "name": "Gong, Zi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2089848430",
            "name": "Hu Wen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2159791607",
            "name": "Guo Ting-ting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3197484278",
            "name": "Lei ZhiChao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1922511425",
            "name": "Li Ting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1902402908",
            "name": "Li Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099813557",
            "name": "Liang Ming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2353447731",
            "name": "Liao Cong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2678428948",
            "name": "Liu Bing-chang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1939503429",
            "name": "Liu, Jiachen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2133740309",
            "name": "Liu Zhi-wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746367334",
            "name": "Lu Shaojun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098953635",
            "name": "Shen Min",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2750249219",
            "name": "Wang Guangpei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1379294413",
            "name": "Wang Huan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1842122615",
            "name": "Wang Zhi",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Xu, Zhaogui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1941167880",
            "name": "Yang Jiawei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1970863567",
            "name": "Ye Qing",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Zhang, Gehao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102355683",
            "name": "Zhang Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2347758954",
            "name": "Zhao Ze-lin",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Zheng, Xunjin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A848178689",
            "name": "Zhou Hailian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2120451495",
            "name": "Zhu LiFu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2363354794",
            "name": "Zhu Xian-ying",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4315706637",
        "https://openalex.org/W4386555118",
        "https://openalex.org/W4385373622",
        "https://openalex.org/W4226364033",
        "https://openalex.org/W3154106427",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4386185625",
        "https://openalex.org/W4384615697",
        "https://openalex.org/W2799226481",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4307934016",
        "https://openalex.org/W4388979610",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4226485558",
        "https://openalex.org/W4383898398",
        "https://openalex.org/W4389519352",
        "https://openalex.org/W4287024925",
        "https://openalex.org/W2147751596",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4283077732",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W4224060952",
        "https://openalex.org/W4384154649",
        "https://openalex.org/W4281758439",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4310428868",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4380993527",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4389104713",
        "https://openalex.org/W4376167329",
        "https://openalex.org/W4361866100"
    ],
    "abstract": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "full_text": "CodeFuse-13B: A Pretrained Multi-lingual Code Large Language\nModel\nPeng Di‚Ä†, Jianguo Li‚Ä†, Hang Yu‚Ä†, Wei Jiang‚Ä†, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen,\nHongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting\nLi, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen,\nGuangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu\nZhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu‚àó\nAnt Group, China\n‚Ä†Corresponding-authors: {dipeng.dp,lijg.zero,hyu.hugo,jonny.jw}@antgroup.com\nABSTRACT\nCode Large Language Models (Code LLMs) have gained signifi-\ncant attention in the industry due to their wide applications in\nthe full lifecycle of software engineering. However, the effective-\nness of existing models in understanding non-English inputs for\nmulti-lingual code-related tasks is still far from well studied. This\npaper introduces CodeFuse-13B, an open-sourced pre-trained code\nLLM 2. It is specifically designed for code-related tasks with both\nEnglish and Chinese prompts and supports over 40 programming\nlanguages. CodeFuse achieves its effectiveness by utilizing a high-\nquality pre-training dataset that is carefully filtered by program\nanalyzers and optimized during the training process. Extensive\nexperiments are conducted using real-world usage scenarios, the\nindustry-standard benchmark HumanEval-x, and the specially\ndesigned CodefuseEval for Chinese prompts. To assess the effec-\ntiveness of CodeFuse, we actively collected valuable human feed-\nback from the AntGroup‚Äôs software development process where\nCodeFuse has been successfully deployed. The results demonstrate\nthat CodeFuse-13B achieves aHumanEval pass@1 score of 37.10%,\npositioning it as one of the top multi-lingual code LLMs with sim-\nilar parameter sizes. In practical scenarios, such as code genera-\ntion, code translation, code comments, and testcase generation,\nCodeFuse performs better than other models when confronted\nwith Chinese prompts.\nCCS CONCEPTS\n‚Ä¢ Software and its engineering ;\nKEYWORDS\ncode large language models, multi-lingual, Chinese prompts\nACM Reference Format:\nPeng Di ‚Ä†, Jianguo Li ‚Ä†, Hang Yu ‚Ä†, Wei Jiang ‚Ä†, Wenting Cai, Yang Cao,\nChaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie\n‚àóNon-corresponding authors are listed in alphabetical order.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0501-4/24/04. . . $15.00\nhttps://doi.org/10.1145/3639477.3639719\nGong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li,\nMing Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun\nLu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Ji-\nawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng,\nHailian Zhou, Lifu Zhu, Xianying Zhu. 2024. CodeFuse-13B: A Pretrained\nMulti-lingual Code Large Language Model. In 46th International Conference\non Software Engineering: Software Engineering in Practice (ICSE-SEIP ‚Äô24),\nApril 14‚Äì20, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3639477.3639719\n1 INTRODUCTION\nCode Large Language Models (Code LLMs) have attracted sub-\nstantial attention in the industry owing to their vast applications\nthroughout the entire software engineering lifecycle. The release of\nCopilot, empowered by Codex [7], served as a significant testament\nto the imminent arrival of the era of intelligent code. One astonish-\ning application, ChatGPT [6, 27], has captivated an incredible user\nbase of over 100 million in two months since its launch. In recent\ncode models such as AlphaCode[21], InCoder[13], SantaCoder[1],\nStarCoder[20], and Code Llama[ 30], the incorporation of fill-in-\nthe-middle capabilities has proven to be particularly valuable for\npractical code completion scenarios.\nWhile these models have practical applications to software de-\nvelopment processes, their effectiveness in comprehending non-\nEnglish inputs for code-related tasks remains unsatisfactory[51]. To\nbridge this gap,CodeGeeX [52] attempted to establish a connection\nbetween code and non-English languages, incorporating vocabu-\nlary tokens from various natural languages. Indeed, by leveraging\nlarge, domain-specific datasets, LLMs can significantly enhance\ntheir effectiveness in applications that necessitate a combination of\nnatural language understanding and domain-specific knowledge,\nincluding specialized terminology.\nThe paper introduces CodeFuse, a Language Model for cod-\ning, which is open-sourced on GitHub 1 and Huggingface 2, an\nopen-source code Language Model (LLM). The CodeFuse project, a\ncollaborative effort within AntGroup, has witnessed monthly model\niterations resulting in consistent performance improvements, as\ndepicted in Table 1. As of this September, we open-sourced two\nversions of CodeFuse, CodeFuse-13B and CodeFuse-CodeLlama-\n34B. CodeFuse-13B underwent fine-tuning by LoRA/QLoRA on\nmultiple code tasks using the self-pretrained base model, while\n1https://github.com/codefuse-ai\n2https://huggingface.co/codefuse-ai\narXiv:2310.06266v2  [cs.SE]  10 Jan 2024\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\nTable 1: CodeFuse project roadmap.\nRelease Model HumanEval\ndate Pass@1\nMar 2023 CodeFuse-1.3B-2K Seq-Length 11.58%\nApr 2023 CodeFuse-6.5B-4K Seq-Length 20.46%\nMay 2023 CodeFuse-13B-Base-4K Seq-Length 32.93%\nJun 2023 CodeFuse-13B (opened in Sep) 37.10%\nSep 2023 CodeFuse-CodeLlama-34B (opened) 74.40%\nCodeFuse-CodeLlama-34B was fine-tuned using CodeLlama-34b-\nPython. Excitingly, CodeFuse-13B surpasses other code LLMs of\nsimilar size, and CodeFuse-CodeLlama-34B outperforms GPT4 and\nChatGPT-3.5 on the HumanEval benchmark.\nThis paper centers around the pre-trained model CodeFuse-13B,\nproviding a comprehensive overview of the development process\nand evaluating its performance in real industrial scenarios. The\nproduction of CodeFuse-13B encompasses several crucial steps,\nincluding:\n‚Ä¢Data collection: We collected about 200+ TB of code-related\ndata, and finally refined it to around 1.6TB (1T Token) of clean\ndata suitable for pre-training.\n‚Ä¢Program feature analysis: We extracted a set of program fea-\ntures from the collected code, including syntax correctness,\ncleanliness score, etc. This analysis serves three purposes: 1)\nensuring high-quality code for pre-training data, 2) provid-\ning AST/CFG/DFG/IR program feature data and extracting\ncode semantics to facilitate program understanding, and 3) pro-\nfiling the code dataset to guide constraint-based instruction\nfine-tuning. The analyzer employed a datalog-based program\nanalysis method, translating analysis into a query system.\n‚Ä¢Pre-training: Using the AntGroup‚Äôs common technology stack,\nwe developed CodeFuse to pre-train a large-scale model with\n13 billion parameters in a stable manner.\n‚Ä¢Instructional fine-tuning: This stage involved various fine-\ntuning techniques, such as supervised instruction fine-tuning\n(SFT), and multi-task instruction fine-tuning (MFT), among\nothers.\n‚Ä¢Model evaluation: We provide a comprehensive evaluation kit\nthat supports both online and offline inference evaluation meth-\nods. This kit drives model training across different downstream\ntasks based on performance indicators and visualizes feedback\non evaluation results, enabling continuous iterations and opti-\nmization of corresponding data, algorithms, and engineering\nchallenges.\n‚Ä¢Model operations: In the context of hundreds or even thou-\nsands of GPU training instances, numerous challenges emerge,\nsuch as timely automatic detection of faulty nodes, automatic\ninitiation of training tasks, and monitoring the convergence of\ntraining progress. We have made significant advancements in\naddressing these challenges through Model Operations.\nWe conducted an industry-based evaluation of CodeFuse by in-\ntegrating it into the software development process at AntGroup. Ad-\nditionally, we developed extensions for several popular Integrated\nDevelopment Environments (IDEs), namely VSCode, JetBrains, and\nData acquisition\n200T raw data including 196T code,\n1.75T Chinese and 1.7T English.\nData cleaning\na) File attribute filtering\nb) Code quality filtering\nFile attribute filter\nFilter by file-level rules including\nfile size, code proportion, etc\nCode quality filter\na) Syntax verification\nb) code feature analysis\nData detoxification\nAntGroup‚Äôs content risk control\nand privacy data protecting\nData deduplication\na) MD5 file-level deduplication\nb) SimHash fine-grained file-level\ndeduplication\nc) Segment-level deduplication\nData resampling\nResampling some languages for the\nmodel‚Äôs training effectiveness\nFigure 1: The diagram of data processing.\nAnt CloudIDE (a Web IDE). Moreover, we developed and open-\nsourced a more comprehensive benchmark, namedCodefuseEval3\n, to support for a broader range of programming scenarios involving\nChinese inputs.\nThe results demonstrate that CodeFuse-13B achieves a Hu-\nmanEval Pass@1 score of 37.10%, making it one of the top models\nwith similar sizes. In practical multi-lingual scenarios, CodeFuse\noutperforms other models with Chinese prompts, such as code\ntranslation, code comments, testcase generation and others.\nWe summarize the key contributions as follows:\n‚Ä¢We introduceCodeFuse-13B, an open-sourced pre-trained code\nLLM with 13B parameters and its training procedure. It is specif-\nically designed for code-related tasks with Chinese prompts\nand supports over 40 programming languages.\n‚Ä¢We have developed CodeFuse extensions for various IDEs.\nThese extensions enable developers to seamlessly integrate\nCodeFuse into their coding workflows, enhancing productivity\nand code generation capabilities.\n‚Ä¢We evaluate the effectiveness of CodeFuse in various appli-\ncation scenarios, including code generation, code translation,\ncode comments, and testcase generation. The results demon-\nstrate thatCodeFuse-13B achieves aHumanEvalPass@1 score\nof 37.10%, outperforming other multi-lingual models with simi-\nlar sizes. Moreover, CodeFuse-13B is better than other models\nin practical scenarios involving Chinese prompts.\n2 DATA PREPARATION\n2.1 Overview of data processing\nSince code LLMs focus more on code-related tasks, there are signif-\nicant differences compared to the methods used in general LLMs,\nin terms of data acquisition, data cleaning, detoxification, dedupli-\ncation, and data resampling, as shown in Figure 1. This paper will\nprimarily focus on how CodeFuse constructs pre-training data for\nthe code domain large model.\nData acquisition. The pre-training data for CodeFuse consists\nof 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of Eng-\nlish raw data, totaling 200TB, that are tokenized into 800 billion\n3 https://github.com/codefuse-ai/codefuse-evaluation.\nCodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model ICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\nJava\n14.0%\nPython\n12.3%\nC++\n11.7%\nJavaScript\n10.5%\nC\n9.8%\nMarkdown\n6.4%\nPHP\n5.6%\nTypeScript\n5.2%\nC#\n5.2%\nGo\n4.3%\nHTML\n3.6%\nCSS\n1.2%\nObjective-C\n1.1%\nOthers\n9.1%\nFigure 2: Distribution of programming languages.\ntokens of code, 100 billion tokens of Chinese corpus, and 100 billion\ntokens of English corpus (see Section 3.1). The original code data\nwere a combination of self-crawled GitHub dataset and opensource\ndataset Stack [19]. The combined dataset is deduplicated and filtered\nto include over 40 programming languages with the distribution\nshown in Figure 2, in which 13 programming languages account for\nmore than 1% each. Java, Python, C++, and JavaScript surpass 10%,\nwhile the remaining nearly 30 programming languages comprise\n9% of the total. The Chinese corpus is sourced from CommonCrawl,\ncomputer-related websites, documentation of programming lan-\nguages and their third-party libraries, etc. The English corpus is\nsampled from various categories in Pile including StackExchange,\nArxiv, Wikipedia, OpenWebText2, Pile-CC, etc.\nData cleaning. The cleaning strategy for code data is divided\ninto two levels. The first-level filtering strategy involves the aspect\nof file attributes, including strategies such as discarding large files\n(e.g., files with more than 10,000 lines or single files larger than 1MB)\nand discarding abnormal text (e.g., lines with an average length\ngreater than 100 or a proportion of alphanumeric characters less\nthan 40%). The second-level filtering strategy relies on AntGroup‚Äôs\nprogram analyses [15, 22, 33, 53, 54] to filter out code data that\ndoes not meet the requirements of syntax correctness and code\nquality. The open-source release of our program feature analyzer is\nplanned, and the details will be introduced in Section 2.2.\nData detoxification. We utilize AntGroup‚Äôs content risk control\nand privacy data protecting capabilities to identify and filter out\nrisky data from the training dataset, ensuring data safety.\nData deduplication. Multiple granularities of deduplication\nare performed to sensuality of training data. First is the global\ndeduplication with file-level MD5. The second is fine-grained file-\nlevel deduplication withSimHash score (e.g.‚â•0.95). Third is segment-\nlevel deduplication based on code analysis to separate codes and\ncomments, and deduplicate code and text segments when document-\nlevel SimHash score is larger than a threshold (e.g. ‚â•0.90).\nData resampling. Language distribution-based resampling\nremoves data for niche programming languages (with a data pro-\nportion below 0.1%) and downsamples HTML, CSS, JSON, and other\nsimilar languages that can negatively impact the model‚Äôs training\neffectiveness.\nThrough these data processing stages, the training data for the\nCodeFuse large model is prepared to ensure high-quality data for\ntraining and enable the model to exhibit excellent performance.\nThe filtered and resampled data are tokenized into a format directly\nusable for the pre-training phase.\n2.2 Program feature analysis\nTo improve the quality of training code data, we propose two\napproaches to analyze code features: static analysis-based and\nmodel-based methods. Static analysis-based method offers cost-\neffectiveness, interpretability, and iterative refinement but may\nstruggle with quantifying complex code features. The model-based\nmethod provides better handling of unquantifiable features but\ncomes with higher costs and weaker interpretability. For mas-\nsive code data, static analysis is preferred first, yet a hybrid ap-\nproach combining static analysis with model-based classification\ncan be used for fine-grained cleansing. This approach leverages the\nstrengths of both methodologies to ensure efficient and effective\nhandling of code features.\nHigh-quality code evaluation model. To effectively evaluate\nthe quality of code, we have proposed several metrics integrated\ninto our model. These metrics serve as indicators of code qual-\nity, and they are designed to provide comprehensive insights into\npotential areas of improvement.\n‚Ä¢Correctness measurement: After performing syntax verifi-\ncation and bug detection, it has been observed that the number\nof bugs is inversely proportional to the quality of the code. A\nhigher bug frequency indicates lower code quality. To provide\na more detailed analysis, bugs have been categorized into three\nlevels: Fatal, Error, and Warning.\n‚Ä¢Readability: Larger methods, classes, and more method param-\neters tend to be associated with lower code quality. This is due\nto the increased complexity and reduced readability that often\naccompanies large methods, classes, and more parameters.\n‚Ä¢Redundancy: The presence of redundant classes is a sign\nof poor code quality. Redundancy in classes often leads to\nincreased complexity and decreased maintainability.\n‚Ä¢Naming style: Identifier names that are either too long or\ntoo short can compromise code quality. Short names may not\nadequately describe the purpose of a variable or function, while\nexcessively long names can hinder readability.\n‚Ä¢Cyclomatic complexity: The cyclomatic complexity is a met-\nric used to measure the complexity of the control flow of a\nmodule. It quantifies the number of independent paths through\nthe code, which can also be understood as the minimum num-\nber of test cases required to cover all possible scenarios.\n‚Ä¢Coupling: It is a measure of the degree of association between\nmodules. The strength of coupling depends on the complex-\nity of the interfaces between modules, the way modules are\ninvoked, and the amount of data transmitted through inter-\nfaces. The coupling between modules refers to the dependency\nrelationship between them, including control relationships,\ninvocation relationships, and data transmission relationships.\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\nImplementation. We utilized AntGroup‚Äôs static analyzer, named\nSparrow, to filter the code. Similar to CodeQL [25] developed by\nGitHub, Sparrow is a datalog-based program analysis tool that trans-\nlates analysis into a query system. To handle the analysis of a\nsubstantial volume of code, we stored the resulting information\noffline in a database along with a datalog solver engine. This setup\nallows us to efficiently query and retrieve program analysis results.\n2.3 Code semantic extraction\nOne of the most crucial tasks for code LLMs is to comprehend the\nsemantics of the code. To support that, it is needed to extract codes\nwith natural language annotations, which serve as interpretations\nof the code‚Äôs semantics. Therefore, we employ static analysis to\nextract code snippets and their corresponding comments from high-\nquality code. This extracted data is then utilized for the Supervised\nFine-tune (SFT) of CodeFuse. The goal of this approach is to ensure\nthat the model not only understands the syntax of the programming\nlanguages but also gains a deep comprehension of the underlying\nlogic and functionality of the code, thereby enhancing its code\nunderstanding capabilities.\nStrategy for selecting code-comment pairs. To ensure the\nquality of code-comment pairs, we focus on extracting functions\nand their corresponding comments from the code. We utilize rule-\nbased approaches to filter code-comment pairs:\n‚Ä¢Meaningless comments: Meaningless comments are detected\nby Sparrow using a set of rules, which include comment key-\nword detection, identification of auto-generated setter/getter\nmethods, and others. These comments are considered irrelevant\nand are subsequently discarded.\n‚Ä¢Code length limits: Code containing fewer than 3 lines is\ndiscarded to ensure sufficient context in the code.\n‚Ä¢Effective code limits: Methods with less than 60% of effective\ncode lines are discarded to ensure a significant proportion of\nmeaningful code.\n‚Ä¢Comment length limits: To maintain readability and man-\nageability, comments longer than 512 characters are discarded.\nImplementation. We utilize tree-sitter, a fast and robust pars-\ning tool, to support multiple programming languages and handle\nlarge code volumes. By leveraging the tree-sitter‚Äôs query function-\nality, we extract function-comment pairs across different languages,\nenabling a unified data structure. The tool efficiently distributes\nand aggregates code parsing results, enhancing parsing speed and\ncompatibility with various programming languages.\nApplication. We have accumulated a dataset of 3.2 billion\nfunction-level code-comment pairs. This dataset spans various\nsources, including GitHub, GeeksforGeeks,StarCoder, and AntGroup‚Äôs\ninternal code repositories, providing a broad and diverse base for\ntraining and evaluation. The dataset is used on the following code-\nrelated tasks (see Section 4):\n‚Ä¢Code generation: This task involves the translation of natural\nlanguage into code snippets. The large dataset aids in under-\nstanding the semantic meaning behind the natural language\nand generating the corresponding code.\n‚Ä¢Code comment: This task focuses on the generation of com-\nments for given code. Leveraging the code-comment pairs in\nour dataset, CodeFuse can effectively generate meaningful and\ncontext-specific comments for any given piece of code.\n‚Ä¢Code explanation: This task deals with providing compre-\nhensive explanations for given code snippets. The vast amount\nof code-comment pairs within our dataset aids in drawing par-\nallels and generating detailed, understandable explanations of\ncode functionality.\n2.4 Code dataset portrait\nCode dataset portrait refers to an automated approach that involves\nannotating, categorizing, and analyzing code in order to capture\nvarious dimensions of large-scale codebases. By performing code\nprofiling, a more thorough and detailed understanding of the code\ndata used for training can be obtained. This understanding facili-\ntates more efficient implementation of techniques such as Super-\nvised Self-Training (SST) and Self-Supervised Fine-Tuning (SFT).\nThe applications of code portrait can be categorized as follows:\n‚Ä¢Deep insight of training data: For each portrait dimension,\nthe portrait analysis can provide measurements like validation\nloss, perplexity, and even feedback errors. These could show\nus which portion of data are more difficult to learn.\n‚Ä¢Augmenting training data: The portrait feedback urges us\nto augment training data by improving the quantity or quality\nof the corresponding weak performed dimension. On the one\nhand, since the distribution of training data could significantly\naffect a model‚Äôs behavior [55], code portrait allows us to im-\nprove the quantity or portion of weak performed dimension\naccording to evaluation feedback. On the other hand, it may\nurge us to check the quality of the weak performed portion of\ndata and find potential ways to further improve the data quality.\nBoth ways may optimize the model in subsequent experiments.\nThe implementation of code portrait is based on mentioned\nSparrow. Based on the code portrait analysis of the code added to\nAntGroup‚Äôs repositories in the past three months, the following\nkey insights have been derived:\n‚Ä¢Java is the dominant language, accounting for approximately\n40.7% of the newly added code. Within the Java codebase, the\nmajority (over 90%) is attributed to the SOFA (Scalable Open\nFinancial Architecture) projects. Therefore, the primary empha-\nsis for code generation should be on Java applications related\nto the SOFA stack.\n‚Ä¢Certain modules within the Java code, namely ‚Äôtest‚Äô, ‚Äômodel‚Äô,\n‚Äôfacade‚Äô, and ‚Äôdal‚Äô, have a higher representation. These sections\nshould be regarded as high-priority areas for code generation,\ngiven their significance within the codebase. Focusing on these\nmodules will help ensure that the generated code adequately\naddresses the needs and requirements of the system.\n‚Ä¢JavaScript constitutes 9.23% of the newly added code, making\nit the second most widely used language. Within the JavaScript\ncodebase, the Bigfish framework stands out as the most predom-\ninant. Therefore, generating code specifically for the Bigfish\nframework should be given priority to cater to the significant\nusage of this framework within the JavaScript codebase.\nCodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model ICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\n(a) CodeFuse has 40 tokens\n(b) CodeLlama has 46 tokens\n(c) CodeGen has 47 tokens\nFigure 3: Tokenization examples of different models applied to the\nsame code snippet.\n3 TRAINING\nIn this section, we introduce the pre-training procedure ofCodeFuse\nincluding tokenization, model architecture and training procedure.\n3.1 Tokenization\nOur tokenizer is BPE-based [34] and designs to avoid Out of Vocab-\nulary (OOV) problems. It has a vocabulary size of 100,864, covering\nkeywords and common words in programming languages as well\nas information techniques related corpus. The tokenizer also sup-\nports effective segmentation of natural languages like Chinese and\nEnglish. It ensures accurate code tokenization. Figure 3 provides\nexample cases to demonstrate tokenization by several well-known\ncode LLMs, highlighting the effectiveness of CodeFuse‚Äôs tokenizer.\nTo verify the tokenization performance of the CodeFuse tok-\nenizer on code, Chinese, and English, we randomly sampled 100,000\nexamples from the training dataset. The token count and compres-\nsion ratio after tokenization are shown in Table 2. The tokenization\nperformance of the CodeFuse tokenizer is significantly better than\nthat of CodeLlama and CodeGen for code, Chinese, and English.\nThis is attributed to its larger vocabulary size and dedicated code-\nspecific vocabulary.\n3.2 Model architecture\nThe model architecture of CodeFuse is an auto-regressive Trans-\nformer, similar to GPT-3 [6], with regular next-token prediction as\nthe learning objective. We made two key changes similar to GPT-\nJ [42]: (1) using Rotary positional embeddings instead of learned\npositional embeddings, and (2) employing parallel attention and\nFigure 4: Comparison CodeFuse architecture with GPT.\nfeed-forward layers (FFN) instead of serial layers as in GPT3. The\ndetails are shown in Figure 4.\nRotary positional embeddings (RoPE). We adopt RoPE [36] as\nposition embedding in Multi-Head Attention instead of the learned\npositional embedding. To balance effectiveness and computational\nefficiency, we apply RoPE only to the first half of embedding vec-\ntors [5, 42].\nParallel attention and FFN layers. In GPT-3, Multi-Head At-\ntention is computed first, and then the result is added with the\nresidual connection before being passed into FFN as shown in Equa-\ntion 1. In our model, Multi-Head Attention and FFN are computed in\nparallel, and then the results of both are added with the residual and\npassed to the next layer as shown in Equation 2. This architecture\ncan improve the computing throughput by about 15%.\nùë•ùë° +1 = ùë•ùë° +ùêπùêπùëÅ (ùêøùëÅ(ùë•ùë° +ùê¥ùë°ùë°ùëõ(ùêøùëÅ(ùë•ùë° )))) (1)\nùë•ùë° +1 = ùë•ùë° +ùê¥ùë°ùë°ùëõ(ùêøùëÅ(ùë•ùë° ))+ ùêπùêπùëÅ (ùêøùëÅ(ùë•ùë° )) (2)\nActivations. We use GeLU [16] activation function, which can\nalleviate the issue of gradient vanishing during the training process.\nMoreover, it introduces a nonlinear transformation similar to the\nsigmoid function, which helps to accelerate the convergence speed\nof the model.\nLayer normalizations. We adopt the pre-layer normalization [4]\nto the input of the Transformer block which is more robust to input\nvariations and improved gradient flow [48].\n3.3 Pre-training\nWe trained CodeFuse based on the GPT-NeoX [ 5] framework,\nwhich is built on Megatron [35] and DeepSpeed and incorporates\ndeep optimizations for algorithms, communication, and model par-\nallelism. We trainedCodeFuse on a 64-node GPU cluster, each with\neight NVIDIA A100-SXM-80GB GPUs. We trained CodeFuse in\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\nTable 2: Comparison of compression rate (C-Rate) of tokenization. C-Rate = #Tokens / #Characters, the lower the better.\nType #Characters CodeFuse CodeLlama CodeGen\n#Tokens C-Rate #Tokens C-Rate #Tokens C-Rate\nCode 338,758,753 86,787,734 0.25 99,180,237 0.29 96,289,455 0.28\nChinese 85,998,939 98,491,170 1.14 121,180,842 1.41 161,977,211 1.88\nEnglish 283,983,202 69,951,060 0.24 78,472,584 0.27 71,393,619 0.25\nvarious sizes with 350M, 1.3B, 6B, and 13B parameters, details as\nshown in Table 3.\nTraining optimizations. CodeFuse-13B is trained with 256-\nway data parallelism, 2-way tensor parallelism, and sequence paral-\nlelism, and reducing memory consumption with DeepSpeed ZeRO-\n1 [29]. The sequence length ofCodeFuse-13B is 4096, and accelerate\nlong sequence model training with Flash Attention [10]. The micro\nbatch size is 16, and the global batch size is 4096, we achieved 180\nTFLOPS and 56% average utilization rate of tensor cores on 512\nGPUs. We use Adam [18] optimizer for training, where the initial\nlearning rate is 1.5e-4, and the min learning rate is 1.5e-5, along\nwith cosine learning rate decay style. We use fp16 mixed precision\ntraining mode. To avoid precision underflow or overflow, we set\nthe initial loss scaling to 32768 and the minimum loss scaling to 1.\n3.4 Supervised Finetuning\nThe supervised finetuning (SFT) of CodeFuse containts several\ndimensions: data collection, instruction augmentation, data format-\nting, training strategy, and fine-tuning framework. The following\nsections will elaborate on each dimension.\nData collection for SFT. As a domain-specific LLM for the\ncode field, it needs to have the following functionalities:\n‚Ä¢Basic natural language understanding capability: The model\nshould provide a satisfactory understanding of complex natural\nlanguage questions for text2code tasks.\n‚Ä¢Common downstream code tasks: Tasks such as Text2Code\n(natural language to code snippet), CodeTrans(code transla-\ntion between programming languages), CodeComment (code\ncommenting), CodeExplain (code explanation), TestCase (gen-\nerating test cases), and more.\n‚Ä¢Multi-turn dialogue capability: It should support multi-turn\nconversations where user queries may or may not be related\nto the previous context, requiring the model to make accurate\nintent judgments.\n‚Ä¢Non-toxic output: The model should generate content that is\nfree from toxicity and harm. Besides adhering to basic human\nvalues and moral ethics, CodeFuse, being a code LLM, needs\nto pay special attention to code-related toxicity. For instance, it\nshould not output content that may pose information security\nissues, such as fishing or Trojan programs.\nSelf-instruct instruction augmentation. We collected question-\nanswer pairs for code-related tasks from the public domain as well\nas code semantic extraction as described in Section 2.3. However,\nthe dataset for code tasks from public domain is much smaller than\nthat of language tasks, and the code semantic extraction is restricted\nfor certain code tasks. This requires new ways to augment datasets\nfor poverty tasks. Thanks to the self-instruct techniques introduced\nin Alpaca [ 39], CodeFuse leverages those high-quality open or\nmanual writing original dataset as seeds, and generate informative\nand contextually relevant outputs with the help of off-the-shelf\nmodels like ChatGPT. This approach is suitable for divergent data\naugmentation.\nInstruction-following description. The mentioned methods\nallow us to construct suitable inputs and outputs for different tasks.\nHowever, it still has a step gap for an LLM to follow instructions,\nthat is instruction description. Some data may already have instruc-\ntions included in the input, while others require adding instruc-\ntion descriptions to familiarize the model with the task-specific\ninstructions. This enables the model to be triggered to follow sim-\nilar instruction descriptions during usage. Moreover, in-context\ndescription and chain-of-thoughts (CoT) information can be added\nto enhance the model‚Äôs performance.\nData format. In practice, data for different scenarios, such as\nmulti-turn conversations, few-shot learning, and CoT tasks, comes\nfrom diverse channels and has complex formats. To handle this,\nwe standardized the original JSON format for these different task\ntypes. Taking into account existing practices in academia and in-\ndustry, we have ultimately decided to adopt ChatML (Chat Markup\nLanguage) [24] and have made optimizations accordingly. We have\nproactively addressed issues such as tokenization errors, instruction\ninjection attacks, and Multi-Role-Playing scenarios in the format\ndesign. LLM model accepts a sequence of text inputs during training\nand inference and converts the original JSON data into a sequence\nof text in ChatML format.CodeFuse dialogue model supports three\nroles by default: System, Human, and Bot. The System role provides\ninitial information and instructions, while the Human and Bot roles\nrepresent user inputs and model-generated responses, respectively.\nOptimizations. During fine-tuning, the model focuses on learn-\ning the Bot‚Äôs output by calculating the loss only for the Bot role.\nAdditionally, CodeFuse introduces multi-task finetuning (MFT)\nsince downstream code tasks are naturally divided into multi-tasks.\nMFT introduces well-designed loss functions based on multi-task\nlearning that enables the model to effectively learn from each task\neven with different sample numbers, difficulties, and convergence\nspeed. MFT can complement different tasks to achieve better results\nthan SFT. Details on MFT will be elaborated in another paper.\n3.5 Model operations\nCodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with\na Hardware FLOPs Utilization (HFU) of approximately 60%. The\ntraining process took approximately 40 days to complete. Several\nCodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model ICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\nTable 3: Family models of CodeFuse.\nModel NumLayers NumHeads HiddenSize SeqLen BatchSize LearningRate Paralells\nCodeFuse-350M 24 16 1024 2048 1024 2e-4 DP=64\nCodeFuse-1.3B 24 16 2048 2048 1024 2e-4 DP=128\nCodeFuse-6B 28 32 4096 4096 2048 1.5e-4 DP=256\nCodeFuse-13B 40 40 5120 4096 4096 1.5e-4 DP=256, TP=2\n<| im_start |>system\nProvide some context and/or instructions to the model.\n<|im_end|>\n<| im_start |>user\nThe user ' s message goes here\n<|im_end|>\n<| im_start |> assistant\nFigure 5: An example of ChatML.\nkey stability-related capabilities were developed to ensure the suc-\ncessful training process. In particular, we developed a cloud-based\nobservability system with two major parts.\n‚Ä¢Training metrics observability is essential for monitoring\nthe training process, including important metrics like train-\ning/validation loss to assess convergence and computational\nFLOPs levels. CodeFuse has deployed a TensorBoard instance\nin the cloud, allowing users to access and analyze these metrics\nthrough a web browser.\n‚Ä¢Infrastructure metrics observability covers various aspects\nsuch as GPU and RDMA. CodeFuse uses DCGM/NVML to\ngather GPU/RDMA performance metrics, ECC errors, and Xid\nerrors. They also employ a node-side detection agent named\nWalle, to capture hardware anomaly information. The GPU Di-\nagnose component handles fault recovery operations. The GPU\ncluster utilizes RDMA high-performance networking technol-\nogy for efficient data transmission and processing.\nWe developed a GPU diagnosis system for proactive discovery\nand automatic handling failures of GPU nodes, as well as auto-\nmatically restarting/recovering terminated training tasks without\nmanual intervention. Within 30 minutes of a failure, the system\nidentifies, isolates, and reschedules faulty GPU cards, allowing\ntraining to continue with the latest checkpoint. To maintain the\nhigh availability of GPU resources, we define the SLO metric for\nthe schedulability of GPU cards, resulting in an increase of GPU\navailability from 87% to 94% for the cluster during the training\ncycle.\n4 EVALUATION\nIn this section, we begin by introducing the models we assessed\nalongside CodeFuse. Our experiments are conducted by using the\nNVIDIA A100-SXM-80GB GPUs with a Linux system. We present\na comprehensive analysis of the performance of all models on the\nHumanEval, HumanEval-x [8] and our CodefuseEval bench-\nmarks. We conduct an evaluation of CodeFuse in comparison to\nCodeGeeX, utilizing a variety of code tasks with Chinese prompts.\n4.1 Evaluation benchmarks and protocols\nHumanEval, its extension HumanEval-x and MBPP are widely\nused benchmarks in the field of code LLMs. These benchmarks\nencompass a vast collection of programming problems, employing\ntest cases to validate the code generated by code LLMs. However,\nwhen it comes to Chinese inputs, there is still a need for evaluation\nmethods in certain code scenarios. To handle this issue, we devel-\noped and open-sourced CodefuseEval 3 to facilitate evaluation in\ncode completion, code generation, cross-program-language code\ntranslation, code commenting, and test case generation scenarios\nwith both English and Chinese prompts. TheCodefuseEval bench-\nmark is an extension of the HumanEval and MBPP benchmarks,\ncovering five programming languages: Python, Java, C++, Go, and\nJavaScript.\nFor code generation tasks including code completion, text2code,\ncode translation, and test cases generation, we adopt the pass@k\nmetric as the evaluation criterion. For code commenting tasks, we\nuse Bleu and BleuRT as the evaluation metrics. To comprehensively\nassess the model‚Äôs code capabilities in the multi-task evaluation,\nthree decoding strategies were employed: temperature sampling,\ngreedy, and beam search. The prompts were formatted using zero-\nshot, allowing the model to generate responses without task-specific\nfine-tuning. When employing the temperature sampling strategy,\nwe set the hyperparameters for pass@1 as follows: temperature=0.2,\ntop_p=0.95, and generaten = 10 samples.\n4.2 Compared models\nWe compare CodeFuse to the following models with similar sizes.\nThe statistic of compared models is from the published reports.\n‚Ä¢GPT-NeoX-20B [5] is a 20 billion parameter autoregressive\nlanguage model trained on the Pile [14].\n‚Ä¢StarCoder [20] is a Code LLM with 15B parameters and a\ncontext size of 8K, which supports infilling capabilities and fast\ninference.\n‚Ä¢CodeGeeX is a language model that has been trained on a\ncollection of 23 programming languages. It is an open-sourced\nmodel with 13 billion parameters. Its training data was se-\nlected from the Pile[14], CodeParrot [47], and other datasets.\nIn addition to these datasets, CodeGeeX also includes its own\nmulti-language benchmark suite, HumanEval-x, which we\ndiscuss below.\n‚Ä¢BaiduErnie [37, 38, 44] (Enhanced Representation through\nkNowledge IntEgration) is a language representation model\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\nenhanced by using knowledge masking strategies. The masking\nstrategy of BaiduErnie inspired by BERT[11] includes phrase-\nlevel strategy and entity-level strategy.\n‚Ä¢CodeGen [26] has two versions, CodeGen-Mono-16B is a vari-\nant of CodeGen-Multi-16B, specifically fine-tuned using addi-\ntional Python code from GitHub.\n‚Ä¢CodeT5+ [45], an encoder-decoder based Code LLM, boasts\nmodular flexibility, accommodating diverse code-related down-\nstream tasks.\n‚Ä¢WizardCoder [23] is trained using the Evol-Instruct technique.\nIt has shown remarkable improvement in performance on Hu-\nmanEval python evaluations compared to previous models.\n‚Ä¢PanGu-Coder2 [32] is trained by RRTF (Rank Responses to\nalign Test&Teacher Feedback) framework with the same Evol-\nInstruct technique as WizardCoder. PanGu-Coder2 has achieved\nthe leading performance on HumanEval among models of\nsimilar size. Like WizardCoder, PanGu-Coder2 is also a mono-\nlingual model.\n‚Ä¢CodeLlama [30] is a family of large language models for code,\nbased on Llama 2. It stands out among other open models with\nits state-of-the-art performance, infilling capabilities, support\nfor large input contexts, and zero-shot instruction following\nability for programming tasks.\n4.3 Evaluation on code generation\nIn our comparison of CodeFuse with existing Code Language Mod-\nels (LLMs) of similar size, we evaluated their code generation perfor-\nmance. To ensure fairness, we gathered statistics for other models\nin Table 4 from existing reports. However, the performance of\nCodeT5+ and CodeLlama on HumanEval-x was not presented in\ntheir respective articles or other published related work. As a result,\nTable 4 lacks data for these models.\nWe present the performance of multiple versions of CodeFuse\nbecause they demonstrate the progress of CodeFuse and achieve-\nments in different tasks. Most of these versions are deployed in\nvarious scenarios. Additionally, we showcase seven current main-\nstream multi-lingual models ranging in size from 13 billion to 16\nbillion parameters. We also present three state-of-the-art code Lan-\nguage Model Models (LLMs), among which PanGu-Coder2, released\nlast month, achieved the highest HumanEval score.\nIn the HumanEval Python pass@1 evaluation, the open-sourced\nversion of CodeFuse-13B outperforms other multi-lingual mod-\nels. However, since mono-lingual models like CodeLlama-Python,\nWizardCoder, and PanGu-Coder2 have made specific optimizations\nfor Python code generation, it is still challenging for CodeFuse to\nsurpass them in Python evaluation.\n4.4 Evaluation on multi-lingual code translation\nWe evaluate CodeFuse on multi-lingual code translation by com-\nparing it to CodeGen-multi-16B and CodeGeeX-13B. Similarly\nto CodeGeeX-13B, which has a dedicated fine-tuned version for\ncode translation called CodeGeeX-13B-FT, CodeFuse-13B-SFT is a\nmulti-task fine-tuned version that includes code translation.\nThe code translation evaluation dataset we used is constructed\nbased on MBXP and HumanEval-x. It includes cases that have been\nreviewed and corrected by experts and has been opened as a part\nof CodefuseEval.\nTable 5 presents the pass@1 results for mutual conversion be-\ntween Java, Python, and C++ using greedy decoding. The table\nclearly shows that CodeFuse-13B-SFT outperforms other models\nwhen translating Python code to the other two languages. Addition-\nally, it achieves the highest average score across all six translation\nscenarios.\n4.5 Evaluation on code-related tasks with\nChinese prompts\nWe conducted an evaluation of CodeFuse on supporting Chinese\nprompts by comparing it to CodeGeeX, which is known for its\nexcellent Chinese language support. Figure 6 showcases examples\nof code generation, code translation, code comments, and test case\ngeneration with prompts in Chinese. In addition to the evaluation of\ncode generation and translation mentioned in the previous sections,\ntestcase generation, code comments, and explanation are included\nin our evaluation.\nIn the code comment and explanation tasks, We specifically\nevaluated our CodeFuse, by integrating it into AntGroup‚Äôs devel-\nopment process. Valuable feedback from developers during daily\nwork was collected to assess performance in real-world scenarios.\nWe did not compare CodeFuse with other models due to the time\nand effort required to deploy a competing model in our daily de-\nvelopment process. After collecting human feedback over several\nmonths, CodeFuse-13B-SFT achieved a Bleu score of 42.42% and a\nBleuRT score of 36.34% as shown in Table 6. These results indicate\nthat CodeFuse is indeed useful in real development scenarios.\nRecently, there have been efforts to automatically generate test\ncases by providing a code snippet to LLMs [31, 43, 49]. In our eval-\nuation, we used the HumanEval-x dataset and selected CodeGeeX\nas the baseline model due to its comparable model size, support for\nChinese prompts, and relevance to industry applications. We made\nminor adjustments to the dataset to adhere to the prompt format\nshown in Figure 6(c). As shown in Table 7, CodeFuse outperforms\nCodeGeeX in the task of testcase generation with Chinese prompts.\n5 RELATED WORK\nLarge language models. LLMs have exhibited remarkable ac-\ncomplishments across a wide range of tasks. Leading technology\ncompanies have made substantial progress in creating highly ca-\npable LLMs. Notable examples include OpenAI‚Äôs GPT3&4 [6, 27],\nGoogle‚Äôs PaLM [2, 9], DeepMind‚Äôs Chinchilla [17] and Gopher [28],\nas well as Anthropic‚Äôs Claude4. However, these models are closed-\nsource and only accessed through specific APIs.\nSeveral open-source LLMs have been released and made valuable\ncontributions to the public. EleutherAI has contributed GPT-NeoX-\n20B [5] and GPT-J-6B [42]. Google has released UL2-20B. Tsinghua\nUniversity has introduced GLM-130B [50, 52]. Meta has released\nLLaMA [40] and LLaMA2 [41].\nCode large language models. Many works have introduced\nLLMs to tackle the challenges of code understanding and generation\nproblems. Codex [7] has powered Copilot for code tasks. Google\nhas proposed PaLM-Coder [9]. These models have shown excep-\ntional performance on popular code completion benchmarks such\nCodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model ICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\na. An example of code generation.\nb. An example of code translation.\nc. An example of testcase generation.\nd. An example of code comments and explanation.\nFigure 6: Examples of CodeFuse generation with prompts in Chinese. The results are generated by CodeFuse VSCode extension, and indicators\nsuch as ‚Äú#‚Äù have been removed for better human readability.\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\nTable 4: Performance comparison of CodeFuse with previous models with similar size on HumanEval-x.\nLingual Models HumanEval-x pass@1\nPython Java C++ JavaScript Go\nCodeFuse CodeFuse-13B-Base 24.83% 23.78% 22.08% 19.62% 18.17%\nCodeFuse-13B-SFT 37.10% 26.22% 19.51% 31.71% 24.39%\nMulti-\nGPT-NeoX-20B 13.83% 8.87% 9.90% 11.28% 5.00%\nCodeGEEX-13B 22.89% 20.04% 17.06% 17.59% 14.43%\nBaidu-ERNIE-3.5-15.5B 35.37% 26.22% 20.11% 34.76% 27.43%\nStarCoder-15.5B 33.57% 30.22% 31.55% 30.79% 17.61%\nCodeGen-multi-16B 19.22% 14.95% 18.05% 18.40% 13.03%\nCodeT5+-16B 30.90%\nCodeLlama-13B 36.00%\nMono-\nCodeLlama-Python-13B 43.30%\nWizardCoder-16B 57.30%\nPanGu-Coder2-15B 61.64%\nTable 5: Performance(pass@1) comparison of CodeFuse with previous models on code translation using greedy decoding\nModels Java to Py C++ to Py C++ to Java Java to C++ Py to Java Py to C++ Average\nCodeFuse-13B-Base 53.66% 55.49% 41.46% 37.80% 48.10% 50.00% 47.75%\nCodeFuse-13B-SFT 66.46% 59.15% 54.27% 47.56% 56.31% 55.40% 56.53%\nCodeGen-multi-16B 52.73% 33.83% 43.20% 41.42% 29.27% 35.94% 39.40%\nCodeGeeX-13B 43.41% 27.18% 22.56% 39.33% 25.84% 26.54% 30.81%\nCodeGeeX-13B-FT 75.03% 62.79% 71.68% 49.67% 41.98% 34.16% 55.89%\nTable 6: Evaluation of Chinese code comments and explanations on\nCodefuseEval and real feedback\nModels Bleu BleuRT\nCodeFuse-13B-Base 36.75% 27.76%\nCodeFuse-13B-SFT 42.42% 36.34%\nTable 7: Evaluation of Chinese testcase generation on CodefuseEval\nModels pass@1 Python pass@1 Java\nCodeFuse-13B-SFT 31.20% 24.32%\nCodeGeeX-13B 22.89% 20.04%\nas HumanEval [8] and MBPP [3]. However, it is important to note\nthat these models are closed-source.\nThere also are several open-source Code LLMs available. Sales-\nforce has developed CodeGen [26], CodeT5 [46], and CodeT5+ [45].\nTsinghua University has contributed CodeGeeX [52], and the Big-\nCode Project has created StarCoder [20]. While recent models like\nWizardCode [23], PanGu-Coder2 [32], and CodeLLaMA [30] have\nachieved impressive scores on HumanEval, it is worth noting that\nthey are weak for multi-lingual prompts. In contrast, CodeFuse is\ndesigned to be a multi-lingual code Language Model (LLM) that sup-\nports various code-related tasks across both English and Chinese\nprompots. This makes CodeFuse a valuable solution for developers\nworking in diverse linguistic environments.\n6 DISCUSSION, CONCLUSION, FUTURE WORK\nExtending MFT framework to support opensource models.\nWe develop a multi-task finetuning framework and open-source\nit as MFTCoder4. This framework can be used to fine-tune both\nour in-house developed CodeFuse-13B and newly emerging open-\nsourced Code LLMs such as StarCoder [20] and CodeLLaMA [30].\nWe fine-tuned StarCoder and CodeLLaMA models with the MFT-\nCoder framework on our collected datasets, and open-sourced the\nfine-tuned version as CodeFuse-CodeLlama-34B and CodeFuse-\nStarCoder-15B. CodeFuse-CodeLlama-34B achieves 74.4% pass@1\nscore on HumanEval, which surpasses the score by GPT4 and\nChatGPT-3.5, and represents the state-of-the-art results for open-\nsourced Language Model Models (LLMs). It is also evident that\nthe data preparation strategy in CodeFuse greatly enhances the\nperformance of other code LLMs.\nDeployment. CodeFuse is deployed in the production envi-\nronment within AntGroup in the form of both IDE plugin and\nweb-based chat. See Figure 6 for some examples. To facilitate the\nmodel response time and service throughput, we introduce a series\nof optimizations for the model service, which include (1) quantiz-\ning the model to 4bits with negligible accuracy loss using auto-\nmatic iterative refined GPTQ [12]; (2) leveraging software optimiza-\ntion provided by Nvidia TensorRT-LLM5; (3) performing service\noptimization through semantic cache and streaming output. The\nproduct now supports daily software developing of more than 5K\nengineers in AntGroup.\n4https://github.com/codefuse-ai/MFTCoder\n5https://developer.nvidia.com/tensorrt-llm-early-access\nCodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model ICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\nConclusion. This paper introduces CodeFuse-13B, an open-\nsourced pre-trained Language Model (LLM) with 13 billion param-\neters designed for code-related tasks with multi-lingual (English\nand Chinese) prompts. It supports over 40 programming languages\nand utilizes a carefully filtered pre-training dataset. Experiments\nusing real-world scenarios and industry benchmarks demonstrate\nthat CodeFuse-13B achieves aHumanEval Pass@1 score of 37.10%,\nmaking it one of the top multi-lingual models with similar parame-\nter sizes. It outperforms other models in code generation, transla-\ntion, comments, and testcase generation tasks with Chinese inputs.\nValuable human feedback from AntGroup‚Äôs software development\nprocess confirms the successful integration of CodeFuse-13B.\nFuture work. Besides models of CodeFuse and the MFTCoder\nframework, we plan to further open-source two significant compo-\nnents of CodeFuse: the CodefuseEval benchmark and the Sparrow\nprogram query system for high-quality code data cleaning. By open-\nsourcing these components, we aim to contribute to the research\ncommunity and facilitate further advancements in the full lifecycle\nof AI native software development.\nREFERENCES\n[1] Loubna Ben Allal, Raymond Li, Denis Kocetkov, et al. 2023. SantaCoder: don‚Äôt\nreach for the stars! arXiv:2301.03988 [cs.SE]\n[2] Rohan Anil, Andrew M. Dai, Orhan Firat, et al. 2023. PaLM 2 Technical Report.\narXiv:2305.10403 [cs.CL]\n[3] Jacob Austin, Augustus Odena, Maxwell Nye, et al. 2021. Program Synthesis with\nLarge Language Models. arXiv:2108.07732 [cs.PL]\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-\ntion. arXiv:1607.06450 [stat.ML]\n[5] Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. GPT-NeoX-20B: An Open-\nSource Autoregressive Language Model. arXiv:2204.06745 [cs.CL]\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, et al. 2020. Language Models are\nFew-Shot Learners. arXiv:2005.14165 [cs.CL]\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, et al. 2021. Evaluating Large Language\nModels Trained on Code. arXiv:2107.03374 [cs.LG]\n[8] Mark Chen, Jerry Tworek, Heewoo Jun, et al. 2021. Evaluating Large Language\nModels Trained on Code. arXiv:2107.03374 [cs.LG]\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. 2022. PaLM: Scaling\nLanguage Modeling with Pathways. arXiv:2204.02311 [cs.CL]\n[10] Tri Dao, Daniel Y. Fu, Stefano Ermon, et al . 2022. FlashAttention: Fast and\nMemory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135 [cs.LG]\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [cs.CL]\n[12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ:\nAccurate post-training quantization for generative pre-trained transformers.\narXiv preprint arXiv:2210.17323 (2022).\n[13] Daniel Fried, Armen Aghajanyan, Jessy Lin, et al. 2023. InCoder: A Generative\nModel for Code Infilling and Synthesis. arXiv:2204.05999 [cs.SE]\n[14] Leo Gao, Stella Biderman, Sid Black, et al. 2020. The Pile: An 800GB Dataset of\nDiverse Text for Language Modeling. arXiv:2101.00027 [cs.CL]\n[15] Ant Group. 2023. Sparrow. http://sparrow.alipay.com.\n[16] Dan Hendrycks and Kevin Gimpel. 2023. Gaussian Error Linear Units (GELUs).\narXiv:1606.08415 [cs.LG]\n[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al . 2022. Training\nCompute-Optimal Large Language Models. arXiv:2203.15556 [cs.CL]\n[18] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Opti-\nmization. arXiv:1412.6980 [cs.LG]\n[19] Denis Kocetkov, Raymond Li, Loubna Ben Allal, et al. 2022. The Stack: 3 TB of\npermissively licensed source code. arXiv:2211.15533 [cs.CL]\n[20] Raymond Li, Loubna Ben Allal, Yangtian Zi, et al . 2023. StarCoder: may the\nsource be with you! arXiv:2305.06161 [cs.CL]\n[21] Yujia Li, David Choi, Junyoung Chung, et al . 2022. Competition-level code\ngeneration with AlphaCode. Science 378, 6624 (dec 2022), 1092‚Äì1097. https:\n//doi.org/10.1126/science.abq1158\n[22] Jiangchao Liu, Jierui Liu, Peng Di, et al. 2023. Hybrid Inlining: A Framework for\nCompositional and Context-Sensitive Static Analysis. In Proceedings of the 32nd\nACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA\n2023, Seattle, WA, USA, July 17-21, 2023 , Ren√© Just and Gordon Fraser (Eds.). ACM,\n114‚Äì126.\n[23] Ziyang Luo, Can Xu, Pu Zhao, et al. 2023. WizardCoder: Empowering Code Large\nLanguage Models with Evol-Instruct. arXiv:2306.08568 [cs.CL]\n[24] MicroSoft. 2023. ChatML. https://github.com/openai/openai-python/blob/main/\nchatml.md.\n[25] Oege de Moor, Mathieu Verbaere, Elnar Hajiyev, et al. 2007. Keynote Address:\n.QL for Source Code Analysis. In Seventh IEEE International Working Conference\non Source Code Analysis and Manipulation (SCAM 2007) . 3‚Äì16. https://doi.org/10.\n1109/SCAM.2007.31\n[26] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, et al . 2023. CodeGen: An\nOpen Large Language Model for Code with Multi-Turn Program Synthesis.\narXiv:2203.13474 [cs.LG]\n[27] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[28] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, et al. 2022. Scaling Language Models:\nMethods, Analysis & Insights from Training Gopher. arXiv:2112.11446 [cs.CL]\n[29] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.\nZeRO: Memory Optimizations Toward Training Trillion Parameter Models.\narXiv:1910.02054 [cs.LG]\n[30] Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, et al. 2023. Code Llama: Open\nFoundation Models for Code. arXiv:2308.12950 [cs.CL]\n[31] Max Sch√§fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An Empirical\nEvaluation of Using Large Language Models for Automated Unit Test Generation.\narXiv:2302.06527 [cs.SE]\n[32] Bo Shen, Jiaxin Zhang, Taihong Chen, et al. 2023. PanGu-Coder2: Boosting Large\nLanguage Models for Code with Ranking Feedback. arXiv:2307.14936 [cs.CL]\n[33] Qingkai Shi, Xiao Xiao, Rongxin Wu, et al. 2018. Pinpoint: fast and precise sparse\nvalue flow analysis for million lines of code. In Proceedings of the 39th ACM\nSIGPLAN Conference on Programming Language Design and Implementation, PLDI\n2018, Philadelphia, PA, USA, June 18-22, 2018 , Jeffrey S. Foster and Dan Grossman\n(Eds.). ACM, 693‚Äì706.\n[34] Yusuke Shibata, Takuya Kida, Shuichi Fukamachi, et al. 1999. Byte Pair Encoding:\nA Text Compression Scheme That Accelerates Pattern Matching. (09 1999).\n[35] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, et al . 2020. Megatron-LM:\nTraining Multi-Billion Parameter Language Models Using Model Parallelism.\narXiv:1909.08053 [cs.CL]\n[36] Jianlin Su, Yu Lu, Shengfeng Pan, et al. 2022. RoFormer: Enhanced Transformer\nwith Rotary Position Embedding. arXiv:2104.09864 [cs.CL]\n[37] Yu Sun, Shuohuan Wang, Yukun Li, et al . 2019. ERNIE 2.0: A Continual Pre-\ntraining Framework for Language Understanding. arXiv:1907.12412 [cs.CL]\n[38] Yu Sun, Shuohuan Wang, Yukun Li, et al. 2019. ERNIE: Enhanced Representation\nthrough Knowledge Integration. arXiv:1904.09223 [cs.CL]\n[39] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, et al. 2023. Stanford Alpaca: An\nInstruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\nalpaca.\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al. 2023. LLaMA: Open and\nEfficient Foundation Language Models. arXiv:2302.13971 [cs.CL]\n[41] Hugo Touvron, Louis Martin, Kevin Stone, et al. 2023. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[42] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autore-\ngressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\n[43] Junjie Wang, Yuchao Huang, Chunyang Chen, et al. 2023. Software Testing with\nLarge Language Model: Survey, Landscape, and Vision. arXiv:2307.07221 [cs.SE]\n[44] Shuohuan Wang, Yu Sun, Yang Xiang, et al. 2021. ERNIE 3.0 Titan: Exploring\nLarger-scale Knowledge Enhanced Pre-training for Language Understanding and\nGeneration. arXiv:2112.12731 [cs.CL]\n[45] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, et al . 2023. CodeT5+:\nOpen Code Large Language Models for Code Understanding and Generation.\narXiv:2305.07922 [cs.CL]\n[46] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5:\nIdentifier-aware Unified Pre-trained Encoder-Decoder Models for Code Under-\nstanding and Generation. arXiv:2109.00859 [cs.CL]\n[47] Thomas Wolf, Lysandre Debut, Victor Sanh, et al . 2020. Transformers: State-\nof-the-Art Natural Language Processing. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations .\nAssociation for Computational Linguistics, Online, 38‚Äì45.\n[48] Ruibin Xiong, Yunchang Yang, Di He, et al. 2020. On Layer Normalization in the\nTransformer Architecture. arXiv:2002.04745 [cs.LG]\n[49] Guixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang Huang, et al. 2021. Auto-\nmated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing. In\nProceedings of the 42nd ACM SIGPLAN International Conference on Programming\nLanguage Design and Implementation (PLDI 2021) . Association for Computing\nMachinery, 435‚Äì450.\n[50] Aohan Zeng, Xiao Liu, Zhengxiao Du, et al. 2022. GLM-130B: An Open Bilingual\nPre-trained Model. arXiv:2210.02414 [cs.CL]\n[51] Wayne Xin Zhao, Kun Zhou, Junyi Li, et al. 2023. A Survey of Large Language\nModels. arXiv:2303.18223 [cs.CL]\n[52] Qinkai Zheng, Xiao Xia, Xu Zou, et al . 2023. CodeGeeX: A Pre-Trained\nModel for Code Generation with Multilingual Evaluations on HumanEval-X.\narXiv:2303.17568 [cs.LG]\nICSE-SEIP ‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Peng Di, et al.\n[53] Zexin Zhong, Jiangchao Liu, Diyu Wu, Peng Di, et al. 2022. Field-Based Static\nTaint Analysis for Industrial Microservices. In 44th IEEE/ACM International Con-\nference on Software Engineering: Software Engineering in Practice, ICSE (SEIP) 2022,\nPittsburgh, PA, USA, May 22-24, 2022 . IEEE, 149‚Äì150.\n[54] Zexin Zhong, Jiangchao Liu, Diyu Wu, Peng Di, et al. 2023. Scalable Compositional\nStatic Taint Analysis for Sensitive Data Tracing on Industrial Micro-Services.\nIn 45th IEEE/ACM International Conference on Software Engineering: Software\nEngineering in Practice, SEIP@ICSE 2023, Melbourne, Australia, May 14-20, 2023 .\nIEEE, 110‚Äì121.\n[55] Xin Zhou, Kisub Kim, Bowen Xu, et al . 2023. The Devil is in the\nTails: How Long-Tailed Code Distributions Impact Large Language Models.\narXiv:2309.03567 [cs.SE]"
}