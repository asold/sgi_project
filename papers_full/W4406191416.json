{
  "title": "Standardized patient profile review using large language models for case adjudication in observational research",
  "url": "https://openalex.org/W4406191416",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A381886325",
      "name": "Martijn J. Schuemie",
      "affiliations": [
        "Johnson & Johnson (United States)",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2930114909",
      "name": "Anna Ostropolets",
      "affiliations": [
        "Columbia University Irving Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4365488401",
      "name": "Aleh Zhuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115817216",
      "name": "Uladzislau Korsik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2800029837",
      "name": "Seung In Seo",
      "affiliations": [
        "Kangdong Sacred Heart Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A30074376",
      "name": "Marc A. Suchard",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2087940392",
      "name": "George Hripcsak",
      "affiliations": [
        "Columbia University Irving Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2135425768",
      "name": "Patrick B. Ryan",
      "affiliations": [
        "Johnson & Johnson (United States)",
        "Columbia University Irving Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A381886325",
      "name": "Martijn J. Schuemie",
      "affiliations": [
        "Health Data Research UK",
        "University of California, Los Angeles",
        "Johnson & Johnson (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2930114909",
      "name": "Anna Ostropolets",
      "affiliations": [
        "Columbia University Irving Medical Center",
        "Health Data Research UK"
      ]
    },
    {
      "id": "https://openalex.org/A4365488401",
      "name": "Aleh Zhuk",
      "affiliations": [
        "Health Data Research UK"
      ]
    },
    {
      "id": "https://openalex.org/A5115817216",
      "name": "Uladzislau Korsik",
      "affiliations": [
        "Health Data Research UK"
      ]
    },
    {
      "id": "https://openalex.org/A2800029837",
      "name": "Seung In Seo",
      "affiliations": [
        "Kangdong Sacred Heart Hospital",
        "Health Data Research UK"
      ]
    },
    {
      "id": "https://openalex.org/A30074376",
      "name": "Marc A. Suchard",
      "affiliations": [
        "University of California, Los Angeles",
        "Health Data Research UK"
      ]
    },
    {
      "id": "https://openalex.org/A2087940392",
      "name": "George Hripcsak",
      "affiliations": [
        "Health Data Research UK",
        "Columbia University Irving Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2135425768",
      "name": "Patrick B. Ryan",
      "affiliations": [
        "Health Data Research UK",
        "Columbia University Irving Medical Center",
        "Johnson & Johnson (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2132096417",
    "https://openalex.org/W4387699659",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W4390578581",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W1126991912",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W2981664222",
    "https://openalex.org/W4224433810"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01433-4\nStandardized patient proﬁle review using\nlarge language models for case\nadjudication in observational research\nCheck for updates\nMartijn J. Schuemie 1,2,3 , Anna Ostropolets1,4, Aleh Zhuk1,5, Uladzislau Korsik1,5, Seung In Seo1,6,\nMarc A. Suchard1,3, George Hripcsak1,4 & Patrick B. Ryan1,2,4\nUsing administrative claims and electronic health records for observational studies is common but\nchallenging due to data limitations. Researchers rely on phenotype algorithms, requiring labor-\nintensive chart reviews for validation. This study investigates whether case adjudication using the\npreviously introduced Knowledge-Enhanced Electronic Proﬁle Review (KEEPER) system with large\nlanguage models (LLMs) is feasible and could serve as a viable alternative to manual chart review. The\ntask involves adjudicating cases identiﬁed by a phenotype algorithm, with KEEPER extracting\npredeﬁned ﬁndings such as symptoms, comorbidities, and treatments from structured data. LLMs\nthen evaluate KEEPER outputs to determine whether a patient truly qualiﬁes as a case. We tested four\nLLMs including GPT-4, hosted locally to ensure privacy. Using zero-shot prompting and iterative\nprompt optimization, we found LLM performance, across ten diseases, varied by prompt and model,\nwith sensitivities from 78 to 98% and speciﬁcities from 48 to 98%, indicating promise for automating\nphenotype evaluation.\nIn the realm of healthcare research, thereuse of data, such as administrative\nclaims and electronic health records (EHRs), for observational studies has\nbecome commonplace. However, these datasets, not originally collected for\nresearch purposes, often lack precise information required to address spe-\nciﬁc research questions. Researchers often must infer important variables\nsuch as exposures and outcomes from available markers such as diagnosis\ncodes and laboratory tests. To extract health outcomes of interest, guidelines\nrecommend ﬁrst crafting conceptual case deﬁnitions and then deriving\noperational deﬁnitions, containing speciﬁc codes such as ICD-10 and\nLOINC to look for, and the logic to combine them\n1. These operational\ndeﬁnitions are often referred to as phenotype algorithms.\nThe validity of observational research hinges on the accuracy of these\nalgorithms, a critical aspect addressed through outcome (and similarly\nexposure) validation. Typically, this involves labor-intensive chart review,\nscrutinizing clinical details to ensure the operational deﬁnition accurately\nrepresents the conceptual case deﬁn i t i o n .Y e t ,c h a r tr e v i e wi st i m e - c o n -\nsuming, subjective, and lacks portability between datasets. FDA guidelines\n1\nadvocate for comprehensive chart review of all potential cases, but the\npracticality of this approach is limited. In practice, often only a small sample\nof identiﬁed cases is reviewed, allowing only the positive predictive value\n(PPV) to be computed, which is insufﬁcient to fully determine potential bias\nfrom outcome misclassiﬁcations\n2. At the very least, guidelines recommend\nreviewing a sample of both identiﬁed cases and non-cases, allowing for\nquantitative bias analysis, which is rarely feasible for rare outcomes.\nIn response to these challenges, our previous work introduced the\nKnowledge-Enhanced Electronic Proﬁle Review (KEEPER) system, as\nillustrated in Fig.1b and described in the Methods section. KEEPER is a\nphenotype evaluation tool that extracts patient’s structured data elements\nrelevant to a phenotype and presents them in a standardized fashion fol-\nlowing clinical reasoning principles, allowing chart reviewers to adjudicate\ncases more accurately and efﬁciently3. KEEPER demonstrated high agree-\nment with manual chart review, while reducing time needed to review and\nincreasing inter-annotator agreement, alleviating some challenges asso-\nciated with traditional chart review methods. Another beneﬁt of KEEPER is\nthat it utilized the Observational Medical Outcomes Partnership (OMOP)\nCommon Data Model (CDM), thereby allowing review for data sources that\ndo not readily provide charts, such as administrative claims.\nBuilding upon this foundation, we extend the scalability and cost-\neffectiveness of KEEPER by incorporating large-language models (LLMs),\nto adjudicate cases based on KEEPER’s output. LLMs, recognized for their\n1Observational Health Data Science and Informatics, New York, NY, USA.2Global Epidemiology Organization, Johnson & Johnson, Titusville, NJ, USA.\n3Department of Biostatistics, UCLA, Los Angeles, CA, USA.4Department of Biomedical Informatics, Columbia University Irving Medical Center, New York, NY,\nUSA. 5Odysseus Data Services, Cambridge, MA, USA.6Division of Gastroenterology, Department of Internal Medicine, Kangdong Sacred Heart Hospital, Hallym\nUniversity College of Medicine, Seoul, Republic of Korea.e-mail: schuemie@ohdsi.org\nnpj Digital Medicine|            (2025) 8:18 1\n1234567890():,;\n1234567890():,;\npotential applications in the medical domain4–6, offer a promising avenue for\nautomating the adjudication process. Given privacy constraints, we evaluate\nin-house hosted LLMs— speciﬁcally, the commercially available GPT-3.5\nTurbo and GPT-4, and the freely available Llama-2 and Sheep-Duck-\nLlama-2.\nTo illustration our approach: one of the diseases evaluated in our study\nis osteoporosis, deﬁned conceptually (per Supplementary Table 3) as“a\nskeletal disorder characterized by decreased bone density and strength,\nleading to fragile bones and an increased risk of fractures.” Our phenotype\nalgorithm identiﬁes cases by theﬁrst recorded diagnosis code that maps to\nthe standard concept of Osteoporosis or any of its descendants within the\nCDM, facilitating cross-system compatibility7. For example, this standard\nconcept encompasses 223 ICD-10 codes, including M81.9 (Osteoporosis,\nunspeciﬁed) and M81.0 (Postmenopausal osteoporosis). Running this\nalgorithm identiﬁes patients with an index date marking the point at which\nthey are believed to have osteoporosis. To assess the algorithm’s operating\ncharacteristics in a speciﬁc data source, we could use either manual chart\nreview or the KEEPER system. We deﬁned sets of concepts for KEEPER to\nextract (see Supplementary Note 2)from certain time windows (Supple-\nmentary Table 1). These include related conditions (e.g.,“Malignant neo-\nplastic disease” within 90 days of index), symptoms (e.g.,“Joint pain of\npelvic region” within 30 days before index), comorbidities (e.g.,“Fracture of\nbone” at any point prior), and relevant treatments (e.g.,“Zoledronic acid”\nprescribed after diagnosis). Prior research suggests that KEEPER’so u t p u t\nprovides enough information for human reviewers to conﬁrm or refute\nosteoporosis case status. Here, we investigate how accurately a large lan-\nguage model (LLM) can make this determination based on KEEPER’s\noutput.\nThis manuscript investigates whether case adjudication using KEEPER\nand LLMs is feasible and could serve as a viable alternative to manual chart\nreview. We outline the prompt engineering guided by a development set\nspeciﬁcally created for this purpose. We then assess performance of the\noptimal prompting strategy on three test sets, comparing results to gold\nstandards derived from human annotation. Additionally, we demonstrate\nhow the combination of KEEPER and LLMs can be used to create a large\nsilver standard without human intervention, allowing estimation of both\npositive predictive value and sensitivity of phenotype algorithms. The\nMethods section describes the KEEPER system and outlines the creation of\nthe development and test sets.\nResults\nPrompt engineering\nUsing a dedicated human-annotated development set to guide our prompt\nengineering, we initiate our approach with a basic system prompt,\ninstructing the LLM to deliver a binary‘yes’or ‘no’response to the question\nof whether the patient had the disease of interest. Following the concept of\n‘chain-of-thought’\n8, we progressively enhanced the prompt. We introduced\ntext prompting the LLM to initiallypresent evidence both in favor and\nagainst the speciﬁed disease. Additionally, we requested the LLM to generate\na clinical narrative aligning with the provided data. Observing a tendency for\nthe LLM to heavily weigh diagnoses,leading to frequent false positive\nclassiﬁcations, we adjusted the system prompt to emphasize that a single\ndiagnosis does not conclusively indicate the case as true. Subsequently, we\nidentiﬁed inconsistency in how the LLM handled uncertainty, occasionally\nresponding‘yes’even when another diagnosis was more likely or‘no’in the\npresence of unreasonable doubt. To address this, explicit instructions on\nhandling uncertainty were incorporated into the prompt. To enhance\nperformance, we further introduced two examples through few-shot\nprompting. The prompting strategy demonstrating the highest area under\nthe receiver operator curve (AUC) on the development set, shown in\nTable 1, was selected as optimal.\nThe ﬁnalized system prompt, outlined in Fig.2, encompasses four\nparts. Part 1 instructs the LLM to discuss evidence for and against the\ndisease. Part 2 prompts the system to generate a clinical narrative. Part 3\nFig. 1 | Overview of the various existing and proposed workﬂows. EHR = Elec-\ntronic Health Records, CDM = Common Data Model, PPV = Positive Predictive\nValue. a Current status quo: in EHR systems, chart review is typically only applied to\nidentiﬁed cases, allowing estimation of the PPV only.b The KEEPER system replaces\ncharts with summaries from structured data, allowing for more efﬁcient review for a\nwider variety of data.c Here we propose to use KEEPER with LLMS, further reducing\nmanual effort.d By using KEEPER with LLMS, review can be scaled up to include\nnon-cases, allowing computation of both PPV and sensitivity.\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 2\nreinforces that a diagnosis alone is insufﬁcient evidence, and Part 4 provides\nguidance on handling uncertainty. TheSupplementary Note 6 contains the\nresponse of GPT-4 to the prompt in Fig.1.\nAfter determining the optimal prompt strategy using GPT-4, we\nevaluated this strategy using Llama-2, which exhibited poor performance.\nConsequently, we opted for Sheep-Duck-Llama-2 (SDL2), as indicated in\nTable 1. For completeness, we also evaluated GPT-3.5 Turbo. Running\nGPT-4 on the development set took approximately 102 min, GPT-3.5 took\n48 min. Running Llama-2 took 50 h, and running SDL2 on the development\nset took 66 h. However, these numbers are subject to change, as LLMs are\ncontinuously improving in speed, and both Llama-2 and SDL2 can now be\nhosted in the cloud as well.\nPerformance on test sets\nFig. 3 illustrates the performance of human reviewers compared to the\noptimal prompt strategies applied to GPT-4 and SDL2. When evaluated\nagainst the gold standard for test set 1, LLMs demonstrate similar levels of\nsensitivity and speciﬁcity to those of human reviewers. In test set 2, LLMs\nshow higher sensitivity, though their speciﬁcity falls at the lower range of\nhuman performance. For test set 3, GPT-4 achieves near-perfect speciﬁcity,\nalbeit with sensitivity at the lower range seen among human reviewers, while\nSDL2 displays higher sensitivitybut substantially lower speciﬁcity. We\nobserve that LLMs generally achieve similar AUC values to human\nreviewers, although with some variability in sensitivity –speciﬁcity\ntrade-offs.\nFor test sets 1 and 2, performance did not vary much per disease (see\nSupplementary Notes 7 and 8). For testset 3 performance did vary greatly\nper disease, both for humans and LLMs, with lower sensitivity and speci-\nﬁcity for acute bronchitis and viral hepatitis A (see Supplementary Note 9).\nPerformance on the highly sensitive set\nGPT-4 required 92 h to annotate a highly sensitive set of 25,000 potential\nrheumatoid arthritis (RA) cases, requiring approximately 20 million tokens\nat the cost of about $900 at the time ofexecution (Februari 2024). From this\nset, only 360 patients (1.4%) were classiﬁed as true cases. Using these 25,000\nannotated patient records as a silver standard, we evaluated the OHDSI RA\nphenotype algorithm, revealing a PPV of 56.5% (95% conﬁdence interval:\n52.4 – 60.5%) and a sensitivity of 93.0% (95% CI: 89.9–95.5%).\nDiscussion\nIn ourﬁndings, zero-shot prompting, with slight modiﬁcations to the sys-\ntem prompt, demonstrated reasonable performance in case adjudication\nusing the KEEPER system, with comparable agreement with the gold\nstandard as human reviewers. The performance of LLMs was notably\ninﬂu e n c e db yt h ec h o i c eo fp r o m p ta n dt h es p e c iﬁc LLM selected. Disease-\ndependent variations in adjudicator performance, even among human\nreviewers, highlight the inherent uncertainty in the task of case adjudication,\npossibly reﬂecting the inherent uncertainty in disease diagnosis itself. For\nexample, sensitivity and speciﬁcity for viral hepatitis A tended to be low,\nmost likely because the data lacked results of laboratory tests, which made it\nhard to interpret orders of panels for different hepatitis.\nWithin these inherent limits, LLMs can contribute to evaluating the\nveracity of a phenotype algorithm to assess the evidence supporting a case’s\nauthenticity. Various use cases emerge depending on how one perceives\nvalue of LLM outputs— f r o mu s i n gL L M sa sac o - p i l o tf o rh u m a na s s e s s -\nment to fully automating adjudication for estimating operating character-\nistics of the phenotype algorithm in each database. The combined use of\nKEEPER with LLMs facilitates the quick adjudication of large case volumes,\nenabling the computation of both PPV and sensitivity for numerous phe-\nnotypes and data sources. In our example, we observed a PPV of 56.6% and\nsensitivity of 93.0% for a speciﬁc RA phenotype algorithm in a speciﬁc\ndatabase. Armed with these operatingcharacteristics, a researcher could\ndecide to not use this phenotype algorithm, or take them into account in\nsome form of quantitative bias analysis\n2.\nWe note that, despite having adjudicated 25,000 cases, the conﬁdence\nintervals around PPV and sensitivity in our demonstration are still wide,\narguing for even larger samples of cases to be adjudicated. This would\ncertainly be prohibitively expensive with a manual approach. For context, a\ntypical price charged for manual chart review is US$100 per case, or US$2.5\nm i l l i o nf o r2 5 , 0 0 0c a s e s .T h ec o s tf o rt h eL L Ma tt h et i m ef o rt h es a m et a s k\nwas US$900, and prices for LLM usage have signiﬁcantly dropped since\nthen. For privacy reasons, patient-level data should be kept within an\norganizationalﬁrewall. Fortunately, many LLMs such as SDL2 are freely\navailable for on-premises hosting, and many commercial models offer\nwithin-ﬁrewall cloud-based solutions.\nCombining LLMs with KEEPER enables our approach to be applied to\nany data within the CDM, including both administrative claims and EHRs.\nOur results on Test set 1 indicate that the performance of this combined\nmethod is comparable to human reviewers using KEEPER, and even to\nhuman reviewers with access to full patient charts. Although LLMs could\ntheoretically be applied to entire charts, this is currently impractical with the\nmodels we evaluated, as the charts exceed the context window limit (e.g.,\n4000 tokens for Llama-2). Newer models introduced since the completion of\nthis work boast larger context windows, typically up to 125,000 tokens, but\nthis would still be insufﬁcient to include the full charts of a patient. While\naccess to full charts might improve performance, the original KEEPER\nexperiment suggests that more information does not necessarily enhance\naccuracy.\nThe work described here representssubstantial effort in manual case\nadjudication; The development set required 358 reviews, test set 1 required 4\nreviewers x 4 diseases x 20 cases = 320 case reviews, test sets 2 required 5 × 4\nx 20 = 400 reviews, and test set 3 required 5 × 6 x 25 = 750 reviews. The total\nnumber of manual reviews is therefore 1828. Despite all this work, we were\nonly able to evaluate our approach on 10 different diseases, with only a\nhandful of cases per disease, limiting our ability to generalize from our\nﬁndings.\nWhile our current approach uses zero-shot and one attempt at few-\nshot prompting, possibly better performance could be achieved using\nautomated prompt optimization\n9 or ﬁne-tuning of the LLMs for the task at\nhand. However, this would require a large training set that would itself be\ninfeasible due to cost and time constraints.\nWhile the adoption of LLMs in clinical care remains debated, our\napplication in enhancing evidence reliability from observational data\nappears promising and low risk.\nMethods\nThe KEEPER system\nKEEPER is fully described previously3. In brief, the design of KEEPER is\nguided by three fundamental principles: adherence to clinical reasoning,\nstandardization, and dimensionality reduction.\nTable 1 | Performance of various prompting strategies and\nLLMs on the development set\nPrompt (using GPT 4) Sensitivity Speci ﬁcity AUC\nYes/no 98.5% 3.2% 0.51\n+ discuss evidence 98.0% 7.3% 0.53\n+ write narrative 97.5% 19.4% 0.58\n+ diagnosis insufﬁcient reminder 81.9% 83.1% 0.82\n+ uncertainty instructions 86.8% 83.9% 0.85\n+ provide examples 82.8% 73.4% 0.78\nLLM (using all but last prompt)\nGPT 4 86.8% 83.9% 0.85\nGPT 3.5 Turbo 82.8% 58.9% 0.71\nLlama-2-70b-chat-hf 99.0% 12.9% 0.56\nSheep-Duck-Llama-2-70b-v1.1 90.2% 62.1% 0.76\nAUC Area under the receiver operator curve.\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 3\nFig. 3 | Sensitivity and speciﬁcity of reviewers for\nthe three test sets.Points indicate sensitivity and\nspeciﬁcity of each human or LLM reviewer against\nthe gold standard. Error bars indicate 95% con-\nﬁdence intervals. For test set 1, the gold standard was\ncreated by external reviewers. For test set 2 and 3, the\ngold standard was the majority vote of human\nreviewers using a leave-one-out approach. Slanted\nlines denote iso-AUC contours, spaced 0.1 apart.\nFig. 2 | Final system prompt and example main\nprompt. The ﬁnalized system prompt with an\nexample main prompt. Part 1 of the system prompt\ninstructs the LLM to discuss evidence for and against\nthe disease. Part 2 prompts the system to generate a\nclinical narrative. Part 3 reinforces that a diagnosis\nalone is insufﬁcient evidence, and Part 4 provides\nguidance on handling uncertainty. To safeguard\npatient privacy, thisﬁgure shows perturbed patient\ndata. The LLMs were provided with the unperturbed\nreal data.\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 4\n1. Adherence to Clinical Reasoning:\nKEEPER is constructed to emulate the diagnostic clinical reasoning\nprocess when applied to patient data by organizing structured data\nbased on clinical presentation, disease history, preliminary diagnosis,\ndiagnostic procedures, differential diagnoses, treatment, follow-up\ncare, and complications.\n2. Standardization:\nTo ensure scalability and applicability across diverse data sources,\ndata extraction is based on the OMOP CDM\n10. KEEPER’so u t p u t sa r e\nstandardized across diseases.\n3. Dimensionality Reduction:\nKEEPER focuses on efﬁciency by extracting only clinically relevant\ninformation for a speciﬁc phenotype, reducing data volume and\nexpediting review.\nIn practice, KEEPER uses structured data in a common format to\nextract pre-speciﬁed elements relevant to the disease during standard time-\nwindows aligned with typical clinical reasoning steps. Conﬁguring KEEPER\nfor a speciﬁc outcome requires specifying the elements– captured as sets of\nstandard concepts in the OHDSIStandardized Vocabularies7 -- to extract\nper KEEPER category. For example, for acute appendicitis the‘Symptoms’\ncategory would specify concepts such as‘nausea’, ‘vomiting’,a n d‘epigastric\npain’, and KEEPER would report if these concepts occurred in the 30 days\nprior to the case index date. The Supplementary Table 1 contains a full list of\nall KEEPER categories and corresponding time windows. KEEPER is\napplied to a cohort of patients, for example those identiﬁed by a phenotype\nalgorithm.\nL a r g el a n g u a g em o d e l s\nDue to privacy concerns surrounding patient data, we cannot transmit\nproﬁles outside the institution’s ﬁrewall. To address this limitation, we\nexamined four locally hosted LLMs. Theﬁrst two, GPT-3.5 Turbo and GPT-\n411, were accessed through a Microsoft Azure service. The third, Llama-2\nfrom Meta12, was obtained via Hugging Face and run on an Amazon EC2\nmachine with 4 NVIDIA A10G Tensor Core GPUs. However, the initial\nversion of Llama-2 exhibited poor performance. Consequently, we opted for\nSheep-Duck-Llama-2, a modiﬁed version of Llama-2ﬁne-tuned on Orca-\nstyle and Alpaca-style datasets, that scored highest on the Huggingface LLM\nLeaderbord at the time\n13–15. For all LLMs we used a temperature of 0.\nPrompt engineering\nWe mainly focus on zero-shot prompting16, meaning we do not include any\nexamples when prompting the LLMs. The output from KEEPER was turned\ninto text by grouping categories together. For instance, disease history,\nsymptoms, comorbidities, and riskfactors were combined into a single\ncategory ‘Diagnoses recorded prior to the visit’. See the Supplementary\nTable 2 for the groupings of categories.\nThe prompt featuring the KEEPER output was accompanied by a\nsystem prompt guiding the LLM on how to process the information. We\nemployed an ad-hoc approach to prompt engineering. Initially, we used a\nstraightforward system prompt instructing the LLM to provide a binary‘yes’\nor ‘no’answer regarding whether the patient had the speciﬁed disease. We\niteratively expanded the system prompt based on the LLM’s performance\nand errors observed during runs on the development set. All iterations and\nperformance metrics are reported in the Results section, and were per-\nformed with GPT-4 alone, as it proved to be the most efﬁcient system. The\ncode for generating the various prompts is available in the KEEPER R\npackage. (https://github.com/OHDSI/Keeper/tree/v0.2.0). The responses of\nthe LLMs are parsed using a simple approach, removing all but the‘Sum-\nmary’ section and looking for keywords such as‘yes’ and ‘no’.( S e eS u p -\nplementary Note 3) For computation ofthe evaluation metrics, a failure to\nchoose between‘yes’and ‘no’was interpreted as‘no’.\nOur evaluation metrics included sensitivity, speciﬁcity, and AUC with\nthe gold standard (human annotation). Because both human and LLM\nreviewer produce binary outputs, we can compute AUC as the mean of the\nsensitivity and the speciﬁcity.\nData sources\nWe used two data sources: Columbia University Irving Medical (CUIMC)\nEHRs and the Optum’sd e - i d e n t iﬁed Clinformatics® Data Mart Database\n(Clinformatics®).\nCUIMC EHRs translated to the OMOP CDM, comprising electronic\nhealth records and data from administrative and ancillary systems for over\nsix million patients. The database encompasses person details, visit infor-\nmation (inpatient and outpatient), conditions (billing diagnoses and pro-\nblem lists), drugs (outpatient pre scriptions and inpatient orders/\nadministrations), devices, measurements (laboratory tests and vital signs),\nand other observations (symptoms). IRB for original KEEPER: Columbia\nUniversity Medical center institutional review board (IRB-AAAS6414).\nClinformatics®, also translated to the OMOP CDM, contains admin-\nistrative health claims for members in large commercial and Medicare\nAdvantage health plans. It encompasses over 65 million unique patients,\nproviding patient-level data from claims related to enrollment, person\ndetails, drug dispensing, procedures, diagnoses, and admission and dis-\ncharge dates. Approximately 30% of the laboratory tests are recorded with\nthe results. The population is geographically diverse, representing 50 of the\nUnited States of America. The use of Clinformatics® was reviewed by the\nNew England Institutional Review Board (IRB) and was determined to be\nexempt from broad IRB approval, as this research project did not involve\nhuman subject research.\nConstruction of development and test sets\nA total of 5 sets with corresponding gold or silver standard case labels were\ncreated, as detailed in Fig.4. See Supplementary Table 3 for case deﬁnitions,\nSupplementary Note 1 for phenotype algorithms, and Supplementary Note\n2 for KEEPER concept sets. All human reviewers were board-certiﬁed\nclinicians.\nTo develop our prompting strategy, weﬁrst created a development set\nencompassing six diseases: acute bronchitis, hyperlipidemia, hypopar-\nathyroidism, osteoporosis, RA, and viral hepatitis type A. These six diseases\nwere chosen to represent a diverse spectrum of therapeutic areas. Case\ndeﬁnitions and phenotype algorithms, detailed in Supplementary Table 3\nand Supplementary Note 1, respectively, were employed to identify cases\nwithin the Clinformatics® database. A non-random sample of 358 patients\nwas selected by manual review, emphasizing challenging cases (cases that\nneither had all markers of a case nor no markers). The gold standard was\ncreated by a single reviewer using the KEEPER output as well as any other\ndata available in the CDM.\nFollowing the selection of ourﬁnal prompting strategy, we applied it to\nthree distinct test sets:\n1. Test Set 1, taken from the original KEEPER paper\n3:\n Comprising four diseases: acute appendicitis, diabetes mellitus type I,\nchronic obstructive pulmonary disorder (COPD), and end-stage renal\ndisease.\n Balanced sample (half— likely cases and half— likely non-cases) of 20\ncases per disease from the CUIMC database. Likely cases were deﬁned\nas those having more restrictive inclusion criteria such as a diagnosis of\nappendicitis followed by antibiotic therapy or appendectomy and\nlikely-non cases were deﬁned through less restrictive phenotypes such\nas an occurrence of an appendicitis code.\n Annotated by two reviewers, each utilizing both KEEPER and full\ncharts separately, resulting in 4 times 80 case adjudications.\n Gold standard was two independent reviewers performing chart review\nusing all available structured and unstructured data. Results were\ndiscussed and iterative chart review continued until all disagreements\nwere resolved.\n2. Test Set 2:\n Same diseases as Test Set 1.\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 5\n Balanced sample (half— likely cases and half— likely non-cases) of 20\ncases per disease in the Clinformatics® database.\n Independently adjudicated byﬁve reviewers using KEEPER.\n Gold standard was the majority vote of human reviewers, leaving a\nreviewer out of the vote whenevaluating that reviewer.\n3. Test Set 3:\n Comprising the six diseases from the development set.\n Random sample of 25 cases per disease from the Clinformatics®\ndatabase.\n Independently adjudicated byﬁve reviewers using KEEPER.\n Gold standard was the majority vote of human reviewers, leaving a\nreviewer out of the vote whenevaluating that reviewer.\nAdditionally, we create aﬁnal set without human review to demonstrate\nthe feasibility of adjudicating a high-sensitivity cohort. A high-sensitivity\ncohort is designed to have near-perfect sensitivity, likely at the cost of very low\nspeciﬁcity, and has been suggested as a way to make adjudication of both cases\nand non-cases feasible when the outcome is rare2. Speciﬁcally, we construct a\nhigh-sensitive cohort for RA, including all patients with any relevant diag-\nnose, symptom, treatment, complication, or laboratory test, and take a ran-\ndom sample of 25,000 patients. GPT-4, using KEEPER annotates the 25,000\nas cases and non-cases, allowing the set to be used to compute PPV and\nsensitivity for any phenotype algorithm. We demonstrate its use on an\nestablished phenotype algorithm for RA from the OHDSI Phenotype Library.\nData availability\nThe patient-level data used in this study cannot be shared, for privacy and\nlicensing reasons. Others could also license the Clinformatics® data\nfrom Optum®.\nCode availability\nThe code used in this study is made available in Supplementary Note 3 and\no n l i n ei nt h eK E E P E RRp a c k a g e :https://github.com/OHDSI/Keeper/tree/\nv0.2.0.\nReceived: 4 March 2024; Accepted: 1 January 2025;\nReferences\n1. FDA. Real-World Data: Assessing Electronic Health Records and\nMedical Claims Data To Support Regulatory Decision-Making for\nDrug and Biological Products: Guidance for Industry.https://www.\nfda.gov/regulatory-information/search-fda-guidance-documents/\nreal-world-data-assessing-electronic-health-records-and-medical-\nclaims-data-support-regulatory (2021).\n2. Lanes, S., Brown, J. S., Haynes, K., Pollack, M. F. & Walker, A. M.\nIdentifying health outcomes in healthcare databases.Pharmacoepi\ndemiol. Drug Saf.24, 1009–1016 (2015).\n3. Ostropolets, A. et al. Scalable and interpretable alternative to chart review\nfor phenotype evaluation using standardized structured data from\nelectronic health records. J. Am. Med. Inform. Assoc. 31, 119–129.\n4. Gilson, A. et al. How does ChatGPT perform on the United States\nMedical Licensing Examination? The implications of large language\nmodels for medical education and knowledge assessment.JMIR\nMed. Educ.9, e45312 (2023).\n5. Liévin, V., Hother, C. E., Motzfeldt, A. G., & Winther, O. Can large\nlanguage models reason about medical questions?Patterns. 5,\nhttps://www.cell.com/patterns/fulltext/S2666-3899(24)00042-4\n(2024).\n6. Eriksen, A., Möller, S. & Ryg, J. Use of GPT-4 to Diagnose Complex\nClinical Cases.NEJM AI(2023).\n7. Reich, C. et al. OHDSI Standardized Vocabularies-a large-scale\ncentralized reference ontology for international data harmonization.J.\nAm. Med. Inform. Assoc.31, 583–590 (2024).\n8. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models.Adv. Neural Inf. Process. Syst.35, 24824–24837\n(2022).\n9. Zhou, Y. et al. Large Language Models Are Human-Level Prompt\nEngineers. Preprint athttps://doi.org/10.48550/arXiv.2211.01910\n(2023).\n10. Hripcsak, G. et al. Observational Health Data Sciences and\nInformatics (OHDSI): opportunities for observational researchers.\nStud. Health Technol. Inform.216, 574–578 (2015).\n11. Brown, T. B. et al. Language models are few-shot learners.Proc. of the\n34th Int. Conf. on Neural Information Processing Systems.\nFig. 4 | Overview of development and test sets used in this study.A development\nset was created to guide prompt engineering. Test set 1 was also used in our prior\nwork on KEEPER, thus providing a benchmark for consistency. Test set 2 mimics 1\nbut uses insurance claims data. Test set 3 takes a truly random sample across more\ndiseases to enhance generalizability. The highly sensitive set demonstrates the use of\nLLMs to annotate a large set of patients, allowing computation of sensitivity and PPV\nof phenotype algorithms.\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 6\n1877–1901. https://dl.acm.org/doi/abs/10.5555/3495724.3495883\n(2020).\n12. Touvron, H. et al. Llama 2: openfoundation andﬁne-tuned chat models.\nPreprint athttps://doi.org/10.48550/arXiv.2307.09288(2023).\n13. Mukherjee, S. et al. Orca: progressive learning from complex\nexplanation traces of GPT-4. Preprint athttps://doi.org/10.48550/\narXiv.2306.02707 (2023).\n14. Lee, A. N., Hunter, C. J. & Ruiz, N. Platypus: quick, cheap, and\npowerful reﬁnement of LLMs. Preprint athttps://arxiv.org/abs/2308.\n07317 (2023).\n15. Open LLM Leaderboard— a Hugging Face Space by HuggingFaceH4.\nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.\n16. Wang, W., Zheng, V. W., Yu, H. & Miao, C. A survey of zero-shot\nlearning: settings, methods, and applications.ACM Trans. Intell. Syst.\nTechnol. 10,1 –37 (2019).\nAcknowledgements\nGH’s received a NLM grant the US National Institutes of Health grant R01\nLM006910.\nAuthor contributions\nM.J.S., A.O., and P.B.R. conceived of the study. M.J.S. performed the\nstatistical analyses and drafted the manuscript. A.O. developed the KEEPER\npackage. M.J.S. and M.A.S. implemented the LLM experiments. A.O., A.Z.,\nU.K., S.I.S., G.H., P.B.R. performed manual adjudication of cases. All\nauthors reviewed and edited the manuscript, and have read and approved\nthe ﬁnal version.\nCompeting interests\nThe authors declare no competing interests. MAS receives contracts and\ngrants from the US National Institutes of Health, US Food & Drug\nAdministration, the US Department of Veterans Affairs and Janssen\nResearch & Development, all outside the scope of this work.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01433-4\n.\nCorrespondenceand requests for materials should be addressed to\nMartijn J. Schuemie.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01433-4 Article\nnpj Digital Medicine|            (2025) 8:18 7",
  "topic": "Adjudication",
  "concepts": [
    {
      "name": "Adjudication",
      "score": 0.8954567909240723
    },
    {
      "name": "Observational study",
      "score": 0.773833692073822
    },
    {
      "name": "Computer science",
      "score": 0.5026919841766357
    },
    {
      "name": "Electronic health record",
      "score": 0.4675793945789337
    },
    {
      "name": "Health records",
      "score": 0.4502682685852051
    },
    {
      "name": "Chart",
      "score": 0.4200159013271332
    },
    {
      "name": "Medicine",
      "score": 0.38155752420425415
    },
    {
      "name": "Political science",
      "score": 0.14374610781669617
    },
    {
      "name": "Pathology",
      "score": 0.1389097273349762
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Health care",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2799503643",
      "name": "Columbia University Irving Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1330063522",
      "name": "Johnson & Johnson (United States)",
      "country": "US"
    }
  ]
}