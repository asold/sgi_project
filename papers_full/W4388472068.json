{
  "title": "GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)",
  "url": "https://openalex.org/W4388472068",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5020026033",
      "name": "Aivin V. Solatorio",
      "affiliations": [
        "World Bank"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3160590016",
    "https://openalex.org/W2128160875",
    "https://openalex.org/W4309651343",
    "https://openalex.org/W2078993594",
    "https://openalex.org/W4394932368",
    "https://openalex.org/W4319452093",
    "https://openalex.org/W4383860414",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4308002090"
  ],
  "abstract": "Predicting human mobility holds significant practical value, with\\napplications ranging from enhancing disaster risk planning to simulating\\nepidemic spread. In this paper, we present the GeoFormer, a decoder-only\\ntransformer model adapted from the GPT architecture to forecast human mobility.\\nOur proposed model is rigorously tested in the context of the HuMob Challenge\\n2023 -- a competition designed to evaluate the performance of prediction models\\non standardized datasets to predict human mobility. The challenge leverages two\\ndatasets encompassing urban-scale data of 25,000 and 100,000 individuals over a\\nlongitudinal period of 75 days. GeoFormer stands out as a top performer in the\\ncompetition, securing a place in the top-3 ranking. Its success is underscored\\nby performing well on both performance metrics chosen for the competition --\\nthe GEO-BLEU and the Dynamic Time Warping (DTW) measures. The performance of\\nthe GeoFormer on the HuMob Challenge 2023 underscores its potential to make\\nsubstantial contributions to the field of human mobility prediction, with\\nfar-reaching implications for disaster preparedness, epidemic control, and\\nbeyond.\\n",
  "full_text": "GeoFormer: Predicting Human Mobility using Generative\nPre-trained Transformer (GPT)\nAivin V. Solatorio‚àó\nasolatorio@worldbank.org\nThe World Bank\nWashington, District of Columbia, USA\nABSTRACT\nPredicting human mobility holds significant practical value, with\napplications ranging from enhancing disaster risk planning to sim-\nulating epidemic spread. In this paper, we present the GeoFormer,\na decoder-only transformer model adapted from the GPT architec-\nture to forecast human mobility. Our proposed model is rigorously\ntested in the context of the HuMob Challenge 2023‚Äîa competi-\ntion designed to evaluate the performance of prediction models\non standardized datasets to predict human mobility. The challenge\nleverages two datasets encompassing urban-scale data of 25,000\nand 100,000 individuals over a longitudinal period of 75 days. Geo-\nFormer stands out as a top performer in the competition, securing a\nplace in the top-3 ranking. Its success is underscored by performing\nwell on both performance metrics chosen for the competition‚Äîthe\nGEO-BLEU and the Dynamic Time Warping (DTW) measures. The\nperformance of the GeoFormer on the HuMob Challenge 2023 un-\nderscores its potential to make substantial contributions to the field\nof human mobility prediction, with far-reaching implications for\ndisaster preparedness, epidemic control, and beyond.\nCCS CONCEPTS\n‚Ä¢ Computing methodologies ‚ÜíNatural language processing ;\nCross-validation; Supervised learning by classification; Model de-\nvelopment and analysis .\nKEYWORDS\nHuman Mobility, GEO-BLEU, Dynamic Time Warping (DTW), Deep\nLearning, Machine Learning, AI, Transformers, GPT, GeoFormer\nACM Reference Format:\nAivin V. Solatorio. 2023. GeoFormer: Predicting Human Mobility using\nGenerative Pre-trained Transformer (GPT). In 1st International Workshop on\nthe Human Mobility Prediction Challenge (HuMob-Challenge ‚Äô23), November\n13, 2023, Hamburg, Germany. ACM, New York, NY, USA, 5 pages. https:\n//doi.org/10.1145/3615894.3628499\n‚àóThe findings, interpretations, and conclusions expressed in this paper are entirely\nthose of the authors. They do not necessarily represent the views of the International\nBank for Reconstruction and Development/World Bank and its affiliated organizations,\nor those of the Executive Directors of the World Bank or the governments they repre-\nsent. GitHub/HuggingFace: @avsolatorio. https://github.com/avsolatorio/geoformer\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nHuMob-Challenge ‚Äô23, November 13, 2023, Hamburg, Germany\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0356-0/23/11. . . $15.00\nhttps://doi.org/10.1145/3615894.3628499\n1 INTRODUCTION\nDigitalization has unlocked a tremendous amount of data useful\nfor assessing human mobility captured through call detail records\n(CDRs) or GPS-enabled devices and smartphones. Human mobil-\nity has become increasingly impactful in today‚Äôs world either as a\ndirect or proxy indicator for various socioeconomic activities, in-\ncluding as input to other applications ranging from disease spread\nmodeling during the COVID-19 pandemic to influencing transport\nand urban planning decisions. Accurate forecasts of human mo-\nbility patterns can empower policymakers, urban planners, and\nhealthcare professionals with valuable insights to better prepare\nfor various scenarios.\nThe HuMob Challenge 2023 aims to find mobility prediction\nmodels that can predict individual human trajectories contained in\ntwo standardized benchmarking datasets [1]. The datasets encom-\npass two distinct types of human mobility: normal period mobility\nand emergency period mobility, each holding unique challenges\nand implications. In response to the challenge, we introduce a novel\napproach to predicting human mobility that leverages a decoder-\nonly transformer model, specifically the generative pre-trained\ntransformer (GPT) architecture [3].\nThe recent popularity of artificial intelligence (AI) applications\nhas largely been driven by the phenomenal and almost human-\nlike capability of generative deep learning models. The release of\nChatGPT, a conversational application of generative AI by OpenAI,\nis arguably the inflection point for the mainstream adoption of\nAI. ChatGPT, at its core, is powered by transformers [ 9]‚Äîa rev-\nolutionary and now a ubiquitous component for a large number\nof state-of-the-art models across use cases [ 8]. With the goal of\nintroducing generative AI in human mobility prediction, this work\nmakes three contributions to the field. Firstly, we present a pioneer-\ning approach that offers an innovative perspective on modeling\nand predicting human mobility using generative deep learning\nmodels. Secondly, we further demonstrate the versatility and cross-\nfunctional applicability of generative models built on transformer\narchitectures, establishing their potential as a fundamental tool in\nthe domain of mobility prediction. Lastly, we layout the process\nof integrating insights from mobility data into transforming the\nhuman mobility problem into a problem that is analogous to those\nin natural language processing (NLP).\nOur analysis and the results of the competition demonstrate that\nthe proposed GPT-based model exhibits promising performance in\npredicting human mobility, reinforcing its potential for applications\nacross various domains. Furthermore, our investigation into the\nsensitivity of evaluation metrics towards generative parameters\nof the models adds a nuanced perspective to the assessment of\nhuman mobility prediction models. In essence, this paper serves as\narXiv:2311.05092v1  [cs.LG]  9 Nov 2023\nHuMob-Challenge ‚Äô23, November 13, 2023, Hamburg, Germany Solatorio, Aivin V.\nFigure 1: Panel (a) shows the snapshot of the raw data structure including the day-of-week (dow) derived variable. Panels\n(b) and (c) show the seasonality of the average daily movement count, marked by the red vertical lines, and the subsequent\nreduction in mobility evident in the emergency period shown in panel (c). Panel (d) shows the rate of out-of-data coordinate\nvalues during the prediction period.\na stepping stone toward more accurate and robust human mobility\nprediction methods, with far-reaching implications for healthcare,\nurban planning, and beyond.\n2 DATA AND METRICS\n2.1 Data\nThe data used in this paper comes from an anonymized set of data\non human mobility in an undisclosed metropolitan area in Japan.\nSpatiotemporal anonymization was conducted to ensure the pri-\nvacy of individuals in the data. Two sets of data were released\ncorresponding to the two tasks in the challenge. A more compre-\nhensive description of the data is detailed in [11].\n2.1.1 Task 1 data. The task 1 of the challenge requires a model\nthat can predict human mobility in a \"business-as-usual\" period.\nMobility trajectories from 100,000 individuals comprise the task 1\ndataset. This dataset is collected over a 75-day period. The dataset\ncontains full mobility trajectories for 80,000 individuals, while the\nremaining individuals only include full mobility information for\n60 days. The remaining 15 days are undisclosed, representing the\nsubset to be predicted by the model.\n2.1.2 Task 2 data. The task 2 of the challenge demands a model\nthat can predict human mobility in an \"emergency\" period that\ncovers unusual human behaviour. Mobility trajectories from 25,000\nindividuals comprise the task 2 dataset. Similar to the task 1, the\ndataset is collected over a 75-day period. The dataset contains full\nmobility trajectories for 22,500 individuals, while the remaining\nindividuals only include full mobility information for 60 days. Again,\nthe remaining 15 days are undisclosed, representing the subset to\nbe predicted by the model.\n2.1.3 Validation and Test split.We sampled 2000 users from each\ndataset to serve as the validation and test set, each having 1000\nusers. Individuals that were included in either the validation or test\nset will only have trajectories until the 60 ùë°‚Ñé day in the training\ndata. The validation set is used to find the optimal checkpoint of\nthe trained generative model. The test set was used primarily for\nassessing the model‚Äôs predictive performance and selecting the\noptimal generative parameters.\n2.2 Metrics\nThe performance of the models in the challenge is evaluated using\ntwo metrics: the Dynamic Time Warping (DTW) and GEO-BLEU\nscore. The DTW is a distance measure commonly used to measure\nthe difference between time series data that may have different\nsequence lengths due to varying rates of observations [5, 10]. The\nGEO-BLEU metric is inspired by the BLEU metric commonly used\nin the natural language processing literature [ 6]. A lower DTW\nscore indicates better performance, whereas a higher GEO-BLEU\nscore corresponds to a more performant model.\n3 MODEL AND METHODS\nIn this section, we provide details on the analysis performed, the\nmodel proposed, and the strategies we implemented to transform\nthe data, train the model, and generate predictions from the model.\nGeoFormer: Predicting Human Mobility using GPT HuMob-Challenge ‚Äô23, November 13, 2023, Hamburg, Germany\n3.1 Analysis\nIt is imperative to understand insights from the data to guide and\njustify modeling decisions. We performed simple exploratory anal-\nysis of the available data to gain some intuition that could be useful\nfor implementing the model. We focused on exploring temporal\ninsights from the data. The analysis of the global properties of the\ndata reveals expected seasonality in mobility. While the actual times\nhave been obfuscated, we can infer the occurrence of nighttime\nand daytime by plotting the distribution of observations by event\nstamp (Figure 1, panels b and c). The evidence of periodicity in the\ndata was a significant insight that drove one of the key modeling\ndecisions, particularly simplifying the model by only learning one-\nweek periods of the data. The drastic shift in mobility intensity\nduring the emergency period seen in the latter part of panel (c)\nprovided an insight into the decreased mobility of individuals. This\nchange in behavior hinted at the likely impropriety of training the\ntask 2 model with the full timeseries in the dataset, i.e., including\nthe normal period in the training despite needing to predict only\nfor the emergency period.\nOne additional insight we explored was the distribution of loca-\ntion visited by individuals after the 60-day period‚Äîthe prediction\nperiod. This insight is essential because it can help guide whether\nwe can fully rely on past trajectory for the inference or not. We\nfound that in the prediction period, some of the coordinates, (x, y)\ntuples, have never appeared in the training period. Panel (d) in Fig.\n1 shows the distribution of the rate of the out-of-training positions\nx, y, and their combination (x, y). While treating the x and y values\nof the location independently, we find that about 20% of the x- or\ny-coordinates have not been seen in the past. Considering exact\nlocation coordinate (x, y), we see that about 40% of locations visited\nin the prediction period by individuals have not been visited in the\npast. This is a crucial insight since it provides a significant bound\nto a model that relies solely on past visited locations.\n3.2 The GeoFormer Model\nOur proposed solution aims to reformulate human mobility as an\nabstracted sequence. This reformulation and abstraction allow us\nto apply models that can learn and generate sequential data. In par-\nticular, we establish an analogy between human mobility sequence\nand sequence of words in a sentence. This abstraction allows us to\nexploit all the deep learning machinery used to model and generate\nsentences, e.g., autoregressive decoder-only transformer models.\nIn the following, we provide the main details of the model we\nused to predict the mobility of users in both tasks. We first define\nthe base model‚Äîwe call the GeoFormer‚Äîand then discuss how we\nrepresented the data to fit the form required by the model.\n3.2.1 Generative Pre-trained Transformer (GPT). The GPT model is\na transformer-based deep learning architecture for autoregressive\nmodeling. The GPT uses a decoder transformer that takes in a\nsequence of tokens. The autoregressive learning is made possible\nby introducing masking with a training task designed to predict\nthe next token in the sequence. This allows a trained GPT model to\ngenerate sequences. Theoretically, the GPT architecture models the\nconditional probability of generating a token ùë• at position ùëó given\nthe past sequence [ùë•0, ùë•1, ùë•2, . . ., ùë•ùëó‚àí1]. Evidence for the impeccable\nability of GPT architecture in modeling sequential data abounds\n[2, 4, 7, 8]. This makes the GPT formulation fully compatible with\nthe problem at hand.\nSpecifically, we define the GeoFormer as a GPT model that learns\nthe conditional probability distribution defined below,\nùë•ùëñùëó ‚àºùëÉ (ùëã |ùë•ùëñ1, ùë•ùëñ2, ..., ùë•ùëñùëó‚àí1) (1)\nwhere ùëñ corresponds to the ùëñùë°‚Ñé individual, ùëó corresponding to\nthe ùëóùë°‚Ñé time period, and ùë•ùëñùëó corresponding to the coordinate of the\nuser ùëñ at time ùëó.\n3.2.2 Input linearization. Reformulating the mobility problem as\nan NLP problem of sentence generation requires transforming the\nmobility data to fit into the chosen framework. We call this process\ninput linearization . We represent the location data as tokens in a\nsequence. We use the fact that one whole day of data is discretized\ninto 30-minute intervals. This means that a full day will have at\nmost 48 timesteps. We represent the daily trajectory for an indi-\nvidual using the full 48 timesteps despite the provided data only\ncontaining timesteps with observed locations, Fig. 1 - panel (a).\nSo, we explicitly assign a special \"empty\" (N) token to timesteps\nwithout observations.\nRepresenting the coordinates strictly as the tuple (x, y) is the\nprecise approach in encoding the location information of individu-\nals. However, this representation is suboptimal due to the existence\nof 500 distinct cells for both x and y coordinate values. Fully defin-\ning the geographic representation would require 250,000 unique\nlocation tokens. To mitigate this \"explosion\" of the token space,\nwe independently represent the location of an individual. This\nmeans that we have 500 tokens for the x coordinates and 500 to-\nkens for y coordinates. We distinguish these tokens as x<pos_id>\nand y<pos_id>, for x and y tokens, respectively shown in Annex\nA.1. That is, a coordinate is composed of two subsequent tokens: an\nx token followed by a y token. This choice is also influenced by the\ninsight we earlier uncovered regarding the significant rates of out-\nof-training values for location (x, y) during the prediction period,\nas shown in Figure 1 - panel (d). That is, using (x, y) jointly will\nresult in lower generalizability. This representation is a reasonable\ntrade-off since the GPT model can learn the conditional probability\nof tokens as well.\nThe choice of the representation and the linearization of the data\nwere largely influenced by the insights obtained from the analysis\nabove. The seasonality observed in the data was considered in de-\nsigning the model input. In particular, we represent the training\ndata as a sequence of an 8-day mobility signature. Since the model\nis autoregressive, this 8-day mobility signature will allow us to gen-\nerate the 8ùë°‚Ñé day trajectory when we have input from the previous\n7 days. This choice was made because of the clear seasonality in\nthe one-week (7-day) period as shown in Figure 1, panels b and c.\nHowever, one important limitation of the approach is worth noting\nwhich is the assumption that a one-week segment of mobility data\nsufficiently models the subsequent day.\nWhile there is no explicit long-term memory for a trajectory of\nthe individual beyond one week, the linearized input is designed\nto condition the model at the individual level. In particular, we\nprefix the individual‚Äôs mobility data with a representation for the\nindividual in the form of user id tokens. The learning algorithm\nHuMob-Challenge ‚Äô23, November 13, 2023, Hamburg, Germany Solatorio, Aivin V.\nis assumed to encode through the user id tokens the general long-\nterm characteristics of the mobility specific to an individual. This\nis useful for predicting and generating the mobility trajectory of\nthe individuals beyond the training data. An example of the fully\nlinearized input is shown in Annex A.2. The full representation\noutlined complies with the input for the autoregressive framework,\nallowing us to generate subsequent mobility information of the\nindividuals.\n3.3 Model Configuration and Training\nThe model consists of 12 transformer layers having 24 attention\nheads, and 768 embedding dimensions with a 10% dropout rate.\nThe optimizer used is AdamW with beta values of (0.9, 0.999) and\nepsilon equal to 1‚àí5. The learning rate scheduler follows a cosine\ncurve with a maximum of 5‚àí4 and a linear warm-up for 20,000\nsteps. A maximum gradient normalization of 5 was set.\nThe model for task 1 was trained for 5 epochs using all available\ndata. For task 2, we fine-tuned the checkpoint corresponding to\nthe task 1 model with the best validation metrics. While the nature\nof the two tasks is different, i.e., task 2 represents an emergency\nperiod, we found that performing fine-tuning of the task 1 trained\nmodel works reasonably well for task 2 as well. However, we limited\nthe dataset to fine-tune the model for task 2 only on data from the\n60ùë°‚Ñé day until the 75ùë°‚Ñé since the data distribution is different prior\nto the prediction period as depicted in Fig. 1 - panel (c).\n3.4 Generating predictions\nPrediction in the context of the GeoFormer is similar to the genera-\ntive process performed in standard text applications of GPT. The\nprocess is autoregressive, meaning that each token in the sequence\nis generated one token at a time, and previously generated tokens\nare used to generate the next. A conditional generation is possible\nwith the appropriate input data design.\nThe inference signature. To help the model generate the predic-\ntion, we exploit the provided signature in the data to be predicted.\nThe data already specifies the time periods for which coordinates\nare to be predicted. So, we generate an expected input pattern from\nthe data and only require the model to fill the values for the needed\ntimes. The signature shown in Annex A.3 indicates the values to\nbe filled by the model as x,y while skipping predictions for times\nrepresented by N.\nLimiting the candidate tokens. Despite the insight that about 20%\nof the x and y tokens in the prediction period have not appeared\nin the training period, we chose to limit the candidate tokens in\nthe generation to those that have been already part of the past\ntrajectories of the individual. We constrained the tokens specific to\nthe day-of-week and the specific timestamp, with a window of 2\ntimestamps before and after. The window is used to account for the\nstochasticity in data collection, which could associate a location\nacross neighboring timestamps due to issues in connectivity and\nother factors. This means that if we want a prediction for 6 a.m. on\na Saturday, we only consider all the x and y locations previously\nvisited by the individual at 5:00 a.m., 5:30 a.m., 6:00 a.m., 6:30 a.m.,\nand 7:00 a.m. on previous Saturdays. Constraining the candidate\nTable 1: Metrics values for the validation, test, and final data\nacross tasks\nMetrics Validation Test Final\nTask 1 GEO-BLEU 0.3047 0.3114 0.3160\nDTW 29.6978 29.8037 26.2161\nTask 2 GEO-BLEU 0.2004 0.2053 0.1828\nDTW 38.0069 43.4332 37.7815\ntokens mitigates hallucinations by the model in generating locations\nthat are too far from the individual‚Äôs likely trajectory.\nGeneration parameters. We experimented with some parameters\nof the generative algorithm in generating the trajectories. The\ntemperature and the top-k parameters were the most useful based\non our assessment. The top-p parameter was also varied, but it\nappears to produce similar effect with the top-k parameter.\n4 RESULTS AND DISCUSSIONS\nWe tracked the metrics on the test data to find the optimal set of\nparameters for the generation of the predictions. Our experiments\nsuggest an inverse relationship between the GEO-BLEU and the\nDTW metrics as we vary the temperature parameter. As the tem-\nperature parameter approaches 1, the probability distribution of the\ntokens becomes more unbiased. In this regime, the GEO-BLEU score\ntends to improve. Decreasing the temperature parameter results\nin better DTW score, but negatively affects the GEO-BLEU score.\nTherefore, optimizing for both metrics requires careful tuning of\nthe temperature parameter.\nAnother parameter we tuned was the top-k parameter. This\nparameter limits the tokens to be considered for generation only to\ntokens with the k highest probability. We varied this parameter and\nfound that k = 5 produces generally better predictions measured\nby both metrics.\nPart of the competition was an intermediate assessment of pre-\ndictions. In this period, we submitted a version of the GeoFormer\npredictions for task 1 using only 6 transformer layers. The Geo-\nFormer scored 0.3037 on the GEO-BLEU and scored 29.07 for the\nDTW on the final test data.\nSummary of the validation, test, and final scores for the models\nwe selected for submission is reported in Table 1. The scores show\na considerable variation in the validation and test DTW scores\ncompared with the scores measured on the final assessment groups,\nversus the GEO-BLEU. The model appears to be more stable when\napplied to the task 1 compared with the task 2 dataset. This may be\ndue to the larger task 1 dataset.\n5 CONCLUSION\nIn this paper, we detailed a generative deep learning model for\npredicting human mobility data. The GeoFormer model achieved\na top score in the challenge having optimal scores for both the\nGEO-BLEU and the DTW metrics across the two types of mobility\ndata tested. We believe that the success of the GeoFormer in the\nHuMob Challenge 2023 will pave the way for more applications\nGeoFormer: Predicting Human Mobility using GPT HuMob-Challenge ‚Äô23, November 13, 2023, Hamburg, Germany\nof generative deep learning models in solving problems related to\nhuman mobility.\nREFERENCES\n[1] MIT. [n. d.]. HuMob Challenge 2023 | MIT Connection Science. https:\n//connection.mit.edu/humob-challenge-2023\n[2] Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre\nDognin, Jerret Ross, Ravi Nair, and Erik Altman. 2021. Tabular Transformers for\nModeling Multivariate Time Series. Institute of Electrical and Electronics Engi-\nneers Inc. https://doi.org/10.1109/ICASSP39728.2021.9414142 ISSN: 15206149.\n[3] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.\nImproving language understanding by generative pre-training. (2018).\n[4] Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever.\n2019. Language Models are Unsupervised Multitask Learners.\n[5] H. Sakoe and S. Chiba. 1978. Dynamic programming algorithm optimization\nfor spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal\nProcessing 26, 1 (1978), 43‚Äì49. https://doi.org/10.1109/TASSP.1978.1163055\n[6] Toru Shimizu, Kota Tsubouchi, and Takahiro Yabe. 2022. GEO-BLEU: similar-\nity measure for geospatial sequences. In Proceedings of the 30th International\nConference on Advances in Geographic Information Systems . 1‚Äì4.\n[7] Aivin Solatorio and Olivier Dupriez. 2023. Generating synthetic data using\nREaLTabFormer, and assessing the probabilistic measure of statistical disclosure\nrisk. In UNECE Expert Meeting on Statistical Data Confidentiality 2023, 26-28\nSeptember 2023, Wiesbaden .\n[8] Aivin V. Solatorio and Olivier Dupriez. 2023. REaLTabFormer: Generating Realis-\ntic Relational and Tabular Data using Transformers. arXiv:2302.02041 [cs.LG]\n[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[10] Taras K Vintsyuk. 1968. Speech discrimination by dynamic programming. Cy-\nbernetics 4, 1 (1968), 52‚Äì57.\n[11] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki,\nEsteban Moro, and Alex Pentland. 2023. Metropolitan Scale and Longitudinal\nDataset of Anonymized Human Mobility Trajectories. arXiv:2307.03401 [cs.SI]\nA DATA PROCESSING\nA.1 Vocabulary\nThe linearized trajectory of an individual consists of tokens from\nthe following set. These tokens are mapped to learnable embeddings\nof the model. The day-of-week tokens are expected to learn the\nvariations across different days in a week. The x and y coordinates\nare independently represented instead of creating unique tokens\nfor a tuple (x, y). This helps reduce the number of tokens in the\nmodel, and also help the model generalize.\n1 # Special tokens:\n2 <eos>, <|data|>, <|sep|>\n3\n4 # The day-of-week tokens:\n5 <|dow0|>, <|dow1|>, <|dow2|>, <|dow3|>, <|dow4|>, <|dow5|>, <|dow6|>\n6\n7 # User id tokens\n8 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n9\n10 # Location tokens:\n11 N\n12 x000, x001, x002, x003, x004, ..., x499\n13 y000, y001, y002, y003, y004, ..., y499\nA.2 Example linearized input\nThis signature represents a trajectory for user 71000. The data starts\non day-of-week 6. This will be an input to the trained model to\ngenerate the trajectory for the next day. The special token <|sep|>\nconditions the start of the prediction.\n1 71000\n2 <|data|>\n3 <|dow6|>NNNNNNNNNNNNNNNNNNNx129y088x129y090x128y086x128y087x131y092N\n4 x132y089x132y091x132y092Nx128y089x126y091NNNx131y092x132y088NN\n5 x132y087NNx126y086NNNNNN\n6 <|dow0|>NNNNNNNNNNNNNNNx135y076x126y084x127y085x133y078x126y086NN\n7 x128y092x126y085x126y086x127y086NNx127y092x127y089x131y092x131y090\n8 x126y088x124y092x127y085Nx127y085NNNx130y086x130y090NNx126y086NNN\n9 <|dow1|>NNNNNNNNNNNNNNNNNx126y087x127y088x127y085x124y093x125y091x125y084\n10 x126y087x131y091NNNNNNNNNx128y089x126y087x126y086x128y087x126y088\n11 x131y090Nx132y093x132y087x131y080x126y086NNN\n12 <|dow2|>NNNNNNNNNNNNNNNNNx128y089Nx130y087x131y091NNNx128y089x127y086N\n13 x127y096x128y101x126y097Nx131y092Nx118y071Nx120y077x126y086x128y087\n14 x126y086Nx131y090NNNNNNx125y088\n15 <|dow3|>NNNNNNNNNNNNNNNNx144y072x149y075NNx149y075NNx161y062x141y075\n16 x131y092x126y086x126y089x125y100x121y102NNx121y101x122y102Nx126y100\n17 x127y089x127y086NNNNNNNNNN\n18 <|dow4|>NNNNNNNNNNNNNNNNNx126y082x141y079x154y074x157y073NNNNNx157y082\n19 x130y087NNx131y092x125y088Nx126y086x128y099x130y094NNNNx142y079NNN\n20 x131y092NNN\n21 <|dow5|>x128y089x126y086NNx126y086NNNNNNNNNNNx131y090NNx128y089x129y089\n22 x131y092NNx132y092NNNNNNNNNNNx131y092NNNNNNx126y086x131y092NNN\n23 <|sep|>\nA.3 Example target signature\nThe target signature is used to inform the generative algorithm\nto limit the scope of prediction only on periods where location\nvalues are expected. The N represents time periods where no data\nis expected, the xy represents the generation of a sequence of coor-\ndinates (x, y) in the given period. For example, when the prediction\ncorresponds to an x, only location tokens for the x coordinate are\nconsidered in the generation. The leading number 6 represents\nthe day-of-week, which will be parsed, helping contextualize the\ngeneration of the coordinates.\n1 6NNNNxyNNNNNNNNNNNNNNxyNxyxyxyNNNxyxyNxyxyNxyxyxyNNNNxyNNxyNNNN\nA.4 Evaluation loss traces\nFigure 2: Evaluation loss traces for the models. The eval-\nuation loss for the task 1 dataset does not diverge despite\ntraining relatively long. On the other hand, the fine-tuned\nmodel for task 2 briefly decreased but eventually diverged.\nWe chose the model for task 2 with the lowest validation\nmetric.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7066241502761841
    },
    {
      "name": "Dynamic time warping",
      "score": 0.6275417804718018
    },
    {
      "name": "Transformer",
      "score": 0.5993617177009583
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4485437572002411
    },
    {
      "name": "Mobility model",
      "score": 0.44339919090270996
    },
    {
      "name": "Image warping",
      "score": 0.43223682045936584
    },
    {
      "name": "Machine learning",
      "score": 0.39263540506362915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37395530939102173
    },
    {
      "name": "Engineering",
      "score": 0.13348481059074402
    },
    {
      "name": "Telecommunications",
      "score": 0.10814997553825378
    },
    {
      "name": "Voltage",
      "score": 0.0936078131198883
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1334329717",
      "name": "World Bank",
      "country": "US"
    }
  ],
  "cited_by": 12
}