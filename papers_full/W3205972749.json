{
  "title": "Taming Sparsely Activated Transformer with Stochastic Experts",
  "url": "https://openalex.org/W3205972749",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224668709",
      "name": "Zuo, Simiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu XiaoDong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135173743",
      "name": "Jiao Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149266698",
      "name": "Kim, Young Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288937756",
      "name": "Hassan, Hany",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2366222013",
      "name": "Zhang, Ruofei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1949403856",
      "name": "Zhao, Tuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao Jian-feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3066373881",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2915573484",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W3127622310",
    "https://openalex.org/W3199518308",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3166104088",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W3119866685"
  ],
  "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models, such as the Switch Transformer, experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions. We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/Stochastic-Mixture-of-Experts.",
  "full_text": "Published as a conference paper at ICLR 2022\nTAMING SPARSELY ACTIVATED TRANSFORMER WITH\nSTOCHASTIC EXPERTS\nSimiao Zuo†∗, Xiaodong Liu ⋄, Jian Jiao⋄, Young Jin Kim⋄, Hany Hassan⋄, Ruofei Zhang⋄,\nTuo Zhao†and Jianfeng Gao⋄\n†Georgia Institute of Technology ⋄Microsoft\n{simiaozuo,tourzhao}@gatech.edu,\n{xiaodl,jian.jiao,youki,hanyh,bzhang,jfgao}@microsoft.com\nABSTRACT\nSparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can eas-\nily scale to have outrageously large amounts of parameters without signiﬁcant\nincrease in computational cost. However, SAMs are reported to be parameter in-\nefﬁcient such that larger models do not always lead to better performance. While\nmost on-going research focuses on improving SAMs models by exploring meth-\nods of routing inputs to experts, our analysis reveals that such research might not\nlead to the solution we expect, i.e., the commonly-used routing methods based\non gating mechanisms do not work better than randomly routing inputs to ex-\nperts. In this paper, we propose a new expert-based model, THOR ( Transformer\nwitH StOchastic Expe Rts). Unlike classic expert-based models, such as the\nSwitch Transformer (Fedus et al., 2021), experts in THOR are randomly acti-\nvated for each input during training and inference. THOR models are trained\nusing a consistency regularized loss, where experts learn not only from train-\ning data but also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine trans-\nlation tasks. Results show that THOR models are more parameter efﬁcient in\nthat they signiﬁcantly outperform the Transformer and MoE models across var-\nious settings. For example, in multilingual translation, THOR outperforms the\nSwitch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model (Kim et al., 2021) that is 18 times larger.\nOur code is publicly available at: https://github.com/microsoft/\nStochastic-Mixture-of-Experts .\n1 I NTRODUCTION\nLarge neural network models have shown to be effective in many natural language processing tasks\nsuch as machine translation (Lewis et al., 2020; Conneau & Lample, 2019), natural language un-\nderstanding (Devlin et al., 2019; Liu et al., 2019; He et al., 2020), and natural language generation\n(Radford et al., 2019; Brown et al., 2020). These models are usually densely activated. That is, a\nmodel uses all its parameters to process all inputs. One drawback of these models is the prohibitive\ntraining cost. Moreover, the extreme size drastically reduces inference speed, further limiting the\nmodels’ practicality.\nTo address these issues, sparsely activated models (SAMs, Shazeer et al. 2017) have been proposed.\nA SAM adaptively selects a subset of its parameters for different inputs during model training and\ninference. This makes it possible to train SAMs that are an order of magnitude larger than densely\nactivated models without signiﬁcant increase in computational cost. For example, the sparsely acti-\nvated GShard (Lepikhin et al., 2020) consists of over 600 billion parameters and the Switch Trans-\nformer (Fedus et al., 2021) 1.5 trillion parameters, while GPT-3 (Brown et al., 2020), which is\narguably the largest densely activated model, consists of only 175 billion parameters.\nThe building block of SAMs is the expert layer, which contains an attention mechanism and multi-\nple feed-forward neural networks (FFNs) in parallel. Each FFN is referred to as an expert. During\n∗Work was done during an internship at Microsoft.\n1\narXiv:2110.04260v3  [cs.CL]  3 Feb 2022\nPublished as a conference paper at ICLR 2022\ntraining, an input is routed to a ﬁxed number of experts, such that the number of ﬂoating point op-\nerations (FLOPs) of one forward pass remains constant, regardless of the total number of experts.\nThus, training SAMs is much more cost-efﬁcient than training densely activated models. For exam-\nple, training of Switch-large (Fedus et al., 2021) and that of T5-large (Raffel et al., 2019) require the\nsame forward FLOPs, despite that the former is 35 times larger (26.3 vs. 0.74 billion parameters).\nHowever, SAMs have been reported to be parameter inefﬁcient. For example, although the Switch-\nlarge model is 35 times larger than T5-large, its performance on the GLUE benchmark (Wang et al.,\n2019a) is only slightly better (88.5 vs. 87.8). There are also cases where the performance of SAMs\nis even worse than smaller densely activated models. For example, the performance of Switch-\nlarge is worse than T5-large on the ARC Reasoning Challenge (66.0 vs. 68.8) (Clark et al., 2018).\nIn another example, although GShard (Lepikhin et al., 2020) shows substantial gains over densely\nactivated models, a diminishing return with larger number of parameters has been observed.\nMost on-going research has focused on improving SAMs by developing effective routing methods.\nSince only a subset of model parameters (i.e., experts) are updated for each input during training,\nwe need to decide which experts to be activated given an input. Existing works (Shazeer et al.,\n2017; Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021) use a gating network for input\nrouting. However, the gating mechanism suffers from the notoriousload imbalance issue: the gate’s\nweight could collapse such that nearly all the inputs are routed to the same expert. Therefore, many\nmethods are proposed to mitigate this issue, such as noisy gating (Shazeer et al., 2017), expert\ncapacity (Lepikhin et al., 2020), load balancing loss (Lepikhin et al., 2020; Fedus et al., 2021), and\nkTop-1 gating (Yang et al., 2021). However, these routing methods have not been proved effective\nto make SAMs more parameter efﬁcient. To understand why SAMs are not parameter efﬁcient, we\nanalyze the performance of several classic MoE models. Our analysis reveals that a SAM does not\nalways outperform a densely activated model of a similar size, conﬁrming the results reported in\nYang et al. (2021). Moreover, we also observe that the widely-used routing method based on the\ngating mechanism does not work better than randomly routing inputs to experts,\nInspired by our ﬁndings, we propose a new SAM, THOR (Transformer witH StOchastic ExpeRts).\nUnlike classic SAMs, such as the Switch Transformer, experts in THOR are randomly activated\n(with no need of any gating mechanism) for each input during training and inference. THOR models\nare trained by minimizing both the cross-entropy loss and a consistency regularization term, such\nthat experts can learn not only from training data but also from other experts as teachers so that all\nthe experts make consistent predictions.\nTo validate the effectiveness of THOR, we have conducted extensive experiments on machine\ntranslation using three settings: low-resource, rich-resource, and multilingual. Results show that\nTHOR models outperform state-of-the-art MoE models by an average of 2 BLEU score on twelve\nlow-resource translation tasks. In the rich-resource setting, THOR achieves new state-of-the-art\nresults on the two widely-used translation benchmarks, WMT’16 En-De and WMT’14 En-Fr. On\nmultilingual translation tasks, the THOR model with 300 million parameters achieves 2 BLEU score\nimprovement over a state-of-the-art MoE model of the same size. Moreover, our model achieves\nstate-of-the-art results on these tasks — the same BLEU score that is achieved by the Z-code MoE\nmodel (Kim et al., 2021) with 5.5 billion parameters (18 times larger).\n2 B ACKGROUND\nTransformer. The Transformer (Vaswani et al., 2017) model has demonstrated its superior perfor-\nmance in many sequence-to-sequence natural language processing tasks, such as neural machine\ntranslation. The model contains an encoder and a decoder. The encoder consists of multiple encoder\nlayers, each having an identical structure. An encoder layer employs a self-attention mechanism and\na feed-forward neural network (FFN). The decoder is similarly constructed, except for an additional\ncross-attention mechanism in each decoder layer.\nSparsely Activated Models. The building block of SAMs is the expert layer, which is similar to the\nTransformer layer. Each of these expert layers contain an attention mechanism and multiple FFNs\nin parallel, where each FFN is referred to as an expert. Let {Ei}N\ni=1 denote the experts, and N\ndenotes the total number of experts. A gating mechanism decides to which expert(s) an input should\nbe routed. At each expert layer, given an input vectorx∈Rd, where dis the embedding dimension,\nthe gate value of routing xto expert Ei is\npi(x) = [Softmax (Wgx)]i , (1)\n2\nPublished as a conference paper at ICLR 2022\nwhere Wg ∈RN×d is the trainable weight matrix of the gating mechanism. Given the gate values\n{pi(x)}N\ni=1, we select the top- K experts to form an activated set of experts T ⊂{1 ···N}, where\n|T| = K. Then the output xout of the expert layer is\nxout =\n∑\ni∈T\npi(x)Ei(x). (2)\nNotice that in Eq. 2, input xonly activates Kinstead of N experts, where K ≪N, e.g., K = 2and\nN = 2048in GShard (Lepikhin et al., 2020). This implies that the number of FLOPs required for\none forward pass does not increase with the number of experts N. Therefore, SAMs can scale to an\nenormous size without any signiﬁcant increase in training time and inference time.\nThe gate weight matrixWg (Eq. 1) is trained together with the rest of the model parameters. Because\nthere is no constraint on the learned weights, it is possible that Wg collapses such that one row\ndominates, i.e., all the inputs are routed to one expert. This problem is referred to asload imbalance.\nExisting works adopt various ad-hoc heuristics to mitigate this issue, e.g., adding Gaussian noise\nto Eq. 1 (noisy gating, Shazeer et al. 2017), limiting the maximum number of inputs that can be\nrouted to an expert (expert capacity, Lepikhin et al. 2020), imposing a load balancing loss (Lepikhin\net al., 2020; Fedus et al., 2021), and using linear assignment (Lewis et al., 2021). There are other\nworks that remove the gating mechanism such that load imbalance is no longer an issue, e.g., by\nincorporating hash functions (Roller et al., 2021). Besides the load imbalance issue, there are also\nheated discussions on how to construct T in Eq. 2. For example, Shazeer et al. (2017); Lepikhin\net al. (2020); Yang et al. (2021) conjecture that routing inputs to K >1 experts is necessary, while\nFedus et al. (2021) argue that using K = 1is sufﬁcient and more computationally efﬁcient.\n3 A NALYSIS OF SPARSELY ACTIVATED MODELS\nWe investigate behavior of the gating mechanism of several classic MoE models. We conduct exper-\niments on a multilingual translation task,{De, Vi}→En. More details are presented in Appendix A.\nWe consider two MoE models proposed in Shen et al. (2019), referred to as MoE(dec) and MoE(tok),\nrespectively, and three variants of the Switch Transformer proposed in Fedus et al. (2021). The\nnumber of experts is set to two for all the MoE models. We compare them with the Transformer\n(Vaswani et al., 2017) model of the same model size.\nFigure 1 shows the validation losses and BLEU scores of three models: Transformer, MoE(dec),\nand MoE(tok). We see that the two MoE models perform very similarly, and neither outperforms\nthe Transformer by a signiﬁcant margin.\nTo interpret the results of Figure 1, we examine the load of each expert and the conﬁdence scores\nof routing inputs to different experts. An expert’s load is deﬁned as the proportion of inputs that are\nassigned to it. For an input that is routed to an expert, its routing conﬁdence score (output of the\ngating mechanism) determines the level of preference, e.g., if the routing conﬁdence score is 0.5,\nthen the gate has no preference for either expert. For each expert, we compute the average routing\nconﬁdence score over all the inputs assigned to it.\nFigure 2 shows that after the early stage of training (i.e., the ﬁrst 200 iterations), the gate weight\ncollapses and nearly all the inputs are routed to expert 2. Also, the average routing conﬁdence score\nof expert 2 is close to 1.0, which means that the gate strongly prefers expert 2 to expert 1. In this\ncase, only one of the experts is sufﬁciently trained. Figure 3 depicts a different scenario, where the\ninputs are randomly dispatched to the experts. Notice that after approximately 4000 iterations, the\ntwo experts are equally loaded, and the probabilities of assigning any input to expert 1 and expert 2\nare almost identical, indicating that the gating mechanism has no preference for either expert.\nWe have identiﬁed two behaviors of the gating mechanism: load imbalance and random routing.\nThe former is also reported in recent papers (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al.,\n2021). We further investigate the Switch Transformer (Fedus et al., 2021), which is a state-of-the-\nart MoE variant that incorporates various methods to resolve the load imbalance issue. In addition,\nbecause behavior of the gating mechanism in the Switch Transformer mimics random routing (see\nAppendix A), we examine the effect of discarding the gate and randomly assigning inputs to experts.\nFigure 4 demonstrates the validation losses and BLEU scores of the Transformer and three variants\nof the Switch Transformer, where inputs are routed according to tokens (referred to as Switch(t)),\nsentences (Switch(s)), or are routed randomly (Switch(r)). Similar to the results in Figure 1, we\n3\nPublished as a conference paper at ICLR 2022\nFigure 1: Validation results of\nMoE(dec) and MoE(tok).\nFigure 2: Gating mechanism of MoE(dec). Left: average rout-\ning conﬁdence; Right: load of experts.\nFigure 3: Gating mechanism of MoE(tok). Left: average\nrouting conﬁdence; Right: load of experts.\nFigure 4: Performance of three vari-\nants of the Switch Transformer.\nsee that the four models perform similarly. This shows that even after we alleviate load imbalance,\nmodel performance is not improved (i.e., the Switch Transformers do not outperform the vanilla\nTransformer), and the performance of the Switch Transformer does not vary much among different\nrouting methods, including random routing.\nWe remark that in this paper, we focus on natural language processing tasks, in particular neural ma-\nchine translation. There are other works in different research ﬁelds (e.g., computer vision) that draw\ndifferent conclusions than ours (Riquelme et al., 2021). We attribute this to the intrinsic differences\nbetween image classiﬁcation and language generation, e.g., each input in the former belongs to a\nclearly-deﬁned category, while no such knowledge exists in the latter.\nIn summary, the experiments reveal\n• A sparsely activated model does not always outperform a densely activated model of the\nsame model size.\n• The widely-used routing method based on the gating mechanism does not work better than\nrandomly routing inputs to experts.\n4 THOR: T RANSFORMER WITH STOCHASTIC EXPERTS\nThe ineffectiveness of the gating mechanism, as shown in our experiments, motivates us to propose\na new expert-based model, THOR ( Transformer witH StOchastic ExpeRts). In THOR, a pair of\nexperts are randomly selected and activated in each layer during a training iteration, and then all the\ninputs in a batch are processed using the same pair of experts. Our method drastically simpliﬁes\nmodel design, and has two additional advantages. First, it eliminates the load imbalance issue be-\ncause randomly selecting a pair of experts in each iteration allows each expert to have a fair chance\nto be sufﬁciently trained. The ad-hoc heuristics, such as the load balancing loss, as discussed in\nSection 2, are no longer needed. Second, unlike the gating mechanism, THOR does not introduce\nany additional model parameters.\nOne problem of THOR is that without a gating mechanism, experts need to be randomly selected\nduring inference, and we may obtain inconsistent inference results due to different random seeds.\nFor example, on a Czech-to-English translation dataset, our experiments show that randomness can\nresult in a 0.5 BLEU score difference.\nTo address this issue, we introduce a consistency regularizer in the training objective of THOR.\nConcretely, let N denotes the number of experts, Lthe number of layers, and El\ni an activated expert\n4\nPublished as a conference paper at ICLR 2022\nFigure 5: Illustration of a training iteration with stochastic experts. For conciseness, we show a\nmodel with only one Transformer layer.\n(which is a FFN) in layer l, where 1 ≤i ≤N and 1 ≤l ≤L. We use p = f(x; {El\ni}L\nl=1) to\nindicate the prediction probability of inputxusing the model fwhere experts {El\ni}L\nl=1 are activated.\nFigure 5 illustrates one training iteration. Notice that instead of activating one expert for each\nlayer in an iteration, we select to activate a pair of experts in THOR. As a result, we obtain two\nprediction probabilities produced by the two selections, respectively: p1 = f(x; {El\ni}L\nl=1)) and\np2 = f(x; {El\nj}L\nl=1)). Then, the training objective of THOR with respect to training samples (x,y)\nin the dataset Dis\nmin\n∑\n(x,y)∈D\nℓ(x,y) = CE(p1; y) + CE(p2; y) +αCR(p1; p2),\nwhere CR(p1; p2) =1\n2 (KL(p1∥p2) + KL(p2∥p1)) .\n(3)\nHere, CE is the cross-entropy loss, the consistency regularizer CR is deﬁned as the average of the\ntwo Kullback–Leibler (KL) divergence terms, and αis a hyper-parameter that controls the strength\nof the regularizer. In mini-batch SGD training, we randomly sample a pair of experts to activate\nat each layer for each batch. During inference, we can also randomly select an expert to activate\nat each layer for each input, similar to that in training. We can also use different expert-selection\nmethods, such as expert-ensemble, as to be discussed in Section 5 (Table 5).\nThe THOR training objective of Eq. 3 forces all the experts to minimize training errors while mak-\ning the same predictions as much as possible. Thus, in each training step, each expert optimizes its\nparameters by learning from both the training data (via minimizing the cross-entropy loss) and its\npaired expert as a teacher (via minimizing the KL divergence). Although these experts are learned\nto make consistent predictions, they converge to different (local) optima given the randomness in-\ntroduced in training, e.g., initialization, mini-batch SGD, random routing, etc. Thus, every expert\nlearns from a set of diverse teachers during the course of training, which helps to improve model’s\nperformance. In addition, by penalizing experts that yield inconsistent predictions from the others,\nthe consistency regularizer also helps reducing the variance of model prediction.\nTHOR is conceptually similar to dropout (Srivastava et al., 2014) since both methods route an input\nto some randomly selected sub-net components (i.e., experts in THOR and neurons in dropout).\nHowever, THOR differs from dropout in several important aspects, making it a better choice for\nefﬁcient training and serving of large-scale neural models. First, THOR can be applied to both\ntraining and inference, while dropout is only used for training. Second, THOR is shown to be more\nrobust in large-scale model training than dropout. For example, our models are less likely to overﬁt\nwith the increase in the number of experts (see Figure 9). Third, THOR leads to a sparse model\n5\nPublished as a conference paper at ICLR 2022\nthat is more structured than that of dropout, such that a large-scale THOR model can be much more\neasily trained using GPU clusters, e.g., by putting different experts on different GPUs in parallel.\n5 E XPERIMENTS\nWe evaluate THOR on neural machine translation. We adopt three settings: low-resource trans-\nlation, rich-resource translation, and multilingual translation. For low-resource and rich-resource\ntranslation, we train all the models usingFairseq1 (Ott et al., 2019). For multilingual translation, we\nuse DeepSpeed MoE2 (Kim et al., 2021) to implement the MoE models. All the experiments are con-\nducted on NVIDIA V100 GPUs. Additional experiments, including model scale-up and comparison\nof inference speed, are deferred to Appendix D.\n5.1 B ASELINE\nWe use two baselines in the experiments.\n• Transformer (Vaswani et al., 2017) achieves superior performance in many sequence-to-\nsequence learning tasks, such as neural machine translation.\n• Switch Transformer (Fedus et al., 2021) is a state-of-the-art MoE model, which employs a\ngating mechanism to route inputs and uses a load balancing loss to reduce load imbalance.\nTo verify the effectiveness of the imposed consistency regularizer in Eq. 3, we also compare\nTHOR with Transformer models trained using two popular regularization methods. We remark\nthat these two methods share similar computational costs with THOR, i.e., they also require two\nforward passes in each training iteration.\n• SMART (Jiang et al., 2020) utilizes a smoothness inducing adversarial regularizer to pe-\nnalize the worst case difference between predictions of a clean input and a perturbed input.\n• R3F (Aghajanyan et al., 2020) uses a regularizer to reduce representational collapse. The\nmethod has shown to be effective in various natural language processing tasks.\nAll the methods are trained for the same number of FLOPs in the experiments for fair comparison.\n5.2 L OW-RESOURCE TRANSLATION\nWe use six language pairs: English to Vietnamese, English to German, and English to French from\nIWSLT; English to Romanian, English to Latvian, and English to Czech from Europarl 3. Dataset\nstatistics are summarized in Table 6 (Appendix B).\nTable 1: Experimental results on low resource datasets. The best result on each dataset is in bold.\nEn-Vi Vi-En En-De De-En En-Fr Fr-En\nTransformer (Vaswani et al., 2017) 31.3 29.4 28.1 34.8 39.2 38.1\nSMART (Jiang et al., 2020) 32.5 30.5 29.3 35.8 40.0 38.8\nR3F (Aghajanyan et al., 2020) 32.2 30.7 29.2 35.7 39.7 38.9\nSwitch (Fedus et al., 2021) 31.7 29.5 28.4 34.6 39.1 38.2\nTHOR 34.0 33.0 31.1 37.8 40.7 40.0\nEn-Ro Ro-En En-Lv Lv-En En-Cs Cs-En\nTransformer (Vaswani et al., 2017) 23.5 25.0 13.6 15.8 16.1 20.4\nSMART (Jiang et al., 2020) 24.6 25.7 14.2 16.3 16.7 21.4\nR3F (Aghajanyan et al., 2020) 23.8 25.8 14.4 16.3 16.8 21.6\nSwitch (Fedus et al., 2021) 23.8 24.4 13.8 16.1 16.1 20.6\nTHOR 25.2 27.1 15.2 17.4 17.6 22.4\n1https://github.com/pytorch/fairseq\n2https://github.com/microsoft/DeepSpeed\n3https://www.statmt.org/europarl\n6\nPublished as a conference paper at ICLR 2022\nTo evaluate THOR with different model sizes, we use the Transformer-base (Vaswani et al.,\n2017) architecture on Europarl datasets, and a smaller model on IWSLT datasets. Compared with\nTransformer-base, the smaller model decreases the hidden dimension from 2048 to 1024, and de-\ncreases the number of heads from 8 to 4 with the dimension of each head doubled. We use two\nexperts for the expert-based models. We remark that even though THOR increases the number of\nparameters, its inference speed (in terms of FLOPs) is the same as Transformer-base because only\none expert is activated for each input. Interested readers refer to Appendix C for more details.\nThe experimental results in Table 1 show that performance of the Switch Transformer is on par\nwith the vanilla Transformer, e.g., its average BLEU score on the 12 datasets is 26.3, the same as\nthe Transformer. The results conﬁrm that SAMs do not outperform densely activated models with\nsimilar model sizes. In contrast, THOR achieves more than 1.0 BLEU score improvement over the\nSwitch Transformer in all the 12 tasks. THOR also signiﬁcantly outperforms the models trained\nusing the two competing regularization methods, SMART and R3F.\n5.3 R ICH -RESOURCE TRANSLATION\nWe use two widely adopted rich-resource transla-\ntion benchmarks: English to German translation\nfrom WMT’16 and English to French translation\nfrom WMT’14. The former dataset consists of\n4.5 million training sentence pairs, and the latter\n36 million pairs. We follow the pre-processing\nsteps in Ott et al. (2018).\nTo evaluate THOR , We use the Transformer-big\narchitecture (Vaswani et al., 2017) and we set the\nnumber of experts for both THOR and the Switch\nTransformer to 4. Interested readers refer to Ap-\npendix C for more details.\nTable 2 reports the BLEU scores and the sacre-\nBLEU scores (Post, 2018) of different mod-\nels. We see that THOR achieves new state-of-\nthe-art results in the setting where neither data\naugmentation nor pre-trained language model\nis used. Speciﬁcally, THOR lifts the previ-\nous state-of-the-art (Liu et al., 2020b;c) by 0.3\nBLEU score on the En-De translation task and\n0.1 BLEU score on the En-Fr translation task.\nTHOR also signiﬁcantly outperforms the models\ntrained using the other two regularization meth-\nods, SMART (Jiang et al., 2020) and R3F (Agha-\njanyan et al., 2020). Similar to what is observed\nin low-resource translation, the Switch Trans-\nformer (Fedus et al., 2021) does not outperform\nthe vanilla Transformer (Ott et al., 2018).\nTable 2: BLEU and sacreBLEU scores on\nWMT’14 En-Fr and WMT’16 En-De. Re-\nsults of Jiang et al. (2020), Aghajanyan et al.\n(2020), and Fedus et al. (2021) are from our\nimplementation.\nBLEU En-De En-Fr\nVaswani et al. (2017) 28.4 41.8\nOtt et al. (2018) 29.3 43.2\nWang et al. (2019b) 29.6 —\nWu et al. (2019a) 29.7 43.2\nSo et al. (2019) 29.8 41.3\nJiang et al. (2020) 29.8 43.4\nWu et al. (2019b) 29.9 43.3\nAghajanyan et al. (2020) 29.4 43.3\nLiu et al. (2020c) 30.1 43.8\nFedus et al. (2021) 29.3 43.0\nTHOR 30.4 43.8\nsacreBLEU En-De En-Fr\nOtt et al. (2018) 28.6 41.4\nJiang et al. (2020) 29.1 41.5\nSo et al. (2019) 29.2 —\nAghajanyan et al. (2020) 29.0 41.5\nLiu et al. (2020c) 29.5 41.8\nFedus et al. (2021) 28.6 41.1\nTHOR 29.6 41.9\n5.4 M ULTILINGUAL TRANSLATION\nWe have collected 10 language pairs from WMT datasets, and built a 64k-entry dictionary for all\nthe languages. The detailed statistics are summarized in Table 7 (Appendix B). Please refer to Kim\net al. (2021) for more details. We do not use multi-task learning or additional monolingual data in\nthe experiments.\nWe use the following model architecture: the embedding dimension is set to 768 and the hidden\ndimension for the FFN is set to 3072; we use 12 encoder layers and 6 decoder layers, where each\nlayer has 12 attention heads, and the dimension of each head is 64. We set the number of experts to\n4 for both THOR and the Switch Transformer.\nTable 3 reports the average BLEU score of translating English to other languages, translating other\nlanguages to English, and the overall score of the 20 tasks. We see that compared with the Switch\n7\nPublished as a conference paper at ICLR 2022\nTransformer of the same size (i.e., 300 million parameters), our model achieves a 2-point improve-\nment in the overall BLEU score. In addition, our model is far more parameter efﬁcient than the\nSwitch Transformer. The THOR model with300 million parameters achieves the same BLEU score\n(24.4) that is achieved by the Switch Transformer with 5.5 billion parameters, which is more than\n18 times larger.\nTable 3: Multilingual translation results. Here “E” means the number of experts.\nEn→Others Others →En Average\nSwitch (32E, 5.5B) — — 24.4\nSwitch (4E, 300M) 20.3 24.6 22.4\nTHOR (4E, 300M) 21.4 27.4 24.4\nFigure 6 shows BLEU scores in all the 20 translation tasks. Notice that THOR outperforms the\nbaseline on 17 out of the 20 tasks. The improvement is in general more signiﬁcant on the tasks\nwith smaller datasets. For example, our model achieves BLEU score improvement of4.7 and 6.7 on\nGu-En (85k) and Hi-En (264k), respectively. On the tasks with larger datasets, the improvement ob-\ntained by our model is less substantial, but still signiﬁcant, e.g., +0.9 BLEU score on Cs-En (10M)\nand +1.1 Fi-En (4.8M). For the only three tasks where our model underperforms the baseline, the\ngaps are small, e.g., −0.4, −0.2, and −0.4 BLEU scores on En-Cs, En-De, and En-Fr, respectively.\nFigure 6: Details of multilingual translation results.\n5.5 A BLATION EXPERIMENTS\nTraining Objective. We examine the relative contributions of the three loss terms used in the\nTHOR training objective of Eq. 3: CE1, CE2 and CR. The result in Table 4 shows that the con-\nsistency regularizer CR is crucial to the model performance, and that dropping one of the two CE\nterms leads to only very small BLEU score loss since the two cross-entropy terms play the same role\nin training.\nInference Methods. We compare three inference methods: (1) Dispatch(s) uses sentence-\nlevel random routing, where all tokens in one sentence are routed to the same expert; (2)\nDispatch(t) uses token-level random routing, where tokens within a sentence are routed to\ndifferent experts; (3) Ensemble, where each sentence is routed to all the N experts, and the N\nhidden representations in each layer are averaged. Note that the number of FLOPs is larger for\nEnsemble because we need to run forward pass for each input through N experts. Table 5 shows\nthat Dispatch(s) and Dispatch(t) perform similarly, and Ensemble yields the best BLEU\nscore with a cost of longer inference time.\n8\nPublished as a conference paper at ICLR 2022\nTable 4: Effect of the three loss\nterms in training object of Eq. 3,\ntested on Cs-En translation.\nLoss terms BLEU\nCE1 + CE2 + CR 22.4\nCE1 + CR 22.2\nCE1 + CE2 20.8\nCE1 20.6\nTable 5: Performance and costs of\nthree inference methods, tested on Cs-\nEn translation.\nBLEU time\nDispatch(s) 22.4 ×1\nDispatch(t) 22.4 ×1\nEnsemble 22.6 ×N\nFigure 7: Effect of the consis-\ntency regularization strength\nαon Cs-En translation.\nFigure 8: Violin plot of per-\nformance consistency on Cs-\nEn translation.\nFigure 9: BLEU vs. model\nsize on De-En translation.\nRegularization strength. To investigate the effect of the regularization strength α, we run ex-\nperiments on the Cs-En translation dataset in the low-resource setting. Figure 7 shows that model\nperformance is not very sensitive to αas long as the value is large enough, say α> 2.0.\nConsistency of Model Prediction. We study the variance of model prediction due to the use\nof randomly activated experts during inference. We compare THOR and the Switch Transformer,\nwhere we remove the trained gate during inference. For each model, we compute the variance of\nmodel prediction based on 20 runs. As shown in Figure 8, THOR makes more consistent predic-\ntions than Switch Transformer due to the use of the consistency regularizer for model training. The\nvariance of THOR is below 0.002, whereas the variance of Switch Transformer is 0.008, four times\nlarger. We remark that by removing the trained router from the Switch Transformer, model perfor-\nmance only marginally decreases (from 20.6 to 20.4). This further indicates that a trained router\nmay not be better than a random router.\nOverﬁtting. We compare the THOR model and the Transformer model regarding how likely they\noverﬁt the training data when the model size increases. We run experiments on the De-En data in\nthe low-resource setting, where the dropout rate of the FFNs in the Transformer is selected such that\nthe number of parameters trained in one iteration is the same as the THOR model. As shown in\nFigure 9, THOR does not show any sign of overﬁtting — we observe a consistent improvement in\nBLEU score as we increase the number of experts from 2 to 8. In contrast, the Transformer model’s\nperformance deteriorates as we increase the hidden dimension of its FFN from2kto 8k. We remark\nthat we also observe the overﬁtting phenomenon on larger datasets, e.g., the Transformer overﬁts on\nthe Cs-En dataset when we set the hidden dimension of its FFN to 16k.\n6 C ONCLUSION\nWe present a new expert-based sparsely activated model, THOR. Unlike existing SAMs, such as\nthe Switch Transformer, experts in THOR are randomly activated for each input during training\nand inference. THOR models are trained using a consistency regularized loss, where every expert\nlearns not only from training data but also from other experts as teachers so that all the experts make\nconsistent predictions. As a result, not only can large-scale THOR models be trained and served as\nefﬁciently as classic MoE models, THOR models also demonstrate a better generalization capability\nin that they are more parameter-efﬁcient, less likely to overﬁt, make more consistent predictions,\nand achieve better results consistently across different settings. We validate the effectiveness of\nTHOR via a comprehensive empirical study on machine translation. In all the three settings (i.e.,\nlow-resource, rich-resource, and multilingual translation), THOR models signiﬁcantly outperform\nthe vanilla Transformer, and Switch Transformer, a state-of-the-art MoE model.\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nWe thank Rukmini Lyer, Kevin Duh, Hao Cheng, Chunyuan Li, Johannes Gehrke, colleagues from\nMicrosoft Bing Ads team and Microsoft Research for their valuable discussions and comments.\nREFERENCES\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal\nGupta. Better ﬁne-tuning by reducing representational collapse. ArXiv preprint, abs/2008.03156,\n2020. URL https://arxiv.org/abs/2008.03156.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\nArXiv preprint, abs/1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nAlexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Hanna M.\nWallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pp. 7057–7067, 2019. URL https://proceedings.neurips.cc/paper/\n2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, Minneapolis, Min-\nnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL\nhttps://aclanthology.org/N19-1423.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. ArXiv preprint, abs/2101.03961, 2021. URL https:\n//arxiv.org/abs/2101.03961.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. ArXiv preprint, abs/2006.03654, 2020. URL https://arxiv.\norg/abs/2006.03654.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART:\nRobust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled reg-\nularized optimization. In Proceedings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics , pp. 2177–2190, Online, 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.acl-main.197. URL https://aclanthology.org/2020.\nacl-main.197.\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and ef-\nﬁcient moe training for multitask multilingual models. ArXiv preprint, abs/2109.10465, 2021.\nURL https://arxiv.org/abs/2109.10465.\n10\nPublished as a conference paper at ICLR 2022\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun (eds.),3rd International Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:\n//arxiv.org/abs/1412.6980.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. ArXiv preprint, abs/2006.16668, 2020. URL https:\n//arxiv.org/abs/2006.16668.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation, and comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics , pp. 7871–7880, Online,\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL\nhttps://aclanthology.org/2020.acl-main.703.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\nSimplifying training of large, sparse models. ArXiv preprint , abs/2103.16716, 2021. URL\nhttps://arxiv.org/abs/2103.16716.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei\nHan. On the variance of the adaptive learning rate and beyond. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-\nview.net, 2020a. URL https://openreview.net/forum?id=rkgz2aEKDr.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the dif-\nﬁculty of training transformers. In Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) , pp. 5747–5763, Online, 2020b. Association\nfor Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.463. URL https://\naclanthology.org/2020.emnlp-main.463.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao. Very deep transformers for neural ma-\nchine translation. ArXiv preprint, abs/2008.07772, 2020c. URL https://arxiv.org/abs/\n2008.07772.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. ArXiv preprint, abs/1907.11692, 2019. URL https://arxiv.org/abs/1907.\n11692.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine Translation: Research Papers , pp. 1–9, Brus-\nsels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301.\nURL https://aclanthology.org/W18-6301.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational Linguistics\n(Demonstrations), pp. 48–53, Minneapolis, Minnesota, 2019. Association for Computational Lin-\nguistics. doi: 10.18653/v1/N19-4009. URL https://aclanthology.org/N19-4009.\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pp. 186–191, Brussels, Belgium, 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.\norg/W18-6319.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-\ntext transformer. ArXiv preprint, abs/1910.10683, 2019. URL https://arxiv.org/abs/\n1910.10683.\n11\nPublished as a conference paper at ICLR 2022\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´e Su-\nsano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\narXiv preprint arXiv:2106.05974, 2021. URL https://arxiv.org/abs/2106.05974.\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large\nsparse models. ArXiv preprint, abs/2106.04426, 2021. URL https://arxiv.org/abs/\n2106.04426.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 1715–1725, Berlin, Germany, 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.\norg/P16-1162.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V . Le, Geoffrey E.\nHinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. In 5th International Conference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL\nhttps://openreview.net/forum?id=B1ckMDqlg.\nTianxiao Shen, Myle Ott, Michael Auli, and Marc’Aurelio Ranzato. Mixture models for diverse\nmachine translation: Tricks of the trade. In Kamalika Chaudhuri and Ruslan Salakhutdinov\n(eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning\nResearch, pp. 5719–5728. PMLR, 2019. URL http://proceedings.mlr.press/v97/\nshen19c.html.\nDavid R. So, Quoc V . Le, and Chen Liang. The evolved transformer. In Kamalika Chaudhuri\nand Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pp. 5877–5886. PMLR, 2019. URL http://proceedings.\nmlr.press/v97/so19a.html.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The journal of machine\nlearning research, 15(1):1929–1958, 2014.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016 ,\npp. 2818–2826. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.308. URL https:\n//doi.org/10.1109/CVPR.2016.308.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pp.\n5998–6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\n7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019 . OpenReview.net, 2019a. URL https://openreview.net/forum?id=\nrJ4km2R5t7.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S.\nChao. Learning deep transformer models for machine translation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics , pp. 1810–1822, Florence,\nItaly, 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1176. URL\nhttps://aclanthology.org/P19-1176.\n12\nPublished as a conference paper at ICLR 2022\nFelix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. In 7th International Conference on Learning Rep-\nresentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019a. URL\nhttps://openreview.net/forum?id=SkVhlh09tX.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao, Tao Qin, Jianhuang Lai, and Tie-Yan\nLiu. Depth growing for neural machine translation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics , pp. 5558–5563, Florence, Italy, 2019b.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1558. URL https://\naclanthology.org/P19-1558.\nAn Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang,\nJiamang Wang, Yong Li, et al. Exploring sparse expert models and beyond. ArXiv preprint,\nabs/2105.15082, 2021. URL https://arxiv.org/abs/2105.15082.\n13\nPublished as a conference paper at ICLR 2022\nA A NALYSIS OF SPARSELY ACTIVATED MODELS\nA.1 T RAINING DETAILS\nWe consider two Mixture-of-Experts (MoE) models proposed in Shen et al. (2019), which are de-\nnoted “MoE(dec)” and “MoE(tok)”. In the ﬁrst variant, each expert is a separate Transformer de-\ncoder. In the second variant, each expert is a different token, i.e., if we route the input to expert one,\nthen we replace the ⟨bos⟩(begin-of-sentence) token in the input sentence with a ⟨expert1⟩token.\nNote that embeddings of these expert tokens are trained together with the rest of the model parame-\nters. These models are equipped with an expectation-maximization optimization framework. Such a\nframework facilitates computing the probability of assigning an input to a speciﬁc expert according\nto the gating mechanism. Please refer to Shen et al. (2019) for details about these models.\nWe use a multilingual translation setting, where we adopt two datasets: De-En from IWSLT’14 and\nVi-En from IWSLT’15. For each dataset, we use byte pair encoding (BPE, Sennrich et al. 2016) with\n10,000 merge operations for pre-processing. Then we concatenate the two pre-processed datasets.\nWe learn a separate dictionary for En and {De+Vi}, which resulted in approximately 9k and 12k\nvocabularies, respectively.\nFor training, we use Adam (Kingma & Ba, 2015) as the optimizer and we set the learning rate to\n0.001. We set the batch size to be equivalent to 64ktokens, e.g., we use 8ktokens per GPU with 8\nGPUs. Other training details follow the Fairseq4 implementation. For inference, we use a beam size\nof 5 and a length penalty of 1.0.\nA.2 A DDITIONAL RESULTS\nWe also plot the average routing conﬁdence score and the load of experts for Switch(s) and Switch(t),\nsimilar to Figure 2 and Figure 3. We ﬁrst investigate the Switch Transformer without the load\nbalancing loss.\nFigure 10: Switch(s) w/o load balancing. Left: average routing conﬁdence; Right: load of experts.\nFigure 11: Switch(t) w/o load balancing. Left: average routing conﬁdence; Right: load of experts.\n4https://github.com/pytorch/fairseq/blob/master/examples/translation/\n14\nPublished as a conference paper at ICLR 2022\nFigure 10 shows the results for Switch(s) without the load balancing loss, where we route inputs to\nexperts on the sentence-level. We see that after about 10k training iterations, the average routing\nconﬁdence score of expert 1 and expert 2 becomes similar, and both of these scores are around0.60.\nMoreover, the load of the experts are not balanced, i.e., there is a 10% difference in the loads (55%\nvs. 45%). We conclude that behavior of the gating mechanism of Switch(s) is similar to Figure 3,\ni.e., the gate is essentially randomly routing inputs to experts without any preference.\nFigure 11 shows the results for Switch(t) without the load balancing loss, where we route inputs to\nexperts on the token-level, i.e., different tokens within the same sentence may be routed to different\nexperts. Similar to the Switch(s) case, the average routing conﬁdence score of both of the two\nexperts converges to around 0.55. This indicates that the gate do not prefer any expert given an\ninput. Moreover, the load of the experts are not balanced, the same as in Figure 10. Based on\nthese observations, we conclude that behavior of the gating mechanism of Switch(t) is also random\nrouting.\nFigure 12: Switch(s) w/ load balancing. Left: average routing conﬁdence; Right: load of experts.\nFigure 13: Switch(t) w/ load balancing. Left: average routing conﬁdence; Right: load of experts.\nFigure 12 and Figure 13 show behavior of the gating mechanism of Switch(s) and Switch(t) equipped\nwith the load balancing loss, respectively. We see that the load balancing loss indeed balances the\nload for both Switch(s) and Switch(t), e.g., there is a less than0.4% imbalance for Switch(s) and less\nthan 0.2% imbalance for Switch(t). In comparison, the imbalance is around 10% for the two Switch\nTransformer variants without the load balancing loss. Also, similar to the case without the load bal-\nancing loss, the average routing conﬁdence score converges to around0.60 for Switch(s) and around\n0.55 for Switch(t). Based on the observations, we conclude that behavior of the gating mechanism\nis still random routing when Switch(s) and Switch(t) are equipped with the load balancing loss.\nB D ATASETS\nStatistics of low-resource datasets are shown in Table 6. The English-Vietnamese, English-German,\nand English-French datasets are from 5 IWSLT’14, ’15, and ’16, respectively. The training data of\n5https://iwslt.org/\n15\nPublished as a conference paper at ICLR 2022\nEnglish-Romanian, English-Latvian, and English-Czech are from Europarl6, and the validation and\ntesting data are from WMT’17.\nStatistics and data sources used in the multilingual translation task are shown in Table 7.\nTable 6: Statistics of low resource translation datasets.\nEn-Vi En-De En-Fr En-Ro En-Lv En-Cs\nTrain 117,055 160,239 218,256 390,746 591,631 619,029\nValidation 5,098 7,283 8,453 1,900 1,949 2,902\nTest 1,268 6,750 1,133 1,999 2,001 3,005\nTable 7: Statistics of multilingual translation datasets. The other language in the translation tasks is\nEnglish (En) for all the datasets.\nLanguage Czech (Cs) German (De) Estonian (Et) Finnish (Fi) French (Fr)\nData source WMT’19 WMT’19 WMT’18 WMT’19 WMT’15\n# Samples 10,273,696 4,613,192 695,227 4,838,576 9,999,995\nLanguage Gujarati (Gu) Hindi (Hi) Latvian (Lv) Romanian (Ro) Turkish (Tr)\nData source WMT’19 WMT’14 WMT’17 WMT’16 WMT’18\n# Samples 85,688 264,199 1,444,235 540,562 182,269\nC T RAINING DETAILS\nC.1 L OW RESOURCE TRANSLATION\nWe build a joined dictionary for the source and target languages for each dataset. To facilitate this,\nwe use byte pair encoding (BPE) with 10,000 and 40,000 split operations for the IWSLT and the\nWMT datasets, respectively. Other pre-processing steps follow the Fairseq implementation.\nFor training, the regularization strength is chosen to be α = 5.0. We set the batch size to be\nequivalent to 32ktokens, i.e., if we have four GPUs, then we set the number of tokens on each GPU\nto be 4k and accumulate gradients for two steps. We use Adam as the optimizer with β1 = 0.9,\nβ2 = 0.98, and we set the learning rate to be 0.0015. We train the model for 40ksteps, and we test\nthe model that yield the highest validation BLEU. For validation and testing, we use a beam size 5\nand a length penalty 1.0. Other training and inference details follow the Fairseq implementation.\nC.2 R ICH RESOURCE TRANSLATION\nStrength of the consistency regularizer is set asα= 2.0. We use Adam (Kingma & Ba, 2015) as the\noptimizer with β1 = 0.9, β2 = 0.98, and the learning rate is chosen as 0.001. For inference, we use\na beam size 4 and a length penalty 0.6 for En-De; we use a beam size 10 and a length penalty 1.0\nfor En-Fr. Other post-processing steps follow Ott et al. (2018). We report both the BLEU score and\nthe sacreBLEU score (Post, 2018), where the latter is a safer token-agnostic version of BLEU.\nC.3 M ULTILINGUAL TRANSLATION\nFor training, we set the batch size to be equivalent to 1.6 million tokens, e.g., 4096 tokens per GPU\nwith 24 GPUs, and we accumulate gradients for 16 steps. We use RAdam (Liu et al., 2020a) as the\noptimizer with parameters β1 = 0.9 and β2 = 0.98. The learning rate is set to be 0.05. Also, we set\nthe dropout ratio to be 0.1, and we use label smoothed cross entropy (Szegedy et al., 2016) with a\nsmoothing factor 0.1. The regularization strength is set to be α= 4.0. For inference, we use a beam\nsize 5 and a length penalty 1.0.\n6https://www.statmt.org/europarl/\n16\nPublished as a conference paper at ICLR 2022\nD A DDITIONAL EXPERIMENTS\nWe further test behavior of THOR and the Switch Transformer when we increase the number of\nexperts. To avoid overﬁtting, we use a small model ( Transformer-IWSLT) on the WMT’16 En-De\ntranslation dataset. In this experiment, the Transformer model has 48M parameters, models with 2,\n16, and 64 experts have 55M, 143M, and 456M parameters, respectively.\nFigure 14: Effects of the number of experts on WMT’16 En-De translation. Left: training perplexity\n(lower the better) with respect to wall-time (measured in GPU hours); Right: validation BLEU\n(higher the better) after training for 180 GPU hours with respect to the number of experts, where the\nsize of Transformer does not change.\nFigure 14 demonstrates the results. In Figure 14 (left), notice that the Switch Transformer trains\nfaster than the vanilla Transformer, and this scaling property is more signiﬁcant when we increase\nthe number of experts.\nFrom Figure 14 (right), we see that with 2 experts, the Switch Transformer behaves slightly worse\nthe vanilla Transformer in terms of validation BLEU. However, when we increase the number of\nexperts, performance of the Switch Transformer continues to improve and outperforms the vanilla\nTransformer with the same number of FLOPs. This indicates that in order for a sparsely activated\nmodel to outperform a densely activated one, we need to scale the former to contain much more\nparameters than the latter. Our observations are consistent with existing literature (Lepikhin et al.,\n2020; Fedus et al., 2021). For example, in Fedus et al. 2021, the sparsely activated Switch-base\noutperforms the densely activated T5-base using the same number of FLOPs. However, the former\nis more than 30 times larger (7.5 billion vs. 0.22 billion parameters).\nOur method is more parameter efﬁcient than the conventional methods. From Figure 14 (right), we\nsee that THOR signiﬁcantly outperforms the vanilla Transformer and the Switch Transformer even\nwith only 2 experts. Moreover, when we increase the number of experts, performance of THOR also\nimproves.\nWe also compare inference speed of Transformer, Switch Transformer, and THOR in Table 8. Note\nthat for THOR , we use the Dispatch(s) method in Table 5. Note that the inference speed of\nSwitch Transformer and THOR is slower than the vanilla Transformer because of the computation\nand communication overhead induced by input routing. Such an overhead is more noticeable when\nthe number of experts is large. We remark that in Fedus et al. 2021, the speed of Switch-base is\nabout half of T5-base (780 vs. 1600 samples per second).\nTable 8: Inference speed (tokens/second).\nTransformer Switch THOR\n# experts — 2 16 64 2 16 64\nSpeed 15.2k 15.0k 10.4k 7.4k 15.1k 10.6k 7.5k\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7815747857093811
    },
    {
      "name": "Transformer",
      "score": 0.7455039024353027
    },
    {
      "name": "Inference",
      "score": 0.6385725140571594
    },
    {
      "name": "Machine learning",
      "score": 0.5136953592300415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5068081021308899
    },
    {
      "name": "Machine translation",
      "score": 0.42388418316841125
    },
    {
      "name": "Engineering",
      "score": 0.09299024939537048
    },
    {
      "name": "Voltage",
      "score": 0.08380690217018127
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}