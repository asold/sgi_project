{
  "title": "LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases",
  "url": "https://openalex.org/W4406759335",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2256831224",
      "name": "Dylan Bouchard",
      "affiliations": [
        "CVS Health (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2733267378",
      "name": "Mohit Singh Chauhan",
      "affiliations": [
        "CVS Health (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2551322909",
      "name": "David Skarbrevik",
      "affiliations": [
        "CVS Health (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2748322985",
      "name": "Viren Bajaj",
      "affiliations": [
        "CVS Health (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3203121954",
      "name": "Zeya Ahmad",
      "affiliations": [
        "CVS Health (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4301655857",
    "https://openalex.org/W2796868841",
    "https://openalex.org/W2920114910",
    "https://openalex.org/W4286982865",
    "https://openalex.org/W4287813526",
    "https://openalex.org/W4287630200",
    "https://openalex.org/W4287658197",
    "https://openalex.org/W4382490931",
    "https://openalex.org/W4287123711",
    "https://openalex.org/W4281754178",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3123340107",
    "https://openalex.org/W4381586491",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W3105042180",
    "https://openalex.org/W4327909986",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4381586841",
    "https://openalex.org/W4391713785",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W6893007900",
    "https://openalex.org/W4390833061",
    "https://openalex.org/W4289438483",
    "https://openalex.org/W6851421509",
    "https://openalex.org/W4289293239",
    "https://openalex.org/W2956281901",
    "https://openalex.org/W3048991518",
    "https://openalex.org/W4400720077",
    "https://openalex.org/W4389072548",
    "https://openalex.org/W4386499643",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W4386730022",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W4287887133",
    "https://openalex.org/W3106489865",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W4361806824"
  ],
  "abstract": null,
  "full_text": "LangFair: A Python Package for Assessing Bias and\nFairness in Large Language Model Use Cases\nDylan Bouchard 1, Mohit Singh Chauhan1, David Skarbrevik1, Viren\nBajaj 1, and Zeya Ahmad1\n1CVS Health Corporation\nDOI:10.21105/joss.07570\nSoftware\n• Review\n• Repository\n• Archive\nEditor: Chris Vernon\nReviewers:\n• @xavieryao\n• @emily-sexton\nSubmitted: 03 December 2024\nPublished: 23 January 2025\nLicense\nAuthors of papers retain copyright\nand release the work under a\nCreative Commons Attribution 4.0\nInternational License (CC BY 4.0).\nSummary\nLarge Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially\ncreating or worsening outcomes for specific groups identified by protected attributes such as\nsex, race, sexual orientation, or age. To help address this gap, we introducelangfair, an\nopen-source Python package that aims to equip LLM practitioners with the tools to evaluate\nbias and fairness risks relevant to their specific use cases.1 The package offers functionality to\neasily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts,\nand subsequently calculate applicable metrics for the practitioner’s use case. To guide in metric\nselection, LangFair offers an actionable decision framework, discussed in detail in the project’s\ncompanion paper, Bouchard (2024).\nStatement of Need\nTraditional machine learning (ML) fairness toolkits like AIF360 (Bellamy et al., 2018), Fairlearn\n(Weerts et al., 2023), Aequitas (Saleiro et al., 2018) and others (Tensorflow, 2020; Vasudevan\n& Kenthapadi, 2020; Wexler et al., 2019) have laid crucial groundwork. These toolkits offer\nvarious metrics and algorithms that focus on assessing and mitigating bias and fairness through\ndifferent stages of the ML lifecycle. While the fairness assessments offered by these toolkits\ninclude a wide variety of generic fairness metrics, which can also apply to certain LLM use\ncases, they are not tailored to the generative and context-dependent nature of LLMs.2\nLLMs are used in systems that solve tasks such as recommendation, classification, text\ngeneration, and summarization. In practice, these systems try to restrict the responses of the\nLLM to the task at hand, often by including task-specific instructions in system or user prompts.\nWhen the LLM is evaluated without taking the set of task-specific prompts into account, the\nevaluation metrics are not representative of the system’s true performance. Representing the\nsystem’s actual performance is especially important when evaluating its outputs for bias and\nfairness risks because they pose real harm to the user and, by way of repercussions, the system\ndeveloper.\nMost evaluation tools, including those that assess bias and fairness risk, evaluate LLMs at the\nmodel-level by calculating metrics based on the responses of the LLMs to static benchmark\ndatasets of prompts (Barikeri et al., 2021; Bartl et al., 2020; Dhamala et al., 2021; Felkner\net al., 2024; Gehman et al., 2020; Y. Huang et al., 2023; Kiritchenko & Mohammad, 2018;\nKrieg et al., 2023; Levy et al., 2021; Li et al., 2020; Nadeem et al., 2020; Nangia et al., 2020;\nNozza et al., 2021; Parrish et al., 2022; Qian et al., 2022; Rudinger et al., 2018; Webster et al.,\n1The repository for langfair can be found at https://github.com/cvs-health/langfair.\n2The toolkits mentioned here offer fairness metrics for classification. In a similar vein, the recommendation\nfairness metrics offered in FaiRLLM ( Zhang et al., 2023 ) can be applied to ML recommendation systems as well\nas LLM recommendation use cases.\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n1\n2018; Zhao et al., 2018) that do not consider prompt-specific risks and are often independent\nof the task at hand. Holistic Evaluation of Language Models (HELM) (Liang et al., 2023),\nDecodingTrust (Wang et al., 2023), and several other toolkits (Gao et al., 2024; Y. Huang et\nal., 2024; Huggingface, 2022; Nazir et al., 2024; Srivastava et al., 2022) follow this paradigm.\nLangFair complements the aforementioned frameworks because it follows a bring your own\nprompts (BYOP) approach, which allows users to tailor the bias and fairness evaluation to their\nuse case by computing metrics using LLM responses to user-provided prompts. This addresses\nthe need for a task-based bias and fairness evaluation tool that accounts for prompt-specific\nrisk for LLMs.3\nFurthermore, LangFair is designed for real-world LLM-based systems that require governance\naudits. LangFair focuses on calculating metrics from LLM responses only, which is more\npractical for real-world testing where access to internal states of model to retrieve embeddings\nor token probabilities is difficult. An added benefit is that output-based metrics, which are\nfocused on the downstream task, have shown to be potentially more reliable than metrics\nderived from embeddings or token probabilities (Delobelle et al., 2022; Goldfarb-Tarrant et al.,\n2021).\nGeneration of Evaluation Datasets\nThe langfair.generatormodule offers two classes,ResponseGeneratorand Counterfactual-\nGenerator, which aim to enable user-friendly construction of evaluation datasets for text\ngeneration use cases.\nResponseGenerator class\nTo streamline generation of evaluation datasets, theResponseGeneratorclass wraps an instance\nof a langchain LLM and leverages asynchronous generation withasyncio. To implement,\nusers simply pass a list of prompts (strings) to theResponseGenerator.generate_responses\nmethod, which returns a dictionary containing prompts, responses, and applicable metadata.\nCounterfactualGenerator class\nIn the context of LLMs, counterfactual fairness can be assessed by constructing counterfactual\ninput pairs (Bouchard, 2024; Gallegos et al., 2024), comprised of prompt pairs that mention\ndifferent protected attribute groups but are otherwise identical, and measuring the differences\nin the corresponding generated output pairs. These assessments are applicable to use cases\nthat do not satisfy fairness through unawareness (FTU), meaning prompts contain mentions\nof protected attribute groups. To address this, theCounterfactualGenerator class offers\nfunctionality to check for FTU, construct counterfactual input pairs, and generate corresponding\npairs of responses asynchronously using alangchain LLM instance.4 Off the shelf, the FTU\ncheck and creation of counterfactual input pairs can be done for gender and race/ethnicity,\nbut users may also provide a custom mapping of protected attribute words to enable this\nfunctionality for other attributes as well.\nBias and Fairness Evaluations for Focused Use Cases\nFollowing Bouchard (2024), evaluation metrics are categorized according to the risks they\nassess (toxicity, stereotypes, counterfactual unfairness, and allocational harms), as well as\n3Experiments in Wang et al. ( 2023) demonstrate that prompt content has substantial influence on the\nlikelihood of biased LLM responses.\n4In practice, a FTU check consists of parsing use case prompts for mentions of protected attribute groups.\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n2\nthe use case task (text generation, classification, and recommendation).5 Table 1 maps the\nclasses contained in thelangfair.metrics module to these risks. These classes are discussed\nin detail below.\nClass Risk Assessed Applicable Tasks\nToxicityMetrics Toxicity Text generation\nStereotypeMetrics Stereotypes Text generation\nCounterfactualMetrics Counterfactual fairness Text generation\nRecommendationMetrics Counterfactual fairness Recommendation\nClassificationMetrics Allocational harms Classification\nTable 1: Classes for Computing Evaluation Metrics in langfair.metrics\nToxicity Metrics\nThe ToxicityMetrics class facilitates simple computation of toxicity metrics from a user-\nprovided list of LLM responses. These metrics leverage a pre-trained toxicity classifier that\nmaps a text input to a toxicity score ranging from 0 to 1 (Gehman et al., 2020; Liang et al.,\n2023). For off-the-shelf toxicity classifiers, theToxicityMetrics class provides four options:\ntwo classifiers from thedetoxify package, roberta-hate-speech-dynabench-r4-target from\nthe evaluate package, andtoxigen available on HuggingFace.6 For additional flexibility, users\ncan specify an ensemble of the off-the-shelf classifiers offered or provide a custom toxicity\nclassifier object.\nStereotype Metrics\nTo measure stereotypes in LLM responses, theStereotypeMetrics class offers two categories\nof metrics: metrics based on word cooccurrences and metrics that leverage a pre-trained\nstereotype classifier. Metrics based on word cooccurrences aim to assess relative cooccur-\nrence of stereotypical words with certain protected attribute words. On the other hand,\nstereotype-classifier-based metrics leverage thewu981526092/Sentence-Level-Stereotype-\nDetector classifier available on HuggingFace (Zekun et al., 2023) and compute analogs of the\naforementioned toxicity-classifier-based metrics (Bouchard, 2024).7\nCounterfactual Fairness Metrics for Text Generation\nThe CounterfactualMetrics class offers two groups of metrics to assess counterfactual fairness\nin text generation use cases. The first group of metrics leverage a pre-trained sentiment classifier\nto measure sentiment disparities in counterfactually generated outputs (see P.-S. Huang et\nal. (2020) for further details). This class uses thevaderSentiment classifier by default but\nalso gives users the option to provide a custom sentiment classifier object.8 The second group\nof metrics addresses a stricter desiderata and measures overall similarity in counterfactually\ngenerated outputs using well-established text similarity metrics (Bouchard, 2024).\nCounterfactual Fairness Metrics for Recommendation\nThe RecommendationMetrics class is designed to assess counterfactual fairness for recommen-\ndation use cases. Specifically, these metrics measure similarity in generated lists of recommen-\ndations from counterfactual input pairs. Metrics may be computed pairwise (Bouchard, 2024),\nor attribute-wise (Zhang et al., 2023).\n5Note that text generation encompasses all use cases for which output is text, but does not belong to a\npredefined set of elements (as with classification and recommendation).\n6https://github.com/unitaryai/detoxify; https://github.com/huggingface/evaluate; https://github.com/mi-\ncrosoft/TOXIGEN\n7https://huggingface.co/wu981526092/Sentence-Level-Stereotype-Detector\n8https://github.com/cjhutto/vaderSentiment\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n3\nFairness Metrics for Classification\nWhen LLMs are used to solve classification problems, traditional machine learning fairness\nmetrics may be applied, provided that inputs can be mapped to a protected attribute. To this\nend, theClassificationMetrics class offers a suite of metrics to address unfair classification\nby measuring disparities in predicted prevalence, false negatives, or false positives. When\ncomputing metrics using theClassificationMetrics class, the user may specify whether to\ncompute these metrics as pairwise differences (Bellamy et al., 2018) or pairwise ratios (Saleiro\net al., 2018).\nSemi-Automated Evaluation\nAutoEval class\nTo streamline assessments for text generation use cases, theAutoEval class conducts a multi-\nstep process (each step is described in detail above) for a comprehensive fairness assessment.\nSpecifically, these steps include metric selection (based on whether FTU is satsified), evaluation\ndataset generation from user-provided prompts with a user-provided LLM, and computation\nof applicable fairness metrics. To implement, the user is required to supply a list of prompts\nand an instance oflangchain LLM. Below we provide a basic example demonstrating the\nexecution ofAutoEval.evaluate with agemini-pro instance.9\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langfair.auto import AutoEval\nllm = ChatVertexAI(model_name='gemini-pro')\nauto_object = AutoEval(prompts=prompts, langchain_llm=llm)\nresults = await auto_object.evaluate()\nUnder the hood, the AutoEval.evaluate method 1) checks for FTU, 2) generates\nresponses and counterfactual responses (if FTU is not satisfied), and 3) calculates\napplicable metrics for the use case. 10 This process flow is depicted in Figure 1.\nFigure 1:Flowchart of internal design of Autoeval.evaluate method\n9Note that this example assumes the user has already set up their VertexAI credentials and sampled a list of\nprompts from their use case prompts.\n10The ‘AutoEval‘ class is designed specifically for text generation use cases. Applicable metrics include toxicity\nmetrics, stereotype metrics, and, if FTU is not satisfied, counterfactual fairness metrics.\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n4\nAuthor Contributions\nDylan Bouchard was the principal developer and researcher of the LangFair project, responsible\nfor conceptualization, methodology, and software development of thelangfair library. Mohit\nSingh Chauhan was the architect behind the structural design of thelangfairlibrary and helped\nlead the software development efforts. David Skarbrevik was the primary author of LangFair’s\ndocumentation, helped implement software engineering best practices, and contributed to\nsoftware development. Viren Bajaj wrote unit tests, contributed to the software development,\nand helped implement software engineering best practices. Zeya Ahmad contributed to the\nsoftware development.\nAcknowledgements\nWe wish to thank Piero Ferrante, Blake Aber, Xue (Crystal) Gu, and Zirui Xu for their helpful\nsuggestions.\nReferences\nBarikeri, S., Lauscher, A., Vulić, I., & Glavaš, G. (2021).RedditBias: A real-world resource\nfor bias evaluation and debiasing of conversational language models . https://doi.org/10.\n48550/arXiv.2106.03521\nBartl, M., Nissim, M., & Gatt, A. (2020). Unmasking contextual stereotypes: Measuring and\nmitigating BERT’s gender bias. In M. R. Costa-jussà, C. Hardmeier, K. Webster, & W.\nRadford (Eds.),Proceedings of the second workshop on gender bias in natural language\nprocessing. https://doi.org/10.48550/arXiv.2010.14534\nBellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P.,\nMartino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha,\nD., Sattigeri, P., Singh, M., Varshney, K. R., & Zhang, Y. (2018).AI Fairness 360: An\nextensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias .\nhttps://doi.org/10.48550/arXiv.1810.01943\nBouchard, D. (2024).An actionable framework for assessing bias and fairness in large language\nmodel use cases . https://doi.org/10.48550/arXiv.2407.10853\nDelobelle, P., Tokpo, E., Calders, T., & Berendt, B. (2022). Measuring fairness with biased\nrulers: A comparative study on bias metrics for pre-trained language models. In M. Carpuat,\nM.-C. de Marneffe, & I. V. Meza Ruiz (Eds.),Proceedings of the 2022 conference of\nthe North American chapter of the association for computational linguistics: Human\nlanguage technologies (pp. 1693–1706). Association for Computational Linguistics.https:\n//doi.org/10.18653/v1/2022.naacl-main.122\nDhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-W., & Gupta,\nR. (2021). BOLD: Dataset and metrics for measuring biases in open-ended language\ngeneration. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency, 862–872. https://doi.org/10.1145/3442188.3445924\nFelkner, V. K., Chang, H.-C. H., Jang, E., & May, J. (2024).WinoQueer: A community-in-\nthe-loop benchmark for anti-LGBTQ+ bias in large language models . https://doi.org/10.\n48550/arXiv.2306.15087\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T.,\nZhang, R., & Ahmed, N. K. (2024).Bias and fairness in large language models: A survey .\nhttps://doi.org/10.48550/arXiv.2309.00770\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L.,\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n5\nHsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang,\nJ., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., … Zou, A. (2024). A\nframework for few-shot language model evaluation (Version v0.4.3). Zenodo. https:\n//doi.org/10.5281/zenodo.12608602\nGehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts:\nEvaluating neural toxic degeneration in language models.Findings. https://doi.org/10.\n18653/v1/2020.findings-emnlp.301\nGoldfarb-Tarrant, S., Marchant, R., Sanchez, R. M., Pandya, M., & Lopez, A. (2021).Intrinsic\nbias metrics do not correlate with application bias . https://doi.org/10.18653/v1/2021.\nacl-long.150\nHuang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama,\nD., & Kohli, P. (2020).Reducing sentiment bias in language models via counterfactual\nevaluation. https://doi.org/10.18653/v1/2020.findings-emnlp.7\nHuang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y., Gao, C., Huang, Y., Lyu, W., Zhang,\nY., Li, X., Sun, H., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Vidgen, B., Kailkhura, B., Xiong,\nC., … Zhao, Y. (2024). TrustLLM: Trustworthiness in large language models.Forty-First\nInternational Conference on Machine Learning . https://doi.org/10.48550/arXiv.2401.05561\nHuang, Y., Zhang, Q., Y, P. S., & Sun, L. (2023).TrustGPT: A benchmark for trustworthy\nand responsible large language models . https://doi.org/10.48550/arXiv.2306.11507\nHuggingface. (2022). GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating\nmachine learning models and datasets. https://github.com/huggingface/evaluate\nKiritchenko, S., & Mohammad, S. M. (2018).Examining gender and race bias in two hundred\nsentiment analysis systems . https://doi.org/10.18653/v1/S18-2005\nKrieg, K., Parada-Cabaleiro, E., Medicus, G., Lesota, O., Schedl, M., & Rekabsaz, N. (2023).\nGrep-BiasIR: A dataset for investigating gender representation bias in information retrieval\nresults. Proceedings of the 2023 Conference on Human Information Interaction and\nRetrieval, 444–448. https://doi.org/10.1145/3576840.3578295\nLevy, S., Lazar, K., & Stanovsky, G. (2021).Collecting a large-scale gender bias dataset for\ncoreference resolution and machine translation. https://doi.org/10.48550/arXiv.2109.03858\nLi, T., Khashabi, D., Khot, T., Sabharwal, A., & Srikumar, V. (2020). UNQOVERing stereo-\ntyping biases via underspecified questions. In T. Cohn, Y. He, & Y. Liu (Eds.),Findings of\nthe association for computational linguistics: EMNLP 2020 (pp. 3475–3489). Association\nfor Computational Linguistics.https://doi.org/10.18653/v1/2020.findings-emnlp.311\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan,\nD., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning,\nC. D., Ré, C., Acosta-Navas, D., Hudson, D. A., … Koreeda, Y. (2023).Holistic evaluation\nof language models . https://doi.org/10.48550/arXiv.2211.09110\nNadeem, M., Bethke, A., & Reddy, S. (2020).StereoSet: Measuring stereotypical bias in\npretrained language models . https://doi.org/10.48550/arXiv.2004.09456\nNangia, N., Vania, C., Bhalerao, R., & Bowman, S. R. (2020, November). CrowS-Pairs: A\nChallenge Dataset for Measuring Social Biases in Masked Language Models.Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing . https:\n//doi.org/10.48550/arXiv.2010.00133\nNazir, A., Chakravarthy, T. K., Cecchini, D. A., Chakravarthy, T. K., Khajuria, R., Sharma, P.,\nMirik, A. T., Kocaman, V., & Talby, D. (2024). LangTest: A comprehensive evaluation\nlibrary for custom LLM and NLP models.Software Impacts, 19(100619). https://doi.org/\n10.1016/j.simpa.2024.100619\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n6\nNozza, D., Bianchi, F., & Hovy, D. (2021). ”HONEST: Measuring hurtful sentence completion\nin language models”.Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies , 2398–2406.\nhttps://doi.org/10.18653/v1/2021.naacl-main.191\nParrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M.,\n& Bowman, S. (2022). BBQ: A hand-built bias benchmark for question answering. In S.\nMuresan, P. Nakov, & A. Villavicencio (Eds.),Findings of the association for computational\nlinguistics: ACL 2022 (pp. 2086–2105). Association for Computational Linguistics.\nhttps://doi.org/10.18653/v1/2022.findings-acl.165\nQian, R., Ross, C., Fernandes, J., Smith, E., Kiela, D., & Williams, A. (2022).Perturbation\naugmentation for fairer NLP . https://doi.org/10.48550/arXiv.2205.12586\nRudinger, R., Naradowsky, J., Leonard, B., & Van Durme, B. (2018). Gender bias in\ncoreference resolution. Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies . https:\n//doi.org/10.48550/arXiv.1804.09301\nSaleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London, J., & Ghani, R.\n(2018). Aequitas: A bias and fairness audit toolkit. arXiv Preprint arXiv:1811.05577 .\nhttps://doi.org/10.48550/arXiv.1811.05577\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R.,\nSantoro, A., Gupta, A., Garriga-Alonso, A., & others. (2022). Beyond the imitation\ngame: Quantifying and extrapolating the capabilities of language models.arXiv Preprint\narXiv:2206.04615. https://doi.org/10.48550/arXiv.2206.04615\nTensorflow. (2020). GitHub - tensorflow/fairness-indicators: Tensorflow’s Fairness Evaluation\nand Visualization Toolkit . https://github.com/tensorflow/fairness-indicators\nVasudevan, S., & Kenthapadi, K. (2020). LiFT: A scalable framework for measuring fairness\nin ML applications.Proceedings of the 29th ACM International Conference on Information\nand Knowledge Management . https://doi.org/10.1145/3340531.3412705\nWang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta,\nR., Schaeffer, R., & others. (2023). DecodingTrust: A comprehensive assessment of\ntrustworthiness in GPT models . https://doi.org/10.48550/arXiv.2306.11698\nWebster, K., Recasens, M., Axelrod, V., & Baldridge, J. (2018). Mind the GAP: A balanced\ncorpus of gendered ambiguous pronouns.Transactions of the Association for Computational\nLinguistics, 6, 605–617. https://doi.org/10.1162/tacl_a_00240\nWeerts, H., Dudík, M., Edgar, R., Jalali, A., Lutz, R., & Madaio, M. (2023). Fairlearn:\nAssessing and Improving Fairness of AI Systems.Journal of Machine Learning Research ,\n24. http://jmlr.org/papers/v24/23-0389.html\nWexler, J., Pushkarna, M., Bolukbasi, T., Wattenberg, M., Viégas, F. B., & Wilson, J. (2019).\nThe what-if tool: Interactive probing of machine learning models.CoRR, abs/1907.04135.\nhttps://doi.org/10.1109/TVCG.2019.2934619\nZekun, W., Bulathwela, S., & Koshiyama, A. S. (2023).Towards auditing large language models:\nImproving text-based stereotype detection . https://doi.org/10.48550/arXiv.2311.14126\nZhang, J., Bao, K., Zhang, Y., Wang, W., Feng, F., & He, X. (2023). Is ChatGPT\nfair for recommendation? Evaluating fairness in large language model recommendation.\nProceedings of the 17th ACM Conference on Recommender Systems , 2012, 993–999.\nhttps://doi.org/10.1145/3604915.3608860\nZhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K.-W. (2018).Gender Bias in\nCoreference Resolution: Evaluation and Debiasing methods . https://doi.org/10.48550/\narXiv.1804.06876\nBouchard et al. (2025). LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases. Journal of Open Source\nSoftware, 10(105), 7570. https://doi.org/10.21105/joss.07570.\n7",
  "topic": "Python (programming language)",
  "concepts": [
    {
      "name": "Python (programming language)",
      "score": 0.902143120765686
    },
    {
      "name": "R package",
      "score": 0.7008584141731262
    },
    {
      "name": "Computer science",
      "score": 0.6507072448730469
    },
    {
      "name": "Programming language",
      "score": 0.6197510957717896
    },
    {
      "name": "Econometrics",
      "score": 0.3420674204826355
    },
    {
      "name": "Mathematics",
      "score": 0.19551333785057068
    }
  ]
}