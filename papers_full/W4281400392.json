{
  "title": "Self-supervised 3D Anatomy Segmentation Using Self-distilled Masked Image Transformer (SMIT)",
  "url": "https://openalex.org/W4281400392",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5088189613",
      "name": "Jue Jiang",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5037462244",
      "name": "Neelam Tyagi",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5058490970",
      "name": "Kathryn R. Tringale",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5016694714",
      "name": "Christopher H. Crane",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5014597008",
      "name": "Harini Veeraraghavan",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3016836174",
    "https://openalex.org/W3093049763",
    "https://openalex.org/W2964744899",
    "https://openalex.org/W3202334605",
    "https://openalex.org/W6778672394",
    "https://openalex.org/W3088389048",
    "https://openalex.org/W3196792593",
    "https://openalex.org/W3033671339",
    "https://openalex.org/W6677952666",
    "https://openalex.org/W3192125374",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W6605755270",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2998544007",
    "https://openalex.org/W3215119337",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6977944434",
    "https://openalex.org/W3049757379",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3014974815",
    "https://openalex.org/W4238070344",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W3206810794",
    "https://openalex.org/W4229045026",
    "https://openalex.org/W3112701542"
  ],
  "abstract": null,
  "full_text": "Self-supervised 3D anatomy segmentation using\nself-distilled masked image transformer (SMIT)\nJue Jiang1, Neelam Tyagi1, Kathryn Tringale2, Christopher Crane2, and\nHarini Veeraraghavan1\n1 1 Department of Medical Physics, Memorial Sloan Kettering Cancer Center\n2 2 Department of Radiation Oncology, Memorial Sloan Kettering Cancer Center\nveerarah@mskcc.org\nAbstract. Vision transformers, with their ability to more eﬃciently\nmodel long-range context, have demonstrated impressive accuracy gains\nin several computer vision and medical image analysis tasks includ-\ning segmentation. However, such methods need large labeled datasets\nfor training, which is hard to obtain for medical image analysis. Self-\nsupervised learning (SSL) has demonstrated success in medical image\nsegmentation using convolutional networks. In this work, we developed a\nself-distillation learning with masked image modeling method to perform\nSSL for vision transformers (SMIT) applied to 3D multi-organ segmenta-\ntion from CT and MRI. Our contribution is a dense pixel-wise regression\nwithin masked patches called masked image prediction, which we com-\nbined with masked patch token distillation as pretext task to pre-train\nvision transformers. We show our approach is more accurate and requires\nfewer ﬁne tuning datasets than other pretext tasks. Unlike prior medi-\ncal image methods, which typically used image sets arising from disease\nsites and imaging modalities corresponding to the target tasks, we used\n3,643 CT scans (602,708 images) arising from head and neck, lung, and\nkidney cancers as well as COVID-19 for pre-training and applied it to\nabdominal organs segmentation from MRI pancreatic cancer patients\nas well as publicly available 13 diﬀerent abdominal organs segmenta-\ntion from CT. Our method showed clear accuracy improvement (average\nDSC of 0.875 from MRI and 0.878 from CT) with reduced requirement\nfor ﬁne-tuning datasets over commonly used pretext tasks. Extensive\ncomparisons against multiple current SSL methods were done. Code will\nbe made available upon acceptance for publication. 3\nKeywords: Self-supervised learning, segmentation, self-distillation, masked\nimage modeling, masked embedding transformer\n1 Introduction\nVision transformers (ViT)[1] eﬃciently model long range contextual information\nusing multi-head self attention mechanism, which makes them robust to occlu-\nsions, image noise, as well as domain and image contrast diﬀerences. ViTs have\n3 This paper has been early accepted by MICCAI 2022.\narXiv:2205.10342v1  [eess.IV]  20 May 2022\nshown impressive accuracy gains over convolutional neural networks (CNN) in\nmedical image segmentation[2,3]. However, ViT training requires a large number\nof expert labeled training datasets that are not commonly available in medical\nimage applications. Self-supervised learning (SSL) overcomes the requirement\nfor large labeled training datasets by using large unlabeled datasets through\npre-deﬁned annotation free pretext tasks. The pretext tasks are based on model-\ning visual information contained in images and provide a surrogate supervision\nsignal for feature learning[4,5,6]. Once pre-trained, the model can be re-purposed\nfor a variety of tasks and require relatively few labeled sets for ﬁne-tuning.\nThe choice of pretext tasks is crucial to successfully mine useful image infor-\nmation using SSL. Pretext tasks in medical image applications typically focus\non learning denoising autoencoders constructed with CNNs to recover images\nin the input space using corrupted versions of the original images[7,8]. Various\ndata augmentation strategies have been used to corrupt images, which include\njigsaw puzzles[9,10], transformation of image contrast and local texture[7], image\nrotations[5], and masking of whole image slices[11]. Learning strategies include\npseudo labels[8,12,13] and contrastive learning[9,14,15,16]. However, CNNs are\ninherently limited in their capacity to model long-range context than transform-\ners, which may reduce their robustness to imaging variations and contrast diﬀer-\nences. Hence, we combined ViT with SSL using masked image modeling (MIM)\nand self-distillation of concurrently trained teacher and student networks.\nMIM has been successfully applied to transformers to capture local context while\npreserving global semantics in natural image analysis tasks[17,18,19,20,19,21].\nKnowledge distillation with concurrently trained teacher has also been used for\nmedical image segmentation by leveraging diﬀerent imaging modality datasets\n(CT and MRI)[22,23]. Self-distillation on the other hand, uses diﬀerent aug-\nmented views of the same image[24] and has been used with contrastive learning\nwith convolutional encoders for medical image classiﬁcation[13].\nSelf-distillation learning with MIM and using a pair of online teacher and a\nstudent transformer encoders have been used for natural image classiﬁcation\nand segmentation[19,24]. However, the pretext tasks focused only on extract-\ning global image embedding as class tokens [CLS][24], which was improved with\nglobal and local patch token embeddings [19]. However, these methods ignored\nthe dense pixel dependencies, which is essential for dense prediction tasks like\nsegmentation. Hence, we introduced a masked image prediction (MIP) pretext\ntask to predict pixel-wise intensities within masked patches combined with the\nlocal and global embedding distillation applied to medical image segmentation.\nOur contributions include: (i) SSL using MIM and self-distillation approach com-\nbining masked image prediction, masked patch token distillation, and global im-\nage token distillation for CT and MRI organs segmentation using transformers.\n(ii) a simple linear projection layer for medical image reconstruction to speed\nup pre-training, which we show is more accurate than multi-layer decoder. (iii)\nSSL pre-training using large 3,643 3D CTs arising from a variety of disease\nsites including head and neck, chest, and abdomen with diﬀerent cancers (lung,\nnaso/oropharynx, kidney) and COVID-19 applied to CT and MRI segmentation.\nTransformer\nEncoder\nTransformer\nEncoder\nMasked Patch Embedding\nPatch Embedding\nstop grad\nstop grad\nRand Patch \nMask\nStudent\nTeacher\nLinear ProjectionLinear Projection\nEMA\nview 1\nview 2\nStudent [CLS] Token of \nStudent Patch Token of\nTeacher [CLS] Token of\nTeacher Patch Token of\nTeacher [CLS] Token of\nTeacher Patch Token of\nH\nW D\nH\nW D\n \nStep 1: \nSelf-supervised  \nPre-training\n \nStep 2: \nFine-tuning \nCT  \nabdomen\nMRI  \nabdomen\nFig. 1: SMIT: Self-distillation with masked image modeling for transformers using SSL.\n(iv) Evaluation of various pretext tasks using transformer encoders related to\nﬁne tuning data size requirements and segmentation accuracy.\n2 Method\nGoal: Extract a universal representation of images for dense prediction tasks,\ngiven an unlabeled dataset of Q images.\nApproach: A visual tokenizer fs(θs) implemented as a transformer encoder\nis learned via self-distillation using MIM pretext tasks in order to convert an\nimage xinto image tokens {xi}N\ni=1, N being the sequence length. Self distillation\nis performed by concurrently training an online teacher tokenizer model ft(θt)\nwith the same network structure asfs(θs) serving as the student model. A global\nimage token distillation (ITD) is performed as a pretext task to match the global\ntokens extracted by ft and fs as done previously[24]. MIM pretext tasks include\nmasked image prediction (MIP) and masked patch token distillation (MPD).\nSuppose {u,v}are two augmented views of a 3D image x. N image patches\nare extracted from the images to create a sequence of image tokens[1], say u=\n{ui}N\ni=1. The image tokens are then corrupted by randomly masking image tokens\nbased on a binary vector m = {mi}N\ni=1 ∈{0,1}with a probability p and then\nreplacing with mask token[20]e[MASK ] such that as ˜u= m⊙uwith ˜ui = e[MASK ]\nat mi = 1 and ˜ui = ui at mi = 0. The second augmented view vis also corrupted\nbut using a diﬀerent mask vector instance m′ as ˜v= m′ ⊙v.\nDense pixel dependency modeling using MIP: MIP involves recovering\nthe original image view ufrom corrupted ˜u, as ˆu= hPred\ns (fs(˜u,θs)), where hPred\ns\ndecodes the visual tokens produced by a visual tokenizer fs(θs) into images (see\nFig.1). MIP involves dense pixel regression of image intensities within masked\npatches using the context of unmasked patches. The MIP loss is computed as\n(dotted green arrow in Fig.1):\nLMIP =\nN∑\ni\nE∥mi ·(hPred\ns (fs( ˜ui,θs))) −ui)∥1 (1)\nhPred\ns is a linear projection with one layer for dense pixel regression. A sym-\nmetrized loss using v and ˜v is combined to compute the total loss for LMIP .\nMasked patch token self-distillation (MPD): MPD is accomplished by\noptimizing a teacher ft(θt) and a student visual tokenizer fs(θs) such that the\nstudent network predicts the tokens of the teacher network. The student net-\nwork fs tokenizes the corrupted version of an image ˜uto generate visual tokens\nφ′ = {φ′\ni}N\ni=1. The teacher network ft tokenizes the uncorrupted version of the\nsame image u to generate visual tokens φ = {φi}N\ni=1. Similar to MIP, MPD is\nonly concerned with ensuring prediction of the masked patch tokens. Therefore,\nthe loss is computed from masked portions (i.e. mi=1) using cross-entropy of\nthe predicted patch tokens (dotted red arrow in Fig.1):\nLMPD = −\nN∑\ni=1\nmi ·PPatch\nt (ui,θt)log(PPatch\ns (˜ui,θs)), (2)\nwhere PPatch\ns and PPatch\nt are the patch token distributions for student and\nteacher networks. They are computed by applying softmax to the outputs of\nhPatch\ns and hPatch\nt . The sharpness of the token distribution is controlled using\na temperature term τs > 0 and τt > 0 for the student and teacher networks,\nrespectively. Mathematically, such a sharpening can expressed as (using notation\nfor the student network parameters) as:\nPPatch\ns (u,θs) = exp(hPatch\ns (fs(uj,θs))/τs\n∑K\nj=1 exp(hPatchs (fs(uj,θs))/τs\n. (3)\nA symmetrized cross entropy loss corresponding to the other view v and ˜v is\nalso computed and averaged to compute the total loss for MPD.\nGlobal image token self-distillation (ITD): ITD is done by matching the\nglobal image embedding represented as class tokens [CLS] distribution P[CLS]\ns\nextracted from the corrupted view ˜uby student using h[CLS]\ns (fs(θs,˜u)) with the\ntoken distribution P[CLS]\nt extracted from the uncorrupted and diﬀerent view v\nby the teacher using h[CLS]\nt (ft(θt,v)) (shown by dotted blue arrow in Fig.1) as:\nLITD = −\nN∑\ni=1\nmi ·P[CLS]\nt (vi,θt)log(P[CLS]\ns (˜ui,θs)) (4)\nSharpening transforms are applied to P[CLS]\nt and P[CLS]\ns similar to Equation 4.\nA symmetrized cross entropy loss corresponding to the corrupted view ˜ v and\nanother u is also computed and averaged to compute the total loss for LITD .\nOnline teacher network update: Teacher network parameters were updated\nusing exponential moving average (EMA) with momentum update, and shown\nto be feasible for SSL[24,19] as: θt = λmθt +(1 −λm)θs, where λm is momentum,\nwhich was updated using a cosine schedule from 0.996 to 1 during training. The\ntotal loss was, Ltotal = LMIP + λMPD LMPD + λITD LITD .\nImplementation details: All the networks were implemented using the Py-\ntorch library and trained on 4 Nvidia GTX V100. SSL optimization was done\nusing ADAMw with a cosine learning rate scheduler trained for 400 epochs with\nan initial learning rate of 0.0002 and warmup for 30 epochs. λMPD =0.1, λITD\n=0.1 were set experimentally. A default mask ratio of 0.7 was used. Centering\nand sharpening operations reduced chances of degenerate solutions[24]. τs was\nTable 1: Accuracy on BTCV standard challenge test set. SP: spleen, RK/LK: right &\nleft kidney, GB: gall bladder, ESO: esophagus, LV: liver, STO: stomach, AOR: aorta,\nIVC: inferior vena cava, SPV: portal & splenic vein, Pan: Pancreas, AG: Adrenals.\nMethod SP RK LK GB ESO LV STO AOR IVC SPV Pan AG AVG\nASPP[29] 0.935 0.892 0.914 0.689 0.760 0.953 0.812 0.918 0.807 0.695 0.720 0.629 0.811\nnnUnet[30] 0.942 0.894 0.910 0.704 0.723 0.948 0.824 0.877 0.782 0.720 0.680 0.616 0.802\nTrsUnet[31] 0.952 0.927 0.929 0.662 0.757 0.969 0.889 0.920 0.833 0.791 0.775 0.637 0.838\nCoTr[2] 0.958 0.921 0.936 0.700 0.764 0.963 0.854 0.920 0.838 0.787 0.775 0.694 0.844\nUNETR[3] 0.968 0.924 0.941 0.750 0.766 0.971 0.913 0.890 0.847 0.788 0.767 0.741 0.856\nSMIT(rand) 0.959 0.921 0.947 0.746 0.802 0.972 0.916 0.917 0.848 0.797 0.817 0.711 0.850\nSMIT(SSL) 0.967 0.945 0.948 0.826 0.822 0.976 0.934 0.921 0.864 0.827 0.851 0.754 0.878\nset to 0.1 and τt was linearly warmed up from 0.04 to 0.07 in the ﬁrst 30 epochs.\nSWIN-small backbone[25] with 768 embedding, window size of 4 ×4 ×4, patch\nsize of 2 was used. The 1-layer decoder was implemented with a linear projection\nlayer with the same number of output channels as input image size. The network\nhad 28.19M parameters. Following pre-training, only the student network was\nretained for ﬁne-tuning and testing.\n3 Experiments and Results\nTraining dataset: SSL pre-training was performed using 3,643 CT patient\nscans containing 602,708 images. Images were sourced from patients with head\nand neck (N=837) and lung cancers (N=1455) from internal and external[26],\nas well as those with kidney cancers[27] (N=710), and COVID-19[28] (N=650).\nGPU limitation was addressed for training, ﬁne-tuning, and testing by image\nresampling (1.5×1.5×2mm voxel size) and cropping (128 ×128×128) to enclose\nthe body region. Augmented views for SSL training was produced through ran-\ndomly cropped 96×96×96 volumes, which resulted in 6×6×6 image patch tokens.\nA sliding window strategy with half window overlap was used for testing[2,3].\nCT abdomen organ segmentation (Dataset I): The pre-trained networks were ﬁne-\ntuned to generate volumetric segmentation of 13 diﬀerent abdominal organs from\ncontrast-enhanced CT (CECT) scans using publicly available beyond the cranial\nvault (BTCV)[32] dataset. Randomly selected 21 images are used for training\nand the remaining used for validation. Furthermore, blinded testing of 20 CECTs\nevaluated on the grand challenge website is also reported.\nMRI upper abdominal organs segmentation (Dataset II): The SSL network was\nevaluated for segmenting abdominal organs at risk for pancreatic cancer radia-\ntion treatment, which included stomach, small and large bowel, liver, and kid-\nneys. No MRI or pancreatic cancer scans were used for SSL pre-training . Ninety\ntwo 3D T2-weighted MRIs (TR/TE = 1300/87 ms, voxel size of 1 ×1×2 mm3,\nFOV of 400 ×450×250 mm 3) and acquired with pnuematic compression belt\nto suppress breathing motion were analyzed. Fine tuning used ﬁve-fold cross-\nvalidation and results from the validation folds not used in training are reported.\nExperimental comparisons: SMIT was compared against representative SSL\nmedical image analysis methods. Results from representative published methods\non the BTCV testing set[30,2,3] are also reported. The SSL comparison methods\nliver AortaP&S veinsStomach Spleen Pancreas Left Adrenal Gall\nManual Label                              Rand ini.                    Model genesis                               3D CPC                         Cub++\nSMIT                                  PCRL                                    Dino                                    iBot SSIM\nliver Small bowelLarge BowelStomach Left Kidney Right Kidney\nCT\nCT\nMRI\nMRI\nManual Label                              Rand ini.                    Model genesis                               3D CPC                         Cub++ \nSMIT                                  PCRL                                    Dino                                    iBot SSIM  \nFig. 2: Segmentation performance of diﬀerent methods on MRI abdomen organs.\nwere chosen to evaluate the impact of the pretext task on segmentation accuracy\nand included (a) local texture and semantics modeling using model genesis[7],\n(b) jigsaw puzzles[10], (c) contrastive learning[16] with (a),(b), (c) implemented\non CNN backbone, (d) self-distillation using whole image reconstruction[24], (e)\nmasked patch reconstruction[18] without self-distillation, (f) MIM using self-\ndistillation[19] with (d),(e), and (f) implemented in a SWIN transformer back-\nbone. Random initialization results are shown for benchmarking purposes using\nboth CNN and SWIN backbones. Identical training and testing sets were used\nwith hyper-parameter adopted from their default implementation.\nCT segmentation accuracy: As shown in Table.1, our method SMIT outper-\nformed representative published methods including transformer-based segmentation[31,3,2].\nSMIT was also more accurate than all evaluated SSL methods (Table.2) for most\norgans. Prior-guided contrast learning (PRCL)[16] was more accurate than SMIT\nfor gall bladder (0.797 vs. 0.787). SMIT was more accurate than self-distillation\nwith MIM[19] (average DSC of 0.848 vs. 0.833) as well as masked image recon-\nstruction without distillation[18] (0.848 vs. 0.830). Fig.2 shows a representative\ncase with multiple organs segmentations produced by the various methods. SMIT\nwas the most accurate method including for organs with highly variable appear-\nance and size such as the stomach and esophagus.\nMRI segmentation accuracy: SMIT was more accurate than all other SSL-\nbased methods for all evaluated organs, including stomach and bowels, which\ndepict highly variable appearance and sizes (Table.2). SMIT was least accurate\nfor small bowel compared to other organs, albeit this accuracy for small bowel\nwas higher than all other methods. Fig.2 shows a representative case with mul-\ntiple organs segmentations produced by the various methods.\nAblation experiments: All ablation and design experiments (1layer decoder\nvs. multi-layer or ML decoder) were performed using the BTCV dataset and\nused the SWIN-backbone as used for SMIT. ML decoder was implemented with\nﬁve transpose convolution layers for up-sampling back to the input image resolu-\ntion. Fig.4 shows the accuracy comparisons of networks pre-trained with diﬀerent\ntasks including full image reconstruction, contrastive losses, pseudo labels[33],\n(a) BTCV CT Abdomen \n (c) Validation accuracy\n(b) Influence of Mask ratio\nFig. 3: (a) Impact of SSL task on ﬁne-tuning sizes, (b) impact of mask ratio on masked\nimage prediction and segmentation accuracy, (c) training convergence.\n（III） GB, RA, LA （IV) IVC,AOR,SPV\n（I） LV,SP,LK,RK （II）STO, PAN, ESO\nFull Image \nReconstruction\nITD\nITD+MIP+M\nPD (SMIT)\nFig. 4: Accuracy variations by organ types using diﬀerent pretext tasks.\nand various combination of the losses ( LMIP ,LMPD ,LITD ). As shown, the ac-\ncuracies for all the methods was similar for large organs depicting good contrast\nthat include liver, spleen, left and right kidney (Fig.4(I)). On the other hand,\norgans with low soft tissue contrast and high variability (Fig.4(II)) and small\norgans (Fig.4(III)) show larger diﬀerences in accuracies between methods with\nSMIT achieving more accurate segmentations. Major blood vessels Fig.4(IV) also\ndepict segmentation accuracy diﬀerences across methods, albeit less so than for\nsmall organs and those with low soft-tissue contrast. Importantly, both full image\nreconstruction and multi-layer decoder based MIP (ML-MIP) were less accurate\nthan SMIT, which uses masked image prediction with 1-layer linear projection\ndecoder (Fig.4 (II,III,IV)). MPD was the least accurate for organs with low soft-\ntissue contrast and high variability (Fig.4(II)), which was improved slightly by\nadding global image distillation (ITD). MIP alone (using 1-layer decoder) was\nsimilarly accurate as SMIT and more accurate than other pretext task based\nsegmentation including ITD[24], MPD+ITD[19].\nImpact of pretext tasks on sample size for ﬁne tuning: SMIT was more\naccurate than all other SSL methods irrespective of sample size used for ﬁne-\ntuning (Fig.3(a)) and achieved faster convergence (Fig.3(c)). It outperformed\niBot[19], which uses MPD and ITD, indicating eﬀectiveness of MIP for SSL.\nImpact of mask ratio on accuracy: Fig.3(b) shows the impact of mask\nratio (percentage of masked patches) in the corrupted image for both the accu-\nracy of masked image reconstruction (computed as mean square error [MSE]) as\nwell as segmentation (computed using DSC metric). Increasing the mask ratio\nCT\nMasked image Multi Layer 1 Layer Original Image\n Masked image Multi Layer 1 Layer Original Image\nCT MRI\nFig. 5: Reconstructed images using 1-layer vs. multi-layer decoder trained with SMIT\nfrom masked images (0.7 masking ratio).\ninitially increased accuracy and then stabilized. Image reconstruction error in-\ncreased slightly with increasing masking ratio. Fig.5 shows a representative CT\nand MRI reconstruction produced using default and multi-layer decoder, wherein\nour method was more accurate even in highly textured portions of the images\ncontaining multiple organs (additional examples are shown in Supplementary\nFig 1). Quantitative comparisons showed our method was more accurate (MSE\nof 0.061 vs. 0.32) for CT (N=10 cases) and 92 MRI (MSE of 0.062 vs. 0.34) than\nmulti-layer decoder.\nTable 2: CT and MRI segmentation accuracy comparisons to SSL methods.\nRand-random; LB-Large bowel, SB - Small bowel.\nMod Organ CNN SWIN\nRand MG[7] CPC[9] Cub++[34] PRCL[16] Rand DINO[24] iBOT[19] SSIM[18] SMIT\nCT\nSp 0.930 0.950 0.940 0.926 0.937 0.944 0.946 0.948 0.950 0.963\nRK 0.892 0.934 0.916 0.928 0.919 0.926 0.931 0.936 0.934 0.950\nLK 0.894 0.918 0.903 0.914 0.921 0.905 0.913 0.919 0.913 0.943\nGB 0.605 0.639 0.718 0.715 0.797 0.694 0.730 0.777 0.761 0.787\nESO 0.744 0.739 0.756 0.768 0.759 0.732 0.752 0.760 0.772 0.772\nLV 0.947 0.967 0.953 0.946 0.954 0.950 0.954 0.956 0.956 0.970\nSTO 0.862 0.879 0.896 0.881 0.877 0.861 0.891 0.900 0.898 0.903\nAOR 0.875 0.909 0.900 0.892 0.894 0.885 0.906 0.901 0.905 0.913\nIVC 0.844 0.882 0.855 0.866 0.851 0.851 0.866 0.879 0.867 0.871\nSPV 0.727 0.739 0.731 0.734 0.760 0.725 0.752 0.759 0.754 0.784\nPan 0.719 0.706 0.726 0.731 0.693 0.688 0.763 0.755 0.764 0.810\nRA 0.644 0.671 0.655 0.665 0.661 0.660 0.651 0.659 0.640 0.669\nLA 0.648 0.640 0.655 0.675 0.680 0.590 0.680 0.681 0.678 0.687\nAVG. 0.795 0.813 0.816 0.819 0.823 0.801 0.826 0.833 0.830 0.848\nMR\nLV 0.921 0.936 0.925 0.920 0.930 0.922 0.920 0.939 0.937 0.942\nLB 0.786 0.824 0.824 0.813 0.823 0.818 0.804 0.833 0.835 0.855\nSB 0.688 0.741 0.745 0.735 0.745 0.708 0.729 0.744 0.759 0.775\nSTO 0.702 0.745 0.769 0.783 0.793 0.732 0.750 0.783 0.775 0.812\nLK 0.827 0.832 0.876 0.866 0.876 0.837 0.911 0.883 0.874 0.936\nRK 0.866 0.886 0.863 0.861 0.871 0.845 0.896 0.906 0.871 0.930\nAVG. 0.798 0.827 0.834 0.830 0.840 0.810 0.835 0.848 0.842 0.875\n4 Discussion and conclusion\nIn this work, we demonstrated the potential for SSL with 3D transformers for\nmedical image segmentation. Our approach, which leverages CT volumes arising\nfrom highly disparate body locations and diseases showed feasibility to produce\nrobustly accurate segmentations from CT and MRI scans and surpassed multi-\nple current SSL-based methods, especially for hard to segment organs with high\nappearance variability and small sizes. Our introduced masked image dense pre-\ndiction pretext task improved the ability of self distillation using MIM to segment\na variety of organs from CT and MRI and with lower requirement of ﬁne tuning\ndataset size. Our method shows feasibility for medical image segmentation.\n5 Conclusion\nReferences\n1. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nInternational Conference on Learning Representations. (2021)\n2. Xie, Y., Zhang, J., Shen, C., Xia, Y.: Cotr: Eﬃciently bridging cnn and transformer\nfor 3d medical image segmentation. In: Medical Image Computing and Computer\nAssisted Intervention. (2021) 171–180\n3. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,\nRoth, H., Xu, D.: Unetr: Transformers for 3d medical image segmentation (2021)\n4. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\njigsaw puzzles. In: European conf. on computer vision, Springer (2016) 69–84\n5. Komodakis, N., Gidaris, S.: Unsupervised representation learning by predicting\nimage rotations. In: Intl Conf on Learning Representations (ICLR). (2018)\n6. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: Proceedings of the IEEE/CVF conference on\nCVPR. (2020) 9729–9738\n7. Zhou, Z., Sodha, V., Pang, J., Gotway, M.B., Liang, J.: Models genesis. Medical\nimage analysis 67 (2021) 101840\n8. Haghighi, F., Taher, M.R.H., Zhou, Z., Gotway, M.B., Liang, J.: Learning\nsemantics-enriched representation via self-discovery, self-classiﬁcation, and self-\nrestoration. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer (2020) 137–147\n9. Taleb, A., Loetzsch, W., Danz, N., Severin, J., Gaertner, T., Bergner, B., Lip-\npert, C.: 3d self-supervised methods for medical imaging. Advances in Neural\nInformation Processing Systems 33 (2020) 18158–18172\n10. Zhu, J., Li, Y., Hu, Y., Ma, K., Zhou, S.K., Zheng, Y.: Rubik’s cube+: A self-\nsupervised feature learning framework for 3d medical image analysis. Medical\nimage analysis 64 (2020) 101746\n11. Jun, E., Jeong, S., Heo, D.W., Suk, H.I.: Medical transformer: Universal brain\nencoder for 3d mri analysis. arXiv preprint arXiv:2104.13633 (2021)\n12. Chen, L., Bentley, P., Mori, K., Misawa, K., Fujiwara, M., Rueckert, D.: Self-\nsupervised learning for medical image analysis using image context restoration.\nMedical image analysis 58 (2019) 101539\n13. Sun, J., Wei, D., Ma, K., Wang, L., Zheng, Y.: Unsupervised representation learn-\ning meets pseudo-label supervised self-distillation: A new approach to rare dis-\nease classiﬁcation. In: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer (2021) 519–529\n14. Chaitanya, K., Erdil, E., Karani, N., Konukoglu, E.: Contrastive learning of global\nand local features for medical image segmentation with limited annotations. Ad-\nvances in Neural Information Processing Systems 33 (2020) 12546–12558\n15. Feng, R., Zhou, Z., Gotway, M.B., Liang, J.: Parts2whole: Self-supervised con-\ntrastive learning via reconstruction. In: Domain Adaptation and Representation\nTransfer, and Distributed and Collaborative Learning. Springer (2020) 85–95\n16. Zhou, H.Y., Lu, C., Yang, S., Han, X., Yu, Y.: Preservational learning improves\nself-supervised medical image models by reconstructing diverse contexts. In: Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision. (2021)\n3499–3509\n17. Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao, R.,\nTang, M., et al.: MST: Masked self-supervised transformer for visual representa-\ntion. Advances in Neural Information Processing Systems 34 (2021)\n18. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Sim MIM: A\nsimple framework for masked image modeling. arXiv preprint:2111.09886 (2021)\n19. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: Image\nBERT pre-training with online tokenizer. In: International Conference on Learning\nRepresentations. (2022)\n20. Bao, H., Dong, L., Wei, F.: BEiT: BERT pre-training of image transformers.\narXiv preprint arXiv:2106.08254 (2021)\n21. He, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P., Girshick, R.: Masked autoencoders\nare scalable vision learners. arXiv preprint arXiv:2111.06377 (2021)\n22. Li, K., Yu, L., Wang, S., Heng, P.A.: Towards cross-modality medical image seg-\nmentation with online mutual knowledge distillation. Proc. AAAI 34(01) (Apr.\n2020) 775–783\n23. Jiang, J., Rimner, A., Deasy, J.O., Veeraraghavan, H.: Unpaired cross-modality\neduced distillation (cmedl) for medical image segmentation. IEEE Transactions on\nMedical Imaging (2021)\n24. Caron, M., Touvron, H., Misra, I., J´ egou, H., Mairal, J., Bojanowski, P., Joulin,\nA.: Emerging properties in self-supervised vision transformers. In: Proceedings of\nthe IEEE/CVF International Conference on Computer Vision. (2021) 9650–9660\n25. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: Proc. of\nthe IEEE Int. Conf. on Computer Vision. (2021) 10012–10022\n26. Aerts, H., E., R.V., Leijenaar, R.T., Parmar, C., Grossmann, P., Carvalho, S.,\nLambin, P.: Data from NSCLC-radiomics. The Cancer Imaging Archive (2015)\n27. Akin, O., Elnajjar, P., Heller, M., Jarosz, R., Erickson, B., Kirk, S., Filippini, J.:\nRadiology data from the cancer genome atlas kidney renal clear cell carcinoma\n[tcga-kirc] collection. The Cancer Imaging Archive (2016)\n28. Harmon, S.A., Sanford, T.H., Xu, S., Turkbey, E.B., Roth, H., Xu, Z., Yang, D.,\nMyronenko, A., Anderson, V., Amalou, A., et al.: Artiﬁcial intelligence for the\ndetection of covid-19 pneumonia on chest ct using multinational datasets. Nature\ncommunications 11(1) (2020) 1–7\n29. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with\natrous separable convolution for semantic image segmentation. In: Proceedings of\nthe European conference on computer vision (ECCV). (2018) 801–818\n30. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a\nself-conﬁguring method for deep learning-based biomedical image segmentation.\nNature methods 18(2) (2021) 203–211\n31. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306 (2021)\n32. Landman, B., Xu, Z., Igelsias, J., Styner, M., Langerak, T., Klein, A.: MICCAI\nmulti-atlas labeling beyond the cranial vault–workshop and challenge (2015)\n33. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision\ntransformers. In: Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision. (2021) 9640–9649\n34. Tao, X., Li, Y., Zhou, W., Ma, K., Zheng, Y.: Revisiting rubik’s cube: Self-\nsupervised learning with volume-wise transformation for 3d medical image segmen-\ntation. In: International Conference on Medical Image Computing and Computer-\nAssisted Intervention, Springer (2020) 238–248",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7909417152404785
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7651749849319458
    },
    {
      "name": "Segmentation",
      "score": 0.7011138796806335
    },
    {
      "name": "Image segmentation",
      "score": 0.5075365304946899
    },
    {
      "name": "Computer vision",
      "score": 0.4857197403907776
    },
    {
      "name": "Medical imaging",
      "score": 0.45969358086586
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44865769147872925
    }
  ]
}