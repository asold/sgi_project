{
  "title": "Research on Large-scale Structured and Unstructured Data Processing based on Large Language Model",
  "url": "https://openalex.org/W4400836109",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2906120260",
      "name": "Bohang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113344289",
      "name": "Gaozhe Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329899690",
      "name": "Ningxin Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174668421",
      "name": "Chaoda Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4285114731",
    "https://openalex.org/W4307498511",
    "https://openalex.org/W4386128198",
    "https://openalex.org/W3194332623",
    "https://openalex.org/W4399795617",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2792093324",
    "https://openalex.org/W4288069875",
    "https://openalex.org/W4392366650",
    "https://openalex.org/W4328114448",
    "https://openalex.org/W2015861736",
    "https://openalex.org/W2963140597",
    "https://openalex.org/W4297895085"
  ],
  "abstract": "Since the beginning of the internet era, there has been an explosion of growth in structured data (such as numbers, symbols, and labels) as well as unstructured data (including images, videos, and text). Efficient and accurate mixed query of these two types of data is a key technology to achieve high-quality information retrieval, and it is also a major challenge that needs to be solved urgently in the industry. In this study, we employ an advanced Transformer model that combines strategies and fine-tuning techniques for multi-task learning. Specifically, the model is first pre-trained on a large-scale, general-purpose dataset to learn different types of data representations and basic language comprehension skills. After that, we fine-tuned the parameters of the model to better suit these specific data processing tasks for specific application scenarios, such as image annotation, video content analysis, and structured data query. At the heart of the model is the self-attention mechanism, which allows the model to automatically emphasize the important parts and ignore irrelevant information when processing the input data. In addition, we have introduced task-specific adaptation layers that are designed to add additional processing power to the original Transformer architecture, such as a semantic analysis layer for unstructured text data and a relational extraction layer for structured data. This combination of general pre-training and task-specific fine-tuning allows the model to flexibly process and integrate information from different data sources, improving processing efficiency and accuracy. Experimental results show that the model performs well in a variety of data processing tasks, significantly improves the accuracy and efficiency of information retrieval, and verifies the strong potential and adaptability of large language models in processing mixed data types.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.858881950378418
    },
    {
      "name": "Transformer",
      "score": 0.5375117659568787
    },
    {
      "name": "Unstructured data",
      "score": 0.5289402604103088
    },
    {
      "name": "Information extraction",
      "score": 0.4603883922100067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44801822304725647
    },
    {
      "name": "Language model",
      "score": 0.43053919076919556
    },
    {
      "name": "Information retrieval",
      "score": 0.43011534214019775
    },
    {
      "name": "Natural language processing",
      "score": 0.4166174530982971
    },
    {
      "name": "Data mining",
      "score": 0.41357845067977905
    },
    {
      "name": "Machine learning",
      "score": 0.3485422730445862
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Big data",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}