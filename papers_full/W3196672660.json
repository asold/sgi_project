{
  "title": "Data Efficient Masked Language Modeling for Vision and Language",
  "url": "https://openalex.org/W3196672660",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3084926053",
      "name": "Yonatan Bitton",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A516862958",
      "name": "Michael Elhadad",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A2251783221",
      "name": "Gabriel Stanovsky",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2098207400",
      "name": "Roy Schwartz",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3172863300",
    "https://openalex.org/W3171353004",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963890019",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W4287125738",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2963349562",
    "https://openalex.org/W3120237956",
    "https://openalex.org/W3130893544",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2794583223",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W4306247074",
    "https://openalex.org/W3168154341",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W4287281411",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3128560592",
    "https://openalex.org/W3035512383",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3184369217",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4306153300",
    "https://openalex.org/W2471094925",
    "https://openalex.org/W3116952214",
    "https://openalex.org/W3133524129",
    "https://openalex.org/W2949831469",
    "https://openalex.org/W3116853161",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3201957104",
    "https://openalex.org/W2969876226"
  ],
  "abstract": "Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3013–3028\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3013\nData Efﬁcient Masked Language Modeling for Vision and Language\nYonatan Bitton♦ Gabriel Stanovsky♦ Michael Elhadad♠ Roy Schwartz♦\n♦School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel\n♠Department of Computer Science, Ben Gurion University, Israel\n{yonatanbitton,gabis,roys}@cs.huji.ac.il elhadad@cs.bgu.ac.il\nAbstract\nMasked language modeling (MLM) is one of\nthe key sub-tasks in vision-language pretrain-\ning. In the cross-modal setting, tokens in the\nsentence are masked at random, and the model\npredicts the masked tokens given the image\nand the text. In this paper, we observe sev-\neral key disadvantages of MLM in this set-\nting. First, as captions tend to be short, in\na third of the sentences no token is sampled.\nSecond, the majority of masked tokens are\nstop-words and punctuation, leading to under-\nutilization of the image. We investigate a range\nof alternative masking strategies speciﬁc to the\ncross-modal setting that address these short-\ncomings, aiming for better fusion of text and\nimage in the learned representation. When pre-\ntraining the LXMERT model, our alternative\nmasking strategies consistently improve over\nthe original masking strategy on three down-\nstream tasks, especially in low resource set-\ntings. Further, our pre-training approach sub-\nstantially outperforms the baseline model on\na prompt-based probing task designed to elicit\nimage objects. These results and our analysis\nindicate that our method allows for better uti-\nlization of the training data.1\n1 Introduction\nPre-trained vision-language (VLP) models such as\nViLBERT (Lu et al., 2019), LXMERT (Tan and\nBansal, 2019) and UNITER (Chen et al., 2020)\nhave recently improved the state-of-the-art across\nvarious vision and language benchmarks. One\nof the primary pre-training objectives of VLP is\nmasked language modeling (MLM). Motivated by\nthe single-modal MLM task, most models perform\nas introduced in BERT (Devlin et al., 2019) for\ntext-only data, randomly masking tokens with a\nprobability of 15% (Shin et al., 2021).\n1Our code, pre-trained, and ﬁne-tuned models are pub-\nlished at https://github.com/yonatanbitton/\ndata_efficient_masked_language_modeling_\nfor_vision_and_language.\nFigure 1: Illustration of our approach. The baseline\nMLM masks a random token with 15% probability,\nwhere ≈50% of the masked tokens are stop-words or\npunctuation. Our method masks words that require the\nimage in order to be predicted (e.g., physical objects).\nOur pre-train masking strategy consistently improves\nover the baseline strategy in two evaluation setups.\nThe main difference in the cross-modal setting2\nis that the model takes into account both the textual\ncontext and the image, and the latter can help it re-\nsolve ambiguities. For example, in Figure 1, given\nthe masked sentence “A [MASK] is eating the car-\nrot”, without the image, the model might predict\nrabbit, since it is correlated with carrot. But the\nimage reveals that the answer is tiger.\nIn this work, we ﬁnd that the MLM pre-training\nmethod is sub-optimal for VLP, as it does not make\nefﬁcient use of the training data. This manifests in\ntwo major shortcomings, common to many popular\npre-train datasets (Lin et al., 2014; Krishna et al.,\n2017; Sharma et al., 2018; Ordonez et al., 2011).\nFirst, we observe that image captions, which form\nthe textual part of these corpora, tend to be much\nshorter than the documents in BERT’s pre-train\ndata. As a result, uniformly masking tokens at 15%\nprobability results in many cases where no token is\nbeing masked (e.g., about one third in LXMERT).\nSecond, we note that 45%–50% of the masked\ntokens are stop-words or punctuation. While this\nseems a common phenomena also in text-only\n2This task is often referred to as “cross-modality MLM”,\nor “MLM conditioned on image regions” (Chen et al., 2020),\nto emphasize the difference from the text-only MLM task.\n3014\ndatasets, we show that this causes the image to\nbe under-used in MLM pre-training for VLP. Ev-\nidently, for the popular LXMERT model, we ﬁnd\nthat the MLM validation accuracy on stop-words\nand punctuation is almost perfect even when omit-\nting the image.\nTo address these limitations, we propose alterna-\ntive strategies aiming to mask words that require\nthe image (e.g., physical objects). We pre-train the\nLXMERT model with these strategies and demon-\nstrate their beneﬁts in two evaluation setups. First,\non three VLP downstream tasks (GQA, Hudson\nand Manning, 2019; VQA, Goyal et al., 2017;\nNLVR2, Suhr et al., 2019), our masking strategies\nconsistently improve over the traditional MLM, es-\npecially in low resource settings. Second, we exper-\niment with prompt based object detection (Radford\net al., 2021), a probing task designed to elicit im-\nage objects by presenting the pre-trained models\nwith prompts such as “A photo of [MASK]” and\ncompare their top predictions with image objects.\nOur results show that our strategy substantially im-\nproves over the baseline sampling approach, even\nwhen trained over only a third of its epochs and\nhalf of its training data.\nIn our analysis, we introduce a new metric ( ∆\nimage loss) to estimate the necessity of the image\nfor a masked word during MLM. We extract the ∆\nimage lossvalue for each token in LXMERT vali-\ndation pre-train data. We then present a hierarchy\nof semantic classes ranked by this metric, and ﬁnd\nthat the frequently masked tokens in our strategies\nindeed increase the image necessity.\nOur main contributions are: (1) We show that the\ncurrent MLM pre-training method is sub-optimal\nfor VLP, and it does not make efﬁcient use of pre-\ntrain data. (2) We propose alternative masking\nstrategies, and show that models trained with these\nstrategies outperform the baseline strategy in two\nevaluation setups, especially in low resource set-\ntings. (3) We introduce the ∆ image loss met-\nric, which aims to explain the relation between\na masked token and the image; we publicly re-\nlease the computed values of this metric for the\nLXMERT validation set; this data may be used in\nfuture work to devise improved masking strategies.\n2 Limitations of MLM Approaches for\nVision and Language\nIn this section, we present the limitations of the\nMLM approach to vision and language tasks. We\nstart by reviewing the way MLM is currently ap-\nplied in cross-modal models, and analyzing the pre-\ntrain datasets used by most models. We observe the\nfollowing two major limitations in the current ap-\nproach: (1) no token is masked in roughly a third of\nthe sentences; (2) a substantial part of the masked\ntokens are stop-words or punctuation, which can\nbe predicted based on textual context alone, and do\nnot require the image.\n2.1 Background\nMultiple studies have been proposed to modify the\nMLM objective in text-only domains (Joshi et al.,\n2020; Sun et al., 2019; Clark et al., 2020; Levine\net al., 2021). However, less research has been ded-\nicated to the implications of MLM in vision and\nlanguage tasks.\nShin et al. (2021) recently reviewed how the\ntransformer architecture (Vaswani et al., 2017)\nhas been incorporated into vision-language cross-\nmodal tasks. They show that most VLP models per-\nform MLM in the same way as introduced in BERT\n(Devlin et al., 2019) for text-only data, randomly\nmasking tokens with 15% probability. Further, vir-\ntually all models are pre-trained on a handful of\npre-training cross-modal datasets, including Con-\nceptual Captions (CC; Sharma et al., 2018); SBU\ncaptions (Ordonez et al., 2011) and the LXMERT\npre-train dataset, which is a combination of COCO\n(Lin et al., 2014), Visual Genome (Krishna et al.,\n2017), VQA (Goyal et al., 2017), VG-QA (Zhu\net al., 2016), and GQA (Hudson and Manning,\n2019).\nImportantly, all these datasets consist of\n<sentence, image> pairs, where the sentence is\nusually a caption describing the image or, in VQA,\nan image-related question.\n2.2 Limitations\nIn many cases, no token is masked. Image cap-\ntions tend to be shorter than the documents in\nBERT pre-train data, such as Wikipedia articles.\nBERT input sequence length is 512 tokens, while\nin VLP datasets the sequence length is ≈20 tokens.\nFor this reason, when masking 15% of the tokens\nin the VLP models, there are cases where no token\nis masked. For example, in LXMERT we ﬁnd that\nin 36% of the sentences, no token is masked.\nMany masked words are stop-words and punc-\ntuation. We observe that over 45-50% of tokens\nmasked by either LXMERT, CC, and SBU are stop-\n3015\nwords or punctuation marks.3 We now describe an\nexperiment that shows that this distribution causes\nthe image to be under-utilized during MLM pre-\ntraining.\nWe follow the approach of amnesic probing\n(Elazar et al., 2021). The intuition is that if the\nimage is being used for cross-modal MLM, then\nthe removal of the image should negatively inﬂu-\nence the ability of the model to solve the task. If\nthe removal of the image has little or no inﬂuence\non the ability to solve cross-modal MLM, then the\nimage is not a contributing factor in this task.\nWe consider the published pre-trained LXMERT\nmodel.4 We evaluate it at inference time with the\nMLM task twice: with and without the image, 5\nusing different masking strategies. We use the\nLXMERT pre-train validation data ( ≈214K sen-\ntences). To estimate the image necessity for a\nmasked token during MLM, we introduce the ∆\nimage lossmetric, which is the difference in vali-\ndation loss of the model prediction with and with-\nout the image. For example, in Figure 2, the loss\nwithout the imagefor predicting “motorcycle” is\n3.96, and the loss with the image is 0.25, the ∆\nimage lossis 3.71. In addition, we report the Ac-\ncuracy@5 metric, which is whether the label is\namong the top 5 most conﬁdent predictions of the\nmodel. We compare three masking strategies, keep-\ning a 15% probability to mask a token: (1) Baseline\nMLM masking strategy, where a token is masked\nuniformly at 15% probability; (2) masking only\nstop-words and punctuation; and (3) masking only\ncontent words, which is the complementary group\nof stop words and punctuation.\nResults are presented in Table 1. We observe that\nthe model validation accuracy on stop-words and\npunctuation is almost perfect (96%) even without\nthe image. On the other hand, in the case of content\nwords, accuracy is much lower without the image,\nand adding it increases accuracy by roughly 20%.\n3 Alternative Masking Strategies\nTo overcome the limitations presented in the previ-\nous section, we introduce several alternative mask-\ning strategies for cross-modal MLM. The proposed\nstrategies use several semantic classes, which are\n3We used nltk and gensim stop words lists.\n4https://github.com/airsplay/lxmert\n5Without the image, we block access to the image and use\nthe model as a single-stream model, without the co-attention\nlayers from the image to the text. The model receives only the\ntext and needs to complete the masked tokens.\nintroduced in Section 3.1, and then used in Sec-\ntion 3.2.\n3.1 Semantic Classes\nObjects, Attributes, and Relationships We use\nthe deﬁnitions of objects, attributes, and relation-\nships as described in Visual Genome (Krishna et al.,\n2017). Objects represent physical entities in the\nimage (e.g., a tiger, or a carrot). Attributes are prop-\nerties of objects, such as colors or physical state\n(e.g., upright). Finally, relationships connect be-\ntween two objects. These can be actions (e.g., a\ntiger is eating a carrot), spatial relations (e.g., the\ntiger is behind the carrot), etc.\nIn order to mask the tokens that belong to those\nsemantic classes, we ﬁrst need to identify them in a\ngiven sentence. Some datasets (e.g., GQA) include\nscene-graph annotations of these classes for each\nimage. We use the annotations as ground-truth and\ndevelop heuristics to identify them automatically.\nFor example, an Object can be reliably annotated\nby identifying nouns which are also in the Visual\nGenome objects list. This simple heuristic achieves\nan accuracy of ≈90% and recall of ≈97% for ien-\ntifying objects on the LXMERT pre-train dataset.\nWe elaborate on these heuristics in Appendix A.1.\nConcreteness We hypothesize the image con-\ntributes more when predicting concrete concepts\n(e.g., tiger) compared to abstract concepts (e.g.,\nhunger). To that end, we use a dataset of lex-\nical concreteness presented in (Brysbaert et al.,\n2014). This dataset provides concreteness scores\n(on a scale of 1-5) for over 91% of the lemmas in\nLXMERT pre-training dataset.\n3.2 Proposed Strategies\nWe consider the following masking strategies:\n• Baseline MLM: the original masking strategy\nas deﬁned in the LXMERT paper, 15% ran-\ndom token masking.\n• Objects: Randomly mask one object word.6\n• Content words: Mask exactly one word in\neach sentence. Instead of almost 50–50 parti-\ntion between masking stop-words and content\nwords, increase the probability to mask con-\ntent word to 80%.\n6In > 97.2% of the sentences there is at least one object.\nIn other cases, we mask a word at random.\n3016\nSentence A person performs a stunt jump on a [MASK].\nMasked token motorcycle\nTop 5 predictions motorcycle, bike, ramp, bicycle, cycle\nTop 5 predictions w/o image building, wall, beach, ﬁeld, street\nLoss 0.25\nLoss w/o image 3.96\n∆image loss 3.71\nFigure 2: An example from the extracted ∆ image lossdata. The masked word is motorcycle. Model predictions\n(“Top 5 predictions”) are better correlated with the image when it is given, and the loss is 0.25. Without the image,\nthe predictions (“Top 5 predictions w/o image”) are tokens that do not appear in the image, and the loss is much\nhigher (3.96). The ∆ image lossis the gap: 3.71.\nMasking strategy With Image Without Image Image Necessity\nMetric image loss (exp) Accuracy @ 5 image loss (exp) Accuracy @ 5∆image loss(exp) Accuracy @ 5\nBaseline MLM 3.2 89% 8.9 78% 5.7 10%\nStop-words & punctuation, 15%1.5 98% 2.9 96% 1.4 2%\nContent words, 15% 9.4 76% 38.7 56% 29.3 20%\nTable 1: Performance of the LXMERT model on the MLM task, when different words are masked, with and\nwithout the image. Accuracy on stop-words and punctuation is almost perfect even when no image is present.\nHowever, for content words, the image does contribute to increased accuracy.\n• Top concrete: Mask one of the top concrete\nwords in the sentence, weighted by their or-\nder.7\n• Stop-words & punctuation: as baseline, mask\nonly stop-words & punctuation, keeping a\n15% probability of masking.\n• Random 1 word: An ablation of masking a\nsingle random word.\nTokenization: The words in the sentences are\ntokenized using BERT tokenizer. For strategies\nrequiring word-level masking (Objects, Content\nwords, Top concrete, Baseline MLM, Random 1\nword), we mask all of the corresponding word-\npieces (e.g., “A tiger is eat #ing” is masked as “A\ntiger is [MASK] [MASK]”).\n4 Experiments\nTo evaluate the value of our proposed strategies, we\nconduct experiments by pre-training models with\ndifferent masking strategies and evaluate them on\n7Of the three words with the highest concreteness value in\nthe sentence, mask the most concrete word with 55% proba-\nbility, the second most concrete with 30% probability, and the\nthird most with 15% probability.\ntwo evaluation setups. We describe the experimen-\ntal setups below.\n4.1 Downstream Tasks\nExperimental setup We pre-train the LXMERT\narchitecture with the proposed masking strategies,\nexperimenting with increasing amounts of pre-\ntraining data (10%, 20%, 50%, 100%), training\nfor 7 epochs.8 All other hyper-parameters are the\nsame as the original implementation. We only mod-\nify the MLM objective, ﬁne-tuning on three down-\nstream tasks (VQA, GQA, NLVR2). For VQA\nand GQA, we report the mean of two experiments\nwith different random seeds. The NLVR2 dataset\nis smaller (≈10% of GQA), so we report three ex-\nperiments with different random seeds. Following\ncommon practice (Tan and Bansal, 2019), we test\nGQA on the test-dev split; NLVR2 on the public\ntest set test-P; and VQA on the minival split. See\ncorresponding papers for more details.\n8While the published LXMERT model was pre-trained\nfor 20 epochs, we pre-train for 7 epochs because we conduct\nmultiple pre-train experiments, and prefer to spend our budget\non more experiments than a few very expensive ones.\n3017\nPublished LXMERTbathroom,beach,city,kitchen,woman\nObjects motorcycle,bathroom,parade,man,crowd\nGround truth objects glasses, gang, motorcycle, shirt, man, parade, ...\nFigure 3: Example of top 5 predictions for the prompt\nbased object detection task, for the prompt “A photo of\na [MASK]”. Green underline indicate that the model\npredicted an object that appear in the ground truth\nobjects (obtained from the scene graph). The model\ntrained with Objects masking strategy is more respon-\nsive to the image content compared to the baseline\nmodel.\nResults Figure 4 presents our downstream tasks\nresults.9 For brevity, we focus on theObjects mask-\ning strategy, though the trend is similar for the\nother alternative strategies. We observe that our\nalternative masking strategies consistently outper-\nform the Baseline MLMstrategy, especially in low\nresource settings. Pre-training with the Objects\nstrategy yields gains of 0.72–0.86% on VQA and\nGQA, and 4% on NLVR2 with 10% of the pre-train\ndata; 0.64–0.95% gains on VQA and GQA, and\n1.35% on NLVR2 with 20%; 0.5–1.02% gains on\nVQA and GQA, and 1.6% in NLVR2 with 50%.\nWith 100%, the improvement is minor in GQA,\nVQA, but still noticeable (1.08%) on NLVR2 (The\nContent words strategy achieves 0.49 gain on GQA\nwith 100%). 10\nAblation studies The gains observed when us-\ning our proposed strategies can result from both\nchanges we made to address the limitations of stan-\ndard MLM presented in Section 2: masking a sin-\ngle word in each sentence (rather than not masking\nany word in some cases) and deciding which word\nto mask (rather than randomly masking tokens).\nTo isolate the contributing factors, we design ad-\nditional experiments. We pre-train with 10% and\n20% of the data with the random 1 wordstrategy,\nand present the mean accuracy on the VQA and\n9Results tables presented in Appendix B.3.\n10Preliminary experiments show that increasing the num-\nber of epochs leads to smaller gains, which emphasizes the\nbeneﬁts of our method in low resource settings.\nGQA in Figure 5. We see that this strategy out-\nperforms the Baseline MLM strategy, but under-\nperforms Objects. In addition, in Appendix B we\nshow experiments of varying masking probabilities\nrather than the baseline’s 15%, with and without\nmultiple masked tokens per sentence, and allow-\ning sentences without any masked token. Out of\nall tested settings, masking a single word achieves\nthe best downstream results. We conclude that the\nbeneﬁt of our proposed strategies comes from both\nchoosing a single word to mask, and masking to-\nkens that are more important.\nFor completeness, we experiment with the stop-\nwords & punctuationstrategy with 10% and 20% of\nthe data on VQA and GQA. As expected, this strat-\negy under-performs the Baseline MLM; by 1.4%\nwhen pre-training with 10% of the data, and 3.37%\nwith 20% the data.\n4.2 Prompt Based Object Detection\nTo further examine the value of our proposed mask-\ning strategies, we examine in what way the pre-\ntrained models trained with different strategies dif-\nfer. To do so, we use prompts, and study whether\na model trained for only completing Objects (for\nexample) will be more responsive to the image con-\ntents compared to the baseline model.\nFor example, given the image in Figure 1, we\ncan query the model using the prompt “A photo\nof a [MASK]”, and count how many of the ob-\njects (“tiger”, “carrot”) are in its topk predictions.\nWe compare our alternative pre-trained models,\npre-trained on 50% of the data, with the origi-\nnal pre-trained LXMERT model. We evaluate\nthem on 2193 images from the LXMERT minival\nsplit, which the model did not observe during pre-\ntraining. Given a (prompt, image) pair, we intersect\neach model’s top k predictions with the ground-\ntruth objects list obtained from the image ground\ntruth scene-graph, available for these images. We\nuse several prompts: “A photo of a [MASK]” (in-\nspired by CLIP (Radford et al., 2021)), “A [MASK]\nin the photo”, and “A [MASK]”. We present a pre-\ncision for different values of k in Figure 6.\nOur models achieve improved precision score\nover published LXMERT, despite training over\nonly a third of its epochs and half of its train-\ning data. The precision metric is simply the num-\nber of correct predictions (intersection of predic-\ntions with ground-truth objects), divided by the\nnumber of predictions. For example, when con-\n3018\nFigure 4: VQA, GQA and NLVR2 downstream tasks\nresults for models with different masking strategies and\nincreasing amounts of pre-train data. The left Y axis de-\nscribes the accuracy, the right Y axis describes the per-\ncentage of the full setup performance (trained with 20\nepochs and 100% of the pre-train data). Our alternative\nmasking strategies consistently improve over the Base-\nline MLM masking strategy, especially in low resource\nsettings.\nsidering ﬁve top predictions (k=5), the published\nLXMERT achieves 10% precision, compared to\n18% precision for the model trained with Content\nwords masking strategy. When k=10, the improve-\nment is 11% →16%, etc. Additional results and\nROC curve are available in Section B.3 in the Ap-\npendix. Our results indicate that our proposed mod-\nels are more responsive to the image compared to\nthe model trained with the Baseline MLM strategy.\nFigure 5: Ablation results for randomly masking a sin-\ngle word. The plot shows the average results for GQA\nand VQA. A model that masks a single word outper-\nforms one with the original strategy of randomly mask-\ning 15% of the tokens, but under-performs a model that\nmasks a single object word. We conclude that the gain\nof our proposed strategies comes from both masking a\nsingle word, and selecting tokens that are more impor-\ntant.\nAn example comparing the Baseline MLM model\nand model trained with Objects masking strategy\nis presented in Figure 3. Four of the top ﬁve pre-\ndictions of the model trained with Objects masking\nstrategy appear in the list of ground-truth objects,\nwhile the model trained with Baseline MLM strat-\negy predicts only one of the ground-truth objects.\n5 Analysis and Discussion\n5.1 Hierarchy of Masked Semantic Classes\nWe have shown that our strategies improve results\nover the Baseline MLM. In this section, we aim to\nunderstand if the tokens we mask make the model\nactively rely on the image. For this purpose, we ex-\ntract the image necessity for a masked token using\nthe ∆ image lossmetric (see Section 2.2) for every\ntoken. We use the original LXMERT pre-trained\nmodel and validation data. For each sentence, we\niterate over each token, mask and predict it with\nand without the image. An example from the ex-\ntracted ∆image lossdata is presented in Figure 2.11\nFollowing, Figure 7 presents a hierarchy of the dif-\nferent semantic classes described in Section 3.1,\nranked by their ∆ image loss.12\nWe draw several observations based on that plot.\nFirst, we note that objects that appear in both text\nand the scene graph (dubbed grounded objects, e.g.,\n11We publish this extracted data for future work.\n12The groups are not mutually exclusive.\n3019\nFigure 6: Precision/recall curve for prompt-base object detection task. Our models substantially improve over the\npublished LXMERT, despite training over only a third of its epochs and half of its training data.\n“tiger”) are more important than non-grounded ob-\njects. Our intuition is that grounded concepts have\nhigher ∆ image losscompared to non-grounded\nconcepts, as the model beneﬁts from masking the\nlatter. For example, consider the sentence “Is there\na tiger in the image?”, for an image without any\ntiger (i.e., tiger is not grounded). In this case, the\nmodel would not have the ability to differentiate\nthe true word (tiger) from any other object in the\nvocabulary that is also not in the image.\nIn addition, we observe that the objects semantic\nclass is the most important one. We see a connec-\ntion between the hierarchy and downstream perfor-\nmance obtained by our different strategies. Stop-\nwords & punctuationare ranked the lowest, and\nindeed pre-training with the Stop-words & punc-\ntuation strategy achieves the lowest results. The\nstrategies of Objects and Top concreteare ranked\nhigh, and indeed they achieve improved results\ncompared to the Baseline MLM.\n5.2 MLM Performance across Word Classes\nMany works (Lu et al., 2019; Tan and Bansal, 2019;\nChen et al., 2020) assume that a VLP model should\ninclude an MLM component that is capable of\npredicting every masked token, including objects,\nproperties, but also stop words and punctuation.\nDoes a model that uses our Objects strategy, and\nmasks only objects, learn to complete words from\nother classes? If not, can such a pre-training strat-\negy be effective?\nTo examine this questions, we extend the experi-\nment described in Section 2 to additional masking\nstrategies, comparing between the different models\npre-trained on 50% of the data. Results are pre-\nsented in Table 2. We see that the model trained\nwith the Baseline MLMmasking strategy is able to\ncomplete masked words from different classes (per-\nformance are above 70% for all cases). However,\nthe model trained with Objects masking strategy\nindeed learned to complete only objects. Nonethe-\nless, its downstream performance is in fact higher\nthan the Baseline MLMmodel. We conclude that\na model does not necessarily need to be able to\ncomplete all semantic classes, and some classes\nare more beneﬁcial than others. For example, the\nObjects model’s performance is quite low on both\ncompleting stop-words (4%), which is considered\nan easy task, and on attributes (22%).\nA possible explanation for these ﬁndings might\nbe that the model is evaluated mostly on retrieving\nobjects, and had we tested it on other classes, its\nperformance would have substantially decreased.\nTo test this hypothesis, we inspect the same model’s\nperformance on questions with answers from dif-\nferent semantic types. To do so, we experiment\nwith the GQA dataset, which includes partition-\ning of the answers into different semantic types,\nincluding Objects, Relations (subject or object of\na described relation, e.g., “what is the girl wear-\ning?\"), and Attributes (the properties or position of\nan object).\nThe results for the semantic type partition are\npresented in Table 3. Comparing between the\nmodels trained with Objects and Baseline MLM\nmasking strategies, the Objects masking strategy\nachieves improved performance in Relationships\nand Attributes, although it never masked these\nkinds of tokens, and its MLM performance on these\nclasses is considerably lower. It seems that mask-\ning only objects might assist the models to learn\nadditional semantic classes.\n3020\nFigure 7: Hierarchy of semantic classes and its importance by the ∆ image lossmetric (Loss without image - Loss\nwith image).\nModel Baseline MLM Objects Content words Top concreteMasking Strategy\nBaseline MLM 87% 27% 70% 36%\nStop-words & punctuation, 15% 98% 4% 80% 13%\nContent words, 15% 74% 57% 62% 62%\nObjects 76% 85% 82% 83%\nAttributes 70% 22% 59% 50%\nRelationships 89% 15% 75% 25%\nTable 2: MLM Validation Accuracy@5 for different pre-training strategies, tested on different masking strategies.\nInterestingly, the model trained with Objects strategy achieves low performance on all semantic classes except\nobjects, but still achieves improved results compared to the model trained with Baseline MLM strategy.\nQuestion\nsemantic type# QuestionsMasking Strategy\nBaseline MLM Objects\nObjects 778 86.89 87.79\nAttributes 5,186 63.17 63.96\nRelations 5,308 49.72 50.47\nTable 3: GQA semantic types partition performance.\nThe model trained with Objects masking strategy\nachieves improved performance compared to the base-\nline model on Relationships and Attributes, although it\nnever masked these kind of tokens.\n6 Related Work\n6.1 Vision Language Pre-training (VLP)\nRecently, many VLP models have been proposed\n(Lu et al., 2019; Tan and Bansal, 2019; Chen et al.,\n2020). The pre-training objectives in many cases\nare: (1) Masked language modeling (MLM), where\na model predicts masked tokens given the sen-\ntence and the image. (2) Masked region modeling\n(MRM), where the model predicts masked visual\nobject features, and (3) Sentence-image matching,\nwhere the model predicts whether the sentence be-\nlongs to the image. Some models also add the\nvisual question answering objective during the pre-\ntraining phase (Tan and Bansal, 2019; Li et al.,\n2021). Previous works have found that the MLM\nobjective is an important pre-training task affecting\nthe quality of the learned representations (Chen\net al., 2020; Huang et al., 2020; Hendricks et al.,\n2021). However, the MRM objective was not al-\nways found to be important (Su et al., 2020; Hen-\ndricks et al., 2021), and the same for sentence-\nimage prediction (Hendricks et al., 2021; Li et al.,\n2019). For this reason, we focus on the MLM ob-\njective.\n6.2 Alternative MLM objectives in vision and\nlanguage\nConcurrently with our work, Zellers et al. (2021)\npresented an approach for pre-training over\nYouTube videos. They suggested a strategy of\ncorrupting highly visual words in the masked lan-\n3021\nguage modeling task, observing that vanilla BERT-\nstyle often masks ungrounded words like “umm”\nor “yeah”. We share the same motivation to mask\nhighly visual words.\n6.3 Challenges in VQA generalization\nVisual understanding Language and vision\ntasks inherently demand deep understanding of\nboth the text and the image. However, many works\nshow that models can succeed on VQA datasets\nusing strong language priors, and by relying on\nsuperﬁcial cues, and there are still challenges to\novercome for tasks with more compositional struc-\nture (Jabri et al., 2016; Zhang et al., 2016; Goyal\net al., 2017; Agarwal et al., 2020; Bitton et al.,\n2021; Dancette et al., 2021). Balanced datasets\nsuch as VQA 2.0 (Goyal et al., 2017) and GQA\n(Hudson and Manning, 2019) have been presented\nto address these challenges. Novel models with\nricher visual representations (Zhang et al., 2021)\nwere also presented, and some works tried to en-\ncourage the model to look at the “correct” image\nregions (Liu et al., 2021; Yang et al., 2020).\nBias Yang et al. (2021) and Hendricks et al.\n(2018) have shown that attention-based vision-\nlanguage models suffer from bias that misleads the\nattention module to focus on spurious correlations\nin training data, and leads to poor generalization.\nSome examples are presented in Appendix B.4, Fig-\nure 9. To mitigate the language priors bias, it may\nbe beneﬁcial to increase the focus on the image\nduring pre-training.\n7 Conclusions\nWe have shown that the current MLM pre-training\nmethod is sub-optimal for visual language pre-\ntraining, as this process tends to focus on stop\nwords and punctuation, and in many cases does\nnot mask any word in the sentence. We proposed\nalternative masking strategies that better utilize the\nimage during pre-training, for example, focusing\non physical objects. We found improved results in\ntwo evaluation setups, especially in low resource\nsettings. We introduced the ∆ image loss met-\nric, which aims to explain the relation between a\nmasked token and the image. Our analysis includes\na hierarchy that describes the necessity of the image\nfor different semantic classes. We publicly release\nthe extracted data with this metric on the LXMERT\npre-train validation data. Future work can use this\ninformation to devise new masking strategies, and\nprogress towards VLP models that better leverage\nthe visual aspect of the cross-modal tasks.\nAcknowledgements\nWe thank the reviewers for the helpful comments\nand feedback. We thank Hao Tan for sharing the\ncode and answering questions regarding LXMERT\npre-training. We also thank Leshem Choshen, Ro-\nnen Tamari, Shahaf Finder, and Nitzan Guetta Bit-\nton for their valuable feedback. This work was sup-\nported in part by the Center for Interdisciplinary\nData Science Research at the Hebrew University\nof Jerusalem, and research gifts from the Allen\nInstitute for AI and Intel Corporation.\nReferences\nVedika Agarwal, Rakshith Shetty, and Mario Fritz.\n2020. Towards causal VQA: revealing and reduc-\ning spurious correlations by invariant and covariant\nsemantic editing. In 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020, pages\n9687–9695. IEEE.\nYonatan Bitton, Gabriel Stanovsky, Roy Schwartz, and\nMichael Elhadad. 2021. Automatic generation of\ncontrast sets from scene graphs: Probing the compo-\nsitional consistency of GQA. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 94–105, Online.\nAssociation for Computational Linguistics.\nMarc Brysbaert, Amy Beth Warriner, and Victor Ku-\nperman. 2014. Concreteness ratings for 40 thousand\ngenerally known english word lemmas. Behavior re-\nsearch methods, 46(3):904–911.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In ECCV.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nCorentin Dancette, Remi Cadene, Damien Teney, and\nMatthieu Cord. 2021. Beyond question-based\nbiases: Assessing multimodal shortcut learning\nin visual question answering. arXiv preprint\narXiv:2104.03149.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\n3022\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral ex-\nplanation with amnesic counterfactuals. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:160–175.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image un-\nderstanding in visual question answering. In 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, pages 6325–6334. IEEE Computer So-\nciety.\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018. Women\nalso snowboard: Overcoming bias in captioning\nmodels. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 771–787.\nLisa Anne Hendricks, John Mellor, Rosalia Schnei-\nder, Jean-Baptiste Alayrac, and Aida Nematzadeh.\n2021. Decoupling the role of data, attention, and\nlosses in multimodal transformers. arXiv preprint\narXiv:2102.00529.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei\nFu, and Jianlong Fu. 2020. Pixel-bert: Aligning im-\nage pixels with text by deep multi-modal transform-\ners. arXiv preprint arXiv:2004.00849.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2019, Long Beach, CA, USA, June 16-\n20, 2019, pages 6700–6709. Computer Vision Foun-\ndation / IEEE.\nAllan Jabri, Armand Joulin, and Laurens Van\nDer Maaten. 2016. Revisiting visual question an-\nswering baselines. In European conference on com-\nputer vision, pages 727–739. Springer.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123(1):32–\n73.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and\nYoav Shoham. 2021. PMI-masking: Principled\nmasking of correlated spans. In Proc. of ICLR.\nChenliang Li, Ming Yan, Haiyang Xu, Fuli Luo, Wei\nWang, Bin Bi, and Songfang Huang. 2021. Semvlp:\nVision-language pre-training by aligning semantics\nat multiple levels. arXiv preprint arXiv:2103.07829.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nYibing Liu, Yangyang Guo, Jianhua Yin, Xuemeng\nSong, Weifeng Liu, and Liqiang Nie. 2021. An-\nswer questions with right image regions: A visual\nattention regularization approach. arXiv preprint\narXiv:2102.01916.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural In-\nformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada ,\npages 13–23.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in Neural In-\nformation Processing Systems 24: 25th Annual Con-\nference on Neural Information Processing Systems\n2011. Proceedings of a meeting held 12-14 Decem-\nber 2011, Granada, Spain, pages 1143–1151.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. arXiv preprint\narXiv:2103.00020.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2556–2565, Melbourne, Australia. Association for\nComputational Linguistics.\nAndrew Shin, Masato Ishii, and Takuya Narihira. 2021.\nPerspectives and prospects on transformer architec-\nture for cross-modal tasks with language and vision.\narXiv preprint arXiv:2103.04037.\n3023\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nJianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh,\nDavid D Cox, Joshua B Tenenbaum, and Chuang\nGan. 2020. Object-centric diagnosis of visual rea-\nsoning. arXiv preprint arXiv:2012.11587.\nXu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai.\n2021. Causal attention for vision-language tasks. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 9847–\n9857.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\n2021. Merlot: Multimodal neural script knowledge\nmodels. arXiv preprint arXiv:2106.02636.\nPeng Zhang, Yash Goyal, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2016. Yin and yang:\nBalancing and answering binary visual questions. In\n2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 5014–5022. IEEE Com-\nputer Society.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. 2021. Vinvl: Making visual repre-\nsentations matter in vision-language models. CVPR\n2021.\nYuke Zhu, Oliver Groth, Michael S. Bernstein, and\nLi Fei-Fei. 2016. Visual7w: Grounded question an-\nswering in images. In 2016 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, pages\n4995–5004. IEEE Computer Society.\n3024\nA Appendix\nReproducibility The experiments have been per-\nformed with the LXMERT model (Tan and Bansal,\n2019) with the public implementation. 13 The ex-\nperiments were performed with NVIDIA RTX2080\nGPUs.\nPre-training data 10% 20% 50% 100%\n# Epochs 7 7 7 7\nBatch size 64 64 100 256\n# GPUs 1 1 3 4\nRuntime 2 days 3 days 3 days 3 days\nTable 4: Pre-training experiments conﬁgurations.\nA.1 Detection of Objects, Attributes and\nRelationships\nUsing the annotated scene-graph as ground\ntruth A simple way to detect objects, attributes,\nand relationships in captions, is to obtain it, given\nthat the image has scene-graph annotation from\nVisual-Genome or GQA. In LXMERT pre-training\ndata, 83% of the sentences have scene-graph anno-\ntations for their corresponding image. For example,\ngiven the sentence, image pair: “The rabbit is eat-\ning the orange carrot”, and an image, the ground\ntruth by the scene-graph will include Objects: rab-\nbit, carrot; Attributes: orange; and Relationships:\neating. When obtained from the scene-graph, we\ncall it “Grounded” (Grounded objects, grounded\nattributes, and grounded relationships).\nPredicting objects, attributes, and relationships\nin each caption: For more general and scalable\nmethod when scene-graph is not available, we can\nuse matching heuristics. We use the Part-of-speech\ntagging (POS), and we aggregate lists of Objects,\nAttribute and Relationships from Visual Genome\ndataset annotations.14 Those are our heuristics:15\n• Objects are words with POS = “NOUN” and\nin Visual Genome objects list.\n• Attributes are words with POS = “ADJ” and\nin Visual Genome attributes list.\n13https://github.com/airsplay/lxmert\n14http://visualgenome.org/api/v0/api_\nhome.html\n15Our full code, including code to detect the semantic type\ntokens will be published\nEpoch Baseline MLMContent wordsObjects Top Concrete\n1 1.70 3.07 3.23 3.26\n2 1.46 2.11 2.28 2.29\n3 1.40 1.97 2.14 2.15\n4 1.36 1.88 2.04 2.05\n5 1.33 1.81 1.96 1.98\n6 1.30 1.75 1.90 1.91\n7 1.27 1.71 1.84 1.86\n8 1.25\n9 1.27\n10 1.23\n11 1.21\n12 1.19\n13 1.17\n14 1.16\n15 1.14\n16 1.12\n17 1.11\n18 1.09\nTable 5: Training loss for models trained in different\nmasking strategies. The training loss for the original is\nobtained from the original model repository. Because\nwe focus on tokens that are more difﬁcult for the model\nto complete, the training loss is higher.\n# items Accuracy Recall\nObjects 7,484,940 89.89 97.39\nAttributes 3,240,096 92.91 79.91\nRelationships 3,195,345 86.42 96.88\nTable 6: Detection performance of Objects, Attributes,\nand Relationships.\n• Relationships are words with POS = “ADP” or\n“VERB”, and in Visual Genome relationships\nlist.\nThose simple rules are our predictions for de-\ntecting Objects, Attributes, and Relationships in a\nsentence.\nValidation of the objects attributes and rela-\ntionships task: We can now evaluate the pre-\ndicted objects, attributes and relationships with the\nground-truth obtained from the scene-graph. The\ngrounding method (matching between the caption\nand the scene-graph) we use is simple: exact match\nbetween the word in the scene-graph and the cap-\ntion. Using a more complex grounding algorithm\nwill not change our predictions, but it can only\nimprove our results (For example, if the caption\nhas “women” that was predicted asObject, and the\nscene-graph has “woman”, it is currently counted\nas “False-Positive” because it’s not exact match).\nResults are presented at Table 6.\n3025\nA.2 Concrete and Abstract deﬁnitions\nThe concreteness annotation dataset (Brysbaert\net al., 2014) is annotated by 20-30 annotators. The\nrating scale is 1-5, when 1 is abstract, and 5 is\nconcrete. This is how they deﬁne concrete: “A con-\ncrete word comes with a higher rating and refers\nto something that exists in reality ; you can have\nimmediate experience of it through your senses\n(smelling, tasting, touching, hearing, seeing) and\nthe actions you do. The easiest way to explain a\nword is by pointing to it or by demonstrating it.”\nThis is how they deﬁne abstract: “An abstract\nword comes with a lower rating and refers to some-\nthing you cannot experience directly through your\nsenses or actions. Its meaning depends on lan-\nguage. The easiest way to explain it is by using\nother words”.\nB Additional Experiments\nB.1 How good is current pre-training?\nWe want to asses contribution of the current\nLXMERT pre-training. We conduct ﬁne-tune ex-\nperiments with LXMERT without pre-tain. Results\nare presented at Table 7. We see that pre-training\nadds ≈6.5 in GQA, ≈4.8 in VQA, and ≈23.8 in\nNLVR2.\nDataset GQAVQA NLVR2\nNo pre-train 53.2465.10 51.07\nPre-training all data\nReported LXMERT GitHub results59.8069.90 74.95\nTable 7: Downstream task performance for limited pre-\ntraining methods.\nB.2 How to change the 15% masking\namount?\nIn Section 2 we discussed that 15% with short cap-\ntions (≈6.86) causes that with third of the cases no\ntoken is masked, in another third 1 token is masked,\nand in the last third, multiple tokens are masked.\nWe isolate those factors by conducting 3 experi-\nments:\n• Not allowing 0 masked (if 0 tokens were\nmasked, sampling 1 token to mask).\n• Not allowing multiple masked (if multiple to-\nkens were masked, sample 1 token from them\nto mask)\n• Masking only 1 word.\nGQA VQA NLVR2\nBaseline MLM 54.4 65.06 58.55\nDon’t allow 0 masked 54.98 65.4 59.45\nDon’t allow multiple masked54.46 65 58.82\nMask 1 word 55.07 65.26 61.25\nTable 8: Changing 15% masking amount. Masking 1\nword achieves the higher downstream tasks results.\nResults are presented at Table 8.\nWe can see that not allowing multiple masked\ntokens helps a bit. Not allowing 0 masked tokens\nhelps more. And masking 1 word is the better\noverall strategy.\nB.3 Full results tables\nB.4 Examples\n3026\n% of pre-train data\nMasking Strategy 10 20 50 100\nBaseline MLM 65.05 ±0.02 65.86 ±0.06 67.14 ±0.2 68.79 ±0.02\nContent words 65.53 ±0.04 66.37 ±0.04 67.86 ±0.08 68.94 ±0.05\nObjects 65.77 ±0.05 66.5 ±0.04 67.64 ±0.08 68.94 ±0.06\nTop concrete 65.54 ±0.21 66.32 ±0.02 67.47 ±0.1 68.8 ±0.03\nTable 9: Full VQA 2.0 results, mean±std\n% of pre-train data\nMasking Strategy 10 20 50 100\nBaseline MLM 54.39 ±0.01 55.14 ±0.02 57.47 ±0.13 58.87 ±0.04\nContent words 55.46 ±0.04 56.27 ±0.33 58.07 ±0.09 59.36 ±0.08\nObjects 55.25 ±0.21 56.08 ±0.10 58.49 ±0.01 59.02 ±0.03\nTop Concrete 55.31 ±0.12 56.56 ±0.35 58.38 ±0.25 58.9 ±0.04\nTable 10: Full GQA results, mean±std\n% of pre-train data\nMasking Strategy 10 20 50 100\nBaseline MLM 59.67 ±1.04 65.1 ±1.13 68.75 ±0.53 70.73 ±0.65\nContent words 61.65 ±0.95 67.25 ±0.48 70.85 ±0.06 71.63 ±0.44\nObjects 63.7 ±0.14 66.45 ±1.2 70.36 ±0.91 71.81 ±0.51\nTop Concrete 62.49 ±0.72 66.4 ±0.56 70.29 ±0.22 71.8 ±0.1\nTable 11: Full NLVR2 results, mean mean±std\n3027\nFigure 8: Precision-recall curve for prompt-base object detection task. Our models achieve improved results over\npublished LXMERT, although trained with a half of the pre-train data and a third of the epochs.\nFigure 9: LXMERT mistakes observed on examples from GQA and VQA. The tendency of VLP models is to\npredict something that is correlated with the text, or common answers. In many cases, the prediction is not an item\nthat even appears in the image.\n3028\nPublished LXMERTbathroom,kitchen,bedroom,beach,city\nObjects bathroom,restroom,sink,toilet,mirror\nGround truth objects tile, toilet, wash cloth, tub, sink, mirror, ...\nPublished LXMERTbeach,ﬁeld,bathroom,woman,man\nObjects beach,ﬁeld,baseball,woman,game\nGround truth objects bat, shirt, catcher, glove, lot, distance, ...\nFigure 10: Additional examples of top 5 predictions for the prompt based object detection task, for the prompt “A\nphoto of a [MASK]”. Green underline indicate that the model predicted an object that appear in the ground truth\nobjects (obtained from the scene graph).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8403481841087341
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6699869632720947
    },
    {
      "name": "Security token",
      "score": 0.6674749851226807
    },
    {
      "name": "Key (lock)",
      "score": 0.6148069500923157
    },
    {
      "name": "Language model",
      "score": 0.5912750363349915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5573263168334961
    },
    {
      "name": "Natural language processing",
      "score": 0.5295028686523438
    },
    {
      "name": "Speech recognition",
      "score": 0.4565548300743103
    },
    {
      "name": "Representation (politics)",
      "score": 0.45210549235343933
    },
    {
      "name": "Sentence",
      "score": 0.43027663230895996
    },
    {
      "name": "Machine learning",
      "score": 0.3345333933830261
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197251160",
      "name": "Hebrew University of Jerusalem",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I124227911",
      "name": "Ben-Gurion University of the Negev",
      "country": "IL"
    }
  ],
  "cited_by": 17
}