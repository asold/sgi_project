{
  "title": "Evaluating Large Language Models on Medical Evidence Summarization",
  "url": "https://openalex.org/W4366823941",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100224089",
      "name": "Liyan Tang",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2233794630",
      "name": "Zhaoyi Sun",
      "affiliations": [
        "Cornell University",
        "Weill Cornell Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A4286553684",
      "name": "Betina Idnay",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2765106379",
      "name": "Jordan G. Nestor",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4366915027",
      "name": "Pierre A. Elias",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2155053510",
      "name": "Ziyang Xu",
      "affiliations": [
        "Massachusetts General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2099000120",
      "name": "Ying Ding",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A1978278429",
      "name": "Greg Durrett",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A3101598460",
      "name": "Justin Rousseau",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2100224089",
      "name": "Liyan Tang",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2233794630",
      "name": "Zhaoyi Sun",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A4286553684",
      "name": "Betina Idnay",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2765106379",
      "name": "Jordan G. Nestor",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4366915027",
      "name": "Pierre A. Elias",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2155053510",
      "name": "Ziyang Xu",
      "affiliations": [
        "Massachusetts General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2099000120",
      "name": "Ying Ding",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A1978278429",
      "name": "Greg Durrett",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A3101598460",
      "name": "Justin Rousseau",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4313294616",
    "https://openalex.org/W2012107441",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4285110479",
    "https://openalex.org/W4281714969",
    "https://openalex.org/W2911471385",
    "https://openalex.org/W2990000928",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4313447794",
    "https://openalex.org/W2034978010",
    "https://openalex.org/W4297458847",
    "https://openalex.org/W1490960179",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4225491237",
    "https://openalex.org/W4319080205",
    "https://openalex.org/W1575164208"
  ],
  "abstract": "Abstract Recent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on various downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the capabilities and limitations of LLMs, specifically GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization across six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality. Our study has demonstrated that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore, informed by our human evaluations, we define a terminology of error types for medical evidence summarization. Our findings reveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain statements, leading to potential harm due to misinformation. Moreover, we find that models struggle to identify the salient information and are more error-prone when summarizing over longer textual contexts.",
  "full_text": "Evaluating Large Language Models on\nMedical Evidence Summarization\nLiyan Tang1*, Zhaoyi Sun2, Betina Idnay3, Jordan G Nestor4,\nAli Soroush3, Pierre A. Elias3, Ziyang Xu5, Ying Ding1, Greg\nDurrett6, Justin Rousseau7†, Chunhua Weng3†, Yifan Peng2†\n1School of Information, The University of Texas at Austin, Austin, TX.\n2Department of Population Health Sciences, Weill Cornell Medicine, New York, NY.\n3Department of Biomedical Informatics, Columbia University, New York, NY.\n4Department of Medicine, Columbia University, New York, NY.\n5Department of Medicine, Massachusetts General Hospital, Boston, MA.\n6Department of Computer Science, The University of Texas at Austin, Austin, TX.\n7Departments of Population Health and Neurology, Dell Medical School, The\nUniversity of Texas at Austin, Austin, TX.\n*Corresponding author(s). E-mail(s): lytang@utexas.edu;\nContributing authors: justin.rousseau@austin.utexas.edu;\ncw2384@cumc.columbia.edu; yip4002@med.cornell.edu;\n†These authors contributed equally to this work.\nAbstract\nRecent advances in large language models (LLMs) have demonstrated remark-\nable successes in zero- and few-shot performance on various downstream tasks,\npaving the way for applications in high-stakes domains. In this study, we system-\natically examine the capabilities and limitations of LLMs, specifically GPT-3.5\nand ChatGPT, in performing zero-shot medical evidence summarization across\nsix clinical domains. We conduct both automatic and human evaluations, cov-\nering several dimensions of summary quality. Our study has demonstrated that\nautomatic metrics often do not strongly correlate with the quality of summaries.\nFurthermore, informed by our human evaluations, we define a terminology of error\ntypes for medical evidence summarization. Our findings reveal that LLMs could\nbe susceptible to generating factually inconsistent summaries and making overly\nconvincing or uncertain statements, leading to potential harm due to misinforma-\ntion. Moreover, we find that models struggle to identify the salient information\nand are more error-prone when summarizing over longer textual contexts.\n1\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nKeywods: Natural Language Processing, Medical Evidence Summarization, Large\nLanguage Model, Zero-shot, Human Evaluation, Error Types, Factual Consistency,\nMedical Harmfulness\n1 Introduction\nFine-tuned pre-trained models have been the leading approach in text summarization\nresearch, but they often require sizable training datasets which are not always available\nin specific domains, such as medical evidence in the literature. The recent success\nof zero- and few-shot prompting with large language models (LLMs) has led to a\nparadigm shift in NLP research [1–4]. The success of prompt-based models (GPT-3.5\n[5], and recently ChatGPT [6]) brings new hope for medical evidence summarization,\nwhere the model can follow human instructions and summarize zero-shot without\nupdating parameters. While recent work has analyzed and evaluated this strategy for\nnews summarization [7] and biomedical literature abstract generation [8], there is no\nstudy yet on medical evidence summarization and evaluation. In this study, we conduct\na systematic study of the potential and possible limitations of zero-shot prompt-based\nLLMs on medical evidence summarization using GPT-3.5 and ChatGPT models. We\nthen explored their impact on the summarization of medical evidence findings in the\ncontext of evidence synthesis and meta-analysis.\n2 Results\nWe made use of Cochrane Reviews obtained from the Cochrane Library and focused on\nsix distinct clinical domains – Alzheimer’s disease, Kidney disease, Esophageal cancer,\nNeurological conditions, Skin disorders, and Heart failure. We collected around ten of\nthe most recent reviews for each of these six domains. Domain experts verified each\nreview to confirm that they have significant research objectives.\nIn our study, we tackle the single-document summarization setting where we focus\non the abstracts of Cochrane Reviews. Different from other types of reviews, abstracts\nof Cochrane Reviews can be read as stand-alone documents [9]. They summarize the\nkey methods, results and conclusions of the review. An abstract does not contain any\ninformation that is not in the main body of the review, and the overall messages should\nbe consistent with the conclusions of the review. In addition, abstracts of Cochrane\nReviews are freely available on the Internet (e.g., MEDLINE). As some readers may\nbe unable to access the full review, abstracts may be the only source readers have to\nunderstand the review results.\nEach abstract includes the Background, Research objectives, Search methods,\nSelection criteria, Data collection and analysis, Main results, and Author’s conclu-\nsions. The average length of the abstract we evaluated can be found in Table 1. We\nchose the Author’s Conclusions section, the last section of the abstract, as the human\nreference summary for the Cochrane Reviews in our study. This section contains the\nmost salient details of the studies analyzed within the specific clinical context. Clin-\nicians often consult the conclusions first when seeking answers to a clinical question,\nbefore deciding whether to read the full abstract and subsequently the entire study.\nIt also allows the authors to interpret the evidence presented in the review, assess\n2\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nthe strength of the evidence, and provide their own conclusions or recommendations\nconcerning the efficacy and safety of the intervention under review.\nWe assessed the zero-shot performance of medical evidence summarization using\ntwo models: GPT-3.5 [5] (text-davinci-003) and ChatGPT [6]. To evaluate the mod-\nels’ capabilities, we designed two distinct experimental setups. In the first setup, the\nmodels were given the entire abstract, excluding theAuthor’s Conclusions(ChatGPT-\nAbstract). In the second setup, the models received both the Objectives and the Main\nResults sections from the abstract as the input (ChatGPT-MainResult and GPT3.5-\nMainResult). Here, we chose Main Results as the input document because it includes\nthe findings of all important benefit and harm outcomes. It also summarizes the\nimpact of the risk of bias on trial design, conduct, and reporting. We did not evaluate\nGPT3.5-Abstract because our pilot study indicated that ChatGPT-MainResult per-\nforms generally better than ChatGPT-Abstract. In both settings, we used [input] +\n“Based on the Objectives, summarize the above systematic review in four sentences”\nas a prompt, emphasizing the importance of referring to the Objectives section for\naspect-based summarization. We decided to summarize the review into four sentences\nsince it is close to the length of human reference summaries on average.\nFigure 1 presents a comparative analysis of summaries generated by ChatGPT-\nMainResult, ChatGPT-Abstract, and GPT3.5-MainResult across six clinical domains,\nas detailed in the Methods section.\n2.1 Automatic evaluation\nTo evaluate the quality of the automatically generated summaries, we employed various\nautomatic metrics (Figure 1A), including ROUGE-L [10], METEOR [11], and BLEU\n[12], comparing them against a reference summary. Their values range from 0.0 to 1.0,\nwith a score of 1.0 indicating the generated summaries are identical to the reference\nsummary. Our findings reveal that all models exhibit similar performance with respect\nto these automatic metrics. A relatively high ROUGE score demonstrates that these\nmodels can effectively capture the key information from the source document. In\ncontrast, a low BLEU score implies that the generated summary is written differently\nfrom the reference summary. Consistent METEOR scores across the models suggest\nthat the summaries maintain a similar degree of lexical and semantic similarity to the\nreference summary.\nWe also assessed the degree of abstraction by measuring the extractiveness\n[13] and the percentage of novel n-grams in the summary with respect to the\ninput. Compared to human-written summaries, those generated by LLMs tend\nto be more extractive, exhibiting significantly lower n-gram novelty (Figure 1B).\nNotably, ChatGPT-MainResult demonstrates a higher level of abstraction compared\nto ChatGPT-Abstract and GPT3.5-MainResult, but there remains a substantial gap\nbetween ChatGPT-MainResult and human reference. Finally, approximately half of\nthe reviews are from 2022 and 2023, which is after the cutoff date of GPT3.5 (Jun.\n2021) and ChatGPT (Sep. 2021). However, we observed no significant difference in\nquality metrics before and after 2022.\n3\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \n0\n10\n20\n30\nROUGE−L METEOR BLEU\nReference−based MetricsA\n0\n25\n50\n75\nExtractiveness % novel 1−gram % novel 2−gram % novel 3−gram\nExtractiveness MetricsB\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult Reference−MainResult\nn.s. *\n***\n0.0\n0.5\n1.0\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nCoherenceC\nn.s. ****\n****\n0.0\n0.5\n1.0\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nFactual ConsistencyD\n**** ****\n****\n0.0\n0.5\n1.0\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nComprehensivenessE\n** n.s.\n****\n0.0\n0.5\n1.0\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nHarmfulnessF\nStrongly Agree Agree Neutral Disagree Strongly Disagree\nFig. 1: Performance of different summarization systems in automatic and human eval-\nuations. (A) Reference-based Metrics (higher scores indicate better summaries). ( B)\nExtractiveness Metrics. (C) Coherence. (D) Factual Consistency. (E) Comprehensive-\nness. (F) Harmfulness. Statistical analysis by Mann-Whitney U test (C-F ), *p-value\n≤ 0.05, **p-value ≤ 0.01, ***p-value ≤ 0.001, ****p-value ≤ 0.0001.\n4\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \n2.2 Human evaluation\nTo obtain a comprehensive understanding of the summarization capabilities of LLMs,\nwe conducted an extensive human evaluation of the model-generated summaries, which\ngoes beyond the capabilities of automatic metrics [14, 15]. Specifically, the lack of stan-\ndardized terminology of error types for medical evidence summarization necessitated\nour use of human evaluation to invent new error definitions. Our evaluation meth-\nods drew from qualitative methods in grounded theory, which involved open coding of\nqualitative descriptions of factual inconsistencies, further contributing to the develop-\nment of error definitions. Additionally, we included a measure of perceived potential\nfor harm, as it is a clinically relevant outcome that automatic metrics are unable to\ncapture. Our evaluation defined summary quality along four dimensions: (1) Coher-\nence; (2) Factual Consistency; (3) Comprehensiveness; and (4) Harmfulness, and the\nresults are presented in Figures 1C-F.\nCoherence refers to the capability of a summary to create a coherent body of infor-\nmation about a topic through connections between sentences. Figure 1C shows that\nannotators rated most of the summaries as coherent. Specifically, summaries gener-\nated by ChatGPT are more cohesive than those generated by GPT3.5-MainResult\n(64% vs 55% in Strong agreement).\nFactual Consistency measures whether the statements in the summary are sup-\nported by the source document. As illustrated in Figure 1D, fewer than 10% of\nsummaries produced by ChatGPT-MainResult exhibit factual inconsistency errors,\nwhich is significantly lower compared to those generated by other LLM configura-\ntions. Medical evidence summaries should be perfectly accurate. To understand the\ntypes of factual inconsistency errors that LLMs produce, we categorize these errors\ninto three types of errors using an open coding approach on annotators’ comments\n(Supplementary Figure 2). Examples can be found in Supplementary Table 1.\nComprehensiveness refers to whether a summary contains comprehensive infor-\nmation to support the systematic review. As shown in Figure 1E, both ChatGPT-\nMainResult and ChatGPT-Abstract provide comprehensive summaries more than 75%\nof the time, with ChatGPT-MainResult having significantly more summaries anno-\ntated as Strongly Agree. In contrast, GPT3.5-MainResult generates noticeably less\ncomprehensive summaries. It would be possible that extending the length of the sum-\nmary would lead to a more comprehensive summary. However, ChatGPT-MainResult\nstrikes a balance between providing enough information and being concise. The next\nevaluation was conducted to determine whether the omission of information relevant\nto the objectives might lead to medical harmfulness.\nHarmfulness refers to the potential of a summary to cause physical or psychologi-\ncal harm or undesired changes in therapy or compliance due to the misinterpretation\nof information. Figure 1F shows the error type distributions from summaries that con-\ntain harmful information, with ChatGPT-MainResult generating the fewest medically\nharmful summaries (less than 10%).\nSupplement Figure 1 further breaks down the human evaluation for each domain.\nWe observe annotation variations across six different clinical domains, and these vari-\nations can be attributed to several factors. (1) The complexity of specific domains or\n5\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nreview types may contribute to the observed variability, as some may be less com-\nplex than others, making it easier for LLMs to summarize. (2) Domain experts might\nevaluate the summaries according to their unique internal interpretations of quality\nmetrics. (3) Individual preferences may influence the decision on what key information\nshould be incorporated in the summary.\n2.3 Human preference\nFigure 2 shows the percentage of times humans express a preference for summaries\ngenerated by a specific summarization model. Notice that we allow multiple summaries\nto be selected as the most or least preferred for each source document. As shown\nin Figure 2A, ChatGPT-MainResult is significantly more preferred among the three\nLLMs configurations, generating the most preferred summaries approximately half\nof the time, outperforming its counterparts by a considerable margin. In Figure 2B,\nwe categorize the considerations driving such preference. We find that ChatGPT-\nMainResult is favored because it produces the most comprehensive summary and\nincludes more salient information. In Figure 2C, the leading reasons for choosing a\nsummary as the least preferred are missing important information, fabricated errors,\nand misinterpretation errors. This aligns with our finding that ChatGPT-MainResult\nis the most preferred since it commits the fewest amount of factual inconsistency errors\nand contains the least harmful or misleading statements.\n0.0\n0.2\n0.4\n0.6\nMost Preferred Summary Least Preferred Summary\nChatGPT−MainResult ChatGPT−Abstract GPT3.5−MainResult\nPreferenceA\n0\n10\n20\n30\nHelpful\ninterpretation\nHighest\nclarity\nHighest\naccuracy\nIncluding\nmore\nimportant\ninformation\nHighest\ncomprehensiveness\nCount\nReasons for Choosing the Most Preferred SummaryB\n0\n5\n10\n15\n20\n25\nLeast\naccurate\nNot\nrelated\nto the\nobjective\nLeast\ncoherency\nLeast\nclarity\nOver\nextrapolation\nMisinterpretation\nerrors\nFabricated\nerrors\nMissing\nimportant\ninformation\nCount\nReasons for Choosing the Least Preferred SummaryC\nFig. 2: Annotator vote distribution for the most and least preferred summaries (A)\nand the reasons for choosing them (B and C) across all clinical domains and models.\n6\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \n3 Discussions\n3.1 Are existing automatic metrics well-suited for evaluating\nmedical evidence summaries?\nResearch has demonstrated that automatic metrics often do not strongly correlate with\nthe quality of summaries [14]. Moreover, there is no off-the-shelf automatic evaluation\nmetric specifically designed to assess the factuality of summaries generated by the most\nrecent summarization systems [7, 16]. We believe this likely extends to the absence of\na tailored factuality metric for evaluating medical evidence summaries generated by\nLLMs as well. In our study, we observed similar results for three model settings when\nusing automatic metrics, which fall short of accurately measuring factual inconsistency,\npotential for medical harmfulness, or human preference for LLM-generated summaries.\nTherefore, human evaluation becomes an essential component to properly assess the\nquality and factuality of medical evidence summaries generated by LLMs at this time,\nand more effective automatic evaluation methods should be developed for this field.\n3.2 What causes Factual Inconsistency?\nWe categorize factual inconsistency errors into three types of errors using an open\ncoding approach on annotators’ comments (Supplementary Figure 2). Examples can\nbe found in Supplementary Table 1.\nFirst, through our qualitative analysis of the annotators’ comments, we discover\nthat auto-generated summaries often contain\nMisinterpretation Errors. These errors\ncan be problematic, as readers might trust the summary’s accuracy without being\naware of the potential for falsehoods or distortions. To better understand these errors,\nwe further categorize them into two main sub types. The first is Contradiction, which\narises when there is a discrepancy between the conclusions drawn from the medical\nevidence results and the summary. For example, a summary might assert that atypical\nantipsychotics are effective on psychosis in dementia, whereas the review indicates that\nthe effect is negligible [17]. The other is theCertainty Illusion, which occurs when there\nis an inconsistency in the degree of certainty between the summary and the source\ndocument. Such errors may cause summaries to be overly convincing or uncertain,\npotentially leading readers to rely too heavily on the accuracy of the presented informa-\ntion. For instance, the abstract of a Cochrane Review [18] asserts moderate-certainty\nevidence that endovascular therapy (ET) plus conventional medical treatment (CMT)\ncompared to CMT alone causes a higher risk of short-term stroke and death. However,\nwe found that the generated summary conveys low-quality confidence.\nFabricated Errors arise when a statement appears in a summary, but no evidence\nfrom the source document can be found to support or refute the statement. For\ninstance, a summary states that exercise could enhance satisfaction and quality of\nlife for patients with chronic neck pain, but the review does not mention those two\noutcomes for patients [19]. Interestingly, in our human evaluation, we did not find\nChatGPT producing any fabricated errors.\nFinally,\nAttribute Errors refer to any errors on non-key elements in the review ques-\ntion (i.e., Patient/ Problem, Intervention, Comparison, and Outcome) and may arise\n7\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nin summaries under three circumstances: (a) Fabricated Attribute: This error occurs\nwhen a summary incorporates an attribute for a specific symptom or outcome that is\nnot referenced in the source document. For example, a review [18] draws conclusions\nabout a population with intracranial artery stenosis (ICAS), but the summary states\nthe population with recent symptomatic severe ICAS, where “recent” and “severe”\ncannot be inferred from the document and would impact one’s interpretation of the\nreview; (b) Omitted Attribute: This error occurs when a summary neglects an attribute\nfor a specific symptom or outcome, such as neglecting to specify the subtype of demen-\ntia discussed in the review, leading to overgeneralization of the conclusion [17]; and\n(c) Distorted Attribute: This error occurs when the specified attribute is incorrect, like\nstating four trials are included in the study while the source document indicates that\nonly two trials are included [20].\nWe observed that ChatGPT-MainResult has the lowest proportion of all three\ntypes of errors when compared to the other two model configurations. Moreover, it is\nimportant to note that LLMs generally generate summaries with few fabricated errors.\nThis is a promising finding, as it is crucial for generated statements to be supported\nby source documents. However, LLMs do display a noticeable occurrence of attribute\nerrors and misinterpretation errors, with ChatGPT-Abstract and GPT3.5-MainResult\ndisplaying a higher incidence of the latter. Drawing inaccurate conclusions or conveying\nincorrect certainty regarding evidence could lead to medical harm as shown in later\nsections.\n3.3 What causes Medical Harmfulness?\nWe further identify three reasons that could potentially cause medical harmfulness:\nmisinterpretation errors, fabricated errors, and incomprehensiveness (such as missing\nPopulation, Intervention, Comparison, Outcome (PICO) elements). Notably, we did\nnot find any instances of medical harmfulness resulting from attribute errors in the\nsummaries we analyzed. However, given the limited number of summaries we exam-\nined, we cannot definitively conclude that attribute errors could never cause harm.\nOur study suggests that medical harmfulness caused by LLMs is mainly due to misin-\nterpretation errors and incomprehensiveness. Although our human evaluation showed\nthat LLMs tend to make relatively few fabricated errors when completing our tasks,\nwe cannot exclude the possibility that such errors could lead to harmful consequences.\nHowever, not all summaries with these errors would bring medical harm. For example,\nalthough the summary makes a significant error by misspecifying the number of trials\nin a study [20], our domain experts do not think this could bring medical harm.\n3.4 How do human-generated summaries compare to\nLLM-generated summaries?\nWe observe that human-generated summaries contain a higher proportion (28%) of\nfabricated errors, resulting in more factual inconsistency and potential for harmfulness\nin human references. However, it is essential to approach this finding with caution, as\nour human evaluation on human-generated summaries relies solely on the abstracts\nof Cochrane Reviews as a proxy. There is a possibility that statements deemed to\n8\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \ncontain fabricated errors could, in fact, be validated by other sections within the full-\nlength Cochrane Review. For example, the severity of ICAS of the studied population\n(see the example for Fabricated Attribute) is not mentioned in the abstract of the\nreview but it is mentioned in the Results section of the whole review. Therefore, we\ndecide to exclude the human reference from our comparison figures. Nevertheless,\ndespite these errors, human-generated summaries are still preferred (34%) compared\nto ChatGPT-Abstract (26%) and GPT3.5-MainResult (21%). It is worth noting that\nhuman-generated summaries may contain valuable interpretations of reviews, which\naccount for why they are chosen as the best summaries. However, it is important to\navoid over-extrapolating from the source document, as this could lead to less desirable\noutcomes (as illustrated in Figures 2B and 2C).\n3.5 Does providing longer input lead to better summaries\ngenerated by LLMs?\nIt is important to emphasize that the main difference between ChatGPT-MainResult\nand ChatGPT-Abstract is that the latter generates summaries based on the entire\nabstract. Our findings show that having longer text actually negatively impacts Chat-\nGPT’s capability to identify and extract the most pertinent information, as evidenced\nby the lower comprehensiveness. Furthermore, a longer context leads to an increased\nlikelihood of ChatGPT making factual inconsistency errors and generating summaries\nthat are more misleading. These factors combined make ChatGPT-Abstract less\npreferred compared to ChatGPT-MainResult in human evaluation.\n3.6 How can we automatically detect factual inconsistency and\nimprove summaries?\nGiven that the types of errors made by the most recent summarization systems are\nconstantly evolving [16], future factuality and harmfulness evaluation should be adapt-\nable to these shifting targets. One possible approach is to leverage the power of LLMs\nto identify potential errors within summaries. However, effectively identifying the most\nimportant information from long contexts and making high-quality summaries remains\na challenging task for LLMs that we have evaluated. Methods such as segment-then-\nsummarize [21] and extract-then-abstract [21] for handling summarizing long context\nare shown to not work well for zero-shot LLM [7].\nFurthermore, the presence of non-textual data, such as tables and figures in\nCochrane Reviews, may increase the complexity of the summarization task. To address\nthese challenges and improve the quality of summaries, future work could explore\nand evaluate the efficacy of GPT-4 in summarizing reviews with longer contexts\nand multiple modalities, while also incorporating techniques for detecting factuality\ninconsistencies and medical harmfulness.\n3.7 Limitations\nOur evaluation of ChatGPT and GPT-3.5 is based on a semi-synthetic task, which\ninvolves summarizing Cochrane Reviews using only their abstracts or part of the\n9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nabstracts. A more genuine task here would be a multi-document summarization set-\nting that involves summarizing all relevant study reports within a review addressing\nspecific research questions. The rationale behind this choice is three-fold. First, the\nabstracts of Cochrane Reviews is a stand-alone documents [9] that should not contain\nany information that is not in the main body of the review, and the overall messages\nshould be consistent with the conclusions of the review. Second, abstracts of Cochrane\nReviews are freely available and may be the only source readers can assess the review\nresults. Finally, we need to accommodate the input length constraints of large lan-\nguage models, as the full Cochrane Review would surpass their capacity. As a result,\nour experiment provides an assessment of these LLMs’ ability to summarize medical\nevidence under a modified summarization framework. Our rigorous systematic evalu-\nation finds that ChatGPT tends to generate less factually accurate summaries when\nconditioned on the entire abstract, which could potentially indicate that the model\nmay be susceptible to distraction from irrelevant information within longer contexts.\nThis finding raises concerns about the model’s effectiveness when presented with the\nfull scope of a Cochrane Review, and suggests that it may not perform optimally in\nsuch scenarios.\nSecondly, the prompt in this study is adapted from previous work [7]. Given the\nlack of a systematic method for searching over the prompt space, it is conceivable that\nfuture studies could potentially discover more effective prompts.\nOur evaluation of LLM-generated summaries focused on six clinical domains, with\none designated expert assigned to each domain. Such evaluation requires domain\nknowledge, making it difficult for non-experts to carry out the evaluation. This con-\nstraint limits the total amount of summaries we are able to annotate. Further, we\nchose to use only the abstracts of Cochrane Reviews to evaluate human-generated sum-\nmaries (Author’s Conclusions section) since examining the entire Cochrane Review is\na time-consuming process. Therefore, it is possible that some of the errors identified\nin human reference summaries may actually be substantiated by other sections of the\nfull-length review.\n4 Methods\n4.1 Materials\nA Cochrane Review is a systematic review of scientific evidence that aims to provide a\ncomprehensive summary of all relevant studies related to a specific research question.\nReviews follow a rigorous methodology, which includes a comprehensive search for\nrelevant studies, the critical appraisal of study quality, and the synthesis of study\nfindings. The primary objective of Cochrane Reviews is to provide an unbiased and\ncomprehensive summary and meta analysis of the available evidence, to help healthcare\nprofessionals make informed decisions about the most effective treatment options.\nIn this work, we utilized Cochrane Reviews extracted from the Cochrane Library,\nwhich is a large database that provides high-quality and up-to-date information about\nthe effects of healthcare interventions. It covers a diverse range of healthcare topics,\nand our study focuses on six specific topics drawn from this resource – Alzheimer’s\ndisease, Kidney disease, Esophageal cancer, Neurological conditions, Skin disorders,\n10\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nand Heart failure. In particular, we have collected approximately ten most-recent\nreviews on each topic. Each review was verified by domain experts to ensure that they\nhave important research objectives. Table 1 shows the basic summary statistics.\nDomain Count Abstract Main results Conclusion\nAlzheimer’s disease 10 678 449 114\nKidney disease 10 887 564 95\nNeurological conditions 10 791 480 106\nSkin disorders 9 1008 763 138\nHeart failure 7 804 542 103\nEsophageal cancer 7 632 397 119\nTable 1: The number of words in the summarization dataset used\nfor human evaluation.\nWe focus on the abstracts of Cochrane Reviews in our study, which can be read\nas stand-alone documents. Each abstract includes the Background, Objectives of the\nreview, Search methods, Selection criteria, Data collection and analysis, Main results,\nand Author’s conclusions.\n4.2 Experimental setup\nIn this study, we aim to evaluate the zero-shot performance of summarizing systematic\nreviews using two OpenAI-developed models: GPT-3.5 (text-davinci-003) and Chat-\nGPT. GPT-3.5, or InstructGPT, is built upon the GPT-3 model but has undergone\nfurther training using reinforcement learning with a human feedback procedure with\nthe goal of providing better outputs preferred by humans. ChatGPT has gathered sig-\nnificant attention due to its ability to generate high-quality and human-like responses\nto conversational text prompts. Despite its impressive capabilities, it remains unclear\nwhether ChatGPT can generalize and perform high quality zero-shot summarization\nof medical evidence reviews. Therefore, we seek to investigate the comparative per-\nformance of ChatGPT and GPT-3.5 in summarizing systematic reviews of medical\nevidence data.\nTo evaluate the capabilities of the models, we have designed two distinct setups for\ninput. In the first setup, the models take the whole abstract except for the Author’s\nConclusions as the input (ChatGPT-Abstract). The second setup relies on taking\nboth the objectives and the Main results sections of the abstract as the model input\n(ChatGPT-MainResult and GPT3.5-MainResult). The objective section outlines the\nspecific research question in the PICO (Population, Intervention, Comparison, Out-\ncome) formulation that the review aims to address, while the Main results section\nprovides a summary of the results of the studies included in the review, including key\noutcomes and any statistical data, while highlighting the strengths and limitations of\nthe evidence.\nIn both settings, we use [input] + “Based on the Objectives, summarize the above\nsystematic review in four sentences” to prompt the model to perform summarization,\nwhere we emphasize the purpose of the summary by providing the Objectives section.\n11\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nWe use the Author’s Conclusions section as the human reference (see explanations in\nthe Introduction) and compare it against the models’ generated outputs.\n4.3 Automatic Evaluation Metrics\nWe select several metrics that have been widely used in text generation and summa-\nrization. ROUGE-L [10] measures the overlap between the generated summary and\nthe reference summary, focusing on the recall of the n-grams. METEOR [11] measures\nthe harmonic mean of unigram precision and recall, based on stemming and synonym\nmatching. BLEU [12] measures the overlap between the generated summary and the\nreference summary, focusing on the precision of the n-grams. In addition, we selected\ntwo reference-free metrics. Extractiveness [13] measures the percentage of words in\na summary that is from the source document. The percentage of novel n-grams sig-\nnifies the proportion of n-grams in the summary that differ from the original source\ndocument.\n4.4 Design of the Human Evaluation\nWe systematically evaluate the quality of generated summaries via human evaluation.\nWe propose to evaluate summary quality along several dimensions: (1) Factual con-\nsistency; (2) Medical harmfulness; (3) Comprehensiveness; and (4) Coherence. These\ndimensions have been previously identified and serve as essential factors in evaluating\nthe overall quality of generated summaries [15, 22, 23]. Factual consistency measures\nwhether the statements in the summary are supported by the systematic review.\nMedical harmfulness refers to the potential of a summary that leads to physical or\npsychological harm or unwanted changes in therapy or compliance due to the misinter-\npretation of information. Comprehensiveness evaluates whether a summary contains\nsufficient information to cover the objectives of the systematic review. Coherence refers\nto the ability of a summary to build a coherent body of information about a topic\nthrough sentence-to-sentence connections.\nTo assess the quality of the generated summaries, we include six domain experts,\nwith each annotating summaries for a specific topic. During the annotation process,\nparticipants are presented with the whole abstract of the systematic review, along with\nfour summaries: (1) the Authors’ conclusion section; (2) ChatGPT-MainResult; (3)\nChatGPT-Abstract; and (4) GPT3.5-MainResult. The order in which the summaries\nare presented is randomized to minimize potential order effects during the evaluation\nprocess. We utilize a 5-point Likert scale for the evaluation of each dimension. If the\nsummary received a low score on any of the dimensions, we further asked participants\nto explain the reason for the low score in a provided text box for each dimension. This\napproach enables us to perform a qualitative analysis of the responses and identify\ncommon themes to define a terminology of error types for medical evidence summa-\nrization where none exists. In addition to evaluating the quality of the summaries, we\nalso asked participants to indicate their most and least preferred summaries and to\nprovide reasons for their choices. This approach enables us to identify specific sub-\ncategories of reasons and gain insights into the potential of using model-generated\nsummaries to assist in completing the systematic review process.\n12\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nThe 5-point Likert scales between models were assessed by the Mann-Whitney U\ntest [24]. The response categories of a 5-point Likert item are coded 1 to 5 which\nwere used as numerical scores in the Mann-Whitney U test for differences. The p-\nvalue reflects if the responses of the summaries generated by two models are different,\nassuming the null hypothesis means there is no difference between the results generated\nby the two models. We used 1,000 bootstrap samples to obtain a distribution of the\nLikert scales and reported p-values.\nAcknowledgments. This work was supported by the National Library of Medicine\n(NLM) of the National Institutes of Health (NIH) under grant number 4R00LM013001\nand 5R01LM009886, NIH Bridge2AI (OTA-21-008), National Science Foundation\nunder grant numbers 2145640, 2019844, and 2303038, and Amazon Research Award.\nThe content is solely the responsibility of the authors and does not necessarily\nrepresent the official views of the NIH and NSF.\nSupplementary materials\n Error type Definition Example Consequence Source document Summary Misinterpretation errors  Contradiction Discrepancy between the conclusions drawn from the medical evidence results and the summary \nThe effect of atypical antipsychotic on psychosis in dementia is negligible  Atypical antipsychotics are effective on psychosis in dementia Potential for falsehoods  \n Certainty Illusion Inconsistency in the degree of certainty between the summary and the source document \nThere is low-quality confidence that endovascular therapy (ET) plus conventional medical treatment (CMT) compared to CMT alone causes a higher risk of short-term stroke and death \nThere is moderate-certainty evidence that ET plus CMT compared to CMT alone causes a higher risk of short-term stroke and death \nOverly convincing or uncertain summary \nFabricated errors      No evidence from the source document can be found to support or refute the statement \nTwo outcomes “help enhance satisfaction and quality of life” are not mentioned in the review  Exercise could help reduce pain and disability and help enhance satisfaction and quality of life for patients with chronic neck pain \nLack of evidence \nAttribute errors      Fabricated Attribute Incorporate an attribute for a specific symptom or outcome that is not referenced in the source document \nPopulation with intracranial artery stenosis (ICAS)   Population with recent symptomatic severe ICAS, where “recent” and “severe” cannot be inferred from the source \nMisinterpretation of the review \n Omitted Attribute Neglect an attribute for a specific symptom or outcome Specify the subtype of dementia  Not specify the subtype of dementia Overgeneralization of the conclusion  Distorted Attribute Generate incorrect attribute  Two trials are included in the study   Four trials are included in the study Misinformation of the study     \nTable 1: An Overview of Error Types.\n13\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \n0.00\n0.25\n0.50\n0.75\n1.00\nCoherence Factual Consistency Comprehensiveness\nAlzheimer's Disease\nHarmfulness\n0.00\n0.25\n0.50\n0.75\n1.00\nNeurology\n0.00\n0.25\n0.50\n0.75\n1.00\nEsophageal Cancer\n0.00\n0.25\n0.50\n0.75\n1.00\nSkin disorders\n0.00\n0.25\n0.50\n0.75\n1.00\nKidney disease\n0.00\n0.25\n0.50\n0.75\n1.00\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nHeart Failure\nStrongly Agree Agree Neutral Disagree Strongly Disagree\nFig. 1: Annotator vote distribution for (A) Coherence, (B) Factual Consistency, (C)\nComprehensiveness, and (D) Harmfulness of summaries generated by different sum-\nmarization systems in 6 clinical domains.\n14\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \n0.00\n0.05\n0.10\n0.15\n0.20\nChatGPT−\nMainResult\nChatGPT−\nAbstract\nGPT3.5−\nMainResult\nFabricated Error Attribute Error Misinterpretation Error\nErrors for factual inconsistency\nFig. 2: Statistics of errors for factual inconsistency across models.\nReferences\n[1] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K.,\nTsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prab-\nhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin,\nJ., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,\nS., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D.,\nIppolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan,\nD., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz,\nA., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B.,\nDiaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J.,\nPetrov, S., Fiedel, N.: PaLM: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311 (2022) arXiv:2204.02311 [cs.CL]\n[2] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q.,\nZhou, D.: Chain-of-thought prompting elicits reasoning in large language models.\nIn: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.)\nAdvances in Neural Information Processing Systems, vol. 35, pp. 24824–24837.\nCurran Associates, Inc., ??? (2022)\n[3] Kojima, T., Gu, S.s., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models\nare Zero-Shot reasoners. In: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D.,\nCho, K., Oh, A. (eds.) Advances in Neural Information Processing Systems, vol.\n35, pp. 22199–22213. Curran Associates, Inc., ??? (2022)\n[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A.,\nKrueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,\nHesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,\n15\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nC., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are\nFew-Shot learners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F.,\nLin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp.\n1877–1901. Curran Associates, Inc., ??? (2020)\n[5] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\nSimens, M., Askell, A., Welinder, P., Christiano, P.F., Leike, J., Lowe, R.: Train-\ning language models to follow instructions with human feedback. In: Koyejo, S.,\nMohamed, S., Agarwal, A., Belgrave, D., Cho, K., Oh, A. (eds.) Advances in Neu-\nral Information Processing Systems, vol. 35, pp. 27730–27744. Curran Associates,\nInc., ??? (2022)\n[6] OpenAI: Introducing ChatGPT. ChatGPT. Accessed: 2023-4-15 (2023)\n[7] Goyal, T., Li, J.J., Durrett, G.: News summarization and evaluation in the era of\nGPT-3. arXiv preprint arXiv:2209.12356 (2022) arXiv:2209.12356 [cs.CL]\n[8] Gao, C.A., Howard, F.M., Markov, N.S., Dyer, E.C., Ramesh, S., Luo, Y., Pear-\nson, A.T.: Comparing scientific abstracts generated by ChatGPT to original\nabstracts using an artificial intelligence output detector, plagiarism detector, and\nblinded human reviewers. bioRxiv (2022)\n[9] Beller, E.M., Glasziou, P.P., Altman, D.G., Hopewell, S., Bastian, H., Chalmers,\nI., Gøtzsche, P.C., Lasserson, T., Tovey, D., PRISMA for Abstracts Group:\nPRISMA for abstracts: reporting systematic reviews in journal and conference\nabstracts. PLoS Med. 10(4), 1001419 (2013) https://doi.org/10.1371/journal.\npmed.1001419\n[10] Lin, C.-Y.: ROUGE: a package for automatic evaluation of summaries. In: Text\nSummarization Branches Out: Proceedings of the ACL-04 Workshop, vol. 8, pp.\n1–8. Barcelona, Spain, ??? (2004)\n[11] Banerjee, S., Lavie, A.: METEOR: an automatic metric for MT evaluation with\nimproved correlation with human judgments. In: Proceedings of the ACL Work-\nshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation\nAnd/or Summarization, pp. 65–72 (2005)\n[12] Papineni, K., Roukos, S., Ward, T., Zhu, W.-J.: BLEU: a method for automatic\nevaluation of machine translation. In: Proceedings of the 40th Annual Meeting\non Association for Computational Linguistics. ACL ’02, pp. 311–318. Association\nfor Computational Linguistics, USA (2002)\n[13] Grusky, M., Naaman, M., Artzi, Y.: Newsroom: A dataset of 1.3 million sum-\nmaries with diverse extractive strategies. In: Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long Papers), pp. 708–719.\nAssociation for Computational Linguistics, Stroudsburg, PA, USA (2018). https:\n//doi.org/10.18653/v1/n18-1065\n[14] Fabbri, A.R., Kry´ sci´ nski, W., McCann, B., Xiong, C., Socher, R., Radev, D.: Sum-\nmEval: Re-evaluating summarization evaluation. Trans. Assoc. Comput. Linguist.\n9, 391–409 (2021)\n[15] Tang, L., Kooragayalu, S., Wang, Y., Ding, Y., Durrett, G., Rousseau, J.F.,\n16\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint \nPeng, Y.: EchoGen: Generating conclusions from echocardiogram notes. In: Pro-\nceedings of the 21st Workshop on Biomedical Language Processing, pp. 359–368.\nAssociation for Computational Linguistics, Dublin, Ireland (2022)\n[16] Tang, L., Goyal, T., Fabbri, A.R., Laban, P., Xu, J., Yahvuz, S., Kry´ sci´ nski,\nW., Rousseau, J.F., Durrett, G.: Understanding factual errors in summarization:\nErrors, summarizers, datasets, error detectors. arXiv preprint arXiv:2205.12854\n(2022) arXiv:2205.12854 [cs.CL]\n[17] M¨ uhlbauer, V., M¨ ohler, R., Dichter, M.N., Zuidema, S.U., K¨ opke, S., Luijendijk,\nH.J.: Antipsychotics for agitation and psychosis in people with alzheimer’s disease\nand vascular dementia. Cochrane Database Syst. Rev. 12(12), 013304 (2021)\nhttps://doi.org/10.1002/14651858.CD013304.pub2\n[18] Luo, J., Wang, T., Yang, K., Wang, X., Xu, R., Gong, H., Zhang, X., Wang,\nJ., Yang, R., Gao, P., Ma, Y., Jiao, L.: Endovascular therapy versus medical\ntreatment for symptomatic intracranial artery stenosis. Cochrane Database Syst.\nRev. 2, 013267 (2023) https://doi.org/10.1002/14651858.CD013267.pub3\n[19] Gross, A., Kay, T.M., Paquin, J.-P., Blanchette, S., Lalonde, P., Christie, T.,\nDupont, G., Graham, N., Burnie, S.J., Gelley, G., Goldsmith, C.H., Forget, M.,\nHoving, J.L., Brønfort, G., Santaguida, P.L., Cervical Overview Group: Exercises\nfor mechanical neck disorders. Cochrane Database Syst. Rev.1(1), 004250 (2015)\n[20] Kamo, T., Wada, Y., Okamura, M., Sakai, K., Momosaki, R., Taito, S.: Repetitive\nperipheral magnetic stimulation for impairment and disability in people after\nstroke. Cochrane Database Syst. Rev. 9(9), 011968 (2022) https://doi.org/10.\n1002/14651858.CD011968.pub4\n[21] Zhang, Y., Ni, A., Mao, Z., Wu, C.H., Zhu, C., Deb, B., Awadallah, A., Radev,\nD., Zhang, R.: Summ N: A Multi-Stage summarization framework for long input\ndialogues and documents. In: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 1592–\n1604. Association for Computational Linguistics, Dublin, Ireland (2022). https:\n//doi.org/10.18653/v1/2022.acl-long.112\n[22] Singhal, K., Azizi, S., Tu, T., Sara Mahdavi, S., Wei, J., Chung, H.W., Scales, N.,\nTanwani, A., Cole-Lewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble, P.,\nKelly, C., Scharli, N., Chowdhery, A., Mansfield, P., Arcas, B., Webster, D., Cor-\nrado, G.S., Matias, Y., Chou, K., Gottweis, J., Tomasev, N., Liu, Y., Rajkomar,\nA., Barral, J., Semturs, C., Karthikesalingam, A., Natarajan, V.: Large lan-\nguage models encode clinical knowledge. arXiv preprint arXiv:2212.13138 (2022)\narXiv:2212.13138 [cs.CL]\n[23] Jeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., St¨ uber, A.T., Topalis, J.,\nWeber, T., Wesp, P., Sabel, B., Ricke, J., Ingrisch, M.: ChatGPT makes medicine\neasy to swallow: An exploratory case study on simplified radiology reports. arXiv\npreprint arXiv:2212.14882 (2022) arXiv:2212.14882 [cs.CL]\n[24] Clason, D., Dormody, T.: Analyzing data measured by individual likert-type\nitems. J. Agric. Educ. 35(4), 31–35 (1994)\n17\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 24, 2023. ; https://doi.org/10.1101/2023.04.22.23288967doi: medRxiv preprint ",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9170899391174316
    },
    {
      "name": "Misinformation",
      "score": 0.8098605275154114
    },
    {
      "name": "Terminology",
      "score": 0.6007152795791626
    },
    {
      "name": "Harm",
      "score": 0.5865404009819031
    },
    {
      "name": "Computer science",
      "score": 0.5616562366485596
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5400979518890381
    },
    {
      "name": "Salient",
      "score": 0.5307405591011047
    },
    {
      "name": "Natural language processing",
      "score": 0.3997398018836975
    },
    {
      "name": "Data science",
      "score": 0.3641303479671478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2867678999900818
    },
    {
      "name": "Psychology",
      "score": 0.2852376103401184
    },
    {
      "name": "Linguistics",
      "score": 0.20837727189064026
    },
    {
      "name": "Social psychology",
      "score": 0.17397457361221313
    },
    {
      "name": "Computer security",
      "score": 0.1546056866645813
    },
    {
      "name": "Epistemology",
      "score": 0.10483753681182861
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}