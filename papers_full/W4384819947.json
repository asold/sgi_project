{
  "title": "Leveraging pre-trained language models for mining microbiome-disease relationships",
  "url": "https://openalex.org/W4384819947",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092500146",
      "name": "Nikitha Karkera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3171139065",
      "name": "Sathwik Acharya",
      "affiliations": [
        "PES University",
        "Systems Biology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2119150818",
      "name": "Sucheendra K. Palaniappan",
      "affiliations": [
        null,
        "Systems Biology Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2012606594",
    "https://openalex.org/W2116846792",
    "https://openalex.org/W3179076314",
    "https://openalex.org/W2807176368",
    "https://openalex.org/W3119962804",
    "https://openalex.org/W2950173838",
    "https://openalex.org/W4306728852",
    "https://openalex.org/W4304116809",
    "https://openalex.org/W4255502791",
    "https://openalex.org/W2322763599",
    "https://openalex.org/W3211263674",
    "https://openalex.org/W2972021296",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W2431459199",
    "https://openalex.org/W4293727927",
    "https://openalex.org/W4286322052",
    "https://openalex.org/W2916981547",
    "https://openalex.org/W3097354806",
    "https://openalex.org/W2099320393",
    "https://openalex.org/W2914431807",
    "https://openalex.org/W4205340501",
    "https://openalex.org/W3132948971",
    "https://openalex.org/W3170383918",
    "https://openalex.org/W3033077186",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W6837901829",
    "https://openalex.org/W6601298480",
    "https://openalex.org/W3170949898",
    "https://openalex.org/W2765928324",
    "https://openalex.org/W4318624508",
    "https://openalex.org/W3157142061",
    "https://openalex.org/W2979072963",
    "https://openalex.org/W3036061157",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W2776589520"
  ],
  "abstract": "Abstract Background The growing recognition of the microbiome’s impact on human health and well-being has prompted extensive research into discovering the links between microbiome dysbiosis and disease (healthy) states. However, this valuable information is scattered in unstructured form within biomedical literature. The structured extraction and qualification of microbe-disease interactions are important. In parallel, recent advancements in deep-learning-based natural language processing algorithms have revolutionized language-related tasks such as ours. This study aims to leverage state-of-the-art deep-learning language models to extract microbe-disease relationships from biomedical literature. Results In this study, we first evaluate multiple pre-trained large language models within a zero-shot or few-shot learning context. In this setting, the models performed poorly out of the box, emphasizing the need for domain-specific fine-tuning of these language models. Subsequently, we fine-tune multiple language models (specifically, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, and BioLinkBERT) using labeled training data and evaluate their performance. Our experimental results demonstrate the state-of-the-art performance of these fine-tuned models ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 score, precision, and recall of over $$&gt;0.8$$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mrow> <mml:mo>&gt;</mml:mo> <mml:mn>0.8</mml:mn> </mml:mrow> </mml:math> compared to the previous best of 0.74. Conclusion Overall, this study establishes that pre-trained language models excel as transfer learners when fine-tuned with domain and problem-specific data, enabling them to achieve state-of-the-art results even with limited training data for extracting microbiome-disease interactions from scientific publications.",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nKarkera et al. BMC Bioinformatics          (2023) 24:290  \nhttps://doi.org/10.1186/s12859-023-05411-z\nBMC Bioinformatics\nLeveraging pre-trained language models \nfor mining microbiome-disease relationships\nNikitha Karkera4, Sathwik Acharya1,3 and Sucheendra K. Palaniappan1,2,4* \nAbstract \nBackground: The growing recognition of the microbiome’s impact on human \nhealth and well-being has prompted extensive research into discovering the links \nbetween microbiome dysbiosis and disease (healthy) states. However, this valuable \ninformation is scattered in unstructured form within biomedical literature. The struc-\ntured extraction and qualification of microbe-disease interactions are important. In \nparallel, recent advancements in deep-learning-based natural language processing \nalgorithms have revolutionized language-related tasks such as ours. This study aims \nto leverage state-of-the-art deep-learning language models to extract microbe-disease \nrelationships from biomedical literature.\nResults: In this study, we first evaluate multiple pre-trained large language models \nwithin a zero-shot or few-shot learning context. In this setting, the models performed \npoorly out of the box, emphasizing the need for domain-specific fine-tuning of these \nlanguage models. Subsequently, we fine-tune multiple language models (specifi-\ncally, GPT-3, BioGPT, BioMedLM, BERT, BioMegatron, PubMedBERT, BioClinicalBERT, \nand BioLinkBERT) using labeled training data and evaluate their performance. Our \nexperimental results demonstrate the state-of-the-art performance of these fine-tuned \nmodels ( specifically GPT-3, BioMedLM, and BioLinkBERT), achieving an average F1 \nscore, precision, and recall of over > 0.8 compared to the previous best of  0.74.\nConclusion: Overall, this study establishes that pre-trained language models excel \nas transfer learners when fine-tuned with domain and problem-specific data, enabling \nthem to achieve state-of-the-art results even with limited training data for extracting \nmicrobiome-disease interactions from scientific publications.\nKeywords: Microbe-disease relationship extraction, Language models, Fine-tuning, \nDeep-learning, Transfer learning, Biomedical informatics, Natural language processing\nIntroduction\nMicroorganisms, in the trillions, are housed and sheltered in the human body. These \nmicroorganisms take up residence in various organs, including the gastrointesti -\nnal tract, mouth, stomach, skin, urogenital tract, and others. Their presence plays a \ncrucial role in maintaining the host’s health and well-being [1 ]. Collectively, these \nmicrobes form what is known as the microbiome. Recent technological advancements \nhave enabled us to study and quantify the microorganisms within our bodies. As a \n*Correspondence:   \nsucheendra@sbi.jp\n1 The Systems Biology Institute, \nTokyo, Japan\n2 Iom Bioworks Pvt Ltd., \nBengaluru, India\n3 PES University, Bengaluru, India\n4 SBX Corporation, Tokyo, Japan\nPage 2 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nresult, we can now establish both correlational and causal relationships between dys -\nbiosis of the microbiome and disease states [2 ].\nStructured knowledge representing the relationship between microorganisms and \ndiseases can greatly contribute in deepening the development of microbiome-based \npreventive and therapeutic measures. Knowledge repositories that encompass collec -\ntive information on microbiome-disease associations is usually constructed based on \nevidence from scientific publications. These manually curated knowledge bases are \nfrequently utilized for downstream analysis and discovery. Several endeavors, such as \nAmadis [3 ], Disbiome [4 ], MicroPhenoDB [5 ], The Virtual Metabolic Human database \n[6], MADET [7 ], gutMDisorder [8 , 9], HMDAD [10], mBodyMap [11], have focused \non at cataloging and organizing this information. While these knowledge bases are of \nhigh quality, their construction is a labor-intensive and expensive process due to the \nsubstantial manual effort required for curation. Furthermore, keeping these databases \nup-to-date poses significant challenges, particularly given the rapid pace of micro -\nbiome research and the continuous accumulation of new findings. To provide per -\nspective, a search for the keyword “microbiome” on PubMed returns over 140,  000 \nabstracts, with more than 27, 000 abstracts published in 2022 alone.\nNatural Language Processing (NLP) techniques have emerged as a promising \napproach to effectively handle the vast amount of scientific literature. These methods \nenable automated analysis of extensive scientific texts and the extraction of relevant \ninformation, which can then be stored in knowledge bases. In recent years, the field of \nNLP has witnessed substantial advancements owing to the emergence of Large Lan -\nguage Models (LLMs) and Generative AI models [12]. Consequently, there has been a \ngrowing interest in leveraging these techniques to tackle problems in the microbiome \nfield as well [13, 14]. Of particular significance is the work presented by Badal et al. \n[13], which highlights the key challenges that must be addressed to establish mean -\ningful knowledge bases for the microbiome disease problem. This research provides \nvaluable insights into the nuances and intricacies of the problems in this subdomain \nand serves as a foundation for our work too.\nTo address the specific challenge of extracting associations between diseases and \nthe microbiome using NLP techniques, the solution skeleton is usually a combination \nof the following steps:\n• Identifying disease and microbe mentioned in scientific texts. This step typically \ninvolves utilizing algorithms such as Named Entity Recognizers (NERs), linguis -\ntic taggers, and dictionaries to locate disease and microbe references within each \ndocument or sentence.\n• Establishing the existence of a relationship or association between pairs of dis -\neases and microbes. Relationship extraction algorithms are commonly employed \nfor this task. Extensive research has been conducted in this area, with notable \ncontributions so far [10, 15–19].\n• Once the presence of a relationship is established, determining the nature of the \nrelationship. For example, investigating whether the presence of a specific bac -\nterium is positively correlated with a particular disease. Specialized relationship \nPage 3 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nextraction algorithms are employed to address this, which is also the primary \nfocus of this paper.\nRelated work\nAt its core, the current problem is that of relation extraction which has decades of prior \nwork in the Bio-NLP domain [20, 21]. However, the traditional models for relation \nextraction are now being surpassed by deep learning based NLP models [22] which are \nshown to be superior in their performance.\nIn terms of related work to the disease-microbiome extraction task, two main works \nare related to ours. First, Park et al. [23], proposed an ensemble model for this problem. \nTheir approach involves two steps: first, a relation detection model based on Hierarchi -\ncal Long Short-Term Memory (LSTM) networks to determine the presence of a disease-\nmicrobe relationship. Second, they extract the specific relation type by employing a \nsubstantial collection of rule sets or patterns, amounting to around 1000. However, this \napproach has limitations as it requires manual maintenance of the rule list for relation \nextraction, making it impractical for large-scale efforts.\nNext is the work by Wu et  al[24] that focuses on a deep-learning strategy for solv -\ning this problem. Their approach first involves preparing training datasets. For this, \nthey start by collecting a large corpus of text from PubMed related to microbiome and \ndiseases and subsequently employ Named Entity Recognition (NER) tools to identify \nmicrobe and disease entities within the text. Next, they manually create two corpora for \nmicrobe-disease interactions: a high-quality gold-standard corpus (GSC) and a silver-\nstandard corpus (SSC) that is known to contain errors. These corpora are then used as \ntraining data. Subsequently, they utilize a deep-learning-based relation extraction algo -\nrithm [25] to train a deep-learning model using the GSC data which did not yield the \nbest results. Subsequently, they implement a 2-step learning process, where the model is \nfirst trained on the error-prone SSC corpus and then fine-tuned using transfer learning \non the GSC corpus. The authors report that this 2-step approach significantly improves \nthe accuracy of relationship extraction, achieving an F-score of 0.7381. For our problem \nstatement, this result represents the current state-of-the-art as reported in the scientific \nliterature. While the approach holds interest, we theorized that the expensive training \nof deep-learning models could be avoided by directly fine-tuning pre-trained models, in \naddition to improved accuracy gains.\nOur contribution\nOur paper offers multiple contributions. Firstly, we recognize the significant \nadvancements in the field of large language models such as GPT-3 [26], BERT [27] \netc. Leveraging the power of these models, we utilize them in our task to achieve \nstate-of-the-art results. The utilization of deep learning and transformer models \nallows us to effectively capture complex patterns and relationships within the lit -\nerature. Secondly, our approach highlights the relevance of deep learning and trans -\nformer models in reducing the requirement for large amounts of training data. \nIn contrast to the study by [24] or other deep learning models, our method ben -\nefits from transfer learning and task-specific fine-tuning. This advantage enables \nPage 4 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nus to achieve excellent results with a smaller amount of training data, making our \napproach efficient and practical. Furthermore, our approach can handle cases where \ndisease-microbe relationships span multiple sentences. Deep learning and trans -\nformer models have the capacity to capture long-range dependencies and contextual \ninformation, allowing us to effectively address the complexity of such relationships.\nTo tackle the task of relationship extraction, we adopt two solution strategies. \nFirst, we treat it as a classification (discriminative) task. Here, the model receives \nthe evidence describing the relationship between a disease and a microbe as input, \nalong with a question probing their relationship. The model’s output is expected to \nprovide the answer from one of the four labels: positive, negative, relate, or NA. This \napproach enables us to utilize the discriminative power of deep learning and trans -\nformer models.\nNext, we reformulate the task as a generative text task. In this setup, we pro -\nvide the model with evidence describing the relationship between a disease and a \nmicrobe, as well as a question as a prompt. The model is then tasked with generating \nthe correct label. This generative approach leverages the expressive nature of deep \nlearning and transformer models to generate informative and accurate labels.\nFor further details on our methodology and experimental setups, we provide com -\nprehensive information in the subsequent sections of our paper, highlighting the \nspecific ways in which deep learning and transformer models contribute to the suc -\ncess of our approach.\nTo establish baselines, we employ various state-of-the-art models, both domain-\nindependent and domain-specific. Initially, we explore the potential of pre-trained \nlanguage models as zero-shot and few-shot learners, without fine-tuning, potentially \neliminating the need for training data or specialized retraining and fine-tuning. Sub -\nsequently, we fine-tune these models using curated training data, obtained from \n[24], to further enhance their capabilities.\nFirstly, among the generative models, BioGPT [28], BioMedLM [29] and GPT-3 \n[26] were considered. These models are designed to generate/complete the response \nbased on the given question and context(prompt). They were a natural choice for \nour task, as they are known to perform well in zero-shot or few-shot learning sce -\nnarios. Additionally, we incorporated the following language models in the discrimi -\nnative setting: BERT [30], ClinicalBERT [31], PubMedBERT [32], BioMegatron [33] \nand BioLinkBERT [34]. With these models, the objective was for the model to clas -\nsify the input into one of the four predefined labels. Details of our study and the \nsummary of the outcomes are shown in Fig. 1 .\nOur experiments show that these models fall short in the zero-shot or few-shot \nsetup, highlighting the need for domain-specific fine-tuning and task-specific train -\ning data. After fine-tuning, among the generative models, GPT-3 performed well. \nHowever, we observed that these models sometimes produced varying outputs \nfor the same prompt, owing to their generative nature, which can pose challenges. \nAmong the discriminative models, the model fine-tuned on BioLinkBERT consist -\nently yielded the best results among the tested models. The next section formally \nintroduces the problem and the solution strategy.\nPage 5 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nProblem formulation and models considered\nOur objective is to extract and determine the relationships between disease and \nmicrobe terms from natural language text. Formally, given a scientific text T  and a \npair of entities (e 1, e2) occurring in T , where e1 ∈ D  and e2 ∈ M  , with D and M repre-\nsenting the sets of all disease and microbe names, respectively, the task is to predict a \nlabel y that represents the relationship between e 1 and e 2. The label y  belongs to the \nset {positive, negative, relate, NA} .\nOne approach to mathematically formulate this problem is by using a supervised \nlearning method. In this approach, the model is trained on a dataset consisting of \nlabeled sentences and entities. The model learns a function f  that maps the scientific \ntext and entity pair (T ,  e1,  e2) to a predicted label y , such that y = f(T ,e1,e2) . The \nfunction f can be realized using various machine-learning techniques and models.\nIn the context of microbe-disease relationships, the following labels are defined \n[24]:\n• (positive): This label indicates a positive correlation between the microbe and the \ndisease. It implies that the microbe can worsen the disease or that its presence \nincreases when the disease occurs.\n• (negative): This label indicates a negative correlation between the microbe and the \ndisease. It suggests that the microbe can act as a treatment for the disease or that \nits presence decreases when the disease occurs.\n• (relate): This label indicates a relationship between the microbe and the disease \nwithout additional information about whether it was positive or negative. It signi -\nfies that they appear to be associated with each other. In a sense, this label can be \nconsidered a super-set of positive and negative labels.\n• (NA): This label indicates that the microbe and the disease mentioned in the text \nare not related to each other.\nFig. 1 Our contributions and study design for extracting disease-microbiome relationships\nPage 6 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nIn our case, given the same scientific text T and entities of interest (e1, e2), and a ques-\ntion posed as follows: “What is the relationship between e1 and e2?” our models are \nexpected to provide an answer from the set positive,  negative, relate, NA. The problem \nformulation in our setting is illustrated in Fig. 2.\nPre‑trained language models considered\nNow, we describe the various pre-trained models that were leveraged for fine-tuning in \nthis study. The choice of models was based on best-in-class for biomedical domain spe -\ncific tasks.\nGenerative setting\nBioMedLM 2.7B [29] is a large language model trained on a dataset of biomedical lit -\nerature and is based on GPT-2 model. It has 2.7 billion parameters. It is effective for a \nvariety of tasks, including natural language inference, question answering, and text sum -\nmarization. BioMedLM 2.7B is a valuable tool for researchers and clinicians who need to \naccess and process biomedical information.\nBioGPT [28] is a domain-specific generative pre-trained transformer language model \nfor biomedical text generation and mining. It is trained on 15 million PubMed abstracts \nand has 1.5 billion parameters. It was developed by Microsoft Research and is based on \nthe GPT-2 language model.\nGPT-3 [26], or Generative Pre-trained Transformer 3, is a state-of-the-art language \nmodel developed by OpenAI. With 175 billion parameters, it exhibits remarkable profi -\nciency in generating coherent and contextually relevant text across various domains. The \nmost competent model available in OpenAI is “text-davinci-003, ” while there are other \nmodels as well. The prompt used for this experiment is detailed in the supplementary \nwebsite [see Additional file 1]. For all experiments, we changed the Temperature param-\neter to 0 making the outputs less random.\nFig. 2 Problem formulation for inferring microbe-disease relationship\nPage 7 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nDiscriminative setting\nBERT(Bidirectional Encoder Representations from Transformers) model [27] is \namong the most well-known and early LLMs based on transformer architectures. It \nwas specifically trained on Wikipedia and Google’s BooksCorpus. BERT is known to \nbe a very good general-purpose model that works well for most language tasks. In \nour case, we used BERT first to see if generic models could perform well for our task \nbefore resorting to domain-specific adaptations. For our experiments, we used the \n“Bert-base-uncased” model from the Hugging Face library [35].\nPubMedBERT is a BERT-based model pre-trained from scratch using 14 million \nabstracts from PubMed. It consistently outperforms all the other BERT models in \nmost biomedical NLP tasks, often by a significant margin as reported in [32]. Specifi -\ncally, microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext is the pre-\ntrained model that was utilized.\nBioMegatron models are pre-trained from scratch on the PubMed dataset. A large \nbiomedical language model pre-trained on a large literature corpus is an excellent \nstarting point for identifying the microbiome-disease relation type. The pre-training \nof this model takes PubMed abstracts and full-text commercial-collection (CC) that \nare free of copyrights. When compared to the prior state-of-the-art (SOTA), BioMeg -\natron significantly outperformed across a variety of tasks. In contrast to models pre-\ntrained on wide domain datasets, [33] demonstrates that language models specialized \nfor a particular domain perform at their best.\nBioLINK-BERT model is trained on abstract data, similar to PubMedBERT, but with \nthe addition of citation links between articles [34]. Unlike previous works, which only \nuse their raw text for pre-training, academic papers have extensive dependencies on \none another through citations (references). Incorporating citation links assists lan -\nguage models in learning the dependencies between papers and the knowledge that \nspans them.\nBioClinicalBERT [ 36] is a BERT-based language model that has been pre-trained \non a dataset of 880 million words of biomedical and clinical text which allows it to \nbetter understand and generate text from both domains. It is a further development \nof BioBERT. BioClinicalBERT is available through the Hugging Face Transformers \nlibrary. It is designed to improve the performance of biomedical natural language \nprocessing (NLP) tasks, such as named entity recognition, and relation extraction.\nPoor performance in zero‑shot and few‑shot setting\nWe first investigated the performance of generative language models in a zero-shot \nand few-shot learning setting. We evaluated all three models, the details of the \nprompts used are detailed in the supplementary website [see Additional file 1].\nZero-shot learning allows the model to extract relationships between disease and \nmicrobiome without requiring specific training dataset for those relationships. By \nleveraging the pre-trained knowledge and semantic understanding encoded within \nthe language model, the model can generalize and infer relationships based on the \nprovided input. This approach is particularly valuable when dealing with unseen \nPage 8 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nrelationship types, as it enables the model to make predictions even for relationships \nit has not encountered during training.\nOn the other hand, few-shot learning extends the capabilities of zero-shot learning by \nenabling the model to learn from a limited number of labeled examples for a specific \nrelationship. Rather than relying solely on pre-encoded knowledge, few-shot learning \nallows the model to adapt and make accurate predictions using the additional labeled \ndata. By leveraging both the pre-trained knowledge and the limited labeled examples, \nfew-shot learning enhances the model’s ability to generalize and extract relationships, \neven in scenarios where labeled data is scarce or new relationship types are introduced. \nWe use a “two-shot-learning” setup to infer the microbe-disease relationship. More spe -\ncifically, two examples of scientific text per class along with its annotation were provided \nfor each of the labels as shown in Fig.  3. A natural language description is also provided \nalong with the prompt for the model to learn about the task as this often improves the \nmodel’s performance (See [26, 37].\nExperimental results\nIn summary, for the zero-shot setting, we found that the performance of the models was \npoor as shown in Table  1. For GPT-3, the outputs are generated within the four labels, \nbut they don’t follow the same casing for every predicted label. It achieved a f1-score of \n0.5 with low precision. A detailed analysis of the results showed that the model performs \nFig. 3 Zero-shot and few-shot learning setup for inferring microbe-disease relationship using GPT-3. The \nGround truth for this example is “negative” . However GPT-3 returns “relate” in zero-shot and “positive” in \nfew-shot setting\nTable 1 Performance metrics of the GPT-3 model in the zero-shot and few-shot setting\nModel Accuracy F1 score Precision Recall\nGPT-3 (zero-shot) 0.48 0.50 0.58 0.48\nGPT-3 (few-shot) 0.57 0.56 0.57 0.57\nPage 9 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \npoorly for the “NA” class. Details can be found in the supplementary website [see Addi -\ntional file  1]. Surprisingly, BioMedLM and BioGPT did not even produce sensible \noutputs.\nEven in the few-shot setting, we noticed that there were only marginal improvements \nin the results with GPT-3 (f1-score of 0.56), while no tangible outputs were generated \nfor BioMedLM and BioGPT as shown in Table  1. It was clear that despite the general \nobservation that generative models provide good outcomes in zero or few-shot learning \nsettings, performance still depends on the task-specific domain. Similar outcomes have \nbeen established previously [38]. For our problem, using models out of the box was of \nlimited utility, this could also be due to the counterintuitive definition of positive and \nnegative labels. For all experiments in this section, default model parameters were used.\nDataset for fine‑tuning: considerations for improved accuracy\nTo fine-tune the various language models for our task, we utilized the human-annotated \ngold-standard corpus (GSC) from [24]. This dataset consists of 1100 sentence instances \nthat describe interactions between the microbiome and diseases, along with correspond-\ning labels of “positive, ” “negative, ” “relate, ” and “NA. ” These sentences were selected by \nemploying a semi-automated pipeline to identify diseases and microbes mentioned in \narticles from PubMed and PubMed Central using the keyword “microbe” . Expert anno -\ntators then reviewed each sentence and assigned them to one of the four categories. For \na comprehensive understanding of how the GSC dataset was constructed, we refer inter-\nested readers to [24]. Out of the 1100 sentences, the distribution of classes is depicted in \nshown in Fig. 4. This dataset served as the basis for fine-tuning our pipeline.\nInterestingly, we identified a significant number of labeling errors in the GSC data -\nset, particularly for the “NA” category. To address this issue, we re-annotated the state -\nments with two postdoctoral-level researchers who re-labeled all the sentences initially \nmarked as “NA” . The annotation process was facilitated using the Doccanno tool [39]. \nFig. 4 Distribution of relation types for GSC after correcting for issues in “NA” relation\nPage 10 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nEach researcher independently re-annotated all the “NA” labels, followed by a collabo -\nrative review of each other’s annotations. Through consensus, a final label was agreed \nupon for each sentence.\nOur analysis revealed that out of the original 258 sentences labeled as “NA” in the GSC \ndataset, 178 sentences were found to be mislabeled. In the supplementary website [see \nAdditional file 1], we provide illustrative examples and a comprehensive list of the re-\nannotated data. As depicted in Fig.  4, there was a substantial decrease in the number of \ntraining data points for the “NA” category, accompanied by an increase in samples for \nthe negative, positive, and relate categories. All further fine-tuning was performed on \nthe corrected dataset. This dataset is available for review in the supplementary website \n[see Additional file 1].\nDomain specific fine‑tuning of different language models\nWe fine-tune all the models using the dataset outlined in the preceding section for the \ntask at hand. Notably, the methodology employed by [24] involved a two-step approach. \nIn the first step, an error-prone silver training corpus was utilized to train a rela -\ntion extraction algorithm [25]. Subsequently, the model trained in the initial step was \nemployed in conjunction with transfer learning techniques for fine-tuning with the GSC \ndataset. The authors demonstrated that this two-step transfer learning process yielded \nstate-of-the-art (SOTA) results. Although this approach is interesting, we hypothesized \nthe costly training of deep-learning models could be circumvented by directly fine-tun -\ning pre-trained models. In essence, our strategy is to take models that were previously \ntrained on extensive generic or domain-specific free text as the foundation, and subse -\nquently fine-tune them specifically for the targeted problem using a minimal quantity of \nhigh-quality training data (in our case, GSC).\nMethodology\nFigure 5 shows the mechanism in which the evidence-question are taken in pairs for the \nfine-tuning purpose for the discriminative class of models namely BERT [30], BioMega -\ntron [33], PubMedBERT [32], BioClinicalBERT [28] and BioLinkBERT [34]. For this, we \nresort to a typical tokenizing procedure of the evidence-question pair to produce token \nIDs and the attention mask. A maximum sequence length of 512 is maintained, as this is \nwhat BERT-based models are limited to. For all base models considered, during the fine-\ntuning process, a learning rate of 5e − 5 , a weight decay of 0.01, number of epochs=7, \nand Adam’s optimizer with a layer-wise learning rate decay of 0.9 was applied. All mod -\nels were trained using an NVIDIA GeForce RTX 2080 Ti with 12GB memory, 16 CPUs, \nand 64GB memory. The fine-tuning process took about 30 min for each model.\nAmong the generative models, GPT-3 model was fine-tuned using the OpenAI API \nof the GPT-3 davinci model. Details of the fine-tuning process are available on the \nOpenAI website (link: https:// openai. com/ api/). The inference parameters for the fine-\ntuned model are t=0.0, top_p=1, max_tokens=1 with other parameters with its default \nsettings. BioMedLM’s stanford-crfm/BioMedLM model was fine-tuned on an A40 \nGPU instance with deepspeed setting for efficiency. This helped in training the model \nwith  18GB GPU memory utilization. The model was trained for 20 epochs with batch_\nsize=2, gradient_accumulation_steps=2, learning_rate=2e-06 with other parameters \nPage 11 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nkept the same as their default settings. It took around 7hrs to complete 20 epochs. Simi -\nlarly, BioGPT was fine-tuned on a single NVIDIA GeForce RTX 2080 Ti with 12GB \nmemory. The model was trained using parameters similar to DDI (Drug-Drug Interac -\ntion) experiment in BioGPT [28] for Relation extraction purposes.\nData preparation for fine‑tuning\nThis section provides details of the data processing and prompt design for the fine-tun -\ning process of different models.\nDiscriminative models\nThe training data format for fine-tuning these models follows a simple structure. Each \nexample consists of an input and an output. The input is represented by two strings, \nan Evidence String and a Question String, separated by a delimiter. For example, the \nEvidence string can be “ Additionally, some members of the phylum such as Faecalibac -\nterium prausnitzii, a member of the Clostridiales-Ruminococcaceae lineage have been \nshown to have anti-inflammatory effects due to the production of the short-chain fatty \nacid butyrate and have been negatively correlated with inflammatory bowel disease. ” \nand Question as “What is the relationship between inflammatory bowel disease and \nClostridiales ?” . The output corresponds to the target label associated with the given \ninput as shown below.\nFig. 5 Illustration and mapping of the evidence and question tokens into the models for our discriminative \nclass of models\nPage 12 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nThis training data format allows for a straightforward mapping between the input evi -\ndence and question, and the corresponding output label. By fine-tuning the pre-trained \nmodels on the GSC dataset encoded as above, the model can learn to effectively under -\nstand the relationship between the evidence and question, and generate accurate labels \nor predictions based on the input provided.\nGenerative models\nThe training data format for GPT-3 model consists of a collection of examples, each rep-\nresented by a prompt and a completion string that corresponds to the label.\nThe “prompt” key corresponds to the text that serves as the input or context for the \nmodel. It contains evidence related to the microbiome and disease relationship. In this \nformat, the prompt text is structured as the evidence string followed by a question \nstring, separated by a line break (“\\n”). The evidence string provides the background or \nsupporting information, while the question string represents the specific question to be \nanswered by the model. For example, a prompt can be:\n “Evidence: Additionally, some members of the phylum such as Faecalibacterium praus-\nnitzii, a member of the Clostridiales-Ruminococcaceae lineage have been shown to have \nanti-inflammatory effects due to production of the short-chain fatty acid butyrate and \nhave been negatively correlated with inflammatory bowel disease107.\\n Question: What \nis the relationship between inflammatory bowel disease107 and Clostridiales ?\\n\\n####\\\nn\\n” .  Here, the ending string “\\n\\n####\\n\\n” acts as a fixed separator to the model. For \ninference, the prompts are designed in the same format as the training dataset including \nthe same separator with the same stop sequence to properly truncate the completion. \nThe training data format allows for multiple examples to be included, each following \nthe same key-value structure. The detailed prompts for BioMedLM and BioGPT which \nare very similar to the GPT-3 prompts can be found on the supplementary website [see \nAdditional file 1].\nResults\nTo mitigate the risk of overfitting, model performance was evaluated using a 5-fold \ncross-validation strategy. The curated dataset was divided into five equal parts, referred \nto as folds. In each iteration, the models were fine-tuned using four folds for training \nand evaluated on the remaining fold. This process was repeated five times, with each \nfold serving as the test set once. The average of the five test scores was calculated to \nPage 13 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nprovide the final metrics of the model. We assessed the performance using several met -\nrics, including Accuracy, Weighted Average F1 score, Precision, and Recall, which align \nwith those reported in [24]. The detailed results are presented in Table  2. The results of \nthe study by [24] are shown in the table using the notation BERETL(MDI ) . The reported \nf1-score reached a peak value of 0.738 along with closely aligned precision and recall \nscores. These results serve as our baseline.\nComing to the performance of the discriminative fine-tuned models, we observed \nsignificant improvements across the entire spectrum. Notably, the model trained \non BioLinkBERT-base yielded the best results, achieving an average F1-score of \n0.804 ± 0.0362 in a 5-fold cross-validation setup. Detailed information regarding all the \nmodels and the fine-tuning parameters can be found on our supplementary website (see \nAdditional file 1). Further, to understand the characteristics of the classifier better, we \nplotted the precision-recall curves as shown in Fig.  6. Notably, the area under the curve \nfor BioLinkBERT-finetuned outperformed others, reaching 0.85, indicating the best \nperformance.\nAmong the generative class of models, we found that the fine-tuned GPT-3 model \nyielded the best overall results, as shown in Table  2. However, in terms of precision, \nthe model fine-tuned on BioMedLM performed well as shown in Table  2. However, \nwe noticed a few observations regarding the use of these generative models. Firstly, we \nsometimes noticed variability in the results with each run of the model depending on the \nparameters used. There were also instances where the model produced empty outputs. \nAdditionally, since these models are generative in nature, the outputs and probabilities \ngenerated by the model do not always align with well-defined class labels. This aspect \nfurther hinders our comprehension of how these models operate and raises concerns \nabout the reliability of their outputs. Due to these limitations, we were unable to gener -\nate a precision-recall curve for GPT-3.\nTo gain deeper insights into the performance of the classifier and generative model, \nwe analyzed the per-class performance metrics for both the fine-tuned generative \nmodels (GPT-3) and the discriminative models (BioLinkBERT model). As expected, \nthe metrics for the negative, positive, and relate classes exhibited satisfactory \nresults. However, we observed poor performance in the NA class for both the fine-\ntuned GPT-3 (refer to Table  4) model and the BioLinkBERT model (refer to Table  3). \nThis deficiency in performance also accounts for the lower overall classification \nTable 2 Performance metrics different fine-tuned language models\nBold indicates the performance of the models which gave the best performance\nModel Accuracy F1 score Precision Recall\nBaseline BERE_TL(MDI) NA 0.738 0.736 0.740\nOur models \n(fine-tuned)\nBert-base-uncased 0.733 ± 0.018 0.731 ± 0.015 0.742 ± 0.02 0.733 ± 0.018\nBioMegatron 0.778 ± 0.008 0.769 ± 0.013 0.771 ± 0.013 0.778 ± 0.008\nPubMedBERT 0.782 ± 0.022 0.778 ± 0.019 0.783 ± 0.021 0.782 ± 0.022\nBioClinicalBERT 0.729 ± 0.032 0.724 ± 0.029 0.731 ± 0.032 0.729 ± 0.032\nBioLinkBERT‑base 0.811 ± 0.029 0.804 ± 0.036 0.813 ± 0.034 0.811 ± 0.028\nBioMedLM 0.806 ± 0.028 0.804 ± 0.028 0.822 ± 0.030 0.806 ± 0.028\nBioGPT 0.732 ± 0.017 0.726 ± 0.017 0.732 ± 0.025 0.736 ± 0.016\nGPT‑3 0.814 ± 0.021 0.810 ± 0.025 0.810 ± 0.021 0.814 ± 0.021\nPage 14 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \nperformance. There are two possible reasons for this outcome. Firstly, as previously \ndiscussed, the distribution of data in the dataset is imbalanced, with a smaller number \nof NA samples. Secondly, there may be inherent challenges in defining the classes in \nthe original problem, which could necessitate further investigation and deliberation. \nHowever, exploring these concerns is beyond the scope of this paper.\nFig. 6 Precision recall curve for the various fine-tuned pre-trained models used in discriminative setting\nTable 3 Per class metrics for BioLinkBERT fine-tuned model\nClass Precision Recall F1‑score\nNA 0.556 ± 0.131 0.447 ± 0.186 0.457 ± 0.096\nNegative 0.827 ± 0.022 0.894 ± 0.043 0.859 ± 0.025\nPositive 0.831 ± 0.015 0.854 ± 0.048 0.841 ± 0.027\nRelate 0.829 ± 0.040 0.798 ± 0.053 0.811 ± 0.025\nTable 4 Per class metrics for GPT-3 fine-tuned model\nClass Precision Recall F1‑score\nNA 0.570 ± 0.112 0.366 ± 0.108 0.431 ± 0.089\nNegative 0.847 ± 0.047 0.881 ± 0.042 0.863 ± 0.041\nPositive 0.819 ± 0.045 0.866 ± 0.034 0.841 ± 0.022\nRelate 0.820 ± 0.027 0.815 ± 0.022 0.817 ± 0.015\nPage 15 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nComparison of outputs of our approach using a web‑based solution\nIn the study [24], the authors utilized their best-performing model ( BERETL ) on a large \ncorpus of text to extract disease-microbiome relationships, which they subsequently \nreleased as the MDIDB database.\nWe aimed to compare the outputs generated by our model with those in the MDIDB \ndatabase. To accomplish this, we devised a straightforward graph and visualization strat-\negy, as illustrated in the first panel of Fig.  7. The process involved running both models \non the original set of evidence statements used in MDIDB and comparing the resulting \ngraphs. In our graph representation, nodes correspond to diseases or microbes, while \nedges represent established relationships between them. Nodes are colored green when \nboth algorithms agree on the nature of the relationship, and red when they disagree. We \nalso developed a web application for this which is accessible on our supplementary web -\nsite [see Additional file 1]. The user interface of the tool allows users to select the num -\nber of edges they wish to visualize from the larger graph. After specifying, for example, \n50 edges in the provided text box, users can click the “generate knowledge graph” button \nto display the corresponding knowledge graph. Zooming and hovering over the edges of \nthe graph provide information on the differences in predictions between the two mod -\nels, including evidence text, for both red and green nodes (as depicted in Fig.  7). This \napproach aims to provide expert researchers with a more comprehensive understanding \nof the performance of the different models.\nDiscussions\nIn this paper, we address several crucial aspects concerning the utility of pre-trained lan-\nguage models and their applicability to relevant challenges in the biomedical domain. \nSpecifically, we focus on the task of extracting disease-microbe relationships from \nscientific publications. To approach this problem, we frame it as a relation extraction \ntask, enabling us to explore the potential of various language models in generative and \ndiscriminative paradigms. Our initial investigation involves assessing the capability of \nFig. 7 Comparing the MDIDB knowledge base generated using BERE (TL) model versus our prediction \nmodel\nPage 16 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \ngenerative models, namely GPT-3, BioGPT, and BioMedLM, in a zero-shot or few-shot \nsetting. We sought to determine if these models can perform well on the task with mini -\nmal fine-tuning or data preparation. However, we find that their results are of poor qual-\nity, highlighting the need for domain-specific adaptations to enhance their usefulness.\nInterestingly, we discover that GPT-3 performs the best when fine-tuned. Sub -\nsequently, we explore the performance of discriminative models, specifically the \nBERT-based models and their domain-specific adaptations such as BioMegatron, Pub -\nMedBERT, BioClinicalBERT, and BioLinkBERT. As expected, fine-tuning these models \nyields state-of-the-art results for the task. We also observe that the quality of the training \ndata significantly influences the accuracy improvements achieved. Our work serves as a \nfoundation for further research on adapting and leveraging language models in the field \nof biomedicine. In conclusion, we have demonstrated that language models in both gen -\nerative and discriminative settings are viable candidates for fine-tuning and constructing \nmodels that yield SOTA results for the microbiome-disease relationship extraction task.\nThere are several avenues for future exploration. For instance, investigating a broader \nrange of models, including Galactica [40], LLaMA [41], GPT-4 [42], etc., could provide \nvaluable insights for these tasks. Additionally, as we are in the era of models like Chat -\nGPT [43], it would be interesting to explore the possibility of fine-tuning similar conver-\nsational models. As a preliminary experiment, we briefly examined ChatGPT using their \npublicly available service, and the results can be found in the supplementary website [see \nAdditional file 1] of this paper. While the initial findings appear promising, we observed \nthat the model produced different outputs for the same prompt, raising concerns about \nthe reliability of the generated responses. These observations align with our experiences \nduring the fine-tuning of GPT-3, indicating the need for further refinement in this area. \nSuch investigations could lead to exciting new research directions.\nFurthermore, we identify limitations in entity recognition and normalization within \nthe GSC dataset. Addressing these issues requires additional work to refine the end-to-\nend pipeline and build accurate and trustworthy knowledge bases. Another important \naspect of this research is that once reliable knowledge bases are established, they can \nserve as a foundation for formulating hypotheses regarding potentially new disease-\nmicrobe associations, thus fostering new knowledge and discoveries [44, 45]. Previous \nstudies, such as those conducted by [46, 47], have explored similar approaches. In addi -\ntion, while we considered the current work purely as a NLP task, augmenting with other \nheterogeneous knowledge networks (such as in [48, 49]) can further improve the predic-\ntion ability of the models.\nAbbreviations\nGPT  Generative Pre-trained Transformer\nBERT  Bidirectional Encoder Representations from Transformers\nMADET  Microbiomics of Anticancer Drug Efficacy and Toxicity\nHMDAD  Human Microbe-Disease Association Database\nNLP  Natural Language Processing\nNER  Named Entity Recognizer\nLLMs  Large Language Models\nLSTM  Long Short-Term Memory\nGSC  Gold Standard Corpus\nSSC  Silver Standard Corpus\nCC  Commercial-Collection\nGPU  Graphics Processing Unit\nDDI  Drug-Drug Interaction\nPage 17 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \nMDIDB  Microbe-Disease Interactions Database\nTL  Transfer Learning\nLLaMA  Large Language Model Meta AI\nSOTA  State-Of-The-Art\nSupplementary Information\nThe online version contains supplementary material available at (https:// doi. org/ 10. 1186/ s12859- 023- 05411-z).\nAdditional file 1: Supplementary material.\nAcknowledgements\nThe authors thank Prof. Hiroaki Kitano from The Systems Biology Institute for his constant support and the ONRG Grant \nfor the Nobel Turing challenge to The Systems Biology Institute. The authors also thank Dr Nicolas Shinada from The Sys-\ntems Biology Institute for his support in re-annotating the GSC dataset. Sathwik Acharya was a undergraduate student \nintern from PES University India for 6 months at the Systems Biology Institute during the period of this research.\nAuthor contributions\nSKP designed and led the study. SA, NK and SKP developed the methods. SA and NK implemented the algorithms, per-\nformed experiments and analysed of the data. NK, SKP and SA wrote the manuscript. All authors reviewed and approved \nthe final manuscript.\nFunding\nThe study is partly funded by ONR Global Grant—N62909-21-1-2032.\nAvailability of data and materials\nAll data, model and code used in this project is available at the supplementary companion website https:// bit. ly/ micro \nbiome LLM\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 22 May 2023   Accepted: 13 July 2023\nReferences\n 1. Sommer F, Bäckhed F. The gut microbiota-masters of host development and physiology. Nat Rev Microbiol. \n2013;11(4):227–38.\n 2. Lozupone CA, Stombaugh JI, Gordon JI, Jansson JK, Knight R. Diversity, stability and resilience of the human gut \nmicrobiota. Nature. 2012;489(7415):220–30.\n 3. Li L, Jing Q, Yan S, Liu X, Sun Y, Zhu D, Wang D, Hao C, Xue D. Amadis: a comprehensive database for association \nbetween microbiota and disease. Front Physiol. 2021;12: 697059.\n 4. Janssens Y, Nielandt J, Bronselaer A, Debunne N, Verbeke F, Wynendaele E, Van Immerseel F, Vandewynckel Y-P , \nDe Tré G, De Spiegeleer B. Disbiome database: linking the microbiome to disease. BMC Microbiol. 2018;18(1):50. \nhttps:// doi. org/ 10. 1186/ s12866- 018- 1197-5\n 5. Yao G, Zhang W, Yang M, Yang H, Wang J, Zhang H, Wei L, Xie Z, Li W. MicroPhenoDB associates metagenomic \ndata with pathogenic microbes, microbial core genes, and human disease phenotypes. Genom Proteom Bioin-\nform. 2020;18(6):760–72. https:// doi. org/ 10. 1016/j. gpb. 2020. 11. 001.\n 6. Noronha A. The virtual metabolic human database: integrating human and gut microbiome metabolism with \nnutrition and disease. Nucleic Acids Res. 2019;47(D1):614–24. https:// doi. org/ 10. 1093/ nar/ gky992.\n 7. Zhang J, Chen X, Zou J, Li C, Kang W, Guo Y, Liu S, Zhao W, Mou X, Huang J, Ke J. MADET: a manually curated \nknowledge base for microbiomic effects on efficacy and toxicity of anticancer treatments. microbiology spec-\ntrum. 2022;10(6):02116–22. https:// doi. org/ 10. 1128/ spect rum. 02116- 22\n 8. Qi C, Cai Y, Qian K, Li X, Ren J, Wang P , Fu T, Zhao T, Cheng L, Shi L, Zhang X. gutMDisorder v2.0: a comprehensive \ndatabase for dysbiosis of gut microbiota in phenotypes and interventions. Nucleic Acids Res. 2022. https:// doi.  \norg/ 10. 1093/ nar/ gkac8 71\n 9. Cheng L, Qi C, Zhuang H, Fu T, Zhang X. gutMDisorder: a comprehensive database for dysbiosis of the gut \nmicrobiota in disorders and interventions. Nucleic Acids Res. 2020;48(D1):554–60. https:// doi. org/ 10. 1093/ nar/  \ngkz843.\nPage 18 of 19Karkera et al. BMC Bioinformatics          (2023) 24:290 \n 10. Ma W, Zhang L, Zeng P , Huang C, Li J, Geng B, Yang J, Kong W, Zhou X, Cui Q. An analysis of human microbe-\ndisease associations. Brief Bioinform. 2017;18(1):85–97.\n 11. Jin H, Hu G, Sun C, Duan Y, Zhang Z, Liu Z, Zhao X-M, Chen W-H. mbodymap: a curated database for microbes \nacross human body and their associations with health and diseases. Nucleic Acids Res. 2022;50(D1):808–16.\n 12. Jo A. The promise and peril of generative AI. Nature. 2023;614(1):214–6.\n 13. Badal VD, Wright D, Katsis Y, Kim H-C, Swafford AD, Knight R, Hsu C-N. Challenges in the construction of knowl-\nedge bases for human microbiome-disease associations. Microbiome 2019;7(1):1–15. Publisher: BioMed Central.\n 14. Wang Q, Xu R. Automatic extraction, prioritization and analysis of gut microbial metabolites from biomedical \nliterature. Sci Rep. 2020;10(1):1–10.\n 15. Lim KMK, Li C, Chng KR, Nagarajan N. MInter: automated text-mining of microbial interactions. Bioinformatics. \n2016;32(19):2981–7.\n 16. Ahmed SAJA, Bapatdhar N, Kumar BP , Ghosh S, Yachie A, Palaniappan SK. Large scale text mining for deriving \nuseful insights: a case study focused on microbiome. Front Physiol. 2022;13\n 17. Xu H, Li X, Zheng C, Liu K, Liu S, Zeng Y, Song Z, Cui S, Xu Y. Gdrebase: a comprehensive, indexed and updated \nknowledge base for relations between human gut microbes and diseases. 2022.\n 18. Qu J, Zhao Y, Yin J. Identification and analysis of human microbe-disease associations by matrix decomposition \nand label propagation. Front Microbiol. 2019;10\n 19. Peng L, Shen L, Liao L, Liu G, Zhou L. RNMFMDA: a microbe-disease association identification method based \non reliable negative sample selection and logistic matrix factorization with neighborhood regularization. Front \nMicrobiol. 2020;11.\n 20. Konstantinova N. Review of relation extraction methods: What is new out there? In: Analysis of Images, Social \nNetworks and Texts: Third International Conference, AIST 2014, Yekaterinburg, Russia, April 10-12, 2014, Revised \nSelected Papers 2014;3:15–28\n 21. Nédellec C, Bossy R, Kim J-D, Kim J-J, Ohta T, Pyysalo S, Zweigenbaum P . Overview of bionlp shared task 2013. In: \nProceedings of the BioNLP Shared Task 2013 Workshop. 2013:1–7.\n 22. Wang H, Qin K, Zakari RY, Lu G, Yin J. Deep neural network-based relation extraction: an overview. Neural Com-\nput Appl. 2022;1–21.\n 23. Park Y, Lee J, Moon H, Choi YS, Rho M. Discovering microbe-disease associations from the literature using a \nhierarchical long short-term memory network and an ensemble parser model. Sci Rep. 2021;11(1):1–12\n 24. Wu C, Xiao X, Yang C, Chen J, Yi J, Qiu Y. Mining microbe-disease interactions from literature via a transfer learn-\ning model. BMC Bioinform. 2021;22(1):1–15.\n 25. Hong L, Lin J, Li S, Wan F, Yang H, Jiang T, Zhao D, Zeng J. A novel machine learning framework for automated \nbiomedical relation extraction from large-scale literature repositories. Nat Mach Intell. 2020;2(6):347–55.\n 26. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell A, et al. \nLanguage models are few-shot learners. Adv Neural Inf Process Syst. 2020;33:1877–901.\n 27. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. \nIn Advances in neural information processing systems 2017:30.\n 28. Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, Liu T-Y. Biogpt: generative pre-trained transformer for biomedical text \ngeneration and mining. Brief Bioinform. 2022;23(6)\n 29. Venigalla A, Frankle J, Carbin M. Biomedlm: a domain-specific large language model for biomedical text. Mosai-\ncML. Accessed: Dec 2022;23\n 30. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: pre-training of deep bidirectional transformers for language \nunderstanding. arXiv preprint arXiv: 1810. 04805 2018.\n 31. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. Biobert: a pre-trained biomedical language representation \nmodel for biomedical text mining. Bioinformatics. 2020;36(4):1234–40.\n 32. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, Naumann T, Gao J, Poon H. Domain-specific language model \npretraining for biomedical natural language processing. 2020. arXiv: 2007. 15779\n 33. Shin H-C, Zhang Y, Bakhturina E, Puri R, Patwary M, Shoeybi M, Mani R. Brigadoon: Larger biomedical domain \nlanguage model. arXiv preprint arXiv: 2010. 06060. 2020.\n 34. Yasunaga M, Leskovec J, Liang P . LinkBERT: Pretraining Language Models with Document Links. 2022. arXiv \npreprint arXiv: 2203. 15827\n 35. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P , Rault T, Louf R, Funtowicz M, et al. Hugging-\nface’s transformers: state-of-the-art natural language processing. arXiv preprint arXiv: 1910. 03771. 2019.\n 36. Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Naumann T, McDermott M. Publicly available clinical bert \nembeddings. arXiv preprint arXiv: 1904. 03323 2019.\n 37. Reynolds L, McDonell K. Prompt programming for large language models: beyond the few-shot paradigm. \n2021:1–7\n 38. Moradi M, Blagec K, Haberl F, Samwald M. Gpt-3 models are poor few-shot learners in the biomedical domain. \narXiv preprint arXiv: 2109. 02555 2021.\n 39. Nakayama H, Kubo T, Kamura J, Taniguchi Y, Liang X. doccano: text annotation tool for human. Software avail-\nable from https:// github. com/ docca no/ docca no 2018.\n 40. Taylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V, Stojnic R. Galactica: a large \nlanguage model for science. arXiv preprint arXiv: 2211. 09085 2022.\n 41. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar F, et al. \nLlama: Open and efficient foundation language models. arXiv preprint arXiv: 2302. 13971. 2023.\n 42. OpenAI: GPT-4 Technical Report 2023. arXiv: 2303. 08774\n 43. OpenAI: ChatGPT: Optimizing Language Models for Dialogue. https:// openai. com/ blog/ chatg  pt/. 2022.\n 44. Kitano H. Nobel turing challenge: creating the engine for scientific discovery. NPJ Syst Biol Appl. 2021;7(1):1–12.\n 45. Kitano H. Artificial intelligence to win the nobel prize and beyond: creating the engine for scientific discovery. AI \nMag. 2016;37(1):39–49.\nPage 19 of 19\nKarkera et al. BMC Bioinformatics          (2023) 24:290 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 46. Bao W, Jiang Z, Huang D-S. Novel human microbe-disease association prediction using network consistency \nprojection. BMC Bioinform. 2017;18(16):173–81.\n 47. Huang Y-A, You Z-H, Chen X, Huang Z-A, Zhang S, Yan G-Y. Prediction of microbe-disease association from the \nintegration of neighbor and graph with collaborative recommendation model. J Transl Med. 2017;15(1):1–11.\n 48. Zhao B-W, Wang L, Hu P-W, Wong L, Su X-R, Wang B-Q, You Z-H, Hu L. Fusing higher and lower-order biological \ninformation for drug repositioning via graph representation learning. IEEE Trans Emerg Comput. 2023;\n 49. Zhao B-W, You Z-H, Hu L, Guo Z-H, Wang L, Chen Z-H, Wong L. A novel method to predict drug-target interactions \nbased on large-scale graph representation learning. Cancers. 2021;13(9):2111.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7604420781135559
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6450428366661072
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6410491466522217
    },
    {
      "name": "Biomedical text mining",
      "score": 0.5804967284202576
    },
    {
      "name": "Machine learning",
      "score": 0.5567092895507812
    },
    {
      "name": "Microbiome",
      "score": 0.5535460710525513
    },
    {
      "name": "Language model",
      "score": 0.5364388227462769
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5322574973106384
    },
    {
      "name": "Natural language processing",
      "score": 0.5192039608955383
    },
    {
      "name": "Relationship extraction",
      "score": 0.5056092739105225
    },
    {
      "name": "Human Microbiome Project",
      "score": 0.4940531551837921
    },
    {
      "name": "Data science",
      "score": 0.45937642455101013
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.43743693828582764
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4259836673736572
    },
    {
      "name": "Information extraction",
      "score": 0.4115406274795532
    },
    {
      "name": "Task (project management)",
      "score": 0.2827598452568054
    },
    {
      "name": "Human microbiome",
      "score": 0.2611921429634094
    },
    {
      "name": "Bioinformatics",
      "score": 0.24447080492973328
    },
    {
      "name": "Text mining",
      "score": 0.165569007396698
    },
    {
      "name": "Biology",
      "score": 0.09225919842720032
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}