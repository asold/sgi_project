{
  "title": "Contextual Embeddings for Ukrainian: A Large Language Model Approach to Word Sense Disambiguation",
  "url": "https://openalex.org/W4386566392",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092831396",
      "name": "Yurii Laba",
      "affiliations": [
        "Ukrainian Catholic University"
      ]
    },
    {
      "id": "https://openalex.org/A5092831397",
      "name": "Volodymyr Mudryi",
      "affiliations": [
        "Lviv University"
      ]
    },
    {
      "id": "https://openalex.org/A5092831385",
      "name": "Dmytro Chaplynskyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226441884",
      "name": "Mariana Romanyshyn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001793121",
      "name": "Oles Dobosevych",
      "affiliations": [
        "Ukrainian Catholic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2573981993",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2922497999",
    "https://openalex.org/W3213432108",
    "https://openalex.org/W2219047839",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1503259811",
    "https://openalex.org/W4386566337",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2127289991",
    "https://openalex.org/W2970773744",
    "https://openalex.org/W2612112834",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "This research proposes a novel approach to the Word Sense Disambiguation (WSD) task in the Ukrainian language based on supervised fine-tuning of a pre-trained Large Language Model (LLM) on the dataset generated in an unsupervised way to obtain better contextual embeddings for words with multiple senses. The paper presents a method for generating a new dataset for WSD evaluation in the Ukrainian language based on the SUM dictionary. We developed a comprehensive framework that facilitates the generation of WSD evaluation datasets, enables the use of different prediction strategies, LLMs, and pooling strategies, and generates multiple performance reports. Our approach shows 77,9% accuracy for lexical meaning prediction for homonyms.",
  "full_text": "Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP), pages 11–19\nMay 5, 2023 ©2023 Association for Computational Linguistics\nContextual Embeddings for Ukrainian: A Large Language Model\nApproach to Word Sense Disambiguation\nYurii Laba\nThe Machine Learning Lab at\nUkrainian Catholic University,\nLviv, Ukraine\nlaba@ucu.edu.ua\nVolodymyr Mudryi\nIndependent Researcher,\nLviv, Ukraine\nvova.mudruy@gmail.com\nDmytro Chaplynskyi\nLang-uk\nKyiv, Ukraine\nchaplinsky.dmitry@gmail.com\nMariana Romanyshyn\nGrammarly\nKyiv, Ukraine\nmariana.scorp@gmail.com\nOles Dobosevych\nThe Machine Learning Lab at\nUkrainian Catholic University,\nLviv, Ukraine\ndobosevych@ucu.edu.ua\nAbstract\nThis research proposes a novel approach to the\nWord Sense Disambiguation (WSD) task in the\nUkrainian language based on supervised fine-\ntuning of a pre-trained Large Language Model\n(LLM) on the dataset generated in an unsu-\npervised way to obtain better contextual em-\nbeddings for words with multiple senses. The\npaper presents a method for generating a new\ndataset for WSD evaluation in the Ukrainian\nlanguage based on the SUM dictionary. We\ndeveloped a comprehensive framework that\nfacilitates the generation of WSD evaluation\ndatasets, enables the use of different prediction\nstrategies, LLMs, and pooling strategies, and\ngenerates multiple performance reports. Our\napproach shows 77,9% accuracy for lexical\nmeaning prediction for homonyms.\n1 Introduction\nWord Sense Disambiguation (WSD) task involves\nidentifying a polysemic word’s correct meaning in\na given context. A task of WSD is applicable in\nvarious NLP fields Sharma and Niranjan (2015),\nsuch as information retrieval, machine translation\nNeale et al. (2016), and question answering. For\nwell-resourced languages, this problem has many\ndifferent approaches for solving that demonstrate\ncompetitive results Navigli (2009).\nHowever, this task has received relatively little\nattention in the Ukrainian language due to the ab-\nsence of sense-annotated datasets. To address this\nissue, we propose a novel approach to the WSD\ntask based on fine-tuning a pre-trained Large Lan-\nguage Model (LLM) to obtain better contextual\nembeddings for words with multiple senses.\nIn this research, we present a method for gen-\nerating a new dataset for WSD evaluation in the\nUkrainian language, which includes lemmas, ex-\nample sentences, and lexical meanings based on\nthe SUM dictionary, of NAS of Ukraine (ULIF-\nNASU). This dataset is used to evaluate the effec-\ntiveness of our proposed method. For supervised\nLLM fine-tuning, we generate the dataset in an un-\nsupervised way based on UberText Chaplynskyi\n(2023).\nAdditionally, we have developed a comprehen-\nsive framework 1 that facilitates the generation of\nWSD evaluation datasets, enables the use of differ-\nent prediction strategies, LLMs, or pooling strate-\ngies, and generates multiple performance reports.\n2 Related works\nEarly approaches in WSD utilized the concept of\nword embeddings, which were generated using pre-\ntrained algorithms such as Word2Vec Mikolov et al.\n(2013) or Glove Pennington et al. (2014). However,\nthese static word embeddings have a notable prob-\nlem that all senses of a homonym word must share\na single vector. To address this issue, several re-\nsearchers have proposed techniques for capturing\npolysemy and generating more informative embed-\ndings Faruqui et al. (2014) or Speer et al. (2017).\nRecently, there has been a trend toward utilizing\ncontextual embeddings generated by LLMs instead\nof pre-trained word embeddings. These contextual\nembeddings provide a more nuanced representation\nof words, capturing context-specific information.\nAs a result, a simple approach such as kNN can\nbe used in combination with these embeddings to\npredict word senses in Word Sense Disambiguation\ntasks accurately Wiedemann et al. (2019).\nWSD can be approached as a binary classifica-\n1More details in Appendix A\n11\ntion problem. One such approach was proposed\nby Huang et al. (2019), which involved adding a\nclassification head to the BERT model Devlin et al.\n(2018). The model takes a pair of sentences as in-\nput, with one sentence containing the target word\nand the other providing one of the possible defini-\ntions of the target word. The model then predicts\nwhether the target word in the sentence has the\nsame meaning as the definition.\nAnother noteworthy approach to Word Sense\nDisambiguation is the one presented by Barba et al.\n(2021), where the model not only takes into account\nthe contextual information of the target word, but\nalso the explicit senses assigned to neighboring\nwords.\nDespite the high performance of the previously\nmentioned supervised approaches for Word Sense\nDisambiguation, their reliance on a large amount\nof annotated sense data can pose a challenge for\ntheir application to under-resourced languages. In\ncontrast, unsupervised methods can also be applied\nto WSD tasks. One of the earliest and most well-\nknown solutions is using sense definitions and se-\nmantic relations from lexical graph databases such\nas Babelfy Moro et al. (2014). However, recent\nworks such as Huang et al. (2019) have shown that\nLLM-based solutions outperform those methods.\nGiven the limitations of prior research, partic-\nularly the shortage of annotated corpora in the\nUkrainian language, we present our proposed so-\nlution of supervised fine-tuning of an LLM on a\ndataset generated in an unsupervised way. Ad-\nditionally, we have prepared a validation dataset\nfor the Ukrainian WSD task, derived from the\nSUM (Dictionary of Ukrainian Language) dictio-\nnary of NAS of Ukraine (ULIF-NASU).\nOur approach will enhance the model’s under-\nstanding of semantic word meaning and improve\nthe performance of the Word Sense Disambigua-\ntion task in the Ukrainian language.\n3 Evaluation Dataset\nTo assess the efficacy of our methodology for ad-\ndressing the Ukrainian WSD task, we have estab-\nlished a validation dataset based on the SUM dic-\ntionary. The SUM dictionary is an appropriate\nresource as it employs componential analysis, a\nlinguistic methodology used to differentiate com-\nmon language phenomena such as polysemy and\nhomonymy, by evaluating the presence or absence\nof shared semantic features among compared units.\nTherefore, the dataset derived from the SUM dictio-\nnary is well-suited for evaluating the performance\nof our approach. According to Ukrainian Lingua-\nInformation Fund (2022), the examples in the SUM\ndictionary were taken from a broad selection of re-\nsources, including fiction (from the end of the 18th\ncentury to the present day), Ukrainian translations\nof the Bible, folklore, publicistic, scientific, and\npopular scientific works, the language of the mass\nmedia, the language of the Internet, etc. Unfor-\ntunately, at the moment of publication, there is\nonly part of the dictionary available (until word\nПIДКУРЮВАЧ (en: lighter, translit: pidkuryu-\nvach)).\nThe dataset was constructed by extracting each\nlemma, its lexical meaning, and examples of us-\nage related to that meaning. While building the\nevaluation dataset, the lemmas with single possible\nlexical senses were filtered out, and the resulting\ndataset consisted of 78,000 samples. Further data\ncleaning was performed to remove lemmas with a\nlength of fewer than three characters, lemmas with\nmissing senses or examples, lemmas that belong\nto functional parts of speech, and lemmas which\nlexical meaning reference for another lemma. After\ncleaning, the dataset consisted of 42,000 samples,\nwith each sample consisting of a lemma, one of\nthe possible lexical meanings of the lemma, and\nexamples of this meaning. Assembling the dataset\ninvolved part-of-speech (POS) detection for each\nlemma using the Stanza library Qi et al. (2020),\nand this information was utilized in the subsequent\nevaluation table.\nDuring our experiments, we observed that many\nlemmas in the Ukrainian language have multiple\nsimilar lexical meanings, which significantly com-\nplicates the task, the examples presented in Table\n1. To address this issue, we built a dataset focusing\non homonymy rather than polysemy.\nHomonyms are unrelated words with the same\nwritten and spelling form but different lexical mean-\nings. To construct a dataset of homonyms, we first\nfiltered out lemmas with fewer than two entries\nin the SUM dictionary. Then, for each remaining\nlemma, we concatenated all the lexical meanings\nand examples of usage of each separate homonym.\nThe resulting dataset consisted of 2,882 homonym\nsamples, each sample including the lemma, its pos-\nsible meanings, and examples for each meaning\n(see Table 2). We used this dataset for further\nmodel evaluation.\n12\nLemma Meaning Example\nКОСА\n(en: braid, transl: kosa)\nЗаплетене волосся\n(en: Braided hair)\nОчi в неї були великi, двi чорнi коси,\nперекинутi наперед, обрамляли лице.\n(en: Her eyes were large, two black braids,\nthrown forward, framed her face.)\nКОСА\n(en: braid, transl: kosa)\nДовге волосся\n(en: Long hair)\nГустi, золото-жовтi коси буйними\nхвилями спадали на її груди i плечi.\n(en: Thick, golden-yellow braids fell\nin wild waves on her chest and shoulders.)\nTable 1: Examples from polysemy dataset (similar lexical meanings)\n4 Approach\n4.1 Task Definition\nIn our approach to Word Sense Disambiguation, for\neach homonym l (target word), we have identified\na set of possible lexical meaning groups, denoted\nas\nGl = {gl1 , ..., gln }\nEach lexical meaning group gli , comprises all the\npossible lexical meanings of a particular lemma\ncorresponding to the homonym. Our objective is\nto predict the correct lexical meaning group gli ,\nfrom all the possible lexical meaning groups of\nthe lemma Gl, based on a list of examples of the\nlemma’s usage.\nTo accomplish this, we first calculate embed-\ndings for the sentence example and obtain the tar-\nget word embedding from it using various pooling\nstrategies, which will be described later. Subse-\nquently, we measure the cosine similarity between\nthe obtained embedding of the target word and the\nembeddings of each lexical meaning group. The\nlexical meaning group with the highest cosine sim-\nilarity is considered to be the predicted context.\nFigure 1 demonstrates an example of the single\nlemma prediction process utilizing our approach.\n4.2 Evaluation\nIn order to evaluate the performance of our WSD\napproach, we have chosen to utilize the accuracy\nmetric. Specifically, for each sample in the dataset,\nwe compare the predicted context of the lemma\n(see Figure 1) with the ground truth context derived\nfrom the corresponding example. Any instances\nwhere the predicted context matches the ground\ntruth context are considered correct predictions,\nand the overall accuracy is calculated based on the\ntotal number of correct predictions.\n4.3 Embedding calculation\nIn the context of natural language processing\n(NLP), word embeddings have emerged as a pow-\nerful technique to represent words in a numerical\nform, which can then be leveraged to perform vari-\nous NLP tasks, including Word Sense Disambigua-\ntion. Each word is mapped to a high-dimensional\nvector of real numbers in word embeddings, which\nencodes its semantic and syntactic information\nbased on its context in a given corpus. By captur-\ning words’ intrinsic meaning and contextual usage,\nword embeddings have demonstrated their effec-\ntiveness in various NLP applications, including\nWSD Huang et al. (2019).\nIn NLP, one of the most effective approaches for\ngenerating high-quality contextualized word em-\nbeddings is leveraging pre-trained LLMs such as\nRoBERTa Liu et al. (2019) or GPT-2 Radford et al.\n(2019). LLMs allow the calculation of word embed-\ndings for individual words or entire sentences. For\ninstance, the BERT (Bidirectional Encoder Repre-\nsentations from Transformers) base model Devlin\net al. (2018) employs 12 layers of transformer en-\ncoders, which utilize a multi-head attention mecha-\nnism to learn context-dependent representations of\ninput tokens. The resultant output vector of each\ntoken from each layer of the BERT model can be\nused as a word embedding.\nVarious pooling strategies can be applied to gen-\nerate embeddings for individual words or entire sen-\ntences, but determining the most effective strategy\nfor a particular task requires experimental investi-\ngation. In this study, we conducted experiments\nto compare the performance of different pooling\nmethods, including:\n1. Mean pooling - computes the average of the\nembeddings for each token from the last hid-\nden state of the model. The last hidden state\n13\nLemma Meaning Example\nКОСА\n(en: braid,\ntransl: kosa)\n[Заплетене волосся\n(en: Braided hair),\nДовге волосся\n(en: Long hair)]\n[Очi в неї були великi, двi чорнi коси,\nперекинутi наперед, обрамляли лице.\n(en: Her eyes were large, two black braids,\nthrown forward, framed her face.);\nГустi, золото-жовтi коси буйними\nхвилями спадали на її груди i плечi.\n(en: Thick, golden-yellow braids fell\nin wild waves on her chest and shoulders.)]\nКОСА\n(en: scythe,\ntransl: kosa)\n[Сiльськогосподарське\nзнаряддя для\nкосiння трави, збiжжя\nтощо, що має вигляд\nвузького зiгнутого леза,\nприкрiпленого до держака.\n(en: An agricultural tool\nfor mowing grass,\ngrain, etc., having the\nform of a narrow\nbent blade attached\nto a handle.)]\n[Внук косу несе в росу.(en: A grandson\ncarries a scythe into the dew.)]\nTable 2: Examples from the homonym dataset\ncorresponds to the sequence of hidden states\nat the output of the model’s final layer.\n2. Max pooling - extracts the maximum value of\nthe embeddings for each token from the last\nhidden state of the model.\n3. Mean Max pooling - calculates the average\nand maximum values of the embeddings for\neach token from the last hidden state of the\nmodel and concatenates the resulting vector.\n4. Concatenate pooling - concatenates the em-\nbeddings from the last four hidden states.\n5. Last four or two pooling - sums the embed-\ndings from the last four or two hidden states.\nBased on our experiments, we concluded that the\nmean pooling shows the best results in the WSD\ntask for the Ukrainian language (see Table 3).\nOur research aimed to determine the most effec-\ntive LLM for generating contextual embeddings.\nTo achieve this, we conducted experiments using\na range of multilingual LLMs and evaluated their\nperformance without fine-tuning. Our results in Ta-\nble 3 demonstrates that one of the SBERT models\nReimers and Gurevych (2019), namely paraphrase-\nmultilingual-mpnet-base-v2 (PMMBv2), produced\nthe highest quality contextual embeddings for our\nWSD task on a homonym dataset. Interestingly, our\nfindings suggest that the SBERT model, initially\ndesigned to improve the semantic representation of\nentire sentences, can also significantly enhance the\nsemantic representation of individual words.\n5 Embeddings improvement\n5.1 Dataset for fine-tuning\nIn order to enhance the quality of embeddings and\nto achieve superior performance on words with\nmultiple lexical senses, we opted to fine-tune our\nbest model, PMMBv2, as a means to improve its\nefficiency. Typically, researchers rely on super-\nvised datasets such as Semcor Miller et al. (1993)\nor SemEval-2007 Pradhan et al. (2007) to enhance\nWSD task performance, consisting of pairs of sen-\ntences and a sense for a particular lemma, along\nwith binary labels indicating the usage of a lemma\nin that particular context. Unfortunately, no such\ndataset is available for the Ukrainian language,\nleading us to pursue fine-tuning our model using a\ndataset generated using our proposed unsupervised\nmethod.\nOur dataset samples consist of an anchor, a posi-\ntive, and a negative example. To define positive and\n14\nFigure 1: Prediction Logic for Lemma \"Замок\" (translit: zamok). In Ukrainian, the lemma \"zamok\" has two\npossible meanings. The first one means a castle, and the second is a lock. In this figure, we depicted prediction\nlogic given an example, lemma of interest, and possible senses.\nnegative examples relative to the anchor sentence\nfor the WSD task, we determined that the positive\nsample should be a sentence with a lemma used\nin the same context as in the anchor sentence. In\ncontrast, the negative sample should be a lemma\nused in a different context.\nIn order to acquire a suitable dataset, an entirely\nunsupervised methodology was employed. The\nDevelopers’ preview of UberText 2.0 Chaplynskyi\n(2023), which comprises of texts from Ukrainian\nperiodicals, was utilized to gather a vast number of\nUkrainian language sentences. Subsequently, we\nfiltered out sentences that did not contain any lem-\nmas from our homonym evaluation dataset (Evalu-\nation Dataset). We removed outliers based on crite-\nria such as length and the presence of punctuation\nsymbols or digits. We also employed langdetect\nShuyo (2010) to remove non-Ukrainian language\nsamples.\nEach dataset sample was then represented as an\nembedding using ukr-roberta Radchenko (2020).\nWe calculated the cosine distance between the an-\nchor embedding and all other sentences in the\ndataset containing the required lemma. We then\nassumed that the sample with the highest cosine\nsimilarity would be the positive sample - contain-\ning a lemma used in the same context as in the\nanchor sentence and that the sample with the low-\nest cosine similarity would be the negative sample,\ncontaining a lemma used in a different context.\nThis dataset is available in two sizes, consisting\nof ~190,000 and ~1,200,000 triplet pairs obtained\nfrom UberText 2.0.\nWe assessed the suitability of our dataset for\nfine-tuning by selecting a subset of examples to\ndetermine if target lemmas in positive and negative\ninstances have distinct lexical meanings. After sam-\npling approximately 100 examples, we found that\n13.1% of the samples constituted relevant triplets.\nIn the Conclusion section of this paper, we will\nprovide future works for enhancing the dataset’s\nquality.\n5.2 Loss\nGiven that we had access to a suitable dataset, we\nopted to employ the TripletMarginLoss Balntas\net al. (2016) for fine-tuning our neural network.\nThe Triplet Margin Loss function is used to opti-\nmize a neural network by minimizing the distance\nbetween the embedding of an anchor sentence and\nthat of a positive example while maximizing the\ndistance between the anchor and a negative exam-\nple. The loss function is defined as follows:\nmax(||a −p||−||a −n||+ M, 0)\n15\nModel Mean Max Mean Max Concatenat Last four Last two\nbert-base-\nmultilingual-cased\n0.602 0.601 0.622 0.576 0.579 0.590\nxlm-roberta-base 0.529 0.492 0.501 0.534 0.531 0.533\nxlm-roberta-large 0.547 0.495 0.502 0.576 0.581 0.576\nxlm-roberta-base-\nuk\n0.528 0.491 0.501 0.535 0.533 0.535\nukr-roberta 0.580 0.559 0.570 0.572 0.570 0.582\nparaphrase-\nmultilingual-\nmpnet-base-v2\n0.735 0.718 0.716 0.644 0.636 0.656\nTable 3: Word Sense Disambiguation (WSD) Accuracy for Ukrainian language with Different Pooling Strategies\nand Pretrained Models without fine-tuning.\nwhere a, p, and n are the embeddings of the anchor,\npositive, and negative sentences, respectively, and\nM is a margin hyperparameter that ensures that the\npositive example is at least closer to the anchor than\nthe negative example. We used Euclidean distance\nas the distance metric in our experiments and set\nM = 1.\n5.3 Training process\nDuring the model’s training, we monitored the per-\nformance of Word Sense Disambiguation accuracy\non 20% of the SUM evaluation dataset to assess if it\nwas being improved with the training process. We\nused 1% of a fine-tuning dataset to calculate train-\ning metrics and the rest 99% for training. We em-\nployed an early stopping mechanism based on the\nWSD accuracy on SUM based evaluation dataset.\nA batch size of 32 and the Adam optimizer with\na learning rate of 2e-6 were used for the model\noptimization. Furthermore, we applied linear learn-\ning rate warm-up over the first 10% of the training\ndata.\n6 Results\nThe Table 4 presents the performance evaluation\nof our proposed method on the SUM evaluation\ndataset for homonyms.\nWe started with the Babelfy as a baseline, which\nwas manually validated on 10% of the randomly\nsampled portion from the WSD evaluation dataset.\nNext, we tested a vanilla PMMBv2 model without\nfine-tuning, followed by a fine-tuned version of\nthe PMMBv2 model using the proposed approach.\nThe models fine-tuned by our approach outperform\nboth Babelfy and vanilla PMMBv2 models. We\nobserved that a larger dataset for fine-tuning led to\nbetter accuracy.\nWe assume that a model trained on a larger\ndataset, which also has a larger average distance\nbetween positive and negative examples, generates\nbetter homonym-specific embeddings. We also ob-\nserved that the model PMMBv2 tuned on 1,2M\ntriplets with filtering out pairs with a small differ-\nence (less than 0.3) between the cosine similarity\nof the anchor and positive examples and that of the\nanchor and negative examples, resulting in the best\naccuracy.\nAs the dataset used for training our model was\nconstructed in an unsupervised manner, there ex-\nisted a possibility of the model being biased to-\nwards the most frequently occurring senses of a\ngiven lemma. To assess this, we evaluated the\nmodel’s accuracy based on the frequency of sense\nusage referring to the SUM dictionary (see Table\n5). Our findings showed that the PMMBv2 model\ntuned on ∼1,2M triplets with filtering performed\nbetter for the less commonly occurring senses.\nTherefore, we can infer that the fine-tuned model\nnot only considers the context but also makes pre-\ndictions that are not solely based on the popularity\nof a sense.\nWe have evaluated our approach on the poly-\nsemy dataset to investigate the correlation between\nthe performance of the model on homonyms and\npolysemous lemmas. The Table 6 shows the accu-\nracy of the model on the polysemy dataset, where\nwe have examined the model’s ability to predict the\nfirst 2/3/all lexical meanings of each lemma. How-\n16\nModel Overall acc. Noun acc. Verb acc. Adj. acc. Adv. acc.\nBabelfy baseline 0.526 - - - -\nPMMBv2 0.735 0.767 0.668 0.752 0.593\nPMMBv2 tuned on\n∼190K triplets 0.77 0.819 0.685 0.743 0.562\nPMMBv2 tuned on\n∼1,2M triplets 0.778 0.825 0.698 0.761 0.531\nPMMBv2 tuned on\n∼1,2M triplets with\nfiltering\n0.779 0.824 0.693 0.759 0.607\nTable 4: Accuracy on the WSD homonym evaluation dataset for Ukrainian Language using Babelfy, PMMBv2, and\nmodels fine-tuned by the proposed approach.\nFrequency\nof\nsense usage\nPMMBv2\nPMMBv2 tuned\non\n∼1,2M triplets\nwith filtering\n1 0.76 0.799\n2 0.703 0.754\n3 0.666 0.773\nTable 5: Accuracy on the WSD evaluation dataset for\nthe Ukrainian Language based on the frequency of sense\nusage for the PMMBv2 baseline and fine-tuned version.\never, we have observed a decrease in performance\nwhen evaluating the polysemy dataset, despite us-\ning better homonym-specific embeddings achieved\nthrough fine-tuning. We hypothesize that this may\nbe due to the challenge of distinguishing between\nsimilar meanings for polysemous words (see Ta-\nble 1). Furthermore, our observations indicate that\nthe model PMMBv2, fine-tuned on 1,2M triplets\nwith filtering out pairs, exhibits an even greater\ndecrease in performance when applied to the poly-\nsemy dataset compared to PMMBv2 fine-tuned on\n1,2M triplets without filtering.\n7 Conclusion\nOur research proposes a novel approach for solving\nthe WSD task in under-resourced languages such as\nUkrainian. We used a supervised approach to fine-\ntune LLMs on the unsupervised dataset generated\nby our method.\nFurthermore, we built an evaluation dataset\nbased on the SUM dictionary, which other re-\nsearchers can use for evaluating the WSD task in\nthe Ukrainian language.\nWe implemented the U-WSD framework during\nthe research, which preprocess and generate evalu-\nation and fine-tuning datasets, perform inference,\nand measure performance.\nOur approach achieved 77.9% accuracy on the\nhomonym dataset, surpassing graph-based methods\nsuch as Babelfy.\nFuture work aims to enhance the quality of the\nfine-tuning dataset by employing several measures.\nThese measures include the removal of nearly iden-\ntical anchor and positive examples, the exclusion\nof named entities detected as the target lemma, and\nthe sampling of a more uniformly representative\nsubset of examples for each lemma. We also want\nto improve the target lemma detection algorithm.\nAdditionally, we plan to explore more advanced\nembedding comparison mechanisms beyond cosine\nsimilarity.\nLimitations\nThe proposed approach has several limitations.\nFirstly, the approach is evaluated on a relatively\nsmall dataset of homonyms, which contains exam-\nple from fiction, folklore, etc. Our dataset might\nnot represent the entire Ukrainian language. Addi-\ntionally, we focus only on homonymy, which may\nlimit the approach’s applicability to real-world sce-\nnarios where both homonymy and polysemy are\npresent.\nDuring our research on WSD, we discovered\na lack of bias control in the SUM and UberText\ndatasets. This deficiency presents a potential issue\nof such as gender, race, or socioeconomic status\nbiases in our model.\nRecreating the fine-tuning process requires a\nGPU with sufficient memory, such as the NVIDIA\nT4 GPU with 16 GB of memory on the AWS in-\n17\nModel First 2\nsenses\nFirst 3\nsenses All senses\nPMMBv2 0.682 0.637 0.608\nPMMBv2 tuned on\n∼190K triplets 0.702 0.66 0.632\nPMMBv2 tuned on\n∼1,2M triplets 0.7 0.656 0.629\nPMMBv2 tuned on\n∼1,2M triplets with\nfiltering\n0.689 0.646 0.618\nTable 6: Accuracy on the WSD polysemy evaluation dataset for Ukrainian Language using, PMMBv2, and models\nfine-tuned by the proposed approach.\nstance g4dn.xlarge.\nTo use the proposed approach for languages\nother than Ukrainian, a dictionary with lemmas\nand their lexical meanings, mechanisms to classify\nparts of speech, and a large dataset with sentences\nfrom various areas to cover lemmas with different\nmeanings are needed.\nEthics Statement\nOur objective is to increase the accessibility of NLP\nresearch by prioritizing under-resourced languages,\nwith a particular focus on Ukrainian language re-\nsearch. Through the development of generalizable\napproaches, we hope to create solutions that can be\napplied to a variety of languages beyond Ukrainian.\nWe are also mindful of the potential real-world im-\npact of our research, and we strive to ensure that\nour work contributes to the advancement of society.\nFinally, we believe in the importance of engaging\nwith the broader NLP community, particularly the\nglobal ACL community, to promote collaboration\nand knowledge-sharing.\nReferences\nVassileios Balntas, Edgar Riba, Daniel Ponsa, and Krys-\ntian Mikolajczyk. 2016. Learning local feature de-\nscriptors with triplets and shallow convolutional neu-\nral networks. In Bmvc, volume 1, page 3.\nEdoardo Barba, Luigi Procopio, and Roberto Navigli.\n2021. Consec: Word sense disambiguation as con-\ntinuous sense comprehension. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1492–1503.\nDmytro Chaplynskyi. 2023. Introducing UberText 2.0:\na corpus of modern Ukrainian at scale. In Proceed-\nings of the Second Ukrainian Natural Language Pro-\ncessing Workshop, pages 1–10, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nManaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris\nDyer, Eduard Hovy, and Noah A Smith. 2014.\nRetrofitting word vectors to semantic lexicons. arXiv\npreprint arXiv:1411.4166.\nLuyao Huang, Chi Sun, Xipeng Qiu, and Xuanjing\nHuang. 2019. Glossbert: Bert for word sense dis-\nambiguation with gloss knowledge. arXiv preprint\narXiv:1908.07245.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nGeorge A Miller, Claudia Leacock, Randee Tengi, and\nRoss T Bunker. 1993. A semantic concordance.\nIn Human Language Technology: Proceedings of\na Workshop Held at Plainsboro, New Jersey, March\n21-24, 1993.\nAndrea Moro, Alessandro Raganato, and Roberto Nav-\nigli. 2014. Entity linking meets word sense disam-\nbiguation: a unified approach. Transactions of the\nAssociation for Computational Linguistics, 2:231–\n244.\nRoberto Navigli. 2009. Word sense disambiguation: A\nsurvey. ACM computing surveys (CSUR), 41(2):1–\n69.\n18\nSteven Neale, Luís Gomes, Eneko Agirre, Oier Lopez\nde Lacalle, and António Branco. 2016. Word sense-\naware machine translation: Including senses as con-\ntextual features for improved translation models. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 2777–2783.\nUkrainian Lingua-Information Fund of NAS of Ukraine\n(ULIF-NASU). 2010. Словник української мови\n[Dictionary of the Ukrainian language], volume 20\nof Словники України [Dictionaries of Ukraine].\nНаук. думка [Nauk. dumka], Kyiv.\nJeffrey Pennington, Richard Socher, and Christo-\npher D Manning. 2014. Glove: Global vectors\nfor word representation. In Proceedings of the\n2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pages 1532–1543.\nSameer Pradhan, Edward Loper, Dmitriy Dligach,\nand Martha Palmer. 2007. Semeval-2007 task-\n17: English lexical sample, srl and all words. In\nProceedings of the fourth international workshop\non semantic evaluations (SemEval-2007), pages\n87–92.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D Manning. 2020. Stanza:\nA python natural language processing toolkit\nfor many human languages. arXiv preprint\narXiv:2003.07082.\nVitalii Radchenko. 2020. Youscan. https://\nyouscan.io/blog/ukrainian-language-model/.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, Ilya Sutskever, et al. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084.\nNeetu Sharma and Prof. S. Niranjan. 2015. Applica-\ntions of word sense disambiguation: A historical\nperspective. INTERNATIONAL JOURNAL OF\nENGINEERING RESEARCH TECHNOLOGY\n(IJERT) NCETEMS-2015 (Volume 3-Issue 10).\nNakatani Shuyo. 2010. Language detection li-\nbrary for java. http://code.google.com/p/\nlanguage-detection/.\nRobyn Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual\ngraph of general knowledge. In Proceedings of\nthe AAAI conference on artificial intelligence,\nvolume 31.\nNAS of Ukraine Ukrainian Lingua-\nInformation Fund. 2022. Ulif. https:\n//en.ulif.org.ua/.\nGregor Wiedemann, Steffen Remus, Avi Chawla,\nand Chris Biemann. 2019. Does bert make any\nsense? interpretable word sense disambiguation\nwith contextualized embeddings. arXiv preprint\narXiv:1909.10430.\nA U-WSD framework\nDuring our research, we implemented the frame-\nwork to aid in working with models and evalu-\nating their performance in the WSD task for the\nUkrainian language, which is available at https:\n//github.com/YuriiLaba/U-WSD. This frame-\nwork consists of three main parts: (1) cleaning\nand generation of the SUM dataset, (2) embedding\ncalculation and prediction running, and (3) perfor-\nmance metric evaluation.\nThe first part includes various dataset-cleaning\ntechniques, such as filtering by the length of the\nlemma, selecting the first n senses or examples for\neach lemma, and more. Additionally, this part al-\nlows the generation of a dataset with lexical mean-\nings for each lemma separately or grouping mean-\nings at the homonym level.\nThe second part enables the selection of differ-\nent models and pooling strategies for calculating\nembeddings for lexical meanings and examples.\nFinally, the third part generates a performance re-\nport based on the part of speech, lemma frequency\nwhich is obtained from the Ubertext dataset Chap-\nlynskyi (2023), and different numbers of top n lexi-\ncal senses of a lemma.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8325159549713135
    },
    {
      "name": "Ukrainian",
      "score": 0.7794848084449768
    },
    {
      "name": "Natural language processing",
      "score": 0.71391761302948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6886338591575623
    },
    {
      "name": "Pooling",
      "score": 0.638088583946228
    },
    {
      "name": "Word-sense disambiguation",
      "score": 0.6326882839202881
    },
    {
      "name": "Task (project management)",
      "score": 0.6108983159065247
    },
    {
      "name": "Word (group theory)",
      "score": 0.5690674781799316
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5031642317771912
    },
    {
      "name": "Language model",
      "score": 0.4251551032066345
    },
    {
      "name": "Linguistics",
      "score": 0.24234619736671448
    },
    {
      "name": "WordNet",
      "score": 0.10460084676742554
    },
    {
      "name": "Psychology",
      "score": 0.060776710510253906
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}