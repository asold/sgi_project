{
  "title": "Improved Fine-Tuning of In-Domain Transformer Model for Inferring COVID-19 Presence in Multi-Institutional Radiology Reports",
  "url": "https://openalex.org/W4308113119",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5078753587",
      "name": "Pierre Chambon",
      "affiliations": [
        "Universit√© Paris-Saclay",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5038843479",
      "name": "Tessa S. Cook",
      "affiliations": [
        "Philadelphia University",
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5087710258",
      "name": "Curtis P. Langlotz",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W4282983782",
    "https://openalex.org/W3166358520",
    "https://openalex.org/W3166148876",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W3166845084",
    "https://openalex.org/W3177477686",
    "https://openalex.org/W3103694015",
    "https://openalex.org/W3094263970",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3006304308",
    "https://openalex.org/W2106411961",
    "https://openalex.org/W2804268694",
    "https://openalex.org/W4238746485",
    "https://openalex.org/W4394662461",
    "https://openalex.org/W3012023297",
    "https://openalex.org/W2113954950",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3006436762",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W2749581528",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W4226466418",
    "https://openalex.org/W2989753407",
    "https://openalex.org/W4220888215",
    "https://openalex.org/W3098350627"
  ],
  "abstract": null,
  "full_text": "Vol:.(1234567890)\nJournal of Digital Imaging (2023) 36:164‚Äì177\nhttps://doi.org/10.1007/s10278-022-00714-8\n1 3\nORIGINAL PAPER\nImproved Fine‚ÄëTuning of¬†In‚ÄëDomain Transformer Model for¬†Inferring \nCOVID‚Äë19 Presence in¬†Multi‚ÄëInstitutional Radiology Reports\nPierre¬†Chambon1 ¬†¬∑ Tessa¬†S.¬†Cook2¬†¬∑ Curtis¬†P .¬†Langlotz3\nReceived: 5 September 2022 / Revised: 5 September 2022 / Accepted: 3 October 2022 / Published online: 2 November 2022 \n¬© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2022\nAbstract\nBuilding a document-level classifier for COVID-19 on radiology reports could help assist providers in their daily clinical \nroutine, as well as create large numbers of labels for computer vision models. We have developed such a classifier by fine-\ntuning a BERT-like model initialized from RadBERT, its continuous pre-training on radiology reports that can be used on \nall radiology-related tasks. RadBERT outperforms all biomedical pre-trainings on this COVID-19 task (P <0.01) and helps \nour fine-tuned model achieve an 88.9 macro-averaged F1-score, when evaluated on both X-ray and CT reports. To build this \nmodel, we rely on a multi-institutional dataset re-sampled and enriched with concurrent lung diseases, helping the model to \nresist to distribution shifts. In addition, we explore a variety of fine-tuning and hyperparameter optimization techniques that \naccelerate fine-tuning convergence, stabilize performance, and improve accuracy, especially when data or computational \nresources are limited. Finally, we provide a set of visualization tools and explainability methods to better understand the \nperformance of the model, and support its practical use in the clinical setting. Our approach offers a ready-to-use COVID-19 \nclassifier and can be applied similarly to other radiology report classification tasks.\nKeywords Radiology¬†¬∑ COVID-19¬†¬∑ Classification¬†¬∑ Natural language processing (NLP)¬†¬∑ Transformer¬†¬∑ BERT\nIntroduction\nTransformers [1 ], which gave birth to BERT [2 ], are \nnow broadly shared through libraries like Hugging Face \nTransformers [ 3] and have reached new state-of-the-art \nperformance.\nThe recent focus has been on developing BERT-like pre-\ntrained models that work well on downstream tasks, which \ninclude various medical or radiology applications. We dis-\ntinguish continuous pre-trainings, where model weights \nare initialized from an already pre-trained BERT and then \nfurther pre-trained on a biomedical dataset [4 ‚Äì6], from the \nfrom-scratch pre-trainings that seem to be even more prom-\nising but require larger amounts of data [7 ‚Äì9]. Other than \na recent attempt at continuous pre-training on radiology \nreports [10], no extensive pre-training research has been \ndone in this domain. In particular, there exists no radiology \npre-training that tackles a diagnosis task such as lung disease \nclassification.\nMany radiology downstream tasks require a fine-tuning \nof these pre-trained models on a task-specific dataset: radi-\nology report summarization [ 11, 12], generation [ 13, 14], \nand token-level or document-level classification [15, 16]. \nBut these previous works typically do not provide diagnostic \noutputs. Rare attempts to classify lung diseases suffer from \nlimited performance. For instance, CheXbert [16] labels \nthe presence of 14 types of observations but achieves only \n0.798 of macro-averaged F1-score, which limits its use in \nthe clinical setting. To the best of our knowledge, only one \nprevious model for COVID-19 classification has been based \non radiology reports [17]. Their work is limited by a small \ndataset that contained only reports suspicious for COVID-\n19. It does not study COVID-19 in the presence of other \nprevalent lung diseases, nor does it rely on modern natural \n * Pierre Chambon \n pchambon@stanford.edu\n Tessa S. Cook \n tessa.cook@pennmedicine.upenn.edu\n Curtis P. Langlotz \n langlotz@stanford.edu\n1 Stanford University, Paris-Saclay University, √âcole Centrale \nParis, Stanford, USA\n2 University of¬†Pennsylvania, Philadelphia, USA\n3 Stanford University, Stanford, USA\n165Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nlanguage processing (NLP) techniques such as transformers \nto maximize performance.\nLimited data remains the main bottleneck for ML projects \nin medicine due to challenges in de-identification of text \nand the cost of labeling. In addition, most NLP tools for \nradiology reports suffer from low generalizability on multi-\ninstitutional data and are rarely optimized during hyperpa-\nrameter tuning, leading to instability or under-performance.\nTo improve the knowledge retained from the fine-tuning \nstep, strategies like ULMFit [18] aim to carefully design the \nfine-tuning process to help the transfer of knowledge, lev -\neraging the learning rate and momentum scheduling or the \nunfreezing of layers. Originally designed to help fine-tune \nLSTM models, we can expect these methods among others \n[7, 19‚Äì21] to show similar improvements on the fine-tuning \nof BERT-like models. Similarly, many algorithms have \nbeen designed to explore the hyperparameter space, such as \nBayesian optimization or population-based training [22‚Äì26].\nIn this context, we propose new methods to develop a \nCOVID-19 document-level classification model for radiol-\nogy reports (see Fig.¬†1). We release RadBERT, a pre-trained \nBERT model on radiology reports, along with its fine-tuned \nversion for the task of COVID-19 classification, using a \nmulti-institutional dataset specifically labeled for this task. \nWe provide a set of fine-tuning strategies that are helpful \nto better optimize the performance of a BERT model on \na downstream task. This includes the comparison of sev -\neral pre-trained models for tasks on radiology reports and \nthe study of the hyperparameter space of a BERT model on \nradiology reports, thus suggesting methods to improve the \nfine-tuning performance.\nCOVID-19 classification is a complex task because of the \nneed to distinguish COVID-19 from other lung diseases and \nother types of focal or multifocal pneumonia. Models and fine-\ntuning strategies that perform well on this task are likely to \nperform well on classification of other diseases. Our solution \napplies to both X-ray and CT reports and therefore represents \na good benchmark to reuse its pre-processing approaches and \nconclusions on both planar and cross-sectional datasets. We \nstudy the performance under data and computational con-\nstraints that we often encounter in medical AI projects, mak-\ning the tools and training strategies we propose reusable for \nother medical text classification tasks.\nMaterials and¬†Methods\nData Collection and¬†Annotation\nOur BERT model for radiology reports was pre-trained \non 4,056,227 reports from 608,140 unique patients being \ntreated at Stanford Health Care from 1992 to 2014. The data-\nset includes more than one thousand different exam types \nacross all body areas.\nThe fine-tuning dataset comprises 19,384 reports col-\nlected during 2020 in an academic health system, Penn \nMedicine. A total of 3520 reports were labeled by the radi-\nologists at the time of clinical interpretation as COVID-19, \n4752 as uncertain COVID-19, and 11,112 as no COVID-\n19. Radiologists all agreed to a consensus statement on the \nmeaning of each label before starting the study. Due to over-\nrepresentation of cases uncertain and positive for COVID-\n19, we resampled our subset of labeled reports among all \nchest reports. We included additional negative cases with co-\nprevalent lung diseases to approximate the actual prevalence \nof COVID-19. The initial dataset contained approximately \n7000 negative cases, which grew to 11,000 reports after \nresampling. The number of reports and their balance vary \nacross the sites of the academic health system (see Fig.¬† 2).\nReports from the fine-tuning dataset correspond to both \nX-rays and CTs of the chest: 16,432 X-rays and 2952 CTs. \nWe split the fine-tuning data into a training set (16,876 \nreports), development set (838 reports), and test set (1654 \nreports), ensuring every patient belongs to a single split.\nFig. 1  Our classification task \nconsists of consuming the \ntext of a radiology report and \ngenerating one of three labels: \nCOVID-19, uncertain COVID-\n19, and no COVID-19 \n\n166 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\nIn addition to our fine-tuning dataset, which includes \na test set of 1654 reports, we built a test set from a sepa-\nrate institution, unseen during training: we collected chest \nX-ray radiology reports from Stanford Health Care from \nApril 1, 2020, to December 31, 2020, resulting in a total \nof 66,000 data points.\nTo label a portion of this test set, we used our baseline \nmodel described in the ‚Äú Baseline Model ‚Äù¬†section¬†to find \ncandidate reports for each label with moderate precision \nbut correct recall (75.3). This baseline model detected \na 4% prevalence of COVID-19 cases, which means that \nsufficient statistical significance would require too many \nreports to label, based on a non-inferiority test with sig-\nnificance 5% and power 80% [27‚Äì 29]. Leveraging our \nbaseline model, we oversampled COVID-19 and uncertain \nCOVID-19 candidates and randomly sampled no COVID-\n19 candidates among the remaining reports. This formed \na test set of 300 reports from Stanford Health Care, which \nwas hand-labeled by a radiologist from the same institu-\ntion with more than 10 years of experience, after training \non a data sample and agreeing on a consensus statement \nwith a radiologist from Penn Medicine. This test set is \ncomposed of 41 COVID-19 cases, 154 uncertain COVID-\n19 cases, and 105 no COVID-19 cases.\nBefore being processed by our model, the radiology \nreports are pre-processed into a common format. Using \nrule-based parsers, we formatted the reports in a consistent \nmanner and removed institution-specific sections that had \nno relevance to the task (e.g., Contrast). We chose to rely \non a rule-based model that omits less important sections \nand clinical material to shorten the report below the 512-\ntoken threshold, whenever the reports were too long (see \nSupplementary Fig.¬†1).\nPre‚Äëtraining Methods\nTo handle our COVID-19 classification task, we develop \na BERT-based approach [2 ] as it has been successful on \nnumerous NLP tasks, and is supported by the availability of \nmany pre-trainings [3].\nAll these BERT pre-trainings, including the ones using \nbiomedical and clinical data, can be characterized by four \nmain features: the pre-training dataset, the weight initiali-\nzation, the vocabulary, and the training techniques. First, \nthe pre-training datasets can be distinguished between in-\ndomain vocabulary and structure, comprised of radiology \nreports, such as [10]; in-domain vocabulary only, containing \nany types of biomedical texts like in the case of BioBERT \n[5] or BlueBERT [6 ]; and out-of-domain vocabulary cor -\nresponding for instance to BaseBERT [2]. Second, we iden-\ntify two weight initialization approaches, either from-scratch \npre-trainings that are initialized with random weights or con-\ntinuous pre-trainings that use weights from a previously pre-\ntrained model. Third, each pre-training uses a pre-defined \nvocabulary, which can be freely chosen to correspond to the \npre-training dataset for all from-scratch pre-trainings, but \nis imposed by the former pre-training vocabulary in case of \ncontinuous pre-trainings. Fourth, each pre-training can lev -\nerage a variety of training strategies: various self-supervised \nobjectives [30] or generators [31].\nAside from the in-domain vocabulary and structure data, \nour pre-training is fairly simple, in that it follows the gen-\neral guidelines from [2]. Using a weight initialization from \nBioBERT, itself first initialized from BaseBERT, we further \npre-train for a few hundred thousand steps. In total, we adjust \n109,493,006 parameters across the embeddings, 12 BERT \nlayers (consisting of attention, linear, layer normalization, \nFig. 2  Our fine-tuning dataset includes radiology reports from 6 sites \nwithin the same health academic system, Penn Medicine. The left \ngraph shows the number of reports provided by each site, dominated \nby three sites. The graph on the right shows that the data imbalance \nremains stable across these three main sites. There is less balance in \nthe three remaining sites\n167Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nand dropout layers) and a pooler layer with tanh activation. \nOur output hidden dimensions have size 768. We do not \nconsider any larger models, as they would not fit on a single \nGPU requiring more work and resources, and would less \neasily compare to other BERT models.\nFine‚ÄëTuning Methods\nWe assess a variety of fine-tuning strategies discussed below \nand provide supplemental illustrations in the Supplementary \nMaterial.\nLearning Rate Strategies\nWe measured the impact of several learning rate strategies \non model performance. Most of these approaches were \ndeveloped by ULMFit when pre-trained LSTMs were the \nstandard approach on NLP tasks.\nIn the equations that follow, /u1D6FC is the learning rate, mod-\neled as a function of time t (the number of training steps or \nepochs so far) and the layer index l.\nDiscriminative learning rate consists of varying the learn-\ning rate across layers, enabling layers at the top to easily \nadjust their weights for the fine-tuning objective while pre-\nventing layers at the bottom to forget pre-training knowledge:\nwith 0 ‚â§ l the layer index from top to bottom, and 0 <ùõø ‚â§ 1 \nparameterizing the method.\nOne-cycle triangular scheduling enables a quick conver -\ngence to an interesting region of the parameter space before \nrefining the weights:\nwith tmin ‚â§ t ‚â§ tmax  the training time and 1 ‚â™ùõæ  parameter-\nizing the method.\nSlanted triangular scheduling speeds up the convergence \nto a region of interest in the parameter space:\nFinal decay adds a few extra training steps to choose the \nbest parameters in a small neighborhood where the perfor -\nmance is satisfying:\nwith tmax < t , t‚àí tmax ‚â™ tmax  and 1 ‚â™ùõΩ  parameterizing the \nmethod.\n/u1D6FCt,l = /u1D6FCt /u1D6FFl\n/u1D6FCt =( 1\n/u1D6FE‚àí 1)max /u1D6FC(\nt1‚àï2 ‚àí t\nt1‚àï2 ‚àí tmin\n/u1D7D9t‚â§t1‚àï2\n+\nt‚àí t1‚àï2\ntmax ‚àí t1‚àï2\n/u1D7D9t‚â•t1‚àï2\n)+ max /u1D6FC\nt1‚àï2 < tmax + tmin\n2\n/u1D6FCt = 1\n/u1D6FEmax /u1D6FC‚àí 1\n/u1D6FE/u1D6FD\nt‚àí tmax\ntmax\nIn parallel, we also use slanted triangular momentum, \nwhich is similar in every aspect to the learning scheduling \nexcept that the variations are reversed, as found empirically \n(see Supplementary Fig.¬†2, [32]).\nUnfreezing Scenarios\nTo avoid catastrophic forgetting phenomena and fine-tune \neach layer only to the extent that is needed, we include \nunfreezing methods in our general fine-tuning approach. We \ndistinguish several scenarios: fine-tuning only the weights \nof the task-specific head, fine-tuning all the weights for the \nsame duration, or fine-tuning different layers for different \namounts of time. The latter can correspond either to the \noriginal gradual unfreezing approach suggested by [18] or \nto our own approach: training only the head at the beginning \nand then the entire transformer (see Supplementary Fig.¬†3).\nRegularization Techniques\nFollowing the ideas of [33] and [34], we try to use higher \nvalues of the learning rate at least at the beginning of the \nfine-tuning phase, then relying on our triangular scheduling \nwith final decay to stabilize the convergence at the end of the \ntraining. As suggested by the authors, this could help speed \nup the convergence and fight overfitting not only by lever -\naging the dropout probability, but also the learning rate and \nthe batch size. We suspect that higher values of the learning \nrate prevent the model from falling into local minima. Larger \nbatch sizes smooth the gradient descent, thus achieving a \nsimilar effect as directly increasing the dropout (see Sup -\nplementary Fig.¬†4).\nHyperparameter Optimization\nThe exploration of the hyperparameter space relies on two \ncomponents: a search algorithm and a trial scheduler [35]. \nThe search algorithm uses the previously acquired knowl-\nedge, that is to say pairs of sets of hyperparameters and \nvalidation score, to suggest the next set of hyperparameters \nto try (hopefully converging to the ideal set of hyperparam-\neters). The trial scheduler helps speed up this exploration \nby either prioritizing certain trials, early stopping others, or \nmerging them.\nFor the search algorithm, we choose to rely on Bayesian \noptimization [24] using a Tree-structured Parzen Estimator \n[22], which has the advantage of working well on continuous \nsearch spaces not too dimensionally heavy and being robust \nto stochastic noise. This estimator uses the validation score \nobtained on each hyperparameter set (viewed as a point of \nthe hyperparameter space) to update its empirical distribu-\ntions, then used to suggest the next hyperparameter set to \ntry. More specifically, it models a distribution of good trials \n168 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\nÃÇ‚Ñôscore<y‚àó and a distribution of bad trials ÃÇ‚Ñôscore>y‚àó with y ‚àó \nbeing a percentile of the total distribution of trials ÃÇ‚Ñô . Then, \nthe ratio of these distributions is used to suggest the next \nof hyperparameters with maximal expected improvement:\nwith the empirical densities being estimated using Parzen \nwindows (see Fig.¬†3).\nFor the trial scheduler, we use a FIFO scheduler and \navoid any early stopping techniques like with ASHA [26], \nwhich can give an unfair advantage to models that learn \nquickly over the first epochs without building solid long-\nterm knowledge.\nExplainability Methods\nOur own use-case, for a COVID-19 classifier, aims at provid-\ning explanations on the model‚Äôs decision for each input report \n(local), using if necessary additional computations (post hoc), \nbeing as correct as possible (both sensitivity and implementa-\ntion invariance) and relying on the words used in the input \nreports, which are easily interpretable by clinicians (feature \nimportance) [36] propose a method called integrated gradi-\nents, which fulfills all these requirements, is gradient-based, \nargmaxx‚ààX\nÃÇ‚Ñôscore<y‚àó(x)\nÃÇ‚Ñôscore>y‚àó(x)\nand provides explanations that we display visually using sali-\nency maps. In particular, we do not use attention-based meth-\nods, which ignore the weights from the other layers of the \nmodel, thus inaccurately depicting the relations between inputs \nand outputs.\nIntegrated gradients consist of examining the input in com-\nparison to a pre-defined baseline input (in our case the dummy \ntext made out of [PAD] tokens only). Then, following a straight \nline in the space of the input, we accumulate the gradients of \nthe model going from the baseline input to the input. This \nmeasures how much and in which direction the model changes \nits decision, when going from a neutral text (the baseline input) \nto the text of interest (the input). We take the gradients rela-\ntively to each word to get scores for each one of them. This \ngives IG(w) the degree of importance of each word w and the \ndirection of its impact (positive or negative) on the decision \nof the model:\nwith T being the input text, T0 the baseline input text, and \nw0 the word corresponding to w in the baseline T0 . As such, \nintegrated gradients are often classified as a path method \nwhich follows the straight line, thus preserving linearity and \nsymmetry along the path [36].\nIG(w)=( w ‚àí w0) /uni222B.dsp\n1\n/u1D716=0\n/u1D715M(T0 + /u1D716(T ‚àí T0))\n/u1D715w\nFig. 3  The Tree-structured Parzen Estimator builds empirical distributions on the hyperparameter space and suggests points that are highly \nlikely under the distribution of good trials while being highly unlikely under the distribution of bad trials\n169Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nResults\nTraining Details\nExperiments were conducted using private compute infra-\nstructures. The pre-training takes 4 days on a single GPU \nNVIDIA Tesla V100 leading to an estimated 5.76 kgCO2 eq \nof total emissions.\nFor the fine-tuning, each training on the full train set of \n‚àº17k reports takes 2:30 min per epoch if training only the \nhead and 8 min per epoch if training the full model (not \ncounting ‚àº 2 min of initialization of the training) on a sin-\ngle GPU NVIDIA Quadro P5000. During hyperparameter \noptimization, we explore 100 hyperparameter sets and train \nas many models. Using 3 GPUs NVIDIA Quadro P5000, \nthis averages to 51 h of total training, i.e., 17 h when paral-\nlelized across the 3 GPUs (see see Supplementary Notes on \nimplementation and Supplementary Table¬†1).\nThe total emissions to run all fine-tuning experiments \nare estimated to be 40.06 kgCO2eq. These estimations were \nconducted using an online tool [37].\nBaseline Model\nWe develop a simple baseline approach that we evaluate on \nthe same test set as the other models, providing context and \nelements of comparison for the results.\nThe baseline relies on a frequentist approach that starts by \ncompiling all the words found in the reports of the training \nset. Then, it computes the frequency of each word among the \nreports of each category (COVID-19, uncertain COVID-19, \nand no COVID-19) and builds an empirical distribution of \nthe vocabulary within each category. So that for each input \nreport of the test set, the baseline assigns the category to \nwhich the report is the most likely to belong, according to \nthe empirical distributions of the vocabulary.\nIn addition, we also compare our results with the scores \nof CheXbert [ 16], especially on its pneumonia  task. That \ntask is easier than ours as it does not require distinguishing \namong several types of pneumonia.\nEvaluation Metrics\nThe experiments are evaluated on a test set from the same \nacademic health system as the training data and on a test \nset from a different academic health system. On each test \nset, we report F1-score, recall, and precision on each of the \nthree categories, namely COVID-19, uncertain COVID-19, \nand no COVID-19. To compare models across these three \ncategories, we rely on macro-averaged metrics, which do not \nbias the scores towards the majority class (no COVID-19) \nas micro-averaged metrics would. Non-parametric bootstrap \nwith 1000 bootstrap samples is used to compute both 95% \npercentile confidence intervals and two-tailed paired-sample \nWilcoxon tests with significance level 0.05. We include the \nBonferroni correction whenever testing multiple hypotheses.\nExperimental Results\nThe results of our best model are shown in Table¬† 1. It is \ntrained on the full dataset of X-rays and CTs and uses both \nour continuous pre-training on radiology reports and all the \nfine-tuning methods that we presented in the ‚Äú Fine-tuning \nMethods‚Äù section, parameterized as described in the ‚ÄúTrain-\ning Details‚Äù section, and optimized following the Tree-\nstructured Parzen Estimator scheme. When we evaluate it \non the test set from the same institution as the training set \ncomprising both X-rays and CTs, this model obtains a macro-\naveraged F1-score of 88.9 (class-wise F1-scores: COVID-19 \n87.6, uncertain COVID-19  84.2, and no COVID-19  94.9), \nwith 95% confidence interval [87.5; 90.3]. The performance \non X-rays only is slightly higher, with a macro-averaged \nF1-score of 90.5, and lower on CTs only, 79.4. This can be \nexplained by the fact that CT reports are longer, contain more \nelaborate descriptions of disease, and are less prevalent in the \ntraining set than X-ray reports.\nWe can leverage the confidence thresholds of the best \nmodel and choose to not retain the 10% fraction of the \nreports where the model is the least confident: in the clini-\ncal setting, this would mean that the model handles 90% \nof the radiology reports and asks for additional help from \na radiologist for the remaining 10%. This is very helpful \nfor the model and helps it achieve 93.0 of macro-average \nF1-scores on the test set with both X-rays and CTs (all class-\nwise F1-scores are around 90 or more). Notice that this does \nnot make the model less competent: we control the reports \ndropped by the model and ensure that it does not only drop \nCOVID-19 reports, which are what we are interested in. \nWhen the threshold is chosen to be 10%, we can measure in \nthe test set that the model drops 15% of COVID-19 reports, \n14.5% of uncertain COVID-19, and 6.5% of no COVID-19. \nDropping more or less reports allows adjustment of perfor -\nmance for the trade-off that best suits its use case.\nIf we compare these results with our frequentist baseline \nmodel, the latter achieves a macro-averaged F1-score of 72.6 \non the test set from the known health system. CheXbert [16], \nwhich was trained on a classification task of several lung \ndiseases including pneumonia using hundreds of thousands \nof reports, achieves a weighted F1-score of 83.5. The use \nof the weighted F1-score metric gives an advantage to the \nmodel as more weight is given to the majority class that is \neasier to classify‚Äîthe reports with no symptoms. In addi-\ntion, their dataset includes only X-rays, which are easier to \nclassify than CTs. In comparison, our best model achieves \n170 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\na weighted F1-score of 92.2 on X-rays only, which cor -\nresponds to an almost +10 improvement, on a task that is \nlikely more difficult. Finally, our model needs only 1+3 \nepochs (1 epoch training the head only, 3 epochs the full \nmodel) compared to 8 epochs for CheXbert. This shows not \nonly the superiority of our approach on the COVID-19 clas-\nsification task, where there exists no other reliable model \n(to the best of our knowledge), but also its potential when \napplied to the classification of other lung diseases.\nAs seen in Table¬†1, when evaluated within a new health \nacademic system (Stanford Health Care), our best model \nachieves 62.6 macro-averaged F1-score on X-rays, with \nrecall around 88 on both COVID-19 and no COVID-19  \nreports but much lower recall, 39.0, on uncertain COVID-\n19 reports. A radiologist from the same institution as the \ntraining data, Penn Medicine, also achieves only 82.0 macro-\naveraged F1-score on this test set, with the lowest recall \nbeing on uncertain COVID-19 reports. Whereas the model is \nable to maintain correct performance on COVID-19 and no \nCOVID-19 reports, we observe a significant drop of perfor-\nmance due not only to a shift of vocabulary and report struc-\nture, but also hypothetically to a shift of definition of uncer-\ntain COVID-19. In this sense, we measured that 84% of the \nmodel mistakes were on uncertain COVID-19 reports, and \nleveraging confidence thresholds help improve performance \non the first two classes (confidence threshold of p=0.9 leads \nto ‚àº 95 recall on both of them) but do not mitigate the low \nperformance on uncertain COVID-19 reports.\nOur best model is based on our continuous pre-training \non radiology reports, which was the best performing pre-\ntraining method compatible with our task, as described in \nTable¬†2. When evaluating on both X-rays and CTs, our pre-\ntraining achieves a macro-averaged F1-score of 88.9, which \nis approximately 1 point of F1-score higher than all other \npre-trainings (P<0.01). Notice that we outperform both other \ncontinuous pre-trainings (BioBERT, BlueBERT, P <0.01) \nand from-scratch pre-training (PubMedBERT, P<0.01). The \nlatter achieved the best performance on CTs only (P <0.01) \nbut was beaten by RadBERT on X-rays (P<0.01): not seeing \nany radiology reports at pre-training time offsets its advan-\ntage of being a from-scratch pre-training.\nTo assess which types of reports to include in the training \nset, we evaluate the performance of the model trained on dif-\nferent compositions of the fine-tuning dataset, as depicted in \nTable¬†3. The drop in performance accounts for the fact that \nall fine-tuning datasets have the size of the smallest of them \nall, the CT dataset, which contains only 2952 reports before \nthe split. The model trained on both X-rays and CTs achieves \nsimilar performance to the model trained on X-rays only \nwhen evaluating on X-rays only, but higher performance per-\nformance to the model trained on X-rays only when evaluat-\ning on CTs only ( P<0.01); the model trained on CTs only \nperforms poorly when evaluated on X-rays only (P <0.01). \nTherefore, we choose to always train on both X-rays and CTs \nregardless of the composition of the test set.\nFinally, we study the impact of the fine-tuning strate-\ngies on the performance of the model and report the results \nin Table¬† 4. We compare two approaches, based on the \nsame pre-training (RadBERT): standard, where we use no \nadvanced fine-tuning strategies as described in the ‚ÄúFine-\ntuning Methods‚Äù section¬†and optimize with a small grid; \nours, where we use all fine-tuning strategies and the Tree-\nstructured Parzen Estimator. When evaluated on both X-rays \nand CTs, the standard approach achieves a macro-averaged \nF1-score of 86.9 whereas ours scores 88.9 (P <0.01), in \nthe setting where we are using the full training set with no \ncomputational constraints. If we restrict the training set \nto 1000 reports and limit the number of epochs to 2, our \napproach now outperforms the standard approach by 3 points \nof F1-score (P <0.01). This underlines that our fine-tuning \napproach has an even bigger advantage when constrained \nand generally converges faster. When the task becomes more \ndifficult, such as the classification on CTs only, we observe \na gap of 6 points of F1-score (P <0.01); the harder the task \nand the stronger the constraints, the better our fine-tuning \napproach.\nFigure¬†4 provides another view in which we observe the \nvalidation loss for 500 trained transformers, with or with-\nout our fine-tuning approach. The yellow runs that leverage \nthese fine-tuning methods almost always achieve low valida-\ntion scores, use higher values of the learning rate, and reach \nthe lowest validation loss values of all runs, compared to the \nblue runs that follow a standard fine-tuning approach. Our \nfine-tuning approach provides faster and more stable train-\ning, which leads to the best performing models on the task.\nDiscussion\nPre‚ÄëTraining Results\nAs observed in the ‚ÄúExperimental Results‚Äù section, our pre-\ntrained model on radiology reports performs best on the \nCOVID-19 classification task among the other biomedical \npre-trainings: its superiority is due to the inclusion of radiol-\nogy reports in the pre-training dataset. In general, accord-\ning to the BLURB benchmark [8 ], the best pre-training on \nbiomedical tasks is PubMedBERT, as it is a from-scratch \npre-training with a biomedical vocabulary. The fact that \nRadBERT outperforms PubMedBERT on the X-rays and \nX-rays+CTs tasks, though RadBERT is a continuous per-\ntraining with general vocabulary, shows the importance of \nunderstanding well the structure of radiology reports on \nradiology-related tasks. We notice that PubMedBERT out-\nperforms RadBERT on the CT-only task: the pre-training \ndataset of RadBERT contains mostly X-rays, and the small \n171Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nnumber of CTs seen is probably not large enough to yield \nan advantage over PubMedBERT. This shows the potential \nof from-scratch pre-training with radiology vocabulary and \npre-trained on radiology reports: such a model would get the \nbest of both worlds and probably dominate on radiology-\nrelated tasks.\nIn addition, we notice that BERT-base achieves surpris-\ningly high scores on the X-ray-only task: unlike RadBERT \nor PubMedBERT, it is capable of leveraging the weights of \nan additional linear layer in the classification head to make \nup for its limited pre-training knowledge and improve medi-\nally by 1 point of F1-score. Nevertheless, this is not enough \nFig. 4  Five hundred transformers using our pre-training on radiol-\nogy reports and fine-tuned for the COVID-19 classification task. \nThe yellow points, using our fine-tuning approach, perform better \nthan the blue points in the vast majority of cases, using a standard \nfine-tuning procedure. This visualization was obtained using the \nWeights & Biases platform [39]\nFig. 5  The red, yellow, and blue lines reflect the preponderance of \nCOVID-19 as detected in radiology reports, all from the same health \nacademic system, by our model. The green line represents the num-\nber of positive cases in the same county (data from the CDC COVID \nTracker [40])\n172 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\nto handle CTs, which have much more medical content, and \nits performance on this task is much lower compared to Rad-\nBERT and PubMedBERT.\nFine‚ÄëTuning Approach\nOut of the four unfreezing scenarios presented in the \n‚ÄúUnfreezing Scenarios ‚Äù section, the best unfreezing strat -\negy consists of training the classification head for one \nepoch (along the pooler layer of the BERT model) and \nthen unfreezing the rest of the transformer for the remain-\ning epochs. Compared to always training the full model, \nthe absolute best score is slightly higher with this method, \nalthough by less than 1 point of F1-score. The major differ-\nence is in training stability: the median of all the trainings \nfollowing our method is 5 points of F1-score higher com-\npared to the naive approach. Including the pooler layer in \nthe first training epoch increases the median slightly, by 1 \npoint of F1-score.\nComparing the other fine-tuning techniques, such as the \ntriangular learning rate with final decay and the discrimi-\nnative learning rate, we notice that all these methods help \nthe training in three ways: stability, absolute performance, \nand speed of convergence. When running many models with \nthese methods, we measure an improvement of 30 points \nof median F1-score across runs, compared to the standard \napproach. This improvement is mainly because the stand-\nard setting is much more sensitive to the value of the fixed \nlearning rate, whereas the triangular scheduling relies on a \nrange of values and can temporally reach higher values of the \nlearning rate, without preventing it from converging. With \nthis stability comes higher absolute performance, which \nachieves 2 additional points of F1-score when training on \nthe full dataset, with no computational constraint and evalu-\nating on both X-rays and CTs (see Table¬† 4). Finally, under \na limit of 2 training epochs, this set of methods achieves 3 \nmore points of F1-score compared to the standard approach, \nas it allows for higher values of the learning rate (at least \ntemporarily). Our best model uses learning rates up to 6e-05 \ncompared to the traditional 2e-05 value recommended for \nBERT models, dividing by more than 2 the number of \nepochs required, compared to CheXbert.\nThe stability of the performance is a strong advantage, \nin particular in settings where the number of trainings is \nlimited. Standard strategies can reach acceptable results too, \nthough this remains less frequent and not as accurate as the \nbest results of our approach. The fact that these methods \nallow for higher values of the learning rate is very beneficial \nfor faster convergence, and may explain the superiority in \nsettings where the data is limited, which happens frequently \nin the medical domain. We experimented the same approach \non CheXpert dataset labeled with 14 lung diseases [38] and \nobserved similar improvements compared to the standard \nfine-tuning approach. Providing a more exhaustive study of \nfine-tuning methods for BERT models, evaluated on a set of \ndiverse tasks, using various optimization algorithms, could \nbe helpful for the field and the subject of future work.\nFig. 6  For each report and model output, integrated gradients underline in green the words that contributed positively to the decision of the \nmodel and in red the ones that contributed negatively\n173Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nWe found that with triangular scheduling, the variation of \nthe learning rate must be controlled: when there is a smaller \nnumber of batches, the gap between the extreme values of \nthe learning rate must be reduced to keep the training sta-\nble. The use of an additional linear layer in the classification \nhead was in most cases useless if not counterproductive, los-\ning 1 point of F1-score. Only when relying on BERT-base (a \npre-training without biomedical knowledge) was this setting \nhelpful. Finally, using a continuous hyperparameter space was \nvery helpful during the hyperparameter optimization phase, \ncompared to a discrete space.\nAside from the metrics provided in the ‚ÄúExperimental \nResults‚Äù section, we can assess the performance of our \nmodel by running it on a large database of clinical reports to \nvisualize the prevalence of disease over time. If we compare \nthe COVID-19 presence in reports on Fig.¬† 5 (red line) with \nthe data from the viral tests of the same county (green line), \nboth superimpose well. The difference in absolute values is \ndue to the difference of scales, but probably also because \nCOVID-19 presence in radiology reports is not directly pro-\nportional to the number of positive tests. Running similar \nmodels on medical reports from populations can be useful \nTable 1  Our best model Test set COVID-19 Uncertain COVID-19 No COVID-19 Macro-average\nF1 (R., P.) F1 (R., P.) F1 (R., P.) F1 (R., P.)\nBest model, known academic health system\nX-rays 89.1 (90.7, 87.5) 87.1 (88.1, 86.1) 95.3 (94.3, 96.2) 90.5 (91.0, 90.0)\nCTs 83.9 (86.9, 81.1) 61.7 (65.9, 58.0) 92.5 (88.5, 96.9) 79.4 (80.4, 78.7)\nX-rays and CTs 87.6 (89.7, 85.7) 84.2 (85.6, 82.8) 94.9 (93.5, 96.3) 88.9 (89.6, 88.3)\nBest model with threshold, known academic health system\nX-rays and CTs 91.9 (93.7, 90.2) 89.9 (89.9, 89.9) 97.3 (96.7, 97.8) 93.0 (93.4, 92.6)\nBaseline, known academic health system\nX-rays and CTs 68.6 (75.3, 63.0) 65.8 (68.8, 63.0) 83.5 (79.3, 88.2) 72.6 (74.5, 71.4)\nBest model, new academic health system\nX-rays 64.9 (87.8, 51.4) 53.3 (39.0, 84.5) 69.7 (87.6, 57.9) 62.6 (71.5, 64.6)\nTable 2  Pretraining strategies Pretraining COVID-19 Uncertain COVID-19 No COVID-19 Macro-average\nF1 (R., P.) F1 (R., P.) F1 (R., P.) F1 (R., P.)\nX-rays\nBERT 88.6 (88.0, 89.2) 86.6 (89.2, 84.3) 95.1 (94.1, 96.2) 90.1 (90.4, 89.9)\nBlueBERT 87.1 (87.5, 86.7) 86.9 (87.5, 86.3) 95.6 (95.2, 96.0) 89.9 (90.1, 89.7)\nBioBERT 87.8 (91.7, 84.3) 87.2 (88.9, 85.6) 95.2 (93.2, 97.2) 90.1 (91.3, 89.0)\nPubMedBERT 86.9 (89.4, 84.6) 85.4 (86.1, 84.7) 94.4 (93.3, 95.5) 88.9 (89.6, 88.3)\nRadBERT (ours) 89.1 (90.7, 87.5) 87.1 (88.1, 86.1) 95.3 (94.3, 96.2) 90.5 (91.0, 90.0)\nCTs\nBERT 81.1 (84.5, 78.0) 59.8 (65.9, 54.7) 91.6 (86.3, 97.6) 77.5 (78.9, 76.8)\nBlueBERT 82.5 (78.6, 86.8) 54.4 (63.6, 47.5) 90.8 (88.5, 93.2) 75.9 (76.9, 75.8)\nBioBERT 79.0 (78.6, 79.5) 57.1 (68.2, 49.2) 91.6 (86.3, 97.6) 75.9 (77.7, 75.4)\nPubMedBERT 86.7 (85.7, 87.8) 64.1 (75.0, 55.9) 92.1 (87.8, 96.8) 81.0 (82.8, 80.2)\nRadBERT (ours) 83.9 (86.9, 81.1) 61.7 (65.9, 58.0) 92.5 (88.5, 96.9) 79.4 (80.4, 78.7)\nX-rays and CTs\nBERT 86.4 (87.0, 85.9) 83.5 (86.6, 80.6) 94.6 (92.9, 96.4) 88.2 (88.9, 87.6)\nBlueBERT 85.9 (85.0, 86.7) 82.9 (84.9, 80.9) 94.9 (94.2, 95.6) 87.9 (88.0, 87.8)\nBioBERT 85.4 (88.0, 83.0) 83.4 (86.6, 80.5) 94.7 (92.2, 97.2) 87.8 (88.9, 86.9)\nPubMedBERT 86.9 (88.3, 85.5) 82.8 (84.9, 80.7) 94.1 (92.5, 95.6) 87.9 (88.6, 87.3)\nRadBERT (ours) 87.6 (89.7, 85.7) 84.2 (85.6, 82.8) 94.9 (93.5, 96.3) 88.9 (89.6, 88.3)\n174 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\nto gain insights on the propagation of diseases and their \nseasonal patterns. Other visualizations can also help com-\npare the performance of different pre-trained and fine-tuned \nclassifiers (see Supplementary Notes on transformer hidden-\nstates visualization and Supplementary Fig.¬†5).\nTo compensate for the lack of interpretability of deep-\nlearning models like BERT and to help providers in their \ndecision process, we provide reports with post hoc expla-\nnations as computed by integrated gradients . As seen on \nFig.¬†6, the presence of observations like ‚Äúbilateral airspace \nopacities‚Äù or ‚Äúmultifocal pneumonia‚Äù are considered as good \nindicators of COVID-19, whereas ‚Äústable‚Äù lowers the con-\nfidence of the model in this decision (see Supplementary \nNotes on error analysis).\nTable 3  Fine-tuning datasets Dataset COVID-19 Uncertain COVID-19 No COVID-19 Macro-average\nF1 (R., P.) F1 (R., P.) F1 (R., P.) F1 (R., P.)\nX-rays\nOnly X-rays 85.9 (81.5, 90.7) 83.6 (87.2, 80.3) 94.1 (93.6, 94.6) 87.9 (87.4, 88.6)\nOnly CTs 51.9 (37.0, 87.0) 23.9 (18.3, 34.4) 83.1 (98.0, 72.1) 53.0 (51.1, 64.5)\nBoth X-rays and CTs 85.3 (88.4, 82.3) 82.7 (86.1, 79.5) 93.7 (91.0, 96.5) 87.2 (88.5, 86.1)\nCTs\nOnly X-rays 66.3 (67.9, 64.8) 32.9 (27.3, 41.4) 83.7 (87.1, 80.7) 61.0 (60.7, 62.3)\nOnly CTs 86.3 (82.1, 90.8) 63.6 (77.3, 54.0) 92.9 (89.2, 96.9) 80.9 (82.9, 80.5)\nBoth X-rays and CTs 71.7 (73.8, 69.7) 47.7 (59.1, 40.0) 84.9 (77.0, 94.7) 68.1 (70.0, 68.1)\nX-rays and CTs\nOnly X-rays 80.1 (77.7, 82.6) 79.1 (80.7, 77.6) 92.5 (92.6, 92.4) 83.9 (83.7, 84.2)\nOnly CTs 63.7 (49.7, 88.7) 30.3 (24.8, 39.2) 84.3 (96.7, 74.7) 59.4 (57.1, 67.5)\nBoth X-rays and CTs 81.5 (84.3, 78.8) 78.2 (83.2, 73.8) 92.5 (88.9, 96.2) 84.1 (85.5, 83.0)\nTable 4  Fine-tuning strategies Strategy COVID-19 Uncertain COVID-19 No COVID-19 Macro-average\nF1 (R., P.) F1 (R., P.) F1 (R., P.) F1 (R., P.)\nX-rays\nConstrained\n¬† ¬† Standard 82.5 (88.4, 77.3) 71.8 (61.1, 87.0) 91.9 (96.2, 87.9) 82.1 (81.9, 84.1)\n¬† ¬† Ours 84.3 (89.8, 79.5) 80.1 (82.8, 77.6) 93.0 (90.0, 96.2) 85.8 (87.5, 84.4)\nFull\n¬† ¬† Standard 86.6 (83.8, 89.6) 85.9 (87.2, 84.6) 95.0 (95.2, 94.8) 89.2 (88.7, 89.7)\n¬† ¬† Ours 89.1 (90.7, 87.5) 87.1 (88.1, 86.1) 95.3 (94.3, 96.2) 90.5 (91.0, 90.0)\nCTs\nConstrained\n¬† ¬† Standard 76.6 (85.7, 69.2) 31.2 (22.7, 50.0) 87.9 (89.2, 86.7) 65.3 (65.9, 68.6)\n¬† ¬† Ours 74.9 (83.3, 68.0) 42.4 (40.9, 43.9) 83.2 (78.4, 88.6) 66.8 (67.6, 66.8)\nFull\n¬† ¬† Standard 80.9 (85.7, 76.6) 51.5 (59.1, 45.6) 88.6 (81.3, 97.4) 73.7 (75.4, 73.2)\n¬† ¬† Ours 83.9 (86.9, 81.1) 61.7 (65.9, 58.0) 92.5 (88.5, 96.9) 79.4 (80.4, 78.7)\nX-rays and CTs\nConstrained\n¬† ¬† Standard 80.8 (87.7, 74.9) 67.9 (56.9, 84.2) 91.3 (95.2, 87.8) 80.0 (79.9, 82.3)\n¬† ¬† Ours 81.6 (88.0, 76.1) 76.2 (78.2, 74.4) 91.6 (88.3, 95.1) 83.1 (84.8, 81.9)\nFull\n¬† ¬† Standard 84.9 (84.3, 85.5) 81.7 (84.2, 79.4) 94.1 (93.2, 95.2) 86.9 (87.2, 86.7)\n¬† ¬† Ours 87.6 (89.7, 85.7) 84.2 (85.6, 82.8) 94.9 (93.5, 96.3) 88.9 (89.6, 88.3)\n175Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\nConclusion\nWe have developed a COVID-19 document-level classifier on \nradiology reports, along with RadBERT, its continuous pre-\ntraining on radiology reports. First, we propose an in-domain \nvocabulary and structured pre-training for any radiology-related \ndownstream task, and show its superiority over other biomedi-\ncal pre-trainings on the specific COVID-19 classification task. \nSecond, we develop a set of fine-tuning and hyperparameter \noptimization methods leading to more stable results, faster \nconvergence, and better absolute performance, especially \nunder data and computational constraints. Third, using these \nstrategies, we further fine-tune RadBERT and achieve 88.9 of \nmacro-averaged F1-score on the COVID-19 document-level \nclassification task, on both X-rays and CTs. Fourth, we rein-\nforce fine-tuned RadBERT to resist distribution shifts using a \nmulti-institutional dataset and evaluating it in a new institution.\nWe hope that our COVID-19 classifier can offer intelli-\ngent assistance to radiologists and providers, as well as help \nmonitor the spread and the evolution of the disease within \nthe clinical setting. Our model could also serve as a weak \nlabeler for computer vision models to detect COVID-19 on \nX-rays and CT scans. We believe that RadBERT can help \nimprove the performance on all radiology-related down-\nstream tasks, such as report generation, summarization, \nand classification. Finally, we aim for our fine-tuning and \nhyperparameter optimization approach to be reused to cre-\nate successful classifiers for other lung diseases, even in the \npresence of a small amount of labeled data. 1\nIn the future, we will be gathering a much larger dataset of \nunlabeled radiology reports across multiple institutions and \nexperiment with from-scratch pre-training approaches. Given \nthe known superiority of from-scratch approaches in other \nsettings and the potential of radiology pre-training, we believe \nthis could further boost all text-based transformer models on \nradiology-related tasks.2\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s10278- 022- 00714-8.\nAcknowledgements We would like to acknowledge Stephanie Bogdan \nfrom Stanford University for her help in managing the project, as well as \nJin Long from Stanford University too, for his assistance in building the \nstatistical foundations of this work. We would also like to acknowledge \nNouha Manai from CentraleSupelec and Paris-Saclay University for \nher help in proofreading and correcting this manuscript. This research \nis part of MIDRC (The Medical Imaging Data Resource Center) and \nwas made possible by the National Institute of Biomedical Imaging and \nBioengineering (NIBIB) of the National Institutes of Health under con-\ntracts 75N92020C00008 and 75N92020C00021. The content is solely \nthe responsibility of the authors and does not necessarily represent the \nofficial views of the National Institutes of Health.\nAuthor Contribution Guarantors of integrity of entire study, P.J.C., \nT.S.C., C.P.L.; study concepts/study design or data acquisition or data \nanalysis/interpretation, all authors; manuscript drafting or manuscript \nrevision for important intellectual content, all authors; approval of final \nversion of submitted manuscript, all authors; agrees to ensure any ques-\ntions related to the work are appropriately resolved, all authors; litera-\nture research, P.J.C.; experimental studies, P.J.C.; statistical analysis, \nP.J.C.; and manuscript editing, all authors.\nFunding This research was made possible by the National Insti-\ntute of Biomedical Imaging and Bioengineering (NIBIB) of the \nNational Institutes of Health under contracts 75N92020C00008 and \n75N92020C00021. The content is solely the responsibility of the \nauthors and does not necessarily represent the official views of the \nNational Institutes of Health.\nData Availability The data of the results are available in the tables \nattached to this study. The raw data used for model training, develop-\nment, and testing cannot unfortunately be made available due to privacy \nconcerns. De-identification and specific data-share agreements might \nbe possible for research purposes upon request to the authors.\nCode Availability The code will be made available in a GitHub reposi-\ntory upon publication. The model weights of both RadBERT and its \nfine-tuned version for COVID-19 classification are available on Hug-\nging Face.\nDeclarations \nEthics Approval and Consent This study involves human subjects and \nwas approved by Stanford University and University of Pennsylvania \nIRBs. The IRBs determined that no consent was necessary.\nCompeting Interests Personal financial interests: Board of directors \nand shareholder, Bunkerhill Health; Option holder, whiterabbit.ai; \nAdvisor and option holder, GalileoCDS; Advisor and option holder, \nSirona Medical; Advisor and option holder, Adra; Advisor and option \nholder, Kheiron; Advisor, Sixth Street; Chair, SIIM Board of Directors; \nMember at Large, Board of Directors of the Pennsylvania Radiologi-\ncal Society; Member at Large, Board of Directors of the Philadelphia \nRoentgen Ray Society; Member at Large, Board of Directors of the \nAssociation of University Radiologists (term just ended in June); \nHonoraria, Sectra (webinars); Honoraria, British Journal of Radiol-\nogy (section editor); Speaker honorarium, Icahn School of Medicine \n(conference speaker); Speaker honorarium, MGH (conference speaker). \nRecent grant and gift support paid to academic institutions involved: \nCarestream; Clairity; GE Healthcare; Google Cloud; IBM; IDEXX; \nHospital Israelita Albert Einstein; Kheiron; Lambda; Lunit; Microsoft; \nNightingale Open Science; Nines; Philips; Subtle Medical; VinBrain; \nWhiterabbit.ai; Lowenstein Foundation; Gordon and Betty Moore \nFoundation; Paustenbach Fund. Grant funding: NIH; Independence \nBlue Cross; RSNA.\nReferences\n 1. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, \nLlion Jones, Aidan¬†N. Gomez, Lukasz Kaiser, and Illia Polo -\nsukhin. Attention is all you need, 2017.\n 2. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina \nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding, 2019.\n 3. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, \nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi \n1 https:// huggi ngface. co/ Stanf ordAI MI/ RadBE RT\n2 https:// huggi ngface. co/ Stanf ordAI MI/ covid- radbe rt\n176 Journal of Digital Imaging (2023) 36:164‚Äì177\n1 3\nLouf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von \nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven \nLe¬†Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and \nAlexander Rush. Transformers: State-of-the-art natural language \nprocessing. In Proceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing: System Demonstra -\ntions, pages 38‚Äì45, Online, October 2020. Association for Com-\nputational Linguistics. 10.18653/v1/2020.emnlp-demos.6. https:// \naclan tholo gy. org/ 2020. emnlp- demos.6.\n 4. Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, \nDi¬†Jindi, Tristan Naumann, and Matthew McDermott. Publicly \navailable clinical BERT embeddings. In Proceedings of the 2nd \nClinical Natural Language Processing Workshop, pages 72‚Äì78, \nMinneapolis, Minnesota, USA, June 2019. Association for Compu-\ntational Linguistics. 10.18653/v1/W19-1909. https:// aclan tholo gy. \norg/ W19- 1909.\n 5. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, \nSunkyu Kim, Chan¬†Ho So, and Jaewoo Kang. Biobert: a pre-\ntrained biomedical language representation model for biomedical \ntext mining. Bioinformatics, Sep 2019. ISSN 1460-2059. 10.1093/\nbioinformatics/btz682. http:// dx. doi. org/ 10. 1093/ bioin forma tics/ \nbtz682.\n 6. Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in \nbiomedical natural language processing: An evaluation of bert and \nelmo on ten benchmarking datasets, 2019.\n 7. Iz¬†Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language \nmodel for scientific text. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 3615‚Äì3620, Hong Kong, China, November 2019. \nAssociation for Computational Linguistics. 10.18653/v1/D19-1371. \nhttps:// aclan tholo gy. org/ D19- 1371.\n 8. Yu¬†Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, \nXiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. \nDomain-specific language model pretraining for biomedical natural \nlanguage processing. ACM Transactions on Computing for Health-\ncare, 3 (1):1‚Äì23, Jan 2022. ISSN 2637-8051. 10.1145/3458754. \nhttp:// dx. doi. org/ 10. 1145/ 34587 54.\n 9. Kamal¬†raj Kanakarajan, Bhuvana Kundumani, and Malaikannan \nSankarasubbu. BioELECTRA:pretrained biomedical text encoder \nusing discriminators. In Proceedings of the 20th Workshop on Bio-\nmedical Language Processing, pages 143‚Äì154, Online, June 2021. \nAssociation for Computational Linguistics. 10.18653/v1/2021.\nbionlp-1.16. https:// aclan tholo gy. org/ 2021. bionlp- 1. 16.\n 10. An¬†Yan, Julian McAuley, Xing Lu, Jiang Du, Eric¬†Y. Chang, \nAmilcare Gentili, and Chun-Nan Hsu. Radbert: Adapting trans -\nformer-based language models to radiology. Radiology: Artificial \nIntelligence, 4(4):e210258, 2022. 10.1148/ryai.210258. https://  \ndoi. org/ 10. 1148/ ryai. 210258.\n 11. Asma Ben¬†Abacha, Yassine Mrabet, Yuhao Zhang, Chaitanya \nShivade, Curtis Langlotz, and Dina Demner-Fushman. Overview \nof the MEDIQA 2021 shared task on summarization in the medi-\ncal domain. In Proceedings of the 20th Workshop on Biomedical \nLanguage Processing, pages 74‚Äì85, Online, June 2021. Associa-\ntion for Computational Linguistics. 10.18653/v1/2021.bionlp-1.8. \nhttps:// aclan tholo gy. org/ 2021. bionlp- 1.8.\n 12. Diwakar Mahajan, Ching-Huei Tsou, and Jennifer¬†J Liang. IBM-\nResearch at MEDIQA 2021: Toward improving factual correct-\nness of radiology report abstractive summarization. In Proceed-\nings of the 20th Workshop on Biomedical Language Processing, \npages 302‚Äì310, Online, June 2021. Association for Computational \nLinguistics. 10.18653/v1/2021.bionlp-1.35. https:// aclan tholo gy. \norg/ 2021. bionlp- 1. 35.\n 13. Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Gen-\nerating radiology reports via memory-driven transformer, 2020.\n 14. Yasuhide Miura, Yuhao Zhang, Emily¬†Bao Tsai, Curtis¬†P. Langlotz, \nand Dan Jurafsky. Improving factual completeness and consistency \nof image-to-text radiology report generation, 2021.\n 15. Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven¬† QH \nTruong, Du¬†Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao \nZhang, Matthew¬†P. Lungren, Andrew¬†Y. Ng, Curtis¬†P. Langlotz, \nand Pranav Rajpurkar. Radgraph: Extracting clinical entities and \nrelations from radiology reports, 2021.\n 16. Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, \nAndrew¬†Y. Ng, and Matthew¬†P. Lungren. Chexbert: Combining \nautomatic labelers and expert annotations for accurate radiology \nreport labeling using bert, 2020.\n 17. Pilar L√≥pez-√öbeda, Manuel¬†Carlos D√≠az-Galiano, Teodoro Mart√≠n-\nNoguerol, Antonio Luna, L.¬†Alfonso Ure√±a-L√≥pez, and M.¬†Teresa \nMart√≠n-Valdivia. Covid-19 detection in radiological text reports \nintegrating entity recognition. Computers in Biology and Medicine, \n127:104066, 2020. ISSN 0010-4825. https://doi.org/10.1016/j.\ncompbiomed.2020.104066. https:// www. scien cedir ect. com/ \n scien ce/ artic le/ pii/ S0010 48252 03039 78.\n 18. Jeremy Howard and Sebastian Ruder. Universal language model \nfine-tuning for text classification, 2018.\n 19. Huangxing Lin, Weihong Zeng, Xinghao Ding, Yue Huang, \nChenxi Huang, and John Paisley. Learning rate dropout, 2019.\n 20. Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How to \nfine-tune bert for text classification?, 2020.\n 21. David¬†A. Wood, Jeremy Lynch, Sina Kafiabadi, Emily Guilhem, \nAisha¬†Al Busaidi, Antanas Montvila, Thomas Varsavsky, Juveria \nSiddiqui, Naveen Gadapa, Matthew Townend, Martin Kiik, Keena \nPatel, Gareth Barker, Sebastian Ourselin, James¬†H. Cole, and \nThomas¬†C. Booth. Automated labelling using an attention model \nfor radiology reports of mri scans (alarm), 2020.\n 22. James Bergstra, R√©mi Bardenet, Yoshua Bengio, and Bal√°zs K√©gl. \nAlgorithms for hyper-parameter optimization. In J.¬†Shawe-Taylor, \nR.¬†Zemel, P.¬†Bartlett, F.¬†Pereira, and K.¬†Q. Weinberger, editors, \nAdvances in Neural Information Processing Systems, volume¬†24. \nCurran Associates, Inc., 2011. https:// proce edings. neuri ps. cc/  \npaper/ 2011/ file/ 86e8f 7ab32 cfd12 577bc 2619b c6356 90- Paper. pdf.\n 23. Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and \nefficient hyperparameter optimization at scale, 2018.\n 24. Peter¬†I. Frazier. A tutorial on bayesian optimization, 2018.\n 25. Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech¬†M. \nCzarnecki, Jeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, \nIain Dunning, Karen Simonyan, Chrisantha Fernando, and Koray \nKavukcuoglu. Population based training of neural networks, \n2017.\n 26. Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina \nGonina, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A \nsystem for massively parallel hyperparameter tuning, 2020.\n 27. CCRB. Sample size calculator: One-sample proportion, 2022. \nhttps:// www2. ccrb. cuhk. edu. hk/ stat/ propo rtion/ OSp_ sup. htm# top.\n 28. Seokyung Hahn. Understanding noninferiority trials, 2012. https:// \ndoi. org/ 10. 3345/ kjp. 2012. 55. 11. 403.\n 29. Sebastian Raschka. Model evaluation, model selection, and algorithm \nselection in machine learning, 2020.\n 30. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime¬†G. Carbonell, \nRuslan Salakhutdinov, and Quoc¬†V. Le. Xlnet: Generalized \nautoregressive pretraining for language understanding. CoRR, \nabs/1906.08237, 2019. http:// arxiv. org/ abs/ 1906. 08237.\n 31. Kevin Clark, Minh-Thang Luong, Quoc¬†V. Le, and Christopher¬†D. \nManning. ELECTRA: pre-training text encoders as discriminators \nrather than generators. CoRR, abs/2003.10555, 2020. https:// arxiv. \norg/ abs/ 2003. 10555.\n 32. Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep \nlearning. Information, 11(2):108, Feb 2020. ISSN 2078-2489. \n10.3390/info11020108. http:// dx. doi. org/ 10. 3390/ info1 10201 08.\n177Journal of Digital Imaging (2023) 36:164‚Äì177 \n1 3\n 33. Leslie¬†N. Smith. A disciplined approach to neural network hyper-\nparameters: Part 1 ‚Äì learning rate, batch size, momentum, and \nweight decay, 2018.\n 34. Leslie¬†N. Smith and Nicholay Topin. Super-convergence: Very \nfast training of neural networks using large learning rates, 2018.\n 35. Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, \nJoseph¬†E Gonzalez, and Ion Stoica. Tune: A research platform \nfor distributed model selection and training. arXiv preprint arXiv: \n1807. 05118, 2018.\n 36. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic \nattribution for deep networks, 2017.\n 37. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and \nThomas Dandres. Quantifying the carbon emissions of machine \nlearning. arXiv preprint arXiv: 1910. 09700, 2019.\n 38. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana \nCiurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, \nRobyn Ball, Katie Shpanskaya, Jayne Seekins, David¬†A. Mong, \nSafwan¬†S. Halabi, Jesse¬†K. Sandberg, Ricky Jones, David¬†B. \nLarson, Curtis¬†P. Langlotz, Bhavik¬†N. Patel, Matthew¬†P. Lungren, \nand Andrew¬†Y. Ng. Chexpert: A large chest radiograph dataset \nwith uncertainty labels and expert comparison, 2019.\n 39. Lukas Biewald. Experiment tracking with weights and biases, \n2020. https:// www. wandb. com/. Software available from wandb.\ncom.\n 40. CDC. Cdc covid data tracker, 2022. https:// covid. cdc. gov/ covid- \ndata- track er/# datat racker- home\nPublisher‚Äôs Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds \nexclusive rights to this article under a publishing agreement with the \nauthor(s) or other rightsholder(s); author self-archiving of the accepted \nmanuscript version of this article is solely governed by the terms of \nsuch publishing agreement and applicable law.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7625724077224731
    },
    {
      "name": "Hyperparameter",
      "score": 0.713702917098999
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6702002882957458
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.5858707427978516
    },
    {
      "name": "Machine learning",
      "score": 0.5335173010826111
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5036141276359558
    },
    {
      "name": "Visualization",
      "score": 0.47861140966415405
    },
    {
      "name": "Fine-tuning",
      "score": 0.41508036851882935
    },
    {
      "name": "Medical physics",
      "score": 0.33942103385925293
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3206064701080322
    },
    {
      "name": "Medicine",
      "score": 0.1371762752532959
    },
    {
      "name": "Pathology",
      "score": 0.11728045344352722
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I277688954",
      "name": "Universit√© Paris-Saclay",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I922845939",
      "name": "Philadelphia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ]
}