{
  "title": "Prompting is not a substitute for probability measurements in large language models",
  "url": "https://openalex.org/W4389518756",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2161984462",
      "name": "Jennifer Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982032445",
      "name": "Roger Lévy",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences",
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4367859810",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2782213998",
    "https://openalex.org/W4376632417",
    "https://openalex.org/W4393385514",
    "https://openalex.org/W4389784425",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4319453517",
    "https://openalex.org/W4306842234",
    "https://openalex.org/W4401123261",
    "https://openalex.org/W4300772090",
    "https://openalex.org/W3167919460",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4322716192",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3037115370",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4290994954",
    "https://openalex.org/W4307311424",
    "https://openalex.org/W4367701273",
    "https://openalex.org/W4287118015",
    "https://openalex.org/W4309419393",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W3199526956",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4322006551",
    "https://openalex.org/W3092323704",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4287891033",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W4309419356",
    "https://openalex.org/W4317465744",
    "https://openalex.org/W3200434522",
    "https://openalex.org/W4310820245",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W4387075507",
    "https://openalex.org/W2950641397",
    "https://openalex.org/W1599880985",
    "https://openalex.org/W4283026156"
  ],
  "abstract": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5040–5060\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPrompting is not a substitute for probability measurements\nin large language models\nJennifer Hu\nKempner Institute\nHarvard University\njenniferhu@fas.harvard.edu\nRoger Levy\nDepartment of Brain and Cognitive Sciences\nMassachusetts Institute of Technology\nrplevy@mit.edu\nAbstract\nPrompting is now a dominant method for eval-\nuating the linguistic knowledge of large lan-\nguage models (LLMs). While other methods\ndirectly read out models’ probability distribu-\ntions over strings, prompting requires models to\naccess this internal information by processing\nlinguistic input, thereby implicitly testing a new\ntype of emergent ability: metalinguistic judg-\nment. In this study, we compare metalinguis-\ntic prompting and direct probability measure-\nments as ways of measuring models’ linguistic\nknowledge. Broadly, we find that LLMs’ met-\nalinguistic judgments are inferior to quantities\ndirectly derived from representations. Further-\nmore, consistency gets worse as the prompt\nquery diverges from direct measurements of\nnext-word probabilities. Our findings suggest\nthat negative results relying on metalinguistic\nprompts cannot be taken as conclusive evidence\nthat an LLM lacks a particular linguistic gener-\nalization. Our results also highlight the value\nthat is lost with the move to closed APIs where\naccess to probability distributions is limited.\n1 Introduction\nFew technologies have been as exciting — and di-\nvisive — for language science as large language\nmodels (LLMs). LLMs are capable of incredibly\nsophisticated linguistic behaviors, which emerge\nthrough statistical learning with massive amounts\nof text data and highly expressive, domain-agnostic\nlearning architectures. On the one hand, the suc-\ncess of these models has sparked a growing move-\nment to treat them as candidate models of human\nlanguage acquisition and processing (e.g., Baroni,\n2022; Warstadt and Bowman, 2022; Wilcox et al.,\n2022; Contreras Kallens et al., 2023) – indeed,\nPiantadosi (2023) even claims that they “refute”\nChomsky’s approach to language. On the other\nhand, linguists have highlighted shortcomings of\nCode and data are available at https://github.com/jennhu/\nmetalinguistic-prompting.\ncurrent models that make them unsuitable as cog-\nnitive theories (e.g., Dupre, 2021; Lan et al., 2022;\nKatzir, 2023; Milway, 2023; Murphy, 2023).\nNo matter their theoretical position, researchers\nneed a way to assess the capabilities of LLMs in\norder to substantiate such claims. The fundamen-\ntal unit of LLM computation is P(token|context),\nwhich, in principle, can be directly read out from a\nmodel by accessing its output layer of vocabulary\nlogits. The distribution that this implies over word\nstrings reflects the model’s linguistic generaliza-\ntions: that is, a generative model of the language\nseen during training, which can be used to evaluate\nthe likelihood of previously unseen strings. Direct\nmeasurements of model-derived string probabilities\nhave revealed capabilities such as syntactic gener-\nalizations (e.g., Linzen et al., 2016; Futrell et al.,\n2019; Hu et al., 2020; Warstadt et al., 2020), seman-\ntic plausibility judgments (Kauf et al., 2022), and\ncertain coherence inferences (Beyer et al., 2021).\nRecently, there has been a growing trend to use\nprompting to evaluate LLMs’ capabilities. Prompt-\ning (popularized by Brown et al., 2020) enables\nend-to-end interaction with models through natural\nlanguage, and can be done entirely through infer-\nence (i.e., without gradient updates). This method\nhas revealed new classes of emergent abilities in\nLLMs, such as arithmetic, instruction-following,\nand grounded conceptual mappings (Brown et al.,\n2020; Wei et al., 2022a; Patel and Pavlick, 2022), as\nwell as the ability to render sentence acceptability\njudgments (Dentella et al., 2023). From a sociolog-\nical angle, prompting has also made LLM evalua-\ntions more accessible for domain experts in linguis-\ntics and cognitive science, sparking new discussion\nabout the limitations of LLMs’ abilities (Katzir,\n2023; Dentella et al., 2023; Ullman, 2023). For ex-\nample, Dentella et al. (2023) prompt GPT-3 to pro-\nduce grammaticality judgments of infrequent lin-\nguistic constructions, and conclude that the model\n“show[s] a critical lack of understanding even of\n5040\nhigh-frequency words” (p. 1). Similarly, Katzir\n(2023) argues that “LLMs are poor theories of hu-\nman linguistic cognition” (p. 2) based on prompting\nChatGPT to compare the well-formedness of two\nEnglish sentences.\nAs these examples illustrate, researchers have\nbeen making substantial theoretical claims based\non prompting. However, there is an important\ncaveat to using prompts to evaluate models’ lin-\nguistic knowledge. Prompt-based methods test not\nonly whether a model represents the generalization\nof interest (e.g., a certain ordering of probabilities),\nbut also whether the model can report the outcome\nof applying the generalization to the sentence in\nthe prompt. In this way, prompting implicitly tests\na new type of emergent ability — metalinguistic\njudgment — which has not yet been systematically\nexplored as a way to evaluate model capabilities.\nTo demonstrate the difference between di-\nrect probability measurements and metalinguistic\nprompting, consider the case of English subject-\nverb agreement. A direct approach might compare\nthe probability assigned by the model to singular\nand plural verbs, given a particular subject noun\nphrase (e.g., Linzen et al., 2016). For example, we\nmight compare P(“is”) and P(“are”), conditioned\non the prefix given by (1-a). In contrast, a prompt-\ning approach might present a sentence prefix, and\npose a question about this linguistic content. For\nexample, we might compare P(“is”) and P(“are”),\nconditioned on the prompt in (1-b).\n(1) a. The keys to the cabinet\nb. Here is a sentence: The keys to the\ncabinet... What word is most likely to\ncome next?\nA model that perfectly performs the metalinguistic\ntask posed in prompt (1-b) should have identical\nprobability distributions over the next word given\n(1-a) and (1-b). However, there is no guarantee\nthat a model’s response to a metalinguistic prompt\nwill match its underlying internal representations.\nIn light of this, how should we interpret models’\nresponses to metalinguistic prompts? How do these\nresponses correspond to models’ internal represen-\ntations? And when should we use metalinguistic\nprompts as opposed to direct measurements? These\nquestions are becoming increasingly important, as\nprompting plays a growing role in the debate about\nLLMs as models of human language processing.\nIn this study, we do not intend to take a stance\non this theoretical debate. Rather, our goal is to\nevaluate the validity of metalinguistic prompting\nas a way of measuring LLMs’ internal knowledge.\nWe pose two research questions: (1) How well do\nmodels perform under direct and metalinguistic\nevaluation methods? and (2) How consistent are\nthe metalinguistic methods with the direct method?\nWe investigate these questions through four exper-\niments, covering a range of tasks and linguistic\ndomains. Our findings (and their supporting fig-\nures) are summarized below:\n1. The metalinguistic judgments elicited from\nLLMs through prompting are not the same as\nquantities directly derived from model repre-\nsentations. (Figures 2 and 3)\n2. Direct probability measurements generally\nyield better or similar task performance, com-\npared to metalinguistic prompting. (Figure 2)\n3. Minimal pairs help reveal models’ general-\nization capacities, compared to isolated judg-\nments. (Figure 2c vs. Figure 2d)\n4. In general, the less similar the task/prompt\nis to a direct probability measurement, the\nworse the alignment between metalinguistic\nand direct measurements. (Figure 3)\nTaken together, our findings suggest that nega-\ntive results relying on metalinguistic prompts can-\nnot be taken as conclusive evidence that an LLM\nlacks a particular linguistic generalization. These\nfindings suggest a possible basis for acompetence–\nperformance distinction in LLMs: namely, the\ndistinction between the information encoded in a\nmodel’s isolated-sentence string probability distri-\nbution versus the model’s behavioral responses to\nprompts. We discuss this topic in greater detail in\nSection 6.1.\nOur results also highlight the value that is lost\nas researchers move toward interacting with LLMs\nthrough closed APIs, where access to models’ un-\nderlying probability distributions is limited. Indeed,\nonly two days before acceptance of this paper to\nEMNLP, the ability to obtain arbitrary token log-\nprobabilities from gpt-3.5-turbo-instruct was\nremoved from the OpenAI API, reinforcing the\ntimeliness of the issue. We urge future research\nto clearly motivate the evaluation methods used to\nassess LLM abilities, and highlight the importance\nof developing and using open-source models with\naccess to internal probabilities.\n5041\nExperiment Targeted ability Task Dataset(s)\n1 (Section 4.1) Word prediction Predict final word in a sentence Pereira et al. (2018); news arti-\ncles from March 2023\n2 (Section 4.2) Semantic plausibility Determine which word (of two options)\nis most likely, given preceding context\nVassallo et al. (2018)\n3a (Section 4.3) Syntax Determine which sentence (of two op-\ntions) is “better”, in isolation\nSyntaxGym (Hu et al., 2020);\nBLiMP (Warstadt et al., 2020)\n3b (Section 4.4) Syntax Determine which sentence (of two op-\ntions) is “better”, given both options\nSyntaxGym (Hu et al., 2020);\nBLiMP (Warstadt et al., 2020)\nTable 1: Overview of experiments in our study.\n2 Related work\nOur study is related to model calibration, or the\nproblem of estimating predictive uncertainty (e.g.,\nGuo et al., 2017; Minderer et al., 2021; Kadavath\net al., 2022; Mielke et al., 2022). Most relevant to\nour work, Kadavath et al. (2022) perform a broad-\nscale evaluation of “honesty” in LMs. In particu-\nlar, they analyze models’ “truthfulness” and “self-\nknowledge”, which are conceptually similar to our\nground-truth and internal consistency evaluations.\nSimilarly, Mielke et al. (2022) find that chatbots’\nexpressions of confidence and doubt are poorly\ncalibrated with the likelihood that the models’ re-\nsponses are actually correct. These approaches dif-\nfer from ours in that they analyze models’ metacog-\nnition by annotating verbalized expressions (e.g., “I\ndon’t know, but...”), or by using models to evaluate\nthe correctness of their own generated answers.\nOther studies have also demonstrated that mod-\nels can respond to prompts in unexpected ways\n(Khashabi et al., 2022; Min et al., 2022; Webson\nand Pavlick, 2022; Webson et al., 2023; Prasad\net al., 2023), in some cases achieving successful\noutcomes even when prompts are misleading or\nincoherent. Similarly, Turpin et al. (2023) find\nthat explanations elicited through chain-of-thought\nprompting (Nye et al., 2021; Wei et al., 2022b) can\nsystematically misrepresent the underlying reasons\nand causes for a model’s prediction. McCoy et al.\n(2023) also demonstrate how LLMs’ responses to\nprompts are highly sensitive to word probabilities,\ndue to their original training objectives.\nBeguš et al. (2023) also evaluate LLMs’ “met-\nalinguistic” abilities, but in a different sense of the\nword. They investigate whether LLMs can perform\ntheoretical analyses of the structures and regulari-\nties of linguistic expressions — for example, draw-\ning the syntactic tree diagram of a given sentence\nin valid LATEX code. Beguš et al.’s tested abilities\nare metalinguistic in the sense that they require\njudgments about linguistic objects given linguistic\ninput. In contrast, our study investigates whether\nmodels can access and report their internal proba-\nbility distributions through linguistic prompts.\n3 General methods\n3.1 Overview of tasks\nOur experiments feature four tasks, summarized\nin Table 1. Together, the tasks cover both word-\nand sentence-level computations, as well as both\nisolated judgments and minimal-pair comparisons.\nSince we aim to analyze the validity of using met-\nalinguistic prompts to reveal linguistic knowledge,\nthe experiments also cover semantic plausibility\nand syntax as linguistic domains of interest.\nAs mentioned in Section 1, word prediction\n(Expt. 1) is a fundamental task for LLMs, and in-\nvolves the most straightforward operation on mod-\nels’ representations: reading out the logits over the\nvocabulary in the output layer. Because we know\nthat models represent next-token-probabilities, we\ncan treat the direct probability measurements as\nproviding ground-truth values. We therefore treat\nword prediction as a baseline task, where we\ncan perform a tightly controlled comparison be-\ntween probability measurements and metalinguistic\nprompting. The other three tasks induce an intu-\nitive (partial) ordering in terms of their similarity\nto the baseline word prediction task (word compari-\nson, sentence judgment, sentence comparison). We\nreturn to this ordering in Section 5.2.\n3.2 Overview of prompts\nIn each experiment, we evaluate models using four\nmethods: a direct method, and three zero-shot met-\nalinguistic prompting methods. The direct method\ninvolves computing probabilities of tokens or full\nsentences based on the models’ internal logits over\nvocabulary items. In contrast, the metalinguistic\n5042\nDirect\nA butterfly is a flying\ninsect with four large\nvocab distribution\nMetalinguistic\nHere is a sentence: A\nbutterfly is a flying insect\nwith four large... What\nword is most likely to\ncome next?\nvocab distribution\n=\n?\n(a)\nDirect\nS1 = Every child\nhas studied. P(S1)\nS2 = Every child\nhave studied. P(S2)\nP(S1) >\nP(S2)?\nMetalinguistic\nHere are two English\nsentences: 1) Every\nchild has studied. 2)\nEvery child have studied.\nWhich sentence is a\nbetter English sentence?\nRespond with either 1 or\n2 as your answer.\nvocab distribution\nP(“1”) > P(“2”)?\n(b)\nFigure 1: Conceptual illustration of direct probability\nmeasurements vs. metalinguistic judgments.(a) Ba-\nsic word prediction task (Experiment 1, Section 4.1). (b)\nSentence comparison task (Experiment 3b, Section 4.4).\nprompts ask a question or specify a task requiring\na judgment about a linguistic expression.\nTaking direct probability measurements as our\nbaseline evaluation method, we also identify an or-\ndering of the three metalinguistic methods in terms\nof their similarity to baseline. In the MetaQues-\ntionSimple and MetaQuestionComplex prompts,\nthe linguistic object of interest (e.g., a sentence\nprefix) is linearly closest and farthest from the po-\nsition where the model is asked to make a predic-\ntion, respectively. The MetaInstruct prompts are\nstructured as an imperative instruction, and fall in\nbetween the two MetaQuestion* prompts. Tables 2-\n4b show example prompts for all experiments.1\n3.3 Models\nWe test six models across all of our experiments:\nthree Flan-T5 models (small, large, XL; Chung\n1To test the generalizability of our findings beyond English,\nwe also conducted preliminary experiments in Mandarin Chi-\nnese. Details and results can be found in Appendix D.\net al., 2022),2 and three GPT-3/3.5 models (text-\ncurie-001/GPT-3, text-davinci-002/GPT-3.5, text-\ndavinci-003/GPT-3.5). Flan-T5 models were ac-\ncessed through Huggingface, and GPT-3/3.5 mod-\nels were accessed through the OpenAI API.\nThe Flan-T5 models have an encoder-decoder\narchitecture and were pre-trained on a span cor-\nruption task before fine-tuning on a large collec-\ntion of instruction-based tasks. To measure next-\nword probabilities under these models, we take the\nsentence prefix ⟨w1, w2, . . . , wn−1⟩(where wn is\nthe word to be predicted) and append the sentinel\ntoken <extra_id_0>. We then create the output\nsequence <extra_id_0> wn, and sum the prob-\nabilities corresponding to the tokens of wn. To\nmeasure full-sentence probabilities, we compute\na pseudo-likelihood inspired by Salazar et al.’s\n(2020) method for scoring sentences under masked\nlanguage models. Going left to right, we mask out\neach word in the sentence using the T5 sentinel\ntokens, and then sum the log probabilities assigned\nto each true word.\nWhile the Flan-T5 models differ only in size, the\nOpenAI models also differ in training regime: text-\ncurie-001 is an autoregressive language model at\nits core (GPT-3; Brown et al., 2020), whereas text-\ndavinci-002 has additional supervised fine-tuning,\nand text-davinci-003 has additional reinforcement\nlearning (Ouyang et al., 2022).3\n4 Details of experiments\n4.1 Experiment 1: Word prediction\nTask and stimuli. The aim of this experiment is\nto evaluate models’ ability to predict the next word\ngiven a preceding context. Instead of a standard\nlanguage modeling task, where models are evalu-\nated on their ability to predict every word in a text,\nwe use the simplified task of predicting the final\nword of a sentence. This makes the metalinguis-\ntic evaluations more tractable, as we only need to\nconstruct a single prompt for each item.\nWe use two datasets with contrasting style.\nOur first dataset, taken from Pereira et al. (2018)\n(“P18”), consists of 384 simple declarative sen-\ntences that state a fact about familiar concepts, such\n2Parameter counts: small 80M, large 780M, XL 3B.\n3Many LLM evaluations use ChatGPT (Piantadosi, 2023;\nKatzir, 2023) or GPT-4 (Beguš et al., 2023; Webb et al., 2023;\nMoskvichev et al., 2023). However, we exclude these models\nfrom our analyses because the OpenAI API does not provide\naccess to token probabilities for chat-based models.\n5043\nType of prompt Example\nDirect A butterfly is a flying insect with four large wings\nMetaQuestionSimple What word is most likely to come next in the following sentence? A butterfly is a flying insect\nwith four large wings\nMetaInstruct You are a helpful writing assistant. Tell me what word is most likely to come next in the following\nsentence: A butterfly is a flying insect with four large wings\nMetaQuestionComplex Here is the beginning of an English sentence: A butterfly is a flying insect with four large... What\nis the best next word? Answer: wings\nTable 2: Example prompts for Experiment 1. Region where we measure probability is marked in boldface. Ground-\ntruth sentence continuations are shown in blue.\nType of prompt Example\nDirect The archer released the { arrow, interview}\nMetaQuestionSimple What word is most likely to come next in the following sentence (arrow, or interview)? The\narcher released the {arrow, interview}\nMetaInstruct You are a helpful writing assistant. Tell me what word is most likely to come next in the following\nsentence (arrow, or interview?): The archer released the {arrow, interview}\nMetaQuestionComplex Here is the beginning of an English sentence: The archer released the... What word is more\nlikely to come next: arrow, or interview? Answer: {arrow, interview}\nTable 3: Example prompts for Experiment 2. Region where we measure probability is marked in boldface.\nSemantically plausible continuations are shown in blue; implausible in red.\nas accordion or butterfly. All pronouns are derefer-\nenced, making the dataset useful for testing predic-\ntion in simple contexts with minimal dependencies.\nThere are at least two concerns with the P18\nsentences. First, their simple structure might not\nbe representative of the text that models encounter\nduring training. Second, they have been publicly\navailable since 2018, making it possible that they\nmay be in the models’ training data. To address\nthese concerns, we constructed a second dataset\n(“News”) containing sentences that are more natu-\nralistic, but highly unlikely to occur in the training\ndata. We used the NewsData tool4 to find English\nnews articles published in the United States in the\ndate range of March 20-26, 2023. 5 The articles\ncover a span of topics, such as business, politics,\nand food. For each article, we construct a prefix by\nconcatenating the headline with the first sentence\n(up to, but not including, the last word), separated\nby the string “ – ”. There are 222 items in total.\nPrompts. Example prompts are shown in Table 2\n(only examples from the P18 corpus are shown, for\nsimplicity). The Direct prompt feeds the sentence\nprefix to the model, and we measure the model’s\nprobability of the ground-truth next word (indicated\n4https://newsdata.io/\n5These are unlikely to be in the Flan-T5 training data,\nas the models were publicly released in 2022. The OpenAI\nmodel documentation also states that text-curie-001 only re-\nceived training data up to October 2019, and the text-davinci-*\nmodels only received training data up to June 2021.\nin blue boldface). The other prompts are designed\nto elicit metalinguistic judgments, through ques-\ntions (MetaQuestionSimple, MetaQuestionCom-\nplex) and instructions (MetaInstruct). As a concep-\ntual illustration, Figure 1a shows a comparison of\nthe Direct and MetaQuestionComplex methods.\nEvaluation. Our measure of task performance is\nthe log probability assigned by each model to the\nground-truth sentence continuation. To measure\ninternal consistency (see Section 5.2), we analyzed\nthe relationship between log probabilities assigned\nto ground-truth continuations, as measured by the\ndirect method and each metalinguistic method.\n4.2 Experiment 2: Word comparison\nTask and stimuli. The aim of this experiment is\nto evaluate models’ ability to judge which of two\nwords is a more likely continuation of a sentence.\nWhile Experiment 1 tested word prediction with-\nout focus on any particular linguistic phenomenon,\nhere we use the word-comparison task to assess\nknowledge of semantic plausibility.\nWe use a set of 395 minimal sentence pairs from\nVassallo et al. (2018). Each pair consists of two\nsentences that differ only in the final word, which\nalters the plausibility of the described event (e.g.,\n“The archer released the arrow/interview”). Each\nsentence has a simple syntactic structure.\nPrompts. The prompts are similar to those from\nExperiment 1, but here we ask models to make a\n5044\nType of prompt Example\nDirect { Every child has studied, Every child have studied}\nMetaQuestionSimple Is the following sentence a good sentence of English? Every child has studied. Respond with\neither Yes or No as your answer. {Yes, No}\nMetaInstruct You are a helpful writing assistant. Tell me if the following sentence is a good sentence of\nEnglish. Every child has studied. Respond with either Yes or No as your answer. {Yes, No}\nMetaQuestionComplex Here is a sentence: Every child has studied. Is the sentence a good sentence of English? Respond\nwith either Yes or No as your answer. Answer: {Yes, No}\n(a)\nType of prompt Example\nDirect { Every child has studied, Every child have studied}\nMetaQuestionSimple Which sentence is a better English sentence? 1) Every child has studied. 2) Every child have\nstudied. Respond with either 1 or 2 as your answer. {1, 2}\nMetaInstruct You are a helpful writing assistant. Tell me which sentence is a better English sentence. 1) Every\nchild has studied. 2) Every child have studied. Respond with either 1 or 2 as your answer. { 1, 2}\nMetaQuestionComplex Here are two English sentences: 1) Every child have studied. 2) Every child has studied. Which\nsentence is a better English sentence? Respond with either 1 or 2 as your answer. Answer: {1, 2}\n(b)\nTable 4: Example prompts for Experiments 3a (a) and 3b (b). Region where we measure probability is marked in\nboldface. Grammatical sentences and correct answer options are shown in blue; ungrammatical/incorrect in red.\ncomparison between two potential continuations\nof the sentence prefix (Table 3). For the Direct\nmethod, we present the model with the shared sen-\ntence prefix, and compare the probability of the\nplausible (e.g., “arrow”; indicated in blue) and im-\nplausible (e.g., “interview”; indicated in red) con-\ntinuations. For the Meta* prompts, we create two\nversions of the prompt by shuffling the order in\nwhich the answer options are presented.\nEvaluation. Accuracy is the proportion of items\nwhere the model assigns higher probability to the\nplausible sentence continuation than to the implau-\nsible continuation. Random performance is 50%.\nTo measure internal consistency, for each evalu-\nation method we computed the item-level log prob-\nability differentials between the plausible and im-\nplausible sentence continuations. We then com-\nputed the correlation between the differentials\nelicited by the direct method and the differentials\nelicited by each metalinguistic method.\n4.3 Experiment 3a: Sentence judgment\nTask and stimuli. The aim of this experiment\nis to evaluate models’ ability to judge whether a\nsentence is a “good” sentence of English. For any\nparticular sentence, we compare the model’s judg-\nment of that sentence to the model’s judgment of\nanother sentence that forms a minimal pair, only\ndiffering in a critical syntactic feature that manipu-\nlates grammaticality or acceptability.\nWe use minimal pairs from two datasets de-\nsigned to test knowledge of English syntax: Syn-\ntaxGym (Hu et al., 2020; Gauthier et al., 2020),\nand the Benchmark of Linguistic Minimal Pairs\n(BLiMP; Warstadt et al., 2020). Since SyntaxGym\nwas not designed for full-sentence probability com-\nparisons, we first converted the SyntaxGym mate-\nrials into sentence-level minimal pairs.6 We then\ntook a random sample of 15 items from each of\nthe 23 remaining suites, resulting in 345 items. For\nBLiMP, we extracted the items that were compat-\nible with the “simple LM method” and then took\na random sample of 30 items from each of the 13\ncategories, resulting in 390 items. See Appendix A\nfor details on the tested phenomena.\nPrompts. For the Direct method, we measure the\nprobability of each sentence in the minimal pair.\nFor the Meta* prompts, we construct a separate\nprompt for each sentence in the minimal pair asking\nwhether the sentence is “a good sentence of English”\n(see Table 4a), and then compare the probability\nassigned by the model to “Yes” versus “No”.\nEvaluation. To measure accuracy for the direct\ncondition, we compute the proportion of items\nwhere the model assigns higher probability to the\ngrammatical sentence in the minimal pair. For\n6We created full sentences by combining content across\nregions, and then turned each success criterion inequality into\na minimal pair (see Hu et al., 2020, for details). For simplic-\nity, we omitted test suites with success criteria involving the\nconjunction of ≥ 3 inequalities, or probability differentials.\n5045\nthe metalinguistic prompts, we report balanced ac-\ncuracy, or the mean of the true positive rate and\ntrue negative rate. A true positive occurs when the\nmodel assigns higher probability to “Yes” than “No”\nfor a grammatical sentence, and a true negative oc-\ncurs when the model assigns higher probability to\n“No” than “Yes” for an ungrammatical sentence.\nTo measure internal consistency, we compare the\nlog probability differentials for each method. For\nthe direct method, the differential is the difference\nin log probability of the grammatical and ungram-\nmatical sentences. For each metalinguistic method,\nthe differential is the difference in log probability\nof the “Yes” token conditioned on the grammatical\nand ungrammatical sentence prompts.\n4.4 Experiment 3b: Sentence comparison\nTask and stimuli. As in Experiment 3a (Sec-\ntion 4.3), the goal is to measure models’ syntactic\njudgments. However, instead of presenting the\nmodel with sentences in isolation and asking for\njudgments, in Experiment 3b we present the model\nwith the minimal pair of sentences, and probe\nwhich sentence it takes to be a “better” sentence of\nEnglish. For our stimuli, we use the same subsets\nof SyntaxGym and BLiMP as in Experiment 3a.\nPrompts. The direct evaluation method is the\nsame as in Experiment 3a: we compare probabili-\nties of each sentence in the minimal pair. For the\nmetalinguistic prompts, we have a single prompt\nfor each minimal pair that presents both sentences\nat once. We assign an identifier (1 or 2) to each\nsentence in the pair, present a multiple-choice ques-\ntion comparing both sentences, and compare the\nprobabilities assigned by the model to each answer\noption (i.e., “1” or “2”). As in Experiment 2, we av-\nerage model results over two versions of the prompt\nthat counterbalance the order of answer options\n(for metalinguistic prompts). Example prompts are\nshown in Table 4b. As a conceptual illustration,\nFigure 1b shows a comparison of the Direct and\nMetaQuestionComplex methods.\nEvaluation. Accuracy is measured as the pro-\nportion of items where the model assigns higher\nprobability to the grammatical sentence in the min-\nimal pair (direct method), or to the answer option\ncorresponding to the grammatical sentence (met-\nalinguistic prompts). Random performance is 50%.\nTo measure internal consistency, we compare the\nlog probability differentials between the grammati-\ncal and ungrammatical sentences (measured by the\ndirect method) to the log probability differentials\nbetween the answer options corresponding to the\ngrammatical and ungrammatical sentences (mea-\nsured by each metalinguistic prompting method).\n5 Results\nWe now return to our main research questions, laid\nout in the Introduction: (1) How well does each\nevaluation method perform on each task? (2) How\nconsistent are the metalinguistic evaluation meth-\nods with the direct evaluation method? We address\nthese in Sections 5.1 and 5.2, respectively.\n5.1 Task performance\nResult #1: Metalinguistic judgments arenot the\nsame as direct measurements. Figure 2 shows\ntask performance for each experiment. At the coars-\nest level, the different methods (hues) yield differ-\nent performance scores, demonstrating that met-\nalinguistic and direct responses are not identical.\nResult #2: Direct measurements generally per-\nform ≥metalinguistic methods. Aross all ex-\nperiments, the direct method nearly always yields\nbest performance of all tested methods. There are\na few exceptions: in Experiment 1, Flan-T5-SM\nperforms best under MetaInstruct, and Flan-T5-XL\nperforms relatively well under MetaQuestionCom-\nplex, as do the davinci models in Experiment 2.\nResult #3: Minimal pairs help reveal models’\ngeneralization capacities. The difference be-\ntween Experiments 3a and 3b lies in the presen-\ntation of minimal pairs. Comparing Figures 2c\nand 2d, we first note that the direct results (darkest\nbars) are identical by definition: they reflect com-\nparisons of full-sentence probabilities. For the met-\nalinguistic prompts, there is an increase in accuracy\ngoing from the isolated sentence judgments (Fig-\nure 2c) to minimal-pair comparisons (Figure 2d),\nfor all models with above-chance performance.\n5.2 Internal consistency\nResult #4: Consistency gets worse as we get\nfurther from direct measurement of next-word\nprobabilities. Figure 3 illustrates alignment be-\ntween direct and metalinguistic measurements (see\nAppendix Figure 6 for by-model internal consis-\ntency results). Each cell shows the average corre-\nlation coefficient (Pearson’s r) between the item-\nlevel differentials measured by the direct method\n5046\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nSyntaxGym\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\nBLiMP\nDirect MetaQuestionSimple MetaInstruct MetaQuestionComplex\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n10.0\n7.5\n5.0\n2.5\n0.0\nLogprob/uni00A0of/uni00A0final/uni00A0word\n(a) Experiment 1: Word prediction\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (b) Experiment 2: Word comparison (Semantic plausibility)\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n(c) Experiment 3a: Sentence judgment (Syntax)\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (d) Experiment 3b: Sentence comparison (Syntax)\nFigure 2: Task performance: Direct probability measurements generally outperform metalinguistic prompts.\n(a) Log probability assigned to ground-truth sentence continuation, averaged over items and datasets. (b) Proportion\nof items where model prefers semantically plausible continuation over implausible continuation. (c)-(d) Proportion\nof items where model prefers grammatical sentence over ungrammatical sentence in minimal pair, averaged over\ndatasets. Error bars denote bootstrapped 95% CIs. Dashed lines indicate random baseline.\nand a particular metalinguistic prompting method.7\nRecall from Section 3 that the tasks and prompts\nin our study induce intuitive orderings in terms of\nhow similar they are to word prediction (at the task-\nlevel) and direct probability measurements (at the\nprompt-level). The columns of Figure 3 (prompt\ntypes) are loosely ordered by similarity to direct\nprobability measurements, and the rows (tasks) are\nloosely ordered by similarity to word prediction.\nBroadly speaking, we find that as distance in either\ndimension increases, the correlations get weaker.\nThese results further support Result #1: while di-\nrect and metalinguistic responses are highly corre-\nlated for some combinations of tasks and prompts,\nthe relationship is far from perfect.\n6 Discussion\nIn this study, we compared metalinguistic prompt-\ning and direct measurements as ways of evaluating\n7For Experiment 1, the correlation is computed between\nthe item-level probabilities of the ground-truth final word.\nLLMs’ linguistic knowledge. Broadly, we find that\nmetalinguistic judgments are inferior to direct mea-\nsurements of token- or sentence-level probabilities.\nWe also find evidence that minimal-pair compar-\nisons help reveal models’ generalization capacities.\nWe do not intend to claim that prompting should\nbe categorically dispreferred in favor of other eval-\nuation methods. Prompting is useful for eliciting\nopen-ended responses, such as chain-of-thought\nreasoning. Metalinguistic prompting could also be\nused to ask questions that would be challenging to\ntranslate into direct probability measurements (e.g.,\n“Which of the following two sentences is more se-\nmantically plausible, but less syntactically well-\nformed?”). In addition, prompting lowers the tech-\nnical barrier for domain experts to conduct LLM\nevaluations, which could contribute to the develop-\nment of more robust behavioral benchmarks.\nWhat do our findings mean for researchers in-\nterested in the linguistic abilities of LLMs? While\nthere are valid reasons to prefer both prompting\n5047\nMeta\nQuestion\nSimple\nMeta\nInstruct\nMeta\nQuestion\nComplex\nWord/uni00A0prediction\n(Exp/uni00A01)\nWord/uni00A0comparison\n(Exp/uni00A02)\nSentence/uni00A0judgment\n(Exp/uni00A03a)\nSentence/uni00A0comparison\n(Exp/uni00A03b)\n0.79 0.78 0.63\n0.5 0.5 0.38\n0.2 0.2 0.2\n0.23 0.24 0.22\nMore/uni00A0distant/uni00A0task\nMore/uni00A0distant/uni00A0prompt\n1.0\n0.5\n0.0\n0.5\n1.0\nFigure 3: Internal consistency: Correlation between\nmetalinguistic and direct responses gets weaker as\nprompts become less direct.Pearson r correlation be-\ntween response magnitudes (averaged over models and\ndatasets) measured by direct prompts versus each met-\nalinguistic prompt. See Appendix C for more details.\nand direct probability measurements, these meth-\nods will not necessarily generate consistent results.\nMore specifically, our findings suggest that nega-\ntive results relying on metalinguistic prompts can-\nnot be taken as conclusive.\nWhile our paper focuses on the value of direct\nprobability measurements for evaluating linguistic\ngeneralizations, other endeavors in cognitive sci-\nence and machine learning also rely on access to\nLLM probabilities. For example, token probabili-\nties enable multiple-choice evaluation and beam\nsearch, as well as investigating mode collapse\n(Janus, 2022) and obtaining quantities for Bayesian\ninference (e.g., Choi et al., 2022; Li et al., 2023;\nLipkin et al., 2023). 8 Thus, the value that is lost\nas we move toward closed APIs extends beyond\nlinguistic analysis, further underscoring the impor-\ntance of using and developing LLMs with open\naccess to internal probabilities.\n6.1 Competence vs. performance in LLMs\nHumans use language in diverse contexts with vary-\ning task demands and constraints, but the underly-\ning linguistic knowledge is relatively stable. This\nis the competence–performance distinction (e.g.,\nYngve, 1960; Chomsky, 1965): an individual’s\nperformance in a particular context may not re-\nflect an individual’s full underlying competence.\nWhether it is productive to distinguish between\n8These examples were inspired by discussion by Tan\nZhi Xuan on Twitter (https://twitter.com/xuanalogue/status/\n1637302507389984769).\n“competence” and “performance” in an AI model\nhas been a topic of debate. Katzir (2023) argues\nthat “[LLMs’] behavior directly reflects their com-\npetence, and when they fail it is their competence\nthat is at fault” (pp. 4-5). We take Katzir to mean\nthat an LLM’s next-word probability distribution\nis deterministic given its architecture, weights, and\nthe preceding context, and this probability distri-\nbution is always computed when the LLM is used.\nTherefore, as Katzir says, while humans may re-\ncover from linguistic errors (e.g., in agreement or\nparsing) given additional time or resources, “fur-\nther time and resources are of no use” to LLMs.\nFirestone (2020) and Lampinen (2023), in contrast,\nargue that performance conditions need to be care-\nfully controlled in both humans and machines in\norder to make fair comparisons between the two.9\nOur work suggests that if a competence–\nperformance distinction is to be made for LLMs, a\nnatural locus is the contrast between the informa-\ntion contained in an LLM’s string probability distri-\nbution (corresponding to its competence) versus the\nbehavior the LLM exhibits when prompted (corre-\nsponding to its performance). A model’s failure to\nexhibit a linguistic generalization when prompted\nmight not reflect a lack of the relevant information\nin its underlying conditional probability distribu-\ntions, but instead an inability to access and behave\nin accordance with that information in response to\na prompt that poses a metalinguistic query. This\nview remains consistent with the fact that LLM be-\nhavioral errors may be corrected when illustrative\nexamples are included in the prompt — whether or\nnot such prompts pose metalinguistic queries, they\noffer opportunity for in-context learning (Brown\net al., 2020) — or by allowing the model to produce\na reasoning trace before outputting an answer (Nye\net al., 2021; Wei et al., 2022b; Kojima et al., 2022).\nTo conclude, prompting is not a substitute for\ndirect probability measurements in LLMs. We un-\nderscore the importance of specifying the assump-\ntions underlying methodological choices in LLM\nevaluation, and using open models with direct ac-\ncess to probabilities for scientific research. If our\ninteractions with LLMs are limited to high-level\nprompting, we lose the opportunity to measure ca-\npabilities that could advance our understanding of\nthese models and their relation to human language.\n9Hahn et al. (2022), for example, show that imposing per-\nformance constraints on LLMs (by degrading context repre-\nsentations) derives human language processing behavior in\ncomplex nested dependency constructions.\n5048\nLimitations\nOur experiments only test three types of metalin-\nguistic prompts, and only perform zero-shot eval-\nuations. In practice, the small number of metalin-\nguistic prompt types was sufficient to illustrate the\ndifference between direct and metalinguistic re-\nsponses. However, it would be beneficial to con-\nsider more types of prompts to determine how\nwell the phenomenon generalizes. We also note\nthat models might achieve better task performance\nunder the metalinguistic prompts with few-shot\nprompting or in-context learning. We did not in-\nclude few-shot analyses due to space limitations,\nand because many recent LLM evaluations rely on\nzero-shot metalinguistic prompts. It remains to be\nseen whether metalinguistic and direct responses\nare better aligned when models have access to ex-\namples in the prompt.\nAnother limitation of our study is that we only\ntested a small class of models. An important direc-\ntion for future work would be to replicate our ex-\nperiments on models of different sizes and training\nobjectives (e.g., chat-based models). We also note\nthat the results from the OpenAI models are not\nnecessarily reproducible due to the models being\nbehind a closed API. Timestamps of our calls to the\nOpenAI API are available in our data files (https:\n//github.com/jennhu/metalinguistic-prompting).\nEthics Statement\nThis work does not release any new models or\ndatasets. Instead, the goal is to provide insights\ninto methodology for evaluating the internal knowl-\nedge of modern LLMs, and in turn contribute to\nthe interpretability of these models. We hope that\nour results illustrate the importance of open access\nto model representations and the risks of relying on\nhigh-level API interactions for scientific research.\nWith that said, the broader ethical concerns\nabout LLMs are still relevant to our work. LLMs\nhave been shown to produce output that is factually\nincorrect, offensive, or discriminatory, and should\ntherefore be used with extreme caution, especially\nin commercial applications or user-facing settings.\nAny demonstrations of LLMs’ linguistic general-\nizations should not imply that they are safe to use,\nor can be expected to behave in a way that is aligned\nwith human preferences and values.\nAcknowledgements\nWe thank Jon Gauthier and the anonymous re-\nviewers for feedback on earlier versions of this\nwork, as well as Peng Qian for translating the\nprompts into Chinese. J.H. gratefully acknowl-\nedges support from an NSF Graduate Research\nFellowship (#1745302), an NSF Doctoral Disserta-\ntion Research Improvement Grant (BCS-2116918),\nand the Simons Center for the Social Brain at MIT.\nR.P.L. gratefully acknowledges support from NIH\ngrant U01-NS121471, a Newton Brain Science\naward, and the Simons Center for the Social Brain\nat MIT.\nReferences\nMarco Baroni. 2022. On the proper role of\nlinguistically-oriented deep net analysis in linguis-\ntic theorizing. In Shalom Lappin and Jean-Philippe\nBernardy, editors, Algebraic Structures in Natural\nLanguage. Taylor & Francis.\nGašper Beguš, Maksymilian D ˛ abkowski, and Ryan\nRhodes. 2023. Large Linguistic Models: Analyz-\ning theoretical linguistic abilities of LLMs.\nAnne Beyer, Sharid Loáiciga, and David Schlangen.\n2021. Is Incoherence Surprising? Targeted Evalua-\ntion of Coherence Prediction from Language Models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4164–4173, Online. Association for Computa-\ntional Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nKristy Choi, Chris Cundy, Sanjari Srivastava, and Ste-\nfano Ermon. 2022. LMPriors: Pre-Trained Language\nModels as Task-Specific Priors.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax.\nMIT Press.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\n5049\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling Instruction-Finetuned Language Mod-\nels.\nPablo Contreras Kallens, Ross Deans Kristensen-\nMcLachlan, and Morten H. Christiansen. 2023.\nLarge Language Models Demonstrate the Potential of\nStatistical Learning in Language. Cognitive Science,\n47(3):e13256. Publisher: John Wiley & Sons, Ltd.\nVittoria Dentella, Elliot Murphy, Gary Marcus, and\nEvelina Leivada. 2023. Testing AI performance on\nless frequent aspects of language reveals insensitivity\nto underlying meaning.\nGabe Dupre. 2021. (What) Can Deep Learning Con-\ntribute to Theoretical Linguistics? Minds and Ma-\nchines, 31(4):617–635.\nChaz Firestone. 2020. Performance vs. competence\nin human–machine comparisons. Proceedings of\nthe National Academy of Sciences, 117(43):26562–\n26571. Publisher: Proceedings of the National\nAcademy of Sciences.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An Online Plat-\nform for Targeted Evaluation of Language Models.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 70–76, Online. Association\nfor Computational Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On Calibration of Modern Neural\nNetworks. In Proceedings of the 34th International\nConference on Machine Learning, volume 70, pages\n1321–1330.\nMichael Hahn, Richard Futrell, Roger P. Levy, and Ed-\nward Gibson. 2022. A resource-rational model of\nhuman processing of recursive linguistic structure.\n119(43).\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A Systematic Assessment of\nSyntactic Generalization in Neural Language Mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nJanus. 2022. Mysteries of mode collapse.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language Models (Mostly) Know\nWhat They Know.\nRoni Katzir. 2023. Why large language models are poor\ntheories of human linguistic cognition: A reply to\nPiantadosi (2023).\nCarina Kauf, Anna A. Ivanova, Giulia Rambelli, Em-\nmanuele Chersoni, Jingyuan S. She, Zawad Chowd-\nhury, Evelina Fedorenko, and Alessandro Lenci.\n2022. Event knowledge in large language models:\nthe gap between the impossible and the unlikely.\nDaniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. 2022. Prompt Waywardness:\nThe Curious Case of Discretized Interpretation of\nContinuous Prompts. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3631–3643, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid,\nYutaka Matsuo, and Yusuke Iwasawa. 2022. Large\nLanguage Models are Zero-Shot Reasoners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 35, pages 22199–22213. Curran Associates,\nInc.\nAndrew Kyle Lampinen. 2023. Can language models\nhandle recursively nested grammatical structures? A\ncase study on comparing models and humans.\nNur Lan, Emmanuel Chemla, and Roni Katzir. 2022.\nLarge Language Models and the Argument From the\nPoverty of the Stimulus.\nBelinda Z. Li, William Chen, Pratyusha Sharma, and\nJacob Andreas. 2023. LaMPP: Language Models as\nProbabilistic Priors for Perception and Action.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the Ability of LSTMs to Learn\nSyntax-Sensitive Dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\n5050\nBenjamin Lipkin, Lionel Wong, Gabriel Grand, and\nJoshua B. Tenenbaum. 2023. Evaluating statistical\nlanguage models as pragmatic reasoners. In Proceed-\nings of the Annual Meeting of the Cognitive Science\nSociety, volume 45.\nR. Thomas McCoy, Shunyu Yao, Dan Friedman,\nMatthew Hardy, and Thomas L. Griffiths. 2023. Em-\nbers of Autoregression: Understanding Large Lan-\nguage Models Through the Problem They are Trained\nto Solve.\nSabrina J. Mielke, Arthur Szlam, Emily Dinan, and\nY-Lan Boureau. 2022. Reducing Conversational\nAgents’ Overconfidence Through Linguistic Calibra-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:857–872.\nDaniel Milway. 2023. A Response to Piantadosi (2023).\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the Role of Demonstrations:\nWhat Makes In-Context Learning Work? In Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 11048–11064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMatthias Minderer, Josip Djolonga, Rob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin\nTran, and Mario Lucic. 2021. Revisiting the Cali-\nbration of Modern Neural Networks. In Advances in\nNeural Information Processing Systems, volume 34,\npages 15682–15694. Curran Associates, Inc.\nArseny Moskvichev, Victor Vikram Odouard, and\nMelanie Mitchell. 2023. The ConceptARC Bench-\nmark: Evaluating Understanding and Generalization\nin the ARC Domain.\nElliot Murphy. 2023. Notes on Large Language Models\nand Linguistic Theory.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2021. Show Your Work: Scratchpads for Intermedi-\nate Computation with Language Models.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nRoma Patel and Ellie Pavlick. 2022. Mapping Language\nModels to Grounded Conceptual Spaces. In Interna-\ntional Conference on Learning Representations.\nFrancisco Pereira, Bin Lou, Brianna Pritchett, Samuel\nRitter, Samuel J. Gershman, Nancy Kanwisher,\nMatthew Botvinick, and Evelina Fedorenko. 2018.\nToward a universal decoder of linguistic meaning\nfrom brain activation. Nature Communications ,\n9(1):963.\nSteven T. Piantadosi. 2023. Modern language models\nrefute Chomsky’s approach to language.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. GrIPS: Gradient-free, Edit-based In-\nstruction Search for Prompting Large Language Mod-\nels. In Proceedings of the 17th Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics, pages 3845–3864, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked Language Model Scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language Models Don’t\nAlways Say What They Think: Unfaithful Explana-\ntions in Chain-of-Thought Prompting.\nTomer Ullman. 2023. Large Language Models Fail on\nTrivial Alterations to Theory-of-Mind Tasks.\nPaolo Vassallo, Emmanuele Chersoni, Enrico Santus,\nAlessandro Lenci, and Philippe Blache. 2018. Event\nKnowledge in Sentence Processing: A New Dataset\nfor the Evaluation of Argument Typicality. In LREC\n2018 Workshop on Linguistic and Neurocognitive\nResources (LiNCR), Miyazaki, Japan.\nYiwen Wang, Jennifer Hu, Roger Levy, and Peng Qian.\n2021. Controlled Evaluation of Grammatical Knowl-\nedge in Mandarin Chinese Language Models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5604–\n5620, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAlex Warstadt and Samuel R. Bowman. 2022. What\nartificial neural networks can tell us about human\nlanguage acquisition. In Shalom Lappin and Jean-\nPhilippe Bernardy, editors, Algebraic Structures in\nNatural Language. Taylor & Francis.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The Benchmark of Linguis-\ntic Minimal Pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8. Pub-\nlisher: MIT Press.\nTaylor Webb, Keith J. Holyoak, and Hongjing Lu. 2023.\nEmergent Analogical Reasoning in Large Language\nModels.\n5051\nAlbert Webson, Alyssa Marie Loo, Qinan Yu, and Ellie\nPavlick. 2023. Are Language Models Worse than\nHumans at Following Prompts? It’s Complicated.\nAlbert Webson and Ellie Pavlick. 2022. Do Prompt-\nBased Models Really Understand the Meaning of\nTheir Prompts? In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 2300–2344, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent Abilities of Large Language Models. Transac-\ntions on Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022b. Chain of Thought Prompt-\ning Elicits Reasoning in Large Language Models. In\nAdvances in Neural Information Processing Systems.\nEthan Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian,\nand Roger P. Levy. 2022. Learning syntactic struc-\ntures from string input. In Shalom Lappin and Jean-\nPhilippe Bernardy, editors, Algebraic Structures in\nNatural Language. Taylor & Francis.\nVictor H. Yngve. 1960. A Model and an Hypothesis for\nLanguage Structure. Proceedings of the American\nPhilosophical Society, 104(5):444–466. Publisher:\nAmerican Philosophical Society.\nA Details of syntactic phenomena\nTable 5 summarizes the syntactic phenomena cov-\nered in the datasets used in Experiments 3a and\n3b, which were taken from SyntaxGym (Hu et al.,\n2020; Gauthier et al., 2020) and BLiMP (Warstadt\net al., 2020).\nB Dataset-level task performance\nFigure 4 shows task performance for each of the\ntested datasets in Experiment 1 (left: P18; right:\nNews). Figure 5 shows task performance for each\nof the tested datasets in Experiments 3a and 3b\n(left: SyntaxGym; right: BLiMP).\nC Relationship between metalinguistic\nand direct predictions\nFigure 6 shows the average Pearson r correlations\nbetween metalinguistic and direct responses, for\neach of the tested models.\nFigures 7 to 10 show the relationship between re-\nsponses measured by the direct and metalinguistic\nprompting methods, for Experiments 1-3b, respec-\ntively.\nD Experiments in Mandarin Chinese\nTo explore the robustness of our results beyond En-\nglish, we performed a preliminary investigation of\nGPT-3.5 (text-davinci-003) on materials in Man-\ndarin Chinese. We first consulted a native speaker\nto translate the metalinguistic prompts into Chinese.\nWe then tested GPT-3.5 on two datasets: (1) a set of\nrecent news articles for word prediction, mirroring\nExperiment 1; and (2) a set of controlled minimal\npairs that cover semantic and syntactic phenomena\n(Wang et al., 2021), mirroring a combination of\nthe phenomena tested in Experiments 2 and 3. We\ntested the tasks of Experiments 3a and 3b on this\nset of minimal pairs.\nFigure 11 shows the results (task performance)\nfor Experiments 1, 3a, and 3b in Chinese. Like\nour English experiments, in all our Chinese exper-\niments we find that metalinguistic prompting and\ndirect probability measurements do not yield the\nsame results. Like our Experiment 1 in English, we\nalso find that in the Chinese word prediction task,\nGPT-3.5 assigns highest probability to the ground-\ntruth continuation under the “Direct” method. We\nalso find that model performance improves when\nusing minimal pairs, mirroring our original findings\ncomparing Experiments 3a and 3b. These findings\ndemonstrate how our English results may general-\nize to languages with different syntactic structures\nand grammatical properties.\nHowever, we also found differences between\nthe Chinese and English results: in the Chinese\nversion of Experiment 3b, the “Direct” method un-\nderperformed the metalinguistic methods (accuracy\nscores: Direct 0.6; others 0.8). One potential ex-\nplanation for this is that according to the OpenAI\ndocumentation, the models are “optimized for use\nin English,” although in practice they may work\nwell for other languages.10 Therefore, the models\nmay not be well-suited to scoring probabilities of\nChinese sentences with no context (as in the Direct\ncondition); instead, they may benefit from seeing\nadditional Chinese text in the prompt before the\nsentence (as in the metalinguistic conditions).\n10https://help.openai.com/en/articles/6742369-how-do-i-\nuse-the-openai-api-in-different-languages\n5052\nDataset Phenomenon # items\nSyntaxGym Center embedding 15\nSyntaxGym Center embedding (modifier) 15\nSyntaxGym Cleft 15\nSyntaxGym Cleft (modifier) 15\nSyntaxGym Filler-gap dependencies (3 sentential embeddings) 15\nSyntaxGym Filler-gap dependencies (4 sentential embeddings) 15\nSyntaxGym Filler-gap dependencies (hierarchy) 15\nSyntaxGym Filler-gap dependencies (object extraction) 15\nSyntaxGym Filler-gap dependencies (prepositional phrase extraction) 15\nSyntaxGym Filler-gap dependencies (subject extraction) 15\nSyntaxGym Subject-verb number agreement (object relative clause) 15\nSyntaxGym Subject-verb number agreement (prepositional phrase) 15\nSyntaxGym Subject-verb number agreement (subject relative clause) 15\nSyntaxGym Reflexive number agreement (object relative clause, feminine) 15\nSyntaxGym Reflexive number agreement (object relative clause, masculine) 15\nSyntaxGym Reflexive number agreement (prepositional phrase, feminine) 15\nSyntaxGym Reflexive number agreement (prepositional phrase, masculine) 15\nSyntaxGym Reflexive number agreement (subject relative clause, feminine) 15\nSyntaxGym Reflexive number agreement (subject relative clause, masculine) 15\nSyntaxGym Subordination 15\nSyntaxGym Subordination (object relative clause) 15\nSyntaxGym Subordination (prepositional phrase) 15\nSyntaxGym Subordination (subject relative clause) 15\nBLiMP Anaphor agreement 30\nBLiMP Argument structure 30\nBLiMP Binding 30\nBLiMP Control raising 30\nBLiMP Determiner-noun agreement 30\nBLiMP Ellipsis 30\nBLiMP Filler-gap dependency 30\nBLiMP Irregular forms 30\nBLiMP Island effects 30\nBLiMP NPI licensing 30\nBLiMP Quantifiers 30\nBLiMP S-selection 30\nBLiMP Subject-verb agreement 30\nTable 5: Coverage of syntactic phenomena in stimuli used in Experiments 3a and 3b.\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\nLogprob/uni00A0of/uni00A0final/uni00A0word\nP18\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\nNews\nDirect MetaQuestionSimple MetaInstruct MetaQuestionComplex\nFigure 4: Task performance for each tested dataset in Experiment 1 (word prediction).\n5053\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nSyntaxGym\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\nBLiMP\nDirect MetaQuestionSimple MetaInstruct MetaQuestionComplex\n(a) Experiment 3a\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nSyntaxGym\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\nBLiMP\nDirect MetaQuestionSimple MetaInstruct MetaQuestionComplex\n(b) Experiment 3b\nFigure 5: Task performance for each tested dataset in Experiments 3a (top) and 3b (bottom).\n5054\nWord/uni00A0prediction\n(Exp/uni00A01)\nWord/uni00A0comparison\n(Exp/uni00A02)\nSentence/uni00A0judgment\n(Exp/uni00A03a)\nSentence/uni00A0comparison\n(Exp/uni00A03b)\n0.86 0.87 0.79\n0.75 0.78 0.52\n0.08 0.09 0.09\n/uni00AD0.32/uni00AD0.23/uni00AD0.26\nFlan/uni00ADT5/uni00A0SM\n0.59 0.56 0.58\n0.45 0.43 0.21\n0.24 0.27 0.25\n0.45 0.48 0.49\nFlan/uni00ADT5/uni00A0LG\n0.58 0.50 0.61\n0.60 0.62 0.44\n0.38 0.37 0.40\n0.44 0.45 0.44\nFlan/uni00ADT5/uni00A0XL\nMeta\nQuestion\nSimple\nMeta\nInstruct\nMeta\nQuestion\nComplex\nWord/uni00A0prediction\n(Exp/uni00A01)\nWord/uni00A0comparison\n(Exp/uni00A02)\nSentence/uni00A0judgment\n(Exp/uni00A03a)\nSentence/uni00A0comparison\n(Exp/uni00A03b)\n0.95 0.95 0.43\n0.40 0.38 0.06\n/uni00AD0.02/uni00AD0.080.04\n/uni00AD0.00/uni00AD0.06/uni00AD0.20\ntext/uni00ADcurie/uni00AD001\nMeta\nQuestion\nSimple\nMeta\nInstruct\nMeta\nQuestion\nComplex\n0.90 0.89 0.72\n0.43 0.44 0.55\n0.30 0.29 0.23\n0.41 0.41 0.42\ntext/uni00ADdavinci/uni00AD002\nMeta\nQuestion\nSimple\nMeta\nInstruct\nMeta\nQuestion\nComplex\n0.87 0.88 0.66\n0.39 0.34 0.50\n0.25 0.27 0.19\n0.42 0.39 0.43\ntext/uni00ADdavinci/uni00AD003\nFigure 6: Internal consistency: Correlation between metalinguistic and direct responses gets weaker as\nprompts become less direct.Pearson r correlation between response magnitudes (averaged over datasets) measured\nby direct prompts versus each metalinguistic prompt.\n5055\n30\n20\n10\n0\nMetaQuestionSimple\nr/uni00A0=/uni00A00.81\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.48\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.44\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n30\n20\n10\n0\nMetaInstruct\nr/uni00A0=/uni00A00.82\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.44\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.34\np/uni00A0</uni00A00.001\n20\n 0\nDirect\n30\n20\n10\n0\nMetaQuestionComplex\nr/uni00A0=/uni00A00.76\np/uni00A0</uni00A00.001\n20\n 0\nDirect\nr/uni00A0=/uni00A00.59\np/uni00A0</uni00A00.001\n20\n 0\nDirect\nr/uni00A0=/uni00A00.57\np/uni00A0</uni00A00.001\n(a) Flan-T5 / P18\n30\n20\n10\n0\nMetaQuestionSimple\nr/uni00A0=/uni00A00.91\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.71\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.72\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n30\n20\n10\n0\nMetaInstruct\nr/uni00A0=/uni00A00.92\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.67\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.66\np/uni00A0</uni00A00.001\n20\n 0\nDirect\n30\n20\n10\n0\nMetaQuestionComplex\nr/uni00A0=/uni00A00.81\np/uni00A0</uni00A00.001\n20\n 0\nDirect\nr/uni00A0=/uni00A00.58\np/uni00A0</uni00A00.001\n20\n 0\nDirect\nr/uni00A0=/uni00A00.65\np/uni00A0</uni00A00.001 (b) Flan-T5 / News\n20\n10\n0\nMetaQuestionSimple\nr/uni00A0=/uni00A00.94\np/uni00A0</uni00A00.001\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.88\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.85\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n20\n10\n0\nMetaInstruct\nr/uni00A0=/uni00A00.94\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.87\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.85\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\n20\n10\n0\nMetaQuestionComplex\nr/uni00A0=/uni00A00.51\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\nr/uni00A0=/uni00A00.84\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\nr/uni00A0=/uni00A00.80\np/uni00A0</uni00A00.001\n(c) GPT-3* / P18\n20\n10\n0\nMetaQuestionSimple\nr/uni00A0=/uni00A00.96\np/uni00A0</uni00A00.001\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.92\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.89\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n20\n10\n0\nMetaInstruct\nr/uni00A0=/uni00A00.97\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.91\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.91\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\n20\n10\n0\nMetaQuestionComplex\nr/uni00A0=/uni00A00.35\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\nr/uni00A0=/uni00A00.60\np/uni00A0</uni00A00.001\n20\n 10\n 0\nDirect\nr/uni00A0=/uni00A00.53\np/uni00A0</uni00A00.001 (d) GPT-3* / News\nFigure 7: Direct vs. metalinguistic responses for Experiment 1.Relationship between log probability assigned by\nmodel to ground-truth sentence continuation under the direct method and each metalinguistic prompting method.\nDashed line indicates x = y.\n5056\n10\n0\n10\nMetaQuestionSimple\nr/uni00A0=/uni00A00.75\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.45\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.60\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n10\n0\n10\nMetaInstruct\nr/uni00A0=/uni00A00.78\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.43\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.62\np/uni00A0</uni00A00.001\n10\n 0 10\nDirect\n10\n0\n10\nMetaQuestionComplex\nr/uni00A0=/uni00A00.52\np/uni00A0</uni00A00.001\n10\n 0 10\nDirect\nr/uni00A0=/uni00A00.21\np/uni00A0</uni00A00.001\n10\n 0 10\nDirect\nr/uni00A0=/uni00A00.44\np/uni00A0</uni00A00.001\n(a) Flan-T5\n20\n0\n20\nMetaQuestionSimple\nr/uni00A0=/uni00A00.40\np/uni00A0</uni00A00.001\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.43\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.39\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n20\n0\n20\nMetaInstruct\nr/uni00A0=/uni00A00.38\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.44\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.34\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\n20\n0\n20\nMetaQuestionComplex\nr/uni00A0=/uni00A00.06\np/uni00A0=/uni00A00.244\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.55\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.50\np/uni00A0</uni00A00.001\n(b) GPT-3*\nFigure 8: Direct vs. metalinguistic responses for Experiment 2.Relationship between log probability differentials\n(plausible −implausible) measured by direct method and each metalinguistic prompting method. Dashed line\nindicates x = y.\n5057\n0\n2MetaQuestionSimple\nr/uni00A0=/uni00A00.07\np/uni00A0=/uni00A00.202\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.38\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.45\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n0\n2MetaInstruct\nr/uni00A0=/uni00A00.17\np/uni00A0=/uni00A00.002\nr/uni00A0=/uni00A00.35\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.42\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\n0\n2MetaQuestionComplex\nr/uni00A0=/uni00A00.19\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.28\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.41\np/uni00A0</uni00A00.001\n(a) Flan-T5 / SyntaxGym\n2\n0\n2\n4\nMetaQuestionSimple\nr/uni00A0=/uni00A00.08\np/uni00A0=/uni00A00.102\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.10\np/uni00A0=/uni00A00.056\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.30\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n2\n0\n2\n4\nMetaInstruct\nr/uni00A0=/uni00A00.00\np/uni00A0=/uni00A00.933\nr/uni00A0=/uni00A00.19\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.31\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\n2\n0\n2\n4\nMetaQuestionComplex\nr/uni00A0=/uni00A0/uni00AD0.02\np/uni00A0=/uni00A00.706\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.22\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.39\np/uni00A0</uni00A00.001 (b) Flan-T5 / BLiMP\n20\n0\n20\nMetaQuestionSimple\nr/uni00A0=/uni00A0/uni00AD0.03\np/uni00A0=/uni00A00.613\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.29\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.24\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n20\n0\n20\nMetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.08\np/uni00A0=/uni00A00.131\nr/uni00A0=/uni00A00.30\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.24\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\n20\n0\n20\nMetaQuestionComplex\nr/uni00A0=/uni00A00.04\np/uni00A0=/uni00A00.447\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.24\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.15\np/uni00A0=/uni00A00.004\n(c) GPT-3* / SyntaxGym\n0\n20MetaQuestionSimple\nr/uni00A0=/uni00A0/uni00AD0.01\np/uni00A0=/uni00A00.844\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.30\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.27\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n0\n20MetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.07\np/uni00A0=/uni00A00.156\nr/uni00A0=/uni00A00.27\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.30\np/uni00A0</uni00A00.001\n0 20\nDirect\n0\n20MetaQuestionComplex\nr/uni00A0=/uni00A00.04\np/uni00A0=/uni00A00.481\n0 20\nDirect\nr/uni00A0=/uni00A00.22\np/uni00A0</uni00A00.001\n0 20\nDirect\nr/uni00A0=/uni00A00.24\np/uni00A0</uni00A00.001 (d) GPT-3* / BLiMP\nFigure 9: Direct vs. metalinguistic responses for Experiment 3a.Relationship between log probability differentials\nmeasured by direct method and each metalinguistic prompting method. For the direct method, we compute the\ndifference in log probabilities assigned to the grammatical and ungrammatical sentences. For each metalinguistic\nprompting method, we compute the difference in log probabilities assigned to the “Yes” token conditioned on the\ngrammatical and ungrammatical sentence prompts (see Section 4.3 for details).\n5058\n5\n0\n5\nMetaQuestionSimple\nr/uni00A0=/uni00A0/uni00AD0.44\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.53\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.46\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n5\n0\n5\nMetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.41\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.57\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.47\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\n5\n0\n5\nMetaQuestionComplex\nr/uni00A0=/uni00A0/uni00AD0.47\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.56\np/uni00A0</uni00A00.001\n20\n 0 20\nDirect\nr/uni00A0=/uni00A00.47\np/uni00A0</uni00A00.001\n(a) Flan-T5 / SyntaxGym\n0\n5MetaQuestionSimple\nr/uni00A0=/uni00A0/uni00AD0.21\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0SM\nr/uni00A0=/uni00A00.37\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0LG\nr/uni00A0=/uni00A00.42\np/uni00A0</uni00A00.001\nFlan/uni00ADT5/uni00A0XL\n0\n5MetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.05\np/uni00A0=/uni00A00.356\nr/uni00A0=/uni00A00.40\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.42\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\n0\n5MetaQuestionComplex\nr/uni00A0=/uni00A0/uni00AD0.05\np/uni00A0=/uni00A00.346\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.42\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.41\np/uni00A0</uni00A00.001 (b) Flan-T5 / BLiMP\n10\n0\n10\nMetaQuestionSimple\nr/uni00A0=/uni00A00.02\np/uni00A0=/uni00A00.763\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.52\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.48\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n10\n0\n10\nMetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.11\np/uni00A0=/uni00A00.035\nr/uni00A0=/uni00A00.52\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.45\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\n10\n0\n10\nMetaQuestionComplex\nr/uni00A0=/uni00A0/uni00AD0.27\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.52\np/uni00A0</uni00A00.001\n25\n 0 25\nDirect\nr/uni00A0=/uni00A00.48\np/uni00A0</uni00A00.001\n(c) GPT-3* / SyntaxGym\n10\n0\n10\nMetaQuestionSimple\nr/uni00A0=/uni00A0/uni00AD0.02\np/uni00A0=/uni00A00.678\ntext/uni00ADcurie/uni00AD001\nr/uni00A0=/uni00A00.30\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD002\nr/uni00A0=/uni00A00.35\np/uni00A0</uni00A00.001\ntext/uni00ADdavinci/uni00AD003\n10\n0\n10\nMetaInstruct\nr/uni00A0=/uni00A0/uni00AD0.01\np/uni00A0=/uni00A00.813\nr/uni00A0=/uni00A00.29\np/uni00A0</uni00A00.001\nr/uni00A0=/uni00A00.33\np/uni00A0</uni00A00.001\n0 20\nDirect\n10\n0\n10\nMetaQuestionComplex\nr/uni00A0=/uni00A0/uni00AD0.13\np/uni00A0=/uni00A00.012\n0 20\nDirect\nr/uni00A0=/uni00A00.32\np/uni00A0</uni00A00.001\n0 20\nDirect\nr/uni00A0=/uni00A00.38\np/uni00A0</uni00A00.001 (d) GPT-3* / BLiMP\nFigure 10: Direct vs. metalinguistic responses for Experiment 3b.Relationship between log probability\ndifferentials measured by direct method and each metalinguistic prompting method. For the direct method,\nwe compute the difference in log probabilities assigned to the grammatical and ungrammatical sentences. For each\nmetalinguistic prompting method, we compute the difference in log probabilities assigned to the “1” or “2” answer\noptions corresponding to the grammatical and ungrammatical sentences (see Section 4.4 for details).\n5059\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nSyntaxGym\nFlan/uni00ADT5/uni00A0SMFlan/uni00ADT5/uni00A0LGFlan/uni00ADT5/uni00A0XLtext/uni00ADcurie/uni00AD001text/uni00ADdavinci/uni00AD002text/uni00ADdavinci/uni00AD003\nBLiMP\nDirect MetaQuestionSimple MetaInstruct MetaQuestionComplex\ntext/uni00ADdavinci/uni00AD003\n6\n4\n2\n0\nLogprob/uni00A0of/uni00A0final/uni00A0word\n(a) Experiment 1\ntext/uni00ADdavinci/uni00AD003\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy (b) Experiment 3a\ntext/uni00ADdavinci/uni00AD003\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy (c) Experiment 3b\nFigure 11: Task performance from preliminary experiments in Mandarin Chinese. See Appendix D for details.\n5060",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7231611013412476
    },
    {
      "name": "Computer science",
      "score": 0.6103153228759766
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6045224666595459
    },
    {
      "name": "Probability distribution",
      "score": 0.47440028190612793
    },
    {
      "name": "Linguistics",
      "score": 0.40292108058929443
    },
    {
      "name": "Natural language processing",
      "score": 0.3794962763786316
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37641626596450806
    },
    {
      "name": "Mathematics",
      "score": 0.28097033500671387
    },
    {
      "name": "Statistics",
      "score": 0.11961996555328369
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210104429",
      "name": "Institute of Cognitive and Brain Sciences",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 26
}