{
  "title": "Exploring Contextualized Neural Language Models for Temporal Dependency Parsing",
  "url": "https://openalex.org/W3101246020",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2234532320",
      "name": "Hayley Ross",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2226269764",
      "name": "Jonathon Cai",
      "affiliations": [
        "RTX (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2282009747",
      "name": "Bonan Min",
      "affiliations": [
        "RTX (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2147923392",
    "https://openalex.org/W1956885672",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W30314283",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3088335873",
    "https://openalex.org/W2538147493",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2760579680",
    "https://openalex.org/W2955301576",
    "https://openalex.org/W2963144333",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2890601481",
    "https://openalex.org/W2917019145",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2577255746",
    "https://openalex.org/W2251325107"
  ],
  "abstract": "Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019). In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a). We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short. Source code and resources are made available at https://github.com/bnmin/tdp_ranking.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8548–8553,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n8548\nExploring Contextualized Neural Language Models for Temporal\nDependency Parsing\nHayley Ross†1, Jonathon Cai2, and Bonan Min2\n1Harvard University\nhayleyross@g.harvard.edu\n2Raytheon BBN Technologies\n{jonathon.cai, bonan.min}@raytheon.com\nAbstract\nExtracting temporal relations between events\nand time expressions has many applications\nsuch as constructing event timelines and time-\nrelated question answering. It is a challenging\nproblem which requires syntactic and seman-\ntic information at sentence or discourse lev-\nels, which may be captured by deep contextu-\nalized language models (LMs) such as BERT\n(Devlin et al., 2019). In this paper, we de-\nvelop several variants of BERT-based temporal\ndependency parser, and show that BERT sig-\nniﬁcantly improves temporal dependency pars-\ning (Zhang and Xue, 2018a). We also present\na detailed analysis on why deep contextual-\nized neural LMs help and where they may fall\nshort. Source code and resources are made\navailable at https://github.com/bnmin/\ntdp_ranking.\n1 Introduction\nTemporal relation extraction has many applications\nsuch as constructing event timelines for news ar-\nticles or narratives as well as time-related ques-\ntion answering. Recently, Zhang and Xue (2018b)\npresented Temporal Dependency Parsing (TDP),\nwhich organizes time expressions and events in a\ndocument to form a Temporal Dependency Tree\n(TDT). Given a previous step which detects time\nexpressions and events, TDP extracts the temporal\nstructure between them. Consider this example:\nExample 1: Kuchma and Yeltsin signed a co-\noperation plan on February 27, 1998. Russia and\nUkraine share similar cultures, and Ukraine was\nruled from Moscow for centuries. Yeltsin and\nKuchma called for the ratiﬁcation of the treaty,\nsaying it would create a “strong legal foundation”.\nFigure 1 shows the corresponding TDT. Com-\npared to previous pairwise approaches for temporal\nA previous version is available at https://arxiv.\norg/abs/2004.14577.\n† Work done during an internship at BBN.\nroot\nDCT\nshare ruled called\nsaying\ncreate\nFeb 27, 1998\nsigned\ndepends on\noverlap before before\noverlap\nafter\ndepends on\noverlap\n\n\n\n\nTIMEXEVENTS\nFigure 1: Temporal Dependency Tree of Example 1.\nDCT is Document Creation Time (March 1, 1998)\nrelation extraction based on TimeML (Pustejovsky\net al., 2003a) which require\n(n\n2\n)\npairs of tempo-\nral relations to be annotated, TDT signiﬁcantly\nreduces the annotation complexity while still pre-\nserving the essential temporal structure between\nevents and temporal relations. TDP is still chal-\nlenging because it requires syntactic and semantic\ninformation at sentence and discourse levels.\nRecently, deep language models such as BERT\n(Devlin et al., 2019) have been shown to be suc-\ncessful at many NLP tasks, because (1) they pro-\nvide contextualized word embeddings that are pre-\ntrained with very large corpora, and (2) BERT in\nparticular is shown to capture syntactic and seman-\ntic information (Tenney et al., 2019, Clark et al.,\n2019), which may include but is not limited to\ntense and temporal connectives. Such information\nis relevant for TDP.\nIn this paper, we present BERT-based TDP mod-\nels, and empirical evidence demonstrating that\nBERT signiﬁcantly improves TDP. We summarize\nthe contributions of this paper as follows:\n•We develop temporal dependency parsers that\nincorporate BERT, from a straightforward us-\nage of pre-trained BERT word embeddings,\nto using BERT’s multi-layer multi-head self-\nattention architecture as an encoder trained\nwithin an end-to-end system.\n8549\n•We present experiments showing signiﬁcant\nadvantages of the BERT-based TDP mod-\nels. Experiments show that BERT improves\nTDP performance in all models, with the best\nmodel achieving a 13 absolute F1 point im-\nprovement over our re-implementation of the\nneural model in (Zhang and Xue, 2018a)1.\n•We lay out a detailed analysis on BERT’s\nstrengths and limitations for this task.\nWe present technical details, experiments, and\nanalysis in the rest of this paper.\n2 Related Work\nMuch previous work has been devoted to classiﬁca-\ntion and annotation of relations between events and\ntime expressions, notably TimeML (Pustejovsky\net al., 2003a) and TimeBank (Pustejovsky et al.,\n2003b), as well as many extensions of it (see Der-\nczynski, 2017 for an overview). TimeML anno-\ntates all explicit relations in the text; at the extreme,\nTimeBank-Dense (Cassidy et al., 2014) annotates\nall\n(n\n2\n)\npairs of relations. Pair-wise annotation has\nthree problems: O(n2) complexity; the possibility\nof inconsistent predictions such as A before B, B\nbefore C, C before A; and forcing annotation of\nrelations left unclear by the document.\nWhile extracting time expressions and events is\nwell handled (e.g. Str ¨otgen and Gertz, 2010, Lee\net al., 2014), relating them is still a challenging\ntask. Previous research on extracting these relations\n(e.g. Bethard et al., 2017, Ning et al., 2017, Lin\net al., 2019) almost always uses pair-wise TimeML-\nannotated data which has rich annotation but also\ninherits the above three complexity and consistency\nissues. To address these issues, Zhang and Xue\n(2018b) present a tree structure of relations be-\ntween time expressions and events (TDT), along\nwith a BiLSTM model (Zhang and Xue, 2018a) for\nparsing text into TDT and a crowd-sourced corpus\n(Zhang and Xue, 2019).\nOrganizing time expressions and events into a\ntree has a number of advantages over traditional\npair-wise temporal annotation. It reduces the an-\nnotation complexity to O(n) and avoids cyclic in-\nconsistencies both in the annotation and the model\noutput. Despite the apparent reduction in labeled\nedges, many additional edge labels can be deduced\nfrom the tree: in Figure 1, we can deduce e.g.\n1We were unable to replicate the F1-score reported for this\ncorpus in Zhang and Xue (2019). The improvement over the\nreported, state-of-the-art result is 8 absolute F1 points.\nthat ruled is before share because ruled is before\nDCT but share overlaps DCT. A ﬁnal advantage of\nTDTs is that they allow underspeciﬁcation where\nthe source document does not explicitly specify\nan order, such as the relation between signed and\ncalled (likely to be overlap, but it is not certain).\nZhang and Xue (2019) is currently the only English-\nlanguage TDP corpus, comprising 196 newswire\narticles.\nIn addition, this paper capitalizes on the now\nwell-documented recent advances provided by\nBERT (Devlin et al., 2019). Besides offering\nricher contextual information, BERT in particular\nis shown to capture syntactic and semantic proper-\nties (Tenney et al., 2019, Clark et al., 2019) relevant\nto TDP, which we show yield improvements over\nZhang and Xue’s original model.\n3 BERT-based Neural Models for\nTemporal Dependency Parsing\nFollowing Zhang and Xue (2018a), we transformed\ntemporal dependency parsing (TDP) to a rank-\ning problem: given a child mention (event or\ntime expression) xi extracted by a previous sys-\ntem, the problem is to select the most appropriate\nparent mention from among the root node, DCT\nor an event or time expression from the window\nxi−k, . . . , xi, . . . , xi+m 2 around xi, along with\nthe relation label (before, after, overlap, depends\non). That is, for each xj in the window, the model\njudges the child-parent candidate pair ⟨xi, xj⟩. A\nTemporal Dependency Tree (TDT) is assembled\nwith an incremental algorithm which selects, for\neach event and time expression in sequence in the\ndocument, the highest-ranked prediction ⟨parent,\nrelation type⟩. The tree structure is enforced by\nselecting the highest probability parent which does\nnot introduce a cycle3.\nWe developed three models that share a similar\noverall architecture (Figure 2): the model takes a\npair of mentions (child and potential parent) as in-\nput and passes each pair through an encoder which\nembeds the nodes and surrounding context into\na dense representation. All models use the same\nwindow approach described above to source parent\ncandidates. Following Zhang and Xue (2018a), lin-\nguistic features are concatenated onto the dense rep-\nresentation, which is then passed to a feed-forward\n2We set k = 10, m= 3in all experiments.\n3In practice, this step to avoid cyclic edges is rare: it is\nrequired for less than 4% of the predicted edges.\n8550\nFigure 2: Model architecture for TDP with three differ-\nent encoders (orange, blue, green boxes). Shown with\nthe ⟨parent, child⟩input pairs for a given child (event\nor time expression) xi. For simplicity, we did not show\n⟨xi, root⟩and ⟨xi, DCT⟩, which are included as can-\ndidate pairs for all xi.\nlayer and a softmax function to generate scores for\neach relation label for each pair.\nWe developed three types of encoder:\n•BILSTM and BILSTM -GLOVE feed the doc-\nument’s word embeddings to a BiLSTM to\nencode the pair as well as the surrounding\ncontext. The word embeddings can be either\nrandomly initialized (identical to Zhang and\nXue, 2018a) (in BILSTM ), or pre-trained from\na large corpus – we used GloVe (Pennington\net al., 2014) (in BILSTM -GLOVE ).\n•BILSTM -BERT replaces the word embeddings\nwith frozen (pre-trained) BERT contextual-\nized word embeddings. We used the BERT-\nbase uncased model4, which has been trained\non English Wikipedia and the BookCorpus.\n•BERT-FT: BERT’s multi-layer multi-head\nself-attention architecture (with pre-trained\nweights) is used directly to encode the pairs.\nIts weights are ﬁne-tuned in the end-to-end\nTDP training process.\nAll models use the same loss function and scor-\ning as in Zhang and Xue (2018a). We present more\ndetails about the two BERT-based models below.\n3.1 Model BILSTM -BERT\nThe ﬁrst model adjusts the model architecture from\nZhang and Xue (2018a) to replace its word em-\nbeddings with frozen BERT embeddings. That is,\nword embeddings are computed via BERT for every\nsentence in the document; then, these word embed-\ndings are processed as in the original model. More\ndetails about the BiLSTM model can be found in\nZhang and Xue (2018a).\n4https://github.com/google-research/\nbert\n3.2 Model BERT-FT\nThis model takes advantage of BERT’s multi-layer\nmulti-head self-attention architecture (Vaswani\net al., 2017) to learn feature representations for\nclassiﬁcation. The embedding of the ﬁrst token\n[CLS] is interpreted as a classiﬁcation output and\nﬁne-tuned.\nTo represent a child-parent pair with context,\nBERT-FT constructs a pseudo-sentence for the (po-\ntential) parent node and a pseudo-sentence for the\nchild node. The pair of pseudo-sentences are con-\ncatenated and separated by the [SEP] token, and\nthen fed into the BERT model. Each pseudo-\nsentence is formed of the word(s) of the node, the\nnode’s label (TIMEX or EVENT), a separator to-\nken ‘:’ and the sentence containing the node, as\nshown in Table 1.\nword(s) label sep sentence\nFebruary\n27, 1998\nTIMEX : Kuchma and Yeltsin signed\na cooperation plan on\nFebruary 27 1998.\ncalled EVENT : Yeltsin and Kuchma called\nfor the ratiﬁcation . . .\nTable 1: A pair of pseudo-sentences in BERT-FT, for\npotential parent February 27, 1998 and child called in\nExample 1 (The correct parent here is DCT).\n4 Experiments\nWe use the training, development and test datasets\nfrom Zhang and Xue (2019) for all experiments\n(182 train / 5 development / 9 test documents, total\n2084 sentences). The documents in the datasets\nare already annotated with events and temporal\nexpressions. This allows us to focus on evaluating\nthe task of constructing temporal dependency trees.\nWe evaluated four conﬁgurations of the encoders\nabove. Firstly BILSTM (RE-IMPLEMENTED ) re-\nimplements Zhang and Xue (2018a)’s model5 in\nTensorFlow (Abadi et al., 2016) for fair compar-\nison. Replacing its randomly-initialized embed-\ndings with GloVe (Pennington et al., 2014) yields\nBILSTM -GLOVE . We also test the models BILSTM -\nBERT and BERT-FT as described in Section 3.\nWe used Adam (Kingma and Ba, 2014) as the\noptimizer and performed coarse-to-ﬁne grid search\nfor key parameters such as learning rate and num-\nber of epochs using the dev set 6. We observed\n5Originally implemented in DyNet (Neubig et al., 2017).\n6We tried all parameter conﬁgurations with learning rates\nin {0.001, 0.0001, 0.0005, 0.00025} and numbers of epochs\nin {50, 75, 100}, and perform 5 runs for each conﬁguration.\nWe observed a mean F1 of 0.58 with variance=0.002 across\n8551\nthat when ﬁne-tuning BERT in the BERT-FT model,\na lower learning rate (0.0001) paired with more\nepochs (75) achieves top performance, compared\nto using learning rate 0.001 with 50 epochs for the\nBiLSTM models. We used NVIDIA Tesla P100\nGPUs for training the models. On a single GPU,\none epoch takes 7.5 minutes for theBERT-FT model\nand 0.8 minutes for the BILSTM -BERT model.\nF1 score\nModel dev test\nRule-based baseline (Zhang and Xue, 2019) 0.15 0.18\nBiLSTM (Zhang and Xue, 2019) 0.53 0.60\nBILSTM (re-impl., Zhang and Xue, 2019) 0.45 0.55\nBILSTM -GLOVE 0.50 0.58\nBILSTM -BERT 0.54 0.61\nBERT-FT 0.59 0.68\nTable 2: Performance of the models.\n.\nTable 2 summarizes the F1 scores7 of our mod-\nels. Results are averaged over 5 runs. We also in-\nclude the rule-based baseline and the performance\nreported in Zhang and Xue (2019), which applies\nthe model of Zhang and Xue (2018a) to the 2019\ncorpus, as a baseline8.\nBILSTM -BERT outperforms the re-implemented\nBILSTM model by 6 points and BILSTM -GLOVE by\n3 points in F1-score, respectively. This indicates\nthat the frozen, pre-trained BERT embeddings im-\nprove temporal relation extraction compared to ei-\nther kind of non-contextualized embedding. Fine-\ntuning the BERT-based encoder (BERT-FT) resulted\nin an absolute improvement of as much as 13 abso-\nlute F1 points over the BiLSTM re-implementation,\nand 8 F1 points over the reported results in Zhang\nand Xue (2019). This demonstrates that contextual-\nized word embeddings and the BERT architecture,\npre-trained with large corpora and ﬁne-tuned for\nthis task, can signiﬁcantly improve TDP.\nWe also calculated the models’ accuracies on\ntime expressions or events subdivided by their type\nof parent: DCT, a time expression other than DCT,\nor another event. Difﬁcult categories are children of\nDCT and children of events. We see that the main\ndifference between BILSTM and BILSTM -BERT is\nits performance on children of DCT: with BERT, it\nscores 0.48 instead of 0.38. Conversely BERT-FT\nall conﬁgurations for all models.\n7Following (Zhang and Xue, 2019), F1 scores are reported.\nFor a document with n nodes, the TDP task constructs a tree\nof n + 1edges, so F1 is essentially the same as the accuracy.\n8We were unable to replicate the F1-score reported in\nZhang and Xue (2019) despite using similar hyperparameters.\nTherefore, we include performances for our re-implementation\nand the reported score in Zhang and Xue (2019) in Table 2.\nsees improvements across the board over BILSTM ,\nwith a 0.21 increase on children of DCT, a 0.14\nincrease for children of other time expressions, and\na 0.11 increase for children of events.\n5 Analysis\nWhy BERT helps:A detailed manual comparison\nof the dependency trees produced by the different\nmodels for articles in the test set shows BERT’s\nadvantages for TDP. The following phenomena are\nattested by many sentences in many documents and\ncorrespond to known properties of BERT.\nFirstly, unlike BILSTM , BERT-FT is able to prop-\nerly relate time expressions occurring syntactically\nafter the event, such as Kuchma and Yeltsinsigned\na cooperation plan on February 27, 1998 in Exam-\nple 1. ( BILSTM falsely relates signed to the “pre-\nvious” time expression DCT). This shows BERT’s\nability to “look forward” with its self-attention, at-\ntending to parents appearing after the child.\nSecondly, BERT-FT is able to capture verb tense\nand use it to determine the correct relation for both\nDCT and chains of events. For example, it knows\nthat present tense (share similar cultures) overlaps\nDCT, while past perfect events ( was ruled from\nMoscow) happen either before DCT or before the\nevent adjacent (salient) to them.\nThirdly, BERT-FT captures syntactic construc-\ntions with implicit temporal relations such as re-\nported speech and gerunds (e.g. in Example 1,\nYeltsin and Kuchmacalled for the ratiﬁcation [. . . ],\nsaying it would create . . ., it identiﬁes that called\nand saying overlap and create is after saying).\nSimilarly, BERT’s ability to handle syntactic\nproperties (Tenney et al., 2019, Clark et al., 2019)\nsuch as embedded clauses may allow it to detect\nthe direction of connectives such as since. While\nall models may identify the matrix clause verb as\nthe correct parent, BERT-FT is much more likely to\nchoose the correct label. ( BILSTM almost always\nchooses ‘before’ for DCT or ‘after’ for children of\nevents, ignoring the connective.)\nLastly, both BERT-FT and BILSTM -BERT are\nmuch better than the BILSTM at identifying context\nchanges (new “sections”) and linking these events\nto DCT rather than to a time expression in the pre-\nvious sections (evidenced by the scores on children\nof DCT). Because BERT’s word embeddings use\nthe sentence as context, the models using BERT\nmay be able to “compare” the sentences and judge\nthat they are unrelated despite being adjacent.\n8552\nEquivalent TDP trees:In cases where BERT-FT\nis incorrect, it sometimes produces an equivalent\nor very similar tree (since relations such as overlap\nare transitive, there may be multiple equivalent\ntrees). Future work could involve developing a\nmore ﬂexible scoring function to account for this.\nLimitations: There are also limitations to BERT-\nFT. For example, it is still fooled by syntactic\nambiguity. Consider this example:\nExample 2:Foreign ministers agreed to set up\na panel to investigate who shot down the Rwandan\npresident’s plane on April 6, 1994.\nA human reading this sentence will infer based\non world knowledge that April 6, 1994 should be\nattached to the embedded clause (who shot down),\nnot to the matrix clause ( agreed), but a syntactic\nparser would produce both parses. BERT-FT in-\ncorrectly attaches agreed to April 6, 1994 : even\nBERT’s contextualized embeddings are not sufﬁ-\ncient to identify the correct parse.\n6 Conclusion and Future Work\nWe present two models that incorporate BERT into\ntemporal dependency parsers, and observe signiﬁ-\ncant gains compared to previous approaches. We\npresent an analysis of where and how BERT helps\nwith this challenging task.\nFor future research, we plan to explore other\ntypes of deep neural LMs such as Transformer-\nXL (Dai et al., 2019) and XLNet (Yang et al., 2019).\nAs discussed in Section 5, we also plan to develop\na more ﬂexible scoring function which can handle\nequivalent trees. Finally, we plan to evaluate BERT-\nFT on other temporal relation datasets as part of a\nlarger pipeline, which will include a mapping be-\ntween TDTs and other temporal relation annotation\nschemas such as the TempEval-3 dataset (UzZa-\nman et al., 2013).\nAcknowledgments\nThis work was supported by DARPA/I2O and\nU.S. Air Force Research Laboratory Contract No.\nFA8650-17-C-7716 under the Causal Exploration\nprogram, and DARPA/I2O and U.S. Army Re-\nsearch Ofﬁce Contract No. W911NF-18-C-0003\nunder the World Modelers program. The views,\nopinions, and/or ﬁndings expressed are those of the\nauthor(s) and should not be interpreted as represent-\ning the ofﬁcial views or policies of the Department\nof Defense or the U.S. Government. This document\ndoes not contain technology or technical data con-\ntrolled under either the U.S. International Trafﬁc in\nArms Regulations or the U.S. Export Administra-\ntion Regulations.\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In 12th USENIX Symposium\non Operating Systems Design and Implementation\n(OSDI 16), pages 265–283.\nSteven Bethard, Guergana Savova, Martha Palmer,\nand James Pustejovsky. 2017. SemEval-2017 task\n12: Clinical TempEval. In Proceedings of the\n11th International Workshop on Semantic Evalua-\ntion (SemEval-2017) , pages 565–572, Vancouver,\nCanada. Association for Computational Linguistics.\nTaylor Cassidy, Bill McDowell, Nathanel Chambers,\nand Steven Bethard. 2014. An annotation frame-\nwork for dense event ordering. Technical report,\nCarnegie-Mellon Univ Pittsburgh PA.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nLeon Derczynski. 2017. Automatically Ordering\nEvents and Times in Text, volume 677 of Studies in\nComputational Intelligence. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKenton Lee, Yoav Artzi, Jesse Dodge, and Luke Zettle-\nmoyer. 2014. Context-dependent semantic parsing\nfor time expressions. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1437–\n1447.\n8553\nChen Lin, Timothy Miller, Dmitriy Dligach, Steven\nBethard, and Guergana Savova. 2019. A bert-based\nuniversal model for both within-and cross-sentence\nclinical temporal relation extraction. In Proceedings\nof the 2nd Clinical Natural Language Processing\nWorkshop, pages 65–71.\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\nMatthews, Waleed Ammar, Antonios Anastasopou-\nlos, Miguel Ballesteros, David Chiang, Daniel Cloth-\niaux, Trevor Cohn, et al. 2017. Dynet: The\ndynamic neural network toolkit. arXiv preprint\narXiv:1701.03980.\nQiang Ning, Zhili Feng, and Dan Roth. 2017. A struc-\ntured learning approach to temporal relation extrac-\ntion. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1027–1037.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nJames Pustejovsky, Jos ´e M Castano, Robert Ingria,\nRoser Sauri, Robert J Gaizauskas, Andrea Set-\nzer, Graham Katz, and Dragomir R Radev. 2003a.\nTimeML: Robust speciﬁcation of event and tempo-\nral expressions in text. New directions in question\nanswering, 3:28–34.\nJames Pustejovsky, Patrick Hanks, Roser Sauri, An-\ndrew See, Robert Gaizauskas, Andrea Setzer,\nDragomir Radev, Beth Sundheim, David Day, Lisa\nFerro, et al. 2003b. The timebank corpus. In Corpus\nlinguistics, volume 2003, page 40. Lancaster, UK.\nJannik Str¨otgen and Michael Gertz. 2010. Heideltime:\nHigh quality rule-based extraction and normaliza-\ntion of temporal expressions. In Proceedings of the\n5th International Workshop on Semantic Evaluation,\npages 321–324. Association for Computational Lin-\nguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601.\nNaushad UzZaman, Hector Llorens, Leon Derczyn-\nski, James Allen, Marc Verhagen, and James Puste-\njovsky. 2013. Semeval-2013 task 1: Tempeval-3:\nEvaluating time expressions, events, and temporal\nrelations. In Second Joint Conference on Lexical\nand Computational Semantics (* SEM), Volume 2:\nProceedings of the Seventh International Workshop\non Semantic Evaluation (SemEval 2013), pages 1–9.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. dAlch ´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 5753–\n5763. Curran Associates, Inc.\nYuchen Zhang and Nianwen Xue. 2018a. Neural rank-\ning models for temporal dependency structure pars-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3339–3349.\nYuchen Zhang and Nianwen Xue. 2018b. Structured in-\nterpretation of temporal relations. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018).\nYuchen Zhang and Nianwen Xue. 2019. Acquiring\nstructured temporal representation via crowdsourc-\ning: A feasibility study. In Proceedings of the\nEighth Joint Conference on Lexical and Computa-\ntional Semantics (* SEM 2019), pages 178–185.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8790055513381958
    },
    {
      "name": "Dependency grammar",
      "score": 0.7411891222000122
    },
    {
      "name": "Natural language processing",
      "score": 0.721896767616272
    },
    {
      "name": "Artificial intelligence",
      "score": 0.71193927526474
    },
    {
      "name": "Parsing",
      "score": 0.6548354625701904
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6436128616333008
    },
    {
      "name": "Timeline",
      "score": 0.5703867077827454
    },
    {
      "name": "Sentence",
      "score": 0.5673945546150208
    },
    {
      "name": "Question answering",
      "score": 0.5537595748901367
    },
    {
      "name": "Language model",
      "score": 0.49247321486473083
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.41849175095558167
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1306686416",
      "name": "RTX (United States)",
      "country": "US"
    }
  ],
  "cited_by": 14
}