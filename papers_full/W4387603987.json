{
  "title": "Transformer-based autoencoder with ID constraint for unsupervised anomalous sound detection",
  "url": "https://openalex.org/W4387603987",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5059231561",
      "name": "Jian Guan",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5102446491",
      "name": "Youde Liu",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5072482416",
      "name": "Qiuqiang Kong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5062281933",
      "name": "Feiyang Xiao",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5104012520",
      "name": "Qiaoxi Zhu",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5101255430",
      "name": "Jiantong Tian",
      "affiliations": [
        "Harbin Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A5100676721",
      "name": "Wenwu Wang",
      "affiliations": [
        "University of Surrey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2122646361",
    "https://openalex.org/W2897802972",
    "https://openalex.org/W3081229243",
    "https://openalex.org/W4372185032",
    "https://openalex.org/W2292996718",
    "https://openalex.org/W2895776009",
    "https://openalex.org/W2031706718",
    "https://openalex.org/W3008923235",
    "https://openalex.org/W2799741169",
    "https://openalex.org/W2896255741",
    "https://openalex.org/W6603145748",
    "https://openalex.org/W6632173229",
    "https://openalex.org/W2594557222",
    "https://openalex.org/W6725081647",
    "https://openalex.org/W1601124178",
    "https://openalex.org/W1650484478",
    "https://openalex.org/W3015356122",
    "https://openalex.org/W4200500401",
    "https://openalex.org/W6600062588",
    "https://openalex.org/W3178175559",
    "https://openalex.org/W6701294789",
    "https://openalex.org/W4214694907",
    "https://openalex.org/W4221140395",
    "https://openalex.org/W4372260076",
    "https://openalex.org/W4385822679",
    "https://openalex.org/W3162150435",
    "https://openalex.org/W2104067967",
    "https://openalex.org/W6600254619",
    "https://openalex.org/W2306289963",
    "https://openalex.org/W2766388946",
    "https://openalex.org/W2982294822",
    "https://openalex.org/W2997122788",
    "https://openalex.org/W3121952123"
  ],
  "abstract": "Abstract Unsupervised anomalous sound detection (ASD) aims to detect unknown anomalous sounds of devices when only normal sound data is available. The autoencoder (AE) and self-supervised learning based methods are two mainstream methods. However, the AE-based methods could be limited as the feature learned from normal sounds can also fit with anomalous sounds, reducing the ability of the model in detecting anomalies from sound. The self-supervised methods are not always stable and perform differently, even for machines of the same type. In addition, the anomalous sound may be short-lived, making it even harder to distinguish from normal sound. This paper proposes an ID-constrained Transformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly score computation for unsupervised ASD. Machine ID is employed to constrain the latent space of the Transformer-based autoencoder (TransAE) by introducing a simple ID classifier to learn the difference in the distribution for the same machine type and enhance the ability of the model in distinguishing anomalous sound. Moreover, weighted anomaly score computation is introduced to highlight the anomaly scores of anomalous events that only appear for a short time. Experiments performed on DCASE 2020 Challenge Task2 development dataset demonstrate the effectiveness and superiority of our proposed method.",
  "full_text": "Guan et al. \nEURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42  \nhttps://doi.org/10.1186/s13636-023-00308-4\nMETHODOLOGY Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nEURASIP Journal on Audio,\nSpeech, and Music Processing\nTransformer-based autoencoder with ID \nconstraint for unsupervised anomalous sound \ndetection\nJian Guan1*  , Youde Liu1, Qiuqiang Kong2, Feiyang Xiao1, Qiaoxi Zhu3, Jiantong Tian1 and Wenwu Wang4 \nAbstract \nUnsupervised anomalous sound detection (ASD) aims to detect unknown anomalous sounds of devices \nwhen only normal sound data is available. The autoencoder (AE) and self-supervised learning based methods are \ntwo mainstream methods. However, the AE-based methods could be limited as the feature learned from normal \nsounds can also fit with anomalous sounds, reducing the ability of the model in detecting anomalies from sound. The \nself-supervised methods are not always stable and perform differently, even for machines of the same type. In addi-\ntion, the anomalous sound may be short-lived, making it even harder to distinguish from normal sound. This paper \nproposes an ID-constrained Transformer-based autoencoder (IDC-TransAE) architecture with weighted anomaly score \ncomputation for unsupervised ASD. Machine ID is employed to constrain the latent space of the Transformer-based \nautoencoder (TransAE) by introducing a simple ID classifier to learn the difference in the distribution for the same \nmachine type and enhance the ability of the model in distinguishing anomalous sound. Moreover, weighted anomaly \nscore computation is introduced to highlight the anomaly scores of anomalous events that only appear for a short \ntime. Experiments performed on DCASE 2020 Challenge Task2 development dataset demonstrate the effectiveness \nand superiority of our proposed method.\nKeywords Anomalous sound detection, Autoencoder, ID classifier, Weighted anomaly score computation\n1 Introduction\nAnomalous sound detection (ASD) aims to detect anom -\nalies from acoustic signals. Since anomalous sounds can \nindicate system error or malicious activities, ASD has \nreceived much attention [1 –5], which has been widely \nused in various applications, such as road surveil -\nlance [6 , 7], animal disease detection [8 ], and industrial \nequipment predictive maintenance [9 ]. Recently, ASD \nhas also been used to monitor the abnormality of indus -\ntrial machinery equipment, such as anomaly detection \nfor surface-mounted device machine [10, 11], and the \nDetection and Classification of Acoustic Scenes and \nEvents (DCASE) challenge Task2 from 2020 to 2023 \n[12–15], to reduce the loss caused by machine damage \nand the cost of manual inspection.\nSupervised learning based methods usually train a \nbinary classifier to detect the anomaly [7, 16]. However, it \nis hard to collect enough anomalous data for supervised \nlearning, as actual anomalous sounds rarely occur in real \nscenarios. In addition, the high diversity of the anomalies \ncan reduce the robustness of supervised methods. There -\nfore, unsupervised methods are often employed to detect \nunknown anomalous sounds without using anomalous \nsound samples.\n*Correspondence:\nJian Guan\nj.guan@hrbeu.edu.cn\n1 College of Computer Science and Technology, Harbin Engineering \nUniversity, Harbin 150001, Heilongjiang, China\n2 Bytedance, Shanghai, China\n3 Centre for Audio, Acoustics and Vibration, University of Technology \nSydney, Ultimo 2007, NSW, Australia\n4 Centre for Vision, Speech and Signal Processing, University of Surrey, \nGuildford GU2 7XH, UK\nPage 2 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nIn unsupervised ASD, a method is to employ the \nautoencoder (AE) to learn the distributions of sound \nsignals and perform anomaly detection. Conventional \nAE-based approaches adopt autoencoder to reconstruct \nmultiple frames of spectrogram to learn the distribution \nof normal sounds, and then the reconstruction error is \nused to obtain the anomaly score for anomaly detection \n[10, 12, 17–19]. However, the conventional AE-based \nmethods do not work well for non-stationary ASD [20], \nas non-stationary normal sounds (e.g., sound signals of \nvalves) can easily have larger reconstruction errors than \nabnormal sounds, thus deteriorating the detection per -\nformance. In [20], an interpolation deep neural network \n(IDNN) method is proposed, which masks the center \nframe of the input, and only uses the reconstruction error \nof the masked center frame to improve non-stationary \nsound reconstruction, without considering the edge \nframes, while the method in [21] adopts a similar strat -\negy as IDNN and applies the local area mask on the input \nand employs attentive neural process (ANP) [22] for the \nreconstruction of the masked input.\nInstead of reconstructing spectrogram feature, the \nmethod in [23] mixes multiple features as the input, and \nadopts a fully connected U-Net for the mixed feature \nreconstruction. To utilize the intra-frame statistics of \nsound signal, a novel group masked autoencoder for dis -\ntribution estimation (Group MADE) is proposed for unsu-\npervised ASD [24, 25], which estimates the density of an \naudio time series and achieves better performance. How -\never, the distributions of normal audio clips from different \nmachines are different even for the same sound class. This \ndifference can be even greater than that between normal \nand anomalous sound, which makes it harder to distin -\nguish normal and anomalous sounds for these purely AE-\nbased methods, as the learned feature from these normal \nsounds may also fit with the anomalous sounds [26].\nMachine identity (ID) has been used as the additional \ncondition for encoding in the latent feature space of \nAE, in order to allow the decoder to provide different \nreconstructions for each machine [27, 28]. However, \nthe encoder is unable to learn the difference in dis -\ntributions for different machines, and as a result, the \nanomalous sound may be well reconstructed. For this \nreason, it could still be difficult to distinguish normal \nand anomalous sound. In addition, the abovemen -\ntioned AE-based methods often use averaged anomaly \nscore for detection, which does not take into account \nthe short-lived condition in anomalous sound, result -\ning in low anomaly scores for anomalous events that \nappear only for a short time, which makes it even more \nchallenging for the AE-based methods.\nTherefore, instead of reconstructing normal sounds \nto learn the feature representation, the self-supervised \nmethods are presented to learn the feature representa -\ntion by utilizing the difference in distributions among dif-\nferent machines [29–36]. The study in [29] uses machine \ntype and machine ID in addition to the machine con -\ndition (normal/abnormal) as training labels for self-\nsupervised classification. The flow-based self-supervised \nmethod [37] adopts normalizing flow (NF) [38, 39] \nmodels, such as generative flow (Glow) [40] and masked \nautoregressive flow (MAF) [41], to obtain the likeli -\nhood estimation for anomaly detection. In this method, \nan auxiliary task is introduced to distinguish the sound \ndata of that machine ID (i.e., target data) from the sound \ndata of other machine IDs with the same machine type \n(i.e., outlier data). Moreover, although the self-supervised \nlearning-based methods can achieve better performance \nthan the AE-based methods, they are not always stable \nand could perform differently even for the machines of \nthe same type.\nIn this paper, we present an ID-constrained Trans -\nformer-based autoencoder (IDC-TransAE) architec -\nture with weighted anomaly score computation for \nunsupervised ASD. Our method includes two stages, \nnamely, spectrogram reconstruction and anomaly \ndetection. First, an IDC-TransAE is introduced to \nreconstruct the spectrogram of normal sounds, where \nTransformer [42] is employed to build the AE architec -\nture, and a simple ID classifier is incorporated into the \nAE. Specifically, the Transformer captures the time-\ndependent information of the sound signal, and the \nclassifier utilizes machine ID to constrain the latent \nspace of AE, so that our proposed IDC-TransAE can \nlearn different distributions of normal machines, even \nwith the same type. In the proposed IDC-TransAE \narchitecture, instead of using the positional encoding \n(PE) for Transformer to provide additional temporal \ninformation, a linear phase embedding (LPE) method \nis proposed to represent the temporal information of \nsound signal by using its phase information, which can \nfurther enhance the classification performance of the \nproposed IDC-TransAE. In addition, the center frame \nprediction (CFP) is also employed in our IDC-TransAE \nto improve the ASD ability for non-stationary signals \n(e.g., Valve). Then, the reconstruction error from the \ntrained IDC-TransAE can be used to calculate the \nanomaly score to detect the anomaly. Here, we intro -\nduce a weighted anomaly score computation method \nvia global weighted ranking pooling (GWRP) [43], \nwhich can highlight the anomaly scores for the anoma -\nlous events that only appear for a short time. Finally, \nwe obtain the final anomaly score with the combina -\ntion of the classification anomaly score and weighted \nreconstruction anomaly score, to obtain more stable \nand consistent detection performance.\nPage 3 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nIn summary, the innovations and contributions of \nthis paper for unsupervised anomalous sound detec -\ntion can be summarized as follows: \n1. We analyze the generalization problem of AE for \nASD and point out the main reason for this problem, \nand propose a solution, i.e., IDC-TransAE, to miti -\ngate the generalization of AE and improve the detec -\ntion performance. To the best of our knowledge, this \nis the first work to clearly point out the main reason \nfor the generalization problem of AE for ASD.\n2. We propose an ID constraint (IDC) classifier to learn \ndifferent audio feature distributions from the same \nmachine type, which can enhance the distinguishing \nability for anomaly detection.\n3. We design a linear phase embedding (LPE) to replace \nthe traditional positional encoding (PE) to preserve \nthe own temporal information of machine sounds by \nthe phase of sounds.\n4. In the anomaly score calculation, we introduce the \nglobal weighted ranking pooling (GWRP) to high -\nlight the anomaly score of sounds with short-time \nnon-stationary anomalies, which obtains a more sta -\nble and consistent detection performance.\n5. Experimental results verify that the proposed IDC-\nTransAE method can mitigate the generalization \nproblem of AE for ASD. Ablation studies and visuali -\nzations further verify the effectiveness of the design \nof ID constraint, LPE and GWRP for ASD. Our study \nemploys the DCASE 2020 Challenge Task2 data -\nset to address AE’s generalization problem in ASD, \nexcluding DCASE 2022 and 2023 datasets tailored \nfor domain-shift and first-shot scenarios beyond our \npaper’s scope.\n2  Preliminary\nThe AE-based methods are widely used for unsupervised \nASD [ 10, 12, 17, 18] An AE model is trained with nor -\nmal sounds to learn their feature distribution. It implicitly \nassumes that it can reconstruct normal sounds better \nthan anomalous sounds, so that anomalous sounds often \nhave larger reconstruction errors than normal sound. The \nreconstruction error is then used for deriving the anomaly \nscores for anomaly detection. Figure  1 shows the AE archi-\ntecture for unsupervised ASD.\nRegarding model training, multiple frames of a spectro-\ngram are usually used as the input, and the same number \nof frames are generated as the output. Suppose X ∈ RN×M \nis the log-Mel spectrogram of the sound signal, where N is \nthe number of frames and M is the feature dimension of \neach frame of X . The loss for the AE model training is\nwhere E(·) and D(·) are the encoder and the decoder of \nAE, respectively.\nThen, the trained AE model can be used to detect the \nanomaly. Y is the test audio clip, split into I segments, \n{Y i}I\ni=1 . Here, Y i∈ RN ×M  is the ith segment and also the \nith input of the model. The reconstruction error ei for Y i is\nwhere Y i = D (E(Y i)) is the corresponding output \nframes, and �·�F denotes Frobenius norm. It results in a \nreconstruction error sequence e ={ ei}I\ni=1 for Y , and the \nmean reconstruction error of e can be used as the anom -\naly score\nHere, A(e)mean represents the anomalous degree of the \naudio clip. The normal or anomaly of the clip is deter -\nmined by H(e, θ) [44]:\n(1)LAE = �X − D(E(X))�2\n2,\n(2)ei = 1\nNM (Y i− Y i) 2\nF,\n(3)A(e)mean = 1\nI\nI∑\ni=1\nei.\n(4)H(e,θ) =\n{\n0 (Normal) A(e)mean ≤ θ\n1 (Anomaly) A(e)mean >θ ,\nFig. 1 Typical architecture of AE for unsupervised ASD uses the reconstruction error between the input and output as the anomaly score\nPage 4 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nwhere θ is a pre-defined threshold value to determine \nwhether an audio clip is anomalous.\nHowever, for normal non-stationary sounds, the AE-\nbased methods tend to give large reconstruction errors \nfor both normal and abnormal sounds; this is because \nthe edge frames of non-stationary sound are hard to \nreconstruct. In [20], IDNN is proposed for non-sta -\ntionary sound ASD, which removes the center frame of \nthe multiple frames as the input, and predicts the \nremoved frame as the output, as shown in Fig.  2. The \ninput multiple frames of IDNN can be expressed as \nX =\n[\nx1 ,··· ,xN+1\n2 −1 ,xN+1\n2 +1 ,··· ,xN\n]T\n , and T denotes \ntransposition. The loss function of IDNN is formu -\nlated as\nwhere x N+1\n2\n is the removed center frame of original input \nframes. Unlike conventional AE-based methods, the \nreconstruction error ei of the ith input is only calculated \nby the center frame.\nHowever, the training procedure does not involve the \nanomalous sound, as a result, the AE-based methods \ncould be limited in the scenario where the learned fea -\nture also fits with the anomalous sound [2]. In this case, \nthe anomalous sound could be well reconstructed with \na smaller reconstruction error than that of the normal \nsounds of different machines, even of the same type. For \nexample, the anomalous sounds from one machine may \nbe similar to the normal sounds of another machine, due \nto different usage of different machines. In this case, the \nAE trained with these different machines of the same \nmachine type can reconstruct the anomalous sounds \nwell, and thus it may not be able to detect these anoma -\nlous sounds.\nIn addition, for anomalous events that only appear for a \nshort time in audio clips, the anomaly score calculated by \nmean reconstruction error is often too small, making it \ndifficult to detect the anomaly.\n(5)LIDNN =\nxN +1\n2\n− D(E(X ))\n\n2\n2\n,\n3  Proposed method\nThis section presents our IDC-TransAE with weighted \nanomaly score computation for unsupervised ASD. We \nintroduce IDC-TransAE to reconstruct the spectrogram \nof normal sounds to learn their distributions, and apply \nGWRP for weighted anomaly score computation to per -\nform anomaly detection.\n3.1  ID constraint Transformer autoencoder\nWe utilize Transformer to exploit temporal information \nfor better reconstruction of normal sounds, where only \nthe encoder layer of Transformer is employed to build \nthe encoder and decoder of our IDC-TransAE architec -\nture. In addition, machine ID is adopted to constrain the \nlatent space of the AE by introducing a simple ID classi -\nfier to learn different representations for different normal \nsounds. The framework of the proposed IDC-TransAE is \nillustrated in Fig. 3.\n3.1.1  Center frame prediction\nFor better reconstruction of the spectrogram of normal \nnon-stationary sound, following IDNN, we introduce a \ncenter frame prediction (CFP) method by removing the \ncenter frame of input frames and predicting the removed \nframe. After removing center frame x N+1\n2\n , the input \nframes can be expressed as\nwhere T denotes the matrix transposition operation. \nUnlike IDNN, the predicted center frame obtained by \nCFP is the average pooling of decoder output in frames \nand then processed by a linear layer, as shown in Fig. 3.\n3.1.2  Linear phase embedding\nTo represent the appropriate positional relationship of \nthe sound signal, we propose a linear phase embedding \n(6)X =\n[\nx1 ,··· ,xN+1\n2 −1 ,xN+1\n2 +1 ,··· ,xN\n]T\n,\nFig. 2 The architecture of IDNN uses the reconstruction error of center frame as the anomaly score\nPage 5 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \n(LPE) method for IDC-TransAE, to replace positional \nencoding (PE), often used in Transformer to provide \nadditional position information via sinusoid function \n[42], which is, however, not strongly correlated with the \nsound signal. In contrast, LPE in the proposed method \npreserves the signal’s temporal information by linearly \nembedding the phase angles of the signal to the same \ndimensions with the input X . The phase angle is obtained \nvia the short-time Fourier transform (STFT).\nAssuming the phase angles corresponding to X are \ndenoted as � =\n[\nφ1 ,··· ,φN+1\n2 −1 ,φN+1\n2 +1 ,··· ,φN\n]T\n , \nwith center frame removed, and F(·) is the linear embed -\nding function, including two linear layers with batch nor-\nmalization. The output X of decoder D(·) can be obtained \nas\nwhere X = [x1 ,··· ,xn,··· ,xN −1 ]T . Then, the average \npooling of X is used to predict the center frame, and the \nreconstruction loss for the center frame is\nwhere W o and bo are the learnable parameters of the last \noutput linear layer. The LPE module helps preserve the \ntemporal information of the signal to enhance the ability \nof the model for anomalous sound detection.\n(7)X = D(E(X + F(/Phi1))),\n(8)Lr =\nx N +1\n2\n−\n(\nW o\n1\nN − 1\nN −1∑\nn=1\nxn + bo\n)\n2\n2\n,\n3.1.3  ID classifier\nWe observed that the performance of the trained AE \nmodel on different machines with the same type could \nbe quite different. The potential cause is the difference in \ndistributions of normal machine sound when individual \nmachines have different usages. However, the trained \nmodel only learns how to reconstruct the general distri -\nbution of different normal machines sounds.\nTo enable the model to learn different representations \nfor different machine sounds even with the same type, we \nintroduce an ID classifier C(·) with machine ID informa -\ntion to constrain the latent feature z of AE. The structure \nof the ID classifier C(·) is given in Fig.  3, which consists \nof a max pooling layer, two linear layers with a ReLU [45] \nfunction and a softmax activation function.\nHere, the latent feature z is the output of the encoder \nof AE, which is the input of the classifier, defined \nas z = E(X + F(/Phi1)) . The output of the ID classifier \nˆl = C(z ) ∈ RK is the probability indicating normal/\nanomalous sound corresponding to the machine ID, and \nK is the number of machines with the same type. Then, \nthe classification error of C(·) can be obtained via a cross-\nentropy loss function [46]\nwhere l ∈ RK is the one-hot vector of machine ID label of \nthe sound signal.\nTherefore, the proposed IDC-TransAE can be jointly \ntrained by minimizing the center frame reconstruction \n(9)Lc = CrossEntropy(l, ˆl),\nFig. 3 The architecture of the proposed IDC-TransAE for normal sound reconstruction. X and /Phi1 are the inputs to the model, which are obtained \nfrom the sound signal by removing the center frame. The final predicted center frame is obtained by average pooling of the output of the decoder \nX in frames and a linear layer, and ˆl  is the predicted machine ID probability of sound signal, which is obtained by max pooling of output of encoder \nz in frames and two linear layers with softmax. IDC-TransAE is optimized by the combination of reconstruction error and classification error\nPage 6 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nerror and the machine ID classification error with the joint \nloss function\nwhere α ∈[ 0, 1) is a hyper-parameter. The magnitude of \nα denotes the extent to which the machine ID classifier \nrestricts z . By jointly training the AE with the ID classi -\nfier, we can improve anomaly detection performance.\n3.2  Weighted anomaly score computation\nFor anomaly detection, the formula A(e)mean in Eq. (3 ) \nusually underestimates the anomaly scores of anomalous \naudio clips when the anomalous events only appear for a \nshort time. One solution is to use the maximal reconstruc-\ntion error as the anomaly score, i.e., max anomaly score \nA(e)max = max(e) , to highlight the anomalies of these \naudio clips. However, it is not robust to use the maximum \nvalue of e as the anomaly score of the whole audio clip, as \nit may overestimate the anomaly scores of some normal \naudio clips.\nTo improve the reliability of the calculated anomaly score, \nwe employ the global weighted rank pooling (GWRP) \nmethod to obtain weighted anomaly score, where GWRP is \na generalization of max and mean, which can highlight the \nanomaly score by setting different weights to reconstruc -\ntion error sequence e . For example, let ˆe = {ˆe1 , ...,ˆeI } be \nsorted by descending order of e , the GWRP anomaly score \ncan be calculated as\nwhere 0 ≤ r ≤ 1 is a hyper-parameter and \nZ (r) = ∑I\ni=1 ri−1 is a normalization term. When r = 0 , \nA(ˆe)gwrp degenerates to A(e)max , and when r = 1 , \nA(ˆe)gwrp becomes A(e)mean . It intends to assign larger \nweights to anomalous audio clips and lower weights to \nnormal audio clips, to generate high anomaly scores for \nthe anomalous events of short duration. In addition, the \nclassification error is combined with the reconstruction \nerror to calculate the anomaly score, to allow the anom -\naly score to increase if the ID classifier misclassifies the \nmachine ID. Finally, the weighted anomaly score can be \ncalculated as\nwhere β ∈[ 0, 1] is a parameter weighting the impact of a \nfalse prediction by the ID classifier on the anomaly score. \nFor clarity, the proposed IDC-TransAE with weighted \nanomaly score computation is denoted as IDC-TransAE-\nW in the following section.\n(10)L total = (1 − α)L r + α L c,\n(11)A(ˆe)gwrp = 1\nZ (r)\nI∑\ni=1\nri−1 ˆei,\n(12)A(ˆe,l,ˆl) = (1 − β) A(ˆe)gwrp + β Lc,\n4  Experiments and results\n4.1  Experimental setup\n4.1.1  Dataset\nWe evaluate our method on the DCASE 2020 Challenge \nTask2 [12] dataset, which comprises parts of MIMII \n[47] and ToyADMOS dataset [48] including the nor -\nmal/anomalous operating sounds of six types of real/\ntoy machines. The MIMII dataset includes four types of \nmachines (i.e., Fan, Pump, Slider, and Valve), with four \ndifferent machines for each machine type. The ToyAD -\nMOS dataset consists of two types of machines (i.e., \nToyCar and ToyConveyor), with four and three different \nmachines for each type, respectively. Each recording is \na single-channel audio of 10-s long with a 16-kHz sam -\npling rate that includes both a target machine’s operat -\ning sound and environmental noise. Following [12], the \ntraining set only includes normal sounds, with around \n6000 items for each machine type, and the test set con -\nsists of both normal and anomalous sounds, including \nabout 500 to 1000 items for normal and anomaly in each \nmachine type.\n4.1.2  Performance metrics\nFollowing [12, 20, 29, 37, 49], we employ area under the \nreceiver operating characteristic (ROC) curve (AUC) \nand the partial-AUC (pAUC) as the performance met -\nrics, where the pAUC is calculated as the AUC over a \nlow false-positive-rate (FPR) range [0,  p] and p = 0.1 as \nin [12]. Higher AUC indicates better model performance. \npAUC reflects the reliability of the ASD system based on \npractical requirements. It is important to increase pAUC \nto avoid the ASD system predicting false alerts frequently \n[12]. In addition, the minimum AUC (mAUC) is adopted \nto represent the worst detection performance achieved \namong individual machines of same machine type, fol -\nlowing [37].\n4.1.3  Implementation details\nThe implementation details of IDC-TransAE can be \nseen in Table  1. We use the log-Mel spectrogram and \nphase angle of the sound signal as the input of our \nIDC-TransAE. The frame size is 1024 with an overlap -\nping 50%, i.e., the number of FFT bins (n_FFT) is 1024, \nand the hop length is 512. The number of Mel filter \nbanks (n_Mels) is set as 128. The number of frames \n(i.e., N) is 5. The dimension of phase angles is 513, \nwhich is embedded to a 128-dimensional vector by \nthe linear function F(·) . Here, F(·) consists of two lin -\near layers with batch normalization. The encoder and \ndecoder of IDC-TransAE include two layers, respec -\ntively. The classifier includes a max pooling layer, two \nlinear layers with a ReLU and a softmax activation \nPage 7 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nfunction. The hyper-parameter α of the joint loss func -\ntion is empirically set as 0.3.\nAdam optimizer [50] is used to optimize our model \nwith a learning rate of 0.0001. For each machine type, \nour model is trained 300 epochs, and the batch size is \nset as 2000. In the joint training stage, we found that \nthe classification loss converges much faster than the \nreconstruction loss, so we adopt a training strategy to \navoid the overfitting of the classifier, by training the \nclassifier every 10 epochs (i.e., using L total loss) and the \nremaining epochs for autoencoder (i.e., using Lr loss). \nIn weighted anomaly score computation, r  and β are \nempirically selected, and the values of r  and β are pro -\nvided in Table 1 .\n4.2  Experimental results and performance analysis\n4.2.1  Comparison with other methods\nTo demonstrate the performance of our method for \nunsupervised ASD, we compare our approach with the \nAE baseline of DCASE 2020 Challenge Task2 [12] and \nmainstream models, including AE-based methods (i.e., \nIDNN [20], ANP-Boot [49], Group MADE [24], and \nIDCAE [ 27]) and self-supervised based methods (i.e., \nMobileNetV2 [29] and Glow_Aff [37]), where IDCAE, \nMobileNetV2 and Glow_Aff employ the ID information \nfor anomalous sound detection.\nTable  2 shows the comparison results in terms of \nAUC and pAUC. Here, IDC-TransAE-W and IDC-\nTransAE-mean represent IDC-TransAE with weighted \nanomaly score computation and mean anomaly score \ncomputation, respectively. In addition, the proposed \nmethods without using ID information are evaluated, i.e., \nTransAE-W and TransAE-mean.\nAs shown in Table 2, the methods with ID information \n(denoted as w/ ID) give better detection performance \nthan the methods without ID information (denoted as \nw/o ID), except IDCAE. The proposed IDC-TransAE-W \nperforms the best in terms of average AUC and pAUC. \nAmongst the methods without using ID information, \nour TransAE-W also achieves the best overall perfor -\nmance. Especially, both TransAE-W and IDC-TransAE-\nW can substantially improve the performance on the \nTable 1 Implementation details for all machine types\nFan Pump Slider Valve ToyCar ToyConveyor\nn_FFT 1024\nn_Mels 128\nHop length 512\nFrames 5\nα 0.3\nr 1.00 1.00 0.96 0.92 1.00 1.00\nβ 0.84 0.82 0.80 0.72 0.62 0.98\nTable 2 Performance comparison in terms of AUC (%) and pAUC (%) for different types of machines\nFan Pump Slider Valve ToyCar ToyConveyor Average\nAUC pAUC AUC pAUC AUC pAUC AUC pAUC AUC pAUC AUC pAUC AUC pAUC \nw/o ID information\nAE baseline [12] 65.91 51.93 70.20 61.69 83.42 65.72 67.78 51.67 78.77 67.58 72.53 60.43 73.10 59.84\nIDNN [20] 65.94 52.48 74.26 62.20 84.34 65.48 83.70 62.02 77.42 62.64 69.36 58.58 75.67 60.57\nANP-Boot [21] 64.80 53.00 65.50 59.00 94.90 83.10 85.20 72.00 72.90 68.10 67.10 54.20 75.07 64.90\nGroup MADE [24] 68.00 53.10 74.10 66.20 94.40 83.70 95.60 85.50 79.50 68.40 74.70 60.30 81.05 69.53\nTransAE-mean 73.91 54.14 77.31 68.96 91.51 74.66 96.09 84.65 80.62 72.65 74.32 59.80 82.29 69.14\nTransAE-W 73.91 54.14 77.31 68.96 94.52 82.33 99.68 98.31 80.62 72.65 74.32 59.80 83.39 72.70\nw/ ID information\nMobileNetV2 [29] 80.19 74.40 82.53 76.50 95.27 85.22 88.65 87.98 87.66 85.92 69.71 56.43 84.34 77.74\nGlow_Aff [37] 74.90 65.30 83.40 73.80 94.60 82.80 91.40 75.00 92.20 84.10 71.50 59.00 85.20 73.90\nIDCAE [27] 77.45 70.32 77.29 70.33 80.04 68.25 78.26 55.80 78.07 74.22 70.29 59.46 76.90 66.40\nIDC-TransAE-mean 80.44 70.21 83.41 79.24 92.17 77.10 94.04 78.94 93.17 87.43 75.69 62.96 86.49 75.98\nIDC-TransAE-W 80.44 70.21 83.41 79.24 96.20 86.38 99.60 98.29 93.40 87.43 75.69 62.96 88.12 80.94\nPage 8 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nnon-stationary sound signal of Valve (i.e., with 13.66% \nand 19.35% pAUC improvements compared to TransAE-\nmean and IDC-TransAE-mean, respectively), which \ndemonstrates the effectiveness of the weighted anomaly \nscore computation for anomalous events appearing for a \nshort time. In addition, the significantly improved aver -\nage pAUC (i.e., 80.94%) shows that the proposed IDC-\nTransAE-W is more reliable than other methods.\nNote that  r in the weighted anomaly score compu -\ntation can be adjusted according to the time length \nof the anomalous event, for example, when r = 1 , \nA( ˆe)gwrp = A(e )mean . This means it is more applicable \nthan mean anomaly score. The influence of r will be dis -\ncussed in Section 4.4\n4.2.2  Detection stability\nTo demonstrate the effectiveness of our method for \nmore stable detection, another experiment is conducted \nto show the worst detection performance on individual \nmachines of the same type, where the self-supervised \nbased methods (i.e., MobileNetV2 and Glow_Aff) and \nthe typical AE-based method (i.e., IDNN) are employed \nfor comparison. The results in terms of mAUC are given \nin Table 3.\nAs can be seen from Tables 2 and 3, the self-supervised \nmethods, i.e., MoblieNetV2 and Glow_Aff, can achieve \nsignificant improvements in average AUC and pAUC, as \ncompared to the AE-based method IDNN. However, they \nperform dramatically different even for the machines \nof the same type, as observed from Tables  2 and 3, e.g., \nMobileNetV2 has much smaller mAUC than AUC on \nFan, Pump, ToyCar, and ToyConveyor. The results dem -\nonstrate the instability of the self-supervised methods.\nEspecially, the average mAUC (i.e., 59.73%) of Mobile -\nNetV2 is lower than that of IDNN (i.e., 64.46%), which \nindicates that the self-supervised classification method \n(i.e., MobileNetV2) indeed easily fails on some individual \nmachines and lacks performance consistency. In contrast, \nthe AE-based method IDNN can provide a relatively \nstable detection performance. Although the flow-based \nself-supervised method (Glow_Aff) can improve detec -\ntion stability to some extent compared to the AE-based \nmethod, our proposed method can achieve the best aver -\nage mAUC performance and obtain more stable perfor -\nmance for some machine types, i.e., Slider, Valve, and \nToyCar.\nAlthough Glow_Aff has a higher mAUC on Pump than \nour proposed method, the model needs to be trained for \neach individual machine which could be limited in real-\nworld applications. In contrast, our proposed method \nonly needs to train one model for each machine type.\n4.2.3  Generalization to anomaly\nTo demonstrate the proposed IDC-TransAE can miti -\ngate the generalization of AE for anomalous sound and \nimprove its detection performance, experiments are con -\nducted to compare it with the typical AE-based method \n(i.e., IDNN).\nFirst, we show the histograms of anomaly score dis -\ntribution on Slider, Valve, and ToyCar using IDNN and \nour proposed IDC-TransAE. For a fair comparison, our \nmethod (i.e., IDC-TransAE-mean) also adopts mean \nanomaly score computation as IDNN, and the results are \nprovided in Fig. 4. Here, the anomaly score is on the hori-\nzontal axis of the histogram, which is normalized to facil-\nitate comparison. The vertical axis represents the number \nof audio samples corresponding to the anomaly score dis-\ntribution on the histogram.\nFrom Fig.  4, we can see that, for IDNN, the anomaly \nscore distribution of the anomalous sound tends to be \nsimilar to that of the normal sound, especially on ToyCar, \nas shown in Fig.  4a. It shows that most anomalous sound \nhave a small anomaly score similar to normal sound. This \nindicates that the AE-based method (i.e., IDNN) is able \nto generalize the representation for anomalous sound, \nwhich reduces its ability to distinguish between normal \nand abnormal sound. In contrast, our proposed method \ncan give higher anomaly scores for the anomalous sound, \nand provide better detection ability than the AE-based \nmethod, as shown in the histograms in Fig.  4b, which \ndemonstrates the effectiveness of our proposed IDC-\nTransAE architecture.\nTo further demonstrate that our proposed IDC-\nTransAE can mitigate its generalization for the anom -\naly, we perform another experiment for non-stationary \nanomalous sound detection (i.e., sound of Valve) as \ncompared with IDNN, where the log-Mel spectro -\ngram reconstruction of normal and anomalous sound \nis illustrated in Fig.  5. From left to right, Fig.  5 shows \nthe original log-Mel spectrograms, the reconstructed \nlog-Mel spectrograms, and the absolute values of their \ndifference.\nTable 3 Performance comparison in terms mAUC (%) among \nthe individual machines of the same type\nMobileNetV2 \n[29]\nGlow_Aff \n[37]\nIDNN [20] IDC-\nTransAE-W\nFan 50.40 49.60 56.56 50.55\nPump 52.90 65.70 61.86 57.27\nSlider 82.80 87.80 74.22 88.64\nValve 67.90 77.70 66.83 99.24\nToyCar 55.70 80.10 64.41 81.35\nToyConveyor 48.70 61.00 62.89 62.31\nAverage 59.73 70.32 64.46 73.23\nPage 9 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nComparing the red box areas illustrated in Fig.  5a and \nb, the proposed IDC-TransAE can provide better normal \nsound reconstruction, as it can achieve smaller recon -\nstruction error for normal sound than that of the typi -\ncal AE-based method (i.e., IDNN). This can be clearly \nobserved in the comparison of the absolute value differ -\nence of original log-Mel spectrogram and reconstrcuted \nlog-Mel spectrogram, as the red box indicated areas in \nFig. 5a and b. Whereas for the anomalous sound recon -\nstruction, our proposed method can give larger recon -\nstruction error than the typical AE-based method, which \nmeans that our method has a better ability to high -\nlight the anomalies when reconstructing the anoma -\nlous sound. This can be observed from the comparison \nbetween the red box areas in Fig.  5c and d, where the \nabsolute value difference shown in Fig.  5d is much more \nclear than that in Fig. 5c. The results further demonstrate \nthat our proposed IDC-TransAE can solve the generaliza-\ntion problem of the AE-based method and has a better \nability in anomaly detection.\nNote that the log-Mel spectrogram of the anomalous \nsound also shows that the anomalies may appear for a \nshort time in the sound, as illustrated in Fig.  5. In this \ncase, the mean anomaly score computation method will \ngive low anomaly scores for the anomalous events that \nonly appear for a short time.\n4.3  Ablation studies\nTo show the effectiveness of different parts of our pro -\nposed IDC-TransAE-W, ablation studies are conducted, \nwhere AUC and pAUC are used as performance metric. \nThe results are given in Table  4. Here, TransAE/PE-W \ndenotes the proposed model without using machine ID \nconstraint (IDC) module and CFP module, and adopts PE \ninstead of LPE, with weighted anomaly score computa -\ntion for anomaly detection. TransAE/PE/CFP-W denotes \nthe TransAE/PE-W using CFP , and TransAE/LPE/\nCFP-W denotes replacing PE with LPE in Transformer/\nPE/CFP-W.\nAs shown in Table  4, TransAE/PE/CFP-W can signifi -\ncantly improve the detection performance for the non-\nstationary sound signal of Valve, with 12.61% AUC and \n30.40% pAUC improvements as compared with TransAE/\nPE-W. To show the effectiveness of LPE, we compare the \nperformance of TransAE/PE/CFP-W and TransAE/LPE/\nCFP-W. The result shows that TransAE/LPE/CFP-W \ncan improve the detection performance on Fan, Slider, \nValve, and ToyConveyor and achieve better average AUC \nFig. 4 The histograms of anomaly scores distribution on Slider, Valve, and ToyCar using IDNN and the proposed IDC-TransAE-mean, where blue \nand orange indicate the anomaly score distribution of normal and anomalous sound, respectively\nPage 10 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nFig. 5 The log-Mel spectrogram reconstruction analysis of IDNN and IDC-TransAE on normal and anomalous Valve’s sound, the “original,” \n“reconstruction,” and “original-reconstruction” represent original spectrogram, reconstructed spectrogram and the absolute value of their difference, \nrespectively\nTable 4 Validation of different modules of IDC-TransAE\nTransAE/PE-W TransAE/PE/CFP-W TransAE/LPE/CFP-W IDC-TransAE-W\nAUC pAUC AUC pAUC AUC pAUC AUC pAUC \nFan 70.06 52.73 72.46 52.95 73.91 54.14 80.44 70.21\nPump 79.76 70.79 77.66 71.30 77.31 68.96 83.41 79.24\nValve 92.88 81.05 94.12 82.16 94.52 82.33 96.20 86.38\nSlider 85.10 62.91 97.71 93.31 99.68 98.31 99.60 98.29\nToyCar 82.95 72.47 81.27 72.69 80.62 72.65 93.40 87.43\nToyConveyor 76.81 63.88 74.35 58.51 74.32 59.80 75.69 62.96\nAverage 81.26 67.31 82.93 71.82 83.39 72.70 88.12 80.94\nPage 11 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nand pAUC performance. It indicates that LPE can better \nrepresent the temporal information of the sound signal \nby using its phase information. By introducing the ID \nclassifier, the proposed IDC-TransAE-W with the IDC \nmodule can achieve the best overall detection perfor -\nmance, giving more than 10% improvement in pAUC on \nFan, Pump, and ToyCar as compared with TransAE/LPE/\nCFP-W. Besides, we can see that the proposed IDC mod -\nule contributes the most to the performance improve -\nment in Table  4, which further verifies the effectiveness \nof the proposed IDC module to enhance the ability of the \nmodel in distinguishing anomalous sound.\nTo further demonstrate the effectiveness of the IDC \nmodule, we compare the performance of IDC-TransAE-\nW and TransAE/LPE/CFP-W in terms of AUC and \npAUC on four different machines of the machine type \nFan. The result is illustrated in Fig.  6. From Fig. 6, we can \nsee that IDC-TransAE-W can significantly improve the \nperformance on ID_02, ID_04, and ID_06, as compared \nwith TransAE/LPE/CFP-W. This means the IDC method \ncan better distinguish the anomalous sound for differ -\nent machines with the same type. The results in Table  4 \nand Fig. 6 verify the effectiveness of different modules of \nour proposed method. To further illustrate the effective -\nness of each module, we give the visualization analysis for \neach module in the following Section 4.4.\n4.4  Visualization analysis\nIn this section, visualization analysis is provided for bet -\nter understanding the experimental results in the abla -\ntion studies. Specifically, the effectiveness of CFP , LPE \nmodule and IDC module in our proposed IDC-TransAE \nmethod are further evaluated. Besides, the influence of \nthe parameter in the GWRP operation of anomaly score \ncalculation is also explored in this section.\n4.4.1  Effectiveness of CFP\nTo show how CFP operation affects the anomaly detec -\ntion for non-stationary sound signals, we compare \nthe histograms of anomaly score distribution between \nTransAE/PE-W and TransAE/PE/CFP-W on Valve. The \nresult is given in Fig.  7. Same as Fig. 4, the anomaly score \nis also normalized to facilitate comparison. By comparing \nFig. 7a and b, we can see that the distribution of normal \nsound samples is on a smaller range of anomaly scores \nwhen adopting the CFP module (i.e., TransAE/PE/CFP-\nW), as illustrated in Fig. 7b. It verifies that CFP operation \ncan improve the reconstruction of non-stationary signals \nas described in [20]. Therefore, it can improve the perfor-\nmance of our proposed method for anomaly detection of \nnon-stationary sound signals.\n4.4.2  Visualization of linear phase embedding\nTo show why the LPE module can enhance the ability \nof the model for anomalous sound detection, we visual -\nize the encoding result of five consecutive input sound \nsignals of TransAE/LPE/CFP , and compare it with the \nencoding result of positional encoding for TransAE/PE/\nCFP , as illustrated in Fig. 8. Here, f1 to f5 are the encod -\ning visualizations corresponding to the five consecu -\ntive input sound signals, respectively, where each input \nincludes four frames.\nFrom Fig. 8a, we can see that the positional encoding vis-\nualization of each input is the same because the positional \nFig. 6 Performance illustration for 4 different machines with the same type, i.e., Fan\nPage 12 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nencoding operation adopts the same cosine representa -\ntion for signal encoding. In contrast, by linearly encoding \nthe phase information of the signal, our proposed LPE can \npreserve the signal’s own temporal information and give \ndifferent encoding representations for each different input \nsignal, as indicated in the red box in Fig.  8b. Therefore, \nour proposed method can learn better latent features with \nunique characteristics from each signal, and enhance the \nability of the model for anomalous sound detection.\n4.4.3  Validation of IDC module\nWe show the t-distributed stochastic neighbor embed -\nding (t-SNE) cluster visualization of the latent features \nto validate the IDC module further. The experiment is \nconducted on the test dataset of the machine type Toy -\nCar, where the proposed method IDC-TransAE with -\nout using ID information (i.e., TransAE/LPE/CFP) is \nemployed for comparison. The result is illustrated in \nFig. 9 .\nFig. 7 The comparison between TransAE/PE-W and TransAE/PE/CFP-W on histograms of anomaly scores distribution of Valve\nFig. 8 Encoding visualization of five consecutive input sound signals using positional encoding (PE) and linear phase embedding (LPE), respectively\nPage 13 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nAs observed from Fig.  9a, the latent features of normal \nand anomalous sound samples from different machines \noverlap with each other when using the method with -\nout IDC module (i.e., TransAE/LPE/CFP). In addition, \nthe latent features of the normal sound samples of one \nmachine may be close to that of the anomalous samples \nfrom other machines, rather than the normal samples \nfrom the same machine, as illustrated in Fig.  9a. It results \nin the latent features of some anomalous samples from \none machine on the manifold of the normal samples from \nanother machine. Thereby these anomalous sounds will \nbe well reconstructed, making it hard to distinguish the \nanomalies and reducing the detection performance. By \nintroducing the IDC module to constrain the latent fea -\nture, the proposed method can reduce the generalization \nof AE for anomalous sound and further improve its dis -\ntinguishing ability that the normal and anomalous latent \nfeatures are well separated, as illustrated in Fig. 9b.\n4.4.4  Influence of parameter r for anomaly detection\nAs mentioned in Section  3.2, we introduce the weighted \nanomaly score computation to highlight the anomalous \nevents that only appear for a short time. The parameter \nr in Eq. (11) will decide the way for anomaly score com -\nputation, i.e., weighted anomaly score computation will \ndegenerate to max anomaly score computation when \nr = 0 , and it will become mean anomaly score compu -\ntation when r = 1 . Therefore, we also carry out another \nexperiment to show the impact of parameter r  on the \nperformance of our proposed IDC-TransAE for anomaly \ndetection. Here, different values of r  from 0 ≤ r ≤ 1 with \nan interval of 0.05 are selected to evaluate the perfor -\nmance of our proposed method in terms AUC and pAUC \non all six machine types. The result is shown in Fig. 10.\nFrom Fig.  10, we can see that the mean score compu -\ntation (i.e., r = 1 ) can achieve the best performance for \nthe machine types of Fan, Pump, ToyCar, and ToyCon -\nveyor. However, it obtains the worst performance for \nthe machine types of Slider and Valve, where the anom -\nalous sound often occurs in a short time. Though using \nmax anomaly score computation ( r = 0 ) can achieve \nbetter performance than adopting mean anomaly score \ncomputation on Slider and Valve, the weighted anom -\naly score computation method can provide the best \nperformance for the machine type of Slider and Valve. \nEspecially, the weighted anomaly score computation \nmethod can significantly improve the pAUC perfor -\nmance over the mean and max anomaly score com -\nputation on Slider and Valve. The result verifies the \neffectiveness of weighted anomaly score computation \nfor the anomalous sound that appears over short time. \nIn addition, the values of r  can be adjusted according to \ndifferent machine types, which makes it more applica -\nble than mean anomaly score and max anomaly score \ncomputation.\n5  Conclusions\nIn this paper, we have presented an IDC-TransAE \narchitecture with weighted anomaly score computa -\ntion for unsupervised ASD, where an ID classifier was \nintroduced to mitigate the generalization of AE for \nanomalous sound and enhance the distinguishing abil -\nity for different machines with the same type. In addi -\ntion, center frame prediction was utilized to improve \nthe reconstruction of the non-stationary sound signal, \nand a linear phase embedding strategy was applied to \npreserve the signal’s temporal information and further \nimprove its distinguishing ability for anomalous sound \nFig. 9 The t-SNE visualization of latent feature on the test dataset for the machine type ToyCar using TransAE/LPE/CFP and IDC-TransAE. Different \ncolor represents different machine ID. The “ • ” and “ × ” denote normal and anomalous samples, respectively\nPage 14 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \ndetection. Moreover, a weighted anomaly score compu -\ntation method was introduced to highlight the anom -\naly scores for anomalous events that only appear for a \nshort time. The experiments demonstrate the effective -\nness and superiority of our proposed method, as com -\npared with the baseline methods.\nAcknowledgements\nThe authors would like to thank the Associate Editor and the anonymous \nreviewers for reviewing the manuscript.\nAuthors’ contributions\nJ. Guan: Conceptualization, Methodology, Writing-Original Manuscript, Fund-\ning Acquisition, and Supervision; Y. Liu: Methodology, Experimental Validation, \nand Data Analysis; Q. Kong: Conceptualization, Methodology, and Writing - \nRevision & Review; F. Xiao: Experimental Validation and Data Analysis; Q. Zhu: \nWriting - Revision & Review; J. Tian: Experimental Validation and Data Analysis; \nW. Wang: Writing - Revision & Review.\nFunding\nThis work was partly supported by the Natural Science Foundation of Hei-\nlongjiang Province under Grant No. YQ2020F010, and a GHfund with Grant No. \n202302026860.\nAvailability of data and materials\nThe datasets for experimental evaluation in this study are from the DCASE \n2020 challenge Task 2, which are available on the internet.\nDeclarations\nCompeting interests\nW. Wang is an editorial board member of EURASIP Journal on Audio Speech \nand Music Processing and also a guest editor of the special issue “AI for Com-\nputational Audition: Sound and Music Processing” , other authors declare that \nthey have no competing interests.\nReceived: 3 June 2023   Accepted: 29 September 2023\nReferences\n 1. V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey. ACM \nComput. Surv. (CSUR) 41(3), 1–58 (2009)\n 2. Y. Koizumi, S. Saito, H. Uematsu, Y. Kawachi, N. Harada, Unsupervised \ndetection of anomalous sound based on deep learning and the neyman-\npearson lemma. IEEE/ACM Trans. Audio Speech Lang. Process. 27(1), \n212–224 (2018)\n 3. R. Chalapathy, S. Chawla, Deep learning for anomaly detection: A survey. \narXiv preprint arXiv: 1901. 03407 (2019)\n 4. E.C. Nunes, Anomalous sound detection with machine learning: A sys-\ntematic review. arXiv preprint arXiv: 2102. 07820 (2021)\n 5. J. Guan, Y. Liu, Q. Zhu, T. Zheng, J. Han, W. Wang, in Proceedings of Interna-\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP), Time-\nweighted frequency domain audio representation with GMM estimator \nfor anomalous sound detection (IEEE, 2023), pp. 1–5\n 6. P . Foggia, N. Petkov, A. Saggese, N. Strisciuglio, M. Vento, Audio surveil-\nlance of roads: A system for detecting anomalous sounds. IEEE Trans. \nIntell. Transp. Syst. 17(1), 279–288 (2015)\n 7. Y. Li, X. Li, Y. Zhang, M. Liu, W. Wang, Anomalous sound detection using \ndeep audio representation and a BLSTM network for audio surveillance of \nroads. IEEE Access 6, 58043–58055 (2018)\n 8. Y. Chung, S. Oh, J. Lee, D. Park, H.H. Chang, S. Kim, Automatic detection \nand recognition of pig wasting diseases using sound data in audio \nsurveillance systems. Sensors 13(10), 12929–12942 (2013)\n 9. D. Henze, K. Gorishti, B. Bruegge, J.P . Simen, in Proceedings of International \nConference On Machine Learning And Applications (ICMLA), AudioForesight: \nA process model for audio predictive maintenance in industrial environ-\nments (IEEE, 2019), pp. 352–357\n 10. D.Y. Oh, I.D. Yun, Residual error based anomaly detection using auto-\nencoder in SMD machine sound. Sensors 18(5), 1308–1321 (2018)\n 11. Y. Park, I.D. Yun, Fast adaptive RNN encoder-decoder for anomaly detec-\ntion in SMD assembly machine. Sensors 18(10), 3573–3583 (2018)\n 12. Y. Koizumi, Y. Kawaguchi, K. Imoto, T. Nakamura, Y. Nikaido, R. Tanabe, \nH. Purohit, K. Suefusa, T. Endo, M. Yasuda, N. Harada, in Proceedings of \nFig. 10 Performance of our proposed IDC-TransAE in terms of AUC and pAUC under different r values for all six machine types\nPage 15 of 16\nGuan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \n \nDetection and Classification of Acoustic Scenes and Events (DCASE) Work-\nshop, Description and discussion on DCASE2020 challenge task2: Unsu-\npervised anomalous sound detection for machine condition monitoring \n(Tokyo, 2020), pp. 81–85\n 13. Y. Kawaguchi, K. Imoto, Y. Koizumi, N. Harada, D. Niizumi, K. Dohi, R. Tan-\nabe, H. Purohit, T. Endo, in Proceedings of Detection and Classification of \nAcoustic Scenes and Events (DCASE) Workshop, Description and discussion \non DCASE2021 challenge task 2: Unsupervised anomalous detection for \nmachine condition monitoring under domain shifted conditions (Barce-\nlona, 2021), pp. 186–190\n 14. K. Dohi, K. Imoto, N. Harada, D. Niizumi, Y. Koizumi, T. Nishida, H. Purohit, \nR. Tanabe, T. Endo, M. Yamamoto, Y. Kawaguchi, in Proceedings of Detection \nand Classification of Acoustic Scenes and Events (DCASE) Workshop, Descrip-\ntion and discussion on DCASE2022 challenge task 2: Unsupervised \nanomalous sound detection for machine condition monitoring applying \ndomain generalization techniques (Nancy, 2022)\n 15. K. Dohi, K. Imoto, N. Harada, D. Niizumi, Y. Koizumi, T. Nishida, H. Purohit, \nR. Tanabe, T. Endo, Y. Kawaguchi, Description and discussion on DCASE \n2023 challenge task 2: First-shot unsupervised anomalous sound detec-\ntion for machine condition monitoring. In arXiv preprint arXiv:  2305. \n07828 (2023)\n 16. M. Zabihi, A.B. Rad, S. Kiranyaz, M. Gabbouj, A.K. Katsaggelos, in Proceed-\nings of Computing in Cardiology Conference (CinC), Heart sound anomaly \nand quality detection using ensemble of neural networks without \nsegmentation (IEEE, Vancouver, 2016), p. 613–616\n 17. T. Tagawa, Y. Tadokoro, T. Yairi, in Proceedings of Asian Conference on \nMachine Learning (ACML), Structured denoising autoencoder for fault \ndetection and analysis (PMLR, Nha Trang City, 2015), p. 96–111\n 18. E. Marchi, F. Vesperini, F. Eyben, S. Squartini, B. Schuller, in Proceedings \nof International Conference on Acoustics, Speech, and Signal Processing \n(ICASSP), A novel approach for automatic acoustic novelty detection \nusing a denoising autoencoder with bidirectional LSTM neural networks \n(IEEE, 2015), pp. 1996–2000\n 19. E. Marchi, F. Vesperini, F. Weninger, F. Eyben, S. Squartini, B. Schuller, in \nProceedings of International Joint Conference on Neural Networks (IJCNN), \nNon-linear prediction with LSTM recurrent neural networks for acoustic \nnovelty detection (IEEE, 2015), pp. 1–7\n 20. K. Suefusa, T. Nishida, H. Purohit, R. Tanabe, T. Endo, Y. Kawaguchi, in \nProceedings of International Conference on Acoustics, Speech and Signal \nProcessing (ICASSP), Anomalous sound detection based on interpolation \ndeep neural network (IEEE, 2020), pp. 271–275\n 21. G. Wichern, A. Chakrabarty, Z.Q. Wang, J. Le Roux, in Proceedings of Work-\nshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), \nAnomalous sound detection using attentive neural processes (IEEE, \n2021), pp. 186–190\n 22. H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, \nY.W. Teh, Attentive neural processes. arXiv preprint arXiv: 1901. 05761 (2019)\n 23. H. Van Truong, N.C. Hieu, P .N. Giao, N.X. Phong, Unsupervised detection \nof anomalous sound for machine condition monitoring using fully con-\nnected U-Net. J. ICT Res. Appl. 15(1), 41–55 (2021)\n 24. R. Giri, F. Cheng, K. Helwani, S.V. Tenneti, U. Isik, A. Krishnaswamy, in \nProceedings of Detection and Classification of Acoustic Scenes and Events \n(DCASE) Workshop, Group masked autoencoder based density estimator \nfor audio anomaly detection (Tokyo, 2020), p. 51–55\n 25. R. Giri, S.V. Tenneti, K. Helwani, F. Cheng, U. Isik, A. Krishnaswamy, Unsu-\npervised anomalous sound detection using self-supervised classification \nand group masked autoencoder for density estimation. Technical report, \nDCASE2020 Challenge (2020)\n 26. V. Zavrtanik, M. Kristan, D. Skočaj, in Proceedings of International Confer-\nence on Computer Vision (ICCV), DRAEM - A discriminatively trained \nreconstruction embedding for surface anomaly detection (IEEE, Canada, \n2021), p. 8330–8339\n 27. S. Kapka, ID-conditioned auto-encoder for unsupervised anomaly detec-\ntion. arXiv preprint arXiv: 2007. 05314 (2020)\n 28. I. Kuroyanagi, T. Hayashi, Y. Adachi, T. Yoshimura, K. Takeda, T. Toda, in \nProceedings of Detection and Classification of Acoustic Scenes and Events \n(DCASE) Workshop, An ensemble approach to anomalous sound detec-\ntion based on Conformer-based autoencoder and binary classifier \nincorporated with metric learning (Barcelona, 2021), pp. 110–114\n 29. R. Giri, S.V. Tenneti, F. Cheng, K. Helwani, U. Isik, A. Krishnaswamy, in \nProceedings of Detection and Classification of Acoustic Scenes and Events \n(DCASE) Workshop, Self-supervised classification for detecting anomalous \nsounds (Tokyo, 2020), p. 46–50\n 30. K. Wilkinghoff, in Proceedings of Detection and Classification of Acoustic \nScenes and Events (DCASE) Workshop, Combining multiple distributions \nbased on sub-cluster adacos for anomalous sound detection under \ndomain shifted conditions (Barcelona, 2021), pp. 55–59\n 31. S. Venkatesh, G. Wichern, A. Subramanian, J. Le Roux, in Proceedings \nof Detection and Classification of Acoustic Scenes and Events (DCASE) \nWorkshop, Improved domain generalization via disentangled multi-task \nlearning in unsupervised anomalous sound detection (Nancy, 2022)\n 32. Y. Liu, J. Guan, Q. Zhu, W. Wang, in Proceedings of International Conference \non Acoustics, Speech and Signal Processing (ICASSP), Anomalous sound \ndetection using spectral-temporal information fusion (IEEE, 2022), pp. \n816–820\n 33. J. Guan, F. Xiao, Y. Liu, Q. Zhu, W. Wang, in Proceedings of International \nConference on Acoustics, Speech and Signal Processing (ICASSP), Anomalous \nsound detection using audio representation with machine ID based \ncontrastive learning pretraining (IEEE, 2023), pp. 1–5\n 34. Z. Hejing, G. Jian, Z. Qiaoxi, X. Feiyang, L. Youde, in Proceedings of \nINTERSPEECH, Anomalous sound detection using self-attention-based \nfrequency pattern analysis of machine sounds (ISCA, Dublin, 2023), p. \n336–340\n 35. F. Xiao, Y. Liu, Y. Wei, J. Guan, Q. Zhu, T. Zheng, J. Han, The DCASE2022 chal-\nlenge task 2 system: Anomalous sound detection with self-supervised \nattribute classification and GMM-based clustering. Technical report, \nDCASE2022 Challenge (2022)\n 36. Y. Wei, J. Guan, H. Lan, W. Wang, Anomalous sound detection system with \nself-challenge and metric evaluation for DCASE2022 challenge task 2. \nTechnical report, DCASE2022 Challenge (2022)\n 37. K. Dohi, T. Endo, H. Purohit, R. Tanabe, Y. Kawaguchi, in Proceedings of Inter-\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP), \nFlow-based self-supervised density estimation for anomalous sound \ndetection (IEEE, 2021), pp. 336–340\n 38. E.G. Tabak, C.V. Turner, A family of nonparametric density estimation \nalgorithms. Commun. Pur. Appl. Math. 66(2), 145–164 (2013)\n 39. L. Dinh, D. Krueger, Y. Bengio, Nice: Non-linear independent components \nestimation. arXiv preprint arXiv: 1410. 8516 (2014)\n 40. D.P . Kingma, P . Dhariwal, in Proceedings of Advances in Neural Informa-\ntion Processing Systems (NIPS), Glow: Generative flow with invertible 1x1 \nconvolutions (Curran Associates, Inc., Montréal, 2018)\n 41. G. Papamakarios, T. Pavlakou, I. Murray, in Proceedings of Advances in \nNeural Information Processing Systems (NIPS), Masked autoregressive flow \nfor density estimation (Curran Associates, Inc., Long Beach, 2017)\n 42. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, \nŁ. Kaiser, I. Polosukhin, in Proceedings of Advances in Neural Information \nProcessing Systems (NIPS), Attention is all you need (Curran Associates, \nInc., Long Beach, 2017)\n 43. A. Kolesnikov, C.H. Lampert, in Proceedings of European Conference on \nComputer Vision (ECCV), Seed, expand and constrain: Three principles \nfor weakly-supervised image segmentation (Springer, 2016), pp. \n695–711\n 44. Y. Koizumi, S. Saito, H. Uematsu, N. Harada, in Proceedings of European Sig-\nnal Processing Conference (EUSIPCO), Optimizing acoustic feature extractor \nfor anomalous sound detection based on Neyman-Pearson lemma (IEEE, \n2017), pp. 698–702\n 45. X. Glorot, A. Bordes, Y. Bengio, in Proceedings of International Conference \non Artificial Intelligence and Statistics (AISTATS), Deep sparse rectifier neural \nnetworks (PMLR, 2011), pp. 315–323\n 46. K.P . Murphy, Machine learning: A probabilistic perspective (MIT press, 2012)\n 47. H. Purohit, R. Tanabe, T. Ichige, T. Endo, Y. Nikaido, K. Suefusa, \nY. Kawaguchi, in Proceedings of Detection and Classification of Acoustic \nScenes and Events (DCASE) Workshop, MIMII Dataset: Sound dataset for \nmalfunctioning industrial machine investigation and inspection (New \nYork University, New York, 2019), p. 209–213\n 48. Y. Koizumi, S. Saito, H. Uematsu, N. Harada, K. Imoto, in Proceedings of \nWorkshop on Applications of Signal Processing to Audio and Acoustics \n(WASPAA), ToyADMOS: A dataset of miniature-machine operating sounds \nfor anomalous sound detection (IEEE, 2019), pp. 313–317\n 49. S. Perez-Castanos, J. Naranjo-Alcazar, P . Zuccarello, M. Cobos, Anoma-\nlous sound detection using unsupervised and semi-supervised \nPage 16 of 16Guan et al. EURASIP Journal on Audio, Speech, and Music Processing         (2023) 2023:42 \nautoencoders and gammatone audio representation. arXiv preprint \narXiv: 2006. 15321 (2020)\n 50. D.P . Kingma, J. Ba, Adam: A method for stochastic optimization. arXiv \npreprint arXiv: 1412. 6980 (2014)\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Autoencoder",
  "concepts": [
    {
      "name": "Autoencoder",
      "score": 0.8502194881439209
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6156322956085205
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5939198136329651
    },
    {
      "name": "Anomaly detection",
      "score": 0.5832960605621338
    },
    {
      "name": "Computer science",
      "score": 0.5621210932731628
    },
    {
      "name": "Computation",
      "score": 0.5593346953392029
    },
    {
      "name": "Transformer",
      "score": 0.5304970741271973
    },
    {
      "name": "Softmax function",
      "score": 0.4439311623573303
    },
    {
      "name": "Unsupervised learning",
      "score": 0.42689594626426697
    },
    {
      "name": "Speech recognition",
      "score": 0.3894374668598175
    },
    {
      "name": "Deep learning",
      "score": 0.16271355748176575
    },
    {
      "name": "Engineering",
      "score": 0.1251918077468872
    },
    {
      "name": "Algorithm",
      "score": 0.10149216651916504
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151727225",
      "name": "Harbin Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    }
  ],
  "cited_by": 12
}