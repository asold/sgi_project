{
  "title": "Large language models and academic writing: Five tiers of engagement",
  "url": "https://openalex.org/W4391319871",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2165453681",
      "name": "Martin Bekker",
      "affiliations": [
        "University of the Witwatersrand"
      ]
    },
    {
      "id": "https://openalex.org/A2165453681",
      "name": "Martin Bekker",
      "affiliations": [
        "University of the Witwatersrand"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2940680165",
    "https://openalex.org/W4365450342",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4321610465",
    "https://openalex.org/W4382361534",
    "https://openalex.org/W2499590469",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4380985699",
    "https://openalex.org/W3177828909"
  ],
  "abstract": "Against a backdrop of the rapidly expanding use of large language models (LLMs) across diverse domains, this discussion breaks LLM usage into tiers of use, offering practical guidance to cautiously embrace the benefits of this significant new tool.",
  "full_text": "1https://doi.org/10.17159/sajs.2024/17147\nVolume 120| Number 1/2\nJanuary/February 2024 \nCommentary\nSignificance:\nAgainst a backdrop of the rapidly expanding use of large language models (LLMs) across diverse domains, \nthis discussion breaks LLM usage into tiers of use, offering practical guidance to cautiously embrace the \nbenefits of this significant new tool.\nIntroduction\n2023 will be remembered as the year of large language models (LLMs), which, led by their brash poster child, \nChatGPT , have changed the world forever. LLM-assisted writing will indelibly alter many writing tasks, offering \nspeed and efficiency, and even automating-away many tasks.\nHowever, academic, scientific, and intellectual integrity are at risk: not only due to mistakes that may creep in via \nthe automation of writing, but also – and more importantly – owing to the loss of the ability to construct well-\ncrafted arguments, ostensibly through the dulling of scholars’ reasoning via the outsourcing of thinking that LLMs \ncould engender. Moreover, ethical principles around intellectual process and ownership ought to be protected \nagainst the vague accountability of black-box algorithms (termed ‘algorithmic opacity’ by Eslami et al. 1) with \nrespect to published or submitted work.2,3\nAcademic journals, along with university and high-school curricula developers and assessment setters, need \nimmediate yet thoughtful guidelines (rules and standards) for using LLMs and AI in the scientific process. In this \nCommentary, I propose a five-tier system that stipulates permissions and prohibitions around the use of LLMs in \nthe academic writing process. I recapitulate what has changed, what – amid all the apparent changes – has stayed \nthe same, and introduce the five tiers of LLM-assistance to academic writing, motivating the lines of distinction and \nsuggesting appropriate uses.\nWhat has changed?\nIn existence for over 40 years, language models are probabilistic models of a human language that can generate \nlikelihoods of a series of words, based on text corpora on which they have been trained. 4 Over the last decade, \nthe size of the training text corpora and the number of weights between concepts held within the models have \nincreased, necessitating affixing ‘large’ to recent models, now known as LLMs. 5 ChatGPT was released in \nNovember 2022, combining the then-most advanced LLM with a chatbot-interface, simplifying the prompting \n(requesting) and serving (receiving responses) process. With their promise of speed and efficiency, ChatGPT and \nother LLMs have had an immediate impact on the academe; demonstrating the ability to automate the writing of \nreports, research and literature papers, exams, and computer code, among others, to various degrees of human \nability, with each iteration showing improvement.\nContemporary LLMs represent a break with the past, based on (1) the speed and scale of information processing, (2) \nan unprecedented function of research process assistance (which includes research summary and data manufacture), \nand (3) the potential for the outsourcing of thought. The first of these three innovations often accompanies new \ntechnological tools. However, the scale and speed at which LLMs perform information processing tasks have now \nsurpassed the human performance of certain tasks within the so-called information economy6, suggesting a quantum \nleap in functioning and utility.\nThe second change is tied to the very nature of the way LLMs can process information. The ability of transformer \nmodels – the deep-learning architecture behind LLMs – to manipulate text (or language, or indeed anything that \ncan be represented as a language) has made LLMs superlative at summarising texts, style transfers (ranging from \ntranslation to mere tone tweaks) and spelling and grammar corrections. Moreover, in addition to LLMs, several \nother AI-related tools that assist in the research process have recently been introduced (with many more to follow \nin their wake). With these, two particular functions spring to mind: first, research summarising tools (e.g. Elicit, \nPerplexity and Consensus) that can ‘find’ work (published but unknown to the scholar), ‘understand’ discourses, \nidentify research gaps, and assist with literature reviews. The second are those that can manufacture artificial \n(or synthetic) data. Here, a scholar might give instructions regarding what a data set should contain, and, in the \nabsence of this being available (as secondary data) or impossible to gather (for, say, ethical reasons), such data \ncan be ‘created’ instantly.\nThe third change relates to the potential for the wholesale contracting out, or ‘outsourcing’ of thought to a (non-\nhuman) algorithm. While cheating is nothing new – academics have long been aware of student essays drafted by \n‘essay mills’, computer code written by friends or copied from online repositories, or even a human double sitting \nfor an exam on behalf of a less-prepared student – LLMs present the academe with a new level of concern. That is, \nfrom students to scientists, writers are now able to turn to LLMs to author academic work from conceptualisation \nthrough research and writing7, putting in peril the principle of scientific advancement through human reasoning (or \nsound thinking as articulated through writing).\nAuthor:\nMartin Bekker1  \nAFFILIAtIoN:\n1School of Electrical and Information \nEngineering, University of the \nWitwatersrand, Johannesburg, South \nAfrica\nCorrESPoNDENCE to:\nMartin Bekker \nEMAIL:\nmartin.bekker@wits.ac.za\nhoW to CItE:\nBekker M. Large language models \nand academic writing: Five tiers \nof engagement. S Afr J Sci. \n2024;120(1/2), Art. #17147. https:// \ndoi.org/10.17159/sajs.2024/17147\nArtICLE INCLuDES:\n\t☐ \tPeer review\n\t☐ \tSupplementary material\nKEYWorDS:\nlarge language models, academic \nintegrity, academic writing, \nauthorship, transparency\nPubLIShED: \n30 January 2024\nLarge language models and academic writing: Five \ntiers of engagement\n© 2024. The Author(s). Published \nunder a Creative Commons \nAttribution Licence.\nVolume 120| Number 1/2\nJanuary/February 2024 2https://doi.org/10.17159/sajs.2024/17147\nCommentary\nLarge language models and academic writing\nPage 2 of 5\nPerils notwithstanding, the immediate benefit of such a tool – one that \ncan fix the register, grammar, and punctuation of a text in seconds \nand apparently at no cost to the user – is immediate and clear. This \nis especially the case for those writing in a second language, which \ncase applies to the majority of academic scholars, who must publish \nin English. This ‘levelling of the playing field’ is to be welcomed \nby the academic community. 2,3,8 Academics often recite that good \nwriting is indistinguishable from good thinking; the corollary of this is \nthat clear, high-quality writing helps not only non-native speakers get \nthe recognition they deserve, but benefits humanity if readers access \nknowledge in clear, correct, and accessible language.\nThe immediate drawback accompanying this new class of technology \nis that, sadly, many things that the academy has long battled to \ncounter – dishonesty, cheating and plagiarism – have almost instantly \nbecome much harder to detect, owing to the increased volume and \nsophistication of the breaches. Moreover, LLMs are known to routinely \nproduce credible untruths (‘hallucinations’, ‘simulated authority’ 8 or \n‘compelling misinformation’ 9) and omit attributions of their source or \ntraining data (plagiarism). Combined with the outsourcing of thought \nitself, such concerns render the use of LLMs for academic work \npotentially disingenuous at best, and at worst, in violation of the norms \nof scientific research. With all of this in mind, the need for practical \nguidance through the ethical minefield of LLMs is clear.\nWhat has remained constant?\nIt is comforting that, despite the impressive and daunting changes \nwrought by the advent of LLMs, in reality, most scientific principles \nendure. First, the three values of beneficence, autonomy, and justice, all \ntied to non-maleficence and the avoidance of suffering10 stand firm; in this \nsense, right is still right, and wrong remains wrong. Similarly, cheating \nand plagiarism remain anathema to the spirit of science, while openness, \nreproducibility, and the sharing (non-obscuring or gatekeeping) of data, \nkeeping in mind all the caveats of harm, still stand as ideals.\nAlso holding firm are peer review as a well-established principle \nbefore publication, and the less-formal review-by-peers; that is, the \nshaping and improving of ideas (and writing) based on conversations, \ncorrespondence, arguments and counsel. Technical help – whether in \nthe form of word processors, spell checkers, pocket calculators and \nsoftware programs, or human editors and proofreaders – remains \naccepted and welcomed.\nthe five-tier system: An ethical guide to  \nusing LLMs\nThe present scramble to incorporate LLM use in academic work \n(or to find ways to ban it) implies that conversations about ethical \nguidelines and AI-use standards are timeous and valuable. Given the \npromise and peril of the new, but guided by the three values (justice, \nautonomy and beneficence), I propose a five-tier system to simplify \nthinking around permissions and prohibitions related to using LLMs \nfor academic writing. While representing increasing ‘levels’ of LLM \nsupport that progress along a seeming continuum, the tiers in fact \nrepresent paradigmatically different types of mental undertakings.\nTier 1: Use ban\nThe first level comprises a complete ban on LLM-based support. This \nmeans that no LLM tools may be used in the preparation of the academic \ntext. This tier therefore implies the highest level of human authorship and \nresearch authenticity.\nGiven its Draconian nature, coupled with the likelihood of inadvertent \nviolations (e.g. the spelling and grammar checks employed by ‘ordinary’ \nword processors use a form of AI, and common word processors will \nsoon be incorporating several other dimensions of LLMs), this tier is \nthe most inviting to be flouted. Difficulty to enforce, lack of benefit and \nabundance of drawbacks (e.g. a step backwards in terms of present \npractice, given, for example, the ubiquity of automatic spelling and \ngrammar checks) make such a tier likely only to be used in very specific \ncircumstances, such as proctored university examinations or other \nformal testing conditions.\nTier 2: Proofing tool\nHere, human-written text may be submitted to an LLM, accompanied by \na prompt instructing the model to fix spelling, grammar, register, tone, \nand style (in the manner that products such as Grammarly might do). A \nproofing tool can be instructed to catch (and recommend remedies for) \ntone and style variations, identify problematic or misused words, and \nhighlight direct translations.\nThe point here, of course, is that the work is presented at the end of \nthe writing process – once the experimental and argumentative thinking \nis complete. Tier 2 does not outsource the thinking (or pain) that goes \ninto the drafting process; rather, it takes fleshed-out thoughts and \ncosmetically enhances (or translates) them in much the same manner \nas would a private or in-house proofing team (or academic translation \nservice).\nFacilitating word-perfect (or near enough) text before submission, this \nlevel of usage can ‘level the playing field’ for non-native speakers and \nearly-career researchers; that is, allowing those authors to display their \narguments and findings to best advantage, smoothing over linguistic \nobjections (tacit or implicit), and thus allowing work to be judged on \nmerit alone. Of course, it may also enable ‘lazy’ writing (perhaps on the \npart of native English writers, knowing that sloppy writing will be fixed \nby an algorithm). Nonetheless, clarity and universality are significant in \nmatters of standards, and thus, under this tier, any author might make \nuse of this LLM proofing assistance.\nTier 3: Copyediting tool\nEditing and proofing are distinguished by their sequential place \nin the preparation of texts, and also in the tasks that they perform: \nthis distinction is echoed in the differences between Tiers 2 and 3. \nGiven Tier 3 permissions, an author may ask for an LLM to alter \ntext beyond correcting linguistic mistakes and aligning stylistic \nrequirements. Shortening wordy text (e.g. reducing an abstract from \n300 to 150 words), expanding for clarity, and rephrasing for precision \nare tasks every academic writer has laboured over and would likely \nwelcome assistance with. Other areas of assistance at this tier would \nbe checking citations for accuracy, style, and appropriateness – all \nthings an LLM editorial function can do, and, in most cases, that paid \nin-house editors do to ensure the quality of publications (see Table 1 \nfor prompt examples).\nAn editorial function can ensure a text is well organised, making changes \nto a text’s structure by reordering paragraphs, or highlighting missing \narguments. This tier will also include the assistance that Tier 2 permits, \ntier Sample prompts\n2\n a) The following is section [x] from my paper, which I aim to \nsubmit to [name of journal]. Proofread the section and suggest \ncorrections for spelling, grammar, tone, and style errors. Ensure \nthe text is clear and free from any direct translations. Also, \nreview the section for tone and style variations, and note any \nsuch pointwise. Finally, identify problematic or misused words, \nlist each, and provide a recommendation for replacement.\n b) Make recommendations to improve the overall readability and \ncoherence of the text. “[text here]”\n3\n a) Edit the below to shorten to 2000 words while preserving \ncontent, intention and clarity.\n b) Check the citations in the following document for accuracy, \nstyle, and appropriateness. List any necessary corrections \npointwise.\n c) Make suggestions for the reorganising of paragraphs in the \ndocument below to improve the overall structure and flow of the \nargument. Briefly motivate each modification.\ntable 1: Examples of initial prompts per Tiers 2 and 3 (these are \nintentionally kept straightforward and unsophisticated)\nVolume 120| Number 1/2\nJanuary/February 2024 3https://doi.org/10.17159/sajs.2024/17147\nCommentary\nLarge language models and academic writing\nPage 3 of 5\nincluding spelling, grammar, tone and style corrections, flagging unusual \nwords, nonsensical or confusing text, and guiding smooth transitions \nbetween paragraphs.\nThis tier would best be used during and after the writing process, and \nwould likely be used iteratively, perhaps once a substantial section has \nbeen written.\nEfficient editing (which can be a tedious and expensive journey) and the \nclarity of resultant texts are among the chief benefits of this tier. If correctly \napplied, critical thinking (and laboratory work or experimentation) would \nhave preceded this step that in principle simply allows for a near-\nflawless write-up. Nonetheless, as with Tier 2, intellectual thoroughness \nand writing rigour may be compromised, ostensibly making this a \ncompromise that must be accepted. This tier raises the point that \ncopyediting is, and should be, regarded as an intellectual contribution \n(although copyeditors are seldom credited in scholarly journal articles \nin the way they may be in the publication of books), underscoring the \nobservation that perhaps all forms of support ought to be acknowledged \nin academic writing.\nTier 4: Drafting consultant\nTier 4 speaks to a process of human–LLM ‘co-creation’ (‘augmented \nwriting’7 or ‘coauthoring’ 2). In addition to the support permitted under \nTiers 2 and 3, this tier permits an iterative back-and-forth of ideas as \none might do with a coauthor, up to the point of (and including) the LLM \nsuggesting the omission of certain arguments, suggesting alternative \n‘interpretations’, or requesting that one rerun experiments or check back \nto confirm previous findings.\nIn this tier, the author can interact with an LLM to plan a research write-up \nand shape and develop an argument, including requesting sample lines \n(e.g. instructing the LLM to ‘compose an opening line’). Thus, this is \nnot merely a tool offering a ‘substantive edit’, but a tool that can ensure \none’s evidence backs up one’s argument (that is, where an LLM might \neven contribute to shaping that argument at earlier stages), and, where \nthis is not the case, can provide suggestions on how to remedy such \ngaps.\nUnlike the previous tiers, Tier 4 implies LLM engagement before the \nwriting process commences, followed by iterative ‘reporting back’ \nsessions with the LLM as the writing advances. Potential benefits include \nthe support such a routine would lend to early-career researchers: \n‘handholding’ and sense-checking their writing process and arguments, \nwhile also offering suggestions and criticisms throughout the process \nto ensure quality. While the approach allowed by this tier is likely to \nsignificantly speed up the writing process, it does appear to tip the scales \nin terms of potential risks. Hallucinations and biases (subtle or not), both \nartefacts of LLMs, are more likely to manifest in co-created works. Also, \nLLMs may establish ways to obscure poor research behind apparently \nbrilliant writing. It also follows that an overreliance on LLMs – here \noperating well beyond argumentative and stylistic considerations – would  \nconstitute a loss of skills, the mastery of which would be expected in \nprofessional scientists or academics.\nTier 5: No limits\nThe fifth tier allows any LLM assistance at any stage. This tier includes \nbrainstorming avenues of research, discussing and suggesting \nhypotheses and ideas, and even allowing the LLM to write text on \nthe author’s behalf. Such a no-holds-barred approach also includes \ninterpreting results, summarising other scholarly work, and suggesting \nthe implication of findings. In other words, Tier 5 permits the outsourcing \nof thought.\nThis tier is likely to be useful for instructing students on AI literacy \nand AI usage, and more generally demonstrating the dangers of \nstochastic models. This tier has limited value to scholarly, peer-reviewed \npublications, as the principle of authorship and the requisite of originality \n(at least as currently conceived) would likely be violated by, for example, \na systematic review conducted solely by an AI agent.\nOverview of tiers\nWith their growing levels of permissions, the tiers represent not only \nincreasing degrees of LLM support, but also increasing levels of LLM \ndependence. Table 2 illustrates the tiers, and typical use-case points \nof entry, alongside the most obvious advantages and disadvantages of \neach.\nAs the tiers ‘progress’, so do the apparent speed and efficiency of tasks, \nas well as the dangers of LLM hallucination and manipulation – both \nsignificant and sensible concerns, given the opaque nature of LLM’s \nstochastic processes and governance, not to mention the monopolising \ntendencies among Big Tech in general. Added to this is the arguably \nless immediately pernicious loss of academic integrity that would \nattend the outsourcing of thinking, and may come to represent a threat \nto humanity’s overall ability to undertake quality scientific research, \nin the unlikely scenario in which LLM reliance is left unchecked and \nunregulated. Moreover, real and unconscionable human exploitation 11 \nand high environmental costs 12, both present in current LLM models, \ncannot be discounted or wished away.\nother AI tools available to the would-be \nacademic author\nWhile not part of the writing process, and thus adjacent to the proposed \ntiers, other academic tools draw on new advances in LLM or other \nrecent machine-learning technologies. The ethical and intellectual \nconsiderations of their use overlap with those of LLMs, as do the \nefficiencies they offer researchers and writers.\nThe first is a class of research summarising tools  already introduced \nin this paper. These can perform scans of literature on a topic (e.g. \nPerplexity), provide one-line summations of research papers (e.g. Elicit), \ngive the apparent scientific consensus on a topic (e.g. Consensus), \ntier Effect / type of tool Place in the writing process Most obvious benefit Most obvious risk\n1 Ban n/a Ensures 100% human authorship and does not \ncompromise academic integrity Inevitably flouted except under exam conditions\n2 Proofing After Increases efficiency, reduces cost Might subtly alter meaning or obscure intentions\n3 Editing During Produces well-organised, word-perfect writing Language may be bland, may foster laziness\n4 Co-creating Start\nOffers an alternative to human partnership–\nmaking interpretative and instructive \nsuggestions, error checks\nAuthorship is opaque, high risk of introducing \nhallucinations and biases, inexplicability, loss of \nautonomy, loss of critical reasoning, outsourcing \nof thought\n5 All All Enables high-speed academic writing, maximum \nsupport for inexperienced academics As per Tier 4, but more extreme\ntable 2: Summary of large language model (LLM) permission tiers, where they come into the writing process, and their most obvious benefits and risks\nVolume 120| Number 1/2\nJanuary/February 2024 4https://doi.org/10.17159/sajs.2024/17147\nCommentary\nLarge language models and academic writing\nPage 4 of 5\nor even create a literature review ‘at the touch of a button’. Such tools \ncan provide excellent assistance in exploring new fields of research or \nfields related to one’s own work (see Jansen et al. 13 for a discussion of \nareas in which LLMs might support survey-based research), but are \nnot substitutes for the process of critically reading to inform and order \none’s own intuitions and conclusions, gradually bringing new ideas into \nrelation with one’s own thought-scape. Moreover, such models may \nplay a role in reifying conventional wisdoms, and in so doing, drown \nout marginal voices (the latter which may also be thought of as ‘majority \nvoices’, considering that most people, including academics, are non-\nWestern, non-white, and non-anglophone, despite the outsized influence \nof Western universities on the global scientific community).\nThe second non-writing LLM-based tool is ‘ automatic data analysis ’ \n(e.g. Langchain), whereby data sets can be loaded up to an LLM for \nstatistical analysis. In one dimension, the use of such technologies is \nequivalent to that of a pocket calculator: a logical time-saver, provided \nthe user understands what the LLM is doing. For example, for at \nleast the past three decades, scientists have routinely used multiple \nregressions, typically executed by statistical software or a coding \nroutine in a software library. Use of statistical software (not least the \nwriting up of results) requires a basic knowledge of statistics and data \nanalysis. Subject to new developments, danger enters when the process \nof statistical analysis is not understood by the scientist, but regarded, \ncrudely, as magic (i.e. it cannot be explained). In all, the use of LLM \ninterfaces for statistical analysis will likely become commonplace, to the \nbenefit of science in general, with the qualification that scientists will still \nbe required to understand at least ‘the bare bones’ of statistical analysis.\nThe third application of AI tools, now in the form of machine learning, is \nthe creation of synthetic data (e.g. Statice). Here, one can ask for data \ncontaining certain characteristics and of a given (potentially vast) size, \nor create data for teaching or illustrative purposes (including instances \nof data unavailability or cases where ethical or legal considerations \nproscribe the gathering of such data, such as sensitive healthcare data \nwith patient identifiers). Such data sets will play an increasing role in \nteaching and testing, and so long as users keep in mind that the data in \nhand are fabricated, will be of great advantage in several domains (and \nare currently used in the training of self-driving cars, models for financial \nservice security, and the pharmaceutical industries).\nIt is reasonable to expect that more AI-based tools will join the arsenal \nof the scientist. While caution (based, as ever, on the beneficence, \nautonomy, and fairness and the avoidance of maleficence principles) \nregarding the tools’ creation, application and implications must be \nexercised, many of these tools will provide important and progressive \nsupport to scientific advancements8, and should be embraced.\nLLM tools and safety principles\nReturning to LLM tools and how they can support the academic writing \nprocess, two inviolable principles merit further reflection: ownership and \ntransparency. AI raises complex questions about these two principles; \nhowever, for the time being, the five-tier system, plus the appropriate \nuse of supplementary material, may help to clarify questions around \nauthorship, responsibility, and where we should stand on the place of \nthe thinking process.\nownership is the tenet that a submitted or published work and all of \nits contents remain the responsibility of a human author, and that the \nauthor is the only accountable party for mistakes or other consequences \nemanating from the work.3 Any academic will be familiar with examples \nof authors choosing to omit their names from a publication (despite \nhaving contributed to the scholarship) when they do not fully agree \nwith the contents, or feel unable to be held responsible for arguments \ncontained in the work. Yet the broader point here is that the owner of \nideas, the agent bearing the risk, and the agent deciding to be listed \nas author of a work is, so far, a human one. In our scientific pursuit, in \n‘advancing on Chaos and the Dark’ it is the human – often individual –  \nthinker who toils, who weighs, who risks. 14 Intellectual progress and \nthe advancement of human thinking assumes the hitherto generally \nunspoken assumption that human thought ought not to be outsourced \nto non-human entities. Differently put, and evoking Chesterton 15, you \ncannot make science without a soul. Even in the scenario in which a \nhuman and an LLM ‘co-create’ a work, the responsibility for the content \nstill must rest somewhere, and, in the spirit of science, this would most \nobviously be the human author. Similarly, a recent US court ruling heard \nthat ‘the guiding human hand’ is a ‘bedrock requirement’ to authorship.16 \nThe idea of blaming an LLM for mistakes appears disingenuously \nevasive and points to a concerning ambiguity over authorship. Certainly, \nunder the first three tiers, as suggested here, authorship, and therefore \nownership, rests with the human writer. The corollary of this is that the \nscientific value of papers, books and artefacts produced under Tiers \n4 and 5 are de facto limited, and are likely to remain so: should this \nchange, we will have to rethink authorship and academic credit anew.\nWhile it is possible that humans will not forever be regarded as the sole \nculpable parties of their publications, recent work drawing on AI advances \ncontinues to confirm the principle of human authorship: despite the \nsuccess of AlphaFold being based on Google DeepMind’s algorithms, \nthe celebrated Nature paper17 lists only the human authors. Of course, \nacknowledgement of AI tools used in research (and acknowledgements \nin general) is categorically different from named authors of a work; that \nis, contribution does not imply attribution.\nThe second principle is that of transparency. Transparency – that is, \nshowing one’s work and thought process – lies at the heart of scientific \naccountability, reproducibility, peer accessibility, and public trust.2 LLMs \nare by their nature opaque 1,5; that does not mean they cannot be used, \nbut rather that we must be open about when and how we use them. \nThe alternative scenario is that readers have to guess whether LLMs \nhave been used in the production of text 3 – here, mistrust is a solvent \nof credibility.\nI suggest that authors who use LLM assistance include a way for their \nreaders to see the prompts (and responses) they used in preparing a \ntext. While the American Psychological Association referencing standard \nhas issued a protocol for referencing ChatGPT , possibly a better way of \nreferencing LLM contributions would be to provide a way for readers \nto access the entire series of human–LLM interactions, including every \nprompt and response. A hyperlink to an online repository (such as \nGitHub or Google documents) may risk being too fleeting a base, while \na text file as supplementary material might suffice in ‘showing one’s \nworking’ in the same way as sharing one’s routine of commands to \nstatistical software, or sharing a codebase when programming.\nDiscussion: AI hype and despair\nThere is nothing emergent – in the complex adaptive systems theory \nsense – in the working of a pocket calculator. Calculators’ answers are \nconsistent, predictable, replicable, and regular. In this sense, LLMs are \nnot like calculators: their massive size and black-box nature appear to \nhave given them, at least by most accounts, emergent capabilities. The \ngeneral public (and technical!) discourse has tended to label this not as \n‘emergent’ but rather as ‘generative’ – which is, all told, succumbing to \nthe language of AI hype. 2023 may well be marked as a high point of AI \noptimism, not least owing to the abilities and potential of LLMs. However, \nthe ability to distinguish helpful innovation from unhelpful hyperbole \n(whether AI saviourism or AI catastrophising) is important in order to \nkeep humankind’s present problems and struggles in perspective, \nrecognise our immediate moral duties, and rationally analyse the extent \nto which a new class of tools can help (or hinder) scientific progress and \nhuman betterment.\nFor scientists across every branch of knowledge, mental panic and \nossification remain our nemeses. We gain most by seeing neither \ncataclysmic doom nor total redemption in technology, but instead \nrecalibrating a new technology’s value based on what it can change, \nand what it can’t.\nAmong the proposed tiers, Tier 1 is techno-pessimistic. This tier assumes \nthat technology per se represents a threat to knowledge production \nand human capabilities. This position is, to my mind, untenable for an \nacademic journal already heavily reliant on LLMs (e.g. in the form of spell-\ncheckers), not to mention calculator-like technology. In contrast, Tiers 2 \nand 3 may be regarded as technology-embracive. Optimistic about the \nVolume 120| Number 1/2\nJanuary/February 2024 5https://doi.org/10.17159/sajs.2024/17147\nCommentary\nLarge language models and academic writing\nPage 5 of 5\nefficiencies LLMs bring to human knowledge and scientific advancement, \nthese tiers advocate for adoption, remove the first-language barriers so \noften inhibiting the global dissemination of great ideas, and may even \nexpedite the writing-up process. If permitted some rumination, one might \nsuggest that Tiers 4 and 5 are leaning towards AI hype: both of these \nsupposing that LLMs are either on the path to true cognitive supremacy \nand should thus be employed at all costs (the slave will soon become \nthe benign master), or, alternatively, taking up the despairing position that \nLLMs will soon be so ubiquitous that any resistance to their use is bound \nto fail. One might argue that King2 and Jansen et al.13, on whose insights \nthis note draws, lean towards the possibility of a Tier 5 future, where AI \nwill become a colleague and coauthor.\nConclusion\nFive tiers of LLM support for academic writing have been introduced, each \noffering a different level of writing support, and each entering the writing \n(and thought) process at a different stage. With some intentionality, the \nprinciples of ownership (plus responsibility) and transparency (sharing \nof prompts) can, and ought to be maintained.\nCompeting interests\nI have no competing interests to declare.\nreferences\n1. Eslami M, Vaccaro K, Lee MK, Elazari Bar On A, Gilbert E, Karahalios K. User \nattitudes towards algorithmic opacity and transparency in online reviewing \nplatforms. In: Proceedings of the 2019 CHI Conference on Human Factors \nin Computing Systems; 2019 May 4–9; Glasgow, Scotland. New York: \nAssociation for Computing Machinery; 2019. p. 1–14. https://doi.org/10.1 \n145/3290605.3300724\n2. King MR. A place for large language models in scientific publishing, apart \nfrom credited authorship. Cell Mol Bioeng. 2023;16:95–98. https://doi.org/1 \n0.1007/s12195-023-00765-z\n3. Li H, Moon JT , Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large \nlanguage models in medicine and medical research. Lancet Digit Health. \n2023;5(6):e333–e335. https://doi.org/10.1016/S2589-7500(23)00083-3\n4. Rosenfeld R. Two decades of statistical language modeling: Where do we \ngo from here? Proc IEEE. 2000;88(8):1270–1278. https://doi.org/10.1109 \n/5.880083\n5. Bender EM, Gebru T , McMillan-Major A, Shmitchell S. On the dangers of \nstochastic parrots: Can language models be too big? In: Proceedings of \nthe 2021 ACM Conference on Fairness, Accountability, and Transparency; \n2021 March 3–10. New York: Association for Computing Machinery; 2021.  \np. 610–623. https://doi.org/10.1145/3442188.3445922\n6. Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E,  \net al. Sparks of artificial general intelligence: Early experiments with GPT-4 \n[preprint]. arXiv;arXiv:2303.12712; 2023. https://doi.org/10.48550/arXiv.23 \n03.12712\n7. Kulesz O. The impact of large language models on the publishing sectors: \nBooks, academic journals, newspapers [master’s thesis]. Sweden: Linnaeus \nUniversity; 2023. Available from: https://urn.kb.se/resolve?urn= urn:nbn:se:l \nnu:diva-119366\n8. Rillig MC, Ågerstrand M, Bi M, Gould KA, Sauerland U. Risks and benefits \nof large language models for the environment. Environ Sci Technol. \n2023;57(9):3464–3466. https://doi.org/10.1021/acs.est.3c01106\n9. Spitale G, Biller-Andorno N, Germani F . AI model GPT-3 (dis) informs us better \nthan humans [preprint]. arXiv; arXiv:2301.11924; 2023. https://doi.org/10.1 \n126/sciadv.adh1850\n10. US National Commission for the Protection of Human Subjects of Biomedical \nand Behavioral Research. The Belmont report: Ethical principles and \nguidelines for the protection of human subjects of research. Washington DC: \nUS Department of Health and Human Services; 1979. Available from: http:// \nwww.hhs.gov/ohrp/regulations-and-policy/belmont-report\n11. Perrigo B. Exclusive: OpenAI used Kenyan workers on less than $2 per hour \nto make ChatGPT less toxic. Time. 18 January 2023. Available from: https:// \ntime.com/6247678/openai-chatgpt-kenya-workers/\n12. Strubell E, Ganesh A, McCallum A. Energy and policy considerations for deep \nlearning in NLP . In: Korhonen A, Traum D, Màrquez L, editors. Proceedings \nof the 57th Annual Meeting of the Association for Computational Linguistics; \n2019 July 28 – August 02; Florence, Italy. Kerrville, TX: Association for \nComputational Linguistics. p. 3645–3650. https://doi.org/10.18653/v1/P1 \n9-1355\n13. Jansen BJ, Jung SG, Salminen J. Employing large language models in survey \nresearch. Nat Lang Process J. 2023;4:100020. https://doi.org/10.1016/j.nl \np.2023.100020\n14. Emerson RW. Self-Reliance (1841). In: Porte J, editor. Essays and lectures. \nNew York: Library of America; 1983. p. 261.\n15. Chesterton GK. Orthodoxy. San Francisco, CA: Ignatius Press/John Lane \nCompany; 1908.\n16. Thaler Sv, Perlmutter S, Register of Copyrights and Director of the U.S. \nCopyright Office, et al., Civil Action No. 22-1564 (BAH) (18.08.2023). US \nDistrict Court for the District of Columbia; 2023.\n17. Jumper J, Evans R, Pritzel A, Green T , Figurnov M, Ronneberger O,  \net al. Highly accurate protein structure prediction with AlphaFold. Nature. \n2021;596(7873):583–589. https://doi.org/10.1038/s41586-021-03819-2",
  "topic": "Mathematics education",
  "concepts": [
    {
      "name": "Mathematics education",
      "score": 0.3915256857872009
    },
    {
      "name": "Computer science",
      "score": 0.3739975094795227
    },
    {
      "name": "Psychology",
      "score": 0.3250272274017334
    }
  ]
}