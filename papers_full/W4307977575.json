{
  "title": "A combined deformable model and medical transformer algorithm for medical image segmentation",
  "url": "https://openalex.org/W4307977575",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2160515717",
      "name": "Zhixian Tang",
      "affiliations": [
        "Jiading District Central Hospital",
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2606556279",
      "name": "Jintao Duan",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2108923890",
      "name": "Yanming Sun",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2137142577",
      "name": "Yanan Zeng",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2147682747",
      "name": "Yile Zhang",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106002726",
      "name": "Xufeng Yao",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2160515717",
      "name": "Zhixian Tang",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2606556279",
      "name": "Jintao Duan",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2108923890",
      "name": "Yanming Sun",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2137142577",
      "name": "Yanan Zeng",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2147682747",
      "name": "Yile Zhang",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106002726",
      "name": "Xufeng Yao",
      "affiliations": [
        "Shanghai University of Medicine and Health Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2977942577",
    "https://openalex.org/W2043921087",
    "https://openalex.org/W4212888320",
    "https://openalex.org/W3011743383",
    "https://openalex.org/W3033039186",
    "https://openalex.org/W2912449434",
    "https://openalex.org/W3207711392",
    "https://openalex.org/W2958620810",
    "https://openalex.org/W2760516000",
    "https://openalex.org/W3124211545",
    "https://openalex.org/W3012412627",
    "https://openalex.org/W3157447953",
    "https://openalex.org/W3046815881",
    "https://openalex.org/W3157877050",
    "https://openalex.org/W2962837118",
    "https://openalex.org/W3161764371",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3091741697",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3160257089",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W3097065222",
    "https://openalex.org/W3198090248",
    "https://openalex.org/W2945757384",
    "https://openalex.org/W2566015103",
    "https://openalex.org/W2905338897",
    "https://openalex.org/W3106788732"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)1 3\nh\nttps://doi.org/10.1007/s11517-022-02702-0\nORIGINAL ARTICLE\nA combined deformable model and medical transformer algorithm \nfor medical image segmentation\nZhixian Tang1,2 · Jintao Duan3 · Yanming Sun3 · Yanan Zeng3 · Yile Zhang3 · Xufeng Yao1\nReceived: 1 March 2022 / Accepted: 19 October 2022 \n© The Author(s) 2022\nAbstract\nDeep learning–based segmentation models usually require substantial data, and the model usually suffers from poor \ngeneralization due to the lack of training data and inefficient network structure. We proposed to combine the deform-\nable model and medical transformer neural network on the image segmentation task to alleviate the aforementioned \nproblems. The proposed method first employs a statistical shape model to generate simulated contours of the target \nobject, and then the thin plate spline is applied to create a realistic texture. Finally, a medical transformer network \nwas constructed to segment three types of medical images, including prostate MR image, heart US image, and tongue \ncolor images. The segmentation accuracy of the three tasks achieved 89.97%, 91.90%, and 94.25%, respectively. The \nexperimental results show that the proposed method improves medical image segmentation performance.\nKeywords\n Medical image segment\nation · Image augmentation · Medical transformer · Deformable model\n1 Introduction\nImaging techniques have become essential for disease \ndiagnosis, surgical planning, and prognostic evaluation \nin medical institutions [1 ]. Precisely segmenting the \nregions of interest (ROI) in these images can assist doc-\ntors in making a correct diagnosis of the disease. In clini-\ncal decision-making, image segmentation technology can \nprovide a reliable basis for computer-aided diagnosis and \ntreatment [2 ]. It is also critical for quantitative analysis \n[3] and surgical navigation [4 ]. Hence, image segmenta-\ntion has important theoretical significance and clinical \nvalue.\nDeep learning-based automatic segmentation algorithms \nhave made significant progress [5 ]. Many deep learning \nmethods have been successfully applied in cell segmentation \n[6], lung segmentation [7], prostate segmentation [8], brain \nstructure segmentation [9 ], and fetal segmentation [10]. \nTraining a robust segmentation model requires a large \nquantity of labeled data. However, physical professionals \nobtain the labeled data manually, which is time-consuming \nand laborious. Thus, the available annotated training data \nis limited. Moreover, the deep features of medical images \nare challenging to excavate. Therefore, many typical deep \nlearning models do not perform well in medical image seg-\nmentation tasks. In addition, different structures and tun-\ning strategies are usually required for various segmentation \ntasks to achieve the best for the respective tasks. Recently, \nlots of methods have been proposed to overcome the above \ndifficulties, which can be roughly divided into the following \ncategories:\nThe first category is to augment the training data. The \nmost typical technology is rigid transformation, includ-\ning rotation, translation, scaling, and tangent. Patch sam-\npling [11] is also an effective data augmentation method. \nFor example, Bertram et al. [12] used a content-sensitive \nsampling strategy for patchwise training. The emergence \nof Generative Adversarial Nets (GAN) provides a new \nidea for image augmentation. Huang [13] and Chong et al. \n[14] used GAN to synthesize brain images, improving \nsubsequent image post-processing performance. Frid-\nAdar et al. [15] utilized GAN to generate some simulated \n * Xufeng Yao \n y\naoxf@sumhs.edu.cn\n1 College of Medical Imaging, Shanghai University \nof \nMedicine & Health Sciences, Shanghai 201318, China\n2 Radiology Department, Shanghai University of Medicine \n& Healt\nh Sciences Affiliated Jiading Hospital, \nShanghai 201800, China\n3 College of Medical Instrumentation, Shanghai University \nof Medicine & Healt\nh Sciences, Shanghai 201318, China\n/ Published online: 3 November 2022\nMedical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nimages and improved the performance of CNN for liver \nlesion classification. However, most of these image data \naugmentation methods ignore the inherent properties of \nthe image, and GAN tends to cause mode collapse.\nThe second category is to adopt the transfer learning \nstrategy. Transfer learning can apply additional data or an \nexisting model to a relevant task. For example, Dou's work \n[16] applied a transfer learning method for cardiac CT image \nsegmentation using a pre-trained model on MR images. \nMartin et al. [17] proposed a 2D to 3D transfer learning \nmethod, the initial weights of the 3D Res-Unet were trans-\nferred from the 2D VGG-16. Transfer learning can speed up \nthe convergence of the model for the second task and even \nimprove its performance.\nThe third type is to integrate information from different \nlayers or extract the long-range dependencies. The most \nclassical network is the U-net proposed by Ronneberger \net al. [18], which combines the image’s low-level and \nhigh-level convolutional features. Thus, it can achieve \nmedical image segmentation with less training data. \nRecently, many U-net variants have been proposed. Rep-\nresentative networks include Attention U-net [19] for CT \nprostate segmentation, R2AU-net [20], U-Net\n \n+\n  \n+\n \n[21] and \nnn-Unet for multi-task segmentation of medical images. \nSome researchers try to change the convolution kernel's \nstructure so that the image's multi-scale information can \nbe utilized. For example, the Atrous convolution kernel \n[22] has a large receptive field, so each convolution output \ncontains an extensive range of information. Wang et al. \n[23] proposed a method that addressed the gridding arti-\nfacts by smoothing the dilated convolution. Dai et al.[24]. \nused deformable convolution and deformable ROI pool-\ning to enhance the transformation modeling capability of \nCNNs. Recently, Image GPT [25] can be perceived as a \nsignificant breakthrough in image processing whose suc-\ncess is mainly attributed to the self-attention mechanisms \ninvestigating the Transformer [26]. The Transformer can \ndig out the long-range dependencies. In the image seg-\nmentation tasks, the Transformer also performed well, \nsuch as MedT [27], Axial-Deeplab [28], TransU-net [29]. \nHowever, taking non-local attention as an example, the \ncomputational load is large, especially when the feature \nmap is large and the computational efficiency is very low.\nInspired by the fundamental mechanism of the Trans-\nformer, we combined the deformable model and medical \ntransformer network for medical image segmentation. First, \nwe established a statistical shape model from the contours \nof the target object in the real training images. Then, we \nused the model to generate simulated contours of the target \nobject. Second, we applied the thin plate spline to create a \nrealistic texture. Third, we introduced the axial-attention and \nbuilt a medical transformer network to segment three types \nof medical images, including prostate MR images, heart US \nimages, and tongue images.\nThe contributions of this paper can be summarized as \nfollows:\na)\n W\ne proposed an image augmentation strategy to allevi-\nate the problem of data scarcity in medical image pro-\ncessing with deep neural networks.\nb)\n The ne\ntwork effectively applied axial attention and the \ndual-scale training strategy to mining the long-range \nfeature information.\nc)\n W\ne built the network and validated the model using \nthree different types of data, including MRI images, \nultrasound images, and color images.\nThe rest of this paper is organized as follows: Section  2 \ndescribes the framework of the method in detail, includ -\ning image enhancement (Section 2.1), gated axial-attention \nmechanism (Section  2.2) and medical transformer (Sec-\ntion 2.3). We evaluate our method on three different datasets \nin Section  3 and discuss the advantages as well as disad-\nvantages of the model in Section 4. We conclude the whole \npaper in Section 5.\n2  Methods\nThe proposed framework for medical image segmentation is \nshown in Fig. 1, containing the following steps.\na)\n Imag\ne preprocessing. We employ intensity normaliza-\ntion and resample the original series to make the spatial \nresolution consistent in each direction.\nb)\n Imag\ne data augmentation. This step generates some \nsimulated images with the deformable model and the \nimproved thin plate spline algorithm.\nc)\n T\nrain the medical transformer network through real \ntraining and simulated data.\nd) T\nest the trained model, and obtain the final segment \nresults.\n2.1  Image da ta augmentation based \non the deformable model\nThe image data strategy combines the statistical shape \nmodel (SSM) and thin plate spline to generate new train-\ning images. The statistical shape model is a commonly \nused statistical method for feature positioning. We first \nbuild a statistical shape model based on the contour shape \n130 Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nof the target organ in training data, which is then used to \ngenerate the simulated shape of the target organ. Given \nN sets of two-dimensional or three-dimensional training \nsamples by their shapes \n{si}N\ni=1 . The shape model can be  \nrepresented by:\nwhere /u1D41B={ b1 ,b2 ,… ,bk} is the shape parameters, including \nscale and rotation parameters. By changing the value of b , \nwe can generate any simulated shapes from this model. Typi-\ncally, the range of b should lie in a hyperrectangle /u1D41B≤ /u1D6FC\n√\n/u1D6CC \nwith /u1D6FC∈ [−1.5, 1.5]  . The simulated shape g enerated by the \nalgorithm is more in line with the distribution of the actual \nhuman organs.\nThen, we generate the texture of each simulated shape \nfrom training data with the 2D thin plate spline algo-\nrithm. The 2D thin plate spline function can be specified \nas follows:\nThe first part is an affine transformation representing \nthe behavior of \nf(x, y) at infinity. The second part is the \nweighted sum of root function U (r)=r 2log(r2) . According \nt\no the function f(x, y) , an y points t =( x, y) in the simulated \nimage can be transformed into the points t\n�\n=( x\n�\n, y\n�\n) in the \nreal image, then insert gray values of point t\n/uni2032.var\n in the real \nimage into the point t  in the simulated image.\nWe can augment the training data with realistic simulated \nimages by combining shape generation and texture interpola-\ntion methods.\n(1)s = s0 + /u1D404/u1D41B\n(2)\nf(x,y)=\n/parenleft.s3a1x\na1y\n/parenright.s3\n+\n/parenleft.s3a2x\na2y\n/parenright.s3\nx +\n/parenleft.s3a3x\na3y\n/parenright.s3\ny +\nn/uni2211.s1\ni=1\n/parenleft.s3w ix\nw iy\n/parenright.s3\nU (/uni007C.x/uni007C.xpi −( x,y)/uni007C.x/uni007C.x)\n2.2  Gated axial‑attention\nDue to the inherent inductive preference of convolutional \nstructures, it lacks the ability to model remote dependencies in \nimages. Transformer constructs use self-attention mechanisms to \nencode long-distance dependencies and learn highly expressive \nfeatures. We add the transformer structure into the network to \nimprove the ability of network feature expression and location.\nWe adopt an axial attention-based method to extend the \nexisting structure. This additional positional bias in Query, \nKey, and Value captures remote interactions with precise \npositional information. For any given input feature \nx , the \naxial self-attention mec\nhanism with relative position encod-\ning and width axis can be written as:\nwhere, rq, rk, rv ∈ ℝW ×W  are axial attentional models in \nthe width direction. Formula (3 ) describes the axial atten-\ntion applied along the tensor width axis. A similar for -\nmula is also used to apply axial attention along the height \naxis. Axial attention can compute non-local contexts with \ngood computational efficiency, encode positional biases \ninto mechanisms, and encode remote interactions in input \nfeature graphs. However, it is difficult to learn in experi -\nments with small-scale data sets that often occur in medi-\ncal image segmentation, so it is not always accurate when \nencoding remote interactions. Adding relative positions to \ntheir respective keys, queries, and values can lead to perfor-\nmance degradation if the relative positions learned are not \nencoded accurately enough. Therefore, we use an improved \n(3)\nyij=\nW/uni2211.s1\n/u1D714=1\nsoftmax(qT\nijkiw + qT\nij\nrq\niw + kT\niwrk\niw)(viw + rv\niw)\nFig. 1  The fr amework of the proposed method\n131Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\naxial block, shown in Figs.  2 and 3, which can control the \neffect of position deviation on non-local context encoding. \nWith the proposed modification, the self-attention mecha-\nnism applied to the width axis can be formally written as:\nFormula (4 ) is very close to Formula (3 ) but adds the \ngating mechanism. \nGQ,GK,GV1\n,GV2\n∈ ℝ are learnable \nparameters that together form the gating mechanism for \ncontrolling the effect of learning relative position encod-\ning on non-local context encoding. Generally, if a relative \nposition code is accurately learned, the gating mechanism \nwill assign a higher weight than those not.\n2.3  Main struc ture of the medical transformer \nnetwork\nThe structure of the encoder is shown in Fig.  3 below. \nEach decoder has typical convolution layers using a 1\n ×\n 1\n \nconvolution kernel, a BN layer, Gated axial-attention lay -\ners, and BN layers. The decoder part has one convolution \nlayer, one up-sampling layer, and one ReLU activation. \nBetween each pair of encoder and decoder, there is a \nskip connection. In the expanded path, each decoder step \nincludes an up-sampling operation of the feature map. \nAfter up-sampling, the number of feature channels will be \nreduced by half, then jump to the corresponding feature \nmap in the contraction path. Then convolution operations \nare used with three convolution layers, all using BN and \nReLU activation functions. It should be emphasized that \nan additional 1\n ×\n 1 con\nvolution layer is connected after \nfeature mapping in the training process.\n(4)\nyij=\nW/uni2211.s1\n/u1D714=1\nsoftmax(qT\nijkiw + G Q qT\nij\nrq\niw + G K kT\niwrk\niw)(G V1\nviw + G V2\nrv\niw)\nDue to the small sample size of medical image data, it is \ndifficult to train the transformer network for medical images \neffectively. We divide our network into two branches. In the \nfirst branch, 1\n ×\n 1 con\nvolution is performed on the whole \nimage, and then a two-layer encoder and a two-layer decoder \nFig. 2  Main s tructure of the \ngated axial attention mechanism \nnetwork\nFig. 3  The s tructure of encoders\n132 Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nare used and skip connections are made. In the second \nbranch, the original image is evenly divided into 16 small \nimages using the multi-layer encoder and decoder method, \nand resampling is carried out. The feature is resampled and \nweighted with the feature image in the first branch, and the \nfinal segmented image is obtained after passing through a \n1\n ×\n 1 con\nvolution layer. This strategy can improve the seg-\nmentation performance of the network and pay attention to \nthe global high-level information. At the same time, the local \nbranch can pay attention to finer details so that the segmenta-\ntion can be more accurate.\nThe model adopts the binary cross-entropy loss, and its \nspecific definition is as follows:\nwhere \np i is the true label of the pixel, and /uni0302.s1p i is the predicted \nprobability of the pixel for all N points.\n3  Results\n3.1  Datasets\nWe used the National Cancer Institute (NCI) Cancer Imag-\ning Program and the Prostate Magnetic Resonance Imag-\ning Public Data released by the International Society of \nBiomedical Imaging (ISBI). The data published by NCI-\nISBI contains 80 sets of 3D data fields, including 60 sets \nof training data sets, 10 sets of validation data sets and 10 \nsets of test data sets. Half of the images are obtained by a \nmagnetic resonance machine with a magnetic field strength \nof 1.5 T, and the other half are acquired by a magnetic \nresonance machine with a magnetic field strength of 3 T. \nBecause one data set does not match the gold standard in \nthe training data, the correct segmentation accuracy can-\nnot be obtained, so it is excluded from the experiment. We \nsliced the three-dimensional data in the transverse direc-\ntion. Then we got 10,155 two-dimensional training images, \n1825 verification images, and 1830 test images.\nThe second dataset is echocardiography. The dataset \ncontains 480 transverse images, and the image size is \n800\n ×\n 600. T\nwo radiologists labeled the region of the left \nventricular valve. The experiment randomly selected 360 \nimages as training sets and the remaining 120 as test sets.\nThe third dataset is the tongue image from the web \n(https://\n \ngithub.\n \ncom/\n \nBioHit/\n \nTonge\n \nImage\n \nDatas\n \net). These \ndata were collected by the professional tongue diagnostic \ninstrument. The public data contains 300 sets of tongue \npictures with labels. To enhance the complexity of the \n(5)\nLoss\n/parenleft.s2\nP,/uni0302.s1P\n/parenright.s2\n= − 1\nN\nN/uni2211.s1\ni=1\np i ∙ log/parenleft.s1/uni0302.s1p i\n/parenright.s1+( 1 − p i)log(1 − /uni0302.s1p i)\ndata, we recruited 100 volunteers from Longhua Hospital \nAffiliated to Shanghai University of Traditional Chinese \nMedicine and collected their tongue images with a digi-\ntal camera. Thus, we collected a total of 400 images. To \nverify the model's ability to handle noisy data, we added \nGaussian noise (\n /u1D707= 0, /u1D70E2 = 0.02  ) t o 20% of the images. \nThen, the data were randomly divided into 320 train-\ning and 80 test datasets. It's important to emphasize that \nobserving the tongue is a unique part of Traditional Chi-\nnese Medicine (TCM)[30]. Segmenting the tongue from \nthe image can provide a solid foundation for subsequent \nquantitative analysis.\n3.2  Metric\nTo verify the performance of the proposed network, we use \nDice’s similarity coefficient (DSC) to measure the segmenta-\ntion algorithm, which is defined explicitly as Formula (6 ), \n(7) and (8), where X and Y represent the algorithm segmen-\ntation result and the gold standard, respectively.\nWe choose to segment three medical images to demon-\nstrate the effectiveness of our method, including prostate \nMR image, heart US image, and tongue images. We aug-\nment the training set by 30% for each task with our proposed \nalgorithm.\n3.3  Performance of the algorithm\nCompare with other prostate segmentation methods: CNN \nbased method [31], Super Voxel-based method [32], U-net \n[18], R2U-net [ 33], Att U-net [ 34] and U-net\n + \n + [2\n1]. \nU-net, R2U-net, Att U-net and U-net\n +\n  +\n ar\ne reproduced \nthrough the article. The hyper-parameters of the above mod-\nels are basically the same, such as the learning-rate is set \nto 1e-4, the epoch is set to 25, the batch-size is 4, all use \nAdam with Momentum optimizer. The accuracy results of \nother methods are derived from the corresponding papers. \nThe comparison of the segmentation accuracy is shown in \nTable 1. The proposed method obtains a competitive result \namong the fully automatic segmentation algorithms. Fig -\nure 4 shows the segmentation results.\n(6)\nDSC = 2/uni007C.var/u1D417∩ /u1D418/uni007C.var\n/uni007C.var/u1D417/uni007C.var+ /uni007C.var/u1D418/uni007C.var\n(7)IoU = /uni007C.var/u1D417∩ /u1D418/uni007C.var\n/uni007C.var/u1D417∪ /u1D418/uni007C.var\n(8)Recall = /uni007C.var/u1D417∩ /u1D418/uni007C.var\n/uni007C.var/u1D418/uni007C.var\n133Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nSince the ultrasound images suffer from low contrast and \npoor imaging quality, we performed image preprocessing, \nincluding mask operation and gray level equalization. We com-\npare our method with other segmentation methods( U-net [18], \nR2U-net [33], Att U-net [34] and U-net\n \n+\n  \n+\n \n[21]). The above \nnetworks are reproduced by their original papers. The segmen-\ntation accuracy is shown in Table 2. The DSC of our method \nis 91.90%. Figure 5 shows the transverse plane segmentation \nresults of echocardiography.\nNo preprocessing was done for the tongue image data -\nset except for scale normalization. We compare our method \nwith other tongue image segmentation methods(U-net [18], \nR2U-net [33], Att U-net [34] and U-net\n + \n + [2\n1]). The above \nnetworks are reproduced by their original papers. The seg-\nmentation accuracy is shown in Table 3. Figure 6 shows the \nsegmentation results of tongue images.\n4  Discussion\nIn recent years, deep learning has achieved outstanding \nresults in many fields. However, the available training data \nfor medical images are often scarce. We combined the \ndeformable model with the medical transformer network to \nachieve medical image segmentation with a small amount of \ndata. Theoretically, this method can effectively increase the \ninformation flow of the network so that less training data can \nbe used to achieve medical image segmentation.\nWe used the three types of medical images to verify \nthe performance of the segmentation algorithm based on \nour method. The steps are as follows: First, we preproc-\nessed the training image, including uniform image size \nand grayscale normalization. Second, we used the statis-\ntic shape model and 3D thin plate spline to achieve the \npurpose of data augmentation. Third, we constructed the \nmedical transformer network structure to segment three \ntypes of medical images. The test results show that the \nsegmentation algorithm proposed in this paper achieved \na DSC of 89.97%, 91.90%, and 94.25% on the prostate \nMR images, heart US images and tongue color images, \nrespectively.\nOf course, the algorithm still has the following limita-\ntions, such as the medical transformer used in this paper \nis still based on the 2D images, and some of the 3D spatial \ninformation of the image is lost. The main reason for the \nTable 1  Segment ation results of different algorithms on prostate MR \nimage test datasets\nMethod DSC IoU Recall\nCNN\n \n+\n \nCRF-RNN 78.20% - -\nSuper voxel 88.23% - -\nU-net 82.02% 69.52% 76.13%\nR2U-net 86.49% 76.20% 84.60%\nAtt U-net 88.91% 80.03% 87.04%\nU-net\n \n+\n  \n+\n 88.51% 79.38% 85.18%\nOur me\nthod 89.97% 81.78% 86.36%\nFig. 4  R esults of prostate image segmentation, where the yellow contours are the gold standard and the red contours are the algorithm segmenta-\ntion result\nTable\n \n2\n  Segment\nation results of different algorithms on heart US \nimage\nMethod DSC IoU Recall\nU-net 87.34% 77.55% 87.18%\nR2U-net 90.04% 81.88% 81.94%\nAtt U-net 90.25% 82.23% 86.18%\nU-net\n \n+\n  \n+\n 90.36% 82.42% 90.02%\nOur me\nthod 91.90% 85.01% 90.18%\n134 Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nabove shortcomings lies in too few labeled training data \n(there are only 59 training data sets).\nOur future work will focus on the following directions. \nFirst, the medical transformer network can be applied to \nother medical image segmentation tasks, such as cardiac \nMR images, etc. Second, the network can be extended to \n3D space, fully using 3D spatial information. Third, the \nnetwork can be further improved to build a deep neural \nnetwork with better performance.\n5  Conclusion\nIn this paper, we combined the deformable model and medi-\ncal transformer network to achieve image segmentation. \nThe proposed method can alleviate the problem of fewer \nlabeled medical images. The method was tested on three \ntypes of medical images, including prostate MR image, heart \nUS image, and tongue color images. Our method achieved \nhigher accuracy than the common model used in medical \nimage segmentation.\nFig. 5  The segment ation result, where the golden contours represent the real annotation result, and the red contours are the segmented image\nTable\n \n3\n  Segment\nation results of different algorithms on tongue color \nimage\nMethod DSC IoU Sen\nU-net 89.69% 81.31% 84.83%\nR2U-net 87.37% 77.96% 84.59%\nAtt U-net 93.19% 88.65% 93.53%\nU-net\n \n+\n  \n+\n 93.77% 88.26% 96.50%\nOur me\nthod 94.25% 89.12% 93.28%\nFig. 6  The segment ation result of the tongue image, where the golden contours represent the real annotation result, and the red contours are the \nsegmented image. (a) Original image of the public dataset. (b) Images with Gaussian noise. (c) Images captured by the digital camera\n135Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\nFunding Sponsor ed by the National Natural Science Foundation of \nChina (61971275), Shanghai Sailing Program (21YF1418600) and \nXuhui District Artificial Intelligence Medical Hospital-region Coop-\neration Project (2020–004).\nDeclarations \nConflict of interest  The authors declared no potential conflicts of in-\nterests.\nOpen Access\n This ar\nticle is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://\n \ncreat\n \niveco\n \nmmons.\n \norg/\n \nlicen\n \nses/\n \nby/4.\n \n0/.\nReferences\n 1 .  Budd S, R obinson EC, Kainz B (2021) A survey on active learning \nand human-in-the-loop deep learning for medical image analysis. \nMed Image Anal 71:102062\n 2\n. R\nadermacher K, Portheine F, Anton M et al (1998) Computer \nassisted orthopaedic surgery with image based individual tem-\nplates. Clin Orthop Relat Res 354:28–38\n 3\n. B\nalochian S, Baloochian H (2022) Edge detection on noisy images \nusing Prewitt operator and fractional order differentiation. Mul-\ntimed Tools Appl 81:9759–9770\n 4\n. Du G, Cao X, Liang J e\nt al (2020) Medical image segmentation \nbased on U-Net: a review. J Imaging Sci Technol 64:20508\n 5\n. Liu L, Cheng J, Quan Q e\nt al (2020) A survey on U-shaped \nnetworks in medical image segmentations. Neurocomputing \n409:244–258\n 6\n. Ar\naújo FHD, Silva RRV, Ushizima DM et al (2019) Deep learning \nfor cell image segmentation and ranking. Comput Med Imaging \nGraph 72:13–21\n 7.\n Be\nvilacqua V, Altini N, Prencipe B et al (2021) Lung segmenta-\ntion and characterization in covid-19 patients for assessing pul-\nmonary thromboembolism: an approach based on deep learning \nand radiomics. Electron 10:2475\n 8\n. Jia H, Xia Y\n, Song Y et al (2020) 3D APA-Net: 3D Adversarial \npyramid anisotropic convolutional network for prostate segmenta-\ntion in mr images. ieee trans med imaging 39:447–457\n 9\n. K\nushibar K, Valverde S, González-Villà S et al (2018) Automated \nsub-cortical brain structure segmentation combining spatial and \ndeep convolutional features. Med Image Anal 48:177–186\n 1\n0. Z\neng Y, Tsui PH, Wu W et al (2021) Fetal ultrasound image seg-\nmentation for automatic head circumference biometry using deeply \nsupervised attention-gated V-Net. J Digit Imaging 34:134–148\n 11.\n P\nérez-García F, Sparks R, Ourselin S (2021) TorchIO: A Python \nlibrary for efficient loading, preprocessing, augmentation and \npatch-based sampling of medical images in deep learning. Com-\nput Methods Programs Biomed 208:106236\n 1\n2. Sa\nbrowsky-Hirsch B, Thumfart S, Hofer R, Fenz W (2020) A \ncontent-driven architecture for medical image segmentation. In: \nACM International Conference Proceeding Series. 89–96\n 1\n3. Huang Y\n, Zheng F, Cong R et al (2020) MCMT-GAN: Multi-Task \nCoherent Modality Transferable GAN for 3D brain image synthe-\nsis. IEEE Trans Image Process 29:8187–8198\n 14.\n Chong C\nK, Ho ETW (2021) Synthesis of 3D MRI Brain images \nwith shape and texture generative adversarial deep neural net-\nworks. IEEE Access 9:64747–64760\n 1\n5 F\nrid-Adar M, Diamant I, Klang E et al (2018) GAN-based syn-\nthetic medical image augmentation for increased CNN perfor -\nmance in liver lesion classification. arXiv Prepr arXiv:180301229\n 1\n6. D\nou Q, Ouyang C, Chen C et al (2018) Unsupervised cross-\nmodality domain adaptation of convnets for biomedical image \nsegmentations with adversarial loss. In: IJCAI International Joint \nConference on Artificial Intelligence. 691–697\n 1\n7. K\nolarik M, Burget R, Travieso-Gonzalez CM, Kocica J (2020) \nPlanar 3D transfer learning for end to end unimodal MRI unbal-\nanced data segmentation. In: Proceedings - International Confer -\nence on Pattern Recognition. 10–15\n 1\n8. R\nonneberger O, Fischer P, Brox T (2015) U-Net: convolutional net-\nworks for biomedical image segmentation. In: International Confer-\nence on Medical Image Computing and Computer-Assisted Interven-\ntion. 234–241\n 1\n9. X\nu X, Lian C, Wang S, et al (2020) Asymmetrical multi-task attention \nU-Net for the segmentation of prostate bed in CT Image. In: Lecture \nNotes in Computer Science (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in Bioinformatics). 470–479\n 2\n0 Zuo Q, Chen S, W\nang Z (2021) R2AU-Net: attention recurrent \nresidual convolutional neural network for multimodal medical \nimage segmentation. secur Commun Networks 4:1–10\n 2\n1. Zhou Z, R\nahman Siddiquee MM, Tajbakhsh N, Liang J (2018) \nUnet++: A nested u-net architecture for medical image segmenta-\ntion. In: Lecture Notes in Computer Science (including subseries \nLecture Notes in Artificial Intelligence and Lecture Notes in Bio-\ninformatics). 3–11\n 2\n2. C\nhen LC, Papandreou G, Kokkinos I et al (2018) DeepLab: \nSemantic image segmentation with deep convolutional nets, atrous \nconvolution, and fully connected CRFs. IEEE Trans Pattern Anal \nMach Intell 40:834–848\n 2\n3. W\nang Z, Ji S (2021) Smoothed dilated convolutions for improved \ndense prediction. Data Min Knowl Discov 35:1470–1496\n 2\n4. Dai J, Qi H, Xiong Y e\nt al (2017) Deformable Convolutional Net-\nworks. In: Proceedings of the IEEE International Conference on \nComputer Vision. 22–29\n 25.\n Chen M, R\nadford A, Child R et al (2020) Generative pretraining \nfrom pixels. In: 37th International Conference on Machine Learn-\ning, ICML 2020. 1669–1681\n 2\n6. Car\nion N, Massa F, Synnaeve G et al (2020) End-to-End object \ndetection with transformers. In: European Conference on Com-\nputer Vision. 213–229\n 2\n7. V\nalanarasu JMJ, Oza P, Hacihaliloglu I, Patel VM (2021) Medical \ntransformer: gated axial-attention for medical image segmenta-\ntion. In: International Conference on Medical Image Computing \nand Computer-Assisted Intervention. 36–46\n 2\n8. W\nang H, Zhu Y, Green B et al (2020) Axial-DeepLab: stand-alone \naxial-attention for panoptic segmentation. In: European Confer -\nence on Computer Vision. 108–126\n 2\n9. Ghali R, Akhloufi MA\n, Jmal M et al (2021) Wildfire segmentation \nusing deep vision transformers. Remote Sens 13:3527\n 3\n0. Y\nuan W, Liu C (2019) Cascaded CNN for real-time tongue seg-\nmentation based on key points localization. In: 2019 4th IEEE \nInternational Conference on Big Data Analytics, ICBDA 2019\n 3\n1. Can YB, Chait\nanya K, Mustafa B et al (2018) Learning to segment \nmedical images with scribble-supervision alone. Int Work Deep \nLearn Med Image Anal 11045:236–244\n 3\n2. T\nian Z, Liu L, Zhang Z et al (2017) A supervoxel-based segmenta-\ntion method for prostate MR images. Med Phys 44:558–569\n 3\n3. Zahangir Alom M, Y\nakopcic C, Taha TM, Asari VK (2018) \nNuclei segmentation with recurrent residual convolutional neural \nnetworks based U-Net (R2U-Net). In: Proceedings of the IEEE \nNational Aerospace Electronics Conference, NAECON.\n136 Medical & Biological Engineering & Computing (2023) 61:129–137\n1 3\n 34. Zhao X, W ang S, Zhao J et al (2020) Application of an attention \nU-Net incorporating transfer learning for optic disc and cup seg-\nmentation. Signal, Image Video Process 15:913–921\nPublisher's note\n Spr\ninger Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nZhixian Tang  r eceived the B.S. \ndegree from the Ocean Univer -\nsity of China, Shandong, China, \nin 2014 and the Ph.D. degree in \nbiomedical engineering from \nFudan University, Shanghai, \nChina, in 2020. He is currently a \nfaculty member in Shanghai Uni-\nversity of Medicine and Health \nScience, Shanghai, China. He is \nalso a researcher in Jiading Hos-\npital, Shanghai, China. His \nresearch interests include medi-\ncal image processing and medi-\ncal imaging techniques.\nJintao Duan  r eceieved the B.S. \ndegree in biomedical engineer -\ning with Shanghai University of \nMedicine and Health Science, \nShanghai, China. His current \nresearch direction is the applica-\ntion of deep learning in medical \ndevices.\nYanming Sun  r ecevied the B.S. \ndegree in biomedical engineering \nwith Shanghai University of Medi-\ncine and Health Science, Shanghai, \nChina. His main research direction \nis the intelligent diagnosis and \nmedical image segmentation.\nYanan Zeng  is cur rently pursuing \nthe B.S. degree in data science \nand big data technology with \nShanghai University of Medicine \nand Health Science, Shanghai, \nChina. Her primary research is \nfocused on computer vision.\nYile Zhang  is cur rently pursuing \nthe B.S. degree in data science \nand big data technology with \nShanghai University of Medicine \nand Health Science, Shanghai, \nChina. Her current research \ninterests include image classifi-\ncation and segmentation.\nXufeng Yao   r eceived the B.S. \ndegree in medical imaging from \nShandong First Medical Univer-\nsity, Shandong, China, in 1999, \nthe M.S and Ph.D. degree in \nFudan University, Shanghai, \nChina, in 2007 and 2012, respec-\ntively. He is currently a professor \nof medical imaging with Shang-\nhai University of Medicine and \nHealth Sciences. His current \nresearch interests include multi-\nmode molecular imaging, medi-\ncal image processing, and imag-\ning genomics.\n137Medical & Biological Engineering & Computing (2023) 61:129–137",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7444049119949341
    },
    {
      "name": "Segmentation",
      "score": 0.6825443506240845
    },
    {
      "name": "Computer science",
      "score": 0.6646959185600281
    },
    {
      "name": "Image segmentation",
      "score": 0.6196119785308838
    },
    {
      "name": "Computer vision",
      "score": 0.5769951939582825
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.52276611328125
    },
    {
      "name": "Artificial neural network",
      "score": 0.5012028217315674
    },
    {
      "name": "Transformer",
      "score": 0.48461470007896423
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.45856890082359314
    },
    {
      "name": "Generalization",
      "score": 0.4109160304069519
    },
    {
      "name": "Image texture",
      "score": 0.41036418080329895
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3902134895324707
    },
    {
      "name": "Mathematics",
      "score": 0.1489650309085846
    },
    {
      "name": "Engineering",
      "score": 0.09890902042388916
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}