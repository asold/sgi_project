{
  "title": "Fostering effective hybrid human-LLM reasoning and decision making",
  "url": "https://openalex.org/W4406188271",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5066187890",
      "name": "Andrea Passerini",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5080213009",
      "name": "Aryo Pradipta Gema",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5019106673",
      "name": "Pasquale Minervini",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5090706427",
      "name": "Burcu Sayin",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5062348600",
      "name": "Katya Tentori",
      "affiliations": [
        "University of Trento"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3046399757",
    "https://openalex.org/W2038550763",
    "https://openalex.org/W2034735942",
    "https://openalex.org/W4400502438",
    "https://openalex.org/W4361793541",
    "https://openalex.org/W2096281683",
    "https://openalex.org/W4389766097",
    "https://openalex.org/W6849479259",
    "https://openalex.org/W4287752385",
    "https://openalex.org/W2245445785",
    "https://openalex.org/W4386913659",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4391591669",
    "https://openalex.org/W2266195732",
    "https://openalex.org/W4323275521",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3159250634",
    "https://openalex.org/W4391801015",
    "https://openalex.org/W4283644319",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4294628862",
    "https://openalex.org/W4386369961",
    "https://openalex.org/W6852405840",
    "https://openalex.org/W6859216173",
    "https://openalex.org/W6872864937",
    "https://openalex.org/W4399365040",
    "https://openalex.org/W4392616695",
    "https://openalex.org/W4392976364",
    "https://openalex.org/W4386555477",
    "https://openalex.org/W4392619035",
    "https://openalex.org/W4391272060",
    "https://openalex.org/W6678447614",
    "https://openalex.org/W3159096324",
    "https://openalex.org/W6858355980",
    "https://openalex.org/W4386942462",
    "https://openalex.org/W3125633690",
    "https://openalex.org/W4283167699",
    "https://openalex.org/W4390043316",
    "https://openalex.org/W4392271152",
    "https://openalex.org/W1980063309",
    "https://openalex.org/W4396781989",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W6738348428",
    "https://openalex.org/W4404307418",
    "https://openalex.org/W2995523160",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W4390437280",
    "https://openalex.org/W4312353379",
    "https://openalex.org/W4402671174",
    "https://openalex.org/W4393722778",
    "https://openalex.org/W6799773180",
    "https://openalex.org/W4210716541",
    "https://openalex.org/W1979410064",
    "https://openalex.org/W4383473036",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W6866029319",
    "https://openalex.org/W4389713766",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6859232797",
    "https://openalex.org/W4392120736",
    "https://openalex.org/W4404356212",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W2188398335",
    "https://openalex.org/W2786935060",
    "https://openalex.org/W6850423384",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W4394958222",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4379928343",
    "https://openalex.org/W4389518711",
    "https://openalex.org/W4383173625",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4392297945",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W6801617135",
    "https://openalex.org/W4378945636",
    "https://openalex.org/W4379259948",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W4391848979",
    "https://openalex.org/W6852407983",
    "https://openalex.org/W6811202154",
    "https://openalex.org/W4393213647",
    "https://openalex.org/W6755123312",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W6853508928",
    "https://openalex.org/W2042612929",
    "https://openalex.org/W6776957785",
    "https://openalex.org/W4403236652",
    "https://openalex.org/W4290994954",
    "https://openalex.org/W2978181611",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W4391013695",
    "https://openalex.org/W2075585362",
    "https://openalex.org/W4386273460",
    "https://openalex.org/W6782482366",
    "https://openalex.org/W4386114306",
    "https://openalex.org/W2141538250",
    "https://openalex.org/W6853526537",
    "https://openalex.org/W4381551003",
    "https://openalex.org/W4388787482",
    "https://openalex.org/W4386721614",
    "https://openalex.org/W4387725416",
    "https://openalex.org/W4399454791",
    "https://openalex.org/W4396570449",
    "https://openalex.org/W4401043365",
    "https://openalex.org/W4381733251",
    "https://openalex.org/W4387892136",
    "https://openalex.org/W4387156634",
    "https://openalex.org/W4378473839",
    "https://openalex.org/W6794173217",
    "https://openalex.org/W4398143508",
    "https://openalex.org/W6838464964",
    "https://openalex.org/W4392735839",
    "https://openalex.org/W2123713131",
    "https://openalex.org/W2109985460",
    "https://openalex.org/W2958514452",
    "https://openalex.org/W4403511027",
    "https://openalex.org/W1856775427",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W4391420959",
    "https://openalex.org/W4307320043",
    "https://openalex.org/W4389521014",
    "https://openalex.org/W4285146641",
    "https://openalex.org/W4390898062",
    "https://openalex.org/W6850912789",
    "https://openalex.org/W4390962930",
    "https://openalex.org/W6852330325",
    "https://openalex.org/W6810162553",
    "https://openalex.org/W4395021982",
    "https://openalex.org/W4386086180",
    "https://openalex.org/W4385714610",
    "https://openalex.org/W6777139412",
    "https://openalex.org/W6797529501",
    "https://openalex.org/W4392019609",
    "https://openalex.org/W6853562202",
    "https://openalex.org/W4392822502",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W4392240262",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W4404310812",
    "https://openalex.org/W4393118716",
    "https://openalex.org/W4387294262",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4389764468",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W3212567773",
    "https://openalex.org/W4392669753",
    "https://openalex.org/W4402955069",
    "https://openalex.org/W4385570681",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2618169590",
    "https://openalex.org/W4385572016",
    "https://openalex.org/W4300980406",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4240224338",
    "https://openalex.org/W2560674852",
    "https://openalex.org/W4385572775",
    "https://openalex.org/W3173618889",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4402671231",
    "https://openalex.org/W2978464814",
    "https://openalex.org/W4315882015",
    "https://openalex.org/W4385571690",
    "https://openalex.org/W4389518991",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4401042525",
    "https://openalex.org/W4399414044",
    "https://openalex.org/W4396833218",
    "https://openalex.org/W4381797997",
    "https://openalex.org/W4393213350",
    "https://openalex.org/W4385262268",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4386566737"
  ],
  "abstract": "The impressive performance of modern Large Language Models (LLMs) across a wide range of tasks, along with their often non-trivial errors, has garnered unprecedented attention regarding the potential of AI and its impact on everyday life. While considerable effort has been and continues to be dedicated to overcoming the limitations of current models, the potentials and risks of human-LLM collaboration remain largely underexplored. In this perspective, we argue that enhancing the focus on human-LLM interaction should be a primary target for future LLM research. Specifically, we will briefly examine some of the biases that may hinder effective collaboration between humans and machines, explore potential solutions, and discuss two broader goals—mutual understanding and complementary team performance—that, in our view, future research should address to enhance effective human-LLM reasoning and decision-making.",
  "full_text": "TYPE Perspective\nPUBLISHED /zero.tnum/eight.tnum January /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nOPEN ACCESS\nEDITED BY\nPradeep K. Murukannaiah,\nDelft University of Technology, Netherlands\nREVIEWED BY\nMichiel Van Der Meer,\nIdiap Research Institute, Switzerland\n*CORRESPONDENCE\nAndrea Passerini\nandrea.passerini@unitn.it\nRECEIVED /one.tnum/four.tnum July /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /one.tnum/nine.tnum December /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /zero.tnum/eight.tnum January /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nPasserini A, Gema A, Minervini P, Sayin B and\nTentori K (/two.tnum/zero.tnum/two.tnum/five.tnum) Fostering eﬀective hybrid\nhuman-LLM reasoning and decision making.\nFront. Artif. Intell./seven.tnum:/one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Passerini, Gema, Minervini, Sayin and\nTentori. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The\nuse, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nFostering eﬀective hybrid\nhuman-LLM reasoning and\ndecision making\nAndrea Passerini/one.tnum*, Aryo Gema /two.tnum, Pasquale Minervini /two.tnum,\nBurcu Sayin/one.tnumand Katya Tentori /three.tnum\n/one.tnumDepartment of Information Engineering and Computer Science, Uni versity of Trento, Trento, Italy,\n/two.tnumSchool of Informatics, University of Edinburgh, Edinburgh, United Kingdom, /three.tnumCenter for Mind/Brain\nSciences, University of Trento, Trento, Italy\nThe impressive performance of modern Large Language Models (LL Ms) across\na wide range of tasks, along with their often non-trivial errors , has garnered\nunprecedented attention regarding the potential of AI and its impact on everyday\nlife. While considerable eﬀort has been and continues to be ded icated to\novercoming the limitations of current models, the potentials an d risks of\nhuman-LLM collaboration remain largely underexplored. In th is perspective,\nwe argue that enhancing the focus on human-LLM interaction should b e a\nprimary target for future LLM research. Speciﬁcally, we will brieﬂ y examine some\nof the biases that may hinder eﬀective collaboration between hum ans and\nmachines, explore potential solutions, and discuss two broader goals—mutual\nunderstanding and complementary team performance—that, in ou r view,\nfuture research should address to enhance eﬀective human-LLM re asoning\nand decision-making.\nKEYWORDS\nhybrid intelligence, human-AI collaboration, LLMs, biases, mutual understanding,\ncomplementary team performance\n/one.tnum Introduction\nThe release of chatGPT has raised unprecedented attention and generated high\nexpectations on the capabilities of AI systems that leverage large language model (LLM)\ntechnologies. These systems have demonstrated impressive results across a wide range of\ntasks (\nLiu et al., 2023b ; Yang et al., 2024 ), such as language translation ( Jiao et al., 2023 ),\ntext summarization ( Pu and Demberg, 2023 ), question-answering ( Bahak et al., 2023 ),\nreasoning ( Bang et al., 2023 ), and text generation ( Chen et al., 2023b ; Jeblick et al., 2022 ),\nprompting questions about the potential emergence of “thinking machines” and artiﬁcial\ngeneral intelligence sparks (\nBubeck et al., 2023 ). However, several studies have highlighted\nthe limitations of these systems. Just to provide a few examples, they have been shown to\nprovide entirely fabricated information (\nHuang et al., 2023 ), to exhibit sensitivity to small\nchanges in the way questions are posed ( Pezeshkpour and Hruschka, 2023 ), and to agree\nwith human opinions regardless of content ( Sharma et al., 2023 ).\nHuman beings are also far from being entirely rational, and not in an obvious way.\nThe deviations of human reasoning from normative benchmarks create an intriguing\npuzzle that is not yet completely understood in Cognitive Science. On the one hand,\nsystematic and persistent biases manifest even in well-motivated and expert individuals\nengaged in simple, high-stakes probability tasks (\nBaron, 2023 ). This suggests that\nreasoning errors do not stem from carelessness, computational limitations, or lack of\neducation, nor are they necessarily caused by “external constraints” such as inadequate\ninformation or time pressure. On the other hand, individuals are often capable of complex\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\ninferences ( Tenenbaum et al., 2011 ; Mastropasqua et al., 2010 ).\nIn particular, evidential reasoning—i.e., the assessments of\nthe perceived impact of evidence—appears to be quite\neﬀective, demonstrating greater accuracy and consistency\nover time compared to corresponding posterior probability\njudgments (\nTentori et al., 2016 ). This holds true even though,\nfrom a formal standpoint, calculating the former is no easier than\ncalculating the latter.\nNotably, there are solid reasons to believe that neither LLM-\nbased AI systems nor humans will turn into completely rational\nagents anytime soon. With regard to the former, the inherent\nmechanisms of LLMs impose signiﬁcant constraints on their\ncapabilities.\nBender and Koller (2020) and Bender et al. (2021)\ncoined the term “stochastic parrots” to highlight the fact that\nLLMs focus on form over meaning and stressed the diﬃculty of\ngetting the latter from the former. More recently,\nMahowald et al.\n(2024) formalized the problem in terms of the distinction between\nformal and functional linguistic competence, arguing that LLM\narchitectures require substantial modiﬁcations to have a chance\nof achieving the latter. Finally,\nXu et al. (2024b) indicated that\ninconsistencies between LLMs and the real world are, to some\nextent, inevitable. In a similar vein, eﬀorts to enhance human\nrationality by using visual aids (\nKhan et al., 2015 ), promoting\naccountability (e.g., Boissin et al., 2023 ), or shaping external\nenvironments (e.g., nudging, Thaler and Sunstein, 2009 ) have often\nyielded modest results that are not easily generalizable to other\ncontexts (\nChater and Loewenstein, 2023 ). The limited eﬀectiveness\nof these interventions suggests that the causes of reasoning biases\nare deeply ingrained in our cognitive processes, and we cannot\nexpect to eradicate them, at least not in the near future.\nHumans and LLMs are not only imperfect yet highly capable,\nbut they also diﬀer signiﬁcantly in their respective strengths and\nweaknesses (\nChang et al., 2023 ; Shen et al., 2023 ; Felin and Holweg,\n2024; Leivada et al., 2024 ). Thus, while mere interaction between\nthe two does not guarantee success, a carefully designed human-\nLLM synergy has the potential to prevent critical problems and\nachieve results that surpass what either could accomplish alone.\nIndeed, recent research highlights human-LLM collaboration\nas a key direction toward realizing genuinely human-centered\nAI. (\nDellermann et al., 2019 ; Akata et al., 2020 ; Lawrence, 2024 ;\nWang et al., 2024 ; Ma et al., 2024 ; Liao and Wortman Vaughan,\n2024). However, in our view, eﬀectively addressing this issue\nnecessitates a signiﬁcant shift in perspective. The primary\nchallenge we must confront—and one that will increasingly be\nfaced in the future—lies not so much in the speciﬁc boundaries\nof human rationality or the current technological limitation of\nLLMs, but rather in the nature and severity of biases that can arise\nfrom their interaction. For this reason, we do not aim to provide\nan exhaustive list of the many cognitive biases that individuals—\nand, in some cases, LLMs—exhibit. Instead, we will focus on\nthree major problems of LLMs— hallucinations, inconsistencies\nand sycophancy—demonstrating how they can impact the\ninterplay with humans. We will then discuss two key desiderata,\nmutual understanding and complementary team performance ,\nwhich, in our opinion, future research should address more\ncomprehensively to foster eﬀective human-LLM reasoning and\ndecision-making.\n/two.tnum Potential weaknesses in human-LLM\ninteraction\nOne of the most well-known problems of LLMs is hallucination,\nwhich refers to their distinct possibility of generating outputs that\ndo not align with factual reality or the input context (\nHuang\net al., 2023 ). Hallucinations in LLMs have several causes, from\nﬂawed data sources ( Lin et al., 2022b ) to architectural biases ( Li\net al., 2023b ; Liu et al., 2023a ). To exacerbate the issue, when\nLLMs engage in hallucination, they maintain an aura of authority\nand credibility by generating responses that appear coherent and\nwell-formed in terms of natural language structure (\nBerberette\net al., 2024 ; Su et al., 2024 ). Such a behavior can easily lead\nto an automation bias (Cummings, 2012 ), where users tend to\nover-rely on information and suggestions from automated systems\ncompared to those from their peers. Indeed, while people can\neasily detect nonsensical or blatantly unrelated outputs from LLMs\nwhen they have a good knowledge of the topic, they are more\nlikely to overlook such errors when they lack expertise in the\nsubject. This creates a paradox: one must already possess the\ncorrect answer to reliably avoid being misled by LLMs. Nonetheless,\nexpertise itself is not a guarantee that everything will go smoothly.\nHumans, including professionals such as, for example, physicians,\noften exhibit a tendency known as overconﬁdence (\nHoﬀrage,\n2022), where they tend to overestimate their abilities or the\naccuracy of their knowledge. Predicting which of these somewhat\nopposite attitudes would prevail in a given interaction between\nhumans and LLMs is extremely diﬃcult. LLMs could, in principle,\ncounteract overconﬁdence by providing negative feedback to users.\nHowever, what might seem like an easy solution runs into another\ncharacteristic of these systems: their tendency toward sycophancy,\nwhich is the inclination to please users by generating responses\nthat are agreeable rather than strictly accurate, especially when\ntrained with biased human feedback or tasked with generating\ncontent in subjective domains (\nSharma et al., 2023 ; Ranaldi\nand Pucci, 2023 ; Wei et al., 2023 ). Furthermore, overly critical\nfeedback may lead to algorithm aversion bias (Dietvorst et al.,\n2014), where users disregard information that conﬂicts with\ntheir previous beliefs, even when it is actually pertinent and\ncorrect. This bias reﬂects the skepticism with which humans—\nespecially professionals in high-stakes ﬁelds like healthcare and\nlaw, where accountability is paramount—often view the advanced\ncapabilities of LLMs (\nPark et al., 2023 ; Cheong et al., 2024 ;\nChoudhury and Chaudhry, 2024 ; Eigner and Händler, 2024 ;\nWatters and Lemanski, 2023 ). Additionally, algorithm aversion\nmay be fueled by a loss of conﬁdence following unsatisfactory\ninitial interactions (\nHuang et al., 2024 ; McGrath et al., 2024 ).\nIn particular, LLM inconsistency—reﬂected in their tendency to\nproduce varying outputs for very similar (or even identical)\ninputs—can easily leave lasting impressions of unreliability. This\nissue is exacerbated by high prompt sensitivity, where the LLM\ntend to provide diﬀerent answers even with slight changes in\nhow questions are phrased (\nPezeshkpour and Hruschka, 2023 ;\nVoronov et al., 2024 ; Mao et al., 2024 ; Sayin et al., 2024 ). As\na consequence, individuals may become increasingly reluctant\nto utilize LLMs when confronted with important reasoning and\ndecision-making tasks.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nLet us now consider a situation that, in principle, would\nbe expected to unfold more smoothly—namely, one in which\nneither LLMs nor humans are outright incorrect. It might be\nassumed that accuracy alone would suﬃce to prevent errors;\nhowever, unfortunately, this is not necessarily the case. A well-\nknown bias that could persist or even intensify in interactions\nwhere humans feel competent and LLMs provide reliable evidence\nis conﬁrmation bias : the tendency to selectively seek, interpret,\nand recall information that supports existing beliefs (\nNickerson,\n1998). Indeed, when users query LLMs based on initial hypotheses\nand the models provide selective answers mainly based on local\ncontext, a vicious cycle can be fueled. A closely related cognitive\nbias that may similarly be exacerbated in interactions with LLMs\nis belief bias , that is the tendency to conﬂate the validity of an\nargument with the conﬁdence placed in its conclusion (\nEvans\net al., 1983 ). For instance, users might fail to realize that evidence\nobtained too easily, thanks to the “eﬃciency” of LLMs in supporting\na cherished hypothesis, is not as comprehensive or conclusive\nwith respect to the hypothesis in question as it may seem.\nAnother risk is overestimating redundant information : without full\ncontrol over the sources LLMs draw from, users may overlook\nredundancy and mistakenly believe they are gaining new evidence\nto support a particular belief or prediction, when in fact they are\nnot (\nBohren, 2016). Similarly, interactions between individuals and\nLLMs might be susceptible to the so-called anchoring to initial\nhypotheses or inquiries ( Tversky and Kahneman, 1974 ), as well as\nto order eﬀects (Hogarth and Einhorn, 1992 ). These biases refer,\nrespectively, to the tendency to rely excessively on reference points\n(even if irrelevant) when making estimates, and to assign greater\nimportance to, or better recall, the ﬁrst or last pieces of information\nencountered, at the expense of less available content.\nEx-post evaluation of interactions between human reasoners\nand LLMs (i.e., the assessment of their interactions after they have\ntaken place) is not immune to errors either. Among the major\nissues, one cannot help but consider the well-known hindsight\nbias, which is the tendency to perceive events, once they have\noccurred, as more predictable than they actually were (\nArkes,\n2013). For instance, individuals might overestimate the accuracy\nof LLM predictions simply because they overlook how often the\noriginal outputs of these models are tentative and inconclusive.\nSimilarly, due to the selective information provided by the models,\nindividuals may underestimate their own initial uncertainties.\nThe concern is that if this misinterpretation of the interaction\noccurs collaboratively, biases like the one discussed above could be\nreinforced rather than mitigated.\nIn conclusion, interactions between the LLM and the user can\namplify their inherent weaknesses or even create new ones. This\nunderscores the urgent need for methodological innovations that\nintegrate LLM behaviors with new, interactively designed solutions;\nwithout this, they may fail or even backﬁre.\n/three.tnum Toward eﬀective human-LLM\ninteraction\nIn this section, we will ﬁrst present potential solutions to\nthree major challenges of LLMs: hallucinations, inconsistencies,\nand sycophancy. We will then discuss how fostering mutual\nunderstanding and enhancing complementary team performance\nare crucial for achieving eﬀective collaboration in reasoning and\ndecision-making between humans and LLMs.\n/three.tnum./one.tnum Detecting and mitigating the impact of\nhallucinations\nHallucinations are extensively studied in the ﬁeld of Natural\nLanguage Processing (NLP), with various approaches proposed\nto prevent, detect, or mitigate their occurrence (\nHuang et al.,\n2023; Ji et al., 2023a ; Rawte et al., 2023 ; Zhang et al., 2023 ).\nFollowing Huang et al. (2023) , we categorize hallucinations into\nfactuality hallucinations , where the model generates responses\nthat contradict real-world facts, and faithfulness hallucinations ,\nwhere the model’s responses are not aligned with user instructions\nor the provided context. The latter can be further divided into\nintrinsic hallucinations, involving responses that directly contradict\nthe context, and extrinsic hallucinations , in which the generated\ncontent cannot be veriﬁed or refuted based on the context (\nMaynez\net al., 2020 ).\nOne way to improve the factuality of model-generated content\nis via retrieval augmented generation (Lewis et al., 2020 ), which\nconditions the generation process on documents retrieved from a\ncorpus such as Wikipedia or Pubmed (\nShuster et al., 2021 ; Xiong\net al., 2024 ; Zakka et al., 2024 ). However, LLMs can still disregard\nprovided information and rely on their parametric knowledge\ndue to intrinsic mechanisms (\nJin et al., 2024 ; Xu et al., 2024a )\nor sensitivity to prompts ( Liu et al., 2024 ). Another solution\nis adapting the generation process—referred to as decoding—to\nproduce more factual responses ( Lee et al., 2022 ; Burns et al., 2023 ;\nMoschella et al., 2023 ; Li et al., 2023a ; Chuang et al., 2023 ), and\npost-editing to reﬁne the originally generated content, leveraging\nthe self-correction capabilities of LLMs (\nDhuliawala et al., 2023 ; Ji\net al., 2023b). Decoding can also be adapted to generate outputs that\nare more faithful to the user instructions or the provided context.\nRecent eﬀorts to mitigate faithfulness hallucinations focus on\ntwo main areas: context consistency , which aims to improve the\nalignment of model-generated responses with user instructions and\nthe provided context (\nTian et al., 2019 ; van der Poel et al., 2022 ;\nWan et al., 2023 ; Shi et al., 2023 ; Gema et al., 2024 ; Zhao et al.,\n2024b); and logical consistency , which seeks to ensure logically\ncoherent responses in multi-step reasoning tasks ( Wang et al.,\n2023a). Decoding-based methods can be coupled with post-hoc\nhallucination detection approaches ( Manakul et al., 2023 ; Min et al.,\n2023; Mishra et al., 2024 ) to deﬁne a reward model and adaptively\nincrease the likelihood of hallucination-free generations ( Wan\net al., 2023 ; Amini et al., 2024 ; Lu et al., 2022 , 2023; Deng\nand Raﬀel, 2023 ). From the user’s perspective, a crucial factor\nin reducing LLM hallucinations is ensuring that queries are\nwell-constructed, unambiguous, and as speciﬁc as possible, since\nvague or poorly phrased prompts can increase the likelihood of\nhallucinations (\nWatson and Cho, 2024 ).\nAlthough the solutions discussed above can help reduce\nhallucinations, they will remain, to some extent, inevitable due to\nthe complexity of the world that LLMs attempt to capture (\nXu\net al., 2024b ). A complementary approach is to enhance humans’\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nawareness in managing such occurrences by enabling LLMs\nto provide uncertainty estimates alongside their outputs. The\napproaches implemented so far in this line of research fall into three\ncategories (\nXiong et al., 2024 ): logit-based estimation, verbalization-\nbased estimation , and consistency-based estimation . Logit-based\nestimation requires access to the model logits and typically\nmeasures uncertainty by calculating token-level probability or\nentropy (\nGuo et al., 2017 ; Kuhn et al., 2023 ). Verbalize-based\nestimation works by directly requesting LLMs to express their\nuncertainty via prompting strategy (\nMielke et al., 2022 ; Lin\net al., 2022a ; Xiong et al., 2024 ; Kadavath et al., 2022 ). Finally,\nconsistency-based estimation works under the assumption that\nthe most consistent response signiﬁes the least hallucination in\nthe LLM generations (\nLin et al., 2023 ; Chen and Mueller, 2023 ;\nWang et al., 2023b ; Zhao et al., 2023 ). Additionally, recent studies\nare exploring a new and promising strategy in which LLMs\nlearn to generate citations (\nGao et al., 2023 ; Huang and Chang,\n2023). In this way, users can assess the reliability of the outputs\nprovided by LLMs by examining, and potentially directly accessing,\ntheir sources.\n/three.tnum./two.tnum Improving robustness\nVariability, prompt brittleness, and inconsistencies in LLM\noutputs across diﬀerent conditions, domains, and tasks (\nGupta\net al., 2023 ; Zhou et al., 2024 ; Tytarenko and Amin, 2024 )\npose signiﬁcant challenges for ensuring eﬀective interaction\nwith humans and can substantially exacerbate their algorithmic\naversion. Eﬀorts to enhance the robustness of LLMs have included\nadjustments during training, as well as post-hoc solutions applied\nafter learning has taken place. Regarding the former, recent\nresearch has increasingly recognized the value of including\ndomain experts within development teams (e.g., Med-Gemini in\nhealthcare;\nSaab et al., 2024 , FinMA in ﬁnance; Xie et al., 2023 ,\nand SaulLM; Colombo et al., 2024 ). Post-training techniques aimed\nat mitigating prompt sensitivity while preserving performance\ninclude in-context learning adjustments (\nGupta et al., 2023 ), task-\nspeciﬁc context attribution (Tytarenko and Amin, 2024 ), and batch\ncalibration (Zhou et al., 2024 ).\nAmong the solutions for enhancing LLM robustness are\nthose that directly involve humans, within both perspectives\nmentioned above.\nZhao et al. (2024c) introduced consistency\nalignment training to better align LLM responses with human\nexpectations, ﬁne-tuning LLMs to provide consistent answers to\nparaphrased instructions Post-training methods involving humans\noften focus on improving in-context learning examples to be\ngiven to the LLMs, by coupling input-output pairs with their\ncorresponding human-generated natural language explanations\n(\nHe et al., 2024 ).\nAnother approach to increasing robustness involves\nintroducing an intermediate step between the user and the\nmodel, known as guardrailing (\nInan et al., 2023 ; Rebedea et al.,\n2023), which literally means ’keeping the model on track.’ This\nstep evaluates the input and/or output of LLMs to determine if\nand how certain enforcement actions should be implemented.\nCommon instances include refraining from providing answers that\ncould lead to misuse or blocking responses that contain harmful,\ninappropriate, or biased content.\n/three.tnum./three.tnum Dealing with sycophancy\nSycophancy is a sort of ‘side eﬀect’ of the attempt to maximize\nuser satisfaction and the training of LLMs on datasets that\ninclude texts generated by humans, where interlocutors often\nseek to meet each other’s expectations. This issue with current\nLLMs is, of course, not independent of other limitations, but\nthey can exacerbate one another. Indeed, LLMs often hallucinate\nand become inconsistent in order to appease user prompts,\nespecially when these are misleading. By compelling LLMs not\nto accommodate these prompts, it could thus lead to a reduction\nof multiple limitations. On this line,\nRrv et al. (2024) showed\nhow popular hallucination mitigation strategies can be eﬀectively\nused also to reduce the sycophantic behavior of LLMs in factual\nstatement generation.\nOther solutions to address sycophancy involve ﬁne-tuning\nLLMs over aggregated preferences of multiple humans (\nSharma\net al., 2023 ), generating synthetic ﬁne-tuning data to change\nmodel behavior ( Wei et al., 2023 ) or applying activation editing\nto steer the internal representations of LLMs toward a less\nsycophantic direction (\nPanickssery et al., 2024 ). To preserve the\noriginal capabilities of the LLM as much as possible, Chen et al.\n(2024) propose supervised pinpoint tuning , where ﬁne-tuning is\nconﬁned to speciﬁc LLM modules identiﬁed as responsible for the\nsycophantic behavior.\nFinally,\nCai et al. (2024) proposed a shift in perspective, termed\nantagonistic AI , a provocative counter-narrative to the prevailing\ntrend of designing AI systems to be agreeable and subservient.\nAccording to this approach, human-LLM interactions could beneﬁt\nfrom confrontational LLMs that challenge users, even to the point\nof being blunt if necessary. More speciﬁcally, the authors argue\nthat forcing users to confront their own assumptions would, at\nleast in certain situations, promote critical thinking. This intriguing\nproposal has yet to be implemented or undergo empirical testing.\nComplementary to this,\nTessler et al. (2024) demonstrated that\nLLMs can assist humans in ﬁnding common ground during\ndemocratic deliberation by facilitating eﬀective perspective-taking\namong group members. We believe these approaches could\nindeed help people identify potential pitfalls in their reasoning\nand decision-making processes if complemented by cognition-\naware interaction strategies to avoid exacerbating algorithmic\naversion bias.\n/three.tnum./four.tnum Fostering mutual understanding\nThe blossoming area of Explainable AI (XAI;\nMiller, 2018 ;\nGunning et al., 2019 ; Longo et al., 2024 ) aims at addressing\nthe problem of explaining the outputs of black-box models to\nhumans, focusing either on single predictions ( local explainability)\nor the entire model ( global explainability). Explanatory interactive\nlearning (\nTeso and Kersting, 2019 ) builds upon XAI approaches to\nallow humans to guide machines in learning meaningful predictive\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\npatterns while avoiding confounders and shortcuts. However,\nXAI faces several challenges, from the lack of faithfulness in the\ngenerated explanations (\nCamburu et al., 2019 ) to the impact of\nhuman cognitive biases on evaluating these explanations ( Bertrand\net al., 2022 ), including the risk of increasing automation\nbias ( Bansal et al., 2021 ; Buçinca et al., 2021 ). The opposite\ndirection—helping machines to understand humans—is equally\nchallenging. Eliciting human knowledge proves inherently diﬃcult,\nas it is often implicit, incomplete, or incorrect (\nPatel et al.,\n1999). Expert judgments, although intuitive, depend on rich\nmental models that manage incomplete or conﬂicting information,\ncomplicating the representation of this knowledge for machine\nlearning models (\nKlein et al., 2017 ; Militello and Anders, 2019 ).\nCompared to other black-box models, LLM-based architectures\noﬀer both advantages and disadvantages in terms of mutual\nunderstanding. A clear advantage is their use of natural language\nfor communication, enabling conversational sessions where human\nfeedback is integrated into subsequent interactions. However,\nthis natural mode of interaction can be misleading for human\npartners. Indeed, empirical studies show that users increase\ntheir trust in LLM responses when these are accompanied by\nexplanations, even if the responses are deceptive (\nSharma et al.,\n2024). Although various attempts to foster human-LLM alignment\nthrough training and interaction strategies have been made ( Wang\net al., 2023c ), LLMs still represent concepts through distributional\nsemantics ( Lenci and Sahlgren, 2023 ), which diﬀers signiﬁcantly\nfrom human semantic understanding ( Bender and Koller, 2020 ).\nOne consequence is that, like many other sub-symbolic machine\nlearning models, LLMs are prone to shortcut learning (\nDu et al.,\n2023), a tendency to rely on non-robust features that are spuriously\ncorrelated with ground-truth supervision in the training data, yet\nfail to generalize in out-of-distribution scenarios. XAI approaches\nare starting to shed light on the reasoning mechanisms of\nLLMs (\nZhao et al., 2024a ), but further research is needed for them\nto produce reliable proxies of the trustworthiness of LLM outputs.\nFinally, eﬀective interaction between humans and LLMs\nrequires a form of mutual understanding that involves a theory\nof mind (ToM;\nPremack and Woodruﬀ, 1978 )—the ability to\ninfer what others are thinking and how this diﬀers from our\nown thoughts, a crucial precondition for eﬀective communication\nand cooperation. Recent studies (\nvan Duijn et al., 2023 ; Kosinski,\n2024; Strachan et al., 2024 ) have shown that larger LLMs, such\nas GPT-4, made signiﬁcant progress in ToM, performing on\npar with, and sometimes even surpassing, humans under certain\nconditions. However, this competence primarily reﬂects an ability\nto simulate human-like responses rather than a genuine mastery\nof the cognitive processes involved in ToM reasoning. Achieving\nauthentic ToM in LLMs will require further advancements,\nsuch as leveraging external memory systems (\nLi and Qiu,\n2023; Schuurmans, 2023 ) and, eventually, developing machine\nmetacognition (Johnson et al., 2024 ).\n/three.tnum./five.tnum Targeting complementary team\nperformance\nMachine learning methods are typically evaluated in terms of\ntheir performance as standalone entities. LLMs are no exceptions to\nthis rule and most research focuses on improving their performance\nover pre-deﬁned benchmarks (\nHendrycks et al., 2021 ; Liang et al.,\n2022; Petroni et al., 2021 ; Chiang et al., 2024 ). A recent trend\nhas started to question this perspective, advocating for explicit\ninclusion of the human component in the development and use of\nthese systems (\nDonahue et al., 2022 ; Hemmer et al., 2021 ; Guszcza\net al., 2022 ; Sayin et al., 2023 ). The notion of complementary team\nperformance (CTP; Bansal et al., 2021 ) has been introduced to\nevaluate whether team accuracy is higher than either the human\nor the AI working alone (\nHemmer et al., 2021 , 2024; Campero\net al., 2022 ). Quite interestingly, studies have shown that human-\nAI teams can outperform humans but often do not exceed the\nperformance of AI alone (\nBansal et al., 2021 ; Hemmer et al., 2021 ),\nhighlighting the complexity of achieving good CTP in practice.\nWithin the machine learning community, researchers have\ndeveloped ad hoc learning strategies to improve CTP. The most\npopular is selective classiﬁcation (Geifman and El-Yaniv, 2017 ),\nwhere the machine selectively abstains from providing predictions\nit deems too uncertain. Several selective classiﬁcation strategies\nhave been proposed in the NLP community, especially in question-\nanswering tasks (\nXin et al., 2021 ; Varshney et al., 2022). A limitation\nof selective classiﬁcation is that it does not take into account the\ncharacteristics of the person to whom the prediction is deferred.\nLearning to defer (\nMadras et al., 2018 ) is an advancement over\nselective classiﬁcation, in which human expertise is being modeled\nand accounted for in choosing when to abstain. Learning to\ncomplement (\nWilder et al., 2021 ) further extends this line of\nresearch by designing a training strategy that directly optimizes\nteam performance. The next challenging yet crucial step will be to\nadapt these strategies to handle arbitrary users and general-purpose\nhuman-LLM reasoning and decision-making tasks.\nA major limitation of current solutions for learning to\ndefer/complement is that they rely on a separation of responsibilities\nbetween the human and the machine.\nBanerjee et al. (2024)\nargued that this is suboptimal because it leaves humans completely\nunassisted in the (presumably diﬃcult) cases where the machine\ndefers, while fostering their automation bias when the machine\ndoes not defer. The authors proposed an alternative strategy,\nlearning to guide , in which the machine is trained to provide helpful\nhints to assist the user in making the right decision.\nOther promising research directions include adapting strategies\nthat have been developed and proven eﬀective in other areas of AI\nto LLMs. Among these is conformal prediction (\nAngelopoulos and\nBates, 2023 ), which allows a model to return prediction sets that,\naccording to a user-speciﬁed probability, are guaranteed to contain\nthe ground truth. This has been empirically shown to improve\nhuman decision-making (\nStraitouri et al., 2023 ; Cresswell et al.,\n2024), and it is beginning to be extended to LLM architectures\n[conformal language modeling (Quach et al., 2024 )]. Another\napproach is mixed-initiative interaction (Allen et al., 1999 ; Barnes\net al., 2015 ), where each agent contributes its strengths to the\ntask, with its level of engagement dynamically adjusted to the\nspeciﬁc issue at hand. Recent studies have introduced methods\nfor formalizing prompt construction to enable controllable\nmixed-initiative dialogue generation (\nChen et al., 2023a ). Finally,\nargumentative decision making (Amgoud and Prade, 2009 ) applies\nargumentation theory to enhance team performance by structuring\ninteractions as sequences of arguments and counter-arguments.\nRecently, argumentative LLMs (\nFreedman et al., 2024 ) have\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nbeen proposed and tested as a method using LLMs to construct\nformal argumentation frameworks that support reasoning\nin decision-making.\n/four.tnum Conclusion\nA human-centered approach to AI has been increasingly\npromoted by governmental institutions ( European Commission,\n2020), with legal requirements in many countries mandating\nhuman oversight for high-stakes applications ( Government\nof Canada, 2019 ; European Commission, 2021 ). Building\non this perspective, we have discussed a range of strategies\nthrough which the main limitations of current LLMs could be\naddressed and proposed two fundamental desiderata—mutual\nunderstanding and complementary team performance—that, in\nour view, should guide future research on LLMs and beyond.\nIndeed, while this manuscript focuses on LLMs due to their\nwidespread adoption, including among lay users, many of\nthe points raised may well apply to multimodal and general-\npurpose foundation models (\nSun et al., 2024 ) when interacting\nwith humans.\nThe advocated shift in perspective would require greater\ninvolvement of cognitive scientists in shaping approaches\nto overcome LLM limitations and assess their eﬀectiveness,\nsigniﬁcantly altering priorities regarding problems and goals for\nthe success of LLMs. Future work could explore new evaluation\nmetrics inspired by cognitive science to better measure the\neﬀectiveness of these approaches. Indeed, only by combining the\nknowledge and exploiting the strengths of both humans and LLMs\ncan we have a real chance to achieve a true partnership—one that\nis not only more eﬀective in reducing human-machine biases but\nalso more transparent and fair.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nAP: Conceptualization, Funding acquisition, Supervision,\nWriting - original draft, Writing - review & editing. AG:\nConceptualization, Writing - original draft. BS: Conceptualization,\nWriting - original draft. PM: Supervision, Writing - review\n& editing, Conceptualization. KT: Conceptualization, Writing -\noriginal draft, Writing - review & editing, Supervision.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. Funded\nby the European Union, Grant Agreement No. 101120763-\nTANGO. AP acknowledges the support of the MUR PNRR\nproject FAIR-Future AI Research (PE00000013) funded by the\nNextGenerationEU. AG was supported by the United Kingdom\nResearch and Innovation (grant EP/S02431X/1), UKRI Centre for\nDoctoral Training in Biomedical AI at the University of Edinburgh,\nSchool of Informatics.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nThe author(s) declared that they were an editorial board\nmember of Frontiers, at the time of submission. This had no impact\non the peer review process and the ﬁnal decision.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nAuthor disclaimer\nViews and opinions expressed are however those of the\nauthor(s) only and do not necessarily reﬂect those of the European\nUnion or the European Health and Digital Executive Agency\n(HaDEA). Neither the European Union nor the granting authority\ncan be held responsible for them.\nReferences\nAkata, Z., Balliet, D., de Rijke, M., Dignum, F., Dignum, V., Ei ben, G., et al.\n(2020). A research agenda for hybrid intelligence: augmentin g human intellect with\ncollaborative, adaptive, responsible, and explainable artiﬁcial i ntelligence. Computer 53,\n18–28. doi: 10.1109/MC.2020.2996587\nAllen, J., Guinn, C., and Horvtz, E. (1999). Mixed-initiative i nteraction. IEEE Intell.\nSyst. Their Appl . 14, 14–23.\nAmgoud, L., and Prade, H. (2009). Using arguments for making and explaining\ndecisions. Artif. Intell. 173, 413–436. doi: 10.1016/j.artint.2008.11.006\nAmini, A., Vieira, T., and Cotterell, R. (2024). Variational b est-\nof-N alignment. CoRR, abs/2407.06057 . doi: 10.48550/arXiv.2407.\n06057\nAngelopoulos, A. N., and Bates, S. (2023). Conformal Prediction: A Gentle\nIntroduction. Norwell, MA: Now Foundations and Trends.\nArkes, H. (2013). The consequences of the hindsight bias in me dical decision\nmaking. Curr. Direct. Psychol. Sci . 22, 356–360. doi: 10.1177/09637214134\n89988\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nBahak, H., Taheri, F., Zojaji, Z., and Kazemi, A. (2023). Eva luating chatGPT as a\nquestion answering system: a comprehensive analysis and compar ison with existing\nmodels. arXiv, abs/2312.07592. doi: 10.48550/arXiv.2312.07592\nBanerjee, D., Teso, S., Sayin, B., and Passerini, A. (2024). Learning to Guide Human\nDecision Makers With Vision-Language Models .\nBang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., et al. (2023).\n“A multitask, multilingual, multimodal evaluation of ChatGPT on re asoning,\nhallucination, and interactivity, ” in Proceedings of the 13th International Joint\nConference on Natural Language Processing and the 3rd Conference of the Asia- Paciﬁc\nChapter of the Association for Computational Linguistics (Volume 1: L ong Papers), eds.\nJ. C. Park, Y. Arase, B. Hu, W. Lu, D. Wijaya, A. Purwarianti, e t al. (Nusa Dua:\nAssociation for Computational Linguistics), 675–718.\nBansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., et al. (2021). Does\nthe whole exceed its parts? the eﬀect of ai explanations on compleme ntary team\nperformance. arXiv, abs/2006.14779. doi: 10.48550/arXiv.2006.14779\nBarnes, M. J., Chen, J. Y., and Jentsch, F. (2015). “Designin g for mixed-initiative\ninteractions between human and autonomous systems in complex environments, ” in\n2015 IEEE International Conference on Systems, Man, and Cybernetics (Hong Kong:\nIEEE), 1386–1390.\nBaron, J. (2023). Thinking and Deciding, 5th Edn . Cambridge: Cambridge University\nPress.\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. ( 2021). “On the\ndangers of stochastic parrots: can language models be too big?” in Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21(New\nYork, NY: Association for Computing Machinery), 610–623.\nBender, E. M., and Koller, A. (2020). “Climbing towards NLU: on m eaning, form,\nand understanding in the age of data, ” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , eds. D. Jurafsky, J. Chai, N. Schluter and J.\nTetreault (Association for Computational Linguistics), 518 5–5198.\nBerberette, E., Hutchins, J., and Sadovnik, A. (2024). Rede ﬁning “hallucination”\nin LLMS: towards a psychology-informed framework for mitigat ing misinformation.\narXiv, abs/2402.01769. doi: 10.48550/arXiv.2402.01769\nBertrand, A., Belloum, R., Eagan, J. R., and Maxwell, W. (2022). “ How cognitive\nbiases aﬀect XAI-assisted decision-making: a systematic r eview, ” inProceedings of the\n2022 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’22 (New York, NY:\nAssociation for Computing Machinery), 78–91.\nBohren, J. A. (2016). Informational herding with model miss peciﬁcation. J. Econ.\nTheor. 163, 222–247. doi: 10.1016/j.jet.2016.01.011\nBoissin, E., Caparos, S., and De Neys, W. (2023). No easy ﬁx for belief bias during\nsyllogistic reasoning? J. Cogn. Psychol . 35, 1–21. doi: 10.1080/20445911.2023.2181734\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvit z, E., Kamar, E., et al.\n(2023). Sparks of artiﬁcial general intelligence: early experim ents with GPT-4. arXiv,\nabs/2303.12712. doi: 10.48550/arXiv.2303.12712\nBuçinca, Z., Malaya, M. B., and Gajos, K. Z. (2021). To trust or to think: cognitive\nforcing functions can reduce overreliance on AI in AI-assist ed decision-making. Proc.\nACM Hum. Comput. Interact . 5:3449287. doi: 10.1145/3449287\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. (2023). Discovering Latent Knowledge\nin Language Models Without Supervision .\nCai, A., Arawjo, I., and Glassman, E. L. (2024). Antagonistic A I. ArXiv,\nabs/2402.07350. doi: 10.48550/arXiv.2402.07350\nCamburu, O.-M., Giunchiglia, E., Foerster, J., Lukasiewicz, T., and Blunsom, P.\n(2019). Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods .\nCampero, A., Vaccaro, M., Song, J., Wen, H., Almaatouq, A., and M alone,\nT. W. (2022). A test for evaluating performance in human-comput er systems. arXiv,\nabs/2206.12390. doi: 10.48550/arXiv.2206.12390\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., et al. ( 2023).\nA survey on evaluation of large language models. arXiv, abs/2307.03109 .\ndoi: 10.48550/arXiv.2307.03109\nChater, N., and Loewenstein, G. (2023). The I-frame and the S- frame: how focusing\non individual-level solutions has led behavioral public policy astr ay. Behav. Brain Sci .\n46:e147. doi: 10.1017/s0140525x22002023\nChen, J., and Mueller, J. (2023). Quantifying uncertainty in a nswers from any\nlanguage model via intrinsic and extrinsic conﬁdence assess ment. arXiv preprint\narXiv:2308.16175. doi: 10.48550/arXiv.2308.16175\nChen, M., Yu, X., Shi, W., Awasthi, U., and Yu, Z. (2023a). “Co ntrollable mixed-\ninitiative dialogue generation through prompting, ” in Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 2: S hort Papers) ,\neds. A. Rogers, J. Boyd-Graber, and N. Okazaki (Toronto, ON: A ssociation for\nComputational Linguistics), 951–966.\nChen, N., Wang, Y., Jiang, H., Cai, D., Li, Y., Chen, Z., et al. ( 2023b). “Large language\nmodels meet Harry Potter: a dataset for aligning dialogue agent s with characters, ”\nin Findings of the Association for Computational Linguistics: EMNL P 2023 , eds. H.\nBouamor, J. Pino, and K. Bali (Singapore: Association for Compu tational Linguistics),\n8506–8520.\nChen, W., Huang, Z., Xie, L., Lin, B., Li, H., Lu, L., et al. (202 4). “From yes-men to\ntruth-tellers: addressing sycophancy in large language models wi th pinpoint tuning, ” in\nForty-ﬁrst International Conference on Machine Learning .\nCheong, I., Xia, K., Feng, K. J. K., Chen, Q. Z., and Zhang, A. X . (2024). “(A)I\nam not a lawyer, but...: engaging legal experts towards responsi ble LLM policies for\nlegal advice, ” in Proceedings of the 2024 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT ’24 (New York, NY: Association for Computing Machinery),\n2454–2469.\nChiang, W., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li , D., et al. (2024).\nChatbot arena: an open platform for evaluating LLMs by human prefe rence. CoRR,\nabs/2403.04132. doi: 10.48550/arXiv.2403.04132\nChoudhury, A., and Chaudhry, Z. (2024). Large language models and\nuser trust: consequence of self-referential learning loop and t he deskilling\nof health care professionals. J. Med. Internet Res . 26:56764. doi: 10.2196/\n56764\nChuang, Y., Xie, Y., Luo, H., Kim, Y., Glass, J. R., and He, P. (20 23). DoLa:\ndecoding by contrasting layers improves factuality in large lang uage models. CoRR,\nabs/2309.03883. doi: 10.48550/arXiv.2309.03883\nColombo, P., Pires, T. P., Boudiaf, M., Culver, D., Melo, R., Cor ro, C., et al.\n(2024). SaulLM-7B: a pioneering large language model for law. CoRR, abs/2403.03883.\ndoi: 10.48550/arXiv.2403.03883\nCresswell, J. C., Sui, Y., Kumar, B., and Vouitsis, N. (2024). C onformal\nprediction sets improve human decision making. arXiv, 2401.13744 .\ndoi: 10.48550/arXiv.2401.13744\nCummings, M. (2012). “Automation bias in intelligent time cri tical decision support\nsystems, ” in Collection of Technical Papers—AIAA 1st Intelligent Systems Technical\nConference.\nDellermann, D., Ebel, P., Sollner, M., and Leimeister, J. M. (2019 ). Hybrid\nintelligence. Bus. Inform. Syst. Eng . 61, 637–643. doi: 10.1007/s12599-019-00595-2\nDeng, H., and Raﬀel, C. (2023). “Reward-augmented decoding: eﬃcient controlled\ntext generation with a unidirectional reward model, ” in EMNLP (Association for\nComputational Linguistics), 11781–11791.\nDhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyi lmaz, A., et al.\n(2023). Chain-of-veriﬁcation reduces hallucination in large language models. CoRR,\nabs/2309.11495. doi: 10.48550/arXiv.2309.11495\nDietvorst, B. J., Simmons, J. P., Massey, C., Dietvorst, B. J ., and Simmons, J. (2014).\nAlgorithm aversion: people erroneously avoid algorithms after se eing them ERR. J. Exp.\nPsychol. 144, 114–126. doi: 10.1037/xge0000033\nDonahue, K., Chouldechova, A., and Kenthapadi, K. (2022). “Hu man-algorithm\ncollaboration: achieving complementarity and avoiding unfair ness, ” inProceedings of\nthe 2022 ACM Conference on Fairness, Accountability, and Transparency, F AccT ’22\n(New York, NY: Association for Computing Machinery), 1639–1 656.\nDu, M., He, F., Zou, N., Tao, D., and Hu, X. (2023). Shortcut lea rning of large\nlanguage models in natural language understanding. Commun. ACM 67, 110—120.\ndoi: 10.1145/3596490\nEigner, E., and Händler, T. (2024). Determinants of LLM-assi sted decision-making.\narXiv, abs/2402.17385. doi: 10.48550/arXiv.2402.17385\nEuropean Commission (2020). White Paper on Artiﬁcial Intelligence: a European\nApproach to Excellence and Trust. White Paper COM(2020) 65 Final . Brussels:\nEuropean Commission.\nEuropean Commission (2021). Proposal for a Regulation Laying Down Harmonised\nRules on Artiﬁcial Intelligence (Artiﬁcial Intelligence Act) .\nEvans, J., Barston, J., and Pollard, P. (1983). On the conﬂict be tween logic and belief\nin syllogistic reasoning. Mem. Cogn. 11, 295–306.\nFelin, T. and Holweg, M. (2024). Theory Is All You Need: AI, Human Cognition, and\nDecision Making.\nFreedman, G., Dejl, A., Gorur, D., Yin, X., Rago, A., and Toni, F. (2024).\nArgumentative large language models for explainable and contesta ble decision-\nmaking. arXiv, 2405.02079. doi: 10.48550/arXiv.2405.02079\nGao, T., Yen, H., Yu, J., and Chen, D. (2023). “Enabling large lan guage\nmodels to generate text with citations, ” in Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing , eds. H. Bouamor, J.\nPino, and K. Bali (Singapore: Association for Computational Li nguistics), 6465–\n6488.\nGeifman, Y., and El-Yaniv, R. (2017). “Selective classiﬁcation for deep neural\nnetworks, ” inAdvances in Neural Information Processing Systems, Vol. 30 , eds. I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanatha n and R. Garnett (Red\nHook, NY: Curran Associates, Inc).\nGema, A. P., Jin, C., Abdulaal, A., Diethe, T., Teare, P., Alex, B. , et al. (2024).\nDeCoRe: decoding by contrasting retrieval heads to mitigat e hallucinations. arXiv\npreprint arXiv:2410.18860. doi: 10.48550/arXiv.2410.18860\nGovernment of Canada (2019). Directive on Automated Decision-Making . Available\nat: https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=32592\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nGunning, D., Steﬁk, M., Choi, J., Miller, T., Stumpf, S., and Yang , G.-\nZ. (2019). XAI–explainable artiﬁcial intelligence. Sci. Robot . 4:eaay7120.\ndoi: 10.1126/scirobotics.aay7120\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). “On ca libration of\nmodern neural networks, ” in International Conference on Machine Learning (Sydney),\n1321–1330.\nGupta, K., Roychowdhury, S., Kasa, S. R., Kasa, S. K., Bhanusha li, A., Pattisapu,\nN., et al. (2023). How robust are LLMs to in-context majority la bel bias? arXiv,\nabs/2312.16549. doi: 10.48550/arXiv.2312.16549\nGuszcza, J., Danks, D., Fox, C., Hammond, K., Ho, D., Imas, A., et al. (2022). Hybrid\nintelligence: a paradigm for more responsible practice. SSRN Electr. J . 2022:4301478.\ndoi: 10.2139/ssrn.4301478\nHe, X., Wu, Y., Camburu, O.-M., Minervini, P., and Stenetorp, P. (2024). “Using\nnatural language explanations to improve robustness of in-cont ext learning, ” in\nProceedings of the 62nd Annual Meeting of the Association for Computati onal Linguistics\n(Volume 1: Long Papers) , eds. L.-W. Ku, A. Martins, and V. Srikumar (Bangkok:\nAssociation for Computational Linguistics), 13477–13499.\nHemmer, P., Schemmer, M., Kühl, N., Vössing, M., and Satzger, G. (2024).\nComplementarity in human-ai collaboration: concept, sources, a nd evidence. arXiv,\nabs/2404.00029. doi: 10.48550/arXiv.2404.00029\nHemmer, P., Schemmer, M., Vössing, M., and Kühl, N. (2021). “H uman-AI\ncomplementarity in hybrid intelligence systems: a structured li terature review, ” in\nPaciﬁc Asia Conference on Information Systems (Dubai).\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., S ong, D., et al.\n(2021). Measuring Massive Multitask Language Understanding . Bangkok: Association\nfor Computational Linguistics.\nHoﬀrage, U. (2022). “Overconﬁdence, ” in Cognitive Illusions: A Handbook on\nFallacies and Biases in Thinking, Judgement and Memory, 3 Edn (Routledge/Taylor &\nFrancis Group), 291–314.\nHogarth, R. M., and Einhorn, H. J. (1992). Order eﬀects in beli ef updating: the\nbelief-adjustment model. Cogn. Psychol. 24, 1–55.\nHuang, J., and Chang, K. C.-C. (2023). Citation: a key to build ing responsible\nand accountable large language models. arXiv preprint arXiv:2307.02185 .\ndoi: 10.48550/arXiv.2307.02185\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., et al. ( 2023). A survey\non hallucination in large language models: principles, taxonomy, ch allenges, and open\nquestions. arXiv, abs/2311.05232. doi: 10.48550/arXiv.2311.05232\nHuang, S.-H., Lin, Y.-F., He, Z., Huang, C.-Y., and Huang, T. -H. K. (2024).\n“How does conversation length impact user’s satisfaction? a c ase study of length-\ncontrolled conversations with LLM-powered chatbots, ” in Extended Abstracts of the\nCHI Conference on Human Factors in Computing Systems, CHI EA ’24 , New York, NY:\nAssociation for Computing Machinery.\nInan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., e t al. (2023). Llama\nguard: LLM-based input-output safeguard for human-ai conver sations. arXiv preprint\narXiv:2312.06674. doi: 10.48550/arXiv.2312.06674\nJeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., Stübe r, A. T., Topalis, J., et al.\n(2022). ChatGPT makes medicine easy to swallow: an exploratory ca se study on\nsimpliﬁed radiology reports. arXiv, abs/2212.14882. doi: 10.1007/s00330-023-10213-1\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., et al. (2023 a). Survey of hallucination\nin natural language generation. ACM Comput. Surv . 55:248. doi: 10.1145/3571730\nJi, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., and Fung, P. (2023b) . “Towards mitigating\nLLM hallucination via self reﬂection, ” in EMNLP (Findings) (Singapore: Association\nfor Computational Linguistics), 1827–1843.\nJiao, W., Wang, W., tse Huang, J., Wang, X., Shi, S., and Tu, Z. (2023). Is ChatGPT\na Good Translator? Yes With GPT-4 as the Engine .\nJin, Z., Cao, P., Chen, Y., Liu, K., Jiang, X., Xu, J., et al. (20 24). Tug-of-war between\nknowledge: exploring and resolving knowledge conﬂicts in retriev al-augmented\nlanguage models. arXiv preprint arXiv:2402.14409 . doi: 10.48550/arXiv.2402.14409\nJohnson, S. G. B., Karimi, A.-H., Bengio, Y., Chater, N., Gerst enberg, T., Larson, K.,\net al. (2024). Imagining and building wise machines: the centr ality of AI metacognition.\narXiv, 2411.02478. doi: 10.48550/arXiv.2411.02478\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., et al. (2022).\nLanguage models (mostly) know what they know. arXiv preprint arXiv:2207.05221 .\ndoi: 10.48550/arXiv.2207.05221\nKhan, A., Breslav, S., Glueck, M., and Hornbæk, K. (2015). Beneﬁ ts of\nvisualization in the mammography problem. Int. J. Hum. Comput. Stud . 83, 94–113.\ndoi: 10.1016/j.ijhcs.2015.07.001\nKlein, G., Shneiderman, B., Hoﬀman, R. R., and Ford, K. M. (2017 ). Why\nexpertise matters: a response to the challenges. IEEE Intell. Syst . 32, 67–73.\ndoi: 10.1109/MIS.2017.4531230\nKosinski, M. (2024). Evaluating Large Language Models in Theory of Mind Tasks .\nKuhn, L., Gal, Y., and Farquhar, S. (2023). “Semantic uncertai nty: linguistic\ninvariances for uncertainty estimation in natural language generation, ” inThe Eleventh\nInternational Conference on Learning Representations, ICLR 2023, K igali, Rwanda, May\n1–5, 2023. Kigali.\nLawrence, C. (2024). Human-centric AI: A Road Map to Human-AI Collaboration .\nAvailable at: https://neclab.eu/technology/case-studies/human-centri city-ai-a-road-\nmap-to-human-ai-collaboration\nLee, N., Ping, W., Xu, P., Patwary, M., Shoeybi, M., and Catan zaro, B.\n(2022). Factuality enhanced language models for open-ended tex t generation. CoRR,\nabs/2206.04624. doi: 10.48550/arXiv.2206.04624\nLeivada, E., Dentella, V., and Günther, F. (2024). Evaluating th e language\nabilities of large language models vs. humans: three caveats. Biolinguistics.\ndoi: 10.5964/bioling.14391\nLenci, A., and Sahlgren, M. (2023). Distributional Semantics. Studies in Natural\nLanguage Processing. Cambridge: Cambridge University Press.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., et al. (2020).\nRetrieval-augmented generation for knowledge-intensive NL P tasks. Adv. Neural\nInform. Process. Syst . 33, 9459–9474. doi: 10.48550/arXiv.2005.11401\nLi, K., Patel, O., Vi’egas, F., Pﬁster, H.-R., and Wattenberg , M. (2023a).\nInference-time intervention: eliciting truthful answers f rom a language model. arXiv,\nabs/2306.03341. doi: 10.48550/arXiv.2306.03341\nLi, X., and Qiu, X. (2023). “MoT: Memory-of-thought enables C hatGPT to self-\nimprove, ” in Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, eds. H. Bouamor, J. Pino, and K. Bali (Singapore: Association for\nComputational Linguistics), 6354–6374.\nLi, Z., Zhang, S., Zhao, H., Yang, Y., and Yang, D. (2023b). Ba tGPT: a bidirectional\nautoregessive talker from generative pre-trained transform er. arXiv, 2307.00360 .\ndoi: 10.48550/arXiv.2307.00360\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasu naga, M.,\net al. (2022). Holistic evaluation of language models. CoRR, abs/2211.09110 .\ndoi: 10.48550/arXiv.2211.09110\nLiao, Q. V., and Wortman Vaughan, J. (2024). AI Transparency in the Age of LLMs:\nA Human-Centered Research Roadmap . Harvard Data Science Review. Available at:\nhttps://hdsr.mitpress.mit.edu/pub/aelql9qy (accessed December 14, 2024).\nLin, S., Hilton, J., and Evans, O. (2022a). Teaching models to e xpress their\nuncertainty in words. arXiv preprint arXiv:2205.14334. doi: 10.48550/arXiv.2205.14334\nLin, S., Hilton, J., and Evans, O. (2022b). “TruthfulQA: measu ring how models\nmimic human falsehoods, ” inProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , eds. S. Muresan, P. Nakov, and\nA. Villavicencio (Dublin: Association for Computational Lingu istics), 3214–3252.\nLin, Z., Trivedi, S., and Sun, J. (2023). Generating with conﬁ dence: uncertainty\nquantiﬁcation for black-box large language models. arXiv preprint arXiv:2305.19187 .\ndoi: 10.48550/arXiv.2305.19187\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. ( 2023a).\nExposing attention glitches with ﬂip-ﬂop language modeling. arXiv, 2306.00946 .\ndoi: 10.48550/arXiv.2306.00946\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., P etroni, F., et al. (2024).\nLost in the middle: how language models use long contexts. Trans. Assoc. Comput.\nLinguist. 12, 157–173. doi: 10.1162/tacl_a_00638\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., et al. (2 023b). Summary of\nchatGPT-related research and perspective towards the future of large language models.\nMeta-Radiology 1:100017. doi: 10.1016/j.metrad.2023.100017\nLongo, L., Brcic, M., Cabitza, F., Choi, J., Confalonieri, R. , Ser, J. D.,\net al. (2024). Explainable artiﬁcial intelligence (XAI) 2.0: a man ifesto of open\nchallenges and interdisciplinary research directions. Inform. Fus . 106:102301.\ndoi: 10.1016/j.inﬀus.2024.102301\nLu, X., Brahman, F., West, P., Jung, J., Chandu, K., Ravichan der, A., et al. (2023).\n“Inference-time policy adapters (IPA): tailoring extreme-sca le LMS without ﬁne-\ntuning, ” in EMNLP (Singapore: Association for Computational Linguistics), 68 63–\n6883.\nLu, X., Welleck, S., West, P., Jiang, L., Kasai, J., Khashabi, D ., et al. (2022).\n“Neurologic a*esque decoding: Constrained text generation wi th lookahead heuristics, ”\nin NAACL-HLT(Seattle: Association for Computational Linguistics), 780– 799.\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., et al. ( 2024). Towards\nhuman-AI deliberation: Design and evaluation of LLM-empowere d deliberative AI for\nAI-assisted decision-making. arXiv, 2403.16812. doi: 10.48550/arXiv.2403.16812\nMadras, D., Pitassi, T., and Zemel, R. (2018). “Predict respon sibly: improving\nfairness and accuracy by learning to defer, ” in Proceedings of the 32nd International\nConference on Neural Information Processing Systems, NIPS’18 (Red Hook, NY: Curran\nAssociates Inc), 6150–6160.\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tene nbaum, J. B., and\nFedorenko, E. (2024). Dissociating language and thought in la rge language models.\nTrends Cogn. Sci . 28, 517–540. doi: 10.1016/j.tics.2024.01.011\nManakul, P., Liusie, A., and Gales, M. (2023). “SelfCheckGPT: ze ro-resource black-\nbox hallucination detection for generative large language mode ls, ” in Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processin g, eds. H.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nBouamor, J. Pino, and K. Bali (Singapore: Association for Compu tational Linguistics),\n9004–9017.\nMao, J., Middleton, S., and Niranjan, M. (2024). “Do prompt posit ions really\nmatter?” in Findings of the Association for Computational Linguistics: NAACL 2024,\neds. K. Duh, H. Gomez, S. Bethard (Mexico City: Association fo r Computational\nLinguistics), 4102–4130.\nMastropasqua, T., Crupi, V., and Tentori, K. (2010). Broadenin g the study of\ninductive reasoning: conﬁrmation judgments with uncertai n evidence. Mem. Cogn. 38,\n941–950. doi: 10.3758/MC.38.7.941\nMaynez, J., Narayan, S., Bohnet, B., and McDonald, R. T. (2020 ). “On faithfulness\nand factuality in abstractive summarization, ” in ACL, eds. D. Jurafsky, J. Chai, N.\nSchluter, Joel Tetreault (Association for Computational Ling uistics), 1906–1919.\nMcGrath, M. J., Cooper, P. S., and Duenser, A. (2024). Users do n ot trust\nrecommendations from a large language model more than AI-sour ced snippets. Front.\nComput. Sci. 6:1456098. doi: 10.3389/fcomp.2024.1456098\nMielke, S. J., Szlam, A., Dinan, E., and Boureau, Y.-L. (2022). Reducing\nconversational agents’ overconﬁdence through linguistic c alibration. Trans. Assoc.\nComput. Linguist. 10, 857–872. doi: 10.1162/tacl_a_00494\nMilitello, L. G., and Anders, S. H. (2019). “Incident-based meth ods for studying\nexpertise, ” inThe Oxford Handbook of Expertise (Oxford Academic), 429–450.\nMiller, T. (2018). Explanation in artiﬁcial intelligence: insigh ts from the social\nsciences. Artif. Intell. 267, 1–38. doi: 10.1016/j.artint.2018.07.007\nMin, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t., Koh, P., et al. (2023).\n“FActScore: ﬁne-grained atomic evaluation of factual precis ion in long form text\ngeneration, ” in Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, eds. H. Bouamor, J. Pino, and K. Bali (Singapore: Association for\nComputational Linguistics), 12076–12100.\nMishra, A., Asai, A., Balachandran, V., Wang, Y., Neubig, G., T svetkov, Y., et al.\n(2024). Fine-grained hallucination detection and editing fo r language models. arXiv,\nabs/2401.06855. doi: 10.48550/arXiv.2401.06855\nMoschella, L., Maiorca, V., Fumero, M., Norelli, A., Locatello, F. , and Rodolá, E.\n(2023). Relative Representations Enable Zero-Shot Latent Space Communicat ion. Kigali:\nICLR.\nNickerson, R. S. (1998). Conﬁrmation bias: a ubiquitous pheno menon in many\nguises. Rev. Gen. Psychol . 2, 175–220.\nPanickssery, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger , E., and Turner, A. M.\n(2024). Steering Llama 2 via Contrastive Activation Addition .\nPark, P. S., Goldstein, S., O’Gara, A., Chen, M., and Hendrycks , D. (2023). AI\ndeception: a survey of examples, risks, and potential solutions. arXiv, abs/2308.14752.\ndoi: 10.48550/arXiv.2308.14752\nPatel, V. L., Arocha, J. F., and Kaufman, D. R. (1999). Expertise and Tacit Knowledge\nin Medicine. Tacit Knowledge in Professional Practice . London: Psychology Press, 75–99.\nPetroni, F., Piktus, A., Fan, A., Lewis, P. S. H., Yazdani, M. , Cao, N. D., et al. (2021).\n“KILT: a benchmark for knowledge intensive language tasks, ” i n NAACL-HLT, eds.\nK. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,\nR. Cotterell, T. Chakraborty, Y. Zhou (Association for Computa tional Linguistics),\n2523–2544.\nPezeshkpour, P., and Hruschka, E. (2023). Large language mode ls sensitivity\nto the order of options in multiple-choice questions. CoRR, abs/2308.11483 .\ndoi: 10.48550/arXiv.2308.11483\nPremack, D., and Woodruﬀ, G. (1978). Does the chimpanzee have a theory of mind?\nBehav. Brain Sci . 1, 515–526.\nPu, D., and Demberg, V. (2023). “ChatGPT vs. human-authored t ext: insights\ninto controllable text summarization and sentence style transf er, ” in Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguis tics (Volume 4:\nStudent Research Workshop), eds. V. Padmakumar, G. Vallejo, and Y. Fu (Toronto, ON:\nAssociation for Computational Linguistics), 1–18.\nQuach, V., Fisch, A., Schuster, T., Yala, A., Sohn, J. H., Jaak kola, T., et al. (2024).\nConformal language modeling. arXiv, abs/2306.10193. doi: 10.48550/arXiv.2306.10193\nRanaldi, L., and Pucci, G. (2023). When large language models cont radict\nhumans? large language models’ sycophantic behaviour. arXiv, abs/2311.09410 .\ndoi: 10.48550/arXiv.2311.09410\nRawte, V., Sheth, A. P., and Das, A. (2023). A survey of hallucin ation in large\nfoundation models. CoRR, abs/2309.05922. doi: 10.48550/arXiv.2309.05922\nRebedea, T., Dinu, R., Sreedhar, M., Parisien, C., and Cohen , J. (2023). NeMo\nguardrails: a toolkit for controllable and safe LLM applications wit h programmable\nrails. arXiv preprint arXiv:2310.10501 .doi: 10.48550/arXiv.2310.10501\nRrv, A., Tyagi, N., Uddin, M. N., Varshney, N., and Baral, C. (2 024). Chaos with\nkeywords: exposing large language models sycophancy to misleadin g keywords and\nevaluating defense strategies. arXiv, abs/2406.03827. doi: 10.48550/arXiv.2406.03827\nSaab, K., Tu, T., Weng, W.-H., Tanno, R., Stutz, D., Wulczyn, E ., et al.\n(2024). Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416 .\ndoi: 10.48550/arXiv.2404.18416\nSayin, B., Minervini, P., Staiano, J., and Passerini, A. (20 24). “Can LLMs correct\nphysicians, yet? Investigating eﬀective interaction metho ds in the medical domain, ” in\nProceedings of the 6th Clinical Natural Language Processing Workshop (Mexico City:\nAssociation for Computational Linguistics), 218–237.\nSayin, B., Yang, J., Passerini, A., and Casati, F. (2023). Va lue-based hybrid\nintelligence. Front. Artif. Intell. Appl . 368, 366–370. doi: 10.3233/FAIA230100\nSchuurmans, D. (2023). Memory Augmented Large Language Models Are\nComputationally Universal.\nSharma, M., Siu, H. C., Paleja, R., and Peña, J. D. (2024). Why Would You Suggest\nThat? Human Trust in Language Model Responses .\nSharma, M., Tong, M., Korbak, T., Duvenaud, D. K., Askell, A., B owman, S. R., et al.\n(2023). Towards understanding sycophancy in language models. arXiv, abs/2310.13548.\ndoi: 10.48550/arXiv.2310.13548\nShen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., et al. (2 023). Large language\nmodel alignment: a survey. arXiv, 2309.15025. doi: 10.48550/arXiv.2309.15025\nShi, W., Han, X., Lewis, M., Tsvetkov, Y., Zettlemoyer, L., an d Yih, S.\n(2023). Trusting your evidence: hallucinate less with context -aware decoding. arXiv,\nabs/2305.14739. doi: 10.48550/arXiv.2305.14739\nShuster, K., Poﬀ, S., Chen, M., Kiela, D., and Weston, J. (2021 ). “Retrieval\naugmentation reduces hallucination in conversation, ” in Findings of the Association for\nComputational Linguistics: EMNLP 2021 , eds. M.-F. Moens, X. Huang, L. Specia, and\nS. W.-t. Yih (Punta Cana: Association for Computational Ling uistics), 3784–3803.\nStrachan, J., Albergo, D., Borghini, G., Pansardi, O., Scaliti , E., Gupta, S., et al.\n(2024). Testing theory of mind in large language models and huma ns. Nat. Hum. Behav.\n24:1882. doi: 10.1038/s41562-024-01882-z\nStraitouri, E., Wang, L., Okati, N., and Rodriguez, M. G. (202 3). “Improving\nexpert predictions with conformal prediction, ” in Proceedings of the 40th International\nConference on Machine Learning, ICML ’23(Honolulu, HI).\nSu, W., Wang, C., Ai, Q., Hu, Y., Wu, Z., Zhou, Y., et al. (2024). Unsupervised\nreal-time hallucination detection based on the internal state s of large language models.\narXiv, abs/2403.06448. doi: 10.48550/arXiv.2403.06448\nSun, J., Zheng, C., Xie, E., Liu, Z., Chu, R., Qiu, J., et al. (20 24). A Survey of Reasoning\nWith Foundation Models.\nTenenbaum, J. B., Kemp, C., Griﬃths, T. L., and Goodman, N. D. (20 11).\nHow to grow a mind: atatistics, structure, and abstraction. Science 331, 1279–1285.\ndoi: 10.1126/science.1192788\nTentori, K., Chater, N., and Crupi, V. (2016). Judging the prob ability of hypotheses\nversus the impact of evidence: which form of inductive infere nce is more accurate and\ntime-consistent? Cogn. Sci. 40, 758–778. doi: 10.1111/cogs.12259\nTeso, S., and Kersting, K. (2019). “Explanatory interactive m achine learning, ” in\nProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES ’19\n(New York, NY: Association for Computing Machinery), 239–24 5.\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadw ick, M. J., Koster, R.,\net al. (2024). AI can help humans ﬁnd common ground in democrati c deliberation.\nScience 386:eadq2852. doi: 10.1126/science.adq2852\nThaler, R., and Sunstein, C. (2009). Nudge: improving decisio ns about health,\nwealth, and happiness. J. Cogn. Psychol . 35, 401–421. doi: 10.1007/s10602-008-9056-2\nTian, R., Narayan, S., Sellam, T., and Parikh, A. P. (2019). Sti cking to the\nfacts: conﬁdent decoding for faithful data-to-text genera tion. CoRR, abs/1910.08684 .\ndoi: 10.48550/arXiv.1910.08684\nTversky, A., and Kahneman, D. (1974). Judgment under uncert ainty: heuristics and\nbiases. Science 185, 1124–1131.\nTytarenko, S., and Amin, M. R. (2024). Breaking free transfo rmer models: task-\nspeciﬁc context attribution promises improved generalizabilit y without ﬁne-tuning\npre-trained LLMs. arXiv, abs/2401.16638. doi: 10.48550/arXiv.2401.16638\nvan der Poel, L., Cotterell, R., and Meister, C. (2022). “Mutual information\nalleviates hallucinations in abstractive summarization, ” in EMNLP, eds. Y. Goldberg,\nZ. Kozareva, Y. Zhang (Abu Dhabi: Association for Computatio nal Linguistics),\n5956–5965.\nvan Duijn, M., van Dijk, B., Kouwenhoven, T., de Valk, W., Sprui t, M., and van der\nPutten, P. (2023). “Theory of mind in large language models: Exa mining performance\nof 11 state-of-the-art models vs. children aged 7–10 on advanc ed tests, ” in Proceedings\nof the 27th Conference on Computational Natural Language Learning (CoNL L), eds. J.\nJiang, D. Reitter, and S. Deng (Singapore: Association for Co mputational Linguistics),\n389–402.\nVarshney, N., Mishra, S., and Baral, C. (2022). “Towards impro ving selective\nprediction ability of NLP systems, ” in Proceedings of the 7th Workshop on\nRepresentation Learning for NLP , eds. S. Gella, H. He, B. P. Majumder, B. Can, E.\nGiunchiglia, S. Cahyawijaya, et al. (Dublin: Association for Co mputational Linguistics),\n221–226.\nVoronov, A., Wolf, L., and Ryabinin, M. (2024). Mind your form at: towards\nconsistent evaluation of in-context learning improvements. CoRR, abs/2401.06766 .\ndoi: 10.48550/arXiv.2401.06766\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nPasserini et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/four.tnum/six.tnum/four.tnum/six.tnum/nine.tnum/zero.tnum\nWan, D., Liu, M., McKeown, K. R., Dreyer, M., and Bansal, M. (20 23). “Faithfulness-\naware decoding strategies for abstractive summarization, ” in EACL, eds. A. Vlachos, I.\nAugenstein (Dubrovnik: Association for Computational Ling uistics), 2856–2872.\nWang, J., Ma, W., Sun, P., Zhang, M., and Nie, J.-Y. (2024). Un derstanding\nuser experience in large language model interactions. arXiv, abs/2401.08329 .\ndoi: 10.48550/arXiv.2401.08329\nWang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., and Ren, X. (2023a ). “SCOTT: self-\nconsistent chain-of-thought distillation, ” in ACL (1), eds A. Rogers, J. Boyd-Graber, N.\nOkazaki (Toronto: Association for Computational Linguisti cs), 5546–5558.\nWang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Naran g, S., et al. (2023b).\n“Self-consistency improves chain of thought reasoning in lang uage models, ” in The\nEleventh International Conference on Learning Representations, ICL R 2023, Kigali,\nRwanda, May 1–5, 2023 . Kigali.\nWang, Y., Zhong, W., Li, L., Mi, F., Zeng, X., Huang, W., et al. ( 2023c). Aligning\nLarge Language Models With Human: A Survey .\nWatson, W., and Cho, N. (2024). Hallucibot: is there no such thi ng as\na bad question? arXiv preprint arXiv:2404.12535 . doi: 10.48550/arXiv.2404.\n12535\nWatters, C., and Lemanski, M. K. (2023). Universal skepticis m of chatgpt: a review\nof early literature on chat generative pre-trained transforme r. Front. Big Data 6.\ndoi: 10.3389/fdata.2023.1224976\nWei, J. W., Huang, D., Lu, Y., Zhou, D., and Le, Q. V. (2023). Si mple\nsynthetic data reduces sycophancy in large language models. arXiv, abs/2308.03958 .\ndoi: 10.48550/arXiv.2308.03958\nWilder, B., Horvitz, E., and Kamar, E. (2021). “Learning to co mplement humans, ” in\nProceedings of the Twenty-Ninth International Joint Conference on Arti ﬁcial Intelligence,\nIJCAI’20(Yokohama).\nXie, Q., Han, W., Zhang, X., Lai, Y., Peng, M., Lopez-Lira, A., et al. (2023). “PIXIU: a\ncomprehensive benchmark, instruction dataset and large langu age model for ﬁnance, ”\nin Advances in Neural Information Processing Systems, Volume 36 , eds. A. Oh, T.\nNaumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Red H ook, NY: Curran\nAssociates, Inc.), 33469–33484.\nXin, J., Tang, R., Yu, Y., and Lin, J. (2021). “The art of abste ntion: selective\nprediction and error regularization for natural language proce ssing, ” inProceedings of\nthe 59th Annual Meeting of the Association for Computational Linguis tics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1 : Long Papers),\neds. C. Zong, F. Xia, W. Li, and R. Navigli (Association for Com putational Linguistics),\n1040–1051.\nXiong, G., Jin, Q., Lu, Z., and Zhang, A. (2024). Benchmarking retrieval-\naugmented generation for medicine. arXiv preprint arXiv:2402.13178 .\ndoi: 10.48550/arXiv.2402.13178\nXiong, M., Hu, Z., Lu, X., Li, Y., Fu, J., He, J., et al. (2024). “ Can LLMs express their\nuncertainty? an empirical evaluation of conﬁdence elicitatio n in LLMs, ” in The Twelfth\nInternational Conference on Learning Representations .\nXu, R., Qi, Z., Wang, C., Wang, H., Zhang, Y., and Xu, W. (2024a ).\nKnowledge conﬂicts for LLMs: a survey. arXiv preprint arXiv:2403.08319 .\ndoi: 10.48550/arXiv.2403.08319\nXu, Z., Jain, S., and Kankanhalli, M. (2024b). Hallucination is i nevitable: an innate\nlimitation of large language models. arXiv. doi: 10.48550/arXiv.2401.11817\nYang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., et al. (2024). Harnessing\nthe power of LLMs in practice: a survey on chatGPT and beyond. ACM Trans. Knowl.\nDiscov. Data 18:3649506. doi: 10.1145/3649506\nZakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moo r, M., et al.\n(2024). Almanac–retrieval-augmented language models for clini cal medicine. NEJM\nAI 1:AIoa2300068. doi: 10.1056/AIoa2300068\nZhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., et al. (2023 ). Siren’s song in the\nAI ocean: a survey on hallucination in large language models. CoRR, abs/2309.01219 .\ndoi: 10.48550/arXiv.2309.01219\nZhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., et al. ( 2024a). Explainability\nfor large language models: a survey. ACM Trans. Intell. Syst. Technol . 15:3639372.\ndoi: 10.1145/3639372\nZhao, R., Li, X., Joty, S., Qin, C., and Bing, L. (2023). “Veri fy-and-edit: a knowledge-\nenhanced chain-of-thought framework, ” in Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Pa pers), eds. A.\nRogers, J. Boyd-Graber, and N. Okazaki (Toronto, ON: Associa tion for Computational\nLinguistics), 5823–5840.\nZhao, Y., Devoto, A., Hong, G., Du, X., Gema, A. P., Wang, H., et a l. (2024b).\nSteering knowledge selection behaviours in LLMs via sae-base d representation\nengineering. arXiv preprint arXiv:2410.15999 . doi: 10.48550/arXiv.2410.15999\nZhao, Y., Yan, L., Sun, W., Xing, G., Wang, S., Meng, C., et al. (2 024c). Improving the\nrobustness of large language models via consistency alignment. arXiv, abs/2403.14221.\ndoi: 10.48550/arXiv.2403.14221\nZhou, H., Wan, X., Proleev, L., Mincu, D., Chen, J., Heller, K., e t al. (2024). Batch\ncalibration: rethinking calibration for in-context learning and prompt engineering.\narXiv, abs/2309.17249. doi: 10.48550/arXiv.2309.17249\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.44897595047950745
    },
    {
      "name": "Psychology",
      "score": 0.3821274936199188
    },
    {
      "name": "Management science",
      "score": 0.35596728324890137
    },
    {
      "name": "Engineering",
      "score": 0.1724025011062622
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 7
}