{
    "title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction",
    "url": "https://openalex.org/W4385718022",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2549717244",
            "name": "Jason Youn",
            "affiliations": [
                "University of California, Davis"
            ]
        },
        {
            "id": "https://openalex.org/A2023777012",
            "name": "Ilias Tagkopoulos",
            "affiliations": [
                "University of California, Davis"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3003897916",
        "https://openalex.org/W1756422141",
        "https://openalex.org/W86887328",
        "https://openalex.org/W1979003656",
        "https://openalex.org/W2759136286",
        "https://openalex.org/W2029249040",
        "https://openalex.org/W2972167903",
        "https://openalex.org/W3155001903",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2949434543",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W2128407051",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2964279602",
        "https://openalex.org/W102708294",
        "https://openalex.org/W4221154018",
        "https://openalex.org/W4225141898",
        "https://openalex.org/W4311565391",
        "https://openalex.org/W2728059831",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2283196293",
        "https://openalex.org/W2250184916",
        "https://openalex.org/W1512387364",
        "https://openalex.org/W1529533208",
        "https://openalex.org/W2982531601",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.",
    "full_text": "Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 217–224\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nKGLM: Integrating Knowledge Graph Structure\nin Language Models for Link Prediction\nJason Youn1,2,3 and Ilias Tagkopoulos1,2,3\n1 Department of Computer Science, University of California, Davis, CA 95616, USA.\n2 Genome Center, University of California, Davis, CA 95616, USA.\n3 USDA/NSF AI Institute for Next Generation Food Systems (AIFS),\nUniversity of California, Davis, CA 95616, USA.\n{jyoun, itagkopoulos}@ucdavis.edu\nAbstract\nThe ability of knowledge graphs to represent\ncomplex relationships at scale has led to their\nadoption for various needs including knowl-\nedge representation, question-answering, and\nrecommendation systems. Knowledge graphs\nare often incomplete in the information they\nrepresent, necessitating the need for knowledge\ngraph completion tasks. Pre-trained and fine-\ntuned language models have shown promise in\nthese tasks although these models ignore the\nintrinsic information encoded in the knowledge\ngraph, namely the entity and relation types. In\nthis work, we propose the Knowledge Graph\nLanguage Model (KGLM) architecture, where\nwe introduce a new entity/relation embedding\nlayer that learns to differentiate distinctive en-\ntity and relation types, therefore allowing the\nmodel to learn the structure of the knowledge\ngraph. In this work, we show that further pre-\ntraining the language models with this addi-\ntional embedding layer using the triples ex-\ntracted from the knowledge graph, followed\nby the standard fine-tuning phase sets a new\nstate-of-the-art performance for the link predic-\ntion task on the benchmark datasets.\n1 Introduction\nKnowledge graph (KG) is defined as a directed,\nmulti-relational graph where entities (nodes) are\nconnected with one or more relations (edges)\n(Wang et al., 2017). It is represented with a set\nof triples, where a triple consists of ( head entity,\nrelation, tail entity) or ( h, r, t) for short, for ex-\nample (Bill Gates, founderOf, Microsoft) as shown\nin Figure 1. Due to their effectiveness in iden-\ntifying patterns among data and gaining insights\ninto the mechanisms of action, associations, and\ntestable hypotheses (Li and Chen, 2014; Silvescu\net al., 2012), both manually curated KGs like DB-\npedia (Auer et al., 2007), WordNet (Miller, 1998),\nKIDS (Youn et al., 2022), and CARD (Alcock et al.,\n2020), and automatically curated ones like Free-\nBase (Bollacker et al., 2008), Knowledge Vault\nBill Gates\nMicrosoft\nWashington\nMelinda French\nJennifer Gates\nfounderOf\nlocatedIn\nbornIn\ndaughterOf\ndaughterOf\ndivorcedWith\nFigure 1: Sample knowledge graph with 6 triples. The\ngraph contains three unique entity types (circle for per-\nson, triangle for company, and square for location) and\n5 unique relation types or 10 if considering both the for-\nward and inverse relations. The task of the knowledge\ngraph completion is to complete the missing links in the\ngraph, e.g., (Bill Gates, bornIn?, Washington) using the\nexisting knowledge graph.\n(Dong et al., 2014), and NELL (Carlson et al.,\n2010) exist. However, these KGs often suffer from\nincompleteness. For example, 71% of the people in\nFreeBase have no known place of birth (West et al.,\n2014). To address this issue, knowledge graph\ncompletion (KGC) methods aim at connecting the\nmissing links in the KG.\nGraph feature models like path ranking algo-\nrithm (PRA) (Lao and Cohen, 2010; Lao et al.,\n2011) attempt to solve the KGC tasks by extract-\ning the features from the observed edges over the\nKG to predict the existence of a new edge (Nickel\net al., 2015). For example, the existence of the\npath Jennifer Gates daughterOf−−−−−−−→Melinda French\ndivorcedWith←−−−−−−−−Bill Gates in Figure 1 can be used as\na clue to infer the triple ( Jennifer Gates, daugh-\nterOf, Bill Gates). Other popular types of models\nare latent feature models such as TransE (Bordes\net al., 2013), TransH (Wang et al., 2014), and Ro-\ntatE (Sun et al., 2019) where entities and relations\nare converted into a latent space using embeddings.\n217\nGenerate pre-training data\nInput knowledge graph Pre-training corpus of hop 1\ne2\ne3\ne4\ne0\ne1\nr2 r4\nr3\nr1\nr1\nr0\ne0 r1 e1\nTriple 1\ne0r1e1\nTriple 2\nTriple 12\ne3r4e4\nPre-train language model\nToken\nembeddings\nPosition\nembeddings\nEntity/relation type\nembeddings\nPre-trained language model\nEtoke4E[s] Etokr4 Etoke3\nE0 E4E2E1 E3\nE E E EEr4\n-1\nE[/s]\nFigure 2: Proposed pre-training approach of the KGLM. First, both the forward and inverse triples are extracted\nfrom the knowledge graph to serve as the pre-training corpus. We then continue pre-training the language model,\nRoBERTa in our case, using the masked language model training objective, with an additional entity/relation-type\nembedding layer. The entity/relation-type embedding scheme shown here corresponds to the KGLMGER, the most\nfine-grained version where both the entity and relation types are considered unique. Note that the inverse relation\ndenoted by -1 is different from its forward counterpart. For demonstration purposes, we assume all entities and\nrelations to have a single token.\nTransE, a representative latent feature model, mod-\nels the relationship between the entities by inter-\npreting them as a translational operation. That is,\nthe model optimizes the embeddings by enforcing\nthe vector operation of head entity embedding h\nplus the relation embedding r to be close to the tail\nentity embedding t for a given fact in the KG, or\nsimply h + r ≈t.\nRecently, pre-trained language models like\nBERT (Devlin et al., 2018) and RoBERTa (Liu\net al., 2019) have shown state-of-the-art perfor-\nmance in all of the natural language processing\n(NLP) tasks. As a natural extension, models like\nKG-BERT (Yao et al., 2019) and BERTRL (Zha\net al., 2021) that utilize these pre-trained language\nmodels by treating a triple in the KG as a textual se-\nquence, e.g., (Bill Gates, founderOf, Microsoft) as\n‘Bill Gates founder of Microsoft’, have also shown\nstate-of-the-art results on the downstream KGC\ntasks. Although such textual encoding (Wang et al.,\n2021) models are generalizable to unseen entities\nor relations (Zha et al., 2021), they still fail to learn\nthe intrinsic structure of the KG as the models are\nonly trained on the textual sequence. To solve this\nissue, a hybrid approach like StAR (Wang et al.,\n2021) has recently been proposed to take advantage\nof both latent feature models and textual encoding\nmodels by enforcing a translation-based graph em-\nbedding approach to train the textual encoders. Yet,\ncurrent textual encoding models still suffer from en-\ntity ambiguation problems (Cucerzan, 2007) where\nan entity Apple, for example, can refer to either the\ncompany Apple Inc. or the fruit. Moreover, there\nare no ways to distinguish forward relation ( Jen-\nnifer Gates, daughterOf, Melinda French) from\ninverse relation ( Melinda French, daughterOf -1,\nJennifer Gates).\nIn this paper, we propose the Knowledge Graph\nLanguage Model (KGLM) (Figure 2), a simple\nyet effective language model pre-training approach\nthat learns from both the textual and structural in-\nformation of the knowledge graph. We continue\npre-training the language model that has already\nbeen pre-trained on other large natural language\ncorpora using the corpus generated by converting\nthe triples in the knowledge graphs as textual se-\nquences, while enforcing the model to better under-\nstand the underlying graph structure and by adding\nan additional entity/relation-type embedding layer.\nTesting our model on the WN18RR dataset for the\nlink prediction task shows that our model improved\nthe mean rank by 21.2% compared to the previous\nstate-of-the-art method (51 vs. 40.18, respectively).\nAll code and instructions on how to reproduce the\nresults are available online.1\n2 Background\nLink Prediction. The link prediction (LP) task,\none of the commonly researched knowledge graph\ncompletion tasks, attempts to predict the missing\nhead entity ( h) or tail entity ( t) of a triple ( h, r,\nt) given a KG G = (E,R), where {h,t}∈ E is\nthe set of all entities and r ∈R is the set of all\nrelations. Specifically, given a single test positive\ntriple (h, r, t), its corresponding link prediction test\ndataset can be constructed by corrupting either the\nhead or the tail entity in the filtered setting (Bordes\net al., 2013) as\n1https://github.com/ibpa/KGLM\n218\nD(h,r,t)\nLP =\n{(h,r,t’) |t′∈(E−{h,t}) ∧(h,r,t′) /∈D}\n∪{(h’,r,t) |h′∈(E−{h,t}) ∧(h′,r,t) /∈D}\n∪{(h,r,t)},\n(1)\nwhere D= Dtrain ∪Dval ∪Dtest is the complete\ndataset. Evaluation of the link prediction task is\nmeasured with mean rank (MR), mean reciprocal\nrank (MRR), and hits@N (Rossi et al., 2021). MR\nis defined as\nMR =\n∑\n(h,r,t)∈Dtest\nrank((h,r,t) |D(h,r,t)\nLP )\n|Dtest| ,\n(2)\nwhere rank(·|·) is the rank of the positive triple\namong its corrupted versions and|Dtest|is the num-\nber of positive test triples. MRR is the same as MR\nexcept that the reciprocal rank 1/rank(·|·) is used.\nHits@N is defined as\nhits@N =\n∑\n(h,r,t)∈Dtest\n{\n1, if rank((h,r,t) |D(h,r,t)\nLP ) <N\n0, otherwise\n|Dtest| ,\n(3)\nwhere N ∈ {1,3,10}is commonly reported.\nHigher MRR and hits@N values are better,\nwhereas, for MR, lower values denote higher per-\nformance.\n3 Proposed Approach\nIn this work, we propose to continue pre-training,\ninstead of pre-training from scratch, the language\nmodel RoBERTaLARGE (Liu et al., 2019) that has al-\nready been trained on English-language corpora of\nvarying sizes and domains, using both the forward\nand inverse knowledge graph textual sequences\n(Figure 2). Following the convention used in the\nKG-BERT and StAR (see Appendix A), we use a\ntextual representation of a given triple, e.g., ( Bill\nGates, founderOf, Microsoft) as ‘Bill Gates founder\nof Microsoft’, to generate the pre-training corpus.\nHowever, instead of extracting only the forward\ntriple as done in the previous work, we extract both\nthe forward and inverse versions of the triple, e.g.,\n(Jennifer Gates, daughterOf, Bill Gates) and (Bill\nGates, daughterOf -1, Jennifer Gates), where the -1\nTable 1: Statistics of the benchmark knowledge graphs\nused for link prediction.\nDataset # ent # rel # train # val # test\nWN18RR 40,943 11 86,835 3,034 3,134\nFB15k-237 14,951 237 272,115 17,535 20,466\nUMLS 135 46 5,216 652 661\nnotation denotes the inverse direction of the corre-\nsponding relation.\nTo enforce the model to learn the knowledge\ngraph structure, we introduce a new embedding\nlayer entity/relation-type embedding (ER-type em-\nbedding) in addition to the pre-existing token and\nposition embeddings of RoBERTa as shown in Fig-\nure 2. This additional layer aims to embed the\ntokens in the input sequence with its corresponding\nentity/relation-type, where the set of entities Ein\nthe knowledge graph can have tE different entity\ntypes depending on the schema of the knowledge\ngraph, (e.g., tE = 3for person, company, and lo-\ncation in Figure 1). Note that many knowledge\ngraphs do not specify the entity types, in which\ncase tE = 1. For the set of relations R, there exist\ntR = 2nR, where nR is the number of unique rela-\ntions in the knowledge graph and the multiplier of\n2 comes from forward and inverse directions (e.g.,\ntR = 10for the sample knowledge graph in Figure\n1).\nIn this work, we propose three different varia-\ntions of ER-type embeddings. KGLM Base is the\nsimplified version where all entities are assigned\na single entity type and relations are assigned ei-\nther forward or inverse relation type regardless of\ntheir unique relation types, resulting in a total of 3\nER-type embeddings. The KGLMGR is a version\nwith granular relation types with tR + 1ER-type\nembeddings. The KGLMGER is the most granular\nversion where we utilize all tE + tR ER-type em-\nbeddings. In other words, all entity types as well\nas all relation types including both directions are\nconsidered.\nTo be specific, we convert a triple (h, r, t) to a\nsequence of tokens w(h,r,t) = ⟨[s]wh\nawr\nbwt\nc[/s] :\na ∈ {1..|h|}& b ∈ {1..|r|}& c ∈ {1..|t|}⟩ ∈\nR(|h|+|r|+|t|+2), where [s] and [/s] are special\ntokens denoting beginning and end of the sequence,\nrespectively. The input to the RoBERTa model is\nthen constructed by adding the ER-type embedding\nt(h,r,t) and the p(h,r,t) position embeddings to the\n219\nTable 2: Link prediction results on the benchmark datasets WN18RR, FB15k-237, and UMLS. Bold numbers denote\nthe best performance for a given metric and class of models. Underlined numbers denote the best performance for\na given metric regardless of the model type. Note that we do not report KGLMGER performance since the tested\ndatasets do not specify entity types in their schema.\nWN18RR FB15k-237 UMLS\nMethod Hits @1 Hits @3 Hits @10 MR MRR Hits @1 Hits @3 Hits @10 MR MRR Hits@10 MR\nModel type: Not based on language models\nTransE .043 .441 .532 2300 .243 .198 .376 .441 323 .279 .989 1.84\nTransH .053 .463 .540 2126 .279 .306 .450 .613 219 .320 - -\nDistMult .412 .470 .504 7000 .444 .199 .301 .446 512 .281 .846 5.52\nComplEx .409 .469 .530 7882 .449 .194 .297 .450 546 .278 .967 2.59\nConvE .390 .430 .480 5277 .46 .239 .350 .491 246 .316 .990 1.51\nRotatE .428 .492 .571 3340 .476 .241 .375 .533 177 .338 - -\nGAAT .424 .525 .604 1270 .467 .512 .572 .650 187 .547 - -\nLineaRE .453 .509 .578 1644 .495 .264 .391 .545 155 .357 - -\nQuatDE .438 .509 .586 1977 .489 .268 .400 .563 90 .365 - -\nModel type: Based on language models\nKG-BERT .041 .302 .524 97 .216 - - .420 153 - .990 1.47\nStAR .243 .491 .709 51 .401 .205 .322 .482 117 .296 .991 1.49\nKGLMBase .305 .518 .730 47.97 .445 - - - - - - -\nKGLMGR .330 .538 .741 40.18 .467 .200 .314 .468 125.9 .289 .995 1.19\nw(h,r,t) token embeddings, as\nX(h,r,t) = w(h,r,t) + p(h,r,t) + t(h,r,t). (4)\nUnlike the segment embeddings in the KG-BERT\nand StAR that were used to mark the input tokens\nwith either the entity (se) or relation (sr), the ER-\ntype embedding now replaces its functionality. Fi-\nnally, we pre-train the model using the masked lan-\nguage model (MLM) training objective (Liu et al.,\n2019).\nFor fine-tuning, we extend the idea of how the\nKG-BERT scores a triple (see Equation 6 in Ap-\npendix A) to take advantage of the ER-type embed-\ndings learned in our pre-training stage. For a given\ntarget triple, we calculate the weighted average\nscore of both directions as\nscoreKGLM (h,r,t) =αSeqCls(X(h,r,t))+\n(1 −α)SeqCls(X(t,r−1,h)),\n(5)\nwhere SeqCls(·) is a RoBERTa model transformer\nwith a sequence classification head on top of the\npooled output (last layer hidden-state of the [CLS]\ntoken followed by dense layer and tanh activation\nfunction), (t,r−1,h) denotes the inverse version of\n(h,r,t), and 0 ≤α ≤1 denotes the weight used\nfor balancing the scores from forward and inverse\nscores. For example, α = 1.0 considers only the\nforward direction score.\n4 Experiments and Results\n4.1 Datasets\nWe tested our proposed method on three bench-\nmark datasets WN18RR, FB15k-237, and UMLS\nas shown in Table 1. WN18RR (Dettmers et al.,\n2018) is derived from WordNet (Miller, 1998), a\nlarge English lexical database of semantic relation-\nships between words, FB15k-237 (Toutanova and\nChen, 2015) is extracted from Freebase (Bollacker\net al., 2008), a large community-drive KG of gen-\neral facts about the world, and UMLS contains\nbiomedical relationships. WN18RR and FB15k-\n237 are subsets of WN18 (Bordes et al., 2013) and\nFB15k (Bordes et al., 2013), respectively, where\nthe inverse relation test leakage problem, i.e. the\nproblem of inverted test triples appearing in the\ntraining set, has been corrected.\n4.2 Settings\nWe used RoBERTa LARGE (Liu et al., 2019), a\nBERTLARGE-based architecture with 24 layers,\n1024 hidden size, 16 self-attention heads, and\n355M parameters, for the pre-trained language\nmodel as it has been shown in a previous study to\nperform better than BERT (hits@1 0.243 vs. 0.222\nand MR 51 vs. 99, link prediction on WN18RR)\n(Wang et al., 2021). For pre-training, we used\nlearning rate = 5e-05, batch size = 32, epoch = 20\n(WN18RR), 10 (FB15k-237), and 1,000 (UMLS),\n220\nTable 3: Breakdown of the original hypothesis and their results on WN18RR. For claim 1, we continued to pre-train\nRoBERTaLARGE using the knowledge graph without the ER-type embeddings. Note that we did not also use\nthe ER-type embeddings layer in the fine-tuning stage. For claim 2, we learned the ER-type embeddings in the\nfine-tuning stage only without any further pre-training.\nER-type embeddings\nModel Continue pre-training Pre-train Fine-tune Hits @1 Hits @3 Hits @10 MR MRR\nClaim 1 o x x 0.331 0.529 0.728 53.5 0.462\nClaim 2 x - o 0.322 0.489 0.672 66.4 0.439\nKGLMGR o o o 0.330 0.538 0.741 40.18 0.467\nand AdamW optimizer (Loshchilov and Hutter,\n2017). For fine-tuning training data, we sampled\n10 negative triples for a positive triple by corrupt-\ning both the head and tail entity 5 times each. We\nused the validation set to find the optimal learning\nrates = {1e−06,5e−07}, batch size = {16,32},\nepochs = {1,2,3,4,5}for WN18RR and FB15k-\n237 and 25, 50, 75, 100 for UMLS, and αfrom\n0.0 to 1.0 with an increment of 0.1. For all exper-\niments, we set α = 0.5 based on the WN18RR\nvalidation set performance. Both pre-training and\nfine-tuning were performed on 3 ×Nvidia Quadro\nRTX 6000 GPUs in a distributed manner using\nthe 16-bit mixed precision and DeepSpeed (Rasley\net al., 2020; Rajbhandari et al., 2020) library in the\nstage-2 setting. We used the Transformers library\n(Wolf et al., 2019).\n4.3 Link Prediction Results\nThe hypothesis behind the KGLM was that learning\nthe ER-type embedding layers in the pre-training\nstage using the corpus generated by the knowl-\nedge graph, followed by fine-tuning has the best\nperformance. To test our hypothesis, we broke\ndown the hypothesis into two separate claims. For\nthe first claim, we only continued pre-training\nRoBERTaLARGE followed by fine-tuning without\nthe ER-type embeddings. This test removes the\ncontribution from the ER-type embeddings and\nsolely tests the performance gained by further pre-\ntraining the model with the knowledge graph as\ninput. Table 3 shows that claim 1 falls behind the\nKGLMGR in all metrics except for hits @1 (0.331\nvs. 0.330, respectively). For the second claim, we\ndid not continue pre-training and instead used the\nRoBERTaLARGE pre-trained weights as-is. We then\nlearned the ER-type embeddings in the fine-tuning\nstage. This test shows if the ER-type embeddings\ncan be learned only during the fine-tuning stage.\nTable 3 shows that KGLMGR outperforms all of the\nmetrics obtained using the second claim. This re-\nsult shows that the combination of these two claims\nworks in a non-linear fashion to maximize perfor-\nmance.\nThe results of performing link prediction on the\nbenchmark datasets are shown in Table 2. Com-\npared to StAR, which had the best performance on\nMR and hits@10 on WN18RR, KGLMGR outper-\nformed all the metrics with 21.2% improved MR\n(40.18 vs. 51, respectively) and 4.5% increased\nhits@10 (0.709 vs. 0.741, respectively). Al-\nthough still inferior compared to the graph embed-\nding approaches, KGLMGR has 35.8% improved\nhits@1 compared to the best language model-based\napproach StAR (0.243 vs. 0.330, respectively).\nAcross all model types, KGLMGR has the best per-\nformance on all metrics for WN18RR except for\nhits@1. Although we did not observe any improve-\nment compared to StAR for the FB15k-237 dataset,\nwe had the best performance on all metrics for\nUMLS with 21.2% improved MR than ComplEx\n(1.19 vs. 1.51, respectively). KGLM GR outper-\nformed KGLMBase in all metrics.\n5 Conclusion\nIn this work, we presented KGLM, which intro-\nduces a new entity/relation (ER)-type embedding\nlayer for learning the structure of the knowledge\ngraph. Compared to the previous language model-\nbased methods that only fine-tune for a given task,\nwe found that learning the ER-type embeddings\nin the pre-training stage followed by fine-tuning\nresulted in better performance. In future work, we\nplan to further test the version of KGLM that takes\ninto account entity types, KGLMGER, on domain-\nspecific knowledge graphs like KIDS (Youn et al.,\n2022) with entity types in their schema.\nLimitations\nAlthough KGLM outperforms state-of-the-art mod-\nels when the training set includes full sentences\n221\n(e.g., UMLS and WN18RR), the model performed\nsimilarly to the state-of-the-art in cases where the\ntraining dataset had only ontological relationships,\nsuch as the /music/artist/origin relation present in\nthe FB15k-237 dataset. One major limitation of\nthe proposed method is the long training and infer-\nence time, which we plan to alleviate by adopting\nSiamese-style textual encoders (Wang et al., 2021;\nLi et al., 2022) in future work.\nEthics Statement\nThe authors declare no competing interests.\nAcknowledgements\nWe would like to thank the members of the\nTagkopoulos lab for their suggestions. This work\nwas supported by the USDA-NIFA AI Institute for\nNext Generation Food Systems (AIFS), USDA-\nNIFA award number 2020-67021-32855 and the\nNIEHS grant P42ES004699 to I.T. J.Y . conceived\nthe project and performed all experiments. Both\nJ.Y . and I.T. wrote the manuscript. I.T. supervised\nall aspects of the project.\nReferences\nBrian P Alcock, Amogelang R Raphenya, Tammy TY\nLau, Kara K Tsang, Mégane Bouchard, Arman\nEdalatmand, William Huynh, Anna-Lisa V Nguyen,\nAnnie A Cheng, Sihan Liu, et al. 2020. Card 2020:\nantibiotic resistome surveillance with the compre-\nhensive antibiotic resistance database. Nucleic acids\nresearch, 48(D1):D517–D525.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIG-\nMOD international conference on Management of\ndata, pages 1247–1250.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in neural information pro-\ncessing systems, 26.\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr\nSettles, Estevam R Hruschka, and Tom M Mitchell.\n2010. Toward an architecture for never-ending lan-\nguage learning. In Twenty-Fourth AAAI conference\non artificial intelligence.\nSilviu Cucerzan. 2007. Large-scale named entity disam-\nbiguation based on wikipedia data. In Proceedings\nof the 2007 joint conference on empirical methods\nin natural language processing and computational\nnatural language learning (EMNLP-CoNLL), pages\n708–716.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In Thirty-second AAAI con-\nference on artificial intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko\nHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,\nShaohua Sun, and Wei Zhang. 2014. Knowledge\nvault: A web-scale approach to probabilistic knowl-\nedge fusion. In Proceedings of the 20th ACM\nSIGKDD international conference on Knowledge dis-\ncovery and data mining, pages 601–610.\nNi Lao and William W Cohen. 2010. Relational re-\ntrieval using a combination of path-constrained ran-\ndom walks. Machine learning, 81(1):53–67.\nNi Lao, Tom Mitchell, and William Cohen. 2011. Ran-\ndom walk inference and learning in a large scale\nknowledge base. In Proceedings of the 2011 con-\nference on empirical methods in natural language\nprocessing, pages 529–539.\nDa Li, Ming Yi, and Yukai He. 2022. Lp-bert: Multi-\ntask pre-training knowledge graph bert for link pre-\ndiction. arXiv preprint arXiv:2201.04843.\nYixue Li and Luonan Chen. 2014. Big biological data:\nchallenges and opportunities. Genomics, proteomics\n& bioinformatics, 12(5):187.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nMaximilian Nickel, Kevin Murphy, V olker Tresp, and\nEvgeniy Gabrilovich. 2015. A review of relational\nmachine learning for knowledge graphs. Proceedings\nof the IEEE, 104(1):11–33.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis, pages 1–\n16. IEEE.\n222\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505–3506.\nAndrea Rossi, Denilson Barbosa, Donatella Firmani,\nAntonio Matinata, and Paolo Merialdo. 2021. Knowl-\nedge graph embedding for link prediction: A com-\nparative analysis. ACM Transactions on Knowledge\nDiscovery from Data (TKDD), 15(2):1–49.\nAdrian Silvescu, Doina Caragea, and Anna Atramentov.\n2012. Graph databases. Artificial Intelligence Re-\nsearch Laboratory Department of Computer Science,\nIowa State University.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. arXiv preprint\narXiv:1902.10197.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd workshop on\ncontinuous vector space models and their composi-\ntionality, pages 57–66.\nThanh Vu, Tu Dinh Nguyen, Dat Quoc Nguyen, Dinh\nPhung, et al. 2019. A capsule network-based em-\nbedding model for knowledge graph completion and\nsearch personalization. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2180–2189.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying\nWang, and Yi Chang. 2021. Structure-augmented\ntext representation learning for efficient knowledge\ngraph completion. In Proceedings of the Web Confer-\nence 2021, pages 1737–1748.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\n2017. Knowledge graph embedding: A survey of\napproaches and applications. IEEE Transactions\non Knowledge and Data Engineering, 29(12):2724–\n2743.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 28.\nRobert West, Evgeniy Gabrilovich, Kevin Murphy,\nShaohua Sun, Rahul Gupta, and Dekang Lin. 2014.\nKnowledge base completion via search-based ques-\ntion answering. In Proceedings of the 23rd interna-\ntional conference on World wide web, pages 515–526.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. arXiv\npreprint arXiv:1909.03193.\nJason Youn, Navneet Rai, and Ilias Tagkopoulos. 2022.\nKnowledge integration and decision support for ac-\ncelerated discovery of antibiotic resistance genes.Na-\nture Communications, 13(1):1–11.\nHanwen Zha, Zhiyu Chen, and Xifeng Yan. 2021. In-\nductive relation prediction by bert. arXiv preprint\narXiv:2103.07102.\nA Previous Work\nA.1 KG-BERT\nKG-BERT (Yao et al., 2019) is a fine-\ntuning method that utilizes the base version\nof the pre-trained language model BERT\n(BERTBASE) (Devlin et al., 2018) as an encoder\nfor entities and relations of the knowledge\ngraph. Specifically, KG-BERT first converts\na triple ( h, r, t) to a sequence of tokens\nw(h,r,t) = ⟨[CLS]wh\na[SEP]wr\nb[SEP]wt\nc[SEP] :\na ∈ {1..|h|}& b ∈ {1..|r|}& c ∈ {1..|t|}⟩,\nwhere wn denotes the n th token of either entity\nor relation, [CLS] and [SEP] are the special\ntokens, while |h|, |r|, and |t|denote the number of\ntokens in the head entity, relation, and tail entity,\nrespectively. This textual token sequence is then\nconverted to a sequence of token embeddings\nw(h,r,t) ∈ Rd×(|h|+|r|+|t|+4), where d is the\ndimension of the embeddings and 4 is from the\nspecial tokens. Then the segment embeddings\ns(h,r,t) = ⟨(se)×(|h|+2)(sr)×(|r|+1)(se)×(|t|+1)⟩,\nwhere se and sr are used to differentiate en-\ntities from relations, respectively, as well as\nthe position embeddings p(h,r,t) = ⟨pi : i ∈\n{1..(|h|+|r|+|t|+4)}⟩ are added to the token\nembeddings w(h,r,t) to form a final input repre-\nsentation X(h,r,t) ∈Rd×(|h|+|r|+|t|+4) that is fed\nto BERT as input. Then, the score of how likely a\ngiven triple (h, r, t) is to be true is computed by\nscoreKG-BERT(h,r,t) =SeqCls(X(h,r,t)). (6)\nKG-BERT significantly improved the MR of the\nlink prediction task compared to the previous state-\nof-the-art approach CapsE (Vu et al., 2019) (97\ncompared to 719, an 86.5% decrease), but suffered\nfrom poor hits@1 of 0.041 due to the entity am-\nbiguation problem and lack of structural learning\n(Wang et al., 2021; Cucerzan, 2007).\n223\nA.2 StAR\nStAR (Wang et al., 2021) is a hybrid model that\nlearns both the contextual and structural informa-\ntion of the knowledge graph by augmenting the\nstructured knowledge in the encoder. It divides\na triple into two parts, ( h, r) and (t), and applies\na Siamese-style transformer with a sequence clas-\nsification head to generate u = Pool(X(h,r)) ∈\nRd×(|h|+|r|+3) and v = Pool(X(t)) ∈Rd×(|t|+2),\nrespectively, where Pool( ·) is the output of the\nRoBERTa’s pooling layer. The first scoring module\nfocuses on classifying the triple by applying a\nscorec\nStAR(h,r,t) =Cls([u; u ×v; u −u; v]),\n(7)\nwhere Cls(·) is a neural binary classifier with a\ndense layer followed by a softmax activation func-\ntion. The second scoring module then adopts the\nidea of how translation-based graph embedding\nmethods like TransE learns the graph structure by\nminimizing the distance between u and v as\nscored\nStAR(h,r,t) =−||u −v||, (8)\nwhere ||·|| is the L2-normalization. During\nthe training, StAR uses a weighted average of\nthe binary cross entropy loss computed using\nscorec\nStAR(h,r,t) and the margin-based hinge loss\ncomputed using scored\nStAR(h,r,t), whereas only\nthe scorec\nStAR(h,r,t) is used for inference. This\napproach shows a new state-of-the performance\nover the metrics MR (51) and hits@10 (0.709), as\nwell as significantly improving the hits@1 com-\npared to the KG-BERT (0.041 to 0.243, a 492.7%\nincrease).\n224"
}