{
  "title": "Lite Transformer with Long-Short Range Attention",
  "url": "https://openalex.org/W3019527251",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2981547106",
      "name": "Zhanghao Wu",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102403465",
      "name": "Zhijian Liu",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095624931",
      "name": "Ji Lin",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2167546943",
      "name": "Yujun Lin",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101446197",
      "name": "Song Han",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962851801",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2886851211",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2963424132",
    "https://openalex.org/W2996167479",
    "https://openalex.org/W2962861284",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2963821229",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3035204081",
    "https://openalex.org/W2985462664",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2319920447",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2625477277",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2809624076",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2963263347"
  ],
  "abstract": "Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at this https URL.",
  "full_text": "Published as a conference paper at ICLR 2020\nLITE TRANSFORMER WITH\nLONG -SHORT RANGE ATTENTION\nZhanghao Wu∗1,2 Zhijian Liu∗1 Ji Lin1 Yujun Lin1 Song Han1\n1Massachusetts Institute of Technology 2Shanghai Jiao Tong University\n{zhwu, zhijian, songhan}@mit.edu\nABSTRACT\nTransformer has become ubiquitous in natural language processing (e.g., machine\ntranslation, question answering); however, it requires enormous amount of com-\nputations to achieve high performance, which makes it not suitable for mobile\napplications that are tightly constrained by the hardware resources and battery. In\nthis paper, we present an efﬁcient mobile NLP architecture, Lite Transformerto\nfacilitate deploying mobile NLP applications on edge devices. The key primitive is\nthe Long-Short Range Attention(LSRA), where one group of heads specializes in\nthe local context modeling (by convolution) while another group specializes in the\nlong-distance relationship modeling (by attention). Such specialization brings con-\nsistent improvement over the vanilla transformer on three well-established language\ntasks: machine translation, abstractive summarization, and language modeling.\nUnder constrained resources (500M/100M MACs), Lite Transformer outperforms\ntransformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite\nTransformer reduces the computation of transformer base model by 2.5×with 0.3\nBLEU score degradation. Combining with pruning and quantization, we further\ncompressed the model size of Lite Transformer by 18.2×. For language modeling,\nLite Transformer achieves 1.8 lower perplexity than the transformer at around\n500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved\nTransformer by 0.5 higher BLEU for the mobile NLP setting without the costly\narchitecture search that requires more than 250 GPU years. Code has been made\navailable at https://github.com/mit-han-lab/lite-transformer.\n1 I NTRODUCTION\nTransformer (Vaswani et al., 2017) is widely used in natural language processing due to its high\ntraining efﬁciency and superior capability in capturing long-distance dependencies. Building on\ntop of them, modern state-of-the-art models, such as BERT (Devlin et al., 2019), are able to learn\npowerful language representations from unlabeled text and even surpass the human performance on\nthe challenging question answering task.\nHowever, the good performance comes at a high computational cost. For example, a single transformer\nmodel requires more than 10G Mult-Adds in order to translate a sentence of only 30 words. Such\nextremely high computational resources requirement is beyond the capabilities of many edge devices\nsuch as smartphones and IoTs. Therefore, it is of great importance to design efﬁcient and fast\ntransformer architecture specialized for real-time NLP applications on the edge. Automatic neural\narchitecture search (Zoph & Le, 2017; So et al., 2019) is a choice for high accuracy model design,\nbut the massive search cost (GPU hours and CO2 emission) raises severe environmental concerns\n(Strubell et al., 2019), shown in Figure 1b.\nIn this paper, we focus on the efﬁcient inference for mobile devices, where the total number of Mult-\nAdds is constrained below 500M. A straightforward way to reduce the computation of the transformer\nis to shrink the embedding size directly. Although it can effectively reduce both model size and\ncomputation, it also weakens the model capacity capturing the long and short distance relationship at\nthe same time. To this end, we systematically studied the computation breakdown of the transformer\n∗ indicates equal contributions.\n1\narXiv:2004.11886v1  [cs.CL]  24 Apr 2020\nPublished as a conference paper at ICLR 2020\nand observed that the computation (Mult-Adds) is dominated by the feed-forward network (FFN). We\ndiscovered that the prevailing bottleneck-structured transformer block is not efﬁcient. We then present\na novel Long-Short Range Attention (LSRA) primitive. LSRA trades off the computation in FFN for\nwider attention layers. It stretches the bottleneck to introduce more dependency capturing capability\nfor the attention layer, and then shrink the embedding size to reduce the total computation amount\nwhile maintaining the same performance. Instead of having one module for “general” information,\nLSRA dedicates specialized heads to model long and short distance contexts. Inspired by Wu et al.\n(2019b), LSRA introduces convolution in a parallel branch to capture local dependencies so that\nthe attention branch can focus on global context capture. By stacking this primitive, we build Lite\nTransformer for mobile NLP applications.\nExtensive experiments demonstrate that our Lite Transformer model offers signiﬁcant improvements\nover the transformer on three language tasks: machine translation, abstractive summarization, and\nlanguage modeling. For machine translation, on IWSLT 2014 German-English, it outperforms the\ntransformer by 3.1 BLEU under 100M Mult-Adds; on WMT 2014 English-German, it surpasses the\ntransformer by 0.4 BLEU under 500M Mult-Adds and 1.2 BLEU under 100M Mult-Adds; on WMT\n2014 English-French, it also achieves consistent improvements over the transformer: 1.2 BLEU under\n500M Mult-Adds and 1.7 BLEU under 100M Mult-Adds. Further, combined with general model\ncompression techniques (pruning and quantization), our Lite Transformer can achieve 18.2×model\nsize compression. For the summarization task, on CNN-DailyMail, it reduces the computation of the\ntransformer base model by 2.4×. For language modeling, it achieves 1.8 lower perplexity than the\ntransformer around 500M Mult-Adds.\nGuided by our design insights, our manually-designed Lite Transformer achieves 0.5 higher BLEU\nthan the AutoML-based Evolved Transformer (So et al., 2019), which requires more than 250 GPU\nyears to search, emitting as much carbon as ﬁve cars in their lifetimes (see Figure 1b). It indicates\nthat AutoML is not a panacea: careful analysis and design insights ( i.e., removing the bottleneck,\nspecialized heads) can effectively prune the search space and improve the sample efﬁciency.\nThe contribution of this paper has four aspects:\n1. We systematically analyze the commonly used computation bottleneck structure in modern neural\nnetworks and ﬁnd that the bottleneck design is not optimal for 1-D attention if using FLOPs as\nﬁgure of merit.\n2. We propose a specialized multi-branch feature extractor, Long-Short Range Attention (LSRA), as\nthe basic building block of our transformer, where convolution helps capture the local context and\nattention concentrates on global context.\n3. We build Lite Transformer based on our LSRA. Under mobile computation resource constraints\n(500M Mult-Adds), our Lite Transformer demonstrates coherent improvement on three widely\nused machine translation datasets. With extra experiments on other tasks, Lite Transformer is\nefﬁcient for multiple language applications.\n4. Even compared to AutoML-searched Evolved Transformer, our Lite Transformer offers 0.5 higher\nBLEU score on WMT En-De dataset under the mobile setting, saving the design cost by 20000×\nin CO2 emissions. It alerts us to rethink the practicality of AutoML in terms of design cost and\n“green AI”.\n2 R ELATED WORK\nRNNs and CNNs. Recurrent neural networks (RNNs) have prevailed various sequence modeling\ntasks for a long time (Sutskever et al., 2014; Luong et al., 2015; Bahdanau et al., 2015; Wu et al.,\n2016). However, RNNs are not easy to parallelize across the sequence due to its temporal dependency.\nRecently, some work has demonstrated that RNN is not an essential component to achieve state-\nof-the-art performance. For instance, researchers have proposed highly-efﬁcient convolution-based\nmodels (Kalchbrenner et al., 2016; Gehring et al., 2017; Kaiser et al., 2018; Wu et al., 2019b).\nConvolution is an ideal primitive to model the local context information; however, it lacks the ability\nto capture the long-distance relationship, which is critical in many sequence modeling tasks.\nTransformers. As an alternative, attention is able to capture global-context information by pairwise\ncorrelation. Transformer (Vaswani et al., 2017) has demonstrated that it is possible to stack the\n2\nPublished as a conference paper at ICLR 2020\n/uni00000014/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni0000000f/uni00000013/uni00000013/uni00000013\n/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000030/uni0000000c\n/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni00000025/uni00000044/uni00000056/uni00000048\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni00000025/uni0000004c/uni0000004a\n/uni00000025/uni00000028/uni00000035/uni00000037/uni00000003/uni0000002f/uni00000044/uni00000055/uni0000004a/uni00000048\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015\n/uni00000030/uni00000052/uni00000045/uni0000004c/uni0000004f/uni00000048/uni00000003/uni00000026/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000057/uni00000056/uni00000014/uni00000014\n/uni00000017/uni00000017\n/uni00000014/uni0000001a/uni00000019\n/uni00000016/uni00000017/uni00000013\n/uni00000014/uni0000000f/uni00000018/uni0000001b/uni00000013\n(a) Parameter numbers of modern NLP models.\n/uni00000013/uni00000015/uni00000013/uni00000013/uni0000000f/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni0000000f/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000000f/uni00000013/uni00000013/uni00000013\n/uni00000026/uni000000322/uni00000003/uni00000028/uni00000050/uni0000004c/uni00000056/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b/uni0000004f/uni00000045/uni00000056/uni0000000c\n/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000003/uni00000037/uni00000055/uni00000044/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni00000028/uni00000059/uni00000052/uni0000004f/uni00000059/uni00000048/uni00000047\n/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni00000038/uni00000036/uni00000003/uni00000046/uni00000044/uni00000055/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000049/uni00000058/uni00000048/uni0000004f\n/uni0000000b/uni00000044/uni00000059/uni0000004a/uni00000011/uni00000003/uni00000014/uni00000003/uni0000004f/uni0000004c/uni00000049/uni00000048/uni00000057/uni0000004c/uni00000050/uni00000048/uni0000000c\n/uni00000024/uni00000050/uni00000048/uni00000055/uni0000004c/uni00000046/uni00000044/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000049/uni00000048\n/uni0000000b/uni00000044/uni00000059/uni0000004a/uni00000011/uni00000003/uni00000014/uni00000003/uni0000005c/uni00000048/uni00000044/uni00000055/uni0000000c\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000003/uni0000004f/uni0000004c/uni00000049/uni00000048\n/uni0000000b/uni00000044/uni00000059/uni0000004a/uni00000011/uni00000003/uni00000014/uni00000003/uni0000005c/uni00000048/uni00000044/uni00000055/uni0000000c\n/uni00000016/uni00000015\n/uni00000019/uni00000015/uni00000019/uni0000000f/uni00000014/uni00000018/uni00000018\n/uni00000014/uni00000015/uni00000019/uni0000000f/uni00000013/uni00000013/uni00000013\n/uni00000016/uni00000019/uni0000000f/uni00000014/uni00000018/uni00000019\n/uni00000014/uni00000014/uni0000000f/uni00000013/uni00000015/uni00000016\n/uni00000014/uni0000001c/uni0000000f/uni00000018/uni00000019/uni0000001a/uni000000ee (b) The design cost measured in CO2 emission (lbs).\nFigure 1: Left: the size of recent NLP models grows rapidly and exceeds the mobile constraints to a\nlarge extent. Right: the search cost of AutoML-based NLP model is prohibitive, which emits carbon\ndioxide nearly 5×the average lifetime emissions of the car.\nself-attentions to achieve state-of-the-art performance. Recently, there have been a lot of variants to\nthe transformer (Ahmed et al., 2017; Ott et al., 2018; Chen et al., 2018; Paulus et al., 2018; Shaw\net al., 2018; Sukhbaatar et al., 2019a;b; Child et al., 2019). Among them, Ott et al. (2018) proposed\nto scale up the batch size; Shaw et al. (2018) leverages the relative position representations; Ahmed\net al. (2017) introduces the weighted multi-head attention; Sukhbaatar et al. (2019a) applies adaptive\nmasks for long-range information on character-level language modeling with very long sequences.\nAll these attempts are orthogonal to our work, as their methods can also be applied in our architecture.\nAutomated Model Design. Due to the vast architecture design space, automating the design with\nneural architecture search (NAS) becomes popular (Zoph & Le, 2017; Zoph et al., 2018; Pham et al.,\n2018; Cai et al., 2019a). To make the design efﬁcient, integrating the hardware resource constraints\ninto the optimization loop begins to emerge, such as MnasNet (Tan et al., 2019), ProxylessNAS (Cai\net al., 2019b) and FBNet (Wu et al., 2019a). In the NLP community, the evolved transformer (So\net al., 2019) adopts the neural architecture search (Zoph & Le, 2017) to design basic blocks and ﬁnds\na better #parameter-BLEU trade-off for the transformer. However, AutoML-based model designs\nrequire signiﬁcant amount of GPU hours to ﬁnd the ‘best’ model, which is not affordable for most\nresearchers.\nModel Acceleration. Apart from designing efﬁcient models directly (Liu et al., 2019b; Li et al.,\n2020), another approach to achieve efﬁcient inference is to compress and accelerate the existing large\nmodels. For instance, some have proposed to prune the separate neurons (Han et al., 2015b; 2016)\nor the entire channels (He et al., 2017; Liu et al., 2017; He et al., 2018); others have proposed to\nquantize the network (Courbariaux et al., 2016; Zhu et al., 2017; Krishnamoorthi, 2018; Wang et al.,\n2019) to accelerate the model inference. Recently, AutoML has also been used to automate the model\ncompression and acceleration (He et al., 2018; Yang et al., 2018; Wang et al., 2019; Liu et al., 2019a).\nAll these techniques are compressing existing models and are therefore orthogonal to our approach.\nWe aim to explore how to make use of the domain knowledge to design an efﬁcient architecture from\nthe beginning, rather than compressing an existing model.\n3 I S BOTTLENECK EFFECTIVE FOR 1-D ATTENTION ?\nThe attention mechanism has been widely used in various applications, including 1-D (language\nprocessing (Vaswani et al., 2017)), 2-D (image recognition), and 3-D (video recognition (Wang et al.,\n2018)). It computes pairwise dot-product between all the input elements to model both short-term\nand long-term relationships. Despite its effectiveness, the operation introduces massive computation.\nAssume the number of elements ( e.g., length of tokens in language processing, number of pixels\nin image, etc.) fed to attention layer is N, and the dimension of features ( i.e., channels) is d, the\ncomputation needed for the dot-product is N2d. For images and videos, N is usually very large. For\nexample, the intermediate feature map in a video network (Wang et al., 2018) has 16 frames, each with\n112×112 resolution, leading to N = 2×105. The computation of convolution and fully-connected\nlayers grows linearly w.r.t.N, while the computation of attention layers grows quadratically w.r.t. N.\nThe computation of attention module will soon overwhelm with a large N.\n3\nPublished as a conference paper at ICLR 2020\nLite Transformer with Long-Short Range Attention, ICLR’20 1\nLite Transformer with\nLong-Short Range Attention\nFlattened\nBase\nAttention FFNFFN\nConv\nGLU\n FC\nLSRA \n(Ours)\n754\n373\n177\n582 \n1,143 \n276 \n76\n1,336 \n1,516 \n529 \n0\n500\n1,000\n1,500\nBase\n(39.9)\nFlattened\n(41.0)\nLSRA (Ours)\n(39.6)\nMult-Adds (M)\n2.5×\nFigure 2: Flattening the bottleneck of transformer blocks increases the proportion of the attention\nversus the FFN, which is good for further optimization for attention in our LSRA.\nTo address the dilemma, a common practice is ﬁrst to reduce the number of channels d using a\nlinear projection layer before applying attention and increase the dimension afterwards (as shown\nin Figure 2). In the original design of transformer (Vaswani et al., 2017), the channel dimension\nin the attention module is 4×smaller than that in the FFN layer. Similarly, in the non-local video\nnetwork (Wang et al., 2018), the channel number is ﬁrst reduced by half before applying the non-local\nattention module. This practice saves the computation by 16×or 4×. Nevertheless, it also decreases\nthe contexts capture ability of attention layers with a smaller feature dimension. The situation could\nbe even worse for language processing, as attention is the major module for contexts capture (unlike\nimages and videos where convolutions conduct the major information capture).\nFor tasks like translation, the length of the input sequence N tends to be small, which is around 20-30\nin common cases. A transformer block consists of an attention (or two for decoder), followed by a\nfeed-forward network (FFN). For the attention layer, the Mult-Adds would be O(4Nd2 + N2d); for\nFFN, the Mult-Adds is O(2 ×4Nd2). Given a small N, it is doubtful if the bottleneck design is a\ngood trade-off between computation and accuracy on 1D attention. To verify the idea, we ﬁrst proﬁle\nthe computation breakdown in the transformer in Figure 2. Surprisingly, for the original transformer\n(denoted as ‘Base’ in the ﬁgure), the FFN layer actually consumes much of the computation. This\nis not desirable since FFN itself cannot perform any contexts captures. In conclusion, due to the\nsmall N, the bottleneck design cannot signiﬁcantly reduce the computation in 1D attention, while\nthe limited beneﬁt for computation reduction is further compromised by the large FFN layer. It\nalso harms the capacity of attention layer due to the smaller dimension, which is the major contexts\ncapture unit in the transformer.\nTherefore, we argue that the bottleneck design is not optimal for 1-D attention. We instead design a\n‘ﬂattened’ version of the transform block that does not reduce and increase the channel dimension.\nWith the new design, the attention part now takes up the major computation in the ﬂattened transformer\nmodel in Figure 2, leaving a larger space for further optimization. We also test the performance\nchange of such modiﬁcation on WMT’14 En-Fr dataset. We can achieve comparable performance at\na slightly larger computation, which can be easily reduced with further optimization that is discussed\nin the next section.\n4 L ONG -SHORT RANGE ATTENTION (LSRA)\nResearchers have tried to understand the contexts captured by attention. Kovaleva et al. (2019)\nand Clark et al. (2020) visualized the attention weights from different layers in BERT. As shown\nin Figure 3b, the weights w illustrate the relationships between the words from the source sentence\nand the target sentence (the same for self-attention). With a larger weight wij (darker color), the i-th\nword in the source sentence pays more attention to the j-th word in the target sentence. And the\nattention maps typically have strong patterns: sparse and diagonal. They represent the relationships\nbetween some particular words: the sparse for the long-term information, and the diagonal for the\ncorrelation in small neighborhoods. We denote the former as “global” relationships and the latter as\n“local”.\n4\nPublished as a conference paper at ICLR 2020\nLocal \nExtractor \nFFN \nFC \nGLU \nEmbedding \nEmbedding \nConv \nAttention \nGlobal \nExtrator \nFFN \n(a) Lite Transformer block\nit\nrequires\nenormous\namount\nof\nresources\nto\nachieve\nhigh\nscores\n.\nit\nrequires\nenormous\namount\nof\nresources\nto\nachieve\nhigh\nscores\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(b) Conventional Attention. It cap-\ntures local information on the diag-\nonal and global context as sparse\npoints. (Redundant)\nit\nrequires\nenormous\namount\nof\nresources\nto\nachieve\nhigh\nscores\n.\nit\nrequires\nenormous\namount\nof\nresources\nto\nachieve\nhigh\nscores\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(c) Attention in LSRA. It is special-\nized for long-term relationships, in-\ndicated as points away from the di-\nagonal. (Efﬁcient)\nFigure 3: Lite Transformer architecture (a) and the visualization of attention weights. Conventional\nattention (b) puts too much emphasis on local relationship modeling (see the diagonal structure). We\nspecialize the local feature extraction by a convolutional branch which efﬁciently models the locality\nso that the attention branch can specialize in global feature extraction (c). More visualizations are\navailable in Figure A1.\nFor a translation task, the attention modules have to capture both global and local contexts, requiring\na large capacity. That is not optimal compared with a specialized design. Taking the hardware design\nas an example, general-purpose hardware like CPUs is less efﬁcient than specialized hardware like\nFPGAs. Here, we should specialize global and local contexts capture. When the model capacity is\nrelatively large, the redundancy can be tolerated and may even provide better performance. However,\nwhen it comes to mobile applications, a model should be more efﬁcient due to the computation and\npower constraints. Thus specialized contexts capture is more demanding. To tackle the problem,\ninstead of having one module for “general” information, we propose a more specialized architecture,\nLong-Short Range Attention (LSRA), that captures the global and local contexts separately.\nAs shown in Figure 3a, our LSRA module follows a two-branch design. The left branch captures\nglobal contexts, while the right branch models local contexts. Instead of feeding the whole input\nto both branches, we split it into two parts along the channel dimension, which will be mixed by\nthe following FFN layer. Such practice reduces the overall computation by 2×. The left branch is\na normal attention module as in Vaswani et al. (2017), while the channel dimension is reduced by\nhalf. For the right branch of local relationships, one natural idea is to apply convolution over the\nsequence. With a sliding window, the diagonal groups can be easily covered by the module. To\nfurther reduce the computation, we replace the normal convolution with a lighter version (Wu et al.,\n2019b) consisting of linear layers and depth-wise convolution. In this manner, we place the attention\nand the convolutional module side by side, encouraging them to have a different perspective of the\nsentence, globally and locally, so that the architecture can then beneﬁt from the specialization and\nachieve better efﬁciency.\nTo have a better insight, we visualized the average attention weights of the same layer for a fully\ntrained basic transformer and our Lite Transformer in Figure 3. It can be easily distinguished that\ninstead of attempting to model both global and local contexts, the attention module in LSRA only\nfocuses on the global contexts capture (no diagonal pattern), leaving the local contexts capture to the\nconvolution branch.\n5 E XPERIMENTAL SETUP\n5.1 M OBILE SETTINGS\nMost of machine translation architectures beneﬁt from the large model size and computational\ncomplexity. However, edge devices, such as mobile phones and IoTs, are highly computationally\nlimited. Those massive architectures are no more suitable for real-world mobile applications. To\nformalize the problem, we deﬁne the mobile settings for NLP models in terms of the amount of\ncomputation and the parameter numbers:\n5\nPublished as a conference paper at ICLR 2020\n• The ﬂoating-point performance of the ARM Cortex-A72 mobile CPU is about 48G FLOPS (4\ncores @1.5GHz). To achieve the peak performance of 50 sentences per second, the model should\nbe less than 960M FLOPs (480M Mult-Adds). That is a common constraint in the computer\nvision community. For example, Liu et al. (2018) also uses 500M Mult-Adds as the constraint\nof its mobile setting. Therefore, we deﬁne the mobile settings for machine translation tasks: the\ncomputation constraint should be under 500M Mult-Adds(or 1G FLOPs) with a sequence of 30\ntokens (general length for machine translation).\n• Additionally, we set a limitation for the parameters of the models. The constraint is based on the\ndownload and space limitation. Large mobile apps will take long time to be downloaded and even\ncost much money when using cellular networks. The run-time memory and disk size also constrain\nthe parameter numbers. The parameters in MobileNet 7M parameters, we round it to the nearest\nmagnitude, 10M parameters, as our mobile constraint.\n5.2 D ATASETS AND EVALUATION\nMachine Translation. The results are based on three machine translation benchmarks: For\nIWSLT’14 German-English (De-En), we follow the settings in Grave et al. (2017) with 160K\ntraining sentence pairs and 10K joint byte pair encoding (BPE) (Sennrich et al., 2016) vocabulary in\nlower case. For WMT English to German (En-De), we train the model on WMT’16 training data\nwith 4.5M sentence pairs, validate on newstest2013, and test on newstest2014, the same as Wu et al.\n(2019b). Moreover, the vocabulary used a 32K joint source and target BPE. For WMT English to\nFranch (En-Fr), we replicate the setup in Gehring et al. (2017) with 36M training sentence pairs from\nWMT’14, validate on newstest2012 and 2013, and test on newstest2014. Also, the 40K vocabulary is\nbased on a joint source and target BPE factorization.\nFor evaluation, we use the same beam decoding conﬁguration used by Vaswani et al. (2017), where\nthere is a beam size of 4 and a length penalty of 0.6. All BLEUs are calculated with case-sensitive\ntokenization∗, but for WMT En-De, we also use the compound splitting BLEU†, the same as Vaswani\net al. (2017). When testing, we average the last 10 model checkpoints for IWSLT De-En and take\nthe model with the lowest perplexity on the validation set for the WMT datasets. We omit the word\nembedding lookup table from the model parameters since the number of entries in the table would\nhighly differ for various tasks using transformer. For the Mult-Adds, we calculate the total number of\nmultiplication-addition pairs for a model translating a sequence with the length of 30 to a sequence\nwith the same length, which is the average length for sentence-level machine translation tasks.\nAbstractive Summarization. We also evaluate our Lite Transformer on CNN-DailyMail\ndataset (Hermann et al., 2015) for abstractive summarization. The dataset contains 280K news\narticles paired with multi-sentence summaries. We truncate the articles to 1000 tokens and use a\n30K BPE vocabulary. We use F1-Rouge as the metric, including Rouge-1 (R-1), Rouge-2 (R-2) and\nRouge-L (R-L) (Lin, 2004)‡. We follow the generation settings in Lewis et al. (2019). We omit the\nword embedding lookup table and softmax layer from both the model parameters and #Mult-Adds\ncalculation. #Mult-Adds is calculated for the documents with the input length of 30, 100, and 1000\nand the output length of 60 (the average tokens for the output of CNN-DailyMail dataset).\nLanguage Modeling. We test our Lite Transformer for language modeling task on WIKITEXT-103,\nwhich comprises about 100M tokens and a 260K BPE vocabulary. We evaluate the perplexity on both\nthe validation set and the training set. The model parameters and #Mult-Adds are also computed for\nthe input with a length of 30, 100, and 1000.\n5.3 A RCHITECTURE\nThe model architecture is based on the sequence to sequence learning encoder-decoder (Sutskever\net al., 2014). For machine translation, our baseline model is based on the one proposed by Vaswani\net al. (2017) for WMT. For IWSLT, we follow the settings in Wu et al. (2019b). We also adopt\nthe same model as on WMT for summarization task. For language modeling, our model is in line\nwith Baevski & Auli (2019) but with smaller model dimension dmodel = 512 and layer number\n∗https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\n†https://github.com/tensorﬂow/tensor2tensor/blob/master/tensor2tensor/utils/get_ende_bleu.sh\n‡https://github.com/pltrdy/ﬁles2rouge\n6\nPublished as a conference paper at ICLR 2020\n#Parameters #Mult-Adds BLEU ∆BLEU\nTransformer (Vaswani et al., 2017) 2.8M 63M 27.8 –\nLightConv (Wu et al., 2019b) 2.5M 52M 28.5 +0.7\nLite Transformer(Ours) 2.8M 54M 30.9 +3.1\nTransformer (Vaswani et al., 2017) 5.7M 139M 31.3 –\nLightConv (Wu et al., 2019b) 5.1M 115M 31.6 +0.3\nLite Transformer(Ours) 5.4M 119M 32.9 +1.6\nTransformer (Vaswani et al., 2017) 8.5M 215M 32.7 –\nLightConv (Wu et al., 2019b) 8.4M 204M 32.9 +0.2\nLite Transformer(Ours) 8.9M 209M 33.6 +0.9\nTable 1: Results on IWSLT’14 De-En. Our Lite Transformer outperforms the transformer (Vaswani\net al., 2017) and the Lightweight convolution network (Wu et al., 2019b) especially in mobile settings.\nWMT’14 En-De WMT’14 En-Fr\n#Parameters #Mult-Adds BLEU ∆BLEU BLEU ∆BLEU\nTransformer (Vaswani et al., 2017) 2.8M 87M 21.3 – 33.6 –\nLite Transformer(Ours) 2.9M 90M 22.5 +1.2 35.3 +1.7\nTransformer (Vaswani et al., 2017) 11.1M 338M 25.1 – 37.6 –\nLite Transformer(Ours) 11.7M 360M 25.6 +0.5 39.1 +1.5\nTransformer (Vaswani et al., 2017) 17.3M 527M 26.1 – 38.4 –\nLite Transformer(Ours) 17.3M 527M 26.5 +0.4 39.6 +1.2\nTable 2: Results on WMT’14 En-De and WMT’14 En-Fr. Our Lite Transformer improves the BLEU\nscore over the transformer under similar Mult-Adds constraints.\nL = 12 for the resource constraint. We use fairseq’s reimplementation (Ott et al., 2019) of the\ntransformer base model as the backbone.\nIn our architecture, we ﬁrst ﬂatten the bottleneck from the transformer base model and then replace\nthe self-attention with the LSRA. More speciﬁcally, we use two specialized modules, an attention\nbranch and a convolutional branch. Both the input and the output of the convolution are transformed\nby fully connected layers (GLU is applied for the input on WMT), and the kernel is dynamically\ncalculated from the input using a fully connected layer in the WMT models. The kernel sizes are [3,\n5, 7, 31×3] for both the encoder and the decoder (Wu et al., 2019b), and the number of heads for each\nmodule is 4 (half of the heads number in the transformer base model). The model for summarization\nis the same as the WMT model. For language modeling, the kernel sizes for the convolution branch\nare [15, 15, 31×4, 63×6].\n5.4 T RAINING SETTINGS\nAll of our training settings for machine translation are in line with Wu et al. (2019b). We use a\ndropout of 0.3 for both the WMT and IWSLT datasets and linearly scale down the dropout ratio\nwhen shrinking the dimension of the embeddings for the WMT datasets. Same as Wu et al. (2019b),\nwe apply Adam optimizer and a cosine learning rate schedule (Kingma & Ba, 2015; Loshchilov\n& Hutter, 2017) for the WMT models, where the learning rate is ﬁrst linearly warm up from 10−7\nto 10−3 followed by a cosine annealing with a single cycle. For IWSLT De-En, we use inverse\nsquare root learning rate scheduling (Vaswani et al., 2017) with the linear warm-up. We use the same\ntraining settings for summarization. For the language modeling task, the training settings are in line\nwith Baevski & Auli (2019). We decrease the dropout ratio for the FFN layer by half in our Lite\nTransformer due to the ﬂattened layer.\nWe train WMT and summarization models on 16 NVIDIA RTX 2080Ti GPUs and IWSLT De-En\non a single GPU for 50K steps. We also accumulate the gradients for 8 batches before each model\nupdate (Ott et al., 2018). The gradients of IWSLT models are not accumulated. The maximum\nnumber of tokens in a batch is 4K for all the models. Label smooth of 0.1 is applied for the prior\n7\nPublished as a conference paper at ICLR 2020\n#Params #Mult-Adds BLEU GPU Hours CO2e Cloud\n(lbs) Computation Cost\nTransformer (Vaswani et al., 2017) 2.8M 87M 21.3 8 ×12 26 $68 - $227\nEvolved Transformer (So et al., 2019) 3.0M 94M 22.0 8 ×274K 626K $1.6M - $5.5M\nLite Transformer(Ours) 2.9M 90M 22.5 8×14 32 $83 - $278\nTransformer (Vaswani et al., 2017) 11.1M 338M 25.1 8 ×16 36 $93.9 - $315\nEvolved Transformer (So et al., 2019) 11.8M 364M 25.4 8 ×274K 626K $1.6M - $5.5M\nLite Transformer(Ours) 11.7M 360M 25.6 8×19 43 $112 - $376\nTable 3: Performance and training cost of an NMT model in terms of CO2 emissions (lbs) and cloud\ncompute cost (USD). The training cost estimation is adapted from Strubell et al. (2019). The training\ntime for transformer and our Lite Transformer is measured on NVIDIA V100 GPU. The cloud\ncomputing cost is priced by AWS (lower price: spot instance; higher price: on-demand instance).\nLite Transformer with Long-Short Range Attention, ICLR’20\n(b) WIKITEXT-103\n1\nLite Transformer\n(a) WMT’14 En-Fr\n• Our Lite Transformer performs well on machine translation (a), abstractive \nsummarization, and language modeling (b).\n(a) BLEU score vs. Mult-Adds (on WMT En-Fr)\nLite Transformer with Long-Short Range Attention, ICLR’20\n(b) WIKITEXT-103\n2\nLite Transformer\n(a) WMT’14 En-Fr\n• Our Lite Transformer performs well on machine translation (a), abstractive \nsummarization, and language modeling (b). (b) PPL vs. Mult-Adds (on WIKITEXT-103)\nFigure 4: Trade-off curve for machine learning on WMT En-Fr and language modeling on\nWIKITEXT-103 dataset. Both curves illustrate that our Lite Transformer outperform the basic\ntransformer under the mobile settings (blue region).\ndistribution over the vocabulary (Szegedy et al., 2016; Pereyra et al., 2017). For language modeling,\nwe train the models on 24 GPUs for 286K steps, the same as the settings in Baevski & Auli (2019).\n6 R ESULTS\n6.1 M ACHINE TRANSLATION\nResults on IWSLT. We ﬁrst report the results on IWSLT’14 De-En dataset. The baseline model\nis in line with Wu et al. (2019b), which provides the best results in the literature with 512 model\ndimension, 1024 FFN hidden dimension, and 4 heads for the attentions. Our Lite Transformer\ngenerally outperforms the transformer base under mobile constraints. With tighter computation\nlimitations, our model achieves more signiﬁcant improvement. That is because, when the dimension\nof the features decreases, it becomes much harder for the “general” attention to extract both the global\nand local features from the rather more compact information within the features. On the contrary,\nwith the specialized LSRA, our model can capture the information from the features more efﬁciently.\nIn Table 1, we present the quantitative results of our Lite Transformer on IWSLT’14 De-En dataset,\ncomparing to the transformer baseline as well as the LightConv (Wu et al., 2019b). Around 100M\nMult-Adds, our model even achieves 1.6 BLEU score improvement than the transformer.\nResults on WMT. We also show the result on the WMT’14 En-De and WMT’14 En-Fr dataset. Sim-\nilar to the IWSLT, our Lite Transformer achieves a better trade-off with regard to transformer (Vaswani\net al., 2017) against the total computation and the number of model parameters under mobile settings.\nThe quantitative results in Table 2 indicates that our specialized Lite Transformer has 1.2 and 1.7\nBLEU score improvement under 100M Mult-Adds and 0.5 and 1.5 around 300M Mult-Adds for\n8\nPublished as a conference paper at ICLR 2020\nLite Transformer with Long-Short Range Attention, ICLR’20 1\nFurther Compress Lite Transformer by 18.2x\n176\n69.2\n17.3 9.7\n39.9 39.6 39.6 39.5\n35\n39\n43\n0\n80\n160\nTransformer Lite Transformer\n(Ours)\n+Quant (8 bits) +Quant (8 bits)\n+Pruning\nBLEU\nModel Size (MB)\n18.2×\nFigure 5: The model size and BLEU score on WMT En-Fr dataset with model compression. Our Lite\nTransformer can be combined with general compression techniques and achieves 18.2×model size\ncompression. ∗‘Quant’ indicates ‘Quantization’.\n#Params #MAdds (30) #MAdds (100) #MAdds (1000) R-1 R-2 R-L\nTransformer 44.1M 2.0G 3.6G 29.9G 41.4 18.9 38.3\nLite Transformer 17.3M 0.8G 1.5G 12.5G 41.3 18.8 38.3\nTable 4: Results on CNN-DailyMail dataset for abstractive summarization. Our Lite Transformer\nachieves similar F1-Rouge (R-1, R-2 and R-L) to the transformer (Vaswani et al., 2017) with more\nthan 2.4×less computation and 2.5 ×less model size. “#MAdds (x)” indicates the #Mult-Adds\nrequired by the model with the input length of x.\n#Params #MAdds (100) #MAdds (1000) Speed (tokens/s) Valid ppl. Test ppl.\nAdaptive Inputs 37.8M 3.9G 50.3G 7.6K 23.2 24.0\nLite Transformer 37.2M 3.9G 48.7G 10.2K 21.4 22.2\nTable 5: Results on WIKITEXT-103 dataset for language modeling. We apply our Lite Transformer\narchitecture on transformer base model with adaptive inputs (Baevski & Auli, 2019) and achieve 1.8\nlower test perplexity under similar resource constraint.\nWMT En-De dataset and WMT En-Fr dataset respectively. We also provide a tradeoff curve on WMT\nEn-Fr in Figure 4a, where our Lite Transformer consistently outperforms the original transformer.\nAmenable to Compression. As an efﬁcient architecture, our Lite Transformer is orthogonal to\ngeneral techniques for model compression (amenable to compression), e.g. pruning, and quantization.\nThe results on WMT’14 En-Fr dataset with those techniques are shown in Figure 5. We quantize\nthe model weight into 8 bits with K-means (Han et al., 2016) and prune the model according to the\nsensitivity of each layer (Han et al., 2015a). With the two model compression techniques, our method\nachieves 18.2×model size compression with negligible BLEU score degradation.\n6.2 C OMPARISON WITH AUTOMATED DESIGN\nComparing with the AutoML-based Evolved Transformer (ET) (So et al., 2019), our Lite Transformer\nalso shows a signiﬁcant improvement in mobile settings. Moreover, within mobile settings, the Lite\nTransformer outperforms the ET by 0.5 and 0.2 BLEU scores under 100M and 300M Mult-Adds,\nrespectively, as shown in Table 3. Our architecture design is different from ET’s design: ET stacks\nattentions and convolutions sequentially, while our Lite Transformer puts them in parallel; also, ET\ndoes not ﬂatten the FFN.\nThough nowadays, neural architecture search has been proved to be very powerful for searching in\na large design space, the huge cost, more than 626155 lbs CO2 emissions and more than 250 GPU\nyears, cannot be ignored. Instead, careful human design with intuitions for speciﬁc tasks can also be\na great choice in practice to save a large number of resources for the earth.\n9\nPublished as a conference paper at ICLR 2020\n6.3 A BSTRACTIVE SUMMARIZATION AND LANGUAGE MODELING\nWe also test our Lite Transformer on longer input. In Table 4, we report results on CNN-DailyMail\ndataset for abstractive summarization. Our model achieves a similar F1-Rouge score as the transformer\nbase model but requires 2.4×less computation and 2.5×storage resources. In Table 5, we provides\nthe results of our Lite Transformer on WIKITTEXT-103 for language modeling task, compared with\nthe adaptive inputs Baevski & Auli (2019) baseline. Under similar resource constraints, our Lite\nTransformer can achieve 3.9 and 1.8 lower perplexity on valid and test set, respectively. In Figure 4b,\nwe show the tradeoff curve for our model and the baseline transformer model on WIKITEXT-103\nbetween the test perplexity and the #Multi-Adds for input sentence with 30 tokens. It indicates that\nour Lite Transformer achieves consistent improvement over the original transformer, especially under\nmobile settings. Despite the translation tasks, the specialization design of LSRA is effective for larger\nscale language tasks.\n7 C ONCLUSION\nIn this paper, we presented Long-Short Range Attention(LSRA), where some heads specialize in the\nlocal context modeling while the others specialize in the long-distance relationship modeling. Based\non this primitive, we design Lite Transformerthat is specialized for the mobile setting (under 500M\nMult-Adds) to facilitate the deployment on the edge devices. Our Lite Transformer demonstrates\nconsistent improvement over the transformer on multiple language applications. It also surpasses the\nEvolved Transformer that requires costly architecture search under mobile settings.\nAcknowledgements. We sincerely thank MIT-IBM Watson AI Lab, Facebook Faculty Award,\nGoogle-Daydream Research Award, and AWS Machine Learning Research Award for supporting this\nresearch.\nREFERENCES\nKarim Ahmed, Nitish Shirish Keskar, and Richard Socher. Weighted Transformer Network for Machine\nTranslation. arXiv, 2017. 3\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In ICLR, 2019.\n6, 7, 8, 9, 10\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to\nAlign and Translate. In ICLR, 2015. 2\nHan Cai, Ji Lin, Yujun Lin, Zhijian Liu, Kuan Wang, Tianzhe Wang, Ligeng Zhu, and Song Han. Automl for\narchitecting efﬁcient and specialized neural networks. IEEE Micro, 2019a. 3\nHan Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct Neural Architecture Search on Target Task and\nHardware. In ICLR, 2019b. 3\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones,\nMike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng\nChen, Yonghui Wu, and Macduff Hughes. The Best of Both Worlds: Combining Recent Advances in Neural\nMachine Translation. In ACL, 2018. 3\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\narXiv, 2019. 3\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What Does BERT Look At? An\nAnalysis of BERT’s Attention. InBlackboxNLP, 2020. 4\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized Neural\nNetworks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv,\n2016. 3\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In NAACL, 2019. 1\n10\nPublished as a conference paper at ICLR 2020\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional Sequence to\nSequence Learning. In ICML, 2017. 2, 6\nEdouard Grave, Armand Joulin, Moustapha Cissé, Hervé Jégou, et al. Efﬁcient softmax approximation for\nGPUs. In ICML, 2017. 6\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efﬁcient neural\nnetwork. In NeurIPS, 2015a. 9\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both Weights and Connections for Efﬁcient Neural\nNetworks. In NIPS, 2015b. 3\nSong Han, Huizi Mao, and William Dally. Deep Compression: Compressing Deep Neural Networks with\nPruning, Trained Quantization and Huffman Coding. In ICLR, 2016. 3, 9\nYihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural Networks. In\nICCV, 2017. 3\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for Model Compression\nand Acceleration on Mobile Devices. In ECCV, 2018. 3\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and\nPhil Blunsom. Teaching machines to read and comprehend. In NeurIPS, 2015. 6\nLukasz Kaiser, Aidan N Gomez, and François Chollet. Depthwise Separable Convolutions for Neural Machine\nTranslation. In ICLR, 2018. 2\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu.\nNeural Machine Translation in Linear Time. arXiv, 2016. 2\nDiederik Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015. 7\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the Dark Secrets of BERT. In\nEMNLP, 2019. 4\nRaghuraman Krishnamoorthi. Quantizing Deep Convolutional Networks for Efﬁcient Inference: A Whitepaper.\narXiv, 2018. 3\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv, 2019. 6\nMuyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efﬁcient\narchitectures for interactive conditional gans. In CVPR, 2020. 3\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches\nOut. ACL, 2004. 6\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille,\nJonathan Huang, and Kevin Murphy. Progressive Neural Architecture Search. In ECCV, 2018. 6\nZechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Tim Kwang-Ting Cheng, and Jian Sun.\nMetaPruning: Meta Learning for Automatic Neural Network Channel Pruning. arXiv, 2019a. 3\nZhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efﬁcient 3d deep learning. In NeurIPS,\n2019b. 3\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning Efﬁcient\nConvolutional Networks through Network Slimming. In ICCV, 2017. 3\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts. In ICLR, 2017. 7\nMinh-Thang Luong, Hieu Pham, and Christopher Manning. Effective Approaches to Attention-based Neural\nMachine Translation. In EMNLP, 2015. 2\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling Neural Machine Translation. In WMT,\n2018. 3, 7\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael\nAuli. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In NAACL Demo, 2019. 7\n11\nPublished as a conference paper at ICLR 2020\nRomain Paulus, Caiming Xiong, and Richard Socher. A Deep Reinforced Model for Abstractive Summarization.\nIn ICLR, 2018. 3\nGabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural\nnetworks by penalizing conﬁdent output distributions. In ICLR Workshop, 2017. 8\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efﬁcient Neural Architecture Search via\nParameter Sharing. In ICML, 2018. 3\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with Subword\nUnits. In ACL, 2016. 6\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-Attention with Relative Position Representations. In\nNAACL, 2018. 3\nDavid So, Quoc Le, and Chen Liang. The Evolved Transformer. In ICML, 2019. 1, 2, 3, 8, 9\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for Deep Learning\nin NLP. In ACL, 2019. 1, 8\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive Attention Span in\nTransformers. In ACL, 2019a. 3\nSainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Augmenting\nself-attention with persistent memory. arXiv, 2019b. 3\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural Networks. In\nNeurIPS, 2014. 2, 6\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception\narchitecture for computer vision. In CVPR, 2016. 8\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le.\nMnasNet: Platform-Aware Neural Architecture Search for Mobile. CVPR, 2019. 3\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is All you Need. In NeurIPS, 2017. 1, 2, 3, 4, 5, 6, 7, 8, 9\nKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-Aware Automated Quantization\nwith Mixed Precision. In CVPR, 2019. 3\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local Neural Networks. In CVPR, 2018.\n3, 4\nBichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda,\nYangqing Jia, and Kurt Keutzer. FBNet: Hardware-Aware Efﬁcient ConvNet Design via Differentiable Neural\nArchitecture Search. In CVPR, 2019a. 3\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay Less Attention with Lightweight\nand Dynamic Convolutions. In ICLR, 2019b. 2, 5, 6, 7, 8\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu,\nStephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil,\nWei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff\nHughes, and Jeffrey Dean. Google’s Neural Machine Translation System: Bridging the Gap between Human\nand Machine Translation. arXiv, 2016. 2\nTien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig\nAdam. NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications. In ECCV, 2018. 3\nChenzhuo Zhu, Song Han, Huizi Mao, and William Dally. Trained Ternary Quantization. In ICLR, 2017. 3\nBarret Zoph and Quoc V Le. Neural Architecture Search with Reinforcement Learning. In ICLR, 2017. 1, 3\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning Transferable Architectures for\nScalable Image Recognition. In CVPR, 2018. 3\n12\nPublished as a conference paper at ICLR 2020\nA.1 A DDITIONAL VISUALIZATION OF ATTENTION WEIGHTS\nIn this section, we show 3 more visualization of attention weights from both the base transformer and\nour LSRA. We use the smallest conﬁguration in our paper for both models fully trained on WMT\nEn-Fr translation and the attention weights are averaged among attention heads in the ﬁrst layer. The\nsentences are sampled from this paper and the ICLR conference website.\nmobile\nphones\nare\nconstrained\nby\nthe\nhardware\nresources\n.\nmobile\nphones\nare\nconstrained\nby\nthe\nhardware\nresources\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(a) Conventional Attention.\nmobile\nphones\nare\nconstrained\nby\nthe\nhardware\nresources\n.\nmobile\nphones\nare\nconstrained\nby\nthe\nhardware\nresources\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30 (b) Attention in LSRA.\ncurrent\nand\nfuture\nconference\ninformation\nwill\nonly\nbe\nprovided\nthrough\nthis\nwebsite\ncurrent\nand\nfuture\nconference\ninformation\nwill\nonly\nbe\nprovided\nthrough\nthis\nwebsite\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(c) Conventional Attention.\ncurrent\nand\nfuture\nconference\ninformation\nwill\nonly\nbe\nprovided\nthrough\nthis\nwebsite\ncurrent\nand\nfuture\nconference\ninformation\nwill\nonly\nbe\nprovided\nthrough\nthis\nwebsite\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30 (d) Attention in LSRA.\nwhen\nyou\nare\nhappy\nthat\nthe\nlength\nis\ncorrect\n,\nrecord\nthe\nvideo\n.\nwhen\nyou\nare\nhappy\nthat\nthe\nlength\nis\ncorrect\n,\nrecord\nthe\nvideo\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n(e) Conventional Attention.\nwhen\nyou\nare\nhappy\nthat\nthe\nlength\nis\ncorrect\n,\nrecord\nthe\nvideo\n.\nwhen\nyou\nare\nhappy\nthat\nthe\nlength\nis\ncorrect\n,\nrecord\nthe\nvideo\n.\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30 (f) Attention in LSRA.\nFigure A1: Conventional attention puts too much emphasis on local relationship modeling (see\nthe diagonal structure). We specialize the local feature extraction by a convolutional branch which\nefﬁciently models locality so that the attention branch can specialize in global feature extraction (c).\nWe provide some more visualizations in Section A.1.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7714806795120239
    },
    {
      "name": "Transformer",
      "score": 0.7712146043777466
    },
    {
      "name": "Machine translation",
      "score": 0.7066136598587036
    },
    {
      "name": "Language model",
      "score": 0.6981325149536133
    },
    {
      "name": "Automatic summarization",
      "score": 0.5740307569503784
    },
    {
      "name": "Perplexity",
      "score": 0.5241572856903076
    },
    {
      "name": "Architecture",
      "score": 0.49499839544296265
    },
    {
      "name": "Computation",
      "score": 0.49330201745033264
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47811073064804077
    },
    {
      "name": "Natural language processing",
      "score": 0.4319475591182709
    },
    {
      "name": "Computer engineering",
      "score": 0.3267137408256531
    },
    {
      "name": "Speech recognition",
      "score": 0.3245187997817993
    },
    {
      "name": "Algorithm",
      "score": 0.14032191038131714
    },
    {
      "name": "Electrical engineering",
      "score": 0.11560171842575073
    },
    {
      "name": "Engineering",
      "score": 0.09399205446243286
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}