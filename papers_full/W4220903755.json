{
    "title": "Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy",
    "url": "https://openalex.org/W4220903755",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2066930154",
            "name": "Oscar N. E. Kjell",
            "affiliations": [
                "Lund University",
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A1243924924",
            "name": "Sverker Sikström",
            "affiliations": [
                "Lund University"
            ]
        },
        {
            "id": "https://openalex.org/A2810052860",
            "name": "Katarina Kjell",
            "affiliations": [
                "Lund University"
            ]
        },
        {
            "id": "https://openalex.org/A2170210807",
            "name": "H. Andrew Schwartz",
            "affiliations": [
                "Stony Brook University"
            ]
        },
        {
            "id": "https://openalex.org/A2066930154",
            "name": "Oscar N. E. Kjell",
            "affiliations": [
                "Stony Brook University",
                "Lund University"
            ]
        },
        {
            "id": "https://openalex.org/A1243924924",
            "name": "Sverker Sikström",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2810052860",
            "name": "Katarina Kjell",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170210807",
            "name": "H. Andrew Schwartz",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2592574207",
        "https://openalex.org/W2119595472",
        "https://openalex.org/W3124068636",
        "https://openalex.org/W2932216334",
        "https://openalex.org/W88719000",
        "https://openalex.org/W2897583329",
        "https://openalex.org/W3167727006",
        "https://openalex.org/W3160621419",
        "https://openalex.org/W2795781768",
        "https://openalex.org/W2140910804",
        "https://openalex.org/W2810451479",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4252468642",
        "https://openalex.org/W3124281538",
        "https://openalex.org/W3123826154",
        "https://openalex.org/W3124410800",
        "https://openalex.org/W2027828087",
        "https://openalex.org/W2120443961",
        "https://openalex.org/W3011756858",
        "https://openalex.org/W2076077424",
        "https://openalex.org/W2236343782",
        "https://openalex.org/W2070808447",
        "https://openalex.org/W2078483536",
        "https://openalex.org/W1983578042",
        "https://openalex.org/W2316993542",
        "https://openalex.org/W3162022378",
        "https://openalex.org/W3171391618",
        "https://openalex.org/W4234698323",
        "https://openalex.org/W2757124792",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W1993247582",
        "https://openalex.org/W1990136720",
        "https://openalex.org/W2026438880",
        "https://openalex.org/W1995875735",
        "https://openalex.org/W2990427812",
        "https://openalex.org/W2033444463",
        "https://openalex.org/W4367600643",
        "https://openalex.org/W260994251"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports\nNatural language analyzed \nwith AI‑based transformers \npredict traditional subjective \nwell‑being measures approaching \nthe theoretical upper limits \nin accuracy\nOscar N. E. Kjell1,2*, Sverker Sikström1, Katarina Kjell1 & H. Andrew Schwartz2\nWe show that using a recent break‑through in artificial intelligence –transformers–, psychological \nassessments from text‑responses can approach theoretical upper limits in accuracy, converging with \nstandard psychological rating scales. Text‑responses use people’s primary form of communication \n–natural language– and have been suggested as a more ecologically‑valid response format than \nclosed‑ended rating scales that dominate social science. However, previous language analysis \ntechniques left a gap between how accurately they converged with standard rating scales and how \nwell ratings scales converge with themselves – a theoretical upper ‑limit in accuracy. Most recently, \nAI‑based language analysis has gone through a transformation as nearly all of its applications, from \nWeb search to personalized assistants (e.g., Alexa and Siri), have shown unprecedented improvement \nby using transformers. We evaluate transformers for estimating psychological well‑being from \nquestionnaire text‑ and descriptive word‑responses, and find accuracies converging with rating \nscales that approach the theoretical upper limits (Pearson r = 0.85, p < 0.001, N = 608; in line with most \nmetrics of rating scale reliability). These findings suggest an avenue for modernizing the ubiquitous \nquestionnaire and ultimately opening doors to a greater understanding of the human condition.\nWords are the natural medium with which a person expresses their state of mind. However, in personality \nand social psychology today, research is dominated by asking participants to express themselves in the form \nof numeric rating scales where complex states of mind are represented by predefined answers from a rating \nscale. A typical social and psychological article uses 20 measures on average, approximately 87% of which are \nclosed-ended numeric rating  scales1. Although rating scales have contributed to important findings in social \nand personality psychology and other fields, they come with drawbacks. A person asked to communicate their \ninner thoughts and emotions in response to a question (e.g., Are you satisfied with your life?), normally provides \na descriptive, open-ended answer in words (e.g., I’m very satisfied. Most of my expectations are met…), rather \nthan closed-ended numeric or category-based answers (e.g., 6  = Agree).\nLanguage reflects, for example, our  personality2,3, daily  emotions4, mental  health5,6, and  behaviors7,8. As such, \n“language is the most common and reliable way for people to translate their internal thoughts and emotions into \na form that others can understand. Words and language, then, are the very stuff of psychology and communica-\ntion”9. This power of language to reflect psychological aspects of a person, is beginning to be possible to quantify. \nRecent evaluations show that psychologically meaningful scores can be produced by artificial intelligence (AI)-\nbased language  assessments5,10. Anywhere a survey is being used, respondents could instead be asked to freely \ndescribe their mental state rather than forced to adhere to closed-ended  responses10.\nThus, as an alternative to rating scales, questionnaire language-based assessments involve measuring mental \nstates with open-ended responses that are quantified and analyzed with techniques from  AI10. The questionnaire \nOPEN\n1Department of Psychology, Lund University, Lund, Sweden. 2Department of Computer Science, Stony Brook \nUniversity, Stony Brook, USA. *email: oscar.kjell@psy.lu.se\n2\nVol:.(1234567890)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nlanguage-based assessments provide structure in its prompt, while also allowing respondents to freely describe \ntheir state of mind. Research shows that asking respondents to answer with descriptive word-responses or \nfree text-responses, can be used to predict corresponding rating  scales10. This was done in seven studies using \nanswers related to satisfaction with life and harmony in life (Pearson’s r = 0.72, p < 0.001). Although this method \nis promising, some hesitation in the adoption of language-based assessments exists due to an accuracy gap . \nLanguage-based assessments’ correlations to rating scales have fallen short of the accuracy degree with which \nrating scales can be trusted (i.e., taking into account their reliability, measurement error) – which can be seen \nas a theoretical upper-limit of possible alignment to a rating scale.\nRecent advances in AI-based text analysis have yielded unprecedented performance gains in many traditional \napplications of AI such as web search, automatic machine translation or question answering. These advances are \nattributed to a new machine learning technique referred to as the transformer11. Transformers are large, general \npurpose statistical models that have been shown to capture the meaning of words in their  context11,12; for example, \nit understands the difference between “I feel great” and “it was a great personal loss” . To date the most widely cited \ntransformer-language-model is BERT (Bidirectional Encoder Representations from Transformers)12. Evaluations \nover a variety of standard AI-tests including language classification tasks (i.e., not psychological assessments \ntasks) demonstrated that the model typically resulted in 10% or greater reductions in error compared to earlier \n models12. Being able to represent the different meanings of a word depending on its context, can enable research-\ners to better capture the meaning of what a person is trying to express.\nBuilding on a wide tradition in the literature of subjective well-being 13–15, we focus on the satisfaction with \nlife  scale16 as our measure of well-being. Further, to demonstrate our results replicate to newer subjective well-\nbeing constructs we also test against the Harmony in Life  Scale17. Most often the cognitive component of sub -\njective well-being is assessed through satisfaction with life, which encourages life evaluations that are based on \ncomparing one’s actual with one’s desired life  circumstances16. More recently subjective well-being has also been \nassessed through harmony in life, which emphasizes the relational aspect of well-being 18 and encourages life \nevaluations that consider one’s interconnectedness with other aspects of one’s  life17,19. Satisfaction with life and \nharmony in life have been found to meaningfully complement each other in capturing a more comprehensive \nunderstanding of well-being17,19–21.\nAssessing subjective well-being is particularly suitable with open-ended responses because it concerns how \nan individual subjectively (and potentially uniquely) thinks about their life (i.e., their life evaluations). The sub-\njective aim to well-being emphasizes that respondents should be able to consider unique aspects of what they \nfind is important and meaningful regarding their  evaluations13,22. Whereas, the closed-ended rating scale format \nrequires respondents to evaluate fixed rating scales and thus does not allow the generation of unique evaluations, \nthis is achieved with open-ended responses. However, despite advantages of natural language – the difficulties \nof quantifying it have previously resulted in imperfect accuracy.\nHere, we examine whether transformers (i.e., BERT) can close the accuracy gap and bring language-based \nassessments closer to a theoretical upper-limit of accuracy as compared to rating scales. We further examine \ndifferent aspects of the language-based assessment method to understand how they may be used to modern-\nize surveys and the way we understand the human condition. In short, the current analyses involve applying \nnumeric representations (called word embeddings) from pre-trained language models to quantify respondents’ \nword- and text-responses; and then training these word embeddings to predict the rating scales using multiple \nlinear regression. The accuracy is measured as the Pearson correlation between predicted and observed scores \nusing cross-validation (described in the Methods section).\nResults\nLanguage‑based assessments as accurate as rating scales’ reliability. The reliability measures for \nthe Harmony in life scale included r = 0.76 for the mean of inter-item correlations, and r = 0.84 for the mean of \nitem-total correlations; and its previously demonstrated test–retest reliability range from r = 0.71 to 0.7710,17. For \nthe Satisfaction with life scale the mean of the inter-item correlations was r = 0.73, and the mean of the item-total \ncorrelations r = 0.82; and the previously demonstrated test–retest reliabilities ranged from r = 0.82 to 0.8410,17.\nIn theory, the reliability of a measure represents a maximum correlation one might expect to that specific \nmeasure given the noise of the  measure23. Thus, we take the 0.71–0.84 reliability scores for the Harmony in life \nscale, and the 0.73–0.84 for the Satisfaction with life scale to define an upper-limit of how accurately an alterna-\ntive measure could expect to converge with these measures, given the noise of the measures. The language-based \nassessments from all word- and text-responses (i.e., responses to questions about both harmony in life and satis-\nfaction with life) using contextualized word embeddings predict the rating scale rivaling its reliability measures. \nObserved and predicted Harmony in life scale scores yield a very strong Pearson correlation of 0.85 ( p < 0.001; \nTable 1), which is significantly stronger than the mean of inter-item correlations, stronger than the test–retest \nreliability measures and in line with the mean of item-total correlations. The correlation between predicted \nand observed scores for the Satisfaction with life scale also yields a strong correlation of r  = 0.80, which is here \nsignificantly stronger than its inter-item correlation, and approximately in line with its test–retest and the mean \nof item-total correlations. Descriptive statistics and correlations among the numeric variables are presented in \nTable SM1 and SM2.\nCurrent language‑based assessments improve previous state‑of‑the‑art. The current language-\nbased assessments for both Harmony in life and Satisfaction with life significantly improve upon the previous \nstate-of-the-art based on a context-free language model and only one word-response format. Using the previ-\nous state-of-the-art method for Harmony in life yields a Pearson r of 0.75 (Table 2); hence our new correlation \n3\nVol.:(0123456789)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nproduces a significant (p < 0.001) increase of 13%. For Satisfaction with life, the previous method produces an r \nof 0.72 , where our new method yields a significant (p < 0.001) increase of 11%c.f.10.\nLanguage‑based assessments can distinguish well‑being dimensions. To be able to differentiate \nbetween concepts, it is important that a measure has noticeably lower correlations with measures from which \nit theoretically differs. This important characteristic of a psychological measure is called discriminant validity . \nEven though the Harmony in life scale and the Satisfaction with life scale correlate very strongly (r = 0.85), \nboth Harmony in life word- and text-responses predict the corresponding rating scale the Harmony in life scale \n(r = 0.79 and 0.74) significantly more accurately than the Satisfaction with life scale (r = 0.66 and 0.61; Table. 3; \nword-responses: bootstrapped p = 0.003; text-responses: bootstrapped p = 0.004). But neither Satisfaction with \nlife word- nor text-responses predict the Satisfaction with life scale significantly better than the Harmony in life \nscale (word-responses: r = 0.75 versus 0.75, bootstrapped p = 0.877; text-responses: r = 0.74 versus 0.71, boot-\nstrapped p = 0.284).\nWhereas, the Harmony in life scale is not significantly more accurately predicted by Harmony in life words \nas compared with Satisfaction with life words (r = 0.79 versus 0.75, p = 0.064), it is significantly more accurately \npredicted by Harmony in life text-responses rather than Satisfaction with life text-responses (r = 0.74 versus 0.71, \np = 0.013). And the Satisfaction with life scale is more accurately predicted by the Satisfaction with life word-\nresponses rather than the Harmony in life word-responses (r = 0.75 versus 0.66, p < 0.001) and Satisfaction with \nlife text-responses rather than Harmony in life text-responses (r = 0.74 versus 0.61, p < 0.001).\nFurther, even though the predicted Harmony in life scale and the predicted Satisfaction with life scale scores \nare very strongly correlated (r = 0.96, p < 0.001; Table 4), it is possible to train the language models to differenti-\nate between the two with significant accuracy. This is achieved by training the word embeddings to predict the \nTable 1.  Comparing Pearson Correlations based on All Responses Combined and Analyzed with \nContextualized Word Embeddings to the Reliability of the Rating Scales. Italic values indiactes results \nfrom other articles/datasets. All correlations were significant at p < 0.001. N = 608. HIL = Harmony in \nlife; SWL = Satisfaction with life; S = Scale. ↑  = significantly higher than Inter-item correlation average; \n*** = p < 0.001, ** = p < 0.001.\nModel HILS SWLS\nBERT contextualized word embeddings from word- and text-responses of HIL and SWL 0.85↑*** 0.80↑**\nReliability measures\nInter-item Pearson correlation average 0.76 0.73\nCorrected item-total Pearson correlation average 0.84 0.82\nTest–retest reliability110 0.71 0.82\nTest–retest reliability217 0.77 0.84\nTable 2.  Comparison of Pearson Correlations Using Contextualized versus Decontextualized Word \nEmbeddings for Individual Word- and Text-Responses. All correlations were significant at p < .001. N = 608. \nHIL = Harmony in life; SWL = Satisfaction with life; S = Scale. Latent Semantic Analysis based on Google \n5-gram, 512 dimensions; number of dimensions were optimized as described  in10 (i.e., based on previous state-\nof-the-art). ↓  = significantly smaller than BERT. See Table SM3 for more comparisons.\nWord Embeddings\nText-response Word-response\nHIL SWL HIL SWL\nHILS SWLS HILS SWLS\nContext BERT 0.74 0.74 0.79 0.75\nNo context\nBERT 1 word/docs 0.54↓ 0.59↓ 0.78 0.75\nLatent Semantic Analysis 0.47↓ 0.46↓ 0.75↓ 0.72\nTable 3.  The Construct Specific Validity of Language Models Using Individual Word- and Text-Responses. \nN = 608. HIL = Harmony in life; SWL = Satisfaction with life; S = Scale. ↓  = significantly higher than SWLS \nprediction.\nLanguage Model Predict\nPearson r\nHarmony in life responses\nSatisfaction with life \nresponses\nWords Text Words Text\nBERT\nHILS 0.79↑ 0.74↑ 0.75 0.71\nSWLS 0.66 0.61 0.75 0.74\n4\nVol:.(1234567890)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\ndifference scores of the rating scales (i.e., the normalized Harmony in life scale-scores minus the normalized \nSatisfaction with life scale-scores). The highest accuracy is achieved using all responses, which yield considerable \naccuracy (r = 0.34, p < 0.001), especially considering the strong correlation between the Harmony in life scale and \nthe Satisfaction with life scale (r = 0.85, p < 0.001).\nContextualized word embeddings best for text response predictions. Next we investigate to what \nextent different aspects of the language-based assessments contribute to its validity. An important research ques-\ntion concerns whether word- or text-response formats are most suitable for capturing mental states. Previous \nresearch shows that descriptive words rather than text-responses yield more accurate predictions of rating scales \n(word-responses: r = 0.72; text-responses: r = 0.49)10. However, those algorithms were unable to capture the word \norder (i.e., a context-free language model). Here, contextualized BERT embeddings are compared with decon-\ntextualized embeddings by employing BERT context-free and a context-free non-transformer model previously \nused for language-based assessments referred to as Latent Semantic  Analysis24.\nThe contextualized word embeddings produce substantial increments for text-responses compared to when \nthe context is removed. Contextualized BERT significantly increases the predictive accuracy of text-responses as \ncompared to both BERT decontextualized and Latent Semantic Analysis embeddings (Table 2). Compared to the \nprevious state-of-the-art (i.e., Latent Semantic Analysis), contextualized word embeddings produce significantly \nhigher accuracy for both the Harmony in life scale (r = 0.74 versus 0.47, p < 0.001) and the Satisfaction with life \nscale (r = 0.74 versus 0.46, p < 0.001).\nThe contextualized embeddings did not create substantial improvements for word responses, where there \nwas only a significant difference between predictions of the Harmony in life scale from Harmony in life word-\nresponses based on BERT versus Latent Semantic Analysis (0.79 versus 0.75, p  < 0.01; for Satisfaction with life \nword-responses: 0.75 versus 0.72, p = 0.056).\nWord responses produce somewhat higher accuracy than text responses. Analyses based on all \nwords versus all text responses analyzed with BERT demonstrate that word-responses produce more accurate \npredictions for the Harmony in life scale (0.83 versus 0.79, p = 0.002; Table 5), but not for the Satisfaction with \nlife scale (0.77 versus 0.75, p = 0.540). That word-responses overall tend to produce a slightly higher accuracy in \nsome situations is also reflected in the information content that the responses carry. There is more information \nin the word- (Diversity Index = 874.5) as compared with the text-responses (Diversity Index = 409.4). This means \nthat word responses, although containing fewer words, contain greater amount of the mathematical concept of \ninformation; in other words, if one was to store these two pieces of data in their most efficient forms, the word-\nresponses would require more bits on the computer (i.e. they would take up more disk space) than the text-\nresponses even though the text-responses were much longer. That the words from the word-responses comprise \nthe highest information is consistent with the results showing that they produce slightly higher correlations, \nsince more information gives the machine learning algorithms more information to use.\nTable 4.  The Discriminant Validity of Language Models: Significantly Predicting the Harmony in life \nscale Minus the Satisfaction with life scale. N = 608. All correlations (Pearson r) were significant at p < .001. \nAccuracy (r) of Predicted HILS minus SWLS = predicting the difference score of the normalized HILS minus the \nnormalized SWLS, where normalization was achieved by respectively subtracting the column mean from each \nscore.\nLanguage Model Responses Predicted HILS correlated with Predicted SWLS Accuracy (r) of Predicted HILS minus SWLS\nBERT\nAll 0.96 0.34\nWords 0.97 0.25\nText 0.96 0.27\nTable 5.  Word- versus Text-Responses: Accuracy (r), Diversity Index, and Mean (SD) number of words. All \ncorrelations were significant at p < 0.001. N = 608. BERT large using the second last layer (L23). HIL = Harmony \nin life; SWL = Satisfaction with life; S = Scale. Diversity index of Words Input is  2entropy, which indicates how \nmany different “types” (i.e. distinct categories) that could theoretically be accounted for by the data.\nLanguage Response r, HILS prediction r, SWLS prediction Diversity Index of Words Input Mean (SD) of N words\nHIL + SWL Words 0.83 0.77 874.5 19.71 (1.42)\nHIL + SWL Text 0.79 0.75 409.4 145.0 (74.8)\nHIL words + Text 0.82 NA 543.7 79.2 (38.4)\nSWL words + Text NA 0.80 518.4 85.5 (46.0)\nHIL Words .79 0.66 807.0 9.8 (1.0)\nHIL Text .74 0.61 380.1 69.4 (38.4)\nSWL Words .75 0.75 653.0 9.9 (0.73)\nSWL Text .71 0.74 379.4 75.6 (45.9)\n5\nVol.:(0123456789)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nUsing multiple response formats and questions improves accuracy. Two response formats were \nsignificantly more accurate than one response format for the following four relevant combinations of com-\nparisons. The tests compared predictions of the Harmony in life scale based on: Harmony in life word- and \ntext-responses versus only Harmony in life word-responses (r = 0.82 versus 0.79, p < 0.001) and only Harmony \nin life text-responses (r = 0.82 versus 0.74, p < 0.001); as well as predictions of the Satisfaction with life scale \nscores based on: Satisfaction with life word- and text-responses versus only Satisfaction with life word-responses \n(r = 0.80 versus 0.75, p < 0.001) and only Satisfaction with life text-responses (r = 0.80 versus 0.74, p < 0.001).\nNext, we examine how accurately multiple  word- and/or text-responses from different topics/constructs \npredict rating scales. Responses from two construct questions produced predictions that were significantly more \naccurate than predictions from one construct question for all but one comparisons. The significance tests included \ncomparing the Harmony in life scale predictions based on: Harmony in life and Satisfaction with life word-\nresponses versus only Harmony in life word-responses (r = 0.83 versus 0.79, p < 0.001); and Harmony in life and \nSatisfaction with life text-responses versus only Harmony in life text-responses (r = 0.79 versus 0.74, p < 0.001). \nAnd comparing the SLWS predictions based on Harmony in life and Satisfaction with life word-responses versus \nonly Satisfaction with life word-responses (r = 0.77 versus r = 0.75, p = 0.039); and Harmony in life and Satisfaction \nwith life text-responses versus Satisfaction with life text-responses ( r = 0.75 versus 0.74, p = 0.111).\nDiscussion\nBeyond state‑of‑the‑art and the reliabilities. Language-based assessments analyzed with modern \ntransformer language models that enable contextualized word embeddings yield unprecedented high predic-\ntive accuracy of rating scales. Combining responses from both word- and text-responses about Harmony in life \nand Satisfaction with life yields the highest accuracy, which is significantly higher than previous methods. The \npredictive accuracy for the Harmony in life scale is even higher than the rating scales’ reliability as it is typically \nmeasured and seen as the theoretically highest limit.\nThese results demonstrate that word- and text-responses contain valuable information in relation to previ -\nously validated rating scales, which further emphasizes the significance of evidence supporting language-based \nassessments. This includes evidence showing that they exhibit higher, or competitive, degrees of validity and \nreliability when compared with rating  scales10. This has, for example, been shown when comparing language-\nbased assessments’ and rating scales’ ability to accurately categorize external stimuli of pictures depicting facial \nexpressions including sad, happy and contemptuous. Another study revealed a significant positive correlation \nbetween theoretically relevant cooperative behavior  and the language-based assessments of harmony in life \n(Pearson’s r = 0.18; and r = 0.35 in participants categorized as prosocials), but not the corresponding rating  scale7.\nAbility to distinguish well‑being dimensions. The high significant predictive accuracies support the \nvalidity of both the rating scales and the language-based assessments. Since the word- and text-responses were \npresented before the rating scales (see the Method section), the items composing the rating scale did not influ-\nence respondents’ view of the targeted psychological construct. Interestingly, the language response for a specific \nconstruct tended to predict its corresponding rating scale the best. The Harmony in life responses predict the \nHarmony in life scale better than the Satisfaction with life scale; however, this was not true for the Satisfaction \nwith life responses and the Satisfaction with life scale. Further, whereas the Satisfaction with life scale is better \npredicted by Satisfaction with life responses than Harmony in life responses; the Harmony in life scale is only \nbetter predicted by the HIL text-responses rather than the Satisfaction with life text-responses.\nLastly, despite the very strong correlation between the rating scales, it is possible to create a discriminant \nmodel that significantly predicts their difference scores. These findings demonstrate that respondents perceive \nthe constructs differently, and are able to describe this with both language and through rating scales. The AI \nmethods demonstrate that individuals show surprising concordance between self-reported rating scales and \nopen-ended questions.\nContextualized word embeddings. Contextualized, as compared to decontextualized, word embeddings \nparticularly increase the predictive accuracy from text-responses. They substantially increase the predictability \nof text-responses, which almost reaches the same accuracy as descriptive word-responses (only word-responses \nfor Harmony in life are significantly more predictive than the text-responses, and also with a small effect size). \nThis is an important finding because it opens up the opportunity to make better use of the text-response format.\nComplementary response formats. Comparing the strongest correlation when only using one response \n(r = 0.79, Table 4) versus using all responses (r = 0.85, Table 3), demonstrates that adding responses increases the \npredictive accuracy. The different response formats complement each other in both predictive ability and practi-\ncal advantages. Descriptive words are less demanding to write (i.e., fewer words to write), text is more natural \n(i.e., less constrained). Further, changing response formats may promote thinking about the same question in \ndifferent ways, from different perspectives.\nComparisons with predictions of subjective states from social media text. Language-based \nassessments based on direct prompts/questions produce considerably stronger correlations than those derived \nfrom individuals’ social media profiles. Research shows that it is also possible to assess individuals’ subjective \nstates of mind by analyzing their social media text (e.g., from Facebook and Twitter). These analyses have been \ndemonstrated to rigorously predict psychological and health related outcomes such as satisfaction with life \n(r = 0.57)25, and personality (r = 0.31-0.42) 2. But, language-based assessments based on prompts are different \n6\nVol:.(1234567890)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nfrom social media text analyses, as they resemble rating scales in directly asking individuals to communicate \ntheir state of mind rather than using naturally occurring data, which potentially explains the current unprec-\nedented high predictive accuracy.\nLimitations, Conclusions and Future Research. This study focuses on examining the relationships \nbetween language-based assessments and rating scales; but, it does not compare which of the two are the \nmost valid or reliable. Even though the rating scales have been validated, and used in a wide range of research \n settings14,26, self-reported measures are not objective truth, and future research should compare rating scale \nand language-based assessments in predicting theoretically relevant behaviors, biological markers etc. Notably, \nlanguage-based assessments can also be constructed to predict these outcomes directly, and thus assess mental \nstates independent of rating scales. Lastly, the sample included respondents online from the USA only, generali-\nzation beyond this should be done with caution.\nWe show that open-ended, text-responses predict rating scales with unprecedented accuracy. The accuracy \nis not only significantly higher than previous methods, but higher than or rivaling the typical ways that scales’ \nreliability are measured–which is normally conceived as the upper theoretical  limit23. We also show that using \nonly 10 descriptive words can reveal a lot of psychological information, that contemporary language models \nproduce very accurate predictions from analyzing text-responses, and that combining responses increases the \npredictive accuracy as compared with only using one response. These results provide promising evidence that \nlanguage, the most natural way of conveying complex psychological traits and states of mind, can be quantified \nto improve and modernize current research methods and improve clinical practices. We envision that these \nmethods are applicable for widespread use in scientific research, including fields such as psychology, neurosci -\nence, or medicine. Hence, these findings suggest an avenue for modernizing self-report human assessment and \nultimately opening doors to a greater understanding of the human condition.\nMethods\nParticipants. The data used here is by convenience pooled from three previously published studies that \nreceived ethical approval from the Regional Ethics Board in Lund, and adhered to Swedish laws (Study 3: N = 92; \nStudy 4: N = 303; Study 5: N = 296)10. Participants were recruited using Mechanical Turk (www. mturk. com). \nFifty-six participants were excluded for not answering the control items correctly, which were for Study 3: N = 13, \nStudy 4, N = 24, and Study 5, N = 19. Out of the remaining 691 participants, an additional 79 were excluded for \nnot reporting USA as nationality; and 4 participants were excluded for not answering all four open-ended ques-\ntions. The final sample comprised 608 participants from the USA (359 females, 249 males, 0 others), with a mean \nage of 35.0 (SD = 12.88, range = 18–74) years. Participants’ perceived household economic situation ranging from \n“1 = Our income does not cover our needs, there are great difficulties” to “7 = Our income covers our needs, we \ncan save” had a reported mean of 4.44 (SD = 1.96).\nInstruments. Open-ended questions for harmony in life (i.e., “Overall in your life, are you in harmony or \nnot?”) and satisfaction with life (i.e., “Overall in your life, are you satisfied or not?”), were coupled with instruc-\ntions to either answer with 10 descriptive words or a text-response.\nThe Harmony in Life Scale17 measures Harmony in life with five items (e.g., “My lifestyle allows me to be in \nharmony”), coupled with closed-ended rating scales ranging from “1 = Strongly Disagree” to “7 = Strongly Agree” .\nThe Satisfaction with Life Scale16 measures Satisfaction with life with five items (e.g., “I am satisfied with my \nlife”) coupled with the same rating scales alternatives as the Harmony in life scale.\nFor more details of measures see SM.\nProcedure. The studies followed Swedish law and received ethical approval from the Regional Ethical Com-\nmittee in Lund, Sweden (2014/396). Participants were first informed about the general purpose of the studies and \ntheir right to withdraw at any time, that their responses were confidential and that they did not involve collect-\ning personal identifiable information – informed consent was then obtained from all participating participants. \nFirst, participants were asked to answer the open-ended questions, which were presented in random order. \nSubsequently, the rating scales were presented in random order. The demographic questions were presented last, \nfollowed by debriefing. The mean time to complete the studies were 14.58 (SD = 9.02), 16.44 (SD = 19.48) and \n20.02 (SD = 10.31) minutes for study 3, 4 and 5,  respectively10.\nAnalytic method. The text analyses were carried out in Text27 (version 0.9.11), which is an R-package28 spe-\ncialized in enabling social scientists to use state-of-the-art natural language processing and machine learning. In \nshort, the current analyses involve applying word embeddings from pre-trained language models to quantify the \nword- and text-responses; and then training the word embeddings to predict the rating scales using regression.\nPre-trained word embeddings. To train high-quality word embeddings requires large amounts of text data; and \nwhen that is not available it is possible to use a general-purpose language representation model built on other \ntext data. This is known as pre-training. We will compare bidirectional contextual and context-free pre-trained \nword embeddings. Bidirectional contextual word embeddings are influenced by other words in a text. In “She \nlooked at the bank account” , the embedding of “bank” is also  influenced by the previous and  following con-\ntext (i.e., “She looked at the … account”). The primary bidirectional pre-trained model used here comes from \nGoogle’s open-source model called BERT (“bert-large-uncased”; henceforth BERT; Devlin et al., 2019). BERT \nrepresents tokens (c.f. words) with 24 layers comprising 1024 dimensions each. Only the second to last layer was \n7\nVol.:(0123456789)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nused based on research demonstrating that this layer yields reliable performances for document- and human-\nlevel  predictions12,29. The contextualized embeddings will be compared with context-free embeddings by letting \nthe BERT model only see one word at a time (1 word/document), and using the context-free Latent Semantic \nAnalysis based model from Kjell et al. (2019).\nThe BERT-large (“bert-large-uncased”) and Latent Semantic Analysis models are also compared with BERT-\nbase7 (“bert-base-uncased”) and  DistilBert23 (“distilbert-base-uncased”). These models are based on text retrieved \nfrom Wikipedia and a Book corpus. BERT-large comprises 24 layers, where we used layer 23 for the analyses; \nthe BERT-base model comprises 12 layers, where we used layer 11 for the analyses; and DistillBERT has 6 layers, \nwhere we used layer 5. For more details about the creation of the BERT model  see7.\nTraining word embeddings to rating scales.  To examine the relationship between word- and text-responses \nwith rating scales, the word embeddings dimensions of the responses are used as predictors in ridge multiple \n regression30 to predict the rating scale scores. Training is employed using tenfold cross-validation, where the \ntraining set is further split for analysis (75% of the training data is used to create models with different penal-\nties) and assessment (25% used to evaluate the different models). The prediction accuracies are evaluated with \nPearson correlation between observed and predicted scores.\nThe training sets were stratified according to the outcome (y), using 4 bins to stratify over. Further, the search \ngrid for the penalty in ridge regression ranged from  10−16–1016, with increases of times 10. For more details  see16. \nThese ranges were based on a wide range of empirical evidence from applications of the DLATK  package31 which \nhas applied the same models for Python rather than R.\nOur models use a convex optimization on top of pre-trained transformers (shown to be ideal for person-level \nassessments when having N people <  100032). Therefore, optimization epochs are run until convergence, which \nis guaranteed. Advanced users may wish to leverage fine-tuning aspects in which maximum epochs and early \nstopping criteria which are exposed through the libraries bindings with the HuggingFace transformers  package33. \nFuture work will provide a guide for such an approach.\nSignificance testing the prediction accuracy between models. To test the difference between two prediction mod-\nels of the same outcome, we first compute the error for each prediction (i.e., y-ˆy ), and then use a paired sample \nt-test to compare whether the errors differ between the two models. To test two prediction models of different \noutcomes (e.g., comparing a model that predicts the Harmony in life scale with one predicting the Satisfaction \nwith life scale) a bootstrapped procedure was used. We used a monte-carlo  simulation34,35 whereby bootstrapped \nresampling was used to create a distribution of accuracies for each model. Subsequently, the overlap of the two \nbootstrapped distributions were compared.\nReliability of the to-be-predicted measure as the upper limits of prediction accuracies. It is important to consider \nthe upper limits of how accurate rating scales can be predicted. Statistically, an observed correlation between two \nmeasures is not only influenced by the relation between the latent traits, but also the reliability of the measures \n(see the Attenuation Correlation  Coefficient23,36); where the reliabilities of the measures limit the upper bound \nof the correlational strength that may be found. For example, the measurement of weight and height can be \nmeasured with a reliability of near 1.0, which means that a correlation of r = 0.4, represents 0.4 out of an upper \nlimit of nearly  1.0. In contrast, self-report measures of psychological constructs tend to exhibit considerably \nlower reliabilities, which thus lowers the possible upper limit. Measures of psychological constructs that are con-\nsidered well-constructed often display reliabilities around r = 0.8. Hence, an r = 0.7 between actual and predicted \nscores of such measures can be seen as 0.7 out of the upper limit of 0.8. Therefore, we compare correlations \nbetween predicted and observed scores with the rating scales’ reliability as measured by their test–retest reli-\nabilities (i.e., across time as retrieved from previous research), the rating scales’ corrected item-total correlations \n(i.e., the mean of the Pearson correlation between each item and the total score of all other items in the scale) and \ninter-item correlation average (i.e., the mean of the Pearson correlations among all individual items). We used \nreliability measures based on Pearson product-moment correlation instead of other reliability metrics, such as \nCronbach’s alpha and McDonald’s omega (which are reported in the supplement material), so that it is directly \ncomparable to the Pearson-correlation between our predicted scores and the observed scores.\nInformation theory. The Diversity Index based on Shannon  Entropy37 (i.e.,  2entropy) is used to measure how \nmuch information a response format comprises. This is a key measure in machine learning as it indicates how \nmuch information the algorithms have at their disposal to learn.\nCutoffs. Alpha was set to 0.05. All correlations were computed as Pearson product-moment correlation coeffi-\ncients (r). Correlations of 0.2-0.39 are interpreted as weak, 0.40-0.59 as moderate, 0.6-0.79 as strong and 0.8–1.0 \nas very strong.\nR‑References. Analyses were carried out in  RStudio38, and included using the following packages: \n text27,  tidyverse39,  entropy40,  stringr41,  tidyr42,  Hmisc43, data.table44,  car45,  rsample46, and  psych47.\nReceived: 29 July 2021; Accepted: 21 February 2022\n\n8\nVol:.(1234567890)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nReferences\n 1. Flake, J. K., Pek, J. & Hehman, E. Construct validation in social and personality research: current practice and recommendations. \nSoc. Psychol. Personal. Sci. https:// doi. org/ 10. 1177/ 19485 50617 693063 (2017).\n 2. Schwartz, H. A. et al. Personality, gender, and age in the language of social media: the open-vocabulary approach. PLoS ONE 8, \ne73791 (2013).\n 3. Argamon, S., Koppel, M., Pennebaker, J. W . & Schler, J. Mining the blogosphere: age, gender and the varieties of self-expression (First \nMonday, Canton, 2007).\n 4. Sun, J., Schwartz, H. A., Son, Y ., Kern, M. L. & Vazire, S. The language of well-being: tracking fluctuations in emotion experience \nthrough everyday speech. J. Personal. Soc. Psychol. 118, 364 (2020).\n 5. Eichstaedt, J. C. et al. Facebook language predicts depression in medical records. Proc. Natl. Acad. Sci. 115, 11203–11208 (2018).\n 6. Kjell, K., Johnsson, P . & Sikström, S. Freely generated word responses analyzed with artificial intelligence predict self-reported \nsymptoms of depression, anxiety, and worry. Front. Psychol. 12, 602581 (2021).\n 7. Kjell, O., Daukantaitė, D. & Sikström, S. Computational language assessments of harmony in life—not satisfaction with life or \nrating scales—correlate with cooperative behaviors. Front. Psychol. https:// doi. org/ 10. 3389/ fpsyg. 2021. 601679 (2021).\n 8. Curtis, B. et al. Can Twitter be used to predict county excessive alcohol consumption rates?. PLoS ONE 13, e0194290 (2018).\n 9. Tausczik, Y . R. & Pennebaker, J. W . The psychological meaning of words: LIWC and computerized text analysis methods. J. Lang. \nSoc. Psychol. 29, 24–54 (2010).\n 10. Kjell, O. N., Kjell, K., Garcia, D. & Sikström, S. Semantic measures: Using natural language processing to measure, differentiate, \nand describe psychological constructs. Psychol. Methods 24, 92 (2019).\n 11. Vaswani, A. et al. Attention is all you need. In Advances in neural information processing systems 5998–6008 (2017).\n 12. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long and Short Papers) 4171–4186 (Association for Computational Linguistics, 2019). \nhttps:// doi. org/ 10. 18653/ v1/ N19- 1423.\n 13. Diener, E. Subjective well-being. Psychol. Bull. 95, 542–575 (1984).\n 14. Pavot, W . & Diener, E. Review of the satisfaction with life scale. In Assessing well-being: the collected works (ed. Diener, E.) 101–117 \n(Springer, Netherlands, 2009). https:// doi. org/ 10. 1007/ 978- 90- 481- 2354-4_5.\n 15. Diener, E., Inglehart, R. & Tay, L. Theory and validity of life satisfaction scales. Soc. Indic. Res. 112, 497–527 (2013).\n 16. Diener, E., Emmons, R. A., Larsen, R. J. & Griffin, S. The satisfaction with life scale. J. Personal. Assess. 49, 71–75 (1985).\n 17. Kjell, O. N. E., Daukantaitė, D., Hefferon, K. & Sikström, S. The harmony in life scale complements the satisfaction with life scale: \nexpanding the conceptualization of the cognitive component of subjective well-being. Soc. Indic. Res. 126, 893–919 (2016).\n 18. Li, C. The philosophy of harmony in classical confucianism. Philos. Compass 3, 13 (2008).\n 19. Kjell, O. N. E. & Diener, E. Abbreviated three-item versions of the satisfaction with life scale and the harmony in life scale yield as \nstrong psychometric properties as the original scales. J. Personal. Assess. https:// doi. org/ 10. 1080/ 00223 891. 2020. 17370 93 (2020).\n 20. Delle Fave, A., Brdar, I., Freire, T., Vella-Brodrick, D. & Wissing, M. P . The eudaimonic and hedonic components of happiness: \nqualitative and quantitative findings. Soc. Indic. Res. 100, 185–207 (2011).\n 21. Delle Fave, A. et al. Lay definitions of happiness across nations: the primacy of inner harmony and relational connectedness. Front. \nPsychol. https:// doi. org/ 10. 3389/ fpsyg. 2016. 00030 (2016).\n 22. Diener, E. Subjective well-being: The science of happiness and a proposal for a national index. Am. Psychol. 55, 34–43 (2000).\n 23. Spearman, C. The proof and measurement of association between two things. Am. J. Psychol. 15, 72–101 (1904).\n 24. Landauer, T. K. & Dumais, S. T. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction, and \nrepresentation of knowledge. Psychol. Rev. 104, 211–240 (1997).\n 25. Schwartz, H. A. et al. Predicting individual well-being through the language of social media. In 516–527 (2016).\n 26. Kjell, O. N. & Diener, E. Abbreviated three-item versions of the satisfaction with life scale and the harmony in life scale yield as \nstrong psychometric properties as the original scales. J. Personal. Assess. 103, 183–194 (2021).\n 27. Kjell, O., Schwartz, H. A. & Giorgi, S. Text: an R-package for analyzing and visualizing human language using natural language \nprocessing and deep learning. (2021).\n 28. R Core Team. R: A Language and Environment for Statistical Computing. (R Foundation for Statistical Computing, 2014).\n 29. V Ganesan, A., Matero, M., Ravula, A. R., Vu, H. & Schwartz, H. A. Empirical Evaluation of Pre-trained Transformers for Human-\nLevel NLP: The Role of Sample Size and Dimensionality. Rev. (2021).\n 30. Hoerl, A. E. & Kennard, R. W . %J T Ridge regression: Biased estimation for nonorthogonal problems. Technometrics  12, 55–67 \n(1970).\n 31. Schwartz, H. A. et al. Dlatk: Differential language analysis toolkit. In 55–60 (2017).\n 32. Ganesan, A. V ., Matero, M., Ravula, A. R., Vu, H. & Schwartz, H. A. Empirical evaluation of pre-trained transformers for human-\nlevel NLP: the role of sample size and dimensionality. ArXiv Prepr. http:// arxiv. org/ abs/ 21050 3484 (2021).\n 33. Wolf, T. et al. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv http:// arxiv. org/ abs/ 19100 3771Abs \n(2019).\n 34. Stine, R. A. Bootstrap prediction intervals for regression. J. Am. Stat. Assoc. 80, 1026–1031 (1985).\n 35. Das, S., Spall, J. C. & Ghanem, R. Efficient Monte Carlo computation of Fisher information matrix using prior information. Comput. \nStat. Data Anal. 54, 272–289 (2010).\n 36. Muchinsky, P . M. The correction for attenuation. Educ. Psychol. Meas. 56, 63–75 (1996).\n 37. Shannon, C. E. A mathematical theory of communication. Bell Syst. Tech. J. 27, 379–423 (1948).\n 38. RStudio Team. RStudio: Integrated Development Environment for R. (RStudio, PBC., 2020).\n 39. Wickham, H. et al. Welcome to the Tidyverse. J. Open Source Softw. 4, 1686 (2019).\n 40. Hausser, J. & Strimmer, K. Entropy: estimation of entropy, mutual information and related quantities. R package version 1.2. 1. \nSee Httpstrimmerlab Org (2014).\n 41. Wickham, H. stringr: Simple, Consistent Wrappers for Common String Operations. (2019).\n 42. Wickham, H. & Henry, L. tidyr: Tidy Messy Data. (2020).\n 43. Jr, Dupont F . E. H. with contributions from C. & others, many. Hmisc: Harrell Miscellaneous. (2020).\n 44. Dowle, M. & Srinivasan, A. data.table: Extension of `data.frame`. (2019).\n 45. Fox, J. & Weisberg, S. An R companion to applied regression (Sage publications, 2018).\n 46. Kuhn, M., Chow, F . & Wickham, H. rsample: General Resampling Infrastructure. (2020).\n 47. Revelle, W . psych: Procedures for Psychological, Psychometric, and Personality Research. (Northwestern University, 2019).\nAuthor contributions\nAll authors contributed to the study design. O.N.E. Kjell and H.A. Schwartz performed the data analyses and \ndrafted the manuscript. All authors were involved in critical revisions and approved the final version of the \nmanuscript for submission.\n9\nVol.:(0123456789)Scientific Reports |         (2022) 12:3918  | https://doi.org/10.1038/s41598-022-07520-w\nwww.nature.com/scientificreports/\nFunding\nOpen access funding provided by Lund University.\nCompeting interests \nO.N.E. Kjell, S. Sverker., and K. Kjell have co-founded a start-up using computational language assessments to \ndiagnose mental health problems. HA Schwartz does not have any competing interest to declare.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 022- 07520-w.\nCorrespondence and requests for materials should be addressed to O.N.E.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}