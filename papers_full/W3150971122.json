{
    "title": "Variational Transformer Networks for Layout Generation",
    "url": "https://openalex.org/W3150971122",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2903482114",
            "name": "Diego Martìn Arroyo",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2965548820",
            "name": "Janis Postels",
            "affiliations": [
                "ETH Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2013077172",
            "name": "Federico Tombari",
            "affiliations": [
                "Technical University of Munich",
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2903482114",
            "name": "Diego Martìn Arroyo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2965548820",
            "name": "Janis Postels",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2013077172",
            "name": "Federico Tombari",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2960053204",
        "https://openalex.org/W1985238052",
        "https://openalex.org/W2557465155",
        "https://openalex.org/W1923184257",
        "https://openalex.org/W125693051",
        "https://openalex.org/W2964334375",
        "https://openalex.org/W2964669873",
        "https://openalex.org/W2810181048",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2983248633",
        "https://openalex.org/W6758151150",
        "https://openalex.org/W2997154779",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6754074526",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2098883970",
        "https://openalex.org/W3000176874",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6733471323",
        "https://openalex.org/W3035050475",
        "https://openalex.org/W6730998768",
        "https://openalex.org/W2154241802",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W2765874585",
        "https://openalex.org/W6637568146",
        "https://openalex.org/W2104518905",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W6780253416",
        "https://openalex.org/W6638836233",
        "https://openalex.org/W6771898956",
        "https://openalex.org/W6640963894",
        "https://openalex.org/W3003711898",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2964346820",
        "https://openalex.org/W6757941371",
        "https://openalex.org/W2978613765",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W6774882322",
        "https://openalex.org/W2971196067"
    ],
    "abstract": "Generative models able to synthesize layouts of different kinds (e.g. documents, user interfaces or furniture arrangements) are a useful tool to aid design processes and as a first step in the generation of synthetic data, among other tasks. We exploit the properties of self-attention layers to capture high level relationships between elements in a layout, and use these as the building blocks of the well-known Variational Autoencoder (VAE) formulation. Our proposed Variational Transformer Network (VTN) is capable of learning margins, alignments and other global design rules without explicit supervision. Layouts sampled from our model have a high degree of resemblance to the training data, while demonstrating appealing diversity. In an extensive evaluation on publicly available benchmarks for different layout types VTNs achieve state-of-the-art diversity and perceptual quality. Additionally, we show the capabilities of this method as part of a document layout detection pipeline.",
    "full_text": "Variational Transformer Networks for Layout Generation\nDiego Martin Arroyo1\nmartinarroyo@google.com\n1Google, Inc\nJanis Postels2\njpostels@vision.ee.ethz.ch\n2ETH Z¨urich\nFederico Tombari1,3\ntombari@google.com\n3Technische Universit¨at M¨unchen\nAbstract\nGenerative models able to synthesize layouts of different\nkinds (e.g. documents, user interfaces or furniture arrange-\nments) are a useful tool to aid design processes and as a ﬁrst\nstep in the generation of synthetic data, among other tasks.\nWe exploit the properties of self-attention layers to capture\nhigh level relationships between elements in a layout, and\nuse these as the building blocks of the well-known Varia-\ntional Autoencoder (VAE) formulation. Our proposed Vari-\national Transformer Network (VTN) is capable of learning\nmargins, alignments and other global design rules without\nexplicit supervision. Layouts sampled from our model have\na high degree of resemblance to the training data, while\ndemonstrating appealing diversity. In an extensive evalua-\ntion on publicly available benchmarks for different layout\ntypes VTNs achieve state-of-the-art diversity and percep-\ntual quality. Additionally, we show the capabilities of this\nmethod as part of a document layout detection pipeline.\n1. Introduction\nLayouts, i.e. the abstract positioning of elements in a\nscene or document, constitute an essential tool for various\ndownstream tasks. Consequently, the ability to ﬂexibly ren-\nder novel, realistic layouts has the potential to yield sig-\nniﬁcant improvements in many tasks, such as neural scene\nsynthesis [36], graphic design or in data synthesis pipelines.\nEven though the task of synthesizing novel layouts has re-\ncently started to gain the attention of the deep learning com-\nmunity [23, 16, 22, 28], it is still a sparsely explored area\nand provides unique challenges to generative models based\non neural networks, namely a non-sequential data structure\nconsisting of varying length samples with discrete (classes)\nand continuous (coordinates) elements simultaneously.\nGenerative models based on neural networks have re-\nceived a signiﬁcant share of attention in recent years, as\nthey proved capable of learning complex, high-dimensional\ndistributions. Common formulations such as Generative\nAdversarial Networks (GANs) [8] and Variational Autoen-\nTextFigure\nText\nText\nText\nList\nText\nTitle\nText\nToolbar\nText\nList ItemList Item\nList Item\nList Item\nList Item\nText\nbox\nshelf\nbox\nchair\npapers desktop\ndesk\ndesk\nmonitor\nDocument UI design Natural Scene Room layout\nFigure 1: Given a random vector z, our novel transformer\nV AE model produces layouts that follow the design con-\nstraints of the training data. It can generate various layouts\ntypes, from documents to objects and scenes.\ncoders (V AEs) [21] have shown impressive results in tasks\nsuch as image translation [43], image synthesis [17], and\ntext generation [2]. A GAN is comprised of an arrangement\nof generator-discriminator neural networks in a zero-sum\nconﬁguration, while a V AE learns a lower bound of the data\ndistribution using an encoder-decoder neural network with a\nregularized bottleneck. Since these are general frameworks,\nthey leave room for adapting the underlying neural architec-\ntures to exploit the properties of the data. For example, the\nweight sharing strategy of Convolutional Neural Networks\n(CNNs) renders them the most common building block for\nimage processing, while for sequential data (e.g., text), Re-\ncurrent Neural Networks (RNNs) or attention modules are\noften the architecture of choice. In particular, the attention\nmechanism has recently demonstrated strong performance\non a variety of tasks, such as language translation [35] and\nobject detection [3], proving its superiority over RNNs re-\ngarding modeling long-term relationships.\nPrior work has built the foundation by proving the ef-\narXiv:2104.02416v1  [cs.CV]  6 Apr 2021\nfectiveness of deep learning to generate novel documents\n[22, 28, 9], natural scenes [16] and User Interface (UI) de-\nsigns [22]. Mostly, the location and size of a given element\ndepends not only on the particularities of its type (e.g. titles\ntend to be small and at the top of a document, while ﬁgures\nor tables usually occupy a signiﬁcant amount of space), but\nalso on their relationship to other elements. One way to\nincorporate this knowledge into modeling a layout distri-\nbution is to deﬁne handcrafted rules, ( e.g. enforcing mar-\ngins, alignment, the allowed number of elements in a docu-\nment. . . ). However, such rules are subjective, hard to deﬁne\nunambiguously and certainly do not generalize to arbitrary\nlayout distributions. Consequently, we refrain from mod-\neling any prior knowledge by i.e. enforcing heuristics, and\ninstead equip the neural architecture itself with an inherent\nbias towards learning the relationship between elements in a\nlayout. This makes the attention mechanism a suitable fun-\ndamental architectural component, since it naturally models\nmany-to-many relationships and is, thus, particularly suit-\nable for discovering relationships in a given layout distribu-\ntion in an unsupervised manner.\nBy instantiating the V AE framework with an attention-\nbased architecture, this work investigates an important gap\nin literature. We explore relevant design choices in great\ndetail - e.g. autoregressive vs. non-autoregressive decoder,\nlearned vs. non-learned prior. Furthermore, we tailor our\nnovel approach to the yet under-explored task of layout\ngeneration, where we demonstrate state-of-the-art perfor-\nmance across various metrics on several publicly available\ndatasets. To summarize, our main contributions are:\n• A novel generative model specialized in layout gener-\nation that incorporates an inductive bias towards high-\nlevel relationships between a large number of elements\nin a layout without annotations.\n• Exploration of strategies for creating a variational bot-\ntleneck on sequences with varying lengths.\n2. Related work\nLayout synthesis The task of layout synthesis has not yet\nbeen exhaustively covered by literature, but fueled increas-\ning interest in the research community in recent years.Lay-\noutGAN [23] is, to the best of our knowledge, the ﬁrst\npaper to apply generative models (in particular GANs) to\nthis task. The authors use a generator network to synthe-\nsize bounding box annotations. In order to use a CNN as\ndiscriminator, LayoutGAN applies a novel differential ren-\nder module to turn a collection of bounding boxes into an\nimage. Similarly to our approach, it uses self-attention to\nmodel many-to-many relationships. However, the authors\nonly evaluate single-column documents with at most nine\nelements, which corresponds to much sparser layouts than\nprovided by common publicly available datasets.\nLayoutV AE[16] proposes an autoregressive model\nbased on a conditional V AE with a conditional prior (con-\nditioned on the number and type of elements in the layout).\nThe authors use an Long Short-Term Memory (LSTM) [13]\nto aggregate information over time. Additionally they pro-\npose using a second conditional V AE to model the dis-\ntribution of category counts which is used as conditional\ninformation during layout generation. Their underlying\nneural architecture is comprised of fully-connected layers\nand LSTMs. Consequently, it is expected that LayoutV AE\nstruggles to model layouts with a large number of elements,\nsince LSTMs do not explicitly model the relationships of all\ncomponents. Unlike LayoutV AE, our work explicitly biases\nthe underlying neural network towards learning the rela-\ntionships between elements in a layout, and only makes the\ndecoder autoregressive (reducing the computational costs).\nFurther, we only train a single V AE for learning the layout\ndistribution instead of resorting to two separate V AEs.\nIn Neural Design Networksthe authors [22] generate\ndocument layouts with an optional set of design constraints.\nInitially, a complete graph for modeling the relationships\nbetween elements is built. The distribution of these relation-\nships is learned using a V AE based on Graph Convolution\nNetworks (GCNs), where the labels of the relationships are\nbased on heuristics. The actual layout is subsequently gen-\nerated by a separate GCN. The resulting raw layout is then\npolished by an additional reﬁnement network. In contrast to\nNeural Design Networks, this work does not rely on labels\nextracted using heuristics on the training data for learning a\nlayout distribution, which is prone to introduce ambiguities\nand unlikely to generalize across datasets. Moreover, our\napproach learns the layout distribution end-to-end without\nrelying on training three separate neural networks.\nSimilarly, READ [28] also uses heuristics to determine\nthe relationships between elements and then trains a V AE\nwhich is based on Recursive Neural Networks (RvNNs) [7]\nto learn the layout distribution.\nContent-aware Generative Modeling of Graphic De-\nsign Layouts [39] trains a V AEGAN conditioned on im-\nages, keywords, attributes of the layout and corresponding\ncoordinates. However, the authors focus on learning the lay-\nout distribution conditioned on additional user input.\nLayout Generation and Completion with Self-\nattention [9] is most relevant to this work. The authors\nperform self-supervised training (i.e. layout completion) us-\ning an autoregressive decoder motivated by Transformers\n[35]. Subsequently, novel layouts are synthesized using\nbeam search [27]. While this generation approach can yield\nstrong results, it requires optimizing additional hyperpa-\nrameters ( e.g. beam size) and, more importantly, it does\nnot have any theoretical guarantees for learning the actual\ndata distribution. The resulting distribution rather depends\non ﬁnding the right level of regularization at training time.\nOnly if the model is regularized appropriately beam search\nwill yield outcomes of sufﬁcient diversity. Since this gener-\nation process lacks theoretical guarantees for capturing the\nfull diversity of the layout distribution and heavily relies on\nheuristics, we directly approximate the distribution using a\nattention-based V AE instead.\nSome works have been proposed with particular focus on\nfurniture arrangement [36, 12]. In the method of Wanget al.\n[36], one CNN places elements in a room by estimating the\nlikelihood of each possible location, while a second CNN\ndetermines when the scene is complete. [30] extends this to\nmodel orientations and room dimensions. Moreover, Hen-\nderson et al. [12] propose to learn a distribution for each\nelement type and model high-order relationships between\nobjects using a direct acyclical graph. Since all of these\nmethods use the now unavailable SUNCG dataset [33] for\ntraining, establishing a comparison with them is difﬁcult.\nAdditionally, tab. 1 provides a high-level comparison\nbetween this work and the most relevant adjacent meth-\nods. We differentiate existing works along four important\ndimensions: 1) Are models equipped with inductive biases\ntowards learning the relationships between elements? 2)\nAre these relationships learned without supervision or are\nadditional labels, using e.g. heuristics, necessary? 3) Can\nlayouts contain an arbitrary number of elements? 4) Does\nthe learning approach provide guarantees for learning the\nunderlying distribution by applying probabilistic methods?\nAttention-based V AEsare a recent development in the\nNatural Language Processing (NLP) literature. The com-\nmon goal is to learn the distribution of real data more ac-\ncurately than with deterministic self-supervised approaches\n[25, 26, 37]. To combine Transformers and V AEs [26] uses\nself-attention layers for the encoder and decoder compo-\nnents. The encoder turns a sentence into a collection of\nhigh-dimensional vectors of the same length as the input.\nThese constitute the V AE bottleneck, and are passed after\nre-parameterization to the decoder to reconstruct the sen-\ntence. By feeding a set of vectors sampled from the prior, a\nsentence of the same length can be generated. Further, [25]\nimplements a conditional V AE (conditioned on the context\nof a conversation) based on the Transformer to improve\ndiversity on the task of response generation. [37] devel-\nops a Transformer-based V AE to enhance variability on the\ntask of story completion. Their encoder and decoder share\nweights while the bottleneck of their V AE is fed into the\npenultimate layer of the decoder.\n3. Variational Transformer Networks\nThis section illustrates the proposed Variational Trans-\nformer Networks. From a high-level perspective VTNs are\nan instance of the V AE framework tailored to the task of lay-\nout synthesis, where the main building blocks of the neural\nnetworks parameterizing the encoder and decoder are atten-\nInductive\nBias\nUnsupervised\nRelationship\nArbitrary\nSize\nDistribution\nLearning\nLayoutGAN [23] \u0013 \u0013 \u0017 \u0013\nLayoutV AE [16] \u0017 \u0013 Practically\ndifﬁcult\n\u0013\nREAD [28] \u0013 \u0017 \u0013 \u0013\nNDN [22] \u0013 \u0017 \u0017 \u0013\nGupta et al. [9] \u0013 \u0013 \u0013 \u0017\nOurs \u0013 \u0013 \u0013 \u0013\nTable 1: Comparison with existing methods. We consider\nwhether methods 1) equip their models with inductive bi-\nases towards learning the relationships between elements,\n2) learn relationships unsupervised, 3) allow layouts of arbi-\ntrary size and 4) have guarantees for learning the underlying\ndistribution by applying probabilistic methods.\ntion layers. Firstly, we brieﬂy revisit the concept of V AEs.\nSubsequently, we explain how VTNs exploit the data format\nof layouts, their architecture and how to train them.\n3.1. Variational Autoencoders\nV AEs are a family of latent variable models that approx-\nimate a data distribution P(X) by maximizing the evidence\nlower bound (ELBO) [21]\nL(θ,φ) = E\nz∼qθ(z|x)\n[log (pφ(x|z))] −KL(qθ(z|x) ||p(z))\n(1)\nwhere pφ(x|z) denotes a decoder parameterized by a neu-\nral network with parameter φ, qθ(z|x) is the approximate\nposterior distribution, similarly parameterized by a neural\nnetwork with weights θ, and p(z) the prior distribution.\n3.2. Exploiting the Data Format of Layouts\nThe central aspect of layout generation is its unique un-\nderlying data format. Layouts are sets of elements of vari-\nable size, where each element can be described by both\ndiscrete and continuous features. More formally, each lay-\nout x in a given dataset X consists of a variable number\nl of bounding boxes. Further, each bounding box xi with\ni ∈[1,...,l ] contains information about its class (for doc-\numents, e.g. text, image. . . ), location and dimension.\nAnother important characteristic of layout datasets is that\nthere exists a high degree of correlation between the individ-\nual elements in a layout. For example, in case of document\nlayouts, titles tend to be positioned at the top of a text. It\nis therefore essential to bias an approach for learning layout\ndistributions towards exploiting the relationships between\nelements. While some methods introduce additional fea-\ntures, such as annotations for the relationships between ele-\nments [22, 28], our approach instead relies solely on bound-\ning box annotations, since additional features are expensive\nto create, prone to ambiguity and fail to generalize across\ndatasets. Therefore, we introduce an inductive bias to learn\nfrom the relationships by using an attention-based neural\nnetwork. Notably, the attention mechanism is an ideal can-\ndidate for exploiting pairwise relationships, since it lever-\nages the pairwise dot product as its fundamental computa-\ntional block for guiding the information ﬂow.\nMoreover, the attention mechanism also helps modeling\nanother aspect of the data - namely a varying and large num-\nber of elements. To mitigate this problem other works have\nrestricted the maximum number of elements occurring in\none layout [23, 22]. However, attention-based architectures\nare well suited for learning the relationships of a large num-\nber of elements, since this is one of the reasons for their\nsuccess in the NLP literature [35]. Notably, RNNs, as used\nby LayoutV AE [16], are also capable of modeling a vary-\ning number of elements in a layout. However, they strug-\ngle with long-term dependencies, i.e. a large number of el-\nements in a layout. This follows from results in the NLP\nliterature and is also observed by us (see section 4.4).\n3.3. Architecture of VTNs\nThe architecture of VTNs is based on Transformers [35],\nwhich are sequence models that consist of an encoder-\ndecoder architecture, where both encoder and decoder use\nattention layers as their fundamental building blocks. We\nrefer to ﬁg. 2 for a schematic overview of our approach.\nThe encoder of the Transformer architecture parameter-\nizes the posterior distribution qθ(z|x) in the V AE frame-\nwork. In particular, qθ(z|x) is parameterized as a multi-\nvariate normal distribution with diagonal covariance matrix,\nwhose parameters are determined by the output of the en-\ncoder network. To train the encoder using backpropagation,\nwe apply the local re-parameterization trick [20]. The orig-\ninal Transformer is a highly specialized language model,\nwhich is usually trained on vast quantities of text data.\nTherefore, it is necessary to adjust the hyper-parameters. It\nis essential to keep the number of attention heads large (here\nnheads = 8) to average out outliers from individual attention\nheads [35]. Similarly, we keep a large model dimensional-\nity (dmodel = 512) and size of the point-wise feed-forward\nlayers (dff = 2048). However, we ﬁnd that the number\nof attention-blocks (see [35]) can be reduced to four with-\nout performance loss. This hints that relationships between\nelements in a layout are less complex than between words\nin language. We further omit the positional encodings used\nin the context of NLP since the features of bounding boxes\nalready contain positional information.\nThe decoder pφ(x|z) in VTNs is a mirrored version of\nthe encoder. Note that this breaks with [35] which adds ad-\nditional attention-layers whose keys and queries are the out-\nput of the encoder. We empirically ﬁnd that feeding the out-\nput of the encoder as an input to the ﬁrst layer of the decoder\nyields better results. We further experiment with another\nmajor architectural choice: the autoregressive decoder, i.e.\npφ(x|z) =∏l\ni=1 pφ(xi|xi−1,z), where ldenotes the num-\nber of bounding boxes in a layout, and a non-autoregressive\nvariant. While the former has more representational power,\nsince theoretically any distribution can be modeled as an\nautoregressive one, it is also more prone to posterior col-\nlapse due to the expressive decoder [2] and requires more\ncomputational resources.\nFurthermore, we consider two distinct prior distribu-\ntions. First, we use the common choice of a ﬁxed multi-\nvariate zero-mean normal distribution. However, this often\nproves too restrictive for learning the true posterior distri-\nbution [4]. In principle there are two avenues to mitigate\nthis issue: use a more expressive parameterization of the\nposterior [19] or the prior distribution [4]. In this work we\nattempt to extend the expressiveness of the prior distribu-\ntion by learning the parameters of the multivariate normal\ndistribution with a diagonal covariance matrix. Since lay-\nouts consist of a varying number of bounding boxes, we\nparameterize the distribution with an LSTM [14].\nImportantly, while an autoregressive decoder enables\nsampling of layouts with varying number of elements - e.g.\nby introducing symbols for start/end of the layout - the non-\nautoregressive decoder requires incorporating this into the\nprior distribution. Therefore, we model the prior in this\ncase as p(z,s) = p(z|s)p(s) where s denotes the number\nof bounding boxes. We learn p(s) during training by count-\ning the number of occurrences of each sequence length.\nFinally, we note that in the case of the autoregressive\ndecoder, we ﬁnd empirically that aggregating the latent rep-\nresentations z across all elements in a layout yields better\nperceptual quality. This corresponds to parameterizing the\nposterior distribution with the output of the encoder aggre-\ngated along the dimension of the layout elements. To this\nend we follow BERT [6] where the ﬁnal hidden state of the\nencoder for the ﬁrst token is used to represent the entire se-\nquence, and used as the ﬁrst element in the the decoder in-\nput. In the case of the non-autoregressive decoder we do not\naggregate the latent representations, but feed them directly\nwith variable dimensionality to the decoder.\n3.4. Optimizing VTNs\nSince we are learning the layout distribution using a\nV AE, we optimize the ELBO deﬁned in eq. (1). However,\na practical optimization challenge of V AEs is the so-called\nposterior collapse [2, 10]. The decoder ignores the informa-\ntion in the latent representation and collapses onto modes of\nthe data distribution. At the same time the posterior distri-\nbution parameterized by the encoder can perfectly match\nthe prior distribution, since it does not need to transmit in-\nformation to the decoder. Therefore, this work follows a\ncommon heuristic by optimizing the β-V AE objective in-\nFigure 2: VTN. The encoder and decoder are parameterized by attention-based neural networks. This biases the network\nto learn relationships between the layout elements and enables processing layouts of arbitrary size. During training (black\narrow) the reconstruction loss and the KL-divergence between the prior p(z) and the approximate posterior distribution are\nminimized. During inference (red arrow) we sample latent representations zfrom the prior and transform those into layouts\nusing our self-attention-based decoder.\nstead of eq. (1)\nL(θ,φ) = E\nz∼qθ(z|x)\n[log(pφ(x|z))] −βKL(qθ(z|x)||p(z))\n(2)\nTo optimize eq. 2 we use Adam [18] and follow the learning\nrate schedule in [35]. To further reduce the risk of posterior\ncollapse, it is common to increase β at the beginning of\ntraining from zero to the desired value. Speciﬁcally, we im-\nplement the exponential beta schedule proposed by [2, 26].\nIn all our experiments we useβ = 1with the autoregressive\ndecoder and β = 0.5 with the non-autoregressive decoder.\nMoreover, we follow [9] in discretizing the location,\nwidth and height of the bounding boxes. Thus each bound-\ning box is represented by a feature vector containing a one-\nhot encoding of the class concatenated with the one-hot en-\ncodings representing the above discretization. We use cate-\ngorical cross-entropy as a reconstruction loss.\nImplementation Details We implement our method us-\ning Tensorﬂow 2 [1] and a NVIDIA V100 GPU for acceler-\nation. We train using the Adam optimizer with a batch size\nof 64 for 30 epochs in the case of the autoregressive decoder\nand 50 epochs using the non-autoregressive version.\n4. Experiments\n4.1. Datasets\nWe evaluate our method on the following publicly avail-\nable datasets of layouts for documents, natural scenes, fur-\nniture arrangements and mobile phone UIs.\nPubLayNet [42] contains 330K samples of machine-\nannotated scientiﬁc documents crawled from the Internet.\nIt has the categories text, title, ﬁgure, list, table.\nRICO [5] is a dataset of user interface designs for mobile\napplications. It contains 91K entries with 27 element cate-\ngories (button, toolbar, list item. . . ). Due to memory con-\nstraints we omit layouts with more than 100 elements 1, in\ntotal removing 0.031% of the data.\nCOCO [24] Contains ∼100K images of natural scenes. We\nuse the Stuff variant, which contains 80 thing and 91 stuff\ncategories, removing small bounding boxes ( ≤2% image\narea), as well as instances that are tagged as “iscrowd”.\nSUN RGB-D[32] is a scene understanding dataset with\n10000 samples, including scenes from [31], [15] and [38].\nThe annotations comprise different household objects. We\ncompute the 2D bounding boxes of the semantic regions\nfrom a top-down perspective.\n4.2. Evaluation methodology\nIt is important to evaluate layouts along two high-level\ndimensions - perceptual quality and diversity. Note that in\nthe case of layouts perceptual quality is prone to subjectiv-\nity and different aspects must be considered from dataset to\ndataset. It is thus difﬁcult to deﬁne a single metric that en-\ntirely covers both aspects. We therefore resort to a set of\nmetrics where each aims at representing an individual as-\npect of either perceptual quality or diversity.\nAlignment and overlap. Some datasets, such as Pub-\nLayNet or RICO, consist of entries with strictly deﬁned\nalignments and small overlaps between bounding boxes.\nConsequently, these properties are an indicator of the per-\nceptual quality of synthesized layouts. We follow Layout-\nGAN [23] in measuring overlaps using the total overlapping\narea among any two bounding boxes inside the whole page\n(overlap index ) and the average Intersection over Union\n(IoU) between elements. Additionally, we quantify align-\nment using the alignment loss proposed by [22].\nUnique matches under DocSim metric.We use the\n1Note that this restriction originates from memory constraints and does\nnot imply that our approach is not capable of learning larger layouts given\nsufﬁcient memory.\nnumber of unique matches between real sets of layouts and\nsynthesized layouts as a proxy for diversity. We use the\nDocSim metric [28] as a similarity metric. Note that, while\nthe number of unique matches primarily analyzes diversity,\nit also partially reﬂects perceptual quality.\nWasserstein distance. A rigorous approach to evalu-\nate diversity would be computing the Wasserstein distance\nbetween the real and learned data distributions. Unfortu-\nnately this is infeasible. However, we can approximate the\nWasserstein distance between real and generated data for\ntwo marginal distributions - the class distribution (discrete)\nand the bounding box distribution (continuous, 4-d vectors\n(xcenter, ycenter, width, height)). In practice, we compute\nthese Wasserstein distances from a ﬁnite set of samples.\n4.3. Quantitative results\nComparison to state of the artA comparison to any of\nthe methods described in section 2 is difﬁcult, since, to the\nbest of our knowledge, none has a publicly available imple-\nmentation2. Similar to the authors of LayoutV AE [16], we\nwere unable to reproduce the results of LayoutGAN [23] on\ndocuments. We reimplement LayoutV AE and the approach\nof Gupta et al. [9]. In the LayoutV AE case, we follow [9]\nand sample category counts from the test dataset. For Gupta\net al., we use a mixture of nucleus sampling with p = 0.9\nand top-k sampling with k = 30. As suggested by the au-\nthors, we found nucleus sampling to improve the diversity\nof the synthesized layouts. We, further, compare against\nNDN [28] on RICO using their proposed alignment metric.\nIn tab. 3 we ablate our model on the prior type and the\ndecoding strategy. We observe that, while the autoregres-\nsive decoder slightly decreases diversity (Wasserstein dis-\ntance class/bounding box and number of unique matches),\nit yields large improvements regarding perceptual quality\n(IoU, overlap index and alignment). Moreover, in the case\nof the non-autoregressive decoder a learned prior yields im-\nprovements regarding perceptual quality. However, when\nusing an autoregressive decoder the learned and non-learned\npriors yield similar results. Therefore, we apply an autore-\ngressive decoder with a non-learned prior in the remaining\nexperiments, since it strikes the optimal balance between\ndiversity, perceptual quality and model simplicity.\nWe report quantitative results for the aforementioned\nmetrics on different datasets. Unless explicitly stated, all\nmetrics are computed on 1000 samples, and the value is av-\neraged across 5 trainings with different random initializa-\ntion. In tabs. 2 and 4 we show the results of our method\nin comparison to existing art. We show that our method\nproduces a large number of distinct layouts that have sim-\nilar alignment metrics as the real data. Furthermore, we\n2Though the LayoutGAN authors recently released an imple-\nmentation, they only did so for a toy example on MNIST:\nhttps://github.com/JiananLi2016/LayoutGAN-Tensorﬂow\nclearly outperform LayoutV AE [16] across all metrics and\ndemonstrate improved diversity at similar perceptual qual-\nity compared to [9], as expected since our method explicitly\napproximates the layout distribution. Given that both Lay-\noutV AE and Guptaet al. generate layouts autoregressively\nand considering our ablation in tab. 3, we note that autore-\ngressive modeling denotes an important element of learning\nlayout distributions.\nFurhtermore, in tab. 5 we also compare our approach\nto Lee et al. [22] on the RICO dataset using their proposed\nalignment metric. We demonstrate superior results when\nno explicit design constraints are given (NDN-none), show-\ning that our method is better at discovering relationships\nwithout supervision. Even in the NDN-all case, where all\nrelationships are given to the network, we show similar per-\nformance despite not relying on this information.\n4.4. Qualitative results\nWe show qualitative results for PubLayNet in ﬁg. 3, as\nwell as a qualitative comparison with existing methods in\nﬁg. 4. In alignment with the quantitative results in section\n4.3, we observe that our approach and [9] yield similar per-\nceptual quality. Furthermore, LayoutV AE [16] struggles to\nmodel layouts with a large number of elements. As previ-\nously discussed, this results from the application of RNNs,\nwhich are inferior at modeling the relationships between\na large number of elements compared with the attention\nmechanism. In ﬁg. 5 we show synthetic samples for RICO\nas well as the closest DocSim match in the real dataset. We\nshow similar results for SUN RGB-D in ﬁg. 7. In order to\nshow the capabilities of our method on the task of natural\nscene generation, we train our model on the COCO-Stuff\ndataset. In ﬁg. 6 we show samples from our network. For\nbetter understanding we feed our generations to a pretrained\ninstance of LostGAN [34] 3. These results show that our\nmethod is capable of capturing relationships between ele-\nments regardless of their distance or position in the input\nsequence. This is observed by the strict margins modeled\nby our network, which resemble those of the real data. In\nthe case of COCO or SUN RGB-D, we show how the net-\nwork identiﬁes joint occurrences of different elements (e.g.\ngiraffe and tree, person and playingﬁeld or table and chair).\n4.5. Layout detection\nThis experiment demonstrates the beneﬁt of our ap-\nproach regarding data augmentation for a downstream task.\nDocument understanding comprises multiple tasks that go\nbeyond simple Optical Character Recognition (OCR). Un-\nderstanding the arrangement of different pieces of text and\nimages and their boundaries (the document layout) is also\nnecessary for applications such as text extraction or to de-\ntermine the reading order in a complex document. While\n3https://github.com/iVMCL/LostGANs\nIoU Overlap Alignment W class ↓ W bbox ↓ # unique matches ↑\nLayoutV AE [16] 0.171 0.321 0.472 - 0.045 241\nGupta et al. [9] 0.039 0.006 0.361 0.018 0.012 546\nOurs (autoregressive) 0.031 0.017 0.347 0.022 0.012 697\nReal data 0.048 0.007 0.353 - - -\nTable 2: Quantitative evaluation on PubLayNet. We generate 1000 layouts with each method and compare them regarding\naverage IoU, overlap index [23], alignment [22], Wasserstein (W) distance of the classes and bounding boxes to the real data\nand the number of unique matches according to the DocSim. Ours (autoregressive) denotes using an autoregressive decoder.\nAutoregressive\ndecoder\nLearned\nprior\nIoU ↓ Overlap ↓ Alignment ↓ W class ↓ W bbox ↓ # unique\nmatches ↑\n\u0017 \u0017 0.259 ±0.114 0.178 ±0.122 0.364 ±0.080 0.011 ±0.007 0.018 ±0.012 813 ±51\n\u0017 \u0013 0.243 ±0.027 0.097 ±0.040 0.381 ±0.010 0.013 ±0.007 0.011 ±0.001 794 ±34\n\u0013 \u0017 0.031 ±0.004 0.017 ±0.006 0.347 ±0.005 0.022 ±0.002 0.012 ±0.001 697 ±13\n\u0013 \u0013 0.032 ±0.002 0.015 ±0.004 0.353 ±0.004 0.022 ±0.005 0.013 ±0.001 677 ±16\nTable 3: Quantitative ablation study on PubLayNet. We generate 1000 layouts and compare them in average IoU, overlap\nindex, alignment, Wasserstein (W) distance of the classes and bounding boxes to the real data and the number of unique\nmatches according to DocSim. We compare our model w/wo autoregressive decoder and with learned/non-learned prior.\nOCR-annotated data is quite abundant, this is not the case\nfor layout detection. Annotating documents is a tedious pro-\ncess which is prone to ambiguity, as the rules that deﬁne\ne.g. what a paragraph is are often subjective. This ambi-\nguity also makes automatic annotators based on heuristics\nfail or be constrained to speciﬁc domains [42]. Most works,\nsuch as PubLayNet [42], LayoutLM [40] or [41] are based\non object detection backbones using CNNs. Here, we use\nour method to create a training dataset for a layout detec-\ntor on the PubLayNet dataset. We use the bounding boxes\ngenerated by our method to guide the rendering. Obtain-\ning realistic text, images, tables or lists for a given domain\nis labor-intensive, therefore, we crop these from the origi-\nnal dataset guided by the ground truth annotations and use\nthe most appropriate one for a particular box according to\nits class and dimensionality. This approach ensures that the\naspect ratio is preserved. In ﬁg. 3 we show several exam-\nRICO\nIoU Overlap Alignment Wclass ↓Wbbox ↓# unique m.↑\n[16] 0.193 0.400 0.416 - 0.045 496\n[9] 0.086 0.145 0.366 0.004 0.023 604\nOurs0.115 0.165 0.373 0.007 0.018 680\nReal 0.084 0.175 0.410 - - -\nCOCO\n[16] 0.325 2.819 0.246 - 0.062 700\n[9] 0.194 1.709 0.334 0.001 0.016 601\nOurs0.197 2.384 0.330 0.0005 0.013 776\nReal 0.192 1.724 0.347 - - -\nTable 4: Extension of Tab. 2 for RICO, COCO\nMethod Alignment\nNDN-none [22] 0.91 ±0.030\nNDN-all [22] 0.32 ±0.020\nOurs 0.37 ±0.009\nReal data 0.0012\nTable 5: Comparison between Neural Design Network [22]\nand our approach using their proposed alignment metric on\nRICO.\nText\nTable\nText\nText\nTitle\nText\nText\nTable\nText\nTextText\nTextTitle\nText\nTextText\nTable\nText\nFigure\nText\nTitle\nText\nTitle\nText\nText\nText\nTextFigure Text\nText\nText\nTitle\nText\nTitle\nText\nFigure 3: Top: Generated layouts from our autoregressive\nmodel for PubLayNet. Bottom: Renderings of the layouts.\nThe supplementary material shows more samples.\nples of this approach. We sample 240000 layouts from our\nmodel to train a Faster R-CNN model [29] with a Resnet-\n50 backbone [11] and evaluate the performance on the test\nset of PubLayNet in tab. 6. We do not perform any postpro-\nLayoutV AE [16]\nTitle\nTitle\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nText\nTextText\nTable\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nTableText\nText\nFigure\nText\nText\nText\nTextGupta et al. [9]\nTitle\nText\nText\nText\nText\nText\nTitle TextText\nText Text\nText\nText\nText\nText Text\nTitleText\nText Text\nText\nText\nText\nText\nText\nTable\nText\nTable\nText\nTable\nText\nText Text\nText\nTitle\nTextText\nFigure\nText\nText Text\nText\nTextOurs\nText\nTitle\nText\nTextTextText\nText\nText\nText\nText\nTextText\nTextText\nText\nText\nTitle\nText\nTitle\nText\nTitle\nTextText\nText\nTextTitle\nText Title\nText\nText\nText\nTable\nTitle\nText\nFigure\nText\nTitle\nText\nTitle\nText\nText\nText\nFigure 4: Qualitative comparison between LayoutV AE,\nGupta et al. and our method on PubLayNet. The RNN of\nLayoutV AE struggles with a large number of elements.\nSynthetic\nToolbarList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Text\nText\nText\nImageText\nAdvertisement\nImage\nText\nText\nInput\nText Button\nTextInput\nText Button\nImage\nImage\nText\nText Button\nText\nText\nText\nText\nText\nTextReal layout\nToolbar\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nToolbar\nAdvertisement\nText\nText\nImage\nText\nText Button\nTextText\nInput\nToolbar\nText Button\nImage\nText\nImageText\nToolbar\nText\nText\nTextReal image\nFigure 5: Generated layouts for RICO and their associated\nDocSim match. The supplement shows more samples.\ncessing on the sampled layouts. For comparison, we run the\nsame experiment with renderings created from real bound-\ning box annotations (“Real layouts”), as well as with actual\ntraining images (“Real PubLayNet”). We compare the mean\naverage precision (mAP) at 0.5 IoU. Our synthesized lay-\nouts alone are capable of achieving a good accuracy score.\ntree\ngiraffe\nhorse\nsnow fence\nfog railing\ntree\nmountain\nbicycle river\ncell phone floor-marbleumbrella\npizza\npaper\ntextile-other\nsky-other metal\ntree\nplant-other\ngrasscar\ntree person\nperson\nplayingfieldperson\nmetal\nFigure 6: Generated layouts on COCO-Stuff (top) and im-\nages generated by LostGAN based on these layouts (bot-\ntom). The supplementary material shows more samples.\nSynthetic\nchair\npillow\npillowpillow\nsofa\npillow\npillow\nchair\nlamp\nchair\ndesk\nchair\nlamp\ndesk\ntable\nsofa\nsofa\ndoor\nthermos\nbook\nboxboxbox\nfile_cabinet\nbook\nboxbox\nbulletin_boardboxboxReal layout\npillowpillow\npillow\nsofa\ncoffee_table\ndesk\nchair\nchair\nchair\nsofa\ntable\nchair\ndesk\nbox\nbox\nbox\nbox\nbox\nchair\ndesk\nboxReal image\nFigure 7: Generated layouts for SUN RGB-D and their as-\nsociated DocSim matches with the corresponding image.\nOurs Real layouts Real PubLayNet\nmAP @ 0.5 IoU 0.769 0.883 0.9646\nTable 6: Detection accuracy scores for a layout detection\nmodel trained with synthetic and real data.\n5. Conclusion and future work\nThis work proposes self-attention layers as fundamental\nbuilding blocks of a V AE and develops a solution tailored\nto layout synthesis, evaluating it on a diverse set of public\ndatasets. Our approach yields state-of-art quantitative per-\nformance across all our metrics (see section 4.3) and layout\nsamples of appealing perceptual quality (see section 4.4).\nWe observe that autoregressive decoding constitutes an im-\nportant ingredient to obtain high quality layouts. We also\ndemonstrate its applicability as a data synthesizer for the\ndownstream task of layout detection (see section 4.5). How-\never, we also note that our proposal can still be improved in\npromising future research directions. Namely, learning to\ngenerate additional properties ( e.g. font or text size) or the\ndimensions of the layout, which could be useful for docu-\nments with varying size, (e.g., leaﬂets). Moreover, it could\nbe interesting to incorporate an end-to-end approach for lay-\nout synthesis, such as ours, into a scene synthesis pipeline.\nReferences\n[1] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy\nDavis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian\nGoodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\nYangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dandelion Man ´e, Rajat Monga,\nSherry Moore, Derek Murray, Chris Olah, Mike Schuster,\nJonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-\nnanda Vi´egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-\nFlow: Large-scale machine learning on heterogeneous sys-\ntems, 2015. Software available from tensorﬂow.org. 5\n[2] Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M.\nDai, Rafal J ´ozefowicz, and Samy Bengio. Generating sen-\ntences from a continuous space. In Yoav Goldberg and Ste-\nfan Riezler, editors,Proceedings of the 20th SIGNLL Confer-\nence on Computational Natural Language Learning, CoNLL\n2016, Berlin, Germany, August 11-12, 2016 , pages 10–21.\nACL, 2016. 1, 4, 5\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm, edi-\ntors, Computer Vision - ECCV 2020 - 16th European Confer-\nence, Glasgow, UK, August 23-28, 2020, Proceedings, Part I,\nvolume 12346 of Lecture Notes in Computer Science, pages\n213–229. Springer, 2020. 1\n[4] Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Pra-\nfulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter\nAbbeel. Variational lossy autoencoder. International Con-\nference on Learning Representations, 2017. 4\n[5] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hib-\nschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ran-\njitha Kumar. Rico: A mobile app dataset for building data-\ndriven design applications. In Proceedings of the 30th An-\nnual Symposium on User Interface Software and Technology,\nUIST ’17, 2017. 5\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In Jill Burstein, Christy\nDoran, and Thamar Solorio, editors,Proceedings of the 2019\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June\n2-7, 2019, Volume 1 (Long and Short Papers) , pages 4171–\n4186. Association for Computational Linguistics, 2019. 4\n[7] Christoph Goller and Andreas K ¨uchler. Learning task-\ndependent distributed representations by backpropagation\nthrough structure. In Proceedings of International Confer-\nence on Neural Networks (ICNN’96), Washington, DC, USA,\nJune 3-6, 1996, pages 347–352. IEEE, 1996. 2\n[8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative Adversarial Networks. Confer-\nence on Neural Information Processing Systems, June 2014.\n1\n[9] Kamal Gupta, Alessandro Achille, Justin Lazarow, Larry\nDavis, Vijay Mahadevan, and Abhinav Shrivastava. Lay-\nout Generation and Completion with Self-attention. arXiv\ne-prints, page arXiv:2006.14615, June 2020. 2, 3, 5, 6, 7, 8\n[10] Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor\nBerg-Kirkpatrick. Lagging inference networks and posterior\ncollapse in variational autoencoders. International Confer-\nence on Learning Representations, 2019. 4\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages\n770–778. IEEE Computer Society, 2016. 7\n[12] Paul Henderson, Kartic Subr, and Vittorio Ferrari. Automatic\nGeneration of Constrained Furniture Layouts.arXiv e-prints,\npage arXiv:1711.10939, Nov. 2017. 3\n[13] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9:1735–80, 12 1997. 2\n[14] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 4\n[15] Allison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T.\nBarron, Mario Fritz, Kate Saenko, and Trevor Darrell. A\ncategory-level 3-d object dataset: Putting the kinect to work.\nIn IEEE International Conference on Computer Vision Work-\nshops, ICCV 2011 Workshops, Barcelona, Spain, Novem-\nber 6-13, 2011, pages 1168–1174. IEEE Computer Society,\n2011. 5\n[16] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Si-\ngal, and Greg Mori. Layoutvae: Stochastic scene layout gen-\neration from a label set. In 2019 IEEE/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Korea\n(South), October 27 - November 2, 2019 , pages 9894–9903.\nIEEE, 2019. 1, 2, 3, 4, 6, 7, 8\n[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 ,\npages 4401–4410. Computer Vision Foundation / IEEE,\n2019. 1\n[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In Yoshua Bengio and Yann LeCun,\neditors, 3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings, 2015. 5\n[19] Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen,\nIlya Sutskever, and Max Welling. Improved variational infer-\nence with inverse autoregressive ﬂow. InAdvances in neural\ninformation processing systems, pages 4743–4751, 2016. 4\n[20] Durk P Kingma, Tim Salimans, and Max Welling. Vari-\national dropout and the local reparameterization trick. In\nAdvances in neural information processing systems , pages\n2575–2583, 2015. 4\n[21] Diederik P. Kingma and Max Welling. Auto-encoding vari-\national bayes. In Yoshua Bengio and Yann LeCun, editors,\n2nd International Conference on Learning Representations,\nICLR 2014, Banff, AB, Canada, April 14-16, 2014, Confer-\nence Track Proceedings, 2014. 1, 3\n[22] Hsin-Ying Lee, Weilong Yang, Lu Jiang, Madison Le, Ir-\nfan Essa, Haifeng Gong, and Ming-Hsuan Yang. Neural\ndesign network: Graphic layout generation with constraints.\nIn Proceedings of European Conference on Computer Vision\n(ECCV), August 2020. 1, 2, 3, 4, 5, 6, 7\n[23] Jianan Li, Jimei Yang, Aaron Hertzmann, Jianming Zhang,\nand Tingfa Xu. Layoutgan: Generating graphic layouts with\nwireframe discriminators. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net, 2019. 1, 2, 3, 4, 5, 6,\n7\n[24] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in\ncontext. In David J. Fleet, Tom ´as Pajdla, Bernt Schiele, and\nTinne Tuytelaars, editors, Computer Vision - ECCV 2014 -\n13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part V , volume 8693 of Lecture\nNotes in Computer Science, pages 740–755. Springer, 2014.\n5\n[25] Zhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan Liu, and\nPascale Fung. Variational transformers for diverse response\ngeneration. CoRR, abs/2003.12738, 2020. 3\n[26] Danyang Liu and Gongshen Liu. A transformer-based vari-\national autoencoder for sentence generation. In Interna-\ntional Joint Conference on Neural Networks, IJCNN 2019\nBudapest, Hungary, July 14-19, 2019 , pages 1–7. IEEE,\n2019. 3, 5\n[27] Mark F. Medress, Franklin S. Cooper, James W. Forgie, C. C.\nGreen, Dennis H. Klatt, Michael H. O’Malley, Edward P.\nNeuburg, Allen Newell, Raj Reddy, H. Barry Ritea, J. E.\nShoup-Hummel, Donald E. Walker, and William A. Woods.\nSpeech understanding systems. Artif. Intell., 9(3):307–316,\n1977. 2\n[28] Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, and Hadar\nAverbuch-Elor. READ: recursive autoencoders for docu-\nment layout generation. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR Workshops\n2020, Seattle, WA, USA, June 14-19, 2020 , pages 2316–\n2325. IEEE, 2020. 1, 2, 3, 6\n[29] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian\nSun. Faster R-CNN: towards real-time object detection\nwith region proposal networks. In Corinna Cortes, Neil D.\nLawrence, Daniel D. Lee, Masashi Sugiyama, and Roman\nGarnett, editors, Advances in Neural Information Process-\ning Systems 28: Annual Conference on Neural Information\nProcessing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, pages 91–99, 2015. 7\n[30] Daniel Ritchie, Kai Wang, and Yu-An Lin. Fast and ﬂexi-\nble indoor scene synthesis via deep convolutional generative\nmodels. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2019, Long Beach, CA, USA, June\n16-20, 2019 , pages 6182–6190. Computer Vision Founda-\ntion / IEEE, 2019. 3\n[31] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nRGBD images. In Andrew W. Fitzgibbon, Svetlana Lazeb-\nnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, edi-\ntors, Computer Vision - ECCV 2012 - 12th European Con-\nference on Computer Vision, Florence, Italy, October 7-13,\n2012, Proceedings, Part V, volume 7576 of Lecture Notes in\nComputer Science, pages 746–760. Springer, 2012. 5\n[32] Shuran Song, Samuel P. Lichtenberg, and Jianxiong Xiao.\nSUN RGB-D: A RGB-D scene understanding benchmark\nsuite. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2015, Boston, MA, USA, June 7-12,\n2015, pages 567–576. IEEE Computer Society, 2015. 5\n[33] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang,\nManolis Savva, and Thomas A. Funkhouser. Semantic scene\ncompletion from a single depth image. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017, pages 190–198.\nIEEE Computer Society, 2017. 3\n[34] Wei Sun and Tianfu Wu. Image synthesis from reconﬁg-\nurable layout and style. In 2019 IEEE/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Ko-\nrea (South), October 27 - November 2, 2019 , pages 10530–\n10539. IEEE, 2019. 6\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V . N. Vishwanathan, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA , pages\n5998–6008, 2017. 1, 2, 4, 5\n[36] Kai Wang, Manolis Savva, Angel X Chang, and Daniel\nRitchie. Deep convolutional priors for indoor scene synthe-\nsis. ACM Transactions on Graphics (TOG), 37(4):70, 2018.\n1, 3\n[37] Tianming Wang and Xiaojun Wan. T-CV AE: transformer-\nbased conditioned variational autoencoder for story comple-\ntion. In Sarit Kraus, editor,Proceedings of the Twenty-Eighth\nInternational Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2019, Macao, China, August 10-16, 2019, pages 5233–\n5239. ijcai.org, 2019. 3\n[38] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.\nSUN3D: A database of big spaces reconstructed using sfm\nand object labels. InIEEE International Conference on Com-\nputer Vision, ICCV 2013, Sydney, Australia, December 1-8,\n2013, pages 1625–1632. IEEE Computer Society, 2013. 5\n[39] Ying Cao Xinru Zheng, Xiaotian Qiao and Rynson W.H.\nLau. Content-aware generative modeling of graphic de-\nsign layouts. ACM Transactions on Graphics (Proc. of SIG-\nGRAPH 2019), 38, 2019. 2\n[40] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,\nand Ming Zhou. Layoutlm: Pre-training of text and layout\nfor document image understanding. In Rajesh Gupta, Yan\nLiu, Jiliang Tang, and B. Aditya Prakash, editors, KDD ’20:\nThe 26th ACM SIGKDD Conference on Knowledge Discov-\nery and Data Mining, Virtual Event, CA, USA, August 23-27,\n2020, pages 1192–1200. ACM, 2020. 7\n[41] Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel\nKifer, and C. Lee Giles. Learning to extract semantic struc-\nture from documents using multimodal fully convolutional\nneural networks. In 2017 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017 , pages 4342–4351. IEEE Computer\nSociety, 2017. 7\n[42] Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Pub-\nlaynet: largest dataset ever for document layout analysis. In\n2019 International Conference on Document Analysis and\nRecognition (ICDAR), pages 1015–1022. IEEE, Sep. 2019.\n5, 7\n[43] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In Computer Vision (ICCV),\n2017 IEEE International Conference on, 2017. 1\nSupplementary: Variational Transformer Networks for Layout Generation\nDiego Martin Arroyo1\nmartinarroyo@google.com\n1Google, Inc\nJanis Postels2\njpostels@vision.ee.ethz.ch\n2ETH Z¨urich\nFederico Tombari1,3\ntombari@google.com\n3Technische Universit¨at M¨unchen\n1. Attention analysis\nThe main claim for the effectiveness of our method is its inductive bias towards the relationships between elements. The\nself-attention layers in our network weigh the relevance of each component regardless of their distance in the input sequence.\nIn this section we analyze the validity of this claim by observing the attention maps on each self-attention layer.\n1.1. Encoder\nThe encoder processes the entire document in a single pass. In the case of PubLayNet, where nheads = 4, we observe\nthat in the ﬁrst layer elements are independent of each other, since no element receives any attention. In subsequent layers,\nelements start to consider others in the computation. In ﬁg. 1 we show a visualization of this process.\nFinal encoding result\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nAttention\nText 0 Text 1 List 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 Text 10 Text 11 Text 12 Text 13 Text 14\nLayer 1\nLayer 2\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14Layer 3\nText 0\nText 1\nText 3\nText 4\nText 5\nText 6\nText 0\nText 1\nText 3\nText 4\nText 5\nText 6\nText 1\nTable 2\nText 4\nText 5\nText 6\nText 7\nText 8\nText 1\nText 3\nText 4\nText 5\nText 6\nText 1\nText 3\nText 4\nText 5\nText 6\nText 7\nText 1\nText 3\nText 4\nText 5\nText 6\nText 7\nText 1\nText 3\nText 5\nText 6\nText 7\nText 8\nText 1\nTable 2\nText 5\nText 6\nText 7\nText 8\nText 9\nText 6\nText 7\nText 8\nText 9 Text 10\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 7\nText 8\nText 10Text 11 Text 12Text 13Text 14\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14Layer 4\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nText 0\nText 1\nTable 2 Text 3\nText 4\nText 5\nText 6\nText 7\nText 8\nText 9 Text 10Text 11 Text 12Text 13Text 14\nFigure 1: Attention visualization. A higher color intensity reﬂects a higher attention weight. In the ﬁrst layer, no attention is\npaid to any element. In subsequent layers, other elements are considered regardless of their distance in the sequence. This is\nparticularly useful to model two-column documents: for example, in the attention map for Text 8, Table 2 has a signiﬁcant\nweight, despite their distance in the input sequence. The network correctly identiﬁes it as a relevant element to consider.\narXiv:2104.02416v1  [cs.CV]  6 Apr 2021\n1.2. Decoder\nDuring the autoregressive decoding, the network not only relies on the encoded document vector z to determine the\nlocation and size of the next element, but also on the result of the previous iterations via self-attention. In ﬁg. 2 this process\nis shown.\nFinal decoding result\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 11Text 12\nText 13Text 14\nAttention\nTitle 1 Text 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 List 10 Text 11 Text 12 Text 13 Text 14\nLayer 1\nTitle 0 Text 1\nText 1Text 2\nText 1Text 2\nText 3\nText 1Text 2\nText 3Text 4\nText 1\nText 3Text 4\nText 5\nText 1\nText 3\nText 5 Text 6\nText 1\nText 5 Text 6\nText 7\nText 1\nText 5 Text 6\nText 7Text 8\nText 1\nText 5 Text 6\nText 8\nText 9\nText 5\nText 7\nText 9List 10\nText 5\nText 7\nText 9List 10\nText 11\nText 5\nText 9\nText 11Text 12\nText 5\nText 9\nText 11Text 12\nText 13\nText 5\nText 9List 10\nText 11\nText 13Text 14Layer 2\nTitle 0 Text 1\nTitle 0 Text 1Text 2\nTitle 0Text 2\nText 3\nTitle 0\nText 3Text 4\nTitle 0\nText 3Text 4\nText 5\nTitle 0 Text 1\nText 4\nText 5 Text 6\nText 1\nText 3Text 4\nText 6\nText 7\nTitle 0\nText 3\nText 5 Text 6\nText 7Text 8\nTitle 0 Text 1\nText 3Text 4\nText 7Text 8\nText 9\nText 1\nText 5\nText 8\nText 9List 10\nText 1\nText 5\nText 7Text 8\nText 9List 10\nText 11\nText 1\nText 7Text 8\nText 9List 10\nText 11Text 12\nTitle 0 Text 1\nText 8\nList 10\nText 11Text 12\nText 13\nTitle 0 Text 1\nText 8\nList 10\nText 11Text 12\nText 13Text 14Layer 3\nTitle 0 Text 1\nTitle 0 Text 1Text 2\nTitle 0 Text 1Text 2\nText 3\nTitle 0 Text 1Text 2\nText 3Text 4\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 3Text 4\nText 5 Text 6\nText 7\nTitle 0Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 1\nText 3\nText 5 Text 6\nText 7Text 8\nText 9\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 6\nText 8\nText 9List 10\nText 11\nText 8\nText 9List 10\nText 11Text 12\nText 8\nText 9List 10\nText 11Text 12\nText 13\nText 9\nText 11Text 12\nText 13Text 14Layer 4\nTitle 0 Text 1\nTitle 0 Text 1Text 2\nTitle 0 Text 1Text 2\nText 3\nTitle 0 Text 1Text 2\nText 3Text 4\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 11\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 11Text 12\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 11Text 12\nText 13\nTitle 0 Text 1Text 2\nText 3Text 4\nText 5 Text 6\nText 7Text 8\nText 9List 10\nText 11Text 12\nText 13Text 14\nFigure 2: Visualization of the relevance of existing elements during the autoregressive decoding.\n2. Latent Space Analysis\nThe properties of its latent space are an important aspect of any V AE. In this section, we show several experiments to\nanalyze the results of interpolating in latent space as well as the effect of each individual latent vector in the non-autoregressive\ndecoder setting.\n2.1. Latent Space Interpolations\nIn ﬁg. 3 we show linear interpolations between two random vectorsz1,z2 ∼N(0,1) and the intermediate results between\nthem on PubLayNet. While the space is not perfectly smooth (some elements only appear in the intermediate samples), the\nresults are not completely arbitrary, and each intermediate value z′= z1 + λ·(z2 −z1),λ ∈(0,1) produces a valid output.\nz1 z2\nFigure 1Text 2\nText 3 Text 4\nText 6\nText 7\nFigure 1\nText 3 Text 4\nText 5Text 6\nText 7\nText 8\nFigure 0 Figure 1\nText 3Text 4\nText 6 Text 7\nText 8\nText 0 List 1\nText 2\nText 3\nText 5\nText 7\nList 8\nText 9\nText 0 List 1\nText 2Text 3\nText 5 Title 6\nText 8\nText 9\nText 0\nFigure 1\nText 3 Text 4\nText 5\nText 7 Figure 8\nText 9\nFigure 0\nText 1\nText 2Text 4\nText 6Figure 7\nText 8Text 9\nFigure 0\nText 1\nText 2Text 4\nText 5Title 6\nText 7 Text 9\nTitle 0\nText 1\nText 4 Text 5Text 6 Text 7\nText 8Text 9\nTitle 0Text 1\nText 2\nText 4\nText 6 Text 7\nText 8 Text 10\nTitle 0\nText 1\nText 2\nText 5Text 6 Text 7\nText 8 Text 10\nTitle 0\nText 1 Text 2\nText 3\nText 5\nText 6\nText 8Text 9\nText 11\nText 12 Text 14\nText 16\nTitle 0\nFigure 1Text 2\nText 3\nText 4Text 6\nText 8 Text 9Title 10Title 11Title 12\nText 14Text 15Text 16 Text 17\nText 1 Text 2\nText 3Text 4\nText 6\nText 8\nText 9 Text 10\nText 13\nText 1 Text 2\nText 3Text 4\nText 6\nText 8\nText 9 Text 11Text 13\nText 2\nText 3 Text 4\nText 6\nTitle 8\nText 9Text 10\nText 12\nText 0\nTable 1\nTitle 2Title 3\nText 5\nText 6Text 8\nText 10\nText 0\nTable 1\nText 3\nText 4Table 5\nText 8 Text 9\nText 0\nTable 1\nText 3\nTable 4\nText 6 Text 7\nText 0\nTable 1\nText 2\nTable 4\nText 7\nText 8\nText 1\nTable 2\nText 3\nText 1\nTable 2\nText 3\nText 5\nText 6\nText 8 Text 9\nTitle 0\nText 2\nText 3\nText 4 Text 5\nText 6\nText 7\nTitle 0\nText 2\nText 3\nText 4 Text 5\nText 6Text 7\nTitle 0\nText 1\nText 2\nText 3Text 4\nTitle 0\nText 1\nText 2 Text 3\nText 4\nText 5\nText 6\nText 7\nTitle 0\nText 1\nText 2\nText 3\nText 4Text 5\nText 6\nText 7\nText 0\nText 1Text 2 Text 3\nText 4\nList 5Text 6\nText 0\nText 1Text 2\nText 3\nList 4\nText 5Text 6\nText 0 Text 1\nText 2 Text 3\nFigure 4\nText 5Text 6\nText 7\nText 0 Text 1Text 2 Text 3\nFigure 4\nText 5Text 6\nText 7\nText 0 Text 1Text 2 Text 3\nTitle 4\nText 5Title 6\nText 0 Text 1Text 2 Text 3\nTitle 4\nText 5Title 6\nTitle 0\nText 1 Text 3\nText 5Text 6\nText 7\nTable 8\nTable 10\nText 11\nTable 12\nTitle 0\nText 1\nText 3Text 4\nList 6\nText 8\nText 9Text 10Text 11\nTable 12\nTitle 0\nText 1 Text 3\nText 5\nList 7\nText 9Text 10\nText 11\nTable 12\nTitle 0\nText 1 Text 3\nText 5\nList 7Text 9Text 10\nText 11\nTable 12\nText 0\nText 1\nTitle 2\nText 3\nText 5\nText 8 Text 9\nText 10\nTable 12\nText 0\nText 1\nText 3Text 5\nTitle 6\nList 7\nText 9Text 10\nText 12\nText 0\nText 1 Title 2\nText 3Text 5\nTitle 6\nList 7\nText 9Title 10\nText 12\nText 0\nText 1 Text 2\nText 4\nTitle 5\nText 6\nText 7\nTable 8\nText 0\nText 1\nTitle 3\nText 4\nTitle 5\nText 7\nText 2\nText 3\nText 5\nText 7Text 9\nText 0\nText 2\nText 3\nText 6\nFigure 1 Text 2\nText 3Text 4\nText 6\nText 7\nText 9 Text 10\nFigure 11\nText 12\nText 1 Text 2\nText 3Text 4\nText 6 Text 7\nText 9 Text 10\nFigure 11\nText 12\nText 1 Text 2\nText 3\nText 4\nText 5Text 7 Title 8\nText 9Text 11\nText 12Text 13Text 14\nTable 15 Text 16\nText 1 Text 2\nText 3Text 4\nText 5Text 7\nText 9 Text 10\nText 11\nText 12\nText 13\nText 14\nText 1 Text 2Text 3 Text 4\nText 5 Text 6\nText 9 Text 10\nText 11 Text 12Text 13Text 14\nTitle 0Title 1 Title 2\nText 3 Text 4\nText 6Text 7\nText 8Text 10 Title 11\nText 12\nText 13\nText 14 Text 15\nText 16\nText 17 Text 18\nTitle 0\nText 2Text 3\nText 4 Text 6Text 7\nText 9\nText 10\nText 11\nText 12\nText 13Text 14\nText 15 Text 16\nTitle 0Title 1\nText 2 Text 4\nText 5Text 6\nText 8\nText 9\nText 10 Text 11\nText 13\nFigure 14Text 15\nText 16Text 17\nTitle 0Text 1\nText 2\nTitle 3\nText 4\nText 6\nText 7 Text 8\nText 9Text 11 Text 12Text 13 Text 14\nText 15 Text 16Text 17 Text 18\nText 0Text 1\nTable 2\nText 3\nText 5\nText 7Text 8 Text 9Text 11Text 12 Text 13Text 14\nText 15Text 16 Text 17\nText 0Text 1\nTable 2 Text 3\nText 5\nText 6\nText 7Text 8\nText 9\nText 11Text 12\nText 13Text 14\nText 15 Text 16\nText 17 Text 18\nFigure 0\nText 1\nText 2Figure 3\nText 4 Text 5\nTitle 7 Text 8Text 9\nFigure 0\nText 1\nText 2\nText 3\nTable 4\nText 5Text 7 Title 8Title 9 Text 10Text 11\nText 0\nTable 1\nText 2\nText 3 List 4\nText 5 Text 6\nText 7List 9\nText 0\nTable 1\nText 2\nList 3Text 4\nText 5\nText 7List 8 Text 9List 10\nText 0\nTable 1\nText 2\nList 3\nText 4\nText 5\nText 7\nText 8\nText 9List 10\nText 0\nText 1Table 2\nText 3\nText 4\nList 5 Text 6\nText 8 Title 9\nText 10Text 11\nTitle 0\nText 1Text 2\nList 3\nText 4\nTitle 5 Text 6\nText 7\nText 9Text 11\nText 0\nText 1Table 2\nText 3Text 4\nText 5\nTable 6Text 8\nList 9\nTable 10 Text 12\nText 0\nText 1Table 2\nText 3Text 4\nText 5\nTitle 6\nList 7\nText 9\nText 10Title 11Title 12Text 14 Text 15\nText 0\nText 1Table 2\nText 3Text 4\nText 5\nTitle 6\nText 7\nText 9\nText 10\nTitle 12Text 13 Text 14\nText 0\nText 1Table 2\nText 3Text 4\nText 5\nText 7\nText 9\nText 10\nTable 11 Title 13Text 14Text 15\nText 0 Text 1\nText 2 Text 4\nFigure 5\nText 6\nText 8\nTable 9\nText 0Text 1\nText 2Text 4\nText 5\nList 6\nText 7Text 8\nTable 9\nText 0 Text 1Title 2\nText 3 Text 5\nText 6 Text 7\nText 8\nText 9\nTable 10\nText 11\nText 0 Text 1\nText 2\nText 3Text 4\nText 6Text 7 Title 8\nText 9\nTitle 10\nText 11\nText 0\nText 1Text 3 List 4\nText 5Text 6\nTitle 7Text 8\nFigure 9\nTitle 0\nText 1\nText 2\nText 4\nText 5\nTitle 6\nText 7Text 8\nTable 9Text 11\nText 0\nFigure 1Title 2\nText 3\nText 5 Title 6\nText 7Title 8\nText 9 Text 11Figure 12\nText 13Text 14\nText 1Text 2\nText 3Figure 4\nTitle 5Text 6\nText 7Text 8 Title 9\nText 10\nTable 11Text 12\nText 1Text 2\nText 3List 4\nTitle 5Text 6\nText 7\nText 8\nText 10\nText 11\nText 1Text 2\nText 3List 4\nText 5 Text 6\nText 7\nTitle 8Text 9\nText 10\nText 12\nText 1Text 2\nText 3List 4\nText 5 Text 6\nText 7Title 8Text 9\nText 10\nText 11\nText 12\nFigure 3: Linear interpolations between two vectors in latent space, and evenly-sampled interpolated values.\n2.2. Adding/Removing Latent Vectors using Non-Autoregressive Decoder\nWhen training, the non-autoregressive decoder layouts x∈Rl×d1 are encoded as latent representations z ∈Rl×d2 1. This\nexperiment aims to investigate the consistency of these latent representations by removing elements from the latent code\nelement by element - i.e. z ∈Rl×d2 →z′ ∈Rl×d2−1. By consistency we mean that removing one latent vector does not\ndrastically change the decoded layout. Qualitative results can be found in ﬁg. 4. We observe that the latent code in fact\nappears largely consistent. Each new latent code appears to only introduce one new element in the layout while minimally\nchanging the relative arrangement. This also implies that our model could be applied to the task of layout completion.\nFigure 4: Investigation of stability of the latent space. From left to right, we add elements to the latent vector. We qualitatively\nobserve that this results in reasonable extensions of the layout.\n1Technically the encoder parameterizes a distribution over z. However, for the sake of simplicity we shall consider only the mean in this experiment\n3. Qualitative results\nThe small amount of samples presented in the main paper is not capable of conveying the diversity and quality of the\nsynthesized data. In order to provide a clearer understanding, in the following we show a larger amount of samples for\neach dataset. These results are rendered using the network output coordinates without any postprocessing or cherry-picking\napplied to them to hide imperfections or failure cases. The ﬁrst rows show samples from other methods for comparison.\n3.1. PubLayNet\nLayoutV AE [16]\nTableText\nText\nText Text\nTable\nTitle\nTitle\nTitle\nTitle\nTitle\nTitle\nTitle\nTitle\nText\nText\nText\nText\nText\nText\nTextText\nText\nTextText\nFigure\nFigure\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nList\nList\nTable\nTable\nTitle\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nFigure\nFigure\nText\nText\nText\nText\nTable\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nText\nTitle\nTitle\nTitle\nText\nText\nText\nText\nText\nTextText\nText\nText\nTextTextGupta et al. [9]\nTextText\nText\nText\nText\nText\nText\nText\nTable\nText\nText Text\nText\nText TitleText\nText\nText\nFigure\nText\nText\nText TextText\nTextTitle\nText\nFigure\nText\nFigure\nText\nTextText\nText Text\nTextText\nText Title\nText\nText\nText\nText TextTitle\nText\nText\nTextText\nFigure\nText\nTextText\nTitleText Title\nText\nFigureTitle\nText\nText\nTextOurs\nText\nTable\nText\nText\nTextTitle\nText\nText\nFigure\nText\nText List\nText\nTitle\nList\nText\nList\nFigure\nText\nText List\nText\nTable\nText\nText\nText\nText\nTitleTitle\nTextText\nText\nTitle\nText\nText\nText\nTable\nText\nText\nText\nText\nText\nFigure\nText\nText\nTable\nText\nText\nListText\nTitleText\nFigure\nText\nText\nTable\nText\nText Text\nTable\nTitle\nText Text\nText\nTitle\nText Text\nText Text\nText\nTitle\nText Text\nText\nTitle\nText\nText\nTextText\nText TextTextText\nTable\nText\nText\nTextTable\nText Text\nTextTitle\nTitle\nText\nTitle\nText\nText Text\nText\nText\nTitle\nText\nTextTitle\nText\nTitle\nText\nTitle\nText\nTitle\nText\nText\nText\nText\nFigure\nText\nText\nTitle\nText\nTextText\nText\nTitle\nText\nTitle\nText\nText\nText\nText\nText\nText\nText\nText\nText Text\nTable\nTitle\nText\nText\nText\nTable\nText\nText\nFigure 5: Additional synthesized layouts on PubLayNet using an autoregressive decoder and the result of feeding the layout\nto a document renderer.\nText ListTextTitle\nTitle\nText Text\nTitle\nText\nTitle\nTextTitle\nText\nTitle\nList\nTitle\nText\nText\nTable\nTitle\nText Text\nText\nTextTitle\nText\nText\nTitle\nText\nText\nTable\nText\nText\nTable\nText Text\nText Text\nTitle\nTextText\nText\nText Text\nText\nTitle\nTitle\nText Text\nList\nTextText\nTitle\nText\nList\nText\nText\nTitleText\nText\nTextText\nTextText\nText\nText\nTitle\nText\nText\nTitle\nText TextText\nText\nText\nText\nTable\nText\nText\nTitle\nText Text\nTitle\nText\nText\nList\nText\nText\nText\nFigure\nText\nText\nText\nText\nTextText\nText\nTable\nText Text\nText Title\nTable Text\nText\nText\nTextText\nText\nText\nText List\nText\nTitle\nText\nList\nTitle\nText\nText\nText\nText\nTable\nText\nTitle\nText Text\nTitle\nText\nTitle\nText Text\nFigure\nText\nTitle\nText Text\nFigure\nText\nList\nList\nText\nTitle\nText\nTitle\nText\nText\nTable\nFigure\nText\nText Text\nText\nText\nText\nFigure\nText\nText Figure\nText\nText\nText\nText\nTable\nFigure\nTable\nText\nTitle\nTextText\nText\nTitle\nText\nTitle\nText\nText\nText\nText\nText\nTable\nText\nText\nText\nText\nTitle\nText\nText\nTitle\nText\nText\nTextTable\nText\nTitleText\nTable\nText\nText\nTable\nText\nFigure\nText\nText\nText\nTextTitle\nTextText\nTitle\nText\nText Text\nText\nTable\nText Text\nTextTitle\nText\nTitle\nTextText Title\nText\nText\nTitleTitle\nText\nTitle\nText\nTextText\nTitleList\nTitle\nTitle\nText\nText\nTitle\nText\nTextText\nText\nTable\nText\nTitle\nText\nText\nText\nText\nFigure\nText\nFigure\nText\nList\nText\nText\nText\nFigure\nText\nText\nText\nText\nText\nTable\nText\nText Text\nTitleTitle\nTextText\nTitle\nTextList\nText\nText\nTable\nText\nList\nFigure 5: (Cont.) Synthesized layouts on PubLayNet using an autoregressive decoder.\nFigure 5: Additional synthesized layouts on PubLayNet using a non-autoregressive decoder.\n3.2. RICO\nLayoutV AE [16]\nList Item\nList Item\nList Item\nList ItemList Item\nList Item\nList Item\nToolbarToolbar\nText\nText\nText\nTextText\nImage\nImage\nImage\nImage\nToolbar\nText\nTextText\nText\nImage\nImage\nImage\nToolbar\nToolbar\nWeb View\nList Item\nList Item\nList ItemList Item\nList Item\nList Item\nToolbar\nList ItemList Item\nList Item\nList ItemList ItemList Item\nList Item\nList Item\nList ItemList Item\nList ItemList ItemImage\nToolbar\nInput\nInput\nTextDrawerGupta et al. [9]\nToolbar Text Text\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList ItemList Item\nToolbarText\nList Item\nText\nList ItemText\nList Item\nList ItemText\nList Item\nList Item\nList Item\nList ItemList Item\nDrawerToolbar\nList Item\nIcon\nAdvertisement\nWeb View\nToolbar\nWeb View\nToolbar\nText\nImage\nText\nTextText\nText\nTextImage\nText\nText\nText\nText\nImage ImageOurs\nImage\nImage\nText Button\nAdvertisement\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText\nList Item\nList Item\nAdvertisement\nText Button\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Button\nText ButtonTextToolbar\nText Button\nText Button\nList Item\nList Item\nList Item\nList Item\nList Item\nInput\nInput\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText\nText Button\nWeb ViewAdvertisement\nIcon TextText\nText\nCard\nText\nIcon IconText\nText\nTextList Item\nInput\nText Button\nImage\nImage\nImage\nImage\nImage\nText List Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Button\nText\nText\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nTextImage Image Image\nText Text Text ButtonText Button\nText Text\nTextList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Button\nText\nText ButtonText\nText Button\nTextText\nText Button\nText Button\nText\nList Item\nText\nText Button\nText Button\nBackground ImageList Item\nList Item\nList Item\nList Item\nList ItemList Item\nAdvertisement\nToolbar\nList Item\nList Item\nList Item\nText\nDrawer\nToolbar\nText Button Text ButtonText ButtonText\nIcon\nBackground ImageText ButtonText Image\nText Button\nText\nText Button\nText ButtonText ButtonText Button\nText Button\nText ButtonText Button\nTextText Button\nText ButtonText\nText Button\nText\nText Button\nText ButtonText Button Text Button\nTextImage\nList Item\nAdvertisement\nBackground ImageWeb View\nBackground Image\nTextImage\nText Button\nText Button\nFigure 6: Synthesized RICO examples.\nImage\nRadio Button\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nImage\nText InputText\nImage\nText\nTextText\nImage Text\nText Text Text Button\nText\nTextList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList ItemList Item List ItemList Item List ItemList Item List Item\nList Item List Item\nList Item List Item\nImage Image\nText Advertisement\nDrawerImage Image\nImage\nWeb View\nText\nText ButtonImage\nImage\nText\nText\nText\nInput\nWeb View\nIcon Icon\nText\nAdvertisement\nText\nImage\nText\nText\nImage\nText\nText\nText\nText\nAdvertisement\nText\nImage\nText\nAdvertisement\nCard\nCard\nBackground Image\nImageIcon\nList ItemImage\nText Button\nText Button\nText Button\nText Text\nText Radio Button\nText Button\nText\nText Button\nText ButtonText Button\nToolbar\nBackground Image\nBackground Image\nToolbar\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Button\nBackground Image\nText\nText\nText\nText\nAdvertisement\nImageImage\nList Item\nList ItemList Item\nImage\nWeb View\nWeb View\nText\nWeb View\nText Button\nText Button\nAdvertisement\nTextWeb View\nText Button\nToolbar\nTextText\nText Button\nText\nText\nText\nText Button Text Button\nImage\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText\nText Button Text Button\nImage\nText Button\nImage Text Button\nImage\nText Button\nText ButtonText Button\nDrawerToolbar\nText Button Text\nText\nText\nText\nText\nText\nText\nList Item\nList Item\nList Item\nList Item\nList ItemList Item\nToolbar\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nImageText TextIcon Icon\nIcon\nText\nText\nTextText\nIconIcon Icon IconIcon Icon Icon Icon Icon\nText Icon Icon Icon Icon\nText Text Text Text\nAdvertisement\nInputText Button\nInputInputText Button\nText Button\nText Button Text Button\nText Button\nText Button\nText\nList ItemText Text\nList Item\nList Item\nList ItemList Item\nList Item\nList Item\nList Item\nList Item\nAdvertisement\nText Image\nImageInput\nImage\nIconText\nText Button\nImageText Button\nText Button Text ButtonImage\nText Button\nText ButtonImage\nText Button Text Button\nImage\nImage Image\nTextText\nText Button\nText Button\nDrawerToolbar\nList ItemList Item\nText\nList Item\nList Item\nList Item\nList ItemList Item\nList Item\nList Item\nImage\nInput\nImage\nList Item\nList Item\nList Item\nList Item\nImage\nText\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList ItemList ItemList Item\nText Image\nImage\nImage\nImageText\nImage\nImage ImageImage Image Icon Icon\nText\nText Text\nText\nText\nText TextText Text\nText\nText Button\nImage\nImageText\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nList Item\nText Button\nText Text Text\nList Item\nList Item\nText\nInputIcon\nImage\nIcon Image\nIcon Text\nIcon\nToolbar\nText Icon\nTextText\nImage\nIcon\nText Button\nIcon\nText ButtonIcon Icon\nText\nAdvertisement\nList ItemList ItemImage\nText\nText\nText\nFigure 6: (Cont.) Synthesized RICO examples.\n3.3. COCO-Stuff\nLayoutV AE [16]\nbus\nroad\nsky-other\ntree\nperson\nsurfboard\nsea\npersonplayingfield\nwall-concrete\nairplane\nsky-other\nbus\nclouds\ngrass\ngiraffe\ngiraffegiraffe\ndirt\ngrass\nsky-other\ntreeGupta et al. [9]\nsky-other\nairplane\ntree\ngiraffe\ngiraffe\ngrass\ntree sky-other\nperson\nfence\nsky-other\nsea\nsky-other\nhill\nzebrazebragrass\nwall-other\nperson\nchairumbrella\npavementOurs\nwall-other\nwall-other\nbuilding-other\nbridge\npersonbush\nroad pavementplant-other\nchair\ncloudsgrass\nroad\nroad\nlight\nground-other\nfence bird\nbirdsand\nfence\nwall-other cage\nfencegrasspavement\ntable\ndog\nwall-concrete\nwall-tile\nbook\nperson\nchair\ngravel\ncow\nhorse\nsnow\nmetal\nwall-other\nceiling-other\ndoor-stuffwall-other\nshelf\ndoor-stuff\nwall-wood\nperson\nwindow-other\nperson\nperson\nplant-otherpavement\nrug catwall-concrete\npaper\nbook\ntable\nfloor-tilewall-wood\nwall-other\nsink\nleaves\nwall-concrete paper\nwall-other\ncabinet\nperson\nwall-concretemotorcycle\nfloor-other\nperson\nbench\nperson\nbranch\nhill\nfence\ngiraffe\nbush\nwall-tile\nwall-other\ncabinet\nsink oven\ntree\ngiraffe\nhorse\nsnow fence\nwall-concretewall-concretestairs\nclothes\nbench\npaper\ndirt\nsky-otherwater-other\npaper\norange\nmetal\ntreebranch\npersongrasssand\ntablelaptopwindow-other\nmouse\ndesk-stuff\ntreeperson\nrailingsnow\nskis\nbuilding-other\nwall-other\ntv\npersonclothes\nbridgeskateboard\nrailing\ntree\nbuilding-otherbush wall-stone\nfloor-tile\nwater-other\nparking meter\npavement\ndog\nsandwich\nfurniture-othertable\nsandwich\nsandwich\nbroccoli\nFigure 7: Synthesized COCO-Stuff examples.\n3.4. SUN RGB-D\nchair\nchair\nchair\nchair\nchair\nchair\nchair\ntable\nchair\nsofa_chair\nendtable\nsofa_chair\nsofa_chair\ncoffee_table\nsofa_chair\nsofa_chair\nchair\nchair\nchair\nchair\nchairchair\nchair\nchairchair\nchair\ntable\nchair\nchair\nchair\nchair\ntable\nchair chair\nchair\ngarbage_bin\nbed\nnight_stand\nchair\nchair\ntable\nchair\nchair\nchair\nchair\ntable\nchair\nchairchair\ntable\nchair\nchair\nchair\ntable\nchair\nchairchair\nchair\nchair\ntable\nchair chair\nchair\nsofa_chair\ntable\nsofa_chair\nsofa_chair\nsofa_chair\nchair\nchairchair\nchair\nchair\nchairchair\nchairchair\nchair\nchair\nchair\nbench\nchair\nchair\nchair\nchair\nchair\ntable\nchair\nchair\ntable\nflower_vase\nchair\nsofa_chair\ntable\nsofa_chair\nsofa_chair\nsofa_chair\nsofa_chair\nsofa_chair\nendtable\nsofa_chair\nchair\nchair\nsofa_chair\ncoffee_table\nsofa_chair\ntable\nchair\ntable\nchair\nchair\nchair\nchair\ncoffee_tablesofa\nsofa\nchair\ntable\nchair chair\nchair\nchair\ndesk\nchair\nchair\nchair\nchair\ntable\nchair\nchair\nchair\nchair\nchair\ndesk\nchairchair\nchair\nchair\ntable\nchair\nchair\nchair\nsofa_chair\nendtable\nsofa_chair\nchair\nchair\nchair\ntable\nchair\nchair\nchair\ntable\nchair\nchair\nchair\nFigure 8: Synthesized SUN RGB-D samples.\n3.5. Variational Transformer Networks for Language Modeling\nThe main motivation for our architectural choices is the success of self-attention in the ﬁeld of natural language processing.\nIt begs the question, how well does our method perform on text generation tasks? We train our autoregressive model on a\ndataset of 800K Amazon book reviews from [1] with no major modiﬁcations to the architecture: we simply replace the ﬁrst\nfully connected layer with a word embedding (learned from scratch) and the output shape to that of our target vocabulary\n(30K words). For this experiment we use 6 self-attention layers on both encoder and decoder. In tab. 1 we show the results of\nour method. The reviews have correct grammar and are highly realistic. This shows that our method can be applied to other\ntasks where the input is a sequence of data.\nReal samples\nA great read focusing on the main character named Shadow. The gods mentioned in the title are a dark and fading lot, and\nthe story could be seen as a sort of parable about the old making way for the new. Good entertainment, though.\nJust ﬁnished this book. I very much liked it. It kept me engaged throughout the entire reading. It sometimes seems as if\nyou’re not fully immersed in Shadow’s like and persona, however, at the end of the book, I felt as though I knew enough\nto feel satisﬁed.\nLong winded and no excitement. It just plodded. On until it ended. complete waste of time, do t plan on reading anymore\nof this author’s material\nThis is a really imaginative story that takes you totally by surprise and all over several dimentions.\nAlthough I spent most of this book being not quite sure what was going on, once I ﬁnished it and took the whole thing in\nI loved it. The revelations at the end are what really sealed the deal for me, ﬁnding them very clever and fulﬁlling.\nGreat book. It felt as though the book moved both slowly and quickly as only a true classic can.\nGreat book! I love the Blossom Street Series! I am hooked on that series! Got the book in a reasonable amount of time!\nVery happy with the book! Came in great condition!\nI have enjoyed Debbie Macomber’s other book series so I decided to try this Blossom a Shop series. I love it and lose\nmyself in the lives of the characters!\nLoved it.\nThis series is awesome as good as all her books .I really enjoyed these books a lot\nAnyone who enjoys reading about women’s friendships will love this book. As a knitter I really like reading about the\nknitting and have taken away some ideas to use in my knitting projects, especially for the charitable knitting group I’m in!Ours\ninteresting ride of danger and suspense i think clancy has been on our lookout to clean ways\nthese books are awesome and can’t wait to read more about the history of the city going through some presidential topped\na really good novel i couldn’t put down there are many ups and downs yet so uplifting\nvery good book about an amazing woman this one leaves you begging and be entertained\ntherapy and exciting parts should be read by everyone real life persons an approach thanks\nthanks for opening my eyes to everyone i’ve never read anything like it was very great\na great read one learns and never boring facts in school so that you found it very believable\na few of the characters were not particularly compelling but they never got any better it let me down with many people\ngood suspense and great writing will open a person’s eyes on other subject matter\ngreat read and interesting ending chapters are sad and far away from you\nvery interesting read hard to put down lehane does have really researched his subject matter but he made it realistic plus\nthe ending in a quit\nwe learn about africa and from this novel every time we really need a man and makes it\na must read for any real good reason it’s been years since the author came to life of your face\ngreat book just like divergent and always worth reading this year messages of speech has been thorough\nlove it all my life truly a page turner and what else is happening great stuff about endurance\nexcellent end to an amazing trilogy energy and suspense can’t wait there is another life\ngood read but takes forever to write about things and its our real lives\nanother success for john o’donohue and the same day i meet him but excellent a lot of true emotions\nunexpected turn in surprises and surprise ending was a UNK think again and again about how worthwhile\na great read and i love his writing and it made me think about leader of tennis authorities\nyes it is thinking me things i never knew about glad you were written or heard from author\ngood read makes you think and feel how it was affecting the people throughout every new books\nTable 1: Top: Real Amazon book reviews from [1]. Bottom: reviews generated with our method.\n4. Convergence tests\nIn order to determine the required number of elements for our model to generalize, we train our autoregressive model on\nvarious subsets of the PubLayNet training data, and evaluate the number of unique DocSim matches on 1000 samples. We\naverage the results across 5 identical trainings. In ﬁg. 9 we show the results. For this particular dataset, 50K training samples\nare enough to generalize well.\n1000 50000 100000 150000 200000 250000 300000\nSize of the training dataset\n300\n400\n500\n600\n700# unique matches\n348\n451\n482\n651 645\n682 681 674 685\nFigure 9: Convergence results on PubLayNet ±1σ.\n5. Distribution analysis\nAs an additional metric for the ability of our method to capture the layout distribution, in ﬁg. 10 we show the frequency\nof each location as the center of a bounding box on 1000 samples of the PubLayNet test dataset and our model.\nAll Text Title Figure List Table\nSynthetic\nReal\nFigure 10: Top: distribution of the bounding box center for synthetic data. Bottom: real data.\nReferences\n[1] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and ﬁne-grained aspects. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pages 188–197, Hong Kong, China, Nov. 2019. Association for Computational\nLinguistics."
}