{
  "title": "Learning Deep Transformer Models for Machine Translation",
  "url": "https://openalex.org/W2948798935",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1751170494",
      "name": "Wang Qiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051554894",
      "name": "Li Bei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097229227",
      "name": "Xiao Tong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114650367",
      "name": "Zhu Jingbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2694037280",
      "name": "Li Changliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225125410",
      "name": "Wong, Derek F.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225125411",
      "name": "Chao, Lidia S.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2798761464",
    "https://openalex.org/W2964088127",
    "https://openalex.org/W1543750907",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2817535134",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W1597944220",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2952564229",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2963302407",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963755523",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2902081112",
    "https://openalex.org/W2963991316",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963599677",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2128892113",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2113104171",
    "https://openalex.org/W2886490473"
  ],
  "abstract": "Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",
  "full_text": "Learning Deep Transformer Models for Machine Translation\nQiang Wang1, Bei Li1, Tong Xiao1,2∗, Jingbo Zhu1,2, Changliang Li3,\nDerek F. Wong4, Lidia S. Chao4\n1NLP Lab, Northeastern University, Shenyang, China\n2NiuTrans Co., Ltd., Shenyang, China\n3Kingsoft AI Lab, Beijing, China\n4NLP2CT Lab, University of Macau, Macau, China\nwangqiangneu@gmail.com, libei neu@outlook.com,\n{xiaotong,zhujingbo}@mail.neu.edu.com,\nlichangliang@kingsoft.com, {derekfw,lidiasc}@um.edu.mo\nAbstract\nTransformer is the state-of-the-art model in\nrecent machine translation evaluations. Two\nstrands of research are promising to im-\nprove models of this kind: the ﬁrst uses\nwide networks (a.k.a. Transformer-Big) and\nhas been the de facto standard for the de-\nvelopment of the Transformer system, and\nthe other uses deeper language representation\nbut faces the difﬁculty arising from learn-\ning deep networks. Here, we continue the\nline of research on the latter. We claim that\na truly deep Transformer model can surpass\nthe Transformer-Big counterpart by 1) proper\nuse of layer normalization and 2) a novel\nway of passing the combination of previous\nlayers to the next. On WMT’16 English-\nGerman, NIST OpenMT’12 Chinese-English\nand larger WMT’18 Chinese-English tasks,\nour deep system (30/25-layer encoder) out-\nperforms the shallow Transformer-Big/Base\nbaseline (6-layer encoder) by 0.4 ∼2.4 BLEU\npoints. As another bonus, the deep model is\n1.6X smaller in size and 3X faster in training\nthan Transformer-Big1.\n1 Introduction\nNeural machine translation (NMT) models have\nadvanced the previous state-of-the-art by learn-\ning mappings between sequences via neural net-\nworks and attention mechanisms (Sutskever et al.,\n2014; Bahdanau et al., 2015). The earliest of\nthese read and generate word sequences using a\nseries of recurrent neural network (RNN) units,\nand the improvement continues when 4-8 layers\nare stacked for a deeper model (Luong et al.,\n2015; Wu et al., 2016). More recently, the system\nbased on multi-layer self-attention (call it Trans-\nformer) has shown strong results on several large-\n∗Corresponding author.\n1The source code is available at https://github.\ncom/wangqiangneu/dlcl\nscale tasks (Vaswani et al., 2017). In particu-\nlar, approaches of this kind beneﬁt greatly from\na wide network with more hidden states (a.k.a.\nTransformer-Big), whereas simply deepening the\nnetwork has not been found to outperform the\n“shallow” counterpart (Bapna et al., 2018). Do\ndeep models help Transformer? It is still an open\nquestion for the discipline.\nFor vanilla Transformer, learning deeper net-\nworks is not easy because there is already a rel-\natively deep model in use 2. It is well known that\nsuch deep networks are difﬁcult to optimize due\nto the gradient vanishing/exploding problem (Pas-\ncanu et al., 2013; Bapna et al., 2018). We note that,\ndespite the signiﬁcant development effort, simply\nstacking more layers cannot beneﬁt the system and\nleads to a disaster of training in some of our exper-\niments.\nA promising attempt to address this issue is\nBapna et al. (2018)’s work. They trained a 16-\nlayer Transformer encoder by using an enhanced\nattention model. In this work, we continue the line\nof research and go towards a much deeper encoder\nfor Transformer. We choose encoders to study be-\ncause they have a greater impact on performance\nthan decoders and require less computational cost\n(Domhan, 2018). Our contributions are threefold:\n•We show that the proper use of layer normal-\nization is the key to learning deep encoders.\nThe deep network of the encoder can be\noptimized smoothly by relocating the layer\nnormalization unit. While the location of\nlayer normalization has been discussed in re-\ncent systems (Vaswani et al., 2018; Domhan,\n2018; Klein et al., 2017), as far as we know,\nits impact has not been studied in deep Trans-\n2For example, a standard Transformer encoder has 6 lay-\ners. Each of them consists of two sub-layers. More sub-layers\nare involved on the decoder side.\narXiv:1906.01787v1  [cs.CL]  5 Jun 2019\nxl F\n⨁\nLN xl+1\nyl\n(a) post-norm residual unit\nxl LN F\n⨁ xl+1\nyl\n(b) pre-norm residual unit\nFigure 1: Examples of pre-norm residual unit and post-\nnorm residual unit. F= sub-layer, and LN = layer nor-\nmalization.\nformer.\n•Inspired by the linear multi-step method\nin numerical analysis (Ascher and Petzold,\n1998), we propose an approach based on dy-\nnamic linear combination of layers (D LCL)\nto memorizing the features extracted from all\npreceding layers. This overcomes the prob-\nlem with the standard residual network where\na residual connection just relies on the output\nof one-layer ahead and may forget the earlier\nlayers.\n•We successfully train a 30-layer encoder, far\nsurpassing the deepest encoder reported so\nfar (Bapna et al., 2018). To our best knowl-\nedge, this is the deepest encoder used in\nNMT.\nOn WMT’16 English-German, NIST\nOpenMT’12 Chinese-English, and larger\nWMT’18 Chinese-English translation tasks,\nwe show that our deep system (30/25-layer\nencoder) yields a BLEU improvement of 1.3∼2.4\npoints over the base model (Transformer-Base\nwith 6 layers). It even outperforms Transformer-\nBig by 0.4 ∼0.6 BLEU points, but requires 1.6X\nfewer model parameters and 3X less training time.\nMore interestingly, our deep model is 10% faster\nthan Transformer-Big in inference speed.\n2 Post-Norm and Pre-Norm Transformer\nThe Transformer system and its variants follow the\nstandard encoder-decoder paradigm. On the en-\ncoder side, there are a number of identical stacked\nlayers. Each of them is composed of a self-\nattention sub-layer and a feed-forward sub-layer.\nThe attention model used in Transformer is multi-\nhead attention, and its output is fed into a fully\nconnected feed-forward network. Likewise, the\ndecoder has another stack of identical layers. It\nhas an encoder-decoder attention sub-layer in ad-\ndition to the two sub-layers used in each encoder\nlayer. In general, because the encoder and the de-\ncoder share a similar architecture, we can use the\nsame method to improve them. In the section, we\ndiscuss a more general case, not limited to the en-\ncoder or the decoder.\n2.1 Model Layout\nFor Transformer, it is not easy to train stacked lay-\ners on neither the encoder-side nor the decoder-\nside. Stacking all these sub-layers prevents the ef-\nﬁcient information ﬂow through the network, and\nprobably leads to the failure of training. Residual\nconnections and layer normalization are adopted\nfor a solution. Let Fbe a sub-layer in encoder or\ndecoder, and θl be the parameters of the sub-layer.\nA residual unit is deﬁned to be (He et al., 2016b):\nxl+1 = f(yl) (1)\nyl = xl + F(xl; θl) (2)\nwhere xl and xl+1 are the input and output of the\nl-th sub-layer, andylis the intermediate output fol-\nlowed by the post-processing functionf(·). In this\nway, xl is explicitly exposed to yl (see Eq. (2)).\nMoreover, layer normalization is adopted to re-\nduce the variance of sub-layer output because hid-\nden state dynamics occasionally causes a much\nlonger training time for convergence. There are\ntwo ways to incorporate layer normalization into\nthe residual network.\n•Post-Norm. In early versions of Transformer\n(Vaswani et al., 2017), layer normalization is\nplaced after the element-wise residual addi-\ntion (see Figure 1(a)), like this:\nxl+1 = LN(xl + F(xl; θl)) (3)\nwhere LN(·) is the layer normalization func-\ntion, whose parameter is dropped for simplic-\nity. It can be seen as a post-processing step of\nthe output (i.e., f(x) = LN(x)).\n•Pre-Norm. In recent implementations (Klein\net al., 2017; Vaswani et al., 2018; Domhan,\n2018), layer normalization is applied to the\ninput of every sub-layer (see Figure 1(b)):\nxl+1 = xl + F(LN(xl); θl) (4)\nEq. (4) regards layer normalization as a part\nof the sub-layer, and does nothing for post-\nprocessing of the residual connection (i.e.,\nf(x) = x).3\nBoth of these methods are good choices for im-\nplementation of Transformer. In our experiments,\nthey show comparable performance in BLEU for a\nsystem based on a 6-layer encoder (Section 5.1).\n2.2 On the Importance of Pre-Norm for Deep\nResidual Network\nThe situation is quite different when we switch to\ndeeper models. More speciﬁcally, we ﬁnd that pre-\nnorm is more efﬁcient for training than post-norm\nif the model goes deeper. This can be explained by\nseeing back-propagation which is the core process\nto obtain gradients for parameter update. Here we\ntake a stack of L sub-layers as an example. Let\nEbe the loss used to measure how many errors\noccur in system prediction, and xL be the output\nof the topmost sub-layer. For post-norm Trans-\nformer, given a sub-layer l, the differential of E\nwith respect to xl can be computed by the chain\nrule, and we have\n∂E\n∂xl\n= ∂E\n∂xL\n×\nL−1∏\nk=l\n∂LN(yk)\n∂yk\n×\nL−1∏\nk=l\n(\n1 + ∂F(xk; θk)\n∂xk\n)\n(5)\nwhere ∏L−1\nk=l\n∂LN(yk)\n∂yk\nmeans the backward pass of\nthe layer normalization, and ∏L−1\nk=l (1 +∂F(xk;θk)\n∂xk\n)\nmeans the backward pass of the sub-layer with the\nresidual connection. Likewise, we have the gradi-\nent for pre-norm 4:\n∂E\n∂xl\n= ∂E\n∂xL\n×\n(\n1 +\nL−1∑\nk=l\n∂F(LN(xk); θk)\n∂xl\n)\n(6)\nObviously, Eq. (6) establishes a direct way to\npass error gradient ∂E\n∂xL\nfrom top to bottom. Its\nmerit lies in that the number of product items on\nthe right side does not depend on the depth of the\nstack.\nIn contrast, Eq. (5) is inefﬁcient for passing gra-\ndients back because the residual connection is not\n3We need to add an additional function of layer normal-\nization to the top layer to prevent the excessively increased\nvalue caused by the sum of unnormalized output.\n4For a detailed derivation, we refer the reader to Appendix\nA.\na bypass of the layer normalization unit (see Fig-\nure 1(a)). Instead, gradients have to be passed\nthrough LN(·) of each sub-layer. It in turn intro-\nduces term ∏L−1\nk=l\n∂LN(yk)\n∂yk\ninto the right hand side\nof Eq. (5), and poses a higher risk of gradient van-\nishing or exploring if Lgoes larger. This was con-\nﬁrmed by our experiments in which we success-\nfully trained a pre-norm Transformer system with\na 20-layer encoder on the WMT English-German\ntask, whereas the post-norm Transformer system\nfailed to train for a deeper encoder (Section 5.1).\n3 Dynamic Linear Combination of\nLayers\nThe residual network is the most common ap-\nproach to learning deep networks, and plays an\nimportant role in Transformer. In principle, resid-\nual networks can be seen as instances of the or-\ndinary differential equation (ODE), behaving like\nthe forward Euler discretization with an initial\nvalue (Chang et al., 2018; Chen et al., 2018b). Eu-\nler’s method is probably the most popular ﬁrst-\norder solution to ODE. But it is not yet accu-\nrate enough. A possible reason is that only one\nprevious step is used to predict the current value\n5(Butcher, 2003). In MT, the single-step property\nof the residual network makes the model “forget”\ndistant layers (Wang et al., 2018b). As a result,\nthere is no easy access to features extracted from\nlower-level layers if the model is very deep.\nHere, we describe a model which makes di-\nrect links with all previous layers and offers ef-\nﬁcient access to lower-level representations in a\ndeep stack. We call it dynamic linear combina-\ntion of layers (D LCL). The design is inspired by\nthe linear multi-step method (LMM) in numerical\nODE (Ascher and Petzold, 1998). Unlike Euler’s\nmethod, LMM can effectively reuse the informa-\ntion in the previous steps by linear combination to\nachieve a higher order. Let {y0,...,y l}be the out-\nput of layers 0 ∼l. The input of layer l+ 1 is\ndeﬁned to be\nxl+1 = G(y0,...,y l) (7)\nwhere G(·) is a linear function that merges pre-\nviously generated values {y0,...,y l}into a new\nvalue. For pre-norm Transformer, we deﬁne G(·)\n5Some of the other single-step methods, e.g. the Runge-\nKutta method, can obtain a higher order by taking several\nintermediate steps (Butcher, 2003). Higher order generally\nmeans more accurate.\n1\n0 1\n0 0 1\n0 0 0 1\nx1\nx2\nx3\nx4\ny0 y1 y2 y3\n(a)\n1\n1 1\n1 1 1\n1 1 1 1\nx1\nx2\nx3\nx4\ny0 y1 y2 y3\n(b)\n1\n0 1\n0 0 1\n.1 .3 .2 .4\nx1\nx2\nx3\nx4\ny0 y1 y2 y3\n(c)\n1.8\n.4 1.2\n.3 .2 .8\n.1 .3 .5 .7\nx1\nx2\nx3\nx4\ny0 y1 y2 y3\n(d)\nFigure 2: Connection weights for 3-layer encoder: (a) residual connection (He et al., 2016a), (b) dense residual con-\nnection (Britz et al., 2017; Dou et al., 2018), (c) multi-layer representation fusion (Wang et al., 2018b)/transparent\nattention (Bapna et al., 2018) and (d) our approach. y0 denotes the input embedding. Red denotes the weights are\nlearned by model.\nto be\nG(y0,...,y l) =\nl∑\nk=0\nW(l+1)\nk LN(yk) (8)\nwhere Wl+1\nk ∈R is a learnable scalar and weights\neach incoming layer in a linear manner. Eq. (8)\nprovides a way to learn preference of layers in dif-\nferent levels of the stack. Even for the same in-\ncoming layer, its contribution to succeeding layers\ncould be different (e.g. Wi\nk ̸= Wk\nk) . Also, the\nmethod is applicable to the post-norm Transformer\nmodel. For post-norm, G(·) can be redeﬁned as:\nG(y0,...,y l) = LN\n( l∑\nk=0\nW(l+1)\nk yk\n)\n(9)\nComparison to LMM.DLCL differs from LMM\nin two aspects, though their fundamental model is\nthe same. First, D LCL learns weights in an end-\nto-end fashion rather than assigning their values\ndeterministically, e.g. by polynomial interpola-\ntion. This offers a more ﬂexible way to con-\ntrol the model behavior. Second, D LCL has an\narbitrary size of the past history window, while\nLMM generally takes a limited history into ac-\ncount (L ´oczi, 2018). Also, recent work shows\nsuccessful applications of LMM in computer vi-\nsion, but only two previous steps are used in their\nLMM-like system (Lu et al., 2018).\nComparison to existing neural methods.Note\nthat D LCL is a very general approach. For ex-\nample, the standard residual network is a special\ncase of DLCL, where Wl+1\nl = 1, and Wl+1\nk = 0\nfor k < l. Figure (2) compares different meth-\nods of connecting a 3-layer network. We see that\nthe densely residual network is a fully-connected\nnetwork with a uniform weighting schema (Britz\net al., 2017; Dou et al., 2018). Multi-layer repre-\nsentation fusion (Wang et al., 2018b) and trans-\nparent attention (call it TA) (Bapna et al., 2018)\nmethods can learn a weighted model to fuse lay-\ners but they are applied to the topmost layer only.\nThe DLCL model can cover all these methods. It\nprovides ways of weighting and connecting lay-\ners in the entire stack. We emphasize that al-\nthough the idea of weighting the encoder layers\nby a learnable scalar is similar to TA, there are\ntwo key differences: 1) Our method encourages\nearlier interactions between layers during the en-\ncoding process, while the encoder layers in TA\nare combined until the standard encoding process\nis over; 2) For an encoder layer, instead of learn-\ning a unique weight for each decoder layer like\nTA, we make a separate weight for each succes-\nsive encoder layers. In this way, we can create\nmore connections between layers6.\n4 Experimental Setup\nWe ﬁrst evaluated our approach on WMT’16\nEnglish-German (En-De) and NIST’12 Chinese-\nEnglish (Zh-En-Small) benchmarks respectively.\nTo make the results more convincing, we also ex-\nperimented on a larger WMT’18 Chinese-English\ndataset (Zh-En-Large) with data augmentation by\nback-translation (Sennrich et al., 2016a).\n4.1 Datasets and Evaluation\nFor the En-De task, to compare with Vaswani\net al. (2017)’s work, we use the same 4.5M pre-\nprocessed data 7, which has been tokenized and\n6Let the encoder depth be M and the decoder depth be\nN (M > N for a deep encoder model). Then TA newly\nadds O(M ×N) connections, which are fewer than ours of\nO(M2)\n7https://drive.google.com/uc?export=\ndownload&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\nModel Param. Batch Updates †Times BLEU ∆\n(×4096) ( ×100k)\nVaswani et al. (2017) (Base) 65M 1 1 reference 27.3 -\nBapna et al. (2018)-deep (Base, 16L) 137M - - - 28.0 -\nVaswani et al. (2017) (Big) 213M 1 3 3x 28.4 -\nChen et al. (2018a) (Big) 379M 16 †0.075 1.2x 28.5 -\nHe et al. (2018) (Big) †210M 1 - - 29.0 -\nShaw et al. (2018) (Big) †210M 1 3 3x 29.2 -\nDou et al. (2018) (Big) 356M 1 - - 29.2 -\nOtt et al. (2018) (Big) 210M 14 0.25 3.5x 29.3 -\npost-norm\nTransformer (Base) 62M 1 1 1x 27.5 reference\nTransformer (Big) 211M 1 3 3x 28.8 +1.3\nTransformer-deep (Base, 20L) 106M 2 0.5 1x failed failed\nDLCL (Base) 62M 1 1 1x 27.6 +0.1\nDLCL-deep (Base, 25L) 121M 2 0.5 1x 29.2 +1.7\npre-norm\nTransformer (Base) 62M 1 1 1x 27.1 reference\nTransformer (Big) 211M 1 3 3x 28.7 +1.6\nTransformer-deep (Base, 20L) 106M 2 0.5 1x 28.9 +1.8\nDLCL (Base) 62M 1 1 1x 27.3 +0.2\nDLCL-deep (Base, 30L) 137M 2 0.5 1x 29.3 +2.2\nTable 1: BLEU scores [%] on English-German translation. Batch indicates the corresponding batch size if\nrunning on 8 GPUs. Times ∝Batch×Updates, which can be used to approximately measure the required\ntraining time. †denotes an estimate value. Note that “-deep” represents the best-achieved result as depth changes.\njointly byte pair encoded (BPE) (Sennrich et al.,\n2016b) with 32k merge operations using a shared\nvocabulary 8. We use newstest2013 for validation\nand newstest2014 for test.\nFor the Zh-En-Small task, we use parts of the\nbitext provided within NIST’12 OpenMT 9. We\nchoose NIST MT06 as the validation set, and\nMT04, MT05, MT08 as the test sets. All the sen-\ntences are word segmented by the tool provided\nwithin NiuTrans (Xiao et al., 2012). We remove\nthe sentences longer than 100 and end up with\nabout 1.9M sentence pairs. Then BPE with 32k\noperations is used for both sides independently,\nresulting in a 44k Chinese vocabulary and a 33k\nEnglish vocabulary respectively.\nFor the Zh-En-Large task, we use exactly the\nsame 16.5M dataset as Wang et al. (2018a),\ncomposing of 7.2M-sentence CWMT corpus,\n4.2M-sentence UN and News-Commentary com-\nbined corpus, and back-translation of 5M-sentence\nmonolingual data from NewsCraw2017. We refer\nthe reader to Wang et al. (2018a) for the details.\n8The tokens with frequencies less than 5 are ﬁltered out\nfrom the shared vocabulary.\n9LDC2000T46, LDC2000T47, LDC2000T50,\nLDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09,\nLDC2004T08\nFor evaluation, we ﬁrst average the last 5 check-\npoints, each of which is saved at the end of an\nepoch. And then we use beam search with a beam\nsize of 4/6 and length penalty of 0.6/1.0 for En-\nDe/Zh-En tasks respectively. We measure case-\nsensitive/insensitive tokenized BLEU by multi-\nbleu.perl for En-De and Zh-En-Small respec-\ntively, while case-sensitive detokenized BLEU is\nreported by the ofﬁcial evaluation script mteval-\nv13a.pl for Zh-En-Large. Unless noted otherwise\nwe run each experiment three times with different\nrandom seeds and report the mean of the BLEU\nscores across runs10.\n4.2 Model and Hyperparameters\nAll experiments run on fairseq-py11 with 8\nNVIDIA Titan V GPUs. For the post-norm Trans-\nformer baseline, we replicate the model setup of\nVaswani et al. (2017). All models are optimized\nby Adam (Kingma and Ba, 2014) with β1 = 0.9,\nβ2 = 0.98, and ϵ = 10 −8. In training warmup\n(warmup = 4000 steps), the learning rate linearly\nincreases from 10−7 to lr =7×10−4/5×10−4 for\n10Due to resource constraints, all experiments on Zh-En-\nLarge task only run once.\n11https://github.com/pytorch/fairseq\nModel (Base, 16L) BLEU\npost-norm\nBapna et al. (2018) 28.0\nTransformer failed\nDLCL 28.4\npre-norm Transformer 28.0\nDLCL 28.2\nTable 2: Compare with Bapna et al. (2018) on\nWMT’16 English-German translation under a 16-layer\nencoder.\nTransformer-Base/Big respectively, after which it\nis decayed proportionally to the inverse square\nroot of the current step. Label smoothing εls=0.1\nis used as regularization.\nFor the pre-norm Transformer baseline, we fol-\nlow the setting as suggested in tensor2tensor 12.\nMore speciﬁcally, the attention dropout Patt = 0.1\nand feed-forward dropout P ff = 0.1 are addition-\nally added. And some hyper-parameters for op-\ntimization are changed accordingly: β2 = 0.997,\nwarmup = 8000 and lr = 10 −3/7×10−4 for\nTransformer-Base/Big respectively.\nFor both the post-norm and pre-norm baselines,\nwe batch sentence pairs by approximate length\nand restrict input and output tokens per batch\nto batch = 4096 per GPU. We set the update\nsteps according to corresponding data sizes. More\nspeciﬁcally, the Transformer-Base/Big is updated\nfor 100k/300k steps on the En-De task as Vaswani\net al. (2017), 50k/100k steps on the Zh-En-Small\ntask, and 200k/500k steps on the Zh-En-Large\ntask.\nIn our model, we use the dynamic linear combi-\nnation of layers for both encoder and decoder. For\nefﬁcient computation, we only combine the out-\nput of a complete layer rather than a sub-layer. It\nshould be noted that for deep models (e.g. L ≥\n20), it is hard to handle a full batch in a single GPU\ndue to memory size limitation. We solve this issue\nby accumulating gradients from two small batches\n(e.g. batch = 2048) before each update (Ott et al.,\n2018). In our primitive experiments, we observed\nthat training with larger batches and learning rates\nworked well for deep models. Therefore all the re-\nsults of deep models are reported with batch =\n8192, lr = 2×10−3 and warmup = 16,000 unless\notherwise stated. For fairness, we only use half of\nthe updates of baseline (e.g. update = 50k) to\nensure the same amount of data that we actually\n12https://github.com/tensorflow/\ntensor2tensor\nsee in training. We report the details in Appendix\nB.\n5 Results\n5.1 Results on the En-De Task\nIn Table 1, we ﬁrst report results on WMT En-De\nwhere we compare to the existing systems based\non self-attention. Obviously, while almost all pre-\nvious results based on Transformer-Big (marked\nby Big) have higher BLEU than those based on\nTransformer-Base (marked by Base), larger pa-\nrameter size and longer training epochs are re-\nquired.\nAs for our approach, considering the post-norm\ncase ﬁrst, we can see that our Transformer base-\nlines are superior to Vaswani et al. (2017) in both\nBase and Big cases. When increasing the en-\ncoder depth, e.g. L= 20, the vanilla Transformer\nfailed to train, which is consistent with Bapna et al.\n(2018). We attribute it to the vanishing gradient\nproblem based on the observation that the gradi-\nent norm in the low layers (e.g. embedding layer)\napproaches 0. On the contrary, post-norm D LCL\nsolves this issue and achieves the best result when\nL= 25.\nThe situation changes when switching to pre-\nnorm. While it slightly underperforms the post-\nnorm counterpart in shallow networks, pre-norm\nTransformer beneﬁts more from the increase in en-\ncoder depth. More concretely, pre-norm Trans-\nformer achieves optimal result when L=20 (see\nFigure 3(a)), outperforming the 6-layer baseline\nby 1.8 BLEU points. It indicates that pre-norm\nis easier to optimize than post-norm in deep net-\nworks. Beyond that, we successfully train a 30-\nlayer encoder by our method, resulting in a fur-\nther improvement of 0.4 BLEU points. This\nis 0.6 BLEU points higher than the pre-norm\nTransformer-Big. It should be noted that although\nour best score of 29.3 is the same as Ott et al.\n(2018), our approach only requires 3.5X fewer\ntraining epochs than theirs.\nTo fairly compare with transparent attention\n(TA) (Bapna et al., 2018), we separately list the\nresults using a 16-layer encoder in Table 2. It\ncan be seen that pre-norm Transformer obtains the\nsame BLEU score as TA without the requirement\nof complicated attention design. However, D LCL\nin both post-norm and pre-norm cases outperform\nTA. It should be worth that TA achieves the best\nresult when encoder depth is 16, while we can fur-\nModel (pre-norm) Param. Valid. MT04 MT05 MT08 Average\nTransformer (Base) 84M 51.27 54.41 49.43 45.33 49.72\nTransformer (Big) 257M 52.30 55.37 52.21 47.40 51.66\nTransformer-deep (Base, 25L) 144M 52.50 55.80 51.98 47.26 51.68\nDLCL (Base) 84M 51.61 54.91 50.58 46.11 50.53\nDLCL-deep (Base, 25L) 144M 53.57 55.91 52.30 48.12 52.11\nTable 3: BLEU scores [%] on NIST’12 Chinese-English translation.\nModel Param. newstest17 newstest18 ∆avg.\nWang et al. (2018a) (post-norm, Base) 102.1M 25.9 - -\npre-norm Transformer (Base) 102.1M 25.8 25.9 reference\npre-norm Transformer (Big) 292.4M 26.4 27.0 +0.9\npre-norm DLCL-deep (Base, 25L) 161.5M 26.7 27.1 +1.0\npre-norm DLCL-deep (Base, 30L) 177.2M 26.9 27.4 +1.3\nTable 4: BLEU scores [%] on WMT’18 Chinese-English translation.\nBase-6L Big-6L Transformer DLCL\n6 1620 25 30 3526.5\n27.0\n27.5\n28.0\n28.5\n29.0\n29.5\nBLEU Score\n(a) WMT En-De\n6 16 20 25 3049.5\n50.0\n50.5\n51.0\n51.5\n52.0\n52.5\nBLEU Score\n(b) NIST Zh-En\nFigure 3: BLEU scores [%] against the encoder depth\nfor pre-norm Transformer and pre-norm DLCL on\nEnglish-German and Chinese-English tasks.\nther improve performance by training deeper en-\ncoders.\n5.2 Results on the Zh-En-Small Task\nSeen from the En-De task, pre-norm is more effec-\ntive than the post-norm counterpart in deep net-\nworks. Therefore we evaluate our method in the\ncase of pre-norm on the Zh-En task. As shown\nin Table 3, ﬁrstly D LCL is superior to the base-\nline when the network’s depth is shallow. Interest-\ningly, both Transformer and DLCL achieve the best\nresults when we use a 25-layer encoder. The 25-\nlayer Transformer can approach the performance\nof Transformer-Big, while our deep model out-\nperforms it by about 0.5 BLEU points under the\nequivalent parameter size. It conﬁrms that our\napproach is a good alternative to Transformer no\nmatter how deep it is.\n5.3 Results on the Zh-En-Large Task\nWhile deep Transformer models, in particular\nthe deep pre-norm DLCL, show better results\n6 16 20 25 301,800\n2,000\n2,200\n2,400\n2,600\nSpeed\nBase-6L Big-6L DLCL\nFigure 4: GPU generation speed (target tokens/sec.)\nagainst the depth of encoder for pre-norm DLCL on\nEnglish-German task (batch size = 32, beam size = 4).\nthan Transformer-Big on En-De and Zh-En-Small\ntasks, both data sets are relatively small, and\nthe improved performance over Transformer-Big\nmight be partially due to over-ﬁtting in the wider\nmodel. For a more challenging task , we report\nthe results on Zh-En-Large task in Table 4. We\ncan see that the 25-layer pre-norm DLCL slightly\nsurpassed Transformer-Big, and the superiority is\nbigger when using a 30-layer encoder. This result\nindicates that the claiming of the deep network de-\nfeating Transformer-Big is established and is not\naffected by the size of the data set.\n6 Analysis\n6.1 Effect of Encoder Depth\nIn Figure 3, we plot BLEU score as a function\nof encoder depth for pre-norm Transformer and\nDLCL on En-De and Zh-En-Small tasks. First of\nall, both methods beneﬁt from an increase in en-\ncoder depth at the beginning. Remarkably, when\nthe encoder depth reaches 20, both of the two deep\nmodels can achieve comparable performance to\nTransformer-Big, and even exceed it when the en-\ncoder depth is further increased in DLCL. Note that\npre-norm Transformer degenerates earlier and is\nless robust than D LCL when the depth is beyond\n20. However, a deeper network (>30 layers) does\nnot bring more beneﬁts. Worse still, deeper net-\nworks consume a lot of memory, making it impos-\nsible to train efﬁciently.\nWe also report the inference speed on GPU in\nFigure 4. As expected, the speed decreases lin-\nearly with the number of encoder layers. Never-\ntheless, our system with a 30-layer encoder is still\nfaster than Transformer-Big, because the encoding\nprocess is independent of beam size, and runs only\nonce. In contrast, the decoder suffers from severe\nautoregressive problems.\n6.2 Effect of Decoder Depth\nEnc. Depth Dec. Depth BLEU Speed\n6 4 27.12 3088.3\n6 6 27.33 2589.2\n6 8 27.42 2109.6\nTable 5: Tokenized BLEU scores [%] and GPU gen-\neration speed (target tokens per second) in pre-norm\nTransformer (Base) on the test set of WMT English-\nGerman (batch size = 32, beam size = 4).\nTable 5 shows the effects of decoder depth on\nBLEU and inference speed on GPU. Different\nfrom encoder, increasing the depth of decoder only\nyields a slight BLEU improvement, but the cost is\nhigh: for every two layers added, the translation\nspeed drops by approximate 500 tokens evenly.\nIt indicates that exploring deep encoders may be\nmore promising than deep decoders for NMT.\n6.3 Ablation Study\nWe report the ablation study results in Table 6. We\nﬁrst observe a modest decrease when removing the\nintroduced layer normalization in Eq. (8). Then\nwe try two methods to replace learnable weights\nwith constant weights: All-One (Wi\nj = 1) and Av-\nerage (Wi\nj = 1/(i+1)). We can see that these two\nmethods consistently hurt performance, in particu-\nlar in the case of All-One. It indicates that making\nthe weights learnable is important for our model.\nMoreover, removing the added layer normaliza-\ntion in the Average model makes BLEU score drop\nby 0.28, which suggests that adding layer normal-\nization helps more if we use the constant weights.\nIn addition, we did two interesting experiments on\nbig models. The ﬁrst one is to replace the base en-\nModel BLEU\npre-norm DLCL-20L 28.80\n- layer norm. 28.67\n- learnable weight (ﬁx 1) 28.22\n- learnable weight (ﬁx 1/N) 28.51\n- layer norm. 28.23\npre-norm Transformer-Base 27.11\n+ big encoder 27.59\npre-norm Transformer-Big 28.72\n+ 12-layer encoder (DLCL) 29.17\nTable 6: Ablation results by tokenized BLEU [%] on\nthe test set of WMT English-German translation.\ncoder with a big encoder in pre-norm Transformer-\nBase. The other one is to use DLCL to train a\ndeep-and-wide Transformer (12 layers). Although\nboth of them beneﬁt from the increased network\ncapacity, the gain is less than the “thin” counter-\npart in terms of BLEU, parameter size, and train-\ning efﬁciency.\n6.4 Visualization on Learned Weights\nWe visually present the learned weights matri-\nces of the 30-layer encoder (Figure 5(a)) and its\n6-layer decoder (Figure 5(b)) in our pre-norm\nDLCL-30L model on En-De task. For a clearer\ncontrast, we mask out the points with an absolute\nvalue of less than 0.1 or 5% of the maximum per\nrow. We can see that the connections in the early\nlayers are dense, but become sparse as the depth\nincreases. It indicates that making full use of ear-\nlier layers is necessary due to insufﬁcient informa-\ntion at the beginning of the network. Also, we ﬁnd\nthat most of the large weight values concentrate on\nthe right of the matrix, which indicates that the im-\npact of the incoming layer is usually related to the\ndistance between the outgoing layer. Moreover,\nfor a ﬁxed layer’s output yi, it is obvious that its\ncontribution to successive layers changes dynam-\nically (one column). To be clear, we extract the\nweights of y10 in Figure 5(c). In contrast, in most\nprevious paradigms of dense residual connection,\nthe output of each layer remains ﬁxed for subse-\nquent layers.\n7 Related Work\nDeep Models. Deep models have been ex-\nplored in the context of neural machine transla-\ntion since the emergence of RNN-based models.\nTo ease optimization, researchers tried to reduce\nthe number of non-linear transitions (Zhou et al.,\ny0 y5 y10 y15 y20 y25 y30\nx31\nx26\nx21\nx16\nx11\nx6\nx1\n−4\n−2\n0\n2\n4\ny0 y1 y2 y3 y4 y5 y6\nx7\nx6\nx5\nx4\nx3\nx2\nx1\n−2\n0\n2\n4\n6\n8\n(b) 6-layer decoder of DLCL\n4.1 3.3 3.2 1.7 2.3 1.1 0.0 0.0 0.1 0.8 0.5\nx11 ∼x21\n0.2 0.5 0.0 0.5 0.2 0.0 0.0 0.1 0.2 0.0\nx22 ∼x31\n(a) 30-layer encoder of DLCL (c) Weight distribution of y10 in the encoder\nFigure 5: A visualization example of learned weights in our 30-layer pre-norm DLCL model.\n2016; Wang et al., 2017). But these attempts are\nlimited to the RNN architecture and may not be\nstraightforwardly applicable to the current Trans-\nformer model. Perhaps, the most relevant work\nto what is doing here is Bapna et al. (2018)’s\nwork. They pointed out that vanilla Transformer\nwas hard to train if the depth of the encoder was\nbeyond 12. They successfully trained a 16-layer\nTransformer encoder by attending the combina-\ntion of all encoder layers to the decoder. In\ntheir approach, the encoder layers are combined\njust after the encoding is completed, but not dur-\ning the encoding process. In contrast, our ap-\nproach allows the encoder layers to interact ear-\nlier, which has been proven to be effective in ma-\nchine translation (He et al., 2018) and text match\n(Lu and Li, 2013). In addition to machine transla-\ntion, deep Transformer encoders are also used for\nlanguage modeling (Devlin et al., 2018; Al-Rfou\net al., 2018). For example, Al-Rfou et al. (2018)\ntrained a character language model with a 64-\nlayer Transformer encoder by resorting to aux-\niliary losses in intermediate layers. This method\nis orthogonal to our D LCL method, though it is\nused for language modeling, which is not a very\nheavy task.\nDensely Residual Connections. Densely\nresidual connections are not new in NMT. They\nhave been studied for different architectures, e.g.,\nRNN (Britz et al., 2017) and Transformer (Dou\net al., 2018). Some of the previous studies ﬁx\nthe weight of each layer to a constant, while\nothers learn a weight distribution by using ei-\nther the self-attention model (Wang et al., 2018b)\nor a softmax-normalized learnable vector (Peters\net al., 2018). They focus more on learning con-\nnections from lower-level layers to the topmost\nlayer. Instead, we introduce additional connectiv-\nity into the network and learn more densely con-\nnections for each layer in an end-to-end fashion.\n8 Conclusion\nWe have studied deep encoders in Transformer.\nWe have shown that the deep Transformer models\ncan be easily optimized by proper use of layer nor-\nmalization, and have explained the reason behind\nit. Moreover, we proposed an approach based on\na dynamic linear combination of layers and suc-\ncessfully trained a 30-layer Transformer system.\nIt is the deepest encoder used in NMT so far. Ex-\nperimental results show that our thin-but-deep en-\ncoder can match or surpass the performance of\nTransformer-Big. Also, its model size is 1.6X\nsmaller. In addition, it requires 3X fewer training\nepochs and is 10% faster for inference.\nAcknowledgements\nThis work was supported in part by the National\nNatural Science Foundation of China (Grant Nos.\n61876035, 61732005, 61432013 and 61672555),\nthe Fundamental Research Funds for the Cen-\ntral Universities (Grant No. N181602013),\nthe Joint Project of FDCT-NSFC (Grant No.\n045/2017/AFJ), the MYRG from the University of\nMacau (Grant No. MYRG2017-00087-FST).\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nUri M Ascher and Linda R Petzold. 1998. Com-\nputer methods for ordinary differential equations\nand differential-algebraic equations, volume 61.\nSiam.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In In Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3028–3033.\nDenny Britz, Anna Goldie, Thang Luong, and Quoc\nLe. 2017. Massive exploration of neural ma-\nchine translation architectures. arXiv preprint\narXiv:1703.03906.\nJ C Butcher. 2003. Numerical Methods for Ordinary\nDifferential Equations. John Wiley & Sons, New\nYork, NY .\nBo Chang, Lili Meng, Eldad Haber, Frederick Tung,\nand David Begert. 2018. Multi-level residual net-\nworks from dynamical systems view. In Interna-\ntional Conference on Learning Representations.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Mike Schuster, Zhifeng Chen,\net al. 2018a. The best of both worlds: Combining\nrecent advances in neural machine translation.arXiv\npreprint arXiv:1804.09849.\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt,\nand David K Duvenaud. 2018b. Neural ordinary\ndifferential equations. In S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 31, pages 6572–6583. Curran\nAssociates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTobias Domhan. 2018. How much attention do you\nneed? a granular analysis of neural machine trans-\nlation architectures. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 1799–1808.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi,\nand Tong Zhang. 2018. Exploiting deep represen-\ntations for neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4253–4262.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016b. Identity mappings in deep residual net-\nworks. In European Conference on Computer Vi-\nsion, pages 630–645. Springer.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7955–7965.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander Rush. 2017. Opennmt:\nOpen-source toolkit for neural machine translation.\nProceedings of ACL 2017, System Demonstrations,\npages 67–72.\nLajos L ´oczi. 2018. Exact optimal values of step-\nsize coefﬁcients for boundedness of linear multistep\nmethods. Numerical Algorithms, 77(4):1093–1116.\nYiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin\nDong. 2018. Beyond ﬁnite layer neural networks:\nBridging deep architectures and numerical differ-\nential equations. In Proceedings of the 35th In-\nternational Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Re-\nsearch, pages 3282–3291, Stockholmsmssan, Stock-\nholm Sweden. PMLR.\nZhengdong Lu and Hang Li. 2013. A deep architec-\nture for matching short texts. In Advances in Neural\nInformation Processing Systems, pages 1367–1375.\nThang Luong, Hieu Pham, and D. Christopher Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421. Associa-\ntion for Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In WMT, pages 1–9. Association for Compu-\ntational Linguistics.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neural\nnetworks. In International Conference on Machine\nLearning, pages 1310–1318.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), volume 1,\npages 2227–2237.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), volume 2, pages\n464–468.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, et al. 2018. Tensor2tensor for neural ma-\nchine translation. Vol. 1: MT Researchers Track,\npage 193.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nMingxuan Wang, Zhengdong Lu, Jie Zhou, and Qun\nLiu. 2017. Deep neural machine translation with lin-\near associative unit. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30\n- August 4, Volume 1: Long Papers, pages 136–145.\nQiang Wang, Bei Li, Jiqiang Liu, Bojian Jiang,\nZheyang Zhang, Yinqiao Li, Ye Lin, Tong Xiao, and\nJingbo Zhu. 2018a. The niutrans machine transla-\ntion system for wmt18. In Proceedings of the Third\nConference on Machine Translation: Shared Task\nPapers, pages 528–534.\nQiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yin-\nqiao Li, and Jingbo Zhu. 2018b. Multi-layer rep-\nresentation fusion for neural machine translation. In\nProceedings of the 27th International Conference on\nComputational Linguistics, pages 3015–3026.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nTong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.\n2012. Niutrans: an open source toolkit for phrase-\nbased and syntax-based machine translation. In Pro-\nceedings of the ACL 2012 System Demonstrations,\npages 19–24. Association for Computational Lin-\nguistics.\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei\nXu. 2016. Deep recurrent models with fast-forward\nconnections for neural machine translation. Trans-\nactions of the Association of Computational Linguis-\ntics, 4(1):371–383.\nA Derivations of Post-Norm\nTransformer and Pre-Norm\nTransformer\nA general residual unit can be expressed by:\nyl = xl + F(xl; θl), (10)\nxl+1 = f(yl), (11)\nwhere xl and xl+1 are the input and output of the\nl-th sub-layer, andylis the intermediate output fol-\nlowed by the post-processing function f(·).\nWe have known that the post-norm Transformer\nincorporates layer normalization (LN(·)) by:\nxl+1 = LN\n(\nxl + F(xl; θl)\n)\n= LN\n(\nxl + Fpost(xl; θl)\n) (12)\nwhere Fpost(·) = F(·). Note that we omit the pa-\nrameter in LN for clarity. Similarly, the pre-norm\nTransformer can be described by:\nxl+1 = xl + F\n(\nLN(xl); θl\n)\n= xl + Fpre(xl; θl) (13)\nwhere Fpre(·) = F(LN(·)). In this way, we can\nsee that both post-norm and pre-norm are special\ncases of the general residual unit. Speciﬁcally, the\npost-norm Transformer is the special case when:\nfpost(x) = LN(x), (14)\nwhile for pre-norm Transformer, it is:\nfpre(x) = x. (15)\nHere we take a stack of Lsub-layers as an ex-\nample. Let Ebe the loss used to measure how\nmany errors occur in system prediction, andxL be\nthe output of the top-most sub-layer. Then from\nthe chain rule of back propagation we obtain:\n∂E\n∂xl\n= ∂E\n∂xL\n∂xL\n∂xl\n(16)\nTo analyze it, we can directly decompose ∂xL\n∂xl\nlayer by layer:\n∂xL\n∂xl\n= ∂xL\n∂xL−1\n∂xL−1\n∂xL−2\n... ∂xl+1\n∂xl\n. (17)\nConsider two adjacent layers as Eq.10 and Eq. 11,\nwe have:\n∂xl+1\n∂xl\n= ∂xl+1\n∂yl\n∂yl\n∂xl\n= ∂f(yl)\n∂yl\n(\n1 + ∂F(xl; θl)\n∂xl\n) (18)\nFor post-norm Transformer, it is easy to know\n∂fpost(yl)\n∂yl\n= ∂LN(yl)\n∂yl\naccording to Eq.(14). Then\nput Eq.(17) and (18) into Eq.(16) and we can ob-\ntain the differential Lw.r.t. xl:\n∂E\n∂xl\n= ∂E\n∂xL\n×\nL−1∏\nk=l\n∂LN(yk)\n∂yk\n×\nL−1∏\nk=l\n(\n1 + ∂F(xk; θk)\n∂xk\n)\n(19)\nEq.(19) indicates that the number of product terms\ngrows linearly with L, resulting in prone to gradi-\nent vanishing or explosion.\nHowever, for pre-norm Transformer, instead\nof decomposing the gradient layer by layer in\nEq. (17), we can use the good nature that xL =\nxl + ∑L−1\nk=l Fpre(xk; θk) by recursively using\nEq. (13):\nxL = xL−1 + Fpre(xL−1; θL−1)\n= xL−2 + Fpre(xL−2; θL−2) + Fpre(xL−1; θL−1)\n···\n= xl +\nL−1∑\nk=l\nFpre(xk; θk)\n(20)\nIn this way, we can simplify Eq.(17) as:\n∂xL\n∂xl\n= 1 +\nL−1∑\nk=l\n∂Fpre(xk; θk)\n∂xl\n(21)\nDue to ∂fpre(yl)\n∂yl\n= 1 , we can put Eq. (21) into\nEq. (16) and obtain:\n∂E\n∂xl\n= ∂E\n∂xL\n×\n(\n1 +\nL−1∑\nk=l\n∂Fpre(xk; θk)\n∂xl\n)\n= ∂E\n∂xL\n×\n(\n1 +\nL−1∑\nk=l\n∂F(LN(xk); θk)\n∂xl\n)\n(22)\nB Training Hyper-parameters for Deep\nModels\nModel Batch Upd. Lr Wu. PPL\npost 4096 100k 7e−4 4k 4.85\npost 8192 50k 2e−3 16k *\npost-20L 4096 100k 7e−4 4k *\npost-20L 8192 50k 2e−3 16k *\npre 4096 100k 1e−3 8k 4.88\npre 8192 50k 2e−3 16k 4.86\npre-20L 4096 100k 1e−3 8k 4.68\npre-20L 8192 50k 2e−3 16k 4.60\nTable 7: Hyper-parameter selection for shallow and\ndeep models based on perplexity on validation set for\nEnglish-German translation. “post-20L” is short for\npost-norm Transformer with a 20-layer encoder. Sim-\nilarly, “pre-20L” denotes the pre-norm Transformer\ncase. * indicates that the model failed to train.\nWe select hyper-parameters by measuring per-\nplexity on the validation set of WMT En-De\ntask. We compare the effects of hyper-parameters\nin both shallow networks (6 layers) and deep\nnetworks (20 layers). We use the standard hyper-\nparameters for both models as the baselines.\nMore concretely, for post-norm Transformer-\nBase, we set batch/update/lr/warmup to\n4096/100k/7×10−4/4k as the original Trans-\nformer, while for pre-norm Transformer-Base, the\nconﬁguration is 4096/100k/10 −3/8k as suggested\nin tensor2tensor. As for deep models, we uni-\nformly use the setting of 8192/50k/2 ×10−3/16k.\nNote that while we use a 2X larger batch size for\ndeep models, we reduce a half of the number of\nupdates. In this way, the amount of seen training\ndata keeps the same in all experiments. A larger\nlearning rate is used to speed up convergence\nwhen we use large batch. In addition, we found\nsimultaneously increasing the learning rate and\nwarmup steps worked best.\nTable 7 report the results. First of all, we can see\nthat post-norm Transformer failed to train when\nthe network goes deeper. Worse still, the shal-\nlow network also failed to converge when switch-\ning to the setting of deep networks. We attribute\nit to post-norm Transformer being more sensitive\nto the large learning rate. On the contrary, in the\ncase of either a 6-layer encoder or a 20-layer en-\ncoder, the pre-norm Transformer beneﬁts from the\nlarger batch and learning rate. However, the gain\nunder deep networks is larger than that under shal-\nlow networks.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.776867151260376
    },
    {
      "name": "Machine translation",
      "score": 0.7116092443466187
    },
    {
      "name": "Deep learning",
      "score": 0.6575946807861328
    },
    {
      "name": "Computer science",
      "score": 0.6558760404586792
    },
    {
      "name": "Artificial intelligence",
      "score": 0.546891987323761
    },
    {
      "name": "NIST",
      "score": 0.47414323687553406
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.4463188946247101
    },
    {
      "name": "Encoder",
      "score": 0.44545885920524597
    },
    {
      "name": "Natural language processing",
      "score": 0.36424338817596436
    },
    {
      "name": "Electrical engineering",
      "score": 0.17356997728347778
    },
    {
      "name": "Engineering",
      "score": 0.17239591479301453
    },
    {
      "name": "Voltage",
      "score": 0.1397998332977295
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 95
}