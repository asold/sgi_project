{
  "title": "A transformer-Based neural language model that synthesizes brain activation maps from free-form text queries",
  "url": "https://openalex.org/W4286250154",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287035955",
      "name": "Ngo, Gia H.",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2128751151",
      "name": "Nguyen, Minh",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A4222454473",
      "name": "Chen, Nancy F.",
      "affiliations": [
        "Institute for Infocomm Research",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A4222551091",
      "name": "Sabuncu, Mert R.",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2151591509",
    "https://openalex.org/W2135176444",
    "https://openalex.org/W2071608556",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2011243436",
    "https://openalex.org/W2078591625",
    "https://openalex.org/W2136435696",
    "https://openalex.org/W2067456724",
    "https://openalex.org/W2105824687",
    "https://openalex.org/W2053239079",
    "https://openalex.org/W2083677571",
    "https://openalex.org/W1974635983",
    "https://openalex.org/W1987869189",
    "https://openalex.org/W3009688448",
    "https://openalex.org/W2044491272",
    "https://openalex.org/W2164025570",
    "https://openalex.org/W4241074797",
    "https://openalex.org/W2077509789",
    "https://openalex.org/W3162862645",
    "https://openalex.org/W6763121668",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W2096856689",
    "https://openalex.org/W2060259639",
    "https://openalex.org/W2134610285",
    "https://openalex.org/W6603694148",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2126395049",
    "https://openalex.org/W4238557639",
    "https://openalex.org/W6718240422",
    "https://openalex.org/W2103655132",
    "https://openalex.org/W2953332965",
    "https://openalex.org/W4220901244",
    "https://openalex.org/W6801748115",
    "https://openalex.org/W6767278183",
    "https://openalex.org/W3093183092",
    "https://openalex.org/W1974687978",
    "https://openalex.org/W2137966626",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W1853382682",
    "https://openalex.org/W1997538961",
    "https://openalex.org/W2951570501",
    "https://openalex.org/W1978394996",
    "https://openalex.org/W6638575559",
    "https://openalex.org/W2138285552",
    "https://openalex.org/W2112135274",
    "https://openalex.org/W2133128608",
    "https://openalex.org/W2144421443",
    "https://openalex.org/W2140255740",
    "https://openalex.org/W2024729467",
    "https://openalex.org/W2141058981",
    "https://openalex.org/W2951412139",
    "https://openalex.org/W3000211149",
    "https://openalex.org/W2124757386",
    "https://openalex.org/W2160350564",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W4253570578",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2799150297",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4233994114",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4253822397",
    "https://openalex.org/W3204413272",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W90362830",
    "https://openalex.org/W4288089799"
  ],
  "abstract": null,
  "full_text": "Medical Image Analysis (2022)\nContents lists available at ScienceDirect\nMedical Image Analysis\njournal homepage: www.elsevier.com/locate/media\nA Transformer-based Neural Language Model that Synthesizes Brain Activation Maps\nfrom Free-Form Text Queries\nGia H. Ngoa,1,∗, Minh Nguyena,1, Nancy F. Chenb,2, Mert R. Sabuncua,c,2\naSchool of Electrical& Computer Engineering, Cornell University, USA\nbInstitute of Infocomm Research (I2R), A*STAR, Singapore\ncRadiology, Weill Cornell Medicine, USA\nA R T I C L E I N F O\nArticle history:\nKeywords: coordinate-based meta-analy-\nsis, transformers, information retrieval,\nimage generation\nA B S T R A C T\nNeuroimaging studies are often limited by the number of subjects and cognitive pro-\ncesses that can be feasibly interrogated. However, a rapidly growing number of neu-\nroscientiﬁc studies have collectively accumulated an extensive wealth of results. Di-\ngesting this growing literature and obtaining novel insights remains to be a major\nchallenge, since existing meta-analytic tools are constrained to keyword queries. In\nthis paper, we present Text2Brain, an easy to use tool for synthesizing brain activa-\ntion maps from open-ended text queries. Text2Brain was built on a transformer-based\nneural network language model and a coordinate-based meta-analysis of neuroimaging\nstudies. Text2Brain combines a transformer-based text encoder and a 3D image gener-\nator, and was trained on variable-length text snippets and their corresponding activation\nmaps sampled from 13,000 published studies. In our experiments, we demonstrate that\nText2Brain can synthesize meaningful neural activation patterns from various free-form\ntextual descriptions. Text2Brain is available at https://braininterpreter.com as\na web-based tool for eﬃciently searching through the vast neuroimaging literature and\ngenerating new hypotheses.\n© 2022 Elsevier B. V . All rights reserved.\n1. Introduction\nA rapidly growing number of functional magnetic resonance\nimaging (fMRI) studies have given us important insights into\nthe mental processes that underpin behavior. However, individ-\nual studies are often power-restricted (Carp, 2012; Button et al.,\n2013), since the number of subjects and mental processes that\ncan be interrogated in a single experiment is limited (Church\net al., 2010). One approach to digest the vast literature and syn-\nthesize across many studies is to perform a meta-analysis of the\nreported results, such as the coordinates of the most signiﬁcant\n∗Corresponding author\ne-mail: ghn8@cornell.edu (Gia H. Ngo)\n1indicates equal contribution\n2indicates equal contribution\neﬀects (e.g., 3D location of peak brain activation in response\nto a task). These meta-analyses usually require expert curation\nof relevant experiments (e.g. (Costafreda et al., 2008; Minzen-\nberg et al., 2009; Shackman et al., 2011)). A critical technical\nchallenge here is the consolidation of synonymous terms. Im-\nportantly, over time, diﬀerent denominations might be used in\ndiﬀerent contexts or invented to reﬁne existing ideas. For in-\nstance, “self-generated thought”, one of the most highly stud-\nied functional domains of the human brain (Smallwood, 2013),\ncan be referred to by varying terms, such as “task-unrelated\nthought” (Andrews-Hanna et al., 2014).\nThe selection of reported results for meta-analysis can be au-\ntomated on data scraped from the published literature (Yarkoni\net al., 2011; Dock`es et al., 2020; Rubin et al., 2017). Two popu-\nlar examples of this direction are Neurosynth (Yarkoni et al.,\n2011) and more recently Neuroquery (Dock `es et al., 2020).\narXiv:2208.00840v1  [q-bio.NC]  24 Jul 2022\n2 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nNeurosynth utilizes automated keyword search to retrieve rel-\nevant studies and statistical tests to ﬁnd summary brain activa-\ntion maps corresponding to the keywords. Unlike Neurosynth,\nNeuroquery is a predictive model that synthesizes activation\nmaps from keywords in the input query. Despite their dif-\nferences in modeling, both Neurosynth and Neuroquery only\nsupport queries consisting predeﬁned keywords. Furthermore,\nNeurosynth does not explicitly handle long queries, while Neu-\nroquery relies on superﬁcial lexical similarity via word co-\noccurences for inference of longer or rarer queries. We pro-\npose an alternative approach named Text2Brain, which builds\non recent neural language models and permits more ﬂexible\nfree-form text queries. Text2Brain captures a more ﬁne-grained\nand implicit semantic similarity via vector representations from\nthe neural language model in order to retrieve more relevant\nstudies. Furthermore, in contrast to tools like Neuroquery, our\nmethod computes synthesized activation maps via a 3D con-\nvolutional neural network (CNN) model, which we empirically\ndemonstrate, can capture coarse and ﬁne details.\nWe compare Text2Brain’s predictions with those from Neu-\nrosynth and Neuroquery, where we used article titles as free-\nform queries. Furthermore, we assess model predictions on\nindependent test datasets, including reliable task contrasts and\nmeta-analytic activation maps of well-studied cognitive do-\nmains predicted from their descriptions. Our analysis shows\nthat Text2Brain generates activation maps that better match\nthe target images than the baselines tools. Given its ﬂexibil-\nity in taking input queries, Text2Brain can be used as an edu-\ncational aid as well as a tool for synthesizing maps based on\npublished results or generating novel hypotheses for future re-\nsearch. Compared to our conference article (Ngo et al., 2021),\nwe have extensively expanded our results and analysis. Speciﬁ-\ncally, we have expanded on the model validation on article titles\nwith a diﬀerent test set (section 3.1 and 4.1), added additional\nevaluation on the contrast maps predicted from their descrip-\ntions (section 4.2). New results and discussion have also been\nadded to this paper, including a high-level conceptual compar-\nison of models (section 2.7), new experiments on predicting\nrepresentative meta-analytic results (section 3.3 and 4.2), and\nquantitative analysis of the models’ robustness to input queries\n(section 3.4 and 5.2).\n2. Datasets and Methods\n2.1. Model overview\nFigure 1 shows the overview of this work, including data\ngeneration, model architecture, and model training. The\nText2Brain model has an encoder-decoder architecture that\nmaps text sequences into brain activation maps (Section 2.2).\nIts transformer-based encoder uses self-attention to encode a\nsnippet of text input into vector representation (Vaswani et al.,\n2017; Devlin et al., 2018). Text2Brain’s 3D convolutional de-\ncoder (CNN) then translates the vector representation into a 3D\nbrain activation map. The Transformer is currently the most\neﬀective approach for modeling text since it can capture long-\ndistance dependency between words and can learn e ﬃciently\nthrough self-supervision from massive text corpora (Jawahar\nActivation coordinates from all tablesAttention in number and language processingGregory Home, Jim WilsonAbstract:Coordinate-basedmeta-analysiscanprovideimportantinsightsintomind-brainrelationships.Apopularapproachforcuratedsmall-scalemeta-analysisisactivationlikelihoodestimation(ALE),whichidentifiesbrainregionsconsistentlyactivatedacrossaselectedsetofexperiments…Discussion:Theco-activationpatternmightbeengagedinattentionalcontrol,especiallyaspectsoftaskmaintenanceandshiftingofattentionalset.Attentionalset-shiftingistheabilitytoswitchbetweenmentalstatesassociatedwithdifferentreactionary…\nCounting > Restxyz324-543-1418………\n+++ ++ ++\nSample title, keywords, sentences from abstract and discussionFinetuned SciBERT3-D DecoderFC\nArticle-averageactivation map\nCountingxyz721-81314-12………\nTask switchingxyz15-251-8213…………\nFig. 1. Overview of data preprocessing, the Text2Brain model, and train-\ning procedure. All activation maps are 3D volumes, but projected to the\nsurface for visualization.\net al., 2019; Raﬀel et al., 2020). On the other hand, 3D CNNs\nare the most dominant architectural design in medical imag-\ning (Milletari et al., 2016; Kamnitsas et al., 2017).\nIn our proposed approach, we ﬁrst extract full text and ac-\ntivation coordinates from each research article to create data\nsamples. Each sample consists of an input snippet from the\nfull text and an output 3D activation map created using the\ncoordinates (Section 2.3). Text2Brain is trained to associate\nthe input text to activations at various spatial locations. Since\nText2Brain’s transformer-based encoder is context-sensitive, it\ncan better extract information from free-form query by reﬁning\nthe vector representation depending on the speciﬁc phrasing of\nthe text inputs (Tenney et al., 2019). In contrast, the classical\nkeyword search mainly exploits co-occurrence of keywords re-\ngardless of context and therefore may struggle on more nuanced\nqueries (Salton and Buckley, 1988). Furthermore, keyword\nsearch approaches store one activation map for each supported\nkeyword, which are in turn linearly combined for queries. This\napproach can limit how many keywords are supported (Yarkoni\net al., 2011; Dock`es et al., 2020). On the other hand, Text2Brain\nstores the text and activation maps content in its parameters and\ncan scale better to diverse input queries (Petroni et al., 2019).\nWe use data augmentation to encourage Text2Brain to construct\nand store rich many-to-one mappings between textual descrip-\ntion and activation maps (Section 2.4). This allows Text2Brain\nto better map semantically similar text queries to similar activa-\ntion maps.\n2.2. Implementation\nFigure 1 bottom left corner shows the Text2Brain model with\nits text encoder and 3D CNN image decoder. Text2Brain’s text\nencoder is based on SciBERT, a BERT model that has been\ntrained using scientiﬁc articles (Beltagy et al., 2019). BERT\nis a transformer-based model with bidirectional self-attention\ntrained via self-supervision to learn semantic representations\nof textual input (Devlin et al., 2018). The text encoder out-\nputs a vector representation of dimension 768. This vector is\nprojected using a fully-connected layer and then reshaped to a\n3D volume of dimension 4 ×5 ×4 voxels with 64 channels\nat each voxel. The image decoder consists of 3 transposed\nNgo & Nguyen et al./ Medical Image Analysis (2022) 3\n3D convolutional layers with 32, 16, 8 channels respectively.\nText2Brain was trained using the Adam optimizer (Loshchilov\nand Hutter, 2018) and the mean-squared error with a batch\nsize of 24 for 2000 epochs. The learning rate for the text\nencoder and image decoder are set at 10 −5 and 3 ×10−2 re-\nspectively. The model’s source code is available at https:\n//github.com/sabunculab/text2brain.\n2.3. Data Preprocessing\nWe used the same set of 13,000 neuroimaging articles previ-\nously released in (Dock`es et al., 2020) in our experiments. Each\narticle contains one or more tables of results that reported coor-\ndinates of peak activation in MNI152 coordinate system (Lan-\ncaster et al., 2007). The activation foci are also publicly re-\nleased by Neuroquery (Dock `es et al., 2020). Following the\nsame procedure as (Dock `es et al., 2020), the set of activation\nfoci associated with each table is used to generate an activation\nmap by placing a Gaussian sphere with full width at half maxi-\nmum (FWHM) of 9mm at each of the coordinates of peak acti-\nvation. The chosen FWHM allows a fair comparison with Neu-\nroquery(Dock`es et al., 2020) in our experiments, and is con-\nsistent with previous work (Wager et al., 2009; Yarkoni et al.,\n2011; Yeo et al., 2015). Supplemental section 6.6 shows an\nanalysis of the eﬀect of the Gaussian kernel’s FWHM used for\npreprocessing on Text2Brain’s predictive accuracy on an inde-\npendent test set. This comparison conﬁrms that the choice of\nthe kernel’s FWHM is reasonable. An article-average activa-\ntion map is also generated by averaging the activation maps of\nall the tables in the article. The text associated with the acti-\nvation maps are extracted from the articles’ full text. The arti-\ncles’ full text are scraped using their PubMedID via the NCBI\nAPI 3 and the Elsevier E-utilities API 4. As there may be multi-\nple text snippets corresponding to the same activation map, the\nnext section (Section 2.4) shows how the corresponding text of\nan activation map is selected.\n2.4. Training\nEach training sample consists of a text-activation map pair\nand correspond to an neuroimaging article. The activation map\nis sampled uniformly at random from the union set of table-\nspeciﬁc maps and article-average map. For each table-speciﬁc\nmap, the ﬁrst sentence of the corresponding table caption is\nchosen as the map’s associated text. Our initial data explo-\nration suggested that the ﬁrst sentence to be the most relevant\ndescription of the activation map. For each article-average map,\nthe associated text that describes the activation map is sampled\nuniformly at random from the following four sources: (1) the\narticle’s title; (2) one of the article’s keywords; (3) the arti-\ncle’s abstract; and (4) a randomly chosen subset of sentences\nfrom the discussion section of the article. This data augmen-\ntation strategy encourages Text2Brain to generalize over input\ntexts of diﬀerent lengths. Furthermore, matching the same ac-\ntivation pattern with multiple diﬀerent text snippets encourages\n3https://www.ncbi.nlm.nih.gov/books/NBK25501/\n4https://dev.elsevier.com/\nthe model to recognize important words common across the\nsnippets and to learn the association between diﬀerent but syn-\nonymous words. Supplemental Figure 12 shows our ablation\nstudy on the sampling strategy. The liberal (and likely noisy)\nconstruction of image-text pairs appears to perform better than\nmore deliberate coupling of image-text snippets strategies (not\nreported) that we tried in our preliminary experiments. We sur-\nmise that simply presenting di ﬀerent text snippets to a target\nbrain image is analogous to another augmentation strategy that\nallows the neural network to pool across samples and learn the\nrelevant words and their weights with respect to the target brain\nmaps. Training with the set up in 2.2 takes approximately 75\nhours on one NvidiaRTX GPU while one inference pass with\nan input query of up to 140 characters takes less than 1 second.\n2.5. Baselines\nWe compare Text2Brain to 2 di ﬀerent baselines: Neu-\nrosynth (Yarkoni et al., 2011) and Neuroquery (Dock `es et al.,\n2020). For a keyword, Neurosynth ﬁrst ﬁnds all neuroimaging\narticles that mention that keyword. Then, one statistical test per\nvoxel is performed across the activation maps corresponding to\nthose studies to determine a signiﬁcant association. Since Neu-\nrosynth was not formulated to handle multiple-word queries, for\nsuch query, we performed statistical test using activation maps\nfrom all articles that contain at least one of the keywords in the\nquery.\nNeuroquery extends Neurosynth’s vocabulary of keywords\nby including more curated keywords from lexicons such as\nMeSH, NeuroNames, and NIF (Lipscomb, 2000; Bowden and\nMartin, 1995; Gardner et al., 2008). The keyword encoding is\nobtained after performing non-negative matrix factorization of\nthe articles’ full text (as a bag of keywords) represented with\nterm frequency - inverse document frequency (TF-IDF) fea-\ntures (Salton and Buckley, 1988). A ridge regression model was\ntrained to map the text encoding to the activation. The inference\nof a keyword is smoothed by a weighed average of its most\nrelated keywords (in the TF-IDF space). For multiple-word\nqueries, the predicted activation map is obtained by averaging\nthe activation maps from all keywords in the input, weighed by\nthe coeﬃcients learnt during training.\n2.6. Evaluation Metrics\nFor thresholded target activation maps such as those com-\nputed by ALE (Eickhoﬀ et al., 2009), the predicted brain maps\nare thresholded to retain the same number of most activated\nvoxels as the target. For example, given an estimated acti-\nvation map by ALE with statistically signiﬁcant clusters of\nactivation that cover 25% of the the brain volume, the brain\nmaps predicted by Text2Brain, Neuroquery, and Neurosynth\nare also thresholded to retain the top 25% most activated vox-\nels in each map. The accuracy of prediction is measured by\nDice score (Dice, 1945) which quantiﬁes the extent of overlap\nbetween the predicted and target brain maps (details are in Sup-\nplemental Section 6.2).\nFurthermore, we use Dice scores at di ﬀerent thresholds to\nestimate the similarity between predicted and target activation\n4 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nNeurosynth Neuroquery Text2Brain\nV ocabulary Fixed Fixed Unlimited\nHandle of\ncomplex query None Lexical\nsimilarity\nSemantic\nsimilarity\nPredictive\nmodels None\nTF-IDF,\nlinear\nregression\nTransformer,\nconvolution\nTable 1. High-level comparison of approaches to meta-analytic brain maps\ngeneration\nmaps at diﬀerent levels of detail (Ngo et al., 2022). This eval-\nuation procedure is similar to that used in (Dock`es et al., 2020)\nfor a thresholded target map, but we apply the same threshold-\ning to both the target and predicted map. For example, at 5%\nthreshold (considering the 5% most activated voxels), the Dice\nscore measures the correspondence of the ﬁne-grained details\nbetween the target and predicted activation maps. At higher\nthresholds (e.g. 25%), the score captures the gross agreement\nbetween activation clusters. We also estimated the area under\nthe Dice curve (AUC) as a summary measure using approxi-\nmated integration of Dice scores across all thresholds from 5%\nup to 30%. Supplemental Figure 9 shows the Dice curve for\nan example pair of target-predicted activation maps. Note that\nthe range of thresholds in the x-axis also conveys the maximum\npercentage of the gray matter mask that has an activation in the\ntarget brain map. For example, if only a proportion of gray\nmatter mask has activation, such as the case of Neuroquery pre-\ndiction that mostly extends up to 30% of the gray matter mask\nor a sparse target activation pattern from the coordinate-based\nmeta-analysis, the x-axis range will not be extended up to 1.\nIn our experiments, all evaluation is performed in the\nMNI152 volumetric space, which is the original space of all\npredicted maps. For visualization, with activation maps that\nmostly concentrate in the cerebral cortex, the original volu-\nmetric images are transformed from MNI152 space to fs LR\nsurface space using Connetome Workbench (Van Essen et al.,\n2013) via the FreeSurfer surface space (Buckner et al., 2011;\nFischl, 2012), with isolated surface clusters of less than 20 ver-\ntices being removed (Wu et al., 2018). Activation maps with\nsigniﬁcant activation in the non-cortical parts of the brain are\nvisualized by cross-sectional slices with signiﬁcant activation\nusing Nilearn (Abraham et al., 2014).\n2.7. High-level model comparison\nText2Brain can better handle input text than prior approaches\nbecause its vocabulary is not limited to a ﬁxed pre-deﬁned set\nof words. In contrast, Neurosynth and Neuroquery rely on\nﬁxed word vocabularies and cannot predict for queries con-\nsisting of out-of-vocabulary words. Besides, Neurosynth’s and\nNeuroquery’s vocabularies are not suﬃciently extensive, cov-\nering only a fraction (under 10%) (Dock `es et al., 2020) of\nterms in relevant neuroimaging lexicons such as Cognitive At-\nlas (Poldrack and Yarkoni, 2016) and NeuroNames (Bowden\nand Martin, 1995). Text2Brain’s usage of byte-pair encoding\nenables the model to handle infrequent and out-of-vocabulary\nwords more gracefully, by breaking down those words into\ndigestable sub-word tokens (Sennrich et al., 2016). Hence,\nText2Brain’s vocabulary is open ended and can scale with train-\ning data to be unlimited in theory. Besides, Text2Brain’s train-\ning is not limited to only training set data. Text2Brain can\nleverage self-supervised learning from non-neuroimaging sci-\nentiﬁc articles, as well as neuroimaging articles that do not re-\nport activation coordinates to learn a better text-to-activation-\nmap transformation. By ﬁnetuning a SciBERT text encoder\npretrained on the larger dataset of scientiﬁc articles (including\nnon-neuroimaging articles), Text2Brain seems to converge on\nan optimum with a more useful representational space of the\ninput text. Supplemental section 6.4 shows the comparison be-\ntween the Text2Brain model that uses pretrained SciBERT text\nencoder versus a randomly initialized text encoder. Evaluation\non predicting article-average activation maps from both sets of\ntest articles in the Neuroquery dataset (similar to section 3.1)\nsuggests that pretraining beneﬁts the Text2Brain’s performance.\nFurthermore, Text2Brain uses contextualized text embeddings\nto model semantic relationship between words so it can deal\nwith nuanced queries more e ﬀectively. Methods such as Neu-\nrosynth and Neuroquery may have diﬃculty dealing with com-\nplex expressions. By simply averaging the keywords’ activa-\ntion maps to arrive at the prediction for a complex query, these\nmethods may fail to account for relationship between words in\nthe query, such as order and semantic. Lastly, while the predic-\ntive approach of Neuroquery constructs the predicted activation\nmap by modelling voxels’ activation independently, Text2Brain\ngenerates the whole-brain activation with a 3D convolutional\ndecoder that takes in the text encoding produced by the lan-\nguage model. By upsampling and computing the whole-brain\nactivation from a bottleneck, Text2Brain can better model both\nthe short and long-distance relationship between voxels.\n3. Experimental Setup\n3.1. Predict activation maps from article title\nTwo test sets were created from the Neuroquery dataset of\n13,000 studies. The ﬁrst test set consists of 1000 randomly sam-\npled articles. The second test set also consists of 1000 articles\nbut was randomly sampled such that the keywords (deﬁned by\nthe articles’ authors) do not appear in the training and valida-\ntion articles. The two test sets are labeled as easy and hard test\nsets respectively. Of the remaining articles, 1000 are randomly\nheld out as a validation set for parameters tuning. For each arti-\ncle, the article-average activation map is predicted from its title\nusing Text2Brain, as well as the Neurosynth and Neuroquery\nbaselines. Both Text2Brain and Neuroquery were trained on\nthe 10,000 articles in the training set. The Text2Brain model is\ntrained using both the articles’ titles and samples from the full-\ntext, while Neuroquery is trained on the articles’ full-text. We\nuse predictions from the publicly available Neurosynth model\nat https://neurosynth.org, which was trained on the articles’ ab-\nstracts. Note that Neurosynth is not a predictive model meant\nfor out-of-sample prediction, but for performing automated sta-\ntistical testing of associations between terms and brain loca-\ntions.\nNgo & Nguyen et al./ Medical Image Analysis (2022) 5\n3.2. Predict activation maps from contrast descriptions\n3.2.1. Individual Brain Charting (IBC) task contrasts\nThe Individual Brain Charting (IBC) project (Pinho et al.,\n2020) estimates an extensive functional atlas of the human brain\nvia fMRI data of subjects measured under a large number of\ntask conditions. In particular, the IBC dataset consists of 180\ntask contrasts measured on 12 subjects. We use the activation\nmaps provided by the IBC project to measure the predictive ac-\ncuracy of Text2Brain and the two baselines over a wide range of\nfunctional domains, given the contrast descriptions from IBC.\n3.2.2. Human Connectome Project (HCP) task contrasts\nWhile the IBC dataset o ﬀers a large number of reference\nbrain maps, the small number of subjects might make some\nresults less reliable. We also utilized the Human Connec-\ntome Project (HCP) data both for reference and a measure\nof reliability of target maps. The HCP dataset consists of\nneuroimaging data from over 1200 subjects, including task\nfMRI (tfMRI) of 86 task contrasts from 7 domains (Barch\net al., 2013), which overlap with 43 contrasts under the IBC\ndataset. We evaluate the model prediction of HCP task con-\ntrasts from their descriptions. While HCP provides detailed de-\nscriptions of task contrasts, we opt for the more concise con-\ntrast descriptions provided by the Individual Brain Charting\n(IBC) as they are more succinct and thus more favorable to the\nbaselines. The IBC contrast descriptions are extracted from\nthe metadata of the activation maps released on Neurovault\nhttps://neurovault.org/images/360528. The list of all IBC de-\nscription of HCP contrasts are included in Supplemental Ta-\nble 6.1. On the other hand, the target (ground-truth) activation\nmaps are the HCP group-average contrast maps, as the large\nnumber of subjects provides more reliable estimates of the con-\ntrast maps. In the analyses of this experiment, we use the agree-\nment between the IBC and HCP maps as a measure of reliabil-\nity. Despite using similar protocols, there are subtle diﬀerences\nbetween the IBC and HCP experiments. For instance, the orig-\ninal HCP language task was conducted in English but the cor-\nresponding language task in the IBC project was conducted in\nFrench.\n3.3. Predict representative meta-analytic brain maps\nThe automated approach to brain map generation of\nText2Brain and the 2 baselines are compared against pub-\nlished brain maps created from a manually curated set of meta-\nanalyses. In particular, 5 cognitive concepts and their cor-\nresponding activation maps of 5 representative meta-analytic\nstudies from ANIMA database (Reid et al., 2016) were selected.\nThe 5 meta-analytic studies were selected for having the most\nnumber of experiments and their di ﬀerent coverage of the hu-\nman brain. The cognitive processes of interest are visual pro-\ncessing, auditory processing, motor execution (Heckner et al.,\n2021), working memory (Rottschy et al., 2012), and pain (Xu\net al., 2020). Each study searches for published neuroimaging\nstudies that contain a set of texts queries relevant to the cogni-\ntive concept of interest. For example, in Rottschy et al. (2012),\nthe phrases to search for working memory-related studies are\n“working memory” and “short-term memory”. The same text\nqueries for discovering relevant studies in the original meta-\nanalysis were used as input to Neurosynth, Neuroquery, and\nText2Brain. Table 3.3 shows the search queries and the num-\nber of experiments included in the original meta-analysis of\nthe 5 chosen cognitive concepts. Activation maps generated\nfrom all text input queries corresponding to each cognitive con-\ncept are averaged to yield a single brain map for each model.\nThe reference brain images for comparison are the activation\nmaps released by the studies and made publicly available on\nANIMA. The reference activation maps are produced by Ac-\ntivation Likelihood Estimation (ALE) (Turkeltaub et al., 2002;\nLaird et al., 2005; Eickho ﬀ et al., 2009) and thresholded to re-\ntain only the statistically signiﬁcant clusters of activation. For\nall reference ALE maps, the cluster-level forming threshold at\nvoxel-level is p < 0.001 and cluster-level corrected threshold is\nset at p < 0.05 by the original authors (Eickho ﬀ et al., 2012).\nFor comparison, the generated brain maps are thresholded to\nkeep the same number of survived voxels as those in the refer-\nence activation maps. The accuracy of each model’s generated\nbrain map is evaluated as the Dice score between the (thresh-\nolded) generated brain map and the target (thresholded) brain\nmap (see Section 2.6).\n3.4. Evaluate robustness of model prediction to semantically-\nequivalent queries\nWith the continual improvement of our understanding of\nthe human brain and mind, neuroscientiﬁc knowledge is also\nan ever evolving repertoire. Several neuroimaging concepts\nhave also been changing, adapting and broadening over time.\nThus, we were interested in examining if our approach is ro-\nbust to semantically equivalent queries. For example, “self-\ngenerated thought”, one of the most intensively examined cog-\nnitive domains in neuroscience, has had its deﬁnition reﬁned\nand assigned di ﬀerent denominations over the years. As a\ncognitive paradigm, di ﬀerent names have been used to refer\nto the set of inward-oriented psychological processes, such as\n“self-generated thought” (Smallwood, 2013), or “task-unrelated\nthought” (Andrews-Hanna et al., 2014). Both terms are asso-\nciated with “default network” (Buckner et al., 2008), the set\nof brain regions with elevated activation when subjects are not\nsubjected to any external stimulus.\nTo assess models’ prediction of synonymous queries, we uti-\nlized the ontology from the Cognitive Atlas (Poldrack et al.,\n2011; Bilder et al., 2009). The Cognitive Atlas is a collaborative\nknowledge base for neuroscience with content such as cognitive\nconcepts, their description and synonyms (aliases) contributed\nby the project’s voluntary participants (Miller et al., 2010). At\nthe time of our experiments, Cognitive Atlas includes 885 con-\ncepts with deﬁnition, 108 of which have at least one alias. We\nconsidered a model to be robust with respect to a speciﬁc cog-\nnitive concept’s deﬁnition if the activation map predicted from\nthe description matches the predicted map from the concept’s\nname. In particular, given a model’s predicted brain maps from\nall 885 Cognitive Atlas concept names and their description, we\nassess if the model’s brain map predicted from a concept’s def-\ninition is one of the k maps (out of 886 possible maps) most\nsimilar to the model’s brain map predicted from the concept’s\n6 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nFunctional domain #Exp Search queries\nVisual processing\n(Heckner 2021) 114\nvisual processing\nface monitor\nface discrimination\nﬁlm viewing\nﬁxation\nﬂashing checkerboard\npassive viewing\nvisual object identiﬁcation\nvisual pursuit\nvisual tracking\nvisuospatial attention\nAuditory processing\n(Heckner 2021) 122\nauditory processing\ndivided auditory attention\nmusic comprehension\noddball discrimination\npassive listening\nphonological discrimination\npitch monitor\npitch discrimination\ntone monitor\ntone discrimination\nMotor execution\n(Heckner 2021) 251\nmotor execution\nwriting\nchewing\nswallowing\ndrawing\nisometric force\nmotor learning\ngrasping\nﬁnger tapping\nbutton press\nﬂexion\nextension\nWorking memory\n(Rottschy 2012) 189 working memory\nshort-term memory\nPain\n(Xu 2020) 222\npain\nnoxious\nnociception\nTable 2. Meta-analytic studies of representative functional domains. The\nstudies were selected from the ANIMA dataset (Reid et al., 2016) that have\nthe most number of experiments and covere a diverse set of brain regions.\nname. In our experiments, top-1, top-5 and top-10 matching ac-\ncuracy were evaluated using Dice AUC metrics. The di ﬀerent\nvalues of k’s account for the uncertainty of the concepts’ natu-\nral language text, e.g., diﬀerent contributors might use diﬀerent\nnames to refer to the same concept. Similarly, models’ robust-\nness with respect to a cognitive concept’s alias is measured by\nthe accuracy of matching the activation maps predicted from the\ntext of a concept’s alias and its name.\nEasy test setHard test set\np = 2.37x10-59\np = 7.51x10-126\np = 2.36x10-60\np = 2.47x10-106\nFig. 2. Evaluation of article-average activation maps predicted from their\ntitles measured in area under the Dice curve (AUC) score. The left and\nright graph show the Dice AUCs of samples from the easy and hard test\nsets, respectively (Section 3.1). The p-values are computed from paired-\nsample t-tests between Text2Brains and each of the 2 baselines.\n4. Results\n4.1. Validation of activation maps predicted from article title\nFigure 2 shows the quality of activation maps predicted from\nthe titles of 1000 articles in each of the two test sets (sec-\ntion 3.1). In the easy test set (the test articles’ keywords can\noverlap with the training articles’), the proposed Text2Brain\nmodel (mean Dice AUC = 0.0636) outperforms Neuroquery\n(mean Dice AUC = 0.0523) and Neurosynth (mean Dice AUC\n= 0.0453). In the hard test set (the test articles’ keywords are not\npresent in the training set), the Text2Brain model (mean Dice\nAUC = 0.0609) also performs better than Neuroquery (mean\nAUC = 0.0499) and Neurosynth (mean AUC= 0.0457). Paired-\nsample t-tests show that the performance diﬀerences in both test\nsets are statistically very signiﬁcant. The p-values when com-\nparing Neuroquery and Neurosynth are p = 5.25 ×10−27 and\np = 2.40 ×10−12. Fig. 2 also indicates how the diﬀerent models\nhandle out-of-sample input text. Text2Brain can make a predic-\ntion for all input texts, evident with positive Dice AUCs for all\nsamples. On the other hand, Neurosynth fails to make predic-\ntion for some article titles in both test sets, resulting in zero Dice\nAUCs for such samples. Similarly, Neuroquery fails to make\nprediction for some samples in the hard test set. These fail-\nure cases are caused by the limited vocabularies of Neurosynth\nand Neuroquery that cannot cover the words in the test input\nqueries. On the other hand, the language model of Text2Brain is\nﬁnetuned from SciBert, which has been pretrained on a broader\nlexicon and utilizes sub-word tokens to extend the vocabulary\nto unseen words (more details in Section 2.7).\n4.2. Prediction of task contrast maps from description\nFig. 3 shows the Dice AUC scores for the prediction\nof Text2Brain, Neuroquery and Neurosynth against the IBC\ngroup-average task contrast maps. Text2Brain (mean Dice AUC\nNgo & Nguyen et al./ Medical Image Analysis (2022) 7\np = 4.11x10-4\np = 1.58x10-9\nFig. 3. Dice AUCs of predicted IBC task activation maps from contrasts’\ndescription. The p-values are estimated from paired-sample t-tests be-\ntween Text2Brain against the two baselines.\nMental motion vs random motion\nMental additions\nMotion cue of motion\n2-back vs 0-back\nListening to story vs mental additions\n0-back vs 2-back\nMove tongue\nMove right foot\nMove left foot\nMove right hand\nMental additions vs listening to story\nBody image versus face, place, tool image\nPlace image versus face, body, tool image\nEmotional face comparison vs shape comparison\nMove right foot vs left foot, hands and tongue\nMove left hand\nMove left foot vs right foot, hands and tongue\nShape comparison\nMove tongue vs hands and feet\nGambling with positive outcome\nTool image versus face, place, body image\nContrast description\n0.00\n0.05\n0.10\n0.15AUC\nText2Brain Neuroquery Neurosynth IBC contrast\nFig. 4. Dice AUCs of predicted HCP task activation maps from contrasts’\ndescription. The graph includes 22 contrasts with the highest HCP-IBC’s\nDice AUC scores and sorted in decreasing order.\n= 0.0507) improves upon both Neuroquery (mean Dice AUC=\n0.0457, p = 4.11 ×10−4), and Neurosynth (mean Dice AUC =\n0.0404, p = 1.58 ×10−9). The p-values are measured by 2-tail\npaired-sample t-test between Text2Brain and the two baselines.\nFig. 4 shows the AUC scores for the prediction of the three\nmodels and the IBC average contrasts, against the HCP tar-\nget maps. The 22 contrasts with above-average HCP-IBC’s\nAUC scores, considered to be the reliable contrasts, are shown.\nAcross all 43 HCP contrasts, Text2Brain (mean AUC = 0.082)\nperforms better than the baselines, i.e. Neuroquery (mean\nAUC = 0.0755, p = 0.08), Neurosynth (mean AUC = 0.047,\np = 1.5 ×10−5), where p-values are computed from the paired\nt-test between Text2Brain’s and the baselines’ prediction. As\nreference, IBC contrasts yield a mean AUC= 0.094 when com-\npared to the corresponding HCP maps (Statistical comparison\nwith Text2Brain, p = 0.077).\nFigure 5 shows the prediction for three contrasts corre-\nspond to diﬀerent HCP task groups, namely “MOTOR”, “LAN-\nGUAGE”, “RELATIONAL” thresholded at the top 25% most\nactivated voxels. The three task groups were chosen to show\nresults for a range of target images with diﬀerent levels of relia-\nbility. The two task groups “MOTOR” and “LANGUAGE” are\nthe two most reliable task (having the highest average HCP-IBC\nAUC across all contrasts), while “RELATIONAL” has the low-\nest average HCP-IBC AUC. Text2Brain’s prediction improves\nover the baselines for the three contrasts. Neurosynth was not\nable to generate activation maps for one of the contrast de-\nscriptions (“Move tongue”). On the other hand, for the “Move\ntongue” contrast, Neuroquery predicts activation in the primary\ncortex, but the peak is in the wrong location, shifted more to-\nward the hand region of the homunculus. Additionally, there is\na false positive prediction in the occipital cortex, which might\nbe an artifact from modeling brain activation coupled with vi-\nsual stimuli-related words describing the motor experiments.\n4.3. Prediction of brain maps from representative meta-\nanalytic studies\nFigure 6 shows the prediction of activation maps for 5 rep-\nresentative meta-analytic studies with the most number of ex-\nperiments from ANIMA (Reid et al., 2016). Among the three\nmodels, Neuroquery has the lowest Dice score on average, with\nprediction on “Visual processing”, “Working memory”, and\n“Pain” that signiﬁcantly deviates from the target maps. On the\nother hand, Neurosynth-derived brain maps consistently match\nwell against the target maps. The high accuracy of Neurosynth\nprediction is expected since the ﬁve chosen cognitive concepts\nare among the most commonly studied concepts with the most\nnumber of experiments reporting activation coordinates in the\nliterature. Given high number of available experiments and the\ninput queries mostly exist in Neurosynth’s predeﬁned keyword\nset, the activation coordinates scraped by automated method by\nNeurosynth would be very similar to the manually curated data\nin the original meta-analysis. Lastly, Text2Brain also predicts\nconsistently reasonable brain maps for all ﬁve cognitive con-\ncepts, and matches the target maps better than Neurosynth for\n“Visual Processing” and “Pain”. Results in Figure 6 shows that\nText2Brain could learn appropriate relationship between com-\nmon search phrases and the activation pattern of a diverse set of\nfunctional domains.\n5. Robustness of models to input queries\n5.1. Example of “self-generated thought” synonyms\nWe examine the prediction for “self-generated thought”,\nwhich is one of the most extensively investigated functional\ndomains, due to its involvement in a wide range of cogni-\ntive processes that do not require external stimuli (Andrews-\nHanna et al., 2014), and is associated with the default net-\nwork (Buckner et al., 2008). The ground-truth map for self-\ngenerated thought, taken from (Ngo et al., 2019), is estimated\nusing activation likelihood estimation (ALE) (Eickho ﬀ et al.,\n2009) applied on activation foci across 167 imaging studies of\n7 tasks selected based on strict criteria (Spreng et al., 2009;\nMar, 2011; Sevinc and Spreng, 2014). The resulting ALE\nmap is thresholded with the cluster-level forming threshold at\nvoxel-level p < 0.001, and cluster-level corrected threshold\n8 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nDice = 0.059Dice = 0.598Dice = 0.560Dice = 0.574Visual featurematching vs fixation\n1x10-4 4x10-4\n 1x10-4 8x10-4\n 1x10-5 6x10-4\n2x10-5 2x10-3\n 4x10-5 6x10-4\nNeurosynth\n2x10-4 1x10-3\n 1x10-4 2x10-3\n 1x10-4 1x10-3\n 8x10-5 1x10-3Move tongueDice = 0.543Dice = 0.471Dice = 0.395\nHCP targetText2BrainNeuroqueryIBC contrast\nTarget-only Prediction-onlyOverlap\nDice = 0.590Dice = 0.423Dice = 0.052Dice = 0.354Listening to storyvs mental additions\n7x10-5 6x10-4\n 1x10-4 1x10-3\n 2x10-5 1x10-3\n8x10-5 8x10-4\n 7x10-5 5x10-4\nFig. 5. Task activation maps predicted from contrasts’ description. Each row shows both the thresholded maps of the top 25% most activated voxels (top)\nand the overlap between predicted and target binarized brain maps. Blue is activation in the target contrast, red is the predicted activation and yellow is\nthe overlap.\nNgo & Nguyen et al./ Medical Image Analysis (2022) 9\nNeurosynthALE targetText2BrainNeuroquery\nDice = 0.447Dice = 0.118Dice = 0.359\nVisualprocessing\n1x10-3 5x10-2\n 5x10-4 1.9x10-3\n 5x10-4 1.7x10-3\n 5x10-4 1.9x10-3\nAuditoryprocessing\nDice = 0.525Dice = 0.535Dice = 0.587\n1x10-3 1.4x10-1\n 4x10-4 1.7x10-3\n 5x10-4 2x10-3\n 5x10-4 2x10-3\nMotorexecution\nDice = 0.480Dice = 0.482Dice = 0.634\n1x10-3 2.2x10-1\n 10-4 10-3\n 10-4 10-3\n 10-4 10-3\nWorkingmemory\nDice = 0.489Dice = 0.220Dice = 0.509\n1x10-3 8.3\n 10-4 1.5x10-3\n 10-4 1.4x10-3\n 10-4 1.3x10-3\nPain\ny = 5x = 5\nL R\nDice = 0.636Dice = 0.364Dice = 0.566\n1x10-3 8.5\n 10-4 1.1x10-3\n 10-4 1.2x10-3\n10-4 1.1x10-3\nTarget-only Prediction-onlyOverlap\nFig. 6. Prediction of brain maps from meta-analytic studies of representative functional domains. The information of the investigated functional domains\nare listed in Table 3.3. Reference and predicted activation maps of the ﬁrst 4 function domains are visualized on the brain surface. The last domain\n(“pain”) is visualized in the volume as most activation concentrates in the non-cortical parts of the brain. For all functional domains, Text2Brain generates\nreasonable activation maps and comparable with the baselines for the common functional domains.\n10 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nALE ground truthfor Self-generatedthought\n6x10-5\n2x10-4\nNeuroqueryNeurosynthText2Brain\nDefault network\n 5x10-5\n1x10-3\nDice = 0.583Dice = 0.428Dice = 0.359\n1x10-4\n1x10-3\n1x10-4\n6x10-4\nSelf-generatedthought\nDice = 0.580Dice = 0.472Dice = 0.391\n1x10-4\n8x10-4\n1x10-4\n6x10-4\n5x10-5\n1x10-3\nTask-unrelatedthought\nDice = 0.455Dice = 0.172Dice = 0.122\n1x10-4\n6x10-4\n1x10-4\n6x10-4\n8x10-5\n8x10-4\nDice = 0.522Dice = 0.246Dice = 0.417Internally-directedthought\n 8x10-5\n1x10-3\n1x10-4\n7x10-4\n1x10-4\n5x10-4\nFig. 7. Prediction of self-generated thought activation map using synonymous queries. While Text2Brain generates consistent prediction across the similar\nqueries, Neurosynth and Neuroquery’s prediction deteriorate on the “internally-directed thought” and “task-unrelated thought” queries.\np < 0.05 (Eickhoﬀ et al., 2012). Figure 7 shows the predic-\ntion of self-generated thought activation map using four dif-\nferent query terms, thresholded to retain the same number of\nactivated voxels as the target map.\nAcross all four queries, Text2Brain’s prediction best matches\nthe ground-truth activation map compared to the baselines. For\nthe “self-generated thought” and “default network” queries, all\napproaches generate activation maps that are consistent with the\nground-truth, which includes the precuneus, the medial pre-\nfrontal cortex, the temporo-parietal junction, and the tempo-\nral pole. Text2Brain and Neuroquery both make reasonable\nprediction from the “internally-directed thought” query while\nNeurosynth’s prediction is largely scattered and does not match\nthe target map. Lastly, Text2Brain can also replicate a similar\nactivation pattern to the target from the query “task-unrelated\nthought”, evident by only a slight drop in the Dice score.\nHowever, Neuroquery and Neurosynth both generate activation\nmaps that diﬀer from the typical default network’s regions, such\nas activation in the prefrontal cortex, and also result in a large\ndrop of the Dice scores.\n5.2. Prediction of Cognitive Atlas concepts from synonymous\nqueries\nFigure 8 shows the accuracy of matching cognitive concept\nnames from the Cognitive Atlas (Poldrack et al., 2011) with\ntheir deﬁnitions and atlases using the di ﬀerent models’ pre-\ndicted brain maps. Prediction by Text2Brain is more robust\nNgo & Nguyen et al./ Medical Image Analysis (2022) 11\nB. Match concept definition with nameA. Match concept alias with name\nFig. 8. Accuracy of matching Cognitve Atlas concept names with their de-\nscription and aliases using models’ predicted brain maps.\nthan both Neuroquery and Neurosynth with respect to the con-\ncept deﬁnition and alias. In particular, Text2Brain has the\nsame top-1 accuracy of matching the brain map predicted from\na concept’s alias with the prediction from the concept name\ncompared to Neurosynth. This result is expected given that\nNeurosynth can yield accurate brain map for keywords that\nare included in their vocabulary. In contrast, Text2Brain im-\nproves over Neurosynth for top-1 accuracy of matching concept\nname with the longer text of concept deﬁnition. Text2Brain is\nmore robust than both Neurosynth and Neuroquery baselines\nin terms of top-5 and top-10 matching accuracies for both con-\ncept aliases and deﬁnitions. Figure 8 indicates that Text2Brain\nprediction is robust to natural language text queries of diﬀerent\nlength and complexity.\n6. Conclusion\nIn this work, we present a model named Text2Brain for\ngenerating activation maps from free-form text query. By\nﬁnetuning a high-capacity SciBert-based text encoder to pre-\ndict coordinate-based meta-analytic maps, Text2Brain captures\nthe rich relationship in the language representational space,\nallowing the model to generalize its prediction for synony-\nmous queries. This is evident in the better performance of\nText2Bran in predicting the self-generated thought activation\nmap using di ﬀerent descriptions of the functional domain.\nText2Brain’s capability to implicitly learn relationships be-\ntween textual terms and images ensures the model can remain\nrelevant and useful even as neuroimaging literature continues\nto evolve with new discoveries and rephrasing of existing con-\ncepts. We also show that Text2Brain accurately predicts most\nof the task contrasts included in the IBC and HCP dataset, val-\nidating its capability to make prediction for longer, arbitrary\nqueries. Text2Brain also preempts failure cases in Neurosynth\nand Neuroquery, where they cannot predict input queries unde-\nﬁned in the vocabulary list, even though these queries are rele-\nvant to neuroscience research (e.g. title of an article). On the\nother hand, we also observed that Text2Brain had di ﬃculties\nhandling queries that involve logical reasoning, such as the di-\nrection of a contrast. For example, while queries such as “A vs\nB” and “B vs A” can be inferred by human to correspond with\ninverted activation maps, Text2Brain sometimes treats one di-\nrection to be the same as the other. We suspect that this type of\nerror is likely due to the model’s inability to generalize “vs” as\nan “subtractive” operator. Resolving such limitation will likely\nrequire modiﬁcations to the language model. Furthore, in the\nfuture, we plan to enhance the interpretability of our approach,\nsuch as to attribute regions of activations in the generated map\nto speciﬁc words in the input query, as well as to e ﬃciently\nmatch activation maps and scientiﬁc descriptions most relevant\nto the synthesized images.\nWe believe that the ﬂexibility of Text2Brain can signiﬁcantly\nlower the barrier for researchers at all stages of their careers\nto synthesize brain activation maps needed for their research.\nFor example, the ability of Text2Brain to generate meaningful\nneural activation patterns of synonymous queries for a func-\ntional domain can improve the accuracy of delineating region-\nof-interests (ROIs) relevant to the functional process, as well as\nto assess the reliability of each ROI. Discovery of these ROIs is\nuseful for several applications such as meta-analytic connectiv-\nity modeling (MACM) (Laird et al., 2013). We look forward to\nsuch application of Text2Brain in aiding future neuroscientiﬁc\nresearch.\nAcknowledgement\nThis work was supported by NIH grants R01LM012719,\nR01AG053949, the NSF NeuroNex grant 1707312, the NSF\nCAREER 1748377 grant and Jacobs Scholar Fellowship.\nReferences\nAbraham, A., Pedregosa, F., Eickenberg, M., Gervais, P., Mueller, A., Kossaiﬁ,\nJ., Gramfort, A., Thirion, B., Varoquaux, G., 2014. Machine learning for\nneuroimaging with scikit-learn. Frontiers in neuroinformatics 8, 14.\nAndrews-Hanna, J.R., Smallwood, J., Spreng, R.N., 2014. The default net-\nwork and self-generated thought: component processes, dynamic control,\nand clinical relevance. Annals of the New York Academy of Sciences 1316,\n29.\nBarch, D.M., Burgess, G.C., Harms, M.P., Petersen, S.E., Schlaggar, B.L., Cor-\nbetta, M., Glasser, M.F., Curtiss, S., Dixit, S., Feldt, C., et al., 2013. Function\nin the human connectome: task-fMRI and individual diﬀerences in behavior.\nNeuroimage 80, 169–189.\nBeltagy, I., Lo, K., Cohan, A., 2019. SciBERT: A pretrained language model\nfor scientiﬁc text. arXiv preprint arXiv:1903.10676 .\nBilder, R.M., Sabb, F.W., Parker, D.S., Kalar, D., Chu, W.W., Fox, J., Freimer,\nN.B., Poldrack, R.A., 2009. Cognitive ontologies for neuropsychiatric phe-\nnomics research. Cognitive neuropsychiatry 14, 419–450.\nBowden, D.M., Martin, R.F., 1995. Neuronames brain hierarchy. Neuroimage\n2, 63–83.\nBuckner, R.L., Andrews-Hanna, J.R., Schacter, D.L., 2008. The brain’s default\nnetwork: anatomy, function, and relevance to disease. .\nBuckner, R.L., Krienen, F.M., Castellanos, A., Diaz, J.C., Yeo, B.T., 2011.\nThe organization of the human cerebellum estimated by intrinsic functional\nconnectivity. Journal of neurophysiology 106, 2322–2345.\nButton, K.S., Ioannidis, J.P., Mokrysz, C., Nosek, B.A., Flint, J., Robinson,\nE.S., Munaf `o, M.R., 2013. Power failure: why small sample size under-\nmines the reliability of neuroscience. Nature reviews neuroscience 14, 365–\n376.\nCarp, J., 2012. The secret lives of experiments: methods reporting in the fMRI\nliterature. Neuroimage 63, 289–300.\n12 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nChurch, J.A., Petersen, S.E., Schlaggar, B.L., 2010. The “Task B problem”\nand other considerations in developmental functional neuroimaging. Human\nbrain mapping 31, 852–862.\nCostafreda, S.G., Brammer, M.J., David, A.S., Fu, C.H., 2008. Predictors of\namygdala activation during the processing of emotional stimuli: a meta-\nanalysis of 385 pet and fmri studies. Brain research reviews 58, 57–70.\nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 .\nDice, L.R., 1945. Measures of the amount of ecologic association between\nspecies. Ecology 26, 297–302.\nDock`es, J., Poldrack, R.A., Primet, R., G ¨oz¨ukan, H., Yarkoni, T., Suchanek,\nF., Thirion, B., Varoquaux, G., 2020. NeuroQuery, comprehensive meta-\nanalysis of human brain mapping. Elife 9, e53385.\nEickhoﬀ, S.B., Bzdok, D., Laird, A.R., Kurth, F., Fox, P.T., 2012. Activation\nlikelihood estimation meta-analysis revisited. Neuroimage 59, 2349–2361.\nEickhoﬀ, S.B., Laird, A.R., Grefkes, C., Wang, L.E., Zilles, K., Fox, P.T.,\n2009. Coordinate-based activation likelihood estimation meta-analysis of\nneuroimaging data: A random-eﬀects approach based on empirical estimates\nof spatial uncertainty. Human brain mapping 30, 2907–2926.\nFischl, B., 2012. Freesurfer. Neuroimage 62, 774–781.\nGardner, D., Akil, H., Ascoli, G.A., Bowden, D.M., Bug, W., Donohue, D.E.,\nGoldberg, D.H., Grafstein, B., Grethe, J.S., Gupta, A., et al., 2008. The\nneuroscience information framework: a data and knowledge environment\nfor neuroscience. Neuroinformatics 6, 149–160.\nHeckner, M.K., Cieslik, E.C., K ¨uppers, V ., Fox, P.T., Eickhoﬀ, S.B., Langner,\nR., 2021. Delineating visual, auditory and motor regions in the human brain\nwith functional neuroimaging: a brainmap-based meta-analytic synthesis.\nScientiﬁc reports 11, 1–11.\nJawahar, G., Sagot, B., Seddah, D., 2019. What does bert learn about the struc-\nture of language?, in: ACL 2019-57th Annual Meeting of the Association\nfor Computational Linguistics.\nKamnitsas, K., Ledig, C., Newcombe, V .F., Simpson, J.P., Kane, A.D., Menon,\nD.K., Rueckert, D., Glocker, B., 2017. E ﬃcient multi-scale 3d cnn with\nfully connected crf for accurate brain lesion segmentation. Medical image\nanalysis 36, 61–78.\nLaird, A.R., Eickhoﬀ, S.B., Rottschy, C., Bzdok, D., Ray, K.L., Fox, P.T., 2013.\nNetworks of task co-activations. Neuroimage 80, 505–514.\nLaird, A.R., Fox, P.M., Price, C.J., Glahn, D.C., Uecker, A.M., Lancaster, J.L.,\nTurkeltaub, P.E., Kochunov, P., Fox, P.T., 2005. ALE meta-analysis: Con-\ntrolling the false discovery rate and performing statistical contrasts. Human\nbrain mapping 25, 155–164.\nLancaster, J.L., Tordesillas-Guti´errez, D., Martinez, M., Salinas, F., Evans, A.,\nZilles, K., Mazziotta, J.C., Fox, P.T., 2007. Bias between MNI and Talairach\ncoordinates analyzed using the ICBM-152 brain template. Human brain\nmapping 28, 1194–1205.\nLipscomb, C.E., 2000. Medical subject headings (mesh). Bulletin of the Med-\nical Library Association 88, 265.\nLoshchilov, I., Hutter, F., 2018. Decoupled Weight Decay Regularization, in:\nProceedings of ICLR.\nMar, R.A., 2011. The neural bases of social cognition and story comprehension.\nAnnual review of psychology 62, 103–134.\nMiller, E., Seppa, C., Kittur, A., Sabb, F., Poldrack, R., 2010. The cognitive\natlas: employing interaction design processes to facilitate collaborative on-\ntology creation. Nature Precedings , 1–1.\nMilletari, F., Navab, N., Ahmadi, S.A., 2016. V-net: Fully convolutional neu-\nral networks for volumetric medical image segmentation, in: 2016 fourth\ninternational conference on 3D vision (3DV), IEEE. pp. 565–571.\nMinzenberg, M.J., Laird, A.R., Thelen, S., Carter, C.S., Glahn, D.C., 2009.\nMeta-analysis of 41 functional neuroimaging studies of executive function\nin schizophrenia. Archives of general psychiatry 66, 811–822.\nNgo, G.H., Eickho ﬀ, S.B., Nguyen, M., Sevinc, G., Fox, P.T., Spreng, R.N.,\nYeo, B.T., 2019. Beyond consensus: embracing heterogeneity in curated\nneuroimaging meta-analysis. NeuroImage 200, 142–158.\nNgo, G.H., Khosla, M., Jamison, K., Kuceyeski, A., Sabuncu, M.R., 2022. Pre-\ndicting individual task contrasts from resting-state functional connectivity\nusing a surface-based convolutional network. NeuroImage 248, 118849.\nNgo, G.H., Nguyen, M., Chen, N.F., Sabuncu, M.R., 2021. Text2brain: Syn-\nthesis of brain activation maps from free-form text query, in: International\nConference on Medical Image Computing and Computer-Assisted Interven-\ntion, Springer. pp. 605–614.\nPetroni, F., Rockt¨aschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y ., Miller,\nA., 2019. Language Models as Knowledge Bases?, in: Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pp. 2463–2473.\nPinho, A.L., Amadon, A., Gauthier, B., Clairis, N., Knops, A., Genon, S.,\nDohmatob, E., Torre, J.J., Ginisty, C., Becuwe-Desmidt, S., et al., 2020. In-\ndividual Brain Charting dataset extension, second release of high-resolution\nfMRI data for cognitive mapping. Scientiﬁc Data 7, 1–16.\nPoldrack, R.A., Kittur, A., Kalar, D., Miller, E., Seppa, C., Gil, Y ., Parker, D.S.,\nSabb, F.W., Bilder, R.M., 2011. The cognitive atlas: toward a knowledge\nfoundation for cognitive neuroscience. Frontiers in neuroinformatics 5, 17.\nPoldrack, R.A., Yarkoni, T., 2016. From brain maps to cognitive ontologies: in-\nformatics and the search for mental structure. Annual review of psychology\n67, 587–612.\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\nY ., Li, W., Liu, P.J., 2020. Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. Journal of Machine Learning Research 21,\n1–67.\nReid, A.T., Bzdok, D., Genon, S., Langner, R., M ¨uller, V .I., Eickhoﬀ, C.R.,\nHoﬀstaedter, F., Cieslik, E.C., Fox, P.T., Laird, A.R., et al., 2016. Anima:\nA data-sharing initiative for neuroimaging meta-analyses. Neuroimage 124,\n1245–1253.\nRottschy, C., Langner, R., Dogan, I., Reetz, K., Laird, A.R., Schulz, J.B., Fox,\nP.T., Eickhoﬀ, S.B., 2012. Modelling neural correlates of working memory:\na coordinate-based meta-analysis. Neuroimage 60, 830–846.\nRubin, T.N., Koyejo, O., Gorgolewski, K.J., Jones, M.N., Poldrack, R.A.,\nYarkoni, T., 2017. Decoding brain activity using a large-scale probabilis-\ntic functional-anatomical atlas of human cognition. PLoS computational\nbiology 13, e1005649.\nSalton, G., Buckley, C., 1988. Term-weighting approaches in automatic text\nretrieval. Information processing & management 24, 513–523.\nSennrich, R., Haddow, B., Birch, A., 2016. Neural machine translation of rare\nwords with subword units, in: Proceedings of ACL. doi: 10.18653/v1/\nP16-1162.\nSevinc, G., Spreng, R.N., 2014. Contextual and perceptual brain processes\nunderlying moral cognition: a quantitative meta-analysis of moral reasoning\nand moral emotions. PloS one 9, e87427.\nShackman, A.J., Salomons, T.V ., Slagter, H.A., Fox, A.S., Winter, J.J., David-\nson, R.J., 2011. The integration of negative aﬀect, pain and cognitive control\nin the cingulate cortex. Nature Reviews Neuroscience 12, 154–167.\nSmallwood, J., 2013. Distinguishing how from why the mind wanders: a\nprocess–occurrence framework for self-generated mental activity. Psycho-\nlogical bulletin 139, 519.\nSpreng, R.N., Mar, R.A., Kim, A.S., 2009. The common neural basis of autobi-\nographical memory, prospection, navigation, theory of mind, and the default\nmode: a quantitative meta-analysis. Journal of cognitive neuroscience 21,\n489–510.\nTenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R.T., Kim, N.,\nVan Durme, B., Bowman, S.R., Das, D., et al., 2019. What do you learn\nfrom context? Probing for sentence structure in contextualized word repre-\nsentations. arXiv preprint arXiv:1905.06316 .\nTurkeltaub, P.E., Eden, G.F., Jones, K.M., Zeﬃro, T.A., 2002. Meta-analysis of\nthe functional neuroanatomy of single-word reading: method and validation.\nNeuroimage 16, 765–780.\nVan Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E., Yacoub, E., Ugurbil,\nK., Consortium, W.M.H., et al., 2013. The wu-minn human connectome\nproject: an overview. Neuroimage 80, 62–79.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I., 2017. Attention is all you need. arXiv preprint\narXiv:1706.03762 .\nWager, T.D., Lindquist, M.A., Nichols, T.E., Kober, H., Van Snellenberg, J.X.,\n2009. Evaluating the consistency and speciﬁcity of neuroimaging data using\nmeta-analysis. Neuroimage 45, S210–S221.\nWu, J., Ngo, G.H., Greve, D., Li, J., He, T., Fischl, B., Eickhoﬀ, S.B., Yeo, B.T.,\n2018. Accurate nonlinear mapping between mni volumetric and freesurfer\nsurface coordinate systems. Human brain mapping 39, 3793–3808.\nXu, A., Larsen, B., Baller, E.B., Scott, J.C., Sharma, V ., Adebimpe, A., Bas-\nbaum, A.I., Dworkin, R.H., Edwards, R.R., Woolf, C.J., et al., 2020. Conver-\ngent neural representations of experimentally-induced acute pain in healthy\nvolunteers: A large-scale fmri meta-analysis. Neuroscience & biobehavioral\nreviews 112, 300–323.\nYarkoni, T., Poldrack, R.A., Nichols, T.E., Van Essen, D.C., Wager, T.D., 2011.\nNgo & Nguyen et al./ Medical Image Analysis (2022) 13\nLarge-scale automated synthesis of human functional neuroimaging data.\nNature methods 8, 665–670.\nYeo, B.T., Krienen, F.M., Eickho ﬀ, S.B., Yaakub, S.N., Fox, P.T., Buckner,\nR.L., Asplund, C.L., Chee, M.W., 2015. Functional specialization and ﬂex-\nibility in human association cortex. Cerebral cortex 25, 3654–3672.\n14 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nSupplementary Material\n6.1. IBC description of HCP task contrasts\nNgo & Nguyen et al./ Medical Image Analysis (2022) 15\nTask Contrast label Contrast description\nLANGUAGE\nMATH Mental additions\nSTORY Listening to story\nMATH-STORY Mental additions vs listening to story\nSTORY-MATH Listening to story vs mental additions\nRELATIONAL\nMATCH Visual feature matching vs ﬁxations\nREL Relational comparison vs ﬁxation\nREL-MATCH Relational comparison vs matching\nSOCIAL\nRANDOM Random motion vs ﬁxation\nTOM Mental motion vs ﬁxation\nTOM-RANDOM Mental motion vs random motion\nEMOTION\nFACES Emotional face comparison\nSHAPES Shape comparison\nFACES-SHAPES Emotional face comparison vs shape comparison\nWM\n2BK BODY Body image 2-back task vs ﬁxation\n2BK FACE Face image 2-back task vs ﬁxation\n2BK PLACE Place image 2-back task vs ﬁxation\n2BK TOOL Tool image 2-back task vs ﬁxation\n0BK BODY Body image 0-back task vs ﬁxation\n0BK FACE Face image 0-back task vs ﬁxation\n0BK PLACE Place image 0-back task vs ﬁxation\n0BK TOOL Tool image 0-back task vs ﬁxation\n0BK-2BK 0-back vs 2-back\n2BK-0BK 2-back vs 0-back\nBODY-A VG Body image versus face, place, tool image\nFACE-A VG Face image versus body, place, tool image\nPLACE-A VG Place image versus face, body, tool image\nTOOL-A VG Tool image versus face, place, body image\nMOTOR\nCUE Motion cue of motion\nLF Move left foot\nLH Move left hand\nRF Move right foot\nRH Move right hand\nT Move tongue\nLF-A VG Move left foot vs right foot, hands and tongue\nLH-A VG Move left hand vs right hand, feet and tongue\nRF-A VG Move right foot vs left foot, hands and tongue\nRH-A VG Move right hand vs left hand, feet and tongue\nT-A VG Move tongue vs hands and feet, Move left hand\nGAMBLING\nPUNISH Negative gambling outcome\nREW ARD Gambling with positive outcome\nPUNISH-REW ARD Negative versus positive gambling outcome\nTable 3. Individual Brain Charting (IBC) description of Human Connectome Project (HCP) task contrasts\n16 Ngo & Nguyen et al./ Medical Image Analysis (2022)\n6.2. Evaluation Metrics\nDice score (Dice, 1945) is used to measure the extent of over-\nlap between a predicted activation map and the target activation\nmap at a given threshold. At a given threshold of x%, Dice\nscore is computed as:\nDice(x) = 2|Prediction(x) ∩Target (x)|\n|Prediction(x)|+ |Target (x)|, (1)\nwhere |Prediction(x)|denotes the number of top x% most ac-\ntivated voxels in the predicted activation map, |Target (x)|de-\nnotes the number of top x% most activated voxels in the tar-\nget map, and |Prediction(x) ∩Target (x)|denotes the number of\nvoxels that overlap between the predicted and target map at the\ngiven threshold.\nDice = 0.490\nDice = 0.503\nDice = 0.534\nDice = 0.422\nDice = 0.433\nDice = 0.455\nDice = 0.321\nDice = 0.372\nDice = 0.394\nText2BrainIBC contrast Neuroquery\nAUC = 0.090\nAUC = 0.111AUC = 0.130\nFig. 9. Example Dice scores evaluated on the “Move tongue” contrast (in\nFig.5. The graph on the left shows the Dice scores computed between the\ntarget HCP activation map and Text2Brain’s, Neurosynth’s, Neuroquery’s\nprediction, and the IBC contrast across thresholds ranging from 5% to\n30%. Note that Neurosynth’s Dice scores are all zeros as it fails to make\nan inference for the input text. The area under the Dice curve (AUC) was\ncomputed as the summary metrics of accuracy across all thresholds (e.g.\nFig 4). The Dice AUC by Neuroquery, Text2Brain, and IBC are 0.090,\n0.111, 0.130, respectively. The brain maps on the right are visualization of\nthe extent of overlaps between predicted and target maps at 10%, 20% and\n30% threshold of most activated voxels. Blue indicates activation in the\ntarget contrast, red is the predicted activation and yellow is the overlap.\n6.3. Ablation study of sampling strategy\nText samples Mean AUC\nTitle + Table caption 0.0648\nTitle + Abstract + Table caption 0.0616\nDiscussion + Abstract 0.0631\nDiscussion + Abstract + Keywords 0.0651\nTitle + Abstract + Keywords +\nDiscussion + Table caption 0.0663\nTable 4. Performance of diﬀerent sampling strategies in predicting article-\naverage activation maps from the articles’ titles in the validation set\n6.4. Ablation study of text encoding pretraining\n0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16\nAUC\nTitle + Table caption\nTitle + Abstract + Table caption\nDiscussion + Abstract\nDiscussion + Abstract + Keywords\nTitle + Abstract + Keywords + Discusison + Table captionmodel\nFig. 10. Performance of diﬀerent sampling strategies in predicting article-\naverage activation maps from the articles’ titles in the validation set. All\nsampling strategies used the same model described in 2.2. The model pa-\nrameters used for evaluation were chosen at the epoch with the best per-\nformance on the validation set.\nStrategy Easy test\nDice AUC\nHard test\nDice AUC\nWith text encoder pretraining 0.0664 0.0609\nNo encoder pretraining 0.0603 0.0581\nNo encoder + tokenizer\npretraining 0.0601 0.0580\nTable 5. Eﬀect of pretraining on predicting article-average activation maps\nfrom the articles’ titles in the two test sets of Section 3.1.\nEasy test setHard test set\np = 9.61x10-20p = 3.75x10-21 p = 1.11x10-13p = 1.89x10-16\nFig. 11. Eﬀect of pretraining on predicting article-average activation maps\nfrom the articles’ titles in the two test sets of Section 3.1. The text encoder\nof the full model was pretrained on non-neuroimaging articles. In the “No\nencoder pretraining” setup, the text encoder’s weights were only trained\non neuroimaging articles from the Neuroquery dataset, and not on non-\nneuroimaging articles. In the “No encoder + tokenizer pretraining” setup,\nthe tokenizer was also not trained on non-neuroimaging articles. The p-\nvalues are estimated from paired-sample t-tests between the full setup with\nthe text encoder’s pretraining against the two setups without pretraining.\nNgo & Nguyen et al./ Medical Image Analysis (2022) 17\n6.5. Eﬀect of smoothness preprocessing\nFWHM of smoothing Gaussian spheres Mean AUC\n5 mm 0.491\n9 mm 0.507\n15 mm 0.474\np = 3.30x10-6\np = 3.40x10-9\nFig. 12. Dice AUCs of predicted IBC task activation maps from contrasts’\ndescription by Text2Brain with di ﬀerent full-width half-max (FWHM) of\nthe Gaussian spheres used for preprocessing the Neuroquery training data.\nThe p-values are estimated from paired-sample t-tests between the 9mm-\nFWHM setup against the two 5mm-FWHM and 15mm-FWHM setups.\n6.6. Eﬀect of HCP contrast’s description length on model pre-\ndictive accuracy\nModel Input type Mean AUC\nText2Brain IBC (short) 0.083\nHCP (long) 0.076 (∆ = −0.007)\nNeuroquery IBC (short) 0.078\nHCP (long) 0.061 (∆ = −0.017)\nNeurosynth IBC (short) 0.072\nHCP (long) 0.067 (∆ = −0.005)\nTable 6. Dice AUCs of predicted HCP task activation maps from contrasts’\nIBC-based description and HCP-based description. ∆s are the diﬀerences\nin Dice AUC between the two types of input.\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\nAUC\nText2Brain\nText2Brain (long input)\nNeuroquery\nNeuroquery (long input)\nNeurosynth\nNeurosynth (long input)model\nFig. 13. Comparison of predicted HCP task activation maps from con-\ntrasts’ IBC-based description and HCP-based description (Table 6.6).\n18 Ngo & Nguyen et al./ Medical Image Analysis (2022)\nTask Contrast\nLanguage Processing task consists of two runs that\neach interleave 4 blocks of a story task and 4 blocks\nof a math task. The goal of including the math\nblocks was to provide a comparison task that was\nattentionally demanding, similar in auditory and\nphonological input, and unlikely to generate\nactivation of anterior temporal lobe regions involved\nin semantic processing, though likely to engage\nnumerosity related processing in the parietal cortex.\nSTORY: The story blocks present participants with brief auditory\nstories (5–9 sentences) adapted from Aesop’s fables, followed by\na 2-alternative forced-choice question that asks participants about\nthe topic of the story.\nMATH: The math task presents trials auditorily and requires sub-\njects to complete addition and subtraction problems. The trials\npresent subjects with a series of arithmetic operations, followed\nby “equals” and then two choices. Participants push a button to\nselect either the ﬁrst or the second answer. The math task is adap-\ntive to maintain a similar level of diﬃculty across participants.\nRELATIONAL PROCESSING task localizes\nactivation in anterior prefrontal cortex in individual\nsubjects. The stimuli are 6 diﬀerent shapes ﬁlled\nwith 1 of 6 diﬀerent textures.\nREL: In the relational processing condition, participants are pre-\nsented with 2 pairs of objects, with one pair at the top of the screen\nand the other pair at the bottom of the screen. They are told that\nthey should ﬁrst decide what dimension diﬀers across the top pair\nof objects (shape or texture) and then they should decide whether\nthe bottom pair of objects also di ﬀer along that same dimension\n(e.g., if the top pair di ﬀers in shape, does the bottom pair also\ndiﬀer in shape).\nMATCH: In the control matching condition, participants are\nshown two objects at the top of the screen and one object at the\nbottom of the screen, and a word in the middle of the screen (either\n“shape” or “texture”). They are told to decide whether the bottom\nobject matches either of the top two objects on that dimension\n(e.g., if the word is “shape”, is the bottom object the same shape\nas either of the top two objects).\nSocial Cognition (Theory of Mind) is an engaging\nand validated video task was chosen as a measure of\nsocial cognition, given evidence that it generates\nrobust task related activation in brain regions\nassociated with social cognition and is reliable\nacross subjects.\nTheory of Mind: an interaction that appears as if the shapes are\ntaking into account each other’s feelings and thoughts.\nRandom: there is no obvious interaction between the shapes and\nthe movement appears random).\nWORKING MEMORY task embeds the category\nspeciﬁc representations component within the work-\ning memory task, by presenting blocks of trials that\nconsisted of pictures of faces, places, tools and body\nparts.\n[stimulus type] contrast: [stimulus type] vs. ﬁxation, collapsing\nacross memory load\n[stimulus type] vs A VG: stimulus type versus all other stimulus\ntypes\n[stimulus type] can be one of the following “body, faces, places,\ntools”\nMOTOR task identiﬁes e ﬀector speciﬁc activations\nin individual subjects. Participants are presented\nwith visual cues that ask them to tap their left or right\nﬁngers, squeeze their left or right toes, or move their\ntongue to map motor areas.\n[movement type]: linear contrasts were computed to estimate ac-\ntivation for [movement type] versus baseline\n[movement type – A VG]: linear contrasts were computed to es-\ntimate activation for [movement type] versus all other movement\ntypes\n[movement type] can be one of the following “left hand, right\nhand, left foot, right foot, tongue”\nTable 7. Long descriptions of Human Connectome Project (HCP) task contrasts. The contrast descriptions are constructed from the original full-text\nin (Barch et al., 2013). We tried to stay consistent with the original text and only included contrast descriptions that do not require signiﬁcant editing of\nthe original text.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.782802939414978
    },
    {
      "name": "Transformer",
      "score": 0.7416254281997681
    },
    {
      "name": "Neuroimaging",
      "score": 0.7190830111503601
    },
    {
      "name": "Encoder",
      "score": 0.6410502791404724
    },
    {
      "name": "Language model",
      "score": 0.5804858207702637
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5577003955841064
    },
    {
      "name": "Artificial neural network",
      "score": 0.5064100027084351
    },
    {
      "name": "Natural language processing",
      "score": 0.45872020721435547
    },
    {
      "name": "Cognition",
      "score": 0.4584185481071472
    },
    {
      "name": "Machine learning",
      "score": 0.38099396228790283
    },
    {
      "name": "Neuroscience",
      "score": 0.15680882334709167
    },
    {
      "name": "Psychology",
      "score": 0.09308260679244995
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I3005327000",
      "name": "Institute for Infocomm Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4387153466",
      "name": "Weill Cornell Medicine",
      "country": "US"
    }
  ]
}