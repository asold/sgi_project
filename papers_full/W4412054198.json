{
  "title": "Optimizing interactions: Strategies for prompt engineering in large language models",
  "url": "https://openalex.org/W4412054198",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1611654145",
      "name": "V. Premalatha",
      "affiliations": [
        "Koneru Lakshmaiah Education Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A4364597397",
      "name": "Anguraj Dinesh Kumar",
      "affiliations": [
        "Koneru Lakshmaiah Education Foundation"
      ]
    },
    {
      "id": "https://openalex.org/A2064675722",
      "name": "Nikhat Parveen",
      "affiliations": [
        "University of Bisha"
      ]
    },
    {
      "id": "https://openalex.org/A1611654145",
      "name": "V. Premalatha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4364597397",
      "name": "Anguraj Dinesh Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2064675722",
      "name": "Nikhat Parveen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385262478"
  ],
  "abstract": "This manuscript delineates an innovative investigation into the rapidly evolving domain of prompt engineering, an essential competency in the contemporary landscape of sophisticated artificial intelligence, particularly concerning Large Language Models (LLMs) such as ChatGPT. Prompt engineering, defined as the meticulous formulation of precise and impactful prompts, is instrumental in directing LLMs to conform to explicit parameters, facilitate intricate procedures, and uphold the integrity of both the quality and quantity of their generated outputs. We present a groundbreaking aggregation of prompt engineering methodologies, systematically articulated as discrete patterns. These patterns bear resemblance to the notion of design patterns within software engineering, providing versatile and adaptable solutions to prevalent challenges encountered during interactions with LLMs. Our investigation elucidates a variety of frameworks for prompt engineering, illuminating their capacity to tackle a diverse array of issues faced in information retrieval operations. We additionally investigate a range of pattern-oriented methodologies that have been demonstrated to provoke augmented responses from AI models. This manuscript aspires to deliver a thorough compendium of these prompt engineering paradigms, presenting invaluable insights and pragmatic strategies that will enable users to fully leverage the potential of their engagements with large language models (LLMs), thereby making a substantial contribution to the domain of AI communication.",
  "full_text": "Edu - Tech Enterprise\nPrema Latha Va* , Dinesh Kumar Anguraja , Dr Nikhat Parveenb \nOptimizing interactions: Strategies for prompt engineering in large language models\nOptimización de interacciones: estrategias para la ingeniería rápida en modelos de \nlenguaje grandes\naDepartment of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation. VADDESWARAM, AP , INDIA-522302. bDepartment \nof Information Science, University of Bisha. P .O. Box 551, Bisha, Saudi Arabia. \n*Corresponding Autor: Prema Latha V \nHow to cite: Prema Latha , V ., Dinesh Kumar, A., & Parveen, N. (2025). Optimizing interactions: Strategies for prompt engineering in large language \nmodels. Edu - Tech Enterprise, 3, 24. https://doi.org/10.71459/edutech202524\nSubmitted: 13-07-2024          Revised: 23-10-2024          Accepted: 19-01-2025          Published: 20-01-2025\nABSTRACT\nThis manuscript delineates an innovative investigation into the rapidly evolving domain of prompt engineering, an \nessential competency in the contemporary landscape of sophisticated artificial intelligence, particularly concerning \nLarge Language Models (LLMs) such as ChatGPT. Prompt engineering, defined as the meticulous formulation \nof precise and impactful prompts, is instrumental in directing LLMs to conform to explicit parameters, facilitate \nintricate procedures, and uphold the integrity of both the quality and quantity of their generated outputs. We present \na groundbreaking aggregation of prompt engineering methodologies, systematically articulated as discrete patterns. \nThese patterns bear resemblance to the notion of design patterns within software engineering, providing versatile and \nadaptable solutions to prevalent challenges encountered during interactions with LLMs. Our investigation elucidates \na variety of frameworks for prompt engineering, illuminating their capacity to tackle a diverse array of issues faced \nin information retrieval operations. We additionally investigate a range of pattern-oriented methodologies that have \nbeen demonstrated to provoke augmented responses from AI models. This manuscript aspires to deliver a thorough \ncompendium of these prompt engineering paradigms, presenting invaluable insights and pragmatic strategies that \nwill enable users to fully leverage the potential of their engagements with large language models (LLMs), thereby \nmaking a substantial contribution to the domain of AI communication.\nKeywords: prompt engineering; large language models (LLMs); artificial intelligence; design patterns; frameworks; \ninformation retrieval; AI communication.\nRESUMEN\nEste manuscrito describe una investigación innovadora en el ámbito en rápida evolución de la ingeniería de \nindicaciones, una competencia esencial en el panorama contemporáneo de la inteligencia artificial sofisticada, en \nparticular en lo que respecta a los modelos de lenguaje grande (LLM) como ChatGPT. La ingeniería de indicaciones, \ndefinida como la formulación meticulosa de indicaciones precisas e impactantes, es fundamental para dirigir los \nLLM a fin de que se ajusten a parámetros explícitos, faciliten procedimientos complejos y mantengan la integridad \ntanto de la calidad como de la cantidad de los resultados generados. Presentamos una innovadora agregación de \nmetodologías de ingeniería de mensajes, articuladas sistemáticamente como patrones discretos. Estos patrones \nse asemejan a la noción de patrones de diseño dentro de la ingeniería de software, proporcionando soluciones \nversátiles y adaptables a los desafíos frecuentes que se encuentran durante las interacciones con los LLM. Nuestra \ninvestigación dilucida una variedad de marcos para la ingeniería de mensajes, iluminando su capacidad para \nabordar una diversa gama de problemas que se enfrentan en las operaciones de recuperación de información. \nAdemás, investigamos una serie de metodologías orientadas a patrones que han demostrado provocar respuestas \naumentadas de los modelos de IA. Este manuscrito aspira a ofrecer un compendio exhaustivo de estos paradigmas de \nEdu - Tech Enterprise, 2025, 3:24\nISSN: 3084-7451\ndoi: 10.71459/edutech202524\nORIGINAL\n© 2025; Los autores. Este es un artículo en acceso abierto, distribuido bajo los términos de una licencia Creative Commons ( https://\ncreativecommons.org/licenses/by/4.0) que permite el uso, distribución y reproducción en cualquier medio siempre que la obra original \nsea correctamente citada \nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n2\ningeniería rápida, presentando conocimientos inestimables y estrategias pragmáticas que permitirán a los usuarios \naprovechar al máximo el potencial de sus interacciones con los modelos de lenguaje grande (LLM), contribuyendo \nasí de manera sustancial al dominio de la comunicación de la IA.\nPalabras clave: ingeniería de código rápido; modelos de lenguaje grande (LLM); inteligencia artificial; patrones de \ndiseño; marcos; recuperación de información; comunicación de IA.\nINTRODUCTION\nThe evolution of artificial intelligence (AI) has been shaped by significant achievements, particularly in the field of \nNatural Language Processing (NLP). The introduction of advanced neural network architectures, like the Transformer \nmodel by Vaswami et al. (2017), marked a major breakthrough in the discipline. These innovations paved the way for \nthe development of powerful language models such as the Generative Pre-trained Transformer (GPT) series (Brown \net al., 2020), which showcased the remarkable ability to produce text that closely mirrors human expression, setting \nnew standards in AI capabilities.\nIn the ever-advancing realm of AI, Large Language Models (LLMs) such as ChatGPT have become a key component \nof NLP , excelling in their ability to generate human-like text. However, the quality and relevance of the content they \nproduce are heavily influenced by the way users interact with them. This dynamic has led to the emergence of ‘prompt \nengineering’—a specialized discipline that blends linguistic creativity with programming precision.\nPrompt engineering goes beyond merely asking questions or giving instructions to an AI. It is a refined practice of \ncarefully crafting prompts to maximize the potential of the underlying model. This skill is critical for ensuring that AI \nsystems interpret context accurately, operate within defined parameters, and achieve the desired outcomes. As LLMs \nbecome integral to various sectors—ranging from creative writing to technical problem-solving—expertise in prompt \nengineering is essential for users seeking to optimize these models.\nThis work delves into the nuances of prompt engineering tailored specifically to LLMs. It examines how these models \ninterpret and respond to prompts, and how a deeper understanding of their mechanisms can be utilized to enhance \nthe quality of interactions. Additionally, it introduces structured strategies and frameworks, inspired by concepts \nin software engineering, that guide the creation of effective prompts. These methods are supported by practical \nexamples and real-world case studies, demonstrating their applicability in diverse scenarios.\nBy exploring these strategies, this work aims to bridge the gap between the untapped capabilities of LLMs and the \npractical obstacles encountered by users. The goal is to equip individuals with the knowledge and techniques needed \nto skilfully direct LLMs, ultimately improving the efficiency and accuracy of their outputs.\nFigure 1.\nA word cloud highlights the key themes, emerging trends, and research priorities in prompt engineering\n\nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n3\nTable 1.\nIllustrating prompt engineering examples\nCategory Example Purpose\nText Summarization “Summarize the key points of this article in three \nsentences. ”\nTo condense lengthy content into concise \nsummaries.\nRole Assignment “Act as a financial advisor and provide investment \ntips for beginners. ”\nTo adopt specific personas and deliver tailored \noutputs.\nInstruction-Based Tasks “Generate a Python script to calculate the \nfactorial of a number. ”\nTo perform precise technical tasks based on \nclear instructions.\nContextual Responses “Based on the following data, predict the sales \ntrend for the next quarter. ”\nTo provide accurate and relevant answers \nwithin specific contexts.\nCreative Writing “Write a short story about a futuristic city where \nAI governs daily life. ”\nTo inspire imaginative and artistic outputs.\nBias Mitigation “Provide an objective analysis of the pros and \ncons of renewable energy sources. ”\nTo minimize biases and maintain neutrality in \nresponses.\nFine-Tuning Outputs “Generate a marketing plan for a tech startup \ntargeting Gen Z. ”\nTo refine interactions and achieve specific \noutcomes.\nDefinition and principles\nPrompt engineering involves crafting text that a generative AI model can interpret and act upon effectively. A prompt \nis essentially a natural language description of the task the AI is expected to accomplish. Effective prompts require \nclarity, specificity, and precision to ensure the AI can fully understand the assigned task. Ambiguity or unclear language \nmay lead to misinterpretation or inaccurate outcomes. To enhance comprehension, providing adequate context \nand explicit instructions is crucial. This includes outlining essential details, clarifying key terms, and specifying the \ndesired format or tone of the output.\nThe use of examples or demonstrations within prompts can further clarify expectations by offering concrete \nreferences. This approach is particularly beneficial for creative tasks, such as writing poetry or crafting scripts, as \nit allows the AI to grasp the intended style and structure. Prompt engineering is often an iterative process, requiring \ncontinuous adjustments based on the AI’s responses. By refining prompts, users can provide additional guidance or \naddress any misunderstandings, ensuring that the output aligns with their goals. This iterative approach enhances the \neffectiveness and precision of interactions with AI systems.\nThe significance of prompt engineering in advancing AI development\nPrompt engineering is an expanding field within artificial intelligence (AI), particularly in relation to large language \nmodels (LLMs). It involves the skillful creation of prompts that act as instructions or queries designed to guide AI \nmodels in producing specific responses. These prompts serve as a vital link between human intentions and AI-\ngenerated outputs, ensuring the accurate execution of tasks.\nThe importance of prompt engineering lies in the fact that, although LLMs are trained on vast datasets of text and \ncode, they do not inherently comprehend language or intent. Prompts play a crucial role by providing context and \ndirection, enabling the AI to interpret tasks correctly and deliver meaningful results.\nEssential functions of prompt engineering in advancing AI development\nPrompt engineering is integral to various aspects of AI development, serving as a tool to:\n1. Enhance Precision and Focus: prompts steer AI towards the core objectives of a task, minimizing \ndistractions and ensuring alignment with the desired output, whether it be concise summaries or creative \nformats such as scripts and poems.\n2. Shape Creativity and Style: by providing clear instructions and examples, prompts guide AI in achieving \nspecific tones, styles, or genres, especially in creative writing or coding applications.\n3. Support Diverse Applications: tailored prompts enable AI models to adapt seamlessly across different \ntasks and domains, unlocking their potential in areas like education, research, and customer service.\n4. Address Bias and Ensure Fairness: carefully constructed prompts can help mitigate biases, eliminate \nstereotypes, and promote balanced and equitable responses from AI systems.\n5. Drive Iterative Refinement and Innovation: prompt engineering involves an ongoing cycle of improvement, \nallowing developers to refine prompts based on feedback and results, fostering adaptability and expanding AI’s \ncapabilities.\nAs AI technology evolves, prompt engineering will remain indispensable for developers, empowering them to enhance \nAI systems’ performance, ensure meaningful outputs, and mitigate biases, ultimately shaping the future of intelligent \nand versatile AI solutions.\nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n4\nMETHOD\nPrompt engineering methods\nPrompt methods encompass techniques or strategies designed to enhance the effectiveness of prompts across \nvarious patterns. Some widely used methods include:\n1. Explicit Instructions: provide clear guidance by stating the task or desired outcome unambiguously.\n2. Style and Tone Specifications: define the preferred style (e.g., formal, informal, poetic) and tone (e.g., \nhumorous, serious, informative) for the AI-generated output.\n3. Example Prompts: offer examples of desired formats or styles to serve as reference points and establish \npatterns for the LLM.\n4. Information Retrieval: enrich the prompt with relevant data from external sources (e.g., knowledge \nbases, databases) to provide contextual support for the AI.\n5. Context Amplification: highlight essential elements within the prompt to ensure the LLM focuses on key \naspects of the task.\n6. Summarization Prompts: direct the LLM to condense complex or lengthy information into concise, \ninformative responses.\n7. Creative Prompting: reformulate or present the task in innovative ways to stimulate creative solutions \nand novel outcomes.\n8. Iterative Refinement: continuously refine prompts based on AI responses to improve guidance and \nensure optimal results.\n9. Prompt Combination: merge multiple prompts into one to elicit comprehensive and multifaceted \nresponses, ideal for tasks involving diverse perspectives.\n10. Diverse Prompting: employ varied styles, formats, and approaches to foster adaptability and versatility \nin the AI’s responses.\n11. Domain-Specific Prompts: customize prompts to suit specific tasks or domains, ensuring relevance \nand effective guidance.\n12. Clear and Concise Language: use simple, precise language to eliminate ambiguity and aid accurate \ninterpretation by the LLM.\n13. Prompt Optimization: tailor prompts to leverage the strengths and address the limitations of the specific \nLLM being used.\n14. Feedback and Experimentation: test different prompts and utilize feedback to refine techniques and \nimprove the prompt engineering process.\nThese methods enable developers to design prompts that effectively guide AI models, ensuring meaningful and \nrelevant outputs.\nPatterns\nFigure 2.\nIllustration of various types of prompt engineering techniques\nInstructional patterns\nInstructional patterns in prompt engineering are designed to create clear and direct prompts, ensuring that large \nlanguage models produce precise and accurate outputs, particularly in the field of Natural Language Processing \n(NLP). Here are some key patterns:\n•\t Direct Instruction: this involves explicitly stating the task or command for the AI, such as “Convert 10 cm \ninto meters. ” These are straightforward and unambiguous requests.\n•\t Step-by-Step Guidelines: this pattern provides sequential instructions for tasks requiring detailed and \nprecise responses, guiding the AI through each step.\n•\t Task-Specific Prompts: these are tailored prompts designed to direct the AI to perform a particular task \nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n5\nor generate a specific type of output.\n•\t Feedback-Based Adjustment: this involves creating an initial prompt, evaluating the AI’s response, and \nrefining the prompt based on feedback to improve subsequent outputs.\n•\t Structured Query Formatting: this pattern focuses on crafting prompts that are clear, direct, and \nformatted to help the AI understand the exact nature of the requested information.\nThese patterns ensure effective communication with AI models, enhancing their ability to deliver accurate and \nrelevant results.\nQuestion-based pattern\nThis prompting strategy focuses on designing prompts in the form of questions, incorporating key elements to ensure \nefficient outputs:\n•\t Clarity and Specificity: questions are structured to be clear and precise, reducing ambiguity and guiding \nthe AI model to generate specific responses. For instance: “What are the differences between a Project \nManager and a Product Manager?”\n•\t Open-Ended Questions: these allow the AI to provide detailed and expansive information. For example: \n“How might artificial intelligence influence the future of the automobile industry?”\n•\t Closed-Ended Questions: these require concise and straightforward answers. For example: “How many \nhours can a student in Germany work full-time?”\nThis approach ensures that the prompts are tailored to elicit accurate and context-relevant responses.\nComparative or contrastive patterns\nThis pattern relies on the AI’s capacity to analyze and interpret complex relationships between subjects, evaluating \nthem based on specific criteria to emphasize their similarities or differences.\nZero-shot and few-shot learning\nThese patterns enable users to create prompts that models can interpret and respond to effectively, even without \ndirect training on a specific task:\n•\t Zero-shot learning is applied when obtaining a large labeled dataset is impractical or unfeasible.\n•\t Few-shot learning showcases the model’s adaptability by allowing it to handle slightly varied tasks with \nminimal guidance.\n•\t Few-shot learning responses can be refined and adjusted further to improve prompts, enhancing both \nthe accuracy and relevance of outputs.\nLimitations and challenges\nThis section explores the challenges and complexities of prompt engineering, particularly in the context of Large \nLanguage Models (LLMs) such as ChatGPT:\n1. Uncertainty and Misinterpretations:\n•\t Challenge: AI models may misinterpret prompts due to insufficient or unclear context.\n•\t Consequence: misunderstandings can lead to irrelevant or nonsensical responses.\n•\t Analysis: examples of vague prompts causing unintended AI behaviors are examined.\n2. Complexity of Human Language:\n•\t Challenge: the nuances and subtleties of human language present difficulties for AI \ncomprehension.\n•\t Consequence: crafting prompts that accurately convey intent becomes challenging.\n•\t Exploration: the section investigates linguistic features like colloquialisms, humor, and cultural \nreferences that pose barriers to AI.\n3. Ethical and Moral Implications:\n•\t Challenge: ensuring AI outputs align with ethical standards and social norms.\n•\t Consequence: poorly constructed prompts may result in biased or unethical responses.\n•\t Discussion: the importance of designing prompts that foster fairness, reduce bias, and prevent \nharmful outputs is emphasized.\n4. Technological Limitations:\n•\t Challenge: the constraints of current AI technology in understanding and generating complex \nconcepts.\nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n6\n•\t Consequence: these limitations affect the effectiveness of prompt engineering.\n•\t Advancements: the section highlights ongoing improvements in AI technology to address these \nchallenges.\n5. User Variability:\n•\t Challenge: differences in user expertise influence the quality of AI interactions.\n•\t Consequence: inequality in AI effectiveness across diverse user groups.\n•\t Solutions: educating users and designing intuitive interfaces are discussed as methods to bridge \nthis gap.\n6. Consistency and Scalability:\n•\t Challenge: developing prompt engineering methods that can be applied uniformly and scaled \nacross various applications.\n•\t Consequence: inconsistent strategies may lead to varying AI performance.\n•\t Discussion: approaches for creating universally applicable prompt engineering techniques are \nexplored.\n7. Privacy and Security Concerns:\n•\t Challenge: balancing effective prompts with the need to protect sensitive information.\n•\t Consequence: privacy considerations may limit the scope of AI applications.\n•\t Strategies: methods to balance privacy protection and the efficiency of prompt engineering are \nexamined.\nThis analysis sheds light on the multifaceted obstacles and opportunities in optimizing prompt engineering for AI \ndevelopment.\nFuture direction\nThis discussion focuses on the challenges, current directions, future opportunities, and development paths of \nprompt-based methods.\nFirstly, challenges in prompt engineering include issues such as data scarcity in medical NLP , interpretability of \nmodels, and inherent difficulties in designing effective prompts. Secondly, current research trends are highlighted, \nencompassing areas such as prompt generation, prompt optimization, multimodal data processing, and deep \nreinforcement learning. These areas aim to enhance the efficiency and usability of prompt-based methods, \ncontributing to advancements in NLP .\nLooking ahead, several key trends and techniques are anticipated in the evolution of prompt engineering:\n•\t Increased Use of Chain-of-Thought Prompting: this approach guides LLMs to produce logical and \ncoherent outputs by providing step-by-step instructions for solving tasks. Its effectiveness in handling complex \nproblems is likely to make it a standard technique.\n•\t Expanded Application of Few-Shot Learning: this method enables LLMs to adapt to new tasks with \nlimited examples, making it valuable for scenarios with scarce training data. Its growing usage will broaden the \nrange of tasks LLMs can tackle.\n•\t Greater Reliance on Templates: structured templates offer a framework for outputs, ensuring consistency \nand quality, especially in tasks requiring specific formats such as email writing or report generation.\n•\t Advancements in Prompt Tuning: by fine-tuning LLM parameters for specific tasks, this technique \noptimizes model performance and addresses areas where outputs may initially be lacking. Prompt tuning is \nexpected to become a more widely adopted method.\nOverall, the future of prompt engineering holds immense potential to transform how humans interact with computers \nand unlock innovative applications across diverse fields. This ongoing progress is poised to redefine possibilities in \nAI and NLP .\nComparison table\nTable 2.\nComparison table\nFeature Using Prompt Engineering Without Using Prompt Engineering\nRelevance of Response Prompts help the model stay focused, leading \nto relevant outputs.\nResponses may lack focus and drift due to unclear \nqueries.\nAccuracy of Response Clear prompts improve accuracy by specifying \nthe required task.\nAmbiguous inputs may result in less accurate \nresponses.\nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n7\nTask Alignment Well-crafted prompts are tailored to specific \ntasks.\nTask alignment depends on the model’s pre-existing \ntraining.\nInteraction Efficiency Reduces the need for repetitive iterations to \nrefine responses.\nMultiple attempts might be needed for satisfactory \nresults.\nUser Intent Clarity Detailed prompts clearly communicate the \nuser’s objectives.\nThe model may misinterpret vague or general inputs.\nUnderstanding Context Context-rich prompts enhance the model’s \nunderstanding.\nRelies on the model’s general knowledge, which \nmight limit comprehension.\nGuided Creativity Prompts steer creativity in desired directions. Creativity is unrestricted but may result in irrelevant \nideas.\nEase of Use Requires skill to design effective prompts. Simpler to use with natural input, needing no special \nformatting.\nPredictability of Output Expected outcomes are easier to achieve with \nstructured prompts.\nResponses can be unpredictable due to lack of \nguidance.\nLevel of Detail Prompts can request specific and detailed \ninformation.\nResponses may be more generalized or lack depth.\nFlexibility for New \nTasks\nHigh flexibility, especially with zero-shot and \nfew-shot approaches.\nLimited adaptability to tasks outside its training data.\nCONCLUSIONS\nThe importance of utilizing large language models (LLMs) continues to grow, making engineering practices essential \nfor unlocking their full potential. By creating well-crafted prompts, users can guide LLMs to produce outputs that \nare precise, insightful, and both concise and informative. Prompt engineering is becoming increasingly critical in \nleveraging the capabilities of LLMs and driving advancements across various fields. It plays a vital role in bridging \nhuman intent with machine execution, enabling seamless communication and collaboration between users and \nLLMs.\nThe application of LLMs allows individuals to utilize their capabilities for a wide range of tasks, from generating creative \ntextual formats to analyzing complex datasets. As LLMs become more advanced and their applications expand, the \ncontinuous development of prompt engineering will play a pivotal role in shaping the trajectory of artificial intelligence.\nThe future of prompt engineering looks bright, with its potential to revolutionize computer interactions and bring \ntransformative changes across industries. As LLMs evolve and engineering techniques improve, innovative and \ngroundbreaking applications are expected to emerge. Prompt engineering holds promise in connecting human \ningenuity with machine intelligence, fostering collaborative problem-solving and creating new opportunities through \nthe synergy of human and AI efforts.\nREFERENCES\nAWS. (2024). What is prompt engineering? - Generative AI. https://aws.amazon.com \nDataCamp. (2024). Prompt engineering: A detailed guide for 2024. https://www.datacamp.com \nDeepLearning.AI. (2024). ChatGPT prompt engineering for developers. https://www.deeplearning.ai \nDiab, M., Herrera, J., & Chernow, B. (2022). Stable Diffusion prompt book. https://example.com \nGu, J., Liu, Q., Xu, Z., Yu, F ., Liang, C., Jiang, J., & Zhang, Y . (2023). A systematic survey of prompt engineering on vision-\nlanguage foundation models. arXiv preprint arXiv:2307.12980. https://arxiv.org/abs/2307.12980 \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask \nlearners. OpenAI. https://openai.com/research/better-language-models \nTechTarget. (2024). What is an AI prompt engineer and how do you become one? https://www.techtarget.com \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention \nis all you need. arXiv preprint arXiv:1706.03762. https://arxiv.org/abs/1706.03762 \nZDNet. (2024). Six skills you need to become an AI prompt engineer. https://www.zdnet.com \nZiegler, A., & Berryman, J. (2023). A developer’s guide to prompt engineering and LLMs . GitHub Blog. https://github.\nblog \nhttps://doi.org/10.71459/edutech202524\nPrema Latha, V . et al. Edu -Tech Enterprice. 3 (2025) \n8\nFINANCING \nNo financing.\nCONFLICT OF INTEREST \nThe authors declare that there is no conflict of interest. \nAUTHORSHIP CONTRIBUTION \nData curation: PremaLatha V , Dinesh Kumar Anguraj, Dr Nikhat Parveen.\nMethodology: PremaLatha V , Dinesh Kumar Anguraj, Dr Nikhat Parveen.\nSoftware: PremaLatha V , Dinesh Kumar Anguraj, Dr Nikhat Parveen.\nDrafting - original draft: PremaLatha V , Dinesh Kumar Anguraj, Dr Nikhat Parveen.\nWriting - proofreading and editing: PremaLatha V , Dinesh Kumar Anguraj, Dr Nikhat Parveen.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5279948711395264
    }
  ],
  "institutions": []
}