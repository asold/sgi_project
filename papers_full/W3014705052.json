{
  "title": "Deep entity matching with pre-trained language models",
  "url": "https://openalex.org/W3014705052",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1858084601",
      "name": "Li Yu-Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2216138974",
      "name": "Li Jinfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2257279151",
      "name": "Suhara Yoshihiko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162208932",
      "name": "Doan AnHai",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A4288862249",
      "name": "Tan, Wang-Chiew",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2886950694",
    "https://openalex.org/W2164456230",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2798416089",
    "https://openalex.org/W2031250218",
    "https://openalex.org/W2154785834",
    "https://openalex.org/W2139077857",
    "https://openalex.org/W2957204582",
    "https://openalex.org/W2014327223",
    "https://openalex.org/W1976437052",
    "https://openalex.org/W2107966677",
    "https://openalex.org/W2984651502",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2145492473",
    "https://openalex.org/W2951147191",
    "https://openalex.org/W2542998387",
    "https://openalex.org/W1981590391",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3014705052",
    "https://openalex.org/W3013103751",
    "https://openalex.org/W3004437239",
    "https://openalex.org/W2798649495",
    "https://openalex.org/W2997591727",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2945883855",
    "https://openalex.org/W2767681556",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2067566391",
    "https://openalex.org/W2775696413",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2056748234",
    "https://openalex.org/W2114764731",
    "https://openalex.org/W2182703380",
    "https://openalex.org/W2945623882",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W2946504770"
  ],
  "abstract": "We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn \"harder\" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%.",
  "full_text": "Deep Entity Matching with Pre-Trained Language Models\nYuliang Li, Jinfeng Li,\nYoshihiko Suhara\nMegagon Labs\n{yuliang,jinfeng,yoshi}@megagon.ai\nAnHai Doan\nUniversity of Wisconsin Madison\nanhai@cs.wisc.edu\nWang-Chiew Tan\nMegagon Labs\nwangchiew@megagon.ai\nABSTRACT\nWe present Ditto, a novel entity matching system based on pre-\ntrained Transformer-based language models. We fine-tune and cast\nEM as a sequence-pair classification problem to leverage such mod-\nels with a simple architecture. Our experiments show that a straight-\nforward application of language models such as BERT, DistilBERT,\nor RoBERTa pre-trained on large text corpora already significantly\nimproves the matching quality and outperforms previous state-of-\nthe-art (SOTA), by up to 29% of F1 score on benchmark datasets. We\nalso developed three optimization techniques to further improve\nDitto’s matching capability. Ditto allows domain knowledge to\nbe injected by highlighting important pieces of input information\nthat may be of interest when making matching decisions. Ditto\nalso summarizes strings that are too long so that only the essential\ninformation is retained and used for EM. Finally, Ditto adapts\na SOTA technique on data augmentation for text to EM to aug-\nment the training data with (difficult) examples. This way,Ditto is\nforced to learn “harder” to improve the model’s matching capability.\nThe optimizations we developed further boost the performance\nof Ditto by up to 9.8%. Perhaps more surprisingly, we establish\nthat Ditto can achieve the previous SOTA results with at most\nhalf the number of labeled data. Finally, we demonstrate Ditto’s\neffectiveness on a real-world large-scale EM task. On matching\ntwo company datasets consisting of 789K and 412K records, Ditto\nachieves a high F1 score of 96.5%.\nPVLDB Reference Format:\nYuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew\nTan. Deep Entity Matching with Pre-Trained Language Models. PVLDB,\n14(1): XXX-XXX, 2021.\ndoi:10.14778/3421424.3421431\n1 INTRODUCTION\nEntity Matching (EM) refers to the problem of determining whether\ntwo data entries refer to the same real-world entity. Consider the\ntwo datasets about products in Figure 1. The goal is to determine\nthe set of pairs of data entries, one entry from each table so that\neach pair of entries refer to the same product.\nIf the datasets are large, it can be expensive to determine the pairs\nof matching entries. For this reason, EM is typically accompanied\nby a pre-processing step, called blocking, to prune pairs of entries\nthat are unlikely matches to reduce the number of candidate pairs\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\nthis license. For any use beyond those covered by this license, obtain permission by\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\nlicensed to the VLDB Endowment.\nProceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.\ndoi:10.14778/3421424.3421431\nto consider. As we will illustrate, correctly matching the candidate\npairs requires substantial language understanding and domain-\nspecific knowledge. Hence, entity matching remains a challenging\ntask even for the most advanced EM solutions.\nWe present Ditto, a novel EM solution based on pre-trained\nTransformer-based language models (or pre-trained language mod-\nels in short). We cast EM as a sequence-pair classification problem to\nleverage such models, which have been shown to generate highly\ncontextualized embeddings that capture better language under-\nstanding compared to traditional word embeddings. Ditto further\nimproves its matching capability through three optimizations: (1)\nIt allows domain knowledge to be added by highlighting important\npieces of the input that may be useful for matching decisions. (2) It\nsummarizes long strings so that only the most essential informa-\ntion is retained and used for EM. (3) It augments training data with\n(difficult) examples, which challenges Ditto to learn “harder” and\nalso reduces the amount of training data required. Figure 2 depicts\nDitto in the overall architecture of a complete EM workflow.\nThere are 9 candidate pairs of entries to consider for matching in\ntotal in Figure 1. The blocking heuristic that matching entries must\nhave one word in common in the title will reduce the number of\npairs to only 3: the first entry on the left with the first entry on the\nright and so on. Perhaps more surprisingly, even though the 3 pairs\nare highly similar and look like matches, only the first and last pair\nof entries are true matches. Our system, Ditto, is able to discern\nthe nuances in the 3 pairs to make the correct conclusion for every\npair while some state-of-the-art systems are unable to do so.\nThe example illustrates the power of language understanding\ngiven by Ditto’s pre-trained language model. It understands that\ninstant immersion spanish deluxe 2.0 is the same as instant immers\nspanish dlux 2 in the context of software products even though\nthey are syntactically different. Furthermore, one can explicitly\nemphasize that certain parts of a value are more useful for deciding\nmatching decisions. For books, the domain knowledge that the\ngrade level or edition is important for matching books can be made\nexplicit to Ditto, simply by placing tags around the grade/edition\nvalues. Hence, for the second candidate pair, even though the titles\nare highly similar (i.e., they overlap in many words), Ditto is\nable to focus on the grade/edition information when making the\nmatching decision. The third candidate pair shows the power of\nlanguage understanding for the opposite situation. Even though\nthe entries look dissimilar Ditto is able to attend to the right parts\nof a value (i.e., the manf./modelno under different attributes) and\nalso understand the semantics of the model number to make the\nright decision.\nContributions In summary, the following are our contributions:\n•We present Ditto, a novel EM solution based on pre-trained\nlanguage models (LMs) such as BERT. We fine-tune and cast EM\narXiv:2004.00584v3  [cs.DB]  2 Sep 2020\ntitle manf./modelno price\ninstant immersion spanish deluxe \n2.0\ntopics \nentertainment 49.99\nadventure workshop 4th-6th grade \n7th edition encore software 19.99\nsharp printing calculator sharp el1192bl 37.63\ntitle manf./modelno price\ninstant immers spanish dlux 2 NULL 36.11\nencore inc adventure workshop 4th-6th \ngrade 8th edition NULL 17.1\nnew-sharp shr-el1192bl two-color \nprinting calculator 12-digit lcd black red NULL 56.0\n✘\n✔\n✔\nFigure 1: Entity Matching: determine the matching entries from two datasets.\nBlocker Matcher\nTable A:\nTable B:\nMatched \nPairs\nSample \n& Label\nTrain\nDitto\nSummarize\nSerialize\nInject DK\nAugment\nCandidate \nPairs\nTrain Advanced Blocking\n①\n② ③\nFigure 2: An EM system architecture with Ditto as the matcher.\nIn addition to the training data, the user of Ditto can specify (1) a\nmethod for injecting domain knowledge (DK), (2) a summarization\nmodule for keeping the essential information, and (3) a data aug-\nmentation (DA) operator to strengthen the training set.\nas a sequence-pair classification problem to leverage such models\nwith a simple architecture. To our knowledge,Ditto is one of the\nfirst EM solutions that leverage pre-trained Transformer-based\nLMs1 to provide deeper language understanding for EM.\n•We also developed three optimization techniques to further im-\nprove Ditto’s matching capability through injecting domain\nknowledge, summarizing long strings, and augmenting train-\ning data with (difficult) examples. The first two techniques help\nDitto focus on the right information for making matching deci-\nsions. The last technique, data augmentation, is adapted from [31]\nfor EM to help Ditto learn “harder” to understand the data in-\nvariance properties that may exist but are beyond the provided\nlabeled examples and also, reduce the amount of training data\nrequired.\n•We evaluated the effectiveness of Ditto on three benchmark\ndatasets: the Entity Resolution benchmark [ 26], the Magellan\ndataset [25], and the WDC product matching dataset [39] of vari-\nous sizes and domains. Our experimental results show thatDitto\nconsistently outperforms the previous SOTA EM solutions in all\ndatasets and by up to 31% in F1 scores. Furthermore, Ditto con-\nsistently performs better on dirty data and is more label efficient:\nit achieves the same or higher previous SOTA accuracy using\nless than half the labeled data.\n•We applied Ditto to a real-world large-scale matching task on\ntwo company datasets, containing 789K and 412K entries re-\nspectively. To deploy an end-to-to EM pipeline efficiently, we\ndeveloped an advanced blocking technique to help reduce the\nnumber of pairs to consider for Ditto. Ditto obtains high ac-\ncuracy, 96.5% F1 on a holdout dataset. The blocking phase also\nhelped speed up the end-to-end EM deployment significantly, by\nup to 3.8 times, compared to naive blocking techniques.\n•Finally, we open-source Ditto at https://github.com/megagonlabs/\nditto.\n1There is a concurrent work [6] which applies a similar idea.\nOutline Section 2 overviews Ditto and pre-trained LMs. Section\n3 describes how we optimize Ditto with domain knowledge, sum-\nmarization, and data augmentation. Our experimental results are\ndescribed in Section 4 and the case study is presented in Section 5.\nWe discuss related work in Section 6 and conclude in Section 7.\n2 BACKGROUND AND ARCHITECTURE\nWe present the main concepts behind EM and provide some back-\nground on pre-trained LMs before we describe how we fine-tune\nthe LMs on EM datasets to train EM models. We also present a\nsimple method for reducing EM to a sequence-pair classification\nproblem so that pre-trained LMs can be used for solving the EM\nproblem.\nNotations Ditto’s EM pipeline takes as input two collections D\nand D′of data entries (e.g., rows of relational tables, XML docu-\nments, JSON files, text paragraphs) and outputs a setM ⊆D ×D′of\npairs where each pair (e, e′)∈ M is thought to represent the same\nreal-world entity (e.g., person, company, laptop, etc.). A data entry\ne is a set of key-value pairs e = {(attri , vali )}1≤i ≤k where attri is\nthe attribute name and vali is the attribute’s value represented as\ntext. Note that our definition of data entries is general enough to\ncapture both structured and semi-structured data such as JSON\nfiles.\nAs described earlier, an end-to-end EM system consists of a\nblocker and a matcher. The goal of the blocking phase is to quickly\nidentify a small subset of D ×D′of candidate pairs of high recall\n(i.e., a high proportion of actual matching pairs are that subset).\nThe goal of a matcher (i.e., Ditto) is to accurately predict, given a\npair of entries, whether they refer to the same real-world entity.\n2.1 Pre-trained language models\nUnlike prior learning-based EM solutions that rely on word em-\nbeddings and customized RNN architectures to train the matching\nmodel (See Section 6 for a detailed summary), Ditto trains the\nmatching models by fine-tuning pre-trained LMs in a simpler ar-\nchitecture.\nPre-trained LMs such as BERT [13] and GPT-2 [41] have demon-\nstrated good performance on a wide range of NLP tasks. They\nare typically deep neural networks with multiple Transformer lay-\ners [51], typically 12 or 24 layers, pre-trained on large text corpora\nsuch as Wikipedia articles in an unsupervised manner. During pre-\ntraining, the model is self-trained to perform auxiliary tasks such\nas missing token and next-sentence prediction. Studies [9, 50] have\nshown that the shallow layers capture lexical meaning while the\ndeeper layers capture syntactic and semantic meanings of the input\nsequence after pre-training.\nA specific strength of pre-trained LMs is that it learns the seman-\ntics of words better than conventional word embedding techniques\nsuch as word2vec, GloVe, or FastText. This is largely because the\nTransformer architecture calculates token embeddings from all the\ntokens in the input sequence and thus, the embeddings it generates\nare highly-contextualized and captures the semantic and contex-\ntual understanding of the words. Consequently, such embeddings\ncan capture polysemy, i.e., discern that the same word may have\ndifferent meanings in different phrases. For example, the word\nSharp has different meanings in “Sharp resolution” versus “Sharp\nTV”. Pre-trained LMs will embed “ Sharp” differently depending\non the context while traditional word embedding techniques such\nas FastText always produce the same vector independent of the\ncontext. Such models can also understand the opposite, i.e., that dif-\nferent words may have the same meaning. For example, the words\nimmersion and immers (respectively, (deluxe, dlux) and (2.0, 2)) are\nlikely the same given their respective contexts. Thus, such language\nunderstanding capability of pre-trained LMs can improve the EM\nperformance.\n2.2 Fine-tuning pre-trained language models\nA pre-trained LM can be fine-tuned with task-specific training\ndata so that it becomes better at performing that task. Here, we\nfine-tune a pre-trained LM for the EM task with a labeled training\ndataset consisting of positive and negative pairs of matching and\nnon-matching entries as follows:\n(1) Add task-specific layers after the final layer of the LM. For EM,\nwe add a simple fully connected layer and a softmax output\nlayer for binary classification.\n(2) Initialize the modified network with parameters from the pre-\ntrained LM.\n(3) Train the modified network on the training set until it con-\nverges.\nThe result is a model fine-tuned for the EM task. See Appendix\nA for the model architecture. In Ditto, we fine-tune the popular\n12-layer BERT model [13], RoBERTa [29], and a 6-layer smaller but\nfaster variant DistilBERT [45]. However, our proposed techniques\nare independent of the choice of pre-trained LMs and Ditto can\npotentially perform even better with larger pre-trained LMs. The\npair of data entries is serialized (see next section) as input to the\nLM and the output is a match or no-match decision. Ditto’s archi-\ntecture is much simpler when compared to many state-of-the-art\nEM solutions today [14, 34]. Even though the bulk of the “work”\nis simply off-loaded to pre-trained LMs, we show that this simple\nscheme works surprisingly well in our experiments.\n2.3 Serializing the data entries for Ditto\nSince LMs take token sequences (i.e., text) as input, a key challenge\nis to convert the candidate pairs into token sequences so that they\ncan be meaningfully ingested by Ditto.\nDitto serializes data entries as follows: for each data entry\ne = {(attri , vali )}1≤i ≤k , we let\nserialize(e)::= [COL]attr1 [VAL]val1 . . . [COL]attrk [VAL]valk ,\nwhere [COL]and [VAL]are special tokens for indicating the start\nof attribute names and values respectively. For example,the first\nentry of the second table is serialized as:\n[COL] title [VAL] instant immers spanish dlux 2 [COL] manf./modelno\n[VAL] NULL [COL] price [VAL] 36.11\nTo serialize a candidate pair (e, e′), we let\nserialize(e, e′)::= [CLS]serialize(e)[SEP]serialize(e′)[SEP],\nwhere [SEP]is the special token separating the two sequences\nand [CLS]is the special token necessary for BERT to encode the\nsequence pair into a 768-dimensional vector which will be fed into\nthe fully connected layers for classification.\nOther serialization schemes There are different ways to seri-\nalize data entries so that LMs can treat the input as a sequence\nclassification problem. For example, one can also omit the special\ntokens “[COL]” and/or “[VAL]”, or exclude attribute names attri\nduring serialization. We found that including the special tokens to\nretain the structure of the input does not hurt the performance in\ngeneral and excluding the attribute names tend to help only when\nthe attribute names do not contain useful information (e.g., names\nsuch as attr1, attr2, ...) or when the entries contain only one column.\nA more rigorous study on this matter is left for future work.\nHeterogeneous schemas As shown, the serialization method of\nDitto does not require data entries to adhere to the same schema. It\nalso does not require that the attributes of data entries to be matched\nprior to executing the matcher, which is a sharp contrast to other EM\nsystems such as DeepER [14] or DeepMatcher2 [34]. Furthermore,\nDitto can also ingest and match hierarchically structured data\nentries by serializing nested attribute-value pairs with special start\nand end tokens (much like Lisp or XML-style parentheses structure).\n3 OPTIMIZATIONS IN DITTO\nAs we will describe in Section 4, the basic version of Ditto, which\nleverages only the pre-trained LM, is already outperforming the\nSOTA on average. Here, we describe three further optimization\ntechniques that will facilitate and challengeDitto to learn “harder”,\nand consequently make better matching decisions.\n3.1 Leveraging Domain Knowledge\nOur first optimization allows domain knowledge to be injected\ninto Ditto through pre-processing the input sequences (i.e., seri-\nalized data entries) to emphasize what pieces of information are\npotentially important. This follows the intuition that when hu-\nman workers make a matching/non-matching decision on two data\nentries, they typically look for spans of text that contain key infor-\nmation before making the final decision. Even though we can also\ntrain deep learning EM solutions to learn such knowledge, we will\nrequire a significant amount of training data to do so. As we will\ndescribe, this pre-processing step on the input sequences is light-\nweight and yet can yield significant improvements. Our experiment\nresults show that with less than 5% of additional training time, we\ncan improve the model’s performance by up to 8%.\nThere are two main types of domain knowledge that we can\nprovide Ditto.\nSpan Typing The type of a span of tokens is one kind of domain\nknowledge that can be provided toDitto. Product id, street number,\npublisher are examples of span types. Span types help Ditto avoid\n2In DeepMatcher, the requirement that both entries have the same schema\ncan be removed by treating the values in all columns as one value under\none attribute.\nmismatches. With span types, for example,Ditto is likelier to avoid\nmatching a street number with a year or a product id.\nTable 1 summarizes the main span types that human workers\nwould focus on when matching three types of entities in our bench-\nmark datasets.\nTable 1: Main span types for matching entities in our benchmark\ndatasets.\nEntity Type Types of Important Spans\nPublications, Movies, Music Persons (e.g., Authors), Year, Publisher\nOrganizations, Employers Last 4-digit of phone, Street number\nProducts Product ID, Brand, Configurations (num.)\nThe developer specifies a recognizer to type spans of tokens\nfrom attribute values. The recognizer takes a text string v as input\nand returns a list recognizer(v)= {(si , ti , typei )}i ≥1 of start/end\npositions of the span in v and the corresponding type of the span.\nDitto’s current implementation leverages an open-source Named-\nEntity Recognition (NER) model [48] to identify known types such\nas persons, dates, or organizations and use regular expressions to\nidentify specific types such as product IDs, last 4 digits of phone\nnumbers, etc.\nAfter the types are recognized, the original text v is replaced\nby a new text where special tokens are inserted to reflect the\ntypes of the spans. For example, a phone number “(866) 246-6453”\nmay be replaced with “( 866 ) 246 - [LAST] 6453 [/LAST] ” where\n[LAST]/[/LAST] indicates the start/end of the last 4 digits and addi-\ntional spaces are also added because of tokenization. In our imple-\nmentation, when we are sure that the span type has only one token\nor the NER model is inaccurate in determining the end position, we\ndrop the end indicator and keep only the start indicator token.\nIntuitively, these newly added special tokens are additional sig-\nnals to the self-attention mechanism that already exists in pre-\ntrained LMs, such as BERT. If two spans have the same type, then\nDitto picks up the signal that they are likelier to be the same\nand hence, they are aligned together for matching. In the above\nexample,\n“. .246- [LAST] 6453 [/LAST] .. [SEP] . . [LAST] 0000 [/LAST]. .”\nwhen the model sees two encoded sequences with the [LAST]\nspecial tokens, it is likely to take the hint to align “6453” with “0000”\nwithout relying on other patterns elsewhere in the sequence that\nmay be harder to learn.\nSpan Normalization The second kind of domain knowledge that\ncan be passed to Ditto rewrites syntactically different but equiva-\nlent spans into the same string. This way, they will have identical\nembeddings and it becomes easier for Ditto to detect that the two\nspans are identical. For example, we can enforce that “VLDB jour-\nnal” and “VLDBJ” are the same by writing them asVLDBJ. Similarly,\nwe can enforce the general knowledge that “5 %” vs. “5.00 %” are\nequal by writing them as “5.0%”.\nThe developer specifies a set of rewriting rules to rewrite spans.\nThe specification consists of a function that first identifies the\nspans of interest before it replaces them with the rewritten spans.\nDitto contains a number of rewriting rules for numbers, including\nrules that round all floating point numbers to 2 decimal places and\ndropping all commas from integers (e.g., “2,020” →“2020”). For\nabbreviations, we allow the developers to specify a dictionary of\nsynonym pairs to normalize all synonym spans to be the same.\n3.2 Summarizing long entries\nWhen the value is an extremely long string, it becomes harder for\nthe LM to understand what to pay attention to when matching.\nIn addition, one limiting factor of Transformer-based pre-trained\nLMs is that there is a limit on the sequence length of the input. For\nexample, the input to BERT can have at most 512 sub-word tokens.\nIt is thus important to summarize the serialized entries down to\nthe maximum allowed length while retaining the key information.\nA common practice is to truncate the sequences so that they fit\nwithin the maximum length. However, the truncation strategy does\nnot work well for EM in general because the important information\nfor matching is usually not at the beginning of the sequences.\nThere are many ways to perform summarization [32, 42, 44]. In\nDitto’s current implementation, we use a TF-IDF-based summa-\nrization technique that retains non-stopword tokens with the high\nTF-IDF scores . We ignore the start and end tags generated by span\ntyping in this process and use the list of stop words from scikit-learn\nlibrary [37]. By doing so, Ditto feeds only the most informative\ntokens to the LM. We found that this technique works well in prac-\ntice. Our experiment results show that it improves the F1 score of\nDitto on a text-heavy dataset from 41% to over 93% and we plan\nto add more summarization techniques to Ditto’s library in the\nfuture.\n3.3 Augmenting training data\nWe describe how we apply data augmentation to augment the\ntraining data for entity matching.\nData augmentation (DA) is a commonly used technique in com-\nputer vision for generating additional training data from existing\nexamples by simple transformation such as cropping, flipping, ro-\ntation, padding, etc. The DA operators not only add more training\ndata, but the augmented data also allows to model to learn to make\npredictions invariant of these transformations.\nSimilarly, DA can add training data that will help EM models\nlearn “harder”. Although labeled examples for EM are arguably\nnot hard to obtain, invariance properties are very important to\nhelp make the solution more robust to dirty data, such as missing\nvalues (NULLs), values that are placed under the wrong attributes\nor missing some tokens.\nNext, we introduce a set of DA operators for EM that will help\ntrain more robust models.\nAugmentation operators for EM The proposed DA operators\nare summarized in Table 2. If s is a serialized pair of data entries\nwith a match or no-match label l, then an augmented example is a\npair (s′, l), where s′is obtained by applying an operator o on s and\ns′has the same label l as before.\nThe operators are divided into 3 categories. The first category\nconsists of span-level operators, such as span_del and span_shuffle.\nThese two operators are used in NLP tasks [31, 57] and shown to be\neffective for text classification. For span_del, we randomly delete\nfrom s a span of tokens of length at most 4 without special tokens\n(e.g., [SEP], [COL], [VAL]). For span_shuffle, we sample a span of\nlength at most 4 and randomly shuffle the order of its tokens.\nTable 2: Data augmentation operators in Ditto. The operators are 3\ndifferent levels: span-level, attribute-level, and entry-level. All sam-\nplings are done uniformly at random.\nOperator Explanation\nspan_del Delete a randomly sampled span of tokens\nspan_shuffle Randomly sample a span and shuffle the tokens’ order\nattr_del Delete a randomly chosen attribute and its value\nattr_shuffle Randomly shuffle the orders of all attributes\nentry_swap Swap the order of the two data entries e and e′\nThese two operators are motivated by the observation that mak-\ning a match/no-match decision can sometimes be “too easy” when\nthe candidate pair of data entries contain multiple spans of text sup-\nporting the decision. For example, suppose our negative examples\nfor matching company data in the existing training data is similar\nto what is shown below.\n[CLS] . . . [VAL] Google LLC . . . [VAL] (866) 246-6453 [SEP] . . .\n[VAL] Alphabet inc . . . [VAL] (650) 253-0000 [SEP]\nThe model may learn to predict “no-match” based on the phone\nnumber alone, which is insufficient in general. On the other hand,\nby corrupting parts of the input sequence (e.g., dropping phone\nnumbers), DA forces the model to learn beyond that, by leveraging\nthe remaining signals, such as the company name, to predict “no-\nmatch”.\nThe second category of operators is attribute-level operators:\nattr_del and attr_shuffle. The operator attr_del randomly deletes an\nattribute (both name and value) and attr_shuffle randomly shuffles\nthe order of the attributes of both data entries. The motivation for\nattr_del is similar to span_del and span_shuffle but it gets rid of\nan attribute entirely. The attr_shuffle operator allows the model to\nlearn the property that the matching decision should be indepen-\ndent of the ordering of attributes in the sequence.\nThe last operator, entry_swap, swaps the order of the pair (e, e′)\nwith probability 1/2. This teaches the model to make symmetric\ndecisions (i.e., F(e, e′)= F(e′, e)) and helps double the size of the\ntraining set if both input tables are from the same data source.\nMixDA: interpolating the augmented data Unlike DA opera-\ntors for images which almost always preserve the image labels, the\noperators for EM can distort the input sequence so much that the\nlabel becomes incorrect. For example, the attr_del operator may\ndrop the company name entirely and the remaining attributes may\ncontain no useful signals to distinguish the two entries.\nTo address this issue,Ditto applies MixDA, a recently proposed\ndata augmentation technique for NLP tasks [31] illustrated in Fig-\nure 3. Instead of using the augmented example directly, MixDA\ncomputes a convex interpolation of the original example with the\naugmented examples. Hence, the interpolated example is some-\nwhere in between, i.e., it is a “partial” augmentation of the original\nexample and this interpolated example is expected to be less dis-\ntorted than the augmented one.\nThe idea of interpolating two examples is originally proposed\nfor computer vision tasks [63]. For EM or text data, since we cannot\ndirectly interpolate sequences, MixDA interpolates their represen-\ntations by the language model instead. In practice, augmentation\nwith MixDA slows the training time because the LM is called twice.\nHowever, the prediction time is not affected since the DA operators\nare only applied to training data. Formally, given an operator o\n(e.g., span deletion) and an original example s, to apply o on s with\nMixDA (as Figure 3 illustrates),\n(1) Randomly sample λ from a Beta distribution λ ∼Beta(α, α)\nwith a hyper-parameter α ∈[0, 1](e.g., 0.8 in our experiments);\n(2) Denote by LM(s)the LM representation of a sequence s. Let\nLM(s′′)= λ·LM(s)+ (1 −λ)·LM(augment(s, o)).\nNamely, LM(s′′)is the convex interpolation between the LM\noutputs of s and the augmented s′= augment(s, o);\n(3) Train the model by feeding LM(s′′)to the rest of the network\nand back-propagate. Back-propagation updates both the LM\nand linear layer’s parameters.\nOriginal Language \nModel \n(BERT)Augmented\nDA-Op\n[.1, .2,\n.5, .1,\n...\n .4, .3]\n[.1, .3,\n.4, .2, \n...\n.4, .1]\n[.1, .25,\n.45, .15,\n...\n.4, .2]\nSequence\nRepresentations\nInput\nSequences\nInterpolate\n(MixUp)\nLinear\n↓↓\nSoftmax\n↓↓\nLoss\nBack propagate\nλ\n1-λ\nFigure 3: Data augmentation with MixDA.\n4 EXPERIMENTS\nWe present the experiment results on benchmark datasets for EM:\nthe ER Benchmark datasets [26], the Magellan datasets [25] and the\nWDC product data corpus [39]. Ditto achieves new SOTA results\non all these datasets and outperforms the previous best results by\nup to 31% in F1 score. The results show thatDitto is more robust to\ndirty data and performs well when the training set is small. Ditto\nis also more label-efficient as it achieves the previous SOTA results\nusing only 1/2 or less of the training data across multiple subsets\nof the WDC corpus. Our ablation analysis shows that (1) using pre-\ntrained LMs contributes to over 50% of Ditto’s performance gain\nand (2) all 3 optimizations, domain knowledge (DK), summarization\n(SU) and data augmentation (DA), are effective. For example, SU\nimproves the performance on a text-heavy dataset by 52%, DK leads\nto 1.2% average improvement on the ER-Magellan datasets and DA\nimproves on the WDC datasets by 2.53% on average. In addition, we\nshow in Appendix B that although Ditto leverages deeper neural\nnets, its training and prediction time is comparable to the SOTA\nEM systems.\n4.1 Benchmark datasets\nWe experimented with all the 13 publicly available datasets used\nfor evaluating DeepMatcher [34]. These datasets are from the ER\nBenchmark datasets [26] and the Magellan data repository [12]. We\nsummarize the datasets in Table 3 and refer to them as ER-Magellan.\nThese datasets are for training and evaluating matching models for\nvarious domains including products, publications, and businesses.\nEach dataset consists of candidate pairs from two structured tables\nof entity records of the same schema. The pairs are sampled from\nthe results of blocking and manually labeled. The positive rate (i.e.,\nthe ratio of matched pairs) ranges from 9.4% (Walmart-Amazon) to\n25% (Company). The number of attributes ranges from 1 to 8.\nAmong the datasets, the Abt-Buy and Company datasets are\ntext-heavy meaning that at least one attributes contain long text.\nAlso, following [34], we use the dirty version of the DBLP-ACM,\nTable 3: The 13 datasets divided into 4 categories of domains. The\ndatasets marked with †are text-heavy (Textual). Each dataset with\n∗has an additional dirty version to test the models’ robustness\nagainst noisy data.\nDatasets Domains\nAmazon-Google, Walmart-Amazon∗ software / electronics\nAbt-Buy†, Beer product\nDBLP-ACM*, DBLP-Scholar*, iTunes-Amazon* citation / music\nCompany†, Fodors-Zagats company / restaurant\nDBLP-Scholar, iTunes-Amazon, and Walmart-Amazon datasets to\nmeasure the robustness of the models against noise. These datasets\nare generated from the clean version by randomly emptying at-\ntributes and appending their values to another randomly selected\nattribute.\nEach dataset is split into the training, validation, and test sets\nusing the ratio of 3:1:1. The same split of the datasets is also used\nin the evaluation of other EM solutions [17, 23, 34]. We list the size\nof each dataset in Table 5.\nThe WDC product data corpus [39] contains 26 million product\noffers and descriptions collected from e-commerce websites [56].\nThe goal is to find product offer pairs that refer to the same product.\nTo evaluate the accuracy of product matchers, the dataset provides\n4,400 manually created golden labels of offer pairs from 4 categories:\ncomputers, cameras, watches, and shoes. Each category has a fixed\nnumber of 300 positive and 800 negative pairs. For training, the\ndataset provides for each category pairs that share the same product\nID such as GTINs or MPNs mined from the product’s webpage. The\nnegative examples are created by selecting pairs that have high\ntextual similarity but different IDs. These labels are further reduced\nto different sizes to test the models’ label efficiency. We summarize\nthe different subsets in Table 4. We refer to these subsets as the\nWDC datasets.\nTable 4: Different subsets of the WDC product data corpus. Each\nsubset (except Test) is split into a training set and a validation set\nwith a ratio of 4:1 according to the dataset provider [39]. The last col-\numn shows the positive rate (%POS) of each category in the xLarge\nset. The positive rate on the test set is 27.27% for all the categories.\nCategories Test Small Medium Large xLarge %POS\nComputers 1,100 2,834 8,094 33,359 68,461 14.15%\nCameras 1,100 1,886 5,255 20,036 42,277 16.98%\nWatches 1,100 2,255 6,413 27,027 61,569 15.05%\nShoes 1,100 2,063 5,805 22,989 42,429 9.76%\nAll 4,400 9,038 25,567 103,411 214,736 14.10%\nEach entry in this dataset has 4 attributes: title, description,\nbrand, andspecTable. Following the setting in [39] for DeepMatcher,\nwe allowDitto to use any subsets of attributes to determine the best\ncombination. We found in our experiments thatDitto achieves the\nbest performance when it uses only the title attribute. We provide\nfurther justification of this choice in Appendix F.\n4.2 Implementation and experimental setup\nWe implemented Ditto in PyTorch [36] and the Transformers\nlibrary [58]. We currently support 4 pre-trained models: Distil-\nBERT [45], BERT [13], RoBERTa [29], and XLNet [61]. We use the\nbase uncased variant of each model in all our experiments. We fur-\nther apply the half-precision floating-point (fp16) optimization to\naccelerate the training and prediction speed. In all the experiments,\nwe fix the max sequence length to be 256 and the learning rate\nto be 3e-5 with a linearly decreasing learning rate schedule. The\nbatch size is 32 if MixDA is used and 64 otherwise. The training\nprocess runs a fixed number of epochs (10, 15, or 40 depending\non the dataset size) and returns the checkpoint with the highest\nF1 score on the validation set. We conducted all experiments on a\np3.8xlarge AWS EC2 machine with 4 V100 GPUs (1 GPU per run).\nCompared methods. We compare Ditto with the SOTA EM\nsolution DeepMatcher. We also consider other baseline methods\nincluding Magellan [ 25], DeepER [ 14], and follow-up works of\nDeepMatcher [17, 23]. We also compare with variants of Ditto\nwithout the data augmentation (DA) and/or domain knowledge\n(DK) optimization to evaluate the effectiveness of each component.\nWe summarize these methods below. We report the average F1 of 5\nrepeated runs in all the settings.\n•DeepMatcher: DeepMatcher [34] is the SOTA matching solu-\ntion. Compared to Ditto, DeepMatcher customizes the RNN ar-\nchitecture to aggregate the attribute values, then compares/aligns\nthe aggregated representations of the attributes. DeepMatcher\nleverages FastText [5] to train the word embeddings. When re-\nporting DeepMatcher’s F1 scores, we use the numbers in [ 34]\nfor the ER-Magellan datasets and numbers in [39] for the WDC\ndatasets. We also reproduced those results using the open-sourced\nimplementation.\n•DeepMatcher+: Follow-up work [23] slightly outperforms Deep-\nMatcher in the DBLP-ACM dataset and [17] achieves better F1 in\nthe Walmart-Amazon and Amazon-Google datasets. According\nto [34], the Magellan system ([25], based on classical ML mod-\nels) outperforms DeepMatcher in the Beer and iTunes-Amazon\ndatasets. We also implemented and ran DeepER [14], which is\nanother RNN-based EM solution. We denote by DeepMatcher+\n(or simply DM+) the best F1 scores among DeepMatcher and\nthese works aforementioned. We summarize in Appendix C the\nimplementation details and performance of each method.\n•Ditto: This is the full version of our system with all 3 optimiza-\ntions, domain knowledge (DK), TF-IDF summarization (SU), and\ndata augmentation (DA) turned on. See the details below.\n•Ditto(DA): This version only turns on the DA (with MixDA) and\nSU but does not have the DK optimization. We apply one of the\nspan-level or attribute-level DA operators listed in Table 2 with\nthe entry_swap operator. We compare the different combinations\nand report the best one. Following [31], we apply MixDA with\nthe interpolation parameter λsampled from a Beta distribution\nBeta(0.8, 0.8).\n•Ditto(DK): With only the DK and SU optimizations on, this\nversion of Ditto is expected to have lower F1 scores but train\nmuch faster. We apply the span-typing to datasets of each domain\naccording to Table 1 and apply the span-normalization on the\nnumber spans.\n•Baseline: This base form of Ditto corresponds simply to fine-\ntuning a pre-trained LM on the EM task. We did not apply any\noptimizations on the baseline. For each ER-Magellan dataset, we\ntune the LM for the baseline and found that RoBERTa generally\nachieves the best performance. Thus, we use RoBERTa in the\nother 3 Ditto variants (Ditto, Ditto(DA), and Ditto(DK)) by\ndefault across all datasets. The Company dataset is the only\nexception, where we found that the BERT model performs the\nbest. For the WDC benchmark, since the training sets are large,\nwe use DistilBERT across all settings for faster training.\nThere is a concurrent work [6], which also applies pre-trained LM to\nthe entity matching problem. The proposed method is similar to the\nbaseline method above, but due to the difference in the evaluation\nmethods ([6] reports the best epoch on the test set, instead of the\nvalidation set), the reported results in [6] is not directly comparable.\nWe summarize in Appendix E the difference between Ditto and\n[6] and explain why the reported results are different.\n4.3 Main results\nTable 5 shows the results of the ER-Magellan datasets. Overall,\nDitto (with optimizations) achieves significantly higher F1 scores\nthan the SOTA results (DM+). Ditto without optimizations (i.e.,\nthe baseline) achieves comparable results with DM+. Ditto out-\nperforms DM+ in all 13 cases and by up to 31% (Dirty, Walmart-\nAmazon) while the baseline outperforms DM+ in 12/13 cases except\nfor the Company dataset with long text.\nIn addition, we found that Ditto is better at datasets with small\ntraining sets. Particularly, the average improvement on the 7 small-\nest datasets is 15.6% vs. 1.48% on average on the rest of datasets.\nDitto is also more robust against data noise than DM+. In the 4\ndirty datasets, the performance degradation of Ditto is only 0.57\non average while the performance of DM+ degrades by 8.21. These\ntwo properties makeDitto more attractive in practical EM settings.\nMoreover, in Appendix D, we show an evaluation of Ditto’s\nlabel efficiency on 5 of the ER-Magellan medium-size datasets. In\n4/5 cases, when trained on less than 20% of the original training\ndata, Ditto is able to achieve close or even better performance\nthan DM+ when the full training sets are in use.\nDitto also achieves promising results on the WDC datasets\n(Table 6). Ditto achieves the highest F1 score of 94.08 when using\nall the 215k training data, outperforming the previous best result\nby 3.92. Similar to what we found in the ER-Magellan datasets, the\nimprovements are higher on settings with fewer training examples\n(to the right of Table 6). The results also show that Ditto is more\nlabel efficient than DeepMatcher. For example, when using only\n1/2 of the data (Large), Ditto already outperforms DeepMatcher\nwith all the training data (xLarge) by 2.89 in All. When using only\n1/8 of the data (Medium), the performance is within 1% close to\nDeepMatcher’s F1 when 1/2 of the data (Large) is in use. The only\nexception is the shoes category. This may be caused by the large\ngap of the positive label ratios between the training set and the test\nset (9.76% vs. 27.27% according to Table 4).\n4.4 Ablation study\nNext, we analyze the effectiveness of each component (i.e., LM, SU,\nDK, and DA) by comparing Ditto with its variants without these\noptimizations. The results are shown in Table 5 and Figure 4.\nThe use of a pre-trained LM contributes to a large portion of the\nperformance gain. In the ER-Magellan datasets (excluding Com-\npany), the average improvement of the baseline compared to Deep-\nMatcher+ is 7.75, which accounts for 78.5% of the improvement of\nTable 5: F1 scores on the ER-Magellan EM datasets. The numbers of\nDeepMatcher+ (DM+) are the highest available found in [17, 23, 34]\nor re-produced by us.\nDatasets DM+ Ditto Ditto\n(DA)\nDitto\n(DK) Baseline Size\nStructured\nAmazon-Google 70.7 75.58 (+4.88) 75.08 74.67 74.10 11,460\nBeer 78.8 94.37 (+15.57) 87.21 90.46 84.59 450\nDBLP-ACM 98.45 98.99 (+0.54) 99.17 99.10 98.96 12,363\nDBLP-Google 94.7 95.6 (+0.9) 95.73 95.80 95.84 28,707\nFodors-Zagats 100 100.00 (+0.0) 100.00 100.00 98.14 946\niTunes-Amazon 91.2 97.06 (+5.86) 97.40 97.80 92.28 539\nWalmart-Amazon 73.6 86.76 (+13.16) 85.50 83.73 85.81 10,242\nDirty\nDBLP-ACM 98.1 99.03 (+0.93) 98.94 99.08 98.92 12,363\nDBLP-Google 93.8 95.75 (+1.95) 95.47 95.57 95.44 28,707\niTunes-Amazon 79.4 95.65 (+16.25) 95.29 94.48 92.92 539\nWalmart-Amazon 53.8 85.69 (+31.89) 85.49 80.67 82.56 10,242\nTextual\nAbt-Buy 62.8 89.33 (+26.53) 89.79 81.69 88.85 9,575\nCompany 92.7 93.85 (+1.15) 93.69 93.15 41.00 112,632\nTable 6: F1 scores on the WDC product matching datasets. The\nnumbers for DeepMatcher (DM) are taken from [39].\nSize xLarge (1/1) Large (1/2) Medium (1/8) Small (1/20)\nMethods DM Ditto DM Ditto DM Ditto DM Ditto\nComputers 90.80 95.45 89.55 91.70 77.82 88.62 70.55 80.76\n+4.65 +2.15 +10.80 +10.21\nCameras 89.21 93.78 87.19 91.23 76.53 88.09 68.59 80.89\n+4.57 +4.04 +11.56 +12.30\nWatches 93.45 96.53 91.28 95.69 79.31 91.12 66.32 85.12\n+3.08 +4.41 +11.81 +18.80\nShoes 92.61 90.11 90.39 88.07 79.48 82.66 73.86 75.89\n-2.50 -2.32 +3.18 +2.03\nAll 90.16 94.08 89.24 93.05 79.94 88.61 76.34 84.36\n+3.92 +3.81 +8.67 +8.02\nthe full Ditto (9.87). While DeepMatcher+ and the baseline Ditto\n(essentially fine-tuning DistilBERT) are comparable on the Struc-\ntured datasets, the baseline performs much better on all the Dirty\ndatasets and the Abt-Buy dataset. This confirms our intuition that\nthe language understanding capability is a key advantage of Ditto\nover existing EM solutions. The Company dataset is a special case\nbecause the length of the company articles (3,123 words on average)\nis much greater than the max sequence length of 256. The SU opti-\nmization increases the F1 score of this dataset from 41% to over 93%.\nIn the WDC datasets, across the 20 settings, LM contributes to 3.41\nF1 improvement on average, which explains 55.3% of improvement\nof the full Ditto (6.16).\nThe DK optimization is more effective on the ER-Magellan datasets.\nCompared to the baseline, the improvement of Ditto(DK) is 1.08\non average and is up to 5.88 on the Beer dataset while the improve-\nment is only 0.22 on average on the WDC datasets. We inspected\nthe span-typing output and found that only 66.2% of entry pairs\nhave spans of the same type. This is caused by the current NER\nmodule not extracting product-related spans with the correct types.\n10k 100k 200k\n80\n85\n90\n95F1 score\nall\n10k 35k 70k\n70\n80\n90\n95\ncomputers\n5k 20k 40k\nTrain+Valid Size\n70\n80\n90\ncameras\n10k 30k 60k\n70\n80\n90\n95\nwatches\n5k 20k 40k\n70\n80\n90\nshoes\nDM\nDitto\nDitto (DA)\nDitto (DK)\nBaseline\nFigure 4: F1 scores on the WDC datasets of different versions of Ditto. DM: DeepMatcher.\nWe expect DK to be more effective if we use an NER model trained\non the product domain.\nDA is effective on both datasets and more significantly on the\nWDC datasets. The average F1 score of the full Ditto improves\nupon Ditto(DK) (without DA) by 1.39 and 2.53 respectively in the\ntwo datasets. In the WDC datasets, we found that the span_del\noperator always performs the best while the best operators are\ndiverse in the ER-Magellan datasets. We list the best operator for\neach dataset in Table 7. We note that there is a large space of\ntuning these operators (e.g., the MixDA interpolation parameter,\nmaximal span length, etc.) and new operators to further improve\nthe performance.\nTable 7: Datasets that each DA operator achieves the best perfor-\nmance. The suffix (S)/(D) and (Both) denote the clean/dirty version\nof the dataset or both of them. All operators are applied with the\nentry_swap operator.\nOperator Datasets\nspan_shuffle DBLP-ACM (Both), DBLP-Google (Both), Abt-Buy\nspan_del Walmart-Amazon(D), Company, all of WDC\nattr_del Beer, iTunes-Amazon(S), Walmart-Amazon(S)\nattr_shuffle Fodors-Zagats, iTunes-Amazon(D)\n5 CASE STUDY: EMPLOYER MATCHING\nWe present a case of applying Ditto to a real-world EM task. An\nonline recruiting platform would like to join its internal employer\nrecords with newly collected public records to enable downstream\naggregation tasks. Given two tables A and B (internal and public)\nof employer records, the goal is to find, for each record in table B,\na record in table A that represents the same employer. Both tables\nhave 6 attributes: name, addr, city, state, zipcode,and phone. Our\ngoal is to find matches with both high precision and recall.\nBasic blocking. Our first challenge is size of the datasets. Table\n8 shows that both tables are of nontrivial sizes even after dedupli-\ncation. The first blocking method we designed is to only match\ncompanies with the same zipcode . However, since 60% of records\nin Table A do not have the zipcode attribute and some large em-\nployers have multiple sites, we use a second blocking method that\nreturns for each record in Table B the top-20 most similar records\nin A ranked by the TF-IDF cosine similarity of name and addr at-\ntributes. We use the union of these two methods as our blocker,\nwhich produces 10 million candidate pairs.\nData labeling. We labeled 10,000 pairs sampled from the results\nof each blocking method (20,000 labels in total). We sampled pairs\nof high similarity with higher probability to increase the difficulty\nof the dataset to train more robust models. The positive rate of all\nTable 8: Sizes of the two employer datasets to be matched.\nTableA TableB #Candidates\noriginal deduplicated original deduplicated Basic blocking\nSize 789,409 788,094 412,418 62,511 10,652,249\nthe labeled pairs is 39%. We split the labeled pairs into training,\nvalidation, and test sets by the ratio of 3:1:1.\nApplying Ditto. The user of Ditto does not need to extensively\ntune the hyperparameters but only needs to specify the domain\nknowledge and choose a data augmentation operator. We observe\nthat the street number and the phone number are both useful signals\nfor matching. Thus, we implemented a simple recognizer that tags\nthe first number string in the addr attribute and the last 4 digits of\nthe phone attribute. Since we would like the trained model to be\nrobust against the large number of missing values, we choose the\nattr_del operator for data augmentation.\nWe plot the model’s performance in Figure 5.Ditto achieves the\nhighest F1 score of 96.53 when using all the training data. Ditto\noutperforms DeepMatcher (DM) in F1 and trains faster (even when\nusing MixDA) than DeepMatcher across different training set sizes.\n12k6k3k2k\n91\n93\n95\n97\nF1 scores vs. size\n12k6k3k2k\n500\n1000\n1500\n2000\ntraining time (s) vs. size\nDM Ditto Ditto (DA) Ditto (DK) Baseline\nFigure 5: F1 and training time for the employer matching models.\nAdvanced blocking. Optionally, before applying the trained model\nto all the candidate pairs, we can use the labeled data to improve the\nbasic blocking method. We leverage Sentence-BERT [43], a variant\nof the BERT model that trains sentence embeddings for sentence\nsimilarity search. The trained model generates a high-dimensional\n(e.g., 768 for BERT) vector for each record. Although this model\nhas a relatively low F1 (only 92%) thus cannot replace Ditto, we\ncan use it with vector similarity search to quickly find record pairs\nthat are likely to match. We can greatly reduce the matching time\nby only testing those pairs of high cosine similarity. We list the\nrunning time for each module in Table 9. With this technique, the\noverall EM process is accelerated by 3.8x (1.69 hours vs. 6.49 hours\nwith/without advanced blocking).\nTable 9: Running time for blocking and matching with Ditto. Ad-\nvanced blocking consists of two steps: computing the representa-\ntion of each record with Sentence-BERT [43] (Encoding) and sim-\nilarity search by blocked matrix multiplication [1] (Search). With\nadvanced blocking, we only match each record with the top-10 most\nsimilar records according to the model.\nBasic Encoding Search Matching\nBlocking (GPU) (CPU) (top-10) (ALL)\nTime (s) 537.26 2,229.26 1,981.97 1,339.36 22,823.43\n6 RELATED WORK AND DISCUSSION\nEM solutions have tackled the blocking problem [2, 8, 16, 35, 54] and\nthe matching problem with rules [11, 15, 47, 53], crowdsourcing [18,\n22, 52], or machine learning [4, 10, 18, 25, 46].\nRecently, EM solutions used deep learning and achieved promis-\ning results [14, 17, 23, 34, 64]. DeepER [14] trains EM models based\non the LSTM [21] neural network architecture with word embed-\ndings such as GloVe [38]. DeepER also proposed a blocking tech-\nnique to represent each entry by the LSTM’s output. Our advanced\nblocking technique based on Sentence-BERT [ 43], described in\nSection 5, is inspired by this. Auto-EM [64] improves deep learning-\nbased EM models by pre-training the EM model on an auxiliary\ntask of entity type detection. Ditto also leverages transfer learning\nby fine-tuning pre-trained LMs, which are more powerful in lan-\nguage understanding. We did not compare Ditto with Auto-EM in\nexperiments because the entity types required by Auto-EM are not\navailable in our benchmarks. However, we expect that pre-training\nDitto with EM-specific data/tasks can improve the performance of\nDitto further and is part of our future work. DeepMatcher intro-\nduced a design space for applying deep learning to EM. Following\ntheir template architecture, one can think of Ditto as replacing\nboth the attribute embedding and similarity representation com-\nponents in the architecture with a single pre-trained LM such as\nBERT, thus providing a much simpler overall architecture.\nAll systems, Auto-EM, DeepER, DeepMatcher, and Ditto for-\nmulate matching as a binary classification problem. The first three\ntake a pair of data entries of the same arity as input and aligns the\nattributes before passing them to the system for matching. On the\nother hand, Ditto serializes both data entries as one input with\nstructural tags intact. This way, data entries of different schemas\ncan be uniformly ingested, including hierarchically formatted data\nsuch as those in JSON. Our serialization scheme is not only appli-\ncable to Ditto, but also to other systems such as DeepMatcher. In\nfact, we serialized data entries to DeepMatcher under one attribute\nusing our scheme and observed that DeepMatcher improved by as\nmuch as 5.2% on some datasets.\nA concurrent work [6] also applies pre-trained LMs to the en-\ntity matching problem and achieves good performance. While the\nproposed method in [6] is similar to the baseline version of Ditto,\nDitto can be further optimized using domain knowledge, data\naugmentation, and summarization. We also present a comprehen-\nsive experiment analysis on more EM benchmarks using a more\nstandard evaluation method. We provide a detailed comparison\nbetween Ditto and [6] in Appendix E.\nExternal knowledge is known to be effective in improving neu-\nral network models in NLP tasks [ 7, 49, 55, 60]. To incorporate\ndomain knowledge, Ditto modularizes the way domain knowl-\nedge is incorporated by allowing users to specify and customize\nrules for preprocessing input entries. Data augmentation (DA) has\nbeen extensively studied in computer vision and has recently re-\nceived more attention in NLP [ 31, 57, 59]. We designed a set of\nDA operators suitable for EM and apply them with MixDA [31], a\nrecently proposed DA strategy based on convex interpolation. To\nour knowledge, this is the first time data augmentation has been\napplied to EM.\nActive learning is a recent trend in EM to train high-quality\nmatching models with limited labeling resources [ 19, 23, 30, 40].\nUnder the active learning framework, the developer interactively\nlabels a small set of examples to improve the model while the up-\ndated model is used to sample new examples for the next labeling\nstep. Although active learning’s goal of improving label efficiency\naligns with data augmentation in Ditto, they are different solu-\ntions, which can be used together; active learning requires human\ninteraction in each iteration, whereas data augmentation does not.\nAccording to [30], one needs to adjust the model size and/or the\ntraining process such that the response time becomes acceptable\nfor user interaction in active learning. Thus, we consider apply-\ning it to Ditto is not straightforward because of the relatively\nlong fine-tuning time of the Ditto. We leave this aspect to future\ndevelopment of Ditto.\nDiscussion. Like other deep learning-based EM solutions, Ditto\nrequires a non-trivial amount of labeled training examples (e.g., the\ncase study requires 6k examples to achieve 95% F1) and Ditto’s\nDA and DK optimizations help reduce the labeling requirement\nto some extent. Currently, the LMs that we have tested in Ditto\nare pre-trained on general English text corpora and thus might not\ncapture well EM tasks with a lot of numeric data and/or specific\ndomains such as the scientific domain. For domain-specific tasks,\na potential solution is to leverage specialized LMs such as SciB-\nERT [3] or BioBERT [27] trained on scientific and biology corpus\nrespectively. For numeric data, a good candidate solution would\nbe a hybrid neural network similar to [20, 62] that combines the\nnumeric features with the textual features.\n7 CONCLUSION\nWe present Ditto, an EM system based on fine-tuning pre-trained\nTransformer-based language models. Ditto uses a simple archi-\ntecture to leverage pre-trained LMs and is further optimized by\ninjecting domain knowledge, text summarization, and data augmen-\ntation. Our results show that it outperforms existing EM solutions\non all three benchmark datasets with significantly less training data.\nDitto’s good performance can be attributed to the improved lan-\nguage understanding capability mainly through pre-trained LMs,\nthe more accurate text alignment guided by the injected knowledge,\nand the data invariance properties learned from the augmented\ndata. We plan to further explore our design choices for injecting\ndomain knowledge, text summarization, and data augmentation. In\naddition, we plan to extend Ditto to other data integration tasks\nbeyond EM, such as entity type detection and schema matching\nwith the ultimate goal of building a BERT-like model for tables.\nREFERENCES\n[1] Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\nto Index: Optimizing Exact Maximum Inner Product Search. In Proc. ICDE ’19 .\nIEEE, 1250–1261.\n[2] Linkage Rohan Baxter, Rohan Baxter, Peter Christen, et al. 2003. A comparison\nof fast blocking methods for record. (2003).\n[3] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language\nmodel for scientific text. arXiv preprint arXiv:1903.10676 (2019).\n[4] Mikhail Bilenko and Raymond J Mooney. 2003. Adaptive duplicate detection\nusing learnable string similarity measures. In Proc. KDD ’03 . 39–48.\n[5] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.\nEnriching word vectors with subword information. TACL 5 (2017), 135–146.\n[6] Ursin Brunner and Kurt Stockinger. 2020. Entity matching with transformer\narchitectures-a step forward in data integration. In EDBT.\n[7] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and Si Wei. 2018. Neural\nnatural language inference models enhanced with external knowledge. In Proc.\nACL ’18. 2406–2417.\n[8] Peter Christen. 2011. A survey of indexing techniques for scalable record linkage\nand deduplication. TKDE 24, 9 (2011), 1537–1555.\n[9] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.\n2019. What Does BERT Look at? An Analysis of BERT’s Attention. In Proc.\nBlackBoxNLP ’19 . 276–286.\n[10] William W Cohen and Jacob Richman. 2002. Learning to match and cluster large\nhigh-dimensional data sets for data integration. In Proc. KDD ’02 . 475–480.\n[11] Nilesh Dalvi, Vibhor Rastogi, Anirban Dasgupta, Anish Das Sarma, and Tamas\nSarlos. 2013. Optimal hashing schemes for entity matching. In Proc. WWW ’13 .\n295–306.\n[12] Sanjib Das, AnHai Doan, Paul Suganthan G. C., Chaitanya Gokhale, Pradap\nKonda, Yash Govind, and Derek Paulsen. [n.d.]. The Magellan Data Repository.\nhttps://sites.google.com/site/anhaidgroup/projects/data.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProc. NAACL-HLT ’19 . 4171–4186.\n[14] Muhammad Ebraheem, Saravanan Thirumuruganathan, Shafiq Joty, Mourad\nOuzzani, and Nan Tang. 2018. Distributed representations of tuples for entity\nresolution. PVLDB 11, 11 (2018), 1454–1467.\n[15] Ahmed Elmagarmid, Ihab F Ilyas, Mourad Ouzzani, Jorge-Arnulfo Quiané-Ruiz,\nNan Tang, and Si Yin. 2014. NADEEF/ER: generic and interactive entity resolution.\nIn Proc. SIGMOD ’14 . 1071–1074.\n[16] Jeffrey Fisher, Peter Christen, Qing Wang, and Erhard Rahm. 2015. A clustering-\nbased framework to control block sizes for entity resolution. In Proc. KDD ’15 .\n279–288.\n[17] Cheng Fu, Xianpei Han, Le Sun, Bo Chen, Wei Zhang, Suhui Wu, and Hao Kong.\n2019. End-to-end multi-perspective matching for entity resolution. In Proc. IJCAI\n’19. AAAI Press, 4961–4967.\n[18] Chaitanya Gokhale, Sanjib Das, AnHai Doan, Jeffrey F Naughton, Narasimhan\nRampalli, Jude Shavlik, and Xiaojin Zhu. 2014. Corleone: Hands-off crowdsourc-\ning for entity matching. In Proc. SIGMOD ’14 . 601–612.\n[19] Sairam Gurajada, Lucian Popa, Kun Qian, and Prithviraj Sen. 2019. Learning-\nBased Methods with Human-in-the-Loop for Entity Resolution. In CIKM. 2969–\n2970.\n[20] Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno,\nand Julian Martin Eisenschlos. 2020. TAPAS: Weakly Supervised Table Parsing\nvia Pre-training. arXiv preprint arXiv:2004.02349 (2020).\n[21] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735–1780.\n[22] Adam Marcus Eugene Wu David Karger and Samuel Madden Robert Miller. 2011.\nHuman-powered Sorts and Joins. PVLDB 5, 1 (2011).\n[23] Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian Popa. 2019.\nLow-resource Deep Entity Resolution with Transfer and Active Learning. InProc.\nACL ’19. 5851–5861.\n[24] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[25] Pradap Konda, Sanjib Das, Paul Suganthan G. C., AnHai Doan, Adel Ardalan,\nJeffrey R. Ballard, Han Li, Fatemah Panahi, Haojun Zhang, Jeffrey F. Naughton,\nShishir Prasad, Ganesh Krishnan, Rohit Deep, and Vijay Raghavendra. 2016.\nMagellan: Toward Building Entity Matching Management Systems. PVLDB 9, 12\n(2016), 1197–1208.\n[26] Hanna Köpcke, Andreas Thor, and Erhard Rahm. 2010. Evaluation of entity\nresolution approaches on real-world match problems. PVLDB 3, 1-2 (2010), 484–\n493.\n[27] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36, 4 (2020),\n1234–1240.\n[28] Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan.\n2020. Deep Entity Matching with Pre-Trained Language Models. arXiv preprint\narXiv:2004.00584 (2020).\n[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[30] Venkata Vamsikrishna Meduri, Lucian Popa, Prithviraj Sen, and Mohamed Sarwat.\n2020. A Comprehensive Benchmark Framework for Active Learning Methods in\nEntity Matching. In SIGMOD. 1133–1147.\n[31] Zhengjie Miao, Yuliang Li, Xiaolan Wang, and Wang-Chiew Tan. 2020. Snippext:\nSemi-supervised Opinion Mining with Augmented Data. In Proc. WWW ’20 .\n[32] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proc.\nEMNLP ’04 . 404–411.\n[33] Tom M Mitchell et al. 1997. Machine learning. Burr Ridge, IL: McGraw Hill 45, 37\n(1997), 870–877.\n[34] Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon Park,\nGanesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018.\nDeep learning for entity matching: A design space exploration. In Proc. SIGMOD\n’18. 19–34.\n[35] George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, and Themis Palpanas.\n2019. Blocking and Filtering Techniques for Entity Resolution: A Survey. arXiv\npreprint arXiv:1905.06167 (2019).\n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPyTorch: An imperative style, high-performance deep learning library. InProc.\nNeurIPS ’19 . 8024–8035.\n[37] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,\nBertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,\nVincent Dubourg, et al . 2011. Scikit-learn: Machine learning in Python. the\nJournal of machine Learning research 12 (2011), 2825–2830.\n[38] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In Proc. EMNLP ’14 . 1532–1543.\n[39] Anna Primpeli, Ralph Peeters, and Christian Bizer. 2019. The WDC training\ndataset and gold standard for large-scale product matching. In Companion Proc.\nWWW ’19 . 381–386.\n[40] Kun Qian, Lucian Popa, and Prithviraj Sen. 2017. Active learning for large-scale\nentity resolution. In CIKM. 1379–1388.\n[41] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).\n[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language models are unsupervised multitask learners. OpenAI\nBlog 1, 8 (2019), 9.\n[43] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings\nusing Siamese BERT-networks. In Proc. EMNLP-IJCNLP ’19 . 3982–3992.\n[44] Alexander M Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention\nmodel for abstractive sentence summarization. In Proc. EMNLP ’15 .\n[45] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. InProc.\nEMC2 ’19.\n[46] Sunita Sarawagi and Anuradha Bhamidipaty. 2002. Interactive deduplication\nusing active learning. In Proc. KDD ’02 . 269–278.\n[47] Rohit Singh, Venkata Vamsikrishna Meduri, Ahmed Elmagarmid, Samuel Madden,\nPaolo Papotti, Jorge-Arnulfo Quiané-Ruiz, Armando Solar-Lezama, and Nan Tang.\n2017. Synthesizing entity matching rules by examples. PVLDB 11, 2 (2017), 189–\n202.\n[48] Spacy. [n.d.]. https://spacy.io/api/entityrecognizer.\n[49] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin\nTian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced representa-\ntion through knowledge integration. arXiv preprint arXiv:1904.09223 (2019).\n[50] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical\nNLP Pipeline. In Proc. ACL ’19 . 4593–4601.\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NIPS ’17 . 5998–6008.\n[52] Jiannan Wang, Tim Kraska, Michael J Franklin, and Jianhua Feng. 2012. CrowdER:\ncrowdsourcing entity resolution. PVLDB 5, 11 (2012), 1483–1494.\n[53] Jiannan Wang, Guoliang Li, Jeffrey Xu Yu, and Jianhua Feng. 2011. Entity match-\ning: How similar is similar. PVLDB 4, 10 (2011), 622–633.\n[54] Qing Wang, Mingyuan Cui, and Huizhi Liang. 2015. Semantic-aware blocking\nfor entity resolution. TKDE 28, 1 (2015), 166–180.\n[55] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:\nKnowledge Graph Attention Network for Recommendation. In Proc. KDD ’19 .\n950âĂŞ958.\n[56] WDC Product Data Corpus. [n.d.]. http://webdatacommons.org/largescaleproductcorpus/v2.\n[57] Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques for\nBoosting Performance on Text Classification Tasks. In Proc. EMNLP-IJCNLP ’19 .\n6382–6388.\n[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al .\n2019. HuggingFace’s Transformers: State-of-the-art Natural Language Processing.\narXiv preprint arXiv:1910.03771 (2019).\n[59] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019.\nUnsupervised data augmentation. arXiv preprint arXiv:1904.12848 (2019).\n[60] Bishan Yang and Tom Mitchell. 2017. Leveraging Knowledge Bases in LSTMs for\nImproving Machine Reading. In Proc. ACL ’17 . 1436–1446.\n[61] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language\nunderstanding. In Proc. NeurIPS ’19 . 5754–5764.\n[62] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020.\nTaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. arXiv\npreprint arXiv:2005.08314 (2020).\n[63] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2018.\nmixup: Beyond empirical risk minimization. In Proc. ICLR ’18 .\n[64] Chen Zhao and Yeye He. 2019. Auto-EM: End-to-end Fuzzy Entity-Matching using\nPre-trained Deep Models and Transfer Learning. In Proc. WWW ’19 . 2413–2424.\nA ARCHITECTURE OF THE PRE-TRAINED\nLANGUAGE MODELS\nFigure 6 shows the model architecture of Ditto’s language models\nsuch as BERT [13], DistilBERT [45], and RoBERTa [29]. Ditto se-\nrializes the two input entries entries as one sequence and feeds it\nto the model as input. The model consists of (1) token embeddings\nand Transformer layers [58] from a pre-trained language model\n(e.g., BERT) and (2) task-specific layers (linear followed by softmax).\nConceptually, the [CLS] token “summarizes” all the contextual\ninformation needed for matching as a contextualized embedding\nvector E′\n[CLS]which the task-specific layers take as input for classi-\nfication.\nSoftMax\n0 / 1\n[CLS] T1 T2 [SEP] Tm\n...\n...\nPre-trained LM\n(e.g., BERT, DistilBERT)\nFirst entity e Second entity e’\nLinear layer\nTask-specific\nTransformer Layer\nTransformer Layer\nattr 1 val 1 ...\n[SEP]\nSerialize\nTokenize...\nE’[CLS] E’1 E’2 E’[SEP] E’m... E’[SEP]\nE[CLS] E1 E2 E[SEP] Em... E[SEP]\nContextualized \nEmbeddings\nEmbeddings\n... attr 1 val 1\nFigure 6: Ditto’s model architecture.\nB TRAINING TIME AND PREDICTION TIME\nEXPERIMENTS\nWe plot the training time required by DeepMatcher and Ditto in\nFigure 7. The running time for Ditto ranges from 119 seconds (450\nexamples) to 1.7 hours (113k examples).Ditto has a similar training\ntime to DeepMatcher although the Transformer-based models used\nby Dittoare deeper and more complex. The speed-up is due to\nthe fp16 optimization which is not used by DeepMatcher. Ditto\nwith MixDA is about 2-3x slower than Ditto(DK) without MixDA.\nThis is because MixDA requires additional time for generating the\naugmented pairs and computing with the LM twice. However, this\noverhead only affects offline training and does not affect online\nprediction.\nTable 11 shows Ditto’s average prediction time per entry pair\nin each benchmark. The results show that DeepMatcher and Ditto\nhave comparable prediction time. Also, the DK optimization only\nadds a small overhead to the prediction time (less than 2%). The\nprediction time between the two benchmarks are different because\nof the difference in their sequence length distributions.\nTable 10: Baseline results from different sources.\nDeepER\n(reproduced)\nDM\n(reproduced)\nDM\n(reported in [34])\nDM (using\nDitto’s input)\nMagellan\n(reported in [34]) ACL ’19 [23] IJCAI ’19 [17]\nStructured\nAmazon-Google 56.08 67.53 69.3 65.78 49.1 - 70.7\nBeer 50 69.23 72.7 - 78.8 - -\nDBLP-ACM 97.63 98.42 98.4 98.86 98.4 98.45 -\nDBLP-Scholar 90.82 94.32 94.7 94.56 92.3 92.94 -\nFodors-Zagats 97.67 - 100 - 100 - -\niTunes-Amazon 72.46 86.79 88 88 91.2 - -\nWalmart-Amazon 50.62 63.33 66.9 61.67 71.9 - 73.6\nDirty\nDBLP-ACM 89.62 97.53 98.1 96.03 91.9 - -\nDBLP-Scholar 86.07 92.8 93.8 93.75 82.5 - -\niTunes-Amazon 67.80 73.08 79.4 70.83 46.8 - -\nWalmart-Amazon 36.44 47.81 53.8 48.45 37.4 - -\nTextual\nAbt-Buy 42.99 66.05 62.8 67.99 43.6 - -\nCompany 62.17 - 92.7 90.70 79.8 - -\n1k 3k 10k 30k 100k\nTraining set size\n102\n103\n104\nTraining time (s)\n2k 10k 50k 200k\nTraining set size\n102\n103\n104\nDM Ditto Ditto(DA) Ditto(DK) Baseline\nFigure 7: Training time vs. dataset size for the ER-Megallan\ndatasets (left) and the WDC datasets (right). Each point corresponds\nto the training time needed for a dataset using different methods.\nDitto(DK) and Baseline do not use MixDA thus is faster than the\nfull Ditto. The DK optimization only adds a small overhead (5%)\nto the training time. DeepMatcher (DM) ran out of memory on the\nCompany dataset so the data point is not reported.\nTable 11: The average prediction time (ms) per data entry pair of\nDitto.\nDitto-DistilBERT Ditto-RoBERTa DMw. DK w/o. DK w. DK w/o. DK\nER-Magellan 8.01 7.87 6.82 6.78 6.62\nWDC 1.82 1.80 2.11 2.11 2.30\nC BREAKDOWN OF THE DM+ RESULTS AND\nEXPERIMENTS\nIn this section, we provide a detailed summary of how we obtain\nthe DeepMatcher+ (DM+) baseline results. Recall from Section\n4.2 that DM+ is obtained by taking the best performance (highest\nF1 scores) of multiple baseline methods including DeepER [ 14],\nMagellan [25], DeepMatcher [34], and DeepMatcher’s follow-up\nwork [17] and [23].\nWe summarize these baseline results in Table 10 on the ER-\nMagellan benchmarks and explain each method next.\nDeepER: The original paper [14] proposes a DL-based framework\nfor EM. Similar to DeepMatcher, DeepER first aggregates both data\nentries into their vector representations and uses a feedforward\nneural network to perform the binary classification based on the\nsimilarity of the two vectors. Each vector representation is obtained\neither by a simple averaging over the GloVe [38] embeddings per\nattribute or a RNN module over the serialized data entry. DeepER\ncomputes the similarity as the cosine similarity of the two vectors.\nAlthough [14] reported results on the Walmart-Amazon, Amazon-\nGoogle, DBLP-ACM, DBLP-Scholar, and the Fodors-Zagat datasets,\nthe numbers are not directly comparable to the presented results\nof Ditto because their evaluation and data preparation methods\nare different (e.g., they used k-fold cross-validation while we use\nthe train/valid/test splits according to [34]). In our experiments, we\nimplemented DeepER with LSTM as the RNN module and GloVe for\nthe tokens embeddings as described in [14] and with the same hyper-\nparameters (a learning rate of 0.01 and the Adam optimizer [24]). We\nthen evaluate DeepER in our evaluation settings. For each dataset,\nwe report the best results obtained by the simple aggregation and\nthe RNN-based method.\nDeepMatcher (DM): We have summarized DM in Section 4.2. In\naddition to simply taking the numbers from the original paper [34],\nwe also ran their open-source version (DM (reproduced)) with the\ndefault settings (the Hybrid model with a batch size of 32 and\n15 epochs). The reproduced results are in general lower than the\noriginal reported numbers in [34] (the 3rd column) because we did\nnot try the other model variants and hyperparameters as in the\noriginal experiments. The code failed in the Fodors-Zagat and the\nCompany datasets because of out-of-memory errors.\nIn addition, one key difference between DM and Ditto is that\nDitto serializes the data entries while DM does not. One might\nwonder if DM can obtain better results by simply replacing its input\nwith the serialized entries produced by Ditto. We found that the\n0.5k 1k 1.5k 2k full\n50\n60\n70\n80F1 score\nAmazon-Google\n0.5k 1k 1.5k 2k full\n96\n97\n98\n99\nDBLP-ACM\n0.5k 1k 1.5k 2k full\nTrain Size\n90\n92\n94\n96\nDBLP-Scholar\n0.5k 1k 1.5k 2k full\n50\n60\n70\n80\n90\nWalmart-Amazon\n0.5k 1k 1.5k 2k full\n60\n70\n80\n90\nAbt-Buy\nDM+ (full)\nDitto\nDitto (DA)\nDitto (DK)\nBaseline\nFigure 8: F1 scores on 5 ER-Magellan datasets using different variants of Ditto. We also plot the score of DeepMatcher+ on the full datasets\n(denoted as DM+(full)) as reference. Recall that full = {11460, 12363, 28707, 10242, 9575}for the 5 datasets respectively.\nresults do not significantly improved overall, but it is up to 5.2% in\nthe Abt-Buy dataset.\nOthers: We obtained the results for Magellan by taking the re-\nported results from [34] and the two follow-up works [17, 23] of\nDeepMatcher (denoted as ACL ’19 and IJCAI ’19 in Table 10). We\ndid not repeat the experiments since they have the same evaluation\nsettings as ours.\nD LABEL EFFICIENCY EXPERIMENTS ON\nTHE ER-MAGELLAN BENCHMARK\nWe also evaluate the label efficiency of Ditto on the ER-Magellan\nbenchmark. We conducted the experiments on 5 representative\ndatasets (Amazon-Google, DBLP-ACM, DBLP-Scholar, Walmart-\nAmazon, and Abt-Buy) of size ∼10k to ∼30k. For each dataset, we\nvary the training set size from 500 to 2,000 and uniformly sample\nfrom the original training set. We then follow the same setting as in\nSection 4 to evaluate the 4 variants of Ditto: baseline, Ditto(DA),\nDitto(DK), and Ditto. We summarize the results in Figure 8. We\nalso plot the result of DM+ trained on the full datasets (denoted as\nDM+ (full)) as a reference. As shown in Figure 8, Ditto is able to\nreach similar or better performance to DM+ on 3 of the datasets\n(Amazon-Google, DBLP-ACM, and Walmart-Amazon) with 2,000\ntrain examples (so ≤20%). With only 500 examples, Ditto is able\nto outperform DM+ trained on the full data in the Abt-Buy dataset.\nThese results confirm thatDitto is more label efficient than existing\nEM solutions.\nE THE DIFFERENCE BETWEEN DITTO AND\nA CONCURRENT WORK\nThere is a concurrent work [6] which also applies pre-trained LMs\nto entity matching and obtained good results. The method proposed\nin [6] is essentially identical to the baseline version ofDitto which\nonly serializes the data entries into text sequences and fine-tunes\nthe LM on the binary sequence-pair classification task. On top of\nthat, Ditto also applies 3 optimizations of injecting domain knowl-\nedge, data augmentation, and summarization to further improve\nthe model’s performance. We also evaluate Ditto more compre-\nhensively as we tested Ditto on all the 13 ER-Magellan datasets,\nthe WDC product benchmark, and a company matching dataset\nwhile [6] experimented in 5/13 of the ER-Magellan datasets.\nOn these 5 evaluated datasets, one might notice that the reported\nF1 scores in [6] are slightly higher compared to the baseline’s F1\nscores shown in Table 5. The reason is that according to [6], for each\nrun on each dataset, the F1 score is computed as the model’sbest F1\nscores on the test set among all the training epochs , while we report\nthe test F1 score of the epoch with the best F1 on the validation set . Our\nevaluation method is more standard since it prevents overfitting the\ntest set (See Chapter 4.6.5 of [33]) and is also used by DeepMatcher\nand Magellan [34]. It is not difficult to see that over the same set\nof model snapshots, the F1 score computed by the [6]’s evaluation\nmethod would be greater or equal to the F1 score computed using\nour method, which explains the differences in the reported values\nbetween us and [6].\nTable 12 summarizes the detailed comparison of the baseline\nDitto, the proposed method in [6], and the full Ditto. Recall that\nwe construct the baseline by taking the best performing pre-trained\nmodel among DistilBERT [45], BERT [13], XLNet [61], and\nRoBERTa [29] following [ 6]. Although the baseline Ditto does\nnot outperform [6] because of the different evaluation method, the\noptimized Ditto is able to outperform [6] in 4/5 of the evaluated\ndatasets.\nTable 12: The F1 scores of the baseline method with different pre-\ntrained LMs. The first 4 columns are performance of the baseline\nDitto using the 4 different LMs. We highlight the LM of the best\nperformance on each dataset, which form the baseline column in\nTable 5. We turned on the summarization (SU) optimization for the\nCompany dataset to get F1 scores closer to the full Ditto.\nDistilBERT XLNet RoBERTa BERT Reported\nin [6] Ditto\nStructured\nAmazon-Google 71.38 74.10 65.92 71.66 - 75.58\nBeer 82.48 48.91 74.23 84.59 - 94.37\nDBLP-ACM 98.49 98.85 98.87 98.96 - 98.99\nDBLP-Scholar 94.92 95.84 95.46 94.93 - 95.6\nFodors-Zagats 97.27 95.30 98.14 95.98 - 100.0\niTunes-Amazon 91.49 74.81 92.05 92.28 - 97.06\nWalmart-Amazon 79.81 77.98 85.81 81.27 - 86.76\nDirty\nDBLP-ACM 98.60 98.92 98.79 98.81 98.90 99.03\nDBLP-Scholar 94.76 95.26 95.44 94.72 95.60 95.75\niTunes-Amazon 90.12 92.70 92.92 92.25 94.20 95.65\nWalmart-Amazon 77.91 61.73 82.56 81.55 85.50 85.69\nTextual\nAbt-Buy 82.47 53.55 88.85 84.21 90.90 89.33\nCompany 93.16 71.93 85.89 93.61 - 93.85\nTable 13: The 4 attributes of the WDC benchmarks used in training\nDitto and DM according to [39].\nAttributes Examples %Available\nTitle Corsair Vengeance Red LED 16GB 2x 8GB\nDDR4 PC4 21300 2666Mhz dual-channel Kit -\nCMU16GX4M2A2666C16R Novatech\n100%\nDescription DDR4 2666MHz C116, 1.2V, XMP 2.0 red-led, Life-\ntime Warranty\n54%\nBrand AMD 19%\nSpecTable Memory Type DDR4 (PC4-21300) Capacity 16GB\n(2 x 8GB) Tested Speed 2666MHz Tested Latency\n16-18-18-35 Tested Voltage 1.20V Registered / Un-\nbuffered Unbuffered Error Checking Non-ECC\nMemory Features - red-led XMP 2.0\n7%\nF EXPERIMENTS ON DIFFERENT WDC\nPRODUCT ATTRIBUTES\nFollowing the settings in [39] for the evaluated models, we evaluate\nDitto on 4 different subsets of the product attributes as input so\nthat Ditto and DeepMatcher are evaluated under the same setting.\nWe list the 4 attributes in Table 13. Note that except for title, the\nattributes can be missing the the data entries. For example, the\nSpecTable attribute only appears in 7% of the entries in the full\ntraining set.\nWe summarize the results in Table 14. Among all the tested\ncombinations (the same as the ones tested for DeepMatcher in\n[39]), the combination consisting of only the title attribute works\nsignificantly better than the others. The difference ranges from 3.2%\n(computer, xlarge) to over 30% (watches, small). According to this\nresult, we only report Ditto’s results on the title attribute while\nallowing DeepMatcher to access all the 4 attributes to ensure its\nbest performance.\nThe performance ofDitto drops when more attributes are added\nis because of the sequence length. For example, for the combination\ntitle+description, we found that the average sequence length grows\nfrom 75.5 ( title only) to 342.7 which is beyond our default max\nlength of 256 tokens. As a results, some useful information from\nthe title attributes is removed by the summarization operator.\nTable 14: F1 scores of Ditto on the WDC datasets with different subsets of the product attributes\ntitle title_description title_description_brand title_description_brand_specTable\nsmall medium large xlarge small medium large xlarge small medium large xlarge small medium large xlarge\nall 84.36 88.61 93.05 94.08 69.51 75.91 81.56 87.62 68.34 75.43 84.80 85.19 67.08 75.55 83.08 84.44\ncameras 80.89 88.09 91.23 93.78 61.64 73.41 79.51 83.61 59.97 73.16 78.60 82.61 55.04 68.81 76.53 80.09\ncomputers 80.76 88.62 91.70 95.45 66.56 75.60 87.39 92.26 65.15 73.55 86.05 90.36 60.82 66.90 84.25 88.45\nshoes 75.89 82.66 88.07 90.10 59.57 69.25 76.33 76.27 57.43 71.57 77.07 77.39 56.57 71.02 76.58 75.63\nwatches 85.12 91.12 95.69 96.53 58.16 70.14 81.03 84.55 59.66 73.06 81.92 84.46 52.49 68.67 79.58 82.48",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    }
  ],
  "cited_by": 254
}