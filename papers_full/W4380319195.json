{
  "title": "A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics",
  "url": "https://openalex.org/W4380319195",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2244826664",
      "name": "Hong-Yu Zhou",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2343472808",
      "name": "Yizhou Yu",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2647365186",
      "name": "Chengdi Wang",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2097655330",
      "name": "Shu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2541483689",
      "name": "Yuanxu Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146716091",
      "name": "Jia Pan",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2101500310",
      "name": "Jun Shao",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2092603001",
      "name": "Guangming Lu",
      "affiliations": [
        "Nanjing University",
        "Nanjing General Hospital of Nanjing Military Command"
      ]
    },
    {
      "id": "https://openalex.org/A2116548023",
      "name": "Kang Zhang",
      "affiliations": [
        "Zhuhai People's Hospital",
        "West China Hospital of Sichuan University",
        "Macau University of Science and Technology",
        "Beijing Academy of Artificial Intelligence",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2098349533",
      "name": "Weimin Li",
      "affiliations": [
        "West China Hospital of Sichuan University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2244826664",
      "name": "Hong-Yu Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343472808",
      "name": "Yizhou Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2647365186",
      "name": "Chengdi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097655330",
      "name": "Shu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2541483689",
      "name": "Yuanxu Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146716091",
      "name": "Jia Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101500310",
      "name": "Jun Shao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2092603001",
      "name": "Guangming Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116548023",
      "name": "Kang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098349533",
      "name": "Weimin Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2906295032",
    "https://openalex.org/W2911462778",
    "https://openalex.org/W3205594709",
    "https://openalex.org/W3028751559",
    "https://openalex.org/W2166606882",
    "https://openalex.org/W4303613786",
    "https://openalex.org/W3094595351",
    "https://openalex.org/W4225593001",
    "https://openalex.org/W3020653337",
    "https://openalex.org/W2788633781",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W3156011032",
    "https://openalex.org/W3210892264",
    "https://openalex.org/W3213233983",
    "https://openalex.org/W3025404316",
    "https://openalex.org/W4292707488",
    "https://openalex.org/W2770241596",
    "https://openalex.org/W3025948831",
    "https://openalex.org/W2944016032",
    "https://openalex.org/W3168947860",
    "https://openalex.org/W3152711969",
    "https://openalex.org/W2949955266",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4320458302",
    "https://openalex.org/W2045616259",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W6765671411",
    "https://openalex.org/W4360939568",
    "https://openalex.org/W4377043947",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3038197756",
    "https://openalex.org/W3217167023",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6745245109",
    "https://openalex.org/W2962858109"
  ],
  "abstract": null,
  "full_text": "Nature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 743\nnature biomedical engineering\nhttps://doi.org/10.1038/s41551-023-01045-x\nArticle\nA transformer-based representation-learning \nmodel with unified processing of multimodal \ninput for clinical diagnostics\nHong-Yu Zhou    1,9, Yizhou Yu    1,9 , Chengdi Wang2,9 , Shu Zhang    3, \nYuanxu Gao    4, Jia Pan1, Jun Shao2, Guangming Lu    5, Kang Zhang    6,7,8   \n& Weimin Li    2 \nDuring the diagnostic process, clinicians leverage multimodal information, \nsuch as the chief complaint, medical images and laboratory test results. \nDeep-learning models for aiding diagnosis have yet to meet this \nrequirement of leveraging multimodal information. Here we report a \ntransformer-based representation-learning model as a clinical diagnostic \naid that processes multimodal input in a unified manner. Rather than \nlearning modality-specific features, the model leverages embedding \nlayers to convert images and unstructured and structured text into visual \ntokens and text tokens, and uses bidirectional blocks with intramodal and \nintermodal attention to learn holistic representations of radiographs, the \nunstructured chief complaint and clinical history, and structured clinical \ninformation such as laboratory test results and patient demographic \ninformation. The unified model outperformed an image-only model and \nnon-unified multimodal diagnosis models in the identification of pulmonary \ndisease (by 12% and 9%, respectively) and in the prediction of adverse clinical \noutcomes in patients with COVID-19 (by 29% and 7%, respectively). Unified \nmultimodal transformer-based models may help streamline the triaging of \npatients and facilitate the clinical decision-making process.\nIt has been common practice in modern medicine to use multimodal \nclinical information for medical diagnosis. For instance, apart from \nchest radiographs, thoracic physicians need to take into account each \npatient’s demographics (such as age and gender), the chief complaint \n(such as history of present and past illness) and laboratory test reports \nto make accurate diagnostic decisions. In practice, abnormal radio-\ngraphic patterns are first associated with symptoms mentioned in the \nchief complaint or abnormal results in the laboratory test report. Then, \nphysicians rely on their rich domain knowledge and years of training to \nmake optimal diagnoses by jointly interpreting such multimodal data1,2. \nReceived: 31 August 2022\nAccepted: 26 April 2023\nPublished online: 12 June 2023\n Check for updates\n1Department of Computer Science, The University of Hong Kong, Pokfulam, China. 2Department of Pulmonary and Critical Care Medicine, Med-X Center \nfor Manufacturing, Frontiers Science Center for Disease-related Molecular Network, West China Hospital, Sichuan University, Chengdu, China. 3AI Lab, \nDeepwise Healthcare, Beijing, China. 4Guangzhou Laboratory, Guangzhou, China. 5Department of Medical Imaging, Jinling Hospital, Nanjing University \nSchool of Medicine, Nanjing, China. 6Zhuhai International Eye Center and Provincial Key Laboratory of Tumor Interventional Diagnosis and Treatment, \nZhuhai People’s Hospital and the First Affiliated Hospital of Faculty of Medicine, Macau University of Science and Technology and University Hospital, \nGuangdong, China. 7Department of Big Data and Biomedical Artificial Intelligence, National Biomedical Imaging Center, College of Future Technology, \nPeking University, Beijing, China. 8Clinical Translational Research Center, West China Hospital, Sichuan University, Chengdu, China. 9These authors \ncontributed equally: Hong-Yu Zhou, Yizhou Yu, Chengdi Wang.  e-mail: yizhouy@acm.org; chengdi_wang@scu.edu.cn; kang.zhang@gmail.com; \nweimi003@scu.edu.cn\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 744\nArticle https://doi.org/10.1038/s41551-023-01045-x\nIn addition, MDT endows IRENE with the ability to perform represen-\ntation learning on top of unstructured raw text, which avoids tedious \ntext structuralization steps in non-unified approaches. For better \nhandling of the differences among modalities, IRENE introduces bidi-\nrectional multimodal attention to bridge the gap between token-level \nmodality-specific features and high-level diagnosis-oriented holistic \nrepresentations by explicitly encoding the interconnections among \ndifferent modalities. This explicit encoding process can be regarded \nas a complement to the holistic multimodal representation learning \nprocess in MDT.\nAs shown in Fig. 2a, MDT is primarily composed of embedding lay-\ners, bidirectional multimodal blocks and self-attention blocks. Because \nof the MDT, IRENE has the ability to jointly interpret multimodal clinical \ninformation simultaneously. Specifically, a free-form embedding layer \nis employed to convert unstructured and structured texts into uniform \ntext tokens (Fig. 2b). Meanwhile, a similar tokenization procedure is \nalso applied to each input image (Fig. 2c). Next, two bidirectional multi-\nmodal blocks (Fig. 2d) are stacked to learn fused mid-level representa-\ntions across multiple modalities. In addition to computing intramodal \nattention among tokens from the same modality, these blocks also \nexplicitly compute intermodal attention among tokens across different \nmodalities (Fig. 2e). These intra- and intermodal attentional operations \nare consistent with daily clinical practices, where physicians need to \ndiscover interconnected information within the same modality as well \nas across different modalities. In reality, these connections are often \nhidden among local patterns, such as words in the chief complaint and \nimage regions in radiographs, and different local patterns may refer to \nthe same lesion or the same disease. Therefore, such connections pro-\nvide mutual confirmations of clinical evidence and are helpful to both \nclinical and AI-based diagnosis. In bidirectional multimodal attention, \neach token can be regarded as the representation of a local pattern, \nand token-level intra- and intermodal attention respectively capture \nthe interconnections among local patterns from the same modality \nand across different modalities. In comparison, previous non-unified \nmethods make diagnoses on top of separate global representations of \ninput data in different modalities and thus cannot exploit the underly-\ning local interconnections. Finally, we stack ten self-attention blocks \n(Fig. 2f) to learn multimodal representations.\nIRENE shares some common traits with vision–language fusion \nmodels29–33, both of which aim to learn a joint multimodal representa-\ntion. However, one most noticeable difference exists in the roles of \ndifferent modalities. IRENE is designed for a scenario where multiple \nmodalities supply complementary semantic information, which can be \nfused and used to improve prediction performance. In contrast, recent \nvision–language fusion approaches31–33 heavily rely on the distillation \nand exploitation of common semantic information among different \nmodalities to provide supervision for model training.\nWe validated the effectiveness of IRENE on two tasks (Fig. 1b ):  \n(1) pulmonary disease identification and (2) adverse clinical outcome \nprediction in patients with COVID-19. In the first task, IRENE outper -\nformed previous image-only and non-unified diagnostic counterparts \nby approximately 12% and 9% (Fig. 1c), respectively. In the second task, \nwe employed IRENE to predict adverse clinical events in patients with \nCOVID-19, that is, admission to the intensive care unit (ICU), mechanical \nventilation (MV) therapy and death. Different from the first task, the \nsecond task relies more on textual clinical information. In this scenario, \nIRENE significantly outperformed non-unified approaches by over 7% \n(Fig. 1d). Particularly noteworthy is the nearly 10% improvement that \nIRENE achieved on death prediction, demonstrating the potential \nfor assisting doctors in taking immediate steps to save patients with \nCOVID-19. When compared to human experts (Fig. 1e) in pulmonary \ndisease identification, IRENE clearly surpassed junior physicians (with \n<7 yr of experience) in the diagnosis of all eight diseases and delivered \na performance comparable to or better than that of senior physicians \n(with >7 yr of experience) on six diseases.\nThe importance of exploiting multimodal clinical information has been \nextensively verified in the literature3–10 in different specialties, includ-\ning but not limited to radiology, dermatology and ophthalmology.\nThe above multimodal diagnostic workflow requires substantial \nexpertise, which may not be available in geographic regions with lim-\nited medical resources. Meanwhile, simply increasing the workload \nof experienced physicians and radiologists would inevitably exhaust \ntheir energy and thus increase the risk of misdiagnosis. T o meet the \nincreasing demand for precision medicine, machine-learning tech -\nniques11 have become the de facto choice for automatic yet intelligent \nmedical diagnosis. Among these techniques, the development of deep \nlearning12,13 endows machine-learning models with the ability to detect \ndiseases from medical images near or at the level of human experts14–18.\nAlthough artificial intelligence (AI)-based medical image diagno-\nsis has achieved tremendous progress in recent years, how to jointly \ninterpret medical images and their associated clinical context remains \na challenge. As illustrated in Fig. 1a, current multimodal clinical deci-\nsion support systems 19–23 mostly lean on a non-unified way to fuse \ninformation from multiple sources. Given a set of input data from \ndifferent sources, these approaches first roughly divide them into \nthree basic modalities, that is, images, narrative text (such as the chief \ncomplaint, which includes the history of present and past illness) and \nstructured fields (for example, demographics and laboratory test \nresults). Next, a text structuralization process is introduced to trans-\nform the narrative text into structured tokens. Then, data in different \nmodalities are fed to different machine-learning models to produce \nmodality-specific features or predictions. Finally, a fusion module is \nemployed to unify these modality-specific features or predictions for \nmaking final diagnostic decisions. In practice, according to whether \nmultiple input modalities are fused at the feature or prediction level, \nthese non-unified methods can be further categorized into early\n19–22 \nor late fusion23 methods.\nOne glaring issue with early and late fusion methods is that \nthey separate the multimodal diagnostic process into two rela -\ntively independent stages: modality-specific model training and \ndiagnosis-oriented fusion. Such a design has one obvious limitation: \nthe inability to encode the connections and associations among differ-\nent modalities. Another non-negligible drawback of these non-unified \napproaches lies in the text structuralization process, which is cum-\nbersome and still labour-intensive, even with the assistance of mod -\nern natural language processing (NLP) tools. On the other hand, \ntransformer-based architectures\n24 are poised to broadly reshape \nNLP25 and computer vision 26. Compared with convolutional neural \nnetworks27 and word embedding algorithms28,29, transformers24 impose \nfew assumptions about the input data form and thus have the potential \nto learn higher-quality feature representations from multimodal input \ndata. More importantly, the basic architectural component in trans -\nformers (that is, the self-attention block) remains nearly unchanged \nacross different modalities 25,26, providing an opportunity to build \na unified yet flexible model to conduct representation learning on \nmultimodal clinical information.\nIn this paper, we present IRENE, a unified AI-based medical diag-\nnostic model designed to make decisions by jointly learning holistic \nrepresentations of medical images, unstructured chief complaint \nand structured clinical information. T o the best of our knowledge, \nIRENE is presumably the first medical diagnostic approach that uses a \nsingle, unified AI model to conduct holistic representation learning on \nmultimodal clinical information simultaneously, as shown in Fig. 1a. At \nthe core of IRENE are the unified multimodal diagnostic transformer \n(MDT) and bidirectional multimodal attention blocks. MDT is a new \ntransformer stack that directly produces diagnostic results from multi-\nmodal input data. This new algorithm enables IRENE to take a different \napproach from previous non-unified methods by learning holistic rep-\nresentations from multimodal clinical information progressively while \neliminating separate paths for learning modality-specific features.  \nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 745\nArticle https://doi.org/10.1038/s41551-023-01045-x\nJunior physician (average) Senior physician (average)\nP = 9.6 × 10 –5\nP = 3.4 × 10 –4\nPulmonary disease identification Adverse outcome prediction of COVID-19\nRadiograph\nChief complaint\nAge: 43 Sex: Male\nHaemoglobin: 0.72\nTotal bilirubin: 0.54\nTotal cholesterol: 0.83\nCreatinine: 0.45\nI started smoking about\nten years ago and\nstarted coughing about\na month ago …\n…\nDemographics\nand lab test results\nModel1\na b\nc\ne\nd\nModel2\nModel3\nDiagnosis\nDiagnosis\nNon-unified IRENE\nFusion module\nText structuring\nTask1: Pulmonary\ndisease identification\nTraining set\n(Nov. 2008–Jun. 2018\nValidation set\n(Jun. 2018–Dec. 2018)\nTesting set\n(Dec. 2018–May 2019)\n100.0\nAUROC\nTrue positive rate\nFalse positive rate\nAUPRC\n97.5\n95.0\n92.5\n90.0\n87.5\n85.0\n82.5\n80.0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nTrue positive rate\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nFalse positive rate\n0 0.2 0.4 0.6 0.8 1.0\nImage only\nCOPD\nOur IRENE (AUC = 0.922) Our IRENE (AUC = 0.907) Our IRENE (AUC = 0.954) Our IRENE (AUC = 0.921)\nOur IRENE (AUC = 0.934) Our IRENE (AUC = 0.918) Our IRENE (AUC = 0.914) Our IRENE (AUC = 0.924)\nBronchiectasis Pneumothorax Pneumonia\nILD Tuberculosis Lung cancer Pleural eﬀusion\nNon-unified\nmodel\nIRENEMultimodal\ntransformer\nImage only Non-unified\nmodel\nIRENEMultimodal\ntransformer\n70\n65\n60\n55\n50\n45\n40\n35\n30\nTraining set (70%)\n(17 hospitals)\nTesting set\n(external 9 hospitals)\nTask2: Clinical outcome\nprediction of COVID-19\nValidation set (30%)\n(17 hospitals)\nFig. 1 | IRENE. a, Contrasting the previous non-unified multimodal diagnosis \nparadigm with IRENE. IRENE eliminates the tedious text structuralization \nprocess, separate paths for modality-specific feature extraction and the \nmultimodal feature fusion module in traditional non-unified approaches. \nInstead, IRENE performs multimodal diagnosis with a single unified \ntransformer. b, Scheme for splitting an original dataset into training, validation \nand testing sets for pulmonary disease identification (left) and adverse clinical \noutcome prediction of COVID-19 (right). c ,d, Comparison of the experimental \nresults from the image-only models, non-unified early fusion methods, \nmultimodal transformer (that is, Perceiver) and IRENE in the two tasks in b . We \ncompared the mean performance of IRENE and the multimodal transformer \nusing independent two-sample t -test (two-sided). Specifically, we repeated \neach experiment ten times with different random seeds, after which P  values \nwere calculated. e, Comparison of IRENE with junior (<7 yr of experience, \nn = 2) and senior (>7 yr of experience, n  = 2) physicians; average performance \nreported for each group. IRENE surpasses the diagnosis performance of junior \nphysicians while performing competitively with senior experts. AUC, area \nunder the curve.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 746\nArticle https://doi.org/10.1038/s41551-023-01045-x\nResults\nDataset characteristics for multimodal diagnosis\nThe first dataset focused on pulmonary diseases. We retrospectively \ncollected consecutive chest X-rays from 51,511 patients between 27 \nNovember 2008 and 31 May 2019 at West China Hospital, which is the \nlargest tertiary medical centre in western China covering a 100 million \npopulation. Each patient is associated with at least one radiograph, a \nshort piece of unstructured chief complaint, history of present and \npast illness, demographics and a complete laboratory test report. The \ndataset is built for eight pulmonary diseases: chronic obstructive pul-\nmonary disease (COPD), bronchiectasis, pneumothorax, pneumonia, \ninterstitial lung disease (ILD), tuberculosis, lung cancer and pleural \neffusion. Discharge diagnoses were extracted from discharge summary \nreports following a standard process described in a previous study16 \nand taken as the ground-truth disease labels. The discharge summary \nreports were produced as follows. An initial report was written by a \njunior physician, which was then reviewed and confirmed by a senior \nphysician. In case of any disagreement, the final decision was made by \na departmental committee comprising at least three senior physicians.\nThe built dataset consisted of 72,283 data samples, among which \n40,126 samples were normal. The distribution of diseases (that is, the \nnumber of relevant cases) is as follows: COPD (4,912), bronchiectasis \n(676), pneumothorax (2,538), pneumonia (21,409), ILD (3,283), tuber-\nculosis (938), lung cancer (2,651) and pleural effusion (4,713). The \nperformance metric is the area under the receiver operating character-\nistic curve (AUROC). We split this dataset into training, validation and \ntesting sets according to each patient’s admission date. Specifically, the \ntraining set included 44,628 patients admitted between 27 November \nNorm\nMLP\nNorm Norm\nNorm\nMLP\nBidirectional multimodal\nattention\nFree-form embedding\na\nb\ne f\nc\nd\nChiComp LabTest Sex Radiograph\nImage embedding\nBidirectional multimodal attention block ×2\nSelf-attention block ×10\nClassification head\nPulmonary disease annotations\nTokenization Linear proj.\nLabTest Sex\nClinical text tokens\nNorm\nNorm\nMLP\nSelf-attention\nClinical text tokens Image patch tokens\nUnified tokens\nPIDropout\nConvolution\nRadiograph\nImage patch tokens\nAge\nLinear proj.\nAge\nMat. mul.\nQ T KT VT VI K1 Q I\nScaled\nsoftmax\nMat. mul.\nMat. mul.\nScaled\nsoftmax\nMat. mul.\nMat. mul.\nScaled\nsoftmax\nMat. mul.\nMat. mul.\nScaled\nsoftmax\nMat. mul.\nClinical text tokens Image patch tokens\nDropout PI\nTokenization\nI started smoking ten years ago\nChiComp\nLinear proj. Linear proj.\nUnified tokensClinical text tokens Image patch tokens\nFig. 2 | Network architecture of IRENE. a, Overall workflow of IRENE in the \nfirst task, that is, pulmonary disease identification. The input data consist of \nfive parts: the chief complaint (ChiComp), laboratory test results (LabT est), \ndemographics (sex and age) and radiograph. Our MDT includes two bidirectional \nmultimodal attention blocks and ten self-attention blocks. The training process \nis guided by pulmonary disease annotations provided by human experts.  \nb, Encoding different types of clinical text in the free-form embedding. \nSpecifically, IRENE accepts unstructured chief complaints as part of the input.  \nc, Encoding a radiograph as a sequence of image patch tokens. d, Detailed design \nof a bidirectional multimodal attention block, which consists of two-layer \nnormalization layers (Norm), one bidirectional multimodal attention layer \nand one MLP. e, Detailed attention operations in the bidirectional multimodal \nattention layer, where representations across multiple modalities are learned \nand fused simultaneously. f, Detailed architecture of a self-attention block. PI, \nposition injection.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 747\nArticle https://doi.org/10.1038/s41551-023-01045-x\n2008 and 1 June 2018. The validation set included 3,325 patients admit-\nted between 2 June 2018 and 1 December 2018. Finally, the trained and \nvalidated IRENE system was tested on 3,558 patients admitted between \n2 December 2018 and 31 May 2019. Although this was a retrospective \nstudy, our data splitting scheme followed the practice of a prospective \nstudy, thus creating a more challenging and realistic setting to verify \nthe effectiveness of different multimodal medical diagnosis systems, \nin comparison to data splitting schemes based on random sampling.\nThe second dataset, MMC (that is, the multimodal COVID-19 \ndataset)19, on which IRENE was trained and evaluated, consisted of \nchest computed tomography (CT) scan images and structured clinical \ninformation (for example, chief complaint that comprises comorbidi-\nties and symptoms, demographics, laboratory test results and so on) \ncollected from patients with COVID-19. The CT images were associ -\nated with inpatients with laboratory-confirmed COVID-19 infection \nbetween 27 December 2019 and 31 March 2020. There were three types \nof adverse event that could happen to patients in MMC, namely admis-\nsion to ICU, MV and death. The training and validation sets came from 17 \nhospitals and the training set had 1,164 labelled cases (70%), while the \nvalidation set had 498 labelled ones (30%). Next, we chose the trained \nmodel with the best performance on the validation set and tested it \non the independent testing set, which comprised 700 cases collected \nfrom 9 external medical centres. The distribution of the three events \nin the testing set was as follows: ICU (155), MV (94), death (59). This was \nan imbalanced classification problem where the majority of patients \ndid not have any adverse outcomes. Against this background, we used \nthe area under the precision-recall curve (AUPRC) instead of AUROC \nas the performance metric, as the former focused more on identifying \nadverse events (that is, ICU, MV and death).\nPulmonary disease identification\nTable 1 and Fig. 3 present the experimental results from IRENE and other \nmethods on the dataset for pulmonary disease identification. As shown \nin Table 1, IRENE significantly outperformed the image-only model, \nthe traditional non-unified early19 and late fusion23 methods and two \nrecent state-of-the-art transformer-based multimodal methods (that \nis, Perceiver30 and GIT33) in identifying pulmonary diseases. Overall, \nIRENE achieved the highest mean AUROC of 0.924 (95% CI: 0.921, 0.927), \nabout 12% higher than the image-only model (0.805, 95% CI: 0.802, \n0.808) that only takes radiographs as the input. In comparison with \ndiagnostic decisions made by non-unified early fusion (0.835, 95% CI: \n0.832, 0.839) and late fusion (0.826, 95% CI: 0.823, 0.828) methods, \nIRENE maintained an advantage of at least 9%. Comparing IRENE to \nGIT (0.848, 95% CI: 0.844, 0.850), we observed an advantage of over 7%. \nEven when compared to Perceiver, the transformer-based multimodal \nclassification model developed by DeepMind, IRENE still delivered \ncompetitive results, surpassing Perceiver (0.858, 95% CI: 0.855, 0.861) \nby over 6%. When carefully checking each disease and comparing IRENE \nagainst the previous best result among all five baselines, we observed \nthat among all eight pulmonary diseases, IRENE achieved the largest \nimprovements on bronchiectasis (12%), pneumothorax (10%), ILD (10%) \nand tuberculosis (9%).\nWe also compared IRENE against human experts who were divided \ninto two groups: one group of two junior physicians (with <7 yr of \nexperience) and a second group of two senior physicians (with ≥7 yr \nof experience). For better comparison, we present the average perfor-\nmance within each group in Fig. 1e. Specifically, we extracted annota-\ntions by human experts from electronic discharge diagnosis records. \nNotably, all physicians from the reader study did not participate in \ndata annotation. We observed that IRENE exhibited advantages over \nthe junior group on all eight pulmonary diseases, especially in the \ndiagnosis of bronchiectasis ( junior, false positive rate (FPR): 0.29, true \npositive rate (TPR): 0.58), pneumonia ( junior, FPR: 0.37, TPR: 0.76), \nILD ( junior, FPR: 0.09, TPR: 0.63) and pleural effusion ( junior, FPR: \n0.35, TPR: 0.86). Compared with the senior group, IRENE was advan-\ntageous in the diagnosis of pneumonia (senior, FPR: 0.21, TPR: 0.80), \ntuberculosis (senior, FPR: 0.07, TPR: 0.17) and pleural effusion (senior, \nFPR: 0.25, TPR: 0.77). In addition, IRENE performed comparably with \nsenior physicians on COPD (senior, FPR: 0.07, TPR: 0.76), ILD (senior, \nFPR: 0.09, TPR: 0.71) and pneumothorax (senior, FPR: 0.08, TPR: 0.79) \nwhile showing slightly worse performance on bronchiectasis (senior, \nFPR: 0.12, TPR: 0.82) and lung cancer (senior, FPR: 0.08, TPR: 0.73).\nAdverse clinical outcome prediction in patients with COVID-19\nTriage of patients with COVID-19 heavily depends on joint interpreta-\ntion of chest CT scans and other non-imaging clinical information. \nIn this scenario, IRENE exhibited even more advantages than it did \nin the pulmonary disease identification task. As shown in Table 2 , \nIRENE consistently achieved impressive performance improvements \non the prediction of the three adverse clinical outcomes for patients \nwith COVID-19; that is, admission to ICU, MV and death. In terms of \nmean AUPRC, IRENE (0.592, 95% CI: 0.500, 0.682) outperformed the \nimage-only model (0.307, 95% CI: 0.237, 0.391), early fusion model22 \n(0.521, 95% CI: 0.435, 0.614) and late fusion model23 (0.503, 95% CI: \n0.422, 0.598) by nearly 29%, 7% and 9%, respectively. As for specific \nclinical outcomes, IRENE (0.712, 95% CI: 0.587, 0.834) achieved about \n5% AUPRC gain over the non-unified early fusion method (0.665, 95% \nCI: 0.548, 0.774) in the prediction of admission to ICU. Similarly, in the \nprediction of MV, IRENE achieved a >6% performance improvement \nwhen compared with the early fusion model. Last but not least, IRENE \n(0.441, 5% CI: 0.270, 0.617) was much more capable of predicting death \nthan the image-only model (0.192, 95% CI: 0.073, 0.333), early fusion \nTable 1 | Comparison with baseline models in the task of pulmonary disease identification\nMethod Mean COPD Bronchiectasis Pneumothorax Pneumonia ILD Tuberculosis Lung cancer Pleural \neffusion\nImage-only 0.805  \n(0.802, 0.808)\n0.847  \n(0.845, 0.851)\n0.746  \n(0.743, 0.748)\n0.789  \n(0.786, 0.791)\n0.845  \n(0.843, 0.848)\n0.799  \n(0.796, 0.801)\n0.769  \n(0.765, 0.772)\n0.825  \n(0.821, 0.830)\n0.819  \n(0.817, 0.822)\nEarly fusion 0.835  \n(0.832, 0.839)\n0.895  \n(0.893, 0.898)\n0.772  \n(0.768, 0.775)\n0.810  \n(0.807, 0.812)\n0.873  \n(0.870, 0.875)\n0.824  \n(0.822, 0.827)\n0.793  \n(0.791, 0.796)\n0.871  \n(0.868, 0.875)\n0.842  \n(0.839, 0.845)\nLate fusion 0.826  \n(0.823, 0.828)\n0.888  \n(0.885, 0.890)\n0.765  \n(0.763, 0.767)\n0.822  \n(0.820, 0.825)\n0.870  \n(0.868, 0.872)\n0.804  \n(0.802, 0.805)\n0.770  \n(0.767, 0.772)\n0.839  \n(0.836, 0.841)\n0.850  \n(0.847, 0.852)\nGIT 0.848  \n(0.844, 0.850)\n0.911  \n(0.907, 0.913)\n0.798  \n(0.796, 0.800)\n0.824  \n(0.821, 0.827)\n0.895  \n(0.893, 0.898)\n0.819  \n(0.816, 0.821)\n0.807  \n(0.804, 0.810)\n0.872  \n(0.871, 0.873)\n0.858  \n(0.855, 0.860)\nPerceiver 0.858  \n(0.855, 0.861)\n0.910  \n(0.907, 0.912)\n0.788  \n(0.784, 0.791)\n0.846  \n(0.842, 0.850)\n0.903  \n(0.901, 0.906)\n0.830  \n(0.827, 0.833)\n0.825  \n(0.823, 0.828)\n0.890  \n(0.887, 0.892)\n0.872  \n(0.869, 0.874)\nIRENE 0.924  \n(0.921, 0.926)\n0.922  \n(0.920, 0.925)\n0.907  \n(0.903, 0.910)\n0.954  \n(0.952, 0.957)\n0.921  \n(0.918, 0.923)\n0.934  \n(0.929, 0.937)\n0.918  \n(0.917, 0.921)\n0.914  \n(0.911, 0.917)\n0.924  \n(0.921, 0.926)\nThe baseline models include the image-only model, the early fusion method, the late fusion approach and two recent transformer-based multimodal classification models (that is, GIT and \nPerceiver). The evaluation metric is AUROC, with 95% confidence intervals in brackets.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 748\nArticle https://doi.org/10.1038/s41551-023-01045-x\nmodel (0.346, 95%: 0.174, 0.544) and late fusion model (0.335, 95% \nCI: 0.168, 0.554). Compared with two transformer-based multimodal \nmodels (that is, GIT and Perceiver), we observed an advantage of over \n6% on average.\nImpact of different modules and modalities in IRENE\nT o investigate the impact of different modules and modalities, we con-\nducted thorough ablative experiments and report their results in Table 3.  \nFirst, we investigated the impact of bidirectional multimodal attention \nblocks (rows 0–2). We found that replacing all bidirectional multimodal \nattention blocks with self-attention blocks led to ~7% performance drop \n(from 0.924 to 0.858) in pulmonary disease identification. This phenom-\nenon verified our intuition that directly learning progressively fused rep-\nresentations from raw data would deteriorate diagnosis performance. \nIn contrast, simply increasing the number of bidirectional multimodal \nattention blocks from two to six did not bring obvious performance \nimprovements (from 0.924 to 0.905), indicating that using two suc-\ncessive bidirectional multimodal attention blocks could be an optimal \nchoice in IRENE. In row 3, we presented the result of using unidirectional \nattention (that is, text-to-image attention). Comparing row 0 with row 3,  \nRadiograph\n(79.8%)\nChiComp\n(16.3%)\nLabTest\n(2.4%)\na\nd\nf\ng\ne\nb cDemographics\n(1.5%)\nOther 90 items\n(96.0%)\nPaCO 2\n(1.7%)\nPaO 2\n(2.3%)\nAge\n(56.8%)\nSex\n(43.2%)\n0.18 0.20\n0.05\n0.11\n0.02 0.06\n0.13\nWith cross\nattention\nWithout cross\nattention\n60\n55\n50\nSum of similarity scores of\nimportant tokens\n45\n40\n35\n30\n0.10\n0 1 0\n0 1\n1 0 1\n0.06 0.02\n0.07\nThick, yellow sputum and dyspnoea after activity for over two years\nFig. 3 | Attention analysis. a, Attention allocated to different types of input \nfrom a patient with COPD, that is, the radiograph, ChiComp, LabT est and \ndemographics. b, Relative importance of laboratory test items. c, Comparison of \nthe importance of sex and age in making a diagnostic decision. d, Visualization \nof the attention assigned to individual pixels in the radiograph. Left: input \nchest X-ray. Right: pixels with different attention values. e, The impact of cross \nattention on the relevance and importance of high-ranking words (from chief \ncomplaints) and image patches (from radiographs) in the pulmonary disease \nidentification task. Specifically, we define high-ranking words and patches as \nthose whose tokens have top 25% cosine similarity scores with the CLS token.  \nf, Normalized importance of every word in the chief complaint. g, Visualization \nof the distribution of attention between every image patch and each of the top 3 \nranked words. The colour bars in d and g illustrate the confidence of IRENE about \na pixel being abnormal, where a bright colour stands for high confidence and a \ndark colour denotes low confidence.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 749\nArticle https://doi.org/10.1038/s41551-023-01045-x\nwe observed that our bidirectional design brought a 4% performance \ngain (from 0.884 to 0.924). Next, we studied the impact of clinical texts \n(rows 4 and 5). The first observation was that using the complementary \nnarrative chief complaint substantially boosted the diagnostic perfor-\nmance because removing chief complaint from the input data reduced \nmodel performance by 6% (from 0.924 to 0.860). Apart from the chief \ncomplaint, we also studied the impact of laboratory test results (row 5).  \nWe observed that including laboratory test results brought about a \n4% performance gain (from 0.882 to 0.924). Then, we investigated the \nimpact of tokenization procedures. We saw that modelling the chief \ncomplaint and laboratory test results of a patient as a sequence of \ntokens (row 0) did perform better than directly passing an averaged \nrepresentation (row 6) to the model. This improvement brought by \nthe tokenization of the chief complaint and laboratory test results veri-\nfied the advantage of token-level intra- and intermodal bidirectional \nmultimodal attention, which exploited local interconnections among \nthe word tokens of the clinical text and the image patch tokens of the \nradiograph in the input data. Lastly, we investigated the impact of the \ninput image in IRENE (row 7) and observed a substantial performance \ndrop (from 0.924 to 0.543). This phenomenon indicated the vital role \nof the input radiograph in pulmonary disease identification. We then \ninvestigated the impact of chief complaints and laboratory test results \non each respiratory disease (Extended Data Fig. 1). When we removed \neither chief complaints or the laboratory test results from the input, \nthe performance decreased on each disease. Specifically, we found that \nintroducing the chief complaint could be most helpful for the diagnosis \nof pneumothorax, lung cancer and pleural effusion, while the laboratory \ntest results affected the diagnosis of bronchiectasis and tuberculosis the \nmost. Clinical interpretations can be found in Supplementary Note 1.\nAttention visualization results\nFigure 3 provides attention visualization results for a case with COPD. In \nFig. 3a, we see that the image modality (that is, the radiograph) played \na significant role in the diagnostic process, and its weight was nearly \n80% in the final decision. The chief complaint was the second most \nimportant factor, accounting for roughly 16% weight. As Fig. 3b shows, \nPaO2 (oxygen pressure in arterial blood) and PaCO2 (partial pressure of \ncarbon dioxide in arterial blood) were the two most important labora-\ntory test items, which are consistent with the observations reported \nin the literature 34. Nonetheless, we see that the total weight of the \nremaining 90 test items was quite large, with distribution over these \n90 laboratory test items being nearly uniform. The reason might be that \nthese laboratory test items could help rule out other diseases. Figure 3c  \nshows that from the perspective of IRENE, age was a more critical factor \nthan sex. Figure 3d provides the attention map of the radiograph, imply-\ning that IRENE would refer to hilar enlargement, hyper-expansion and \nflattened diaphragm as the most important pieces of evidence for the \ndiagnosis of COPD. In addition, IRENE could also identify large black \nareas due to bullae as relatively important evidence. Figure 3e sum-\nmarizes the experimental results with and without cross attention, \nwhere we present the sum of similarity scores of important (top 25%) \ntokens (that is, words and image patches) with the CLS token which is \nthe start token that aggregates the information of the rest tokens. We \nfound that with cross attention, the sum of similarity scores became \nlarger, indicating that cross attention has improved the identification \nof important tokens compared with the model without cross attention. \nIn Fig. 3f, IRENE recognized ‘sputum’ , ‘dyspnoea’ and ‘years’ as the three \nmost important words in the chief complaint. Figure 3g provides the \ncross-attention maps between each of the top three important words \nand the image. The word ‘sputum’ is primarily associated with the tra-\nchea and the lower pulmonary lobes in the image. The high attention \narea of the trachea could be reasonable because trachea is often the \nlocation where sputum might occur. The high attention region in the \nleft lower lobe had reduced vascular markings, while both the left and \nright lower lobes of the lungs were hyperinflated. Hyperinflated lungs \nand reduced vascular markings are common symptoms of COPD, which \noften has abnormal sputum production. Our model has also associated \nthe word ‘dyspnoea’ with most areas of the lungs in the image because \ndyspnoea can be caused by a variety of pulmonary abnormalities that \ncould occur anywhere in the lungs. Lastly, our model has identified the \nareas surrounding the bronchi as the image regions associated with the \nword ‘years’ , which implies ‘years’ should be associated with chronic \ndiseases, such as chronic bronchitis, which is often part of COPD.\nDiscussion\nIRENE is more effective than the previous non-unified early \nand late fusion paradigm in multimodal medical diagnosis\nThis is the most prominent observation obtained from our experi-\nmental results, and it holds for the tasks of pulmonary disease identi-\nfication and the triage of patients with COVID-19. Specifically, IRENE \noutperforms previous early fusion and late fusion methods by an aver-\nage of 9% and 10%, respectively, for identifying pulmonary diseases. \nMoreover, IRENE achieves about 3% performance gains on all eight \ndiseases and substantially improves the diagnostic performance on \nfour diseases (that is, bronchiectasis, pneumothorax, ILD and tuber-\nculosis) by boosting their AUROC by over 10%. We believe that these \nperformance benefits are closely related to several capabilities of \nIRENE. First, IRENE is built on top of a unified transformer (that is, \nMDT). MDT directly produces diagnostic decisions from multimodal \ninput data and learns holistic multimodal representations progres-\nsively and implicitly. In contrast, the traditional non-unified approach \ndecomposes the diagnosis problem into several components which, in \nmost cases, consist of data structuralization, modality-specific model \ntraining and diagnosis-oriented fusion. In practice, these components \nare hard to optimize and may prevent the model from learning holistic \nand diagnosis-oriented features. Second, inspired by the daily activi-\nties of physicians, IRENE applies intra-directional and bidirectional \nintermodal attention to tokenized multimodal data for exploiting the \nlocal interconnections among complementary modalities. In contrast, \nthe previous non-unified paradigm directly makes use of the extracted \nglobal modality-specific representations or predictions for diagno-\nsis. In practice, the token-level attentional operations in bidirectional \nmultimodal attention helps capture and encode the interconnections \namong the local patterns of different modalities into the fused repre-\nsentations. Furthermore, IRENE is designed to conduct representation \nlearning directly on unstructured raw texts. In contrast, the previous \nnon-unified approach relies on non-clinically pre-trained NLP models \nto provide word embeddings, which inevitably distracts the diagnosis \nsystem from its intended functionality.\nTable 2 | Comparison with baseline models in the task \nof adverse clinical outcome prediction in patients with \nCOVID-19\nMethod Mean Admission to \nICU\nNeed for MV Death\nImage-only 0.307  \n(0.237, 0.391)\n0.482  \n(0.355, 0.636)\n0.247  \n(0.136, 0.398)\n0.192  \n(0.073, 0.333)\nEarly fusion 0.521  \n(0.435, 0.614)\n0.665  \n(0.548, 0.774)\n0.551  \n(0.397, 0.699)\n0.346  \n(0.174, 0.544)\nLate fusion 0.503  \n(0.422, 0.598)\n0.647  \n(0.535, 0.759)\n0.533  \n(0.388, 0.685)\n0.330  \n(0.164, 0.531)\nGIT 0.514  \n(0.442, 0.605)\n0.653  \n(0.546, 0.743)\n0.554  \n(0.411, 0.702)\n0.335  \n(0.168, 0.554)\nPerceiver 0.526  \n(0.448, 0.611)\n0.652  \n(0.529, 0.771)\n0.566  \n(0.406, 0.715)\n0.360  \n(0.201, 0.543)\nIRENE 0.592  \n(0.500, 0.682)\n0.712  \n(0.587, 0.834)\n0.624  \n(0.473, 0.754)\n0.441  \n(0.270, 0.617)\nThe evaluation metric is AUPRC, with 95% confidence intervals in brackets.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 750\nArticle https://doi.org/10.1038/s41551-023-01045-x\nThe superiority of the aforementioned abilities has been partly \nverified in the second task: the prediction of adverse outcomes in \npatients with COVID-19. From Table 2, we see that IRENE holds a 7% \naverage performance gain over the early fusion approach and an aver-\nage of 9% advantage over the late fusion one. This performance gain \nis a little lower than that in the pulmonary disease identification task \nas there are no unstructured texts in the MMC dataset that IRENE can \nuse. Nonetheless, IRENE can still leverage its unified and bidirectional \nmultimodal attention mechanisms to better serve the goal of rapidly \ntriaging patients with COVID-19. For example, IRENE boosts the per-\nformance of MV and death prediction by 7% and 10%, respectively. \nSuch substantial performance improvements brought by IRENE are \nvaluable in the real world for allocating appropriate medical resources \nto patients in a timely manner, as medical resources are usually limited \nduring a pandemic.\nIRENE provides a better transformer-based choice for jointly \ninterpreting multimodal clinical information\nWe compared IRENE to GIT 33 and Perceiver 30, two representative \ntransformer-based models that fuse multimodal information for \nclassification. GIT performs multimodal pre-training on tens of mil-\nlions of image-text pairs by using the common semantic information \namong different modalities as supervision signals. However, these \ncharacteristics have two obvious deficiencies in the medical diagnosis \nscenario. First, it is much harder to access multimodal medical data in \nthe amount of the same order of magnitude. Second, multimodal data \nin the medical diagnosis scenario provide complementary instead \nof common semantic information. Thus, it is impractical to perform \nlarge-scale multimodal pre-training, as in GIT, using a limited amount of \nmedical data. These deficiencies are also reflected in the experimental \nresults. For instance, the average performance of GIT is about 7% and 8% \nlower than that of IRENE in the pulmonary disease identification task \nand adverse outcome prediction of COVID-19 task, respectively. These \nadvantages show that token-level bidirectional multimodal attention \nin IRENE can effectively use a limited amount of multimodal medical \ndata and exploit complementary semantic information.\nPerceiver simply concatenates multimodal input data and takes \nthe resulting one-dimensional (1D) sequence as the input instead of \nlearning fused representations among modality-specific low-level \nembeddings as in IRENE. This poses a potential problem: the modality \nthat makes up the majority of the input would have a larger impact \non final diagnostic results. For example, since an image often has a \nmuch larger number of tokens than a text, Perceiver would inevitably \nassign more weight to the image instead of the text when making \npredictions. However, it is not always true that images play a more \nimportant role in daily clinical decisions. T o some extent, this point \nis also reflected in our experimental observations. For example, \nPerceiver yields clear performance improvements (2% gain on aver -\nage in Table 1) over the early fusion model in identifying pulmonary \ndiseases where the input radiograph serves as the main informa -\ntion source. However, in the task of rapidly triaging patients with \nCOVID-19, the performance of Perceiver is only comparable to that \nof the early fusion method. The underlying reason is that CT images \nare not as helpful in this task as radiographs in pulmonary disease \nidentification. In contrast, IRENE demonstrates satisfactory perfor -\nmance in both tasks by learning holistic multimodal representations \nthrough bidirectional multimodal attention. Our method encourages \nfeatures from different modalities to evenly blend into each other, \nwhich prevents the learned representations from being dominated \nby high-dimensional inputs.\nIRENE helps reduce reliance on text structuralization in the \ntraditional workflow\nIn traditional non-unified multimodal medical diagnosis methods, \nthe usual way to deal with unstructured texts is text structuralization. \nRecent text structuralization pipelines in non-unified approaches 19–23 \nseverely rely on artificial rules and the assistance of modern NLP \ntools. For example, text structuralization requires human annota -\ntors to manually define a list of alternate spellings, synonyms and \nabbreviations for structured labels. On top of these preparations, \nspecialized NLP tools are developed and applied to extract structured \nfields from unstructured texts. As a result, text structuralization \nsteps are not only cumbersome but also costly in terms of labour and \ntime. In comparison, IRENE abandons such tedious structuralization  \nsteps by directly accepting unstructured clinical texts as part of \nthe input.\nOutlook\nNLP technologies, particularly transformers, have contributed sig -\nnificantly to the latest AI diagnostic tools using either text-based elec-\ntronic health records35 or images36. We have described an AI framework \nconsisting of a unified MDT and bidirectional multimodal attention \nblocks. IRENE is distinct from previous non-unified methods in that \nit progressively learns holistic representations of multimodal clini-\ncal data while avoiding separate paths for learning modality-specific \nfeatures in non-unified techniques. This approach may be enhanced \nby the latest development of large language models37,38.\nIn real-world scenarios, IRENE may help streamline patient care, \nsuch as triaging patients and differentiating between those patients \nwho are likely to have a common cold from those who need urgent inter-\nvention for a more severe condition. Furthermore, as the algorithms \nbecome increasingly refined, these frameworks could become a diag-\nnostic aid for physicians and assist in cases of diagnostic uncertainty \nor complexity, thus not only mimicking physician reasoning but also \nTable 3 | An ablation study of IRENE, removing or replacing individual components\nRow HA (2) HA (0) HA (6) Unidirection Image ChiComp LabTest Tokenization Mean\n0 √ √ √ √ √ 0.924 (0.921, 0.926)\n1 √ √ √ √ √ 0.858 (0.850, 0.867)\n2 √ √ √ √ √ 0.905 (0.899, 0.910)\n3 √ √ √ √ √ √ 0.884 (0.880, 0.888)\n4 √ √ √ √ 0.860 (0.855, 0.864)\n5 √ √ √ √ 0.882 (0.873, 0.891)\n6 √ √ √ √ 0.894 (0.886, 0. 900)\n7 √ √ √ √ 0.543 (0.525, 0.569)\nHA (N) denotes the presence of N bidirectional multimodal attention block(s) in the MDT, while the remaining blocks are self-attention blocks (12 blocks in total). Image denotes the input \nradiograph. Unidirection means we only compute text-to-image attention in multimodal attention blocks. ChiComp stands for the chief complaint. LabTest denotes laboratory test results. \nTokenization stands for the tokenization procedures for the chief complaint and laboratory test results. For each row in the table, check marks denote the associated modules are used in the \nmodel while blank spaces indicate the associated modules are removed. The evaluation metric is AUROC, with 95% confidence intervals in brackets.\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 751\nArticle https://doi.org/10.1038/s41551-023-01045-x\nfurther enhancing it. The impact of our work may be most obvious in \nareas where there are few and uneven distribution of healthcare provid-\ners relative to the population.\nThere are several limitations that would need to be considered \nduring the deployment of IRENE in clinical workflows. First, the cur -\nrently used datasets are limited in both size and diversity. T o resolve \nthis issue, more data would need to be collected from additional medi-\ncal institutions, medical devices, countries and ethnic groups, with \nwhich IRENE can be trained to enhance its generalization ability under \na broader range of clinical settings. Second, the clinical benefits of \nIRENE need to be further verified. Thus, multi-institutional multina -\ntional studies would be needed to further validate the clinical utility \nof IRENE in real-world scenarios. Third, it is important to make IRENE \nadaptable to a changing environment, such as dealing with rapidly \nmutating SARS-CoV-2 viruses. T o tackle this challenge, the model could \nbe trained on multiple cohorts jointly or one could resort to other \nmachine-learning technologies, such as online learning. Moreover, \nIRENE fails to consider the problem of modal deficiency, where one \nor more modalities may be unavailable. T o deal with this problem, one \ncan refer to masked modelling25. For instance, during the training stage, \nsome modalities could be randomly masked to imitate the absence of \nthese modalities in clinical workflows.\nMethods\nImage and textual clinical data\nIn the pulmonary disease identification task, chest X-ray (CXR) images \nwere collected from West China Hospital. All CXRs were collected as part \nof the patients’ routine clinical care. For the analysis of CXR images, all \nradiographs were first de-identified to remove any patient-related infor-\nmation. The CXR images consisted of both anterior and posterior views. \nThere were three types of textual clinical data: the unstructured chief \ncomplaint (that is, history of present and past illness), demographics \n(age and gender) and laboratory test results. Specifically, the chief com-\nplaint is unstructured, while demographics and laboratory test results \nare structured. We set the maximum length of the chief complaint to \n40. If a patient’s chief complaint had more than 40 words, we only took \nthe first 40; otherwise, zero padding was used to satisfy the length \nrequirement. There were 92 results in each patient’s laboratory test \nreport (see Supplementary Note 2), most of which came from a blood \ntest. We normalized every test result by minimum-maximum (min-max) \nscaling so that every normalized value was between 0 and 1, where the \nminimum and maximum values in min-max scaling were determined \nusing the training set. In particular, −1 denoted missing values.\nIn the second task, that is, adverse clinical outcome prediction \nfor patients with COVID-19, the available clinical data were divided \ninto four categories: demographics (age and gender), the structured \nchief complaint consisting of comorbidities (7) and symptoms (9) and \nlaboratory test results (19) (see Supplementary Note 3 for more details). \nWe also applied median imputation to fill in missing values.\nInstitutional Review Board/Ethics Committees approvals were \nobtained from West China Hospital and all participating hospitals. \nAll patients signed a consent form. The research was conducted in a \nmanner compliant with the United States Health Insurance Portability \nand Accountability Act. It adhered to the tenets of the Declaration of \nHelsinki and complied with the Chinese Center for Disease Control and \nPrevention policy on reportable infectious diseases and the Chinese \nHealth and Quarantine Law.\nBaseline models\nWe include five baseline models in our experimental performance \ncomparisons, including the diagnosis model purely based on medi-\ncal images (denoted as Image-only), the traditional non-unified early \nand late fusion methods with multimodal input data and two recent \nstate-of-the-art transformer-based multimodal classification methods \n(that is, GIT and Perceiver). Implementation details are discussed below.\nImage-only. In the pulmonary disease identification task, we built the \npure medical image-based diagnosis model on top of ViT26, one of the \nmost well-known and widely adopted transformer-based deep neural \nnetworks for image understanding. Our ViT-like network architecture \nhad 12 blocks and each block consisted of one self-attention layer 24, \none multilayer perceptron (MLP) and two-layer normalization lay -\ners39. There were two fully connected (FC) layers in each MLP, where \nthe number of hidden nodes was 3,072. The input size of the first FC \nlayer was 768. Between the two FC layers, we inserted a GeLU activation \nfunction40. After each FC layer, we added a dropout layer41, where we \nset the dropout rate to 0.3. The output size of the second FC layer was \nalso 768. Each input image was divided into a number of 16 × 16 patches. \nThe output CLS token was used for performing the final classification. \nWe used the binary cross-entropy loss as the cost function during the \ntraining stage. Note that before the training stage, we performed super-\nvised ViT pre-training on MIMIC-CXR42 to obtain visual representations \nwith more generalization power. In the task of rapidly triaging patients \nwith COVID-19, as in ref. 22, we first segmented pneumonia lesions \nfrom CT scans, then trained multiple machine-learning models (that \nis, logistic regression, random forest, support vector machine, MLP \nand LightGBM) using image features extracted from the segmented \nlesion areas and finally chose the optimal model according to their \nperformance on the validation set.\nNon-unified early and late fusion.  There are a number of existing \nmethods using the archetypical non-unified approach to fuse mul -\ntimodal input data for diagnosis. For better adaptation to different \nscenarios, we adopted different non-unified models for different tasks. \nSpecifically, we modified the previously reported early fusion method19 \nfor our first task (that is, pulmonary disease identification). In practice, \na ViT model extracts image features from radiographs and the fea -\nture vector at its CLS token is taken as the representation of the input \nimage. Similar to the image-only baseline, supervised pre-training on \nMIMIC-CXR42 was applied to the ViT to obtain more powerful visual \nfeatures before we carried out the formal task. T o process the three \ntypes of clinical data (that is, the chief complaint, demographics and \nlaboratory test results), we employed three independent MLPs to \nconvert different types of textual clinical data to features, which were \nthen concatenated with the image representation. The rationale is that \nboth images and textual data should be represented in the same feature \nspace for the purpose of cross referencing. Since the chief complaint \nincludes unstructured texts, we first needed to transform them into \nstructured items. T o achieve this goal, we trained an entity recognition \nmodel to highlight relevant clinical symptoms in the chief complaint. \nNext, we used BERT25 to extract features for all such symptoms, to which \naverage pooling was applied to produce a holistic representation for \neach patient’s chief complaint. Then, we used a three-layer MLP to \nfurther transform this holistic feature into a latent space similar to that \nof the image representation. The input size of this three-layer MLP was \n768 and the output size was 512. The number of hidden nodes was 1,024. \nAfter each FC layer, we added a ReLU activation and a dropout layer, \nwith the dropout rate set to 0.3. Likewise, for laboratory test results, we \nalso applied an MLP with the same architecture but independent weight \nparameters to transform those test results into a 1D feature vector. The \ninput size of this laboratory test MLP was 92 and the output size was 512. \nThe MLP model for demographics had two FC layers, where the input \nsize was 2 and the output size was 512. The hidden layer had 512 nodes. \nThe feature fusion module included the concatenation operation and \na three-layer MLP, with the number of hidden nodes set to 1,024. The \noutput from the MLP in the feature fusion module was passed to the \nfinal classification layer for making diagnostic decisions. During the \ntraining stage, we jointly trained the ViT-like model and all MLPs using \nthe binary cross-entropy loss. As for the late fusion baseline, we com-\nbined the predictions of the image- and text-based classifiers following \nref. 23. Specifically, we trained a ViT model with radiographs and their \nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 752\nArticle https://doi.org/10.1038/s41551-023-01045-x\nassociated labels. T o construct the input to the text-based classifier, we \nconcatenated laboratory test results, demographics and the holistic \nrepresentation (obtained via averaging extracted features of symp -\ntoms, similar to the early fusion method) of the chief complaint. Then, \nwe forwarded the constructed input through a three-layer MLP, whose \ninput and output dimensions were 862 and 8, respectively. Then, we \ntrained the MLP with the same labels used for training the ViT model. \nFinally, we averaged the predicted probabilities of the image- and \ntext-based classifiers to obtain the final prediction.\nIn the second task, we followed a proposed early fusion method22, \nwhere image features, structured chief complaint (comorbidities and \nsymptoms) and laboratory test results had been concatenated as the \ninput. Then, we trained multiple machine-learning models and chose \nthe optimal model using previously introduced artificial rules 22. For \nthe late fusion baseline, we trained 5 machine-learning models (logistic \nregression, random forest, support vector machine, MLP and Light -\nGBM) each for image features, structured chief complaints and labora-\ntory test results following the protocol used in ref. 22. Then, we took \nthe average of the predicted probabilities of these 15 machine-learning \nmodels as the adverse outcome prediction.\nGIT. GIT33 is a generative image-to-text transformer that unifies vision–\nlanguage tasks. We took GIT-Base as a baseline in our comparisons. Its \nimage encoder is a ViT-like transformer and its text decoder consists \nof six standard transformer blocks 24. In practice, we fine-tuned the \nofficially released pre-trained model on our own datasets. For fair -\nness, we adopted the same set of fine-tuning hyperparameters used \nfor IRENE. In the pulmonary disease identification task, we first for -\nwarded each radiograph through the image encoder to extract an image \nfeature. Next, we concatenated this image feature with the averaged \nword embedding (using BERT) of the chief complaint as well as the \nfeature vectors of the demographics and laboratory test results. The \nconcatenated features were then passed to the text decoder to make \ndiagnostic predictions. In the task of adverse clinical outcome predic-\ntion for patients with COVID-19, we first averaged the image features of \nCT slices. Then, the averaged image feature was concatenated with the \nfeature vectors of the clinical comorbidities and symptoms, laboratory \ntest results and demographics. Next, we forwarded the concatenated \nmultimodal features through the text decoder to predict adverse \noutcomes for patients with COVID-19.\nPerceiver.  This is a very recent state-of-the-art transformer-based \nmodel 30 from DeepMind, proposed for tackling the classification \nproblem with multimodal input data. A variant of Perceiver 30, that \nis, Perceiver IO 43, introduces the output query on top of Perceiver \nto handle additional types of task. As making diagnostic decisions \ncan be considered as a type of classification, we adopted Perceiver \ninstead of Perceiver IO as one of our baseline models. Our Perceiver \narchitecture followed the setting for ImageNet classification 30,44 \nand had six cross-attention modules. Each cross-attention module \nwas followed by a latent transformer with six self-attention blocks. \nThe input of Perceiver consists of two arrays: the latent array and \nbyte array. Following ref. 30 , we initialized the latent array using a \ntruncated zero-mean normal distribution, with standard deviation \nset to 0.02 and truncation bounds set to (−2, 2). The byte array con-\nsisted of multimodal data. In the pulmonary disease identification \ntask, we first flattened the input image into a 1D vector. Then, we \nconcatenated it with the averaged word embedding (using BERT) of \nthe chief complaint as well as 1D feature vectors of the input demo -\ngraphics and laboratory test results. This resulted in a long 1D vec-\ntor, which was taken as the byte array. In the task of adverse clinical \noutcome prediction of COVID-19, we also flattened the input image \ninto a 1D vector, which was then concatenated with the feature vec-\ntors of the clinical comorbidities and symptoms, laboratory test \nresults and demographics. The learning process of Perceiver can be \nsummarized as follows: the latent array evolves by iteratively extract -\ning higher-quality features from the input byte array by alternating \ncross-attention and latent self-attention computations. Finally, the \ntransformed latent array serves as the representation used for diag -\nnosis. Note that similar to the image-only and non-unified baselines, \nwe pre-trained Perceiver on MIMIC-CXR 42. During pre-training, we \nused zero padding in the byte array for the non-existent clinical text \nin every multimodal input.\nIRENE\nIn practice, we forwarded multimodal input data (that is, medical \nimages and textual clinical information) to the MDT for acquiring \nprediction logits. During the training stage, we computed the binary \ncross-entropy loss between the logits and ground-truth labels. Spe-\ncifically, we used pulmonary disease annotations (8 diseases) and real \nadverse clinical outcomes (3 clinical events) as the ground-truth labels \nin the first and second tasks, respectively.\nMDT is a unified transformer, which primarily consists of two \nstarting layers for embedding the tokens from the input image and text, \nrespectively, two stacked bidirectional multimodal attention blocks \nfor learning fused mid-level representations by capturing intercon -\nnections among tokens from the same modality and across different \nmodalities, ten stacked self-attention blocks for learning holistic multi-\nmodal representations and enhancing their discriminative power, and \none classification head for producing prediction logits.\nThe multimodal input data in the pulmonary disease identification \ntask (that is, the first task) consisted of five parts: a radiograph, the \nunstructured chief complaint that includes history of present and past \nillness, laboratory test results, each patient’s gender and age, which \nwere denoted as xI, xcc, xlab, xsex and xage, respectively. We passed xI to a \nconvolutional layer, which produced a sequence of visual tokens. Next, \nwe added standard learnable 1D positional embedding21,23 and dropout \nto every visual token to obtain a sequence of image patch tokens XI\n1∶N. \nMeanwhile, we applied word tokenization to Xcc to encode each word \nfrom the unstructured chief complaint. Specifically, we used a \npre-trained BERT23 to generate an embedded feature vector for each \nword in xcc, after which we obtained a sequence of word tokens Xcc\n1∶Ncc. \nWe also applied a similar tokenization procedure to xlab, where min-max \nscaling was first employed to normalize every component of x lab. We \nthen passed each normalized component to a shared linear projection \nlayer to obtain a sequence of latent embeddings Xlab\n1∶Nlab. We also per-\nformed linear projections on x sex and xage to obtain encoded feature \nvectors Xsex  and Xage . Subsequently, we concatenated  \n{Xcc\n1∶Ncc ,Xlab\n1∶Nlab ,Xsex,Xage} together to produce a sequence of clinical text \ntokens XT\n1∶ ̂N, where ̂N = Ncc +Nlab +2. In practice, we set Ncc and Nlab to \n40 and 92, respectively.\nAs for the task of adverse clinical outcome prediction for patients \nwith COVID-19, its multimodal input data also consisted of five parts: \na set of CT slices, structured chief complaint (comorbidities and symp-\ntoms), laboratory test results, each patient’s gender and age, which are \ndenoted as xI, xcc, xlab, xsex and xage, respectively. Each CT slice was con-\nverted to a sequence of image patch tokens XI\n1∶N as in the first task. \nDifferent from the first task, the chief complaint was structured. T o \nconvert xcc to tokens, we conducted a shared linear projection to each \ncomponent, which generated a sequence of embeddings Xcc\n1∶Ncc. A linear \nprojection layer was applied to xlab to acquire Xlab\n1∶Nlab. As for xsex and xage, \nwe performed linear projections to obtain encoded Xsex and Xage as in \nthe first task. Finally, we directly concatenated {Xcc\n1∶Ncc ,Xlab\n1∶Nlab ,Xsex,Xage} \nto produce ̂N clinical text tokens XT\n1∶ ̂N, where ̂N = Ncc +Nlab +2. We set \nNcc and Nlab to 16 and 19, respectively.\nThe first two layers of MDT were two stacked bidirectional multi-\nmodal attention blocks. Suppose the input of the first bidirectional \nmultimodal attention block consists of Xl\nI and Xl\nT, where l(= 0) stands \nfor the layer index, X0\nI = XI\n1∶N denotes the assembly of image patch \ntokens and X0\nT = XT\n1∶ ̂N represents the bag of clinical text tokens. The \nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 753\nArticle https://doi.org/10.1038/s41551-023-01045-x\nprocess of generating the query, key and value matrices for each modal-\nity in the bidirectional multimodal attention block was as follows:\nQl\nI,Kl\nI\n,Vl\nI\n= LP(Norm(Xl\nI\n)),\nQl\nT,Kl\nT\n,Vl\nT\n= LP(Norm(Xl\nT\n)),\nwhere LP(⋅) and Norm(⋅) represent linear projection and layer normali-\nzation, respectively. The forward pass inside a bidirectional multimodal \nattention block could be summarized as:\n𝔛𝔛l\nI = Attention(Ql\nI,Kl\nI\n,Vl\nI\n)+λAttention(Ql\nI\n,Kl\nT,Vl\nT\n),\n𝔛𝔛l\nT = Attention(Ql\nT,Kl\nT\n,Vl\nT\n)+λAttention(Ql\nT\n,Kl\nI,Vl\nI\n),\nwhere Attention (Ql\nI,Kl\nI\n,Vl\nI\n) and Attention (Ql\nT,Kl\nT\n,Vl\nT\n) capture the \nintramodal connections in the image and text modalities, respectively. \nAttention (Ql\nI,Kl\nT,Vl\nT\n) and Attention (Ql\nT,Kl\nI,Vl\nI\n) dig out the intermodal \nconnections between the image and text. Next, both intra- and inter-\nmodal connections were encoded into latent representations 𝔛𝔛l\nI and \n𝔛𝔛l\nT. We set λ to 1.0 as it gave rise to the best performance in our prelimi-\nnary experiments. Attention (Q, K, V) included two matrix multiplica-\ntions (mat. mul.) and one scaled softmax operation:\nAttention(Q,K,V) = softmax(QK⊤\n√dk\nV),\nwhere ⊤ stands for the matrix transpose operator, dk is a scaling \nhyper-parameter, which was set to 64. Next, we introduced residual \nlearning45 and forwarded the resulting 𝔛𝔛l\nI,𝔛𝔛l\nT to the following normaliza-\ntion layer and MLP:\nXl+1\nI = MLP(Norm(𝔛𝔛l\nI))++Xl\nI,\nXl+1\nT = MLP(Norm(𝔛𝔛l\nT))++Xl\nT,\nwhere Xl+1\nI  and Xl+1\nT  were passed to the next bidirectional multimodal \nattention block as the input, resulting in Xl+2\nI  and Xl+2\nT . Then, we com-\nbined tokens in Xl+2\nI  and Xl+2\nT  to produce a bag of unified tokens, which \nwere passed to the subsequent self-attention blocks24. We also allocated \nmultiple heads 24 in both bidirectional multimodal attention and \nself-attention blocks, where the number of heads was set to 12. This \nmultihead mechanism allowed the model to perform attention opera-\ntions in multiple representation subspaces simultaneously and aggre-\ngate the results afterwards.\nLastly, we applied average pooling to the unified tokens gener-\nated from the last self-attention block to obtain a holistic multimodal \nrepresentation for medical diagnosis. This representation was passed \nto a two-layer MLP to produce final prediction logits. During the \ntraining stage, we calculated the binary cross-entropy loss between \nthese logits and their corresponding pulmonary disease annota -\ntions (the first task) or real adverse clinical outcomes (the second \ntask). A loss function value was computed for every patient case. \nSpecifically, in the first task, each patient case contained one radio -\ngraph and related textual clinical information. In the second task, \neach patient case involved multiple CT slices, and these CT slices \nshared the same textual clinical information. We forwarded each CT \nslice and its accompanying textual clinical information to MDT to \nobtain one holistic representation. Since we had multiple CT slices, \nwe obtained a number of holistic representations (equal to the num -\nber of CT slices) for the same patient. Then, we performed average \npooling over these holistic representations to compute an averaged \nrepresentation, which was finally passed to a two-layer MLP and the \nbinary cross-entropy loss.\nImplementation details\nFor the pulmonary disease identification task, we first resized each \nradiograph to 256 × 256 pixels during the training stage, then cropped \na random portion of each image, where the area ratio between the \ncropped patch and the original radiograph was randomly deter -\nmined to be between 0.09 and 1.0. The cropped patch was resized to \n224 × 224, after which a random horizontal flip was applied to increase \nthe diversity of training data. In the validation and testing stages, each \nradiograph was first resized to 256 × 256 pixels, and then a square \npatch at the image centre was cropped. The size of the square crop \nwas 224 × 224. The processed radiographs were finally passed to the \nimage-only model, non-unified-chest, Perceiver and IRENE as input \nimages. In the task of adverse clinical outcome prediction for patients \nwith COVID-19, the input images were CT scans. We first used the lesion \ndetection and segmentation methodologies proposed in ref. 46. This \nis a deep learning algorithm based on a multiview feature pyramid \nconvolutional neural network47,48, which performs lesion detection, \nsegmentation and localization. This neural network was trained and \nvalidated on 14,435 participants with chest CT images and definite \npathogen diagnosis. On a per-patient basis, the algorithm showed \nsuperior sensitivity of 1.00 (95% CI: 0.95, 1.00) and an F1-score of 0.97 \nin detecting lesions from CT images of patients with COVID-19 pneu-\nmonia. Adverse clinical outcomes of COVID-19 were presumed to be \nclosely related to the characteristics of pneumonia lesion areas. For \neach patient’s case, we cropped a 3D CT subvolume by computing the \nminimum 3D bounding box enclosing all pneumonia lesions. Next, we \nresized all 3D subvolumes from different patients to a uniform size, \nwhich was 224 × 224 × 64. Lastly, we sampled 16 evenly spaced slices \nfrom every 3D subvolume along its third dimension.\nBefore we performed the formal training procedure, we \npre-trained our MDT on MIMIC-CXR42, as what was done for the baseline \nmodels. Similar to Perceiver, during pre-training, we used zero padding \nfor non-existent textual clinical information in every multimodal input. \nIn the formal training stage, we used AdamW49 as the default optimizer \nas we found empirically that it gave better performance on baseline \nmodels and IRENE. The initial learning rate was set to 3 × 10−5 and the \nweight decay was 1 × 10 −2. We trained each model for 30 epochs and \ndecreased the initial learning rate by a factor of 10 at the 20th epoch. \nThe batch size was set to 256 in the training stage of both tasks. It is \nworth noting that in the task of adverse clinical outcome prediction \nof COVID-19, we first extracted holistic feature representations from \n16 CT slices (cropped and sampled from the same CT volume). Next, \nwe applied average pooling to these 16 holistic features to obtain an \naveraged representation, which represented all pneumonia lesion \nareas in the entire CT volume. The binary cross-entropy loss was then \ncomputed on top of this averaged representation. During the train -\ning stage, we evaluated model performance on the validation set and \ncalculated the validation loss after each epoch. The model checkpoint \nthat produced the lowest validation loss was saved and then tested on \nthe testing set. We employed learnable positional embeddings in all \nViT models. IRENE was implemented using PyT orch50 and the training \nstage was accelerated using NVIDIA Apex with the mixed-precision \nstrategy51. In practice, we can finish the training stage of either task \nwithin 1 d using four NVIDIA GPUs.\nWe adopted the standard attention analysis strategy for vision \ntransformers. For each layer in the transformer, we averaged the atten-\ntion weights across multiple heads (as we used multihead self-attention \nin IRENE) to obtain an attention matrix. T o account for residual connec-\ntions, we added an identity matrix to each attention matrix and nor -\nmalized the resulting weight matrices. Next, we recursively multiplied \nthe weight matrices from different layers of the transformer. Finally, \nwe obtained an attention map that included the similarity between \nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755 754\nArticle https://doi.org/10.1038/s41551-023-01045-x\nevery input token and the CLS token. Since the CLS token was used \nfor diagnostic predictions, these similarities indicated the relevance \nbetween the input tokens and prediction results, which could then \nbe used for visualization. For cross-attention results, we performed \nvisualization with Grad-CAM52.\nNon-parametric bootstrap sampling was used to calculate 95% \nconfidence intervals. Specifically, we repeatedly drew 1,000 bootstrap \nsamples from the unseen test set. Each bootstrap sample was obtained \nthrough random sampling with replacement, and its size was the same \nas the size of the test set. We then computed AUROC (the first task) or \nAUPRC (the second task) on each bootstrap sample, after which we had \n1,000 AUROC or AUPRC values. Finally, we sorted these performance \nresults and report the values at 2.5 and 97.5 percentiles, respectively.\nT o demonstrate the statistical significance of our experimental \nresults, we first repeated the experiments for IRENE and the best per-\nforming baseline (that is, Perceiver) five times with different random \nseeds. Then, we used independent two-sample t -test (two-sided) to \ncompare the mean performance of IRENE and the best baseline results, \nand calculate P values.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nRestrictions apply to the availability of the developmental and valida-\ntion datasets, which were used with permission of the participants \nfor the current study. De-identified data may be available for research \npurposes from the corresponding authors on reasonable request.\nCode availability\nThe custom code is available at https://github.com/RL4M/IRENE.\nReferences\n1. He, J. et al. The practical implementation of artificial intelligence \ntechnologies in medicine. Nat. Med. 25, 30–36 (2019).\n2. Liang, H. et al. Evaluation and accurate diagnoses of pediatric \ndiseases using artificial intelligence. Nat. Med. 25, 433–438 \n(2019).\n3. Boehm, K. M., Khosravi, P., Vanguri, R., Gao, J. & Shah, S. P. \nHarnessing multimodal data integration to advance precision \noncology. Nat. Rev. Cancer 22, 114–126 (2022).\n4. Li, J., Shao, J., Wang, C. & Li, W. The epidemiology and  \ntherapeutic options for the COVID-19. Precis. Clin. Med. 3, 71–84 \n(2020).\n5. Comfere, N. I. et al. Provider-to-provider communication in \ndermatology and implications of missing clinical information in \nskin biopsy requisition forms: a systematic review. Int. J. Dermatol. \n53, 549–557 (2014).\n6. Shao, J. et al. Radiogenomic system for non-invasive identification \nof multiple actionable mutations and PD-L1 expression in \nnon-small cell lung cancer based on CT images. Cancers 14, 4823 \n(2022).\n7. Huang, S. C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, \nM. P. Fusion of medical imaging and electronic health records \nusing deep learning: a systematic review and implementation \nguidelines. npj Digit. Med. 3, 136 (2020).\n8. Wang, C. et al. Non-invasive measurement using deep learning \nalgorithm based on multi-source features fusion to predict PD-L1 \nexpression and survival in NSCLC. Front. Immunol. 13, 828560 \n(2022).\n9. Zhang, K. et al. Clinically applicable AI system for accurate \ndiagnosis, quantitative measurements, and prognosis of \nCOVID-19 pneumonia using computed tomography. Cell 181, \n1423–1433.e11 (2020).\n10. Kermany, D. S. et al. Identifying medical diagnoses and treatable \ndiseases by image-based deep learning. Cell 172, 1122–1131.e29 \n(2018).\n11. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and \nmedicine. Nat. Med. 28, 31–38 (2022).\n12. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, \n436–444 (2015).\n13. Schmidhuber, J. Deep learning in neural networks: an overview. \nNeural Netw. 61, 85–117 (2015).\n14. Wang, G. et al. A deep-learning pipeline for the diagnosis and \ndiscrimination of viral, non-viral and COVID-19 pneumonia from \nchest X-ray images. Nat. Biomed. Eng. 5, 509–521 (2021).\n15. Zhou, H. Y. et al. Generalized radiograph representation learning \nvia cross-supervision between images and free-text radiology \nreports. Nat. Mach. Intell. 4, 32–40 (2022).\n16. Tang, Y. X. et al. Automated abnormality classification of chest \nradiographs using deep convolutional neural networks. npj Digit. \nMed. 3, 70 (2020).\n17. Wang, C. et al. Development and validation of an \nabnormality-derived deep-learning diagnostic system for major \nrespiratory diseases. npj Digit. Med. 5, 124 (2022).\n18. Rajpurkar, P. et al. ChexNet: radiologist-level pneumonia \ndetection on chest x-rays with deep learning. Preprint at  \nhttps://arxiv.org/abs/1711.05225v3 (2017).\n19. Mei, X. et al. Artificial intelligence-enabled rapid diagnosis of \npatients with COVID-19. Nat. Med. 26, 1224–1228 (2020).\n20. Yala, A., Lehman, C., Schuster, T., Portnoi, T. & Barzilay, R. A deep \nlearning mammography-based model for improved breast cancer \nrisk prediction. Radiology 292, 60–66 (2019).\n21. Zhang, K. et al. Deep-learning models for the detection and \nincidence prediction of chronic kidney disease and type 2 \ndiabetes from retinal fundus images. Nat. Biomed. Eng. 5, \n533–545 (2021).\n22. Xu, Q. et al. AI-based analysis of CT images for rapid triage of \nCOVID-19 patients. npj Digit. Med. 4, 75 (2021).\n23. Akselrod-Ballin, A. et al. Predicting breast cancer by applying \ndeep learning to linked health records and mammograms. \nRadiology 292, 331–342 (2019).\n24. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. \nSyst. 30, 5998–6008 (2017).\n25. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nPreprint at https://arxiv.org/abs/1810.04805v2 (2018).\n26. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers \nfor image recognition at scale. Preprint at https://arxiv.org/\nabs/2010.11929v2 (2020).\n27. LeCun, Y. et al. Handwritten digit recognition with a \nback-propagation network. Adv. Neural Inf. Process. Syst. 2, \n396–404 (1989).\n28. Mikolov, T., Chen, K., Corrado, G. & Dean, J. Efficient estimation of \nword representations in vector space. Preprint at https://arxiv.org/\nabs/1301.3781v3 (2013).\n29. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. \nDistributed representations of words and phrases and their \ncompositionality. Adv. Neural Inf. Process. Syst. 26, 3111–3119 \n(2013).\n30. Jaegle, A. et al. Perceiver: general perception with iterative \nattention. In Proc. 38th International Conference on Machine \nLearning (eds Meila, M. & Zhang, T.) 4651–4663 (PMLR, 2021).\n31. Li, J. et al. Align before fuse: vision and language representation \nlearning with momentum distillation. Adv. Neural Inf. Process. \nSyst. 34, 9694–9705 (2021).\n32. Su, W. et al. VL-bert: pre-training of generic visual-linguistic \nrepresentations. Preprint at https://arxiv.org/abs/1908.08530v4 \n(2020).\nNature Biomedical Engineering | Volume 7 | June 2023 | 743–755\n 755\nArticle https://doi.org/10.1038/s41551-023-01045-x\n33. Wang, J. et al. GIT: A generative image-to-text transformer for \nvision and language. Preprint at https://arxiv.org/abs/ \n2205.14100v5 (2022).\n34. Pauwels, R. A., Buist, A. S., Calverley, P. M., Jenkins, C. R. & Hurd, S. S.  \nGlobal strategy for the diagnosis, management, and prevention \nof chronic obstructive pulmonary disease. NHLBI/WHO Global \nInitiative for Chronic Obstructive Lung Disease (GOLD) Workshop \nsummary. Am. J. Respir. Crit. Care Med. 163, 1256–1276 (2001).\n35. Li, Y. et al. BEHRT: transformer for electronic health records.  \nSci. Rep. 10, 7155 (2020).\n36. Xia, K. & Wang, J. Recent advances of transformers in medical \nimage analysis: a comprehensive review. MedComm Futur. Med. \n2, e38 (2023).\n37. Wang, D., Feng, L., Ye, J., Zou, J. & Zheng, Y. Accelerating the \nintegration of ChatGPT and other large-scale AI models into \nbiomedical research and healthcare. MedComm-Future Med. 2, \ne43 (2023).\n38. Moor, M. et al. Foundation models for generalist medical artificial \nintelligence. Nature 616, 259–265 (2023).\n39. Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer normalization. Preprint at \nhttps://arxiv.org/abs/1607.06450v1 (2016).\n40. Hendrycks, D. & Gimpel, K. Gaussian error linear units (GELUs). \nPreprint at https://arxiv.org/abs/1606.08415 (2016).\n41. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & \nSalakhutdinov, R. Dropout: a simple way to prevent neural networks \nfrom overfitting. J. Mach. Learn. Res. 15, 1929–1958 (2014).\n42. Johnson, A. E. et al. MIMIC-CXR, a de-identified publicly available \ndatabase of chest radiographs with free-text reports. Sci. Data 6, \n317 (2019).\n43. Jaegle, A. et al. Perceiver IO: a general architecture for structured \ninputs & outputs. Preprint at https://arxiv.org/abs/2107.14795v1 \n(2021).\n44. Deng, J. et al. ImageNet: a large-scale hierarchical image \ndatabase. In IEEE Conference on Computer Vision and Pattern \nRecognition 248–255 (IEEE, 2009).\n45. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \nimage recognition. In IEEE Conference on Computer Vision and \nPattern Recognition 770–778 (IEEE, 2016).\n46. Ni, Q. et al. A deep learning approach to characterize 2019 \ncoronavirus disease (COVID-19) pneumonia in chest CT images. \nEur. Radiol. 30, 6517–6527 (2020).\n47. Li, Z. et al. in Medical Image Computing and Computer Assisted \nIntervention—MICCAI 2019 (eds Shen, D. et al.) 13–21  \n(Springer, 2019).\n48. Zhao, G. et al. Diagnose like a radiologist: hybrid neuro-probabilistic \nreasoning for attribute-based medical image diagnosis. IEEE Trans. \nPattern Anal. Mach. Intell. 44, 7400–7416 (2022).\n49. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. \nPreprint at https://arxiv.org/abs/1711.05101 (2017).\n50. Paszke, A. et al. Pytorch: an imperative style, high-performance \ndeep learning library. Adv. Neural. Inf. Process. Syst. 32,  \n8026–8037 (2019).\n51. Micikevicius, P. et al. Mixed precision training. Preprint at  \nhttps://arxiv.org/abs/1710.03740 (2017).\n52. Selvaraju, R. R. et al. Grad-cam: visual explanations from deep \nnetworks via gradient-based localization. In IEEE International \nConference on Computer Vision 618–626 (IEEE, 2017).\nAcknowledgements\nThis research was supported by the National Natural Science \nFoundation of China (82100119, 92159302, 91859203), Hong Kong \nResearch Grants Council through General Research Fund (Grant \n17207722), Macau Science and Technology Development Fund, Macau \n(0007/2020/AFJ, 0070/2020/A2, 0003/2021/AKP).\nAuthor contributions\nH.-Y.Z., Y.Y., K.Z. and W.L. conceived the idea and designed the \nexperiments. H.-Y.Z., C.W. and S.Z. implemented and performed the \nexperiments. H.-Y.Z., Y.Y., C.W., S.Z., J.P., J.S., Y.G., G.L., K.Z. and W.L. \nanalysed the data and experimental results. H.-Y.Z., Y.Y., C.W., Y.G., K.Z. \nand W.L. wrote the paper. All authors commented on the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at https://doi.org/10.1038/\ns41551-023-01045-x.\nSupplementary information The online version contains \nsupplementary material available at https://doi.org/10.1038/s41551-\n023-01045-x.\nCorrespondence and requests for materials should be addressed to \nYizhou Yu, Chengdi Wang, Kang Zhang or Weimin Li.\nPeer review information Nature Biomedical Engineering thanks Jong \nChul Ye, Pranav Rajpurkar and Dawei Yang for their contribution to the \npeer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds \nexclusive rights to this article under a publishing agreement with \nthe author(s) or other rightsholder(s); author self-archiving of the \naccepted manuscript version of this article is solely governed by the \nterms of such publishing agreement and applicable law.\n© The Author(s), under exclusive licence to Springer Nature Limited \n2023\nNature Biomedical Engineering\nArticle https://doi.org/10.1038/s41551-023-01045-x\nExtended Data Fig. 1 | Impact of the chief complaint (a) or laboratory test results (b) on each respiratory disease. Specifically, we remove either the chief \ncomplaint or the laboratory test results from the input and report the performance drop on each disease. The evaluation metric is AUROC.\n\n\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5598172545433044
    },
    {
      "name": "Transformer",
      "score": 0.5420281887054443
    },
    {
      "name": "Representation (politics)",
      "score": 0.42432767152786255
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36581194400787354
    },
    {
      "name": "Engineering",
      "score": 0.2386702597141266
    },
    {
      "name": "Voltage",
      "score": 0.11944049596786499
    },
    {
      "name": "Electrical engineering",
      "score": 0.10404819250106812
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}