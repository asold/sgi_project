{
  "title": "Invalid SMILES are beneficial rather than detrimental to chemical language models",
  "url": "https://openalex.org/W4393317648",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2664148688",
      "name": "Michael A. Skinnider",
      "affiliations": [
        "Ludwig Cancer Research",
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1992156271",
    "https://openalex.org/W2017254234",
    "https://openalex.org/W2061843861",
    "https://openalex.org/W2035753075",
    "https://openalex.org/W1998693213",
    "https://openalex.org/W1980581986",
    "https://openalex.org/W2027478081",
    "https://openalex.org/W2110791536",
    "https://openalex.org/W2077488126",
    "https://openalex.org/W2907470641",
    "https://openalex.org/W1934481644",
    "https://openalex.org/W2914542247",
    "https://openalex.org/W4365442601",
    "https://openalex.org/W4220798957",
    "https://openalex.org/W2953641781",
    "https://openalex.org/W3043461363",
    "https://openalex.org/W2784270883",
    "https://openalex.org/W2971690404",
    "https://openalex.org/W2897337442",
    "https://openalex.org/W3174777205",
    "https://openalex.org/W3127493072",
    "https://openalex.org/W4309047034",
    "https://openalex.org/W4378802816",
    "https://openalex.org/W4313703391",
    "https://openalex.org/W2963028280",
    "https://openalex.org/W3111676828",
    "https://openalex.org/W3200806939",
    "https://openalex.org/W4280562526",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W4229590462",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W4361989906",
    "https://openalex.org/W2786565076",
    "https://openalex.org/W4294285719",
    "https://openalex.org/W4320728073",
    "https://openalex.org/W2994678679",
    "https://openalex.org/W3164193774",
    "https://openalex.org/W3113447514",
    "https://openalex.org/W4221145170",
    "https://openalex.org/W4297951436",
    "https://openalex.org/W2786103815",
    "https://openalex.org/W4289549256",
    "https://openalex.org/W2803526748",
    "https://openalex.org/W3036527662",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W3185391990",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W3147983081",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2153693853",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W4362503491",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4382808681",
    "https://openalex.org/W3181468767",
    "https://openalex.org/W2925830236",
    "https://openalex.org/W3164743754",
    "https://openalex.org/W2945551948",
    "https://openalex.org/W4387169268",
    "https://openalex.org/W4283270512",
    "https://openalex.org/W4362589790",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W4225876201",
    "https://openalex.org/W4300961340",
    "https://openalex.org/W3158331815",
    "https://openalex.org/W3167734706",
    "https://openalex.org/W4385406507",
    "https://openalex.org/W2777416523",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2989615256",
    "https://openalex.org/W3011286504",
    "https://openalex.org/W3094686696",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W2064963922",
    "https://openalex.org/W2066273100",
    "https://openalex.org/W2070072705",
    "https://openalex.org/W2033495141",
    "https://openalex.org/W3194841450",
    "https://openalex.org/W4281658583",
    "https://openalex.org/W3118695441",
    "https://openalex.org/W4306986298",
    "https://openalex.org/W4393592097",
    "https://openalex.org/W6911306409",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W3099414221",
    "https://openalex.org/W3199356076"
  ],
  "abstract": "Abstract Generative machine learning models have attracted intense interest for their ability to sample novel molecules with desired chemical or biological properties. Among these, language models trained on SMILES (Simplified Molecular-Input Line-Entry System) representations have been subject to the most extensive experimental validation and have been widely adopted. However, these models have what is perceived to be a major limitation: some fraction of the SMILES strings that they generate are invalid, meaning that they cannot be decoded to a chemical structure. This perceived shortcoming has motivated a remarkably broad spectrum of work designed to mitigate the generation of invalid SMILES or correct them post hoc. Here I provide causal evidence that the ability to produce invalid outputs is not harmful but is instead beneficial to chemical language models. I show that the generation of invalid outputs provides a self-corrective mechanism that filters low-likelihood samples from the language model output. Conversely, enforcing valid outputs produces structural biases in the generated molecules, impairing distribution learning and limiting generalization to unseen chemical space. Together, these results refute the prevailing assumption that invalid SMILES are a shortcoming of chemical language models and reframe them as a feature, not a bug.",
  "full_text": "Nature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 437\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-024-00821-x\nArticle\nInvalid SMILES are beneficial rather than \ndetrimental to chemical language models\nMichael A. Skinnider    1,2 \nGenerative machine learning models have attracted intense interest for \ntheir ability to sample novel molecules with desired chemical or biological \nproperties. Among these, language models trained on SMILES (Simplified \nMolecular-Input Line-Entry System) representations have been subject to \nthe most extensive experimental validation and have been widely adopted. \nHowever, these models have what is perceived to be a major limitation: \nsome fraction of the SMILES strings that they generate are invalid, meaning \nthat they cannot be decoded to a chemical structure. This perceived \nshortcoming has motivated a remarkably broad spectrum of work designed \nto mitigate the generation of invalid SMILES or correct them post hoc. Here \nI provide causal evidence that the ability to produce invalid outputs is not \nharmful but is instead beneficial to chemical language models. I show that \nthe generation of invalid outputs provides a self-corrective mechanism that \nfilters low-likelihood samples from the language model output. Conversely, \nenforcing valid outputs produces structural biases in the generated \nmolecules, impairing distribution learning and limiting generalization \nto unseen chemical space. T ogether, these results refute the prevailing \nassumption that invalid SMILES are a shortcoming of chemical language \nmodels and reframe them as a feature, not a bug.\nOver the past century, more than 100 million small molecules have \nbeen synthesized in the search for new drugs and materials 1. These \nefforts have explored only an infinitesimal subset of chemical space, \nthe size of which is estimated at over 1060 molecules2. Yet, remarkably \nand often serendipitously, this limited exploration of chemical space \nhas led to the discovery of numerous molecules that can modulate bio-\nlogical processes. That our extremely limited exploration of chemical \nspace has already yielded so many medically or industrially valuable \ncompounds suggests that more efficient approaches to chemical space \nexploration could help address many of the most pressing challenges \nfacing humanity.\nChemical space is so large that its exhaustive enumeration is essen-\ntially impossible. Instead, searches for bioactive molecules generally \nfocus on particular subsets of chemical space 3,4. Historically, these \nsubsets were defined primarily by rule-based approaches, in which \nnew molecules were generated by iterative application of predefined \nchemical transformations to a ‘starter’ population5–12. More recently, \ngenerative models based on deep neural networks have emerged as a \npowerful framework for chemical space exploration 13–16. Given a set \nof molecules as input, these models are able to learn the chemistries \nimplicitly embedded within this training set, and then leverage this \nunderstanding to sample unseen molecules from the same areas of \nchemical space.\nInitial demonstrations that deep generative models could \ndesign novel molecules with desired physicochemical or bio -\nlogical properties 17–24 have triggered the development of myriad \napproaches to molecule generation. These methods differ not only \nin the architectures of the underlying neural networks, but also \nin the conceptual frameworks they use to represent molecules: \nfor instance, as chemical graphs 25,26, as combinations of substruc -\ntures27 or as three-dimensional objects 28,29. Thus far, however, these \napproaches have not consistently surpassed the empirical state of \nReceived: 11 September 2023\nAccepted: 5 March 2024\nPublished online: 29 March 2024\n Check for updates\n1Lewis-Sigler Institute for Integrative Genomics, Princeton University, Princeton, NJ, USA. 2Ludwig Institute for Cancer Research, Princeton University, \nPrinceton, NJ, USA.  e-mail: skinnider@princeton.edu\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 438\nArticle https://doi.org/10.1038/s42256-024-00821-x\nnatural language processing to learn the statistical properties of \nthese strings and generate new ones.\nLike a human language, the SMILES syntax imposes strict rules \non which strings are syntactically valid. This means that chemical \nthe art established by the earliest deep neural network approaches \nbased on chemical language models 30,31. These models represent \nmolecules as strings of text (commonly using the SMILES format 32; \nFig. 1a), and adapt neural network architectures from the field of \nLanguage modelTraining molecules\nNovel molecules from \nthe same chemical space\nMolecular string\nrepresentations\nSMILES\nSELFIES\na\nMolecules sampled\nfrom ChEMBL\nTrain\nCLMs\nString \nrepresentations\nSMILES\nGenerated\nGenerated\nSELFIES\nSample \nmolecules\nCompare properties\nto training set\nSELFIES\nTraining\nset\nSMILES\nTraining \nset\nc\nr = 0.77\nP = 1.3 × 10 –18\n–6\n–4\n–2\n0\n∆ FCD SMILES – SELFIES\n70 80 90 100\nValid (%)\n90\n92\n94\n96\n98\n100\nValid (%)\nSMILES SELFIES\n1\n2\n3\nFréchet ChemNet distance\nSMILES SELFIES\nd e f\nN\nN\nO\nN\nO N\nValid\nInvalid\nb\nO\nO\nOHO\nFig. 1 | Language models that can generate invalid outputs outperform \nmodels that cannot. a, Schematic overview of chemical space exploration with \nchemical language models. Language models are trained on a set of chemical \nstructures represented as strings (for example, in SMILES or SELFIES formats). \nSampling new strings from the trained model enables generation of novel \nmolecules from the same chemical space as the training set. b, Illustration of \ninvalid SMILES. A single character substitution in the SMILES string for caffeine, \ntop, creates a syntactically invalid SMILES string that does not correspond to \nany chemical structure, bottom. c, Experimental framework to benchmark \nlanguage models trained on SMILES versus SELFIES. CLM, chemical language \nmodel. d, Proportion of valid molecules generated by language models trained \non SMILES versus SELFIES representations (n = 10 each; P = 2.0 × 10–10, paired \nt-test). e, Fréchet ChemNet distance between generated and training molecules \nfor language models trained on SMILES versus SELFIES representations (lower \nis better; n = 10 each; P = 1.1 × 10\n–9, paired t-test). f, Relationship between the \nproportion of valid SMILES generated by chemical language models, and the \ndifference in Fréchet ChemNet distance (FCD) between each model and an \nequivalent model trained on SELFIES representations of the same training set. \nInset text shows the Pearson correlation coefficient and P value. The line and \nshaded area show linear regression and 95% confidence interval, respectively.\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 439\nArticle https://doi.org/10.1038/s42256-024-00821-x\nlanguage models can generate SMILES that do not correspond to any \nvalid chemical structure (Fig. 1b). The generation of invalid SMILES is \nwidely perceived to be an important shortcoming of chemical language \nmodels. This perception has motivated an enormous amount of work to \naddress this shortcoming and encourage generation of valid molecules, \nwhether by developing alternative textual representations of mole -\ncules33–35, developing methods that generate valid SMILES by design36,37, \nor developing methods to correct invalid SMILES post hoc 38–40. The \ngeneration of invalid SMILES is also frequently cited as a motivation to \neschew the language modelling framework and develop models that \ngenerate chemical graphs directly25,27,41–47 and used in benchmark suites \nto quantify the performance of generative models48,49.\nThat the generation of invalid SMILES is so widely perceived to be \na limitation of chemical language models might be seen as surprising. \nRemoving invalid SMILES from the output of a chemical language \nmodel is a simple post hoc processing step that does not carry sub -\nstantial computational cost. Moreover, despite the assumption that \ngenerating invalid SMILES is a shortcoming, several benchmarks have \nidentified that language models trained on SMILES outperform those \ntrained on SELFIES (SELF-referencIng Embedded Strings)34, a textual \nrepresentation that produces 100% valid output by design, as well as \nmodels that generate chemical graphs directly 50–52. These observa -\ntions raise the possibility that the ability to generate invalid SMILES is  \nactually a desirable property for a generative model: in other words, \nthat generating invalid SMILES is a feature, not a bug.\nIn this study, I set out to empirically test the possibility that inva-\nlid SMILES are beneficial, rather than harmful, to chemical language \nmodels. I show that invalid SMILES are sampled with significantly lower \nlikelihoods than valid SMILES, suggesting that filtering invalid SMILES \nprovides an intrinsic mechanism to identify and remove low-quality \nsamples from the model output. I then exploit the design of the  \nSELFIES language by removing the valency constraints that ensure \nvalid molecule generation and obtain causal evidence that generat -\ning invalid outputs improves the performance of chemical language \nmodels. I elucidate the mechanism by which imposing valency con -\nstraints impairs distribution learning, and show that these constraints \nbias chemical space exploration towards molecules with specific \nstructural properties and impair generalization to unseen chemical \nspace. Finally, I show that language models can correctly elucidate \ncomplex chemical structures from minimal analytical data, and that \nmodels capable of generating invalid outputs outperform models \nthat cannot on this task.\nResults\nModels that generate invalid outputs outperform models  \nthat do not\nPrevious benchmarks suggested that chemical language models \ntrained on SMILES could outperform those trained on SELFIES, a \n \nformat in which every string corresponds to a valid molecule by design.  \nSpecifically, these benchmarks showed that language models trained \non SMILES strings generated unseen molecules whose physicochemical \nproperties better matched those of the molecules in the training set50,51. \nI initially set out to reproduce this observation. I trained chemical lan-\nguage models on random samples of molecules from the ChEMBL data-\nbase53, providing either SMILES or SELFIES representations of the same \nmolecules as input. The trained models were then used to sample new \nmolecules from the same chemical space as the training set, and model \nperformance was evaluated by calculating metrics that captured the \nsimilarity between generated molecules and the training set (Fig. 1c).\nAs expected, models trained on SELFIES strings produced valid \nmolecules at a rate of 100%, compared to an average of 90.2% for mod-\nels trained on SMILES (Fig. 1d). Nonetheless, models trained on SMILES \ngenerated novel molecules that matched the training set significantly \nbetter than models trained on SELFIES, as quantified by the Fréchet \nChemNet distance (Fig. 1e). This conclusion was unchanged when using \nother metrics to quantify performance, such as the Murcko scaffold \nsimilarity between the training and generated molecules 54, and was \nrecapitulated when integrating multiple metrics into a single measure \nof model performance using principal component analysis (PCA), as \npreviously described\n50 (Extended Data Fig. 1a–d).\nThe superior performance of models trained on SMILES was robust \nboth to the data used to train the chemical language models, and to the \narchitecture of the models themselves. I reproduced this result when \n(1) training models on smaller or larger samples of molecules from \nChEMBL; (2) training models on molecules from a different chemical \ndatabase, GDB-13 (ref. 55); (3) training models on more or less chemi-\ncally diverse training sets; (4) performing data augmentation by SMILES \nor SELFIES enumeration56,57; or (5) using a language model based on the \ntransformer architecture58 instead of one based on long short-term \nmemory (LSTM) networks (Extended Data Fig. 1e–s).\nLanguage models trained on SELFIES typically generated novel \nmolecules at a higher rate than models trained on SMILES, but models \ntrained on either representation were able to achieve a very high rate \nof novelty (>99%) except when deliberately constructing training sets \nwith a low degree of chemical diversity (Extended Data Fig. 2).\nT ogether, these results demonstrate that language models trained \non SMILES robustly outperformed those trained on SELFIES. Moreover, \nacross all models tested, I found that the magnitude of this differ -\nence in performance was strongly and negatively correlated with the \nproportion of valid SMILES (Fig. 1f ): in other words, models trained \non SMILES performed proportionately better when generating more \ninvalid outputs.\nInvalid SMILES are low-likelihood samples\nThese findings expose an apparent contradiction. The presence of inva-\nlid outputs is widely perceived to be a central shortcoming of generative \nmodels based on SMILES strings. However, models that can generate \ninvalid outputs robustly outperformed models that—by design—can \nonly generate valid outputs.\nI sought to identify the mechanisms underlying this contra -\ndiction. One potential explanation is that invalid SMILES represent \nlow-likelihood samples from the language model. Removing invalid \nSMILES would, therefore, function as a mechanism to filter low-quality \nsamples from the model output. Notably, this hypothesis is con -\nsistent with the observed anticorrelation between invalid SMILES  \ngeneration and model performance (Fig. 1f): filtering out a larger num-\nber of low-quality samples would expectantly result in proportionately  \nbetter performance.\nIf this hypothesis were correct, one would expect that invalid \nSMILES are sampled with larger losses than valid SMILES from the same \nmodel. This was, indeed, found to be the case (Fig. 2a,b ). Moreover, \nthis difference was not limited to a single subtype of invalid SMILES: \nall major categories of invalid SMILES 38 were sampled with higher \nlosses than their valid counterparts (Fig. 2c–e). These differences were  \nmediated, in part, by the increased lengths of invalid SMILES, but \npersisted when comparing the average losses with which individual \ntokens were sampled within valid versus invalid SMILES (Extended Data \nFig. 3a–d). Conversely, SMILES that were sampled with smaller losses \nwere more likely to be valid (Fig. 2f). These findings were robust to vary-\ning the size and composition of the training dataset or the architecture \nof the language model (Extended Data Fig. 3e–m).\nInvalid outputs improve performance\nThese results establish that invalid SMILES are enriched among \nlow-likelihood samples from chemical language models. This find -\ning suggests that removing invalid SMILES has the effect of filtering \nlow-quality samples from the model output, which in turn would be \nexpected to improve performance on distribution-learning metrics \nsuch as the Fréchet ChemNet distance. However, these data pro -\nvide correlative rather than causal evidence for the notion that the \nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 440\nArticle https://doi.org/10.1038/s42256-024-00821-x\nability to generate (and then discard) invalid outputs improves model  \nperformance.\nT o obtain such evidence, I took advantage of the design of SELFIES \nthemselves. Within the SELFIES library, the generation of chemically \nvalid graphs is enforced by a set of constraints on the valence of each \natom: for example, the specification that a carbon atom cannot partici-\npate in more than four covalent bonds34,59. These valency constraints \nprovide a natural mechanism to test the relationship between output \nvalidity and model performance. I modified the default valency con-\nstraints within the SELFIES library to allow pentavalent carbons, a modi-\nfication I refer to as ‘T exas SELFIES’60. Under these modified constraints, \nlanguage models trained on SELFIES can generate chemically invalid \noutputs. Remarkably, however, these chemically invalid constraints \nsignificantly improved performance: decoding T exas SELFIES yielded \nsamples of novel molecules that were more similar to the training set \nthan those decoded with the default and chemically valid constraints \n(Fig. 3a and Extended Data Fig. 4a–d).\nNext, I tested the effect of removing valency constraints entirely \n(‘unconstrained SELFIES’), and found that this further improved per-\nformance (Fig. 3a and Extended Data Fig. 4e–h). Invalid SELFIES were \nsampled with larger losses than their valid counterparts (Fig. 3b,c), \ncorroborating the trends observed for invalid SMILES, and supporting \nthe notion that removing valency constraints provided a mechanism \nto filter low-quality samples from the model output.\nThe superior performance of unconstrained SELFIES was robust to \nvariations in the training dataset or model architecture (Extended Data \nFig. 4j,l,m,p,r). Moreover, I identified a significant correlation between \nthe proportion of invalid SELFIES generated and the improvement in \nperformance after removing valency constraints (Fig. 3d): in other \nwords, models performed proportionately better when generating \nmore invalid SELFIES.\nWhereas removing valency constraints improved the perfor -\nmance of language models trained on SELFIES, these were generally \nstill outperformed by models trained on SMILES, pointing to residual \ndifferences in performance as a function of molecular representation.\nThese results provide causal evidence that allowing chemical lan-\nguage models to produce invalid outputs improves their performance.\nEnforcing valid outputs biases chemical space exploration\nI sought to clarify the mechanisms by which the ability to produce  \ninvalid outputs improved the performance of chemical language mod-\nels. I hypothesized that these differences in performance reflected \ndifferences in the chemical space explored by models trained on \nSMILES versus SELFIES. T o address this possibility, I computed a series \nof properties for each generated molecule, and compared the result-\ning property distributions to those of the training set. By far the larg-\nest difference between models trained on SMILES versus SELFIES in \nthis analysis involved their propensity to generate cyclic molecules.  \nMolecules generated as SELFIES were markedly depleted for aromatic \nrings (Fig. 4a,b) and enriched for aliphatic rings (Fig. 4c,d), relative both \nto the training set and to molecules generated as SMILES. Smaller but \nstatistically significant differences were observed for a range of other \nstructural properties, reflecting pervasive differences in the chemical \nspace explored by generative models trained on SMILES versus SELFIES \n(Fig. 4e and Extended Data Fig. 5a–t).\nT ogether, these experiments identified significant differences in \nthe chemical space explored by language models trained on SMILES \nversus SELFIES. T o establish whether a causal relationship existed,  \nI compared the SELFIES that could be successfully parsed without \nchemical valency constraints to those that required the imposition \nof these constraints in order to produce a valid chemical graph. This \ncomparison allowed me to directly assess how removing invalid SELFIES \ninfluenced the distributions of structural properties among the gener-\nated molecules. Remarkably, I observed that the most profound differ-\nences between valid and invalid SELFIES again involved their propensity \nto contain aromatic and aliphatic rings. Invalid SELFIES were signifi -\ncantly depleted for aromatic rings, and enriched for aliphatic rings, \nrelative to both the training set and to valid SELFIES (Fig. 4f,g). Con -\nversely, disabling the valency constraints, and allowing the model to \ngenerate invalid SELFIES, reversed the structural differences between \nmolecules generated as SMILES versus SELFIES.\nThis reversal led me to ask whether other structural differences \nbetween molecules generated as SMILES versus SELFIES were also \nreversed when disabling valency constraints. Indeed, I observed that \nthe differences in structural properties between SMILES and SELFIES \nwere strongly and significantly correlated to those between valid and \ninvalid SELFIES (Fig. 4h and Extended Data Fig. 6a–w). Thus, the struc-\ntural differences between molecules generated as SMILES versus SELF-\nIES can be attributed at least in part to the correction of invalid outputs.\nT ogether, these experiments expose the mechanism underly -\ning differences in performance between generative models trained \non SMILES versus SELFIES. The imposition of valency constraints in \nSELFIES prevents the generation of invalid outputs, but results in an \noverrepresentation of aliphatic rings and an underrepresentation of \n20\n40\n60\nLoss\nInvalid\nSMILES\nValid\nSMILES\n0\n0.5\n1.0\n1.5\nCohen's d\nValid vs invalid\nSMILES\nAromaticity error\nBond exists\nParentheses error\nSyntax error Unclosed ring Valence error\nValid\n25\n50\n75\n100\nLoss\n0\n1\n2\n3\nCohen's d\nAromaticity error\nBond exists\nParentheses error\nSyntax error Unclosed ring Valence error\n0\n2\n4\n6\n8\nSMILES (%)\nAromaticity \nerror\nBond exists\nParentheses \nerror\nSyntax error\nUnclosed ring\nValence error\n60\n70\n80\n90\n100\nValid SMILES (%)\n1 2 3 4 5 6 7 8 9 10\nLoss, decile\na b c d e f\nFig. 2 | Invalid SMILES are low-likelihood samples from chemical language \nmodels. a, Losses of valid versus invalid SMILES sampled from a representative \nchemical language model (n = 107 SMILES; P < 10–15, two-sided t-test). b, Effect \nsizes (Cohen’s d) comparing the losses of valid versus invalid SMILES sampled \nfrom n = 10 chemical language models, demonstrating consistent effects \n(P = 1.5 × 10–13, one-sample t-test). c, Losses of valid SMILES versus invalid SMILES \nsampled from a representative chemical language model, classified into six \ndifferent categories based on RDKit error messages\n38 (n = 107 SMILES; all P < 10–15, \ntwo-sided t-test). d, Effect sizes (Cohen’s d) comparing the losses of valid SMILES \nversus six different categories of invalid SMILES across n = 10 chemical language \nmodels, demonstrating consistent effects (all P ≤ 1.4 × 10–10, one-sample t-test).  \ne, Frequencies of each invalid SMILES error type, shown as the mean proportion  \nof all generated SMILES across ten chemical language models. f, Proportion of \nvalid SMILES within each decile of loss in samples of 500,000 strings from ten \nchemical models (P < 10–15, two-sided Jonckheere–T erpstra test).\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 441\nArticle https://doi.org/10.1038/s42256-024-00821-x\naromatic rings in the resulting molecules. These systematic biases in \nthe chemical composition of the generated molecules are reflected \nin poor performance on distribution-learning metrics, such as the \nFréchet ChemNet distance. Removing valency constraints, and allow-\ning the model to generate invalid outputs, corrects these biases and \nimproves performance.\nStructural biases limit generalization\nAn ideal generative model would sample evenly from the chemical \nspace surrounding the molecules in the training set. The observation of \nstructural biases in the outputs of language models trained on SELFIES \nis at odds with this goal. I therefore sought to test whether, in addi-\ntion to introducing biases in the chemical space explored by genera-\ntive models, the choice of representation would also constrain their  \ncapacity for generalization.\nT o test this hypothesis, I made use of an exhaustively explored \nchemical space: that of the GDB-13 database, which enumerates all  \n~975 million drug-like molecules containing up to 13 heavy atoms.  \nFollowing an experimental design proposed previously, I trained \nchemical language models on small samples from GDB-13, using either \nSMILES or SELFIES to represent these molecules 61. I then drew sam -\nples of 100 million strings from each language model, and calculated  \nthe total proportion of GDB-13 that was correctly reproduced within \nthe language model output.\nLanguage models trained on SELFIES generated significantly more \nvalid molecules than those trained on SMILES, as expected (Fig. 5a). \nHowever, a substantial fraction of the molecules generated as SELF -\nIES were outside the chemical space defined by the GDB-13 database \n(Fig. 5b). Consequently, despite generating fewer molecules overall, \nmodels trained on SMILES explored a significantly larger proportion of \nthe GDB-13 chemical space than models trained on SELFIES (Fig. 5c,d). \nThat models trained on SELFIES showed a greater propensity to explore \noutside the chemical space of GDB-13, but a lower coverage of GDB-13 \nitself, can be rationalized on the basis that models trained on SELFIES \nshow a diminished capacity to generalize from the chemical space of \nthe training set.\nInvalid outputs improve structure elucidation\nT o explore the implications of these findings further, I applied  \nchemical language models to a task in which efficient navigation of \nunknown chemical space is of central importance: namely, structure \nelucidation of complex natural products. Recent work has shown \nthat chemical language models can generate novel molecules that \nmatch experimentally measured properties. One particularly exciting  \nobservation is that language models can not only generate plausible \nchemical structures, but even prioritize the most likely ones on the \nbasis of as little experimental data as an accurate mass measurement62.  \nHowever, thus far this possibility has only been demonstrated for a \nsubset of drug-like molecules, and it remains unclear whether the same \napproach could be applied to structure elucidation of more complex  \nmolecules. In Supplementary Note 1 and Extended Data Figs. 7–9, I show \nthat language models can contribute to the structure elucidation of a range \nof complex small molecules including natural products, environmental  \npollutants, and food-derived compounds, and that the ability to  \ngenerate invalid outputs improves performance on these tasks.\nDiscarding invalid SMILES is fast and easy\nOne potential criticism of generating (and then discarding) invalid \noutputs is that the process of parsing every sample from the model \nto establish its validity necessarily requires additional computa -\ntional resources63. However, filtering invalid SMILES is a lightweight \npost-processing step that does not substantially increase the compu-\ntational requirements of a chemical language model. Parsing 1 million \nSMILES can be achieved with the RDKit in an average of 7.5 minutes on \na single CPU, and determining the validity of a SMILES string requires \njust a single line of code (Extended Data Fig. 10).\nDiscussion\nThat chemical language models trained on SMILES strings can pro -\nduce invalid outputs is widely (if not universally) perceived to be an \nimportant deficiency of these models. This perception has motivated \na remarkably broad spectrum of work in the field of chemical artifi -\ncial intelligence, including the development of alternative molecu-\nlar representations, mechanisms that encourage generation of valid  \noutputs, approaches to correct invalid outputs post hoc, and models \nthat generate chemical graphs directly. Here I provide direct and causal \nevidence that the ability to produce invalid outputs is not harmful \nbut is instead beneficial to chemical language models, and elucidate \nthe mechanisms underlying this effect. I show that language models \ntrained on SMILES, a representation that can lead to both syntactically \na b c d\n1\n2\n3\nFrechet ChemNet distance\nSMILES SELFIES\nTexas\nSELFIES\nUnconstrained\nSELFIES\n20\n40\n60\nLoss\nInvalid\nSELFIES\nValid\nSELFIES\n0\n0.2\n0.4\n0.6\n0.8\nCohen's d\nValid vs invalid\nSELFIES\nr = 0.72\nP = 2.0 × 10\n–15–4\n–3\n–2\n–1\n0\n∆ FCD\n30 40 50 60 70 80 90\nValid (%)\nFig. 3 | Invalid outputs improve the performance of chemical language \nmodels. a, Fréchet ChemNet distance between training and generated molecules \nfor language models trained on SMILES or SELFIES representations, with SELFIES \nvalency constraints modified to allow pentavalent carbons (‘T exas SELFIES’) \nor removed entirely (‘unconstrained SELFIES’; n = 10 each; both P ≤ 3.0 × 10\n–5 \ncompared with default valency constraints, paired t-test). b, Losses of valid \nversus invalid SELFIES sampled from a representative chemical language model \nwith valency constraints disabled when parsing generated SELFIES (n = 107 \nSELFIES; P < 10–15, t-test). c, Effect sizes (Cohen’s d) comparing the losses of \nvalid versus invalid SMILES sampled from n = 10 chemical language models, \ndemonstrating consistent effects (P = 5.6 × 10–14, two-sided one-sample t-test). \nd, Relationship between the proportion of valid SELFIES generated with valency \nconstraints disabled, and the difference in Fréchet ChemNet distance when \nparsing generated SELFIES with or without valency constraints. Inset text shows \nthe Pearson correlation coefficient and P value. The line and shaded area show \nlinear regression and 95% confidence interval, respectively.\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 442\nArticle https://doi.org/10.1038/s42256-024-00821-x\nand semantically invalid outputs, outperform models trained on  \nSELFIES, a representation that enforces the generation of valid outputs \nby design (Fig. 1). Invalid SMILES are sampled with significantly lower \nlikelihoods than valid SMILES, implying that filtering invalid SMILES \npreferentially removes low-quality samples from the model output \n(Fig. 2). I leverage the design of the SELFIES representation by removing \nthe valency constraints that enforce valid molecule generation, allow-\ning me to show causally that generating (and then removing) invalid \noutputs improves language model performance (Fig. 3). I further show \nthat the imposition of valency constraints results in biased explora-\ntion of chemical space, reflected in an overrepresentation of aliphatic \nrings and an underrepresentation of aromatic rings in the generated \nmolecules (Fig. 4), and that these biases in turn impair generalization \nto unseen chemical space (Fig. 5). Finally, I apply chemical language \nmodels to structure elucidation of natural products, and show that  \n(1) language models can develop remarkably accurate hypotheses \nabout unknown chemical structures from minimal analytical data, \nand (2) models capable of generating invalid outputs significantly \noutperform models that cannot on this task (Extended Data Fig. 7).\nCollectively, these results challenge the often-voiced assumption \nthat invalid SMILES are a problem that must be addressed by developing \nnew computational approaches. They suggest that further efforts to \nenforce the generation of valid molecules are unlikely to improve model \nperformance. Instead, these results advocate for a more widespread \nrecognition that removing invalid outputs is a simple and computation-\nally efficient post-processing step that does not necessarily reflect a \nfundamental flaw in the underlying model. More broadly, these results \nsupport a redirection of efforts towards improving the performance \nof generative models of molecules through directions other than  \nmaximizing output validity. Indeed, several recent studies have high-\nlighted opportunities to improve molecule generation despite the \ngeneration of invalid SMILES64–67.\nThat language models trained on SMILES outperformed those \ntrained on SELFIES on distribution-learning metrics does not imply \nthe latter should never be preferred. A number of recent works have \npresented computational approaches in which the robustness of the \nSELFIES representation has been a central consideration, including \napplications to model interpretability 68,69 and inverse design 70,71. In \nother words, while I demonstrate that the ability to generate invalid \noutputs is beneficial to chemical language models in general, there are \nspecific scenarios in which validity is a more important consideration.\nI found that removing the valency constraints that enforce valid \nmolecule generation in SELFIES greatly reduced the difference in per-\nformance between models trained on SMILES versus SELFIES, but did \nnot abolish it entirely (Fig. 3a). This observation suggests that there are \nresidual differences between the two representations that go beyond \nthe presence of invalid outputs and reflect deeper aspects of how  \nthey represent chemical structures. Elucidating the mechanisms under-\nlying these differences will be an important direction for future work.\nThe application of chemical language models to structure \nelucidation of natural products indicates that these models can \ndevelop remarkably accurate hypotheses about complex and unseen \nP = 0.0011\nr = 0.82\n−0.50\n−0.25\n0\n0.25\n0.50\n∆Valid − Invalid\n−0.25 0 0.25 0.50\n∆SMILES − SELFIES\n0\n25\n50\n75\n100\nMolecules (%)\nTraining set\nSMILES SELFIES\nAromatic rings\n5+\n4\n3\n2\n1\n0\n0\n25\n50\n75\n100\nMolecules (%)\nTraining set\nSMILES SELFIES\nAliphatic rings\n5+\n4\n3\n2\n1\n0\n−0.6\n−0.4\n−0.2\n0\nEﬀect size\nAromatic rings\nSMILES SELFIES\n−0.2\n−0.1\n0\n0.1\n0.2\nEﬀect size\nAliphatic rings\nSMILES SELFIES\n0\n0.1\n0.2\n0.3\n0.4\nEﬀect size\nAliphatic rings\nInvalid Valid\n−0.8\n−0.6\n−0.4\n−0.2\n0\nEﬀect size\nAromatic rings\nInvalid Valid\na b c d\nf g h\nAromatic rings\nAliphatic rings\n% stereocentres\nHydrogen donors\n% rotatable \nbonds\nMolecular weight\nHydrogen acceptors\nTopological \ncomplexityTPSA\n% sp3 carbons LogP\n% heteroatoms\n0\n3\n6\n9\n−log 10 (P)\n−0.50 −0.25 0 0.25 0.50\n∆SMILES − SELFIES\ne\nFig. 4 | Enforcing valid outputs biases chemical space exploration. a, Number \nof aromatic rings in generated molecules sampled from representative chemical \nlanguage models trained on the same molecules in SMILES versus SELFIES \nformat, and in the training set molecules themelves. b, Effect sizes (Cohen’s d) \ncomparing the number of aromatic rings in generated molecules from chemical \nlanguage models trained on SMILES versus SELFIES to the molecules in the \ntraining set (n = 10 each; P = 1.2 × 10\n–11, paired t-test). c, As in a, but showing \naliphatic rings. d, As in b, but showing aliphatic rings (P = 1.2 × 10–7, paired t-test). \ne, Volcano plot showing differences in structural properties between molecules \ngenerated as SMILES versus SELFIES (statistical significance versus mean \ndifference in effect size, paired t-test). Dotted line shows P = 0.05. f, Effect sizes \n(Cohen’s d) comparing the number of aromatic rings in generated molecules \nfrom chemical language models to the molecules in the training set, shown \nseparately for valid versus invalid SELFIES when parsing generated SELFIES \nwithout valency constraints (n = 10 each; P = 7.6 × 10\n–11, paired t-test). g, As in f, but \nshowing aliphatic rings (P = 2.6 × 10–12, paired t-test). h, Differences in structural \nproperties (mean effect sizes) are correlated between molecules generated as \nSMILES versus SELFIES (x-axis) and valid versus invalid SELFIES when parsing \ngenerated SELFIES without valency constraints (y-axis). Inset text shows the \nPearson correlation coefficient and P value. The line and shaded area show linear \nregression and 95% confidence interval, respectively.\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 443\nArticle https://doi.org/10.1038/s42256-024-00821-x\nchemical structures from as little data as an accurate mass measurement.  \nStructures proposed by the chemical language model were more simi-\nlar to the true molecule than those obtained by searching in PubChem, \nor even by searching in the natural product-like chemical space of the \ntraining set itself. This latter observation emphasizes the degree to \nwhich the model has learned to extrapolate beyond the training set and \ninto unseen chemical space. Of course, the evaluation scenario here is \nby design unrealistic, and I do not mean to suggest that it is possible \nto perform complete structure elucidation of complex natural prod-\nucts from an accurate mass alone (not least because it is theoretically \nimpossible to discriminate between different structures with the same \nmolecular formula using only MS1 information). Instead, my intention \nin this experiment is to highlight that given minimal analytical data, \nlanguage models can develop remarkably good hypotheses—some -\ntimes even better than those based on much richer analytical data. \nThis is exciting because, to my knowledge, these structural hypotheses \nrepresent a new source of information that is not currently used by any \nmethods for structure elucidation of unknown molecules72. Integrating \nthe novel chemical structures prioritized by chemical language models \nwith computational approaches that leverage MS/MS or retention \ntime information could provide a powerful mechanism to accelerate \nstructure elucidation of unknown molecules in biological systems.\nMethods\nDatasets\nMy experiments initially focused on training chemical language models \non random samples of molecules from the ChEMBL database\n53. ChEBML \n(version 28) was obtained from ftp.ebi.ac.uk/pub/databases/chembl/\nChEMBLdb/latest/chembl_28_chemreps.txt.gz. Duplicate SMILES and \nSMILES that could not be parsed by the RDKit were removed. Salts and \nsolvents were removed by splitting molecules into fragments and \nretaining only the heaviest fragment containing at least three heavy \natoms, using code adapted from the Mol2vec package 73. Charged \nmolecules were neutralized using code adapted from the RDKit Cook-\nbook. Molecules with atoms other than Br, C, Cl, F , H, I, N, O, P or S were \nremoved, and molecules were converted to their canonical SMILES \nrepresentations.\nRandom samples of between 30,000 and 300,000 molecules \nwere then drawn from the preprocessed SMILES. In most experiments, \nmolecules were sampled randomly to achieve uniform coverage of \nChEMBL chemical space. Separately, the effect of the chemical diversity \nof the training set on model performance was assessed by sampling \ntraining sets of molecules with decreasing chemical diversity 50. This \nwas achieved by selecting a ‘seed’ molecule at random from ChEMBL \nand then computing the Tanimoto coefficient (T c) between the seed \nmolecule and the remainder of the database. The database was then \nfiltered to retain only molecules with a T c greater than some target \nminimum value. A minimum T c of zero reflects random selection of \nmolecules across the entire database, whereas increasing the minimum \nT c selects molecules that are increasingly similar to the seed molecule \n(that is, decreasing diversity). The Tanimoto coefficient was calculated \non Morgan fingerprints74 with a radius of 3, folded to 1,024 bits.\nPast studies reported that data augmentation by non-canonical \nSMILES enumeration56 could significantly improve the performance  \nof chemical language models 62,75,76. I therefore also tested the \neffect of SMILES enumeration, which was performed using the  \nSmilesEnumerator class available from http://github.com/EBjerrum/ \nSMILES-enumeration , with augmentation factors of 10× or 30×.  \nSELFIES enumeration was performed by first enumerating non-  \ncanonical SMILES and then converting these to SELFIES, which I verified \nproduced multiple SELFIES representing the same molecules. Finally, \nconversion from SMILES to SELFIES was performed using the SELFIES \npackage (version 2.1.1).\nT o verify that the results were not specific to the chemical space \npopulated by molecules from ChEMBL, I also trained chemical language \nmodels on the GDB-13 database55. The GDB-13 database was obtained \nfrom Zenodo (https://doi.org/10.5281/zenodo.5172018) and underwent \npreprocessing identical to that described above for ChEMBL.\nFor each set of parameters tested (for example, training dataset \nsize, degree of data augmentation, or input database), ten independent \ntraining datasets were created with random seeds in order to evaluate \nvariability in model performance and enable statistical comparisons. In \ntotal, I trained 180 models (90 on SMILES and 90 on SELFIES) that differed \naccording to the random seed, the size of the training dataset (30,000, \n100,000 or 300,000 molecules), the chemical diversity (T c ≥ 0.0,  \n0.05, 0.10, or 0.15), the degree of SMILES/SELFIES augmentation \n(canonical, 10×, 30×) and the input database (ChEMBL versus GDB-13).  \nModels shown in the main text were trained on 100,000 molecules \nfrom ChEMBL, without data augmentation or chemical diversity filters.\nChemical language models\nFor most experiments, I trained chemical language models based on \nLSTMs, which have been widely adopted in the field and have been \nsubject to extensive experimental validation. LSTMs were implemented \nin PyT orch, adapting code from the REINVENT package77. Briefly, each \nSMILES was converted into a sequence of tokens by splitting the SMILES \nstring into its constituent characters, except for atomic symbols \ncomposed of two characters (Br, Cl) and environments within square \nSMILES\nSELFIES\n0\n1\n2\n3\n4\n5\nProportion of GDB (%)\n0 25 50 75\nSampled molecules (10 6 )\n70\n72\n74\n76\n78\n80\nMolecules (10 6 )\nMolecules (10 6 )\nMolecules (10 6 )\nMolecules\nin GDB\nSELFIES SMILES\n14\n16\n18\n20\n22\n24\n26\nMolecules\nnot in GDB\nSELFIES SMILES\n99\n99.2\n99.4\n99.6\n99.8\n100\nNovel,\nvalid molecules\nSELFIES SMILES\na b c d\nFig. 5 | Structural biases limit generalization to unseen chemical space. \n a, Numbers of valid, novel molecules within samples of 100 million strings  \nfrom chemical language models trained on a subset of 1 million molecules \nfrom the GDB-13 database, represented as SMILES versus SELFIES (n = 10 each; \nP = 1.9 × 10\n–10, paired t-test). b, As in a, but showing the number of sampled \nmolecules outside the chemical space of the GDB-13 database within samples \nof 100 million strings (n = 10 each; P = 8.5 × 10–7). c, As in a, but showing the \nnumber of molecules from the full GDB-13 database reproduced within samples \nof 100 million strings (n = 10 each; P = 1.1 × 10\n–7). d, Saturation curve showing \nthe proportion of the full GDB-13 database reproduced after sampling a given \nnumber of valid molecules from chemical language models trained on SMILES \nversus SELFIES.\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 444\nArticle https://doi.org/10.1038/s42256-024-00821-x\nbrackets, such as [nH]. SELFIES were tokenized using the split_selfies \nfunction from the selfies package. The vocabulary of the RNN then \nconsisted of all unique tokens detected in the training data, as well \nas start-of-string and end-of-string characters and a padding token. \nThe architecture of the language models consisted of a three-layer \nLSTM with a hidden layer of 1,024 dimensions, an embedding layer \nof 128 dimensions, and a linear decoder layer. Models were trained to \nminimize the cross-entropy loss of next-token prediction using the \nAdam optimizer with β1 = 0.9 and β2 = 0.999, with a batch size of 64 and \na learning rate of 0.001. T en percent of the molecules in the training set \nwere reserved as a validation set and used to perform early stopping \nwith a patience of 50,000 minibatches. A total of 500,000 SMILES \nstrings were sampled from each trained model after completion of  \nmodel training.\nT o confirm that these results were robust to the specific architecture \nof the chemical language models, I also trained a series of models based \non the generative pretrained transformer (GPT) architecture78. Models \nwere implemented in PyT orch, adapting code and hyperparameters \nfrom MolGPT79. The architecture consisted of an embedding layer of 256 \ndimensions, which was concatenated with a learned positional encoding \nand passed through eight transformer blocks. Each block comprised \neight masked self-attention heads and a feed-forward network with a hid-\nden layer of 1,024 dimensions using GELU activation, both preceded by \nlayer normalization. Finally, the outputs of the transformer blocks were \npassed through a single linear decoder layer with layer normalization. \nThe transformer models were trained as described above for language \nmodels based on LSTMs, except that the learning rate was set to 0.0005.\nEvaluation of model performance\nI evaluated the performance of chemical language models trained \non SMILES versus SELFIES by drawing samples of 500,000 SMILES \nor SELFIES strings from the trained models, and then quantifying \nthe similarity between the generated molecules and the molecules in \nthe training set. I used multiple complementary metrics to evaluate  \nsimilarity. As the primary measure of model performance, I measured \nthe chemical similarity between the generated molecules and the train-\ning set as quantified by the Fréchet ChemNet distance 80. This metric \nwas previously found to be among the most reliable for evaluating \nchemical language models 50 and is included in multiple benchmark \nsuites48,49. As secondary measures of model performance, I also calcu-\nlated a series of other metrics that were found to be similarly reliable, \nincluding the Jensen–Shannon distances between the Murcko scaffold \ncompositions 54, natural product-likeness scores 81, and fraction of \natoms in each molecule that are stereocenters in both the generated \nand training molecules. Molecules from the training set were filtered \nbefore calculating any of the above metrics to ensure that models were \nnot being rewarded for simply reproducing the training molecules.  \nI additionally integrated these metrics into a single measure of model \nperformance using PCA, as previously demonstrated in a study in which \nchemical language models were trained on datasets of between 1,000 \nand 500,000 molecules50. PCA was shown to recover a first principal \ncomponent that correlated strongly with the size of the training data-\nset, and thus also the quality of the learned model, while accounting \nfor the covariance between the underlying evaluation metrics. PCA was \nperformed using the reference dataset collected in the prior study using \nthe ‘CLMeval’ R package (https://github.com/skinnider/CLMeval),  \nand evaluation metrics for models analysed in the present study were \nprojected onto this PC space. Finally, I confirmed that models trained \non SELFIES generated 100% valid molecules whereas models trained \non SMILES did not. Valid molecules were defined as those that could be \nparsed by RDKit, using the Chem.MolFromSmiles function.\nAnalysis of invalid SMILES\nT o investigate the properties of invalid SMILES, I drew samples of  \n10 million SMILES strings from trained chemical language models, and \nidentified invalid SMILES as those that could not be parsed by RDKit.  \nI then compared the losses with which valid versus invalid SMILES were \nsampled from the chemical language model by calculating Cohen’s d, \nas implemented in the R package effsize. Separately, I divided SMILES \ninto ten bins according to their losses and calculated the proportion \nof valid SMILES in each bin, testing for the presence of a trend with the \nJonckheere–T erpstra test as implemented in the R package clinfun. \nLast, for each invalid SMILES, the parsing errors returned by RDKit \nwere classified into six different error types using code developed as \npart of the UnCorrupt SMILES approach 38 (available from GitHub at \nhttps://github.com/LindeSchoenmaker/SMILES-corrector), and the \nlosses for invalid SMILES from each error type were compared to those \nof valid SMILES with Cohen’s d.\nRemoval of SELFIES valency constraints\nT o causally test the relationship between the generation of invalid \noutput and model performance, I leveraged the design of the SELFIES \nlibrary by modifying the valency constraints that ensure semantically \ncorrect output. I initially modified the default constraints (encoded in \nthe _DEFAUL T_CONSTRAINTS dictionary) by allowing carbon atoms \nto participate in five bonds (‘T exas SELFIES’). I then parsed gener -\nated SELFIES under this modified set of constraints, removed outputs \nthat could not be parsed by the RDKit, and re-calculated the Fréchet \nChemNet distance and other distribution-learning metrics. I also tested \nthe effect of removing valency constraints entirely by setting all values \nin the _DEFAUL T_CONSTRAINTS dictionary to 999 (‘unconstrained \nSELFIES’), following a previous suggestion 59, after which I removed \ninvalid SELFIES and re-calculated the distribution-learning metrics.\nProperties of generated molecules\nT o understand why the generation of invalid outputs improved  \nperformance on distribution-learning metrics, I calculated a series \nof structural properties for molecules generated as SMILES versus  \nSELFIES using the RDKit. I calculated the same properties for molecules \ngenerated as SELFIES that could be parsed without valency constraints, \nversus molecules parsed under the default constraints. The list of \nproperties examined was as follows:\n 1. The molecular weight of each molecule.\n 2. The computed octanol–water partition coefficient82 of each \nmolecule.\n 3. The topological complexity83 of each molecule.\n 4. The topological polar surface area84 of each molecule.\n 5. The proportion of carbon atoms in each molecule that were sp3 \nhybridized.\n 6. The proportion of rotatable bonds in each molecule.\n 7. The proportion of atoms in each molecule that were \nstereocentres.\n 8. The fraction of heteroatoms in each molecule.\n 9. The number of aliphatic rings in each molecule.\n 10. The number of aromatic rings in each molecule.\n 11. The total number of rings in each molecule.\n 12. The number of hydrogen donors in each molecule.\n 13. The number of hydrogen acceptors in each molecule.\nFor each of these properties, I compared the distributions of  \nvalues for generated molecules versus the training set using Cohen’s \nd. I then tested for statistically significant differences using a paired \nt-test. Separately, I computed the Pearson correlation between the \nmean difference in effect sizes in comparisons of (1) molecules gener-\nated as SELFIES versus SMILES and (2) molecules generated as valid \nversus invalid SELFIES.\nGeneralization to unseen chemical space\nT o evaluate the ability of chemical language models to generalize \nto unseen chemical space surrounding the training set, I adapted an \nNature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 445\nArticle https://doi.org/10.1038/s42256-024-00821-x\nexperimental design proposed previously 61. I trained chemical lan -\nguage models on samples of 100,000 molecules from the GDB-13 \ndatabase, represent either as SMILES or SELFIES, and then drew samples \nof 100 million strings from each trained model. I then intersected these \nsamples with the remainder of the GDB-13 database by comparison of \ncanonical SMILES. For each language model, I computed (1) the number \nof novel and unique molecules; (2) the number of generated molecules \nin GDB-13 (excluding the training set); and (3) the number of generated \nmolecules not in either GDB-13 or the training set. The experiment was \nrepeated ten times with different training sets to gauge variability and \nenable statistical comparison.\nStructure elucidation with chemical language models\nPrevious work had reported that chemical language models could con-\ntribute to structure elucidation of unknown small molecules using mass \nspectrometry, given minimal analytical data as input62. Specifically, it \nwas found that when drawing a very large sample of molecules from a \ntrained language model, the frequency with which any given unique \nmolecule appeared in this output provided a model-intrinsic measure \nthat could be used to develop structural hypotheses from an accurate \nmass measurement. These hypotheses could then be further refined \nby integrating the sampling frequency with MS/MS data, using existing \ntools for MS/MS interpretation85. Here, I tested (1) whether this princi-\nple would apply to other classes of small molecules, including complex \nnatural products and (2) whether the choice of representation would \ninfluence the accuracy of structure elucidation using this approach.\nI evaluated the performance of chemical language models on this \ntask using four databases representing three different categories of \nsmall molecules. These included natural products from the LOTUS86 \nand COCONUT87 databases, food-derived compounds from the FooDB \ndatabase, and environmental compounds from the NORMAN suspect \nlist88. All four databases were preprocessed as described above for \nChEMBL, and then split at random (that is, without scaffold splitting) \ninto ten folds. For each test fold, a language model was trained on the \n90% of molecules comprising the training set, after which a total of 100 \nmillion strings were sampled from the trained model. The sampled mol-\necules were then parsed with the RDKit, invalid outputs were discarded, \nand the frequency with which each canonical SMILES appeared in the \nmodel output was tabulated. Then, for each molecule in the held-out \ntest set, the model output was searched with the exact mass of this \nquery molecule (plus or minus a 10 part per million window) and the \ngenerated molecules were sorted in descending order by their sampling \nfrequencies to provide a ranked list of structural hypotheses. A window \nof 10 ppm was used to allow for differences between the measured \n(experimental) and theoretical mass; the accuracy of the experimental \nmeasurements is usually expressed as a mass error in parts per million as\n(massmeasured −masstheoretical)/masstheoretical ×10\n6\nThe top-k  accuracy was then calculated as the proportion of \nheld-out molecules for which the correct chemical structure appeared \nwithin the k top-ranked outputs from the language model when ordered \nby sampling frequency (in the case of ties, molecules were ordered at \nrandom). A similar evaluation was carried out at the level of molecular \nformulas, whereby the top-k accuracy was calculated as the proportion \nof held-out molecules for which the correct formula appeared within \nthe k top-ranked model outputs. In addition, I calculated the Tanimoto \ncoefficient between each held-out molecule and the top-ranked chemi-\ncal structure proposed by the language model, using Morgan finger-\nprints with a radius of 3 as above. The entire process was repeated in \nten-fold cross-validation.\nAs a baseline, I compared this language model-directed approach \nto searching by exact mass in PubChem, mimicking one approach to \nassigning chemical structures or molecular formulas based on MS1 \nmeasurements. I also compared the language model approach to \nsearching by exact mass in the training set itself, recognizing that this \nwould by definition lead to a top-k accuracy of 0 for any value of k, but \nwith the goal of comparing the Tanimoto coefficients between the \ntrue molecule and structures prioritized by the language model versus  \nmolecules from the training set as a means to assess generalization \nbeyond the training set. In both baselines, the same 10 ppm error \nwindow was used, within which molecules were ordered at random.\nFinally, I repeated the procedures above with language models \ntrained on SELFIES representations of the same databases, using identi-\ncal folds. In addition, I parsed molecules generated as SELFIES without \nvalency constraints as described above, discarded invalid outputs, and \nthen re-calculated the sampling frequency of each generated molecule \nafter canonicalization.\nCASMI 2022\nT o further place the performance of chemical language models in \ncontext, I benchmarked the language model trained on the LOTUS \ndatabase against 19 submissions to the CASMI 2022 competition (two \nsubmissions, Nikolic_KUCA and Nikolic_POSO, were excluded because \nthese represented manual rather than computational approaches, per \nthe organizers of the competition). In this competition, 500 commer-\ncially available compounds were profiled by mass spectrometry; four \ncompounds (identifiers 81, 282, 432, 476) were subsequently excluded \nfrom the competition. Entrants were provided with the accurate m/z \nand retention times for each compound, and an accompanying mzML \nfile containing MS/MS data. I tested the performance of the chemical \nlanguage model given only the accurate m/z value as input. T o simu-\nlate de novo elucidation of unknown molecules, I averaged sampling \nfrequencies across all 10 cross-validation folds, removing training \nset molecules from the generative model output for each fold. This \nprocedure ensured that sampling frequencies could be generated for \nknown natural products without data leakage. For each accurate m/z \nvalue, I considered multiple potential adducts ([M + H]+, [M + NH4]+ and \n[M + Na]+ in the positive mode and [M – H]–, [M + Cl]– and [M + FA – H]– in \nthe negative mode) and retrieved sampled molecules with the cor -\nresponding exact masses, plus or minus a 10 ppm error window as \nabove. Generated molecules were then sorted in descending order \nby sampling frequency across all potential adduct types to produce a \nranked list of hypotheses for each target m/z value.\nVisualization\nThroughout the manuscript, the box plots show the median (horizontal \nline), interquartile range (hinges) and smallest and largest values no \nmore than 1.5 times the interquartile range (whiskers), and the error \nbars show the standard deviation.\nData availability\nDatasets used to train chemical language models, unprocessed samples \nof 10 million molecules from each model trained on ChEMBL or GDB-13, \nand samples of 100 million molecules from each cross-validation fold \nof LOTUS, COCONUT, FooDB and NORMAN, represented as canonical \nSMILES and sorted by sampling frequency, are available via Zenodo \n(https://doi.org/10.5281/zenodo.8321735)89.\nCode availability\nSource code used to train models, analyse generated molecules and \nreproduce the figures, along with relevant intermediate data files, is \navailable via Zenodo (https://doi.org/10.5281/zenodo.10680855)90.\nReferences\n1. Reymond, J.-L. The chemical space project. Acc. Chem. Res. 48, \n722–730 (2015).\n2. Bohacek, R. S., McMartin, C. & Guida, W. C. The art and practice of \nstructure-based drug design: a molecular modeling perspective. \nMed. Res. Rev. 16, 3–50 (1996).\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 446\nArticle https://doi.org/10.1038/s42256-024-00821-x\n3. Lipinski, C. & Hopkins, A. Navigating chemical space for biology \nand medicine. Nature 432, 855–861 (2004).\n4. Dobson, C. M. Chemical space and biology. Nature 432, 824–828 \n(2004).\n5. Lameijer, E.-W., Kok, J. N., Bäck, T. & Ijzerman, A. P. The molecule \nevoluator. An interactive evolutionary algorithm for the design of \ndrug-like molecules. J. Chem. Inf. Model. 46, 545–552 (2006).\n6. van Deursen, R. & Reymond, J.-L. Chemical space travel. \nChemMedChem 2, 636–640 (2007).\n7. Nicolaou, C. A., Apostolakis, J. & Pattichis, C. S. De novo drug \ndesign using multiobjective evolutionary graphs. J. Chem. Inf. \nModel. 49, 295–307 (2009).\n8. Virshup, A. M., Contreras-García, J., Wipf, P., Yang, W. & Beratan, D. N.  \nStochastic voyages into uncharted chemical space produce a \nrepresentative library of all possible drug-like compounds. J. Am. \nChem. Soc. 135, 7296–7303 (2013).\n9. Li, L. et al. MyCompoundID: using an evidence-based \nmetabolome library for metabolite identification. Anal. Chem. 85, \n3401–3408 (2013).\n10. Djoumbou-Feunang, Y. et al. BioTransformer: a comprehensive \ncomputational tool for small molecule metabolism prediction \nand metabolite identification. J. Cheminform. 11, 2 (2019).\n11. Jeffryes, J. G. et al. MINEs: open access databases of \ncomputationally predicted enzyme promiscuity products for \nuntargeted metabolomics. J. Cheminform. 7, 44 (2015).\n12. Jensen, J. H. A graph-based genetic algorithm and generative \nmodel/Monte Carlo tree search for the exploration of chemical \nspace. Chem. Sci. 10, 3567–3572 (2019).\n13. Anstine, D. M. & Isayev, O. Generative models as an emerging \nparadigm in the chemical sciences. J. Am. Chem. Soc. 145, \n8736–8750 (2023).\n14. Bilodeau, C., Jin, W., Jaakkola, T., Barzilay, R. & Jensen, K. F. \nGenerative models for molecular discovery: recent advances \nand challenges. WIREs Comput. Mol. Sci. https://doi.org/10.1002/\nwcms.1608 (2022).\n15. Schwalbe-Koda, D. & Gómez-Bombarelli, R. Generative models for \nautomatic chemical design. In Machine Learning Meets Quantum \nPhysics (eds Schütt, K. T. et al.) Vol. 968, 445–467 (Springer, 2020).\n16. Vanhaelen, Q., Lin, Y.-C. & Zhavoronkov, A. The advent of \ngenerative chemistry. ACS Med. Chem. Lett. 11, 1496–1505 (2020).\n17. Merk, D., Friedrich, L., Grisoni, F. & Schneider, G. De novo design \nof bioactive small molecules by artificial intelligence. Mol. Inform. \n37, 1700153 (2018).\n18. Zhavoronkov, A. et al. Deep learning enables rapid identification \nof potent DDR1 kinase inhibitors. Nat. Biotechnol. 37, 1038–1040 \n(2019).\n19. Merk, D., Grisoni, F., Friedrich, L. & Schneider, G. Tuning artificial \nintelligence on the de novo design of natural-product-inspired \nretinoid X receptor modulators. Commun. Chem. 1, 68 (2018).\n20. Moret, M., Helmstädter, M., Grisoni, F., Schneider, G. & Merk, D. \nBeam search for automated design and scoring of novel ROR \nligands with machine intelligence. Angew. Chem. Int. Ed. 60, \n19477–19482 (2021).\n21. Grisoni, F. et al. Combining generative artificial intelligence  \nand on-chip synthesis for de novo drug design. Sci. Adv. 7, \neabg3338 (2021).\n22. Li, Y. et al. Generative deep learning enables the discovery of \na potent and selective RIPK1 inhibitor. Nat. Commun. 13, 6891 \n(2022).\n23. Ballarotto, M. et al. De novo design of Nurr1 agonists via \nfragment-augmented generative deep learning in low-data \nregime. J. Med. Chem. 66, 8170–8177 (2023).\n24. Moret, M. et al. Leveraging molecular structure and bioactivity \nwith chemical language models for de novo drug design.  \nNat. Commun. 14, 114 (2023).\n25. Li, Y., Zhang, L. & Liu, Z. Multi-objective de novo drug design with \nconditional graph generative model. J. Cheminform. 10, 33 (2018).\n26. Mercado, R. et al. Practical notes on building molecular graph \ngenerative models. Appl. AI Lett. https://doi.org/10.1002/ail2.18 (2020).\n27. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational \nautoencoder for molecular graph generation. In Proc. 35th \nInternational Conference on Machine Learning (eds Dy, J. &  \nKrause, A.) Vol. 80, 2323–2332 (PMLR, 2018).\n28. Li, Y., Pei, J. & Lai, L. Structure-based de novo drug design using \n3D deep generative models. Chem. Sci. 12, 13664–13675 (2021).\n29. Xie, W., Wang, F., Li, Y., Lai, L. & Pei, J. Advances and challenges \nin de novo drug design using three-dimensional deep generative \nmodels. J. Chem. Inf. Model. 62, 2269–2279 (2022).\n30. Segler, M. H. S., Kogej, T., Tyrchan, C. & Waller, M. P. Generating \nfocused molecule libraries for drug discovery with recurrent \nneural networks. ACS Cent. Sci. 4, 120–131 (2018).\n31. Gómez-Bombarelli, R. et al. Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Cent. \nSci. 4, 268–276 (2018).\n32. Weininger, D. SMILES, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules.  \nJ. Chem. Inf. Model. 28, 31–36 (1988).\n33. O’Boyle, N. & Dalke, A. DeepSMILES: an adaptation of SMILES for \nuse in machine-learning of chemical structures. https://doi.org/ \n10.26434/chemrxiv.7097960 (2018).\n34. Krenn, M., Häse, F., Nigam, A., Friederich, P. & Aspuru-Guzik, A. \nSelf-referencing embedded strings (SELFIES): a 100% robust \nmolecular string representation. Mach. Learn. Sci. Technol. 1, \n045024 (2020).\n35. Cheng, A. H. et al. Group SELFIES: a robust fragment-based \nmolecular string representation. Digital Discov. 2, 748–758 (2023).\n36. Dai, H., Tian, Y., Dai, B., Skiena, S. & Song, L. Syntax-directed \nvariational autoencoder for structured data. Preprint at https://\ndoi.org/10.48550/arXiv.1802.08786 (2018).\n37. Kusner, M. J., Paige, B. & Hernández-Lobato, J. M. Grammar \nvariational autoencoder. Preprint at https://doi.org/10.48550/\narxiv.1703.01925 (2017).\n38. Schoenmaker, L., Béquignon, O. J. M., Jespers, W. & van Westen, \nG. J. P. UnCorrupt SMILES: a novel approach to de novo design.  \nJ. Cheminform. 15, 22 (2023).\n39. Zheng, S., Rao, J., Zhang, Z., Xu, J. & Yang, Y. Predicting \nretrosynthetic reactions using self-corrected transformer neural \nnetworks. J. Chem. Inf. Model. 60, 47–55 (2020).\n40. Bilsland, A. E., McAulay, K., West, R., Pugliese, A. & Bower, J. \nAutomated generation of novel fragments using screening \ndata, a dual SMILES autoencoder, transfer learning and syntax \ncorrection. J. Chem. Inf. Model. 61, 2547–2559 (2021).\n41. Walters, W. P. & Barzilay, R. Applications of deep learning in \nmolecule generation and molecular property prediction. Acc. \nChem. Res. 54, 263–270 (2021).\n42. Guo, M. et al. Data-efficient graph grammar learning for molecular \ngeneration. Preprint at https://doi.org/10.48550/arxiv.2203.08031 \n(2022).\n43. De Cao, N. & Kipf, T. MolGAN: An implicit generative model for \nsmall molecular graphs. Preprint at https://doi.org/10.48550/\narXiv.1805.11973 (2018).\n44. Li, Y., Vinyals, O., Dyer, C., Pascanu, R. & Battaglia, P. Learning \ndeep generative models of graphs. Preprint at https://doi.org/ \n10.48550/arXiv.1803.03324 (2018).\n45. Ma, T., Chen, J. & Xiao, C. Constrained generation of semantically \nvalid graphs via regularizing variational autoencoders. Preprint at \nhttps://doi.org/10.48550/arXiv.1809.02630 (2018).\n46. Liu, Q., Allamanis, M., Brockschmidt, M. & Gaunt, A. L. \nConstrained graph variational autoencoders for molecule design. \nPreprint at https://doi.org/10.48550/arxiv.1805.09076 (2018).\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448\n 447\nArticle https://doi.org/10.1038/s42256-024-00821-x\n47. Zang, C. & Wang, F. MoFlow: an invertible flow model for \ngenerating molecular graphs. In Proc. 26th ACM SIGKDD \nInternational Conference on Knowledge Discovery & Data Mining \nhttps://doi.org/10.1145/3394486.3403104 (ACM, 2020).\n48. Polykovskiy, D. et al. Molecular sets (MOSES): a benchmarking \nplatform for molecular generation models. Front. Pharmacol. 11, \n565644 (2020).\n49. Brown, N., Fiscato, M., Segler, M. H. S. & Vaucher, A. C. GuacaMol: \nbenchmarking models for de novo molecular design. J. Chem. Inf. \nModel. 59, 1096–1108 (2019).\n50. Skinnider, M. A., Stacey, R. G., Wishart, D. S. & Foster, L. J. \nChemical language models enable navigation in sparsely \npopulated chemical space. Nat. Mach. Intell. 3, 759–770 (2021).\n51. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models \ncan learn complex molecular distributions. Nat. Commun. 13, \n3293 (2022).\n52. Mahmood, O., Mansimov, E., Bonneau, R. & Cho, K. Masked graph \nmodeling for molecule generation. Nat. Commun. 12, 3156 (2021).\n53. Mendez, D. et al. ChEMBL: towards direct deposition of bioassay \ndata. Nucleic Acids Res. 47, D930–D940 (2019).\n54. Bemis, G. W. & Murcko, M. A. The properties of known drugs. 1. \nMolecular frameworks. J. Med. Chem. 39, 2887–2893 (1996).\n55. Blum, L. C. & Reymond, J.-L. 970 million druglike small molecules \nfor virtual screening in the chemical universe database GDB-13.  \nJ. Am. Chem. Soc. 131, 8732–8733 (2009).\n56. Bjerrum, E. J. SMILES enumeration as data augmentation for \nneural network modeling of molecules. Preprint at https://doi.org/ \n10.48550/arXiv.1703.07076 (2017).\n57. Born, J. et al. Chemical representation learning for toxicity \nprediction. Digital Discov. https://doi.org/10.1039/D2DD00099G \n(2023).\n58. Vaswani, A. et al. Attention is all you need. Preprint at  \nhttps://doi.org/10.48550/arxiv.1706.03762 (2017).\n59. Lo, A. et al. Recent advances in the self-referencing embedded \nstrings (SELFIES) library. Digital Discov. 2, 897–908 (2023).\n60. Kiappes, J. L. in Technology-Enabled Blended Learning \nExperiences for Chemistry Education and Outreach (eds Fung, F. M. \n& Zimmermann, C.) Ch. 3, 43–64 (Elsevier, 2021).\n61. Arús-Pous, J. et al. Exploring the GDB-13 chemical space using \ndeep generative models. J. Cheminform. 11, 20 (2019).\n62. Skinnider, M. A. et al. A deep generative model enables \nautomated structure elucidation of novel psychoactive \nsubstances. Nat. Mach. Intell. 3, 973–984 (2021).\n63. Elton, D. C., Boukouvalas, Z., Fuge, M. D. & Chung, P. W. Deep \nlearning for molecular design—a review of the state of the art. \nMol. Syst. Des. Eng. https://doi.org/10.1039/C9ME00039A (2019).\n64. Özçelik, R., de Ruiter, S. & Grisoni, F. Structured state-space \nsequence models for de novo drug design. Preprint at ChemRxiv \nhttps://doi.org/10.26434/chemrxiv-2023-jwmf3 (2023).\n65. Guo, J. et al. Improving de novo molecular design with curriculum \nlearning. Nat. Mach. Intell. 4, 555–563 (2022).\n66. Mokaya, M. et al. Testing the limits of SMILES-based de novo \nmolecular generation with curriculum and deep reinforcement \nlearning. Nat. Mach. Intell. https://doi.org/10.1038/ \ns42256-023-00636-2 (2023).\n67. Born, J. & Manica, M. Regression transformer enables concurrent \nsequence regression and generation for molecular language \nmodelling. Nat. Mach. Intell. https://doi.org/10.1038/ \ns42256-023-00639-z (2023).\n68. Wellawatte, G. P., Seshadri, A. & White, A. D. Model agnostic \ngeneration of counterfactual explanations for molecules. Chem. \nSci. 13, 3697–3705 (2022).\n69. Gandhi, H. A. & White, A. D. Explaining molecular properties with \nnatural language. Preprint at ChemRxiv https://doi.org/10.26434/\nchemrxiv-2022-v5p6m-v3 (2022).\n70. Nigam, A., Pollice, R., Krenn, M., Gomes, G. D. P. & Aspuru-Guzik, A. \nBeyond generative models: superfast traversal, optimization, \nnovelty, exploration and discovery (STONED) algorithm for \nmolecules using SELFIES. Chem. Sci. 12, 7079–7090 (2021).\n71. Shen, C., Krenn, M., Eppel, S. & Aspuru-Guzik, A. Deep molecular \ndreaming: inverse machine learning for de-novo molecular design \nand interpretability with surjective representations. Mach. Learn. \nSci. Technol. 2, 03LT02 (2021).\n72. Hu, G. & Qiu, M. Machine learning-assisted structure annotation \nof natural products based on MS and NMR data. Nat. Prod. Rep. \n40, 1735–1753 (2023).\n73. Jaeger, S., Fulle, S. & Turk, S. Mol2vec: unsupervised machine \nlearning approach with chemical intuition. J. Chem. Inf. Model. 58, \n27–35 (2018).\n74. Rogers, D. & Hahn, M. Extended-connectivity fingerprints.  \nJ. Chem. Inf. Model. 50, 742–754 (2010).\n75. Arús-Pous, J. et al. Randomized SMILES strings improve the quality \nof molecular generative models. J. Cheminform. 11, 71 (2019).\n76. Moret, M., Friedrich, L., Grisoni, F., Merk, D. & Schneider, G. \nGenerative molecular design in low data regimes. Nat. Mach. \nIntell. 2, 171–180 (2020).\n77. Blaschke, T. et al. REINVENT 2.0: an AI tool for de novo drug \ndesign. J. Chem. Inf. Model. 60, 5918–5922 (2020).\n78. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving \nlanguage understanding by generative pre-training.  \nhttps://openai.com/research/language-unsupervised (2018).\n79. Bagal, V., Aggarwal, R., Vinod, P. K. & Priyakumar, U. D. MolGPT: \nMolecular generation using a transformer-decoder model.  \nJ. Chem. Inf. Model. 62, 2064–2076 (2022).\n80. Preuer, K., Renz, P., Unterthiner, T., Hochreiter, S. & Klambauer, G.  \nFréchet chemnet distance: a metric for generative models for \nmolecules in drug discovery. J. Chem. Inf. Model. 58, 1736–1741 (2018).\n81. Ertl, P., Roggo, S. & Schuffenhauer, A. Natural product-likeness \nscore and its application for prioritization of compound libraries. \nJ. Chem. Inf. Model. 48, 68–74 (2008).\n82. Wildman, S. A. & Crippen, G. M. Prediction of physicochemical \nparameters by atomic contributions. J. Chem. Inf. Comput. Sci. \n39, 868–873 (1999).\n83. Bertz, S. H. The first general index of molecular complexity. J. Am. \nChem. Soc. 103, 3599–3601 (1981).\n84. Ertl, P., Rohde, B. & Selzer, P. Fast calculation of molecular polar \nsurface area as a sum of fragment-based contributions and its \napplication to the prediction of drug transport properties. J. Med. \nChem. 43, 3714–3717 (2000).\n85. Wang, F. et al. CFM-ID 4.0: More accurate ESI-MS/MS spectral \nprediction and compound identification. Anal. Chem. 93, \n11692–11700 (2021).\n86. Rutz, A. et al. The LOTUS initiative for open knowledge \nmanagement in natural products research. eLife 11, e70780 (2022).\n87. Sorokina, M., Merseburger, P., Rajan, K., Yirik, M. A. & Steinbeck, C. \nCOCONUT online: collection of open natural products database. \nJ. Cheminform. 13, 2 (2021).\n88. Mohammed Taha, H. et al. The NORMAN Suspect List \nExchange (NORMAN-SLE): facilitating European and worldwide \ncollaboration on suspect screening in high resolution mass \nspectrometry. Environ. Sci. Eur. 34, 104 (2022).\n89. Skinnider, M. A. Molecules used to train or generated by \nchemical language models. Zenodo https://doi.org/10.5281/\nzenodo.8321735 (2023).\n90. Skinnider, M. A. Code used to train chemical language models \nand analyze generated molecules. Zenodo https://doi.org/ \n10.5281/zenodo.10680855 (2024).\nAcknowledgements\nM.A.S. acknowledges support from Ludwig Cancer Research.\nNature Machine Intelligence | Volume 6 | April 2024 | 437–448 448\nArticle https://doi.org/10.1038/s42256-024-00821-x\nAuthor contributions\nM.A.S. designed and performed experiments and wrote the \nmanuscript.\nCompeting interests\nThe author declares no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-024-00821-x.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-024-00821-x.\nCorrespondence and requests for materials should be addressed to \nMichael A. Skinnider.\nPeer review information Nature Machine Intelligence thanks Tao Huan, \nSaer Samanipour, and the other, anonymous, reviewer(s) for their \ncontribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n1\n2\n3\n4\n5\nFrechet ChemNet distance\nTransformer\nSMILESSELFIES\n85\n90\n95\n100\n% valid\nTransformer\nSMILESSELFIES\n−1.0\n−0.5\n0.0\n0.5\n1.0\nPC1\nTransformer\nSMILESSELFIES\n0.70\n0.72\n0.74\nJSD, Murcko scaffolds\nSMILESSELFIES\na\n0.05\n0.10\n0.15\n0.20\nJSD, natural product−likeness\nSMILESSELFIES\nb\n0.10\n0.12\n0.14\n0.16\nJSD, % stereocentres\nSMILESSELFIES\nc\n−0.5\n0.0\n0.5\n1.0\n1.5\nPC1\nSMILESSELFIES\nd\n90\n92\n94\n96\n98\n100\n70\n80\n90\n100\n% valid\n30,000 molecules 300,000 molecules\nSMILESSELFIES SMILESSELFIES\ne\n0.5\n1.0\n1.5\n2.0\n2.5\n2\n4\n6\nFrechet ChemNet distance\n30,000 molecules 300,000 molecules\nSMILESSELFIES SMILESSELFIES\nf\n0.5\n1.0\n1.5\n−2.0\n−1.5\n−1.0\n−0.5\n0.0\n0.5\nPC1\n30,000 molecules 300,000 molecules\nSMILESSELFIES SMILESSELFIES\ng\n97\n98\n99\n100\n% valid\nGDB−13\nSMILESSELFIES\nh\n0.05\n0.10\n0.15\n0.20\nFrechet ChemNet distance\nGDB−13\nSMILESSELFIES\ni\n2.0\n2.2\n2.4\n2.6\nPC1\nGDB−13\nSMILESSELFIES\nj\n90\n92\n94\n96\n98\n100\n88\n90\n92\n94\n96\n98\n100\n90\n95\n100\n% valid\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nSMILESSELFIES SMILESSELFIES SMILESSELFIES\nk\n1\n2\n3\n4\n1\n2\n3\n1\n2\n3\nFrechet ChemNet distance\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nSMILESSELFIES SMILESSELFIES SMILESSELFIES\nl\n−1\n0\n1\n2\n−1\n0\n1\n−0.5\n0.0\n0.5\n1.0\nPC1\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nSMILESSELFIES SMILESSELFIES SMILESSELFIES\nm\n92\n94\n96\n98\n100\n92\n94\n96\n98\n100\n% valid\n10x augmentation 30x augmentation\nSMILESSELFIES SMILESSELFIES\nn\n0.5\n1.0\n1.5\n2.0\n0.5\n1.0\n1.5\n2.0\nFrechet ChemNet distance\n10x augmentation 30x augmentation\nSMILESSELFIES SMILESSELFIES\no\n0.4\n0.8\n1.2\n1.6\n0.4\n0.8\n1.2\n1.6\nPC1\n10x augmentation 30x augmentation\nSMILESSELFIES SMILESSELFIES\np q r s\nExtended Data Fig. 1 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 1 | Chemical language models trained on SMILES robustly \noutperform models trained on SELFIES. a, Jensen–Shannon distances between \nthe Murcko scaffold compositions of generated and training molecules, shown \nfor language models trained on SMILES versus SELFIES representations (n = 10 \neach; p = 3.6 × 10\n–6, paired t-test). b, Jensen–Shannon distances between the \nnatural product-likeness scores of generated and training molecules, shown for \nlanguage models trained on SMILES versus SELFIES representations (n = 10 each; \np = 1.8 × 10\n–9, paired t-test). c, Jensen–Shannon distances between the fraction \nof atoms in each molecule that are stereocenters in generated and training \nmolecules, shown for language models trained on SMILES versus SELFIES \nrepresentations (n = 10 each; p = 0.10, paired t-test). d, PC1 scores integrating \nmultiple distribution-learning metrics for language models trained on SMILES \nversus SELFIES representations (higher is better; n = 10 each; p = 3.5 × 10\n–8, \npaired t-test). e, Proportion of valid molecules generated by language models \ntrained on SMILES versus SELFIES representations, for models trained on 30,000 \nor 300,000 molecules (n = 10 each; all p ≤ 6.7 × 10\n–8, paired t-test). f, Fréchet \nChemNet distances between generated and training molecules for language \nmodels trained on SMILES versus SELFIES representations, for models trained on \n30,000 or 300,000 molecules (n = 10 each; all p ≤ 3.0 × 10\n–8, paired t-test). g, PC1 \nscores integrating multiple distribution-learning metrics for language models \ntrained on SMILES versus SELFIES representations, for models trained on 30,000 \nor 300,000 molecules (n = 10 each; all p ≤ 6.9 × 10–6, paired t-test). h, As in e, but \nfor models trained on molecules from the GDB-13 database (p = 2.6 × 10–9). i, As \nin f, but for models trained on molecules from the GDB-13 database (p = 0.011). \nj, As in g, but for models trained on molecules from the GDB-13 database (p = 5.2 \n× 10–4). k, As in e, but for models trained on sets of molecules with decreasing \nchemical diversity, as quantified by the minimum Tanimoto coefficient (T c) \nfrom the seed molecule (all p ≤ 2.4 × 10\n–7). l, As in f, but for models trained on sets \nof molecules with decreasing chemical diversity (all p ≤ 3.2 × 10–7). m, As in g, \nbut for models trained on sets of molecules with decreasing chemical diversity \n(all p ≤ 4.0 × 10\n–7). n, As in e, but for models trained with data augmentation by \nnon-canonical SMILES or SELFIES enumeration (all p ≤ 8.0 × 10–11). o, As in f, but \nfor models trained with data augmentation by non-canonical SMILES or SELFIES \nenumeration (all p ≤ 3.9 × 10\n–7). p, As in g, but for models trained with data \naugmentation by non-canonical SMILES or SELFIES enumeration (p ≤ 8.0 × 10–6). \nq, As in e, but for models based on the transformer architecture (p = 3.1 × 10–9).  \nr, As in f, but for models based on the transformer architecture (p = 1.7 × 10–8).  \ns, As in g, but for models based on the transformer architecture (p = 7.9 × 10–9).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n99.25\n99.5\n99.75\n100\n% novel\nTransformer\nSMILESSELFIES\n85\n90\n95\n100\n% novel\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nSMILESSELFIES SMILESSELFIES SMILESSELFIES\n99.96\n99.97\n99.98\n99.99\n100\n% novel\nGDB−13\nSMILESSELFIES\n99.85\n99.9\n99.95\n100\n% novel\n10x augmentation 30x augmentation\nSMILESSELFIES SMILESSELFIES\n99\n99.2\n99.4\n99.6\n99.8\n100\n% novel\n30,000 molecules 300,000 molecules\nSMILESSELFIES SMILESSELFIES\n97.5\n98\n98.5\n99\n99.5\n100\n% novel\nSMILESSELFIES\na b c\ne\nd\nf\nExtended Data Fig. 2 | Chemical language models trained on SMILES and \nSELFIES generate novel molecules at a high rate. a, Proportion of novel \nmolecules generated by language models trained on SMILES versus SELFIES \nrepresentations (n = 10 each; p = 0.025, paired t-test). b, As in a, but for models \ntrained on 30,000 or 300,000 molecules (p = 0.98 and 7.8 × 10\n–6, respectively).  \nc, As in a, but for models trained on molecules from the GDB-13 database \n(p = 0.078). d, As in a, but for models trained on sets of molecules with decreasing \nchemical diversity (p ≤ 0.032). e, As in a, but for models trained with data \naugmentation by non-canonical SMILES or SELFIES enumeration (p ≤ 2.5 × 10–5).  \nf, As in a, but for models based on the transformer architecture (p = 0.0028).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n0.0\n0.5\n1.0\n1.5\nCohen's d\nTransformer\nValid vs\n.\ninvalid\nSMILES\n0\n2\n4\n6\nCohen's d\nTransformer\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nCohen's d\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nValid vs\n.\ninvalid\nSMILES\nValid vs\n.\ninvalid\nSMILES\nValid vs\n.\ninvalid\nSMILES\n0.0\n0.5\n1.0\n1.5\n2.0\n0\n1\n2\n3\n0\n1\n2\n3\n4\nCohen's d\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nCohen's d\n10x\naugmentation\n30x\naugmentation\nValid vs\n.\ninvalid\nSMILES\nValid vs\n.\ninvalid\nSMILES\n0\n2\n4\n0\n2\n4\nCohen's d\n10x\naugmentation\n30x\naugmentation\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\n0.0\n0.5\n1.0\n1.5\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nCohen's d\n30,000\nmolecules\n300,000\nmolecules\nValid vs\n.\ninvalid\nSMILES\nValid vs\n.\ninvalid\nSMILES\ne\n0\n1\n2\n3\n4\n0\n1\n2\n3\nCohen's d\n30,000\nmolecules\n300,000\nmolecules\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nf\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCohen's d\nGDB−13\nValid vs\n.\ninvalid\nSMILES\ng\n0\n1\n2\n3\n4\nCohen's d\nGDB−13\nAromaticity error\nBond e\nxists\nParentheses error\nSyntax errorUnclosed r\ning\nValence error\nh\ni j\nk l m\n0\n25\n50\n75\n100\nLength\nInvalid\nSMILES\nValid\nSMILES\n0.00\n0.25\n0.50\n0.75\n1.00\nCohen's d\nValid vs\n.\ninvalid\nSMILES\n0.25\n0.50\n0.75\nLoss\nInvalid\nSMILES\nValid\nSMILES\n0.0\n0.1\n0.2\n0.3\nCohen's d\nValid vs\n.\ninvalid\nSMILES\na b c d\nn\nExtended Data Fig. 3 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 3 | Invalid SMILES are low-likelihood samples from \nchemical language models across architectures and training datasets.  \na, Lengths of valid versus invalid SMILES sampled from a representative chemical \nlanguage model (n = 107 SMILES; p < 10–15, two-sided t-test). b, Effect sizes \n(Cohen’s d) comparing the lengths of valid versus invalid SMILES sampled from \nn = 10 chemical language models, demonstrating consistent effects  \n(p = 1.5 × 10–13, two-sided one-sample t-test). c, Per-character losses of valid \nversus invalid SMILES sampled from a representative chemical language model \n(n = 10\n7 SMILES; p < 10–15, two-sided t-test). d, Effect sizes (Cohen’s d) comparing \nper-character losses of valid versus invalid SMILES sampled from n = 10 chemical \nlanguage models, demonstrating consistent effects (p = 2.4 × 10–5, two-sided \none-sample t-test). e, Effect sizes (Cohen’s d) comparing the losses of valid versus \ninvalid SMILES sampled from n = 10 chemical language models trained on 30,000 \nor 300,000 molecules (all p ≤ 1.5 × 10–12, two-sided t-test). f, Losses of valid \nSMILES versus invalid SMILES sampled from a representative chemical language \nmodel trained on 30,000 or 300,000 molecules, classified into six different \ncategories based on RDKit error messages (n = 10\n7 SMILES; all p ≤ 1.1 × 10–9,  \ntwo-sided one-sample t-test). g, As in e, but for models trained on molecules  \nfrom the GDB-13 database (p = 2.0 × 10–11). h, As in f, but for models trained on \nmolecules from the GDB-13 database (all p ≤ 4.5 × 10–10). i, As in e, but for models \ntrained on sets of molecules with decreasing chemical diversity, as quantified  \nby the minimum Tanimoto coefficient (T c) from the seed molecule (all  \np ≤ 1.1 × 10–12). j, As in f, but for models trained on sets of molecules with \ndecreasing chemical diversity (all p ≤ 4.3 × 10–9). k, As in e, but for models trained \nwith data augmentation by non-canonical SMILES or SELFIES enumeration  \n(all p ≤ 1.2 × 10–13). l, As in f, but for models trained with data augmentation by \nnon-canonical SMILES or SELFIES enumeration (all p ≤ 4.3 × 10–10). m, As in e,  \nbut for models based on the transformer architecture (p = 4.3 × 10–14). n, As in f, \nbut for models based on the transformer architecture (all p ≤ 6.5 × 10–8).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n0.0\n0.2\n0.4\n0.6\n0.8\nCohen's d\nTransformer\nValid vs\n.\ninvalid\nSELFIES\n0.00\n0.25\n0.50\n0.75\n0.0\n0.2\n0.4\n0.6\n0.8\nCohen's d\n30,000\nmolecules\n300,000\nmolecules\nValid vs\n.\ninvalid\nSELFIES\nValid vs\n.\ninvalid\nSELFIES\n0.5\n1.0\n1.5\n2.0\n2.5\n2\n4\n6\nFrechet ChemNet distance\n30,000 molecules 300,000 molecules\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n0.0\n0.5\n1.0\nCohen's d\nGDB−13\nValid vs\n.\ninvalid\nSELFIES\n0.05\n0.10\n0.15\n0.20\nFrechet ChemNet distance\nGDB−13\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n0.0\n0.3\n0.6\n0.9\n0.00\n0.25\n0.50\n0.75\n1.00\nCohen's d\n10x\naugmentation\n30x\naugmentation\nValid vs\n.\ninvalid\nSELFIES\nValid vs\n.\ninvalid\nSELFIES\n0.5\n1.0\n1.5\n2.0\n0.5\n1.0\n1.5\n2.0\nFrechet ChemNet distance\n10x augmentation 30x augmentation\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n80\n85\n90\n95\n100\n% valid\nSMILESSELFIES\nTexas\nSELFIES\na\n0.70\n0.72\n0.74\nJSD, Murcko scaffolds\nSMILESSELFIES\nTexas\nSELFIES\nb\n0.05\n0.10\n0.15\n0.20\nJSD, natural product−likeness\nSMILESSELFIES\nTexas\nSELFIES\nc\n0.10\n0.12\n0.14\n0.16\nJSD, % stereocenters\nSMILESSELFIES\nTexas\nSELFIES\nd\n60\n70\n80\n90\n100\n% valid\nSMILESSELFIES\nUnconstr\nained\nSELFIES\ne\n0.70\n0.72\n0.74\nJSD, Murcko scaffolds\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nf\n0.05\n0.10\n0.15\n0.20\nJSD, natural product−likeness\nSMILESSELFIES\nUnconstr\nained\nSELFIES\ng\n0.10\n0.12\n0.14\n0.16\nJSD, % stereocenters\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nh\ni j k l\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n0.0\n0.2\n0.4\n0.6\n0.8\nCohen's d\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nValid vs\n.\ninvalid\nSELFIES\nValid vs\n.\ninvalid\nSELFIES\nValid vs\n.\ninvalid\nSELFIES\n1\n2\n3\n4\n1\n2\n3\n1\n2\n3\nFrechet ChemNet distance\nTc ≥ 0.05 Tc ≥ 0.10 Tc ≥ 0.15\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nm n\no p q r\n1\n2\n3\n4\n5\nFrechet ChemNet distance\nTransformer\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nExtended Data Fig. 4 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 4 | Generating invalid outputs robustly improves the \nperformance of chemical language models. a, Fréchet ChemNet distance \nbetween training and generated molecules for language models trained on \nSMILES or SELFIES representations, and with SELFIES valency constraints \nmodified to allow pentavalent carbons (‘T exas SELFIES’; n = 10 each; p = 2.8 × 10\n–14  \ncompared to default valency constraints, paired t-test). b, Jensen–Shannon \ndistances between the Murcko scaffold compositions of training and generated \nmolecules for language models trained on SMILES or SELFIES representations, \nand with SELFIES valency constraints modified to allow pentavalent carbons \n(n = 10 each; p = 7.6 × 10–5 compared to default valency constraints, paired t-test). \nc, Jensen–Shannon distances between the natural product-likeness scores \nof training and generated molecules for language models trained on SMILES \nor SELFIES representations, and with SELFIES valency constraints modified \nto allow pentavalent carbons (n = 10 each; p = 1.0 × 10\n–5 compared to default \nvalency constraints, paired t-test). d, Jensen–Shannon distances between the \nfraction of atoms that are stereocenters in training and generated molecules \nfor language models trained on SMILES or SELFIES representations, and with \nSELFIES valency constraints modified to allow pentavalent carbons (n = 10 each; \np = 0.058 compared to default valency constraints, paired t-test). e-h, As in a-d, \nbut showing the removal of SELFIES valency constraints entirely (‘unconstrained \nSELFIES’; p = 2.6 × 10–15, p = 5.6 × 10–16, p = 8.5 × 10–8, and p = 0.38, respectively). \ni, Effect sizes (Cohen’s d) comparing the losses of valid versus invalid SELFIES \nsampled from n = 10 chemical language models trained on 30,000 or 300,000 \nmolecules (all p ≤ 4.5 × 10–14, two-sided one-sample t-test). j, Fréchet ChemNet \ndistances between generated and training molecules for language models \ntrained on SMILES versus SELFIES representations, and with SELFIES valency \nconstraints disabled, for models trained on 30,000 or 300,000 molecules (n = 10 \neach; all p ≤ 3.2 × 10\n–5, paired t-test). k-l, As in i-j, but showing language models \ntrained on molecules from the GDB-13 database (losses, p = 1.0 × 10–11; Fréchet \nChemNet distances, p = 0.17). m-n, As in i-j, but showing language models trained \non sets of molecules with decreasing chemical diversity (losses, all p ≤ 8.7 × 10\n–10; \nFréchet ChemNet distances, all p ≤ 5.4 × 10–6). o-p, As in i-j, but showing language \nmodels trained with data augmentation by non-canonical SMILES enumeration \n(losses, all p ≤ 3.3 × 10\n–15; Fréchet ChemNet distances, all p ≤ 2.1 × 10–4). q-r, As in \ni-j, but showing language models based on the transformer architecture (losses, \np = 5.3 × 10\n–14; Fréchet ChemNet distances, p = 8.8 × 10–11).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n200\n400\n600\nMolecular weight\nTraining setSMILESSELFIES\n−0.15\n−0.10\n−0.05\n0.00\nEffect size\nMolecular weight\nSMILES SELFIES\n0\n3\n6\nLogP\nTraining setSMILESSELFIES\n−0.10\n−0.05\n0.00\n0.05\nEffect size\nLogP\nSMILES SELFIES\n0\n500\n1000\n1500\n2000\nTopological complexity\nTraining setSMILESSELFIES\n−0.2\n−0.1\n0.0\nEffect size\nTopological complexity\nSMILES SELFIES\na b c d e f\n0\n50\n100\n150\nTopological polar surface area\nTraining setSMILESSELFIES\n−0.15\n−0.10\n−0.05\n0.00\nEffect size\nTPSA\nSMILES SELFIES\n0.00\n0.25\n0.50\n0.75\n1.00\n% sp3 carbons\nTraining setSMILESSELFIES\n−0.1\n0.0\n0.1\nEffect size\n% sp3 carbons\nSMILES SELFIES\n0.0\n0.1\n0.2\n0.3\n0.4\n% rotatable bonds\nTraining setSMILESSELFIES\n0.00\n0.05\n0.10\n0.15\n0.20\nEffect size\n% rotatable bonds\nSMILES SELFIES\ng h i j k l\n0.00\n0.05\n0.10\n0.15\n% stereocenters\nTraining setSMILESSELFIES\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\nEffect size\n% stereocenters\nSMILES SELFIES\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n% heteroatoms\nTraining setSMILESSELFIES\n−0.10\n−0.05\n0.00\nEffect size\n% heteroatoms\nSMILES SELFIES\nm n o p\n0\n25\n50\n75\n100\n% of molecules\nTraining setSMILESSELFIES\n# of hydrogen donors\n10+\n8−9\n6−7\n4−5\n2−3\n0−1\nq\n−0.15\n−0.10\n−0.05\n0.00\nEffect size\n# of hydrogen donors\nSMILES SELFIES\nr\n0\n25\n50\n75\n100\n% of molecules\nTraining setSMILESSELFIES\n# of hydrogen acceptors\n10+\n8−9\n6−7\n4−5\n2−3\n0−1\ns\n−0.20\n−0.15\n−0.10\n−0.05\n0.00\nEffect size\n# of hydrogen acceptors\nSMILES SELFIES\nt\nExtended Data Fig. 5 | Biased exploration of chemical space by language \nmodels trained on SMILES versus SELFIES. a, Molecular weights of molecules \ngenerated by chemical language models trained on SMILES versus SELFIES, as \ncompared to the molecules in the training set (for valid molecules parsed from \nn = 10\n7 sampled SMILES or SELFIES). Horizontal line shows the median molecular \nweight of molecules in the training set. b, Effect sizes (Cohen’s d) comparing \nthe molecular weights of generated molecules from chemical language models \ntrained on SMILES versus SELFIES to the molecules in the training set (n = 10 each; \np = 2.9 × 10–4, paired t-test). c-d, As in a-b, but showing octanol–water partition \ncoefficients (p = 0.24). e-f, As in a-b, but showing topological complexity (p = 3.1 \n× 10\n–3). g-h, As in a-b, but showing topological polar surface area (p = 4.0 × 10–3). \ni-j, As in a-b, but showing the fraction of sp3 carbons (p = 8.3 × 10–3). k-l, As in a-b, \nbut showing the fraction of rotatable bonds (p = 8.5 × 10–5). m-n, As in a-b, but \nshowing the fraction of stereocenters (p = 1.0 × 10–5). o-p, As in a-b, but showing \nthe fraction of heteroatoms (p = 0.31). q-r, As in a-b, but showing the number \nof hydrogen donors (p = 8.0 × 10\n–5). s-t, As in a-b, but showing the number of \nhydrogen acceptors (p = 2.4 × 10–3).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n0\n25\n50\n75\n100\n% of molecules\nTraining set\nInvalidValid\n# of aromatic rings\n5+\n4\n3\n2\n1\n0\n0\n25\n50\n75\n100\n% of molecules\nTraining set\nInvalidValid\n# of aliphatic rings\n5+\n4\n3\n2\n1\n0\n200\n400\n600\nMolecular weight\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n−0.2\n−0.1\n0.0\n0.1\nEffect size\nMolecular weight\nInvalid Valid\n0.0\n2.5\n5.0\n7.5\nLogP\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n−0.1\n0.0\n0.1\n0.2\nEffect size\nLogP\nInvalid Valid\n0\n500\n1000\n1500\n2000\nTopological complexity\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n−0.3\n−0.2\n−0.1\n0.0\nEffect size\nTopological complexity\nInvalid Valid\n0\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n50\n100\n150\n200\nTopological polar surface area\n0.00\n0.25\n0.50\n0.75\n1.00\n% sp3 carbons\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n0.0\n0.1\n0.2\nEffect size\n% sp3 carbons\nInvalid Valid\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n% rotatable bonds\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\n0.0\n0.1\n0.2\n0.3\n0.4\nEffect size\n% rotatable bonds\nInvalid Valid\n−0.1\n0.0\nEffect size\nTPSA\nInvalid Valid\na b c d e f\ng h i j k l\n0.00\n0.05\n0.10\n0.15\n% stereocenters\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\nm\nEffect size\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% stereocenters\nInvalid Valid\nn\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n% heteroatoms\nTraining set\nInvalid\nSELFIES\nValid\nSELFIES\no\n−0.10\n−0.05\n0.00\nEffect size\n% heteroatoms\nInvalid Valid\np\n0\n25\n50\n75\n100\n% of molecules\nTraining set\nInvalidValid\n# of hydrogen donors\n10+\n8−9\n6−7\n4−5\n2−3\n0−1\n−0.05\n0.00\n0.05\n0.10\nEffect size\n# of hydrogen donors\nInvalid Valid\n0\n25\n50\n75\n100\n% of molecules\nTraining set\nInvalidValid\n# of hydrogen acceptors\n10+\n8−9\n6−7\n4−5\n2−3\n0−1\n−0.3\n−0.2\n−0.1\n0.0\n0.1\nEffect size\n# of hydrogen acceptors\nInvalid Valid\nq r s t\nu v\nAliphatic rings\n% stereocentres\nAromatic rings\nTPSA\n% sp3 carbonsLogP\n% heteroatoms2.5\n5.0\n7.5\n10.0\n12.5\n−0.2 0.0 0.2\n∆Valid −Invalid\n−log10(P)\nMolecular weight\n% rotatable bondsHydrogen acceptors\nw\nExtended Data Fig. 6 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 6 | Biased exploration of chemical space by language \nmodels trained on SELFIES with and without valency constraints.  \na, Molecular weights of molecules generated by chemical language models \ntrained on SELFIES, shown separately for valid versus invalid SELFIES when \nparsing generated SELFIES without valency constraints, as compared to the \nmolecules in the training set (for molecules parsed from n = 107 sampled SMILES \nor SELFIES). Horizontal line shows the median molecular weight of molecules in \nthe training set. b, Effect sizes (Cohen’s d) comparing the molecular weights of \ngenerated molecules from chemical language models trained on SELFIES to the \nmolecules in the training set, shown separately for valid versus invalid SELFIES \nwhen parsing generated SELFIES without valency constraints (n = 10 each; \np = 2.7 × 10\n–11, paired t-test). c-d, As in a-b, but showing octanol–water partition \ncoefficients (p = 3.2 × 10–9). e-f, As in a-b, but showing topological complexity \n(p = 1.6 × 10–10). g-h, As in a-b, but showing topological polar surface area  \n(p = 6.4 × 10–10). i-j, As in a-b, but showing the fraction of sp3 carbons (p = 2.9 × 10–9). \nk-l, As in a-b, but showing the fraction of rotatable bonds (p = 1.0 × 10–12).  \nm-n, As in a-b, but showing the fraction of stereocenters (p = 9.4 × 10–12).  \no-p, As in a-b, but showing the fraction of heteroatoms (p = 0.026). q-r,  \nAs in a-b, but showing the number of hydrogen donors (p = 7.0 × 10–10). s-t, As in  \na-b, but showing the number of hydrogen acceptors (p = 6.9 × 10–12). u, As in a, \nbut showing the number of aromatic rings. v, As in a, but showing the number \nof aliphatic rings. w, Volcano plot showing differences in structural properties \nbetween molecules generated as valid versus invalid SELFIES when parsing \ngenerated SELFIES without valency constraints (statistical significance versus \nmean difference in effect size, paired t-test). Dotted line shows p = 0.05.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n12.6% 1.1%\n0\n4\n8\n12\n16\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem\n0\n10\n20\n30\n40\nTop−k accuracy (%)\n1 3 10 30\nk\n17.1% 1.0%\n0\n5\n10\n15\n20\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem\n0\n20\n40\n60\nTop−k accuracy (%)\n1 3 10 30\nk\n83.6% 32.7%\n0\n25\n50\n75\n100\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem20\n40\n60\n80\n100\nTop−k accuracy (%)\n1 3 10 30\nk\n2.4%4.6% 2.7%\n0\n2\n4\n6\nTop−1 accuracy (%)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILES\nSELFIES\nUnconstrained \nSELFIES\n0\n5\n10\n15\n20\nTop−k accuracy (%)\n1 3 10 30\nk\nLanguage model\nPubChem\n0\n5\n10\n15\n20\nTop−k accuracy (%)\n1 3 10 30\nk\n4.6% 0.4%\n0\n2\n4\n6\nTop−1 accuracy (%)\nLanguage model\nPubChem\nOH\nO\nHO\nOH O\nO\nO\nHO\nO\nN\nN\nO\nHN\n0.23 0.21 0.15 0.11\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nTanimoto coefficient\nLanguage model\nTraining set\nLanguage model\n(random)PubChem\n29 1 63\n0\n50\n100\n150\n200\n# of candidate formulas\nLanguage model\nTraining setPubChem\n0.72 0.70 0.57 0.51\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTanimoto coefficient\nLanguage model\nTraining set\nLanguage model\n(random)PubChem\n0.26 0.17 0.12 0.12\n0.0\n0.2\n0.4\n0.6\nTanimoto coefficient\nLanguage model\nTraining set\nLanguage model\n(random)PubChem\n0.23 0.19 0.20\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nTanimoto coefficient\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n 9,750 14,396  8,142\n0\n10\n20\n30\n40\n50\n# of candidate structures (103)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nc\nb d e f\ng h i j\nk l m\nNatural products: chemical structures Natural products: molecular formulas\nFood-derived compounds Environmental contaminants\nTrain chemical language model\nSample from \ntrained model\n \nTabulate generated\nmolecule frequencies\n \nRank molecules\nby frequency\n \nSearch by mass of \nheld-out molecule\nString \nrepresentations\nTraining \nmolecules\nLanguage \nmodel\n100 million \nsampled strings\n# of molecules\nFrequency\nHeld-out\nmolecules\nm/z = 126.1252 Frequency\nMolecule\na\nExtended Data Fig. 7 | Language models facilitate structure elucidation of \ncomplex natural products. a, Experimental framework for structure elucidation \nof complex natural products from minimal analytical data (here, an accurate \nmass measurement) based on the sampling frequency of generated molecules. \nb, T op-1 accuracy, left, and top-k accuracy, right, of structure elucidation for \nheld-out natural products by a chemical language model trained on the LOTUS \ndatabase\n63, as compared to searching by accurate mass in the PubChem database. \nError bars and shaded areas show the mean and standard deviation across n = 10 \ncross-validation folds, respectively. c, Examples of natural product structures \ncorrectly elucidated by the language model. d, Tanimoto coefficients comparing \nchemical structures prioritized by the language model to the true held-out \nnatural product (n = 137,400), as compared to searching by accurate mass in the \nPubChem database or the LOTUS training set, or a random molecule sampled \nfrom the output of the language model without regard to sampling frequency.  \ne, As in b, but showing the accuracy of molecular formula annotation rather than \nfull structure elucidation. f, Number of candidate molecular formulas proposed \nby the language model for each held-out natural product (n = 137,400), as \ncompared to searching by accurate mass in PubChem or the LOTUS training set. \ng, As in b, but showing the accuracy of a language model trained on food-derived \ncompounds from the FooDB database. h, As in d, but showing the Tanimoto \ncoefficients of prioritized chemical structures from a language model trained on \nthe FooDB database (n = 52,880 compounds). i, As in b, but showing the accuracy \nof a language model trained on environmental compounds from the NORMAN \nsuspect list exchange. j, As in d, but showing the Tanimoto coefficients of \nprioritized chemical structures from a language model trained on the NORMAN \ndataset (n = 63,732 compounds). k, T op-1 accuracy, left, and top-k accuracy, right, \nof structure elucidation for held-out natural products by chemical language \nmodels trained on the LOTUS database, represented as SMILES or SELFIES. For \nmodels trained on SELFIES, results are shown separately for structures parsed \nwith or without valency constraints (‘unconstrained SELFIES’). l, Tanimoto \ncoefficients comparing chemical structures prioritized by the language model to \nthe true held-out natural product, for the same three language model variants as \nin k (n = 137,400 compounds). m, Number of candidate structures generated for \neach held-out natural product by chemical language models, for the same three \nlanguage model variants as in k (n = 137,400 compounds).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n4.3% 0.3%\n0\n1\n2\n3\n4\n5\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem\n0\n5\n10\n15\n20\nTop−k accuracy (%)\n1 3 10 30\nk\n59.8% 30.7%\n0\n25\n50\n75\n100\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem20\n40\n60\n80\n100\nTop−k accuracy (%)\n1 3 10 30\nk\n65.3% 49.5%\n0\n20\n40\n60\n80\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem20\n40\n60\n80\n100\nTop−k accuracy (%)\n1 3 10 30\nk\n95.9% 70.8%\n0\n25\n50\n75\n100\nTop−1 accuracy (%)\nLanguage model\nPubChem\nLanguage model\nPubChem20\n40\n60\n80\n100\nTop−k accuracy (%)\n1 3 10 30\nk\ne COCONUT: chemical structures f COCONUT: molecular formulas\nNORMAN: molecular formulasFooDB: molecular formulasg h\nMean Tc = 0.23\n≥99.1% of pairs\nDensity\n0.00 0.25 0.50 0.75 1.00\nTanimoto coefficient\na\n0\n0\n3\n3\n10\n1\n12\n12\n11\n14\n18\n23\n25\n30\n37\n7\n32\n58\n58\n57\n21\n6\nPriority\n0 50 100 150 200 250\nElser\nYoung\nFrigerio/Andujar_patroon\nHong_pred and pub\nHong_pub\nLanguage model\nNikolic_POSO\nRessom\nFrigerio/Andujar_msdial\nAlcazar\nWei_MSKU_3\nWei_MSKU_2\nWei_MSKU_1\nNikolic_KUCA\nWang\nNamini/Mosley\nTsugawa\nMehta\nXing\nDuhrkop\nNothias\nRutz\n# of attempts\n0\n5\n10\n15\n31\n7\n62\n62\n64\n9\nBonus\n0 50 100 150 200 250\nElser\nHong_pub\nMehta\nLanguage model\nWang\nHong_pred and pub\nXing\nNothias\nRutz\nDuhrkop\n# of attempts\n0\n5\n10\n15\n20\n25\n1 3 10 30\nk\nTop−k accuracy (%)\nPriority\nBonus\nb c\nd\nExtended Data Fig. 8 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 8 | Language models contribute to structure elucidation \nof unknown molecules. a, Distribution of Tanimoto coefficients between \nrandom pairs of molecules from the LOTUS database of natural products. Vertical \nline shows the mean Tanimoto coefficient comparing chemical structures \nprioritized by the language model to the true held-out natural product, greater \nthan 99.1% of pairs. b, Accuracy of a chemical language model trained in  \ncross-validation on the LOTUS natural product database versus 19 submissions \nto the CASMI 2022 ‘priority’ competition (dark grey and coloured bars, number \nof correct solutions; light grey bars, number of attempts). Arrows highlight de \nnovo structure elucidation methods. c, As in b, but showing the CASMI 2022 \n‘bonus’ competition. d, T op-k accuracy of a chemical language model trained \nin cross-validation on the LOTUS database in the CASMI ‘priority’ and ‘bonus’ \ncompetitions. e, T op-1 accuracy, left, and top-k accuracy, right, of structure \nelucidation for held-out natural products by a chemical language model trained \non the COCONUT database of natural products, as compared to searching by \naccurate mass in PubChem. Error bars and shaded areas show the standard \ndeviation across n = 10 cross-validation folds. f, As in e, but showing the accuracy \nof molecular formula annotation rather than full structure elucidation. g, As in f, \nbut showing molecular formula annotation for food-derived compounds from \nthe FooDB database. h, As in f, but showing molecular formula annotation for \nenvironmental compounds from the NORMAN suspect list exchange.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n10.8%12.6% 11.1%\n0\n4\n8\n12\n16\nTop−1 accuracy (%)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILES\nSELFIES\nUnconstrained\nSELFIES\n10\n20\n30\n40\nTop−k accuracy (%)\n1 3 10 30\nk\n15.1%17.1% 15.3%\n0\n5\n10\n15\n20\n25\nTop−1 accuracy (%)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILES\nSELFIES\nUnconstrained\nSELFIES\n20\n40\n60\nTop−k accuracy (%)\n1 3 10 30\nk\n12%12.6% 12.3%\n 1.9%4.2% 2.1%\nNone One or more\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n0\n4\n8\n12\n16\nTop−1 accuracy (%)\n15.1%17.1% 15.3%\n0\n5\n10\n15\n20\n25\nTop−1 accuracy (%)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\nSMILES\nSELFIES\nUnconstrained \nSELFIES\n20\n40\n60\nTop−k accuracy (%)\n1 3 10 30\nk\n0.72 0.69 0.69\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTanimoto coefficient\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n375 742 554\n0\n500\n1000\n1500\n2000\n# of candidate structures\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n0.26 0.24 0.24\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nTanimoto coefficient\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n1,552 2,489 1,858\n0\n2000\n4000\n6000\n# of candidate structures\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n0.23 0.21 0.19\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nTanimoto coefficient\nSMILESTrainingset\nSELFIES\n0.20 0.17 0.18\n0.0\n0.1\n0.2\n0.3\nTanimoto coefficient\nSMILESSELFIES\nUnconstr\nained\nSELFIES\n6,187 8,623 4,525\n0\n10\n20\n30\n# of candidate structures (103)\nSMILESSELFIES\nUnconstr\nained\nSELFIES\na b c d\ne f g\nh i j\nFood-derived compounds\nEnvironmental contaminants\nNatural products (COCONUT)\nRingsRingsk\nExtended Data Fig. 9 | See next page for caption.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\nExtended Data Fig. 9 | Generating and discarding invalid outputs improves \nstructure elucidation. a, Tanimoto coefficients comparing chemical structures \nprioritized by the language model to true held-out natural products (n = 137,400) \nfrom the LOTUS database, for language models trained on SMILES versus \nSELFIES, as compared to searching by accurate mass in the training set. b, T op-1 \naccuracy, left, and top-k accuracy, right, of structure elucidation for held-out \nnatural products by n = 10 chemical language models trained on the COCONUT \ndatabase, represented as SMILES or SELFIES. For models trained on SELFIES, \nresults are shown separately for structures parsed with or without valency \nconstraints (‘unconstrained SELFIES’). c, Tanimoto coefficients comparing \nchemical structures prioritized by the language model to the true held-out \nnatural product (n = 405,947), for the same three language model variants shown \nin b. d, Number of candidate structures generated for each held-out natural \nproduct (n = 405,947) by chemical language models, for the same three language \nmodel variants as in b. e-g, As in b-d, but for food-derived compounds from the \nNORMAN suspect list exchange. h-j, As in b-d, but for environmental compounds \nfrom the FooDB database. k, T op-1 accuracy of structure elucidation for held-out \nnatural products by n = 10 chemical language models trained on the LOTUS \ndatabase, represented as SMILES or SELFIES, shown separately for acyclic natural \nproducts versus natural products containing at least one ring system (p = 0.016, \ntwo-way ANOVA).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00821-x\n1.3 h 2.9 h 3.4 h\n1\n2\n3\n4\nTime, hours\nSMILESTexas\nSELFIES\nUnconstr\nained\nSELFIES\na b\n3.1 h 3.7 h\n3.0\n3.5\n4.0\n4.5\nTime, hours\nSMILES SELFIES\nc\n3.6 h 4.1 h\n3.5\n4.0\n4.5\nTime, hours\nSMILES SELFIES\nd\n4.0 h 4.1 h\n3.6\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8\nTime, hours\nSMILES SELFIES\ne\nExtended Data Fig. 10 | Discarding invalid SMILES is fast and easy. a, Time  \n(in hours) required to parse n = 10 samples of 10 million SMILES or SELFIES \nstrings and discard invalid outputs. For SELFIES, results are shown only when \nparsing SELFIES with valency constraints that allow for invalid output generation.  \nb, Python code snippet demonstrating the ease with which it can be determined \nif a SMILES string is invalid. c, Time (in hours) required to train n = 10 chemical \nlanguage models on samples of 100,000 molecules from the ChEMBL database, \nrepresented as SMILES versus SELFIES. Training was performed on Dell EMC \nC4140 GPU compute nodes equipped with NVIDIA T esla V100 GPUs. Language \nmodels trained on SELFIES required an average of 0.6 h longer to train, relative \nto models trained on SMILES representations of the same molecules. d, As in c, \nbut showing the total time (in hours) required to train chemical language models, \nsample 500,000 outputs from the trained models, and discard invalid outputs. \ne, As in c, but showing the total time (in hours) required to train chemical \nlanguage models, sample 4 million outputs from the trained models, and discard \ninvalid outputs. Filtering invalid SMILES would not increase computational \nrequirements, relative to a model trained on SELFIES, until approximately  \n4 million molecules.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.3813023567199707
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3777424097061157
    },
    {
      "name": "Intensive care medicine",
      "score": 0.3474549651145935
    },
    {
      "name": "Medicine",
      "score": 0.33515065908432007
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}