{
  "title": "Automatic Evaluation of Attribution by Large Language Models",
  "url": "https://openalex.org/W4389518925",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2235717150",
      "name": "Xiang Yue",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2111695320",
      "name": "Boshi Wang",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2335811381",
      "name": "Ziru Chen",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A1988944048",
      "name": "Kai Zhang",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2107570501",
      "name": "Yu Su",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2105996224",
      "name": "Huan Sun",
      "affiliations": [
        "The Ohio State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W3106234277",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3201233724",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3153046263",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4206850841",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4366327559",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2970716846",
    "https://openalex.org/W2988092105",
    "https://openalex.org/W4377865953",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4226157795",
    "https://openalex.org/W2909737760",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4364384032",
    "https://openalex.org/W3101757358",
    "https://openalex.org/W3035214886",
    "https://openalex.org/W4377164430",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4353007635",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4287121227",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W1801866228",
    "https://openalex.org/W3153712677",
    "https://openalex.org/W4287855110",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4323076548",
    "https://openalex.org/W4311731003",
    "https://openalex.org/W2995628494",
    "https://openalex.org/W1840435438"
  ],
  "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4615–4635\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAutomatic Evaluation of Attribution by Large Language Models\nXiang Yue Boshi Wang Ziru Chen Kai Zhang Yu Su Huan Sun\nThe Ohio State University\n{yue.149,wang.13930,chen.8336,zhang.13253,su.809,sun.397}@osu.edu\nAbstract\nA recent focus of large language model (LLM)\ndevelopment, as exemplified by generative\nsearch engines, is to incorporate external refer-\nences to generate and support its claims. How-\never, evaluating the attribution, i.e., verifying\nwhether the generated statement is fully sup-\nported by the cited reference, remains an open\nproblem. Although human evaluation is com-\nmon practice, it is costly and time-consuming.\nIn this paper, we investigate automatic evalu-\nation of attribution given by LLMs. We be-\ngin by defining different types of attribution\nerrors, and then explore two approaches for\nautomatic evaluation: prompting LLMs and\nfine-tuning smaller LMs. The fine-tuning data\nis repurposed from related tasks such as ques-\ntion answering, fact-checking, natural language\ninference, and summarization. We manually cu-\nrate a set of test examples covering 12 domains\nfrom a generative search engine, New Bing.\nOur results on this curated test set and simu-\nlated examples from existing benchmarks high-\nlight both promising signals and challenges.\nWe hope our problem formulation, testbeds,\nand findings will help lay the foundation for\nfuture studies on this important problem.1\n1 Introduction\nGenerative large language models (LLMs) (Brown\net al., 2020; Ouyang et al., 2022; Chowdhery et al.,\n2022; OpenAI, 2023a,b, inter alia) often struggle\nwith producing factually accurate statements, re-\nsulting in hallucinations (Ji et al., 2023). Recent\nefforts aim to alleviate this issue by augmenting\nLLMs with external tools (Schick et al., 2023) such\nas retrievers (Shuster et al., 2021; Borgeaud et al.,\n2022) and search engines (Nakano et al., 2021;\nThoppilan et al., 2022; Shuster et al., 2022).\nIncorporating external references for generation\ninherently implies that the generated statement is\n1Our code and dataset are available at: https://github.\ncom/OSU-NLP-Group/AttrScore\nbacked by these references. However, the valid-\nity of such attribution, i.e., whether the generated\nstatement is fully supported by the cited reference,\nremains questionable. 2 According to Liu et al.\n(2023), only 52% of the statements generated by\nstate-of-the-art generative search engines such as\nNew Bing and PerplexityAI are fully supported by\ntheir respective cited references.3\nInaccurate attribution compromises the trustwor-\nthiness of LLMs, introducing significant safety\nrisks and potential harm. For instance, in health-\ncare, an LLM might attribute incorrect medical ad-\nvice to a credible source, potentially leading users\nto make harmful health decisions. Similarly, in\nfinance, faulty investment advice attributed to a re-\nliable source may cause substantial financial losses.\nTo identify attribution errors, existing attributed\nLLMs (Nakano et al., 2021; Thoppilan et al., 2022)\nrely heavily on human evaluation, which is both\nexpensive and time-consuming. For instance, the\naverage cost of annotating a single (query, answer,\nreference) example is about $1 in Liu et al. (2023).\nIn the actual use of attributed LLMs, it is the user\nwho needs to be wary of the attribution and manu-\nally verify it, which puts a tremendous burden on\ntheir side. Therefore, effective and reliable meth-\nods to automatically evaluate attribution and iden-\ntify potential attribution errors are highly desired.\nTowards this goal, we take the first step by intro-\nducing AttrScore (Figure 1), a framework designed\nfor automatic evaluation of attribution and identi-\nfication of specific types of attribution errors. We\npropose a new problem formulation that catego-\nrizes attribution into three types: 1) attributable:\nthe reference fully supports the generated state-\nment; 2) extrapolatory: the reference lacks suffi-\ncient information to support the generated state-\n2Attribution primarily refers to “the act of attributing some-\nthing” in this paper, which is similar to “verifiability” as de-\nfined in Liu et al. (2023).\n3www.bing.com/new, www.perplexity.ai\n4615\nQ: What is the temperature range on\nthe moon?\nA: The average temperature on the\nmoon can range from -298 degrees F\n(-183 degrees C) at night to 224 degrees\nF (106 degrees C) during the day [1].\nThe temperature on the Moon\nThe average temperature on the Moon (at\nthe equator and mid latitudes) varies from\n-298 degrees Fahrenheit (-183 degrees\nCelsius), at night, to 224 degrees\nFahrenheit (106 degrees Celsius) during\nthe day. Because the Moon has no...\nReferences\n[1] coolcosmos.ipac.caltech.edu\nQ: What is the highest price of a gallon\nof regular gas in the US in 2022?\nA: According to a news article by NPR,\nas of June 2022, the highest average\nprice of a gallon of regular gas in the\nUS was $6.34 in California [1].\nIn 10 states, a gallon of gas nowcosts more than $5\n...U.S. gas prices hit a new record on\nMonday, with a gallon of regular gas costing\nan average of $4.865 (not adjusting for\ninflation), according to the most recent data\nfrom American Automobile Association...\nReferences\n[1] npr.org/states-highest-gas-prices\nQ: What was the unemployment\nrate in Germany in 2020?\nA: According to [1], the unemployment\nrate in Germany for 2020 was 4.31%,\nwhich was a 1.17% increase from 2019.\nIt's important to note that the COVID...\nGermany Unemployment Rate\n1991-2023\nUnemployment refers to the share of the\nlabor force that is without work but\navailable for and seeking...Germany\nunemployment rate for 2020 was 3.81%,\na 0.67% increase from 2019.\nReferences\n[1] macrotrends.net/unemployment-rate\nAs an Attribution Validator, your task is to\nverify whether a given a reference can support\nthe answer to a provided question. A contradictory\nerror means the answer contradicts the fact in the\nattribution, while an extrapolatory error means that\nthere is not enough information in the attribution..\nAttrScore\nAttributable\n Extrapolatory Contradictory\n(1) Prompt LLMs with a clear\nevaluation instruction\n (2) Fine-tune LMs on a set of\ndiverse repurposed datasets \nQA Fact-Checking\nNLI Summarization\nFigure 1: We make the first step towards automatically evaluating attribution and identifying specific types of\nerrors with AttrScore. We explore two approaches in AttrScore: (1) prompting LLMs, and (2) fine-tuning LMs on\nsimulated and repurposed datasets from related tasks.\nment, and 3) contradictory: the generated state-\nment directly contradicts the cited reference. Un-\nlike existing work (Bohnet et al., 2022) that uses\nbinary categorization (i.e., attributable or not) and\nLiu et al. (2023) that defines the degree of refer-\nence support for the generated statement as “full”,\n“partial”, or “no support”, our fine-grained error\ncategorization aids humans in better understanding\nthe type of an attribution error made by an LLM.\nThis not only enhances safe system usage but also\nprovides valuable insights for future development\nof mechanisms tailored to correct specific errors.\nWe explore two approaches in AttrScore: 1)\nprompting LLMs and 2) fine-tuning LMs on simu-\nlated and repurposed data from related tasks such\nas question answering (QA), fact-checking, natural\nlanguage inference (NLI), and summarization. For\nevaluation, unlike existing work (Liu et al., 2023;\nGao et al., 2023) that only uses queries from exist-\ning benchmarks, we curate a set of test examples\ncovering 12 different domains from a generative\nsearch engine, New Bing. This is the first eval-\nuation set for measuring the attribution of LLMs\nwith queries created based on real-life interactions,\nhence avoiding the data contamination issue.\nOur results indicate that both approaches show\nreasonable performance on our curated and simu-\nlated test sets; yet there is still substantial room for\nfurther improvement. Major sources of evaluation\nfailures include insensitivity to fine-grained infor-\nmation comparisons, such as overlooking contex-\ntual cues in the reference, disregard for numerical\nvalues, and failure in performing symbolic opera-\ntions. In light of these findings, we discuss poten-\ntial directions for improving AttrScore, including\ntraining models to be more strongly conditioned on\nthe reference, and augmenting them with external\ntools for numerical and logical operations.\nWith the new formulation of attribution errors,\nthe development of AttrScore, the introduction of\nnew test sets, and the insights into challenges and\npotential directions for future work, we hope our\nwork can help lay the foundation for the important\ntask of automatically evaluating LLM attributions.\n2 Problem Formulation\nThe primary task in this paper is to evaluate attribu-\ntion, which involves verifying whether a reference\n4616\nprovides sufficient support for a generated answer\nto a user’s query. Our task setting prioritizes one\nreference per statement, a unit task that more com-\nplex scenarios can be decomposed to. We study\nsuch a setting as it forms the basis for dealing with\nmultiple references or distinct segments (Liu et al.,\n2023; Gao et al., 2023).\nPrior work, such as Rashkin et al. (2021); Gao\net al. (2022); Bohnet et al. (2022), mainly focuses\non binary verification, i.e., determining if a refer-\nence supports the generated answer or not. We\npropose advancing this task by introducing a more\nfine-grained categorization. Specifically, we clas-\nsify attributions into three distinct categories:4\n• Attributable: The reference fully supports the\ngenerated answer.\n• Extrapolatory: The reference lacks sufficient\ninformation to validate the generated answer.\n• Contradictory: The generated answer contra-\ndicts the information presented in the reference.\nTo illustrate, consider a contradictory example\n(Figure 1). The query is “What was the unemploy-\nment rate in Germany in 2020?”, and the generated\nanswer is “4.31%”. However, the reference states\nthat the rate was “3.81%”, contradicting the gen-\nerated answer. An extrapolatory instance, on the\nother hand, would be a query about the “gas price\nin California”. While the reference is relevant, it\ndoes not contain specific information to verify the\ncorrectness of the generated answer.\nFollowing these examples, we see the impor-\ntance of granularity in error classification. A fine-\ngrained classification allows us to pinpoint the na-\nture of the errors, be it contradiction or extrapo-\nlation. Users can better understand the type of\nerrors an LLM might make, enabling them to use\nthe model more safely. Additionally, such an er-\nror identification system can guide future training\nprocesses of attributed LLMs, leading to specific\nmechanisms’ development to correct such errors.\nOur categorization also offers a departure from\nthe existing approach (Liu et al., 2023), which em-\nphasizes on degree of support (“full”, “partial”,\nor “none”) rather than attribution error types. Our\napproach highlights specific issues in attribution\n4We acknowledge that while these categories are generally\nmutually exclusive, complex scenarios might blur the bound-\naries between them. However, such cases are very rare. For\nthe purpose of this study, we maintain their exclusivity to\nenable clear and focused error analysis.\nevaluation for more effective error management\nand system improvement.\nFormally, the task of attribution evaluation in-\nvolves a natural language query q, a generated an-\nswer a, and a reference x from an attributed LLM.\nThe goal is to develop a function, denoted asf, that\ninputs (q, a, x) and outputs a class label indicating\nwhether “according to x, the answer a to the query\nq is attributable, extrapolatory or contradictory.”5\n3 Automatic Evaluation of Attribution\nFollowing our problem definition, we introduce\ntwo approaches for automatic evaluation of attri-\nbution: prompting LLMs and fine-tuning LMs on\nsimulated and repurposed data from related tasks.\n3.1 Prompting LLMs\nRecent research (Fu et al., 2023) has demonstrated\nthe possibility of prompting LLMs to evaluate the\nquality of generated text using their emergent ca-\npabilities (Wei et al., 2022b), such as zero-shot in-\nstruction (Wei et al., 2022a) and in-context learning\n(Brown et al., 2020). Following this approach, we\nprompt LLMs, such as ChatGPT (OpenAI, 2023a),\nusing a clear instruction that includes definitions of\nthe two types of errors (as shown in Figure 1) and\nan input triple of the query, answer, and reference\nfor evaluation. The complete prompt used in our\nstudy can be found in Appendix Table 6.\n3.2 Fine-tuning LMs on Repurposed Data\nThe primary challenge in fine-tuning LMs for auto-\nmatic attribution evaluation is the lack of training\ndata. One potential approach is to hire annotators to\ncollect real samples, but the cost can be prohibitive.\nHere, we first repurpose datasets from three re-\nlated tasks (fact-checking, NLI, and summariza-\ntion). We then propose to further simulate more\nrealistic samples from existing QA benchmarks.\nRepurpose data from fact-checking, NLI, and\nsummarization tasks.Given the connections be-\ntween our attribution evaluation task and the tasks\nof fact-checking, NLI, and summarization, we pro-\npose to utilize datasets from these fields to enrich\nour training examples. Fact-checking data and NLI\ndata, with their emphasis on assessing the consis-\ntency and logical relationship between claims (hy-\npothesis) and evidence (premise), mirrors our task’s\n5It is important to note that this evaluation focuses on the\n“verifiability” of the answer based on the reference. It does not\nmeasure the “relevance”, i.e., whether the answer correctly\nresponds to the query (Liu et al., 2023).\n4617\nQuery: Which apostle had a\nthorn in his side?\nLong Ans: Paul was an apostle\nwho had a thorn in his side [1].\nThorn in the flesh\nThorn in the flesh is a phrase of\nNew Testament origin used to\ndescribe a chronic infirmity,\nannoyance, or trouble in one's\nlife, drawn from Paul the\nApostle's use of the phrase in\nhis Second Epistle to the\nCorinthians 12 : 7 -- 9\nReferences\n[1] en.wikipedia.org/wiki/\nThorn_in_the_flesh\nThorn in the flesh\nThorn in the flesh is a phrase of\nNew Testament origin used to\ndescribe a chronic infirmity,\nannoyance, or trouble in one's\nlife, drawn from Paul the\nApostle's use of the phrase in\nhis Second Epistle to the\nCorinthians 12 : 7 -- 9\nReferences\n[1] en.wikipedia.org/wiki/\nThorn_in_the_flesh\nThorn in the flesh\nThorn in the flesh is a phrase of\nNew Testament origin used to\ndescribe a chronic infirmity,\nannoyance, or trouble in one's\nlife, drawn from John the\nApostle's use of the phrase in\nhis Second Epistle to the\nCorinthians 12 : 7 -- 9\nReferences\n[1] en.wikipedia.org/wiki/\nThorn_in_the_flesh\nThorn (letter)\nThorn or þorn (Þ, þ) is a letter\nin the Old English, Old Norse,\nOld Swedish and modern\nIcelandic alphabets, as well as\nmodern transliterations of the\nGothic alphabet, Middle Scots,\nand some dialects of Middle\nEnglish. It was also used ...\nReferences\n[1] https://en.wikipedia.org/\nwiki/Thorn_(letter)\n(A): Attributable (B): Contradictory (C): Contradictory (D): Extrapolatory\nQuery: Which apostle had a\nthorn in his side?\nLong Ans: Phillip had a thorn\nin his side [1].\nQuery: Which apostle had a\nthorn in his side?\nLong Ans: Paul was an apostle\nwho had a thorn in his side [1].\nQuery: Which apostle had a\nthorn in his side?\nLong Ans: The apostle who had\na thorn in his side is Paul [1].\nShort Ans: Paul [1] Short Ans: Phillip [1] Short Ans: Paul [1] Short Ans: Paul [1]\nFigure 2: Examples simulated from open-domain QA. We 1) use the original (question, answer, context) pair as an\nattributable instance (A), 2) substitute the answer or the answer span in the context to simulate a contradictory error\nexample (B, C), and 3) replace the context with alternatives to simulate an extrapolatory error example (D). In order\nfor models trained the simulated data to generalize well to the long answer setting in real-life search engines like\nNew Bing, we convert the short answer to a long one (using ChatGPT).\nobjective of checking the supporting relationship\nbetween reference documents and generated state-\nments. Summarization datasets, especially those\ninvolving the detection of hallucinations (including\nboth intrinsic and extrinsic (Maynez et al., 2020),\ncould provide a useful starting point for identify-\ning attribution inconsistencies. Nevertheless, these\ndatasets would require suitable adaptation. We\nkeep their original data sequences and modify their\ndata label space to suit the specific needs of the\nattribution evaluation definition. Additional infor-\nmation on this can be found in Appendix A.\nSimulate data from open-domain QA. QA bench-\nmarks provide an ideal platform for data simula-\ntion, as they comprise questions, their correspond-\ning ground truth answers, and reference contexts.\nThese elements can be directly employed as at-\ntributable examples (Figure 2, A). In open-domain\nQA datasets, answers are typically brief text spans.\nTo cater to the long answer setting in most at-\ntributed LLMs, we convert these short answers\ninto longer sentences using ChatGPT. For simulat-\ning contradictory errors, we propose two methods:\n(1) The first involves modifying the correct answer\nwith an alternative candidate from an off-the-shelf\nQA model, an answer substitution model, or a ran-\ndom span generator (Figure 2, B). (2) The second\nretains the original answer but replaces the answer\nspan in the reference context with a comparable\ncandidate (Figure 2, C). To emulate extrapolatory\nerrors, we employ a BM25 retriever on the ques-\ntion, retrieving relevant external documents from\nresources such as Wikipedia, which do not contain\nthe ground truth answers (Figure 2, D). More de-\ntails regarding the simulation of these errors from\nQA datasets can be found in Appendix A.\n4 Experimental Setup\n4.1 Datasets\nThis section presents the datasets utilized for train-\ning and testing methods for automatic attribu-\ntion evaluation. In particular, we develop two\nevaluation sets, AttrEval-Simulation and AttrEval-\nGenSearch, derived from existing QA datasets\nand a generative search engine, respectively. The\ndataset statistics are presented in Table 1.\nTraining data. To repurpose and simulate train-\ning examples, we follow the method in Section\n3.2 based on four similar tasks’ datasets. For\nQA, we consider NaturalQuestions (Kwiatkowski\net al., 2019). For fact-checking, we include FEVER\n(Thorne et al., 2018), Adversarial FEVER (Thorne\net al., 2019), FEVEROUS (Aly et al., 2021), VI-\nTAMINC (Schuster et al., 2021), MultiFC (Augen-\nstein et al., 2019), PubHealth (Kotonya and Toni,\n2020), and SciFact (Wadden et al., 2020). For NLI,\nwe include SNLI (Bowman et al., 2015), MultiNLI\n(Williams et al., 2018), ANLI (Nie et al., 2020)\nand SciTail (Khot et al., 2018). For summarization,\nwe include XSum-Halluc. (Maynez et al., 2020),\nXENT (Cao et al., 2022), and FactCC (Kryscinski\n4618\nSplit Related Tasks Data Sources #Samples\nTrain\nQA NaturalQuestions 20K\nFact-checking\nFEVER, VITAMINC,\nAdversarial FEVER,\nFEVEROUS, SciFact\nPubHealth, MultiFC\n20K\nNLI SNLI, MultiNLI\nANLI, SciTail 20K\nSummarization XSum-Hallucinations,\nXENT, FactCC 3.8K\nTest\nQA\nPopQA, EntityQuestions,\nHotpotQA, TriviaQA,\nWebQuestions, TREC\n4K\n- Annotated samples from a\ngenerative search engine 242\nTable 1: Statistics of the training and test datasets for\nattribution evaluation. We include the distributions of\nthe labels and data sources in Appendix B.\net al., 2020). We use all examples in the summariza-\ntion task datasets, and sample 20K examples from\nQA, fact-checking, and NLI task datasets. We com-\nbine all the simulated datasets to create the training\nset for our main experiment.\nAttrEval-Simulation. For testing, we first simu-\nlate examples from six out-of-domain QA datasets:\nHotpotQA (Yang et al., 2018), EntityQuestions\n(Sciavolino et al., 2021), PopQA (Mallen et al.,\n2022), TREC (Baudis and Sedivý, 2015), Trivi-\naQA (Joshi et al., 2017), and WebQuestions (Be-\nrant et al., 2013). Note that we intend to use dif-\nferent QA datasets for training and testing, as to\ntest the model’s generalization ability, and evalu-\nate its performance across a diverse set of domains\nand question formats. Our manual examination\nindicates that 84% of 50 randomly sampled exam-\nples accurately align with their category, and the\nlabeling errors are primarily due to incorrect an-\nnotations in the original QA datasets or heuristics\nused to formulate comparable answer candidates\nfor contradictory errors and to retrieve negative\npassages for extrapolatory errors.\nAttrEval-GenSearch. To examine the real-life ap-\nplication of automatic attribution evaluation, ap-\nproximately 250 examples from the New Bing\nsearch engine are annotated carefully by the au-\nthors. This process comprises two subtasks: creat-\ning queries and verifying attributions. To avoid the\nissue of training data contamination, new queries\nare manually created across 12 domains (Figure\n3).6 To facilitate and motivate query annotation,\n6The “AI/NLP Research” domain is inspired by recent\ndiscussions on social media about testing LLMs’ knowledge\non researchers, e.g., “Is XX a co-author of the paper XX?”\nDaily Life\n7.1%\nPet & Animal\n5.5%\nMed & Health\n6.7%\nHistory\n8.2%\nLaw\n5.1%\nScience\n6.7%\nEcon. & Fin.\n18.4%\nAI/NLP Research\n24.3%\nBusiness\n5.9%\nGeography\n2.7%\nMath\n1.2%\nGame&Anime\n8.2%\nFigure 3: Domain distribution of our annotated AttrEval-\nGenSearch test set (covering 12 domains in total).\nkeywords from a specific domain are randomly\ngenerated using ChatGPT, and relevant facts within\nthat domain are compiled from the Web.7\nIn the verification process, queries are sent to the\nNew Bing search engine under a balanced mode fol-\nlowing Liu et al. (2023), which balances accuracy\nand creativity. The validity of the output generated\nby New Bing is evaluated, where we consider only\nthe first sentence that answers the question along\nwith its reference. As we state in Section 2, our\nevaluation emphasizes the error type in a single\nreference per statement. In the case of a sentence\nhaving multiple references or distinct segments (for\nexample, “XXX [1][2]” or “XXX [1] and YYY\n[2]”), each reference or segment is treated as a\nseparate sample, and the attributions are verified\nindividually. Finally, the samples are categorized\nby the annotators as attributable, contradictory, or\nextrapolatory. Detailed annotation guidelines can\nbe found in Appendix D.\n4.2 Implementation Details\nIn the configuration of “prompting LLMs”, we\ntest Alpaca (Taori et al., 2023), Vicuna (Chiang\net al., 2023), ChatGPT (OpenAI, 2023a) and GPT-\n4 (OpenAI, 2023b), where we use OpenAI’s offi-\ncial APIs ( gpt-3.5-turbo, gpt-4-0314 )8, and\nweights from Alpaca and Vicuna from the official\nrepository9. For Alpaca and Vicuna inference, doc-\numents are tokenized and truncated at a maximum\nof 2048 tokens. We generate text with a temper-\nature of 0. The prompts for the task of evaluat-\ning attribution are provided in Appendix Table 6,\n7We make an effort to collect new facts post-2021 to test\nabout “knowledge confliction” (Zhou et al., 2023; Xie et al.,\n2023) between parametric and external knowledge.\n8platform.openai.com/docs/api-reference/chat.\nGiven GPT-4’s high cost and slow inference speed, we\nevaluate it on 500 random samples from AttrEval-Simulation.\n9https://github.com/tatsu-lab/stanford_alpaca,\nhttps://github.com/lm-sys/FastChat\n4619\nSetting Model (Size) AttrEval-Simulation AttrEval-GenSearch\nAttri. Contra. Extra. Overall Attr. Contra. Extra. Overall\nZero-shot\nAlpaca (7B) 50.0 4.0 1.4 33.6 50.7 8.6 3.6 34.3\nAlpaca (13B) 48.3 5.6 2.2 33.5 50.6 6.1 19.3 34.7\nVicuna (13B) 46.3 8.3 21.6 34.6 54.4 13.3 26.1 41.4\nChatGPT 45.7 17.9 52.7 43.2 61.2 20.6 53.3 55.0\nGPT-4 58.7 23.2 61.5 55.6 87.3 45.0 89.6 85.1\nFew-shot\nAlpaca (7B) 45.4 8.2 9.6 31.9 49.6 5.2 13.5 37.2\nAlpaca (13B) 38.9 20.1 2.2 33.1 50.5 10.3 5.6 34.8\nVicuna (13B) 35.4 37.2 0.3 32.6 50.6 9.1 8.4 34.1\nChatGPT 46.6 27.6 35.8 39.2 62.6 26.8 49.5 53.3\nGPT-4 61.1 31.3 68.8 60.0 85.2 53.3 88.9 84.3\nFine-tuned\nRoberta (330M) 62.5 54.6 74.7 65.0 47.2 25.2 62.3 49.8\nGPT2 (1.5B) 63.6 54.6 71.9 63.5 51.1 18.6 60.7 47.4\nT5 (770M) 45.9 57.1 71.6 59.1 58.5 24.3 72.5 61.6\nFlan-T5 (770M) 57.3 50.1 70.5 59.3 64.3 27.6 72.9 64.5\nFlan-T5 (3B) 48.1 48.7 67.1 55.7 77.7 44.4 80.0 75.2\nFlan-T5 (11B) 48.4 49.9 66.5 55.4 81.6 38.9 76.9 72.7\nLLaMA (7B) 62.2 50.7 74.6 62.8 77.9 41.1 78.3 72.5\nAlpaca (7B) 66.8 41.1 76.8 64.5 73.0 30.2 80.0 72.5\nAlpaca (13B) 63.6 48.9 75.8 63.6 77.5 34.5 79.4 73.3\nVicuna (13B) 66.2 49.1 78.6 66.0 69.4 37.7 79.9 72.1\nTable 2: The performance (F1 score) of AttrScore with different models on AttrEval-Simulation and AttrEval-\nGenSearch sets. The best-performing result in each setting is in bold. The results show both promising signals and\nchallenges (e.g., all models struggle with contradictory errors) in automatic evaluation of attribution.\nand our main results are averaged over 4 different\nprompts. For the few-shot prompting setting, we\nmanually write 3 examples as demonstrations for\nboth test sets as shown in Table 7. If LLMs yield\nan attribution label with an explanation, we extract\nthe predicted label with regular expression.\nIn the “fine-tuning LMs” setting, we fine-tune\nfour types of LMs of various scales: Roberta\n(340M) (Liu et al., 2019), (FLAN-)T5 (770M, 3B,\n11B) (Raffel et al., 2020; Chung et al., 2022), GPT2\n(1.5B) (Radford et al., 2019), LLaMA (7B), Alpaca\n(7B, 11B) (Taori et al., 2023), and Vicuna (7B, 11B)\n(Chiang et al., 2023). Our implementation utilizes\nthe Huggingface library (Wolf et al., 2019) and\nAlpaca examples. The training is performed on 4\nA100 80GB GPUs with a maximum of 512 tokens.\nFor the LLaMA family of models, we use a batch\nsize of 32 and train for 1 epoch. For the other mod-\nels, we use a batch size of 64 and 3 epochs. We set\nthe learning rate as 2e-5 and use a cosine learning\nrate decay with 0.03 warm-up steps.\nMetrics. For evaluation, we present the F1 score\nfor each individual class as well as the micro-F1\nscore, which is equivalent to the overall accuracy.\n5 Results\n5.1 Overall Performance\nTable 2 presents an evaluation of different models\non both the simulated dataset (AttrEval-Simulation)\nand the annotated dataset on New Bing (AttrEval-\nGenSearch). Our primary findings are as follows:\nGPT-4 achieves promising results,reaching an\noverall accuracy of 81-83% on AttrEval-GenSearch\nand significantly outperforming other models. This\nsuggests a promising potential for employing GPT-\n4 for automatic attribution evaluation to alleviate\nhuman annotation workloads, aligning with the\nemerging trend that uses GPT-4 for different evalu-\nation tasks (Chiang et al., 2023; Zheng et al., 2023).\nHowever, it may still not be sufficiently accurate for\npractical use. We also note some potential concerns\nof bias (see Limitations Section 8).\nAutomatic attribution evaluation presents sub-\nstantial challenges. This complex task requires\nnot only understanding the reference information\nbut also comparing it with the information in the\nstatement, all of which can significantly vary across\ndifferent datasets and test conditions. Against these\nchallenges, models other than GPT-4 exhibit sub-\noptimal performance in zero-shot and few-shot set-\ntings. Fine-tuning LMs on the simulated datasets\nfrom related tasks significantly improves the per-\n4620\nError Type\n(percentage)\nExample\nFine-grained in-\nformation insen-\nsitivity\n(30.6%)\nQuery:Was ketchup a medicine before?\nAnswer:: In the early 1800s , it was believed that ketchup could be used to treat indigestion, jaundice, and diarrhea\nReference:According to John Cook Bennett, tomatoes could be used to treat diarrhea, ... Soon, Bennett was publishing tomato ketchup\nrecipes in pill form ... According to Dr. Bennett, this tomato blend can help people with diarrhea, indigestion, rheumatism, and jaundice.\n▷Ground Truth:Extrapolatory\n▷GPT-4:Attributable.The reference supports the claim that in the early 1800s, ketchup was believed to be used as a medicine to treat\nindigestion, jaundice, and diarrhea. It mentions John Cook Bennett’s belief in the medicinal properties of tomatoes and his publication of\ntomato ketchup recipes in pill form, which were sold as patent medicines in supermarkets and pharmacies.\nMisunderstanding\nthe task defini-\ntion and logical\nrelation implied\nby labels\n(22.2%)\nQuery:When GTA6 will be published?\nAnswer:Rockstar Games hasnot officially announced the release date for GTA 6 yet.\nReference:The most likely GTA 6 release date is holiday 2024, and although there is no official confirmation, this estimate is confirmed\nby Microsoft’s stating that it “is expected” in 2024 in a legal filing and assorted insider leaks.\n▷Ground Truth:Attributable\n▷GPT-4:Extrapolatory. The reference provides an estimated release date for GTA 6 based on a legal filing from Microsoft and insider\nleaks, butit does not confirm an official announcement from Rockstar Games regarding the release date.\nFailing on sym-\nbolic operators\n(13.9%)\nQuery:What’s the average salary for a software engineer working at Amazon in 2022?\nAnswer:According to Indeed.com, the average salary for a software engineer working at Amazon in the United States is$131,930per\nyear.\nReference:Average salary$132,147Salary estimated from 3,612 employees, users, and past and present job advertisements on Indeed\nin the past 12 months. Last updated: April 18, 2023.\n▷Ground Truth:Contradictory\n▷GPT-4:Attributable. The reference does provide an average salary of $132,147 for a software engineer working at Amazon, which is\nvery close to the claim’s figure of $131,930.\nTable 3: Qualitative analysis on misclassified examples from prompting GPT-4 in AttrScore.\nformance. For instance, the Vicuna (13B) model\nsees the overall accuracy on the two test sets rise\nfrom 34.6% and 41.4% in the zero-shot setting to\n66.0% and 71.3%, respectively. And the fine-tuned\nFLAN-T5 (770M) model can even surpass Chat-\nGPT on both test sets. Despite this, there is still a\nlarge room for further improvement. Some models\nthat yielded better results on the simulated test set\nmay be less effective on the annotated test set, indi-\ncating a lack of consistency across diverse testing\nsettings, signaling generalizability challenges.\nModels struggle most notably with contradic-\ntory errors. Detecting contradictions is partic-\nularly complex because it requires the model to\nweigh one piece of information in the statement\nagainst another in the reference, a process that ne-\ncessitates advanced fine-grained information com-\nparison and reasoning capabilities. Consequently,\neven the best-performing model GPT-4 and the fine-\ntuned models often fail when faced with contradic-\ntory inputs, most often treating them as attributable\n(see qualitative analysis in Section 5.2).\n5.2 Qualitative Analysis\nTo shed light on the space for future improvements\nin attribution evaluation, we qualitatively examine\nall the error examples of GPT-4 in the zero-shot\nsetting. Representative examples are in Table 3.\nOur first observation is that a significant portion\n(30.6%) of errors happen due to fine-grained in-\nformation insensitivity: failure in comparing very\nfine-grained information such as numerical values,\nnumbers, dates, and time. Besides, the model mis-\nunderstands task definition and misinterprets logi-\ncal relations implied by labels (22.2%). The model\nalso struggles with symbolic operators (13.9%).\nFor example, it fails to distinguish ‘equal to’ ( =)\nand ‘approximately equal to’ (≈) in numeric com-\nparisons. In the left cases, the model tends to over-\nlook the context clues and does not make judgments\nby conditioning on the reference (e.g., potentially\nrelying on its own parametric knowledge).\nOur observations point to two potential direc-\ntions for improvement: 1) training or prompting\nmodels to be more faithful and strongly conditioned\non the reference (Zhou et al., 2023), especially pay-\ning attention to fine-grained information; and 2)\naugmenting an LM-based evaluation method with\nexternal tools for different types of numerical and\nlogical operations that are hard to be accurately\nperformed only by the LM itself (Chen et al., 2020;\nMialon et al., 2023). Similarly, we do qualitative\nanalysis for ChatGPT in Appendix Section E.\n5.3 Ablation Study\nIn this section, we perform an ablation study to\ntest how each task influences the fine-tuned LMs’\nresults and analyze the prompt sensitivity in zero-\nshot and few-shot settings for prompting LLMs.\nContribution of individual task.We show the per-\nformance of models fine-tuned on individual task\ndatasets and their combinations in Figure 4. We se-\nlect a representative from each group of the models\nunder the fine-tuned setting in Table 2. Our findings\n4621\nRoberta\n(330M)\nFlanT5\n(770M)\nGPT2\n(1.5B)\nAlpaca\n(7B)\n20\n30\n40\n50\n60\n70\n80Overall Micro F1\nAttrEval - Simulation\nRoberta\n(330M)\nFlanT5\n(770M)\nGPT2\n(1.5B)\nAlpaca\n(7B)\n20\n30\n40\n50\n60\n70\n80\nAttrEval - GenSearch\nQA NLI Fact. Sum. Combined\nFigure 4: The influence of individual task data. Com-\nbining datasets generally improves model performance.\nsuggest that examples from our simulated QA and\nfact-checking task most significantly improve per-\nformance for the attribution evaluation task, hinting\nat a strong link between these tasks. Furthermore,\nintegrating various related task datasets generally\nleads to better performance, particularly on out-of-\ndomain test instances in AttrEval-GenSearch.\nSensitivity of prompts.The choice of prompts\nused to evaluate language models can have an im-\npact on their performance. We evaluate the sensitiv-\nity of prompts for AttrScore under both zero-shot\nand few-shot settings of Alpaca (7B) and ChatGPT.\nWe show four types of prompts as mentioned ear-\nlier: a prompt designed specifically for our evalua-\ntion setting (Attri.), an NLI prompt, a fact-checking\nprompt (Fact.), and a summarization hallucination\ndetection prompt (Sum.). These prompts are pre-\nsented in Appendix Table 6. As shown in Table 4,\nfact-checking and NLI prompts generally perform\nbetter, as similar tasks may have been seen during\ntheir instruction tuning phase.\n6 Related Work\nAttributed LMs. Generative LMs often produce\nhallucinations (Maynez et al., 2020; Dziri et al.,\n2021; Lee et al., 2018; Shuster et al., 2021; Wang\nand Sennrich, 2020; Xiao and Wang, 2021; Ji et al.,\n2023). To alleviate the issue, recent work proposes\nto augment LLMs (Mialon et al., 2023) with exter-\nnal tools (Schick et al., 2023; Li et al., 2023; Qin\net al., 2023) such as retrievers (Guu et al., 2020;\nLewis et al., 2020; Shuster et al., 2021; Izacard\nand Grave, 2021; Izacard et al., 2022; Borgeaud\net al., 2022; Trivedi et al., 2022; Qian et al., 2023)\nand search engines (Nakano et al., 2021; Komeili\net al., 2022; Thoppilan et al., 2022; Yao et al., 2022;\nGlaese et al., 2022; Shuster et al., 2022; Peng et al.,\n2023). Incorporating external references for gen-\neration inherently implies that the generated state-\nment is backed by these references. However, the\nModels Task\nPrompts\nZero-shot Few-shot\nSim. Gen. Sim. Gen.\nAlpaca\nAttr. 34.8 34.4 31.3 33.5\nNLI 32.1 35.5 32.1 33.6\nFact. 34.0 33.9 32.7 46.7\nSum. 33.6 33.5 31.6 34.8\nAverage 33.6 34.3 31.9 37.2\nChatGPT\nAttr. 37.2 45.1 37.6 51.4\nNLI 45.0 61.7 35.8 56.1\nFact. 44.8 54.9 43.2 54.9\nSum. 45.6 58.1 40.2 50.6\nAverage 43.2 55.0 39.2 53.3\nTable 4: Sensitivity of prompts for prompting LLMs\non AttrEval-Simulation (Sim.) and -GenSearch (Gen.).\nThe prompts include a prompt for attribution (Attri.),\na NLI prompt, a fact-checking prompt (Fact.), and a\nsummarization hallucination detection prompt (Sum.).\nvalidity of such attribution remains questionable.\nEvaluation of attribution. To evaluate attribu-\ntion, Liu et al. (2023) conduct a human evaluation\nto audit the verifiability of responses from genera-\ntive search engines. They find that these engines\nfrequently contain unsupported statements and in-\naccurate citations, which strengthen the need to\ncarefully examine the attribution of generations\n(Rashkin et al., 2021). However, human evalua-\ntions are very expensive and time-consuming. Gao\net al. (2022); Bohnet et al. (2022); Gao et al. (2023)\npropose to automatically evaluate attribution by lev-\nering NLI models (Honovich et al., 2022; Kamoi\net al., 2023; Gekhman et al., 2023). We study\nthis problem in a more comprehensive and realistic\nmanner: 1) we explore how helpful other relevant\ntasks besides NLI are to attribution evaluation; 2)\nour evaluation setting is based on both benchmark\nexamples and real examples.\n7 Conclusion\nIn this paper, we investigate the important prob-\nlem of automatically evaluating attribution given\nby LLMs. We begin by defining different types of\nattribution errors and then explore two approaches\nfor automatic evaluation: prompting LLMs and\nfine-tuning smaller LMs. We experiment with both\nsimulated test examples and manually curated test\nexamples from a real-life generative search engine.\nThe results highlight both promising signals and re-\nmaining challenges for the automatic evaluation of\nattribution. We hope our work could lay the foun-\ndation for future studies on this important problem.\n4622\n8 Limitations\nCurrently, smaller models in AttrScore are fine-\ntuned on the combination of simulated or repur-\nposed datasets from related tasks. However, this\ndataset still has gaps from the real scenario. More-\nover, the error patterns in these simulated datasets\nmight be overly simplistic and lack diversity, which\ncan limit the models’ ability to effectively handle\nmore complex and varied real-world errors. It is\nalso worth noting that these simulated datasets may\ncontain noise and erroneous labels, which could fur-\nther impede the models’ learning and subsequent\nperformance. How to obtain higher-quality train-\ning data for attribution evaluation at scale can be a\nmajor focus for future development.\nOur annotated evaluation set, AttrEval-\nGenSearch, is derived from New Bing, which uses\nGPT-4 as its backbone. It is crucial to note that\nwe also use GPT-4 for evaluating attribution on\nAttrEval-GenSearch, which achieves the best per-\nformance with around 85% overall accuracy. Some\nbias might come from GPT-4 both generating the\ntest examples and evaluating the attribution, which\ncould potentially skew our understanding of the\nmodel’s true performance. We therefore caution\nagainst over-optimism. We also acknowledge that\nthe size of AttrEval-GenSearch is moderate, which\nmay not fully represent the real use setting of\nattributed LLMs.\nWhile acknowledging current limitations, sev-\neral promising directions emerge for future re-\nsearch and enhancement. For example, one can\ndiversify data sources to include examples from a\nvariety of generative search engines, not just New\nBing. In addition, it may be beneficial to annotate\nlarger-scale queries that cover a broad spectrum of\ntopics, styles, and perspectives.\n9 Ethics Statement\nThis research project involves evaluating attribu-\ntion given by attributed LLMs. We collect and\nannotate data for evaluation using publicly avail-\nable information on the web, with the assistance\nof a generative search engine, New Bing. We ac-\nknowledge that LLMs have the potential to repro-\nduce and amplify harmful information present in\nthe data. We made an effort to mitigate this risk by\ncarefully selecting our evaluation data and by con-\nducting analyses to identify and mitigate potential\nrisks in the process.\nAcknowledgements\nThe authors would like to thank the anonymous re-\nviewers and colleagues from the OSU NLP group\nfor their thoughtful comments. This research was\nsupported in part by NSF IIS 1815674, NSF CA-\nREER 1942980, NSF OAC-2112606, and Ohio\nSupercomputer Center (OSC, 1987). The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as represent-\ning the official policies, either expressed or implied,\nof the U.S. government. The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copy-\nright notice herein.\nReferences\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. FEVEROUS: fact extraction and verifi-\ncation over unstructured and structured information.\nIn Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nIsabelle Augenstein, Christina Lioma, Dongsheng\nWang, Lucas Chaves Lima, Casper Hansen, Chris-\ntian Hansen, and Jakob Grue Simonsen. 2019. Mul-\ntiFC: A real-world multi-domain dataset for evidence-\nbased fact checking of claims. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4685–4697, Hong Kong,\nChina.\nPetr Baudis and Jan Sedivý. 2015. Modeling of the\nquestion answering task in the yodaqa system. In Ex-\nperimental IR Meets Multilinguality, Multimodality,\nand Interaction - 6th International Conference of the\nCLEF Association, CLEF 2015, Toulouse, France,\nSeptember 8-11, 2015, Proceedings, volume 9283 of\nLecture Notes in Computer Science, pages 222–228.\nSpringer.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA.\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\n2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models. ArXiv\npreprint, abs/2212.08037.\n4623\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 2206–2240.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMeng Cao, Yue Dong, and Jackie Cheung. 2022. Hal-\nlucinated but factual! inspecting the factuality of\nhallucinations in abstractive summarization. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3340–3354, Dublin, Ireland.\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou,\nDawn Song, and Quoc V . Le. 2020. Neural symbolic\nreader: Scalable integration of distributed and sym-\nbolic representations for reading comprehension. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\nArXiv preprint, abs/2210.11416.\nNouha Dziri, Andrea Madotto, Osmar Zaïane, and\nAvishek Joey Bose. 2021. Neural path hunter: Re-\nducing hallucination in dialogue systems via path\ngrounding. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2197–2214, Online and Punta Cana, Do-\nminican Republic.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. ArXiv\npreprint, abs/2302.04166.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\net al. 2022. Rarr: Researching and revising what\nlanguage models say, using language models. ArXiv\npreprint, abs/2210.08726.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. ArXiv preprint, abs/2305.14627.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-\ning factual consistency evaluation with large lan-\nguage models. ArXiv preprint, abs/2305.11171.\nAmelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\net al. 2022. Improving alignment of dialogue agents\nvia targeted human judgements. ArXiv preprint ,\nabs/2209.14375.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929–3938.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022 , pages 3905–3920.\nAssociation for Computational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\n4624\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. ArXiv preprint,\nabs/2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada.\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and\nGreg Durrett. 2023. Wice: Real-world entailment for\nclaims in wikipedia. ArXiv preprint, abs/2303.01432.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the Thirty-\nSecond AAAI Conference on Artificial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nficial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artificial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5189–5197.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8460–8478, Dublin, Ireland.\nNeema Kotonya and Francesca Toni. 2020. Explainable\nautomated fact-checking for public health claims. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7740–7754, Online.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara Fan-\nnjiang, and David Sussillo. 2018. Hallucinations in\nneural machine translation. In Interpretability and\nRobustness in Audio, Speech, and Language Work-\nshop, Conference on Neural Information Processing\nSystems (NeurIPS 2018), Montreal, Canada.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms. ArXiv\npreprint, abs/2304.08244.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023.\nEvaluating verifiability in generative search engines.\nArXiv preprint, abs/2304.09848.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. ArXiv preprint,\nabs/2212.10511.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factual-\nity in abstractive summarization. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1906–1919, Online.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ramakanth Pasunuru, Roberta\nRaileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave,\nYann LeCun, and Thomas Scialom. 2023. Aug-\nmented language models: a survey. ArXiv preprint,\nabs/2302.07842.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\n4625\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. ArXiv preprint,\nabs/2112.09332.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online.\nOpenAI. 2023a. Chatgpt (mar 14 version) [large lan-\nguage model]. https://chat.openai.com/chat.\nOpenAI. 2023b. GPT-4 technical report. ArXiv preprint,\nabs/2303.08774.\nOSC. 1987. Ohio supercomputer center.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. ArXiv\npreprint, abs/2302.12813.\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\nLearning to generate factually correct articles for\nqueries by grounding on large web corpus. ArXiv\npreprint, abs/2304.04358.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels. ArXiv preprint, abs/2304.08354.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2021. Measuring attribution in natu-\nral language generation models. ArXiv preprint ,\nabs/2112.12870.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nArXiv preprint, abs/2302.04761.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624–643, Online.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6138–6148, Online\nand Punta Cana, Dominican Republic.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage. ArXiv\npreprint, abs/2208.03188.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. ArXiv preprint, abs/2201.08239.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 809–819, New Orleans, Louisiana.\nJames Thorne, Andreas Vlachos, Oana Cocarascu,\nChristos Christodoulopoulos, and Arpit Mittal. 2019.\nThe FEVER2.0 shared task. In Proceedings of the\n4626\nSecond Workshop on Fact Extraction and VERifica-\ntion (FEVER), pages 1–6, Hong Kong, China.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. ArXiv preprint ,\nabs/2212.10509.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534–7550, Online.\nChaojun Wang and Rico Sennrich. 2020. On exposure\nbias, hallucination and domain shift in neural ma-\nchine translation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3544–3552, Online.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022b. Emergent abilities of large language models.\nArXiv preprint, abs/2206.07682.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. ArXiv preprint,\nabs/1910.03771.\nYijun Xiao and William Yang Wang. 2021. On hal-\nlucination and predictive uncertainty in conditional\nlanguage generation. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n2734–2744, Online.\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and\nYu Su. 2023. Adaptive chameleon or stubborn\nsloth: Unraveling the behavior of large language\nmodels in knowledge conflicts. ArXiv preprint ,\nabs/2305.13300.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. ArXiv preprint, abs/2210.03629.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\nArXiv preprint, abs/2306.05685.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompt-\ning for large language models. ArXiv preprint ,\nabs/2303.11315.\n4627\nA Data Simulation\nA.1 Simulation - QA\nAttributable. Since we have questions, and their\nground truth answers and reference contexts, we\ncan directly treat them as “Attributable” examples.\nContradictory. To simulate contradictory errors,\nwe consider two methods. The first method in-\nvolves modifying the correct answer by replacing\nit with a different candidate generated from an off-\nthe-shelf QA model, an answer substitution model,\nor a random span generator. The second method\ninvolves keeping the original answer and replacing\nthe answer span in the reference context with a sim-\nilar candidate. The QA model, the answer substitu-\ntion model, and the random span generator are all\nimplemented by prompting a FLAN-T5-XL (3B)\n(Chung et al., 2022) with different task prompts in\nAppendix Table 5.\nExtrapolatory. To simulate extrapolatory errors,\nwe employ a BM25 retriever to retrieve external\ndocuments that do not contain ground truth answers\nfrom knowledge sources like Wikipedia or the Web.\nAnd then we replace the original paragraph with\none of the retrieved documents. For the answer,\nwe either keep the original ground truth answer or\nleverage a QA model to generate an answer. Here\nare more details for constructing negative retrieved\ndocuments in each dataset.\nFollowing previous work (Karpukhin et al.,\n2020), we utilize the passages from Wikipedia\ndumps for constructing evidence for NaturalQues-\ntions (Kwiatkowski et al., 2019), WebQues-\ntions (Berant et al., 2013), and TREC (Baudis and\nSedivý, 2015) datasets. In particular, we regard\nthe highest-ranked passage including answers from\nBM25 as positive evidence and the top passage\nwithout answers as negative evidence.\nFor TriviaQA (Joshi et al., 2017), we select the\npassage with the highest overlap with answers from\nweb texts as positive evidence and the top-ranked\nwiki passage without answers from BM25 as neg-\native evidence. We exclude examples where the\npositive evidence has an overlap ratio of less than\n0.5 with answers. For HotpotQA (Yang et al.,\n2018), we combine the ground truth passages pro-\nvided as positive evidence and randomly select\ntwo out of eight passages provided as negative evi-\ndence. Similarly, in PopQA (Mallen et al., 2022),\nwe find positive evidence from Wikipedia content\nthrough the provided link and retrieve negative evi-\ndence from Wikipedia dumps using BM25. In En-\ntityQuestions (Sciavolino et al., 2021), we match\npositive evidence in Wikipedia texts searched by\nthe question entity and retrieve negative evidence\nvia BM25.\nConverting short answers to long sentences.\nSince many of the attributed LLMs generate long\nsentences to the query, to make it our simulated\ndata more realistic, we convert short answers to\nlong answers using ChatGPT. Specifically, we\nprompt ChatGPT with the instruction “Convert\na given question and answer pair into plain sen-\ntences. [Question] [Answer]”.\nA.2 Simulation - Fact Checking\nWith provided Wiki content as evidence in\nFEVER (Thorne et al., 2018) and Adversarial\nFEVER datasets (Thorne et al., 2019), we repur-\npose ‘SUPPORTS’ examples as attributable, ‘RE-\nFUTES’ as contradictory, and ‘NOT ENOUGH\nINFO’ as extrapolatory. Using the same label\nmapping, we apply this approach to the claim\nand evidence provided in VITAMINC (Schuster\net al., 2021), after removing duplicated examples\nas shown in FEVER. For FEVEROUS (Aly et al.,\n2021), we concatenate all pieces of evidence, in-\ncluding tables and texts, and prepend an increasing\nindex as the final evidence. We then ground the\nlabel into our three categories using the same label\nmapping. Regarding natural claim datasets with\nvarious label spaces, we keep the top 6 classes\nout of 117 in MultiFC (Augenstein et al., 2019)\nand map them to our defined three categories. In\nPUBHEALTH (Kotonya and Toni, 2020), we con-\nsider both ‘unproven’ and ‘mixture’ classes as ex-\ntraplanetary. We also regard the abstract of the\narticle as evidence. For SciFact (Wadden et al.,\n2020), we repurpose ‘SUPPORT’ as attributable\nand ‘CONTRADICT’ as contradictory. Addition-\nally, we randomly select one sentence from the\nabstract of other articles as evidence for the ‘Not\nenough information’ class to construct extrapola-\ntory examples.\nA.3 Simulation - NLI\nNatural language inference (NLI) aims to de-\ntermine whether a hypothesis is true given a\npremise. In NLI datasets such as SNLI (Bow-\nman et al., 2015), MultiNLI (Williams et al., 2018),\nANLI (Nie et al., 2020), and SciTail (Khot et al.,\n4628\n2018), the hypothesis is considered the claim and\nthe premise is regarded as the evidence. The orig-\ninal labels in NLI datasets, namely ‘Entailment’,\n‘Contradictory’, and ‘Neutral’, are mapped to ‘At-\ntributable’, ‘Contradictory’, and ‘Extrapolatory’.\nA.4 Simulation - Summarization\nSummarization involves condensing a given pas-\nsage or article into brief sentences while preserving\nits original meaning. To simulate contradictory ex-\namples, we use datasets with annotations of halluci-\nnations. In terms of XSum-Hallucination (Maynez\net al., 2020), we merge examples with the same\nID and consider those with the most intrinsic hallu-\ncination as contradictory and those with the most\nextrinsic hallucination as extrapolatory. Paired full\narticles and ground truth summaries are treated as\nattributable examples. For XENT (Cao et al., 2022),\n‘Non-factual Hallucination’ and ‘Intrinsic Halluci-\nnation’ are seen as contradictory, ‘Factual Halluci-\nnation’ as extrapolatory, and ‘Non-hallucinated’ as\nattributable. Each article and reference are paired\nas attributable examples. Finally, we resplit the\nmanually annotated dev and test sets for training\nand evaluation in FactCC (Kryscinski et al., 2020),\nwith ‘INCORRECT’ labeled as extrapolatory and\n‘CORRECT’ as attributable.\nB Label and Subset Distributions of\nTraining and Test Sets\nWe show the label and data sources’ distributions\nof training and AttrEval-Simulation sets in Figure\n5 and Figure 6.\nC Prompts for LLMs as AttrScore\nWe show different kinds of prompts for using\nLLMs as AttrScore in Table 6. And we show the\nfew-shot demonstrations in Table 7.\nD Generative Search Engine Examples\nAnnotation Protocol\nWe show the detailed annotation guidelines in the\nfollowing.\n4629\nA n n o t a t i o n G u i d e l i n e s \n O v e r v i e w \n T h a n k y o u f o r p a r t i c i p a t i n g i n t h i s a n n o t a t i o n t a s k . T h e g o a l o f t h i s t a s k i s t o c r e a t e a q u e r y a n d v e r i f y w h e t h e r a g i v e n r e f e r e n c e d o c u m e n t f u l l y s u p p o r t s t h e g e n e r a t i o n o f t h e q u e r y . \n T h e r e a r e t w o s u b - a n n o t a t i o n t a s k s : 1 . C r e a t e a q u e r y b a s e d o n a f e w g i v e n k e y w o r d s u n d e r a t o p i c . 2 . V e r i f y w h e t h e r a g i v e n a n s w e r t o a q u e r y i s f u l l y s u p p o r t e d b y i t s r e f e r e n c e s . \nT ask 1: Create a query for a specific domain. \n Y o u w i l l b e s h o w n a l i s t o f k e y w o r d s ( e . g . , i n f l a t i o n r a t e , C P I , G D P , u n e m p l o y m e n t r a t e , e t c . ) f r o m a s p e c i f i c d o m a i n o r t o p i c ( e . g . , e c o n o m i c s ) a n d a d e m o q u e s t i o n ( e . g . , W h a t w a s t h e u n e m p l o y m e n t r a t e i n G e r m a n y i n 2 0 2 0 ? ) a s a n i n s p i r a t i o n . T h e n y o u w i l l b e a s k e d t o c r e a t e a n e w q u e r y b a s e d o n t h e s e k e y w o r d s . \nT ask 2: V erify whether the generated statement is supported by its reference. \n Y o u w i l l b e s h o w n a u s e r q u e r y , a g e n e r a t i v e s e a r c h e n g i n e ’ s r e s p o n s e , a n d a s s o c i a t e d r e f e r e n c e s . Y o u w i l l n e e d t o r e a d t h e q u e r y , r e s p o n s e , a n d r e f e r e n c e c a r e f u l l y a n d v e r i f y w h e t h e r t h e c i t e d e v i d e n c e f u l l y s u p p o r t s t h e g e n e r a t i o n o f t h e q u e r y . \n Y o u w i l l b e g i v e n t h r e e o p t i o n s : a . ‘ ‘ A t t r i b u t a b l e ’ ’ : T h e r e f e r e n c e f u l l y s u p p o r t s t h e g e n e r a t i o n o f t h e q u e r y . b . ‘ ‘ E x t r a p o l a t o r y ’ ’ : T h e g e n e r a t i o n c o n t r a d i c t s t h e r e f e r e n c e . c . ‘ ‘ C o n t r a d i c t o r y ’ ’ : T h e r e f e r e n c e d o e s n o t c o n t a i n s u f f i c i e n t i n f o r m a t i o n t o i n f e r t h e c o r r e c t n e s s o f t h e g e n e r a t i o n . \n4630\nC o n s i d e r t h e f o l l o w i n g e x a m p l e s : ( 1 ) ‘ ‘ A t t r i b u t a b l e ’ ’ Q u e r y : W h a t i s t h e t e m p e r a t u r e r a n g e o n t h e m o o n ? R e s p o n s e : T h e a v e r a g e t e m p e r a t u r e o n t h e m o o n c a n r a n g e f r o m - 2 9 8 d e g r e e s F ( - 1 8 3 d e g r e e s C ) a t n i g h t t o 2 2 4 d e g r e e s F ( 1 0 6 d e g r e e s C ) d u r i n g t h e d a y [ 1 ] . R e f e r e n c e : T h e a v e r a g e t e m p e r a t u r e o n t h e M o o n ( a t t h e e q u a t o r a n d m i d l a t i t u d e s ) v a r i e s f r o m - 2 9 8 d e g r e e s F a h r e n h e i t ( - 1 8 3 d e g r e e s C e l s i u s ) , a t n i g h t , t o 2 2 4 d e g r e e s F a h r e n h e i t ( 1 0 6 d e g r e e s C e l s i u s ) d u r i n g t h e d a y . B e c a u s e t h e M o o n h a s n o . . . A n n o t a t i o n : A t t r i b u t a b l e \n ( 2 ) ‘ ‘ E x t r a p o l a t o r y ’ ’ Q u e r y : W h a t i s t h e h i g h e s t p r i c e o f a g a l l o n o f r e g u l a r g a s i n t h e U S i n 2 0 2 2 ? R e s p o n s e : A c c o r d i n g t o a n e w s a r t i c l e b y N P R , a s o f J u n e 2 0 2 2 , t h e h i g h e s t a v e r a g e p r i c e o f a g a l l o n o f r e g u l a r g a s i n t h e U S w a s $ 6 . 3 4 i n C a l i f o r n i a [ 1 ] . R e f e r e n c e : U . S . g a s p r i c e s h i t a n e w r e c o r d o n M o n d a y , w i t h a g a l l o n o f r e g u l a r g a s c o s t i n g a n a v e r a g e o f $ 4 . 8 6 5 ( n o t a d j u s t i n g f o r i n f l a t i o n ) , a c c o r d i n g t o t h e m o s t r e c e n t d a t a f r o m t h e A m e r i c a n A u t o m o b i l e A s s o c i a t i o n . . . A n n o t a t i o n : E x t r a p o l a t o r y \n ( 3 ) ‘ ‘ C o n t r a d i c t o r y ’ ’ Q u e r y : W h a t w a s t h e u n e m p l o y m e n t r a t e i n G e r m a n y i n 2 0 2 0 ? R e s p o n s e : A c c o r d i n g t o [ 1 ] , t h e u n e m p l o y m e n t r a t e i n G e r m a n y f o r 2 0 2 0 w a s 4 . 3 1 % , w h i c h w a s a 1 . 1 7 % i n c r e a s e f r o m 2 0 1 9 . H o w e v e r , i t i s i m p o r t a n t t o n o t e t h a t t h e C O V I D - 1 9 p a n d e m i c h a d a s i g n i f i c a n t i m p a c t o n u n e m p l o y m e n t r a t e s i n G e r m a n y a n d a r o u n d t h e w o r l d . R e f e r e n c e : G e r m a n y u n e m p l o y m e n t r a t e f o r 2 0 2 0 w a s 3 . 8 1 % . A n n o t a t i o n : C o n t r a d i c t o r y \n4631\nTasks Prompts\nQA Context: [Context]\\n\nBased on Context, [Question]\nAnswer\nSubstitution\nPlease provide a related term or substitution for the given input, which should be different from the input.\\n\n\"Input: Biden; Output: Obama\\n\"\n\"Input: 1949; Output: 1358\\n\"\n\"Input: University of Maryland; Output: University of Cambridge\\n\"\n\"Input: 09/12/2014; Output: 03/30/2008\\n\"\n\"Input: $431; Output: $769;\\n\"\n\"Input: [Ground Truth Answer]; Output: \",\nRandom Span\nGeneration Extract a phrase from the given passage. \\n Passage: [Context]\nTable 5: Prompts for QA, answer substitution, and random span generation when simulating contradictory errors\nAttributable Contradictory Extrapolatory\n0\n10\n20\n30\n40\n50Percentage (%)\n42.48%\n28.51% 29.01%\nLabel distribution in combined_train\nAttributable Contradictory Extrapolatory\n0\n10\n20\n30\n40Percentage (%)\n33.33% 33.33% 33.33%\nLabel distribution in attreval_simulation\nAttributable Contradictory Extrapolatory\n0\n10\n20\n30\n40\n50\n60Percentage (%)\n33.47%\n13.64%\n52.89%\nLabel distribution in attreval_gensearch\nFigure 5: Label distribution of training and test sets.\nXSUMHUL\n1.6%\nVITAMINC\n13.0%\nSNLI\n15.4%\nSCITAIL\n1.2%\nPUBHEALTH\n0.5%\nMULTINLI\n11.3%\nMULTIFC\n0.5%\nNQ\n31.3%\nSCIFACT\n0.1%\nENTFA\n1.8%\nFACTCC\n2.6%\nANLI\n4.6%\nFEVER\n16.1%\nDistribution of data source in the combined training set\nWEBQUESTIO\n3.6%\nTRIVIAQA\n22.7%\nTREC\n1.9%\nPOPQA\n21.6%\nENTITYQUEST\n26.0%\nHOTPOTQA\n24.2%\nDistribution of data source in the AttrEval-Simulation\nFigure 6: Data source distribution of combined training and AttrEval-Simulation sets.\nE Additional Qualitative Analysis\nThe qualitative results of ChatGPT are shown in\nTable 8. Our first observation is that a significant\nportion (79.4%) of errors happen due to ChatGPT\noverlooking the context clues and does not make\njudgments by conditioning on the reference (e.g.,\npotentially relying on its own parametric knowl-\nedge). For the remaining error cases, they are:\n1) fine-grained information insensitivity (13.8%):\nfailure in comparing very fine-grained informa-\ntion such as numerical values, numbers, dates, and\ntime; 2) failure in performing symbolic operations\n(6.8%): the model fails to verify the claim which\nrequires performing symbolic operations over the\nreference, such as verifying set relationships.\nF Author Contribution Statement\nXiang Yue conceived the project, conceptualized\nand designed the study, conducted experiments,\nwrote the manuscript, and annotated New Bing test\nexamples. Boshi Wang provided critical feedback\nand edits, revised the manuscript, contributed to the\n4632\nPrompt Types Prompts\nAttribution\n### Instruction:\nAs an Attribution Validator, your task is to verify whether a given context can support the claim.\nA claim can be either a plain sentence or a question followed by its answer. Specifically, your\nresponse should clearly indicate the relationship: Attributable, Contradictory or Extrapolatory.\nA contradictory error occurs when you can infer that the answer contradicts the fact presented\nin the context, while an extrapolatory error means that you cannot infer the correctness of the\nanswer based on the information provided in the context.\n### Input:\nClaim: [Question Answer] or [Plain Sentence] \\n\\n\nContext: [Context]\n### Response:\nFact-Checking\n### Instruction:\nFact-check a claim based on the given evidence.\nOptions: Supported, Refuted or Not Enough Information\n### Input:\nClaim: <Claim>\\n\\n\nEvidence: <Evidence>\n### Response:\nNLI\n### Instruction:\nRead the following and determine if the hypothesis can be inferred from the premise.\nOptions: Entailment, Contradiction, or Neutral\n### Input:\nHypothesis: <Hypothesis>\\n\\n\nPremise: <Premise>\n### Response:\nSummarization\nHallucination\nDetection\n### Instruction:\nRead the following and determine whether the source text can support the summary.\nOptions: Support, Contradicts, or Not Enough Information\n### Input:\nSummary: <Summary>\\n\\n\nSource: <Source>\n### Response:\nTable 6: Prompt variations for test the sensitivity of different prompts on the results. We use the “Attribution”\nprompt for our main experiments as default as it achieves the best performance overall.\nconceptualization of the study, conducted experi-\nments for ChatGPT, and annotated New Bing test\nexamples. Ziru Chen set up the training code base,\nannotated New Bing test examples, and conducted\nexperiments for fine-tuning Roberta, Flan-T5, and\nGPT-2. Kai Zhang contributed to all the simulation\ndata preprocessing, revised the manuscript, and an-\nnotated New Bing test examples. Yu Su and Huan\nSun secured funding for the project, provided su-\npervision and guidance throughout the study, con-\ntributed to the conceptualization and design of the\nstudy, and edited the whole manuscript. All authors\napproved the final version of the manuscript.\n4633\nFew-shot demonstrations\nHere are some demonstration examples for you.\n### Input:\nClaim: In what year was the writer of the opera \"Mazeppa\" born? The writer of the opera \"Mazeppa\" was born in 1840.\nReference: Mazeppa, properly Mazepa, is an opera in three acts (six scenes) by Pyotr Ilyich Tchaikovsky. The libretto\nwas written by Victor Burenin and is based on Pushkin\\’s poem \"Poltava\". Pyotr Ilyich Tchaikovsky (25 April/7 May\n1840 – 25 October/6 November 1893), often anglicized as Peter Ilich Tchaikovsky, was a Russian composer of the\nromantic period, some of whose works are among the most popular music in the classical repertoire. He was the first\nRussian composer whose music made a lasting impression internationally, bolstered by his appearances as a guest\nconductor in Europe and the United States. Tchaikovsky was honored in 1884, by Emperor Alexander III, and awarded\na lifetime pension.\n### Response:\nAttributable. From the given reference, Pyotr Ilyich Tchaikovsky was the writer of \"Mazeppa\", who was born in 1840\nas stated in the given claim.\n-\n### Input:\nClaim: In what part of Buenos Aires Province is the city that has The Juan Carlos Castagnino Municipal Museum\nof Art located? It is located in the southeast part.\nReference: The Juan Carlos Castagnino Municipal Museum of Art is a museum of fine arts in Mar del Plata, Argentina.\nMar del Plata is an Argentine city in the east part of Buenos Aires Province located on the coast of the Atlantic Ocean.\nIt is the head of General Pueyrredón Partido. Mar del Plata is the second largest city in Buenos Aires Province. The name\n\"Mar del Plata\" has the meaning of \"sea of the Silver region\" or \"adjoining sea to the (River) Silver region\". Mar del Plata\nis one of the major fishing ports and the biggest seaside beach resort in Argentina.\n### Response:\nContradictory. The reference says that Mar del Plata is in the east part of Buenos Aires, so The Juan Carlos Castagnino\nMunicipal Museum of Art is also in the east part of Buenos Aires, not southeast.\n-\n### Input:\nClaim: In what year was Louis Joseph de Frances\\’s mother born? Louis Joseph de Frances’s mother was born in 1755.\nReference: Lakshman Joseph de Saram is a film composer and classical musician. Born in Colombo, Sri Lanka and\neducated at the Royal College, Colombo, the High School of Performing Arts, Manhattan School of Music and Juilliard\nPre-College in New York City, Joseph de Saram is influential in the music of South Asian art cinema, having scored many\ninternational award-winning films like \\’Between Two Worlds\\’ and \\’Akasa Kusum.\\’ His best-known score is to the 2012\nfilm \"Bel Ami\". Joseph de Saram is also artistic director of The Chamber Music Society of Colombo. Louis Joseph de\nLorraine \"Duke of Guise\" and Duke of Angoulême, (7 August 1650 – 30 July 1671) was the only son of Louis, Duke of\nJoyeuse and Marie Françoise de Valois, the only daughter of Louis-Emmanuel d\\’Angoulême, Count of Alès, Governor of\nProvence and son of Charles de Valois Duke of Angoulême, a bastard of Charles IX of France.\n### Response:\nExtrapolatory. The answer to the question cannot be inferred from the given reference since the person \"Louis Joseph de\nFrances\" is not mentioned. We also cannot find any evidence in the Reference which contradicts with the provided answer.\n-\nNow here is the example that needs your response:\nTable 7: Few-shot demonstrations for prompting LLMs as AttrScore.\n4634\nError Type(percentage)Example\nOverlookingcontextualcues(79.4%)\nQuery:Was ketchup a medicine before?Answer:: Yes, that’s true! Ketchup was sold as a remedy for indigestion, diarrhea, jaundice and rheumatism in the 1830sReference:Decades later, in the 1870s, American entrepreneur Henry Heinz created a modern version of ketchup with ripe tomatoes,distilled vinegar, brown sugar, salt and a variety of spices. By 1905, the company had sold five million bottles of ketchup, cementing itsplace as the condiment we know and love today.Ground Truth:ExtrapolatoryChatGPT:Attributable. The reference supports the claim thatketchup was sold as aremedyfor various ailments in the 1830s,\nindicating that it was used as a medicine before.\nFine-grainedinformationinsensitivity(13.8%)\nQuery:How many total miles of coastline does Norway have?Answer:The official length was recalculated in 2011 as100,915 km , including fjords and islands.Reference:A straight line along Norway’s sea borders (the coastal perimeter) is 2,650 kilometers (1,650 mi) long. Along the coast thereare many fjords, islands, and bays, resulting in a low-resolution coastline of over25,000 kilometers (16,000 mi).Ground Truth:ContradictoryChatGPT:Attributable. The reference provides information that the official length of Norway’s coastline, including fjords and islands, is100,915 km. Therefore, the claim that Norway has a total of 100,915 km of coastline is supported by the Reference.\nFailing onsymbolicoperators(6.8%)\nQuery:Is Andrew Ng an author of the recent paper “generative agents: interactive simulacra of human behavior”?Answer:Yes, Andrew Ng is one of the authors of the paper “Generative agents: Interactive simulacra of human behavior”.Reference:Computer Science > Human-Computer Interaction [Submitted on 7 Apr 2023] Generative Agents: Interactive Simulacra ofHuman BehaviorJoon Sung Park, Joseph C. O’Brien,Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein .Ground Truth:ContradictoryChatGPT:Attributable. The reference clearly states that Andrew Ng is one of the authors of the paper \"Generative Agents: InteractiveSimulacra of Human Behavior\", which supports the claim that he is an author of the paper.\nTable 8: Qualitative analysis on misclassified examples from prompting ChatGPT in AttrScore.\n4635",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8548941612243652
    },
    {
      "name": "Computer science",
      "score": 0.7881283760070801
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6466182470321655
    },
    {
      "name": "Attribution",
      "score": 0.6291891932487488
    },
    {
      "name": "Generative grammar",
      "score": 0.5917446613311768
    },
    {
      "name": "Natural language",
      "score": 0.5539290308952332
    },
    {
      "name": "Inference",
      "score": 0.5381788611412048
    },
    {
      "name": "Statement (logic)",
      "score": 0.5095303654670715
    },
    {
      "name": "Natural language processing",
      "score": 0.5078399777412415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5076462030410767
    },
    {
      "name": "Question answering",
      "score": 0.4692535102367401
    },
    {
      "name": "Test (biology)",
      "score": 0.46528807282447815
    },
    {
      "name": "Focus (optics)",
      "score": 0.4504835605621338
    },
    {
      "name": "Language model",
      "score": 0.43523988127708435
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.42673587799072266
    },
    {
      "name": "Information retrieval",
      "score": 0.4091424345970154
    },
    {
      "name": "Data science",
      "score": 0.4064655005931854
    },
    {
      "name": "Machine learning",
      "score": 0.3703661561012268
    },
    {
      "name": "Programming language",
      "score": 0.20047500729560852
    },
    {
      "name": "Linguistics",
      "score": 0.10405737161636353
    },
    {
      "name": "Psychology",
      "score": 0.09607291221618652
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    }
  ]
}