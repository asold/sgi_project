{
  "title": "Detecting Argumentative Fallacies in the Wild: Problems and Limitations of Large Language Models",
  "url": "https://openalex.org/W4389518216",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2901746334",
      "name": "Ramon Ruiz-Dolz",
      "affiliations": [
        "University of Dundee"
      ]
    },
    {
      "id": "https://openalex.org/A2011820214",
      "name": "John Lawrence",
      "affiliations": [
        "University of Dundee"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094099490",
    "https://openalex.org/W2963055610",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385573809",
    "https://openalex.org/W3175362033",
    "https://openalex.org/W2807031262",
    "https://openalex.org/W4385567117",
    "https://openalex.org/W2167471601",
    "https://openalex.org/W2970487286",
    "https://openalex.org/W3113763975",
    "https://openalex.org/W1602079327",
    "https://openalex.org/W1527048733",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962768347",
    "https://openalex.org/W2740947917",
    "https://openalex.org/W4285602624",
    "https://openalex.org/W581684831",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Previous work on the automatic identification of fallacies in natural language text has typically approached the problem in constrained experimental setups that make it difficult to understand the applicability and usefulness of the proposals in the real world. In this paper, we present the first analysis of the limitations that these data-driven approaches could show in real situations. For that purpose, we first create a validation corpus consisting of natural language argumentation schemes. Second, we provide new empirical results to the emerging task of identifying fallacies in natural language text. Third, we analyse the errors observed outside of the testing data domains considering the new validation corpus. Finally, we point out some important limitations observed in our analysis that should be taken into account in future research in this topic. Specifically, if we want to deploy these systems in the Wild.",
  "full_text": "Proceedings of the 10th Workshop on Argument Mining, pages 1–10\nDecember 7, 2023. ©2023 Association for Computational Linguistics\n1\nDetecting Argumentative Fallaciesin the Wild: Problems and Limitations of\nLarge Language Models\nRamon Ruiz-Dolz and John Lawrence\nCentre for Argument Technology (ARG-tech)\nUniversity of Dundee\nUnited Kingdom\n{rruizdolz001,j.lawrence}@dundee.ac.uk\nAbstract\nPrevious work on the automatic identification\nof fallacies in natural language text has typi-\ncally approached the problem in constrained\nexperimental setups that make it difficult to\nunderstand the applicability and usefulness of\nthe proposals in the real world. In this paper,\nwe present the first analysis of the limitations\nthat these data-driven approaches could show\nin real situations. For that purpose, we first\ncreate a validation corpus consisting of natural\nlanguage argumentation schemes. Second, we\nprovide new empirical results to the emerging\ntask of identifying fallacies in natural language\ntext. Third, we analyse the errors observed out-\nside of the testing data domains considering\nthe new validation corpus. Finally, we point\nout some important limitations observed in our\nanalysis that should be taken into account in\nfuture research in this topic. Specifically, if we\nwant to deploy these systems in the Wild.\n1 Introduction\nIn the field of the automatic analysis of natural lan-\nguage argumentative discourse, the identification\nof fallacies plays an important role since it can be a\ndetermining feature to measure the quality of argu-\nmentation (Wachsmuth et al., 2017). Furthermore,\nthe automatic identification of fallacies can also be\nhelpful for the development of disinformation de-\ntection systems and critical thinking tools (Visser\net al., 2020). Studied since the times of the ancient\nGreece by Aristotle (Aristotle, 1978), a fallacy was\nseen as an argumentation strategy used to deceive\nan opponent in a debate and unfairly get the reason.\nThis definition evolved with time (Van Eemeren\nand Grootendorst, 1984; Hamblin, 1970) extending\nthe instrumental notion of the Aristotelian fallacy\nto more modern theories of logic and mathemat-\nics. A more recent (and complete) definition was\nprovided by Walton (1995), where fallacies are de-\nfined as “important, baptizable types of errors or\ndeceptive tactics of argumentation that tend to fool\nor trip up participants in argumentation in various\nkinds of everyday discussions”. This definition is\nless constrained and more accurate to the natural\nlanguage challenges we may face these days.\nDetecting a piece of fallacious reasoning, how-\never, is not trivial and requires knowledge in a\nbroad number of areas that make this task challeng-\ning. First, it is important to be able to analyse the\nlogical reasoning underlying natural language ar-\nguments. For that purpose, it is required to distil\nthe abstract and formal components from the in-\nformal natural language argument. This first case\nis that of formal fallacies (Oliver, 1967). Second,\nsolid knowledge on the domain of discussion is of\nutmost importance. An argument can be logically\nsound but still fallacious, such is the case of infor-\nmal fallacies (Walton, 1987). Therefore, only with\na complete analysis is it possible to determine if\na natural language argument is a fallacy or not, as\nwell as the underlying reasons why it is fallacious.\nA consistent way to conduct this analysis is to rely\non validated models of argument which capture the\nnotion of fallacy. Different models have been pro-\nposed and studied in the literature; such is the case\nof the pragma-dialectic theory of argumentation\n(Van Eemeren and Grootendorst, 2016) in which\nthe authors define ten rules to guide argumentative\ndiscussions. The fulfilment of these rules allows\nto create a fruitful discussion, but an argument that\nbreaks any of these rules is considered a fallacy.\nAnother good example of these models is the argu-\nmentation schemes proposed by Walton (Walton\net al., 2008). An argumentation scheme combines\nthe abstract representation of the underlying logic\nof a natural language argument with a set of critical\nquestions that must be successfully answered to\nprove the validity of an argument. The argumenta-\ntion scheme model is very interesting w.r.t. fallacy\nanalysis, since an argument being fallacious is not\ndetermined by belonging to a specific class, but\ndepending on the answers provided to the set of\n2\ncritical questions. For example, a natural language\nargument belonging to the Ad Hominem scheme\nis not a fallacy per se, but it must be structured as\nfollows:\nCharacter Attack Premise: a is a person of bad\ncharacter.\nConclusion: a’s argument α should not be ac-\ncepted.\nAnd it is only considered to be fallacious if any\nof the following critical questions cannot be suc-\ncessfully answered,\n• CQ1: How well supported by evidence is\nthe allegation made in the character attack\npremise?\n• CQ2: Is the issue of the character relevant in\nthe type of dialogue in which the argument\nwas used?\n• CQ3: Is the conclusion of the argument that α\nshould be rejected, or is the conclusion that α\nshould be assigned a reduced weight of credi-\nbility?\nTherefore, with the argumentation scheme\nparadigm, it is possible to partially dissociate the\nnatural language and the logic of the argument, al-\nlowing for a more informed analysis of the reasons\nof an argument being fallacious.\nIn this paper, we integrate the concept of argu-\nmentation schemes in the evaluation of machine\nlearning and Transformer-based language models\nfor the automatic detection of fallacies in natural\nlanguage arguments. It is our objective to under-\nstand the way these models, as they have been\nproposed in most of the previous work in this topic,\nare able to learn the reasons behind a fallacy and\ngeneralise to data outside of the training domain.\nOur contribution is therefore threefold: (i) we cre-\nate a fallacy validation corpus consisting of natural\nlanguage argumentation schemes; (ii) we provide\nnew empirical results for the emerging task of iden-\ntifying fallacies in natural language text; and (iii)\nwe analyse the observed errors inside and outside\nof the testing data domains considering the argu-\nmentation scheme validation corpus, and point out\nsome of the main limitations of relying exclusively\non LLMs when addressing complex natural lan-\nguage reasoning problems.\n2 Related Work\nThe automatic detection of fallacies in natural lan-\nguage texts is an emerging topic of research within\nthe area of Natural Language Processing. One of\nthe first efforts in developing a database of fallacies\nwas done in (Habernal et al., 2017) creating “ Ar-\ngotario”, an educative platform where participants\ncould improve their debating skills. Through gami-\nfication, the authors collected fallacies registered by\nthe participants belonging to one of the following\nfive classes: ad hominem, appeal to emotion, red\nherring, hasty generalisation, irrelevant authority.\nA direct continuation of this work was presented\nin (Habernal et al., 2018a), where the resulting cor-\npus from the use of “ Argotario” containing 430\nannotated arguments was released. In that work,\narguments belonging to the previous five classes\nplus a no fallacy set of arguments were compiled,\nand a set of preliminary results of experiments with\na Support Vector Machine (SVM) and a Bidirec-\ntional Long Short-Term Memory (BiLSTM) neural\nnetwork were reported.\nAimed at better understanding the linguistic fea-\ntures underlying the Ad Hominem argument, Haber-\nnal et al. developed a corpus from user discussions\nin the Change My View subreddit on the Reddit\nsocial network (Habernal et al., 2018b). For that\npurpose they retrieved the comments that were re-\nmoved by the administrators because they were la-\nbelled as rude or hostile by the community, match-\ning one of the non breakable rules proposed in\n(Van Eemeren and Grootendorst, 2016) as part of\nthe pragma-dialectic theory of argumentation. The\nauthors also reported a set of fallacy detection ex-\nperiments with a Convolutional Neural Network\n(CNN) in which they used this corpus consisting\nof 7,242 samples balanced between non-fallacious\nand ad hominem classes.\nThe automatic identification of argumentative\nfallacies has also been studied from the propaganda\nviewpoint in (Da San Martino et al., 2019), where\nthe authors annotate news articles containing up\nto 18 propaganda techniques and report a series\nof experiments on propaganda classification. This\nperspective on fallacious argumentation was con-\ntinued in a shared task organised for the SemEval\nforum (Da San Martino et al., 2020) aimed at the\nautomatic classification of natural language propa-\nganda.\nBased on the pragma-dialectic theory\n(Van Eemeren and Grootendorst, 2016) eight\nclasses of fallacious arguments were annotated in a\ncorpus of informal fallacies in online discussions\nby Sahai et al. (2021). More than 1,700 fallacious\n3\ncomments retrieved from Reddit were annotated\ninto the classes of Appeal to authority, Appeal to\nmajority, Appeal to nature, Appeal to tradition,\nAppeal to worse problems, Black-or-white, Hasty\ngeneralisation, and Slippery slope fallacies.\nFurthermore, the authors report results on the\nbinary task of classifying natural language text as\nfallacious or not, and on the 8-class classification\nproblem of determining the type of fallacy to\nwhich each fallacious comment belongs to. For the\nexperiments, the authors consider more advanced\nmodels based in the Transformer architecture, and\nthe granularity network that performed the best in\n(Da San Martino et al., 2019).\nA simplified version of the task is presented in\n(Goffredo et al., 2022), where another corpus of\nfallacious argumentation is released. In this pa-\nper, the annotation of fallacious arguments is done\nfrom the transcripts of 31 political debates of the\nU.S. Presidential Campaigns. The authors anno-\ntate six different types of fallacy: Ad Hominem,\nAppeal to Emotion, Appeal to Authority, Slippery\nSlope, False Cause, and Slogans. In addition to\nthese classes, 11 sub-classes are also annotated,\nproviding additional information of the fallacious\narguments. In their experiments, the best results\nare reported with a Transformer-based architecture\nthat combines natural language with argumentative\nfeatures. The experimental results reported in that\nwork are exclusively focused on the task of classify-\ning fallacies, assuming that the fallacy has already\nbeen detected.\nRecently, (Alhindi et al., 2022) al explores the\nuse of multitask instruction-based prompting to\ndectect 28 different fallacies across five datasets.\nThe authors compare the use of T5 (Raffel et al.,\n2020) and GPT-3 (Brown et al., 2020) for prompt-\nbased fallacy classification, and a fine-tuned BERT\n(Devlin et al., 2018) model for a more classic\nbaseline. From their results, it is possible to ob-\nserve how the multitask instruction-based prompt-\ning with T5 achieves a significant increase in perfor-\nmance compared to the GPT-3 and BERT baselines.\nHowever, the methodology applied in this paper is\nsimilar to the one followed in previous work, in\nwhich the fallacy type of a text sequence is deter-\nmined by only taking the natural language of the\nsequence into account.\nFinally, one of the most recent papers in the au-\ntomatic detection of logical fallacies proposes a\nnew task for pre-training language models based\non the structure of arguments (Jin et al., 2022).\nFor that purpose, the authors release a corpus con-\nsisting of 2,449 argumentative samples labelled\ninto one of 13 different fallacy types. A set of\nexperiments comparing Large Language Models\n(LLMs) as zero-shot classifiers with Transformer-\nbased models fine-tuned on the corpus is reported,\nemphasising on the importance of looking at struc-\ntural reasoning features for this type of classifica-\ntion problems.\nWe can observe how, in the past years, a var-\nied set of relatively small corpora have been anno-\ntated and publicly released. Most of them, how-\never, share a similar paradigm for addressing the\nautomatic identification of argumentative fallacies.\nShort spans of text are labelled with one of the cor-\nresponding fallacy labels, but no attention is given\nto the underlying logic that makes the argument\nfallacious or not. Furthermore, all the reported ex-\nperiments are done in a similar way, the natural\nlanguage text is used as the input to learn a set\nof N classes (varying from one corpus to another)\ndirectly from the text, and no in-depth error anal-\nyses are reported in most of these works. These\nlimitations might raise some concerns, such as the\nimpact of non-fallacious arguments being labelled\nas fallacious (false positives) while they are not,\njust because they share similar words or natural\nlanguage patterns. To have a better understanding\nof these cases, and the potential problems of rely-\ning only in deep learning algorithms for addressing\na complex problem such as the identification of nat-\nural language fallacies, the argumentation scheme\nmodel of arguments presents itself as a promising\nalternative to the models considered in the litera-\nture.\n3 Data\nIn order to validate our hypothesis and to provide\nan evaluation outside of the training domain, we\ndecided to use two different corpora in our exper-\niments. First, the fallacy detection corpus, which\nconsists of a partial combination of the data de-\nscribed in (Sahai et al., 2021) and (Goffredo et al.,\n2022). Second, the argumentation scheme valida-\ntion dataset, a small collection of natural language\nargumentation schemes that we created in this work\nin order to evaluate the inferences done by the pre-\ndictive models to detect a natural language fallacy\noutside of the domains considered during train-\ning. With this second dataset, it is our objective\n4\nto observe how well the model generalises when\ndetecting natural language fallacies following a dif-\nferent model or structure than the one considered\nin the data used for training, similar to what would\nhappen when deploying the predictive models in\nthe Wild.\nAs depicted in Table 1, the fallacy detection\ncorpus used in this work consists of four fallacy\nclasses and the non-fallacious class. We selected\nthe fallacy classes of Appeal to Authority, Appeal\nto Majority, Slippery Slope and Ad Hominem since\nthey represent the majority of the natural language\nfallacies commonly used in human dialogues and\ndebates.\nSince the annotation in both corpora was based\non similar fallacy theory, our fallacy detection cor-\npus combines some of the natural languages falla-\ncies annotated in U.S. presidential debates (Gof-\nfredo et al., 2022), with some others annotated in\nsocial media discussions (Sahai et al., 2021) and\nthe non-fallacious class. The decision of combin-\ning both corpora is twofold. First, we wanted to\naddress the automatic detection of natural language\nfallacies (not just classifying them as done in (Gof-\nfredo et al., 2022)) so non-fallacious samples were\nneeded. The assumption done in (Goffredo et al.,\n2022) of knowing beforehand that some piece of\nnatural language is fallacious represents a signifi-\ncant limitation of the contribution since knowing\nthe fallacious condition of an input is not trivial,\nand represents an important challenge in the area.\nThe second reason to combine both corpora is to\nhave a more balanced distribution of samples when\ncomparing fallacious to non-fallacious samples,\nand to expand the natural language domains in\nwhich fallacies can be observed during training.\nA sample in our fallacy detection corpus con-\nsists of a short snippet of text where the fallacious\n(or not) reasoning has been identified, a natural\nlanguage context in which the fallacy has been de-\ntected (a paragraph in the case of the debates, and\nthe previous comment of the text snippet in the case\nof the social media discussions), and the annotated\nlabel. In order to homogenise the natural language\ncontext in data belonging to both corpora, for the\nsamples extracted from the debate corpus we con-\nsidered as the context only the sentences before\nand after the text snippet.\nAimed at validating the performance of ma-\nchine learning and deep learning systems to detect\nnatural language fallacies, we developed a small\nNatural Language Input\nFallacy IdentifierMulti-class\nFallacy Classifier\nFigure 1: Multi-class and cascaded approaches.\ndataset containing natural language argumentation\nschemes (Walton et al., 2008). In this dataset, we\nincluded seven different types of argumentation\nschemes matching the fallacy classes included in\nthe fallacy detection corpus: Argument from Ex-\npert Opinion (AFEO), Argument from Position to\nKnow (AFPK), Argument from Popular Practice\n(AFPP), Argument from Popular Opinion (AFPO),\nSlippery Slope Argumentation Scheme (SSAS),\nGeneric Ad Hominem (GAH), and Circumstantial\nAd Hominem (CAH). This way, we can easily re-\nlate each argumentation scheme with one of the\nfour fallacy classes included in the fallacy detec-\ntion corpus, the Appeal to Authority with AFEO\nand AFPO, the Appeal to Majority with AFPP and\nAFPO, the Slippery Slope with the SSAS, and the\nAd Hominem with the GAH and CAH. It is impor-\ntant to remember that, argumentation schemes are\nnot fallacious by definition as they are the fallacy\nclasses used to annotate previous corpora, but they\ncan only be considered as fallacious if and only if\nsome of the critical questions cannot be success-\nfully answered. Taking this into consideration, in\nour argumentation scheme validation dataset, we\nincluded two natural language instances of each\nscheme, one in which all the critical questions can\nbe answered (i.e., valid reasoning), and another that\nfails in some aspect (i.e., fallacy). Therefore, our\nargumentation scheme validation dataset consists\nof fourteen natural language arguments specifically\ndesigned to validate the inference process of the\npredictive models in the task of automatically de-\ntecting natural language fallacies. These natural\nlanguage argumentation schemes have been com-\npiled in Table 2.\n5\nSamples Appeal to Authority Appeal to Majority Slippery Slope Ad Hominem Fallacy Total Non-fallacious\n(Sahai et al., 2021) 212 196 228 - 636 1650\n(Goffredo et al., 2022) 208 - 48 146 402 -\nTotal 420 196 276 146 1038 1650\nTable 1: Class distribution of the fallacy detection corpus.\n4 Experiments\n4.1 Method\nTo extend the experimental results previously re-\nported in the literature, we consider the two dif-\nferent approaches to the automatic detection of\nargumentative fallacies depicted in Figure 1. First,\nwe consider a multi-class classification problem in\nwhich fallacy classes and the non-fallacious class\nare considered in the same level. In this case, we\nwill be facing a five-class classification problem.\nSecond, we consider a cascaded approach in which\nwe first try to discriminate fallacies from valid rea-\nsoning. For that purpose, we combine a two-class\nclassifier in charge of detecting fallacies, with a\nfour-class classification model that determines the\nspecific type of the fallacy (i.e., Authority, Majority,\nSlipepry Slope, and Ad Hominem).\n4.2 Experimental Setup\nIn our experiments, we have considered three dif-\nferent implementations of the fallacy classifiers\nproposed in our method. Aimed at covering some\nof the state-of-the-art general approaches in NLP,\nwe used a Support Vector Machine combined with\nnatural language embeddings (eSVM), a fine-tuned\nRoBERTa for sequence classification, and zero-\nshot prompting GPT-3.5- TURBO and GPT-4 with-\nout any additional training. We also considered two\nversions of each input in our experiments: (i) we\nused as our input the text snippet only, and (ii) we\ncombined the snippet with its context.\nRegarding the eSVM, the best results were ob-\ntained with the radial basis function kernel, a\ngamma equal to one divided by the number of fea-\ntures, and C equal to 1000. On the other hand, for\nfine-tuning the RoBERTa model, we trained the\nmodel for 20 epochs with a learning rate of 1e-5\nand a weight decay of 0.01. Finally, the prompt\nused in our experiments with GPT-3.5- TURBO and\nGPT-4 to automatically detect and classify natural\nlanguage fallacies was designed in three sequential\nmessages as follows:\n▶ You task is to detect a fallacy in the Text Snip-\npet. The label can be “Slippery Slope”, “Ap-\npeal to Authority”, “Ad Hominem”, “Appeal\nto Majority” or “None”.\n▶ Text Snippet: [SAMPLE]\n▶ Label:\nThe first paragraph of the prompt was adapted\nfor each of the different situations proposed in our\nmethod. For example by removing “None” for\nfallacy classification (4-class), and grouping the\nfallacy labels into “Fallacy” for fallacy identifica-\ntion (2-class).\nIn all of our experiments, we considered an\n80-10-10 split of our data into train, develop-\nment, and test respectively. Furthermore, we\nremoved all the duplicated text snippets from\nthe U.S. presidential debates corpus to prevent\nthe occurrence of the same natural language\nsnippets in train and test at the same time, as\nhappened in the experiments reported in (Gof-\nfredo et al., 2022). The best performing hyper-\nparameters described above were selected based\non the best performance in the development split.\nThe code and the data used in our experiments\ncan be publicly accessed athttps://github.com/\nraruidol/ArgumentMining23-Fallacy.\n5 Results\nWe have grouped the analysis of our results into\ntwo sections. First, we evaluate our models on the\ntest split of the fallacy detection corpus. Second,\nwe evaluate these same models when used to de-\ntect or classify fallacies in the Wild (i.e., outside\nof the training/testing data domain), for which pur-\npose we use the argumentation scheme validation\ndataset.\n5.1 Experimental Evaluation\nRegarding the experimental evaluation, we mea-\nsured the performance of the models by calculating\nthe precision, recall, and macro f1 of the predic-\ntions done over the test samples. Table 3 contains\nthe results of the multi-class classification exper-\niments, Table 4 contains the results of the fallacy\n6\nArg. Scheme CQs Natural Language Argumentation Schemes\nAFEO ✓\nMajor Premise: “Prof Whittaker is a professor of virology at the Cornell University College”\nMinor Premise: “Prof Whittaker said that viruses can be spread by sneezing”\nConclusion: “Viruses can be spread by sneezing”\nAFEO ✗\nMajor Premise: “Stephen Hawking was an expert on AI”\nMinor Premise: “Stephen Hawking said that AI could spell the end of the human race”\nConclusion: “AI could spell the end of the human race”\nAFPK ✓\nMajor Premise: “Alice lives in New York”\nMinor Premise: “Alice says that New York City Hall is in Lower Manhattan”\nConclusion: “New York City Hall is in Lower Manhattan”\nAFPK ✗\nMajor Premise: “David is a cab driver in London”\nMinor Premise: “David says that the best way to get to Tower Bridge is by cab”\nConclusion: “The best way to get to Tower Bridge is by cab”\nAFPP ✓\nMajor Premise: “Most people wear black clothes at a funeral”\nMinor Premise: “If most people wear black clothes at a funeral, that is acceptable to do”\nConclusion: “It is acceptable to wear black clothes at a funeral”\nAFPP ✗\nMajor Premise: “Most people drive at least 10 miles per hour over the speed limit”\nMinor Premise: “If most people drive at least 10 miles per hour over the speed...\n...limit, that is acceptable to do”\nConclusion: “It is acceptable to drive at least 10 miles per hour over the speed limit”\nAFPO ✓\nGeneral Acceptance Premise: “The majority of climate scientists agree that humans...\n...are causing global warming and climate change”\nPresumption Premise: “If the majority of climate scientists agree that humans...\n...are causing global warming and climate change, there is a reason to believe that is true”\nConclusion: “There is reason to believe that humans...\n...are causing global warming and climate change”\nAFPO ✗\nGeneral Acceptance Premise: “The majority of people we asked agreed that the Earth may be flat ”\nPresumption Premise: “If the majority of people we asked agreed that the Earth...\n...may be flat, there is a reason to believe that is true”\nConclusion: “There is reason to believe that the Earth may be flat”\nSSAS ✓\nFirst Step Premise: “I should go out with my friends rather than study for the exam”\nRecursive Premise: “If I don’t pass the exam, this might affect my GPA, which...\n...in turn might impact my chances of going to a good college”\nBad Outcome Premise: “Not going to a good college would be a disaster”\nConclusion: “I should not go out with my friends rather than study for the exam”\nSSAS ✗\nFirst Step Premise: “We should lower the legal drinking age from 21 to 18 in line with other countries”\nRecursive Premise: “If we lower it to 18, next it will be 17, then 16, 15, etc. ”\nBad Outcome Premise: “If we lower the legal drinking age, we’ll have ten-year-olds getting drunk in bars!”\nConclusion: “We should not lower the legal drinking age ”\nGAH ✓ Character Attack Premise: “Steve has cheated on a number of past exams”\nConclusion: “We should doubt Steve’s claim that someone else copied his work in this exam”\nGAH ✗ Character Attack Premise: “The CEO was convicted of a DUI in college”\nConclusion: “We should doubt the CEO’s sales report”\nCAH ✓\nArgument Premise: “The car salesman argued that I should buy a gas car because...\n...they are more reliable than electric cars”\nInconsistent Commitment Premise: “The car salesman chose to drive an electric car”\nCredibility Questioning Premise: “The car salesman is not credible in this case”\nConclusion: “The car salesman’s argument that I should buy a gas car is not valid”\nCAH ✗\nArgument Premise: “Mark argued that you should not take illegal drugs as they can have dangerous side effects”\nInconsistent Commitment Premise: “Mark has taken illegal drugs in the past”\nCredibility Questioning Premise: “Mark is not credible in this case”\nConclusion: “Mark’s argument that you should not take illegal drugs is not valid”\nTable 2: Argumentation Scheme validation dataset. A ( ✓) indicates that the argument successfully answers its\ncritical questions. A (✗) indicates that some of the critical questions cannot be successfully answered and thus, the\nargument is a fallacy.\n7\nModel Precision Recall Macro-F1\nRB 21.6 24.6 18.6\neSVM 68.3 55.8 60.3\nRoBERTa 68.2 65.3 66.5\nGPT-3.5-TURBO 59.0 46.2 45.5\nGPT-4 53.5 55.0 51.7\neSVM+[ctx] 67.3 50.0 54.4\nRoBERTa+[ctx] 62.0 58.4 59.9\nGPT-3.5-TURBO+[ctx] 50.2 32.1 35.8\nGPT-4+[ctx] 54.4 51.2 50.8\nTable 3: Precision, Recall and Macro-F1 results of the\n5-class fallacy detection task. [ctx] represents the con-\ntextual information added to the input of each model.\ndetection (i.e., 2-class classification) experiments,\nand Table 5 contains the results of the fallacy (i.e.,\n4-class) classification experiments. We have also\nincluded the random baseline (RB) in order to rel-\nativise the results with respect to the class com-\nplexity of each instance of the task. From all these\nresults, we have identified two interesting patterns.\nFirst of all, for a corpus of this size (i.e., ∼2000\nsamples) and distribution, the best results were\nconsistently achieved by fine-tuning the RoBERTa\narchitecture. The eSVM model performed slightly\nworse and the worst performing approach was the\nzero-shot prompts for the GPT-3.5- TURBO and\nGPT-4 model. It is important to mention that in\nthe zero-shot prompting experiments, no parame-\nters were specifically fine-tuned for our data, and\ntaking this into account, the results were surpris-\ningly good compared to a random or a majority\nbaseline. Furthermore, we could observe an im-\nportant difference between GPT-3.5- TURBO and\nGPT-4 when prompted to detect and classify fal-\nlacies in natural language. We found out that in\nall of the fallacy detection and classification tasks\nGPT-4 significantly outperformed GPT-3.5- TURBO .\nSpecifically in the cascaded approach, GPT-4 was\nable to outperform GPT-3.5- TURBO in more than a\n20% with respect to macro F1 reaching a maximum\nimprovement of a 58% in the fallacy classification\ntask. After removing the negative samples, theGPT-\n4 model is able to focus on more relevant linguistic\naspects of the text snippets than its predecessor,\nresulting in a significant improvement in this task\n(see Table 5). Finally, we were also able to observe\nthat in general, better results were achieved by the\ncascaded approach. Therefore, when addressing\na fallacy identification problem, given the linguis-\ntic complexity of this task, it is better to do it by\nseparating the detection and the classification than\ndoing both tasks at the same time.\nThe second pattern that we were able to observe\nis that, including the context as we did in our ex-\nperiments was not helpful at all. Adding more\ncontextual information to the text snippet resulted\nin redundant information that made the task more\ndifficult for the predictive models. Given the gen-\neralised bad performance of the models when just\nincluding the adjacent text of the snippet to the in-\nput, we consider that argumentative context should\nbe brought into consideration from a different per-\nspective (e.g., explicitly modelling the underlying\nreasoning of the argument). Since the detection\nof fallacious reasoning is a task that involves the\nanalysis of finer grained reasoning and logical as-\npects of natural language, it might be a better idea\nto support the natural language input with some\nstructural and argumentative features in the line of\nwhat was proposed in (Jin et al., 2022), rather than\njust including the adjacent text. However, we could\nnot integrate such features in our experiments since\npart of the fallacy detection corpus did not contain\nsuch annotations. Finally, we would also like to\npoint out that from the consistent drop of perfor-\nmance observed between all of our experiments\nwith and without context, the development of an\neffective segmentation algorithm that focuses on\nthe relevant linguistic aspects of the text is of ut-\nmost importance when addressing a high linguistic\ncomplexity task such as the automatic detection of\nargumentative fallacies.\n5.2 Evaluation in the Wild\nIn order to validate the behaviour of these models\nwhen making predictions outside of the training do-\nmains, we have used the validation dataset created\non the basis of the argumentation scheme model\nof argument (see Table 2). For this validation in\nthe Wild, we have selected the best model of the ex-\nperimental evaluation considering both fine-tuning\nand prompt-based models independently. As de-\npicted in Table 6, we have evaluated the RoBERTa\nand GPT-4 models considering both the multi-class\nand the fallacy identification tasks (i.e., 5-class and\n2-class classification problems respectively) pro-\nposed at the beginning of this paper.\nFirstly, looking at the 5-class classification re-\n8\nModel Precision Recall Macro-F1\nRB 47.1 47.0 46.4\neSVM 77.8 77.5 77.7\nRoBERTa 79.8 79.6 79.6\nGPT-3.5-TURBO 41.7 46.2 40.6\nGPT-4 53.2 53.2 51.1\neSVM+[ctx] 76.8 74.0 74.8\nRoBERTa+[ctx] 78.0 78.8 78.3\nGPT-3.5-TURBO+[ctx] 47.1 48.8 43.5\nGPT-4+[ctx] 56.6 56.7 54.1\nTable 4: Precision, Recall and Macro-F1 results of the\n2-class fallacy detection task. [ctx] represents the con-\ntextual information added to the input of each model.\nModel Precision Recall Macro-F1\nRB 22.9 22.1 22.4\neSVM 69.6 65.5 67.1\nRoBERTa 75.4 78.0 76.2\nGPT-3.5-TURBO 51.7 46.4 44.6\nGPT-4 60.4 60.0 58.3\neSVM+[ctx] 79.7 72.1 74.8\nRoBERTa+[ctx] 72.3 72.6 72.3\nGPT-3.5-TURBO+[ctx] 45.9 38.1 35.1\nGPT-4+[ctx] 58.7 57.0 55.7\nTable 5: Precision, Recall and Macro-F1 results of the\n4-class fallacy classification task. [ ctx] represents the\ncontextual information added to the input of each model.\nArg. Scheme CQsRoBERTa GPT-45-class 2-class 5-class 2-class\nAFEO ✓ Authority Fallacy None None\nAFEO ✗ Authority Fallacy Authority Fallacy\nAFPK ✓ None Fallacy None None\nAFPK ✗ Authority Fallacy Authority Fallacy\nAFPP ✓ None Fallacy Majority None\nAFPP ✗ None Fallacy Majority Fallacy\nAFPO ✓ Majority Fallacy Authority None\nAFPO ✗ Majority Fallacy Majority Fallacy\nSSAS ✓ None Fallacy Slippery Slope None\nSSAS ✗ Slippery Slope Fallacy Slippery Slope Fallacy\nGAH ✓ None None Ad Hominem Fallacy\nGAH ✗ Ad Hominem Fallacy Ad Hominem Fallacy\nCAH ✓ None None Ad Hominem Fallacy\nCAH ✗ None None Ad Hominem Fallacy\nTable 6: Evaluation in the Wild of the fallacy detection\nLLMs.\nsults, we can observe different behaviour between\nRoBERTa and GPT-4. In the case of RoBERTa, it\nfailed to distinguish the fallacious aspects of the un-\nderlying logic of four argumentation schemes. We\ncan see this problem with both AFEO that are clas-\nsified as an authority fallacy, both AFPP that are\nclassified as non-fallacious while both AFPO are\nlabelled as an appeal to majority fallacy, and both\nCAH that are classified as non-fallacious. This be-\nhaviour can be attributed to the fact that they look\ntoo similar to the samples labelled as fallacious (in\nthe case of AFEO and AFPO) or non-fallacious\n(in the case of AFPP and CAH) in the training cor-\npora. Only for three out of the seven argumentation\nschemes was the model able to correctly distinguish\nbetween fallacious and non-fallacious instances of\nthe same scheme, this is the case of AFPK, SSAS,\nand GAH. Differently, GPT-4 only managed to cor-\nrectly distinguish between an instance of the same\nargumentation scheme being fallacious or not in\nthe AFEO and AFPO. All the rest of the argumenta-\ntion schemes were labelled as fallacious belonging\nto each of its respective fallacy classes. It is inter-\nesting to mention that GPT-4 also failed to identify\nthe fallacy type in the valid AFPO, since the word\n“scientist” appeared, the model predicted that it was\nan appeal to authority fallacy, being it not a fallacy\nand being structured as a popular opinion scheme,\nmeaning that the authority was not a relevant aspect\nin the argumentative reasoning.\nSecondly, looking at the 2-class classification\nresults, the observed behaviour between RoBERTa\nand GPT-4 was also significantly different. In the\ncase of RoBERTa, except for the Ad Hominem\nschemes, all the other argumentation schemes were\nlabelled as fallacious regardless of their logic. The\nmodel was also not able to correctly discriminate a\nfallacy in the case of CAH arguments, where both\nof them were labelled as non-fallacious. Only the\nnatural language GAH schemes were correctly dis-\ncriminated between fallacious or not. On the other\nhand, GPT-4 performed surprisingly well in this\ninstance of the task. All the schemes apart from\nthe Ad Hominem ones were correctly classified as\nfallacious or not. However, both GAH and CAH\nschemes were labelled as fallacious, regardless of\nthe actual reasons (e.g., critical questions) of being\nfallacious.\n6 Discussion\nIn this paper, we present the first analysis of the lim-\nitations of approaching the fallacy detection prob-\n9\nlem with LLMs. For that purpose, we provide a\nnew viewpoint to the existing work done in the\nautomatic identification of natural language falla-\ncies through the use of the argumentation scheme\nmodel of arguments. The argumentation scheme\nmodel allows us to partially dissociate the logic\nof the argument from the natural language of it,\nevidencing the limitation that LLMs have when\nused to approach complex natural language tasks\nwhere logical reasoning is involved. For that pur-\npose, we first ran a set of experiments training a\nmachine learning and a deep learning algorithm\nplus prompting two LLMs on existing annotated\ncorpora for fallacy identification, resulting in new\nbaselines for this task. Second, we evaluated the\nbest performing models on a specifically created ar-\ngumentation scheme validation dataset that helped\nus to understand how well were these models able\nto identify fallacies based on the logic of the argu-\nment rather than over-fitting to a natural language\npattern not relevant for the definition of a fallacy.\nFrom our findings we have been able to observe\nthat there is still much more work to do in this area,\nand that relying exclusively on LLMs to approach\nsuch a challenging task in the Wild may not be the\nbest option.\nAcknowledgements\nThis work has been supported by the ‘AI for Citi-\nzen Intelligence Coaching against Disinformation\n(TITAN)’ project, funded by the EU Horizon 2020\nresearch and innovation programme under grant\nagreement 101070658, and by UK Research and\ninnovation under the UK governments Horizon\nfunding guarantee grant numbers 10040483 and\n10055990.\nReferences\nTariq Alhindi, Tuhin Chakrabarty, Elena Musi, and\nSmaranda Muresan. 2022. Multitask instruction-\nbased prompting for fallacy recognition. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 8172–8187,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nAristotle. 1978. De Sophisticis Elenchis (On Sophistical\nRefutations). Harvard University Press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nG Da San Martino, Alberto Barrón-Cedeno, Henning\nWachsmuth, Rostislav Petrov, and Preslav Nakov.\n2020. Semeval-2020 task 11: Detection of propa-\nganda techniques in news articles. arXiv preprint\narXiv:2009.02696.\nGiovanni Da San Martino, Seunghak Yu, Alberto\nBarrón-Cedeno, Rostislav Petrov, and Preslav Nakov.\n2019. Fine-grained analysis of propaganda in news\narticle. In Proceedings of the 2019 conference on\nempirical methods in natural language processing\nand the 9th international joint conference on natu-\nral language processing (EMNLP-IJCNLP), pages\n5636–5646.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nPierpaolo Goffredo, Shohreh Haddadan, V orakit V orak-\nitphan, Elena Cabrio, and Serena Villata. 2022. Fal-\nlacious argument classification in political debates.\nIn Thirty-First International Joint Conference on Ar-\ntificial Intelligence {IJCAI-22}, International Joint\nConferences on Artificial Intelligence Organization,\npages 4143–4149.\nIvan Habernal, Raffael Hannemann, Christian Pol-\nlak, Christopher Klamm, Patrick Pauli, and Iryna\nGurevych. 2017. Argotario: Computational argu-\nmentation meets serious games. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 7–12.\nIvan Habernal, Patrick Pauli, and Iryna Gurevych.\n2018a. Adapting serious game for fallacious argu-\nmentation to german: Pitfalls, insights, and best prac-\ntices. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nIvan Habernal, Henning Wachsmuth, Iryna Gurevych,\nand Benno Stein. 2018b. Before name-calling: Dy-\nnamics and triggers of ad hominem fallacies in web\nargumentation. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n386–396.\nCharles L Hamblin. 1970. Fallacies. Advanced Rea-\nsoning Forum.\nZhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu\nShen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan,\nRada Mihalcea, and Bernhard Schoelkopf. 2022.\nLogical fallacy detection. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\npages 7180–7198, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\n10\nJames Willard Oliver. 1967. Formal fallacies and other\ninvalid arguments. Mind, 76(304):463–478.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSaumya Sahai, Oana Balalau, and Roxana Horincar.\n2021. Breaking down the invisible wall of informal\nfallacies in online discussions. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 644–657.\nFrans H Van Eemeren and Rob Grootendorst. 1984.\nSpeech acts in argumentative discussions. Dordrecht:\nForis Publications.\nFrans H Van Eemeren and Rob Grootendorst. 2016.\nArgumentation, communication, and fallacies: A\npragma-dialectical perspective. Routledge.\nJacky Visser, John Lawrence, and Chris Reed. 2020.\nReason-checking fake news. Communications of the\nACM, 63(11):38–40.\nHenning Wachsmuth, Nona Naderi, Ivan Habernal, Yu-\nfang Hou, Graeme Hirst, Iryna Gurevych, and Benno\nStein. 2017. Argumentation quality assessment: The-\nory vs. practice. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 250–255.\nDouglas Walton, Christopher Reed, and Fabrizio\nMacagno. 2008. Argumentation schemes. Cam-\nbridge University Press.\nDouglas N Walton. 1987. Informal fallacies, volume 4.\nJohn Benjamins Publishing.\nDouglas N. Walton. 1995. A pragmatic theory of fallacy.\nStudies in rhetoric and communication. University of\nAlabama Press, Tuscaloosa.",
  "topic": "Argumentative",
  "concepts": [
    {
      "name": "Argumentative",
      "score": 0.8453955054283142
    },
    {
      "name": "Computer science",
      "score": 0.8113890886306763
    },
    {
      "name": "Argumentation theory",
      "score": 0.7220920324325562
    },
    {
      "name": "Natural language",
      "score": 0.655243992805481
    },
    {
      "name": "Identification (biology)",
      "score": 0.6352549195289612
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5880563259124756
    },
    {
      "name": "Natural language processing",
      "score": 0.5842947959899902
    },
    {
      "name": "Task (project management)",
      "score": 0.5827823281288147
    },
    {
      "name": "Point (geometry)",
      "score": 0.5450842380523682
    },
    {
      "name": "Language identification",
      "score": 0.5282644033432007
    },
    {
      "name": "Language understanding",
      "score": 0.4941387474536896
    },
    {
      "name": "Natural language understanding",
      "score": 0.4790017306804657
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4785673916339874
    },
    {
      "name": "Data science",
      "score": 0.369179368019104
    },
    {
      "name": "Linguistics",
      "score": 0.15479102730751038
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177639307",
      "name": "University of Dundee",
      "country": "GB"
    }
  ]
}