{
  "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers",
  "url": "https://openalex.org/W3176153963",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1658219663",
      "name": "Yang Jianwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225388594",
      "name": "Li, Chunyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370394005",
      "name": "Zhang, Pengchuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222155691",
      "name": "Dai, Xiyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1585946068",
      "name": "Xiao, Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101029194",
      "name": "Yuan Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao, Jianfeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3167695527",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W3110402800",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2991062542",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3173631098",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2990248977",
    "https://openalex.org/W3138006590",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3164823914",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3035087309",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3175466730",
    "https://openalex.org/W3145586615",
    "https://openalex.org/W2986357608",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2546696630",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3165150763",
    "https://openalex.org/W3162696321",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W3171660447",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W3119588134",
    "https://openalex.org/W2982103617",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3138486308",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W3150177490",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3111156583",
    "https://openalex.org/W1538131130",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3015468748"
  ],
  "abstract": "Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). In this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal self-attention, we propose a new variant of Vision Transformer models, called Focal Transformer, which achieves superior performance over the state-of-the-art vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8 Top-1 accuracy, respectively, on ImageNet classification at 224x224 resolution. Using Focal Transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art Swin Transformers for 6 different object detection methods trained with standard 1x and 3x schedules. Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.",
  "full_text": "Focal Self-attention for Local-Global Interactions in\nVision Transformers\nJianwei Yang1 Chunyuan Li1 Pengchuan Zhang1 Xiyang Dai2 Bin Xiao2\nLu Yuan2 Jianfeng Gao1\n1Microsoft Research at Redmond, 2Microsoft Cloud + AI\n{jianwyan,chunyl,penzhan,xidai,bixi,luyuan,jfgao}@microsoft.com\nAbstract\nRecently, Vision Transformer and its variants have shown great promise on various\ncomputer vision tasks. The ability of capturing short- and long-range visual depen-\ndencies through self-attention is the key to success. But it also brings challenges\ndue to quadratic computational overhead, especially for the high-resolution vision\ntasks (e.g., object detection). Many recent works have attempted to reduce the\ncomputational and memory cost and improve performance by applying either\ncoarse-grained global attentions or Ô¨Åne-grained local attentions. However, both\napproaches cripple the modeling power of the original self-attention mechanism of\nmulti-layer Transformers, thus leading to sub-optimal solutions. In this paper, we\npresent focal self-attention, a new mechanism that incorporates both Ô¨Åne-grained\nlocal and coarse-grained global interactions. In this new mechanism, each token\nattends its closest surrounding tokens at Ô¨Åne granularity and the tokens far away\nat coarse granularity, and thus can capture both short- and long-range visual de-\npendencies efÔ¨Åciently and effectively. With focal self-attention, we propose a new\nvariant of Vision Transformer models, calledFocal Transformer, which achieves\nsuperior performance over the state-of-the-art (SoTA) vision Transformers on a\nrange of public image classiÔ¨Åcation and object detection benchmarks. In particular,\nour Focal Transformer models with a moderate size of 51.1M and a larger size\nof 89.8M achieve 83.5% and 83.8% Top-1 accuracy, respectively, on ImageNet\nclassiÔ¨Åcation at 224 √ó224. When employed as the backbones, Focal Transform-\ners achieve consistent and substantial improvements over the current SoTA Swin\nTransformers [44] across 6 different object detection methods. Our largest Focal\nTransformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO\nmini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating\nnew SoTA on three of the most challenging computer vision tasks.\n1 Introduction\nNowadays, Transformer [60] has become a prevalent model architecture in natural language pro-\ncessing (NLP) [ 22, 6]. In the light of its success in NLP, there is an increasing effort on adapt-\ning it to computer vision (CV) [ 48, 51]. Since its promise Ô¨Årstly demonstrated in Vision Trans-\nformer (ViT) [23], we have witnessed a Ô¨Çourish of full-Transformer models for image classiÔ¨Åca-\ntion [57, 63, 67, 44, 80, 59], object detection [ 9, 91, 84, 20] and semantic segmentation [ 61, 65].\nBeyond these static image tasks, it has also been applied on various temporal understanding tasks,\nsuch as action recognition [41, 83, 11], object tracking [15, 62], scene Ô¨Çow estimation [39].\nIn Transformers, self-attention is the key component making it unique from the widely used con-\nvolutional neural networks (CNNs) [38]. At each Transformer layer, it enables the global content-\ndependent interactions among different image regions for modeling both short- and long-range\nPreprint. Under review.\narXiv:2107.00641v1  [cs.CV]  1 Jul 2021\nHead 2\nHead 1\nHead 3\n2\nQuery Patch\n1\n2\n3\nFine\nCoarse\nClose\nFar\nAttended Patches\n1\n3\nFigure 1: Left: Visualization of the attention maps of the three heads at the given query patch (blue)\nin the Ô¨Årst layer of the DeiT-Tiny model [57]. Right: An illustrative depiction of focal self-attention\nmechanism. Three granularity levels are used to compose the attention region for the blue query.\ndependencies. Through the visualization of full self-attentions1, we indeed observe that it learns to\nattend local surroundings (like CNNs) and the global contexts at the same time (See the left side of\nFig. 1). Nevertheless, when it comes to high-resolution images for dense predictions such as object\ndetection or segmentation, a global and Ô¨Åne-grained self-attention becomes non-trivial due to the\nquadratic computational cost with respect to the number of grids in feature maps. Recent works\nalternatively exploited either a coarse-grained global self-attention [63, 67] or a Ô¨Åne-grained local\nself-attention [44, 80, 59] to reduce the computational burden. However, both approaches cripple the\npower of the original full self-attention i.e., the ability to simultaneously model short- and long-range\nvisual dependencies, as demonstrated on the left side of Fig. 1.\nIn this paper, we present a new self-attention mechanism to capture both local and global interactions\nin Transformer layers for high-resolution inputs. Considering that the visual dependencies between\nregions nearby are usually stronger than those far away, we perform the Ô¨Åne-grained self-attention\nonly in local regions while the coarse-grained attentions globally. As depicted in the right side of\nFig. 1, a query token in the feature map attends its closest surroundings at the Ô¨Ånest granularity as itself.\nHowever, when it goes to farther regions, it attends to summarized tokens to capture coarse-grained\nvisual dependencies. The further away the regions are from the query, the coarser the granularity is\nAs a result, it can effectively cover the whole high-resolution feature maps while introducing much\nless number of tokens in the self-attention computation than that in the full self-attention mechanism.\nAs a result, it has the ability to capture both short- and long-range visual dependencies efÔ¨Åciently.\nWe call this new mechanism focal self-attention, as each token attends others in a focal manner.\nBased on the proposed focal self-attention, a series of Focal Transformer models are developed, by 1)\nexploiting a multi-scale architecture to maintain a reasonable computational cost for high-resolution\nimages [63, 67, 44, 80], and 2) splitting the feature map into multiple windows in which tokens share\nthe same surroundings, instead of performing focal self-attention for each token [59, 80, 44].\nWe validate the effectiveness of the proposed focal self-attention via a comprehensive empirical\nstudy on image classiÔ¨Åcation, object detection and segmentation. Results show that our Focal\nTransformers with similar model sizes and complexities consistently outperform the SoTA Vision\nTransformer models across various settings. Notably, our small Focal Transformer model with\n51.1M parameters can achieve 83.5% top-1 accuracy on ImageNet-1K, and the base model with\n89.8M parameters obtains 83.8% top-1 accuracy. When transferred to object detection, our Focal\nTransformers consistently outperform the SoTA Swin Transformers [ 44] for six different object\ndetection methods. Our largest Focal Transformer model achieves 58.9 box mAP and 51.3 mask\nmAP on COCO test-dev for object detection and instance segmentation, respectively, and 55.4 mIoU\non ADE20K for semantic segmentation. These results demonstrate that the focal self-attention is\nhighly effective in modeling the local-global interactions in Vision Transformers.\n2 Method\n2.1 Model architecture\nTo accommodate the high-resolution vision tasks, our model architecture shares a similar multi-scale\ndesign with [63, 80, 44], which allows us to obtain high-resolution feature maps at earlier stages.\nAs shown in Fig. 2, an image I ‚ààRH√óW√ó3 is Ô¨Årst partitioned into patches of size 4 √ó4, resulting\n1DeiT-Tiny model, checkpoint downloaded fromhttps://github.com/facebookresearch/deit.\n2\nùêª √óùëä√ó3\nPatch Embedding\nFocal Transformer Layer √óN1\nùêª\n4 √óùëä\n4 √óùëë ùêª\n8 √óùëä\n8 √ó2ùëë ùêª\n16√ó ùëä\n16√ó4ùëë\nFocal Transformer Layer√óN2\nùêª\n32√ó ùëä\n32√ó8ùëë\nFocal \nSelf-Attention\nLayerNorm\nLayerNorm\nMulti-layer \nPerceptron\nPatch Embedding\nFocal \nSelf-Attention\nLayerNorm\nLayerNorm\nMulti-layer \nPerceptron\nFocal Transformer Layer√óN3\nPatch Embedding\nFocal \nSelf-Attention\nLayerNorm\nLayerNorm\nMulti-layer \nPerceptron\nFocal Transformer Layer√óN4\nPatch Embedding\nFocal \nSelf-Attention\nLayerNorm\nLayerNorm\nMulti-layer \nPerceptron\nStage 1 Stage 2 Stage 3 Stage 4\nFigure 2: Model architecture for our Focal Transformers. As highlighted in light blue boxes, our\nmain innovation is the proposed focal self-attention mechanism in each Transformer layer.\nin H\n4 √óW\n4 visual tokens with dimension 4 √ó4 √ó3. Then, we use a patch embedding layer which\nconsists of a convolutional layer with Ô¨Ålter size and stride both equal to 4, to project these patches\ninto hidden features with dimension d. Given this spatial feature map, we then pass it to four stages\nof focal Transformer blocks. At each stage i ‚àà{1, 2, 3, 4}, the focal Transformer block consists of\nNi focal Transformer layers. After each stage, we use another patch embedding layer to reduce the\nspatial size of feature map by factor 2, while the feature dimension is increased by 2. For image\nclassiÔ¨Åcation tasks, we take the average of the output from last stage and send it to a classiÔ¨Åcation\nlayer. For object detection, the feature maps from last 3 or all 4 stages are fed to the detector head,\ndepending on the particular detection method we use. The model capacity can be customized by\nvarying the input feature dimension d and the number of focal Transformer layers at each stage\n{N1, N2, N3, N4}.\nStandard self-attention can capture both short- and long-range interactions at Ô¨Åne-grain, but it suffers\nfrom high computational cost when it performs the attention on high-resolution feature maps as noted\nin [80]. Take stage 1 in Fig. 2 as the example. For the feature map of size H\n4 √óW\n4 √ód, the complexity\nof self-attention is O((H\n4 √óW\n4 )2d), resulting in an explosion of time and memory cost considering\nmin(H, W) is 800 or even larger for object detection. In the next, we describe how we address this\nwith the proposed focal self-attention.\n2.2 Focal self-attention\n0 10 20 30 40 50 60 70 80\nNumer of T okens\n100\n101\n102\n103\nReceptive Field Size\nRegular Self-Attention\nFocal Self-Attention\nFigure 3: The size of receptive Ô¨Åeld (y-\naxis) with the increase of used tokens\n(x-axis) for standard and our focal self-\nattention. For focal self-attention, we as-\nsume increasing the window granularity\nby factor 2 gradually but no more than 8.\nNote that the y-axis is logarithmic.\nIn this paper, we propose focal self-attention to make\nTransformer layers scalable to high-resolution inputs. In-\nstead of attending all tokens at Ô¨Åne-grain, we propose\nto attend the Ô¨Åne-grain tokens only locally, but the sum-\nmarized ones globally. As such, it can cover as many\nregions as standard self-attention but with much less cost.\nIn Fig. 3, we show the area of receptive Ô¨Åeld for standard\nself-attention and our focal self-attention when we gradu-\nally add more attended tokens. For a query position, when\nwe use gradually coarser-grain for its far surroundings,\nfocal self-attention can have signiÔ¨Åcantly larger receptive\nÔ¨Åelds at the cost of attending the same number of visual\ntokens than the baseline.\nOur focal mechanism enables long-range self-attention\nwith much less time and memory cost, because it attends\na much smaller number of surrounding (summarized) tokens. In practice, however, extracting the\nsurrounding tokens for each query position suffers from high time and memory cost since we need to\nduplicate each token for all queries that can get access to it. This practical issue has been noted by a\nnumber of previous works [59, 80, 44] and the common solution is to partition the input feature map\ninto windows. Inspired by them, we resort to perform focal self-attention at the window level. Given\n3\n5√ó5\n6√ó6\n8√ó8\nLevel 2\nSub-window Pooling\nSub-window Pooling\nSub-window Pooling\n4√ó4 1√ó1\n1√ó11√ó1\n2√ó2 1√ó1\nFlatten\nFlatten\nFlatten\n‚Ä¶ ‚Ä¶‚Ä¶\n64 tokens\n36 tokens\n25 tokens\nInput feature map\nWindow partition size:\nvalue\nkey\nLinear \nProjection\nFlatten\n4√ó4\n‚Ä¶\nLinear \nProjection\n‚Ä¶\n125 tokens\n‚Ä¶\n‚Ä¶\nLinear \nProjection\n‚Ä¶\nQuery16 tokens\nMulti-head Self-Attention\nùë†ùëù = 4\nùë†ùë§1 = 1; ùë†ùëü1 = 8\nùë†ùë§2 = 2;ùë†ùëü2 = 6\nùë†ùë§3 = 4; sr3 = 5\nùëÄ = 20;ùëÅ = 20\nFigure 4: An illustration of our focal self-attention at window level. Each of the Ô¨Ånest square cell\nrepresents a visual token either from the original feature map or the squeezed ones. Suppose we have\nan input feature map of size 20 √ó20. We Ô¨Årst partition it into 5 √ó5 windows of size 4 √ó4. Take\nthe 4 √ó4 blue window in the middle as the query, we extract its surroundings tokens at multiple\ngranularity levels as its keys and values. For the Ô¨Årst level, we extract the 8 √ó8 tokens which are\nclosest to the blue window at the Ô¨Ånest grain. Then at the second level, we expand the attention region\nand pool the surrounding 2 √ó2 sub-windows, which results in 6 √ó6 pooled tokens. At the third level,\nwe attend even larger region covering the whole feature map and pool4 √ó4 sub-windows. Finally,\nthese three levels of tokens are concatenated to compute the keys and values for the 4 √ó4 = 16\ntokens (queries) in the blue window.\na feature map of x ‚ààRM√óN√ód with spatial size M √óN, we Ô¨Årst partition it into a grid of windows\nwith size sp √ósp. Then, we Ô¨Ånd the surroundings for each window rather than individual tokens. In\nthe following, we elaborate the window-wise focal self-attention.\n2.2.1 Window-wise attention\nAn illustration of the proposed window-wise focal self-attention is shown in Fig. 4. We Ô¨Årst deÔ¨Åne\nthree terms for clarity:\n‚Ä¢ Focal levels L ‚Äì the number of granularity levels we extract the tokens for our focal self-attention.\nIn Fig. 1, we show 3 focal levels in total for example.\n‚Ä¢ Focal window size sl\nw ‚Äì the size of sub-window on which we get the summarized tokens at level\nl ‚àà{1, ..., L}, which are 1, 2 and 4 for the three levels in Fig. 1.\n‚Ä¢ Focal region size sl\nr ‚Äì the number of sub-windows horizontally and vertically in attended regions\nat level l, and they are 3, 4 and 4 from level 1 to 3 in Fig. 1.\nWith the above three terms {L, sw, sr}, we can specify our focal self-attention module, proceeded in\ntwo main steps:\nSub-window pooling. Assume the input feature map x ‚ààRM√óN√ód, where M √óN are the spatial\ndimension and d is the feature dimension. We perform sub-window pooling for all L levels. For the\nfocal level l, we Ô¨Årst split the input feature map x into a grid of sub-windows with size sl\nw √ósl\nw.\nThen we use a simple linear layer fl\np to pool the sub-windows spatially by:\nxl = fl\np(ÀÜx) ‚ààR\nM\nslw\n√óN\nslw\n√ód\n, ÀÜx = Reshape(x) ‚ààR\n( M\nslw\n√óN\nslw\n√ód)√ó(sl\nw√ósl\nw)\n, (1)\nThe pooled feature maps {xl}L\n1 at different levels l provide rich information at both Ô¨Åne-grain and\ncoarse-grain. Since we set sl\nw = 1for the Ô¨Årst focal level which has the same granularity as the input\nfeature map, there is no need to perform any sub-window pooling. Considering the focal window size\nis usually very small (7 maximally in our settings), the number of extra parameters introduced by\nthese sub-window pooling are fairly negligible.\nAttention computation. Once we obtain the pooled feature maps {xl}L\n1 at all L levels, we compute\nthe query at the Ô¨Årst level and key and value for all levels using three linear projection layers fq, fk\n4\nand fv:\nQ = fq(x1), K = {Kl}L\n1 = fk({x1, ..., xL}), V = {V l}L\n1 = fv({x1, ..., xL}) (2)\nTo perform focal self-attention, we need to Ô¨Årst extract the surrounding tokens for each query token\nin the feature map. As we mentioned earlier, tokens inside a window partition sp √ósp share the same\nset of surroundings. For the queries inside the i-th window Qi ‚ààRsp√ósp√ód, we extract the sl\nr √ósl\nr\nkeys and values from Kl and V l around the window where the query lies in, and then gather the\nkeys and values from all L to obtain Ki = {K1\ni , ..., KL\ni }‚ààR s√ód and Vi = {V 1\ni , ..., VL\ni }‚ààR s√ód,\nwhere s is the sum of focal region from all levels, i.e.,, s = ‚àëL\nl=1(sl\nr)2. Note that a strict version\nof focal self-attention following Fig. 1 requires to exclude the overlapped regions across different\nlevels. In our model, we intentionally keep them in order to capture the pyramid information for the\noverlapped regions. Finally, we follow [44] to include a relative position bias and compute the focal\nself-attention for Qi by:\nAttention(Qi, Ki, Vi) =Softmax(QiKT\ni‚àö\nd\n+ B)Vi, (3)\nwhere B = {Bl}L\n1 is the learnable relative position bias. It consists of L subsets for L focal levels.\nSimilar to [44], for the Ô¨Årst level, we parameterize it to B1 ‚ààR(2sp‚àí1)√ó(2sp‚àí1), considering the\nhorizontal and vertical position range are both in [‚àísp + 1, sp ‚àí1]. For the other focal levels,\nconsidering they have different granularity to the queries, we treat all the queries inside a window\nequally and use Bl ‚ààRsl\nr√ósl\nr to represent the relative position bias between the query window and\neach of sl\nr √ósl\nr pooled tokens. Since the focal self-attention for each window is independent of\nothers, we can compute Eq. (3) in parallel. Once we complete it for the whole input feature map, we\nsend it to the MLP block for proceeding computation as usual.\n2.2.2 Complexity analysis\nWe analyze the computational complexity for the two main steps discussed above. For the input\nfeature map x ‚ààRM√óN√ód, we have M\nslw\n√óN\nslw\nsub-windows at focal level l. For each sub-window,\nthe pooling operation in Eq.1 has the complexity of O((sl\nw)2d). Aggregating all sub-windows brings\nus O((MN )d). Then for all focal levels, we have the complexity of O(L(MN )d) in total, which is\nindependent of the sub-window size at each focal level. Regarding the attention computation in Eq. 3,\nthe computational cost for a query window sp √ósp is O((sp)2 ‚àë\nl(sl\nr)2d), and O(‚àë\nl(sl\nr)2(MN )d)\nfor the whole input feature map. To sum up, the overall computational cost for our focal self-attention\nbecomes O((L + ‚àë\nl(sl\nr)2)(MN )d). In an extreme case, one can set sL\nr = 2 max(M, N)/sL\nw to\nensure global receptive Ô¨Åeld for all queries (including both corner and middle queries) in this layer.\n2.3 Model conÔ¨Åguration\nWe consider three different network conÔ¨Ågurations for our focal Transformers. Here, we simply\nfollow the design strategy suggested by previous works [63, 67, 44], though we believe there should\nbe a better conÔ¨Åguration speciÔ¨Åcally for our focal Transformers. SpeciÔ¨Åcally, we use similar design\nto the Tiny, Small and Base models in Swin Transformer [ 44], as shown in Table 1. Our models\ntake 224 √ó224 images as inputs and the window partition size is also set to 7 to make our models\ncomparable to the Swin Transformers. For the focal self-attention layer, we introduce two levels,\none for Ô¨Åne-grain local attention and one for coarse-grain global attention. Expect for the last stage,\nthe focal region size is consistently set to 13 for the window partition size of 7, which means that\nwe expand 3 tokens for each window partition. For the last stage, since the whole feature map is\n7 √ó7, the focal region size at level 0 is set to 7, which is sufÔ¨Åcient to cover the entire feature map.\nFor the coarse-grain global attention, we set its focal window size same to the window partition size\n7, but gradually decrease the focal region size to get {7, 5, 3, 1}for the four stages. For the patch\nembedding layer, the spatial reduction ratio pi for four stages are all {4, 2, 2, 2}, while Focal-Base\nhas a higher hidden dimension compared with Focal-Tiny and Focal-Small.\n3 Related work\nVision Transformers. The Vision Transformer (ViT) was Ô¨Årst introduced in [23]. It applies a standard\nTransformer encoder, originally developed for NLP [60], to encode image by analogously splitting an\n5\nOutput Size Layer Name Focal-Tiny Focal-Small Focal-Base\nstage 1\n56√ó56 Patch Embedding p1 = 4;c1 = 96 p1 = 4;c1 = 96 p1 = 4;c1 = 128\n56√ó56 Transformer\nBlock\n[ s0w,r={1,13}\ns1w,r={7,7}\n]\n√ó2\n[ s0w,r={1,13}\ns1w,r={7,7}\n]\n√ó2\n[ s0w,r={1,13}\ns1w,r={7,7}\n]\n√ó2\nstage 2\n28√ó28 Patch Embedding p2 = 2;c2 = 192 p2 = 2;c2 = 192 p2 = 2;c2 = 256\n28√ó28 Transformer\nBlock\n[ s0w,r={1,13}\ns1w,r={7,5}\n]\n√ó2\n[ s0w,r={1,13}\ns1w,r={7,5}\n]\n√ó2\n[ s0w,r={1,13}\ns1w,r={7,5}\n]\n√ó2\nstage 3\n14√ó14 Patch Embedding p3 = 2;c3 = 384 p3 = 2;c3 = 384 p3 = 2;c3 = 512\n14√ó14 Transformer\nBlock\n[ s0w,r={1,13}\ns1w,r={7,3}\n]\n√ó6\n[ s0w,r={1,13}\ns1w,r={7,3}\n]\n√ó18\n[ s0w,r={1,13}\ns1w,r={7,3}\n]\n√ó18\nstage 4\n7√ó7 Patch Embedding p4 = 2;c4 = 768 p4 = 2;c4 = 768 p4 = 2;c4 = 1024\n7√ó7 Transformer\nBlock\n[ s0w,r={1,7}\ns1w,r={7,1}\n]\n√ó2\n[ s0w,r={1,7}\ns1w,r={7,1}\n]\n√ó2\n[ s0w,r={1,7}\ns1w,r={7,1}\n]\n√ó2\nTable 1: Model conÔ¨Ågurations for our focal Transformers. We introduce three conÔ¨Ågurations Focal-\nTiny, Focal-Small and Focal-Base with different model capacities.\nimage into a sequence of visual tokens. It has demonstrated superior performance to convolutional\nneural networks (CNNs) such as the ResNet [ 34] on multiple image classiÔ¨Åcation benchmarks,\nwhen trained with sufÔ¨Åcient data [23] and careful data augmentation and regularization [57]. These\nadvancements further inspired the applications of transformer to various vision tasks beyond image\nclassiÔ¨Åcation, such as self-supervised learning [ 16, 10, 40], object detection [ 9, 91, 84, 20] and\nsemantic segmentation [61, 65, 86]. Apart from the downstream tasks, another line of work focus\non improving the original vision transformers from different perspectives, such as data-efÔ¨Åcient\ntraining [57], improved patch embedding/encoding [18, 75, 32], integrating convolutional projections\ninto transformers [67, 74], multi-scale architectures and efÔ¨Åcient self-attention mechanisms for high-\nresolution vision tasks [63, 67, 44, 80, 17]. We refer the readers to [37, 31, 37] for comprehensive\nsurveys. This paper focuses on improving the general performance of vision transformer with the\nproposed focal self-attention mechanism. In the following, we particularly discussed the most related\nworks regarding attention mechanisms.\nEfÔ¨Åcient global and local self-attention . Transformer models usually need to cope with a large\nnumber of tokens, such as long documents in NLP and high-resolution images in CV . Recently,\nvarious efÔ¨Åcient self-attention mechanisms are proposed to overcome the quadratic computational\nand memory cost in the vanilla self-attention. On one hand, a number of works in both NLP and\nCV resort to coarse-grained global self-attention by attending the downsampled/summarized tokens,\nwhile preserving the long-range interactions [50, 47, 63, 67, 32]. Though this approach can improve\nthe efÔ¨Åciency, it loses the detailed context surrounding the query tokens. On the other hand, the local\nÔ¨Åne-grained attention, i.e., attending neighboring tokens within a constant window size, is another\nsolution for both language [3, 78, 1] and vision [59, 44, 80]. In this paper, we argue that both types of\nattentions are important and the full-attention ViT models indeed have learned both of them, as shown\nin Fig. 1 left. This is also supported by the recent advanced CNN models [36, 66, 64, 71, 2, 8, 52],\nwhich showed that global attention or interaction can effectively improve the performance. Our\nproposed focal self-attention is the Ô¨Årst to reconcile the global and local self-attention in a single\ntransformer layer. It can capture both local and global interactions as vanilla full attention but in more\nefÔ¨Åcient and effective way, particularly for high-resolution inputs.\n4 Experiments\n4.1 Image classiÔ¨Åcation on ImageNet-1K\nWe compare different methods on ImageNet-1K [21]. For fair comparison, we follow the training\nrecipes in [ 57, 63]. All models are trained for 300 epochs with a batch size 1024. The initial\nlearning rate is set to 10‚àí3 with 20 epochs of linear warm-up starting from 10‚àí5. For optimization,\nwe use AdamW [ 45] as the optimizer with a cosine learning rate scheduler. The weight decay\nis set to 0.05 and the maximal gradient norm is clipped to 5.0. We use the same set of data\naugmentation and regularization strategies used in [57] after excluding random erasing [87], repeated\n6\nModel #Params. FLOPs Top-1 (%)\nResNet-50 [34] 25.0 4.1 76.2\nDeiT-Small/16 [57] 22.1 4.6 79.9\nPVT-Small [63] 24.5 3.8 79.8\nViL-Small [80] 24.6 5.1 82.0\nCvT-13 [67] 20.0 4.5 81.6\nSwin-Tiny [44] 28.3 4.5 81.2\nFocal-Tiny (Ours) 29.1 4.9 82.2\nResNet-101 [34] 45.0 7.9 77.4\nPVT-Medium [63] 44.2 6.7 81.2\nCvT-21 [67] 32.0 7.1 82.5\nViL-Medium [80] 39.7 9.1 83.3\nSwin-Small [44] 49.6 8.7 83.1\nFocal-Small (Ours) 51.1 9.1 83.5\nResNet-152 [34] 60.0 11.0 78.3\nViT-Base/16 [23] 86.6 17.6 77.9\nDeiT-Base/16 [57] 86.6 17.5 81.8\nPVT-Large [63] 61.4 9.8 81.7\nViL-Base [80] 55.7 13.4 83.2\nSwin-Base [44] 87.8 15.4 83.4\nFocal-Base (Ours) 89.8 16.0 83.8\nTable 2: Comparison of image classiÔ¨Åcation\non ImageNet-1K for different models. Except\nfor ViT-Base/16, all other models are trained\nand evaluated on 224 √ó224 resolution.\nBackbone RetinaNet Mask R-CNN\nAPb APb APm\nResNet-50 [34] 36.3 38.0 34.4\nPVT-Small 40.4 40.4 37.8\nViL-Small [80] 41.6 41.8 38.5\nSwin-Tiny [44] 42.0 43.7 39.8\nFocal-Tiny (Ours) 43.7(+1.7) 44.8(+1.1) 41.0(+1.3)\nResNet-101 [34] 38.5 40.4 36.4\nResNeXt101-32x4d [70]39.9 41.9 37.5\nPVT-Medium [63] 41.9 42.0 39.0\nViL-Medium [80] 42.9 43.4 39.7\nSwin-Small [44] 45.0 46.5 42.1\nFocal-Small (Ours) 45.6(+0.6) 47.4(+0.9) 42.8(+0.7)\nResNeXt101-64x4d [70]41.0 42.8 38.4\nPVT-Large [63] 42.6 42.9 39.5\nViL-Base [80] 44.3 45.1 41.0\nSwin-Base [44] 45.0 46.9 42.3\nFocal-Base (Ours) 46.3(+1.3) 47.8(+0.9) 43.2(+0.9)\nTable 3: Comparisons with CNN and Transformer\nbaselines and SoTA methods on COCO object detec-\ntion. The box mAP ( APb) and mask mAP (APm)\nare reported for RetinaNet and Mask R-CNN trained\nwith 1√óschedule. More detailed comparisons with\n3√óschedule are in Table 4.\naugmentation [4, 35] and exponential moving average (EMA) [49]. The stochastic depth drop rates\nare set to 0.2, 0.2 and 0.3 for our tiny, small and base models, respectively. During training, we crop\nimages randomly to 224 √ó224, while a center crop is used during evaluation on the validation set.\nIn Table 2, we summarize the results for baseline models and the current state-of-the-art models\non image classiÔ¨Åcation task. We can Ô¨Ånd our Focal Transformers consistently outperforms other\nmethods with similar model size (#Params.) and computational complexity (GFLOPs). SpeciÔ¨Åcally,\nFocal-Tiny improves over the Transformer baseline DeiT-Small/16 by 2.0%. Meanwhile, using the\nsame model conÔ¨Åguration (2-2-6-2) and a few extra parameters and computations, our Focal-Tiny\nimproves over Swin-Tiny by 1.0 point (81.2% ‚Üí82.2%). When we increase the window size from 7\nto 14 to match the settings in ViL-Small [80], the performance can be further improved to 82.5%. For\nsmall and base models, our Focal Transformers still achieves slightly better performance than the\nothers. Notably, our Focal-Small with 51.1M parameters can reach 83.5% which is better than all\ncounterpart small and base models using much less parameters. When further increasing the model\nsize, our Focal-Base model achieves 83.8%, surpassing all other models using comparable parameters\nand FLOPs. We refer the readers to our appendix for more detailed comparisons.\n4.2 Object detection and instance segmentation\nWe benchmark our models on object detection with COCO 2017 [43]. The pretrained models are\nused as visual backbones and then plug into two representative pipelines, RetinaNet [42] and Mask\nR-CNN [33]. All models are trained on the 118k training images and results reported on 5K validation\nset. We follow the standard to use two training schedules, 1√óschedule with 12 epochs and 3√ó\nschedule with 36 epochs. For 1√óschedule, we resize image‚Äôs shorter side to800 while keeping its\nlonger side no more than 1333. For 3√óschedule, we use multi-scale training strategy by randomly\nresizing its shorter side to the range of [480, 800]. Considering this higher input resolution, we\nadaptively increase the focal sizes at four stages to (15, 13, 9, 7), to ensures the focal attention covers\nmore than half of the image region (Ô¨Årst two stages) to the whole image ( last two stages). With\nthe focal size increased, the relative position biases are accordingly up-sampled to corresponding\nsizes using bilinear interpolation. During training, we use AdamW [45] for optimization with initial\nlearning rate 10‚àí4 and weight decay 0.05. Similarly, we use 0.2, 0.2 and 0.3 stochastic depth\ndrop rates to regularize the training for our tiny, small and base models, respectively. Since Swin\nTransformer does not report the numbers on RetinaNet, we train it by ourselves using their ofÔ¨Åcial\ncode with the same hyper-parameters with our Focal Transformers.\n7\nBackbone #Params FLOPs RetinaNet 3x schedule + MS Mask R-CNN 3x schedule + MS\n(M) (G) APb APb50 APb75 APS APM APL APb APb50 APb75 APm APm50 APm75\nResNet50 [34] 37.7/44.2 239/26039.0 58.4 41.8 22.4 42.8 51.6 41.0 61.7 44.9 37.1 58.4 40.1\nPVT-Small[63] 34.2/44.1 226/24542.2 62.7 45.0 26.2 45.2 57.2 43.0 65.3 46.9 39.9 62.5 42.8\nViL-Small [80] 35.7/45.0 252/17442.9 63.8 45.6 27.8 46.4 56.3 43.4 64.9 47.0 39.6 62.1 42.4\nSwin-Tiny [44] 38.5/47.8 245/26445.0 65.9 48.4 29.7 48.9 58.1 46.0 68.1 50.3 41.6 65.1 44.9\nFocal-Tiny (Ours) 39.4/48.8 265/29145.5 66.3 48.8 31.2 49.2 58.7 47.2 69.4 51.9 42.7 66.5 45.9\nResNet101 [34] 56.7/63.2 315/33640.9 60.1 44.0 23.7 45.0 53.8 42.8 63.2 47.1 38.5 60.1 41.3\nResNeXt101-32x4d [70]56.4/62.8 319/34041.4 61.0 44.3 23.9 45.5 53.7 44.0 64.4 48.0 39.2 61.4 41.9\nPVT-Medium [63] 53.9/63.9 283/30243.2 63.8 46.1 27.3 46.3 58.9 44.2 66.0 48.2 40.5 63.1 43.5\nViL-Medium [80] 50.8/60.1 339/26143.7 64.6 46.4 27.9 47.1 56.9 44.6 66.3 48.5 40.7 63.8 43.7\nSwin-Small [44] 59.8/69.1 335/35446.4 67.0 50.1 31.0 50.1 60.3 48.5 70.2 53.5 43.3 67.3 46.6\nFocal-Small (Ours) 61.7/71.2 367/40147.3 67.8 51.0 31.6 50.9 61.1 48.8 70.5 53.6 43.8 67.7 47.2\nResNeXt101-64x4d [70]95.5/102 473/493 41.8 61.5 44.4 25.2 45.4 54.6 44.4 64.9 48.8 39.7 61.9 42.6\nPVT-Large[63] 71.1/81.0 345/36443.4 63.6 46.1 26.1 46.0 59.5 44.5 66.0 48.3 40.7 63.4 43.7\nViL-Base [80] 66.7/76.1 443/36544.7 65.5 47.6 29.9 48.0 58.1 45.7 67.2 49.9 41.3 64.4 44.5\nSwin-Base [44] 98.4/107 477/496 45.8 66.4 49.1 29.9 49.4 60.3 48.5 69.8 53.2 43.4 66.8 46.9\nFocal-Base (Ours) 100.8/110.0514/53346.9 67.8 50.3 31.9 50.3 61.5 49.0 70.1 53.6 43.7 67.6 47.0\nTable 4: COCO object detection and segmentation results with RetinaNet [42] and Mask R-CNN [34].\nAll models are trained with 3√óschedule and multi-scale inputs (MS). The numbers before and after\n‚Äú/‚Äù at column 2 and 3 are the model size and complexity for RetinaNet and Mask R-CNN, respectively.\nIn Table 3, we show the performance for both CNN-based models and the current Transformer-\nbased state-of-the-arts methods. The bbox mAP ( APb) and mask mAP (APm) are reported. Our\nFocal Transformers outperform the CNN-based models consistently with the gap of 4.8-7.1 points.\nCompared with the other methods which also use multi-scale Transformer architectures, we still\nobserve substantial gains across all settings and metrics. Particularly, our Focal Transformers brings\n0.7-1.7 points of mAP against the current best approach Swin Transformer [ 44] at comparable\nsettings. Different from the other multi-scale Transformer models, our method can simultaneously\nenable short-range Ô¨Åne-grain and long-range coarse-grain interactions for each visual token, and thus\ncapture richer visual contexts at each layer for better dense predictions. To have more comprehensive\ncomparisons, we further train them with 3√óschedule and show the detailed numbers for RetinaNet\nand Mask R-CNN in Table 4. For comprehension, we also list the number of parameters and the\nassociated computational cost for each model. As we can see, even for 3√óschedule, our models can\nstill achieve 0.3-1.1 gain over the best Swin Transformer models at comparable settings.\nMethod Backbone #Param FLOPs APb APb50 APb75\nC. Mask R-CNN [7]\nR-50 82.0 739 46.3 64.3 50.5\nSwin-T 85.6 742 50.5 69.3 54.9\nFocal-T 86.7 770 51.5(+1.0)70.6 55.9\nATSS [81]\nR-50 32.1 205 43.5 61.9 47.0\nSwin-T 35.7 212 47.2 66.5 51.3\nFocal-T 36.8 239 49.5(+2.3)68.8 53.9\nRepPointsV2 [72]\nR-50 43.4 431 46.5 64.6 50.3\nSwin-T 44.1 437 50.0 68.5 54.2\nFocal-T 45.4 491 51.2(+1.2)70.4 54.9\nSparse R-CNN [55]\nR-50 106.1 166 44.5 63.4 48.2\nSwin-T 109.7 172 47.9 67.3 52.3\nFocal-T 110.8 196 49.0(+1.1)69.1 53.2\nTable 5: Comparison with ResNet-50, Swin-Tiny across\ndifferent object detection methods. We use Focal-Tiny as\nthe backbone and train all models using 3√óschedule.\nTo further verify the effectiveness of our\nproposed Focal Transformers, we fol-\nlow [ 44] to train four different object\ndetectors including Cascade R-CNN [7],\nATSS [81], RepPoints [ 72] and Sparse\nR-CNN [55]. We use Focal-Tiny as the\nbackbone and train all four models using\n3√óschedule. The box mAPs on COCO\nvalidation set are reported in Table 5. As\nwe can see, our Focal-Tiny exceeds Swin-\nTiny by 1.0-2.3 points on all methods.\nThese signiÔ¨Åcant and consistent improve-\nments over different detection methods\nin addition to RetinaNet and Mask R-\nCNN suggest that our Focal Transformer\ncan be used as a generic backbone for a\nvariety of object detection methods.\nBesides the instance segmentation re-\nsults above, we further evaluate our model on semantic segmentation, a task that usually requires\nhigh-resolution input and long-range interactions. We benchmark our method on ADE20K [ 88].\nSpeciÔ¨Åcally, we use UperNet [68] as the segmentation method and our Focal Transformers as the\nbackbone. We train three models with Focal-Tiny, Focal-Small, Focal-Base, respectively. For all\nmodels, we use a standard recipe by setting the input size to 512 √ó512 and train the model for 160k\niterations with batch size 16. In Table 7, we show the comparisons to previous works. As we can see,\n8\nMethod #Param FLOPs mini-val test-dev\nAPb APm APb APm\nX101-64x4d [70] 155M 1033G 52.3 46.0 - -\nEfÔ¨ÅcientNet-D7 [56] 77M 410G 54.4 - 55.1 -\nGCNet‚àó[8] - 1041G 51.8 44.7 52.3 45.4\nResNeSt-200 [79] - - 52.5 - 53.3 47.1\nCopy-paste [28] 185M 1440G 55.9 47.2 56.0 47.4\nBoTNet-200 [52] - - 49.7 - - -\nSpineNet-190 [24] 164M 1885G 52.6 - 52.8 -\nCenterNet2 [90] - - - - 56.4 -\nSwin-L (HTC++) [44] 284M 1470G57.1 49.5 57.7 50.2\nSwin-L (DyHead) [19] 213M 965G56.2 - - -\nSwin-L‚Ä†(HTC++) [44] 284M - 58.0 50.4 58.7 51.1\nSwin-L‚Ä†(DyHead) [19] 213M - 58.4 - 58.7 -\nSwin-L‚Ä†(QueryInst) [26] - - 56.1 - 56.1 -\nFocal-L (HTC++) (Ours)265M 1165G57.0 49.9 - -\nFocal-L (DyHead) (Ours)229M 1081G56.4 - - -\nFocal-L‚Ä†(HTC++) (Ours)265M - 58.1 50.9 58.4 51.3\nFocal-L‚Ä†(DyHead) (Ours)229M - 58.7 - 58.9 -\nTable 6: Comparison with state-of-the-art methods\non COCO object detection and instance segmenta-\ntion. The numbers are reported on 5K val set and\ntest-dev. Augmented HTC [13] (denoted by HTC++)\nand DyHead [19] are used as the detection methods.\n‚Ä†means multi-scale evaluation.\nBackbone Method #Param FLOPs mIoU +MS\nResNet-101 DANet [46] 69M 1119G 45.3 -\nResNet-101 ACNet [27] - - 45.9 -\nResNet-101 DNL [73] 69M 1249G 46.0 -\nResNet-101 UperNet [68] 86M 1029G 44.9 -\nHRNet-w48 [54] OCRNet [77] 71M 664G45.7 -\nResNeSt-200 [79] DLab.v3+ [14] 88M 1381G48.4 -\nSwin-T [44] UperNet [68] 60M 945G 44.5 45.8\nSwin-S [44] UperNet [68] 81M 1038G 47.6 49.5\nSwin-B [44] UperNet [68] 121M 1188G 48.1 49.7\nTwins-SVT-L [17] UperNet [68] 133M - 48.8 50.2\nMiT-B5 [69] SegFormer [69] 85M - 51.0 51.8\nViT-L/16‚Ä†[23] SETR [85] 308M - 50.3 -\nSwin-L‚Ä°[44] UperNet [68] 234M 3230G 52.1 53.5\nViT-L/16‚Ä°[23] Segmenter [53] 334M - 51.8 53.6\nSwin-L‚Ä°[44] K-Net [82] - - - 54.3\nSwin-L‚Ä°[44] PatchDiverse [29] 234M - 53.1 54.4\nVOLO-D5 [76] UperNet [68] - - - 54.3\nFocal-T (Ours) UperNet [68]62M 998G 45.8 47.0\nFocal-S (Ours) UperNet [68]85M 1130G48.0 50.0\nFocal-B (Ours) UperNet [68]126M1354G49.0 50.5\nFocal-L‚Ä°(Ours) UperNet [68]240M3376G54.0 55.4\nTable 7: Comparison with SoTA methods for\nsemantic segmentation on ADE20K [ 88] val\nset. Both single- and multi-scale evaluations\nare reported at the last two columns. ‚Ä°means\npretrained on ImageNet-22K.\nour tiny, small and base models consistently outperforms Swin Transformers with similar size on\nsingle-scale and multi-scale mIoUs.\n4.3 Comparing with system-level SoTA methods\nTo compare with SoTA at the system level, we build Focal Large model by increasing the hidden\ndimension in Focal-Base from 128 to 196 while keeping all the others the same, similar to Swin\nTransformers. To achieve the best performance, the common practice is to pretrain on ImageNet-22K\nand then transfer the model to down stream tasks [67, 44]. However, due to the limited resources,\nwe partially initialize our model with the pretrained Swin Transformer checkpoint 2, considering\nour network architecture is similar with Swin Transformers except for the window shift and focal\nself-attention. SpeciÔ¨Åcally, we reuse the parameters in Swin-Large model but remove the window\nshift operation and randomly initialize our own window pooling layer in Eq. (1) and local-to-global\nrelative position bias in Eq. (3). Then, we Ô¨Ånetune our model on ImageNet-1K to learn the focal-\nspeciÔ¨Åc parameters. The resulting model is used as the backbone and further Ô¨Ånetuned on object\ndetection and semantic segmentation tasks.\nComparison with SoTA detection systems. For object detection on COCO, we Ô¨Årst follow Swin\nTransformer to also use HTC [ 13] as the detection method in that it reported SoTA performance\non COCO detection when using Swin Transformer as the backbone. For fair comparison, we also\nuse soft-NMS [ 5], instaboost [ 25] and a multi-scale training strategy with shorter side in range\n[400, 1400] while the longer side is no more than 1600. We train the model using AdamW [45] with\nbase learning rate 1e-4 and weight decay 0.1. The model is trained using standard 3√óschedule. The\nbox and mask mAPs on COCO validation set and test-dev are reported in Table 6. We show both\nsingle-scale evaluation and multi-scale evaluation results. Our Focal-Large model with multi-scale\ntest achieve 58.1 box mAP and 50.9 mask mAP on mini-val set, which are better than the reported\nnumbers for Swin-Large in [44]. When evaluating our model on the test-dev set, it achieves 58.4 box\nmAP and 51.3 mask mAP, which is slightly better than Swin Transformer. Note that because our\nmodel does not include global self-attention layer used in Swin Transformer at the last stage, it has\nsmaller model size and fewer FLOPs. More recently, DyHead [19] achieves new SoTA on COCO,\nwhen combined with Swin-Large. We replace the Swin-Large model with our Focal-Large model,\nand use the same 2√ótraining schedule as in [19]. We report the box mAPs for both mini-val and\n2Pretrained models are available at https://github.com/microsoft/Swin-Transformer\n9\nModel W-Size FLOPs Top-1 (%) APb APm\nSwin-Tiny 7 4.5 81.2 43.7 39.8\n14 4.9 82.1 44.0 40.5\nFocal-Tiny 7 4.9 82.2 44.9 41.1\n14 5.2 82.3 45.5 41.5\nTable 8: Impact of different window sizes (W-\nSize). We alter the default size 7 to 14 and ob-\nserve consistent improvements for both methods.\nModel W-Shift Top-1 (%) APb APm\nSwin-Tiny - 80.2 38.8 36.4\n‚úì 81.2 43.7 39.8\nFocal-Tiny - 82.2 44.8 41.0\n‚úì 81.9 44.9 41.1\nTable 9: Impact of window shift (W-Shift) on\nSwin Transformer and Focal Transformer. Tiny\nmodels are used.\nWindow +Local +Global +Local+Global\nFocal-Tiny ablated models\n78\n79\n80\n81\n82\n83T op-1 Accuracy (%)\n80.1\n81.4 81.6\n82.2\n36\n38\n40\n42\n44\n46\n48\n50\nBbox mAP\n38.8\n43.9\n42.2\n44.8\nFigure 5: Ablating Focal-Tiny model by adding\nlocal, global and both interactions, respectively.\nBlue bars are for image classiÔ¨Åcation and orange\nbars indicate object detection performance. Both\nlocal and global interactions are essential to ob-\ntain good performance. Better viewed in color.\nDepths Model #Params. FLOPs Top-1 (%)APb APm\n2-2-2-2 Swin 21.2 3.1 78.7 38.2 35.7\nFocal 21.7 3.4 79.9 40.5 37.6\n2-2-4-2 Swin 24.7 3.8 80.2 41.2 38.1\nFocal 25.4 4.1 81.4 43.3 39.8\n2-2-6-2 Swin 28.3 4.5 81.2 43.7 39.8\nFocal 29.1 4.9 82.2 44.8 41.0\nTable 10: Impact of the change of model depth.\nWe gradually reduce the number of transformer\nlayers at the third stage from original 6 to 4 and\nfurther 2. It apparently hurts the performance but\nour Focal Transformers has much slower drop rate\nthan Swin Transformer.\ntest-dev. Our Focal-Large clearly bring substantial improvements over both metrics, reaching new\nSoTA on both metrics.\nComparison with SoTA semantic segmentation systems. We further use the pretrained Focal-\nLarge model as the backbone for semantic segmentation. We follow the same setting as in [ 44].\nSpeciÔ¨Åcally, we use input image size640√ó640 and train the model for 160k iterations with a batch size\nof 16. We set the initial learning to 6e-5 and use a polynomial learning rate decay. The weight decay is\nset to 0.01. For multi-scale evaluation, we use the same scaling ratios [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]\nas in previous works. In Table 7, we see that our Focal-Large achieves signiÔ¨Åcantly better performance\nthan Swin-Large. In both single-scale and multi-scale evaluation, Focal-Large has more than 1 point\nmIoU improvement, which presents a new SoTA for semantic segmentation on ADE20K. These\nencouraging results verify the effectiveness of our proposed focal self-attention mechanism in\ncapturing long-range dependencies required by dense visual prediction tasks.\n4.4 Ablation studies\nWe conduct ablation studies to inspect the model‚Äôs capacity from different aspects. Focal-Tiny is\nconsidered on both image classiÔ¨Åcation and object detection tasks.\nEffect of varying window size . Above we have demonstrated that both short- and long-range\ninteractions are necessary. Based on this, a natural question is that whether increasing the window\nsize can further help the model learning giving an enlarged receptive Ô¨Åeld. In Table 8, we show the\nperformance of Swin-Tiny and Focal-Tiny with window size 7 and 14. Clearly, a larger window size\nbrings gain for both methods on all three metrics and our Focal-Tiny model consistently outperforms\nSwin-Tiny using both window sizes. Comparing the second and third row, we Ô¨Ånd our model beats\nSwin even using much smaller window size (7 v.s. 14). We suspect the long-range interactions in our\nmodel is the source of this gain.\nThe necessity of window shift. In [44], the authors proposed window shift operations to enable the\ncross-window interactions between two successive layers. In contrast, the visual tokens in our Focal\nTransformer can always communicate with those in other windows at both Ô¨Åne- and coarse-grain. A\nnatural question is whether adding the window shift to our Focal Transformers can further lead to\n10\nimprovements. To investigate this, we remove the window shift from Swin Transformer while add\nit to our Focal Transformer. As shown in Table 9, Swin Transformer shows a severe degradation\nafter removing the window shift. However, our Focal Transformer is even hurt on classiÔ¨Åcation task.\nThese results indicate that the window shift is not a necessary ingredient in our model. As such, our\nmodel can get rid of the constraint in Swin Transformer that there should be an even number of layers\nin each stage for the alternative window shift operation.\nContributions of short- and long-range interaction . We attempt to factorize the effect of short-\nrange Ô¨Åne-grain and long-range coarse-grain interactions in our Focal Transformers. We ablate\nthe original Focal-Tiny model to: a) Focal-Tiny-Window merely performing attention inside each\nwindow; b) Focal-Tiny-Local attending the additional Ô¨Åne-grain surrounding tokens and c) Focal-\nTiny-Global attending the extra coarse-grain squeezed tokens. We train them using the same setting\nas Focal-Tiny and report their performance on image classiÔ¨Åcation and object detection using Mask\nR-CNN 1√óschedule. As we can see from Fig. 5, Focal-Tiny-Window suffers from signiÔ¨Åcant drop on\nboth image classiÔ¨Åcation (82.2‚Üí80.1) and object detection (44.8‚Üí38.3). This is expected since the\ncommunication across windows are totally cut off at each Transformer layer. After we enable either\nthe local Ô¨Åne-grain or global coarse-grain interactions (middle two columns), we observe signiÔ¨Åcant\njumps. Though they prompt richer interactions from different paths, Ô¨Ånally both of them enable the\nmodel to capture more contextual information. When we combine them together, we observe further\nimprovements on both tasks. This implies that these two type of interactions are complementary to\neach other and both of them should be enabled in our model. Another observation is that adding\nlong-range tokens can bring more relative improvement for image classiÔ¨Åcation than object detection\nand vice versa for local tokens. We suspect that dense predictions like object detection more rely on\nÔ¨Åne-grained local context while image classiÔ¨Åcation favors more the global information.\nModel capacity against model depth . Considering our focal attention prompts local and global\ninteractions at each Transformer layer, one question is that whether it needs less number of layers to\nobtain similar modeling capacity as those without global interactions. To answer this, we conduct the\nexperiments by reducing the number of Transformer layers at stage 3 in Swin-Tiny and Focal-Tiny\nfrom the original 6 to 4 and 2. In Table 10, we show the performance and model complexity for each\nvariant. First, we can Ô¨Ånd our model outperforms Swin model consistently with the same depth. More\nimportantly, using two less layers, our model achieves comparable performance to Swin Transformer.\nParticularly, Focal-Tiny with 4 layers achieves 81.4 on image classiÔ¨Åcation which is even better\nthan original Swin-Tiny model with 6 layers (highlighted in gray cells). Though we do not explore\ndifferent architectures for our Focal Transformer, these results suggest that we can potential Ô¨Ånd even\nmore efÔ¨Åcient and effective architectures.\n5 Conclusion\nIn this paper, we have presented focal self-attention to enable efÔ¨Åcient local-global interactions in\nvision Transformers. Different from previous works, it performs the local self-attention at Ô¨Åne-grain\nand global self-attention at coarse-grain, which results in an effective way to capture richer context\nin both short and long-range at a reasonable cost. By plugging it into a multi-scale transformer\narchitecture, we propose Focal Transformers, which demonstrates its superiority over the SoTA\nmethods on both image classiÔ¨Åcation, object detection and segmentation. With these extensive\nexperimental results, the proposed focal attention is shown as a generic approach for modeling\nlocal-global interactions in vision Transformers for various vision tasks.\nLimitations and future work. Though extensive experimental results showed that our focal self-\nattention can signiÔ¨Åcantly boost the performance on both image classiÔ¨Åcation and dense prediction\ntasks, it does introduce extra computational and memory cost, since each query token needs to\nattend the coarsened global tokens in addition to the local tokens. Developing some practical or\nmethodological techniques to reduce the cost would be necessary to make it more applicable in\nrealistic scenarios. Our ablation study on the number of used Transformer layers in Table 10 indeed\nshed light on the potential way to reduce the cost via reducing the number of transformer layers.\nHowever, we merely scratched the surface and further study on this aspect is still needed. In Focal\nTransformers, we chose multi-scale architecture as the base so that it can work for high-resolution\nprediction tasks. However, we believe our focal attention mechanism is also applicable to monolithic\nvision Transformers and Transformers in both vision and language domain. We leave this as a\npromising direction to further explore in the future.\n11\nReferences\n[1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula, and Sumit Sanghai. Etc:\nEncoding long and structured data in transformers. arXiv preprint arXiv:2004.08483, 2020.\n[2] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented\nconvolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 3286‚Äì3295, 2019.\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[4] Maxim Berman, Herv√© J√©gou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs Douze. Multigrain: a\nuniÔ¨Åed image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019.\n[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms‚Äìimproving object detection\nwith one line of code. In Proceedings of the IEEE international conference on computer vision, pages\n5561‚Äì5569, 2017.\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[7] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 6154‚Äì6162, 2018.\n[8] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In Proceedings of the IEEE/CVF International Conference on Computer\nVision Workshops, pages 0‚Äì0, 2019.\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npages 213‚Äì229. Springer, 2020.\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294,\n2021.\n[11] Shuning Chang, Pichao Wang, Fan Wang, Hao Li, and Jiashi Feng. Augmented transformer with adaptive\ngraph for temporal action proposal generation. arXiv preprint arXiv:2103.16024, 2021.\n[12] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiÔ¨Åcation, 2021.\n[13] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974‚Äì4983, 2019.\n[14] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In Proceedings of the European\nconference on computer vision (ECCV), pages 801‚Äì818, 2018.\n[15] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking.\narXiv preprint arXiv:2103.15436, 2021.\n[16] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual\ntransformers. arXiv preprint arXiv:2104.02057, 2021.\n[17] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\nShen. Twins: Revisiting spatial attention design in vision transformers. arXiv preprint arXiv:2104.13840,\n2021.\n[18] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional positional encodings for vision transformers. Arxiv preprint 2102.10882, 2021.\n[19] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic\nhead: Unifying object detection heads with attentions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7373‚Äì7382, 2021.\n12\n[20] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object\ndetection with transformers. arXiv preprint arXiv:2011.09094, 2020.\n[21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255.\nIeee, 2009.\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. NAACL, 2019.\n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[24] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan\nSong. Spinenet: Learning scale-permuted backbone for recognition and localization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11592‚Äì11601, 2020.\n[25] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost:\nBoosting instance segmentation via probability map guided copy-pasting. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 682‚Äì691, 2019.\n[26] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu.\nInstances as queries, 2021.\n[27] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive context\nnetwork for scene parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 6748‚Äì6757, 2019.\n[28] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret\nZoph. Simple copy-paste is a strong data augmentation method for instance segmentation. arXiv preprint\narXiv:2012.07177, 2020.\n[29] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers with patch\ndiversiÔ¨Åcation, 2021.\n[30] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, and\nMatthijs Douze. Levit: a vision transformer in convnet‚Äôs clothing for faster inference. arXiv preprint\narXiv:22104.01136, 2021.\n[31] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint arXiv:2012.12556, 2020.\n[32] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer,\n2021.\n[33] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2961‚Äì2969, 2017.\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì778, 2016.\n[35] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten HoeÔ¨Çer, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8129‚Äì8138, 2020.\n[36] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 7132‚Äì7141, 2018.\n[37] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.\n[38] Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The\nhandbook of brain theory and neural networks, 3361(10):1995, 1995.\n[39] Bing Li, Cheng Zheng, Silvio Giancola, and Bernard Ghanem. Sctn: Sparse convolution-transformer\nnetwork for scene Ô¨Çow estimation. arXiv preprint arXiv:2105.04447, 2021.\n13\n[40] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao.\nEfÔ¨Åcient self-supervised vision transformers for representation learning. arXiv preprint arXiv:2106.09785,\n2021.\n[41] Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, and Wanqing Li. Trear:\nTransformer-based rgb-d egocentric action recognition. arXiv preprint arXiv:2101.03904, 2021.\n[42] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense object\ndetection. In Proceedings of the IEEE international conference on computer vision, pages 2980‚Äì2988,\n2017.\n[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.\n[44] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[46] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning\nand matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n299‚Äì307, 2017.\n[47] R. Pappagari, P. Zelasko, J. Villalba, Y . Carmiel, and N. Dehak. Hierarchical transformers for long\ndocument classiÔ¨Åcation. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop\n(ASRU), pages 838‚Äì844, 2019.\n[48] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In International Conference on Machine Learning, pages 4055‚Äì4064. PMLR,\n2018.\n[49] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\njournal on control and optimization, 30(4):838‚Äì855, 1992.\n[50] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers\nfor long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n[51] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019.\n[52] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition, 2021.\n[53] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic\nsegmentation. arXiv preprint arXiv:2105.05633, 2021.\n[54] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human\npose estimation. In CVPR, 2019.\n[55] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li,\nZehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals.\narXiv preprint arXiv:2011.12450, 2020.\n[56] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105‚Äì6114. PMLR, 2019.\n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[58] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√© J√©gou. Going deeper\nwith image transformers, 2021.\n[59] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efÔ¨Åcient visual backbones. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12894‚Äì12904, 2021.\n14\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[61] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end\npanoptic segmentation with mask transformers. arXiv preprint arXiv:2012.00759, 2020.\n[62] Ning Wang, Wengang Zhou, Jie Wang, and Houqaing Li. Transformer meets tracker: Exploiting temporal\ncontext for robust visual tracking. arXiv preprint arXiv:2103.11681, 2021.\n[63] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[64] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 7794‚Äì7803, 2018.\n[65] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.\nEnd-to-end video instance segmentation with transformers. arXiv preprint arXiv:2011.14503, 2020.\n[66] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention\nmodule. In Proceedings of the European conference on computer vision (ECCV), pages 3‚Äì19, 2018.\n[67] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. UniÔ¨Åed perceptual parsing for scene\nunderstanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418‚Äì434,\n2018.\n[69] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer:\nSimple and efÔ¨Åcient design for semantic segmentation with transformers, 2021.\n[70] Saining Xie, Ross Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492‚Äì1500, 2017.\n[71] Jianwei Yang, Zhile Ren, Chuang Gan, Hongyuan Zhu, and Devi Parikh. Cross-channel communication\nnetworks. In Proceedings of the 33rd International Conference on Neural Information Processing Systems,\npages 1297‚Äì1306, 2019.\n[72] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for\nobject detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n9657‚Äì9666, 2019.\n[73] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled\nnon-local neural networks. In European Conference on Computer Vision, pages 191‚Äì207. Springer, 2020.\n[74] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution\ndesigns into visual transformers. arXiv preprint arXiv:2103.11816, 2021.\n[75] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[76] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. V olo: Vision outlooker for visual\nrecognition. arXiv preprint arXiv:2106.13112, 2021.\n[77] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.\narXiv preprint arXiv:1909.11065, 2019.\n[78] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. arXiv\npreprint arXiv:2007.14062, 2020.\n[79] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\nMueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.\n[80] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n15\n[81] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-\nbased and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 9759‚Äì9768, 2020.\n[82] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards uniÔ¨Åed image\nsegmentation, 2021.\n[83] Jiaojiao Zhao, Xinyu Li, Chunhui Liu, Shuai Bing, Hao Chen, Cees GM Snoek, and Joseph Tighe. Tuber:\nTube-transformer for action detection. arXiv preprint arXiv:2104.00969, 2021.\n[84] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object detection\nwith adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.\n[85] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[86] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6881‚Äì6890, 2021.\n[87] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 34, pages 13001‚Äì13008, 2020.\n[88] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 633‚Äì641, 2017.\n[89] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[90] Xingyi Zhou, Vladlen Koltun, and Philipp Kr√§henb√ºhl. Probabilistic two-stage detection. arXiv preprint\narXiv:2103.07461, 2021.\n[91] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n16\nA Appendix\nA.1 Image classiÔ¨Åcation\nWe present the exhaustive comparison with previous works in Table 11. We compare our method\nwith both CNN-based and Transformer-based methods. We categorize different methods into groups\nbased on two properties:\n‚Ä¢ Scale ‚Äì the scale of feature maps in a model. It can be either a single-scale or multi-scale. In\nsingle-scale models, all feature maps have the same size across different stages. For multi-scale\nmodels, there are usually feature maps with different resolutions with the proceeding stages.\n‚Ä¢ Locality ‚Äì the locality of operations in a model. It can be either global or local. Local operations\ncan be a convoluional layer in CNN models or a transformer layer which conducts local self-\nattention. However, global operations such as the standard self-attention, produce the output feature\nmap by gather information from all inputs.\nBased on this criterion, all CNN models are natural multi-scale because their feature map sizes\ngradually decrease at different stages. Recently, a number of works attempt to integrate the global\noperations into CNNs by introducing squeeze-and-excitation (SE) layer [36], channel-wise attention\nlayer [66] and even self-attention layer [2, 52]. As we can see, the combination of local and global\noperations signiÔ¨Åcantly improve the performance for image classiÔ¨Åcation. Particularly, BotNet-S1-\n110 achieves 82.8 top-1 accuracy with moderate number of parameters (61.6M).\nOn the contrary, Transformers [60] by nature performs global self-attention by which each visual token\ncan interact with all others. Even without multi-scale design as in CNNs, a number of Transformer-\nbased works such as TNT [32], DeepViT [89] and CaiT [58] achieve superior performance to CNN\nmodels with comparable model size and computational cost. To accommodate the high resolution\nfeature maps, some recent works replace global self-attention with more efÔ¨Åcient local self-attention\nand demonstrate comparable performance on image classiÔ¨Åcation while much promising results on\ndense prediction tasks such as object detection and semantic segmentation [44].\nIn this paper, we present focal attention which is the Ô¨Årst to combine global self-attention and local\nself-attention in an efÔ¨Åcient way. Replacing either the global self-attention or the local self-attention\nwith our focal self-attention, we achieve better performance than both. These results along with the\nCNN models augmented by local and global computations demonstrate that combining local and\nglobal interactions are more effective than either of them. In the table, we also report the speed for\ndifferent methods. Using the same script provided by [44], we run the test on a single Tesla-V100\nwith batch size 64. Accordingly, our Focal Transformer has slower running speed though it has\nsimilar FLOPs as Swin Transformer. This is mainly due to two reasons: 1) we introduce the global\ncoarse-grain attention, and it introduces the extra computations; 2) though we conduct our focal\nattention on the windows, we swill observe that extracting the surrounding tokens around local\nwindows and the global tokens across the feature map are time-consuming.\nA.2 Object detection and segmentation\nFor completeness, we report the full metrics for RetinaNet and Mask R-CNN trained with 1x schedule\nin Table 12. As we can see, our Focal Transformers consistently outperform previous works including\nthe state-of-the-art Swin Transformers on all metrics. We observe that our models trained with 1x\nschedule generally have more gain against the previous best models than 3x schedule (+1.2 v.s. +0.8\nand +1.0 v.s. +0.7 box mAP for RetinaNet and Mask R-CNN, respectively). This indicates that\nour models have faster learning convergences compared with previous works. Compared with the\nlocal-attention based methods, e.g., Swin Transformer, integrating the long-range interactions can\nhelp capture more visual dependencies and thus help the model to learn faster.\nA.3 Model inspections\nLearning speed comparison. As we brieÔ¨Çy discussed earlier, our model shows faster learning speed\non object detection task. In Fig. 6, we show the top-1 validation accuracy of our models and Swin\nTransformers for image classiÔ¨Åcation task. Accordingly, our Focal Transformers have much faster\nlearning speed as well. For example, Focal-Tiny has 75.7% top-1 accuracy at 100-th epoch while\nSwin-Tiny has 73.9% top-1 accuracy. Similarly, Focal-Small achieves 78.3% at 100-th epoch, which\nis 2.0 point higher than Swin-Small. Even for the base models, this gap is still maintained for a\n17\nArchitecture Scale Locality Model #Params. (M) FLOPs (G) Top-1 (%)\nConvolutional\nNeural NetworkMultiple\nLocal\nResNet-50 [34] 25.0 4.1 76.2\nResNet-101 [34] 45.0 7.9 77.4\nResNet-152 [34] 60.0 11.0 78.3\nLocal\n+Global\nSE-ResNet-50 [36] 28.1 8.2 77.5\nSE-ResNet-101 [36] 49.3 15.6 78.4\nSE-ResNet-152 [36] 66.8 23.1 78.9\nCBAM-ResNet-50 [66] 28.1 3.9 77.3\nCBAM-ResNet-101 [66] 49.3 7.6 78.5\nAttAug-ResNet-50 [2] 25.8 8.3 77.7\nAttAug-ResNet-101 [2] 45.4 16.1 78.1\nAttAug-ResNet-152 [2] 61.6 23.8 79.1\nBotNet-S1-59 [52] 33.5 7.3 81.7\nBotNet-S1-110 [52] 54.7 10.9 82.8\nTransformer\nSingle Global\nViT-B/16 [23] 86.6 17.6 77.9\nViT-L/16 [23] 307 190.7 76.5\nDeiT-S/16 [57] 22.0 4.6 79.9\nDeiT-B/16 [57] 86.6 17.5 81.8\nTNT-S [32] 23.8 5.2 81.3\nTNT-B [32] 65.6 14.1 82.8\nCPVT-S [18] 23.0 4.6 81.5\nCPVT-B [18] 88.0 17.6 82.3\nDeepViT-S [89] 27.0 6.2 82.3\nDeepViT-L [89] 55.0 12.5 83.1\nCaiT-S36 [58] 68.0 13.9 83.3\nLeViT-256 [30] 18.9 1.1 81.6\nLeViT-384 [30] 39.1 2.3 82.6\nMultiple\nGlobal\nT2T-ViT-19 [75] 39.2 8.9 81.9\nT2T-ViT-24 [75] 64.1 14.1 82.3\nCrossViT-S [12] 26.7 5.6 81.0\nCrossViT-B [12] 104.7 21.2 82.2\nPVT-S [63] 24.5 3.8 79.8\nPVT-M [63] 44.2 6.7 81.2\nPVT-L [63] 61.4 9.8 81.7\nCvT-13 [67] 20.0 4.5 81.6\nCvT-21 [67] 32.0 7.1 82.5\nViL-S [80] 24.6 5.1 82.0\nLocal\nViL-M [80] 39.7 9.1 83.3\nViL-B [80] 55.7 13.4 83.2\nSwin-T [44] 28.3 4.5 81.2\nSwin-S [44] 49.6 8.7 83.1\nSwin-B [44] 87.8 15.4 83.4\nLocal\n+Global\nTwins-SVT-S [17] 24.0 2.8 81.3\nTwins-SVT-B [17] 56.0 8.3 83.1\nTwins-SVT-L [17] 99.2 14.8 83.3\nFocal-T (Ours) 29.1 4.9 82.2\nFocal-S (Ours) 51.1 9.1 83.5\nFocal-B (Ours) 89.8 16.0 83.8\nTable 11: Full comparison of image classiÔ¨Åcation on ImageNet-1k for different model architectures.\nWe split the methods into two super-groups which use CNNs or Transformers as the main skeleton.\nNote that they are inclusive to each other in some methods.\nlong duration until the end of the training. We attribute this faster learning speed to the long-range\ninteractions introduce by our focal attention mechanism in that it can help to capture the global\ninformation at very beginning.\nAttention scores for different token types. In our main submission, we have shown both local and\nglobal attentions are important. Here, we study how much local and global interactions occur at\neach layer. Using Focal-Tiny trained on ImageNet-1K as the target, we show in Fig. 7 the summed\nup attention scores for three type of tokens: 1) local tokens inside the window; 2) local tokens\nsurrounding the window and 3) global tokens after the window pooling. To compute these scores, we\naverage over the all local windows and then also take the average over all heads. Finally, we sum up\nthe attention scores that belongs to the aforementioned three type of tokens. These attention scores\n18\nBackbone #Params FLOPs RetinaNet 1x schedule Mask R-CNN 1x schedule\n(M) (G) APb APb50 APb75 APS APM APL APb APb50 APb75 APm APm50 APm75\nResNet50 [34] 37.7/44.2 239/26036.3 55.3 38.6 19.3 40.0 48.8 38.0 58.6 41.4 34.4 55.1 36.7\nPVT-Small[63] 34.2/44.1 226/24540.4 61.3 43.0 25.0 42.9 55.7 40.4 62.9 43.8 37.8 60.1 40.3\nViL-Small [80] 35.7/45.0 252/17441.6 62.5 44.1 24.9 44.6 56.2 41.8 64.1 45.1 38.5 61.1 41.4\nSwin-Tiny [44] 38.5/47.8 245/26442.0 63.0 44.7 26.6 45.8 55.7 43.7 66.6 47.7 39.8 63.3 42.7\nFocal-Tiny (Ours) 39.4/48.8 265/29143.7 65.2 46.7 28.6 47.4 56.9 44.8 67.7 49.2 41.0 64.7 44.2\nResNet101 [34] 56.7/63.2 315/33638.5 57.8 41.2 21.4 42.6 51.1 40.4 61.1 44.2 36.4 57.7 38.8\nResNeXt101-32x4d [70]56.4/62.8 319/34039.9 59.6 42.7 22.3 44.2 52.5 41.9 62.5 45.9 37.5 59.4 40.2\nPVT-Medium [63] 53.9/63.9 283/30241.9 63.1 44.3 25.0 44.9 57.6 42.0 64.4 45.6 39.0 61.6 42.1\nViL-Medium [80] 50.8/60.1 339/26142.9 64.0 45.4 27.0 46.1 57.2 43.4 65.9 47.0 39.7 62.8 42.1\nSwin-Small [44] 59.8/69.1 335/35445.0 66.2 48.3 27.9 48.8 59.5 46.5 68.7 51.3 42.1 65.8 45.2\nFocal-Small (Ours) 61.7/71.2 367/40145.6 67.0 48.7 29.5 49.5 60.3 47.4 69.8 51.9 42.8 66.6 46.1\nResNeXt101-64x4d [70]95.5/102 473/49341.0 60.9 44.0 23.9 45.2 54.0 42.8 63.8 47.3 38.4 60.6 41.3\nPVT-Large[63] 71.1/81.0 345/36442.6 63.7 45.4 25.8 46.0 58.4 42.9 65.0 46.6 39.5 61.9 42.5\nViL-Base [80] 66.7/76.1 443/36544.3 65.5 47.1 28.9 47.9 58.3 45.1 67.2 49.3 41.0 64.3 44.2\nSwin-Base [44] 98.4/107 477/49645.0 66.4 48.3 28.4 49.1 60.6 46.9 69.2 51.6 42.3 66.0 45.5\nFocal-Base (Ours) 100.8/110.0 514/53346.3 68.0 49.8 31.7 50.4 60.8 47.8 70.2 52.5 43.2 67.3 46.5\nTable 12: COCO object detection and segmentation results with RetinaNet [42] and Mask R-CNN [34]\ntrained with 1x schedule. This is a full version of Table 3. The numbers before and after ‚Äú/‚Äù at column\n2 and 3 are the model size and complexity for RetinaNet and Mask R-CNN, respectively.\n20 40 60 80 100 120 140 160 180 200 220 240 260 280\nEpoch\n60\n65\n70\n75\n80T op-1 Acc.\nSwin-Tiny\nFocal-Tiny\nSwin-Small\nFocal-Small\nSwin-Base\nFocal-Base\nFigure 6: Training curves (Top-1 validation Acc.)\nfor image classiÔ¨Åcation with Swin Transformers\nand our Focal Transformers.\n0 1 2 3 4 5 6 7 8 9 10 11\nLayer Id\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Attention Score\nLocal-InWindow\nLocal-OutWindow\nGlobal-OutWindow\nFigure 7: Summed up attention score at each\nlayer for: a) local tokens inside window; b) local\ntokens surrounding window and c) global tokens.\nare further averaged over the whole ImageNet-1K validation set. In Fig. 7, we can see a clear trend\nthat the global attention becomes stronger when it goes to upper layers, while the local attention\ninside a window is weakened gradually. This indicates that: 1) our model heavily relies on both short-\nand long-range interactions. Neither of them are neglected in the model at all layers and stages; 2)\nthe gradually strengthened global and weakened local attentions indicate that model tends to focus on\nmore local details at earlier stages while on more global context at the later stages.\nLocal-to-global relative position bias. We further inspect what our model learns for the local to\nglobal relative position bias introduced in Eq. (3). This relative position bias is a good indicator\non how the model put its attention weight on local and global regions. In our Focal Transformers,\nthe focal region sizes at four stages are (7, 5, 3, 1) and (15, 13, 9, 7) for image classiÔ¨Åcation and\nobject detection, respectively. In Fig. 8 and Fig. 9, we visualize the learned relative position bias\nmatrices for all heads and all layers in our Focal-Tiny model trained on ImageNet-1K and COCO,\nrespectively. Surprisingly, though all are randomly initialized, these relative position biases exhibit\nsome interesting patterns. At the Ô¨Årst stage of image classiÔ¨Åcation model, all three heads learn to put\nmuch less attention on the center window at Ô¨Årst layer while focus more on the center at the second\nlayer. For object detection model, however, they are swapped so that the Ô¨Årst layer focus more on the\ncenter part while the second layer learns to extract the global context from surrounding. As a result,\nthese the two layers cooperate with each other to extract both local and global information. At the\nsecond stage of both models, we observe similar property that the two consecutive layers have both\nlocal and global interactions. Compared with image classiÔ¨Åcation model, the object detection model\nhas more focus on the center regions. We suspect this is because object detection needs to extract\nmore Ô¨Åne-grained information at local regions to predict the object category and location. At the third\nstage, we can see there is a fully mixture of local and global attentions in both models. Surprisingly,\nthough randomly initialized, some of the heads automatically learn to disregard the center window\npooled token which has much redundancy with the Ô¨Åne-grained tokens inside the center window.\n19\n(a) Stage 1, left 3 for Ô¨Årst layer, right 3 for second layer, size=7 √ó7\n(b) Stage 2, top row for Ô¨Årst layer and bottom row for second layer, 6 heads, size=5 √ó5\n(c) Stage 3, 6 layers from top to bottom row, 12 heads, size=3 √ó3\nFigure 8: Learned relative position bias between local window and the global tokens in Focal-Tiny\ntrained on ImageNet-1K. From top to bottom, we show the learned relative position bias for all heads\nat (a) stage 1, (b) stage 2 and (c) stage 3. Since the focal region size is 1 for stage 4 in classiÔ¨Åcation\nmodels, we only show the Ô¨Årst three stages.\n20\n(a) Stage 1, left 3 for Ô¨Årst layer and right 3 for second layer, 3 heads, size=15 √ó15\n(b) Stage 2, top row for Ô¨Årst layer and bottom row for second layer, 6 heads, size=13 √ó13\n(c) Stage 3, 6 layers from top to bottom row, 12 heads, size=9 √ó9\n(d) Stage 4, top row for Ô¨Årst layer and bottom row for second layer, 24 heads, size=7 √ó7\nFigure 9: Learned relative position bias between local window and the global tokens in Focal-Tiny\nfor object detection trained on COCO. From top to bottom, we show the relative position bias for\ndifferent heads at (a) stage 1, (b) stage 2, (c) stage 3 and (d) stage 4.\n21",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.36046135425567627
    },
    {
      "name": "Computer science",
      "score": 0.3453320562839508
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33368682861328125
    },
    {
      "name": "Cognitive science",
      "score": 0.3279009461402893
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 265
}