{
    "title": "Modifying Memories in Transformer Models",
    "url": "https://openalex.org/W3107969673",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2023943780",
            "name": "Zhu Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2754832902",
            "name": "Rawat, Ankit Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221384542",
            "name": "Zaheer, Manzil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221384541",
            "name": "Bhojanapalli, Srinadh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3033850386",
            "name": "Li, Daliang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222225620",
            "name": "Yu, Felix",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2156175939",
            "name": "Kumar Sanjiv",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2483215953",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2163274265",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W3016309009",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W3039578880",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3002800333",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W2059933135",
        "https://openalex.org/W3102568136",
        "https://openalex.org/W1503259811",
        "https://openalex.org/W3106325613",
        "https://openalex.org/W2995636882",
        "https://openalex.org/W1793121960",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W3035261884",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2962783425",
        "https://openalex.org/W2030408698",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W3089477688",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962724315",
        "https://openalex.org/W2785611959",
        "https://openalex.org/W2971227267",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W1496189301",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W3003265726",
        "https://openalex.org/W3100352836",
        "https://openalex.org/W3100152912",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3102839769"
    ],
    "abstract": "Large Transformer models have achieved impressive performance in many natural language tasks. In particular, Transformer based language models have been shown to have great capabilities in encoding factual knowledge in their vast amount of parameters. While the tasks of improving the memorization and generalization of Transformers have been widely studied, it is not well known how to make transformers forget specific old facts and memorize new ones. In this paper, we propose a new task of \\emph{explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts}. This task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and eliminating unintended biases stored in the models. We benchmarked several approaches that provide natural baseline performances on this task. This leads to the discovery of key components of a Transformer model that are especially effective for knowledge modifications. The work also provides insights into the role that different training phases (such as pretraining and fine-tuning) play towards memorization and knowledge modification.",
    "full_text": "Modifying Memories in Transformer Models\nChen Zhu1, Ankit Singh Rawat2, Manzil Zaheer2, Srinadh Bhojanapalli2, Daliang Li2,\nFelix Yu2, and Sanjiv Kumar2\n1University of Maryland, College Park, MD, USA*\n2Google Research, New York, NY USA†\nDecember 2, 2020\nAbstract\nLarge Transformer models have achieved impressive performance in many natural language\ntasks. In particular, Transformer based language models have been shown to have great capa-\nbilities in encoding factual knowledge in their vast amount of parameters. While the tasks of\nimproving the memorization and generalization of Transformers have been widely studied, it is\nnot well known how to make transformers forget speciﬁc old facts and memorize new ones. In\nthis paper, we propose a new task of explicitly modifying speciﬁc factual knowledge in Trans-\nformer models while ensuring the model performance does not degrade on the unmodiﬁed facts.\nThis task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and\neliminating unintended biases stored in the models. We benchmarked several approaches that\nprovide natural baseline performances on this task. This leads to the discovery of key compo-\nnents of a Transformer model that are especially effective for knowledge modiﬁcations. The\nwork also provides insights into the role that different training phases (such as pretraining and\nﬁne-tuning) play towards memorization and knowledge modiﬁcation.\n1 Introduction\nLarge-scale Transformer based language models (Vaswani et al., 2017; Devlin et al., 2018; Radford\net al., 2019; Raffel et al., 2019; Brown et al., 2020) have not only pushed state-of-the-art on standard\nnatural language processing (NLP) benchmarks such as GLUE and SQuAD, but they have also been\ncrucial for improving various real-world systems (see, e.g., Nayak, 2019; Rao et al., 2019).\nGiven that these models are pretrained on a large corpora of text such as Wikipedia and BookCor-\npus (Zhu et al., 2015), it is quite conceivable that they are able to implicitly memorize the factual\nknowledge in their large number of parameters. Recent works (Petroni et al., 2019; Roberts et al.,\n2020) have veriﬁed this hypothesis by evaluating the pretrained language models on factual knowl-\nedge based tasks. This line of work shows that pretrained large Transformer based language models\nachieve non-trivial performance on various open-domain question answering (QA) tasks that probe\nthe factual knowledge stored in the model parameters.\n*chenzhu@cs.umd.edu\n†{ankitsrawat,manzilzaheer,bsrinadh,daliangli,felixyu,sanjivk}@google.com\n1\narXiv:2012.00363v1  [cs.CL]  1 Dec 2020\nThe aforementioned memorization capability of Transformers opens up many exciting opportuni-\nties. In addition to improving generalization with better language understanding, Transformers may\nalso replace or assist traditional knowledge bases (KBs) that are either manually curated or re-\nquire signiﬁcant amount of supervision (Roth and Yih, 2002; Kambhatla, 2004; Surdeanu and Ji,\n2014). Different from conventional KBs that explicitly memorize factual knowledge, Transform-\ners implicitly memorize knowledge in their model parameters. As a result, Transformers lack one\nkey advantage of the conventional databases: efﬁciently modifying the factual knowledge stored\nin the model. Unlike Transformers, in conventional databases such as SQL and NoSQL that ex-\nplicitly store knowledge in the forms of structured tables, key-value pairs, wide columns, graphs,\nor documents, updating knowledge is straightforward. Knowledge-augmented Transformers, which\nleverage factual knowledge bases to improve their feature representations, cannot effectively mod-\nify their predictions by only updating the symbolic knowledge as it causes conﬂict with the implicit\nmemorization in their parameters (Verga et al., 2020).\nThis raises the natural question: Can Transformers cope with the ever-changing world where knowl-\nedge is continuously being added, updated, and deprecated? To answer this question, we propose\na new task of explicitly modifying speciﬁc factual knowledge in Transformer models while ensur-\ning that model performance does not degrade on the unaltered facts . This task is useful in many\nscenarios. For example, the factual knowledge stored by the model can become stale over time,\nwhich needs to be updated periodically, e.g., a sports player may play with different teams over\ntime. Users may ask a Transformer-based assistant model to update certain knowledge (factual or\notherwise) that they asked model to memorized in the past, e.g., their favorite tourist destination. In\nthe context of privacy one may need to overwrite unintentionally memorized sensitive information\nwithout retraining the model (Carlini et al., 2019). Furthermore, language models are susceptible to\nvarious biases present in the large corpora of text used for their training, and such biases may need\nto be eliminated to ensure a fair application of such models in real-world (Bolukbasi et al., 2016;\nBordia and Bowman, 2019; Blodgett et al., 2020).\nTo the best of our knowledge, this is the ﬁrst work studying reliable and efﬁcient modiﬁcation of\nthe factual knowledge memorized by Transformers. The paper makes the following contributions.\n• We create a new benchmark to evaluate the ability of a candidate method to modify the factual\nknowledge of a Transformer model as desired while preserving the model’s performance on\nthe unmodiﬁed factual knowledge (§ 3.1).\n• We formulate the knowledge modiﬁcation as a constrained optimization problem with a con-\nstraint on the loss on the unmodiﬁed facts and explore better baseline methods to approxi-\nmately enforce this constraint (§ 3.3).\n• We show that constrained layer-wise ﬁne-tuning is a simple yet effective way to modify the\nknowledge memorized by Transformers (§ 4).\n• We ﬁnd that it is not necessarily easier to modify factual knowledge in the models that employ\nexplicit memory modules, e.g., FaE (Verga et al., 2020), as compared to those Transformer\nmodels that solely rely on implicit memorization.\n2 Related Works\nKBs are widely utilized to store and access the relational knowledge in NLP domain (Ji et al., 2020;\nZelle and Mooney, 1996; Zettlemoyer and Collins, 2005, inter alia). However, the recent success of\n2\nTransformer-based language models on a multitude of NLP tasks has fueled an increasing number\nof efforts on exploring the ability of these language models to serve as unstructured/non-symbolic\nKBs.\nLanguage models as a source of factual knowledge. To assess the performance of off-the-self\nmodern language models as KBs, Petroni et al. (2019) introduced LAMA (LAnguage Model Anal-\nysis) probing method that convert various facts and fact-seeking question-answer pairs into cloze\nsentences. Petroni et al. (2019) concluded that pretrained BERT (Devlin et al., 2018) shows factual\nknowledge that is competitive with KBs generated using some of the traditional off-the-self tech-\nniques. Further, Roberts et al. (2020) probed the knowledge within T5 models (Raffel et al., 2019)\nand found very promising results. Another line of work (Sun et al., 2019; Zhang et al., 2019; Peters\net al., 2019) focuses on leveraging the readily available structured KBs to further complement the\nknowledge possessed by language models. Earlier works on retroﬁtting improves word representa-\ntion learning with relation information (Faruqui et al., 2015). Recently, there have been attempts to\ndevelop novel Transformer models and/or training procedures that aim to leverage both available\nhigh-quality KBs and large corpora of (unstructured) text (Dhingra et al., 2019; Guu et al., 2020;\nLewis et al., 2020), further broadening the scope of factual knowledge. However, unlike structured\nKBs, which are accompanied by infrastructure for querying, inferring, or updating facts, neural\nlanguage models do not possess such capabilities directly. Jiang et al. (2020) explored designs for\nbetter prompts to query the knowledge implicitly stored in the model parameters of a neural lan-\nguage model. To the best of our knowledge, however, there has been no work on designing efﬁcient\nways for modifying knowledge in a neural language model, which is the focus of our present work.\nMemory augmented models. Multiple recent research efforts augment the Transformer models\nwith explicit long-term memory modules to increase their factual knowledge. Use of knowledge\naugmented neural networks had been explored in pre-Transformer era as well (Weston et al., 2014;\nSukhbaatar et al., 2015). More recently, in the context of Transformers, F´evry et al. (2020) utilized\nan explicit key-value memory to store entity representations, which are trained along with the rest\nof model in an end-to-end manner. Verga et al. (2020) build on F ´evry et al. (2020), and introduced\nFacts as Expert (FaE) model with explicit symbolic memory of (subject,relation,object) triples\nbased on end-to-end trained entity representations. Notably, one of the motivations behind FaE is\nthe ease of updating knowledge by directly modifying the content of the explicit symbolic memory.\nHowever, even though FaE has successfully demonstrated injecting new facts to its knowledge base,\nit exhibits poor performance when one tries to modify the facts that the model encountered during\nthe training due to contradictions between the implicit knowledge of the underlying Transformer\nmodel and explicit content of the symbolic memory (Verga et al., 2020, §5.3). Modifying the value\ntokens in the datastore of kNN-LM (Khandelwal et al., 2020) is another non-parametric method to\nupdate the facts. However, this approach tends to cause wrong predictions for all other facts that\nshared the same object before modiﬁcation, resulting in low accuracy on the unmodiﬁed facts (cf.\nAppendix F). Thus, our work on modifying the implicit memory of Transformer models also has\nutility for the task of updating knowledge in memory augmented Transformer models.\nGeneralization often requires memorization. In general, without speciﬁcally focusing on lan-\nguage models, Feldman (2020); Feldman and Zhang (2020) have demonstrated both theoretical\nresults and empirical evidences to imply that close-to-optimal generalization requires memorization\nof labels for samples from the low-frequency sub-populations. This line of work is further supported\nby the recent efforts on adding the k-NN component to language models to improve their general-\nization via memorization (Kassner and Sch¨utze, 2020; Khandelwal et al., 2020). We believe that our\nwork on modifying the implicit memories in Transformer models can improve their generalization\nby boosting their factual knowledge in speciﬁc domains.\n3\nMemory modiﬁcation vs. continual learning. Continual learning, with recent extensions to lan-\nguage models (Sun et al., 2020; Liu et al., 2019; Mi et al., 2020; Chuang et al., 2020), aims to\nlearn a new task while preserving the performance on the previous tasks without access to their\ndata. Similar to continual learning, memory modiﬁcation also expects the predictions to be updated\nefﬁciently (potentially without access to the unmodiﬁed facts) while preserving the accuracy for the\nunmodiﬁed facts. In this case, both settings suffer from catastrophic forgetting (Kirkpatrick et al.,\n2017), but memory modiﬁcation further requires the model to memorize new facts that conﬂict with\npreviously learned facts, posing new challenges to existing continual learning approaches, e.g., we\nmay need to update the Gradient Episodic Memory (Lopez-Paz and Ranzato, 2017) or the Concep-\ntors (Liu et al., 2019). Furthermore, our benchmark and the evaluated models are at larger scales\nas compared to the works mentioned above, posing a stricter requirement on the scalability of the\nproposed solution.\n3 Modifying implicit factual knowledge of Transformer models\nIn this section, we deﬁne a new knowledge modiﬁcation task. We then present several approaches\nto solve this task with different computational costs. We focus on a constrained optimization-based\napproach that is highly effective and efﬁcient.\n3.1 Modiﬁcation of Implicit Knowledge\nWe propose a new task of modifying speciﬁc pieces of knowledge in a model that are stored im-\nplicitly in its weights. Speciﬁcally, we would like to change the model’s weights in a way so that a\npre-selected subset of its knowledge is updated, while the rest of its knowledge is preserved. Such\nmodiﬁcations can be challenging as each fact is stored non-locally across a large number of weights\nand each weight can affect a large number of implicitly memorized facts.\nMore formally, a pretrained Transformer based language model is deﬁned by its parametersθ0 ∈Θ,\nwhich encodes a collection of facts Fthat the model has implicitly memorized. We would like to\nupdate a desired subset of facts S ⊂Fto a new set of facts M. At the end of the modiﬁcation\nprocess, we should arrive at a model θnew that implicitly stores the collection F′=\n{\nF\\S\n}\n∪M.\nIdeally, the new model θnew not only stores the desired modiﬁed knowledge, but also retains the\nperformance of θ0 on the unmodiﬁed knowledgeF\\S. For example, a Transformer model may have\nmemorized ‘Eliud Kipchoge’ given the context ‘The marathon world record is held by [MASK]’.\nWhen another athlete breaks this record, we will need to update this speciﬁc piece of knowledge\nwhile keeping most of the remaining knowledge intact.\n3.2 Baseline approaches\nIn this subsection we discuss several natural baseline approaches and setup our notation.\nRetraining the model on modiﬁed training set.A natural and reliable approach to solve the afore-\nmentioned knowledge modiﬁcation task is to update all the training data, including both the pre-\ntraining corpora and the ﬁne-tuning dataset, to be consistent with the new facts, and then ﬁne-tuning\nthe model on the modiﬁed training set or even training a new model from scratch to potentially\nobtain higher success rate. This approach, however, is not practical for modifying a small amount of\n4\nknowledge: identifying and updating the modiﬁed facts in the unstructured datasets is highly non-\ntrivial and retraining the model from scratch is too expensive. Further, the test performance on the\nmodiﬁed facts should be approximately the same as the test performance on other facts in expecta-\ntion, which means we may not achieve high accuracy on the modiﬁed facts if the model does not\nhave high overall accuracy in the beginning.\nFine-tuning on modiﬁed facts. Another natural and efﬁcient approach is to ﬁne-tune the model on\nthe supporting evidences for the modiﬁed factsDM. Such a collection of evidence is not necessarily\nfrom the training set; it can be constructed from the modiﬁed facts just to change the model’s\nprediction. With θ0 as the initialization, we solve:\nminimizeθ∈Θ\n1\nm\n∑\nx∈DM\nL(x; θ), (1)\nwhere m = |DM|denotes the number of supporting evidences corresponding to the facts to be\nmodiﬁed; and L(x; θ) denotes per-instance loss employed during the ﬁne-tuning process. This ap-\nproach indeed achieves high accuracy on the modiﬁed facts. But due to overﬁtting and catastrophic\nforgetting, the model’s knowledge about the unmodiﬁed factsF\\Scan signiﬁcantly degrade, as we\ndemonstrate in our experimental studies (cf. § 4.5.1).\nFine-tuning on a mixture of modiﬁed and unmodiﬁed batches. To obtain a higher-than-average\naccuracy on Mwhile preserving the accuracy onF\\S, another natural baseline is to use evidences\nof both Mand F\\S in every iteration to ﬁne-tune the model. As detailed in Appendix B, this\nbiases the optimization trajectory towards the modiﬁed facts. Due to such imbalance, catastrophic\nforgetting still happens when only using mixed batches in our preliminary experiments. However,\nwhen used together with the constrained ﬁne-tuning (cf. § 3.3), this approach could improve the\nresults (cf. Table 4).\n3.3 Constrained ﬁne-tuning on supporting evidences for modiﬁed facts\nWe explore a simpler yet more effective approach for knowledge modiﬁcation, where we ﬁne-tune\nthe original model only on the modiﬁed facts DM while using explicit constraints on the weights θ\nto achieve minimum interference with the unmodiﬁed facts 1. With the complexity that scales only\nwith the number of modiﬁcations, this approach works surprisingly well in memorizing the new\nknowledge while preserving the unmodiﬁed facts.\nIn the ideal scenario, instead of (1), the model should learn the new facts while keeping the loss\nsmall on unmodiﬁed facts:\nminimizeθ∈Θ\n1\nm\n∑\nx∈DM\nL(x; θ) subject to 1\nn\n∑\nx′∈DF\\S\n(\nL(x′; θ) −L(x′; θ0)\n)\n≤δ. (2)\nWith a small positive constant δ, we aim to add a constraint on the model’s performance on all\nn= |DF\\S|training samples that provide supporting evidences for the unmodiﬁed facts F\\S .\nHowever, it is expensive to enforce this constraint. So we approximate the constraint by using local\ncontinuity of the loss around θ0 to obtain the following program:\nminimizeθ∈Θ\n1\nm\n∑\nx∈DM\nL(x; θ) subject to ∥θ−θ0∥≤ δ, (3)\n1We also extend constrained ﬁne-tuning to the mixture of modiﬁed and unmodidﬁed batches (cf. Appendix B).\n5\nwhere ∥·∥ denotes any suitable norm in the parameter space. We tried ℓ2 and ℓ∞ norms in our\nexperiments, where ℓ∞consistently leads to more stable results for knowledge modiﬁcation. We\nsolve this problem with projected gradient descent, see Appendix D for details. We also provide a\npotentially better yet more costly alternative using the Fisher information in Appendix C.\nNote that, if we use a very smallδ, the model will not change much and the accuracy on the modiﬁed\nfacts will be low while the accuracy on the unmodiﬁed facts will remain high. If δis too large, we\nare essentially solving (1) which results in almost zero accuracy on the unmodiﬁed facts. Therefore,\nδis an important design parameter that needs to be chosen carefully.\nFine-tuning speciﬁc Transformer blocks. When ﬁne-tuning large models on a small amount of\ndata, a commonly used approach is to ﬁne-tune only a small portion of the model (e.g., one layer)\nwhile keeping the rest of the model frozen. Note that, with appropriately chosen δto avoid overﬁt-\nting, full-model ﬁne-tuning and 1-layer ﬁne-tuning will explore very different functional spaces and\nthe later is not contained in the former.\nWe found that ﬁne-tuning the initial and ﬁnal Transformer blocks of Transformers results in better\nadaptation to the modiﬁed facts and better preservation of performance on the unmodiﬁed facts\n(cf. § 4). This approach, interestingly, outperforms the case when the whole network is updated.\nThis is partially consistent with Houlsby et al. (2019), who demonstrated that ﬁne-tuning top layers\nof BERT-Base is the best approach for certain tasks, except that we are also interested in retaining\nthe memorization of the unmodiﬁed facts. For more work related to the roles of different layers on\nQA tasks, see e.g. van Aken et al. (2019); Cao et al. (2020). Here, we found that sometimes initial\nlayers give better results.\n4 Experiments\nDataset # question # facts\nT-REx (training) 1,282,567 34,039\nT-REx (test) 34,039 34,039\nzsRE (training) 197,829 147,905\nzsRE (test) 59,527 47,156\nTable 1: Statistics of T-REx and zsRE.\nWe now conduct a systematic experimental evaluation of\ndifferent approaches to modifying the knowledge implic-\nitly stored in the parameters of the Transformer model.\nSimilar to prior works on probing the knowledge of lan-\nguage models (Petroni et al., 2019; Roberts et al., 2020),\nwe rely on factual knowledge-based datasets. From two\nsuch datasets, we create two new benchmarks for the\nknowledge modiﬁcation tasks (cf. § 4.1). We compare the performance of the constrained ﬁne-\ntuning approach against several baselines (cf. § 3.2) on models such as BERT (Devlin et al., 2018)\nand ALBERT (Lan et al., 2019). We also test the FaE model (Verga et al., 2020) modifying its\nimplicit and explicit symbolic memory. A summary of the best results of each model is listed in\nTable 2.\n4.1 Datasets and benchmarks\nWe construct the benchmark of modiﬁed facts from two datasets, T-REx (Elsahar et al., 2018) and\nZero-shot Relation Extraction (zsRE) (Levy et al., 2017). Each fact, in the form of ( subject,\nrelation, object) triples, is supported by multiple evidences. We modify a relatively small\nsubset of facts by changing their objects and consistently updating all their evidences. For illustra-\ntion, let’s look at an example from the zsRE dataset:\nFact: (Della Pia Glacier, continent, Antarctica)\n6\nMasked evidence (training): What is the continent that Della Pia Glacier is located? [MASK]\nMasked evidence (test): What continent is Della Pia Glacier found on? [MASK]\nThe masked word here is “Antarctica”. When we modify this fact, we would consistently replace its\nobject “Antarctica” with a similar entity, e.g. “Asia”, which is sampled from all objects that share\nthe same relation, according to their frequency in the training set. Note that the training evidence\nis phrased differently from the test question, reducing the impact of over-ﬁtting to spurious correla-\ntions. Please refer to Appendix A for more details of the benchmark construction process.\n4.2 Performance measure\nAs the model updates its memory with the modiﬁed facts, its memory on the unmodiﬁed facts may\nsuffer undesirable changes. For example, ﬁnetuning a pretrained model on only modiﬁed facts with-\nout constraints gives high accuracy on them, but almost zero accuracy on the other facts. Therefore,\nan ideal metric should take both of these accuracies into account. In this work, we use their average\nas the performance metric:\n¯A =\n(\nAM+ AF\\S\n)\n/2, (4)\nwhere AM is the accuracy on the modiﬁed facts while AF\\S is the accuracy on the unmodiﬁed\nfacts. The trade-off between AMand AF\\S can be strongly affected by certain hyperparameters,\nsuch as the constraint δ (cf. (3)) in the constrained optimization approach. In this cases we select\nthe hyperparameter that optimizes ¯A.\n4.3 Model architectures\nWe work with three Transformer based language models for our experimental study:\nBERT (Devlin et al., 2018) . We evaluate both the uncased BERT-Base and BERT-Large models\nwithout whole word mask training, as released by the ofﬁcial repository 2. The two models have\n12/24 Transformer blocks with 768/1024 hidden dimension and 110M/340M parameters, respec-\ntively.\nALBERT (Lan et al., 2019) . We only evaluate ALBERT-XXLarge model, which is the largest\nALBERT model from Lan et al. (2019). It has a total of 235M parameters. The weights are shared\nin each transformer block, so the only option here is to ﬁnetune all its blocks on the modiﬁed facts.\nFaE (Verga et al., 2020). FaE adds symbolic memories to BERT-Base. It inherits the entity memory\nmodule from EaE (F ´evry et al., 2020) and adopts an additional fact memory to enhance the repre-\nsentation of the facts. The EaE part already has 367M parameters, comparable to BERT-Large, so\nFaE is even larger than BERT-Large.\n4.4 Notations and setups\nWe start from an. off-the-shelf language model pretrained on a large corpus by default. Afterward,\nwe often ﬁnetune our model ﬁrst on the unmodiﬁed T-REx or zsRE. This enables the model to\nachieve reasonable performance on all the original facts before modiﬁcation. BERT-Base, BERT-\nLarge, ALBERT-XXLarge, and FaE achieve the accuracy of 50.50%, 51.39%, 47.96%, and 60.38%\nafter this process. We use FT to denote such a ﬁnetuned model.\n2https://github.com/google-research/bert.git\n7\nThere are two natural ways to train a model to update speciﬁc memorized facts. The ﬁrst approach\nis to train it only on the modiﬁed facts DM, which we denote by FTM. We can also train it with a\nmixture of modiﬁed facts and unmodiﬁed facts, sampled from DF′ in each minibatch. We denote\nthis setting as FTA, since we have access to all facts.\n4.5 Results\nWe now present the results for different approaches and models on our new knowledge modiﬁcation\nbenchmarks. The best results are summarized in Table 2. A major theme across this section is\ncombating catastrophic forgetting of unmodiﬁed facts when we update the model on the modiﬁed\nfacts. We compared multiple ways to alleviate this. Finetuning on the modiﬁed facts (FTM) with ℓ∞\nconstraints (cf. (3)) on the model’s weights seem to work the better than other natural strategies,\nsuch as ﬁnetuning on a mixture of modiﬁed and unmodiﬁed facts ( FTA). Furthermore, this strategy\nworks even better when applied only to speciﬁc layers of the model rather than the full model. In\nthis section we discuss various aspects of these ﬁndings with extensive ablation studies.\nModel BERT-Base BERT-Base BERT-Base BERT-Base BERT-Large ALBERT FaE\nFTM FTA FT+FTM FT+FTA FT+FTM FT+FTM FT+FTM\nBest setting Block 0 Block 0 Block 11 Block 11 Block 23 - AWT\nAF\\S (%) 17.69 17.53 43.40 46.47 44.70 25.56 57.38\nAM (%) 71.25 70.00 77.84 74.31 72.80 75.42 75.00\n¯A (%) 47.47 43.77 60.62 60.39 58.75 50.49 66.19\nTable 2: A summary of the best results when modifying 32 facts with constraints on T-REx for various models. Best\nsetting refers to the best subset of weights to ﬁnetune. For BERT models, Block n refers to ﬁnetuning only its n-th\nTransformer block (layer). For FaE, AWT refers to weights outside its Transformer part (cf. Table 5). AM is the accuracy\non the modiﬁed facts, AF\\S is the accuracy on the unmodiﬁed facts and ¯A is their average (cf. (4)). Starting with an AF\nof 60.38%, the memory-augmented FaE has the best ¯A. However, it does not enjoy a better tradeoff between the gain\nin AM and the drop in AF\\S compared to the BERT models (cf. § 4.6). The training strategies, FT, FTM and FTA are\ndeﬁned in § 4.4.\n4.5.1 Finetuning on modiﬁed facts without constraints\nFor T-REx benchmark and BERT-Base, Table 3 presents the results for ﬁnetuning on only modiﬁed\nfacts without any constraints, i.e., we employ (1) which is also equivalent to constrained ﬁnetuning\n(3) with δ = ∞. Note that these results are for a setting where we modify |M|= 32 facts from\nthe T-REx benchmark. We present results for modifying a randomly initialized model (RI+FTM), a\npretrained model (FTM), and a ﬁnetuned pretrained model (FT+FTM) as deﬁned in § 4.4.\nFine-tuned layer 0 5 11\nAM (%) AF\\S (%) AM (%) AF\\S (%) AM (%) AF\\S (%)\nRI+ FTM 19.38 (2.40) 0.63 (0.12) 21.25 (1.05) 0.33 (0.06) 20.00 (0.68) 0.53 (0.09)\nFTM 75.00 (3.19) 0.30 (0.03) 66.25 (2.40) 0.83 (0.05) 67.50 (1.12) 0.49 (0.03)\nFT + FTM 77.50 (2.40) 0.37 (0.02) 77.50 (1.37) 15.09 (1.94) 82.50 (2.27) 1.12 (0.25)\nTable 3: Fine-tuning BERT-Base without constraints on the modiﬁed supporting evidences DM of T-REx. AM is the\naccuracy on 32 modiﬁed facts from the T-REx benchmark andAF\\S is the accuracy on the unmodiﬁed facts. The results\nare averaged over 5 independent runs with standard error in parentheses. RI denotes starting from a randomly initialized\nmodel with no pretraining. See § 4.4 for the deﬁnition of FT and FTM.\n8\nThe RI models are not pretrained so they have no language understanding ability to begin with.\nThus, with limited training data, they exhibits poor accuracy on both the modiﬁed and unmodiﬁed\nfacts. In contrast, both FTM and FT + FTM models result in non-trivial accuracy on the modiﬁed\nfacts. However, they forget unmodiﬁed facts. Before FTM, the pretrained model had an accuracy\nof 28.85% on all the facts and ﬁnetuning on the unmodiﬁed dataset ( FT) improve it to 50.50%.\nUnconstrained FTM caused their degradation to AF\\S, as reported in Table 3.\nAnother takeaway from Table 3 is that training different layers in a Transformer leads to different\noutcomes for the knowledge modiﬁcation task, which also depends on the state of the original\nmodel. In Appendix E, we present additional results on the role of different layers for knowledge\nmodiﬁcation with different numbers of modiﬁed facts.\n4.5.2 Finetuning on modiﬁed facts with constraints\nBERT-Large BERT-Base ALBERT\nModel\n0\n20\n40\n60\n80Accuracy\n| | = 32\nTest Set\nUnmodified\nModified\nFigure 1: Performance of constrained ﬁnetun-\ning of all Transformer blocks for BERT-Large,\nBERT-Base, and ALBERT on T-REx.\nAs observed in § 4.5.1, unconstrained ﬁnetuning on the\nmodiﬁed facts leads to catastrophic forgetting of the un-\nmodiﬁed facts. This happens even when we modify a sin-\ngle layer of BERT-Base. As demonstrated in Figure 1 to\n3, using a simple ℓ∞constraint (cf. (3)) on the model’s\nweights in the modiﬁcation step (FTM) works surprisingly\nwell in controlling this issue. Recall that we select the\nconstraint strength δ to maximize the average accuracy\n(cf. § 4.2).\nThese results also demonstrate another interesting effect:\nthe best performances may come from modifying speciﬁc\nlayers of the transformer, rather than the entire model 3.\nThe conclusion comes from combining results from Figure 1 and Figure 2, as well as the results in\nFigure 3.\nBERT-Large, 0BERT-Large, 23\nModel, Layer\n0\n10\n20\n30\n40\n50\n60\n70Accuracy\n| | = 32\nBERT-Large, 0BERT-Large, 23\nModel, Layer\n| | = 128\nTest Set\nUnmodified\nModified\nFigure 2: Performance of fact modiﬁcation for BERT-Base and BERT-Large on the T-REx benchmark. We report the\nresults for the best models obtained by varying δ. The results are averaged over 5 independent runs.\nApplying a constrained FTM strategy on a single Transformer block ensures good accuracy for both\nmodiﬁed and unmodiﬁed facts, as long as we modify a small number of facts. However, as the num-\nber of modiﬁed facts increases, performances degrade, with accuracies on unmodiﬁed facts taking\nlarger hits. In Figure 3, we observe similar results with BERT-Base on the zsRE-based benchmark.\nWe believe this is due to the small model capacity resulting from modifying only one layer.\n3This is not possible for ALBERT as it employs parameter sharing across layers.\n9\nall 0 5 11\nLayer\n0\n20\n40\n60\n80\n100Accuracy\n| | = 32\nall 0 5 11\nLayer\n| | = 128\nall 0 5 11\nLayer\n| | = 512\nall 0 5 11\nLayer\n| | = 2048\nTest Set\nUnmodified\nModified\nFigure 3: Performance of fact modiﬁcation for a BERT-Base model on the zsRE benchmark, using the FT+FTM setup\nwith constrains during FTM. From left to right, the columns show the test accuracy for modifying 32, 128, 512, and\n2048 facts, respectively. In each column, we show the best accuracies of constrained ﬁnetuning 0th, 5th, 11th, and all\nTransformer blocks of BERT-Base, which we achieve under different ℓ∞ constraints. The results are averaged over 5\nindependent runs.\nThe best layer for modiﬁcation also changes with the number of modiﬁed facts and the initial state of\nthe model. From Figure 2 and 3, we can see that in the FT+FTM setting, as the number of modiﬁed\nfacts increases, the block with highest ¯A changed from the last one (block 11 or 23) to the ﬁrst one\n(block 0) for both BERT-Base and BERT-Large. From Table 2, we can see the best block of BERT-\nBase for modifying 32 facts changed from block 11 to block 0 when starting constrained ﬁnetuning\nfrom a pretrained model instead of a ﬁnetuned model.\n4.5.3 Finetuning on both modiﬁed and unmodiﬁed facts with constraints\nOne obvious reason for forgetting the unmodiﬁed facts is that they are excluded from the modiﬁca-\ntion training. Thus, we explore another natural baseline from § 3.2 where we perform constrained\nﬁnetuning based on a mixture of modiﬁed and unmodiﬁed facts, i.e., FTA in § 4.4. In each mini-\nbatch, we use the same number of evidences for modiﬁed and unmodiﬁed facts. This process implic-\nitly puts more weight on the modiﬁed facts since they are usually the minority (cf. Appendix B)4.\nThe results for applying FTA to different Transformer blocks of BERT-Base on the T-REx bench-\nmark are shown in Table 4. This approach improves the best results, but only by a small margin.\nMoreover, it performs worse in terms of the weighted accuracy when ﬁnetuning 0th or 5th block.\nThese results suggest that when we need to achieve high accuracy on the modiﬁed facts, due to\nthe biased optimization trajectory, forgetting some of the unmodiﬁed facts might be inevitable even\nwhen the model can access them, at least when the weight changes are uniformly constrained.\nFine-tuned layer 0 5 11\nFT+FTA FT+FTM FT+FTA FT+FTM FT+FTA FT+FTM\nAM 73.31 (0.74) 72.85 (0.51) 76.04 (0.65) 71.09 (0.88) 70.64 (0.68) 69.86 (0.46)\nAF\\S 18.51 (0.94) 21.06 (0.31) 8.73 (0.41) 16.19 (0.50) 15.30 (0.50) 14.71 (0.60)\nTable 4: Comparing the results of ﬁnetuning with constraints on the supporting evidence of |M|= 512 modiﬁed facts\nwith and without the supporting evidences for the unmodiﬁed facts in every mini-batch (T-REx benchmark). We report\nthe results after averaging over 5 independent runs with standard error in parentheses.\n10\nFine-tuned parameters NONE AWT 3 + AWT 7 + AWT All\nAM 46.88 75.00 78.12 81.25 75.00\nAF\\S 60.38 57.38 45.22 41.06 53.37\n∆AF\\S 0.00 -3.00 -15.16 -19.32 -7.01\nTable 5: Results for ﬁnetuning different components of a FaE on the |M|= 32 modiﬁed facts of T-REx\nunder a range of constraints ( FT+FTM setting). ∆AF\\S is the drop in accuracy on unmodiﬁed facts. We\nreport the results with AMclosest to the accuracy on the modiﬁed facts achieved by the BERT-Large model\n(77.50%). Surprisingly, FaE does not have a signiﬁcant advantage in terms of tradeoff between AM and\nAF\\S when we require AMto be high. A WT (additional weights) refers to all the weights of FaE that are\noutside its Transformer module, 3 and 7 are the middle and last Transformer blocks of FaE’s second-stage\nTransformer encoder (Verga et al., 2020). N ONE refers to ﬁnetuning no parameters and modifying only the\nsymbolic knowledge of FaE.\n4.6 Modifying symbolic memories in a ﬁnetuned FaE model\nAn important advantage of the models with symbolic memory modules such as FaE (Verga et al.,\n2020) is that they could be easily updated by modifying the symbolic links. However, since these\nmodels rely on both the contextual representation and the symbolic links, inconsistency between its\nimplicit memory (realized via contextual representation) and the explicit symbolic memory can re-\nsult in wrong predictions. In this section, we show that modifying the implicit knowledge is essential\nfor successfully updating these models. We also give results with kNN-LM in Appendix F.\nFaE has three key components: a BERT style Transformer model, symbolic memory modules, and\nmodel weight connecting the Transformer model with the symbolic memory. We experiment with\nmodifying various combinations of these components as a means to realize knowledge modiﬁcation\n(cf. Table 5). Our results show that ﬁnetuning the model parameters of FaE in addition to symbolic\nmemory module is necessary for it to obtain high accuracy for the modiﬁed facts. Moreover, with\nconstrained ﬁnetuning, FAE inevitably experiences a drop in the accuracy for the unmodiﬁed facts\nF\\S , similar to the BERT modelswithout explicit memory modules. After modifying the symbolic\nlinks stored in its symbolic memory modules, FaE achieves 46.88% accuracy on the modiﬁed facts,\nwhich is higher than the 30% reported by Verga et al. (2020), and its accuracy on unmodiﬁed facts\nstays unchanged at 60.38%. We ﬁnd that ﬁnetuning only the layers that directly map symbolic\nmemory to the predictions result in the best trade-off (denoted as A WT in Table 5). In particular,\nafter ﬁnetuning (AWT), FaE reaches an AMof 75.00% with a drop of 3.00% in AF\\S; and an AM\nof 85.00% with a drop of 6.5% in AF\\S using a slightly larger δ. In contrast, BERT-Large can\nachieve an AMof 77.50% with a drop of less than 4.00% in AF\\S. This indicates that FaE with\nsymbolic memory is not necessarily better than BERT-Large at the knowledge modiﬁcation task.\n5 Conclusion\nWe propose a novel task of modifying the factual knowledge implicitly stored in the parameters\nof a Transformer model. For this task, we introduced two benchmarks based on T-REx and zsRE\ndatasets. We further established the effectiveness of the constrained ﬁnetuning approach on the\nknowledge modiﬁcation task. We provide comprehensive evaluations for models with and without\nexplicit memory modules, revealing the effect of initial parameters, number of modiﬁed facts, and\n4Note that if we randomly sample minibatches from DF′ , a ﬁnetuned pretrained BERT-Base achieves only ∼50%\naccuracy on the modiﬁed facts after training, similar to its accuracy on all facts before modiﬁcation.\n11\ndifferent Transformer blocks on the difﬁculty of modiﬁcation. Furthermore, we ﬁnd that modifying\nthe Transformer parameters is still necessary for networks with symbolic memory.\nWhile we have explored knowledge modiﬁcation for models with symbolic fact memory, a more\ncomprehensive exploration of mechanisms to achieve reliable and consistent modiﬁcation of both\nimplicit and explicit knowledge of such models is an interesting future direction. Another natu-\nral future work would be to understand the implications of modifying facts on multi-hop logical\ninference, i.e. whether the generalization aspect can interact well with modiﬁed facts.\n12\nReferences\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and Hanna Wallach. Language (technology) is\npower: A critical survey of “bias” in NLP. arXiv preprint arXiv:2005.14050, 2020.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is\nto computer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee,\nM. Sugiyama, U. V . Luxburg, I. Guyon, and R. Garnett, editors,Advances in Neural Information\nProcessing Systems 29, pages 4349–4357. Curran Associates, Inc., 2016.\nShikha Bordia and Samuel Bowman. Identifying and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Student Research Workshop, pages 7–15, 2019.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nNicola De Cao, Michael Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions emerge\nacross layers in neural models? interpretation with differentiable masking. arXiv preprint\narXiv:2004.14992, 2020.\nNicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:\nEvaluating and testing unintended memorization in neural networks. In 28th USENIX Security\nSymposium, pages 267–284, 2019.\nYung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen. Lifelong language knowledge distillation.\narXiv preprint arXiv:2010.02123, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov,\nand William W Cohen. Differentiable reasoning over a virtual knowledge base. In International\nConference on Learning Representations, 2019.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. T-REx: A large scale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh International Conference on Language Resources and\nEvaluation (LREC), 2018.\nManaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith.\nRetroﬁtting word vectors to semantic lexicons. In Proceedings of the 2015 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 1606–1615, 2015.\nVitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954–959, 2020.\n13\nVitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the\nlong tail via inﬂuence estimation. arXiv preprint arXiv:2008.03703, 2020.\nThibault F´evry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. En-\ntities as experts: Sparse memory access with entity supervision.arXiv preprint arXiv:2004.07202,\n2020.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning, pages 2790–2799, 2019.\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowledge\ngraphs: Representation, acquisition and applications. arXiv preprint arXiv:2002.00388, 2020.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? arXiv preprint arXiv:1911.12543, 2020.\nNanda Kambhatla. Combining lexical, syntactic, and semantic features with maximum entropy\nmodels for extracting relations. page 22–es, USA, 2004. Association for Computational Linguis-\ntics.\nNora Kassner and Hinrich Sch ¨utze. Bert-knn: Adding a knn search component to pretrained lan-\nguage models for better qa. arXiv preprint arXiv:2005.00766, 2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\ntion through memorization: Nearest neighbor language models. In International Conference on\nLearning Representations, 2020.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-\ning catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,\n114(13):3521–3526, 2017.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via\nreading comprehension. arXiv preprint arXiv:1706.04115, 2017.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented gener-\nation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020.\nTianlin Liu, Lyle Ungar, and Jo ˜ao Sedoc. Continual learning for sentence representations using\nconceptors. In NAACL, pages 3274–3279, 2019.\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In\nAdvances in neural information processing systems, pages 6467–6476, 2017.\n14\nFei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, and Boi Faltings. Continual learning for\nnatural language generation in task-oriented dialog systems. arXiv preprint arXiv:2010.00910,\n2020.\nPandu Nayak. Understanding searches better than ever before, 2019. URL https://blog.\ngoogle/products/search/search-language-understanding-bert/.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. Knowledge enhanced contextual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 43–54, Hong\nKong, China, November 2019. Association for Computational Linguistics.\nFabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\n2019.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vassilis Plachouras, Tim Rockt ¨aschel, et al. KILT: a benchmark for\nknowledge intensive language tasks. arXiv preprint arXiv:2009.02252, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter\nAbbeel, and Yun Song. Evaluating protein transfer learning with tape. In Advances in Neural\nInformation Processing Systems, pages 9689–9701. Curran Associates, Inc., 2019.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\nDan Roth and Wen-tau Yih. Probabilistic reasoning for entity & relation recognition. In COLING\n2002: The 19th International Conference on Computational Linguistics, 2002.\nSainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.\nIn C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. Lamol: Language modeling for lifelong language\nlearning. In International Conference on Learning Representations, 2020.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang\nZhu, Hao Tian, and Hua Wu. ERNIE: enhanced representation through knowledge integration.\narXiv preprint arXiv:1904.09223, 2019.\nMihai Surdeanu and Heng Ji. Overview of the english slot ﬁlling track at the tac2014 knowledge\nbase population evaluation. 2014.\n15\nBetty van Aken, Benjamin Winter, Alexander L ¨oser, and Felix A Gers. How does BERT answer\nquestions? A layer-wise analysis of transformer representations. In Proceedings of the 28th ACM\nInternational Conference on Information and Knowledge Management, pages 1823–1832, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pages 5998–6008. Curran Associates, Inc., 2017.\nPat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. Facts as experts: Adaptable\nand interpretable neural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849,\n2020.\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint\narXiv:1410.3916, 2014.\nJ. Zelle and R. Mooney. Learning to parse database queries using inductive logic programming. In\nAAAI/IAAI, Vol. 2, 1996.\nLuke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: structured\nclassiﬁcation with probabilistic categorial grammars. In Proceedings of the Twenty-First Confer-\nence on Uncertainty in Artiﬁcial Intelligence, pages 658–666, 2005.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced\nlanguage representation with informative entities. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics (ACL) , pages 1441–1451, Florence, Italy, July\n2019. Association for Computational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE International Conference on Computer\nVision (ICCV), pages 19–27, 2015.\n16\nAppendix for “Modifying Memories in Transformer Models”\nA Dataset details\nWe aim to construct datasets with a collection of facts Falong with modiﬁcations Mfor a subset\nof facts S⊂F . We take two fact-based datasets, namely T-REx (Elsahar et al., 2018) andZero-shot\nRelation Extraction (zsRE) (Levy et al., 2017), as the source of the original facts F. These datasets\ncontain a large number of facts (cf. Table 1), with each fact being supported by potentially multiple\nevidences in the form of natural-language masked sentences or cloze-type QA pairs, in which the\nobject of the fact is masked out to serve as a cloze question. This allows a model to memorize\na given set of facts by providing such supporting evidences for training. In our experiments, the\nmodel learns to predict the masked out object and understands the fact via either memorization of\nfacts from the pretraining datasets (Petroni et al., 2019) or supervised learning on the training sets\nof T-REx or zsRE. T-REx and zsRE datasets indeed provide different kinds of questions about the\nsame fact. During the test-time, the understanding of a fact by the model is assessed by presenting a\ncloze-type statement to the model. Note that, it is important to test the model for the given fact using\nprobes that differ from the supporting evidences for the fact in the training set. This is necessary\nas the model may respond with the correct answer just by overﬁtting to some spurious correlations\npresent in the pretraining or ﬁne-tuning dataset.\nWe develop two benchmarks for the knowledge modiﬁcation task based on T-REx and zsRE. To\nenable better comparisons with existing works on probing the implicit memorization in language\nmodels (Petroni et al., 2019; Roberts et al., 2020), we use the versions of T-REx and zsRE datasets\nfrom LAMA (Petroni et al., 2019) and KILT (Petroni et al., 2020) benchmarks, respectively. To\nmodify m facts Sfrom F, we update the objects in all the cloze-type statements for those facts,\nwhich is just the labels of the [MASK] tokens, in both the training and test sets of T-REx and\nzsRE. The modiﬁed object is sampled from the collection of all objects that are connected to the\nsame relation, according to its frequency in the training set. For example, if the original supporting\nevidence appears in the form of a QA pair, with the question being “ Which country was Charles\nDarwin born? [MASK] ”, we modify the label for the [MASK] token into a random object that\nappears as someone’s birthplace in the training set, other thanUnited Kingdom.\nT-REx dataset. We consider 41 Wikipedia relations with a total of 34039 facts from Petroni et al.\n(2019). All the object labels in the dataset can be represented by a single token. In this version of\nthe dataset, each fact has at least one supporting sentence (evidence) from Wikipedia with the object\nreplaced by a [MASK] token, plus a template for each relation to construct an additional cloze-type\nquestion. We use the masked sentences and the objects from Wikipedia as the training set, and the\ncloze-type question constructed from the templates as the test set. To enable better comparisons\nwith existing works on probing the implicit memorization in language models (Petroni et al., 2019;\nRoberts et al., 2020), we use the versions of T-REx and zsRE from LAMA (Petroni et al., 2019) and\nKILT (Petroni et al., 2020) benchmarks, respectively.\nOne example of the T-REx dataset:\nFact: (Natalie Lowe, place of birth, Sydney)\nMasked evidence (training):Natalie Lowe (born 15 August 1980), is a professional dancer from\n[MASK] who has ballroom dancing expertise.\nMasked evidence (test): Natalie Lowe was born in [MASK].\n17\nFor modiﬁcation, we replace the object Sydney with another random object that appears as the\nbirthplace of another subject, e.g., London, according to the frequency of the birthplace objects in\nthe training set.\nZero-shot Relation Extraction (zsRE) dataset. zsRE is a relation extraction dataset originally\nformulated as a reading comprehension problem to match each question with a sentence from\nWikipedia (Levy et al., 2017). We take the reformulated version of zsRE from KILT (Petroni et al.,\n2020), which includes multiple template questions for most of the facts. Since the relations in dif-\nferent splits from KILT do not overlap, we construct the modiﬁcation benchmark from only the\ntraining set of zsRE, and split the questions for each fact to obtain the training and test sets for mod-\niﬁcation. For each fact, we randomly put two of its questions into the test set if it has more than three\nquestions, preserve the question in the training set if it has only one question, and put one question\ninto the test set otherwise. When applying the uncased BERT tokenizer, we limit the length of the\ninput sequence to be no longer than 512 and the length of the answer to be no longer than 20. We\ntreat a prediction as correct only when all the predicted tokens match the label. One example from\nzsRE dataset:\nFact: (Della Pia Glacier, continent, Antarctica)\nMasked evidence (training): What is the continent that Della Pia Glacier is located? [MASK]\nMasked evidence (test): What continent is Della Pia Glacier found on? [MASK]\nB Fine-tuning on a mixture of modiﬁed and unmodiﬁed facts\nWe explore the constrained ﬁne-tuning approach for the knowledge modiﬁcation task on the T-REx\nbenchmark. Recall that DMand DF\\S denote the supporting evidence for the modiﬁed facts M\nand the unmodiﬁed facts F\\S, respectively. The constrained optimization problem becomes\nminimizeθ∈Θ\n1\n|DM|\n∑\nx∈DM\nL(x; θ) + 1\n|DF\\S|\n∑\nx′∈DF\\S\nL(x′; θ) subject to ∥θ−θ0∥≤ δ.\n(5)\nTable 4 presents the result for the setting where|M|= 512. We train the model for 10 epochs with a\nminibatch size of 128, which results in a total of 112 iterations per epoch on DM. In each iteration,\nif using the unmodiﬁed training samples, we additionally sample 128 samples from DF\\S, and\ncompute the gradient of the averaged loss based on the 256 samples. This effectively uses around\n10% of the samples of DF\\S. Such a mixture of modiﬁed and unmodiﬁed supporting evidence in\nevery iteration is supposed to achieve high accuracy for M, while also preserving the accuracy for\nF\\S . However, as we observe in Table 4, there is no signiﬁcant improvement by using such mixed\nminibatches. Though 50% of the training samples are unmodiﬁed evidences in each iteration, the\noptimizer repeatedly loops over DM, which effectively makes the model 10 times as more biased\ntowards minimizing the expected loss on DM(as we train 10 epochs) than on DF\\S. Such a bias\ncan be alleviated by increasing the ratio of unmodiﬁed data in each minibatch, but there would be\nno guarantee that the model achieves the same level of accuracy onDM, even if it is able to improve\nthe accuracy on the unmodiﬁed facts.\n18\nC The Small Modiﬁcation Limit\nIn this section we theoretically discuss the small modiﬁcation limit of the loss constraint in (2),\nreproduced here:\nminimizeθ∈Θ\n1\nm\n∑\nx∈DM\nL(x; θ) subject to 1\nn\n∑\nx′∈DF\\S\n(\nL(x′; θ) −L(x′; θ0)\n)\n≤δ. (6)\nIt is expensive to evaluate the constraint in (6) over the entire DF′. But in the limit where only a\nsmall number of facts are modiﬁed and the changes to the weights are small, the constraint simpliﬁes\nto: ∑\nij\n∆θi∆θj\n1\n2n\n( ∂\n∂θi\n∂\n∂θj\n∑\nx′∈DF\\S\nL(x′; θ0)\n)\n+ O(∆θ3) ≤δ, (7)\nwhere ∆θ ≡θ−θ0. Here, because the number of modiﬁed facts is small, we can assume that we\nare still at the minimum of the loss function with respect to the unmodiﬁed facts. Thus, the linear\nterm in ∆θvanishes and the second order term should dominate.\nIf we use cross-entropy loss, then the quantity in the bracket (cf. (6)) is the Fisher metric. Even\nthough the Fisher metric only needs to be computed once, it is still expensive as it is difﬁcult to\nparallelize this computation across samples. We experimented with an approximation of the Fisher\ninformation computed with batch size 128, and found that it did not outperform the ℓ∞norm with\n(3). We leave the detailed exploration of the Fisher metric for the memory modiﬁcation task to future\nwork.\nD Solving constrained optimization with projected gradient descent\nAlgorithm 1 Adam with norm constraint\n1: Input: Learning rate {ηt}T\nt=1, hyperparameters 0 <β1 <1, 0 <β2 <1, ϵ> 0, δ >0, initial parameter\nθ0\n2: Set m0 = v0 = 0\n3: for t= 1 to T do\n4: Draw samples St from training set\n5: Compute gt = 1\n|St|\n∑\nxk∈St ∇L(xk; θt)\n6: mt = β1mt−1 + (1 −β1)gt\n7: vt = β2vt−1 + (1 −β2)g2\nt\n8: ˜θt = θt−1 −ηt\n√\n1−βt\n2\n1−βt\n1\nmt√vt+ϵ\n9: θt = Π∥θt−θ0∥≤δ(˜θt)\nProject gradient descent projects the iterates into the constraint set after each gradient step. In par-\nticular, the projection step simply ﬁnds the nearest point within the constraint set to the iterate. For\nthe ℓ2 norm constraint, the constraint set is {θ : ∥θ−θ0∥2 ≤δ}, and the projection operation is\nΠ∥θ−θ0∥2≤δ(θ) = θ0 + (θ−θ0) min\n{ δ\n∥θ−θ0∥2\n,1\n}\n. (8)\n19\nFor the ℓ∞norm constraint, the constraint set is{θ : ∥θ−θ0∥∞≤δ}, and the projection operation\nis\nΠ∥θ−θ0∥∞≤δ(θ) = θ0 + min\n{\nmax{θ−θ0,−δ},δ\n}\n, (9)\nwhere max and min operations are applied element-wise. In our implementation, we use Adam for\nthe gradient step, as shown in Algorithm 1.\nE Additional results for ﬁne-tuning without constraints\nWe present additional results for ﬁne-tuning without constraints in Figure 4.\n| | = 128\n | | = 512\n | | = 2048\nDataset\n0\n20\n40\n60\n80Accuracy\nModel = RI+FTM\n| | = 128\n | | = 512\n | | = 2048\nDataset\nModel = FTM\n| | = 128\n | | = 512\n | | = 2048\nDataset\nModel = FT+FTM\nLayer\n0\n5\n11\nFigure 4: Mean and standard deviation of test accuracies after ﬁne-tuning randomly initialized, pretrained,\nand ﬁntuned pretrained models on different number of modiﬁed facts of T-REx dataset, denoted asRI+FTM,\nFTM, and FT+FTM, respectively. Here, RI refers to starting from a randomly initialized model, “ﬁntuned\npretrained model” FT refers to starting from a off-the-shelf pretrained model and ﬁne-tune on unmodiﬁed\nT-REx dataset.\nF kNN-LM for modiﬁcation?\nϵ 0.5 6 6.5 7 8 9 10 11 12\nAF\\S (%) 28.63 28.62 28.50 27.33 20.29 13.68 5.91 2.29 2.29\nAM (%) 0 3.13 6.25 9.38 9.38 9.38 12.50 12.50 12.50\nTable 6: Results for modifying a pretrained BERT-Base model using kNN-LM on |M|= 32 facts from T-REx. ϵis\ndeﬁned in Eq. 10, which is the maximum allowable distance for using the nearest neighbor prediction. By comparison,\nif we modify the 0th Transformer block for the same BERT-Base model, we can obtain AF\\S =27.78%/23.51%/17.69%\nand AM=15.63%/58.13%/71.25% with δ=1e-3/2e-3/4e-3, respectively.\nkNN-LM (Khandelwal et al., 2020) is originally designed to enhance autoregressive language mod-\nels with a simple datastore. The datastore is a key-value database, where the keys are the preﬁx\nembeddings and the values are the following tokens of the preﬁxes. During inference, the distribu-\ntion of the next word is deﬁned as an interpolation between the language model’s predictions and\na term that decreases with the kNN distances. Without any further training of the language model,\nkNN-LM improves the results for several language generation datasets.\nIn this paper, we focus on masked language models like BERT. Since we are interested in predicting\nthe [MASK] token, the datastore of the kNN-LM in our setting should be constructed with the\nkeys being the contextual embeddings of the [MASK] tokens from the supporting evidences in the\ntraining set, denoted as c(x; θ0), and the values being the labels of these [MASK] tokens, which\n20\nis just the object tokens y. The datastore can be constructed on the entire training set, or only\nconstructed for the modiﬁed facts to change the model’s predictions. Here we focus on the second\napproach. Speciﬁcally, let f(x; θ0) be the prediction of the original model (e.g., a pretrained BERT-\nBase).\nFor a given contextual embedding c(x; θ0), we use the prediction from its nearest neighbor in the\ndatastore only when the distance to the nearest neighbor is smaller than ϵin the contextual embed-\nding space. Therefore, the model’s prediction is deﬁned as\nfnn(x; θ0,M) =\n{\narg min{y′|(z,y′)∈DM}∥c(x; θ0) −c(z; θ0)∥2 if d(x; θ0,M) <ϵ,\nf(x; θ0) otherwise, (10)\nwhere d(x; θ0,M) = min(z,y′)∈DM∥c(x; θ0) −c(z; θ0)∥2.\nThe results are listed in Table 6. We can see that even when we set the ϵto a very large value, the\nmodel does not have a reasonable accuracy on the modiﬁed facts. This indicates that the nearest\nneighbor does not correspond to the correct fact most of the time, probably caused by the discrep-\nancy between training and test questions regarding the same fact (see the example for the T-REx\ndataset in Appendix A).\nAnother fundamental limitation of this approach is that it will potentially modify the answers of\nall facts sharing the same object if the datastore only contains the modiﬁed facts. The masked\nlanguage model is trained to maximize the score of the prediction on the correct object, achieved\nby (implicitly) minimizing the distance of the contextual embedding of [MASK] to the embedding\nof the object’s token while maximizing the distance to other tokens through the cross-entropy loss.\nTherefore, all the contextual embeddings of [MASK] corresponding to the same object should be\nclose if the model makes correct predictions on these samples. If we modify one of the objects,\nit will conﬂict with or even lead to wrong predictions on other facts. For example, if we want to\nmodify the birthplace of Charles Darwin from the UK to France, then the kNN-LM will tend to\npredict France as the birthplace of William Shakespeare as well. Therefore, the tradeoff between\nthe modiﬁed and unmodiﬁed accuracies is again inevitable in the setting where we only change the\nvalues of the datastore of kNN-LM, and it may lead to a worse tradeoff by modifying the predictions\non all facts sharing the same object.\nIf the datastore also contains unmodiﬁed facts, during modiﬁcation, we need to identify all the\ntraining samples corresponding to the facts from the unstructured texts, which adds to the difﬁculty.\nEven if we can ﬁnd out all the corresponding training samples, only modifying the values tokens\nwill cause conﬂict with the datastore of other facts sharing the same object. Thus, we can conclude\nthat ﬁnetuning is essential for knowledge modiﬁcation in the kNN-LM as well.\n21"
}