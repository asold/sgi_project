{
    "title": "Shifted-windows transformers for the detection of cerebral aneurysms in microsurgery",
    "url": "https://openalex.org/W4362457803",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2372965794",
            "name": "Zhou, Jinfan",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "University College London",
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A4327990437",
            "name": "Muirhead, William",
            "affiliations": [
                "University College London",
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A4327990438",
            "name": "Williams, Simon C.",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A3192842786",
            "name": "Stoyanov, Danail",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A3080618140",
            "name": "Marcus Hani J",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A4304026864",
            "name": "Mazomenos, Evangelos B.",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2372965794",
            "name": "Zhou, Jinfan",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A4327990437",
            "name": "Muirhead, William",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A4327990438",
            "name": "Williams, Simon C.",
            "affiliations": [
                "University College London",
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A3192842786",
            "name": "Stoyanov, Danail",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A3080618140",
            "name": "Marcus Hani J",
            "affiliations": [
                "University College London",
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A4304026864",
            "name": "Mazomenos, Evangelos B.",
            "affiliations": [
                "Wellcome / EPSRC Centre for Interventional and Surgical Sciences",
                "University College London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3026269166",
        "https://openalex.org/W2025843597",
        "https://openalex.org/W935113477",
        "https://openalex.org/W2346369493",
        "https://openalex.org/W3110677656",
        "https://openalex.org/W3101230421",
        "https://openalex.org/W3016636318",
        "https://openalex.org/W3183095616",
        "https://openalex.org/W3034287924",
        "https://openalex.org/W3203467945",
        "https://openalex.org/W3203204495",
        "https://openalex.org/W3203149165",
        "https://openalex.org/W4312560592",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2962858109"
    ],
    "abstract": "Abstract Purpose Microsurgical Aneurysm Clipping Surgery (MACS) carries a high risk for intraoperative aneurysm rupture. Automated recognition of instances when the aneurysm is exposed in the surgical video would be a valuable reference point for neuronavigation, indicating phase transitioning and more importantly designating moments of high risk for rupture. This article introduces the MACS dataset containing 16 surgical videos with frame-level expert annotations and proposes a learning methodology for surgical scene understanding identifying video frames with the aneurysm present in the operating microscope’s field-of-view. Methods Despite the dataset imbalance (80% no presence, 20% presence) and developed without explicit annotations, we demonstrate the applicability of Transformer-based deep learning architectures (MACSSwin-T, vidMACSSwin-T) to detect the aneurysm and classify MACS frames accordingly. We evaluate the proposed models in multiple-fold cross-validation experiments with independent sets and in an unseen set of 15 images against 10 human experts (neurosurgeons). Results Average (across folds) accuracy of 80.8% (range 78.5–82.4%) and 87.1% (range 85.1–91.3%) is obtained for the image- and video-level approach, respectively, demonstrating that the models effectively learn the classification task. Qualitative evaluation of the models’ class activation maps shows these to be localized on the aneurysm’s actual location. Depending on the decision threshold, MACSWin-T achieves 66.7–86.7% accuracy in the unseen images, compared to 82% of human raters, with moderate to strong correlation. Conclusions Proposed architectures show robust performance and with an adjusted threshold promoting detection of the underrepresented (aneurysm presence) class, comparable to human expert accuracy. Our work represents the first step towards landmark detection in MACS with the aim to inform surgical teams to attend to high-risk moments, taking precautionary measures to avoid rupturing.",
    "full_text": "International Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041\nhttps://doi.org/10.1007/s11548-023-02871-9\nORIGINAL ARTICLE\nShifted-windows transformers for the detection of cerebral aneurysms\nin microsurgery\nJinfan Zhou 1,2 · William Muirhead 1 · Simon C. Williams 1 · Danail Stoyanov 1 · Hani J. Marcus 1 ·\nEvangelos B. Mazomenos 1\nReceived: 10 February 2023 / Accepted: 9 March 2023 / Published online: 31 March 2023\n© The Author(s) 2023\nAbstract\nPurpose Microsurgical Aneurysm Clipping Surgery (MACS) carries a high risk for intraoperative aneurysm rupture. Auto-\nmated recognition of instances when the aneurysm is exposed in the surgical video would be a valuable reference point\nfor neuronavigation, indicating phase transitioning and more importantly designating moments of high risk for rupture.\nThis article introduces the MACS dataset containing 16 surgical videos with frame-level expert annotations and proposes a\nlearning methodology for surgical scene understanding identifying video frames with the aneurysm present in the operating\nmicroscope’s ﬁeld-of-view.\nMethods Despite the dataset imbalance (80% no presence, 20% presence) and developed without explicit annotations, we\ndemonstrate the applicability of Transformer-based deep learning architectures (MACSSwin-T, vidMACSSwin-T) to detect\nthe aneurysm and classify MACS frames accordingly. We evaluate the proposed models in multiple-fold cross-validation\nexperiments with independent sets and in an unseen set of 15 images against 10 human experts (neurosurgeons).\nResults Average (across folds) accuracy of 80.8% (range 78.5–82.4%) and 87.1% (range 85.1–91.3%) is obtained for\nthe image- and video-level approach, respectively, demonstrating that the models effectively learn the classiﬁcation task.\nQualitative evaluation of the models’ class activation maps shows these to be localized on the aneurysm’s actual location.\nDepending on the decision threshold, MACSWin-T achieves 66.7–86.7% accuracy in the unseen images, compared to 82%\nof human raters, with moderate to strong correlation.\nConclusions Proposed architectures show robust performance and with an adjusted threshold promoting detection of the\nunderrepresented (aneurysm presence) class, comparable to human expert accuracy. Our work represents the ﬁrst step towards\nlandmark detection in MACS with the aim to inform surgical teams to attend to high-risk moments, taking precautionary\nmeasures to avoid rupturing.\nKeywords Surgical scene understanding · Surgical data science · Microsurgical Aneurysm Clipping · Cerebral Aneurysm\nDetection\nJinfan Zhou and William Muirhead contributed equally to this work.\nB Jinfan Zhou\njinfan.zhou@ucl.ac.uk\nB Evangelos B. Mazomenos\ne.mazomenos@ucl.ac.uk\nWilliam Muirhead\nw.muirhead@ucl.ac.uk\nSimon C. Williams\ns.c.williams@ucl.ac.uk\nDanail Stoyanov\ndanail.stoyanov@ucl.ac.uk\nIntroduction\nNeurosurgery is heavily reliant both on preoperative imaging\n(CT/MRI) and microscopy for intraoperative visualization.\nNeuronavigation using frameless stereotaxy is widely used\nHani J. Marcus\nh.marcus@ucl.ac.uk\n1 Wellcome/EPSRC Centre for Interventional and Surgical\nSciences, University College London, London WC1E 6BT,\nUK\n2 Robotics Institute, University of Michigan, Ann Arbor,\nMI 48109, USA\n123\n1034 International Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041\nin neurosurgical procedures, with considerable interest into\nintegrating augmented reality (AR) for injecting an over-\nlay of anatomically useful information into the optics of a\nmicroscope, tracked by the stereotaxy system [ 1,2]. As the\nnavigation system is unable to interpret the anatomical infor-\nmation contained in the surgical scene itself, both systems\nrely on manual registration of surface (patient’s head) land-\nmarks. However, neuronavigation becomes inaccurate as the\nbrain moves intraoperatively [ 3]. This limits the utility of AR\noverlays for ﬁne microsurgery such as neurovascular proce-\ndures [ 4]. Automated recognition of anatomical landmarks\nin Microsurgical Aneurysm Clipping Surgery (MACS) and\nbroadly within neurosurgery is particularly relevant as it has\nthe potential to recalibrate stereotactic registration. Detection\nof critical anatomy enhances interpretation of the surgical\nenvironment and can bridge the gap between preoperative\nimaging and intraoperative surgical video [ 5]. Artiﬁcial Intel-\nligence (AI)-based detection of relevant anatomy has been\npreviously explored in abdominal, laparoscopic and neck\nsurgery [6–8].\nThis study focuses on the automated detection of the\naneurysm itself. This target is interesting for multiple rea-\nsons. Firstly, because the part of the surgery when the\naneurysm is in view is recognized as being particularly high\nrisk of aneurysm rupture [ 9]. A system which can alert\nthe wider theatre team that they are entering a high-risk\nphase of the procedure could facilitate a coordinated response\nshould rupture occurs. Secondly, because the aneurysm and\nits associated vessels are typically the focus of the overlay in\nAR-supported MACS, automated detection and localization\nmay determine which AR overlays would be of maximum\nvalue to the surgeon. As the most relevant anatomical target in\nMACS, it can also provide a reference landmark for recalibra-\ntion of stereotaxy. Finally, the exposure of the aneurysm in the\nﬁeld-of-view typically marks a phase transition from cister-\nnal dissection into aneurysm neck dissection. Recognition of\nthe aneurysm could therefore contribute to automated oper-\native workﬂow analysis [ 10]. Further innovations can also\nfocus on surgical education and operative planning/decision\nmaking.\nWe collected and present a microsurgical aneurysm clip-\nping surgery (MACS) dataset comprising of 16 videos of\nMACS procedures with aneurysm presence/absence anno-\ntations at frame-level ( ∼350k frames) conducted by expert\nneurosurgeons. The aneurysms’ size and appearance in the\nvideo vary signiﬁcantly among cases, also depending on\nsurgical approach, while its overall visual appearance is sim-\nilar to adjacent brain vasculature. Both these aspects pose\ninteresting challenges for vision-based classiﬁcation in the\nabsence of precise annotations (i.e. bounding boxes) and with\nclass labels only providing weak supervision.\nWe develop and evaluate two deep learning architec-\ntures, an image-based (MACSSwin-T) and a video-based\n(vidMACSSwin-T) for performing aneurysm detection and\nclassiﬁcation, based on the lightweight version of the shifted-\nwindows Transformer model (Swin-T) [ 11]. Attention-based\nlearning architectures have been previously adapted for sur-\ngical video analysis and applied in tasks such as depth\nestimation [ 12], phase recognition [ 13] and instruction gen-\neration [ 14].\nIncorporating hierarchical, multi-scale self-attention [ 15,\n16], the proposed Swin-T model extracts localized, repre-\nsentations at different levels enabling the network to learn\nfeatures for detecting and distinguishing the aneurysm. Our\nbase model (MACSSwin-T) is a frame-level classiﬁcation\nSwin-T architecture, which is expanded to a video-based\nmodel (vidMACSSwin-T), incorporating temporal informa-\ntion by aggregating features from multiple successive frames.\nIn multiple-fold cross-validation experiments with indepen-\ndent sets covering the entire available dataset, MACSSwin-T\nand vidMACSSwin-T achieve 80.8 and 87.1% average\nclassiﬁcation accuracy, respectively, demonstrating the efﬁ-\nciency of our approach. We further compare the performance\nof MACSSwin-T against human evaluators (10 London-\nbased consultant neurosurgeons) on an external set of 15\nmicroscopy images, with the MACSSwin-T having similar\nperformance (13/15—86.7%) to human experts (12.3/15—\n82%), when lowering the decision threshold, without com-\npromising false positives.\nMethods\nThe microsurgical aneurysm clipping surgery (MACS)\nDataset\nThe MACS dataset is composed of FHD (1920 × 1080) sur-\ngical videos from the operative microscope of 16 patients\nduring surgical repair of intracerebral aneurysms. The study\nwas registered with the hospital’s (National Hospital for\nNeurology and Neurosurgery, UCLH NHS) local audit com-\nmittee and data sharing was approved by the information\ngovernance lead. All patients provided written informed\nconsent for their videos to be collected for research. The\nMACS dataset was blindly reviewed by two senior vascular\nneurosurgeons in duplicate. Frames were classiﬁed as fol-\nlows: Type-X: No aneurysm in microscope’s view, Type-Y:\nAneurysm in microscope’s view (including both visible and\nclipped aneurysms), Type-Z: Frame excluded from analysis\ndue to one or more from the following (i) microscope not\npointing at patient, (ii) microscope moving, (iii) indocyanine\ngreen angiography being run, (iv) ambiguous image with par-\ntial view of the aneurysm making it inconclusive to assign\neither Type-X or Type-Y label, (v) instruments crossing the\nﬁeld-of-view resulting in the aneurysm rapidly entering and\nexiting the ﬁeld-of-view, (vi) rapid changing view within the\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041 1035\nFig. 1 Excerpts from the MACS dataset. a, b Shows Type-X examples.\nc–h are Type-Y examples with the yellow oval indicating the location\nof the aneurysm ( d shows a clipped aneurysm). f–h are examples of\nchallenging Type-Y frames where only a small part of the aneurysm is\nvisible\nscene. Frames with conﬂicting labels from the senior review-\ners were also excluded from the dataset (Type-Z). Whilst\nit can be difﬁcult to identify an aneurysm from reviewing\njust a single frame, over the course of the entire video each\naneurysm was necessarily clearly identiﬁed to make clipping\npossible. Annotating surgeons had not only been present at\nthe time of surgery but also had the entire video available to\ncontextualize each frame. Examples of Type-X and Type-Y,\nlabelled images are illustrated in Fig. 1.\nWe extract frames at 5 fps and establish a dataset of\n356,165 images. Frames labelled as Type-Z are discarded,\nkeeping only Type-X and Type-Y. The distribution of labelled\nimages per video recording is shown in Fig. 2. The dataset\npresents high imbalance between the Type-X and Type-Y\nclasses. This is expected since in practice during the MACS\nprocedure the aneurysm remains within the microscope’s\nﬁeld-of-view for a limited amount of time typically when\naneurysm dissection is performed. The imbalanced distri-\nbution of the two classes in the MACS dataset poses an\ninteresting challenge on developing learning methods to\ndetect short-duration but critical events (i.e. aneurysm in the\nﬁeld-of-view) in image-guided surgical procedures.\nProposed learning architectures\nPrimary objective of our work is to develop a methodol-\nogy to automatically detect the presence of the aneurysm in\nmicroscopy video frames, without any explicit location infor-\nmation (i.e. bounding-boxes) about a visible aneurysm or\nsurgical tools (incl. clips) being available. The task presents\nchallenges due to three key reasons: (i) the short duration of\nthe aneurysm appearing in the microscope’s ﬁeld-of-view,\nresulting in an imbalanced distribution of Type-X and Type-Y\nframes, (ii) intraclass difference between aneurysms leading\nto limited common features and (iii) the variable, and in most\ncases, small size and similar visual appearance (colour and\nmorphological texture) of the aneurysm, compared to the rest\nof the brain vasculature which is also present in the surgical\nscene.\nWe formulate our problem as a frame classiﬁcation task\nand adapt the tiny version of the Transformer model using\nshifted-windows (Swin-T) [ 11] to tackle it. The proposed\narchitecture is illustrated in Fig. 3. The MACSSwin-T model\nextracts features at 4 stages, where each stage consists of\nmultiple consecutive Swin Transformer blocks. Each block\nis composed by a shifted-window multi-head self-attention\n(MSA) layer and a 2-layer MLP with GELU activation func-\ntions in between. Global average pooling is applied to the\nfeature maps, resulting in a 768-dimensional feature vector,\nprocessed by a single-layer perceptron with softmax activa-\ntion to predict the ﬁnal class (aneurysm presence/absence).\nWe incorporate temporal frame sequences to formulate\naneurysm detection as a video classiﬁcation task and propose\nvidMACSSwin-T (shown also in Fig. 3), adapted from [ 17],\nto address it. The vidMACSSwin-T model is a 3D version of\nthe MACSSwin-T model, which takes multiple frames (32)\nas input. It maintains the general structure of MACSSwin-T,\nwhile expanding the token to be a 3D patch. Accordingly,\nwe replace the standard MSA in the Swin-T block with the\n3D-shifted window MSA module and keep the rest of the\ncomponents unchanged. We use the I3D head [ 18] to obtain\nthe output and use it as the prediction of the centre frame..\nWeighted cross-entropy is the loss function to account for\nthe data imbalance in the dataset. In Eq. ( 1), ˆy\ni and yi are\nthe predicted score and ground-truth label, respectively, of\nsample i , and N\nb is the number of batch samples. We set the\nweights (wX , wY ) for each class as the ratio of the number of\nsamples of the other class ( NY , NX ) over the total samples\nand normalize them to add to “1”. NX and NY is the total\nnumber of Type-X and Type-Y samples in MACS.\n123\n1036 International Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041\nFig. 2 Dataset distribution of the MACS dataset. Each video comprises two types of frames for analysis, namely Type-X and Type-Y\nLwbce = 1\nNb\nNb∑\ni =1\n[\nwY yi log ˆyi + wX (1 − yi )log(1 −ˆyi )\n]\nwX = NY\nNX + NY\n,w Y = NX\nNX + NY\n(1)\nExperiments and results\nImplementation details\nWe initialize MACSSwin-T with a pretrained version of\nSwin-T on ImageNet-21K. We then freeze the ﬁrst 3 stages\nand train the network, for 300 epochs, on our MACS dataset\nusing a batch size of 128. Input images are normalized and\nresized to 224 × 224, due to memory and timing consider-\nations. We follow the data augmentation methods in [ 11].\nAdamW is employed for optimization with a base learning\nrate of 5e−4 and exponential decay rates β\n1 and β2 of 0.9 and\n0.99. We warm up the training for 20 epochs with the warm-\nup learning rate to be 5e −7. VidMACSSwin-T is initialized\nwith a pretrained Swin-T model on ImageNet-21K [ 17].\nHyperparameters were set according to [ 11,17], and exper-\nimentation showed that their value has small effect on the\nperformance of the MACSSwin-T/vidMACSSwin-T mod-\nels. Frames are resized to 224 × 224 without cropping. We\ntrain the model with AdamW optimizer for 30 epochs, using\ninitial learning rates of 3e −5 and 3e−4, for the backbone net-\nwork and I3D prediction head, respectively. We use a cosine\ndecay learning rate scheduler and 3 epochs of linear warm-\nup. The batch size is 64. Because the MACS videos in our\ndataset are long (10k–50k frames), we divide each video into\nmultiple samples. Following a similar approach to [ 17], for\ntraining vidMACSSwin-T, we group 64 consecutive frames\nof the same class as one sample and from each sample, uni-\nformly extract a sequence of 32 frames as the input to the\nnetwork. During inference, for all extracted frames, we group\n64 ×4 consecutive frames to deﬁne a single sample. We then\nuniformly sample these (64 × 4) consecutive frames to for-\nmulate 4 sequences of 32 frames, perform predictions on all\n4 sequences and average them to produce the ﬁnal classiﬁca-\ntion label. Models and experimentation took place with the\nPyTorch (v1.12.1) framework. We develop the models on an\nNVIDIA RTX A6000 GPU. Training MACSSwin-T requires\nabout 6417 MB of memory and inference time is 10.67 msec\n(97.3 fps).\nMultiple-fold cross-validation\nWe carry out multiple-fold cross-validation and split the\ndataset into fourfold on the basis of the 16 available videos\ncreating independent training (12 videos) and validation (4\nvideos) sets in each fold. This partitioning allows us to eval-\nuate the model’s performance and consistency on the entire\nMACS dataset. The training/validation splits are selected to\nhave a similar class ratio (ranging between 3.82:1 and 4.17:1)\nfor both the training and validation sets, shown in Fig. 4.\nLoss weights are set to W\nX = 0.2 and WY = 0.8, in all folds\naccording to the overall MACS dataset distribution.\nResults are listed in Table 1 for both models in each\nfold. For the single-frame MACSSwin-T model, the vali-\ndation accuracy ranges from 78.5 to 82.4% with a mean\naccuracy of 80.8%. The mean precision and recall rates\nare 51.3 and 63.8%, respectively, resulting in a mean F1-\nscore of 56.8%. The results are promising to indicate that the\nMACSSwin-T architecture learns to correctly recognize the\npresence/absence of the aneurysm. The model avoids intro-\nducing signiﬁcant bias towards the dominant Type-X class,\nsince the recall rate is signiﬁcantly higher than the precision\nrate. Experiments with different initialization seeds demon-\nstrate robust model behaviour (std < 0.1o na l l4m e t r i c s ) .\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041 1037\nFig. 3 a The proposed vidMACSSwin-T architecture for aneurysm\ndetection. Features extracted from the 4 stages are fed to the aneurysm\nclassiﬁcation head, to produce the ﬁnal prediction (i.e. Type-X or Type-\nY ); b, c Detailed structures for Swin Transformer blocks and Video\nSwin Transformer Blocks\nFig. 4 Number of training and validation samples for each cross-validation fold\n123\n1038 International Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041\nTable 1 Results from fourfold\ncross-validation experiments Model Fold Accuracy (%) Precision (%) Recall (%) F1 score (%)\nMACSSwin-T 1, 2 81.3, 78.5 51.3, 45.5 66.5, 55.3 58.0, 49.9\n3, 4 80.9, 82.4 51.5, 56.9 70.0, 63.4 59.4, 60.0\nMean – 80.85 1 .36 3 .85 6 .8\nvidMACSSwin-T 1, 2 86.2, 85.1 65.1, 66.4 61.0, 42.3 63.0, 51.7\n3, 4 91.3, 85.8 93.0, 93.0 59.5, 32.8 72.5, 48.4\nMean – 87.17 9 .44 8 .95 8 .9\nThe vidMACSSwin-T model using temporal informa-\ntion achieves a signiﬁcantly higher mean accuracy at 87.1%\ncompared to MACSSwin-T. Average precision increases to\n79.4%, while the recall is 48.9%. Across folds, the F1 score\nof vidMACSSwin-T is higher than that of MACSSwin-T. We\nargue that the result (85.8%) of the fourth fold is not due to\nthe model being biased to negatives. Although the recall is\nlow (32.8%), the proportion of negatives is 79.2%, while the\naccuracy is higher (85.8%—exceeding the highest accuracy\n82.4% of MACSSwin-T), which means vidMACSSwin-T\nstill has the ability to recognize positives samples. Overall,\nthe cross-fold validation experiments with independent sets\njustify the selection of the Swin-T architecture and proposed\nmodels development strategy for the intended aneurysm clas-\nsiﬁcation task.\nVisualizing the class activation maps\nFigure 5 illustrates the class activation maps for 5 Type-Y\ninput frames, using Grad-CAM [ 19], obtained at the ﬁnal\nnormalization layer after the completion of training in each\nfold. By comparing neurosurgeons’ annotations (provided\nindependently for these 5 input images) of the aneurysm\nlocation (top row) to the generated activation maps (bot-\ntom row), we conclude that the self-attention mechanism on\nthe MACSSwin-T drives the model to focus on the desired\nlocation of the image and correctly localizes the exposed\naneurysms. We also highlight that this is achieved in the\npresence of adjacent vasculature with similar appearance and\nwithout providing to the network, any information on the\naneurysms’ location.\nComparison against humans (neurosurgeons)\nTo demonstrate the performance of our method against a\ngold standard and establish a preliminary baseline, 10 consul-\ntant neurosurgeons, 2 females and 8 males (age 35–64) from\nLondon-based NHS trusts were surveyed. Human assessors\nwere asked to classify, after visual inspection, 15 frames (8\nType-X and 7 Type-Y) selected from microscopy videos from\n4 MACS procedures not included in the 16 ones, we used\nfor model development. Speciﬁcally, frames were extracted\nfrom the 4 operative videos, unseen to the models at a rate of\n5 fps. Fifty frames (50) were initially selected randomly and\nunderwent blinded senior review by two vascular neurosur-\ngeons in duplicate, where frames were classiﬁed as Type-X\n(aneurysm not in-frame), Type-Y (aneurysm in-frame), or\nType-Z (excluded). The ﬁnal dataset of 15 images shared\nto the 10 expert neurosurgeons was randomly selected from\nimages with concordant reviews from the pool of 50 images.\nHuman assessors reviewed and labelled the 15 test images as\nstill frames, without access to the videos of the procedures.\nA total of 150 individual frame-level reviews (80 Type-X:\nno aneurysm, 70 Type-Y: aneurysm present) was obtained.\nHuman classiﬁcations were tested against the outputs of the\nMACSSwin-T model, that makes an inference based on a\nsingle frame. For completeness we report results with the\nvidMACSSwin-T, but do not perform a direct comparison as\nthe human assessors reviewed individual images, instead of\nFig. 5 Activation maps of the\nSwin-T model from Type-Y\ninput images. The ﬁrst row\nshows the aneurysm areas as\nannotated by the expert\nneurosurgeons. The second row\nshows the activation maps of the\nﬁnal normalization layer after\ntraining is completed. The\nmodel’s activation is localized in\nthe same areas as the manual\nannotations\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041 1039\nTable 2 Comparison against\nconsultant neurosurgeons Method Accuracy (%) Precision (%) Recall (%) F1 score (%)\nMACSSwin-T (0.5) 66.7 (10/15) 100.0 (2/2) 28.6 (2/7) 44.4\nMACSSwin-T (0.4) 73.3 (11/15) 100.0 (3/3) 42.9 (3/7) 60.0\nMACSSwin-T (0.3) 86.7 (13/15) 100.0 (5/5) 71.4 (5/7) 83.3\nvidMACSSwin-T 73.3 (11/15) 100.0 (3/3) 42.9 (3/7) 60.0\nHuman 82.0 (123/150) 81.2 (56/69) 80.0 (56/70) 80.6\nFig. 6 Challenging image samples (from the set of 15) yielding detec-\ntion errors for both neurosurgeons and AI (MACSSWin-T) models. a\nIs negative (Type-X), however, the vessel the bipolar forceps points to,\nis probably misinterpreted by humans as the aneurysm leading to low\ndetection rate (40%). AI consistently detects it as Type-X. b–d Are\npositive (Type-Y), but only a tiny part of the aneurysms is visible and\nalso they visually blend (overlay) over adjacent vessels which possibly\nmakes it difﬁcult to distinguish. Humans achieve (50, 50 and 100%)\nwhile AI initially fails with DT = 0.5, but correctly classiﬁes ( b), ( d)\nwith DT = 0.3. Videos of atherosclerosed (white dome) aneurysms\nlike in ( e) and with indocyanine green like in ( f) are rarely seen dur-\ning training. Due to the largely different visual appearance, AI initially\n(DT = 0.5) fails to recognize these two aneurysms. Setting DT = 0.4,\ngives correct classiﬁcation for ( f) showing that the clip (blue colour),\ntypically an indication of exposure, provides a strong enough visual\ncue for AI to correctly identify the aneurysm. Humans assisted by the\npresence of the white dome and clip achieve very good performance in\nthese two cases (90, 100%). c, e are the ones AI fails to detect correctly\nvideos. Models were retrained on the entire MACS dataset\nand tested only on the 15 images.\nNeurosurgeon and model results are listed in Table 2.I n\ntotal, 123/150 (82%) of human reviews were correct in identi-\nfying or excluding the presence of an aneurysm. For Type-X\nimages, 67/80 (84%) reviews correctly excluded the pres-\nence of an aneurysm, while for Type-Y frames, 56/70 (80%)\nreviews correctly identiﬁed the presence of an aneurysm\nin the image. Neurosurgeons’ individual accuracy ranged\nfrom 68.7%(10/15) to 100%(15/15) with 11/15 (73.3%) and\n123\n1040 International Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041\n13/15(86.7%) observed in most cases (3 each). Due to the\nsigniﬁcant class imbalance in the MACS dataset, we expect\nour models to be biased towards Type-X (negative) samples.\nIn order to promote prediction of Type-Y (positive) samples,\nwe reduce the decision threshold (DT) of the output softmax\nlayer, in MACSSwin-T from 0.5 (initial) to 0.3, and inves-\ntigate the model’s behaviour when encouraged to classify\nType-Y samples with lower conﬁdence. We have experi-\nmentally veriﬁed in CV experiments that smaller than 0.3\nthreshold values do not further promote the detection of Type-\nY samples. MACSSwin-T accuracy ranges between 66.7%\nand 86.7%, with excellent precision (100%). We observe that\nthe recall rate improves with reduced thresholds (0.3, 0.4)\nand the model correctly recognizes up to 3 Type-Y samples\npreviously classiﬁed as Type-X. We also remark that none\nof the Type-X images is wrongly classiﬁed with decreased\nDT. Overall, lowering the DT is beneﬁcial for the model as it\ncorrectly classiﬁes previously missed Type-Y samples albeit\nwith lower conﬁdence. Different to our model that always\n(for all DTs) classiﬁes Type-X frames correctly, humans ﬁnd\nthis challenging and sometimes misdetect them, probably\nbecause of tool presence or adjacent vasculature being per-\nceived as the aneurysm. Figure 6 shows challenging samples\nwhere misclassiﬁcations are observed from both humans and\nthe MACSSWin-T model.\nInter-rater agreement, between the 10 human assessors,\nwas found to be moderate with Fleiss Kappa value 0.502 ( p-\nvalue = 0, agreement is not accidental). Strong agreement\nis seen in 8 samples (5 negatives and 3 positives) with 9 or\nmore similar classiﬁcations, while 1 negative (4 neg–6 pos,\nFig. 6a) and 2 positives (5 neg–5 pos, Fig. 6b, c) have very\nweak agreement. The agreement between the MACSSwin-T\nmodel and humans is evaluated with the Matthews correla-\ntion coefﬁcient (MCC), which for binary classiﬁcation tasks\nequals the Pearson correlation. We provide the range of MCC\nfor MACSSwin-T (for DT = 0.3 and DT = 0.5) and the\n10 assessors. For DT = 0.5 the MCC ranges 0.08–0.48\nand shows moderate (0.3–0.5) positive correlation with the\nmajority (8) of raters. For DT = 0.3, the MCC ranges 0–\n0.75, showing strong (0.5–0.7) and very strong ( > 0.7),\npositive correlation with 7 raters. There is only one rater (who\nachieves 10/15 accuracy) with low correlation. This rater also\nhas very low MCC ( < 0.3) with the remaining 9. Overall,\nMACSSwin-T presents moderate correlation against human\nraters for DT = 0.5 and strong for DT = 0.3.\nConclusion\nThis article introduced a dataset of 16 MACS videos, with\nframe-level annotations on aneurysm presence/absence, and\nproposed Transformer-based models for automated frame-\nlevel aneurysm detection. In addition to having a small size,\naneurysm exposure in MACS is a critical but short-term\nevent occurring only during a particular procedural phase\n(aneurysm dissection). This results in the aneurysm anno-\ntated as visible, only in a fraction (ratio approximately 1:4 in\nour dataset) of video frames.\nWe develop our models (MACSSwin-T, vidMACSSwin-\nT), with frame-level annotations and weakly supervise them\nusing cross-entropy loss with weights adjusted for the\nclass imbalance in the MACS dataset. The self-attention\nmodule produces meaningful localized representations even\nin the absence of localized training signals (i.e. bound-\ning boxes), enabling the models to efﬁciently detect the\naneurysm and distinguish it from adjacent brain vascula-\nture with similar appearance. We achieve consistent results\nwith an average accuracy of 80.8% (precision 51.3%, recall\n63.8%) and 87.1% (precision 79.4%, recall 48.9%), for the\nMACSSwin-T and vidMACSSwin-T models, respectively,\nin multiple-fold cross-validation, with independent train-\ning/validation sets. Although the task challenges are reﬂected\nin the obtained accuracy, our models achieve similar results\nto human assessors (86.7–82%), in an independent test set,\nwith an adjusted detection threshold.\nFuture work will focus on three directions: (i) pre-\nprocessing, (ii) temporal information aggregation and (iii)\nweakly supervision, to optimize model development.\nSupplementary Information The online version contains supplemen-\ntary material available at https://doi.org/10.1007/s11548-023-02871-\n9.\nFunding This research was funded in whole, or in part, by the\nWellcome/EPSRC Centre for Interventional and Surgical Sciences\n[203145/Z/16/Z]; the Engineering and Physical Sciences Research\nCouncil (EPSRC) [EP/P027938/1, EP/R004080/1, EP/P012841/1]; the\nRoyal Academy of Engineering Chair in Emerging Technologies\nScheme [CiET1819/2/36] and the NIHR UCLH/UCL BRC Neuro-\nscience. For the purpose of open access, the author has applied a CC\nBY public copyright licence to any author accepted manuscript version\narising from this submission.\nData availability Relevant code, models and the MACS dataset will be\npublicly released upon acceptance at the UCL WEISS Open Data Server\nhttps://www.ucl.ac.uk/interventional-surgical-sciences/weiss-open-re\nsearch/weiss-open-data-server . Supplementary videos are attached,\nshowing the MACSSWin-T model’s predictions from the multiple-fold\ncross-validation experiments.\nDeclarations\nConﬂict of interest The authors declare no conﬂict of interest.\nEthical approval This article does not contain any studies with human\nparticipants performed by any of the authors.\nInformed consent This article does not contain patient data. Human\nassessors consented to anonymously participate in the survey presented.\n123\nInternational Journal of Computer Assisted Radiology and Surgery (2023) 18:1033–1041 1041\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Lee D, Yu HW, Kim S, Yoon J, Lee K, Chai YJ, Choi JY , Kong H-J,\nLee KE, Cho HS, Kim HC (2020) Vision-based tracking system\nfor augmented reality to localize recurrent laryngeal nerve during\nrobotic thyroid surgery. Sci Rep 10(1)\n2. Cabrilo IPB, Schaller K (2014) Augmented reality in the surgery of\ncerebral aneurysms: a technical report. Oper Neurosurg 10(2):252–\n261\n3. Kantelhardt SR, Gutenberg A, Neulen A, Keric N, Renovanz M,\nGiese A (2015) Video-assisted navigation for adjustment of image-\nguidance accuracy to slight brain shift. Oper Neurosurg 11(4):504–\n511\n4. Meola A, Cutolo F, Carbone M, Cagnazzo F, Ferrari M, Ferrari\nV (2017) Augmented reality in neurosurgery: a systematic review.\nNeurosurg Rev 40(4):537–548\n5. Chadebecq F, Vasconcelos F, Mazomenos E, Stoyanon D (2020)\nComputer vision in the surgical operating room. Visc Med 36:456–\n462\n6. Madani A, Namazi B, Altieri MS, Hashimoto DA, Rivera AM,\nPucher PH, Navarrete-Welton A, Sankaranarayanan G, Brunt\nLM, Okrainec A, Alseidi A (2022) Artiﬁcial intelligence for\nintraoperative guidance: using semantic segmentation to identify\nsurgical anatomy during laparoscopic cholecystectomy. Ann Surg\n276(2):363–369\n7. Tokuyasu T, Iwashita Y , Matsunobu Y , Kamiyama T, Ishikake M,\nSakaguchi S, Ebe K, Tada K, Endo Y , Etoh T, Nakashima M,\nInomata M (2020) Development of an artiﬁcial intelligence sys-\ntem using deep learning to indicate anatomical landmarks during\nlaparoscopic cholecystectomy. Surg Endosc 35:1651–1658\n8. Gong J, Holsinger FC, Noel JE, Mitani S, Jopling J, Bedi N, Koh\nYW, Orloff LA, Cernea CR, Yeung S (2021) Using deep learning\nto identify the recurrent laryngeal nerve during thyroidectomy. Sci\nRep 11(1):1–11\n9. Muirhead WR, Grover PJ, Toma AK, Stoyanov D, Marcus HJ, Mur-\nphy M (2021) Adverse intraoperative events during surgical repair\nof ruptured cerebral aneurysms: a systematic review. Neurosurg\nRev 44(3):1273–1285\n10. Khan DZ, Luengo I, Barbarisi S, Addis C, Culshaw L, Dorward NL,\nHaikka P, Jain A, Kerr K, Koh CH, Layard-Horsfall H, Muirhead W,\nPalmisciano P, Vasey B, Stoyanov D, Marcus HJ (2021) Automated\noperative workﬂow analysis of endoscopic pituitary surgery using\nmachine learning: development and preclinical evaluation (ideal\nstage 0). J Neurosurg 1:1–8\n11. Liu Z, Lin Y , Cao Y , Hu H, Wei Y , Zhang Z, Lin S, Guo B (2014)\nSwin transformer: hierarchical vision transformer using shifted\nwindows. In: ICCV , pp 10012–10022\n12. Long Y , Li Z, Yee CH, Ng CF, Taylor RH, Unberath M, Dou\nQ (2021) E-dssr: efﬁcient dynamic surgical scene reconstruction\nwith transformer-based stereoscopic depth perception. In: MIC-\nCAI. Springer, Berlin, pp 415–425\n13. Czempiel T, Paschali M, Ostler D, Kim ST, Busam B, Navab N\n(2021) Opera: attention-regularized transformers for surgical phase\nrecognition. In: MICCAI. Springer, Berlin, pp 604–614\n14. Zhang J, Nie Y , Chang J, Zhang JJ (2021) Surgical instruction\ngeneration with transformers. In: MICCAI. Springer, Berlin, pp\n290–299\n15. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser L, Polosukhin I (2017) Attention is all you need. NIPS\n30\n16. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S,\nUszkoreit J, Houlsby N (2021) An image is worth 16 × 16 words:\ntransformers for image recognition at scale. In: ICLR\n17. Liu Z, Ning J, Cao Y , Wei Y , Zhang Z, Lin S, Hu H (2022) Video\nSwin transformer. In: CVPR, pp 3202–3211\n18. Carreira J, Zisserman A (2017) Quo vadis, action recognition? A\nnew model and the kinetics dataset. In: CVPR, pp 6299–6308\n19. Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra\nD (2017) Grad-cam: visual explanations from deep networks via\ngradient-based localization. In: IEEE ICCV , pp 618–626\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}