{
  "title": "Testing the Generalization of Neural Language Models for COVID-19 Misinformation Detection",
  "url": "https://openalex.org/W3212007825",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224451961",
      "name": "Wahle, Jan Philip",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A4227598462",
      "name": "Ashok, Nischal",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A4223315846",
      "name": "Ruas, Terry",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A3160266820",
      "name": "Meuschke, Norman",
      "affiliations": [
        "University of Wuppertal"
      ]
    },
    {
      "id": "https://openalex.org/A4222750702",
      "name": "Ghosal, Tirthankar",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A3162852365",
      "name": "Gipp, Bela",
      "affiliations": [
        "University of Wuppertal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4213327489",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3011345566",
    "https://openalex.org/W2798935874",
    "https://openalex.org/W3135257712",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3093277491",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3115413024",
    "https://openalex.org/W3013473577",
    "https://openalex.org/W3023556039",
    "https://openalex.org/W2900979343",
    "https://openalex.org/W2742330194",
    "https://openalex.org/W4213122582",
    "https://openalex.org/W4234918518",
    "https://openalex.org/W6600655081",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3007198457",
    "https://openalex.org/W3037025384",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3032089915",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3138008297",
    "https://openalex.org/W3139157128",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W4318764177",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3098835531",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3020786614",
    "https://openalex.org/W3174401451",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3101739755",
    "https://openalex.org/W3024622987",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4287693250",
    "https://openalex.org/W4324148013",
    "https://openalex.org/W3046340454",
    "https://openalex.org/W3118895645",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "A drastic rise in potentially life-threatening misinformation has been a\\nby-product of the COVID-19 pandemic. Computational support to identify false\\ninformation within the massive body of data on the topic is crucial to prevent\\nharm. Researchers proposed many methods for flagging online misinformation\\nrelated to COVID-19. However, these methods predominantly target specific\\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\\nTransformer-based models on five COVID-19 misinformation datasets that include\\nsocial media posts, news articles, and scientific papers to fill this gap. We\\nshow tokenizers and models tailored to COVID-19 data do not provide a\\nsignificant advantage over general-purpose ones. Our study provides a realistic\\nassessment of models for detecting COVID-19 misinformation. We expect that\\nevaluating a broad spectrum of datasets and models will benefit future research\\nin developing misinformation detection systems.\\n",
  "full_text": "arXiv:2111.07819v5  [cs.CL]  10 Nov 2022\nRelated papers at https://jpwahle.com/pub/\nJ. P . W ahle, T . Ruas, T . Foltýnek, N. Meuschke, and B. Gipp (20 22). T esting the\nGeneralization of Neural Language Models for COVID-19 Misi nformation De-\ntection. In: Smits, M. (eds) Information for a Better W orld: Shaping the Global\nFuture. iConference 2022. Lecture Notes in Computer Scienc e(), vol 13192.\nSpringer, Cham. doi: 10.1007/978-3-030-96957-8_33\nClick to download:\nBibT eX | RIS | ENW\nT esting the Generalization of Neural Language Models\nfor COVID-19 Misinformation Detection\nJan Philip W ahle * 1[0000− 0002− 2116− 9767], Nischal Ashok *2[0000− 0002− 7022− 5948],\nT erry Ruas1[0000− 0002− 9440− 780X], Norman Meuschke 1[0000− 0003− 4648− 8198],\nTirthankar Ghosal 3[0000− 0002− 2358− 522X], and Bela Gipp 1[0000− 0001− 6522− 3019]\n1 University of Wuppertal, Rainer-Gruenter-Straße, 42119 W uppertal, Germany\nlast@uni-wuppertal.de\n2 Indian Institute of T echnology Patna, Bihar-801106, India\n1801cs33@iitp.ac.in\n3 Charles University , Malostranské nám ˇestí 25, 118 00 Praha, Czech Republic\nghosal@ufal.mff.cuni.cz\nAbstract. A drastic rise in potentially life-threatening misinforma tion has been a\nby-product of the COVID-19 pandemic. Computational suppor t to identify false\ninformation within the massive body of data on the topic is cr ucial to prevent\nharm. Researchers proposed many methods for ﬂagging online misinformation\nrelated to COVID-19. However, these methods predominantly target speciﬁc con-\ntent types (e.g., news) or platforms (e.g., T witter). The me thods’ capabilities to\ngeneralize were largely unclear so far. W e evaluate ﬁfteen T ransformer-based\nmodels on ﬁve COVID-19 misinformation datasets that includ e social media\nposts, news articles, and scientiﬁc papers to ﬁll this gap. W e show tokenizers and\nmodels tailored to COVID-19 data do not provide a signiﬁcant advantage over\ngeneral-purpose ones. Our study provides a realistic asses sment of models for de-\ntecting COVID-19 misinformation. W e expect that evaluatin g a broad spectrum\nof datasets and models will beneﬁt future research in develo ping misinformation\ndetection systems.\nKeywords: COVID-19 · Transformers · Health · Social Media.\n1 Introduction\nThe COVID-19 pandemic has claimed more than four million liv es by the time of writ-\ning this paper, and the number of infections remains high\n4. The behavior of individuals\nstrongly affects the risk of infection. In turn, the quality of information individuals re-\nceive strongly inﬂuences their actions [ 23,10]. The novelty and rapid global spread of\nthe SARS-CoV -2 virus has also led to countless life-threate ning incidences of misin-\nformation spread on the topic. Controlling COVID-19 and com bating possible future\npandemics early, requires reducing misinformation and inc reasing the distribution of\nfacts on the subject [ 5,35].\n* Equal contribution.\n4 https://coronavirus.jhu.edu/map.html\n1\n2\nResearchers worldwide collaborate on automating the detec tion of false information\non COVID-19. 5 The initiatives build collections of scientiﬁc papers, soc ial media posts,\nand news articles to analyze their content, spread, source, and propagators [ 33,7,19].\nNatural Language Processing (NLP) research has extensivel y studied options to\nautomate the identiﬁcation of fake news [ 27], primarily by applying recent language\nmodels. Researchers proposed adaptions of well-known Tran sformer models, such as\nCOVID-T witter-BER T [20], to identify false information on COVID-19 from speciﬁc\nsources. However, most prior studies analyze speciﬁc conte nt types (e.g., news) or plat-\nforms (e.g., T witter). These limitations prevent reliable conclusions regarding the gen-\neralization of the proposed language models.\nT o ﬁll this gap, we apply 15 Transformer models to ﬁve COVID-1 9 misinformation\ntasks. W e compare Transformer models optimized on COVID-19 datasets to state-of-\nthe-art neural language models. W e exhaustively apply mode ls to different tasks to test\ntheir generalization on unknown sources. The code to reprod uce our experiments, 6 and\nthe datasets used are publicly available.\n2 Related W ork\nThe same way word2vec [\n18] inspired many models in NLP [ 4,26,25], the excellent per-\nformance of BER T [ 8], a Transformer-based model [ 28], caused its numerous adaption\nfor language tasks [ 34,6,29,30]. Domain-speciﬁc models build on top of Transformers\ntypically outperform their baselines for related tasks [ 12]. For example, SciBER T [ 2]\nwas pre-trained on scientiﬁc documents and typically outpe rforms BER T for scientiﬁc\nNLP tasks, such as determining document similarity [ 22].\nMany models for COVID-19 misinformation detection employ d omain-speciﬁc pre-\ntraining to improve their representation. COVID-T witter- BER T [20] was pre-trained on\n160M tweets and evaluated for sentiment analysis of tweets, e.g., about COVID vac-\ncines. BioClinicalBER T [ 1] was trained into clinical narratives to incorporate lingu istic\ncharacteristics from the biomedical and clinical domains.\nCui et al. [ 7] investigated the misinformation detection task by compar ing tradi-\ntional machine learning and deep learning techniques. Simi larly, Zhou et al. [ 36] ex-\nplored statistical learners, such as SVM, and neural networ ks to classify news as cred-\nible or not. The results of both studies show deep learning ar chitectures as the most\nprominent alternatives for the respective datasets.\nAs papers on COVID-19 are recent, some contributions are onl y available as pre-\ntrained models. COVID-BER T 7 and COVID-SciBER T8 are pre-trained on the CORD-\n19 dataset and only available via the Huggingface API. Other s, such as COVID-CQ [ 19]\nand CMU-MisCov19 [ 17] are used to either investigate intrinsic details (e.g., ho w dense\nmisinformed communities are) or to explore the applicabili ty of statistical techniques.\nAlthough related works provide promising approaches to cou nter misinformation\nrelated to COVID-19, none of them explore multiple datasets . Research in many NLP\n5 W e collectively refer to fake news, disinformation, and mis information as false information.\n6 https://github.com/ag-gipp/iConference22_COVID_misinformation\n7 https://tinyurl.com/86cpx6u2\n8 https://tinyurl.com/9w24pc93\nT esting the Generalization of neural LMs for COVID-19 Misin formation Detection 3\nareas already uses diverse benchmarks to compare models [ 32,31]. T o the best of our\nknowledge, our study is the ﬁrst to systematically test Tran sformer-based methods on\ndifferent data sources related to COVID-19.\n3 Methodology\nModels. Our study includes 15 Transformer-based models, which are detailed in Ap-\npendix\nA.2. W e categorize the models into the following three groups:\nGeneral-Purpose Baselines . The ﬁrst group consists of general-purpose Trans-\nformer models without domain-speciﬁc training, i.e., BER T [8], RoBER T a [16], BAR T [ 15],\nDeBER T a [11]. These baselines show how vanilla Transformer-based mode ls perform\non the COVID-19 misinformation detection task.\nIntermediate Pre-T raining. The second group contains models trained on spe-\nciﬁc content types and domains, i.e., SciBER T [ 2], BER T weet [ 21], and BioClinical-\nBER T [ 1]. For example, SciBER T adapts BER T for scientiﬁc articles. These models\nshow the effect of intermediate pre-training on speciﬁc sou rces compared to general-\npurpose training (e.g., whether BER T weet is superior to BER T for misinformation on\nT witter). Moreover, we compare the models in this group to la nguage models optimized\nusing intermediate pre-training on COVID-19 data (third gr oup).\nCOVID-19 Intermediate Pre-T raining. The third group comprises models em-\nploying an intermediate pre-training stage on COVID-19 dat a. Due to task-speciﬁc pre-\ntraining, we expect these models to achieve better results t han the models in groups one\nand two. W e include a model that optimizes the pre-training o bjective on a large T wit-\nter corpus, i.e., CT -BER T [ 20], two models trained on the CORD-19 dataset (COVID-\nBER T9 and COVID-SciBER T 10 ), and two popular models from the huggingface API\nfor which the intermediate pre-training sources are not rel eased yet (ClinicalCOVID-\nBER T11 and BioCOVID-BER T 12 ). W e pre-train RoBER T a, BAR T , and DeBER T a on\nthe CORD-19 dataset to compare them to the models we used as ge neral-purpose base-\nlines.\nData. W e compile an evaluation set from six popular datasets for d etecting COVID-19\nmisinformation in social media, news articles, and scienti ﬁc publications, i.e., CORD-\n19 [\n33], CoAID [ 7], COVID-CQ [ 19], ReCOV ery [ 36], CMU-MisCov19 [ 17], and\nCOVID19FN.13 T able 1 gives an overview of the datasets and Appendix A.1 presents\nmore details. For CORD-19, we only use abstracts in the datas et, as they provide an\nadequate trade-off between size and information density. A dditionally, less than 50%\nof the articles in CORD-19 are available as full texts. For Co AID and ReCOV ery, we\nonly extract news articles to reduce a bias towards T witter p osts in our evaluation. All\nremaining datasets are used in their original composition.\nW e use CORD-19 to extend the pre-training of general-purpos e models and all other\ndatasets to evaluate the models for a downstream task. CORD- 19 consists of scientiﬁc\n9 https://tinyurl.com/86cpx6u2\n10 https://tinyurl.com/9w24pc93\n11 https://tinyurl.com/kebysw\n12 https://tinyurl.com/4xx9vdkm\n13 https://tinyurl.com/4ne9vtzu\n4\narticles, while the other datasets primarily contain news a rticles and T witter content. W e\nchose different domains for training and evaluation to test the models’ generalization\ncapabilities and avoid overlaps between the datasets.\nT able 1.An overview of the COVID-related datasets. CORD-19 has no sp eciﬁc T ask or Label as\nit provides a general collection of documents. † Details on the labels are given in Memon et al.\n[\n17].\nCorpus |Corpus|T ask Domain Source(s) Label(s)\nCORD-19 ([ 33]) 497 906 - Scientiﬁc articles CZI, PMC, BioRxiv , MedRxiv -\nCoAID ([ 7]) 302 926 Misinformation detection Healthcare misinforma tion T witter, news, social media { true, false }\nReCOV ery ([36]) 142 849 Credibility classiﬁcation Low information credi bility T witter, news { reliable, unreliable }\nCOVID-CQ ([ 19]) 14 374 Efﬁcacy of treatments Drug treatment T witter { neutral, against, for }\nCMU-MisCov19 ([ 17]) 4 573 Communities detection Misinformed communities T wi tter { 17 labels † }\nCOVID19FN (2020) 2 800 Misinformation detection Misinform ation in news Poynter { true, false }\n4 Experiments\nOverview .Our study includes three experiments. The ﬁrst experiment t ests how static\nword embeddings and frozen contextual embeddings perform c ompared to ﬁne-tuned\nlanguage models. The second experiment studies whether tok enizers speciﬁcally tai-\nlored to a COVID-19 vocabulary are superior to general-purp ose ones.\n14 The third\nexperiment evaluates and compares all 15 Transformer model s on the ﬁve evaluation\ndatasets.\nT raining & Evaluation.T o compare general-purpose baselines and COVID-19 in-\ntermediate pre-trained models, we perform pre-training on the CORD-19 dataset for\nthree models (RoBER T a, BAR T , and DeBER T a) and use pre-train ed conﬁgurations for\nthe remaining models (BER T , SciBER T , BioCOVID-BER T , Clini calCOVID-BER T).\nW e then ﬁne-tune all models for each of the ﬁve test tasks (COVID-CQ, CoAID, Re -\nCOV ery, CMU-MisCov19, and COVID19FN). W e use a split of 80% a nd 20% of the\ndocuments in a dataset for training and testing, respective ly. This split is the most com-\nmon conﬁguration for the tested datasets [ 36,19] and is comparable to other studies [ 7].\nW e use 10% of the train dataset as a hold-out validation set.\n5 Results & Discussion\nStatic and Frozen Embeddings. T able\n2 compares the classiﬁcation results of a base-\nline composed of BiLSTM and GlobalV ectors (GloV e) [ 4] to the frozen embeddings\nof three Transformer models for the COVID-CQ dataset. The re sults show no signiﬁ-\ncant difference between GloV e and the frozen models. Howeve r, ﬁne-tuning the same\nthree models end-to-end generally increases their perform ance. Therefore, we choose\nto ﬁne-tune neural language models for the classiﬁcation of COVID-19 misinformation.\n14 General-purpose refers to the tokenizers released with the pre-trained models.\nT esting the Generalization of neural LMs for COVID-19 Misin formation Detection 5\nT okenizer Ablation. T able 3 shows the results on COVID-CQ for the best conﬁg-\nuration of the models using a standard 15 tokenizer for pre-training and ﬁne-tuning. W e\nexpected adjusting the tokenizer to the CORD-19 dataset wou ld improve the results,\nas it adds valuable tokens to the vocabulary, which are often not present in standard\ntokenizers. However, using specialized tokenizers decrea sed the performance. The con-\ntent in CORD-19 originates from the scientiﬁc domain. W e hyp othesize tweets lack\nsimilar token relations, which causes the performance drop on the COVID-CQ dataset.\nTherefore, we use the standard tokenizer for our full evalua tion experiments.\nFull Evaluation . T able 4 reports the results of our full evaluation. All results are\nstatistically signiﬁcant using bootstrap and permutation tests ( p < . 05) [ 9]. General-\npurpose baselines achieved the best result for two of the ﬁve datasets (BAR T on CoAID\nand BAR T on COVID19FN). For two datasets (ReCOV ery and COVID -CQ), a model\nwe pre-trained on CORD-19 data (COVID-RoBER T a) performed b est. CT -BER T achieved\nthe best result on CMU-MisCov19, an expected outcome as the d atasets consist only of\nT witter content. BER T weet, which was also trained on T witte r data, does not achieve\nbetter results than general-purpose baselines. W e expecte d a minor drop in performance\nfor BER T weet compared to CT -BER T as the former was not traine d on COVID-19\nvocabulary, but better a performance than general-purpose models as BER T weet was\ntrained mainly on T witter data.\nT able 2.F1-Macro scores of neural language\nmodels and a baseline (BiLSTM+GloV e) for\nthe COVID-CQ dataset. The static and frozen\nmodels use a stacked BiLSTM; ﬁne-tuned\nmodels were pre-trained on the CORD-19\ndataset and ﬁne-tuned for the task.\nT ype Models F1-Macro\nstatic GloV e .71\nfrozen BERT .72\nfrozen RoBERT a .70\nfrozen SciBERT .68\nﬁne-tuned BERT .75\nﬁne-tuned RoBERT a .80\nﬁne-tuned SciBERT .76\nT able 3. F1-Macro scores of BART and\nRoBERT a on the COVID-CQ dataset using\ndifferent Pre-Training and Fine-Tuning T ok-\nenizers. All models were pre-trained on the\nCORD-19 dataset.\nModels PT T ok. FT T ok. F1-Macro\nRoBERT a Standard Standard .78\nRoBERT a COVID Standard .73\nRoBERT a COVID COVID .72\nBART Standard Standard .77\nBART COVID Standard .73\nBART COVID COVID .70\nAll models achieved low scores for CMU-MisCov19, making it t he most chal-\nlenging dataset in our evaluation. The best results were obt ained for CoAID. Overall,\ngeneral-purpose baselines achieved comparable results to COVID-19 intermediate pre-\ntrained models for all datasets. For example, the best mean r esult for the dataset ReCOV -\nery was achieved by COVID-RoBER T a (F1=.91, std=.03) while t he general-purpose\nmodel BAR T (F1=.90, std=.01) was only .01 score points worse . W e observe similar re-\nsults for COVID-CQ, where the best model COVID-RoBER T a (F1= .78, std=.03) has a\n15 Pre-Trained tokenizer provided by HuggingFace\n6\nT able 4.A verage F1-Macro scores and standard deviation over three r andomly sampled runs of\nneural language models for COVID datasets. The table is divi ded into three parts: general-purpose\nbaselines, intermediate pre-trained, and COVID-19 interm ediate pre-trained models. COVID\nAware means the model was pre-trained on CORD-19 ( ✓), pre-trained on a different dataset\n(✗), or the dataset was not reported ( ?). Intermediate Training means the model was pre-trained\nin a speciﬁc domain. Boldface indicates the highest value for each dataset. † Models trained on\nCORD-19. ∗ Large version of the model.\nIT CA Model CMU-MisCov19 CoAID ReCOV ery COVID19FN COVID-CQ\n- - BERT .54 ± .03 .93 ± .01 .78 ± .02 .65 ± .01 .76 ± .01\n- - RoBERT a .53 ± .03 .95 ± .01 .81 ± .01 .73 ± .02 .64 ± .01\n- - BART .49 ± .03 .96± .01 .90 ± .01 .83± .01 .75 ± .01\n- - DeBERT a .52 ± .04 .95 ± .01 .75 ± .01 .67 ± .03 .63 ± .02\n✓ - SciBERT .46 ± .02 .95 ± .01 .77 ± .01 .76 ± .01 .61 ± .02\n✓ - BioClinicalBERT .48 ± .02 .89 ± .01 .82 ± .01 .81 ± .02 .63 ± .02\n✓ - BERT weet .51 ± .03 .88 ± .02 .84 ± .03 .65 ± .01 .75 ± .01\n✓ ✗ CT -BERT∗ .58± .04 .94 ± .01 .80 ± .02 .40 ± .01 .63 ± .02\n✓ ? ClinicalCOVID-BERT .50 ± .03 .93 ± .01 .82 ± .01 .78 ± .02 .74 ± .02\n✓ ? BioCOVID-BERT ∗ .45± .01 .91 ± .02 .81 ± .01 .68 ± .03 .63 ± .02\n✓ ✓ COVID-BERT † .46± .02 .94 ± .01 .85 ± .03 .73 ± .02 .72 ± .01\n✓ ✓ COVID-SciBERT † .34± .02 .92 ± .03 .78 ± .03 .67 ± .02 .76 ± .03\n✓ ✓ COVID-RoBERT a .40 ± .04 .92 ± .04 .91± .03 .67 ± .03 .78± .03\n✓ ✓ COVID-BART .33 ± .01 .91 ± .06 .89 ± .03 .66 ± .05 .77 ± .07\n✓ ✓ COVID-DeBERT a .30 ± .01 .92 ± .05 .89 ± .02 .81 ± .03 .73 ± .01\nscore difference of .02 to the second-best model BER T (F1=.7 6, std=.01). W e conclude\nthat pre-training language models on COVID data before ﬁne- tuning on a misinforma-\ntion task did not generally provide an advantage for the test ed datasets in this paper.\n6 Conclusion & Future W ork\nThis study empirically evaluated 15 Transformer models for ﬁve COVID-19 misinfor-\nmation tasks. Our analysis shows domain-speciﬁc models and tokenizers do not gener-\nally perform better in the classiﬁcation of misinformation . W e conclude that the vocab-\nulary related to COVID-19 and possibly text-patterns about COVID-19 do not have a\nsigniﬁcant effect on the models’ ability to classify misinf ormation.\nThe main limitation of our study is the non-standardized pre -training of models\ndue to the models’ diversity. T o reliably detect misinforma tion across content types and\nplatforms, researchers need access to diverse data. W e see t his study as an initial step to\ncompile a benchmark for COVID-19 data similar to widely adop ted natural language un-\nderstanding benchmarks (e.g., GLUE, SuperGLUE) which enab le an evaluation across\ndiverse sets of misinformation domains, sources, and tasks .\nControlling the current and future pandemics requires reli able detection of false in-\nformation propagated through many streams and having diffe rent unique features. This\nstudy is a ﬁrst step for researchers and policymakers to devi se and deploy systems that\nreliably ﬂag misinformation related to COVID-19 from a broa d spectrum of sources.\nT esting the Generalization of neural LMs for COVID-19 Misin formation Detection 7\nThe usefulness of NLP models increases signiﬁcantly if they are applicable to multi-\nple tasks [ 31]. W e anticipate future NLP technologies for detecting misi nformation need\nto adopt the trend of evaluating on several benchmark datase ts. This work provides a\nﬁrst milestone in evaluating general model capabilities an d questioning the advantage\nof domain-speciﬁc model pre-training.\nAlthough COVID-19 accelerated the propagation of misinfor mation and disinfor-\nmation, these problems are not unique to the current pandemi c. The effects of COVID-\n19 misinformation and disinformation on elections, ethica l biases, and the portrayal of\nethical groups [ 3] can have similar or even more severe consequences on societ y than\nmisinformation related to COVID-19. Therefore, identifyi ng false information streams\nacross domains will remain a challenging problem, and ident ifying which models can\ngeneralize for many sources is crucial.\nReferences\n1. Alsentzer, E., Murphy , J.R., Boag, W ., W eng, W .H., Jin, D. , Naumann, T ., McDermott,\nM.B.A.: Publicly A vailable Clinical BERT Embeddings. arXi v:1904.03323 [cs] (Jun 2019),\nhttp://arxiv.org/abs/1904.03323\n2. Beltagy , I., Lo, K., Cohan, A.: SciBERT: A Pretrained Lang uage Model for Scientiﬁc T ext.\nIn: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural La nguage Processing (EMNLP-\nIJCNLP). pp. 3613–3618. Association for Computational Lin guistics, Hong Kong, China\n(2019). https://doi.org/10/ggcgtm\n3. Benkler, Y ., Farris, R., Roberts, H.: Network Propaganda , vol. 1. Oxford University Press\n(Oct 2018). https://doi.org/10.1093/oso/9780190923624.001.0001\n4. Bojanowski, P ., Grave, E., Joulin, A., Mikolov , T .: Enric hing W ord V ectors with Subword\nInformation. Transactions of the Association for Computat ional Linguistics 5, 135–146 (Dec\n2017). https://doi.org/10/gfw9cs\n5. Cinelli, M., Quattrociocchi, W ., Galeazzi, A., V alensis e, C.M., Brugnoli, E., Schmidt, A.L.,\nZola, P ., Zollo, F ., Scala, A.: The COVID-19 social media inf odemic. Scientiﬁc Reports\n10(1), 16598 (Dec 2020). https://doi.org/10.1038/s41598-020-73510-5\n6. Clark, K., Luong, M.T ., Le, Q.V ., Manning, C.D.: ELECTRA: Pre-training T ext En-\ncoders as Discriminators Rather Than Generators. arXiv:20 03.10555 [cs] (Mar 2020),\nhttp://arxiv.org/abs/2003.10555\n7. Cui, L., Lee, D.: CoAID: COVID-19 Healthcare Misinformat ion Dataset. arXiv:2006.00885\n[cs] (Aug 2020), http://arxiv.org/abs/2006.00885\n8. Devlin, J., Chang, M.W ., Lee, K., T outanova, K.: BERT: Pre -training of Deep Bidi-\nrectional Transformers for Language Understanding. arXiv :1810.04805 [cs] (May 2019),\nhttp://arxiv.org/abs/1810.04805\n9. Dror, R., Baumer, G., Shlomov , S., Reichart, R.: The Hitch hiker’s Guide to T esting Sta-\ntistical Signiﬁcance in Natural Language Processing. In: P roceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics ( V olume 1: Long Papers). pp.\n1383–1392. Association for Computational Linguistics, Me lbourne, Australia (Jul 2018).\nhttps://doi.org/10.18653/v1/P18-1128\n10. Hale, T ., Angrist, N., Goldszmidt, R., Kira, B., Petheri ck, A., Phillips, T ., W ebster, S.,\nCameron-Blake, E., Hallas, L., Majumdar, S., T atlow , H.: A G lobal Panel Database of\nPandemic Policies (Oxford COVID-19 Government Response Tr acker). Nature Human Be-\nhaviour 5(4), 529–538 (Apr 2021). https://doi.org/10.1038/s41562-021-01079-8\n8\n11. He, P ., Liu, X., Gao, J., Chen, W .: DeBERT a: Decoding-enh anced BERT with Disentangled\nAttention. arXiv:2006.03654 [cs] (Jan 2021),\nhttp://arxiv.org/abs/2006.03654\n12. Howard, J., Ruder, S.: Universal Language Model Fine-tu ning for T ext Classiﬁcation. In:\nProceedings of the 56th Annual Meeting of the Association fo r Computational Linguistics\n(V olume 1: Long Papers). pp. 328–339. Association for Compu tational Linguistics, Mel-\nbourne, Australia (Jul 2018). https://doi.org/10.18653/v1/P18-1031\n13. Johnson, A.E., Pollard, T .J., Shen, L., Lehman, L.H., Fe ng, M., Ghassemi, M., Moody , B.,\nSzolovits, P ., Celi, L.A., Mark, R.G.: MIMIC-III, a freely a ccessible critical care database.\nScientiﬁc Data 3, 160035 (May 2016). https://doi.org/10.1038/sdata.2016.35\n14. Lee, J., Y oon, W ., Kim, S., Kim, D., Kim, S., So, C.H., Kang , J.: BioBERT: a pre-trained\nbiomedical language representation model for biomedical t ext mining. Bioinformatics pp. 1–\n7 (Sep 2019). https://doi.org/10.1093/bioinformatics/btz682\n15. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohame d, A., Levy , O., Stoyanov , V .,\nZettlemoyer, L.: BART: Denoising Sequence-to-Sequence Pr e-training for Natural Language\nGeneration, Translation, and Comprehension. In: Proceedi ngs of the 58th Annual Meeting\nof the Association for Computational Linguistics. pp. 7871 –7880. Association for Computa-\ntional Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.703\n16. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Le vy , O., Lewis, M., Zettle-\nmoyer, L., Stoyanov , V .: RoBERT a: A Robustly Optimized BERT Pretraining Approach.\narXiv:1907.11692 [cs] (Jul 2019), http://arxiv.org/abs/1907.11692\n17. Memon, S.A., Carley , K.M.: Characterizing COVID-19 Mis information Com-\nmunities Using a Novel T witter Dataset. arXiv:2008.00791 [ cs] (Sep 2020),\nhttp://arxiv.org/abs/2008.00791\n18. Mikolov , T ., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed Representations\nof W ords and Phrases and their Compositionality . arXiv:131 0.4546 [cs, stat] (Oct 2013),\nhttp://arxiv.org/abs/1310.45464\n19. Mutlu, E.C., Oghaz, T ., Jasser, J., Tutunculer, E., Raja bi, A., T ayebi, A., Ozmen, O.,\nGaribay , I.: A stance data set on polarized conversations on T witter about the efﬁcacy\nof hydroxychloroquine as a treatment for COVID-19. Data in B rief 33, 106401 (2020).\nhttps://doi.org/10.1016/j.dib.2020.106401\n20. Müller, M., Salathé, M., Kummervold, P .E.: COVID-T witt er-BERT: A Natural Language\nProcessing Model to Analyse COVID-19 Content on T witter. ar Xiv:2005.07503 [cs] (May\n2020), http://arxiv.org/abs/2005.07503\n21. Nguyen, D.Q., V u, T ., Tuan Nguyen, A.: BERT weet: A pre-tr ained language model for En-\nglish T weets. In: Proceedings of the 2020 Conference on Empi rical Methods in Natural Lan-\nguage Processing: System Demonstrations. pp. 9–14. Associ ation for Computational Lin-\nguistics, Online (2020). https://doi.org/10.18653/v1/2020.emnlp-demos.2\n22. Ostendorff, M., Ruas, T ., Blume, T ., Gipp, B., Rehm, G.: A spect-based Document Similar-\nity for Research Papers. In: Proceedings of the 28th Interna tional Conference on Computa-\ntional Linguistics. pp. 6194–6206. International Committ ee on Computational Linguistics,\nBarcelona, Spain (Online) (2020). https://doi.org/10.18653/v1/2020.coling-main.545\n23. Pennycook, G., McPhetres, J., Zhang, Y ., Lu, J.G., Rand, D.G.: Fighting COVID-\n19 Misinformation on Social Media: Experimental Evidence f or a Scalable\nAccuracy-Nudge Intervention. Psychological Science 31(7), 770–780 (Jun 2020).\nhttps://doi.org/10.1177/0956797620939054\n24. Press, O., Smith, N.A., Lewis, M.: Shortformer: Better L anguage Modeling using Shorter\nInputs. arXiv:2012.15832 [cs] (Dec 2020), http://arxiv.org/abs/2012.15832\n25. Ruas, T ., Ferreira, C.H.P ., Grosky , W ., de França, F .O., de Medeiros, D.M.R.: Enhanced\nW ord Embeddings Using Multi-Semantic Representation thro ugh Lexical Chains. Informa-\ntion Sciences 532, 16–32 (Sep 2020). https://doi.org/10.1016/j.ins.2020.04.048\nT esting the Generalization of neural LMs for COVID-19 Misin formation Detection 9\n26. Ruas, T ., Grosky , W ., Aizawa, A.: Multi-sense embedding s through a word sense\ndisambiguation process. Expert Systems with Applications 136, 288–303 (Dec 2019).\nhttps://doi.org/10.1016/j.eswa.2019.06.026\n27. Shu, K., Sliva, A., W ang, S., T ang, J., Liu, H.: Fake News D etection on Social Media: A\nData Mining Perspective. ACM SIGKDD Explorations Newslett er 19(1), 22–36 (Sep 2017).\nhttps://doi.org/10.1145/3137597.3137600\n28. V aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jon es, L., Gomez, A.N., Kaiser, L., Polo-\nsukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference\non Neural Information Processing Systems. p. 6000–6010. NI PS’17, Curran Associates Inc.,\nRed Hook, NY , USA (2017), https://arxiv.org/abs/1706.03762\n29. W ahle, J.P ., Ruas, T ., Foltynek, T ., Meuschke, N., Gipp, B.: Identifying Machine-Paraphrased\nPlagiarism. In: Proceedings of the iConference (February 2 022)\n30. W ahle, J.P ., Ruas, T ., Meuschke, N., Gipp, B.: Are Neural Language Models Good Plagia-\nrists? A Benchmark for Neural Paraphrase Detection. In: Pro ceedings of the ACM/IEEE\nJoint Conference on Digital Libraries (JCDL). IEEE, W ashin gton, USA (Sep 2021)\n31. W ang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Mich ael, J., Hill, F ., Levy , O., Bow-\nman, S.: SuperGLUE: A Stickier Benchmark for General-Purpo se Language Understanding\nSystems. In: W allach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F ., Fox, E., Garnett,\nR. (eds.) Advances in Neural Information Processing System s 32, pp. 3266–3280. Curran\nAssociates, Inc. (2019), https://arxiv.org/abs/1905.00537\n32. W ang, A., Singh, A., Michael, J., Hill, F ., Levy , O., Bowm an, S.R.: GLUE: A Multi-T ask\nBenchmark and Analysis Platform for Natural Language Under standing. arXiv:1804.07461\n[cs] (Feb 2019), https://arxiv.org/abs/1804.0746\n33. W ang, L.L., Lo, K., Chandrasekhar, Y ., Reas, R., Y ang, J. , Burdick, D., Eide, D., Funk, K.,\nKatsis, Y ., Kinney , R., Li, Y ., Liu, Z., Merrill, W ., Mooney , P ., Murdick, D., Rishi, D., Shee-\nhan, J., Shen, Z., Stilson, B., W ade, A., W ang, K., W ang, N.X. R., Wilhelm, C., Xie, B., Ray-\nmond, D., W eld, D.S., Etzioni, O., Kohlmeier, S.: CORD-19: T he COVID-19 Open Research\nDataset. arXiv:2004.10706 [cs] (Jul 2020), http://arxiv.org/abs/2004.10706\n34. Y ang, Z., Dai, Z., Y ang, Y ., Carbonell, J., Salakhutdino v , R., Le, Q.V .: XLNet: Generalized\nAutoregressive Pretraining for Language Understanding. a rXiv:1906.08237 [cs] (Jun 2019),\nhttps://arxiv.org/abs/1804.0746\n35. Zarocostas, J.: How to ﬁght an infodemic. The Lancet 395(10225), 676 (Feb 2020).\nhttps://doi.org/10.1016/S0140-6736(20)30461-X\n36. Zhou, X., Mulay , A., Ferrara, E., Zafarani, R.: ReCOV ery : A Multimodal Repository for\nCOVID-19 News Credibility Research, p. 3205–3212. Associa tion for Computing Machin-\nery , New Y ork, NY , USA (2020). https://doi.org/10.1145/3340531.3412880\n10\nA Appendix\nA.1 Dataset Details\nCOVID-19 Open Research Dataset(CORD-19) [\n33] is the largest open source dataset\nabout COVID-19 and coronavirus-related research (e.g. SAR S, MERS). CORD-19 is\ncomposed of more than 280K scholarly articles from PubMed, 16 bioRxiv,17 medRxiv,18\nand other resources maintained by the WHO. 19 W e use this dataset to extend the gen-\neral pre-training from selected neural language models (cf . Section 3) into the COVID-\nspeciﬁc vocabulary and features.\nCovid-19 heAlthcare mIsinformation Dataset (CoAID) [ 7] focuses on healthcare\nmisinformation, including fake news on websites, user enga gement, and social media.\nCoAID is composed of 5 216 news articles, 296 752 related user engagements, and 958\nposts about COVID-19, which are broadly categorized under t he labels true and false.\nT witter Stance Dataset (COVID-CQ) [ 19] is a dataset of user-generated T witter\ncontent in the context of COVID-19. More than 14K tweets were processed and anno-\ntated regarding the use of Chloroquine and Hydroxychloroquine as a valid treatment\nor prevention against the coronavirus. COVID-CQ is compose d of 14 374 tweets from\n11 552 unique users labeled as neutral, against, or favor.\nReCOV ery[36] explores the low credibility of information on COVID-19 (e .g.,\nbleach can prevent COVID-19) by allowing a multimodal inves tigation of news and\ntheir spread on social media. The dataset is composed of 2 029 news articles on the\ncoronavirus and 140 820 related tweets labeled as reliable or unreliable.\nCMU-MisCov19 [17] is a T witter dataset created by collecting posts from unkno w-\ningly misinformed users, users who actively spread misinfo rmation, and users who\ndisseminate facts or call out misinformation. CMU-MisCov1 9 is composed of 4 573\nannotated tweets divided into 17 classes (e.g., conspiracy, fake cure , news, sarcasm).\nThe high number of classes and their imbalanced distributio n make CMU-MisCov19 a\nchallenging dataset.\nCOVID19FN20 is composed of approximately 2 800 news articles extracted m ainly\nfrom Poynter 21 categorized as either real or fake.\nA.2 Model Details\nGeneral-Purpose Baselines. BER T [\n8] mainly captures general language characteris-\ntics using a bidirectional Masked Language Model (MLM) and Next Sentence Predic-\ntion (NSP) tasks. RoBER T a [ 16] improves BER T with additional data, compute budgets,\nand hyperparameter optimizations. RoBER T a also drops the N SP as it contributes little\nto the model representation. BAR T [ 15] optimizes an auto-regressive forward-product\n16 https://pubmed.ncbi.nlm.nih.gov/\n17 https://www.biorxiv.org/\n18 https://www.medrxiv.org/\n19 https://www.who.int/\n20 https://tinyurl.com/4mryzj5k\n21 https://www.poynter.org/ifcn/\nT esting the Generalization of neural LMs for COVID-19 Misin formation Detection 11\nand auto-encoding MLM objective simultaneously. DeBER T a [ 11] improves the atten-\ntion mechanism using a disentanglement of content and posit ion.\nIntermediate Pre-T rained.SciBER T [ 2] optimizes the MLM for 1.14M randomly\nselected papers from Semantic Scholar 22. BioClinicalBER T [ 1] specializes on 2M notes\nin the MIMIC-III database [ 13], a collection of disidentiﬁed clinical data. BER T weet\n[21] optimizes BER T on 850M tweets each containing between 10 an d 64 tokens.\nCOVID-19 Intermediate Pre-T rainedCOVID-T witter-BER T [20] (CT -BER T) uses\na corpus of 160M tweets for domain-speciﬁc pre-training and evaluates the resulting\nmodel’s capabilities in sentiment analysis, such as for twe ets about vaccines. BioClini-\ncalBER T [1] ﬁne-tunes BioBER T [ 14] into clinical narratives in the hope to incorporate\nlinguistic characteristics from both the clinical and biom edical domains.\nCui et al. [ 7] propose CoAID and investigate the misinformation detecti on task by\ncomparing traditional machine learning (e.g., logistic re gression, random forest) and\ndeep learning techniques (e.g., GRU). In a similar layout, Z hou et al. [ 36] compare\ntraditional statistical learners, such as SVM and neural ne tworks (e.g., CNN), to classify\nnews as credible or not. In both studies, the results show dee p learning architectures as\nthe most prominent options.\nA.3 Evaluation Details\nPre-T raining.W e use the data from the abstracts of the CORD-19 dataset for p re-\ntraining. For pre-processing the CORD-19 abstract data, we consider only alphanumer-\nical characters. W e use a sequence length of 128 tokens, whic h reduces training time\nwhile being competitive to longer sequence lengths when ﬁne -tuning [\n24]. W e mask\nwords randomly with a probability of .15, a common conﬁgurat ion for Transformers\n[8,11], and perform the MLM with the following remaining paramete rs: a batch size\nof 16 for all the base models, and eight for the large models, t he Adam Optimizer\n(α = 2e − 5, β1 = .9, β2 = .999, ǫ = 1e − 8), and a maximum of ﬁve epochs. All\nexperiments were performed on a single NVIDIA GeForce GTX 10 80 Ti GPU with\n11 GB of memory.\nFine-T uning.The classiﬁcation model applies a randomly initialized ful ly-connected\nlayer to the aggregate representation of the underlying Tra nsformer (e.g., [CLS] for\nBER T) with dropout ( p = .1) to learn the annotated target classes with cross-entropy\nloss for ﬁve epochs and with a sequence length of 200 tokens. W e use the same conﬁg-\nuration of the optimizer as in pre-training.\n22 https://www.semanticscholar.org/",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.9772591590881348
    },
    {
      "name": "Computer science",
      "score": 0.7882430553436279
    },
    {
      "name": "Flagging",
      "score": 0.7175361514091492
    },
    {
      "name": "Social media",
      "score": 0.6646004319190979
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.6048778891563416
    },
    {
      "name": "Data science",
      "score": 0.5087493062019348
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3733710050582886
    },
    {
      "name": "Information retrieval",
      "score": 0.35476481914520264
    },
    {
      "name": "World Wide Web",
      "score": 0.24137413501739502
    },
    {
      "name": "Computer security",
      "score": 0.20880073308944702
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I167360494",
      "name": "University of Wuppertal",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I132153292",
      "name": "Indian Institute of Technology Patna",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}