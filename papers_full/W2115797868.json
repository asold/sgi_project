{
  "title": "Incorporating POS Tagging into Language Modeling",
  "url": "https://openalex.org/W2115797868",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4294520095",
      "name": "Heeman, Peter A.",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A4305616170",
      "name": "Allen, James F.",
      "affiliations": [
        "University of Rochester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2441154163",
    "https://openalex.org/W2144158482",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2116625254",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2166394306",
    "https://openalex.org/W1571096757",
    "https://openalex.org/W1965112520",
    "https://openalex.org/W2165487751",
    "https://openalex.org/W3085162807",
    "https://openalex.org/W3003494796",
    "https://openalex.org/W2143179123",
    "https://openalex.org/W2099345940",
    "https://openalex.org/W1594031697",
    "https://openalex.org/W2061271742"
  ],
  "abstract": "Language models for speech recognition tend to concentrate solely on recognizing the words that were spoken. In this paper, we redefine the speech recognition problem so that its goal is to find both the best sequence of words and their syntactic role (part-of-speech) in the utterance. This is a necessary first step towards tightening the interaction between speech recognition and natural language understanding.",
  "full_text": "arXiv:cmp-lg/9705014v1  22 May 1997\nINCORPORATING POS TAGGING INTO LANGUAGE MODELING ∗\nPeter A. Heeman\nFrance T´ el´ ecom CNET\nTechnopole Anticipa - 2 Avenue Pierre Marzin\n22301 Lannion Cedex, France.\nheeman@lannion.cnet.fr\nJames F. Allen\nDepartment of Computer Science\nUniversity of Rochester\nRochester NY 14627, USA\njames@cs.rochester.edu\nAbstract\nLanguage models for speech recognition tend\nto concentrate solely on recognizing the words\nthat were spoken. In this paper, we rede-\nﬁne the speech recognition problem so that\nits goal is to ﬁnd both the best sequence of\nwords and their syntactic role (part-of-speech)\nin the utterance. This is a necessary ﬁrst\nstep towards tightening the interaction between\nspeech recognition and natural language un-\nderstanding.\n1 INTRODUCTION\nFor recognizing spontaneous speech, the acoustic signal\nis to weak to narrow down the number of word candi-\ndates. Hence, speech recognizers employ a language\nmodel that prunes out acoustic alternatives by taking\ninto account the previous words that were recognized.\nIn doing this, the speech recognition problem is viewed\nas ﬁnding the most likely word sequenceˆW given the\nacoustic signal (Jelinek, 1985).\nˆW = arg max\nW\nPr(W |A)\n= arg max\nW\nPr(A|W ) Pr(W )\nPr(A)\n= arg max\nW\nPr(A|W ) Pr(W )\nThe last line involves two probabilities that need to be\nestimated—the ﬁrst due to the acoustic modelPr(A|W )\nand the second due to the language modelPr(W ). The\nprobability due to the language model can be expressed\nas the following, where we rewrite the sequenceW ex-\n∗\nIn proceedings of Eurospeech’97. This research work\nwas completed while the ﬁrst author was at the University of\nRochester. The authors would like to thank Geraldine Damnati,\nKyung-ho Loken-Kim, Tsuyoshi Morimoto, Eric Ringger and\nRamesh Sarukkai. This material is based upon work supported\nby the NSF under grant IRI-9623665 and by ONR under grant\nN00014-95-1-1088.\nplicitly as the sequence ofN words W1,N.\nPr(W1,N) =\nN∏\ni=1\nPr(Wi|W1,i-1)\nTo estimate the probability distribution, a training cor-\npus is typically used from which the probabilities can be\nestimated by relative frequencies. Due to sparseness of\ndata, one must deﬁne equivalence classes amongst the\ncontextsW1,i-1, which can be done by limiting the con-\ntext to ann-gram language model (Jelinek, 1985) and\nalso by grouping words into words classes (Brown et al.,\n1992).\nSeveral attempts have been made to incorporate shal-\nlow syntactic information to give better equivalence\nclasses, where the shallow syntactic information is ex-\npressed as part-of-speech (POS) tags (e.g. (Jelinek,\n1985), (Niesler and Woodland, 1996)). A POS tag indi-\ncates the syntactic role that a particular word is playing in\nthe utterance, e.g. whether it is a noun or a verb, etc. The\napproach is to use the POS tags of the prior few words to\ndeﬁne the equivalence classes. This is done by summing\nover all POS possibilities as shown below.\nPr(Wi|W1,i-1)\n=\n∑\nP1,i\nPr(Wi|P1,iW1,i-1) Pr(P1,i|W1,i-1)\n=\n∑\nP1,i\nPr(Wi|P1,iW1,i-1) Pr(Pi|P1,i-1W1,i-1) Pr(P1,i-1|W1,i-1)\nFurthermore, the following two assumptions are made to\nsimplify the context.\nPr(Wi|P1,iW1,i-1) ≈ Pr(Wi|Pi)\nPr(Pi|P1,i-1W1,i-1) ≈ Pr(Pi|P1,i-1)\nHowever, this approach does not lead to an improve-\nment in the performance of the speech recognizer. For\ninstance, Srinivas (Srinivas, 1996) reports that such a\nmodel results in a 24.5% increase in perplexity over\na word-based model on the Wall Street Journal, and\nNiesler and Woodland (Niesler and Woodland, 1996) re-\nport an 11.3% increase (but a 22-fold decrease in the\nnumber of parameters of such a model). Only by inter-\npolating in a word-based model is an improvement seen\n(Jelinek, 1985).\nA more major problem with the above approach is that\nin a spoken dialogue system, speech recognition is only\nthe ﬁrst step in understanding a speaker’s contribution.\nOne also needs to determine the syntactic structure of the\nwords involved, its semantic meaning, and the speaker’s\nintention in making the utterance. This information is\nneeded to help the speech recognizer constrain the alter-\nnative hypotheses. Hence, we need a tighter coupling\nbetween speech recognition and the rest of the interpre-\ntation process.\n2 REDEFINING THE PROBLEM\nAs a starting point, we re-examine the approach of us-\ning POS tags in the speech recognition process. Rather\nthan view POS tags as intermediate objects solely to\nﬁnd the best word assignment, we redeﬁne the goal of\nthe speech recognition process so that it ﬁnds the best\nword sequenceand the best POS interpretation given the\nacoustic signal.\nˆW ˆP = arg max\nW P\nPr(W P|A)\n= arg max\nW P\nPr(A|W P) Pr(W P)\nThe ﬁrst termPr(A|W P) is the acoustic model, which\ntraditionally excludes the category assignment. The sec-\nond term Pr(W P) is the POS-based language model.\nJust as before, we rewrite the probability ofPr(W P) as\na product of probabilities of the word and POS tag given\nthe previous context.\nPr(W1,NP1,N)\n=\n∏\ni=1,j\nPr(WiPi|W1,i-1P1,i-1)\n=\n∏\ni=1,j\nPr(Wi|W1,i-1P1,i) Pr(Pi|W1,i-1P1,i-1)\nThe ﬁnal probability distributions are similar to those\nused for POS tagging of written text (Charniak et al.,\n1993; Church, 1988; DeRose, 1988). However, these ap-\nproaches simplify the probability distributions as is done\nby previous attempts to use POS tags in speech recogni-\ntion language models.1 As we will show in Section 4.1,\nsuch simpliﬁcations lead to poorer language models.\n3 ESTIMATING THE PROBABILITIES\nThe probability distributions that we now need to es-\ntimate are more complicated then the traditional ones.\nOur approach is to use the decision tree learning algo-\nrithm (Bahl et al., 1989; Black et al., 1992; Breiman et\n1A notable exception is the work of Blacket al.(Black et\nal., 1992), who use a decision tree to learn the probability dis-\ntributions for POS tagging.\nal., 1984), which uses information theoretic measures to\nconstruct equivalence classes of the context in order to\ncope with sparseness of data. The decision tree algorithm\nstarts with all of the training data in a single leaf node.\nFor each leaf node, it looks for the question to ask of the\ncontext such that splitting the node into two leaf nodes\nresults in the biggest decrease inimpurity, where the im-\npurity measures how well each leaf predicts the events\nin the node. Heldout data is used to decide when to stop\ngrowing the tree: a split is rejected if the split does not re-\nsult in a decrease in impurity with respect to the heldout\ndata. After the tree is grown, the heldout dataset is used\nto smooth the probabilities of each node with its parent\n(Bahl et al., 1989).\n3.1 Word and POS Classiﬁcation Trees\nTo allow the decision tree to ask about the words and\nPOS tags in the context, we cluster the words and POS\ntags using the algorithm of Brownet al.(Brown et al.,\n1992) into a binary classiﬁcation tree. The algorithm\nstarts with each word (or POS tag) in a separate class, and\nsuccessively merges classes that result in the smallest lost\nin mutual information in terms of the co-occurrences of\nthese classes. By keeping track of the order that classes\nwere merged, we can construct a hierarchical classiﬁca-\ntion of the words. Figure 1 shows a classiﬁcation tree\nthat we grew for the POS tags. The binary classiﬁcation\ntree gives an implicit binary encoding for each word and\nPOS tag, which we show after each POS tag in the ﬁgure.\nThe decision tree algorithm can then ask questions about\nthe binary encoding of the words, such as ‘is the third bit\nof the POS tag encoding equal to one?’, and hence can\nask about which partition a word is in.\nUnlike other work that uses classiﬁcation trees as\nthe basis for the questions used by a decision tree\n(e.g. (Black et al., 1992)), we treat the word identities\nas a further reﬁnement of the POS tags. This approach\nhas the advantage of avoiding unnecessary data fragmen-\ntation, since the POS tags and word identities will not be\nviewed as separate sources of information. We grow the\nclassiﬁcation tree by starting with a unique class for each\nword and each POS tag that it takes on. When we merge\nclasses to form the hierarchy, we only allow merges if all\nof the words in both classes have the same POS tag. The\nresult is a word classiﬁcation tree for each POS tag. This\napproach to growing the word trees simpliﬁes the task,\nsince we can take advantage of the hand-coded linguistic\nknowledge (as represented by the POS tags). Further-\nmore, we can better deal with words that can take on\nmultiple senses, such as the word “loads”, which can be\na plural noun (NNS ) or a present tense third-person verb\n(PRP ).2\n2Words-POS combinations that occur only once in the train-\ning corpus are grouped together in the class<unknown >,\nMUMBLE 0 0 0 0 0 0 0 0 0\nUH D 1 0 0 0 0 0 0 0 0\nUH FP 1 0 0 0 0 0 0 0\nFRAGMENT 1 0 0 0 0 0 0\nCC D 1 0 0 0 0 0 DOD 0 0 0 0 0 1 0 0 0 0\nDOP 1 0 0 0 0 1 0 0 0 0\nDOZ 1 0 0 0 1 0 0 0 0\nSC 1 0 0 1 0 0 0 0\nEX 0 0 1 0 1 0 0 0 0\nWP 1 0 1 0 1 0 0 0 0\nWRB 1 1 0 1 0 0 0 0RB D 1 1 0 0 0 0AC 1 0 0 0\nCAN 0 0 0 0 1 0 0\nMOD 1 0 0 0 1 0 0\nABR 1 0 0 1 0 0\nPUSH 0 1 0 1 0 0\nPOP 1 1 0 1 0 0\nTURN 0 1 1 0 0\nTONE 1 1 1 0 0\nDO 0 0 0 0 0 0 1 0\nHA VE 1 0 0 0 0 0 1 0\nBE 1 0 0 0 0 1 0\nVB 1 0 0 0 1 0 BEG 0 0 0 0 0 0 1 0 0 1 0\nHA VEG 1 0 0 0 0 0 1 0 0 1 0\nBEN 1 0 0 0 0 1 0 0 1 0\nPPREP 0 1 0 0 0 1 0 0 1 0\nRBR 1 1 0 0 0 1 0 0 1 0PDT 1 0 0 1 0 0 1 0\nRB 1 0 1 0 0 1 0\nVBG 0 0 1 1 0 0 1 0\nVBN 1 0 1 1 0 0 1 0\nRP 1 1 1 0 0 1 0\nHA VED 0 0 0 0 0 0 1 0 1 0\nHA VEZ 1 0 0 0 0 0 1 0 1 0\nBED 1 0 0 0 0 1 0 1 0\nVBZ 1 0 0 0 1 0 1 0\nBEZ 1 0 0 1 0 1 0\nVBD 0 0 0 1 0 1 0 1 0\nVBP 1 0 0 1 0 1 0 1 0\nHA VEP 1 0 1 0 1 0 1 0\nBEP 1 1 0 1 0 1 0\nMD 0 1 1 0 1 0\nTO 1 1 1 0 1 0DP 0 1 1 0\nPRP 1 1 1 0\nCC 0 0 0 1\nPREP 1 0 0 1\nJJ0 0 0 0 1 0 1\nJJS1 0 0 0 1 0 1\nJJR 1 0 0 1 0 1\nCD 1 0 1 0 1\nDT 0 0 1 1 0 1\nPRP$ 1 0 1 1 0 1\nWDT 1 1 1 0 1\nNN 0 0 1 1\nNNS 1 0 1 1\nNNP 1 1 1\nFigure 1: POS Classiﬁcation Tree\n<unknown > 0 0 0 0\nthem 1 0 0 0\nme 0 1 0 0\nus 1 1 0 0it1 0\nthey0 0 0 1\nwe 1 0 0 1\nyou 1 0 1\ni1 1\nFigure 2: A Word Classiﬁcation Tree\nIn Figure 2, we give the classiﬁcation tree for the per-\nsonal pronouns (PRP ). It is interesting to note that the\nclustering algorithm distinguished between the subjec-\ntive pronouns ‘I’, ‘we’, and ‘they’, and the objective pro-\nnouns ‘me’, ‘us’, and ‘them’. The pronouns ‘you’ and\n‘it’ can take either case, and the algorithm partitioned\nthem according to their most common usage in the train-\ning corpus. Although distinct POS tags could have been\nadded to distinguish between these two cases, it seems\nthat the clustering algorithm can make up for some of\nthe shortcomings of the tagset.3\n3.2 Composite Questions\nIn the previous section, we discussed the elementary\nquestions that can be asked of the words and POS tags\nin the context. However, there might be a relevant parti-\ntioning of the data that can not be expressed in that form.\nFor instance, a good partitioning of a node might involve\nasking whether questionsq1 and q2 are both true. Us-\ning elementary questions, the decision tree would need\nto ﬁrst ask questionq1 and then askq2 in the true subn-\node created byq1. This means that the false case has\nbeen split into two separate nodes, which could cause\nunnecessary data fragmentation.\nUnnecessary data fragmentation can be avoided by al-\nlowing composite questions. Bahlet al.(Bahl et al.,\n1989) introduced a simple but effective approach for con-\nstructing composite questions. Rather than allowing any\nboolean combination of elementary questions, they re-\nstrict the typology of the combinations topylons, which\nhave the following form (truemaps all data into the true\nsubset).\npylon⇒ true\npylon⇒ (pylon∧ elementary)\npylon⇒ (pylon∨ elementary)\nThe effect of any binary question is to divide the data\ninto true and false subsets. The advantage of pylons is\nthat each successive elementary question has the effect\nof swapping data from the true subnode into the false or\nvice versa. Hence, one can compute the change in node\nwhich is unique for each POS tag.\n3The words included in the<unknown > class are the re-\nﬂexive pronouns ‘themselves’, and ‘itself’, which each oc-\ncurred once in the training corpus.\nimpurity that results from each successive elementary\nquestion that is added. This allows one to use a greedy\nalgorithm to build the pylon by successively choosing the\nelementary question that results in the largest decrease in\nnode impurity.\nWe actually employ a beam search and explore the\nbest 10 alternatives at each level of the pylon. Again we\nmake use of the heldout data to help pick the best pylon,\nbut we must be careful not to make too much use of it for\notherwise it will become as biased as the training data.\nIf the last question added to a candidate pylon results in\nan increase in node impurity with respect to the heldout\ndata, we remove that question and stop growing that al-\nternative. When there are no further candidates that can\nbe grown, we choose the winning pylon as the one with\nthe best decrease in node impurity with respect to the\ntraining data. The effect of using composite questions is\nexplored in Section 4.3.\n4 RESULTS\nTo demonstrate our model, we have tested it on the\nTrains corpus (Heeman and Allen, 1995), a collection of\nhuman-human task-oriented spoken dialogues consisting\nof 6 and half hours worth of speech, 34 different speak-\ners, 58,000 words of transcribed speech, with a vocab-\nulary size of 860 words. To make the best use of the\nlimited amount of data, we use a 6-fold cross validation\nprocedure, in which we use each sixth of the corpus for\ntesting data, and the rest for training data.\nA way to measure a language model is to compute the\nperplexityit assigns to a test corpus, which is an estimate\nof how well the language model is able to predict the\nnext word. The perplexity of a test set ofN words w1,N\nis calculated as follows,\n2− 1\nN\n∑ N\ni=1 log2 ˆPr(wi|w1,i− 1)\nwhere ˆPr is the probability distribution supplied by the\nlanguage model. Full details of how we compute the\nword-based perplexity are given in (Heeman, 1997). We\nalso measure the error rate in assigning the POS tags.\nHere, as in measuring the perplexity, we run the language\nmodel on the hand-transcribed word annotations.\n4.1 Effect of Richer Context\nTable 1 gives the perplexity and POS tagging error rate\n(expressed as a percent). To show the effect of the richer\nmodeling of the context, we vary the amount of context\ngiven to the decision tree. As shown by the perplexity\nresults, the context used for traditional POS-based lan-\nguage models (second column) is very impoverished. As\nwe remove the simpliﬁcations to the context, we see the\nperplexity and POS tagging rates improve. By using both\nthe previous words and previous POS tags as the context,\nwe achieve a 43% reduction in perplexity and a 5.4% re-\nduction in the POS error rate.\nContext forWi Pi Pi-3,i Pi-3,iWi-3,i-1 Pi-3,iWi-3,i-1\nContent forPi Pi-3,i-1 Pi-3,i-1 Pi-3,i-1 Pi-3,i-1Wi-3,i-1\nPOS Error Rate 3.13 3.10 3.03 2.97\nPerplexity 42.32 32.11 29.49 24.17\nTable 1: Using Richer Contexts\n4.2 Constraining the Decision Tree\nAs we mentioned earlier, the word identity information\nWi− j is viewed as further reﬁning the POS tag of the\nword Pi− j . Hence, questions about the word encoding\nare only allowed if the POS tag is uniquely deﬁned. Fur-\nthermore, for both POS and word questions, we restrict\nthe algorithm so that it only asks about more speciﬁc bits\nof the POS tag and word encodings only if it has already\nuniquely identiﬁed the less speciﬁc bits. In Table 2, we\ncontrast the effectiveness of adding further constraints.\nThe second column gives the results of adding no further\nconstraints, the third column only allows questions about\na POS tagPi− j− 1 only ifPi− j is uniquely determined,\nand the fourth column adds the constraint that the word\nWi− j must also be uniquely identiﬁed before questions\nare allowed ofPi− j− 1.\nFrom the table, we see that it is worthwhile to force the\ndecision tree to fully explore a POS tag for a word in the\ncontext before asking about previous words. Hence, we\nsee that the decision tree algorithm needs help in learn-\ning that it is better to fully explore the POS tags. How-\never, we see that adding the further constraint that the\nword identity should also be fully explored results in a\ndecrease in performance of the model. Hence, we see\nthat it is not worthwhile for the decision tree to fully ex-\nplore the word information (which is the basis of class-\nbased approaches to language modeling), and it is able to\nlearn this on its own.\n4.3 Effect of Composites\nThe next area we explore is the beneﬁt of composite\nquestions in estimating the probability distributions. The\nsecond column of Table 3 gives the results if compos-\nite questions are not employed, the third column gives\nthe results if composite questions are employed, and the\nfourth gives the results if we employ a beam search in\nﬁnding the best pylon (with up to 10 alternatives). From\nNone POS Full\nPOS Error Rate 3.19 2.97 3.00\nPerplexity 25.64 24.17 24.39\nTable 2: Adding Additional Constraints\nthe results, we see that the use of pylons reduces the word\nperplexity rate by 4.7%, and the POS error rate by 2.3%.\nFurthermore, we see that using a beam search, rather than\nan entirely greedy algorithm accounts for some of the im-\nprovement.\nNot Used Single 10\nPOS Error Rate 3.04 3.04 2.97\nPerplexity 25.36 24.36 24.17\nTable 3: Effect of Composite Questions\n4.4 Effect of Larger Context\nIn Table 4, we look at the effect of the size of the con-\ntext, and compare the results to a word-based backoff\nlanguage model (Katz, 1987) built using the CMU toolkit\n(Rosenfeld, 1995). For a bigram model, it has a per-\nplexity of 29.3, in comparison to our word perplexity of\n27.4. For a trigram model, the word-based model has\na perplexity of 26.1, in comparison to our perplexity of\n24.2. Hence we see that our POS-based model results in\na 7.2% improvement in perplexity.\nBigram Trigram 4-gram\nPOS Error Rate 3.19 2.97 2.97\nPerplexity 27.37 24.26 24.17\nWord-based Model 29.30 26.13\nTable 4: Using Larger Contexts\n5 CONCLUSION\nIn this paper, we presented a new way of incorporating\nPOS information into a language model. Rather than\ntreating POS tags as intermediate objects solely for rec-\nognizing the words, we redeﬁne the speech recognition\nproblem so that its goal is to ﬁnd the best word sequence\nand their best POS assignment. This approach allows\nus to use the POS tags as part of the context for esti-\nmating the probability distributions. In fact, we view the\nword identities in the context as a reﬁnement of the POS\ntags rather than viewing the POS tags and word identi-\nties as two separate sources of information. To deal with\nthis rich context, we make use of decision trees, which\ncan use information theoretic measures to automatically\ndetermine how to partition the contexts into equivalence\nclasses. We ﬁnd that this model results in a 7.2% re-\nduction in perplexity over a trigram word-based model\nfor the Trains corpus of spontaneous speech. Currently,\nwe are exploring the effect of this model in reducing the\nword error rate.\nIncorporating shallow syntactic information into the\nspeech recognition process is just the ﬁrst step. In other\nwork (Heeman, 1997; Heeman and Allen, 1997), this\nsyntactic information, as well as the techniques intro-\nduced in this paper, are used to help model the oc-\ncurrence of dysﬂuencies and intonational phrasing in a\nspeech recognition language model. Our use of deci-\nsion trees to estimate the probability distributions proves\neffective in dealing with the richer context provided by\nmodeling these spontaneous speech events. Modeling\nthese events improves the perplexity to 22.5, a 14% im-\nprovement over the word-based trigram backoff model,\nand reduces the POS error rate by 9%.\nReferences\n[Bahl et al.1989] Bahl, L. R., P. F. Brown, P. V . deSouza,\nand R. L. Mercer. 1989. A tree-based statistical lan-\nguage model for natural language speech recognition.\nIEEE Transactions on Acoustics, Speech, and Signal\nProcessing, 36(7):1001–1008.\n[Black et al.1992] Black, E., F. Jelinek, J. Lafferty,\nR. Mercer, and S. Roukos. 1992. Decision tree mod-\nels applied to the labeling of text with parts-of-speech.\nIn Proceedings of the DARPA Speech and Natural\nLanguage Workshop, pages 117–121. Morgan Kauf-\nmann.\n[Breiman et al.1984] Breiman, L., J. H. Friedman, R. A.\nOlshen, and C. J. Stone. 1984.Classiﬁcation and Re-\ngression Trees. Monterrey, CA: Wadsworth & Brooks.\n[Brown et al.1992] Brown, P. F., V . J. Della Pietra, P. V .\ndeSouza, J. C. Lai, and R. L. Mercer. 1992. Class-\nbasedn-gram models of natural language.Computa-\ntional Linguistics, 18(4):467–479.\n[Charniak et al.1993] Charniak, E., C. Hendrickson,\nN. Jacobson, and M. Perkowitz. 1993. Equa-\ntions for part-of-speech tagging. InProceedings\nof the National Conference on Artiﬁcial Intelligence\n(AAAI ’93).\n[Church1988] Church, K. 1988. A stochastic parts pro-\ngram and noun phrase parser for unrestricted text. In\nProceedings of the 2nd Conference on Applied Natu-\nral Language Processing, pages 136–143, Febuary.\n[DeRose1988] DeRose, S. J. 1988. Grammatical cate-\ngory disambiguation by statistical optimization.Com-\nputational Linguistics, 14(1):31–39.\n[Heeman1997] Heeman, P. A. 1997. Speech repairs, in-\ntonational boundaries and discourse markers: Model-\ning speakers’ utterances in spoken dialog. Doctoral\ndissertation. In preparation.\n[Heeman and Allen1995] Heeman, P. A. and J. F. Allen.\n1995. The Trains spoken dialog corpus. CD-ROM,\nLinguistics Data Consortium, April.\n[Heeman and Allen1997] Heeman, P. A. and J. F. Allen.\n1997. Intonational boundaries, speech repairs, and\ndiscourse markers: Modeling spoken dialog. InPro-\nceedings of the 35th Annual Meeting of the Associa-\ntion for Computational Linguistics, Madrid, July.\n[Jelinek1985] Jelinek, F. 1985. Self-organized language\nmodeling for speech recognition. Technical report,\nIBM T.J. Watson Research Center, Continuous Speech\nRecognition Group, Yorktown Heights, NY .\n[Katz1987] Katz, S. M. 1987. Estimation of probabilities\nfrom sparse data for the language model component\nof a speech recognizer.IEEE Transactions on Acous-\ntics, Speech, and Signal Processing, pages 400–401,\nMarch.\n[Niesler and Woodland1996] Niesler, T. R. and P. C.\nWoodland. 1996. A variable-length category-based\nn-gram language model. InProceedings of the In-\nternational Conference on Audio, Speech and Signal\nProcessing (ICASSP), pages 164–167.\n[Rosenfeld1995] Rosenfeld, R. 1995. The CMU statisti-\ncal language modeling toolkit and its use in the 1994\nARPA CSR evaluation. InProceedings of the ARPA\nSpoken Language Systems Technology Workshop, San\nMateo, California. Morgan Kaufmann.\n[Srinivas1996] Srinivas, B. 1996. “Almost parsing” tech-\nniques for language modeling. InProceedings of\nthe 4rd International Conference on Spoken Language\nProcessing (ICSLP-96), pages 1169–1172.",
  "topic": "Utterance",
  "concepts": [
    {
      "name": "Utterance",
      "score": 0.8423573970794678
    },
    {
      "name": "Computer science",
      "score": 0.7606152296066284
    },
    {
      "name": "Natural language processing",
      "score": 0.6151354908943176
    },
    {
      "name": "Speech recognition",
      "score": 0.5233834981918335
    },
    {
      "name": "Spoken language",
      "score": 0.5092915296554565
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5048745274543762
    },
    {
      "name": "Natural language",
      "score": 0.4926162362098694
    },
    {
      "name": "Sequence (biology)",
      "score": 0.48452991247177124
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.45239463448524475
    },
    {
      "name": "Language model",
      "score": 0.4355010390281677
    },
    {
      "name": "Natural language understanding",
      "score": 0.4187001585960388
    },
    {
      "name": "Linguistics",
      "score": 0.41043293476104736
    },
    {
      "name": "History",
      "score": 0.057220250368118286
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "cited_by": 14
}