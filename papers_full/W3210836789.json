{
    "title": "Detecting dementia from speech and transcripts using transformers",
    "url": "https://openalex.org/W3210836789",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3148694876",
            "name": "Loukas Ilias",
            "affiliations": [
                "National Technical University of Athens"
            ]
        },
        {
            "id": "https://openalex.org/A2111578312",
            "name": "Dimitris Askounis",
            "affiliations": [
                "National Technical University of Athens"
            ]
        },
        {
            "id": "https://openalex.org/A1087370589",
            "name": "John Psarras",
            "affiliations": [
                "National Technical University of Athens"
            ]
        },
        {
            "id": "https://openalex.org/A3148694876",
            "name": "Loukas Ilias",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111578312",
            "name": "Dimitris Askounis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1087370589",
            "name": "John Psarras",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6748656883",
        "https://openalex.org/W6781962837",
        "https://openalex.org/W6772499777",
        "https://openalex.org/W4211139529",
        "https://openalex.org/W6781094158",
        "https://openalex.org/W6797050280",
        "https://openalex.org/W2074037951",
        "https://openalex.org/W3201447839",
        "https://openalex.org/W6790997423",
        "https://openalex.org/W2017608047",
        "https://openalex.org/W3131203259",
        "https://openalex.org/W6785703739",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6766307071",
        "https://openalex.org/W6785214930",
        "https://openalex.org/W2239141610",
        "https://openalex.org/W6672893027",
        "https://openalex.org/W6746941363",
        "https://openalex.org/W6675668128",
        "https://openalex.org/W6800554122",
        "https://openalex.org/W6763591306",
        "https://openalex.org/W6745428062",
        "https://openalex.org/W2991435809",
        "https://openalex.org/W3124172077",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6727253422",
        "https://openalex.org/W6725739302",
        "https://openalex.org/W3199225381",
        "https://openalex.org/W6782198553",
        "https://openalex.org/W6783300742",
        "https://openalex.org/W6684370013",
        "https://openalex.org/W3155456108",
        "https://openalex.org/W6774503009",
        "https://openalex.org/W6776063285",
        "https://openalex.org/W3129165846",
        "https://openalex.org/W3170936667",
        "https://openalex.org/W6784656178",
        "https://openalex.org/W6792909391",
        "https://openalex.org/W2970737019",
        "https://openalex.org/W6844447855",
        "https://openalex.org/W6800980725",
        "https://openalex.org/W6785042501",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W3160231559",
        "https://openalex.org/W6778069516",
        "https://openalex.org/W3003074332",
        "https://openalex.org/W6784382936",
        "https://openalex.org/W6798135548",
        "https://openalex.org/W6794266317",
        "https://openalex.org/W3197196325",
        "https://openalex.org/W3162212351",
        "https://openalex.org/W6749781174",
        "https://openalex.org/W6782944601",
        "https://openalex.org/W3128781814",
        "https://openalex.org/W6791539774",
        "https://openalex.org/W6774092362",
        "https://openalex.org/W6784461840",
        "https://openalex.org/W3173926131",
        "https://openalex.org/W6674914833",
        "https://openalex.org/W6754259177",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W6763650382",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W6729342207",
        "https://openalex.org/W6784736153",
        "https://openalex.org/W3163938617",
        "https://openalex.org/W2401231614",
        "https://openalex.org/W3196818043",
        "https://openalex.org/W4246999471",
        "https://openalex.org/W3015877095",
        "https://openalex.org/W3094848124",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963918968",
        "https://openalex.org/W3106959347",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4250163638",
        "https://openalex.org/W2526050071",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3096812425",
        "https://openalex.org/W4238614906",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W3197368257",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W4248805241",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W3095738426",
        "https://openalex.org/W3198786495",
        "https://openalex.org/W1598866093",
        "https://openalex.org/W3097641059",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W3000086239",
        "https://openalex.org/W2764103545",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3145157056",
        "https://openalex.org/W3097779478",
        "https://openalex.org/W4205185581",
        "https://openalex.org/W2964137095",
        "https://openalex.org/W3096609969",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2740956487",
        "https://openalex.org/W2614103613",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W3097109903",
        "https://openalex.org/W3099782249",
        "https://openalex.org/W4249914127",
        "https://openalex.org/W3094909210",
        "https://openalex.org/W2593831809",
        "https://openalex.org/W3006926732",
        "https://openalex.org/W2913668833",
        "https://openalex.org/W3097288651",
        "https://openalex.org/W3198116506",
        "https://openalex.org/W4365806309",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W3163044982",
        "https://openalex.org/W3198521429",
        "https://openalex.org/W4255556797",
        "https://openalex.org/W3196716035",
        "https://openalex.org/W2964051877",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W2963194800",
        "https://openalex.org/W4249564993",
        "https://openalex.org/W2279098554",
        "https://openalex.org/W2891147667",
        "https://openalex.org/W2787020095",
        "https://openalex.org/W3183859557",
        "https://openalex.org/W3094726021"
    ],
    "abstract": null,
    "full_text": "Detecting Dementia from Speech and Transcripts using\nTransformers\nLoukas Iliasa,∗, Dimitris Askounisa and John Psarrasa\naDecision Support Systems Laboratory, School of Electrical and Computer Engineering, National Technical University of\nAthens, Zografou, Athens, 15780, Greece\nARTICLE INFO\nKeywords:\nDementia\nSpeech\nlog-Mel spectrogram\nMel-frequency cepstral coeﬃcients\nVision Transformer\nGated Multimodal Unit\nCrossmodal Attention\nABSTRACT\nAlzheimer'sdisease(AD)constitutesaneurodegenerativediseasewithseriousconsequencesto\npeoples' everyday lives, if it is not diagnosed early since there is no available cure. Alzheimer's\nis the most common cause of dementia, which constitutes a general term for loss of memory.\nDue to the fact that dementia aﬀects speech, existing research initiatives focus on detecting\ndementiafromspontaneousspeech.However,littleworkhasbeendoneregardingtheconversion\nof speech data to Log-Mel spectrograms and Mel-frequency cepstral coeﬃcients (MFCCs) and\nthe usage of pretrained models. Concurrently, little work has been done in terms of both the\nusage of transformer networks and the way the two modalities, i.e., speech and transcripts, are\ncombinedinasingleneuralnetwork.Toaddresstheselimitations,ﬁrstwerepresentspeechsignal\nas an image and employ several pretrained models, with Vision Transformer (ViT) achieving\nthe highest evaluation results. Secondly, we propose multimodal models. More speciﬁcally,\nour introduced models include Gated Multimodal Unit in order to control the inﬂuence of\neach modality towards the ﬁnal classiﬁcation and crossmodal attention so as to capture in an\neﬀectivewaytherelationshipsbetweenthetwomodalities.Extensiveexperimentsconductedon\nthe ADReSS Challenge dataset demonstrate the eﬀectiveness of the proposed models and their\nsuperiority over state-of-the-art approaches.\n1. Introduction\nAlzheimer's disease (AD) constitutes a neurodegenerative disease, meaning that it becomes worse with time.\nMoreover, it is thought to begin 20 years or more before symptoms arise, with minor changes in the brain that are\nunnoticeabletothepersonaﬀected[4].Duetothefactthatthereisnoavailabletreatment,itisimportanttobediagnosed\nearlybeforeaﬀectingthepatient'severydaylifetoahighdegree.However,thediagnosisofAlzheimer'sdiseasethrough\nMagnetic Resonance Imaging (MRI), Positron emission tomography (PET) images, or electroencephalogram (EEG)\nsignals has been proven to be time and cost-ineﬀective. Alzheimer's is the most common cause of dementia, Thus,\nresearch works have moved their interest towards dementia identiﬁcation from spontaneous speech, since the speech\nof AD patients is aﬀected to a high degree.\nResearch works using audio data to categorize people into AD and non-AD patients use mainly acoustic features\nextractedfromspeech,suchaseGeMAPS[19],durationofspeechetc.Afterhavingextractedtherespectivefeaturesets,\nthey train traditional machine learning classiﬁers, such as Support Vector Machines (SVM), Decision Trees (DT) etc.\nHowever,featureextractionconstitutesatime-consumingprocedure,doesnotgeneralizewelltodatafromnewpatients,\nand often demands some level of domain expertise. Log-Mel Spectrograms and Mel-frequency cepstral coeﬃcients\n(MFCCs)arebeingusedextensivelyinheartsoundclassiﬁcation[33],emotionrecognition[47],depressiondetection\n[79],etc.Inaddition,pretrainedmodelsonthedomainofcomputervision,includingAlexNet,MnasNet,EﬃcientNet,\nVGG,etc.,havebeenexploitedextensivelyinmanytasks,includingAlzheimer'sdiseasedetectionthroughMRIs[17],\ndetection of epileptic seizures using EEG signals [55, 58], facial emotion recognition [25], analysis of online political\nadvertisements [60], heart sound classiﬁcation [33], voice pathology diagnosis [24], etc. Thus, the representation of\nspeech signal as an image constitutes a motivation for exploiting image-based models. However, limited research has\nconsideredspeechinsuchaway[46,80,37].Therefore,inthisstudyweconverteachaudioﬁleintoanimageconsisting\nof three channels, namely log-Mel spectrograms (and MFCCs), their delta, and delta-delta. Contrary to [80, 37], we\nuse the delta and delta-delta features for adding more information [36, 22]. Next, we employ many pretrained models,\n∗Corresponding author\nlilias@epu.ntua.gr (L. Ilias);askous@epu.ntua.gr (D. Askounis);john@epu.ntua.gr (J. Psarras)\nORCID(s):0000-0002-4483-4264 (L. Ilias);0000-0002-2618-5715 (D. Askounis)\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 1 of 20\narXiv:2110.14769v3  [cs.CL]  17 Jan 2023\nDetecting Dementia from Speech and Transcripts using Transformers\nincluding AlexNet, VGG16, DenseNet, EﬃcientNet, Vision Transformer, etc. and compare their performances. Our\nmain motivation is to ﬁnd the best model for extracting acoustic features and exploiting it in the multimodal setting.\nMoreover, another limitation of the current research works lies in the usage of multimodal models. To be more\nprecise, research works train ﬁrst acoustic and language models separately and then use the majority voting approach\nforclassifyingpeopleintoADandnon-ADpatients[14,62,69].Thisfactincreasessubstantiallythetrainingtimeand\ndoesnottakeintoaccounttheinter-andintra-modalinteractions.Otherresearchworksaddorconcatenateacousticand\nlanguagerepresentationsduringtraining[80,49,42].Thisapproachmaydecreasetheperformanceofthemultimodal\nmodels in comparison with the unimodal ones, since the diﬀerent modalities are treated equally. In addition, there are\nstudies,whichconcatenatethefeaturesfromdiﬀerentmodalitiesattheinputlevel(earlyfusionapproaches)[12,53,44].\nLittle workhas been done interms of exploiting techniques tocontrol the inﬂuence ofeach modality towards theﬁnal\nclassiﬁcation and capturing the inter- and intra-modal interactions. Speciﬁcally, the authors in [56, 57] used feed-\nforward highway layers with a gating mechanism. However, the authors did not experiment with replacing the gating\nmechanism with a simple concatenation operation. Thus, the addition of the introduced gating mechanism cannot\nguarantee increase in the performance. To tackle this limitation, we propose new methods, which can be trained in an\nend-to-endtrainableway,tocombinetherepresentationsofthediﬀerentmodalities.Firstly,weconverteachaudioﬁle\nintoanimageconsistingofthreechannels,namelylog-Melspectrograms(andMFCCs),theirdelta,anddelta-delta.We\npass these images through a Vision Transformer, which is the best performing model among the proposed pretrained\nmodels,i.e.,AlexNet,VGG16,DenseNet,EﬃcientNet,etc.EachtranscriptispassedthroughaBERTmodel.Next,we\nproposeaGatedMultimodalUnitinordertoassignmoreimportancetothemostrelevantmodalitywhilesuppressing\nirrelevant information. In addition, we introduce crossmodal attention so as to model crossmodal interactions.\nOur main contributions can be summarized as follows:\n• We propose multimodal deep learning models to detect AD patients from speech and transcripts. We also\nintroduce a multimodal gate mechanism, so as to control the inﬂuence of each modality towards the ﬁnal\nclassiﬁcation.\n• We introduce the crossmodal attention and show that crossmodal models outperform the multimodal ones.\n2. Related Work\nInthissection,wedescribethelandscapeofrelevantworksonthedetectionofdementiafromspontaneousspeech.\nThese works are divided into two approaches,(i) methods using only the acoustic modality (Section 2.1) and(ii)\nmultimodal methods using both the textual (transcripts) and the acoustic modality (Section 2.2). Finally, Section 2.3\nprovides an overview of the related work along with the limitations. Also, we discuss how our work diﬀers from the\nexisting research initiatives.\n2.1. Detecting Dementia only from Speech\nMeghananietal.[46]usedtheADReSSChallengeDatasetandproposedthreedeeplearningmodelstodetectAD\npatients using only speech data. Firstly, they converted the audio ﬁles into Log-Mel spectrograms and MFCCs along\nwiththeirdeltaanddelta-delta,inordertocreateanimageconsistingofthreechannels.Next,theydividedtheimages\nintonon-overlappingsegmentsof224framesandpassedeachframethroughﬁveconvolutionlayersfollowedbyLSTM\nlayers. In the second proposed model, they replaced the ﬁve convolution layers with a pretrained ResNet18 model.\nFinally,theytrainedamodelconsistingofBiLSTMsandCNNlayers.ResultsshowedthatLog-Melspectrogramsand\nMFCCs are eﬀective for the AD detection problem. One limitation of this study is that the authors employed only one\nimage-based pretrained model, i.e., ResNet18.\nGauder et al. [23] used the ADReSSo Challenge Dataset and extracted a set of features from speech, namely\neGeMAPS [19], trill [65], allosaurus [39], and wav2vec2 [5], where each feature vector was fed into two convolution\nlayers.Then,theoutputsoftheconvolutionlayerswereconcatenatedandwerepassedthroughaglobalaveragepooling\nlayer followed by a dense layer, in order to get the ﬁnal output. Results from an ablation study showed that trill and\nwav2vec2 constituted the best features. The main limitations of this study are the feature extraction process and the\nconcatenation of the feature representations.\nBalagopalan and Novikova [7] used the ADReSSo Challenge Dataset and introduced three approaches to\ndiﬀerentiate AD from non-AD patients by extracting 168 acoustic features from the speech audio ﬁles, computing\nthe embeddings of the audio ﬁles using wav2vec2, and ﬁnally combining the aforementioned approaches by simply\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 2 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nconcatenating the two representations. Results showed that a Support Vector Machine trained on the acoustic features\nyieldedthehighestprecision,whereastheSVMclassiﬁertrainedontheconcatenationoftheembeddingsachievedthe\nhighest accuracy, recall, and F1-score. The limitation of this study lies on the feature extraction process, the train of\ntraditional machine learning classiﬁers, and the usage of the concatenation operation, where the same importance is\nassigned to the features.\nRef. [13] used the ADReSS Challenge Dataset and introduced two approaches targeting at diagnosing dementia\nonly from speech. Firstly, after employing VGGish [29], they used the features extracted via VGGish and trained\nshallow machine learning algorithms to detect AD patients. Next, they proposed a convolutional neural network for\nspeech classiﬁcation, namely DemCNN, and claimed that DemCNN outperformed the other approaches. The main\nlimitation of this research work is the train of shallow machine learning classiﬁers using the VGGish features, which\nincrease the training time.\nThe authors in [2] proposed a feature extraction approach. Speciﬁcally, they extracted 54 acoustic features,\nincluding duration, intensity, shimmer, MFCCs, etc. Finally, they trained the LIBSVM with a radial basis kernel\nfunction.Thelimitationofthisstudyliesonthefeatureextractionprocessandthetrainofonlyonetraditionalmachine\nlearning classiﬁer. In addition, the authors have not applied feature selection or dimensionality reduction techniques.\nResearch works [1, 26] used the DementiaBank Dataset and exploited a set of acoustic features along with\nshallowmachinelearningclassiﬁers.Morespeciﬁcallyin[1],theauthorsextractedasetof121features,includingthe\nfundamental frequency, the frequency alteration from cycle to cycle, the F0 amplitude variability, features assessing\nthe voice quality, spectral features, etc. The authors expanded this feature set with some statistical sub-features, i.e.,\nmin, max, mean, etc. and thus increased the number of features to 811. After employing feature selection techniques,\nthe authors applied two classiﬁcation algorithms, namely SVM and Stochastic Gradient Descent for classifying\nsubjects into AD, non-AD patients, and Mild Cognitive Impairment (MCI) groups in a cross-visit framework. In [26],\nthe authors extracted a set of features, including the emobase, ComParE [20], eGeMAPS, and MRCG functionals\n[11] and performed three experiments, namely segment level classiﬁcation, majority vote classiﬁcation, and active\ndata representation. The authors exploited many classiﬁers, including Decision Trees, k-Nearest Neighbours, Linear\nDiscriminant Analysis (LDA), Random Forests, and Support Vector Machines. The limitations of these studies lie on\nthetediousprocedureoffeatureextraction,whichdemandsdomainexpertise.Also,bothstudiestrainshallowmachine\nlearning classiﬁers.\nBertinietal.[9]usedtheDementiaBankDatasetandemployedanautoencoderusedintheaudiodatadomaincalled\nauDeep [21] and passed the encoded representation (latent vector) to a multilayer perceptron, in order to detect AD\npatients.Resultsshowedsigniﬁcantimprovementsoverstate-of-the-artapproaches.Themainlimitationofthisstudyis\nthewaythespeechsignalisrepresentedasimage.Speciﬁcally,thespeechsignalisconvertedtoalog-Melspectrogram.\nOn the contrary, the addition of delta and delta-delta features as channels of the image adds more information, since\nthese features add dynamic information to the static cepstral features.\nReference[27]usedtheADReSSChallengeDatasetandexploitedi-vectorsandx-vectorsalongwithdimensional-\nity reduction techniques for training and testing the shallow machine learning algorithms, namely linear discriminant\nanalysis (LDA), the decision tree (DT) classiﬁer, the k-nearest neighbors classiﬁer, a support vector machine (SVM),\nand a Random Forest (RF) classiﬁer. The limitation of this study lies on the feature extraction process and the train of\ntraditional machine learning classiﬁers.\nTheauthorsin[37]introducedtheOpenVoiceBrainModel(OVBM),whichuses16biomarkers.Audioﬁleswere\nconverted into MFCCs. The ResNet has been used by eight biomarkers for feature representation. Finally, the authors\nhave applied Graph Neural Networks (GNNs) and have extracted a personalized subject saliency map. The limitation\nof this study lies on the way the speech signal is represented as an image. Speciﬁcally, the authors convert the speech\nsignal only to MFCCs. On the contrary, the addition of delta and delta-delta features as channels of the image adds\nmore information, since these features add dynamic information to the static cepstral features. In addition, the authors\ntrain multiple models increasing in this way both the training time and computational resources.\n2.2. Multimodal Architectures\nRef. [6] extracted three sets of features, namely lexicosyntactic, acoustic, and semantic features. In terms of the\nlexicosyntactic features, the authors extracted the proportion of POS-tags, average sentiment valence of all words in\na transcript, and many more. Regarding the acoustic features, MFCCs, fundamental frequency, statistics related to\nzero-crossing rate, etc. were exploited. With regards to the semantic features, the authors extracted proportions of\nvarious information content units used in the picture. Next, they performed feature selection by using the ANOVA\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 3 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nand trained four machine learning classiﬁers, including SVM, neural network, RF, and NB. Results showed that SVM\noutperformed the other approaches in the multimodal framework. The limitation of this study lies on the way the\nfeatures from diﬀerent modalities are combined. More speciﬁcally, the authors apply an early fusion strategy, where\nthey fuse the features at the input level. This approach does not capture the inter- and intra-modal interactions. In\naddition, another limitation is the feature extraction procedure.\nThe authors in [10] introduced three speech-based systems and two text-based systems for diagnosing dementia\nfromspontaneousspeech.Also,theyproposedmethodsforfusingthediﬀerentmodalities.Intermsofthespeechbased\nsystems,theauthorsextractedi-vectors,x-vectors,andrhythmicfeaturesandtrainedanSVMandaLinearDiscriminant\nAnalysis(LDA)classiﬁer.Regardingthetext-basedmodels,theauthorsﬁne-tunedaBERTmodelandtrainedanSVM\nclassiﬁer using linguistic features. Finally, the authors exploited three fusion strategies based on late fusion approach.\nTherefore, the main limitation of this study is the late fusion approach for fusing the diﬀerent modalities.\nIn [54], an early fusion approach was proposed. Speciﬁcally, the authors extracted a set of acoustic features, i.e.,\narticulation, prosody, i-vectors, and x-vectors, and a set of linguistic features, including word2vec, BERT, and BERT-\nBase trained with the Spanish Unannotated Corpora (BETO) embeddings. The authors concatenated these sets of\nfeatures and trained a Radial Basis Function-Support Vector Machine. The main limitation of this paper is the early-\nfusion approach.\nResearch works [12] extracted a set of acoustic and linguistic features using the ADReSSo Challenge Dataset.\nNext, they concatenated these sets of features and trained a Logistic Regression classiﬁer. They also proposed three\nlabelfusionapproaches,namelymajorityvoting,averagefusion,andweightedaveragefusion,basedonthepredictions\nof several neural networks. The limitations of this study are related to the early and late fusion strategies introduced\nfor detecting AD patients.\nRef. [14] used the ADReSS Challenge Dataset and introduced neural network architectures which use language\nandacousticfeatures.Regardingthemultimodalapproach,theauthorsfusethepredictionsofthethreebestperforming\nmodels using a majority vote approach and show that label fusion outperforms the neural networks using either only\nspeechortranscripts.Thelimitationofthisstudyliesontheusageofalatefusionstrategy,i.e.,majorityvoteapproach.\nInthisway,multiplemodelsmustbetrainedseparatelyincreasingthetrainingtime.Also,theinter-modalinteractions\ncannot be captured.\nEdwards et al. [18] used the ADReSS Challenge Dataset and exploited both acoustic and linguistic features and\ncombined them in order to detect AD patients. In terms of acoustic features, they applied feature selection techniques\nincluding correlation feature selection (CFS) and recursive feature elimination with cross-validation (RFECV), in\norder to reduce the dimensionality of the respective feature set. Results of the feature selection techniques indicated\na surge in the classiﬁcation performance. The authors chose the ComParE2016 features as the best feature set for\nfurtherexperiments.Regardinglinguisticcharacteristics,theyextractedfeaturesfromtranscripts,employedpretrained\nembeddings (Word2Vec, GloVe, Sent2Vec) or models (ELECTRA, RoBERTa), and exploited phoneme written\npronunciation using CMUDict [75]. The authors stated that the fusion of phonemes and audio features achieved the\nhighest classiﬁcation results. This study comes with some limitations. Speciﬁcally, the authors introduce a feature\nextraction process and fuse the features of diﬀerent modalities via an early fusion strategy.\nPompili et al. [53] used the ADReSS Challenge Dataset and extracted also acoustic and linguistic features from\nspeech. Results showed that a simple merge of these feature sets via an early fusion approach did not improve the\nclassiﬁcation results, since the predictive ability of linguistic features completely overrides acoustic ones.\nAuthors in [62] used the ADReSS Challenge Dataset and introduced three deep learning architectures for\ncategorizing people into AD patients and non-AD ones, as well as predicting the Mini-Mental State Exam (MMSE)\nscores, using disﬂuency, acoustic, and intervention features. Regarding the multimodal approach, they followed three\nsimilar approaches, namely hard, soft, and learnt ensemble. With regards to the hard ensemble, a majority vote was\ntakenbetweenthepredictionsofthethreeindividualmodels.Regardingthesoftensemble,aweightedsumoftheclass\nprobabilities was computed for the ﬁnal decision. Finally, in terms of the learnt ensemble, a logistic regression voter\nwas trained using class probabilities as inputs. Thus, the limitation of this study is pertinent to the late fusion strategy,\nwhere multiple models must be trained and tested separately.\nRef. [50] used the ADReSSo Challenge Dataset and proposed a similar approach. After training acoustic and\nlanguage models, the authors explored model fusion by using the output scores of the models as the inputs for a\nLogistic Regression (LR) classiﬁer to obtain the ﬁnal prediction.\nMartinc and Pollak [44] used the ADReSS Challenge Dataset and extracted a set of audio, tf-idf, readability, and\nembedding features and trained traditional machine learning algorithms, including XGBoost, Random Forest (RF),\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 4 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nSVM, and Logistic Regression, with Logistic Regression achieving the highest accuracy on the test set. The authors\nalso stated that readability features and the duration of the audio proved to be eﬀective features. The main limitations\nof this study are: (i) the feature extraction procedure, i.e., demands domain expertise, the best features may not be\nextracted, (ii) early fusion strategy, and (iii) train of traditional machine learning classiﬁers.\nKoo et al. [34] used the ADReSS Challenge Dataset and proposed a deep learning model consisting of BiLSTMs,\nCNNs,andself-attentionmechanismandexploitedbothtextual,i.e.,transformer-basedmodels,psycholinguistic,repet-\nitiveness, and lexical complexity features, and acoustic features, i.e., openSMILE and VGGish features. Speciﬁcally,\nthey passed each modality through a self attention layer, where key, value, and query corresponded to one single\nmodality. However, the authors concatenated the outputs of the attention layer, which correspond to the two diﬀerent\nmodalities, and passed them through a CNN layer. The main limitations of this study are pertinent to the feature\nextraction process and the concatenation of the representation vectors of the two modalities into one vector.\nAuthors in [51] used the ADReSS Challenge Dataset and introduced both acoustic and transcript-based models,\nwhiletheyproposedalsoamethodtofusethetwomodels.Morespeciﬁcally,theyﬁrstobtainedthescoresfromacoustic\nand transcript-based models for all utterances from the testing folds during the cross-validation stage and then they\nemployed these predictions in a cross-validation scheme to train and test fusion of scores using a GBR model. The\nusage of a late fusion strategy constitutes the limitation of this study.\nPan et al. [49] used the ADReSSo Challenge Dataset and introduced also a multimodal approach, where they\nexploited wav2vec for extracting acoustic embeddings from the audio ﬁles and employed also BERT for extracting\nembeddingsfortranscripts.Finally,theyconcatenatedthesetworepresentationsandtrainedthemodelfordetectingAD\npatients.Theconcatenationoftherepresentationvectorsofthetwodiﬀerentmodalitiesconstitutesthemainlimitation\nof this study, since the concatenation operation does not capture the inherent correlations between the two modalities.\nZhu et al. [80] proposed both unimodal and multimodal approaches. Regarding unimodal models, they employed\nﬁrst MobileNet and YamnNet to discriminate between AD patients and non-AD ones. They converted audio ﬁles into\nMFCCfeatures,duplicatedtheMFCCfeaturemaptwiceandmadetheMFCCfeaturemapasa(p,t,3)-matrix,inorder\nto match with the module input of the proposed architectures. They used also BERT and Speech BERT. In terms of\nthe multimodal models, the authors exploited Speech BERT, YamnNet, Longformer, and BERT. After extracting the\nrepresentations of audio and transcripts, they used the add and concatenation operation to fuse these two modalities.\nResults on the ADReSS Challenge Dataset showed that the concatenation operation of the representations extracted\nvia BERT and Speech BERT outperformed the unimodal models. The limitations of this study are the following: (i)\nthe way the speech signal is represented as an image. More speciﬁcally, this study duplicates the MFCC feature map\ntwice and makes the MFCC feature map as a (p, t, 3)-matrix. On the contrary, the delta and delta-delta features can\nbe used for adding more information [36, 22]. (ii) In terms of the multimodal models, the authors fuse the diﬀerent\nmodalities via an add and concatenation operation. These methods do not capture the inherent correlations between\nthe two modalities.\nResearch work [42] employed also a bi-modal model consisting of Dense, GRU, CNN, BiLSTM, and attention\nlayers. The authors fused the two modalities by concatenating their respective representations. Results on the\nADReSS Challenge Dataset showed an improvement of evaluation results of the multimodal approach over unimodal\narchitectures. The usage of the concatenation operation for fusing the two modalities constitutes a limitation of this\nstudy. Also, the feature extraction process proposed by the authors, constitutes another limitation.\nResearch work [63] used the ADReSS Challenge Dataset and extracted a set of acoustic and linguistic features\nto train and test algorithms on the AD classiﬁcation and MMSE prediction task. Regarding the fusion of the two\nmodalities, the authors chose the three best-performing acoustic models along with the best-performing language\nmodel, and computed a ﬁnal prediction by taking a linear weighted combination of the individual model predictions.\nThemethodforfusingthetwomodalities,i.e.,latefusionstrategy,constitutesthemainlimitationofthisstudy.Another\nlimitation is the tedious procedure of feature extraction.\nIn [69], the authors applied majority voting of the acoustic and language models in terms of the multimodal\napproach. Regarding the acoustic models, they extracted handcrafted features as well as embeddings via deep neural\nnetworks, including VGGish, YAMNet, openl3: Music, and openl3: Environment sounds. In terms of the language\nmodels, apart from the handcrafted features, embeddings from BERT base (cased and uncased), BERT large (cased\nanduncased),RoBERTa(baseandﬁne-tunedonbiomedicaldata),anddistilledversionsofBERTandRoBERTa,have\nbeenexploited.Afterapplyingfeatureaggregationtechniques,theauthorstrainedshallowmachinelearningclassiﬁers\nfortheADclassiﬁcationandMMSEregressiontask.Themethodforfusingthetwomodalities,i.e.,latefusionstrategy,\nconstitutes the main limitation of this study.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 5 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nThe authors in [48] proposed a late fusion strategy combining both acoustic and text-based models to detect\nAD patients. Speciﬁcally, their text-based model consists of CNNs, BERT, and SentenceBERT, while the proposed\nacoustic-basedmodelconsistsofaVGGishmodel.Finally,theauthorscombinedthetwoprobabilitiesobtainedbythe\ntwo aforementioned models using a late fusion strategy and obtained an accuracy of 85.30%. The method for fusing\nthe two modalities, i.e., late fusion strategy, constitutes the main limitation of this study.\nIn [43], the authors introduced an approach, which accounts for temporal aspects of both linguistic and acoustic\nfeatures. In terms of the acoustic features, the authors exploited the eGeMAPS feature set, while they used GloVE\nembeddingswithregardstothelanguagefeatures.Next,theActiveDataRepresentation[26]withsomemodiﬁcations\nwasemployed.TheauthorsusedaRandomForestClassiﬁerforperformingtheirexperiments.Theauthorsperformed\na series of experiments and stated that the majority vote approach yielded the best result. The method for fusing the\ntwo modalities, i.e., late fusion strategy, constitutes the main limitation of this study.\nOn the other hand, Rohanian et al. [56] extracted a set of acoustic and linguistic features and employed a neural\nnetwork architecture consisting of two branches of BiLSTMs, one branch for each modality. In order to control the\ninﬂuence of the two modalities, they introduced feed-forward highway layers [67] with gating units. Results showed\nthat the multimodal approach yielded better results than the unimodal ones. Research work [57] proposed a similar\napproach where the authors employed also BERT except for LSTMs to extract embeddings from linguistic features.\nHowever, theauthors didnot experiment withreplacing the gatingmechanism witha simple concatenationoperation.\nThus,theadditionoftheintroducedgatingmechanismcannotguaranteeincreaseintheperformance.Ourworkdiﬀers\nfrom[56,57],sinceweuseBERTandVisionTransformertoextractfeaturesfromtranscriptsandimagerepresentations\nof speech respectively. Also, we propose a diﬀerent multimodal gating mechanism.\n2.3. Related Work Review Findings\nIntermsofthearchitecturesusingonlyspeechdata,itisevidentthatcurrentresearchworks[2,23,7,13,1,26,27]\nhave been focused mainly on acoustic feature extraction and then the usage of shallow machine learning algorithms,\ni.e.,SVM,LR,RFetc.,orCNNsandBiLSTMs.Thestudyin[46],whichhasconvertedaudioﬁlesintoimagesofthree\nchannels, namely log-Mel spectrograms (and MFCCs), their delta, and delta-delta, has exploited only one pretrained\nmodel of the domain of computer vision, i.e., ResNet18. In addition, the study introduced in [80] has converted the\naudioﬁlesintoMFCCfeatures,hasduplicatedtheMFCCfeaturemaptwiceandhasmadetheMFCCfeaturemapasa\n(p,t,3) -matrix.Next,thisstudyhasemployedYAMNet,MobileNet,andSpeechBERT.However,thelimitationofthis\nstudy lies on the way images are created. On the contrary, delta and delta-delta coeﬃcients are used for recognizing\nspeech better, since the dynamics of the power spectrum, i.e., trajectories of MFCCs over time, are understood better.\nRegarding the multimodal models, the majority of the research works have either concatenated or added the\nrepresentations corresponding to the two diﬀerent modalities [80, 49, 42, 34]. However, the concatenation operation\nassignsequalimportancetoeachmodalityanditneglectstheinter-andintra-modalinteractions.Otherresearchworks\nhave trained several language and acoustic models separately and then use majority voting for the ﬁnal classiﬁcation\nof the people as AD patients or non-AD patients [14, 62, 69, 43, 12]. Late fusion approaches have been also proposed\nincluding[10,50,51,63,48,68,12].However,theseapproachesincreasesubstantiallythecomputationtime,whilethe\ninter-modal interactions are not captured. In addition, there are studies [54, 6, 12, 53, 44, 18] proposing early fusion\napproaches, meaning that the features corresponding to the diﬀerent modalities are concatenated at the input level.\nNone of these works capture the inter- and intra-modal interactions.\nTherefore, our work diﬀers signiﬁcantly from the aforementioned research works, since we(i) exploit several\npretrained models, including the Vision Transformer, on the vision domain and compare their performances,(ii)\nintroduce multimodal models consisting of BERT and Vision Transformer, which can be trained in an end-to-end\ntrainableway,(iii)introducemultimodalmodelswithagatingmechanismsoastocontroltheinﬂuenceofeachmodality\ntowards the ﬁnal classiﬁcation, and(iv) introduce the cross-attention mechanism to learn crossmodal interaction and\nshow that crossmodal interaction outperforms the competitive multimodal methods.\n3. Dataset\nTheADReSSChallengedataset[40]hasbeenusedforconductingourexperiments.Thisdatasetconsistsof78AD\npatients and 78 non-AD patients. Each participant (PAR) has been assigned by the interviewer (INV) to describe the\nCookieTheftpicturefromtheBostonDiagnosticAphasiaExam[8].Thisdatasetismatchedforgender,soastomitigate\nbias in the prediction task. Also, it has been carefully selected so as to mitigate common biases often overlooked in\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 6 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nevaluationsofADdetectionmethods,includingrepeatedoccurrencesofspeechfromthesameparticipant(commonin\nlongitudinal datasets), variations in audio quality, and imbalances of age distribution. Moreover, this dataset has been\nsplit into a train and test set. Speciﬁcally, the train set consists of 108 people, 54 AD and 54 non-AD patients, while\nthe test set comprises 48 people, 24 of whom are AD patients and 24 are non-AD ones.\n4. Proposed Predictive Models using only Speech\nIn this section, we describe the models used for detecting AD patients using only speech. Our main motivation of\nexploitingthesepretrainedmodelsistoﬁndthebestperformingoneandexploititinthemultimodalsetting,whichwill\nbediscussedindetailinSection5.Firstly,weusethePythonlibrarylibrosa[45]forconvertingtheaudioﬁlesintoLog-\nMel spectrograms (and MFCCs), their delta, and delta-delta. We extract Log-Mel spectrograms with 224 Mel bands,\nwindow length equal to 2048, hop length equal to 1024, and a Hanning window. For extracting MFCCs, we use 40\nMFCCs,aHanningwindow,windowlengthequalto2048,andahoplengthof512.Weemploythefollowingpretrained\nmodels:GoogLeNet(Inceptionv1) [70],ResNet50[28],WideResNet-50-2[78],AlexNet[35],SqueezeNet1_0[31],\nDenseNet-201[30],MobileNetV2[61],MnasNet1_0[71],ResNeXt-5032×4d[77],VGG16[66],EﬃcientNet-B2 1\n[72], andVision Transformer[16].\nFor all the models, we add a classiﬁcation layer with two units at the top of the models. Regarding the Vision\nTransformer, the output of the Vision Transformer (zL\n0) serving as the image representation is passed through a dense\nlayer with two units in order to get the ﬁnal output.\n4.1. Experiments\nAll experiments are conducted on a single Tesla P100-PCIE-16GB GPU.\nExperimental Setup Firstly,wedividethetrainsetprovidedbytheChallengeintoatrainandavalidationset(65-35%).\nAllmodelshavebeentrainedwithanAdamoptimizerandalearningrateof1e-5.Wetraintheproposedarchitectures\nﬁvetimes.Weapply ReduceLROnPlateau,wherewereducethelearningratebyafactorof0.1,ifthevalidationlosshas\nstoppeddecreasingforthreeconsecutiveepochs.Also,weapply EarlyStoppingandstoptrainingifthevalidationloss\nhasstoppeddecreasingforsixconsecutiveepochs.Weminimizethecross-entropylossfunction.Wetesttheproposed\nmodels using the ADReSS Challenge test set. We average the results obtained by the ﬁve repetitions. All models have\nbeen created using the PyTorch library [52]. We have used the Transformers library [76] for exploiting the Vision\nTransformer2,3.\nEvaluation Metrics Accuracy, Precision, Recall, F1-Score, and Speciﬁcity have been used for evaluating the results\noftheintroducedarchitectures.Thesemetricshavebeencomputedbyregardingthedementiaclassasthepositiveone.\n4.2. Results\nTheresultsoftheproposedmodelsmentionedinSection4,whichreceiveasinputeitherlog-MelSpectrogramsor\nMFCCs, are reported in Table 1.\nIn terms of the proposed models with log-Mel Spectrograms as input, as one can easily observe, the Vision\nTransformer constitutes our best performing model outperforming the other pretrained models in terms of all the\nevaluationmetricsexceptspeciﬁcity.Tobemoreprecise,VisionTransformersurpassestheothermodelsinaccuracyby\n2.08-14.58%,inprecisionby1.64-11.22%,inrecallby5.00-39.17%,andinF1-scoreby2.85-21.85%.Thesecondbest\nperforming model is the AlexNet achieving accuracy and F1-score equal to 62.92% and 66.91% respectively. VGG16\nconstitutesthethirdbestmodelachievingF1-scoreandAccuracyequalto65.55%and61.25%respectively.Theother\npretrained models achieve almost equal accuracy results ranging from 53.33% to 59.16% except for DenseNet-201,\nwhich performs very poorly with the accuracy accounting for 50.42%.\nIn terms of the proposed models with MFCCs as input, we observe that the Vision Transformer constitutes the\nbest performing model attaining an Accuracy score of 63.33% and an F1-score of 60.30%. Speciﬁcally, it surpasses\ntheothermodelsinAccuracyby0.41-9.17%,inF1-scoreby0.10-6.24%,andinPrecisionby0.13-12.85%.AlexNetis\nthe second best performing model achieving an Accuracy of 62.92%, while it surpasses the other models in Accuracy\n1We experimented with EﬃcientNet-B0 to B7, but EﬃcientNet-B2 was the best performing model.\n2google/vit-base-patch16-224-in21k\n3We also use the ViTFeatureExtractor.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 7 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nby 2.93-8.76%. MnasNet1_0, GoogleNet, and VGG16 achieve almost equal accuracy scores ranging from 59.17% to\n59.99% with the MnasNet1_0 achieving the highest Accuracy score. Next, SqueezeNet1_0 and DenseNet-201 yield\nequalaccuracyscoresaccountingfor58.75%,withSqueezeNet1_0outperformingDenseNet-201inF1-scoreby0.72%.\nMobileNetV2achievesanAccuracyscoreof57.92%followedbyEﬃcientNet-B2,whoseaccuracyaccountsfor57.08.\nEﬃcientNet-B2 yields the highest Recall equal to 65.00%, surpassing the other models by 5.00-10.84%. ResNeXt-50\n32×4dachieves the worst accuracy score accounting for 54.16%.\nIn both cases, i.e., log-Mel spectrograms and MFCCs, we observe that Vision Transformer constitutes our best\nperforming model. This can be justiﬁed by the fact that all the other pretrained models are based on Convolutional\nNeuralNetworks(CNNs).Onthecontrary,theVisionTransformerdoesnotimplyanyconvolutionlayer.Speciﬁcally,\nthe image is split in patches and is fed to the Vision Transformer network, which exploits the concept of the self-\nattention mechanism introduced in [74]. Therefore, we believe that the diﬀerence in performance is attributable to the\ntransformer encoder, which consists of multi-head self-attention and is implemented in the Vision Transformer.\nTable 1: Performance comparison among proposed models (using only\nspeech) on the ADReSS Challenge test set. Reported values are mean±\nstandarddeviation.Resultsareaveragedacrossﬁveruns.Bestresultsper\nevaluation metric and method are in bold.\nEvaluation metrics\nArchitecture Precision Recall F1-score Accuracy Speciﬁcity\nlog-Mel Spectrogram\nGoogLeNet (Inception v1) 57.01 70.00 60.92 57.08 44.17\n±4.70 ±19.08 ±7.43 ±4.86 ±24.80\nResNet50 58.93 41.66 47.91 55.00 68.33\n±9.31 ±6.97 ±3.61 ±4.86 ±14.58\nWideResNet-50-2 52.99 64.16 57.70 53.75 43.33\n±1.95 ±10.74 ±5.39 ±2.43 ±8.58\nAlexNet 60.07 75.83 66.91 62.92 50.00\n±2.60 ±9.28 ±5.35 ±4.04 ±2.64\nSqueezeNet1_0 57.13 74.16 64.52 59.16 44.16\n±2.61 ±1.66 ±2.18 ±3.12 ±4.99\nDenseNet-201 50.49 70.00 58.46 50.42 30.83\n±3.58 ±7.64 ±3.81 ±4.82 ±10.74\nMobileNetV2 54.92 73.33 62.69 56.66 40.00\n±1.51 ±7.73 ±3.70 ±2.43 ±4.25\nMnasNet1_0 56.66 70.00 59.84 55.83 41.66\n±6.08 ±22.88 ±7.42 ±4.45 ±28.99\nResNeXt-50 32×4d 53.69 64.16 58.09 53.75 43.33\n±3.99 ±6.24 ±2.06 ±4.45 ±13.59\nVGG16 58.89 74.16 65.55 61.25 48.33\n±1.18 ±6.66 ±3.27 ±2.12 ±3.33\nEﬃcientNet-B2 54.16 58.33 55.46 53.33 48.33\n±6.44 ±7.91 ±3.48 ±5.53 ±15.72\nVision Transformer (ViT) 61.71 80.83 69.76 65.00 49.16\n±2.93 ±6.24 ±1.61 ±2.76 ±10.34\nMFCCs\nGoogLeNet (Inception v1) 60.77 55.00 57.49 59.58 64.17\n±3.84 ±6.66 ±4.19 ±3.39 ±6.77\nResNet50 56.38 59.16 57.30 56.25 53.33\n±3.83 ±8.50 ±3.45 ±3.49 ±12.47\nWideResNet-50-2 55.87 54.16 54.06 55.00 55.83\n±3.86 ±11.79 ±4.51 ±2.12 ±14.34\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 8 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nAlexNet 65.88 55.00 59.53 62.92 70.83\n±5.94 ±7.64 ±5.13 ±4.04 ±8.33\nSqueezeNet1_0 58.82 59.16 58.55 58.75 58.33\n±2.89 ±9.65 ±5.13 ±2.76 ±8.33\nDenseNet-201 59.40 56.66 57.83 58.75 60.83\n±3.31 ±4.25 ±2.22 ±2.43 ±6.77\nMobileNetV2 57.76 57.50 57.22 57.92 58.33\n±2.44 ±11.61 ±6.38 ±3.58 ±5.89\nMnasNet1_0 63.42 56.66 57.63 59.99 63.33\n±10.27 ±14.81 ±9.56 ±6.77 ±17.95\nResNeXt-50 32×4d 53.16 60.00 55.88 54.16 48.33\n±3.19 ±14.09 ±8.32 ±3.73 ±6.77\nVGG16 59.20 60.00 59.49 59.17 58.33\n±2.75 ±3.33 ±1.61 ±2.12 ±5.89\nEﬃcientNet-B2 56.40 65.00 60.20 57.08 49.16\n±5.89 ±7.26 ±5.51 ±5.98 ±10.34\nVision Transformer (ViT) 66.01 55.83 60.30 63.33 70.83\n±3.36 ±4.25 ±1.89 ±1.66 ±5.89\n5. Proposed Predictive Models using Speech and Transcripts\nInthissection,wedescribethemodelsusedfordetectingADpatientsusingtranscriptsalongwiththeiraudioﬁles.\nWehaveexploitedthepythonlibraryPyLangAcq[38]forhavingaccesstothemanualtranscripts,sincethedatasethas\nbeencreatedusingtheCHAT[41]codingsystem.Forprocessingtheaudioﬁles,weusethesameprocedurementioned\nin Section 4. We mention below the proposed models used in our experiments.\nBERT + ViTInthismodelwepasseachtranscriptionthroughapretrainedBERTmodel[74,15]andgettheoutputof\nthe BERT model (CLS token). Regarding the audio ﬁles, we convert them into Log-Mel spectrograms (and MFCCs),\ntheir delta, and delta-delta for constructing an image consisting of three channels and pass the image through the ViT.\nWe exploit the Vision Transformer, since it constitutes the best performing model as discussed in Section 4.2. The\noutput of the ViT (zL\n0) is concatenated with the output of the BERT and then the resulting vector is passed through a\ndense layer with 512 units and a ReLU activation function followed by a dense layer consisting of two units to get the\nﬁnal output. The proposed model is illustrated in Fig. 1.\nBERT + ViT + Gated Multimodal UnitInthismodelwepasseachtranscriptionthroughapretrainedBERTmodeland\ngettheoutputoftheBERTmodel(CLStoken).Regardingtheaudioﬁles,weconvertthemintoLog-Melspectrograms\n(and MFCCs), their delta, and delta-delta for constructing an image consisting of three channels and pass the image\nthrough the ViT. We exploit the Vision Transformer, since it constitutes the best performing model as discussed in\nSection 4.2. We get the output of the ViT (zL\n0). Next, we employ the Gated Multimodal Unit (GMU) introduced by\n[3], in order to control the contribution of each modality towards the ﬁnal classiﬁcation. The equations governing the\nGMU are described below:\nℎt =tanh( Wtft +bt) (1)\nℎv =tanh( Wvfv +bv) (2)\nz=\u001b(Wz[ft;fv]+ bz) (3)\nℎ=z∗ℎt +(1− z)∗ ℎv (4)\nΘ={ Wt,W v,W z} (5)\nwhere ft and fv denote the text and image representations respectively,Θthe parameters to be learned, and [.;.]\nthe concatenation operation. Speciﬁcally,Wt ∈128,W v ∈128,W z ∈128.\nThe outputh of the gated multimodal unit is passed through a dense layer consisting of two units.\nThe proposed model is illustrated in Fig. 2.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 9 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nFigure 1:BERT + ViT\nFigure 2:BERT + ViT + Gated Multimodal Unit\nBERT + ViT + Crossmodal AttentionSimilar to the previous models, we pass each transcription through a BERT\nmodel,andeachimagethroughaViTmodel.WeexploittheVisionTransformer,sinceitconstitutesthebestperforming\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 10 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nmodel as discussed in Section 4.2. The image representation can be denoted asX\u000b ∈ ℝB,T\u000b,d\u000b, while the text\nrepresentation can be represented asX\f ∈ ℝB,T\f,d\u000b, whereB constitutes the batch size,T(.) the sequence length,\nand d\u000b the feature dimension. Next, we employ the crossmodal attention [73, 59, 64]. Speciﬁcally, we employ two\ncrossmodal attentions, one from text to image representations and another one from image to text representations.\nFormally, the crossmodal attention from text to image representation is given by the equations below.\nSpeciﬁcally, we deﬁne the queries, keys, and values as:\nQ\u000b =X\u000bWQ\u000b,K\f =X\fWK\f,V\f =X\fWV\f (6)\n, whereWQ\u000b ∈d\u000b×dk,WK\f ∈d\u000b×dk, andWV\f ∈d\u000b×dv are learnable parameters.\nTherefore,\nQ\u000b ∈B×T\u000b×dk,K\f ∈B×T\f×dk,V\f ∈B×T\f×dv (7)\nThe latent adaptation from\fto\u000bis presented as the crossmodal attention, given by the equations below:\nY\u000b =CM\f→\u000b(X\u000b,X\f)\n=softmax\nHQ\u000bKT\n\f\n√\ndk\nI\nV\f\n=softmax\n⎛\n⎜\n⎜⎝\nX\u000bWQ\u000bWT\nK\f\nXT\n\f\n√\ndk\n⎞\n⎟\n⎟⎠\nX\fWV\f\n(8)\nThe scaled (by\n√\ndk) softmax is a score matrix, where the(i,j)-th entry measures the attention given by thei-th\ntimestepofmodality \u000btothe j-thtimestepofmodality \f.The i-thtimestepof Y\u000b isaweightedsummaryof V\f,with\nthe weight determined byi-th row in softmax(·).\nSimilarly, the crossmodal attention from image to text representation is given by the equations below:\nQ\f =X\fWQ\f,K\u000b =X\u000bWK\u000b,V\u000b =X\u000bWV\u000b (9)\nQ\f ∈B×T\f×dk,K\u000b ∈B×T\u000b×dk,V\u000b ∈B×T\u000b×dv (10)\nY\f =CM\u000b→\f(X\f,X\u000b)\n=softmax\nH\nQ\fKT\n\u000b\n√\ndk\nI\nV\u000b\n=softmax\nHX\fWQ\fWT\nK\u000b\nXT\n\u000b\n√\ndk\nI\nX\u000bWV\u000b\n(11)\nTheoutputsofthecrossmodalattentionlayers,i.e., Y\u000b andY\f,areconcatenatedandpassedthroughaglobalaverage\npooling layer followed by a dense layer with two units. The proposed model is illustrated in Fig. 3.\n5.1. Experiments\nAll experiments are conducted on a single Tesla P100-PCIE-16GB GPU.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 11 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nFigure 3:BERT + ViT + Crossmodal Attention\nComparison with state-of-the-art approaches\n1. Unimodal state-of-the-art approaches (only transcripts)\n• BERT [32]: This method trains a BERT model using transcripts.\n2. Multimodal state-of-the-art approaches (speech and transcripts)\n• top-3latefusion[43]:Thismethodproposesalatefusionapproachofthethreebestfeatureconﬁgurations,\nnamelyTemporal +char4grams,New +char4grams,andchar4grams. Theauthorstrain aRandomForest\nClassiﬁer.\n• Audio + Text (Fusion) [69]: The authors introduce three models for detecting AD patients using only\nspeech data and three models for detecting AD patients using only text data. Finally, they use a majority\nlevel approach, where the ﬁnal prediction corresponds to the class getting the most votes from the six\naforementioned models.\n• SVM[6]:Thismethodextractslexicosyntactic,semantic,andacousticfeatures,performsfeatureselection\nusing ANOVA, and ﬁnally trains a Support Vector Machine Classiﬁer.\n• Fusion Maj. (3-best) [14]: This method uses a majority vote of three approaches, namely Bag-of-Audio-\nWords, zero-frequency ﬁltered (ZFF) signals, and BiLSTM-Attention network.\n• LSTM with Gating (Acoustic + Lexical + Dis) [56]: This research work extracts a set of features from\nspeechandtranscripts,passestherespectivesetsoffeaturesthroughtwobranchesofBiLSTMs,onebranch\nfor each modality. Next the authors introduce feed-forward highway layers with a gating mechanism.\n• System 3: Phonemes and Audio [18]: This method transcribes the segment text into phoneme written\npronunciationusingCMUDictandcombinesthisrepresentationoffeatureswithfeaturesextractedviathe\naudio.\n• Fusion of system [53]: This method merges features extracted via speech and transcripts and trains a\nSupport Vector Machine Classiﬁer. Features of speech constitute the x-vectors. In terms of the language\nfeatures, (i) a Global Maximum pooling, (ii) a bidirectional LSTM-RNNs provided with an attention\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 12 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nmodule, and (iii) the second model augmented with part-of-speech (POS) embeddings are trained on the\ntop of a pretrained BERT model.\n• Bimodal Network (Ensembled Output) [34]: In this research work, the outputs of the top 5 bimodal\nnetworks with high validation results are ensembled and used as the ﬁnal submission.\n• GFI, NUW, Duration, Character 4-grams, Suﬃxes, POS tag, UD [44]: This method exploits the gunning\nfog index, number of unique words, duration of the audio ﬁle, character 4-grams, suﬃxes, pos-tags, and\nUniversal dependency features in a tf-idf setting. Logistic Regression is trained with the corresponding\nfeature sets.\n• Acoustic&Transcript[51]:Thismethodemploysthescoresfromthewholetrainingsubsettotrainaﬁnal\nfusion GBR model that is used to perform the fusion of scores coming from the acoustic and transcript-\nbased models for the challenge evaluation.\n• Dual BERT [80]: This method employs a Speech BERT and a Text BERT and concatenates their\nrepresentations.\n• Model C [42]: This method extracts features from segmented audio and passes them through GRU layers.\nRegardingthetranscripts,thismethodextractspos-tagsandpassesboththetranscriptsandpos-tagsthrough\ntwoseparateCNNlayers.ThentheoutputsoftheCNNlayersarepassedthroughaBiLSTMlayercoupled\nwithanAttentionLayer.Theauthorsalsoextractadiﬀerentsetoffeaturesfrombothtranscriptsandaudio\nﬁles and pass them to a dense layer. The respective outputs are concatenated and passed to a dense layer,\nwhich gives the ﬁnal output.\n• Majority vote (NLP + Acoustic) [63]: This method obtains ﬁrstly the best-performing acoustic and\nlanguage-based models. Next, it computes a weighted majority-vote ensemble meta-algorithm for clas-\nsiﬁcation. The authors choose the three best-performing acoustic models along with the best-performing\nlanguage model, and compute a ﬁnal prediction by taking a linear weighted combination of the individual\nmodel predictions.\nExperimental Setup Firstly, we divide the train set provided by the Challenge into a train and a validation set (65-\n35%). Next, we train the proposed architectures ﬁve times with an Adam optimizer and a learning rate of 1e-5. We\napply ReduceLROnPlateau, where we reduce the learning rate by a factor of 0.1, if the validation loss has stopped\ndecreasing for three consecutive epochs. Also, we applyEarlyStopping and stop training if the validation loss has\nstopped decreasing for six consecutive epochs. We minimize the cross-entropy loss function. All models have been\ncreated using the PyTorch library [52]. We use the BERT base uncased version. We test the proposed models using\nthe test set provided by the Challenge. We average the results obtained by the ﬁve repetitions.\nEvaluation Metrics Accuracy, Precision, Recall, F1-Score, and Speciﬁcity have been used for evaluating the results\noftheintroducedarchitectures.Thesemetricshavebeencomputedbyregardingthedementiaclassasthepositiveone.\n5.2. Results\nThe results of the proposed models mentioned in Section 5 are reported in Table 2. Also this table presents a\ncomparison of our introduced models with both unimodal and multimodal state-of-the-art approaches.\nRegarding our proposed transformer-based models with log-Mel spectrogram as input, one can observe that\nBERT+ViT+Crossmodal Attention constitutes our best performing model surpassing the other introduced models\nin F1-score and Accuracy, while it achieves equal Recall score with BERT+ViT+Gated Multimodal Unit. More\nspeciﬁcally, BERT+ViT+Crossmodal Attention outperforms BERT+ViT in recall by a margin of 10.84%, in F1-\nscore by 3.22%, and in accuracy by 2.08%, conﬁrming that the crossmodal attention improves the performance of the\nmultimodal models. Also, it outperforms BERT+ViT+Gated Multimodal Unit in F1-score by 2.77% and in Accuracy\nby 3.33%. In addition, BERT+ViT+Gated Multimodal Unit surpasses BERT+ViT in Recall and F1-score by 10.84%\nand 0.45% respectively. Although BERT+ViT surpasses the other proposed models in Speciﬁcity by 6.67-13.34%, it\nmust be noted that F1-score is a more important metric than Speciﬁcity in health-related tasks, since high Speciﬁcity\nand low F1-score means that AD patients are misdiagnosed as non-AD ones.\nAs one can easily observe, our best performing model, namely BERT+ViT+Crossmodal Attention, surpasses\nthe performance of the multimodal state-of-the-art models, except [43, 69], in Accuracy by 3.13-15.41%, while\nit outperforms the research works in Recall by 3.67-29.17% and in F1-score by 3.29-18.93%. At the same time,\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 13 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nBERT+ViT+Crossmodal Attention obtains a higher accuracy score than BERT [32] outperforming it by 0.83%.\nBERT+ViT+CrossmodalAttentionoutperformsBERTinF1-scoreby1.96%.Atthesametime,thestandarddeviations\nofBERT+ViT+CrossmodalAttentioninbothF1-scoreandAccuracyarelowerthanthestandarddeviationsofBERT\n[32].Thisfactindicatesthesuperiorityofourintroducedmodelandshowsthatitcancaptureeﬀectivelytheinteractions\nbetween the two modalities. Regarding BERT+ViT, we can observe that it surpasses the multimodal state-of-the-\nart models, except [43, 69], in Accuracy and F1-score by 1.05-13.33% and 0.07-15.71% respectively. Thus, the\ncombination of transformer networks, i.e., BERT and ViT, outperforms or obtains comparable performance to the\nmultimodal state-of-the-art approaches. Although BERT+ViT surpasses Fusion Maj. (3-best) [14] in F1-score by a\nsmallmarginof0.07%,itmustbenotedthatourproposedmodelismorecomputationallyandtimeeﬀective,sincethe\nmethod in [14] trains three diﬀerent models in order to enhance the classiﬁcation performance. We observe also that\nBERT+ViTperformsworsethanBERT[32].Wespeculatethatthisdiﬀerenceof1.25%inAccuracyisattributableto\nthe concatenation operation. In terms of BERT+ViT+Gated Multimodal Unit, it also outperforms the state-of-the-art\napproaches in F1-score and Accuracy except for [14, 43, 69]. Although BERT [32] outperforms BERT+ViT+Gated\nMultimodal Unit in terms of F1-score and Accuracy, the results show that BERT+ViT+Gated Multimodal Unit can\nbetter capture the relevant information of the two modalities on the test set in comparison to the performances of the\nexisting research initiatives proposing multimodal models.\nRegarding our proposed transformer-based models with MFCCs as input, one can observe that BERT + ViT +\nCrossmodal Attention constitutes our best performing model attaining an Accuracy score of 87.92% and an F1-score\nof87.99%.Speciﬁcally,itoutperformstheintroducedmodelsinAccuracyby2.50-3.76%,inF1-scoreby1.92-3.65%,\nand in Recall by 3.33-10.00%. Similarly to the proposed transformer-based models with log-Mel spectrogram, we\nobserve that the crossmodal attention yields better results than the concatenation operation and the gated multimodal\nunit. In addition, we observe that the BERT+ViT+Gated Multimodal Unit surpasses BERT+ViT in Accuracy by\n1.26%. However, BERT+ViT outperforms BERT+ViT+Gated Multimodal Unit in F1-score by 1.73%.\nIn comparison with the existing research initiatives, we observe that BERT+ViT+Crossmodal Attention im-\nproves the performance obtained by BERT [32]. Speciﬁcally, Accuracy is improved by 0.42%, F1-score sees an\nimprovement of 1.26%, and Recall is improved by 7.50%. On the contrary, BERT+ViT and BERT+ViT+Gated\nMultimodal Unit obtain worse performance than BERT [32]. Compared with the multimodal state-of-the-art ap-\nproaches, BERT+ViT+Crossmodal Attention surpasses the research works, except [43, 69], in Accuracy by 2.72-\n15.00%,inF1-scoreby2.59-18.23%,andinRecallby1.16-26.66%.BERT+ViT+GatedMultimodalUnitoutperforms\nthe research works, except [43, 69], in Accuracy by 0.22-12.50%. Finally, BERT+ViT surpasses the research works,\nexcept [43, 69, 14], in Accuracy by 1.16-11.24%, while it outperforms the research work [14] in F1-score by 0.67%.\n6. Discussion\nTheidentiﬁcationofdementiafromspontaneousspeechconstitutesahottopicinrecentyearsduetothefactthatitis\ntimeandcost-eﬃcient.Althoughseveralresearchworkshavebeenproposedtowardsdiagnosingdementiafromspeech,\nthere are still limitations. For example, most methods extract features from speech or transcripts and train traditional\nMachineLearningclassiﬁers.Anothersigniﬁcantlimitationhastodowiththewaythediﬀerentmodalities,e.g.,speech\nand transcripts, are combined in a single neural network. Speciﬁcally, research works train separately speech-based\nandtext-basednetworksandthenusemajorityvotingapproaches,thusincreasingsigniﬁcantlythetrainingtime.Other\nresearch works add or concatenate the text and image representations, thus treating equally the two modalities and\nobtainingsuboptimalperformance.Furthermore,althoughtransformershaveachievedstate-of-the-artresultsinmany\ndomains,theirpotentialhasnotbeenfullyexploitedinthetaskofdementiadetectionusingspeechdata.Tothebestof\nourknowledge,thisistheﬁrststudyemployingtheVisionTransformerfordetectingdementiaonlyfromspeech.This\nstudyaimsalsotoﬁllgapswithregardstotheusageofmultimodalmodelsbyintroducingtheGatedMultimodalUnit\nand the crossmodal attention layers, which have not been applied before in the task of dementia identiﬁcation from\nspontaneous speech. From the results obtained in this study, we found that:\n• Finding 1:The Vision Transformer (receiving as input images consisting of log-Mel spectrogram, delta, and\ndelta-delta) outperformed the other pretrained models, i.e., ResNet50, WideResNet-50-2, AlexNet, etc., in all\nthe evaluation metrics except for Speciﬁcity. Similarly, the Vision Transformer (receiving as input images\nconsisting of MFCCs, delta, and delta-delta) obtained higher scores by the other models in Accuracy, F1-\nscore, and Precision. We believe that the Vision Transformer constitutes our best performing model due to the\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 14 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nTable 2\nPerformance comparison among proposed models (using both speech and transcripts) and state-of-the-art approaches on\nthe ADReSS Challenge test set. Reported values are mean±standard deviation. Results are averaged across ﬁve runs.\nEvaluation metrics\nArchitecture Precision Recall F1-score Accuracy Speciﬁcity\nUnimodal state-of-the-art approaches (only transcripts)\nBERT [32] 87.19 81.66 86.73 87.50 93.33\n±3.25 ±5.00 ±4.53 ±4.37 ±5.65\nMultimodal state-of-the-art approaches (speech and transcripts)\ntop-3 late fusion [43] - - - 93.75 -\nAudio + Text (Fusion) [69] - 87.50 - 89.58 91.67\nSVM [6] 80.00 83.00 82.00 81.30 79.00\nFusion Maj. (3-best) [14] - - 85.40 85.20 -\nLSTM with Gating (Acoustic + Lexical + Dis) [56] 81.82 75.00 78.26 79.17 83.33\nSystem 3: Phonemes and Audio [18] 81.82 75.00 78.26 79.17 83.33\nFusion of system [53] 94.12 66.67 78.05 81.25 95.83\nBimodal Network (Ensembled Output) [34] 89.47 70.83 79.07 81.25 91.67\nGFI,NUW,Duration,Character 4-grams,Suﬃxes,\nPOS tag,UD [44] - - - 77.08 -\nAcoustic & Transcript [51] 70.00 88.00 78.00 75.00 83.00\nDual BERT [80] 83.04 83.33 82.92 82.92 82.50\n±3.97 ±5.89 ±1.86 ±1.56 ±5.53\nModel C [42] 78.94 62.50 69.76 72.92 83.33\nMajority vote (NLP + Acoustic) [63] - - - 83.00 -\nProposed Transformer-based models (log-Mel Spectrogram)\nBERT+ViT 90.73 80.83 85.47 86.25 91.67\n±2.74 ±2.04 ±1.70 ±1.67 ±2.64\nBERT+ViT+Gated Multimodal Unit 80.92 91.67 85.92 85.00 78.33\n±2.30 ±3.73 ±2.37 ±2.43 ±3.12\nBERT+ViT+Crossmodal Attention 86.13 91.67 88.69 88.33 85.00\n±3.26 ±4.56 ±2.12 ±2.12 ±4.25\nProposed Transformer-based models (MFCCs)\nBERT+ViT 86.72 85.83 86.07 84.16 86.66\n±2.05 ±6.77 ±2.69 ±1.02 ±3.12\nBERT+ViT+Gated Multimodal Unit 90.57 79.16 84.34 85.42 91.66\n±2.80 ±5.89 ±3.53 ±2.95 ±2.64\nBERT+ViT+Crossmodal Attention 87.09 89.16 87.99 87.92 86.66\n±2.40 ±5.65 ±2.79 ±2.43 ±3.12\ntransformerencoderandthemulti-headself-attention.Onthecontrary,alltheotherpretrainedmodelsarebased\non convolutional neural networks.\n• Finding 2: We compared the performance achieved between BERT and BERT+ViT and showed that\nBERT+ViT achieved slightly worse results. We speculated that this diﬀerence may be attributable to the usage\nofasimpleconcatenationofthetextandimagerepresentations.Asimpleconcatenationoperationassignsequal\nimportance to the diﬀerent modalities. In addition, we compared the performance of BERT+ViT on the test set\nwith 13 research works and showed that BERT+ViT outperformed most of the research works in F1-score and\nAccuracy. Thus, transformers achieve comparable performance to state-of-the-art approaches.\n• Finding 3: Results on the ADReSS Challenge test set showed that BERT+ViT+Gated Multimodal Unit\n(with log-Mel spectrogram) yielded a higher F1-score than BERT+ViT (with log-Mel spectrogram), while\nBERT+ViT+Gated Multimodal Unit (with MFCCs) yielded a higher Accuracy score than BERT+ViT (with\nMFCCs). In addition, we compared the performance of BERT+ViT+Gated Multimodal Unit on the test set\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 15 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nwith13multimodalresearchworksandshowedthatBERT+ViT+GatedMultimodalUnitachievedcomparable\nperformance.\n• Finding4: WepresentedanewmethodtodetectADpatientsconsistingofBERT,ViT,andcrossmodalattention\nlayers and showed that crossmodal interactions outperform the competitive multimodal models. We compared\nour best performing model (BERT+ViT+Crossmodal Attention with log-Mel spectrogram as input) with 13\nresearch works on the ADReSS Challenge test set and showed that our introduced model outperformed 11 of\nthesestrongbaselinesinAccuracyandF1-scorebyalargemarginof3.13-15.41%and3.29-18.93%respectively.\nMoreover,theincorporationofthecrossmodalattentionenhancedtheperformanceobtainedbyBERTby0.83%\nin Accuracy and by 1.96% in F1-score. In terms of BERT+ViT+Crossmodal Attention (with MFCCs), we\nobserved that it outperformed 11 of 13 strong baselines in Accuracy and F1-score by a large margin of 2.72-\n15.00% and 2.59-18.23% respectively, while it achieved better performance than BERT. Also, we observed that\nthevariancesofBERT+ViT+CrossmodalAttentionbyusingeitherlog-MelSpectrogramorMFCCsarelower\nthan BERT [32].\nAlso, we observed that BERT+ViT+Crossmodal Attention outperforms both BERT+ViT and BERT + ViT\n+ Gated Multimodal Unit. Speciﬁcally, BERT+ViT+Crossmodal Attention performs better than BERT+ViT,\nsinceBERT+ViTfusesthefeaturesofdiﬀerentmodalitiesthroughaconcatenationoperation.Theconcatenation\noperation ignores inherent correlations between diﬀerent modalities. In addition, BERT+ViT+Crossmodal\nAttention outperforms BERT+ViT+Gated Multimodal Unit. This can be justiﬁed by the fact that the Gated\nMultimodal Unit is inspired by the ﬂow control in recurrent architectures, such as GRU or LSTM. Speciﬁcally,\nthe Gated Multimodal Unit controls only the information ﬂow from each modality and does not capture\ninteractions between text and image. On the contrary, the usage of the crossmodal attention layers captures\nthe crossmodal interactions, enabling one modality for receiving information from another modality. More\nspeciﬁcally, we pass textual information to speech and speech information to text. Therefore, we observe that\ncontrollingtheﬂowofinformationfromthetwomodalitiesisnotsuﬃcient.Onthecontrary,learningcrossmodal\ninteractions is more important.\nIn addition, we observed that our best performing model, i.e., BERT+ViT+Crossmodal Attention, outperforms\nmost of the strong baselines. This fact justiﬁes our initial hypothesis that early and late fusion strategies and the\nusage of concatenation or add operation introduced by other studies do not capture eﬀectively the inter-modal\ninteractions of diﬀerent modalities, thus obtain in this way suboptimal performance.\nOnelimitationofthecurrentresearchworkhastodowiththelimitednumberofsamplesintheADReSSChallenge\ndataset,i.e.,78ADand78non-ADpatients.However,asmentionedinSection3,onecannotoverlookthatthisdataset\nismatchedforgenderandage,soastomitigatebiasinthepredictiontask.Concurrently,incontrasttootherdatasets,it\nhasbeencarefullyselectedsoastomitigatecommonbiasesoftenoverlookedinevaluationsofADdetectionmethods,\nincluding repeated occurrences of speech from the same participant and variations in audio quality. Moreover, it is\nbalanced, since it includes 78 AD and 78 non-AD patients. It is also used widely by a lot of research works dealing\nwith the task of dementia identiﬁcation from speech.\n7. Conclusion and Future Work\nIn this paper, we have proposed methods to diﬀerentiate AD from non-AD patients using either only speech\nor both speech and transcripts. Regarding the models using only speech, we exploited several pretrained models\nused extensively in the computer vision domain, with the Vision Transformer achieving the highest F1-score and\naccuracy accounting for 69.76% and 65.00% respectively. Next, we employed three neural network models in which\nwe combined speech and transcripts. We exploited the Gated Multimodal Unit, in order to control the inﬂuence of\neach modality towards the ﬁnal classiﬁcation. In addition, we experimented with crossmodal interactions, where\nwe used the crossmodal attention. Results showed that crossmodal attention can enhance the performance of\ncompetitive multimodal approaches and surpass state-of-the-art approaches. More speciﬁcally, models incorporating\nthe crossmodal attention yielded accuracy equal to 88.83% on the ADReSS Challenge test set. In the future, we plan\nto investigate more methods on how to combine speech and text representations more eﬀectively, including optimal\ntransport. In addition, we aim to use wav2vec2.0 for creating images.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 16 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\nDeclaration of competing interest\nTheauthorsdeclarethattheyhavenoknowncompetingﬁnancialinterestsorpersonalrelationshipsthatcouldhave\nappeared to inﬂuence the work reported in this paper.\nReferences\n[1] Al-Hameed, S., Benaissa, M., Christensen, H., 2017. Detecting and predicting alzheimer's disease severity in longitudinal acoustic data, in:\nProceedingsoftheInternationalConferenceonBioinformaticsResearchandApplications2017,AssociationforComputingMachinery,New\nYork, NY, USA. p. 57–61. URL:https://doi.org/10.1145/3175587.3175589, doi:10.1145/3175587.3175589.\n[2] Ammar, R.B., Ayed, Y.B., 2019. Evaluation of acoustic features for early diagnosis of alzheimer disease, in: International Conference on\nIntelligent Systems Design and Applications, Springer. pp. 172–181.\n[3] Arevalo,J.,Solorio,T.,Montes-yGomez,M.,González,F.A.,2020. Gatedmultimodalnetworks. NeuralComputingandApplications,1–20.\n[4] Association, A., 2019. 2019 alzheimer's disease facts and ﬁgures. Alzheimer's & dementia 15, 321–387.\n[5] Baevski,A.,Zhou,H.,Mohamed,A.,Auli,M.,2020. wav2vec2.0:Aframeworkforself-supervisedlearningofspeechrepresentations. arXiv\npreprint arXiv:2006.11477 .\n[6] Balagopalan, A., Eyre, B., Rudzicz, F., Novikova, J., 2020. To BERT or not to BERT: Comparing Speech and Language-Based Approaches\nfor Alzheimer's Disease Detection, in: Proc. Interspeech 2020, pp. 2167–2171. doi:10.21437/Interspeech.2020-2557.\n[7] Balagopalan, A., Novikova, J., 2021. Comparing Acoustic-Based Approaches for Alzheimer's Disease Detection, in: Proc. Interspeech 2021,\npp. 3800–3804. doi:10.21437/Interspeech.2021-759.\n[8] Becker, J.T., Boiler, F., Lopez, O.L., Saxton, J., McGonigle, K.L., 1994. The Natural History of Alzheimer's\nDisease: Description of Study Cohort and Accuracy of Diagnosis. Archives of Neurology 51, 585–594. URL:\nhttps://doi.org/10.1001/archneur.1994.00540180063015, doi: 10.1001/archneur.1994.00540180063015,\narXiv:https://jamanetwork.com/journals/jamaneurology/articlepdf/592905/archneur_51_6_015.pdf.\n[9] Bertini, F., Allevi, D., Lutero, G., Calzà, L., Montesi, D., 2022. An automatic alzheimer's disease classiﬁer based on spontaneous\nspoken english. Computer Speech & Language 72, 101298. URL: https://www.sciencedirect.com/science/article/pii/\nS0885230821000991, doi:https://doi.org/10.1016/j.csl.2021.101298.\n[10] Campbell,E.L.,Docio-Fernandez,L.,Jiménez-Raboso,J.,Gacia-Mateo,C.,2021.Alzheimer'sDementiaDetectionfromAudioandLanguage\nModalities in Spontaneous Speech, in: Proc. IberSPEECH 2021, pp. 270–274. doi:10.21437/IberSPEECH.2021-57.\n[11] Chen, J., Wang, Y., Wang, D., 2014. A feature study for classiﬁcation-based speech separation at low signal-to-noise ratios. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing 22, 1993–2002. doi:10.1109/TASLP.2014.2359159.\n[12] Chen,J.,Ye,J.,Tang,F.,Zhou,J.,2021. AutomaticDetectionofAlzheimer'sDiseaseUsingSpontaneousSpeechOnly,in:Proc.Interspeech\n2021, pp. 3830–3834. doi:10.21437/Interspeech.2021-2002.\n[13] Chlasta,K.,Wołk,K.,2021. Towardscomputer-basedautomatedscreeningofdementiathroughspontaneousspeech. FrontiersinPsychology\n11, 4091. URL:https://www.frontiersin.org/article/10.3389/fpsyg.2020.623237, doi:10.3389/fpsyg.2020.623237.\n[14] Cummins, N., Pan, Y., Ren, Z., Fritsch, J., Nallanthighal, V.S., Christensen, H., Blackburn, D., Schuller, B.W., Magimai-Doss, M., Strik, H.,\nHärmä, A., 2020. A Comparison of Acoustic and Linguistics Methodologies for Alzheimer's Dementia Recognition, in: Proc. Interspeech\n2020, pp. 2182–2186. doi:10.21437/Interspeech.2020-2635.\n[15] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of deep bidirectional transformers for language understanding,\nin: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies,Volume1(LongandShortPapers),AssociationforComputationalLinguistics,Minneapolis,Minnesota.pp.4171–4186. URL:\nhttps://aclanthology.org/N19-1423, doi:10.18653/v1/N19-1423.\n[16] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,\nUszkoreit, J., Houlsby, N., 2021. An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference\non Learning Representations. URL:https://openreview.net/forum?id=YicbFdNTTy.\n[17] Ebrahimi-Ghahnavieh, A., Luo, S., Chiong, R., 2019. Transfer learning for alzheimer's disease detection on mri images, in: 2019 IEEE\nInternational Conference on Industry 4.0, Artiﬁcial Intelligence, and Communications Technology (IAICT), pp. 133–138. doi:10.1109/\nICIAICT.2019.8784845.\n[18] Edwards, E., Dognin, C., Bollepalli, B., Singh, M., 2020. Multiscale System for Alzheimer's Dementia Recognition Through Spontaneous\nSpeech, in: Proc. Interspeech 2020, pp. 2197–2201. doi:10.21437/Interspeech.2020-2781.\n[19] Eyben, F., Scherer, K.R., Schuller, B.W., Sundberg, J., André, E., Busso, C., Devillers, L.Y., Epps, J., Laukka, P., Narayanan, S.S., Truong,\nK.P., 2016. The geneva minimalistic acoustic parameter set (gemaps) for voice research and aﬀective computing. IEEE Transactions on\nAﬀective Computing 7, 190–202. doi:10.1109/TAFFC.2015.2457417.\n[20] Eyben,F.,Weninger,F.,Gross,F.,Schuller,B.,2013.Recentdevelopmentsinopensmile,themunichopen-sourcemultimediafeatureextractor,\nin: Proceedings of the 21st ACM International Conference on Multimedia, Association for Computing Machinery, New York, NY, USA. p.\n835–838. URL:https://doi.org/10.1145/2502081.2502224, doi:10.1145/2502081.2502224.\n[21] Freitag,M.,Amiriparian,S.,Pugachevskiy,S.,Cummins,N.,Schuller,B.,2018. audeep:Unsupervisedlearningofrepresentationsfromaudio\nwithdeeprecurrentneuralnetworks.JournalofMachineLearningResearch18,1–5.URL: http://jmlr.org/papers/v18/17-406.html.\n[22] Furui, S., 1986. Speaker-independent isolated word recognition based on emphasized spectral dynamics, in: ICASSP '86. IEEE International\nConference on Acoustics, Speech, and Signal Processing, pp. 1991–1994. doi:10.1109/ICASSP.1986.1168654.\n[23] Gauder,L.,Pepino,L.,Ferrer,L.,Riera,P.,2021.AlzheimerDiseaseRecognitionUsingSpeech-BasedEmbeddingsFromPre-TrainedModels,\nin: Proc. Interspeech 2021, pp. 3795–3799. doi:10.21437/Interspeech.2021-753.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 17 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\n[24] Ghoniem, R.M., 2019. Deep genetic algorithm-based voice pathology diagnostic system, in: International Conference on Applications of\nNatural Language to Information Systems, Springer. pp. 220–233.\n[25] Giannopoulos, P., Perikos, I., Hatzilygeroudis, I., 2018. Deep learning approaches for facial emotion recognition: A case study on fer-2013,\nin: Advances in hybridization of intelligent methods. Springer, pp. 1–16.\n[26] Haider,F.,delaFuente,S.,Luz,S.,2020.Anassessmentofparalinguisticacousticfeaturesfordetectionofalzheimer'sdementiainspontaneous\nspeech. IEEE Journal of Selected Topics in Signal Processing 14, 272–281. doi:10.1109/JSTSP.2019.2955022.\n[27] Haulcy,R.,Glass,J.,2021. Classifyingalzheimer'sdiseaseusingaudioandtext-basedrepresentationsofspeech. FrontiersinPsychology11,\n3833. URL:https://www.frontiersin.org/article/10.3389/fpsyg.2020.624137, doi:10.3389/fpsyg.2020.624137.\n[28] He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, in: 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 770–778. doi:10.1109/CVPR.2016.90.\n[29] Hershey, S., Chaudhuri, S., Ellis, D.P.W., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., Slaney,\nM.,Weiss,R.J.,Wilson,K.,2017. Cnnarchitecturesforlarge-scaleaudioclassiﬁcation,in:2017IEEEInternationalConferenceonAcoustics,\nSpeech and Signal Processing (ICASSP), pp. 131–135. doi:10.1109/ICASSP.2017.7952132.\n[30] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected convolutional networks, in: 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 2261–2269. doi:10.1109/CVPR.2017.243.\n[31] Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016. Squeezenet: Alexnet-level accuracy with 50x fewer\nparameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360 .\n[32] Ilias,L.,Askounis,D.,2022. Explainableidentiﬁcationofdementiafromtranscriptsusingtransformernetworks. IEEEJournalofBiomedical\nand Health Informatics 26, 4153–4164. doi:10.1109/JBHI.2022.3172479.\n[33] Koike, T., Qian, K., Kong, Q., Plumbley, M.D., Schuller, B.W., Yamamoto, Y., 2020. Audio for audio is better? an investigation on transfer\nlearningmodelsforheartsoundclassiﬁcation,in:202042ndAnnualInternationalConferenceoftheIEEEEngineeringinMedicine&Biology\nSociety (EMBC), pp. 74–77. doi:10.1109/EMBC44109.2020.9175450.\n[34] Koo, J., Lee, J.H., Pyo, J., Jo, Y., Lee, K., 2020. Exploiting Multi-Modal Features from Pre-Trained Networks for Alzheimer's Dementia\nRecognition, in: Proc. Interspeech 2020, pp. 2217–2221. doi:10.21437/Interspeech.2020-3153.\n[35] Krizhevsky, A., 2014. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997 .\n[36] Kumar, K., Kim, C., Stern, R.M., 2011. Delta-spectral cepstral coeﬃcients for robust speech recognition, in: 2011 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4784–4787. doi:10.1109/ICASSP.2011.5947425.\n[37] Laguarta,J.,Subirana,B.,2021. Longitudinalspeechbiomarkersforautomatedalzheimer'sdetection. FrontiersinComputerScience3. URL:\nhttps://www.frontiersin.org/articles/10.3389/fcomp.2021.624694, doi:10.3389/fcomp.2021.624694.\n[38] Lee, J.L., Burkholder, R., Flinn, G.B., Coppess, E.R., 2016. Working with CHAT transcripts in Python. Technical Report TR-2016-02.\nDepartment of Computer Science, University of Chicago.\n[39] Li,X.,Dalmia,S.,Li,J.,Lee,M.,Littell,P.,Yao,J.,Anastasopoulos,A.,Mortensen,D.R.,Neubig,G.,Black,A.W.,Metze,F.,2020. Universal\nphone recognition with a multilingual allophone system, in: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 8249–8253. doi:10.1109/ICASSP40776.2020.9054362.\n[40] Luz, S., Haider, F., de la Fuente, S., Fromm, D., MacWhinney, B., 2020. Alzheimer's Dementia Recognition Through Spontaneous Speech:\nThe ADReSS Challenge, in: Proc. Interspeech 2020, pp. 2172–2176. doi:10.21437/Interspeech.2020-2571.\n[41] MacWhinney, B., 2014. The CHILDES project: Tools for analyzing talk, Volume II: The database. Psychology Press.\n[42] Mahajan, P., Baths, V., 2021. Acoustic and language based deep learning approaches for alzheimer's dementia detection from spontaneous\nspeech. Frontiers in Aging Neuroscience 13, 20. URL:https://www.frontiersin.org/article/10.3389/fnagi.2021.623607,\ndoi:10.3389/fnagi.2021.623607.\n[43] Martinc,M.,Haider,F.,Pollak,S.,Luz,S.,2021. Temporalintegrationoftexttranscriptsandacousticfeaturesforalzheimer'sdiagnosisbased\nonspontaneousspeech. FrontiersinAgingNeuroscience13. URL: https://www.frontiersin.org/articles/10.3389/fnagi.2021.\n642647, doi:10.3389/fnagi.2021.642647.\n[44] Martinc, M., Pollak, S., 2020. Tackling the ADReSS Challenge: A Multimodal Approach to the Automated Recognition of Alzheimer's\nDementia, in: Proc. Interspeech 2020, pp. 2157–2161. doi:10.21437/Interspeech.2020-2202.\n[45] McFee, B., Metsai, A., McVicar, M., Balke, S., Thomé, C., Raﬀel, C., Zalkow, F., Malek, A., Dana, Lee, K., Nieto, O., Ellis, D.,\nMason, J., Battenberg, E., Seyfarth, S., Yamamoto, R., viktorandreevichmorozov, Choi, K., Moore, J., Bittner, R., Hidaka, S., Wei, Z.,\nnullmightybofo, Hereñú, D., Stöter, F.R., Friesch, P., Weiss, A., Vollrath, M., Kim, T., Thassilo, 2021. librosa/librosa: 0.8.1rc2. URL:\nhttps://doi.org/10.5281/zenodo.4792298, doi:10.5281/zenodo.4792298.\n[46] Meghanani, A., C. S., A., Ramakrishnan, A.G., 2021. An exploration of log-mel spectrogram and mfcc features for alzheimer's dementia\nrecognitionfromspontaneousspeech,in:2021IEEESpokenLanguageTechnologyWorkshop(SLT),pp.670–677. doi: 10.1109/SLT48900.\n2021.9383491.\n[47] Meng, H., Yan, T., Yuan, F., Wei, H., 2019. Speech emotion recognition from 3d log-mel spectrograms with deep learning network. IEEE\nAccess 7, 125868–125881. doi:10.1109/ACCESS.2019.2938007.\n[48] Mittal,A.,Sahoo,S.,Datar,A.,Kadiwala,J.,Shalu,H.,Mathew,J.,2021. Multi-modaldetectionofalzheimer'sdiseasefromspeechandtext.\narXiv:2012.00096.\n[49] Pan, Y., Mirheidari, B., Harris, J.M., Thompson, J.C., Jones, M., Snowden, J.S., Blackburn, D., Christensen, H., 2021. Using the Outputs of\nDiﬀerent Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer's Dementia Detection Through Spontaneous\nSpeech, in: Proc. Interspeech 2021, pp. 3810–3814. doi:10.21437/Interspeech.2021-1519.\n[50] Pappagari, R., Cho, J., Joshi, S., Moro-Velázquez, L., /uni017Belasko, P., Villalba, J., Dehak, N., 2021. Automatic Detection and Assessment\nof Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios, in: Proc. Interspeech 2021, pp. 3825–3829.\ndoi:10.21437/Interspeech.2021-1850.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 18 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\n[51] Pappagari, R., Cho, J., Moro-Velázquez, L., Dehak, N., 2020. Using State of the Art Speaker Recognition and Natural Language\nProcessing Technologies to Detect Alzheimer's Disease and Assess its Severity, in: Proc. Interspeech 2020, pp. 2177–2181. doi:10.21437/\nInterspeech.2020-2587.\n[52] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf,\nA., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S., 2019. Pytorch: An imperative\nstyle, high-performance deep learning library, in: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., Garnett, R. (Eds.),\nAdvances in Neural Information Processing Systems 32. Curran Associates, Inc., pp. 8024–8035. URL:http://papers.neurips.cc/\npaper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf .\n[53] Pompili, A., Rolland, T., Abad, A., 2020. The INESC-ID Multi-Modal System for the ADReSS 2020 Challenge, in: Proc. Interspeech 2020,\npp. 2202–2206. doi:10.21437/Interspeech.2020-2833.\n[54] Pérez-Toro, P.A., Vásquez-Correa, J.C., Arias-Vergara, T., Klumpp, P., Sierra-Castrillón, M., Roldán-López, M.E., Aguillón, D., Hincapié-\nHenao, L., Tóbon-Quintero, C.A., Bocklet, T., Schuster, M., Orozco-Arroyave, J.R., Nöth, E., 2021. Acoustic and linguistic analyses to\nassess early-onset and genetic alzheimer's disease, in: ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 8338–8342. doi:10.1109/ICASSP39728.2021.9414009.\n[55] Raghu, S., Sriraam, N., Temel, Y., Rao, S.V., Kubben, P.L., 2020. Eeg based multi-class seizure type classiﬁcation using convolutional\nneural network and transfer learning. Neural Networks 124, 202–212. URL:https://www.sciencedirect.com/science/article/\npii/S0893608020300198, doi:https://doi.org/10.1016/j.neunet.2020.01.017.\n[56] Rohanian, M., Hough, J., Purver, M., 2020. Multi-Modal Fusion with Gating Using Audio, Lexical and Disﬂuency Features for Alzheimer's\nDementia Recognition from Spontaneous Speech, in: Proc. Interspeech 2020, pp. 2187–2191. doi:10.21437/Interspeech.2020-2721.\n[57] Rohanian,M.,Hough,J.,Purver,M.,2021.Alzheimer'sDementiaRecognitionUsingAcoustic,Lexical,DisﬂuencyandSpeechPauseFeatures\nRobust to Noisy Inputs, in: Proc. Interspeech 2021, pp. 3820–3824. doi:10.21437/Interspeech.2021-1633.\n[58] Roy, A.D., Islam, M.M., 2020. Detection of epileptic seizures from wavelet scalogram of eeg signal using transfer learning with alexnet\nconvolutional neural network, in: 2020 23rd International Conference on Computer and Information Technology (ICCIT), pp. 1–5. doi:10.\n1109/ICCIT51783.2020.9392720.\n[59] Sánchez Villegas, D., Aletras, N., 2021. Point-of-interest type prediction using text and images, in: Proceedings of the 2021 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing,AssociationforComputationalLinguistics,OnlineandPuntaCana,DominicanRepublic.\npp. 7785–7797. URL:https://aclanthology.org/2021.emnlp-main.614, doi:10.18653/v1/2021.emnlp-main.614.\n[60] Sánchez Villegas, D., Mokaram, S., Aletras, N., 2021. Analyzing online political advertisements, in: Findings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics, Online. pp. 3669–3680. URL:https://\naclanthology.org/2021.findings-acl.321, doi:10.18653/v1/2021.findings-acl.321.\n[61] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C., 2018. Mobilenetv2: Inverted residuals and linear bottlenecks, in: 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4510–4520. doi:10.1109/CVPR.2018.00474.\n[62] Sarawgi, U., Zulﬁkar, W., Soliman, N., Maes, P., 2020. Multimodal Inductive Transfer Learning for Detection of Alzheimer's Dementia and\nits Severity, in: Proc. Interspeech 2020, pp. 2212–2216. doi:10.21437/Interspeech.2020-3137.\n[63] Shah,Z.,Sawalha,J.,Tasnim,M.,Qi,S.a.,Stroulia,E.,Greiner,R.,2021. Learninglanguageandacousticmodelsforidentifyingalzheimer's\ndementia from speech. Frontiers in Computer Science 3, 4. URL:https://www.frontiersin.org/article/10.3389/fcomp.2021.\n624659, doi:10.3389/fcomp.2021.624659.\n[64] Sharma,B.,Madhavi,M.,Li,H.,2021. Leveragingacousticandlinguisticembeddingsfrompretrainedspeechandlanguagemodelsforintent\nclassiﬁcation,in:ICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.7498–7502.\ndoi:10.1109/ICASSP39728.2021.9413388.\n[65] Shor,J.,Jansen,A.,Maor,R.,Lang,O.,Tuval,O.,deChaumontQuitry,F.,Tagliasacchi,M.,Shavitt,I.,Emanuel,D.,Haviv,Y.,2020. Towards\nLearning a Universal Non-Semantic Representation of Speech, in: Proc. Interspeech 2020, pp. 140–144. doi:10.21437/Interspeech.\n2020-1242.\n[66] Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 .\n[67] Srivastava, R.K., Greﬀ, K., Schmidhuber, J., 2015. Highway networks. arXiv preprint arXiv:1505.00387 .\n[68] Syed, M.S.S., Syed, Z.S., Lech, M., Pirogova, E., 2020. Automated Screening for Alzheimer's Dementia Through Spontaneous Speech, in:\nProc. Interspeech 2020, pp. 2222–2226. doi:10.21437/Interspeech.2020-3158.\n[69] Syed,Z.S.,Syed,M.S.S.,Lech,M.,Pirogova,E.,2021. Automatedrecognitionofalzheimer'sdementiausingbag-of-deep-featuresandmodel\nensembling. IEEE Access 9, 88377–88390. doi:10.1109/ACCESS.2021.3090321.\n[70] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper with\nconvolutions,in:2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.1–9. doi: 10.1109/CVPR.2015.7298594.\n[71] Tan,M.,Chen,B.,Pang,R.,Vasudevan,V.,Sandler,M.,Howard,A.,Le,Q.V.,2019. Mnasnet:Platform-awareneuralarchitecturesearchfor\nmobile, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823. doi:10.1109/CVPR.2019.\n00293.\n[72] Tan,M.,Le,Q.,2019. EﬃcientNet:Rethinkingmodelscalingforconvolutionalneuralnetworks,in:Chaudhuri,K.,Salakhutdinov,R.(Eds.),\nProceedingsofthe36thInternationalConferenceonMachineLearning,PMLR.pp.6105–6114.URL: https://proceedings.mlr.press/\nv97/tan19a.html.\n[73] Tsai,Y.H.H.,Bai,S.,Liang,P.P.,Kolter,J.Z.,Morency,L.P.,Salakhutdinov,R.,2019. Multimodaltransformerforunalignedmultimodallan-\nguagesequences,in:Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,AssociationforComputational\nLinguistics, Florence, Italy. pp. 6558–6569. URL:https://aclanthology.org/P19-1656, doi:10.18653/v1/P19-1656.\n[74] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I., 2017. Attention is all\nyou need, in: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (Eds.), Advances in\nNeural Information Processing Systems, Curran Associates, Inc. URL: https://proceedings.neurips.cc/paper/2017/file/\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 19 of 20\nDetecting Dementia from Speech and Transcripts using Transformers\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[75] Weide, R., 2005. The carnegie mellon pronouncing dictionary [cmudict. 0.6]. Pittsburgh, PA: Carnegie Mellon University .\n[76] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,\nS., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush, A.M., 2020. Transformers:\nState-of-the-art natural language processing, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations, Association for Computational Linguistics, Online. pp. 38–45. URL:https://www.aclweb.org/anthology/\n2020.emnlp-demos.6.\n[77] Xie,S.,Girshick,R.,Dollár,P.,Tu,Z.,He,K.,2017. Aggregatedresidualtransformationsfordeepneuralnetworks,in:2017IEEEConference\non Computer Vision and Pattern Recognition (CVPR), pp. 5987–5995. doi:10.1109/CVPR.2017.634.\n[78] Zagoruyko, S., Komodakis, N., 2016. Wide residual networks. arXiv preprint arXiv:1605.07146 .\n[79] Zhao,Z.,Li,Q.,Cummins,N.,Liu,B.,Wang,H.,Tao,J.,Schuller,B.W.,2020. HybridNetworkFeatureExtractionforDepressionAssessment\nfrom Speech, in: Proc. Interspeech 2020, pp. 4956–4960. doi:10.21437/Interspeech.2020-2396.\n[80] Zhu, Y., Liang, X., Batsis, J.A., Roth, R.M., 2021. Exploring deep transfer learning techniques for alzheimer's dementia detection. Frontiers\nin Computer Science 3, 22. URL:https://www.frontiersin.org/article/10.3389/fcomp.2021.624683, doi:10.3389/fcomp.\n2021.624683.\nL.Ilias, D.Askounis, and J.Psarras:Preprint submitted to Elsevier Page 20 of 20"
}