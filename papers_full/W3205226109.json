{
  "title": "DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer",
  "url": "https://openalex.org/W3205226109",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2986309317",
      "name": "Haozhe Ji",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2162268045",
      "name": "Minlie Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2115613106",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W3022187094",
    "https://openalex.org/W3115433372",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2971074500",
    "https://openalex.org/W1605174196",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2963411289",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2789543585",
    "https://openalex.org/W2587284713",
    "https://openalex.org/W2963878748",
    "https://openalex.org/W4391602018",
    "https://openalex.org/W2970574558",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W3098708719",
    "https://openalex.org/W4294623686",
    "https://openalex.org/W3167880383",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W3100714086",
    "https://openalex.org/W2797585226",
    "https://openalex.org/W2914855263",
    "https://openalex.org/W1497300277",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3102401511",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2889009749",
    "https://openalex.org/W3037337776",
    "https://openalex.org/W2962821399",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2963730239",
    "https://openalex.org/W2962942158"
  ],
  "abstract": "Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4208–4224\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4208\nDISCO DVT: Generating Long Text with Discourse-Aware Discrete\nVariational Transformer\nHaozhe Ji1, Minlie Huang1∗\n1Department of Computer Science and Technology, Institute for Artiﬁcial Intelligence,\nState Key Lab of Intelligent Technology and Systems,\nBeijing National Research Center for Information Science and Technology,\nTsinghua University, Beijing 100084, China\njhz20@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn\nAbstract\nDespite the recent advances in applying pre-\ntrained language models to generate high-\nquality texts, generating long passages that\nmaintain long-range coherence is yet challeng-\ning for these models. In this paper, we propose\nDISCO DVT, a discourse-aware discrete vari-\national Transformer to tackle the incoherence\nissue. D ISCO DVT learns a discrete variable\nsequence that summarizes the global structure\nof the text and then applies it to guide the gen-\neration process at each decoding step. To fur-\nther embed discourse-aware information into\nthe discrete latent representations, we intro-\nduce an auxiliary objective to model the dis-\ncourse relations within the text. We conduct\nextensive experiments on two open story gen-\neration datasets and demonstrate that the latent\ncodes learn meaningful correspondence to the\ndiscourse structures that guide the model to\ngenerate long texts with better long-range co-\nherence.1\n1 Introduction\nGenerating passages that maintain long-range co-\nherence is a long-standing problem in natural lan-\nguage generation (NLG). Despite the recent ad-\nvances of large pre-trained language generation\nmodels (Radford et al., 2019; Lewis et al., 2020)\nin various NLG tasks such as summarization and\ndialogue generation that target generating locally\ncoherent texts which are relatively short, it is still\nchallenging for pre-trained models to generateglob-\nally coherent passages spanning over dozens of\nsentences.\nGlobal coherence in human texts is represented\nby the topic-maintenance and natural transition\nbetween viewpoints (Jurafsky and Martin, 2000).\nAs illustrated in Figure 1, discourse relations such\n∗ Corresponding author\n1The source code is available at https://github.\ncom/cdjhz/DiscoDVT.\n[The player begins in a tiny hamlet, near which he/she used to live.]1 \n[After  clearing out an abandoned mine,]2   \n[the player finds a scrap of parchment that reveals the death of his godparents.]3 \n[The player then  returns to the hamlet to find it pillaged,]4   \n[and  decides to travel to Bjarnarhaven.]5 \n[Upon activating the amulet, the player is informed of his past by his dead father,]6  \n[after  which the player is transported to the town of Crossroads, and Part I ends.]7 \n[The town of Crossroads is run by a Jarl who at first does not admit the player,]8   \n[but  later provides advice and rewards.]9  \n[The player then  enters the nearby ruined titular Castle of the Winds.]10 \n[EDU Segment] Sentence Segment Discourse RelationLatent Code\n...... \nFigure 1: An example from Wikiplots where a discrete\nlatent variable sequence abstracts the discourse struc-\nture of the text. Inter-sentence and intra-sentence dis-\ncourse relations are indicated by the bolded discourse\nmarkers.\nas causal, temporal succession between contigu-\nous text segments are commonly indicated by the\nhighlighted discourse markers which bind collo-\ncated text segments into a global structure (Hobbs,\n1985). Although pre-trained language models are\ninspected to perform reasonably well in associat-\ning topic-related concepts, they can hardly arrange\ncontents with well-structured discourses (See et al.,\n2019; Ko and Li, 2020).\nIn this work, we urge the revival of variational\nautoencoder (V AE) with its global representation\nability to tackle the incoherence issue in long text\ngeneration in the era of pre-trained language mod-\nels. To represent texts with high-level structures,\nwe propose to learn a latent variable sequence with\neach latent code abstracting a local text span. In-\nstead of the commonly used continuous latent vari-\nables, we resort to learn discrete latent codes that\nnaturally correspond to interpretable categories in\nnatural languages (Zhao et al., 2018). For the latent\ncodes to capture the explicit discourse structure of\nthe texts as shown in Figure 1, we further design\nan auxiliary objective on the latent representations\nto model the discourse relations.\nWe name the proposed model as DISCO DVT,\ni.e., Discourse-aware Discrete Variational\nTransformer. The main idea is to learn a discrete\n4209\nlatent variable sequence that summarizes the long\ntext to reconstruct the original text by guiding\nthe decoding process. The learning schema\nis shown in Figure 2 (a). At the encoding\nphase, to capture the high-level structure of\nthe text, we ﬁrst use a bidirectional encoder to\nobtain contextualized token representations and\nthen apply 1-dimensional convolutional neural\nnetworks (1D CNNs) to abstract text segments\nat the temporal scale (§3.2.1). To condense\nthe continuous representations into categorical\nfeatures, we map them into a one-hot categorical\ndistribution over a ﬁxed latent vocabulary, and\nobtain the discrete variable sequence (§3.2.2). At\nthe decoding phase, to apply the global discrete\nlatent codes to guide the local text realization,\nthe latent embeddings are ﬁrst rescaled to the\ntext length with transposed 1D CNNs, and then\nadded to the embedding layer of the decoder for\nstep-wise control (§3.2.3). For the latent codes to\nabstract the discourse structure of the text, we use\nexplicit discourse relations from Penn Discourse\nTreeBank 2.0 (PDTB, Prasad et al., 2008) and\nextract adjacent elementary discourse units (EDUs)\nfrom texts as shown in Figure 1 and introduce an\nanxiliary objective to embed the relations into the\nlatent representations. Once the discrete latent\ncodes are learned, we adopt an autoregressive\nTransformer to model the prior distribution as a\nsequence transduction task (§3.4).\nWe summarize our contributions in three folds:\n(1) We propose a novel latent variable model\nthat learns discrete latent variable sequence from\nthe long text and applies it to guide the generation\nprocess to maintain long-term coherence.\n(2) We further acquire the discourse relation in-\nformation and introduce an auxiliary objective for\nthe discrete latent codes to abstract the discourse\nstructure of the text.\n(3) We conduct extensive experiments on two\nopen story generation datasets with automatic and\nhuman evaluation. Results demonstrate that our\nmodel outperforms baselines in generating coher-\nent long texts with interpretable latent codes.\n2 Related Work\n2.1 Long Text Generation\nPrior works endeavored to solve the incoherence\nissue in long text generation can be mainly cate-\ngorized into model structure modiﬁcations, gener-\nation mechanism modiﬁcations, and prior knowl-\nedge injection.\nTo model the hierarchical nature of human texts,\nLi et al. (2015) proposed a hierarchical RNN de-\ncoder to learn sentence-level representations within\nthe paragraph. Shen et al. (2019) augmented the\nhierarchical model with multi-level latent variables,\nand Shao et al. (2019) further incorporates a plan-\nning mechanism to pre-arrange the order and the\ngroup of the input keywords.\nAnother line of works proposed to decompose\nlong text generation into multiple stages (Fan et al.,\n2018; Yao et al., 2019; Fan et al., 2019; Tan et al.,\n2020) where the model ﬁrst generates a rough\nsketch, such as key phrases or summaries, and then\nexpands it into the complete long text with ﬁne\ndetail. However, the multi-step generation method\nis known to have the stage-level exposure bias (Tan\net al., 2020), i.e., the discrepancy of middle-stage\noutputs during training and inference, which can ac-\ncumulate error through stages and impair the ﬁnal\ngeneration quality.\nThe ﬁnal direction is to inject prior external\nknowledge into pre-trained language models for\ncommonsense story generation (Guan et al., 2020;\nXu et al., 2020). However, these methods may\nnot be generalizable to different data genres such\nas ﬁctional stories and do not provide long-range\nguidance during text generation.\n2.2 Discrete Latent Variable Models\nIn text generation, continuous Gaussian V AEs have\nbeen explored to model response diversity (Zhao\net al., 2017; Serban et al., 2017) and high-level\nstructures, such as template and order (Wiseman\net al., 2018; Shen et al., 2019; Shao et al., 2019).\nAside from Gaussian latent variables in the con-\ntinuous space (Kingma and Welling, 2014), re-\ncent works also explored V AEs in the discrete\nspace (Rolfe, 2017) with the merit of explainabil-\nity and revealed the correspondence between the\nlatent codes and categorical features, e.g., dialogue\nacts (Zhao et al., 2018) and POS tags (Bao et al.,\n2021). Recently, van den Oord et al. (2017) pro-\nposed a vector-quantized variational autoencoder\n(VQ-V AE) which circumvents the posterior col-\nlapse problem by learning quantized one-hot pos-\nterior that can be adapted with powerful autore-\ngressive decoders. In image and speech generation,\nRazavi et al. (2019); Dieleman et al. (2018) ex-\ntended VQ-V AE with hierarchical latent variables\nto capture different input data resolutions and gen-\n4210\nEmbedding\n1D CNN\n Transposed 1D CNN\nDiscrete\nBottleneck\n... ... ... \nEDU1 EDU2\nEmbedding\n... \n+ \n<s> ...<s> ...\n</s>...\nDiscourse Relation Modeling\nGumbel-Softmax\n0.0 \n0.0 \n1.0 \n-1.0 \n(a) (b)\n... \nGenerator\nPosterior Network\nFigure 2: Overview of D ISCO DVT. (a) Learning discrete latent codes via encoding and reconstructing the target\ntext with discourse relation modeling where the latent representations are required to predict discourse relations\nwithin the text. (b) The discrete variational bottleneck maps the output of the encoder into a categorical distribution\nover a ﬁxed latent vocabulary.\nerate high-ﬁdelity visual and audio data with high-\nlevel structures. To our knowledge, in the domain\nof text generation, our work is the ﬁrst attempt that\nexplores discrete latent variable models scaling up\nto the size of large pre-trained language models to\nsolve the incoherence issue in long text generation.\n3 Methodology\n3.1 Task Deﬁnition and Model Overview\nWe formulate the long text generation task as a\nconditional generation problem, i.e., generating a\nmulti-sentence text y = (y1,··· ,yM) given an in-\nput prompt x = (x1,··· ,xN). Current pre-trained\ngeneration models, e.g., BART, adopt Transformer-\nbased encoder-decoder structure that bidirection-\nally encodes x and maximizes the log-likelihood\nLLM of predicting y at the decoder side.\nHowever, existing models can hardly maintain\nlong-range coherence when generating long texts\nthat span hundreds of words. We propose to\nlearn a discrete sequence of latent variables z =\n(z1,··· ,zL) to abstract the high-level structure of\nthe text at the temporal scale ( Lis much shorter\nthan M) and categories (each zl takes value from\nthe latent vocabulary with size K, which is much\nsmaller than the text vocabulary).\nOur model maximizes the evidence lower bound\n(ELBO, Kingma and Welling, 2014) of the log-\nlikelihood of the generative model: p(y,z|x) =\npθ(y|z,x)pψ(z|x) where the generator and the\nprior network are parametrized by θand ψ, respec-\ntively. Since we want z to capture the internal\nstructure of the text instead of speciﬁc topic infor-\nmation, we posit it to be independent of the input\nprompt x and formulate the posterior network as\nqφ(z|y). The same formulation is also adopted\nby Zhao et al. (2018) to learn interpretable latent\nvariables. We give the ELBO in the following:\nLELBO = Ez∼qφ log pθ(y|z,x)−\nDKL(qφ(z|y)∥pψ(z|x)). (1)\nDue to the discrete nature of z, qφ(z|y) deﬁnes\na sequence of one-hot distribution over the discrete\nvocabulary K at each position. Thus, the second\nterm of the ELBO can be interpreted as a sequence\ntransduction objective that autoregressively ﬁts the\nprior model to the target sequence z given by the\nposterior: pψ(z|x) = ∏\nlpψ(zl|z<l,x).\nWe follow van den Oord et al. (2017) and sep-\narate the learning process into two stages. In the\nﬁrst training stage, we train the posterior network\nand the generator to optimize the ﬁrst term of\nthe ELBO to learn discrete latent codes of the\ntext (§3.2). We further propose a discourse-aware\nobjective for the latent representations to model\nhigh-level discourse relations of the text (§3.3). In\nthe second training stage, we adopt another Trans-\nformer model as the prior network that predicts\nthe discrete latent codes given the input prompt\n(§3.4). During the inference stage, we ﬁrst sample\na sequence of latent variables from the prior net-\nwork given the input prompt, and then inject it into\nthe generator to guide the local text realization by\nrandomly sampling text tokens.\n3.2 Learning Discrete Latent Codes\nIn this section, we introduce the procedure of learn-\ning discrete latent codes from the long text. Given\nthe text y, the idea is to encode it into a latent vari-\nable sequence z that preserves high-level structure\nto guide text reconstruction with the input x. y is\nﬁrst encoded into contextualized representations\nwith a bidirectional Transformer encoder, and then\nabstracted into z with 1D CNNs and the discrete\n4211\nvariational bottleneck. To guide text generation,\nwe ﬁrst embed z into the embedding matrix, then\nrescale it to the original length ofy with transposed\n1D CNNs, and ﬁnally inject it into the decoder’s\nembedding layer for step-wise control.\n3.2.1 Temporal Abstraction with CNNs\nTo abstract high-level features that correspond to\nthe global structure of the text, we adoptc-layer 1D\nCNNs that decrease the text length 2c times where\neach layer halves the input size. The similar ar-\nchitecture was also explored in non-autoregressive\nmachine translation (Kaiser et al., 2018) but for the\npurpose of parallel decoding.\nFormally, given the input text representations\nHe = [ he\n1,··· ,he\nM], the output of CNNs is de-\nnoted as Oe = [oe\n1,··· ,oe\nL]. Intuitively, stacked\nCNNs extract contiguous n-gram features from the\ntext sequence with each code abstracting a contigu-\nous text span with ﬂexible boundaries.\nAt the decoding phase, to smoothen the high-\nlevel representations at the temporal level for con-\ntinuous local text generation, we adopt transposed\nCNNs with the symmetric structure to rescale the\ncode embedding matrix Oz = [oz\n1,··· ,oz\nL] into\nlow-level features Hz = [hz\n1,··· ,hz\nM].\n3.2.2 Discrete Variational Bottleneck\nTo enforce z to preserve salient information for\ntext reconstruction with interpretable categories,\nwe introduce a discrete variational bottleneck that\ndiscretizes the CNN outputs Oe into categorical\nfeatures. Intuitively, the bottleneck controls the\ninformation capacity of z by mapping continuous\nrepresentations to a discrete space.\nWe give a formal description of the discretiza-\ntion. Figure2 (b) presents an example at the l-th po-\nsition2. oe is ﬁrst mapped into logits t = Wzoe ∈\nRK through a linear transformation. The discrete\ncode zat this position is deﬁned as\nz= argmax\nk∈K\ntk. (2)\nDuring training, to backpropagate gradients, we\napply the Gumbel-Softmax trick (Jang et al., 2017;\nMaddison et al., 2017) to provide a differentiable\nrelaxation of the argmax operation.\nwk = exp((tk + gk)/τ)∑K\nk=1 exp((tk + gk)/τ)\n, (3)\n2We ommit the subscript l in the following derivation.\nwhere g1,··· ,gK are i.i.d samples from the Gum-\nbel distribution, and τ is the temperature that con-\ntrols the tightness of the relaxation. As τ anneals\nfrom τmax to nearly 0 during training, the soft cat-\negorical distribution wk becomes a reasonable esti-\nmation of the one-hot distribution.\nThis categorical distribution is then multiplied\nto the learnable code embeddings Ez to obtain the\ncode embedding matrix oz = Ezw.\n3.2.3 Generation with Step-Wise Control\nFor the high-level latent codes to explicitly guide\nthe local text realization, the code embedding ma-\ntrix Oz is ﬁrst rescaled into Hz. It is then added\nto the decoder’s input embedding layer with token\nembeddings {em}M\nm=1 and positional encodings\n{pm}M\nm=1 at each decoding position. The new in-\nput embeddings are{hz\nm+em+pm}M\nm=1. Because\nof the residual structure in the Transformer, the in-\nformation of hz\nm can be effectively transmit to the\nhigher layers with positional awareness.\nIntuitively, each latent code controls the detailed\ngeneration of a local text span while different codes\nsummarize diverse high-level patterns in the text.\nThe reconstruction goal is thus to maximize the\nfollowing expectation of log-likelihood:\nLrecon = Ez∼qφ(z|y) log pθ(y|z,x). (4)\n3.3 Discourse Relation Modeling\nIn order to abstract the discourse structure of the\ntext into the latent representations, we design an\nauxiliary discourse-aware objective to embed the\ndiscourse relation information into the discrete la-\ntent codes. We focus on explicit discourse relations\nrather than implicit discourse signals, e.g., sentence\norder (Bosselut et al., 2018), for they cannot ex-\npress the canonical ways adjacent sentences linked\ntogether. We select a set of unambiguous discourse\nmarkers Dfrom PDTB (Prasad et al., 2008) which\nindicate high-level discourse coherence. As sug-\ngested by Prasad et al. (2008), about 90% of ex-\nplicit discourse relations appear either in the same\nsentence or between adjacent sentences. Thus, for\nintra-sentence relations, we parse the sentence and\nextract adjacent EDUs with connected discourse\nmarkers based on appropriate dependency patterns\nfollowing Nie et al. (2019). The processing de-\ntails and annotation examples are provided in the\n§A.3.2.\nThe discourse annotation results of a single pas-\n4212\nsage are formalized as follows:\nA= {(si,ei),di,i+1}S\ni=1, (5)\nwhere Sis the total number of EDUs in y, si/ei are\nthe start/end position of the i-th EDU, and di,i+1\nis the discourse label between the i-th and i+ 1-th\nEDUs.\nNext, we derive the discourse relation model-\ning objective formally. We ﬁrst obtain the aver-\naged latent representation of the i-th EDU ¯hi by\nmean-pooling the corresponding latent embeddings\n[hz\nsi,··· ,hz\nei]. Then we use bi-afﬁne transforma-\ntion to model the relation between two adjacent\nrepresentations and maximize the log probability\nas follows:\np(di,i+1|z) = softmax(¯h⊤\ni Wd¯hi+1 + bd), (6)\nLdisc = Ez∼qφ(z|y)\n|A|−1∑\ni=1\nlog p(di,i+1|z). (7)\n3.4 Autoregressive Prior Modeling\nIn the second stage, we propose to use a Trans-\nformer encoder-decoder to learn the prior distri-\nbution of the discrete latent codes given the input\nprompt by minimizing the KL divergence term in\nEq.(1) with respect to ψ. To facilitate training, we\nutilize the parameters of a pre-trained text encoder\nto initialize the encoder of the prior model and\ntrain the decoder from scratch. The optimization\nobjective is equivalent to maximize the following\nlog-likelihood.\nLprior = Ez∼qφ(z|y)\nL∑\nl=1\nlog p(zl|z<l,x). (8)\nIn practice, we approximate the expectation by tak-\ning argmax from qφ. Compared to sampling, this\napproach reduces the learning variance.\n3.5 Additional Learning Techniques\nIn preliminary experiments, we found two addi-\ntional techniques that essentially guide the model\nto learn meaningful latent abstraction of the text,\ndescribed below.\nEntropy Regularization. We discover that the\npre-trained decoder tends to utilize very few dis-\ncrete codes from the whole code vocabulary, which\nundermines the expressiveness of the discrete bot-\ntleneck. To ensure the model uses the full capacity\nof the discrete bottleneck, we add an entropy-based\nregularization that encourages the diverse selection\nDataset Input len. Output len. Train Val Test\nWritingPrompts 28.4 674.6 273K 15K 15K\nWikiplots 3.4 354.8 101K 5K 5K\nTable 1: Data statistics of WritingPrompts and\nWikiplots include average input/output length and the\nnumber of examples in each data split.\nof discrete latent codes across time steps. Specif-\nically, we calculate the average categorical distri-\nbution ¯p = 1\nL\n∑L\nl=1 softmax(tl) across time steps\nwhere tl is the code logits at the position l. Then,\nwe maximize the entropy of the average distribu-\ntion:\nLentr = −\nK∑\nk=1\n¯pklog ¯pk. (9)\nThe overall objective to be maximized in the ﬁrst\nstage is the weighted sum of the aforementioned\nobjectives: Lrecon + λ1Lentr + λ2Ldisc.\nWarm-Start Training. At the beginning of train-\ning, if the discrete bottleneck does not produce\nmeaningful latent embeddings, the pre-trained gen-\nerator will regard them as injected noise, which\ndegrades the generation performance on the down-\nstream tasks. To mitigate this issue, we ﬁx the\nGumbel temperature to τmax and warm-start the\nmodel on contiguous texts collected from BookCor-\npus (Zhu et al., 2015) by maximizing the following\nobjective: Lrecon + λ1Lentr.\n4 Experiments\n4.1 Datasets\nWe evaluate our model on two open story genera-\ntion datasets, WritingPrompts and Wikiplots. Writ-\ningPrompts (Fan et al., 2018) is a story generation\ndataset collected from Reddit where users compose\nﬁctional stories inspired by short story prompts.\nWikiPlots3 corpus contains story plots of various\ngenres, e.g., movies, novels, which are extracted\nfrom Wikipedia with story titles. The data statis-\ntics are shown in Table 1. More details of data\nprocessing are provided in §A.3.1.\n4.2 Implementation Settings\nWe utilize the state-of-the-art pre-trained text gen-\neration model BART to initialize the components\nof our model, including the posterior encoder, prior\nencoder, and the generator. Due to limited computa-\ntional resources, we use the pre-trained checkpoint\n3www.github.com/markriedl/WikiPlots\n4213\nof BARTbase for our model and other pre-trained\nbaselines we implemented. We use 3-layer 1D\nCNNs with kernel size 4, stride 2, and 0s padding\non both sides that downsamples the text sequence\ninto 8 times shorter discrete latent codes. We set\nthe latent vocabulary size K = 256 as a tradeoff of\nlatent capacity and computational overhead. In pre-\nliminary studies, we found that further increasing\nthe latent vocabulary size requires a longer time\nto converge while receives little pay back in text\ndiversity and quality. We collect 322K contiguous\ntexts from BookCorpus for warm-start training. We\nanneal the Gumbel temperature from τmax = 0.9\nto τmin = 0 .1 in the ﬁrst 20K steps during ﬁne-\ntuning. We set λ1 = 0 .1,λ2 = 0 .1. We adopt\nAdamW (Loshchilov and Hutter, 2019) as the op-\ntimizer. More training details are provided in the\n§A.3.3.\nDuring inference, we randomly sample 1,000\nprompts from each test set for automatic evalua-\ntion. We use nucleus sampling (Holtzman et al.,\n2020) with p = 0.9, a temperature of 1.0, and a\nminimum sequence length of 100 subwords. The\nsame inference settings are applied to all the base-\nlines for fair comparisons.\n4.3 Baselines\nWe compare our model to the following baselines:\nSeq2Seq is a Transformer-based sequence-to-\nsequence model which adopts the same architecture\nas BART without the pre-trained parameters.\nBART (Lewis et al., 2020) is implemented by di-\nrectly ﬁne-tuning the pre-trained BART model on\nthe downstream datasets.\nBART-LM is implemented by ﬁrst post-training\nBART on BookCorpus with the language modeling\nobjective for the same number of steps as ours and\nthen ﬁne-tuning on the downstream datasets. This\nbaseline is proposed to investigate the side effect\nof the language modeling objective on the decoder\nin the warm-start stage.\nBART-CV AEis inspired by recent literature that\nincorporates continuous latent variables to large\npre-trained models (Li et al., 2020), which serves\nas a counterpart to our discrete variable model. We\nimplement a CV AE with modules initialized by\nthe pre-trained parameters of BART. The sampled\nlatent variable is added to the embedding layer\nof the generator’s decoder as our model (same at\nevery position). We adopt the KL thresholding\nstrategy (Kingma et al., 2016) that maximizes the\nKL term with a constant β = 0.1 to mitigate the\nposterior collapse issue.\nAristotelian Rescoring (AR) is a recent work that\nincorporates content-planning in BART on Writ-\ningPrompts (Goldfarb-Tarrant et al., 2020). It ﬁrst\ngenerates an SRL-based plot given the prompt and\nthen revises the plot with several rescorers inspired\nby Aristotle’s writing principle and ﬁnally gener-\nates the long text based on the plot and the prompt.\nWe keep the original model conﬁgurations in the\npaper that adopt BARTlarge for text generation and\nRoBERTalarge for plot rescoring.\n4.4 Automatic Evaluation\nEvaluation Metrics. We adopt the following auto-\nmatic metrics to evaluate the generated stories in\nterms of (1) relevance, (2) diversity and (3) repeti-\ntion. (1-a) BLEU (B-n) measures the precision of\nn-grams of the generated texts which are present\nin the references (Papineni et al., 2002). (1-b) MS-\nJaccard (MSJ-n) measures the similarity between\nthe model distribution and the real data distribu-\ntion by the Jaccard Index between two multi-sets\nof n-grams (Montahaei et al., 2019). (2-a) reverse-\nBLEU (rB-n) measures the recall of generated n-\ngrams which reﬂects the diversity of the generation\nresults (Shi et al., 2018). (2-b) Distinct (D-n) mea-\nsures the fraction of unique n-grams among all\nthe generated n-grams (Li et al., 2016). (3) To-\nken Repetition (rep- l) calculates the fraction of\nthe identical token that occurs in the previous l\ntokens (Welleck et al., 2020).\nResults Analysis. We show the automatic eval-\nuation results on WritingPrompts and Wikiplots\nin Table 2. By comparing DISCO DVT to other\nbaselines, we have the following observations.\nOn both datasets, DISCO DVT outperforms all\nthe baselines in generating texts with higher n-\ngram overlaps and n-gram distribution similarity\nto the reference texts indicated by a higher BLEU\nand MSJ score, respectively.\nIn terms of diversity, DISCO DVT outperforms\nBART and BART-LM by large margins in terms\nof Distinct while slightly underperforms BART-\nCV AE. However, when evaluating diversity jointly\nwith quality, DISCO DVT surpasses BART-CV AE\nwith higher reverse-BLEU score. We further exam-\nine the generated examples of BART-CV AE and\nfound that its diversity mainly comes from generat-\ning more spurious combinations of words that do\nnot appear in the references. This is also evident in\n4214\nModels B-1↑ B-2↑ MSJ-2↑ MSJ-3↑ rB-1↑ rB-2↑ D-4↑ D-5↑ rep-8↓ rep-16↓\nDataset: Wikiplots\nSeq2Seq 13.72 05.66 25.75 17.48 18.14 07.75 69.76 91.59 10.55 22.88\nBART 16.67 06.78 35.07 23.49 19.63 08.20 86.04 96.23 08.69 19.88\nBART-LM 17.63 07.24 36.86 24.55 20.36 08.56 84.91 95.84 08.69 19.86\nBART-CV AE 18.16 06.74 31.35 19.37 20.31 07.74 91.45 98.39 11.00 22.48\nDISCO DVT 20.57** 08.39** 42.48** 27.38** 22.34** 09.30** 90.85 98.08 07.50** 17.34**\nDataset: WritingPrompts\nSeq2Seq 20.93 09.03 42.02 30.55 24.52 10.85 72.04 90.71 10.86 23.46\nBART 21.64 09.41 45.07 32.31 24.95 11.04 77.99 92.32 09.54 21.70\nBART-LM 21.76 09.43 45.46 32.50 24.91 11.01 77.65 92.20 09.48 21.53\nBART-CV AE 20.96 07.94 33.32 21.38 22.80 08.82 88.14 97.31 14.01 25.52\nAR 20.95 08.29 43.70 28.64 22.48 09.06 82.91 92.40 12.74 22.19\nDISCO DVT 24.10** 10.16** 50.00** 34.76** 26.26** 11.29* 84.66 96.00 09.03** 19.74**\nTable 2: Automatic evaluation results on WritingPrompts and Wikiplots. ↑/↓indicates the higher/lower score, the\nbetter. Scores marked with * and ** indicate a signiﬁcance of p< 0.05 and p< 0.01 in the t-test respectively.\nthe drastic performance drop of the MSJ score.\nTo quantitatively evaluate the repetition problem\nof the generated texts, we calculate the token repeti-\ntion ratio in two different ranges. The results show\nthat DISCO DVT consistently outperforms all the\nbaselines in generating texts with less repetition in\nboth local sentences and contents in a longer range.\nCompared to other baselines, AR achieves\nhigher diversity but underperforms in other\nreference-based metrics. We conjecture that the\nmulti-step scorer suffers from the stage-level ex-\nposure bias (Tan et al., 2020) that may impair the\ngeneration performance.\n4.5 Ablation Study\nWe ﬁrst show the ablation study of different train-\ning objectives in Table 3. We ﬁrst observe a certain\nperformance drop when removing Ldisc or Lentr\nduring ﬁne-tuning. Since existing automatic met-\nrics cannot evaluate the discourse structure of texts,\nwe further present a discourse-level evaluation to\nemphasize the effectiveness of Ldisc in Section 4.7.\nThen we highlight the signiﬁcance of warm-start\ntraining to learn meaningful latent codes, as the\nmodel only uses few latent codes and degenerates\nto the vanilla BART model when removing it. We\nalso alter the number of CNN layers and analyze\nthe distribution of code utilization and the genera-\ntion performance in the §A.1.\n4.6 Human Evaluation\nFor human evaluation, we perform pair-wise com-\nparisons with three strong baselines based on\nBART. We choose the Wikiplots dataset on which\nannotators could reach an acceptable agreement\ngiven the relatively shorter passage length. We\nModels B-1↑ MSJ-2↑ D-4↑ rep-8↓\nDISCO DVT 20.57 42.48 90.85 07.50\nw/o Ldisc 19.47 40.80 89.97 07.70\nw/o Lentr 19.30 40.15 89.68 07.84\nw/o Warm-start 18.22 35.48 90.05 09.71\nTable 3: Ablation study of different training objectives\non Wikiplots.\nrandomly sample 100 prompts from the test set of\nWikiplots and obtain the generated texts from the\nthree baselines and ours, resulting in 400 texts in\ntotal. We hired three annotators from Amazon Me-\nchanical Turk to give a preference (win, lose, or tie)\nin terms of coherence and diversity independently.\nCoherence measures whether the story stays on\ntopic and is well-structured with correct logical,\ntemporal, and causal relations. Informativeness\nmeasures whether the story contains informative\ndetails and is engaging on the whole. The ﬁnal de-\ncisions are made by majority voting among the an-\nnotators. As shown in Table 4, DISCO DVT signiﬁ-\ncantly outperforms baselines in both coherence and\ninformativeness, demonstrating that DISCO DVT\neffectively captures the high-level discourse struc-\nture to guide local text realization with details. The\nresults show moderate inter-annotator agreement\n(0.4 ≤κ< 0.6).\n4.7 Discourse-Level Evaluation\nWe conduct a comprehensive evaluation of the gen-\nerated texts on the discourse level. To evaluate\nthe discourse coherence, we extract text pairs con-\nnected by discourse markers from the generated\ntexts and train a classiﬁer to predict the relations.\nWe then analyze the distribution of discourse rela-\ntions and show that DISCO DVT uses more diverse\n4215\nso (50% ), as (25% ), and (10% )\nand (100% )\nwhen (44% ), however (33% ), so (20% )\nas (54% ), so (23% ), after (15% )\nLatent Code IDTop-3 Discourse markers (Percentage)\n131 \n233 \n33 \n62 \n. Gordon, and the detective's daughter, Ms White, must solve an unusual set of grisly murders & disappear \n216 \n<s>The story is about a fictional detective agency, the Re-edition. The agency's chief investigate officer, Sgt\n142 \nances. During these events, the murders occur at the same exact time and place as those at the crime scene,\nso that no one can see the murders. However, things are further complicated when members of the agency\nbegin receiving threats from mysterious strangers who appear to be the target of these attacks. Meanwhile ...\n136 \n131 42 24 \n231 28 233 \n33 62 219 \n219 90 76 \n76 while (38% ), meanwhile (33% ), however (19% )\nFigure 3: A generated example of D ISCO DVT with correspondent latent codes intuitively assigned to text seg-\nments of 8 bpe encodings. We list the top-3 frequent discourse markers for speciﬁc latent codes that account for\nexplicit discourse relations in the text.\nModels Coherence\nWin Lose Tie κ\nDISCO DVT vs. BART 0.53** 0.35 0.12 0.40\nDISCO DVT vs. BART-LM 0.54** 0.40 0.06 0.42\nDISCO DVT vs. BART-CV AE 0.53** 0.34 0.14 0.43\nModels Informativeness\nWin Lose Tie κ\nDISCO DVT vs. BART 0.52** 0.36 0.12 0.42\nDISCO DVT vs. BART-LM 0.49* 0.39 0.11 0.47\nDISCO DVT vs. BART-CV AE 0.53** 0.38 0.09 0.42\nTable 4: Human evaluation results on Wikiplots.\nScores indicate the percentage of Win, Lose, or Tie\nwhen comparing D ISCO DVT with a baseline. κ de-\nnotes Fleiss’ kappa (Fleiss, 1971), which measures the\ninter-annotator agreement. Scores marked with * and\n** denote signiﬁcant differences with p < 0.05 and\np< 0.01 (sign test) respectively.\nConcession Causal Temporal Conjunction0.2\n0.4\n0.6\nMacro-Accuracy\nBART\nBART-LM\nBART-CVAE\nDiscoDVT w/o Ldisc\nDiscoDVT\nFigure 4: Macro-accuracy of classifying text pairs ex-\ntracted from the passages generated by different mod-\nels under the four categories of discourse relations.\ndiscourse patterns as appeared in human texts.\nWe ﬁrst ﬁne-tune a BERT model4 on a dataset\nfor discourse marker classiﬁcation (Nie et al.,\n2019) and then train on text pairs extracted from\nWikiplots to bridge the domain gap.\nWe manually group the discourse markers in D\ninto four categories based on their most frequent\nsenses (Prasad et al., 2008). Because of the severe\nclass imbalance within each group, we use macro-\naccuracy. The results are presented in Figure 4. We\nshow that DISCO DVT achieves higher accuracy on\n4We ﬁne-tune on the complete set for one epoch achieving\n77.4% test accuracy that matches the number reported by Nie\net al. (2019).\nModels BART BART-LM BART-CV AE D ISCO DVT\nKLD ↓ 0.0308 0.0364 0.1700 0.0032\nTable 5: KL divergence (KLD) between the discourse\nrelation distribution by a model and that by the human.\neach category than baselines, especially on tempo-\nral relations. We also notice a minor improvement\nover BART-LM on causal relations since these re-\nlations are systematically harder for the model to\npredict (Nie et al., 2019). Finally, we show the\neffectiveness of discourse relation modeling by the\nnoticeable accuracy drop when ablating Ldisc. We\npresent more details and analysis in the §A.2.1.\nTo analyze the distribution of discourse relations,\nwe ﬁrst show the KL divergence between the distri-\nbution generated by a model and that by the human\nin Table 5. DISCO DVT achieves the lowest KL,\nindicating that the latent codes effectively learn\nthe discourse structure of human texts. We further\ndemonstrate that DISCO DVT can generate more\ndiverse discourse relations in the §A.2.2.\n4.8 Codes Study\nTo analyze the correspondence between discrete\nlatent codes and texts, we present a generated ex-\nample of DISCO DVT with latent codes in Figure\n3. We intuitively assign each discrete latent code\nto the continuous bpe encodings of length 8, which\nmatch the scaling ratio of the CNN. We highlight\nthe discourse markers in each text segment and an-\nalyze the corresponding latent code on the right.\nWe list the top-3 frequent discourse markers for\neach latent code with percentage5. We can see that\nthe latent codes learn meaningful correspondence\nto the discourse relations that guide the model to\ngenerate coherent and logical texts. Besides, we\nalso discover that some latent codes learn patterns\nindicating the beginning or ending of the story, e.g.,\n5For each latent code, the discourse markers are extracted\nfrom 4-grams that repeat at least two times on the test set.\n4216\ncode ID 216’s most frequent 4-gram pattern isThe\nstory is about/set/based. More generation exam-\nples of different models are provided in the §B.\n5 Ethics Statement\nWe observe that the proposed model may some-\ntimes generate inaccurate or ﬁctitious contents due\nto the systematic biases of model pre-training on\nthe web corpora and the open-domain character-\nistics of the story generation datasets. We recom-\nmend the users to carefully examine the ethical\nramiﬁcations of the generated contents in the real-\nworld applications and demonstrations.\n6 Conclusion\nWe presentDISCO DVT, a discourse-aware discrete\nvariational Transformer for long text generation.\nDISCO DVT learns a discrete variable sequence\nthat summarizes the global structure of the text,\nwhich is then applied to guide the step-wise de-\ncoding process to maintain a coherent discourse\nstructure. We further introduce a discourse-aware\nobjective to the discrete latent representations to\nmodel discourse relations within the text. Exten-\nsive experiments demonstrate that the DISCO DVT\ncan generate long texts with better long-range co-\nherence with interpretable latent codes.\nAcknowledgments\nThis work was supported by the National Science\nFoundation for Distinguished Young Scholars (with\nNo. 62125604) and the NSFC projects (Key project\nwith No. 61936010 and regular project with No.\n61876096). This work was also supported by the\nGuoqiang Institute of Tsinghua University, with\nGrant No. 2019GQG1 and 2020GQG0005.\nReferences\nYu Bao, Shujian Huang, Tong Xiao, Dongqi Wang,\nXinyu Dai, and Jiajun Chen. 2021. Non-\nautoregressive translation by learning target categor-\nical codes. CoRR, abs/2103.11405.\nAntoine Bosselut, Asli Celikyilmaz, Xiaodong He,\nJianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018.\nDiscourse-aware neural rewards for coherent text\ngeneration. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers) , pages 173–184. Association for\nComputational Linguistics.\nSander Dieleman, Aäron van den Oord, and Karen Si-\nmonyan. 2018. The challenge of realistic music gen-\neration: modelling raw audio at scale. In Advances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018,\nMontréal, Canada, pages 8000–8010.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 889–898. Association for Computa-\ntional Linguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 2650–2660. Association for Computa-\ntional Linguistics.\nJoseph L. Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological Bulletin ,\n76(5):378–382.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty,\nRalph M. Weischedel, and Nanyun Peng. 2020.\nContent planning for neural story generation with\naristotelian rescoring. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, November\n16-20, 2020 , pages 4319–4338. Association for\nComputational Linguistics.\nJian Guan, Fei Huang, Minlie Huang, Zhihao Zhao,\nand Xiaoyan Zhu. 2020. A knowledge-enhanced\npretraining model for commonsense story genera-\ntion. Trans. Assoc. Comput. Linguistics, 8:93–108.\nJ. Hobbs. 1985. On the coherence and structure of dis-\ncourse.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-\nical reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nDaniel Jurafsky and James H. Martin. 2000. Speech\nand language processing - an introduction to natural\nlanguage processing, computational linguistics, and\nspeech recognition. Prentice Hall series in artiﬁcial\nintelligence. Prentice Hall.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models\n4217\nusing discrete latent variables. In Proceedings of the\n35th International Conference on Machine Learning,\nICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 2395–2404. PMLR.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In 2nd International\nConference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Con-\nference Track Proceedings.\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling. 2016.\nImproved variational inference with inverse autore-\ngressive ﬂow. In Advances in Neural Information\nProcessing Systems, volume 29. Curran Associates,\nInc.\nWei-Jen Ko and Junyi Jessy Li. 2020. Assessing dis-\ncourse relations in language generation from GPT-\n2. In Proceedings of the 13th International Confer-\nence on Natural Language Generation, pages 52–59,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiu-\njun Li, Yizhe Zhang, and Jianfeng Gao. 2020. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 4678–4699. Association for Computa-\ntional Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, San Diego California, USA, June 12-17, 2016 ,\npages 110–119. The Association for Computational\nLinguistics.\nJiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.\nA hierarchical neural autoencoder for paragraphs\nand documents. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing of the Asian Fed-\neration of Natural Language Processing, ACL 2015,\nJuly 26-31, 2015, Beijing, China, Volume 1: Long\nPapers, pages 1106–1115. The Association for Com-\nputer Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nChris J. Maddison, Andriy Mnih, and Yee Whye Teh.\n2017. The concrete distribution: A continuous re-\nlaxation of discrete random variables. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations ,\npages 55–60.\nEhsan Montahaei, Danial Alihosseini, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diver-\nsity and quality in text generation models. CoRR,\nabs/1904.03971.\nAllen Nie, Erin Bennett, and Noah D. Goodman. 2019.\nDissent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the 57th\nConference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- Au-\ngust 2, 2019, Volume 1: Long Papers , pages 4497–\n4510. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311–318. ACL.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bon-\nnie Webber. 2008. The Penn Discourse TreeBank\n2.0. In Proceedings of the Sixth International\nConference on Language Resources and Evaluation\n(LREC’08), Marrakech, Morocco. European Lan-\nguage Resources Association (ELRA).\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAli Razavi, Aäron van den Oord, and Oriol Vinyals.\n2019. Generating diverse high-ﬁdelity images\nwith VQ-V AE-2. In Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 14837–14847.\nJason Tyler Rolfe. 2017. Discrete variational autoen-\ncoders. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings . Open-\nReview.net.\n4218\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learn-\ning, CoNLL 2019, Hong Kong, China, November\n3-4, 2019, pages 843–861. Association for Compu-\ntational Linguistics.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron C. Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating di-\nalogues. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence, February 4-9,\n2017, San Francisco, California, USA, pages 3295–\n3301. AAAI Press.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei\nXu, and Xiaoyan Zhu. 2019. Long and diverse text\ngeneration with planning-based hierarchical varia-\ntional model. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7,\n2019, pages 3255–3266. Association for Computa-\ntional Linguistics.\nDinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun\nChen, Xin Wang, Jianfeng Gao, and Lawrence Carin.\n2019. Towards generating long and coherent text\nwith multi-level latent variable models. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 2079–2089. Association for Computa-\ntional Linguistics.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Toward diverse text generation with\ninverse reinforcement learning. In Proceedings\nof the Twenty-Seventh International Joint Confer-\nence on Artiﬁcial Intelligence, IJCAI 2018, July 13-\n19, 2018, Stockholm, Sweden , pages 4361–4367. ij-\ncai.org.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P.\nXing, and Zhiting Hu. 2020. Progressive generation\nof long text. CoRR, abs/2006.15720.\nAäron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. 2017. Neural discrete representation\nlearning. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 6306–6315.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nSam Wiseman, Stuart M. Shieber, and Alexander M.\nRush. 2018. Learning neural templates for text gen-\neration. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 3174–3187. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGATRON-CNTRL: control-\nlable story generation with external knowledge us-\ning large-scale language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 2831–2845. Associ-\nation for Computational Linguistics.\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling.\nIn The Thirty-Third AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2019, The Thirty-First Innova-\ntive Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 7378–7385. AAAI Press.\nTiancheng Zhao, Kyusong Lee, and Maxine Eskénazi.\n2018. Unsupervised discrete sentence representa-\ntion learning for interpretable neural dialog genera-\ntion. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018, Vol-\nume 1: Long Papers, pages 1098–1107. Association\nfor Computational Linguistics.\nTiancheng Zhao, Ran Zhao, and Maxine Eskénazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 654–664. Association\nfor Computational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\n4219\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\n4220\nA Appendices\nA.1 Ablation Study on CNN Layers\nSince the CNN layers are essential structures in\nour model for abstracting high-level features of the\ntext, we conduct an ablation study to see the effect\nof varying the number of CNN layers. Figure 5\n(a) plot the distribution of code utilization of the\ngenerated examples when using different number\nof CNN layers. The code utilization is calculated\nas the type number of used latent codes divided\nby the length of the latent codes. A high code uti-\nlization reﬂects a sufﬁcient utilization of the whole\ninformation capacity of the variational bottleneck\nwhere each latent code learns more meaningful in-\nformation of distinct patterns in the text (Kaiser\net al., 2018).\nWe observe that the code utilization is maxi-\nmized when using 3 CNN layers, while either in-\ncreasing or decreasing the number of CNN layers\nleads to a decline in the code utilization. We con-\njecture that when using shallow CNN layers, each\nlatent code only captures local text features and\ncannot learn high-level information for long text\nreconstruction. While when using more CNN lay-\ners, a larger receptive ﬁeld for each latent code\nincreases its modeling complexity for longer text\nchunks. As shown in Figure 5 (b), we present the\ngeneration performance in MSJ-2 and observe a\nsimilar tendency to the results of the average code\nutilization.\n(a) (b)\n1 2 3 4\n# CNN layers\n40%\n60%\n80%\n100%\nCode Util.\n1 2 3 4\n# CNN layers\n0 . 38\n0 . 40\n0 . 42\nMSJ-2\nFigure 5: Results of our model with a different number\nof CNN layers. (a) Distribution of code utilization. (b)\nGeneration performance in MSJ-2.\nA.2 Details on Discourse-Level Evaluation\nA.2.1 More Analysis on Discourse Coherence\nWe present ﬁne-grained accuracies for different\ndiscourse markers under the four high-level cate-\ngories in Table 6. We observe that our proposed\nDISCO DVT achieves the highest accuracy on six\nout of ten classes of discourse markers comparing\nto the chosen baselines. Moreover, DISCO DVT’s\nperformance is more balanced across different dis-\ncourse markers, while baseline models have low ac-\ncuracy in speciﬁc types of discourse markers, e.g.,\nBART achieves 30% accuracy onbefore. Finally,\nwe show that even without the discourse-aware ob-\njective, the proposed discrete bottleneck learns to\nabstract some commonly used discourse relations\nand achieves the highest accuracy on and and also.\nA.2.2 Evaluating Discourse-Level Diversity\nTo quantitatively understand the discourse diversity\nof the generated texts, we propose to assess the\ndiversity of discourse relations in the generated\ntexts.\nWe ﬁrst calculate the proportion of different dis-\ncourse relations and discourse markers used in the\ntexts generated by DISCO DVT on Wikiplots and\nshow the results in Figure 6. The model shows a\ndiverse preference for different discourse relations\nthat enrich the discourse structure of the generated\npassages.\nWe further compare the utilization percentage\nof discourse relations across different models and\nshow the results in Table 7. DISCO DVT exhibits\nmore discourse diversity than the other baselines\nindicated by a higher entropy score and resem-\nbles the golden distribution most closely. Specif-\nically, BART and BART-CV AE mainly generate\ncommonly used discourse markers, such as and in\nconjunction, while express less other complicated\nrelations, such as temporal and causal relations.\nWhen ablating the discourse relation modeling ob-\njective, we also observe a decline in discourse di-\nversity.\nConcession (1.9%)\nCausal (2.6%)\nTemporal\n(28.9%)\nConjunction\n(66.6%)\nalthough\nso\nbecause\nbefore\nafter\nas\nthen\nstill\nalso\nand\nFigure 6: Percentage of different discourse markers and\nhigh-level discourse relations in the generated texts of\nDISCO DVT on Wikiplots.\nA.3 Experimental Details\nA.3.1 Data Preprocessing\nBookCorpus: We collect a subset of 322k contigu-\nous text segments with at least 512 bpe subwords\nfrom the BookCorpus. We reserve the ﬁrst 512\nsubwords of each example for training.\n4221\nModel Concession Causal Temporal Conjunction\nalthough because so before after as then and also still\nBART 0.290 0.463 0.450 0.300 0.615 0.564 0.730 0.839 0.511 0.389\nBART-LM 0.333 0.501 0.460 0.500 0.620 0.486 0.667 0.844 0.529 0.500\nBART-CV AE 0.220 0.412 0.429 0.375 0.529 0.377 0.721 0.755 0.585 0.483\nDISCO DVT 0.377 0.548 0.435 0.674 0.623 0.547 0.731 0.842 0.580 0.550\nw/o Ldisc 0.317 0.361 0.425 0.458 0.612 0.549 0.667 0.846 0.612 0.434\nTable 6: Fine-grained classiﬁcation accuracies for different discourse markers under four high-level categories.\nModels Conc. Caus. Temp. Conj. Entr. ↑\nGolden 2.28% 2.56% 29.06% 66.10% 1.17\nBART 0.84% 1.66% 23.37% 74.13% 0.97\nBART-LM 0.56% 1.66% 24.30% 73.49% 0.96\nBART-CV AE 0.20% 1.70% 14.07% 84.02% 0.73\nDISCO DVT 1.86% 2.59% 28.94% 66.61% 1.15\nw/o Ldisc 1.74% 2.10% 27.44% 68.73% 1.10\nTable 7: Percentage of four categories of discourse\nrelations, i.e., Concession, Causal, Temporal and\nConjunction in the generated samples of different mod-\nels and the golden references. The Entropy is calcu-\nlated over these categories in bits.\nWikiplots: We use the ofﬁcial split of Wikiplots.\nWe preserve a maximum number of 16 subwords\nfor the input title and 512 subwords for the story,\nrespectively.\nWritingPrompts: We use the ofﬁcial split of Writ-\ningPrompts. We strip the accents and the newline\nmarkers. We preserve a maximum number of 64\nsubwords for the input prompt and 512 subwords\nfor the story, respectively.\nA.3.2 Details on Discourse Annotations\nPreparation\nWe focus on a subset of unambiguous discourse\nmarkers Dincluding although, so, because, before,\nafter, as, then, and, also and still. An instance of\ndiscourse annotation consists of a discourse con-\nnective linking a pair of arguments where the ﬁrst\nargument (Arg1) is the main clause and the sec-\nond argument (Arg2) is syntactically bound to the\nconnective.\nPrasad et al. (2008) found that 61% of discourse\nmarkers and the two arguments appear (with quite\nﬂexible order) in the same sentence, and 30% link\none argument to the immediately previous sentence.\nDue to their high coverage, we focus on automat-\nically extracting these two types of discourse pat-\nterns.\nWe resort to universal dependency grammar that\nprovides sufﬁcient information to extract the dis-\ncourse markers and the associated two arguments.\nFor each discourse marker of interest, we follow\nNie et al. (2019) and use appropriate dependency\npatterns to extract intra-sentence discourse rela-\ntions as shown in Figure 7. In each example, Arg1\nis in italics, Arg2 is in boldface, and the discourse\nmarker is underlined.\nWe ﬁrst parse each sentence into a dependency\ntree with the Stanford CoreNLP toolkit (Manning\net al., 2014). Then we identify discourse markers\nand the spans of Arg1 and Arg2 based on the depen-\ndency patterns and ensure that the two arguments\ntogether with the marker cover the whole sentence.\nIf there are multiple discourse markers in one sen-\ntence, we preserve the one that divides the sentence\nmore evenly. If the parsing results reveal that there\nis only one argument in the sentence that connects\nwith the discourse marker, we heuristically label\nthe previous adjacent sentence as Arg1 (see the\nnext sentence pattern in Figure 7).\nFor each story in the dataset, we ﬁrst split it into\nindividual sentences and then apply the above steps\nfor extraction until all the adjacent EDUs (either\na complete sentence or a parsed sub-sentence) are\nlabeled with proper relations. The label candidates\nare the combination of discourse markers from D\nand two possible directions that indicate how these\ntwo text spans are linked together. For example,\nin Figure 7 (a), the label of the text pair will be\nalthough_arg2_arg1 according to the order\nof the two arguments. If no discourse relation is\nidentiﬁed from the pair of text spans, they are la-\nbeled with unknown.\nA.3.3 Details on Training Settings\nTo improve the reproducibility of our model, we\nprovide the detailed training settings in this section.\nWe implement our codes based on the reposi-\ntory of Huggingface’s Transformers (Wolf et al.,\n2020). For the posterior network, we initialize\nthe Transformer encoder with the pre-trained pa-\nrameters of the encoder of BARTbase (82M param-\n4222\n(a)Although[61 billion people have perished,]Arg2[Paul’s prescient visions indicate that\nthis is far from the worst possible outcome for humanity.]Arg1\nmark advcl\n(b)[Father Matthew decides to send him to Rome,]Arg1so[he can attend an exorcism class\ntaught by his friend]Arg2\nadvcl\nmark\n(c)Because[the detectives do not believe her]Arg2[, she decides to contact Gerard herself.]Arg1\nmark\nadvcl\n(d)[He damages the stabilizer]Arg1before[his teammates can tie him up in the shuttle.]Arg2\nmark\nadvcl\n(e)After[Cho calms him down,]Arg2[he follows the captain’s order to ﬁx the drive.]Arg1\nmark advcl\n(f)As[his powers drain,]Arg2[Luthor wishes the experience to continue.]Arg1\nmark\nadvcl\n(g)[Mason destroys the chips,]Arg1then[surrenders to Hummel.]Arg2\nparataxis\nadvmod\n(h)[Nick blames Jerry for forcing him into the profession]Arg1and[asks him to get away.]Arg2\nconj\ncc\n(i)[Kenny, revealed to be alive and an undercover FBI agent.]Arg1[He alsoimplies that\nLampone is another undercover agent.]Arg2\nnext sentence*\n(j)[She strikes out across the dense sawgrass marshes]Arg1still[miles from home.]Arg2\nadvmod\ndep\nFigure 7: Dependency patterns of the 10 discourse markers in Dwith annotated examples from Wikiplots. Labels\nabove the arrows are grammatical relations deﬁned in Manning et al. (2014).*: Thenext sentence pattern identiﬁes\nthe adjacent two sentences as Arg1 and Arg2.\neters). The generator model is initialized with\nthe pre-trained checkpoint of BARTbase (140M pa-\nrameters). Other randomly initialized parameters,\nincluding the CNN layers, the transposed CNN\nlayers, the latent code embeddings, etc., sum up\nto 2.5M parameters. The prior network is also a\nTransformer encoder-decoder that uses the same\narchitecture as BARTbase (140M parameters).\nWarm-Start Training\nWe collect 322K contiguous texts from Book-\nCorpus (Zhu et al., 2015) and keep the ﬁrst 512 bpe\nsubwords of each example for training. Since the\nwarm-start training aims at initializing the latent\nembeddings for reconstructing the target text, we\n4223\ndo not feed any input to the encoder. We use a ﬁxed\nGumbel temperature of 0.9 and a ﬁxed learning rate\nof 1e-4. We use a batch size of 4 and a gradient\naccumulation step of 4 and train on the collected\ndata for one epoch which takes about 7 hours on 1\nGeForce RTX 2080 (11G).\nFine-tuning\nFor ﬁne-tuning the generator and the poste-\nrior network for text reconstruction, we anneal\nthe Gumbel temperature from τmax = 0 .9 to\nτmin = 0 .1 using exponential decay schedule\nwhere the Gumbel temperature τ at step T is:\nmax[τmin,τmax×exp(−10−4 ×T)]. We linearly\ndecrease the learning rate from 1e-4 to 0 through-\nout the ﬁne-tuning. We use a batch size of 4 and a\ngradient accumulation step of 4. We ﬁne-tune for\nﬁve epochs on Wikiplots and one epoch on Writing-\nPrompts, which takes about 12 hours and 6 hours\non 1 GeForce RTX 2080 (11G), respectively.\nFor ﬁne-tuning the prior network, we initial-\nize the encoder with the pre-trained parameters\nof the encoder of BARTbase. We linearly decrease\nthe learning rate from 1e-4 to 0 during training.\nThe maximum target sequence length is set to\nMaxLength = 64 . We use a batch size of 128\nand a gradient accumulation step of 8. We ﬁne-\ntune the model for 100 epochs which takes about\n13 hours on 1 GeForce RTX 2080 (11G). During\ninference, we randomly sample a sequence of la-\ntent codes from the prior network autoregressively\nand set the minimum sequence length to 38 and 44\nfor Wikiplots and WritingPrompts, respectively.\nWe present the hyper-parameter search space in\nTable 8. The number of hyper-parameter search\ntrials was 10. We adopt a manual search to select\nthe hyper-parameters, and the selection criterion\nwas BLEU-1 on the validation set when ﬁne-tuning\non the downstream datasets.\nHyper-parameter Search Space\nLearning Rate choice[8e-5, 1e-4, 2e-4]\nTraining Epoch choice[1, 2, 3, 4, 5]\nBatch Size choice[4, 8, 16]\nInput Length choice[16, 32, 64]\nOutput Length 512\nLatent V ocabulary Size choice[32, 64, 128, 256, 512]\nTop-p choice[0.8, 0.9, 1.0]\nMaximum Gradient Norm 1.0\nOptimizer AdamW\nEpsilon (for AdamW) 1e-8\nTable 8: Hyper-parameter search space of DISCO DVT\nduring ﬁne-tuning. choice indicates that the listed num-\nbers will be chosen with the same probability.\nB Generation Examples\nWe present the generation examples from different\nmodels on Wikiplots in Table 9. We observe that\nbaselines including BART-LM and BART-CV AE\nsuffer from repetition problem as the contexts grow\nlonger while DISCO DVT maintains a better dis-\ncourse structure of the text by the global discrete la-\ntent codes and stays on topic throughout the whole\nstory (indicated by the frequent usage of the key-\nword “Probe 7”). Finally, we observe that existing\nmodels have potential spelling mistakes when gen-\nerating complicated entities in the open-domain\nscenario. This phenomenon is severe in BART\nas in the example, which even hinders readability.\nWe suggest that a future direction to mitigate this\nissue is to construct entity plots by substituting en-\ntity mentions with placeholders and apply the copy\nmechanism.\n4224\nTitle Probe 7, Over and Out\nBART In a ﬁeld hospital, dr DavidMihalya discovers a corpse in a nearby car. His search leads him to a facility\nwhere his own corpse is being guarded by a secret military unit. Mihalai visits the unit and discovers\nthat the building is a military barracks built in 1940, and that a number of the soldiers are killed in an\nexplosion. Muhalya is taken to the military base to be interrogated, but is brought into conﬂict with his\ncommanding ofﬁcer, Captain James Mihalia, who also appears to be involved. Mohalya asks Mihali\nto give him a call as a replacement for Mihail. Mishali says that he will be sent to investigate, and he\ngoes to see Mihanya. Muthali says they were sent to an undisclosed location when he was a child, and\nthey are now in the same military camp. Mighali and Mihaela begin to see each other, and the two are\nseen to have a casual sex. Makhalya says she was \"happy about her\" as Mihiya said he never saw her.\nMumbali is shocked at Mihmya’s response and tries to get her to get to knowMihyasa, but she rejects\nhim.\nBART-LM In a ﬁeld where men are in labor and women are selling women, a research ﬁrm is in session with an\neccentric scientist who is trying to determine a cure for a blood disorder. The scientist goes to a hospital\nto visit the woman and discovers that her liver is growing to a dangerously high size, and she has a\nblood clot. The technician explains that she has to be careful not to leave the woman to go through an\nexperimental procedure to remove the clot, but she is unable to live with the strain and her liver can’t\nsurvive without a transplant. After the procedure, the team ﬁnds that the woman’s liver has enlarged to\na lethal size. However, in the process, the patient gets out of the hospital and is sent to a large hospital\nin the countryside where he meets with a young woman named Elizabeth. Elizabeth does not recognise\nElizabeth, but Elizabeth quickly recognises Elizabeth’s condition. Elizabeth begins to suspect Elizabeth\nand Elizabeth’s secret relationship and believes Elizabeth to be linked to Elizabeth.\nBART-CV AE The series tells the story of Peter Smith, a boy from a small town, who fell in love, and decided to ﬁnd\nthe way to end the romance. Peter and his brother and friend, Mike, a successful lawyer, return to their\ntown, and have the prospect of dating and marrying. After Peter discovers that Mike is with Paul, he\ndecides to take his sister, Laura, with him and give them and his friends the word \"love\" and so on,\nuntil his sister says \"Yes\". It is up to Peter to resolve this dispute, but to reach an end with the men as\ngood-luck, and help people in other ways. After a short visit to the church, Paul discovers that Pete and\nLaura are a couple, and has made a difference of how people think. He becomes concerned and asks\nLaura about Peter, so as to make her believe that she is, in fact, married to Peter. In order to resolve\nthe situation, Peter uses a combination of the words \"Love\" and \"Dollar\" (for \"love\") in the name of\na woman, but, for Peter, he can only say the words at the end and, when Peter, Laura and Peter end\nup in a bed in Peter’s home, as far as Peter sees, is lying on the bed. Peter thinks about what to do at\nthe hospital, and Laura tells him to go look at Pete. Peter asks Peter about Pete and goes, and Pete\nmentions that David’s wife is being married with two children, but Peter says that the children can not\nbe married, or that he isn’t with her for such a short period of time. Peter goes over to Peter’s house,\nand he asks Peter for a second time.\nDISCO DVT The story begins in the near future in the year 2009. An alien race called the Invaders appear and\nbreak the Galactic Federation and destroy the Cardassians from their planet. A group of mercenaries\ncalled the \"Blue Angels of New Generation\" are tasked with trying to destroy the Deﬁant. They use\na device called a Probe 7 in order to kill the Invaders. The probe must send a distress signal to the\nFederation starship, commanded by the \"Sister’s\" pilot, ProfessorMoriarty (voiced by Arthur Fairchild\nfrom the ﬁlm The Secret Intelligence Service). Professor Moriory, along with the Enterprise, arrive and\nsuccessfully intercept the Invaders while the ship remains on orbit and attacks the Federation Fleet on\nthe planet. The Blue Angels then inﬁltrate the ﬂeet to set a trap for the Invaders, hoping that they will\ndestroy the ﬂeet. As the Blue Angels use this technology, the Red Angels of The Invaders retaliate by\ndestroying the Enterprise before they reach the Federation ﬂeet. The Invaders then proceed to destroy\nall of Earth’s radio stations and ﬁre theProbe 7 into the \"Dominic Channel\". Professor Moriorthy then\nuses Probe 7 to gain access to his ship’s central control, which contains an orbiting outpost called the\nBlack Mesa.\nTable 9: We present the ﬁrst ten sentences of the stories generated by different models. We highlight the obvious\nrepetitions, unreasonable descriptions, and potential spelling mistakes in the generated stories. The generated\nkeywords that match the title are presented in boldface.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7494587898254395
    },
    {
      "name": "Transformer",
      "score": 0.6542827486991882
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5601454973220825
    },
    {
      "name": "Language model",
      "score": 0.4820050299167633
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4791395366191864
    },
    {
      "name": "Decoding methods",
      "score": 0.4626098573207855
    },
    {
      "name": "Latent variable",
      "score": 0.456962525844574
    },
    {
      "name": "Natural language processing",
      "score": 0.42640161514282227
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3438646197319031
    },
    {
      "name": "Algorithm",
      "score": 0.24475574493408203
    },
    {
      "name": "Mathematics",
      "score": 0.12192761898040771
    },
    {
      "name": "Voltage",
      "score": 0.09706586599349976
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 17
}