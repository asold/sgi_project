{
  "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering",
  "url": "https://openalex.org/W4389010441",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5007848957",
      "name": "Angus Addlesee",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5091646178",
      "name": "Weronika Sieińska",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5003439010",
      "name": "Nancie Gunson",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5087944836",
      "name": "Daniel Hernández García",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5005673019",
      "name": "Christian Dondrup",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5010949145",
      "name": "Oliver Lemon",
      "affiliations": [
        null,
        "Centre de Robotique",
        "Heriot-Watt University Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4366597729",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2594894191",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2972345724",
    "https://openalex.org/W3177271673",
    "https://openalex.org/W3200895474",
    "https://openalex.org/W4384520696",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W1993567041",
    "https://openalex.org/W1511137888",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4321340837",
    "https://openalex.org/W4288345685",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385822365",
    "https://openalex.org/W4223956331",
    "https://openalex.org/W1792761184",
    "https://openalex.org/W4312985860",
    "https://openalex.org/W4285290300",
    "https://openalex.org/W4384261711",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W2468710617",
    "https://openalex.org/W763498075",
    "https://openalex.org/W4389520756",
    "https://openalex.org/W4389009543",
    "https://openalex.org/W4322760437",
    "https://openalex.org/W1542318638",
    "https://openalex.org/W4224131719",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3045153201",
    "https://openalex.org/W2886871786",
    "https://openalex.org/W2483402000",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W2318802957",
    "https://openalex.org/W4200253303",
    "https://openalex.org/W2250263036",
    "https://openalex.org/W3094531461",
    "https://openalex.org/W3162010807",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4226047536"
  ],
  "abstract": "Angus Addlesee, Weronika Sieińska, Nancie Gunson, Daniel Hernandez Garcia, Christian Dondrup, Oliver Lemon. Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue. 2023.",
  "full_text": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 229–241\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n229\nMulti-party Goal Tracking with LLMs: Comparing Pre-training,\nFine-tuning, and Prompt Engineering\nAngus Addlesee,\nWeronika Siei´nska, Nancie Gunson,\nDaniel Hernandez Garcia, Christian Dondrup\nHeriot-Watt University, Edinburgh\n{a.addlesee, w.sieinska, n.gunson,\nd.hernandez_garcia, c.dondrup}@hw.ac.uk\nOliver Lemon\nHeriot-Watt University, Edinburgh\nAlana AI\nEdinburgh Centre for Robotics\no.lemon@hw.ac.uk\nAbstract\nThis paper evaluates the extent to which cur-\nrent Large Language Models (LLMs) can cap-\nture task-oriented multi-party conversations\n(MPCs). We have recorded and transcribed\n29 MPCs between patients, their companions,\nand a social robot in a hospital. We then anno-\ntated this corpus for multi-party goal-tracking\nand intent-slot recognition. People share goals,\nanswer each other’s goals, and provide other\npeople’s goals in MPCs – none of which occur\nin dyadic interactions. To understand user goals\nin MPCs, we compared three methods in zero-\nshot and few-shot settings: we fine-tuned T5,\ncreated pre-training tasks to train DialogLM\nusing LED, and employed prompt engineering\ntechniques with GPT-3.5-turbo, to determine\nwhich approach can complete this novel task\nwith limited data. GPT-3.5-turbo significantly\noutperformed the others in a few-shot setting.\nThe ‘reasoning’ style prompt, when given 7%\nof the corpus as example annotated conversa-\ntions, was the best performing method. It cor-\nrectly annotated 62.32% of the goal tracking\nMPCs, and 69.57% of the intent-slot recogni-\ntion MPCs. A ‘story’ style prompt increased\nmodel hallucination, which could be detrimen-\ntal if deployed in safety-critical settings. We\nconclude that multi-party conversations still\nchallenge state-of-the-art LLMs.\n1 Introduction\nSpoken Dialogue Systems (SDSs) are increasingly\nbeing embedded in social robots that are expected\nto seamlessly interact with people in populated\npublic spaces like museums, airports, shopping\ncentres, or hospital waiting rooms (Foster et al.,\n2019; Tian et al., 2021; Gunson et al., 2022). Un-\nlike virtual agents or voice assistants (e.g. Alexa,\nSiri, or Google Assistant), which typically have\ndyadic interactions with a single user, social robots\nare often approached by pairs and groups of indi-\nviduals (Al Moubayed et al., 2012; Moujahid et al.,\n2022). Families may approach a social robot in\n1 U1: What time was our appointment?\n2 U2: We have an appointment at 10.30pm.\n3 U1: Ok.\nTable 1: An example extract from our new corpus. This\nexample illustrates that people complete other user’s\ngoals in an MPC. The system must understand that U1’s\nquestion was answered by U2, and it does not need to\nanswer this question as if it was a dyadic interaction.\nFurther annotated examples can be found in Table 3.\na museum, and patients are often accompanied by\na family member when visiting a hospital. In these\nmulti-party scenarios, tasks that are considered triv-\nial for SDSs become substantially more complex\n(Traum, 2004; Zhong et al., 2022; Addlesee et al.,\n2023). In multi-party conversations (MPCs), the\nsocial robot must determine which user said an ut-\nterance, who that utterance was directed to, when\nto respond, and what it should say depending on\nwhom the robot is addressing (Hu et al., 2019; Gu\net al., 2021, 2022a). These tasks are collectively\nreferred to as “who says what to whom” in the\nmulti-party literature (Gu et al., 2022b), but these\ntasks alone provide no incentive for a system to ac-\ntually help a user reach their goals. State of the art\n“who says what to whom” systems can, therefore,\nonly mimic what a good MPC looks like (Addlesee\net al., 2023), but for practical systems we also need\nto know what each user’s goals are. We therefore\npropose two further tasks that become substantially\nmore complex when considered in a multi-party\nsetting: goal tracking and intent-slot recognition\n(Addlesee et al., 2023).\nDialogue State Tracking (DST) is a well-\nestablished task (Lee et al., 2021; Feng et al., 2022)\nthat is considered crucial to the success of a di-\nalogue system (Williams et al., 2016). DST cor-\npora are abundant (Henderson et al., 2014a,b), but\nthey only contain dyadic conversations. No cor-\npus exists containing MPCs with goal tracking or\n230\nintent-slot annotations, yet there are important dif-\nferences. Consider the example in Table 1 (from\nour new corpus, detailed in Section 2). In turn 1,\nwe can identify that User 1 (U1) wants to know\ntheir appointment time. Before the social robot had\ntime to answer, User 2 (U2) answered in turn 2.\nThis obviously does not occur in a dyadic interac-\ntion, yet this understanding is essential for natural\nsystem behaviour. The SDS must determine that\nit should not repeat the answer to the question, so\ndata must be collected to learn this. Other major\ndifferences exist too. For example, current DST\ncorpora do not contain a concept of ‘shared goals’\n(Eshghi and Healey, 2016). If two people approach\na café counter, the barista must determine whether\nthe two people are separate (two individuals want-\ning to get coffee), or together (two friends with the\nshared goal to get coffee) (Keizer et al., 2013). The\ninteraction changes depending on this fact, it would\nbe unusual to ask “are you paying together” to two\nindividuals. Shared goals can commonly be iden-\ntified through explicit dialogue. For example, the\nuse of ‘we’ in “We are looking for the bathrooms”.\nSimilar to answering each other’s questions, people\nmay also ask questions on behalf of others. In our\ncorpus, a person said “ARI, the person that I’m\naccompanying feels intimidated by you, and they’d\nlike to know where they can eat”.\nIn this paper, we present several contributions.\n(1) We collected a corpus of multi-party interac-\ntions between a social robot and patients with their\ncompanions in a hospital memory clinic. (2) This\ncorpus was annotated for the standard “who says\nwhat to whom” tasks, but also for multi-party goal\ntracking and intent-slot recognition. We followed\ncurrent DST annotation instructions, tweaked to en-\nable annotation of multi-party phenomena (detailed\nin Section 2). (3) We then evaluated Large Lan-\nguage Models (LLMs) on these two new tasks us-\ning our collected corpus. Models were pre-trained,\nfine-tuned, or prompt engineered where applica-\nble (detailed in Section 3). It is not possible to\ncollect enormous corpora from patients in a hos-\npital, so models were evaluated in zero-shot and\nfew-shot settings. We found that the GPT-3.5-turbo\nmodel significantly outperformed others on both\ntasks when given a ‘reasoning’ style prompt.\n2 Dataset and Tasks\nFor the initial data collection, we partnered with\na hospital in Paris, France, and secured ethical ap-\nproval as part of the EU SPRING project 1. We\nthen recorded, transcribed, translated (from French\nto English), anonymised, and annotated 29 multi-\nparty conversations (774 turns). These MPCs were\nbetween patients of the memory clinic, their com-\npanion (usually a family member), and a social hu-\nmanoid robot created by PAL Robotics called ARI\n(Cooper et al., 2020). We hired a professional trans-\nlator to avoid machine translation errors, and to\nenable faster experimentation as we are not French\nspeakers. Future work based upon the findings in\nthis paper will be evaluated in both English and\nFrench.\nWe used a wizard-of-oz setup as this task is\nnew, and we required this data to design a multi-\nparty SDS for use in the hospital. A robot oper-\nator was therefore controlling what ARI said by\nselecting one of 31 response options (task-specific\nanswers and some common responses like “yes”,\n“no”, “please”, “thank you”, and “I don’t know”).\nFollowing our previously published data collec-\ntion design (Addlesee et al., 2023), each partici-\npant was given one or two goals, and asked to con-\nverse with ARI to try to achieve their goal. Both\nparticipants were given the same goals in some\ncases to elicit dialogues containing ‘shared goal’\nbehaviour. In order to encourage lexical diversity,\nwe provided pictograms to give each participant\ntheir goals. For example, if we told the patient that\nthey want a latte, they would likely use the specific\nword “latte” (Novikova et al., 2016), so we instead\ngave the participants pictograms as seen in the top-\nright of Figure 1. This worked as people didn’t just\nask for coffee when given this image, some asked\nfor hot chocolate or herbal tea instead.\nIn this paper, we evaluated each model on both\nmulti-party goal tracking, and multi-party intent-\nslot recognition. These are two related, yet distinct\ntasks. If ARI asked the user “Are you hungry?”,\nand the user responded “yes”, then the intent of\nthat turn is an affirmation, but the user’s goal is\nalso established as wanting to eat. As explained in\nSection 1, standard DST annotation schemes are de-\nsigned for dyadic interactions, which do not enable\nannotation of multi-party behaviours. Each turn\nis annotated with its intent and slot values where\napplicable, but goal annotations require both the\ngoal and the user whose goal is being established.\nWhen a goal is detected in a dyadic interaction, no\nuser information is needed as there is only a single\n1https://spring-h2020.eu/\n231\nFigure 1: A sample of the pictograms used to represent\nuser goals, given to patients and companions. These\nelicited dialogues without restricting vocabulary.\nuser. In multi-party interactions, multiple users can\nhave multiple active goals. These goals may be dif-\nferent, they may be shared (see Table 2), users may\nanswer each other’s goals (see Table 1), and one\nuser may provide another user’s goal, for example\nby saying “My wife would love a coffee”.\nAn annotated extract from an MPC in our col-\nlected corpus can be found in Table 2. In turn 1, U1\nstates that “we’d like a coffee”, indicating that U1\nand their companion U2 would both like a coffee.\nThis turn is annotated with two intents: greet\n(due to the “hello”), and request. This request\nintent has a slot value to indicate that the request\nis for a beverage – coffee. The goal tracking anno-\ntation signifies that a goal has been established in\nthis turn with ‘G’. The goal is shared by ‘U1+U2’,\nand their goal is to drink a coffee. In turn 2, ARI\nresponds informing both users where the café is,\nhence the inform intent annotation. The goal\ntracking annotation is the same as turn 1, but starts\nwith ‘AG’ (for ‘answer-goal’) instead of simply\n‘G’. This indicates that this goal has been answered,\nwhich is critical knowledge for the system to track\nwhich goals remain open. In this example, the\ngoal is explicitly closed in turn 3, indicated by the\ncorresponding ‘CG’ (close-goal) goal tracking an-\nnotation. Not all goals are explicitly closed by the\nuser. A dialogue manager could decide to implic-\nitly close an answered goal if the user does not\nreopen it within three turns, for example. We only\nannotate explicit goal closures, like the one in turn\n3. There are two intents annotated in both turns 1\nand 3 in Table 2, and multiple goal annotations can\nsimilarly exist, separated by a semicolon. For ex-\nample, “I’m hungry but need the toilet first” simul-\ntaneously opens two goals. All of these annotations\nwere completed using the ELAN tool (Brugman\net al., 2004), and then mapped into JSON for model\ntraining2.\nWith these two sets of annotations, we can eval-\nuate various LLMs on two tasks: (1) multi-party\nintent-slot recognition; and (2) multi-party goal\ntracking. It is not possible to collect vast quanti-\nties of interactions with patients in the hospital, so\nthese models must be able to learn from a corpus\nof limited size. We therefore decided to mask an-\nnotations in a randomised window selected from\neach MPC, providing the model with the surround-\ning context and speaker labels. That is, a random\nnumber of turns was selected in each MPC, and\nthen the annotations were replaced by a ‘[MASK]’\ntoken. An example of this is shown in Table 3.\nAs the corpus size is limited, the window selec-\ntion could potentially heavily impact model perfor-\nmance. We therefore randomised the selected win-\ndow three times for each conversation and train/test\nsplit, and these exact same windows were used to\ntrain and test each model. To clarify, all train/test\nsplits and windows were randomised for multiple\nruns, but they were unchanged between each model.\nFor example, run 1 with the 20/80 split in Section\n4 for T5 contained the exact same test set, with the\nexact same window, as run 1 with the 20/80 split for\nDialogLED. This holds true for both tasks. Each\nmasked window was bookended with a ‘[start]’ and\n‘[end]’ tag to help the models learn this task too\n(Zhong et al., 2022). A shortened example from\nour corpus can be seen in Table 3.\n3 Experimental Procedure\nWe evaluated three different models (each detailed\nbelow): T5 (Raffel et al., 2020), DialogLM using\nLED (DialogLED) (Zhong et al., 2022), and GPT-\n3.5-turbo3. Each approach was evaluated in a zero-\nshot and few-shot setting, with various train/test\nsplits. We could not provide more data to GPT-3.5-\nturbo due to context window size, but the train/test\n2Mapping code, annotated data, and training hyperpa-\nrameters can be found here: https://github.com/\nAddleseeHQ/mpgt-eval.\n3https://platform.openai.com/docs/\nmodels/gpt-3-5\n232\nUser Utterance Intent-Slot Annotation Goal Tracking Annotation\n1 U1: Hello, we’d like a coffee. Where can we go? greet() ; request(beverage(coffee)) G(U1+U2, drink(coffee))\n2 ARI: You have to enter the building behind you. inform(directions(cafe)) AG(U1+U2, drink(coffee))\n3 U2: Ok, well thank you very much. acknowledge(); thank() CG(U1+U2, drink(coffee))\nTable 2: A corpus example displaying shared goals with both intent-slot and goal tracking annotations.\nUser Masked Goal Tracking Utterance Gold Annotation\n1 ARI: Hello, my name is ARI. How can I help you? -\n[start] -\n2 U1: My friend is intimidated by you, where can they eat? [MASK] G(U2, eat())\n3 ARI: There’s a cafeteria on the ground floor, near the courtyard. [MASK] AG(U2, eat())\n[end] -\n4 U2: My appointment is in room 17, where is it? G(U2, go-to(room_17)) -\nTable 3: A corpus example illustrating the goal tracking task. This process was the same for intent-slot recognition,\nwith the corresponding annotations. Note that U1 asks U2’s question, and this is reflected in the annotation.\nsplits for T5 and DialogLED were: 0/100 (zero-\nshot), 20/80, 50/50, and 80/20. This allowed us\nto determine how each model learned to do these\ntasks when given more training examples. As de-\nscribed in Section 2, we ran each experiment three\ntimes with randomised splits and windows, but\nthese remained the same between-models to avoid\nfew-shot problems such as recency bias (Zhao et al.,\n2021). We trained all the T5-Large and DialogLED\nmodels on a machine containing a 16Gb NVIDIA\nGeForce RTX 3080 Ti GPU with 64Gb RAM and\nan Intel i9-12900HK processor.\n3.1 T5-Large\nOlder GPT models (GPT-3 and below) are pre-\ntrained with the next token prediction objective on\nhuge corpora (Radford et al., 2019; Brown et al.,\n2020), an inherently directional task. The creators\nof T5 added two more objectives and give it the\ngoal of minimising the combined loss function\n(Raffel et al., 2020) across all three tasks. The\ntwo additional tasks were de-shuffling, and BERT-\nstyle de-masking (Devlin et al., 2018). This latter\npre-training task involves ‘corrupting’ tokens in the\noriginal text, which T5 must then predict. Impor-\ntantly, this enabled T5 to work bidirectionally, be-\ncoming particularly good at using the surrounding\ncontext to predict tokens in corrupted sentences.\nThis is not dissimilar to our task, in which the\nmodel must learn to use the surrounding MPC turns\nto predict the annotations that are masked. T5 also\nachieves state-of-the-art results on related tasks like\n(Lee et al., 2021; Marselino Andreas et al., 2022),\nalbeit, fine-tuned on larger datasets.\nWe used T5-Large in both a zero-shot setting,\nand fine-tuned with various train/test splits. T5\nallows fine-tuning with a given named task like\n‘answer the question’, or ‘translate from French\nto German’. We used ‘predict goals’ and ‘predict\nintent-slots’ for goal tracking and intent-slot recog-\nnition, respectively, giving the same task names as\ninput during testing. As the corpus is very small,\nthere was no model performance boost beyond 3\nepochs, which was expected (Mueller et al., 2022).\n3.2 DialogLM using LED (DialogLED)\nMPCs reveal unique new communication chal-\nlenges (Addlesee et al., 2023), as detailed in Sec-\ntion 1, so some LLMs have been developed specifi-\ncally for the multi-party domain (Hu et al., 2019;\nGu et al., 2021, 2022a). Microsoft published Di-\nalogLM (Zhong et al., 2022), a pre-trained LLM\nbased upon UniLMv2 (Bao et al., 2020), but\nspecifically designed for multi-party tasks. Along-\nside the base model, they released two variations:\nDialogLM-sparse for long dialogues over 5,120\nwords, and DialogLM using LED (DialogLED)\nwhich outperformed the others. DialogLED builds\non Longform-Encoder-Decoder (LED) (Beltagy\net al., 2020), an attention mechanism that scales\nlinearly with sequence length. Transformer-based\nmodels typically scale quadratically with the se-\nquence length, restricting their ability to process\nlong dialogues.\nDialogLED was pre-trained on five objectives\ndesigned specifically for MPCs, and the model’s\ngoal was to minimise the combined loss of all of\nthese tasks. Their state-of-the-art results showed\nthat their pre-training tasks did encourage the LLM\nto ‘understand’ multi-party interactions. The five\n233\ntasks were: (1) speaker masking, the model has\nto predict who spoke; (2) turn splitting, the model\nhas to recognise when two utterances are likely the\nsame turn; (3) turn merging, the opposite of (2),\nwhere the model has to recognise when the turns\nwere likely separate; (4) text infilling, the model\nhas to predict masked tokens within the turn; and\n(5) turn permutation, the model has to correctly\nre-order jumbled turns.\nWe cloned their repository4 and added two new\ntasks: (6) goal masking, the model has to predict\ngoal tracking annotations; and (7) intent-slot mask-\ning, the model has to predict intent-slot annota-\ntions. In the zero-shot setting, we simply ran the\ntest set through base DialogLED. We then ran their,\nnow modified, code to run our few-shot evaluations\nthree times for each data split.\n3.3 GPT-3.5-turbo\nLarger LLMs are not inherently better at following\na user’s intent (Ouyang et al., 2022) as they have no\nincentive to help the user achieve their goal, only\nto generate realistic looking outputs. This leads\nto significant problems, including the generation\nof false, biased, and potentially harmful responses.\nGPT-3 was therefore fine-tuned on prompts with\nhuman-feedback to create InstructGPT (Ouyang\net al., 2022). OpenAI later followed this same\napproach to create the now famous ChatGPT family\nof models. At the time of writing, GPT-4 is the\nmost powerful of these models, but it is currently\nin a waiting list phase. OpenAI recommends their\nGPT-3.5-turbo model while waiting as the next best\noption. We therefore decided to evaluate this model\non the same two tasks.\nUnlike T5 or DialogLED, there is no way to\nfine-tune your own version of GPT-3.5-turbo, or to\nedit their pre-training steps. People instead mould\nthe model’s behaviour through prompt-engineering\n(Lester et al., 2021; Wei et al., 2022; Weng, 2023).\nThe newer GPT models allow developers to pro-\nvide huge contexts, called prompts, containing in-\nstructions for the model to follow. GPT-3.5-turbo\nallows prompts of up to 4,096 tokens. Although\nthese models have only exploded in popularity re-\ncently, there are many suggested prompt ‘styles’\nsuggested online by conversation designers who\nare implementing these models in the real-world.\nWe have analysed this space and devised six prompt\nstyles for the two tasks. In the zero-shot setting,\n4https://github.com/microsoft/DialogLM\nonly the prompt and the masked MPC is provided\nto the model. In the few-shot setting, we addition-\nally provide the model with 7% of the corpus as\nexamples. This is crucial to highlight. T5 and Di-\nalogLED were trained on 20% of the corpus, 50%\nof the corpus, and finally 80% of the corpus. GPT-\n3.5-turbo’s maximum context size can only fit 7%\nof the corpus, less than the other models.\nThe prompt styles we used were the following\n(the actual prompts are included in Appendix A):\n• Basic: This is our baseline prompt. It very\nsimply tells the model what it is going to get as\ninput, and what we want as output. It contains\nno further special instructions.\n• Specific: GPT practitioners report that when\nprompts are more detailed and specific, per-\nformance is boosted (Ye et al., 2023).\n• Annotation: For annotation tasks, we would\ngive fellow humans annotation instructions.\nIn this prompt, we provide the model with\nannotation instructions.\n• Story: This model was pre-trained on a very\nlarge quantity of data, including novels, film\nscripts, journalistic content, etc... It may\nbe possible that by phrasing the prompt like\na story, performance may be boosted due to\nits likeness to its training data.\n• Role-play: Similar to the story prompt, it is\nreported that these models are very good at\nrole-playing5. People ask ChatGPT to pretend\nto be a therapist, a lawyer, or even alter-egos\nthat have no safety limitations (Taylor, 2023).\nWe tell GPT-3.5-turbo that it is a ‘helpful as-\nsistant listening to a conversation between two\npeople and a social robot called ARI’.\n• Reasoning: Finally, recent work suggests that\nthese models improve in performance if you\nexplain the reasoning for desired outputs (Fu\net al., 2022). We therefore added one ficti-\ntious turn to this prompt, and explained the\nreasoning behind its annotation.\n4 Results\nWe evaluated T5, DialogLED, and GPT-3.5-turbo\nas described in Section 3 on multi-party goal track-\n5https://github.com/f/\nawesome-chatgpt-prompts\n234\ning, and multi-party intent-slot recognition. Out-\nputs were annotated as either ‘exact’, ‘correct’, or\n‘partial’ to distinguish each model’s performance\nbeyond simple accuracy. Exact matches were\nstrictly annotated, but slight differences are allowed\nif the annotation meaning remains unchanged.\nFor example: ‘ G(U1, go-to(lift))’ and\n‘G(U1, go-to(lifts))’ (note the plural\n‘lifts’). Outputs were marked as exact if every\n[MASK] in the MPC was exact, and marked\nas correct if every [MASK] was more broadly\naccurate. For example, if the annotation con-\ntained ‘drink(coffee)’ and the model output\n‘drink(hot_drink)’, we considered this cor-\nrect. The output was marked as partially correct if\nat least 60% of the [MASK] tags were correctly an-\nnotated. This latter metric allows us to distinguish\nbetween models that generate nonsense, and those\nthat roughly grasp the task. Our inter-annotator\nagreements were 0.765 and 0.771 for goal tracking\nand intent-slot recognition, respectively. These are\nless than 0.8, and this was due to the broad defi-\nnition of ‘correct’. We plan to design automatic\nmetrics for our future work (see Section 5).\n4.1 MPC Goal Tracking Results\nThe goal tracking results can be found in Table 4.\nAn ANOV A test (Fisher, 1992) indicated that there\nwas an overall significant difference between the\nmodel’s results. We therefore ran a Tukey HSD\ntest (Tukey, 1949) that showed that the GPT-3.5-\nturbo model in the few-shot setting did significantly\noutperform all the other models.\nFirstly, the T5-Large model performed poorly,\neven when it was trained on 80% of our corpus.\nUpon further analysis, it generated complete non-\nsense in the zero-shot setting, but did start to gen-\nerate strings that looked reasonable with only 20%\nof the data. Given the 50/50 train/test split, T5\nconsistently replaced the [MASK] tokens, but did\nstill hallucinate turns. When given 80% of the data\nas training data, the T5 model preserved the orig-\ninal dialogue, and replaced the [MASK] tokens\nwith goal annotations, they were just all completely\nwrong. This steady improvement as we increased\nthe amount of training data suggests that T5 could\nbe a viable option for similar tasks, just not where\ndata is limited (such as our hospital use case).\nThe DialogLED model also generated nonsense\nin the zero-shot setting, but very quickly learned\nthe task. Even with just 20% of the data used for\ntraining, DialogLED reliably preserved the origi-\nnal dialogue and replaced the [MASK] tokens with\ngoal annotations. Most of the annotations were in-\ncorrect, for example ‘G(U2, eat(ticket))’,\nbut DialogLED did correctly detect some goals\nopening, being answered, and being closed cor-\nrectly, achieving a non-zero partial score. Given\nmore training data, DialogLED did begin to use the\nsurrounding contextual dialogue turns more accu-\nrately, but almost every result contained an incor-\nrect prediction. This was often the mis-detection\nof shared goals, or closing goals early. Like T5,\nDialogLED would need a larger training set to ac-\ncurately complete this task. This model learned the\ntask quickly, so may need fewer examples.\nIn the zero-shot setting, GPT-3.5-turbo roughly\n‘understood’ the task, generating many partially\ncorrect outputs. With all the prompt styles, it did\nfrequently reformat the dialogue. This was particu-\nlarly true when using the roleplay prompt, it would\noutput all the goals per interlocutor, for example,\nrather than per turn. The worst zero-shot GPT-\n3.5-turbo prompt was the ‘story’ style, not even\ngenerating one partially correct output. This was\ndue to its increased hallucination. The story prompt\nnoticeably produced more fictitious turns, and also\nrephrased and removed turns in the original dia-\nlogue. We believe this is likely because a story\nscenario is naturally a fictitious topic. The ‘rea-\nsoning’ style prompt performed remarkably well,\ngenerating five times more correct outputs than the\nsecond-best prompt style, and generating 79.31%\npartially correct outputs, showing that it can grasp\nthe concept of the task. The reasoning prompt com-\nmonly mis-identified shared goals, unfortunately.\nIn the few-shot setting, GPT-3.5-turbo’s results\nimproved significantly compared to every other\napproach. We would like to highlight again that\neach run’s example prompts provided to the model\nwere exactly the same for each prompt style. Per-\nformance differences were only due to the given\nprompt style. The ‘reasoning’ prompt once again\noutperformed the others across all metrics, gen-\nerating correct outputs 62.32% of the time, and\npartially correct 94.20% of the time. In our fu-\nture work (see Section 5), we plan to utilise this\nprompt style’s impressive performance on limited\ndata. The ‘story’ prompt was the only style to suc-\ncessfully attribute goals to other speakers, as in\nTable 3, but it still suffered from increased halluci-\nnation, which is not appropriate in a safety-critical\n235\nModel train/test % Prompt Style Exact % Correct % Partial %\nT5 0/100 - 0 0 0\nT5 20/80 - 0 ± 0 0 ± 0 0 ± 0\nT5 50/50 - 0 ± 0 0 ± 0 0 ± 0\nT5 80/20 - 0 ± 0 0 ± 0 0 ± 0\nDialogLED 0/100 - 0 0 0\nDialogLED 20/80 - 0 ± 0 0 ± 0 5.80 ± 1.45\nDialogLED 50/50 - 0 ± 0 2.38 ± 2.38 1.19 ± 0.63\nDialogLED 80/20 - 0 ± 0 0 ± 0 20 ± 11.55\nGPT 3.5-turbo 0/100 Basic 0 3.45 31.03\nGPT 3.5-turbo 0/100 Specific 0 3.45 24.14\nGPT 3.5-turbo 0/100 Annotation 0 6.90 44.83\nGPT 3.5-turbo 0/100 Story 0 0 0\nGPT 3.5-turbo 0/100 Role-play 0 0 6.90\nGPT 3.5-turbo 0/100 Reasoning 3.45 34.48 79.31\nGPT 3.5-turbo 7/80* Basic 11.59 ± 3.83 30.43 ± 10.94 86.96 ± 6.64\nGPT 3.5-turbo 7/80* Specific 20.29 ± 3.83 43.48 ± 9.05 92.75 ± 2.90\nGPT 3.5-turbo 7/80* Annotation 14.49 ± 5.80 28.99 ± 3.83 82.61 ± 4.35\nGPT 3.5-turbo 7/80* Story 17.39 ± 6.64 36.23 ± 13.83 86.96 ± 4.35\nGPT 3.5-turbo 7/80* Role-play 18.84 ± 7.25 46.38 ± 12.38 92.75 ± 5.22\nGPT 3.5-turbo 7/80* Reasoning 27.54 ± 1.45 62.32 ± 9.50 94.20 ± 5.80\nTable 4: The final multi-party goal tracking results for each model in both the zero- and few-shot settings.\n*We could not fit more than 7% of the training examples in GPT-3.5-turbo’s context window. We therefore used\nfewer examples than with T5 and DialogLED. The same 80% test sets were still used to enable model comparison.\nsetting. We suspect that the other prompt styles\nfailed to do this because of the rarity of this phe-\nnomenon in our corpus. We are eliciting more of\nthese in ongoing experiments with a deployed sys-\ntem, not wizard-of-oz (Addlesee et al., 2023).\n4.2 MPC Intent-slot Recognition Results\nThe results for each model on the intent-slot recog-\nnition task can be found in Table 5. As with the\ngoal tracking results, an ANOV A test (Fisher, 1992)\nindicated that there was an overall significant differ-\nence between our model’s results. We therefore ran\na Tukey HSD test (Tukey, 1949) that showed that\nthe GPT-3.5-turbo model in the few-shot setting\nsignificantly outperformed all the other models.\nAs intent-slot annotations are well-established,\nT5 and DialogLED both started generating sensible-\nlooking outputs with only a few training examples.\nThe T5 outputs were all incorrect again, however.\nDialogLED consistently improved as it was trained\non progressively more data, annotating almost half\nof the MPCs partially correctly, and beginning to\naccurately annotate full MPCs. Given a larger cor-\npus, we expect that DialogLED could potentially\ngenerate competitive results, but this is not the case\nfor T5 in this setting with limited data.\nGPT-3.5-turbo in the zero-shot setting also\nachieved higher partial scores, compared to the\ngoal tracking results, due to the fact that intent-slot\nrecognition is a more established task. Turns were\ncommonly annotated with multiple gold goals, but\nthis model tended to only output one per turn. For\nexample: “Hello ARI, where is the café?” would\nonly have the prediction ‘greet’, missing the re-\nquest to locate the café entirely. This prevented the\nmodel from achieving higher correct scores.\nIn the few-shot setting, however, GPT-3.5-turbo\nsignificantly outperformed all the other models.\nThe difference was remarkable. Almost all of the\npredictions were partially correct, and the ‘reason-\ning’ prompts correctly annotated 70% of the MPCs.\nOther models tended to falter when anaphoric ex-\npressions couldn’t be resolved with just the pre-\nvious turn. They also struggled to identify the\n‘suggest’ intent, for example, when one person\nsaid “do you want to go to the toilet?”. These were\nmisclassified as request intents, likely due to their\nprominence in the corpus, and influence on the re-\nsults due to GPT-3.5-turbo’s limited input context.\n236\nModel train/test % Prompt Style Exact % Correct % Partial %\nT5 0/100 - 0 0 0\nT5 20/80 - 0 ± 0 0 ± 0 0 ± 0\nT5 50/50 - 0 ± 0 0 ± 0 0 ± 0\nT5 80/20 - 0 ± 0 0 ± 0 0 ± 0\nDialogLED 0/100 - 0 0 0\nDialogLED 20/80 - 0 ± 0 0 ± 0 5.80 ± 2.90\nDialogLED 50/50 - 0 ± 0 0 ± 0 38.10 ± 10.38\nDialogLED 80/20 - 0 ± 0 13.33 ± 6.67 46.67 ± 6.67\nGPT 3.5-turbo 0/100 Basic 0 3.45 51.72\nGPT 3.5-turbo 0/100 Specific 0 0 13.79\nGPT 3.5-turbo 0/100 Annotation 0 3.45 20.69\nGPT 3.5-turbo 0/100 Story 0 0 24.14\nGPT 3.5-turbo 0/100 Role-play 0 0 20.69\nGPT 3.5-turbo 0/100 Reasoning 0 27.59 82.76\nGPT 3.5-turbo 7/80* Basic 17.39 ± 6.64 36.23 ± 12.88 97.10 ± 2.90\nGPT 3.5-turbo 7/80* Specific 27.54 ± 1.45 60.87 ± 9.05 94.20 ± 1.45\nGPT 3.5-turbo 7/80* Annotation 18.84 ± 1.45 40.58 ± 6.32 91.30 ± 4.35\nGPT 3.5-turbo 7/80* Story 26.09 ± 4.35 47.83 ± 10.04 94.20 ± 3.83\nGPT 3.5-turbo 7/80* Role-play 20.29 ± 3.83 49.27 ± 12.88 97.10 ± 1.45\nGPT 3.5-turbo 7/80* Reasoning 37.68 ± 1.45 69.57 ± 10.94 100 ± 0\nTable 5: The final multi-party intent-slot recognition results for each model in both the zero- and few-shot settings.\n*We could not fit more than 7% of the training examples in GPT-3.5-turbo’s context window. We therefore used\nfewer examples than with T5 and DialogLED. The same 80% test sets were still used to enable model comparison.\n5 Conclusion and Future Work\nMulti-party conversations (MPCs) elicit complex\nbehaviours which do not occur in the dyadic in-\nteractions that today’s dialogue systems are de-\nsigned and trained to handle. Social robots are\nincreasingly being expected to perform tasks in\npublic spaces like museums and malls, where con-\nversations often include groups of friends or fam-\nily. Multi-party research has previously focused\non speaker recognition, addressee recognition, and\ntweaking response generation depending on whom\nthe system is addressing. While this work is vital,\nwe argue that these collective “who says what to\nwhom” tasks do not provide any incentive for the\nsocial robot to complete user goals, and instead en-\ncourage it to simply mimic what a good MPClooks\nlike. In this paper, we have detailed how the tasks\nof goal tracking and intent-slot recognition differ\nin a multi-party setting, providing examples from\nour newly collected corpus of MPCs in a hospital.\nWe found that, given limited data, ‘reasoning’ style\nprompts enable GPT-3.5-turbo to perform signifi-\ncantly better than other models.\nWe found that other prompt styles also perform\nwell, but prompts that are story-like increase model\nhallucination. With the introduction of prompt fine-\ntuning with human feedback (Ouyang et al., 2022),\ngenerative LLMs do now have some incentive to\navoid misleading or harming the user, providing\noutputs prepended with caveats, but the issue is\nnot solved. OpenAI claims that GPT-4 generates\n40% fewer hallucinations than GPT-3 (Hern and\nBhuiyan, 2023), but these models should still not be\napplied directly in a hospital or other safety-critical\nsetting without further evaluation. In the hospital\nsetting, users are more likely to be from vulnerable\npopulation groups, and are more likely to be older\nadults that are not familiar with the capabilities of\ntoday’s models. Multiple researchers and hospi-\ntal staff members are present when conducting our\ndata collections, so that if hallucinations do occur,\nthey can be quickly corrected. We will, therefore,\nbe able to evaluate response grounding, Guidance6,\nand other hallucination prevention strategies to de-\ntermine whether these models can ever be used\nsafely in a high-risk setting. These further exper-\niments will also elicit further MPCs that can be\nannotated for various multi-party tasks.\nUser inputs must be processed on external\n6https://github.com/microsoft/guidance\n237\nservers when using industry LLMs, like GPT-3.5-\nturbo and Google’s Bard. For this reason, these\nspecific models cannot be deployed in the hos-\npital setting. Patients may reveal identifiable or\nsensitive information during our data collection,\nwhich we subsequently remove from the corpus.\nThis data must stay contained within approved\ndata-controlled servers in the SPRING project. In\nthis paper, we have reported the remarkable per-\nformance of an industry LLM, when given limited\ndata, compared to prior model architectures. We\nwill analyse open and transparent instruction-tuned\ntext generators (Liesenfeld et al., 2023), which are\nable to meet our data security requirements.\nThe accessibility of today’s SDSs is critical\nwhen working with hospital patients (Addlesee,\n2023). Speech production differs between the ‘av-\nerage’ user, and user groups that remain a minority\nin huge training datasets. For example, people with\ndementia pause more frequently and for longer du-\nrations mid-sentence due to word-finding problems\n(Boschi et al., 2017; Slegers et al., 2018). We are\nutilising knowledge graphs to ensure that SDSs\nare transparent, controllable, and more accessible\nfor these user groups (Addlesee and Eshghi, 2021;\nAddlesee and Damonte, 2023a,b), and we see the\nunification of large language models and knowl-\nedge graphs (Pan et al., 2023) as the near-term\nfuture of our field.\nWe plan to design and run subsequent experi-\nments in both the hospital memory clinic, and a\nnewly established mock waiting room in our lab.\nThis space will allow us to collect additional MPCs\nwith more than two people, replicating scenarios\nin which whole families approach a social robot.\nWe plan to evaluate whether prompt engineering\ncan work modularly for N users. For example, we\ncould use GPT-4 to correct speaker diarization (Mu-\nrali et al., 2023), then to handle multi-party goal\ntracking, and then to generate responses to the user.\nThis experimental setup will allow us to quickly\ntest new ideas, such as automatic prompt optimiza-\ntion (Pryzant et al., 2023) in the lab, maximising\nthe benefit of patients’ time in the hospital.\nAcknowledgements\nThis research was funded by the EU H2020 pro-\ngram under grant agreement no. 871245 (https:\n//spring-h2020.eu/). We would also like\nto thank our anonymous reviewers for their time\nand valuable feedback.\nReferences\nAngus Addlesee. 2023. V oice assistant accessibility.\nIn The International Workshop on Spoken Dialogue\nSystems Technology, IWSDS 2023.\nAngus Addlesee and Marco Damonte. 2023a. Under-\nstanding and answering incomplete questions. In\nProceedings of the 5th Conference on Conversational\nUser Interfaces.\nAngus Addlesee and Marco Damonte. 2023b. Under-\nstanding disrupted sentences using underspecified\nabstract meaning representation. In Interspeech.\nAngus Addlesee and Arash Eshghi. 2021. Incremen-\ntal graph-based semantics and reasoning for conver-\nsational AI. In Proceedings of the Reasoning and\nInteraction Conference (ReInAct 2021), pages 1–7,\nGothenburg, Sweden. Association for Computational\nLinguistics.\nAngus Addlesee, Weronika Siei´nska, Nancie Gunson,\nDaniel Hernández García, Christian Dondrup, and\nOliver Lemon. 2023. Data collection for multi-party\ntask-based dialogue in social robotics. In The In-\nternational Workshop on Spoken Dialogue Systems\nTechnology, IWSDS 2023.\nSamer Al Moubayed, Jonas Beskow, Gabriel Skantze,\nand Björn Granström. 2012. Furhat: a back-\nprojected human-like robot head for multiparty\nhuman-machine interaction. In Cognitive Be-\nhavioural Systems: COST 2102 International Train-\ning School, Dresden, Germany, February 21-26,\n2011, Revised Selected Papers , pages 114–130.\nSpringer.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. In International conference on machine\nlearning, pages 642–652. PMLR.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nVeronica Boschi, Eleonora Catricala, Monica Consonni,\nCristiano Chesi, Andrea Moro, and Stefano F Cappa.\n2017. Connected speech in neurodegenerative lan-\nguage disorders: a review. Frontiers in psychology,\n8:269.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nHennie Brugman, Albert Russel, and Xd Nijmegen.\n2004. Annotating multi-media/multi-modal re-\nsources with elan. In LREC, pages 2065–2068.\n238\nSara Cooper, Alessandro Di Fava, Carlos Vivas, Luca\nMarchionni, and Francesco Ferro. 2020. Ari: The\nsocial assistive robot and companion. In 2020 29th\nIEEE International Conference on Robot and Human\nInteractive Communication (RO-MAN), pages 745–\n751. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nArash Eshghi and Patrick GT Healey. 2016. Collec-\ntive contexts in conversation: Grounding by proxy.\nCognitive science, 40(2):299–324.\nYue Feng, Aldo Lipani, Fanghua Ye, Qiang Zhang, and\nEmine Yilmaz. 2022. Dynamic schema graph fusion\nnetwork for multi-domain dialogue state tracking.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 115–126.\nRonald Aylmer Fisher. 1992. Statistical methods for\nresearch workers. Springer.\nMary Ellen Foster, Bart Craenen, Amol Deshmukh,\nOliver Lemon, Emanuele Bastianelli, Christian Don-\ndrup, Ioannis Papaioannou, Andrea Vanzo, Jean-\nMarc Odobez, Olivier Canévet, et al. 2019. MuM-\nMER: Socially intelligent human-robot interaction in\npublic spaces. arXiv preprint arXiv:1909.06749.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nJia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-\nHua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang.\n2022a. HeterMPC: A Heterogeneous Graph Neu-\nral Network for Response Generation in Multi-Party\nConversations. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 5086–5097.\nJia-Chen Gu, Chongyang Tao, and Zhen-Hua Ling.\n2022b. WHO Says WHAT to WHOM: A Survey\nof Multi-Party Conversations. In Proceedings of the\nThirty-First International Joint Conference on Artifi-\ncial Intelligence (IJCAI-22).\nJia-Chen Gu, Chongyang Tao, Zhenhua Ling, Can Xu,\nXiubo Geng, and Daxin Jiang. 2021. MPC-BERT:\nA pre-trained language model for multi-party con-\nversation understanding. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3682–3692.\nNancie Gunson, Daniel Hernández Garcia, Weronika\nSiei´nska, Angus Addlesee, Christian Dondrup, Oliver\nLemon, Jose L Part, and Yanchao Yu. 2022. A\nvisually-aware conversational robot receptionist. In\nProceedings of the 23rd Annual Meeting of the Spe-\ncial Interest Group on Discourse and Dialogue, pages\n645–648.\nMatthew Henderson, Blaise Thomson, and Jason D\nWilliams. 2014a. The second dialog state tracking\nchallenge. In Proceedings of the 15th annual meet-\ning of the special interest group on discourse and\ndialogue (SIGDIAL), pages 263–272.\nMatthew Henderson, Blaise Thomson, and Jason D\nWilliams. 2014b. The third dialog state tracking chal-\nlenge. In 2014 IEEE Spoken Language Technology\nWorkshop (SLT), pages 324–329. IEEE.\nAlex Hern and Johana Bhuiyan. 2023. Openai says new\nmodel gpt-4 is more creative and less likely to invent\nfacts. The Guardian.\nWenpeng Hu, Zhangming Chan, Bing Liu, Dongyan\nZhao, Jinwen Ma, and Rui Yan. 2019. GSN: A graph-\nstructured network for multi-party dialogues. In Pro-\nceedings of the Twenty-Eighth International Joint\nConference on Artificial Intelligence (IJCAI-19).\nSimon Keizer, Mary Ellen Foster, Oliver Lemon, An-\ndre Gaschler, and Manuel Giuliani. 2013. Training\nand evaluation of an MDP model for social multi-\nuser human-robot interaction. In Proceedings of the\nSIGDIAL 2013 Conference, pages 223–232.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021.\nDialogue state tracking with a language model using\nschema-driven prompting. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4937–4949.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nAndreas Liesenfeld, Alianda Lopez, and Mark Dinge-\nmanse. 2023. Opening up chatgpt: Tracking open-\nness, transparency, and accountability in instruction-\ntuned text generators. In Proceedings of the 5th In-\nternational Conference on Conversational User In-\nterfaces, pages 1–6.\nVinsen Marselino Andreas, Genta Indra Winata, and\nAyu Purwarianti. 2022. A comparative study on\nlanguage models for task-oriented dialogue systems.\narXiv e-prints, pages arXiv–2201.\nMeriam Moujahid, Helen Hastie, and Oliver Lemon.\n2022. Multi-party interaction with a robot recep-\ntionist. In 2022 17th ACM/IEEE International Con-\nference on Human-Robot Interaction (HRI) , pages\n927–931. IEEE.\nAaron Mueller, Jason Krone, Salvatore Romeo, Saab\nMansour, Elman Mansimov, Yi Zhang, and Dan Roth.\n2022. Label semantic aware pre-training for few-\nshot text classification. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8318–\n8334.\n239\nPrasanth Murali, Ian Steenstra, Hye Sun Yun, Ameneh\nShamekhi, and Timothy Bickmore. 2023. Improv-\ning multiparty interactions with a robot using large\nlanguage models. In Extended Abstracts of the 2023\nCHI Conference on Human Factors in Computing\nSystems, pages 1–8.\nJekaterina Novikova, Oliver Lemon, and Verena Rieser.\n2016. Crowd-sourcing NLG data: Pictures elicit\nbetter data. In Proceedings of the 9th International\nNatural Language Generation conference, pages 265–\n273, Edinburgh, UK. Association for Computational\nLinguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2023. Unifying large\nlanguage models and knowledge graphs: A roadmap.\narXiv preprint arXiv:2306.08302.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with \"gradient descent\" and\nbeam search. arXiv preprint arXiv:2305.03495.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nAntoine Slegers, Renee-Pier Filiou, Maxime Montem-\nbeault, and Simona Maria Brambati. 2018. Con-\nnected speech features from picture description in\nalzheimer’s disease: A systematic review. Journal of\nAlzheimer’s Disease, 65(2):519–542.\nJosh Taylor. 2023. Chatgpt’s alter ego, dan: users jail-\nbreak ai program to get around ethical safeguards.\nThe Guardian.\nLeimin Tian, Pamela Carreno-Medrano, Aimee Allen,\nShanti Sumartojo, Michael Mintrom, Enrique Coron-\nado Zuniga, Gentiane Venture, Elizabeth Croft, and\nDana Kulic. 2021. Redesigning human-robot inter-\naction in response to robot failures: A participatory\ndesign methodology. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1–8.\nDavid Traum. 2004. Issues in multiparty dialogues. In\nAdvances in Agent Communication: International\nWorkshop on Agent Communication Languages, ACL\n2003, Melbourne, Australia, July 14, 2003. Revised\nand Invited Papers, pages 201–211. Springer.\nJohn W Tukey. 1949. Comparing individual means in\nthe analysis of variance. Biometrics, pages 99–114.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nLilian Weng. 2023. Prompt engineering. lilian-\nweng.github.io.\nJason Williams, Antoine Raux, and Matthew Henderson.\n2016. The dialog state tracking challenge series: A\nreview. Dialogue & Discourse, 7(3):4–33.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang,\nHyeongu Yun, Yireun Kim, and Minjoon Seo. 2023.\nIn-context instruction learning. arXiv preprint\narXiv:2302.14691.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nMing Zhong, Yang Liu, Yichong Xu, Chenguang Zhu,\nand Michael Zeng. 2022. DialogLM: Pre-trained\nmodel for long dialogue understanding and summa-\nrization. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 11765–\n11773.\nA Full GPT-3.5-turbo Prompts\nHere are the full prompts given to GPT-3.5-turbo\nfor each task. We used six styles described in Sec-\ntion 3. The masked MPC was appended to each\nprompt in the zero-shot setting. In the few-shot\nprompts (see Section A.2), we appended examples\nwith “input:” + masked MPC #1 + “output:” + gold\noutput #1 + ‘input:” + masked MPC #2 + “output:”\n+ gold output #2 + “input:” + test set masked MPC\n+ “output:”7.\nA.1 Zero-shot Goal Tracking\n• Basic: This conversation has a window be-\ntween [start] and [end]. Return this window\nwith the [MASK] tags replaced with the goal\nannotations:\n• Specific: This is a conversation between two\npeople and a robot called ARI. There is a sec-\ntion of the conversation between the [start]\nand [end] tags. I want you to return this\nsection of the conversation, but I want you\nto replace the [MASK] tags with the user\ngoals. Do not change any of the other words\n7The examples given were randomised per run, and the\nappendix page limit doesn’t fit the full 4,096 token prompts.\n240\nin the section, only replace [MASK]. Every\n[MASK] should be replaced. Here is the con-\nversation:\n• Annotation: This is a conversation between\ntwo people and a robot called ARI. I want\nyou to first extract the text between [start] and\n[end]. There are [MASK] tags in the extracted\ntext. I want you to replace the [MASK] tags\nwith goal annotations. Do not change any\nof the other text. If the person’s goal can be\ndetermined by that turn, add an ’@’ symbol\nfollowed by ’G’ (G for goal), and then brack-\nets with the speaker ID and what their goal is.\nIf it is a shared goal, you can annotate both\nspeakers with a ’+’ sign between them. For\nexample, if you think U1 and U2 share the\ngoal, you can write U1+U2. If you think the\ngoal is being answered, you can do the same\nbut with ’AG’ (AG for Answer Goal) instead\nof ’G’. Finally, if you think the person is clos-\ning the goal, you can do the same annotation\nusing ’CG’ (CG for Close Goal) instead of ’G’\nor AG’. Here is the conversation:\n• Story: There once was a conversation be-\ntween a patient, a companion, and a robot\ncalled ARI. One bit of the conversation was\nconfusing. A helpful researcher noted the start\nwith [start], and the end with [end]. The con-\nfusing bits are marked with [MASK]. Can you\nhelp us figure out the goals that should replace\nthe [MASK] tags? The conversation is this:\n• Role-play: You are listening to a conversation\nbetween two people and a robot called ARI.\nYou are a helpful assistant that needs to figure\nout what goals the people have. You need to\npay attention to the [MASK] tags between the\n[start] and [end] tags in the given conversa-\ntion. Your job is to replace these [MASK]\ntags with the correct goal annotations. Here is\nthe conversation:\n• Reasoning: I will give you a conversation\nbetween two people and a robot called ARI.\nYou need to return the text between [start]\nand [end] with the [MASK] tags replaced by\nuser goals. Let’s step through how to figure\nout the correct annotation. If the conversa-\ntion included ’U1: I really need the toilet\n[MASK]’, then we would first know that the\nspeaker is called U1. The turn also ends with\n[MASK], so we know that we need to replace\nit with a goal. We know that U1 needs the\ntoilet, so their goal is to go to the nearest toi-\nlet. Goals always begin with the ’@’ symbol,\nand then a ’G’ if we have found a person’s\ngoal. We would therefore replace [MASK]\nwith @ G(U1, go-to(toilet)). If someone tells\nU1 where the toilets are, they have answered\ntheir goal. We would therefore annotate that\nturn with @ AG(U1, go-to(toilet)). We use\nAG here to indicate Answer Goal. Finally, if\nU1 then said thank you, we know their goal\nhas been met. We would annotate the thank\nyou with @ CG(U1, go-to(toilet)) because\nU1’s goal is finished. CG stands for Close\nGoal. Do this goal tracking for each [MASK]\nin this conversation:\nA.2 Few-shot Intent-slot Recognition\n• Basic: Each conversation has a window be-\ntween [start] and [end]. Return this window\nwith the [MASK] tags replaced with the intent-\nslot annotations. Here are some examples.\n• Specific: Each of these conversations is be-\ntween two people and a robot called ARI.\nThere is a section of each conversation be-\ntween the [start] and [end] tags. I want you\nto return this section of the conversation, but\nI want you to replace the [MASK] tags with\nthe user intents and slots. Do not change any\nof the other words in the section, only replace\n[MASK]. Every [MASK] should be replaced.\nHere are some examples.\n• Annotation: Each of these conversations is\nbetween two people and a robot called ARI.\nI want you to first extract the text between\n[start] and [end]. There are [MASK] tags in\nthe extracted text. I want you to replace the\n[MASK] tags with intent-slot annotations. Do\nnot change any of the other text. If the per-\nson’s intent can be determined by that turn,\nadd a ’#’ symbol followed by their intent and\nthen brackets with the slots within. There\nare not always slots, so the brackets can be\nempty. Sometimes there are multiple intents,\nsplit them with a semi-colon ’;’. Here are\nsome examples.\n• Story: There once was a conversation be-\ntween a patient, a companion, and a robot\ncalled ARI. One bit of the conversation was\n241\nconfusing. A helpful researcher noted the start\nwith [start], and the end with [end]. The con-\nfusing bits are marked with [MASK]. Can\nyou help us figure out the intents and slots\nthat should replace the [MASK] tags? Here\nare some examples.\n• Role-play: You are listening to a conversation\nbetween two people and a robot called ARI.\nYou are a helpful assistant that needs to figure\nout what goals the people have. You need to\npay attention to the [MASK] tags between the\n[start] and [end] tags in the given conversation.\nYour job is to replace these [MASK] tags with\nthe correct intent-slot annotations. Here are\nsome examples.\n• Reasoning: I will give you a conversation\nbetween two people and a robot called ARI.\nYou need to return the text between [start]\nand [end] with the [MASK] tags replaced by\nuser intents and slots. Let’s step through how\nto figure out the correct annotation. If the\nconversation included ’U1: Hello, I’d like to\nknow where the doctor’s office is? [MASK]’\nthen we know there is a missing intent-slot\nannotation because of the [MASK] tag. U1\nfirst said hello, greeting their interlocutor, so\nwe know their intent is greet. This has no\nslots, so we have the annotation ’# greet()’ to\nstart. U1 also asked where the doctor is, so\ntheir second intent is a request. The slot is\nthe room that the doctor is in, as that is what\nthey are requesting. Their second intent is\ntherefore ’# request(doctor(room)). As there\nare multiple intents, the [MASK] is replaced\nby ’# greet() ; request(doctor(room))’. The ’;’\nis only used because there was more than one\nintent. Do this intent-slot annotation for each\n[MASK] in this conversation. Here are some\nexamples.",
  "topic": "Garcia",
  "concepts": [
    {
      "name": "Garcia",
      "score": 0.84353107213974
    },
    {
      "name": "Tracking (education)",
      "score": 0.6753682494163513
    },
    {
      "name": "Computer science",
      "score": 0.4673472046852112
    },
    {
      "name": "Political science",
      "score": 0.40767189860343933
    },
    {
      "name": "Artificial intelligence",
      "score": 0.349915087223053
    },
    {
      "name": "Humanities",
      "score": 0.29400572180747986
    },
    {
      "name": "Sociology",
      "score": 0.2604174017906189
    },
    {
      "name": "Art",
      "score": 0.19215989112854004
    },
    {
      "name": "Pedagogy",
      "score": 0.12032407522201538
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4403386650",
      "name": "Centre de Robotique",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210085930",
      "name": "Heriot-Watt University Malaysia",
      "country": "MY"
    }
  ],
  "cited_by": 11
}