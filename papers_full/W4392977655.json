{
  "title": "Forum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work Scholarship: Selecting the Right Tool for the Task",
  "url": "https://openalex.org/W4392977655",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1978571930",
      "name": "B. Victor",
      "affiliations": [
        "Wayne State University"
      ]
    },
    {
      "id": null,
      "name": "L Goldkind",
      "affiliations": [
        "Fordham University"
      ]
    },
    {
      "id": "https://openalex.org/A2133292877",
      "name": "B. Perron",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4284670538",
    "https://openalex.org/W4386721614",
    "https://openalex.org/W4399167099",
    "https://openalex.org/W4387390444"
  ],
  "abstract": "The emergence of large language models (LLMs) like ChatGPT, Gemini, and Claude offers significant potential for the social work profession.However, these LLMs are not without their ethical and practical challenges, particularly concerning the accuracy of the information provided by these models.This commentary explores the importance of developing digital literacy among Forum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work Scholarship: Selecting the Right Tool for the Task",
  "full_text": " \n \nCOPYRIGHT 2024 BY INTERNATIONAL FEDERATION OF SOCIAL WORKERS \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nInternational Journal of Social Work Values and Ethics \nVolume 21, Number 1 (2024)  \n \nISSN: 2790-6345 \nDOI: 10.55521/10-021-100 \nwww.jswve.org \n  \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 200 \n \nForum: The Limitations of Large Language Models and \nEmerging Correctives to Support Social Work Scholarship: \nSelecting the Right Tool for the Task \nDOI: 10.55521/10-021-112 \n \nBryan G. Victor, PhD \nWayne State University \nbvictor@wayne.edu \n \nLauri Goldkind, PhD \nFordham University \ngoldkind@fordham.edu \nBrian E. Perron, PhD \nUniversity of Michigan - Ann Arbor \nbeperron@umich.edu \n \nAcknowledgements: ChatGPT-4 was used to support the writing of this article. \nVictor, B., Goldkind, L. & Perron, B. (2024). Forum: The Limitations of Large Language Models \nand Emerging Correctives to Support Social Work Scholarship: Selecting the Right Tool for \nthe Task. International Journal of Social Work Values and Ethics, 21(1), 200-207. \nhttps://doi.org/10.55521/10-021-112 \nThis text may be freely shared among individuals, but it may not be republished in any medium \nwithout express written consent from the authors and advance notification of IFSW. \nAbstract \nThe emergence of large language models (LLMs) like ChatGPT, Gemini, and \nClaude offers significant potential for the social work profes sion. However, \nthese LLMs are not without their ethical and practical challenges, particularly \nconcerning the accuracy of the information provided by these models. This \ncommentary explores the importance of developing digital literacy among \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 201 \n \nsocial work professionals to effectively navigate the capabilities and limitations \nof LLMs. Through an understanding that LLMs are designed to generate hu-\nman-like text outputs rather than serve as tools for information retrieval, users \ncan align their expectations and us es of these models accordingly. The paper \nhighlights a specific instance where ChatGPT produced inaccurate scholarly ref-\nerences as a clear example of a model output with factually incorrect infor-\nmation, an occurrence often referred to as a hallucination. The authors then de-\nscribe recent technology advancements such as the integration of Internet \nsearch capability with LLMs and an approach known as retrieval -augmented \ngeneration that can enhance the ability of LLMs to provide users with more ac-\ncurate and rel evant information. The commentary ends with a call for con-\ncerted efforts to equip social work students, practitioners, educators, and schol-\nars with the skills needed to use emerging AI technologies ethically and effec-\ntively. \nKeywords: Digital literacy, large language models, hallucinations, generative artificial intelli-\ngence, retrieval-augmented generation, social work \n \nThe arrival of widely available large language models (LLMs) like ChatGPT, \nGemini and Claude presents exciting opportunities for social work students, \npractitioners, and researchers but also raises concerns about the ethical and \neffective use of these technologies. An important step in using LLMs is \ntherefore the development of digital literacy. Users sho uld understand \nwhat these models can and cannot do effectively given how they were de-\nveloped. At their core, the purpose of LLMs is to generate text that is rele-\nvant to the user’s request (i.e., prompt) and that mimics human language. \nThis functionality is  designed to complement rather than replace other \ntechnology tools such as search engines and digital databases. Understand-\ning, even at a very preliminary level, how the models work, may support us-\ners to most effectively match an LLM to the appropriate and ethical task.  \nA commonly raised concern -- and known limitation -- of LLMs is the \nfrequency with which their outputs contain factually incorrect information, \ngenerally referred to as hallucinations (Rawte et al., 2023; Yao et al., 2023). \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 202 \n \nUnderstanding this limitation begins by recognizing that LLMs are not com-\nprehensive information repositories. Rather, they are models that perform \nword predictions. They are trained on a vast – albeit finite – collection of in-\nternet documents. The models excel at various natural language tasks such \nas generating, summarizing, and classifying text. However, LLMs are not \ndesigned for information retrieval, particularly for specialized topics where \nthe most relevant information is often behind paywalls and inaccessible \nduring their training.  LLMs hallucinate when they lack complete infor-\nmation for precise word predictions.   \nAn example of such a hallucinatory “conversation” was recently pre-\nsented by an editor from International Journal of Social Work Values and Ethics \nto the authors. He described a set of hallucinations received from ChatGPT \npertaining to scholarly references. The editor shared his long interest in cri-\ntiques of Durkheim’s work on suicide but had experienced difficulty locat-\ning anything in the scientific literature, especially work by psychologists. \nCurious about the potential of generative AI to support their search, they \nturned to ChatGPT for assistance. The following is an excerpt from the edi-\ntor’s exchange with the model that was provided to us: \nEditor’s prompt: You gave me this citation: Lester, D. (2014). Durkheim's the-\nory of suicide as applied to the family: A critical evaluation. Journal of \nFamily Issues, 35 (13), 1826-1840. However, I went to the library, and it \ndoesn't exist in the Journal of Family Issues . Instead, ther e is an article \nentitled “A Double-Edge Sword.” \nLLM output: I'm sorry for the confusion. It seems that the citation I gave you \nwas incorrect. According to my web search, the article by Lester is ac-\ntually published in the journal Suicide and Life -Threatening Behavior , \nnot the Journal of Family Issues. The correct citation should be: \nLester, D. (2014). Durkheim's Theory of Suicide as Applied to the Fam-\nily: A Critical Evaluation. Suicide and Life -Threatening Behavior, 44(6), \n631-636. \nThe article entitled A Dou ble-Edge Sword is by a different author, \nCampbell, and it is about the effects of family support on suicidal be-\nhavior. It is not related to Durkheim's theory of suicide.  \n       I apologize for the mistake, and I hope this helps. \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 203 \n \nEditor’s prompt: I looked up the Lester's article in  Suicide and Life-Threatening \nBehavior, 44(6), 631-636.  It is not there. \nLLM output I'm very sorry for the trouble. It seems that the citation I gave you \nwas wrong again. According to my web search, the article by Lester is \nactually published in the journal Suicide and Life -Threatening Behavior, \nbut in a different year: 2015, not 2014. The correct citation should be:  \nLester, D. (2015). Durkheim's Theory of Suicide as Applied to the Fam-\nily: A Critical Evaluation. Suicide and Life -Threatening Behavior, 44(5), \n547-552. \nI apologize for the inconvenience, and I hope this helps.  \nEditor’s prompt: Volume 44 is in 2014 not 2015. I searched the entire volume \nfor 2015. There are no articles with Lester as the author. \n \nWhile frustrating, the i nability of ChatGPT to identify the requested refer-\nences is not surprising given that probabilistically generating human -like \nwriting and not information retrieval (e.g., locating accurate sources, news \nupdates, etc.) is the core function of the model. In this case, the user asked \nthe model to perform a task it is not designed to execute. Thus, while LLMs \ncan often generate factually correct information across various topics, digi-\ntal databases and database aggregators like ProQuest and EbscoHost re-\nmain the gold standard for accessing academic literature.  \nThat said, there are emerging technology solutions that effectively in-\ntegrate LLMs into information retrieval tasks. For instance, the integration \nof LLMs with Internet search capabilities is a significant step forward, en-\nhancing their ability to access up -to-date information. An example of this \nimprovement is demonstrated in a screenshot showing how ChatGPT -4, \nwhen prompted, can perform an Internet search to deliver an accurate re-\nsponse that matches the user's query (blue highlighted text contains hyper-\nlinks to sources): \n \n \n \n \n \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 204 \n \n \nPicture 1: ChatGPT Screenshot \n \nComputer scientists and AI engineers have also developed a process known \nas retrieval-augmented generation (RAG) that substantially reduces -- and in \nsome instances eliminates -- the incorrect information generated by these \nmodels. This approach enables LLMs to access and utilize information that \nextends far beyond their initial training datasets. RAG models a chieve this \nby first searching a curated database of information to find content relevant \nto the user’s prompt before generating a response. This approach allows \nLLMs to incorporate up -to-date and specific information, improving accu-\nracy and relevance, and  resolving the limitations of relying on outdated \nand/or inaccurate training data (Cai et al., 2022; IBM, 2023). \nA number of solutions using RAG are now widely available with easy -\nto-use, no -code user interfaces. Below we briefly demonstrate how using \nConsensusGPT, a retrieval -augmented model that links ChatGPT to the \n\nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 205 \n \nSemantic Scholar database, allows the use of natural language queries to \nidentify real scholarship on a topic of interest (Consensus, 2024). We began \nwith the following prompt: \nI have been ana lyzing Durkheim’s work on suicide in an effort to assist \nclinical social workers with clients facing suicidal ideation. I have been \nhaving problems finding criticism of Durkheim’s work published by psy-\nchologists. Could you provide me with some citations fo r articles that I \nmight consider? It is okay to tell me if you couldn't find anything.  \n \nAfter identifying relevant content in the Semantic Scholar database, the \nmodel then provided the following summary with links to the articles: \n \nI found several papers t hat discuss or critique Durkheim's work on sui-\ncide, particularly from perspectives that might be relevant to psycholo-\ngists. Below are some citations for articles that provide insights into crit-\nicisms or discussions around Durkheim's theories on suicide:  \n• Durkheim's Suicide and Problems of Empirical Research by H. Selvin \n(1958) discusses the empirical analysis methods Durkheim used in \nhis work and their implications for current research (Selvin, 1958). \nhttps://consensus.app/papers/durkheims-suicide-problems-empir-\nical-research-sel-\nvin/2e10ee937121540a95e4b32b89b79cc1/?utm_source=chatgpt  \n• Durkheim and French Psychiatric Views on Suicide During the 19th \nCentury by G. Berríos & M. Mohanna (1990) addresses Durkheim's \ncriticisms of 19th-century French psychiatry's vie ws on suicide, aim-\ning to correct the historical record (Berríos & Mohanna, 1990).[link \nnot included] \n• Psychological Models of Suicide by S. Barzilay & A. Apter (2014) pre-\nsents a critical discussion of psychological theories of suicidal behav-\nior following Durkheim and Freud, addressing the biological, social, \nand psychological nature of suicide (Barzilay & Apter, 2014). \nhttps://consensus.app/papers/models-suicide-barzi-\nlay/bdfab556368f565887ac8ffb300fed36/?utm_source=chatgpt  \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 206 \n \n• The Return of the Native: A Cultura l and Social -Psychological Cri-\ntique of Durkheim's Suicide Based on the Guarani-Kaiowá of South-\nwestern Brazil by C. Hamlin & R. Brym (2006) critiques Durkheim's \ntheory of suicide by incorporating cultural and social -psychological \nconsiderations, based on th e case study of the Guarani -Kaiowá \n(Hamlin & Brym, 2006). https://consensus.app/papers/return- na-\ntive-cultural-socialpsychological-critique-ham-\nlin/561898a122015fb7a28367e528d2ca91/?utm_source=chatgpt  \nThese papers provide various perspectives on Durkheim's w ork on sui-\ncide, including empirical critiques, historical corrections, psychological \nmodel discussions, and cultural -social psychological critiques. They \ncould serve as a starting point for understanding the range of criticisms \nof Durkheim's theories from a psychological viewpoint. \n \nWe verified that each of these papers does exist, and that the summary pro-\nvided by Consensus GPT was accurate based on a review of the abstracts. \nMoving forward, we think it is imperative that all social workers are \ntrained in t he ethical and effective use of LLMs. Our professional focus \nshould be on developing the digital literacy of students, practitioners, edu-\ncators, and scholars given the ongoing integration of generative AI technol-\nogies across social work education and practice. We see a clear place for LLM \nknowledge and skill development in the MSW curriculum, a call we make \nwith other colleagues in a forthcoming article in the Journal of Social Work \nEducation (Rodriguez et al., in press). We also support doctoral training i n \ngenerative AI along with a wide range of continuing education opportuni-\nties to ensure that digital literacy is maintained throughout one’s career. In \ndoing so, we can harness the full potential of generative AI technologies \nwhile ensuring their ethical and effective deployment. \n \nCorrespondence concerning this article should be directed to Bryan G. Vic-\ntor, Wayne State University, 5447 Woodward Avenue, Detroit, MI 48202, \nUnited States. E-mail: bvictor@wayne.edu\n \nForum: The Limitations of Large Language Models and Emerging Correctives to Support Social Work \nScholarship: Selecting the Right Tool for the Task \n \n \n \n \nINTERNATIONAL JOURNAL OF SOCIAL WORK VALUES AND ETHICS • VOLUME 21(1) | 207 \n \nReferences \nCai, D., Wang, Y., Liu, L., & Shi, S. (2022, July). Recent advances in retrieval-\naugmented text generation. In Proceedings of the 45th Interna-\ntional ACM SIGIR Conference on Research and Development in In-\nformation Retrieval (pp. 3417-3419). \nConsensus (2024, January 10). Introducing: Consensus GPT, your AI re-\nsearch assistant. https://consensus.app/home/blog/introducing-\nresearchgpt-by-consensus/ \nIBM (2023, August 5). What is retrieval-augmented generation? https://re-\nsearch.ibm.com/blog/retrieval-augmented-generation-RAG \nRawte, V., Sheth, A., & Das, A. (2023). A survey of hallucination in large \nfoundation models. Cornell University. \nhttps://doi.org/10.48550/arXiv.2309.05922 \nRodriguez, M.Y., Goldkind, L., Victor, B.G., Hiltz, B., & Perron, B.E. (in \npress). Introducing generative artificial intelligence into the MSW \ncurriculum: A proposal for the 2029 Educational Policy and Ac-\ncreditation Standards. Journal of Social Work Education. \nYao, J. Y., Ning, K. P., Liu, Z. H., Ning, M. N., & Yuan, L. (2023). LLM lies: Hal-\nlucinations are not bugs, but features as adversarial examples. \nCornell University. https://doi.org/10.48550/arXiv.2310.01469 \n \n \n \n  ",
  "topic": "Scholarship",
  "concepts": [
    {
      "name": "Scholarship",
      "score": 0.7657684087753296
    },
    {
      "name": "Task (project management)",
      "score": 0.7066966891288757
    },
    {
      "name": "Work (physics)",
      "score": 0.5649359226226807
    },
    {
      "name": "Computer science",
      "score": 0.4398709535598755
    },
    {
      "name": "Psychology",
      "score": 0.39582207798957825
    },
    {
      "name": "Data science",
      "score": 0.39022961258888245
    },
    {
      "name": "Political science",
      "score": 0.22796189785003662
    },
    {
      "name": "Engineering",
      "score": 0.14039814472198486
    },
    {
      "name": "Law",
      "score": 0.057930976152420044
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185443292",
      "name": "Wayne State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I164389053",
      "name": "Fordham University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}