{
  "title": "MST: Masked Self-Supervised Transformer for Visual Representation",
  "url": "https://openalex.org/W3192125374",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2223401012",
      "name": "Li, Zhaowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001718419",
      "name": "Chen Zhi-yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095316391",
      "name": "Yang Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1894711984",
      "name": "Li Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173613335",
      "name": "Zhu, Yousong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223418435",
      "name": "Zhao, Chaoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135385777",
      "name": "Deng Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231558180",
      "name": "Wu Liwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1962426794",
      "name": "Zhao Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108897329",
      "name": "Tang Ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2664671475",
      "name": "Wang, Jinqiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3108262825",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W3123072794",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3106528393",
    "https://openalex.org/W3122240496",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3106539090",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3095121901",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3204138855",
    "https://openalex.org/W3130807600",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3172615411",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W3110674625",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2887997457"
  ],
  "abstract": "Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\\%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training.",
  "full_text": "MST: Masked Self-Supervised Transformer for\nVisual Representation\nZhaowen Li‚Ä†‚ãÜ‚àó Zhiyang Chen‚Ä†‚ãÜ‚àó Fan Yang‚ó¶ Wei Li‚ó¶ Yousong Zhu‚Ä†\nChaoyang Zhao‚Ä† Rui Deng‚ó¶‚àá Liwei Wu‚ó¶ Rui Zhao‚ó¶ Ming Tang‚Ä†\nJinqiao Wang‚Ä†‚ãÜ\n‚Ä†National Laboratory of Pattern Recognition, Institute of Automation, CAS\n‚ãÜSchool of ArtiÔ¨Åcial Intelligence, University of Chinese Academy of Sciences\n‚ó¶SenseTime Research\n‚àáUniversity of California, Los Angeles\n{zhaowen.li,zhiyang.chen,yousong.zhu,chaoyang.zhao}@nlpr.ia.ac.cn\n{tangm,jqwang}@nlpr.ia.ac.cn\n{yangfan1,liwei1,dengrui,wuliwei,zhaorui}@sensetime.com\nAbstract\nTransformer has been widely used for self-supervised pre-training in Natural\nLanguage Processing (NLP) and achieved great success. However, it has not been\nfully explored in visual self-supervised learning. Meanwhile, previous methods\nonly consider the high-level feature and learning representation from a global\nperspective, which may fail to transfer to the downstream dense prediction tasks\nfocusing on local features. In this paper, we present a novel Masked Self-supervised\nTransformer approach named MST, which can explicitly capture the local context\nof an image while preserving the global semantic information. SpeciÔ¨Åcally, inspired\nby the Masked Language Modeling (MLM) in NLP, we propose a masked token\nstrategy based on the multi-head self-attention map, which dynamically masks some\ntokens of local patches without damaging the crucial structure for self-supervised\nlearning. More importantly, the masked tokens together with the remaining tokens\nare further recovered by a global image decoder, which preserves the spatial\ninformation of the image and is more friendly to the downstream dense prediction\ntasks. The experiments on multiple datasets demonstrate the effectiveness and\ngenerality of the proposed method. For instance, MST achieves Top-1 accuracy of\n76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which\noutperforms supervised methods with the same epoch by 0.4% and its comparable\nvariant DINO by 1.0%. For dense prediction tasks, MST also achieves 42.7% mAP\non MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only\nwith 100-epoch pre-training.\n1 Introduction\nAs Yann LeCun said, ‚Äúif intelligence is a cake, the bulk of the cake is unsupervised learning‚Äù. This\nsentence reÔ¨Çects that Un-/Self-supervised Learningplayed a central role in the resurgence of deep\nlearning. Common approaches focus on designing different pretext tasks [10, 29, 14, 3, 4, 6, 5, 13, 1,\n36] and aim to learn useful representations of the input data without relying on human annotations. It\nthen uses those representations in downstream tasks, such as image classiÔ¨Åcation, objection detection,\nand semantic segmentation.\n‚àóWork done as an intern at SenseTime Research.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.05656v2  [cs.CV]  24 Oct 2021\nIn computer vision, previous methods focus on designing different pretext tasks. One of the most\npromising directions among them is contrastive learning/instance discrimination [ 17, 23], which\nregards each instance in the training dataset as a single category. Based on instance discrimination\n[14, 4, 6, 5, 13, 1], some methods show the effectiveness in the image classiÔ¨Åcation task. They\nsuccessfully bridge the performance gap between self-supervised and full-supervised methods.\nHowever, almost all of self-supervised learning methods, which formulate the learning as image-level\nprediction using global features, are suboptimal in the pixel-level predictions [ 14, 1, 13], such as\nobject detection and semantic segmentation. Also, InfoMin [ 35] Ô¨Ånds that high-level features do\nnot truly matter in transferring to dense prediction tasks. Here, current self-supervised learning may\noverÔ¨Åt to image classiÔ¨Åcation while not being well tamed for downstream tasks requiring dense\nprediction.\nMeanwhile, large-scale pre-trained models have become the prevailing formula for a wide variety\nof Natural Language Processing (NLP) tasks due to its impressive empirical performance. These\nmodels typically abstract semantic information from massive unlabeled corpora in a self-supervised\nmanner. The Masked Language Modeling (MLM) [10] has been widely utilized as the objective for\npre-training language models. In the MLM setup, a certain percentage of tokens within the input\nsentence are randomly masked, and the objective is to predict the original information of the masked\ntokens based only on its context. In NLP tasks, we found that the different mask strategies used in\nthe MLM framework had a great impact on the performance of the model. However, in the Ô¨Åeld of\nvision, images have higher-dimensional, noisy, and redundant format compared to text. The main\ninformation of input images is randomly distributed in tokens. If tokens are randomly masked, it will\nlead to poor performance. Some of previous methods use random tokens, such as iGPT [3] and ViT\n[11]. iGPT trains self-supervised Transformers using an amount of 6801M parameters and achieves\n72.0% Top-1 accuracy on ImageNet by masking and reconstructing pixels, while ViT trains ViT-B\nmodel on the JFT-300M dataset, and the result is signiÔ¨Åcantly lower than the supervised model.\nThe random MLM is prone to mask the tokens of crucial region for images, resulting in misunder-\nstanding, and is not suitable for directly applying to self-supervised vision Transformers. In order\nto avoid masking the tokens of crucial region, we propose a masked token strategy based on the\nmulti-head self-attention map, which dynamically masks some tokens of patches without damaging\nthe crucial structure for self-supervised learning. Notably, the strategy would not increase the training\ntime. Also, predicting original tokens alone may cause the model to over-emphasize local region, and\ntherefore suppress the ability to recognize objects. Hence, in this paper, we present a novel Masked\nSelf-supervised Transformer approach named MST, which can explicitly capture the local context\nof an image while preserving the global semantic information. In addition, a global image decoder\nis further exploited to recover the spatial information of the image and is thus more friendly to the\ndownstream dense prediction tasks.\nWe validate our method on multiple visual tasks. In particular, on the ImageNet linear evaluation\nprotocol, we reach 76.9% top-1 accuracy with DeiT-S and achieve the state-of-the-art performance.\nOverall, we make the following contributions:\n‚Ä¢ We propose a new masked self-supervised transformer approach called MST. It makes full use\nof self-attention map to guide the masking of local patches, thus enhancing the understanding\nof local context semantics in pre-training without damaging the crucial structure.\n‚Ä¢ Our method can effectively recover the spatial information of the image by a global image\ndecoder, which is vital for the downstream dense prediction task and greatly improves the\nversatility and scalability of the pre-training model.\n‚Ä¢ Extensive experiments demonstrate the effectiveness and transfer ability of our method. SpeciÔ¨Å-\ncally, the results on ImageNet [9], MS COCO [18] and Cityscapes [8] show that our method\noutperforms previous state-of-the-art methods.\n2 Related Works\n2.1 Self-supervised visual representation learning\nFollowing MLM paradigm in NLP [10, 25], iGPT [3] trains self-supervised Transformers by masking\nand reconstructing pixels, while ViT [ 11] masks and reconstructs patches. Recently, the most\ncompetitive pretext task for self-supervised visual representation learning is instance discrimination\n2\nTransformer Encoder\nLinear Projection\nLinear Projection\n Transformer Encoder\nMLPHead\nMLPHead\nC MExtralearnable[mask]embedding*PositionembeddingProbability distributions\nTeacher\nStudent\nSelf-AttentionMap\nùëÉ!\nExtralearnable[class]embedding\nùêø\"#(ùëÉ$,ùëÉ!)\nùêø%&!$'%()*(ùêº',ùêº*$)\nùêº' ùêº*$\nDecoder\nùëÉ$\nAttention-guided Mask Strategy\nFigure 1: The pipeline of our MST. Both student and teacher share the same architecture with different parameters.\nInspired by the MLM in NLP, the attention-guided mask strategy is Ô¨Årst introduced to mask the tokens of the\nstudent network based on the output self-attention map of the teacher network. The basic principle is to mask\nsome patches with low responses and does not destroying the important foreground regions. Then, a global\nimage decoder is used to reconstruct the original image based on the masked and unmasked tokens. Finally, the\ntotal loss function consists of the self-supervised cross entropy loss and the restoring loss.\n[14, 4, 6, 5, 13, 1]. The learning objective is simply to learn representations by distinguishing each\nimage from others, and this approach is quite intractable for large-scale datasets. MoCo [14] improves\nthe training of instance discrimination methods by storing representations from a momentum encoder\ninstead of the trained network. SimCLR [4] shows that the memory bank can be entirely replaced\nwith the elements from the same batch if the batch is large enough. In order to avoid comparing\nevery pair of images and incur overÔ¨Åtting, BYOL [ 13] directly bootstraps the representations by\nattracting the different features from the same instance. SwA V [1] maps the image features to a set of\ntrainable prototype vectors and proposes multi-crop data augmentation for self-supervised learning\nto increase the number of views of an image. MoCov3 [7] and DINO [2] apply the self-supervised\nlearning methods of computer vision to Transformers and achieve superior performance in image\nclassiÔ¨Åcation task. These works achieve comparable results compared to supervised ImageNet [9]\npre-training. The success of these methods suggest that it is of central importance to learn invariant\nfeatures by matching positive samples. However, almost all of these self-supervised learning methods\nformulate the learning process as image-level prediction using global features, so they lack the ability\nto pay attention to local features.\n2.2 Self-supervised dense prediction learning\nBased on the existing instance discrimination, some researchers propose self-supervised dense\nprediction methods. Self-EMD [19] adopts Earth Mover‚Äôs Distance (EMD) to compute the similarity\nbetween two embedding. Insloc [ 33] pastes image instances at various locations and scales onto\nbackground images. The pretext task is to predict the instance category given the composited images\nas well as the foreground bounding boxes. PixPro [ 31] directly applies contrastive learning at the\npixel level. DenseCL [28] presents dense contrastive learning by optimizing a pairwise contrastive\nloss at the pixel level between two views of input images. V ADeR [24] and FlowE [32] also learn\ndense image representations for downstream detection and segmentation tasks. Meanwhile, HED [34]\nfocuses on downstream segmentation task. These methods also show the effectiveness in detection\nand segmentation tasks but get poor performance on image classiÔ¨Åcation tasks. In a word, these\nmethods overÔ¨Åt a single task and cannot train a general pre-training model.\n3\nFigure 2: Illustration of our attention-guided mask strategy. It improves by preserving key patterns in images,\ncompared with the original random mask. Description of images from left to right: (a) the input image, (b)\nattention map obtained by self-attention module, (c) random mask strategy which may cause loss of crucial\nfeatures, (d) our attention-guided mask strategy that only masks nonessential regions. In fact, the masked strategy\nis to mask tokens.\n3 Methods\nThe pipeline of our proposed MST is shown in Figure 1. We propose a Masked Self-supervised\nTransformer (MST) approach, which creatively introduces attention-guided mask strategy and uses\nit to complete image restoration task. Our method is combined with some classical components\nof instance discrimination, such as the momentum design, asymmetric data augmentations, and\nmulti-crop strategies. Here, we Ô¨Årst review the basic instance discrimination method in 3.1. Then, the\nmechanism and effect of our attention-guided mask strategy are explained in 3.2. Finally, we describe\nthe reconstruction branch and the training target of our method in 3.3.\n3.1 The basic instance discrimination method\nAs noted in prior works[4, 14, 13, 29, 1], many existing augmentation policies adopt random resized\ncropping, horizontal Ô¨Çipping, color jittering and so on. We generate multiple views for each image\nxunder random data augmentation according to multi-crop [ 1]. This operation can acquire two\nstandard resolution crops x1 and x2 representing the global view and sample N low-resolution crops\nindicating partial view. They are encoded by two encoders, teacher network ft and student network\nfs, parameterized by Œ∏t and Œ∏s respectively, and outputting vectors Ot and Os. Both encoder fs and\nft consist of a Transformer backbone and a projection head [5], which share the same architecture\nwith different parameters. The parameters Œ∏t of Ô¨Åxed encoder ft is updated by the moving-average of\nŒ∏s according to Eq (1).\nŒ∏t = m‚àóŒ∏t + (1‚àím) ‚àóŒ∏s (1)\nGiven a Ô¨Åxed teacher network ft, the student network fs learns the parameters Œ∏s by minimizing\ncross entropy loos as Eq (2).\nLCE (Œ∏s) =\n‚àë\ni‚àà{1,2}\nN+2‚àë\nj=1\njÃ∏=i\n‚àíft(Œ∏t; xi)log(fs(Œ∏s; xj)) (2)\n3.2 Masked token strategy\nRandom mask strategy. Inspired of the MLM strategy for natural language pre-training, we apply\nthe random mask strategy to self-supervised learning. Given a dataset Qwithout manual annotations,\nand e = (e1,...,e n) denote a image of ntokens, where i= 1,...,n . Let m = (m1,...,m n) denote a\nbinary vector of length n, where mi ‚àà{0,1}, representing the mask over image. According to BERT\n[10], the m can be obtained with probability pby Eq (3), and the pis 0.15 by default.\nmi =\n{1, prob i <p\n0, otherwise (3)\n4\nAlgorithm 1 Pseudo code of attention-guided mask strategy in a PyTorch-like style.\n# l(): linear projection\n# f_s: backbone + projection head\n# f_t: backbone + projection head\n# mask_embedding: learnable token\n# p: mask probability\ne_t = f_t.l(x) # linear projection\npatch_attention, _ = f_t.Transformer(e_t)\nimportance = measure_importance(patch_attention) # acquire threshold\ne_s = f_s.l(x) # linear projection\nmask = M(e_s, patch_attention, importance) # generate mask\ne_s_masked = (1-mask) * e_s + mask * mask_embedding\n_, _ = f_s.Transformer(e_s_masked)\ndef M(embedding, patch_attention, importance):\nB, L, _ = embedding.shape\nmask_tokeep = zeros((B, L))\nmask_remove = bernoulli(ones((B, L)) * p)\nmask = where(importance > patch_attention, mask_tokeep, mask_remove)\nreturn mask\nAccording to Eq (3), the tokens of crucial and nonessential regions have the same probability of being\nmasked. As shown in Figure 2 (c), we observe that the random mask strategy may eliminate tokens\nof crucial regions that are responsible for recognizing objects, resulting in indistinguishable semantic\nfeatures for input images. The random mask strategy is prone to mask crucial regions for images, and\nsuppress the ability of network to recognize objects. It is not suitable to directly apply this strategy\nto self-supervised vision Transformers and the overall performance would deteriorate if the mask\nstrategy is not properly modulated.\nAttention-guided mask strategy. In this section, we propose our attention-guided mask strategy\nfor dynamically controlling the Ô¨Ådelity of masked tokens and thereby decreasing the probability of\nmasking crucial regions in self-supervised Transformer. Meanwhile, our strategy does not increase\nadditional time consumption. Our algorithm is shown as Alg. 1.\nOur framework consists of two networks, teacher network ft and student network fs, with the same\ntransformer architecture. Let xdenotes the input image. It is Ô¨Årstly projected to a sequence of n1-d\ntokens e = e1,...,e n, and then processed by several self-attention layers. Each self-attention layer\n[27] owns three groups of embeddings for one token, denoted as Qi(query), Ki(key), Vi(value). The\nattention map is calculated as the correlation between the query embedding of class token Qcls and\nkey embeddings of all other patches K. It is averaged for all heads as Eq (4). We output the attention\nmap from the last layer in the teacher network to guide our strategy.\nAttn= 1\nH\nH‚àë\nh=1\nSoftmax(Qcls\nh ¬∑KT\nh‚àö\nd\n) (4)\nWe sort the attention of different patches for each image in ascending order, and take the sorted\nattention value of 1/numof total tokens as the threshold œÑ, where numis the hyperparameter for\nselection. This means that the lowest 1/numof total tokens are selected as the masked candidates.\nThe student model receives the importance of different patches and generates the mask m with\nprobability p, according to the Bernoulli distribution as Eq (5). probi refers to the probability of\nrandomly generated.\nmi =\n{1, prob i <p and Attni <œÑ\n0, otherwise (5)\n5\nWe use m‚äôe to denote the Ô¨Ånal masked tokens as Eq (6). Follow the BERT [10], the masked regions\nare Ô¨Ålled with a learnable mask embedding [MASK]. Our strategy can ensure the patches with the\nhighest scores are always presented (in Figure 2).\n(m ‚äôe) =\n{[MASK], m i = 1\nei, m i = 0 (6)\nThe attention-guided mask strategy can beneÔ¨Åt pre-training models in two ways:\n1. The models utilize contextual information to understand the relationship of different patches,\nthus preserving the global semantic information of the image while paying more attention to the\nlocal details of the image.\n2. Our strategy can avoid masking crucial regions while replacing nonessential regions with the\nlearnable mask embedding, making the models focus on the crucial regions.\n3.3 Masked self-supervised transformer\nIn MLM, mask denotes the complementary set of mask, that is, mask = 1 ‚àímask. The loss\nfunction of MLM pre-training strategy over one data is shown as Eq (7), where P(xi|Œ∏,mask ‚äôt)\nis the probability of the network correctly predicting ti given the masked token. t indicates the text\ntokens. That is, the network only restores the masked tokens.\nlMLM (Œ∏; t,m) =‚àílogP(mask ‚äôt|Œ∏,mask ‚äôt) =‚àí\n‚àë\ni:mi=1\nlogP(ti|Œ∏,mask ‚äôt) (7)\nThere are a sub-sequence M ‚äÇ[1,n] such that each index i independently has probability p of\nappearing in M, and the overall loss function for training the network is shown as Eq (8). Qis the\ndataset in Eq (8). In pre-training, the MLM strategy minimizes the overall loss over pre-training\ndataset.\nLMLM (Œ∏) = E\nt‚àºQ\nE\nM\nlMLM (Œ∏; t,mask). (8)\nHowever, MLM only predicts the masked tokens according to Eq (8). Different from original MLM,\nour method encourage the network reconstruct the original input images. We argue that a pixel-level\nrestoration task can make the network avoid overÔ¨Åtting patch prediction, therefore enhancing the\nability to capture the pixel-level information and recovering spatial structure from a Ô¨Åner grain. Since\nconvolution neural networks (CNNs) have the ability of inductive biases, the restoration task adopts\nCNN as the decoder module, with convolution layers and up-sampling operations alternately stacked.\nTo maximally mitigate the adversarial effect [37], the up-sampling operations are restricted to 2√ó.\nHence, a total of 4 operations are needed for reaching the full resolution from H\n16 √óW\n16 . And the\nrunning mean and running variance of BN are only updated from the global views. The global\nimage decoder consists of the Transformer and decoder. The restoration task is only performed on\nthe student network fs(¬∑). For a decoder g(¬∑) with parameters Œ∏g, its loss function over a image\nx‚ààRH√óW and a mask m ‚àà(0,1)n as Eq (9).\nlrestoring(Œ∏s,Œ∏g; x,m) = E\nH√óW\n|x‚àíg(Œ∏g; fs(Œ∏s; x,m))| (9)\nThe overall loss function for training the network is shown as Eq (10), and we only need the parameters\nŒ∏s of student network fs.\nLrestoring(Œ∏s) =Lrestoring(Œ∏s,Œ∏g) = E\nx‚àºX\nE\nH√óW\n|x‚àíg(Œ∏g; fs(Œ∏s; x,m))| (10)\nTherefore, the total loss is shown as Eq (11), and the MST minimizes the loss over ImageNet [ 9]\ndataset in pre-training.\nLtotal(Œ∏s) =Œª1 ‚àóLCE (Œ∏s; x) +Œª2 ‚àóLrestoring(Œ∏s; x) (11)\n6\nTable 1: Comparison of popular self-supervise learning methods on ImageNet. Throughput (im/s) is\ncalculated on a single NVIDIA V100 GPU with batch size 128. ‚Ä†adopts the linear probing of DINO.\nMethod Architecture Parameters epoch im/s Linear k-NN\nSupervised\nRes50[16] 23\n100 1237 76.5 -\nMoCov2 [6] 800 1237 71.1 61.9\nBYOL [13] 1000 1237 74.4 64.8\nSwA V [1] 800 1237 75.3 65.7\nSupervised\nDeiT-S[26] 21\n300 1007 76.4 -\nSwA V [1] 300 1007 67.1 -\nSimCLR [4] 300 1007 69.0 -\nBYOL [13] 300 1007 71.0 -\nMoCov3 [7] 300 1007 72.5 -\nMOBY [30] 300 1007 72.8 -\nBYOL [13] 800 1007 71.4 66.6\nMoCov2 [6] 800 1007 72.7 64.4\nSwA V [1] 800 1007 73.5 66.3\nDINO [2] 300 1007 75.2 72.8\nDINO‚Ä†[2] 300 1007 75.9 72.8\nDINO‚Ä†[2] 800 1007 77.0 74.5\nOurs ‚Ä† 100 1007 75.0 72.1\nOurs 300 1007 76.3 75.0\nOurs ‚Ä† 300 1007 76.9 75.0\nSupervised\nSwin-T[20] 28\n300 755 81.2 -\nMoBY [30] 100 755 70.9 57.34\nOurs 100 755 73.8 66.20\n4 Experiments\nSeveral experiments with MST are conducted in this section. We Ô¨Årst train self-supervised models\nwith different transformer architectures on ImageNet benchmark, and then examine their transfer\ncapacity with downstream tasks like object detection and semantic segmentation. After that, ablation\nstudies are introduced to elaborate on how our method could achieve state-of-the-art performance.\n4.1 Pre-training settings\nDataset and Models Our method is validated on the popular ImageNet 1k dataset [9]. This dataset\ncontains 1.28M images in the training set and 50K images in the validation set from 1000 classes.\nWe only use the training set during the process of self-supervised learning. As to models, we\nchoose the classical DeiT-S [26] and popular Swin-T [20] as representatives of all transformer-based\narchitectures. After the backbone, a 3-layer MLP with hidden dimension 2048 is added as the\nprojection head. When evaluating our pretrained model, we both use the k-NN algorithm and train a\nlinear classiÔ¨Åcation for 100 epochs as former works. Top-1 accuracy is reported.\nTraining ConÔ¨Ågurations Our model is optimized by AdamW [22] with learning rate 2 √ó10‚àí3 and\nbatch size 1024. Weight decay is set to be 0.04. We adopt learning rate warmup [12] in the Ô¨Årst 10\nepochs, and after warmup the learning rate follows a cosine decay schedule [21]. The model uses\nmulti-crop similar to [1] and data augmentations similar to [13]. The setting of momentum, tempera-\nture coefÔ¨Åcient, and weight decay follows [2]. The coefÔ¨Åcient Œª1 of basic instance discrimination\ntask is set as 1.0 while the restoration task Œª2 is set as 0.6.\n4.2 Compared with other methods on ImageNet\nWe compare our method with other prevailing algorithms in Table 1. All these methods share the\nsame backbone for fair comparison. Our 300-epoch model achieves 76.9% top-1 accuracy with linear\nprobing. It outperforms previous best algorithm DINO by 1.7% at the same training epochs, and\neven approaches the performance of DINO with a much longer training schedule (77.0% with 800\nepochs). It should be emphasized that our algorithm relieves the need of extreme long training time\nfor self-supervised learning, and is able to obtain a decent result (75.0%) with only 100 epochs.\n7\nMST is general to be applied with any other transformer-based architectures. Here we use the popular\nSwin-T for an example. It has similar amount of parameters with DeiT-S. Using the same training\nepochs, MST outperforms MoBY by 1.8%, which is a self-supervised learning method designed\ndelicately for Swin-T. Swin-T shares the same hyperparameters with DeiT-S, there it can still be\nimproved by further tuning.\n4.3 Object detection and instance segmentation\nSince Swin-Transformer achieves state-of-the-art under supervised training, it is adopted as the\nbackbone to validate the transfer ability of our method in the task of object detection and instance\nsegmentation. We perform object detection experiments with MS COCO [ 18] dataset and Mask\nR-CNN detector [15] framework. MS COCO is a popular benchmark for object detection, with 118K\nimages in training set and 5K images for validation. This dataset contains annotations for 80 classes.\nBox AP and mask AP are reported on the validation set. As to training settings, we follow the default\n1x schedule with 12 epochs. The shorter edges of the input images are resized to be 800 and the\nlonger edges are limited by 1333 pixels. AdamW optimizer is used, and all hyper-parameters follow\nthe original paper.\nIn Table 2, we show the performance of the learned representation by different self-supervised\nmethods and supervised training. For fair comparison, all these methods are pre-trained with 100\nepochs. We observe that our method achieves the best results with 42.7% bbox mAP and 38.8%\nmask mAP. It outperforms the ImageNet supervised model by 1.2% and 0.5%, and MoBY results by\n1.2% and 0.5% with the same epoch. The results indicate that MST not only performs well on image\nclassiÔ¨Åcation task, but also performs well on downstream dense prediction task. Therefore it has a\nstrong transfer ability.\nTable 2: Results of object detection and instance segmentation Ô¨Åne-tuned on MS COCO.\nMethod Backbone Epoch box AP mask AP\nAPbbox APbbox\n50 APbbox\n75 APmask APmask\n50 APmask\n75\nSupervised Swin-T [20] 300 43.7 66.6 47.7 39.8 63.3 42.7\n100 41.6 64.6 45.4 38.4 61.5 41.0\nMoBY [30]\nSwin-T [20] 100\n41.5 64.1 45.2 38.3 61.0 40.8\nDINO [2] 42.2 64.6 46.3 38.7 61.5 41.3\nOurs 42.7 65.1 46.7 38.8 61.8 42.5\n4.4 Semantic segmentation\nSETR [37] provide a semantic segmentation framework for standard Vision Transformer. Hence, we\nadopt the SETR as the semantic segmentation strategy on Cityscapes [8]. Cityscapes contains 5000\nimages, with 19 object categories annotated in pixel level. There are 2975, 500, and 1525 images in\ntraining, validation, and testing set respectively. We follow the training conÔ¨Åg as original SETR. For\nfair comparison, we both use the 300-epoch pretrained model for DINO and our method.\nAs shown in Table 3, it illustrates the comparison of supervised method, DINO, and our method on\nthis evaluation. Our method achieves the highest mIoU 74.7% and mAcc 82.35%. It outperforms\nboth supervised results (+2.71% mIoU and +2.05% mAcc) and DINO pretrained results (+1.08%\nmIoU and +1.03% mAcc). Our model is also suitable to transfer for the semantic segmentation task.\nTable 3: Results of semantic segmentation Ô¨Åne-tuned on Cityscapes.\nMethod Backbone Pre-Epochs Schedule mIoU mAcc aAcc\nSupervised DeiT-S [26] 300 40K 71.33 80.30 94.99\nDINO [2] DeiT-S [26] 100 40K 72.96 81.32 95.37\nOurs 74.04 82.35 95.42\n8\nTable 4: Linear probe results of different\nmask strategy (DeiT-S).\nMask Strategy Top-1 acc (%)\nNone 73.1\nRandom Mask 63.2\nAttention-Guided 73.7\nTable 5: The setting of hyper-parameters for attention-\nbased mask strategy.\nnum\np 0.05 0.10 0.15\n1 63.2 61.4 60.6\n2 73.7 64.4 62.7\n4 73.6 73.6 66.7\n8 73.6 73.9 73.6\n4.5 Ablation studies\nIn this section, we conduct some ablation studies to elaborate on the effectiveness of our method. All\nablation experiments are conducted under 100-epoch setting. By default, only the cls token from the\nlast layer is used to train the linear classiÔ¨Åer.\n4.5.1 Impact of different mask strategy\nTable 4 shows the impact of different mask strategies. We train DeiT-S with random mask strategy[10],\nattention-guided mask strategy and no mask. For fair comparison, all methods mask with the same\nprobability p. It can be observed that the performance of random mask strategy degrades. This\nstrategy would probably suppress the ability to recognize the object in the images (from 73.1 to 63.2).\nRandom mask strategy may destroy the tokens of crucial regions of original image which may be\nindispensable for recognizing object. The masked input may have incomplete or even misleading\ninformation. On the contrary, the performance of our attention-guided mask strategy has a steady\nimprovement (from 73.1 to 73.7). Essential regions are mostly preserved, which could be a strong\nproof of our hypothesis.\n4.5.2 Impact of different mask hyper-parameters\nTable 5 validates the performance of different mask hyper-parameters under attention-guided mask\nstrategy. We sort the attention map of different patches for each image in ascending order, and split\nthe Ô¨Årst 1/numpatches as the masked candidates. Removing these candidates can force the network\nto learn local features from adjacent patches, therefore strengthening the capacity of modeling local\ncontext without destroying the semantics. These candidates are masked according to the probability\np. Top-1 accuracy of linear evaluation on ImageNet is shown in Table 5. When numis set to 8, any\nchoice of pcan get a robust result, which suggests that the last 1/8 patches are relatively safe to be\nmask candidates.\n4.6 Impact of w/o BN\nFormer work [2] found that the performance will be better if dropping BN in the projection head. We\nargue that the degradation is not caused by BN. As shown in Table 6, normal BN downgrades the\nperformance of baseline model, while the update rule introduced in Section 3.3 helps improve top-1\naccuracy slightly. This may be due to the need to keep consistent structure with the global image\nDecoder since the image Decoder consists of Conv-BN-ReLu.\nTable 6: Impact of Batch Normalization in projection head.\nw/o BN w/ BN\nBaseline 72.4 71.6\nOurs 73.1 73.9\n9\n5 Conclusion\nIn this paper, we investigate the two problems of current visual self-supervised learning, namely lack\nof local information extraction and loss of spatial information. To overcome the above problems, we\npropose a new self-supervised learning method based on transformer called MST. The proposed MST\nexploits an attention-guided mask strategy to capture the local relationships between patches while\nalso preserving the global semantic information. It is noted that the attention-guided mask strategy is\nbased on the multi-head self-attention map extracted from the teacher model and does not cause extra\ncomputation cost. In addition, a global image decoder is further used under the attention-guided mask\nstrategy to recover the spatial information of the image, which is vital for dense prediction tasks. The\nproposed method shows good versatility and scalability in multiple downstream visual tasks.\nBroader Impact\nThe MST is to provide pre-trained models with better feature extraction capabilities for popular\ncomputer vision tasks. Therefore, the potential positive societal impact of our method:\n‚Ä¢ Improved road safety in autonomous driving by detecting pedestrians.\n‚Ä¢ Safe human robot collaboration in factories with robots taking on risk prone jobs thus saving\nlives.\n‚Ä¢ Assistive robots for elderly care.\nMeanwhile, the potential negative societal impact:\n‚Ä¢ Large scale drone surveillance.\n‚Ä¢ Face recognition leads to privacy exposure.\nAcknowledgments and Disclosure of Funding\nThis work was supported by National Natural Science Foundation of China under Grants\nNo.61772527, No.61976210, No.62002357, No.61876086, No.61806200, No.62002356,\nNo.62006230, and No.62076235.\nReferences\n[1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning\nof visual features by contrasting cluster assignments. In: Advances in Neural Information\nProcessing Systems. vol. 33, pp. 9912‚Äì9924 (2020)\n[2] Caron, M., Touvron, H., Misra, I., J√©gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerg-\ning properties in self-supervised vision transformers. arXiv: Computer Vision and Pattern\nRecognition (2021)\n[3] Chen, M., Radford, A., Child, R., Wu, J.K., Jun, H., Luan, D., Sutskever, I.: Generative\npretraining from pixels. In: Proceedings of the International Conference on Machine Learning\n(ICML). vol. 1, pp. 1691‚Äì1703 (2020)\n[4] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709 (2020)\n[5] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., Hinton, G.E.: Big self-supervised models\nare strong semi-supervised learners. In: Advances in Neural Information Processing Systems.\nvol. 33, pp. 22243‚Äì22255 (2020)\n[6] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive\nlearning. arXiv preprint arXiv:2003.04297 (2020)\n[7] Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers.\narXiv preprint arXiv:2104.02057 (2021)\n10\n[8] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth,\nS., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: Proceedings\nof the Conference on Computer Vision and Pattern Recognition (CVPR). pp. 3213‚Äì3223 (2016)\n[9] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical\nimage database. In: Proceedings of the Conference on Computer Vision and Pattern Recognition\n(CVPR) (2009)\n[10] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.N.: Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers). pp. 4171‚Äì4186 (2018)\n[11] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16\nwords: Transformers for image recognition at scale. In: International Conference on Learning\nRepresentations (ICLR) (2021)\n[12] Goyal, P., Doll√°r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A.,\nJia, Y ., He, K.: Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint\narXiv:1706.02677 (2017)\n[13] Grill, J.B., Strub, F., Altch√©, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires,\nB.A., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu, K., Munos, R., Valko, M.: Bootstrap your\nown latent: A new approach to self-supervised learning. In: Advances in Neural Information\nProcessing Systems (NeurIPS). vol. 33, pp. 21271‚Äì21284 (2020)\n[14] He, K., Fan, H., Wu, Y ., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual\nrepresentation learning. arXiv preprint arXiv:1911.05722 (2019)\n[15] He, K., Gkioxari, G., Doll√°r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the International\nConference on Computer Vision (ICCV) (2017)\n[16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceed-\nings of the Conference on Computer Vision and Pattern Recognition (CVPR) (2016)\n[17] Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A.,\nBengio, Y .: Learning deep representations by mutual information estimation and maximization.\nInternational Conference on Learning Representations (ICLR) (2019)\n[18] Lin, T.Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll√°r, P., Zitnick, C.L.:\nMicrosoft coco: Common objects in context. In: Proceedings of the European Conference on\nComputer Vision (ECCV) (2014)\n[19] Liu, S., Li, Z., Sun, J.: Self-emd: Self-supervised object detection without imagenet. arXiv\npreprint arXiv:2011.13677 (2020)\n[20] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.: Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)\n[21] Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983 (2016)\n[22] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference\non Learning Representations (2018)\n[23] Oord, A.v.d., Li, Y ., Vinyals, O.: Representation learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748 (2018)\n[24] Pinheiro, P.O., Almahairi, A., Benmalek, R.Y ., Golemo, F., Courville, A.: Unsupervised learning\nof dense visual representations. arXiv preprint arXiv:2011.05499 (2020)\n[25] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by\ngenerative pre-training (2018)\n[26] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J√©gou, H.: Training data-efÔ¨Åcient\nimage transformers & distillation through attention. arXiv preprint arXiv:2012.12877 (2020)\n[27] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,\nPolosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference\non Neural Information Processing Systems. vol. 30, pp. 5998‚Äì6008 (2017)\n11\n[28] Wang, X., Zhang, R., Shen, C., Kong, T., Li, L.: Dense contrastive learning for self-supervised\nvisual pre-training. arXiv preprint arXiv:2011.09157 (2020)\n[29] Wu, Z., Xiong, Y ., Yu, S.X., Lin, D.: Unsupervised feature learning via non-parametric instance\ndiscrimination. In: Proceedings of the Conference on Computer Vision and Pattern Recognition\n(CVPR) (2018)\n[30] Xie, Z., Lin, Y ., Yao, Z., Zhang, Z., Dai, Q., Cao, Y ., Hu, H.: Self-supervised learning with swin\ntransformers. arXiv preprint arXiv:2105.04553 (2021)\n[31] Xie, Z., Lin, Y ., Zhang, Z., Cao, Y ., Lin, S., Hu, H.: Propagate yourself: Exploring pixel-level\nconsistency for unsupervised visual representation learning. arXiv preprint arXiv:2011.10043\n(2020)\n[32] Xiong, Y ., Ren, M., Zeng, W., Urtasun, R.: Self-supervised representation learning from Ô¨Çow\nequivariance. arXiv preprint arXiv:2101.06553 (2021)\n[33] Yang, C., Wu, Z., Zhou, B., Lin, S.: Instance localization for self-supervised detection pretrain-\ning. arXiv preprint arXiv:2102.08318 (2021)\n[34] Zhang, X., Maire, M.: Self-supervised visual representation learning from hierarchical grouping.\narXiv preprint arXiv:2012.03044 (2020)\n[35] Zhao, N., Wu, Z., Lau, R.W.H., Lin, S.: What makes instance discrimination good for transfer\nlearning. In: International Conference on Learning Representations (ICLR) (2021)\n[36] Zhao, Z., Samel, K., Chen, B., Song, L.: Proto: Program-guided transformer for program-guided\ntasks. arXiv preprint arXiv:2110.00804 (2021)\n[37] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J., Xiang, T., Torr, P.H.S.,\nZhang, L.: Rethinking semantic segmentation from a sequence-to-sequence perspective with\ntransformers. arXiv preprint arXiv:2012.15840 (2020)\n12\nAppendix: Masked Self-Supervised Transformer for Visual\nRepresentation\nA The setting of computation resources\nIn ablation studies, the MST with 1024 images is trained in 128 AMD DCUs that are publicly\navailable in Sugon Cloud. For verifying the generality of the results, the pre-trained model is used to\nvalidate downstream experiments for 32 Nvidia Tesla V100 GPUs. Meanwhile, the same random\nseed is set for fair comparison. Also, we report the average result after running multiple experiments.\nFor 100 epochs, the standard error of linear probing is 0.2236%. For 300 epochs, the standard error\nof linear probing is 0.1581%.\nB Data augmentation\nThe image augmentation pipeline consists of the following transformations: random resized cropping,\nhorizontal Ô¨Çipping, color jittering, grayscale conversion, Gaussian blurring, solarization, and multi-\ncrop. The random resized cropping and multi-crop transformations are always applied, while the rest\nof transformations are applied randomly, with some probability. This probability is different for the\ntwo distorted views in the blurring and solarization transformations. We use the same augmentation\nparameters as BYOL besides multi-crop. The multi-crop follows SwA V [1] and DINO [2]. Each\ninput image with 224 √ó224 is transformed twice to produce the two distorted views.\nC BatchNorm\nFollowing [5, 13, 7], we adopt SyncBN as our default BatchNorm. The running mean and running\nvariance of BN of MST only are updated from different images in the same batch while SimCLR\n[4] is updated from total images in teacher and student batches. The two kinds of BN inÔ¨Çuence the\ngradient variance. Hence, the two implementations should lead to different results. Meanwhile, the\nrunning mean and running variance of BN are only updated from the global views when our method\nadopts masked self-supervised Transformer.\nD k-NN classiÔ¨Åcation\nAccording to Wu et al. [ 29], we evaluate the quality of features with a simple weighted kNearest\nNeighbor classiÔ¨Åer. We freeze the parameters of pre-trained model and extract the features of class\nembedding for the train and validation dataset. As shown in Table 7, we evaluate different values for\nkand Ô¨Ånd that the setting of 10 is consistently leading to the best accuracy across our runs. More\nimportantly, we evaluate Top-1 accuracy in the validation dataset.\nTable 7: The setting of k. We report Top-1 accuracy on ImageNet validation dataset by using 300-epoch\npre-trained DeiT-S model.\nMethod Architecture epoch k k-NN Top-1 (%)\nOurs DeiT-S 300\n10 75.0\n20 74.8\n100 72.9\n200 71.8\nE Linear probing\nFollowing the popular setting of self-supervised learning, we evaluate the representation quality by\nlinear probing. After self-supervised pre-training, we remove the MLP heads and train a supervised\nlinear classiÔ¨Åer on frozen features. We use SGD optimizer, with a batch size of 1024, weight decay\nof 0 and learning rate of 0.00024 during 100 epochs on ImageNet training dataset, using only random\nresized cropping and Ô¨Çipping augmentation. Meanwhile, we evaluate single-crop Top-1 accuracy in\n13\nAlgorithm 2 Pseudo code of MST in a PyTorch-like style.\n# f_s: backbone + projection head\n# f_t: backbone + projection head\n# g: decoder\n# m: momentum coefficient\n# temp: temperature coefficient\n# O_i: output class tokens\n# Atten_i: output self-attention map\n# Res_i: the input tokens of decoder\n# v_1: the coefficient of restoration task\n# v_2: the coefficient of basic instance discrimination task\nfor x in loader: # load a batch x with B samples\nx1, x2 = augment(x), augment(x) # random data augmentation\nwith torch.no_grad():\nO_t1, _, Atten_1, O_t2, _, Atten_2 = f_t(x1), f_t(x2)\nO_s1, R_1, _, O_s2, R_2, _ = f_s(x1, Atten_1), f_s(x2,Atten_2)\nRe_1, Re_2 = g(R_1), g(R_2)\nloss1 = 0.5 * (L1_loss(Re_1, x1) + L1_loss(Re_2,x2))\nloss2 = 0.5 * (Loss(O_t1, O_s2) + Loss(O_t2, O_s1))\nloss = v_1 * loss1 + v_2 * loss2\nloss.backward()\nupdate(f_s)\nf_t = m * f_t + (1 - m) * f_s\nupdate(m) # update momentum coefficient\ndef L1_Loss(p,q):\nloss = abs(p-q).sum().mean()\nreturn loss\ndef Loss(O_t,O_s):\nO_t = softmax(O_t/temp,dim = 1)\nO_s = softmax(O_s/temp,dim = 1)\nloss = -(O_t * log(O_s)).sum(dim=1).mean()\nreturn loss\nthe validation dataset. For the linear probing of DeiT-S, we adopt the class tokens of last layer as the\ninput, following the common practice. However, DINO [2] concatenates the late few blocks as the\ninput to the linear classiÔ¨Åer. For fair comparison, we adopt the linear probing of DINO as the Ô¨Ånal\nresult while reporting common linear probing on ablation studies. The results can be observed by\nTable 8.\nTable 8: Comparison of different strategies of linear probing. We report Top-1 accuracy on ImageNet\nvalidation dataset by using 100-epoch pre-trained DeiT-S model. ‚Ä†adopts the linear probing of DINO.\nMethod Architecture epoch Linear Top-1 (%) k-NN Top-1 (%)\nOurs DeiT-S 100 75.0 72.1\nOurs ‚Ä† 73.9 72.1\nF Impact of longer training\nFrom Table 9, we observe that longer training improves the performance of our method with DeiT-S\nregardless of the kind of linear probing. This phenomenon is consistent with previous self-supervised\nlearning methods.\nG Implementation pseudo code\nThe complete algorithm of our method is shown as Alg. 2. Our model is optimized by AdamW\n[22] with learning rate 2 √ó10‚àí3 and batch size 1024. The initial weight decay is set to be 0.04.\n14\nTable 9: Impact of longer training. ‚Ä†adopts the linear probing of DINO.\nMethod Architecture epoch Linear (%) k-NN (%)\nOurs DeiT-S 100 73.9 72.1\nOurs 300 76.3 75.0\nOurs ‚Ä†\nDeiT-S 100 75.0 72.1\nOurs ‚Ä† 300 76.9 75.0\nAfter warmup [12] in the Ô¨Årst 10 epochs, the learning rate follows a cosine decay schedule [ 21].\nThe model uses multi-crop similar to [ 1] and data augmentations similar to [ 13]. The setting of\nmomentum, temperature coefÔ¨Åcient, and weight decay follows [ 2]. The coefÔ¨Åcient Œª1 of basic\ninstance discrimination task is set as 1.0 while the restoration task Œª2 is set as 0.6.\nTable 10: Differences with BERT.\nMask strategy Mask replacement style Reconstructing DINO loss Linear (%)\nRandom BERT Masked tokens No 61.0\nRandom BERT Masked tokens Yes 71.9\nOur BERT Original image Yes 73.5\nOur Our Original image Yes 73.9\nH Differences with BERT\nIn Table 10, we conduct the experiment by using pure MLM with DeiT-S under 100 epochs, the result\nis about 40% with the same experimental conÔ¨Åguration. Then we further adjust its learning rate and\nother hyperparameters, the best result is only 61%, which is far lower than that of the DINO by 10.6%\n(71.6% in Table 6) and also lower than the vanilla supervised result by 7.7% (the vanilla supervised\nresult is 68.7%). It shows the pure MLM method may be not suitable for computer vision tasks.\nMoreover, We experiment with the contrastive loss + BERT solution (that‚Äôs DINO+pure MLM), the\nlinear result is 71.9%. Our method outperforms its result by 2.0% (73.9%). The result proves our\nmethod is better than the original MLM method. Meanwhile, we further conduct the experiment by\nonly replacing the [mask] token with the strategy of pure MLM for our method, the linear result is\n73.5%, which also behinds our result. These results fully demonstrate the better setting of MLM for\ncomputer vision and further highlight the technical contributions of our paper.\nI The impact of the random mask strategy with different sampling ratios\nIn the Ô¨Årst line of Table 5, we already show the results of the random mask strategy with different\nsampling ratios. We also have tried a small p (0.01) with random masking, the result without BN is\n71.1%. When the p is smaller, the performance will be better. The result without BN is best (72.6%,\ncontrastive loss + restore loss) when p is set to 0.\nJ The impact of loss weight\nEmpirically, we set the restoration coefÔ¨Åcient Œª2 to 0.6, which makes the contrastive loss and the\nrestoration loss roughly equally weighted. We have also tried several different settings ofŒª2 (e.g.,\n0.2, 0.4, 0.6, 0.8), the result is 73.7%, 73.5%, 73.9% and 73.6% respectively. It can be observed that\nthe results are also insensitive to Œª2, and the best performance is achieved when the two losses are\nequally weighted.\nK Masking is done after the linear projection\nThe goal of linear projection is to map the image patches into tokens/embeddings. Following the\nsetting of MLM, the masking (token) strategy should be done after linear projection.\n15\nL Visualization of the attention maps\nAs shown in Figure 3, we provide the attention maps of supervised and our method. These images\nconsist of original images, attention maps of supervised method, and attention maps of our method.\nWe observe that the visualization of attention maps of our method is clearer than the supervised.\n16\nFigure 3: Comparison of the attention map of supervised and our method. Description of images from left to\nright: (a) the input image, (b) attention map obtained by supervised method, (c) attention map obtained by our\nmethod.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7956450581550598
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6563963294029236
    },
    {
      "name": "Transformer",
      "score": 0.6038870811462402
    },
    {
      "name": "Feature learning",
      "score": 0.5528160333633423
    },
    {
      "name": "Segmentation",
      "score": 0.4806562662124634
    },
    {
      "name": "Generality",
      "score": 0.47475722432136536
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4599769115447998
    },
    {
      "name": "Supervised learning",
      "score": 0.4484795928001404
    },
    {
      "name": "Security token",
      "score": 0.41884058713912964
    },
    {
      "name": "Machine learning",
      "score": 0.3887401819229126
    },
    {
      "name": "Artificial neural network",
      "score": 0.23283135890960693
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 29
}