{
  "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
  "url": "https://openalex.org/W4386566752",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2636027369",
      "name": "Peter Hase",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2124289572",
      "name": "Mona Diab",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A137861795",
      "name": "Asli Celikyilmaz",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A2096862909",
      "name": "Xian Li",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2075600992",
      "name": "Zornitsa Kozareva",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2070810387",
      "name": "Mohit Bansal",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A2109311255",
      "name": "Srinivasan Iyer",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287758766",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3036126139",
    "https://openalex.org/W4230262515",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4310299640",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4287820586",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W3119806891"
  ],
  "abstract": "Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2714–2731\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nMethods for Measuring, Updating, and Visualizing Factual Beliefs in\nLanguage Models\nPeter Hase1,2 Mona Diab1 Asli Celikyilmaz1 Xian Li1\nZornitsa Kozareva1 Veselin Stoyanov1 Mohit Bansal2 Srinivasan Iyer1\n1Meta AI 2UNC Chapel Hill\n{peter, mbansal}@cs.unc.edu\n{mdiab, aslic, xianl, zori, ves, sviyer}@fb.com\nAbstract\nLanguage models can memorize a considerable\namount of factual information during pretrain-\ning that can be elicited through prompting or\nfinetuning models on tasks like question an-\nswering. In this paper, we discuss approaches\nto measuring model factual beliefs, updating\nincorrect factual beliefs in models, and visualiz-\ning graphical relationships between factual be-\nliefs. Our main contributions include: (1) new\nmetrics for evaluating belief-updating meth-\nods focusing on the logical consistency of be-\nliefs, (2) a training objective for Sequential,\nLocal, and Generalizing updates (SLAG) that\nimproves the performance of existing hypernet-\nwork approaches, and (3) the introduction of\nthe belief graph, a new form of visualization\nfor language models that shows relationships\nbetween stored model beliefs. Our experiments\nsuggest that models show only limited consis-\ntency between factual beliefs, but update meth-\nods can both fix incorrect model beliefs and\ngreatly improve their consistency. Although\noff-the-shelf optimizers are surprisingly strong\nbelief-updating baselines, our learned optimiz-\ners can outperform them in more difficult set-\ntings than have been considered in past work.1\n1 Introduction\nPretrained language models have been shown to\nstore a large amount of factual information about\nthe world that can be elicited by cloze prompting\n(Petroni et al., 2019), few-shot learning (Brown\net al., 2020), or finetuning models for question\nanswering or true/false statement classification\n(Roberts et al., 2020). We refer to this kind of\nstored information as model factual beliefs.2\n1Code is available at https://github.com/\npeterbhase/SLAG-Belief-Updating.\n2We use the term factual belief rather than knowledge\nas in related work (Zhu et al., 2020; De Cao et al., 2021)\nbecause “belief” is a weaker term than “knowledge.” In a\ntraditional view of knowledge as Justified True Belief, it is\nmore difficult to describe information as knowledge than as a\nbelief (Schwitzgebel, 2019).\nWhile pretrained models clearly store factual be-\nliefs, it is not well understood how to efficiently\nedit the stored beliefs. Model editing is an exciting\nrecent direction of research with several practical\nuses cases (Sinitsin et al., 2020; Zhu et al., 2020;\nDe Cao et al., 2021; Mitchell et al., 2021). For\nLMs, these uses include updating factually inaccu-\nrate outputs and preventing other unwanted model\noutputs (e.g. toxic generated text) without expen-\nsive data curation and retraining efforts. These are\nimportant applications given that LMs (1) struggle\nwith future data when trained on data from the past\n(Lazaridou et al., 2021; Dhingra et al., 2021), (2)\noften generate morally undesirable text (Gehman\net al., 2020; Bender et al., 2021), and (3) simply\ngive inaccurate outputs for tasks like question an-\nswering (Lin et al., 2021). Notably, there is good\nevidence that scaling models to larger sizes will not\nfix these particular problems or may even exacer-\nbate them (Lazaridou et al., 2021; Gehman et al.,\n2020; Lin et al., 2021).\nIn the remainder of this paper, we present new\nmethods for measuring, updating, and visualizing\nfactual beliefs in LMs. We further describe each\nof these three contributions below. Figure 1 repre-\nsents the core ideas behind measuring and updating\nfactual beliefs, while belief visualization is done\nvia belief graphs(shown later in Figure 2).\nMeasuring factual beliefs.We measure the de-\ngree to which LMs possess consistent factual be-\nliefs using models finetuned on fact verification and\nquestion answering tasks. Beyond simply checking\nindividual model responses, we want to assess the\nstructural properties of model outputs: Are they\nconsistent under paraphrase? Are they logically\nconsistent? Does changing one belief correctly\nchange other entailed beliefs? Does it erroneously\nchange other unrelated beliefs? Past work has fo-\ncused primarily on consistency under paraphrase\n(Elazar et al., 2021; De Cao et al., 2021; Mitchell\net al., 2021). Here, we adapt data from Talmor\n2714\nSLAG: Sequential, Local, and Generalizing Model Updates\n(Main Input)\n(Entailed Data)\n(Local Neutral Data)\n(Paraphase Data)\n(Random Data)\nA viper is a vertebrate.\nA viper has a brain.\nA viper is venemous.\nChile is a country.\nVipers are vertebrates. \nFigure 1: Relying only on a Main Input Mi, we want to update a language model’s weights in order to (1) change\nthe output for Mi to a desired output y∗\ni , (2) change the output for paraphrases of Mi, (3) appropriately change\noutputs for data Ei entailed by the tuple (Mi,y∗\ni ), and (4) avoid changing outputs for other logically neutral data\nLNi, even if it is similar (local) to Mi. This is done iteratively for T requested updates.\net al. (2020) to measure consistency under entail-\nment (including for contrapositives), and we use\nthe Wikidata5m dataset (Wang et al., 2021) to con-\nstruct logically neutral belief pairs for checking\nthat models do treat these beliefs as independent.\nUpdating factual beliefs.We propose a Sequen-\ntial, Local, and Generalizing belief update objec-\ntive (SLAG) that substantially improves the per-\nformance of the comparable KNOWLEDGE EDI-\nTOR method from De Cao et al. (2021). KNOWL -\nEDGE EDITOR is a learned optimizer that edits a\nmodel’s weights to change its prediction on an in-\nput while satisfying other desiderata, like consis-\ntency under paraphrase. Principally, we identify\nmore difficult training data for the learned opti-\nmizer, and we learn to apply many small edits\nrather than one big edit. These changes markedly\nimprove the update success rate and lower the\nrate at which other beliefs are corrupted. We also\nfind that KNOWLEDGE EDITOR almost totally fails\nwhen updating multiple beliefs in a row as opposed\nto a changing a single belief. However, by explic-\nitly training the optimizer to update multiple beliefs\nsequentially, we recover much of the lost perfor-\nmance. Lastly, we advocate that these methods be\nevaluated for their ability to fix false or morally\nundesirable model beliefs, rather than to arbitrarily\nchange beliefs to plausible alternatives as in past\nwork (De Cao et al., 2021; Mitchell et al., 2021).\nVisualizing belief graphs.We explore a new form\nof visualization for understanding language mod-\nels, the belief graph. Given a set of factual beliefs,\nwe construct belief graphs by changing each model\nbelief and checking what other beliefs are sensitive\nto those changes. Each belief becomes a node, and\ndirected edges between nodes show that updating\none belief changes the other. We discuss graph met-\nrics that help summarize the dependencies between\nmodel beliefs.\nWe summarize our main conclusions as follows:\n1. ∼100M parameter models exhibit limited belief-\nlike qualities, as paraphrase consistency scores\nare under 70%, and models show mixed levels\nof consistency under entailment (Sec. 5.1).\n2. Off-the-shelf optimizers are quite effective up-\ndate methods, often outperforming learned opti-\nmizers when updating a single belief (Sec. 5.2).\n3. When updating multiple beliefs in a row, per-\nformance greatly declines across methods, but\nSLAG can improve learned optimizers’ perfor-\nmance beyond strong baselines (Sec. 5.2).\n4. Belief graphs reveal many nonsensical depen-\ndencies between model beliefs, and they show\nthe presence of “core” model beliefs that are\nconnected to many other stored facts (Sec. 6).\n2 Related Work\nMeasuring factual beliefs in language models.\nMuch past work has explored how information is\nstored and represented in pretrained language mod-\nels (Rogers et al., 2020). Petroni et al. (2019) pro-\nvide evidence that LMs store relational information\nbetween entities, and Roberts et al. (2020) show\nthat LMs can answer open-ended questions. Subse-\nquent work has further explored how much knowl-\nedge is stored in LMs (Heinzerling and Inui, 2021).\nMost relevant to our work are studies from Tal-\nmor et al. (2020) and Elazar et al. (2021). Talmor\net al. (2020) train LMs to perform True/False clas-\nsification of factual claims, and they measure how\n2715\nbeliefs correlate between entailed facts. We use\ntheir LeapOfThought data as a part of our SLAG\nobjective (Eq. 1) and to measure model consis-\ntency under entailment before and after updating\nbeliefs in models. Meanwhile, Elazar et al. (2021)\nmeasure the consistency of model predictions for\nparaphrased inputs. We adopt their metric for para-\nphrase consistency as a measure of belief. In other\nrecent work, Kassner et al. (2021) measure consis-\ntency under entailment and paraphrase for factual\nbelief with a new small-scale dataset, BeliefBank.\nUpdating factual beliefs in language models.\nApproaches to making targeted updates to model\nbeliefs vary along a few dimensions. First is\nwhether the methods alter model training or oper-\nate in a post-training setting. Sinitsin et al. (2020)\nuse a meta-learning objective during training to\nencourage ease of editing afterwards. A larger\nfamily of methods perform post-training model\nupdates: Dai et al. (2021) propose a hand-crafted\nalgorithm that edits model weights, while Zhu\net al. (2020) use projected gradient descent for\nbatches of points. De Cao et al. (2021) train a\nhypernetwork (learned optimizer) that processes\nmodel gradients in order to produce a new model\nthat (1) gives the desired output for an input, while\n(2) satisfying other objectives like minimizing\nchanges in predictions for other data. Mitchell et al.\n(2021) focus on scaling up the underlying hypernet-\nwork architecture, which is a complementary but\northogonal research direction that is not the focus\nof this paper. In a different approach, Kassner\net al. (2021) “update” model beliefs by adding in\nrelevant information to the input at test time. But\nthis approach does not change the model weights\nand hence does not influence model outputs\non all other potentially relevant inputs. Lastly,\nMeng et al. (2022) provide a specialized method\nfocused on rank-one updates to MLP matrices in\nTransformer-based LMs, but they do not address\nthe problem of updating multiple model beliefs\nand do not measure model consistency under\nentailment or unintended corruption of local\nneutral beliefs (metrics (5) and (6) in Sec. 3).\nVisualizing factual beliefs in language models.\nWe do not know of any prior work on visualizing\ndependencies between factual beliefs in language\nmodels, although our approach is notably inspired\nby older AI methods like Bayes Nets (Pearl, 2009).\nDifferent from Bayes Nets, we draw dependencies\nbetween two individual nodes when editing the\nmodel to change one belief also results in a change\nto the other belief, rather than there being a\nprobabilistic model specifying the relationship\nbetween the two beliefs.\n3 Updating Beliefs in Language Models\nHere we describe the problem of updating model\nbeliefs and our learned optimizer method. We also\ndiscuss metrics for measuring factual beliefs below,\nwhile our Belief Graphs are presented in Sec. 6.\nProblem statement and metrics. We suppose\nwe have a model fθ = pθ(y|x) parametrized by\nθ. For an input xi that has some undesired model\noutput ˆyi = arg maxypθ(y|x), we wish to obtain\na new model θ∗that produces a desired output y∗\ni\nfor xi. This new model θ∗ should also fulfill a\nfew other desiderata. As in past work (De Cao\net al., 2021; Mitchell et al., 2021), we operational-\nize these desiderata in the following metrics:\n1. Update Success Rate (Main Input): The pro-\nportion of Main Inputs xi for which the up-\ndated model gives the desired output y∗\ni.\n2. Update Success Rate (Paraphrase): The pro-\nportion of paraphrases of xi for which the\nupdated model gives the same new prediction\nas it does for xi (averaged across xi).\n3. Retain Rate ( All Data): The proportion of\nthe updated model’s predictions which are un-\nchanged for all data besides the Main Input.\n4. ∆-Acc (All Data): The change in accuracy on\nall other data besides the Main Input.\nIn practice, Retain Rate (All Data) and ∆-Acc are\ncomputed with random subsets of a dataset, since\nthese must be computed after every belief update.\nWe add two metrics to those used in past work:\n5. Update Success Rate ( Entailed Data): The\nnew model’s accuracy on data that is logically\nentailed by the new Main Input prediction.\n6. Retain Rate (Local Neutral): The proportion\nof the updated model’s predictions which are\nunchanged for data that is similar to the Main\nInput but still logically neutral.\nWe use Update Success Rate ( Entailed Data) to\nmeasure logical consistency for an updated model,\nsince changing one belief entails changes in logi-\ncally entailed beliefs. Retain Rate (Local Neutral)\n2716\nDataset Data Type Input Label(s)\nzsRE Main Input Player Ali Kanaan plays for what team? {Sporting Al Riyadi Beirut}Paraphrase What team is Ali Kanaan associated with?\nWikidata5m\nMain Input Mary Good has relation ‘award received’ to {Garvan-Olin Medal; Arkansas\nWomen’s Hall of Fame; etc.}Paraphrase Mary Lowe Good has relation ‘winner of’ to\nLocal Neutral Mary Good has relation ‘educated at’ to {The University of Arkansas; U\nArkansas; etc.}\nFEVER Main Input Tardigrades are also known as space bears. True\nMain Input The Lion belongs to the genus Vulpes. False\nLeapOfThought Main Input A viper is a vertebrate. True\nEntailed Data A viper has a brain. True\nTable 1: Example datapoint from each dataset, and auxiliary data that accompanies the Main Input.\nuses special Local Neutral data. Unlike random\ndata, Local Neutral data is guaranteed to be logi-\ncally independent of the Main Input, while still be-\ning similar (local) to it, which we ensure by using\ndata with the same subject entity. Together, these\nsix metrics better cover the criteria for belief out-\nlined by Newen and Starzak (2020). We compute\nthe metrics using data of the kind shown in Table 1.\nEvaluation procedure. To date, methods have\nbeen evaluated on the basis of their ability to\nchange model predictions for all data. Moreover,\nthe desired labels {y∗\ni}n\ni=1 on sequence prediction\ntasks have each been selected from the model’s pre-\ndictive beam search (De Cao et al., 2021; Mitchell\net al., 2021). We propose for evaluation to focus\non a more valuable but difficult setting: changing\nthe predictions on incorrect points to be correct.\nSequential updates. The standard evaluation in\npast work is to update a single model belief, evalu-\nate the new model, then rollback the update before\nrepeating the process for each test point. We ob-\ntain sequential versions of all metrics by applyingr\nmodel updates in a row before checking the metrics,\nmeaning there are floor(n/r) measurements for a\ntest set of npoints. We consider it important to\nevaluate a sequential setting because, in practice, it\nis likely that model developers will want to update\nmany factual beliefs of a trained model over time.\nBelief updating method.As our base architecture,\nwe use the KNOWLEDGE EDITOR architecture from\nDe Cao et al. (2021), which is a hypernetwork that\ntakes in model gradients as inputs and outputs a\nnew update to apply to the model parameters. For\nfurther details of this method, we refer readers to\nAppendix A. Let it suffice for now to observe that\na new model is given as a differentiable function\nθ∗= θ+ gϕ(xi,ˆyi,y∗\ni,θ)\nusing the learned optimizer gϕ, current LM weights\nθ, Main Input xi, current prediction ˆyi, and desired\nmodel output y∗\ni. Then, we can package the above\nupdate as θ(k+1) = θ(k) + gϕ(xi,ˆyi,y∗\ni,θ(k)), and\nobtain new model parameters via a looped update,\nθ∗= θ(k) +\nK−1∑\nj=0\ngϕ(xi,ˆyi,y∗\ni,θ(k+j))\n= Update(xi,ˆyi,y∗\ni,θ(k); ϕ,K)\ntaking K small steps from initial parameters θ(k).\nDe Cao et al. (2021) use such a loop at test time;\nwe incorporate the loop into training to align the\ntrain and test-time distributions.\nLearned optimizer training.The training objec-\ntive for KNOWLEDGE EDITOR includes differen-\ntiable terms corresponding to Update Success for\nthe Main Input and paraphrases, as well as Retain\nRate for all other data. We also consider terms\nfor Update Success on entailed data and the Local\nNeutral Retain Rate, when this is possible given\navailable data. The overall objective requires sev-\neral kinds of additional data for each point, which\nwe denote by DR for other random data, DLN for\nlocal neutral data, DE for entailed data, and DP for\nparaphrases of xi. For a data point xi with desired\nprediction y∗\ni, the full objective is then:\nL(ϕ; xi,ˆyi,y∗\ni,θ) = λ1LTask(fθ∗ (xi),y∗\ni)\n+ λ2\n1\n|DP|\n∑\nxP∈DP\nLTask(fθ∗ (xP),y∗\ni)\n+ λ3\n1\n|DE|\n∑\nxE,yE∈DE\nLTask(fθ∗ (xE),yE)\n+ λ4\n1\n|DLN|\n∑\nxLN∈DLN\nKL(fθ∗ (xLN)||fθ(xLN))\n+ λ5\n1\n|DR|\n∑\nxR∈DR\nKL(fθ∗ (xR)||fθ(xR)) (1)\n2717\nwhere LTask is the loss used to get gradients for fθ.\nWe use the Cross Entropy loss for binary classifica-\ntion and sequence-to-sequence tasks.\nWe optimize this objective w.r.t. ϕ using\nAdamW (Loshchilov and Hutter, 2019). To obtain\nupdate labels {y∗\ni}n\ni=1, we always use the oppo-\nsite class in binary classification. For sequence-to-\nsequence tasks, we use the correct label when ˆyi\nis incorrect, and when ˆyi is correct, we randomly\nselect another label from the training data. This\nchoice is in contrast to De Cao et al. (2021) and\nMitchell et al. (2021), who use samples from the\nmodel beam search as update labels for all points.\nSLAG objective.To prepare the update method for\na sequential-update setting, we consider training\ngϕ to update multiple datapoints in a row. Using\nthe per-datapoint loss in Eq. 1, we obtain our Se-\nquential, Local, and Generalizing (SLAG) loss for\na set of rMain Inputs D= {xi,ˆyi,y∗\ni}r\ni=1 as\nLSequential(ϕ; D,θt)=\nr∑\ni=1\nL(ϕ; xi,ˆyi,y∗\ni,θt+i) (2)\nwhere θt+i = Update(xi,ˆyi,y∗\ni,θt+i−1; ϕ,K) are\nthe model parameters obtained from updating on\nthe first ipoints in D(starting from θt). This objec-\ntive allows us to train gϕ to update multiple beliefs\nin a row. To ensure training with this objective is\nstill efficient, we limit how far back through the up-\ndate history we backpropagate when computing the\ngradient w.r.t. ϕfor each term in the RHS sum of\nEq. 2. Each parameter vector θt depends on ϕand\nθt−1. We always apply the stop-gradient function\nto the most recent vector θt−1 to prevent backprop-\nagating through it (visualized in Appendix Fig. 3).\nThis choice allows our memory use to remain con-\nstant in r(see Appendix Fig. 4).\n4 Experiment Setup\nDatasets. We run experiments with four datasets\n(example data shown in Appendix Table 15). (1)\nFEVER includes 115,409 True/False factual claims\n(Thorne et al., 2018). We use the original test set\nof 10,444 points, and we randomly split the train-\ning data into 94,469 train points and 10,496 dev\npoints. (2) zsRE includes 151,631 questions based\non relational knowledge from Wikipedia, which\nwe randomly shuffle into train/dev/test splits with\n80/10/10% of the data (Levy et al., 2017). Tal-\nmor et al. (2020) introduce (3) the LeapOfThought\ndataset, consisting of factual claims that are en-\ntailed to be true or false depending on a context\nBelief Consistency ↑\nDataset Paraphrase Entailed Contrapos.\nLeapOfThought - 85.6 (1.1) 16.5 (2.7)\nzsRE 69.5 (1.1) - -\nWikidata5m 25.8 (0.5) - -\nTable 2: Belief metric results across datasets.\nParaphrase Consistency ↑\nDataset Model Incorrect Model Correct\nzsRE 61.39 (1.33) 91.82 (1.17)\nWikidata5m 24.55 (0.48) 37.20 (2.06)\nTable 3: Paraphrase consistency by the correctness of\nthe model prediction on the Main Input.\nfact. We filter the data so that the context facts are\nunique, then shuffle the resulting 14,939 points into\ntrain/dev/test splits with 60/10/30% of the data.\nIn order to get Local Neutral data, we construct\n(4) a sequence prediction task using Wikidata5m,\na relational knowledge base with over 20 million\ntriplets (Wang et al., 2021). Each input consists\nof an entity e1 and relation r, and the label is an-\nother entity e2 that completes the triplet. All inputs\ncome in pairs that share the same entity e1 but use\ndifferent relations with different labels. In general,\nthe completion e2 to the Main Input triplet (e1, r1,\ne2) has no logical consequences for its paired in-\nput, (e1, r2, ?). The paired points are also local to\nthe Main Input, i.e. they pertain to the same entity\ne1 as the Main Input. We obtain four paraphrases\nfor each Main Input using different aliases for the\nentity and synonyms of the relation. We construct\na train set of 150k points and dev/test sets of 10k\npoints each. See Appendix B for further details.\nModels. We train five models with different ran-\ndom seeds for each dataset, using RoBERTa-base\nfor binary tasks and BART-base for sequence-to-\nsequence tasks (accuracies in Appendix Table 14).\nFor each of the five models, we train one learned\noptimizer using SLAG and one with the objective\nfrom De Cao et al. (2021), which we list as KE in\ntables below. Our model selection criterion is the\nmean of: average Update Success Rate (across data\ntypes), Retain Rate (only for Local Neutral data),\nand ∆-Acc for All Data. We tune the SLAG objec-\ntive terms for each task separately (see Appendix\nTable 10 for final selections; results discussed in\nAppendix E). Other hyperparameters are given\nin Appendix B. To summarize the differences\nbetween SLAG and KNOWLEDGE EDITOR : (1) we\nuse Ktrain=Ktest rather than Ktrain=1; (2) we adopt\n2718\nSingle-Update Setting Update Success Rate Retain Rate ∆-Acc\nDataset Method Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER\nAdamW 100 (0.0) - - - 98.80 (0.2) 0.22 (0.1)\nKE 99.98 (<0.1) - - - 98.28 (0.3) -0.24 (0.1)\nSLAG 99.99 (<0.1) - - - 98.41 (0.2) -0.20 (0.1)\nLeapOfThought\nSGD 100 (0.0) - 72.48 (4.6) - 95.52 (0.4) 1.23 (0.8)\nKE 99.78 (0.4) - 74.48 (4.4) - 93.50 (1.3) -1.33 (1.1)\nSLAG 100 (0.0) - 75.50 (4.3) - 94.92 (1.4) -1.31 (1.2)\nzsRE\nSGD 99.36 (0.1) 94.44 (0.6) - - 74.73 (0.4) -0.43 (0.1)\nKE 84.73 (1.4) 89.26 (1.8) - - 71.55 (2.4) -2.19 (0.4)\nSLAG 94.29 (0.4) 94.71 (0.5) - - 80.48 (1.3) -0.29 (0.1)\nWikidata5m\nSGD 98.05 (0.3) 68.78 (0.8) - 41.46 (1.0) 58.62 (0.6) -1.97 (0.3)\nKE 74.57 (2.9) 58.05 (2.2) - 40.84 (1.8) 53.58 (2.2) -3.03 (0.5)\nSLAG 87.59 (0.6) 80.70 (0.9) - 47.85 (1.0) 63.51 (1.3) -1.71 (0.3)\nTable 4: Belief update metrics for off-the-shelf optimizers, KNOWLEDGE EDITOR (KE) from De Cao et al. (2021),\nand SLAG, with rtest = 1. Bolded numbers are the best in their group at a statistical significance threshold of\np<. 05 (or lower). Our SLAG objective improves over KE, but off-the-shelf optimizers perform surprisingly well.\ntraining labels using real data labels rather than\nalternatives from the model’s beam search; (3) our\nobjective terms differ following tuning; and (4) we\ncan optimize for updating multiple beliefs in a row.\nBaselines. We use off-the-shelf optimizers as base-\nlines. We tune the baseline hyperparameters sep-\narately for each dataset, selecting among several\nkinds of optimizers, learning rates, and the num-\nber of update steps. The selection criterion is the\nsame as the criterion outlined for learned optimiz-\ners above. The resulting baselines are surprisingly\nstrong (see Appendix Table 12 for final selections).\nHypothesis testing. We obtain 95% confidence\nintervals and perform hypothesis tests via block\nbootstrap, resampling model seeds and data points\n(Efron and Tibshirani, 1994). For ablation experi-\nments, we run only one model seed per condition.\n5 Experiment Results\n5.1 Do LMs have consistent factual beliefs?\nWe measure Paraphrase Consistency, Entailment\nAcc, and Contrapositive Acc for finetuned task\nmodels. Paraphrase Cons. is the fraction of para-\nphrase pairs where the model produces the same\noutput (Elazar et al., 2021). Entailment Acc is the\nmodel accuracy on data that is entailed by the Main\nInput. For LeapOfThought (see Table 1), “Main\nInput xi is true” implies “entailed input xE has\nlabel yE,” but the inverse ( ¬A ⇒¬B) does not\nnecessarily hold. Therefore, we compute Entail-\nment Acc only where the Main Input prediction is\ncorrect. We do know that the contrapositive holds:\n“Entailed input xE does not have label yE” implies\nUpdate Success Rate ↑ ∆-Acc ↑\nDesired Label Main Input Paraphrase All Data\nBeam Label 97.41 (0.3) 97.03 (0.4) -0.30 (0.1)\nCorrect Label 94.46 (0.7) 94.45 (0.7) -0.24 (0.1)\nTable 5: Evaluation difficulty by desired model output,\nfor a learned optimizer trained with SLAG on zsRE.\nthat “Main Input xi is false.” So for Contrapositive\nAcc, we measure how often the model follows this\nrule, when the antecedent holds of its prediction.\nBelief measurement results.Table 2 shows the be-\nlief metrics for each dataset. We find that ∼100M\nparameter models show limited evidence of having\nconsistent factual beliefs. Paraphrase consistency\nis 69.50% (±1.09) for zsRE and much lower for\nWikidata5m (25.84%±0.53). While entailment ac-\ncuracy is high for LeapOfThought (85.63%±1.08),\nthe model is consistent under the contrapositive\nonly 16.51% (±2.71) of the time. Overall, these\nresults are not nearly as consistent as we would\nhope for factual beliefs to be. Interestingly, the\nmetrics are much higher when the model predic-\ntion on the Main Input is correct (Table 3).\n5.2 Can we update factual beliefs in LMs?\nFirst, we compare two evaluation procedures for\nsequence prediction tasks: correcting model be-\nliefs versus changing them to an alternative from\nthe model’s beam search. We do so for zsRE us-\ning SLAG. Next, we compare belief update per-\nformance between KNOWLEDGE EDITOR , SLAG,\nand off-the-shelf optimizers. We report results in\nsingle-update ( rtest = 1 ) and sequential-update\n(rtest = 10 ) settings. See Appendix Fig. 5 for\n2719\nSequential-Update Setting Update Success Rate Retain Rate ∆-Acc\nDataset Method Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER\nAdamW 92.81 (1.3) - - - 91.86 (1.4) 1.16 (0.6)\nKE 74.13 (1.8) - - - 39.86 (0.7) -27.13 (1.3)\nSLAG 91.27 (2.9) - - - 70.30 (5.8) -11.96 (4.5)\nLeapOfThought\nSGD 100 (0.0) - 61.34 (5.0) - 82.62 (0.8) -4.93 (1.0)\nKE 96.14 (2.3) - 49.27 (6.0) - 72.45 (0.9) -15.03 (1.0)\nSLAG 100 (0.0) - 50.46 (5.5) - 74.02 (1.1) -13.03 (1.3)\nzsRE\nSGD 82.71 (0.6) 90.81 (0.7) - - 40.49 (0.6) -2.38 (0.3)\nKE 0.10 (<0.1) 36.55 (1.4) - - 0.05 (<0.1) -20.98 (0.7)\nSLAG 87.57 (0.6) 92.20 (0.7) - - 47.19 (0.7) -1.74 (0.3)\nWikidata5m\nSGD 56.82 (0.8) 54.49 (0.7) - 6.40 (0.4) 26.37 (0.6) -3.96 (0.4)\nKE 0 (0.0) 40.84 (0.9) - 0 (0.0) 0 (0.0) -10.05 (0.6)\nSLAG 58.27 (1.0) 65.51 (0.9) - 7.36 (0.5) 27.76 (0.7) -3.62 (0.4)\nTable 6: Belief update results when a model is sequentially updated rtest=10 times. Here, SLAG uses rtrain=R. On\nsequence prediction tasks in this setting, SLAG can outperform the off-the-shelf optimizers across metrics.\nan ablation across rtest.\nCorrecting beliefs vs. changing factual beliefs.\nGiven the results in Table 5, we find that correcting\nmodel outputs is harder than simply changing them\nto a plausible alternative. Update Success rises by a\nfull 2.96 (±0.48; p<1e−4) points for Main Inputs\nand 2.58 (±0.81; p<1e−4) for Paraphrases, while\n∆-Acc is virtually unchanged. This suggests that\nthat past work has overestimated the efficacy of\nbelief update methods for actually fixing models.\nHenceforth we evaluate methods according to their\nability to update model beliefs to be true.\nUpdate method results (single update).Table 4\nshows the results in a single-update setting. First,\nwe find that off-the-shelf optimizers are very effec-\ntive across the board. The baselines show Main\nInput Update Success Rates of 98%+ across tasks\nwith competitive or even positive∆-Acc scores.3\nWhen strongly tuned, these baselines outperform\nlearned optimizers on most metrics here.\nHowever, SLAG surpasses the baselines in a few\nplaces. All Data Retain Rate on zsRE rises by\n5.77 points (±1.43; p<1e−4), and on Wikidata5m\nParaphrase Update Success rises by 11.92 (±1.20;\np<1e−4) and the Local Neutral Retain Rate by\n6.40 (±1.41; p<1e−4). SLAG also greatly im-\nproves over KE for sequence prediction tasks.\nInterestingly, we observe that belief updates\ngreatly improve paraphrase consistency and entail-\nment accuracy (SLAG results in Table 7). Updates\nimprove Paraphrase consistency by 33.14±1.46 on\n3Positive ∆-Acc values are possibly due to distribution\nshift in the test split. In FEVER, for instance, the train and\ndev data are 73% True, while test data is 50% True. On the\ndev split, AdamW achieves a negative ∆-Acc, -0.18 (±0.11).\nMetric Before Update After Update\nEntailment Acc 58.30 (5.7) 75.50 (4.3)\nPara. Cons (zsRE) 61.39 (1.3) 94.53 (0.6)\nPara. Cons (Wiki) 24.69 (0.5) 84.56 (0.9)\nTable 7: Entailment Acc and Paraphrase Consistency\nrise greatly after model updates to incorrect points.\nzsRE and 59.87±1.09 on Wikidata5m, while En-\ntailment Acc rises by 17.20±7.10 points.\nUpdate method results (sequential updates).\nWe give results for a sequential update setting\n(rtest=10) in Table 6. Immediately we see this is a\nmuch more difficult evaluation, as metrics are gen-\nerally far lower for each dataset. Next, we observe\nthat learned optimizers with SLAG (rtrain=10) out-\nperform baselines on sequence prediction tasks.\nOn zsRE, we improve Update Success for Main\nInputs by 4.86 ( ±0.83; p=1e−4) and for Para-\nphrases by 1.39 ( ±0.93; p=.004), with better ∆-\nAcc by 0.64 ( ±0.35; p=.0005). Improvements\ntrend in the same direction for Wikidata5m and are\nall statistically significant except for the gain in ∆-\nAcc. The jump on Paraphrases in particular is large\n(11.02±1.17; p<1e−4). In comparison, using the\nKNOWLEDGE EDITOR training objective leads to\ndrastic drops in performance.\nLearned optimizers still struggle compared to\nbaselines on binary datasets. Here, AdamW and\nSGD achieve high update update success with\nmuch better ∆-Acc scores, by 13.12 ( ±4.51;\np=1e−4) on FEVER and 8.16 (±1.63; p=1e−4)\non LeapOfThought.\n2720\nMiddle-earth is a real place.\n[y: false]\nHot Right Now is mistakenly\nattributed to DJ  Fresh.\n[y: false]\nThere are no musical or creative\nworks in existence that have\nbeen created by Phillip Glass.\n[y: false]\nThe Daily Show is incapable\nof focusing on recent news\nstories.\n[y: false]\nThe Chrysler Building was\nalways the world's shortest\nbuilding.\n[y: false]\nShane McMahon officially\nretired on the first day of\n2010.\n[y: false]\nBessie Smith died on April\n26, 1937.\n[y: false]\nDespicable Me 2 was written\nby Cinco Paul.\n[y: true]\nHot Right Now is from Nextlevelism.\n[y: true]\nFigure 2: A non-random subgraph of the belief graph for a model trained on FEVER. Directed edges from uto\nvindicate that changing the model belief in ucauses the belief in vto change. The ground-truth label is given in\nbrackets for each point, and node color shows the model’s accuracy before any updates (green=correct).\n6 Belief Graphs\nWe now construct belief graphs to better under-\nstand the connections between model beliefs. We\nform a graph from a set of datapoints by updating\neach prediction and checking what other predic-\ntions change. We represent each datapoint as its\nown node in a belief graph. Whenever updating\na datapoint uchanges the prediction for point v,\nwe draw a directed edge from uto v. Following\nSec. 5.2, we use off-the-shelf optimizers to change\nthe model output to the opposite of its original pre-\ndiction for every datapoint. For FEVER we obtain\na graph of 10,444 nodes, and for LeapOfThought\nwe obtain a graph with 8642 nodes, which is dou-\nble the test set size because we include both Main\nInputs and Entailed Data as their own nodes.\nWe visualize part of a belief graph in Fig. 2. This\nfigure shows a non-random subgraph intended to\ngive a representative view of the data (we give three\nrandom subgraphs in Appendix E). On inspection,\nwe do not see any clear reasons for beliefs being\nconnected or not connected. We come to the same\nconclusion looking at other random subgraphs (see\nAppendix Figures 9, 10, and 11). Whether or not\nchanging one belief changes another appears essen-\ntially random, which is a novel negative result on\nthe organization of internal model beliefs. How-\never, we do observe some aggregate trends. First,\nit appears that incorrect predictions are the most\nsensitive to model updates. On FEVER, incorrect\nbeliefs change around 4% of the time when other\nbeliefs are updated, while correct beliefs change\nonly 2.5% of the time. Second, we find that Local\nNeutral beliefs are much harder to avoid changing\nthan simply random data. On Wikidata5m (Table\n4), we observe that the Retain Rate on All Data is\nDataset\nMetric FEVER LeapOfThought\n# Nodes 10,444 8,642\n% Edgeless 0.0 0.0\n# Edges Total 1.88m 9.71m\n# In Edges (95th perc.) 1,088 5,347\n# Out Edges (95th perc.) 390 3,087\n% Update-Transitivity 66.64 24.38*\nTable 8: Belief graph summary statistics. *We compute\nUpdate-Transitivity for LeapOfThought with n= 4000\npoints due to computational cost.\n61.51±1.33, while for Local Neutral data it is a\nfull 15.66 points lower.\nWe highlight a few summary statistics here from\nTable 8 for a broader view of the graphs. First,\n% Edgeless is the proportion of nodes which have\nno in or out edges. Since this is 0 for both datasets,\nevery belief can be changed by editing the right\nbelief. # In Edges is the number of in edges at the\n95th percentile, meaning 5% of beliefs have more in\nedges than this value, and the same holds of # Out\nEdges. These values grow to a rather large fraction\nof the overall datasets, suggesting that (1) some\nbeliefs are sensitive to changes in a large fraction\nof all beliefs, and (2) some beliefs are influential\nto hundreds of other beliefs when changed. Inter-\nestingly, this implies that some factual beliefs are\n“core” beliefs in the model, such that changing these\nindividual beliefs requires greatly changing the\noverall distribution of factual beliefs in the model.\nLastly, % Update-Transitivity represents the an-\nswer to the question: if updating belief A changes\nbelief B, and updating belief B changes belief C,\nwhat proportion of the time does updating A change\nC? For these datasets, a logically consistent model\nshould display 100% Update-Transitivity (see Ap-\n2721\npendix D for a caveat on this metric). We find\nthat belief updates often yield intransitive results\nfor both datasets, another negative result for belief\nconsistency. It would be valuable for future work\nto extend this analysis of belief graphs to explore\nwhy language models demonstrate these surprising\nconnections and inconsistencies between beliefs.\n7 Conclusion\nWe first measure the presence of consistent factual\nbeliefs in language models, and we propose to eval-\nuate learned optimizers for whether they can make\nmodel beliefs more truthful. Then we show that\nour SLAG objective greatly improves learned op-\ntimizer performance, outperforming off-the-shelf\noptimizers when updating multiple model beliefs\nin a row. Finally, we introduce belief graphs to\nvisualize connections between model beliefs. We\nfind that model beliefs are highly interconnected,\nwith some “core” beliefs influencing hundreds of\nother beliefs.\nEthics Statement\nBelief update methods may be used to either cor-\nrect undesired beliefs or induce problematic beliefs\nin LMs, and it is not clear whether these capabil-\nities could be separated. We propose to evaluate\nmethods only on the basis of their ability to correct\nmistaken model beliefs, but the malicious use case\nremains. We are uncertain about how a bad belief\nwould influence the general behavior of a model\n(e.g. answers to many questions), but it is possible\nthat a belief update method could instill bad beliefs\nin a capable LM with far-reaching implications\nfor model behavior. That said, we hope that these\nmethods will instead be used to update undesirable\nmoral, social, and factual beliefs in large LMs.\nLimitations\nWe note a few limitations of our work:\n(1) Neural learned optimizers require large\namounts of training data to successfully edit even a\nfew model beliefs at test time.\n(2) Our experiments are limited by available\ndatasets in terms of both metrics we can calculate\nand objectives we can optimize for. There is also\nsome noise in each dataset which we catalogue in\nAppendix C.\n(3) We conduct experiments with ∼100M pa-\nrameter models as in past work. While the belief-\nupdating problem is still clearly unsolved given\nour results, it will also be valuable for future work\nto scale to larger models which may exhibit more\nconsistent factual beliefs. That said, we believe our\ncontributions are still valuable since our metrics,\nobjectives, and belief visualization method can all\nbe easily applied to larger models, and hypernet-\nworks have already been extended to work with\nlarger models (Mitchell et al., 2021).\n(4) Currently, models may have seemingly ran-\ndom interdependencies between factual beliefs,\nlimiting the insights available from our belief\ngraphs. We believe that as models become more\nconsistent and more truthful, the usefulness of be-\nlief graphs as a tool for understanding connections\nbetween beliefs will increase.\n(5) Lastly, we do not currently account for un-\ncertainty in factual beliefs. The data we use comes\nin the form of declarative statements and answers\nto questions which take what is called a veridi-\ncal stance toward a proposition, displaying a “full\ncommitment” to that proposition’s truthfulness (Gi-\nannakidou and Mari, 2020). It will be valuable\nfor future work to explore two dimensions of un-\ncertainty in beliefs: (1) expression of uncertainty\nin language, via partial or trivial commitments\n(like “X might be Y”) and (2) expression of uncer-\ntainty mathematically, via probabilities assigned\nby a model to utterances or True/False values. In\nthis paper we treat a belief as “updated” when the\nmodel output changes, but this ignores any under-\nlying change in the distribution pθ(y|x) that could\noccur even if its mode does not change.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\n2722\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. arXiv preprint arXiv:2104.08696.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nEMNLP, pages 6491–6506. Association for Compu-\ntational Linguistics.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2021. Time-aware language mod-\nels as temporal knowledge bases. arXiv preprint\narXiv:2106.15110.\nBradley Efron and Robert J Tibshirani. 1994. An Intro-\nduction to the Bootstrap. CRC press.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of EMNLP.\nAnastasia Giannakidou and Alda Mari. 2020. A linguis-\ntic framework for knowledge, belief, and veridicality\njudgement. HAL.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. Beliefbank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. arXiv preprint arXiv:2109.14723.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nSebastian Ruder, Dani Yogatama, et al. 2021. Mind\nthe gap: Assessing temporal generalization in neural\nlanguage models. In NeurIPS.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancouver,\nCanada. Association for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309.\nAlbert Newen and Tobias Starzak. 2020. How to ascribe\nbeliefs to animals. Mind & Language.\nJ. Pearl. 2009. Causality. Cambridge University Press.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nEric Schwitzgebel. 2019. Belief. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of Philosophy, Fall\n2019 edition. Metaphysics Research Lab, Stanford\nUniversity.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In ICLR.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically reason\nover implicit knowledge. In NeurIPS.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\n2723\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\narXiv preprint arXiv:2012.00363.\nA Learned Optimizer Details\nArchitecture. KNOWLEDGE EDITOR is a learned\noptimizer g: X×Y×Y× Θ →Θ that produces\nnew model weights by applying an adjusted gra-\ndient step to a model. For reference, we give a\nglossary of symbols used here in Table 9. For ad-\nditional details beyond what is presented here, we\nrefer readers to De Cao et al. (2021).\nAt a high level, gϕ first encodes an input xi and\nrequested prediction change into a vector h, then\nprocesses hinto two low-rank matrices Aand B\nthat are used to transform the model gradient on xi,\n∇θL(xi,y∗\ni). For Transformer models, the method\nedits only attention and feed-forward weights, so\nall model gradients match the shape of an associ-\nated weight matrix of shape d1 ×d2. Formally, a\nnew model θ∗is obtained using a learned optimizer\ngϕ as follows:\nh= LSTM([x; ˆy; y∗])\n{u,v,γ,δ }= {MLPi(h)}4\ni=1\nA= softmax(u)vT\nB = softmax(γ)δT\nη= σ(MLP(h))\nθ∗= θ+ η(A◦∇θL(xi,y∗\ni) + B)\nwhere ϕconsists of all LSTM and MLP parameters.\nTraining Algorithm. The learned optimizer ob-\njective is optimized w.r.t. ϕwith AdamW through\na standard procedure of randomly sampling mini-\nbatches without replacement (Loshchilov and Hut-\nter, 2019). Within each batch, one datapoint is\nrandomly selected as the Main Input, and the re-\nmaining points are used as DR. To obtain update\nlabels {y∗\ni}n\ni=1, we always use the opposite class\nin binary classification. For sequence-to-sequence\nSymbol Glossary\nfθ Language Model\ngϕ Learned optimizer\nxi Main Input\nˆyi LM output on xi\ny∗\ni Desired output\n∇θL(xi,y∗\ni) Gradient of LM\nUpdate(xi,ˆyi,y∗\ni,θ) Update one LM belief\nL(ϕ; xi,ˆyi,y∗\ni,θ) Belief update objective for xi\nLSequential(ϕ; D,θt) Sequential objective (SLAG)\nK # gradient steps in Update(·)\nr # beliefs updated in LSequential\nTable 9: Symbol descriptions for the learned optimizer.\ntasks, we use the correct label when ˆyi is incorrect,\nand when ˆyi is correct, we randomly select another\nlabel from the training data. This choice is in con-\ntrast to De Cao et al. (2021) and Mitchell et al.\n(2021), who use samples from the model beam\nsearch as update labels for all points.\nB Additional Training Details\nB.1 Compute Costs.\nLearned optimizer memory.The hypernetwork\nhas 92m trainable parameters for RoBERTa-base\n(which is 125m parameters), and 105m param-\neters for BART-base (which is 139m parame-\nters). To increase training efficiency, we limit\nhow far into the task model history we backprop-\nagate. As shown in Fig. 3, when backpropagat-\ning through task model parameters θt = θt−1 +\nUpdate(xi,ˆyi,y∗\ni,θt−1; ϕ), we continue backprop-\nagating through Update(xi,ˆyi,y∗\ni,θt−1) but not\nθt−1, which is also dependent on ϕ. That is, we ap-\nply a stop-gradient function to θt−1. This way, we\ncompute the derivative ∇ϕUpdate(xi,ˆyi,y∗\ni,θt; ϕ).\nonly once for each t, rather than recomputing these\ngradients for all subsequent time steps. These\nchoices allow the memory use of our training algo-\nrithm to remain constant in r. We make the same\nchoice for our Klooped steps in a single applica-\ntion of the Update function, so the gradient for the\nupdate at step kdepends only on gϕ(xi,ˆyi,y∗\ni,θ(k))\nand not θ(k−1). See Fig. 4 for a graph of memory\nuse depending on rand k.\nExperiment runtimes. We now give runtimes\nfor experiments in the paper. Building the belief\ngraphs takes 25 hours for FEVER ( n = 10,444)\nand 17.5 hours for LeapOfThought ( n = 8642 )\non an NVIDIA RTX 2080 GPU. Computing sum-\nmary statistics for graphs takes 3 hours on FEVER\nand 3 hours for LeapOfThought for statistics be-\n2724\nTask Model\nOptimizer Backprop\nStop Gradient\nSequential Backprop Graph\nFigure 3: The backpropagation graph for sequential model updates.\nsides Update-Transitivity. We compute Update-\nTransitivity for LeapOfThought with a subset of\n4000 points, which takes 45 hours.\nAll other experiments are run on a NVIDIA\nV100 32GB GPU. Training the task models takes\n7 minutes for LeapOfThought, 45 minutes for\nFEVER, 4 hours for zsRE, and 10 hours for Wiki-\ndata5m. Training the learned optimizer with r= 1\ntakes 2.3 hours for LeapOfThought, 5 hours for\nFEVER, 9.5 hours for zsRE, and 16 hours for\nWikidata5m. Training the learned optimizer with\nr= 10 takes 53 minutes for LeapOfThought, 2.9\nhours for FEVER, 7 hours for zsRE, and 12.5 hours\nfor Wikidata5m. Computing update statistics with\nthe off-the-shelf optimizers with r= 1 takes 4 min-\nutes for LeapOfThought, 30 minutes for FEVER,\n2.3 hours for zsRE, and 3.9 hours for Wikidata5m.\nWith r = 10 , the baselines require 1 minute for\nLeapOfThought, 15 minutes for FEVER, 54 min-\nutes for zsRE, and 1.8 hours for Wikidata5m. Total\nruntimes for each experiment should take into ac-\ncount multiple conditions and multiple seeds of\neach model being run.\nB.2 Hyperparameters and Objective Terms.\nTraining hyperparameters. We fit our RoBERTa-\nbase and BART-base task models to their respec-\ntive datasets with the following hyperparameters:\nWe train for 10 epochs on the binary tasks, and\n20 for the sequence-to-sequence tasks. When pre-\ndicting with BART-base, we use a beam search\nwith width 5. In each case, we use AdamW from\ntorch.optim with a LR of 1e-5 and weight de-\ncay of 1e-4. We select the best model according\nto the best dev set accuracy, checkpointing after\neach training epoch. The learned optimizers are\noptimized with AdamW, using a learning rate of\n3e-4 and weight decay of 0. We train the learned\noptimizer for 5 epochs on each dataset except for\nDataset rtest K Objective\nFEVER 1 5 Main\n10 1 Main\nLeapOfThought 1 5 Main\n10 1 Main\nzsRE 1 5 Main\n10 5 Main\nWikidata5m 1 5 Main+Para\n10 5 Main+Para\nTable 10: Final hyperparameters and objective terms of\nthe learned optimizer for each task.\nLeapOfThought, which we train for 10 epochs\ngiven its smaller size. The learned optimizers are\nalso selected based on dev set performance, with\ncheckpointing after each training epoch. Their se-\nlection criterion is a raw average of Update Success\nRate (averaged over each kind of data), Retain Rate\n(Local Neutral) and ∆-Acc, with terms dropped\nwhen they cannot be computed given the available\ndata. Note that dev epochs with zsRE and Wiki-\ndata5m are fairly slow, so in order to speed up our\nexperiments we compute dev epochs with a subset\nof 4000 dev points.\nLearned optimizer. We give the final hyperparam-\neter and objective terms used in each experiment in\nTable 10. Our objective ablation is given in 17, and\nwe select the best performing condition for each\ndataset according to dev set performance, using the\nsame selection criterion outlined previously. We\nkeep all weight coefficients λi equal rather than\ntuning them. Main refers to the first term in Eq.\n1, plus the KL term with random data. We use\nKtrain ≤5 for all experiments. For results across\nKvalues on zsRE, see Fig. 8.\nBaseline update method. We tune a baseline off-\nthe-shelf optimizer separately for each dataset, us-\ning rtest = 1 . Our performance criterion is the\n2725\nRelation % Test Data\nPlace of Birth 11.00\nAward Received 11.00\nCause of Death 5.66\nPlace of Death 11.00\nPlace of Burial 8.33\nEducated At 11.00\nChild 11.00\nOccupation 11.00\nSpouse 11.00\nSibling 9.01\nTable 11: Wikidata relations and their proportion of the\ntest data.\nDataset Optimizer LR Num. Steps\nFEVER AdamW 1e-6 100\nLeapOfThought SGD 1e-2 100\nzsRE SGD 1e-1 10\nWikidata5m SGD 1e-1 10\nTable 12: Final hyperparameters of the baseline update\nmethod for each task.\nsame as with learned optimizers, a raw average of\nUpdate Success Rate (averaged over each kind of\ndata), Retain Rate (Local Neutral) and ∆-Acc. The\ngrid search is over the following parameters: The\noff-the-shelf optimizers are from torch.optim\nand include {AdamW, SGD, and RMSProp} with\ndefault arguments (except for the learning rate).\nWe consider a number of maximum steps in {5,\n10, 100}. The learning rates we consider depend\non the optimizer: {1e-4, 1e-5, 1e-6} for AdamW,\n{1e-4, 1e-5, 1e-6} for RMSProp, and {1e-1, 1e-2,\n1e-3} for SGD. The LR ranges were selected af-\nter some initial manual exploration of the space.\nOur final hyperparameter values are shown in Ta-\nble 12 for each dataset. For comparison, De Cao\net al. (2021) use RMSProp with 100 update steps.\nThe LR for zsRE and Wikidata5m may seem quite\nhigh, but this is the condition that actually does the\nleast damage to the model’s accuracy on other data,\n∆-Acc. The baseline optimizes all of the train-\nable parameters in the language model, unlike the\nlearned optimizer which optimizes only attention\nand feedforward weights for purposes of parameter\nefficiency.\nB.3 Wikidata5m Additional Details.\nWe construct four paraphrases per Main Input by\nselecting from a set of alternative phrasings for the\nentity and relation in the Main Input. The syntax\nfor each paraphrase follows the same simple tem-\nplate as the Main Input, in contrast to zsRE where\nsyntax differs between paraphrases. A couple de-\ntails remain. Some relations are one-to-many, and\ntherefore we accumulate valid completing entities\nfrom the data as possible answers; later we com-\npute accuracy as an exact match with any possible\nanswer. All 10 relations appear in each split of the\ndata. Only 33.80% and 37.18% of the entities in\nthe dev and test splits are seen in the training data,\nthough we do not find that models perform better\non entities seen in training.\nB.4 LeapOfThought Additional Details\nThe LeapOfThought dataset consists of a fact and a\nclaim for each datapoint, where the truth of the fact\nimplies that the claim has label yi (True/False). All\nof the facts in the data are true, while half of the\nclaims are true and half are false. When training\nthe learned optimizer, we treat the the facts as the\nMain Input when training the learned optimizer\nand claims as entailed data. When training the\nTrue/False classifier, we fit to the claims, for which\ntest accuracy is 83.65 ( ±1.05). This seems to\ngeneralize well to the facts, as test accuracy here is\n93.66 (±0.87), although as the low contrapositive\naccuracy suggests (Table 3), the model seems to be\ntoo prone to predicting true for this data.\nSince very few of the Main Inputs are predicted\nas false, we run into a small dilemma when fit-\nting the learned optimizer with the use of the en-\ntailed data objective term. The entailment between\nfact and claim only holds when the fact is true, so\nwe can only compute the objective when updat-\ning a point from false to true. This ends up being\nless than 10% of the training data. We ultimately\nchoose to oversample points that fit this descrip-\ntion during training of the learned optimizer, which\nallows the learned optimizer to fully fit to the en-\ntailed data. Also note that during learned optimizer\ntraining, we include Entailed Data from other data\npoints besides the Main Inputin the KL term in Eq.\n1, and we measure ∆-Acc using both Main Inputs\nand Entailed Data.\nC Dataset Sources and Noise\nHere we give sources and licenses for each dataset,\nand we document some shortcomings of each\ndataset, with reference to examples in Table 15.\nDataset sources and licenses.FEVER and zsRE\nare available through the KILT4 resource and are\n4https://github.com/\nfacebookresearch/KILT/?fbclid=\nIwAR2WiFkl-7KLIQAoNI9bJgBVKWgsAQEDV342vV5_\nPcsKA881vpuXaELKBz0\n2726\nK\nK\n0\n10\n20\n30\n1 2 4 6 8 10\nr\nMemory Used (GB)\nMemory Usage byr\nFigure 4: Training memory usage in terms of K and r hyperparameters in our implementation, for a learned\noptimizer trained for a BART-base model on zsRE, using a batch size of 16. For comparison, the orange dashed\nline shows the memory use of training the BART-base model on zsRE, using the same batch size. Our use of the\nstop-gradient function limits the growth of runtime and memory w.r.t. bothKand r. By accumulating gradients\nacross points, memory w.r.t. ris kept constant. The same trick could be applied to the K looped gradient steps\ninside the Update function, at the trade-off of backpropagating Ktimes per point rather than one time.\nOurs De Cao et al. (2021) Mitchell et al. (2021)\nUpdate Success Rate (Main Input) Success rate Edit success\nUpdate Success Rate (Paraphrase) Equivalence accuracy Edit success\nUpdate Success Rate (Entailed Data) - -\nRetain Rate (Local Neutral) - -\nRetain Rate (All Data) Retain accuracy -\n∆-Acc (All Data) Performance deterioration Drawdown\nTable 13: A glossary of terms used in work on model update methods. Note metrics are not always calculated\nin exactly the same way. For instance, Performance deterioration is a ratio in accuracies rather than difference in\naccuracies, and edit success from Mitchell et al. (2021) combines two metrics in our case. The performance metric\nin Zhu et al. (2020) is an average of Update Success Rate (Main Input) and ∆-Acc.\navailable under the MIT license (Petroni et al.,\n2021). LeapOfThought data can be constructed\nthrough their available code5 and is also available\nunder the MIT license. The source data for Wiki-\ndata5m data can be downloaded through the KE-\nPLER6 code repository (Wang et al., 2021) and\nis available under the MIT license. Use of each\ndataset is in accordance with their intended licensed\nuses. The zsRE and Wikidata5m datasets do refer\nto people by name as they reference public figures\non Wikipedia. All datasets are in English.\nFEVER. Some claims are slightly vague or am-\nbiguous when taken on their own. For instance\n“Doug Ducey was the CEO of Cold Stone Cream-\nery and offered many opportunities to new hires”\nis rated True, though this will depend heavily on\nwhat one thinks “many opportunities” means. Sim-\nilar whether or not “L.A. Guns is a tattoo shop”\ndepends on which “L.A. Guns” one is referring to,\nthe tattoo shop or metal band. Of course, this is a\ngeneric issue of language, and not unique to this\ndataset. Some inputs seem to be a matter of person\nopinion: “Los Angeles is known for its food” is\nrated False.\n5https://github.com/alontalmor/\nLeapOfThought\n6https://github.com/THU-KEG/KEPLER\nLeapOfThought. Many examples use an “is a”\nrelation, producing sentences like “A sunlight is a\ngood health.” This could be more false than true,\nbut it’s a fairly nonsensical statement to begin with.\nThere are also other nonsensical or vague examples\nin the data: ”A friar is the opposite of mineral” is\nlabeled False. “A detective desires equal opportu-\nnity.” is labeled True. It is not immediately clear\nwhat conditions would make these statements true\nor false.\nzsRE. Some questions invoke potentially one-to-\nmany or temporally dependent relations, though\nthere is only one ground-truth answer per ques-\ntion in this dataset. For instance, a paraphrase of\nthe question about Gifford Pinchot in Table 15 is:\n”What disease did Gifford Pinchot have?” A per-\nson might have had many diseases over their life\nwhich could all be valid responses. The answer is\nespecially ambiguous for spatial relations, where a\nvalid answer might refer to a city, region, country,\nprovince, or continent.\nWikidata. Aliases sometimes vary greatly even\nas they refer to the same person, or they are sim-\nply noisy. For example, as shown in Table 15,\n“SusunW” appears in an entity name, but this is\nactually a username of someone who contributed\n2727\nDataset Model Acc Paraphrase Cons ↑ Entailment Acc ↑ Contrapositive Acc ↑\nFEVER RoBERTa-base 78.29 (0.86) - - -\nLeapOfThought RoBERTa-base 93.66 (0.87) - 85.63 (1.08) 16.51 (2.71)\nzsRE BART-base 21.01 (0.64) 69.50 (1.09) - -\nWikidata5m BART-base 10.21 (0.59) 25.84 (0.53) - -\nTable 14: Model accuracy and belief metric results and for four datasets.\nDataset Data Type Input Label(s)\nzsRE\nMain Input What did Gifford Pinchot die of? {Leukemia}Paraphrase How did Gifford Pinchot die?\nMain Input Player Ali Kanaan plays for what team? {Sporting Al Riyadi Beirut}Paraphrase What team is Ali Kanaan associated with?\nWikidata5m\nMain Input Margarita Nolasco Armas has relation ‘place\nof birth’ to {Orizaba, Veracruz; Orizaba;\netc.}Paraphrase SusunW/Margarita Nolasco Armas has rela-\ntion ‘born at’ to\nLocal Neutral Margarita Nolasco Armas has relation ‘place\nof death’ to\nMexico City; Ciudad de Mexico;\netc.\nMain Input Mary Good has relation ‘award received’ to {Garvan-Olin Medal; Arkansas\nWomen’s Hall of Fame; etc.}Paraphrase Mary Lowe Good has relation ‘winner of’ to\nLocal Neutral Mary Good has relation ‘educated at’ to {The University of Arkansas; U\nArkansas; etc.}\nFEVER Main Input Tardigrades are also known as space bears. True\nMain Input The Lion belongs to the genus Vulpes. False\nLeapOfThought\nMain Input A viper is a vertebrate. True\nEntailed Data A viper has a brain. True\nMain Input A amaranth is a herb. True\nEntailed Data A amaranth has a nose. False\nTable 15: Example datapoint from each dataset, and auxiliary data that accompanies the Main Input.\nto the Wikipedia article for Margarita Nolasco Ar-\nmas. Meanwhile, other aliases for J.R.R Tolkien\ninclude “Tolkienian” and “Mabel Suffield,” his\nmother. Rephrasings of relations might also create\nconfusing inputs, e.g. switching “child” with “has\nkids,” “daughter”, or “son.” Similar to zsRE, some\nrelations are also one-to-many and temporally de-\npendent (like occupation), though we hope that\nby using many valid answers we circumvent this\nissue to some extent when calculating prediction\ncorrectness.\nD Metric Computation and Bootstrap\nDetails\nMetric computation. The only computationally\ndifficult metric to calculate is ∆-Acc, which re-\nquires computing the updated language model’s\naccuracy on other data after every single belief up-\ndate. We randomly sample other data after every\nupdate for this purpose, using n = 30 points for\nzsRE and Wikidata5m and n = 200 points for\nFEVER and LeapOfThought. We ensure that all\nevaluation data is used at some point during this\nsampling by preferentially selecting data that has\nbeen infrequently selected before. We note that\nparaphrase consistency is easy to evaluate for a\nsmall number of paraphrases per datapoint, as we\nhave for both zsRE and Wikidata5m. Additionally,\non LeapOfThought, we compute ∆-Acc using both\nMain Inputs and Entailed Data.\nUpdate-Transitivity caveat. The % Update-\nTransitivity metric represents the answer to the\nquestion: if updating belief A changes belief B,\nand updating belief B changes belief C, what pro-\nportion of the time does updating A change C?\nWe would treat this as a normative metric that we\nhope to maximize, except we do not know in gen-\neral whether there is a confounding belief D that\ndetermines the relationship between B and C. If\nchanging A also changed a confounding belief D,\nthen we might not be able to expect that C should\nchange too. That said, when we have no reason to\nthink there are such confounding beliefs, we would\nexpect a logically consistent model to display 100%\nUpdate-Transitivity of their beliefs. In Fig. 2, for\ninstance, we see no reason to suspect there are con-\n2728\nFEVER ZSRE\n1 2 4 6 8 10 1 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nrtest\nUpdate Success Rate\nMethod by rtrain\nBaseline\nr=1\nr matches test\nAblation by r\nFigure 5: Ablation across values of rfor training and testing. On zsRE, our method outperforms the baseline when\nrtest = 10, and the gap is likely to increase as rtest rises further. When using a non-sequential objective from past\nwork, performance declines drastically as rtest rises.\nfounding beliefs for the relationship between the\ndate Bessie Smith died and the writer of Despicable\nMe 2, and therefore we would expect that updat-\ning the belief about what album Hot Right Now is\non would change the belief in Despicable Me 2’s\nauthorship (which it does).\nBootstrap computation.We account for sample\nand seed variance by block bootstrap (Efron and\nTibshirani, 1994). When there is a single statistic\nper data point, like Main Input Update Success, we\nform a matrix of shape n×sfor ndata points and\nsmodel seeds (where the seed was used for both\ntask model training and learned optimizer train-\ning). We then resample rows and columns of this\nmatrix 10,000 times, which was sufficient for con-\nvergence. When we perform hypothesis tests for\nthe difference in statistics between conditions, we\npair the data points by using the same rows of this\nmatrix at each step of the bootstrap (i.e. we conduct\npaired tests). For metrics involving multiple data\npoints per Main Input, like paraphrases or other\nrandom data, we make a simplifying assumption\nwhere we do not resample the multiple data points\nbut just compute the average metric for those data\npoints and treat that as the ground-truth statistics\nfor the Main Input. We explored using a full 3-\ndimensional bootstrap, where we resample among\nthese extra datapoints by constructing a matrix of\nshape n×s×n, but it was quite slow and gave\nsimilar results to the block bootstrap.\nE Additional Results\nAblation across num. sequential steps. Fig.\n5 shows the results for an ablation across rtest\nusing two kinds of learned optimizers: SLAG 1,\nUpdate Success Rate ∆-Acc\nDesired Label Main Input Paraphrases All Data\nBeam Label 91.19 (0.5) 92.07 (0.8) -0.39 (0.1)\nHard Label 94.46 (0.7) 94.45 (0.7) -0.24 (0.1)\nTable 16: Update metrics by optimizer training labels.\nwhere rtrain = 1 , and a SLAG condition where\nrtrain = rtest. It is critical to the success of learned\noptimizers to train them to update points sequen-\ntially when this is a desired application. Further,\nsequential updating with sequence prediction tasks\nis the only setting where we see learned optimizers\noutperform baselines across all relevant metrics.\nChoosing training labels for learned optimizers.\nIn early experiments, we found that it is beneficial\nto use all data points (including correctly predicted\npoints) as Main Inputs during training, rather than\nrestricting training to only incorrectly predicted\npoints. We still focus on correcting wrong outputs\nat test time. But so we must select what label to\nuse during optimizer training. To get a Hard Label,\nwe use the correct label for incorrectly predicted\npoints, and for correctly predicted points, we sim-\nply draw a label randomly from the labels in the\ntraining data. The alternative Beam Label condi-\ntion uses a sample from the model’s beam search\nfor a data point, as done in past work (De Cao\net al., 2021; Mitchell et al., 2021). We show up-\ndate metrics for zsRE split by the desired label in\nTable 16. If one’s goal is to fix wrong model out-\nputs, then it is much better to use either the correct\nlabel or a random label as the desired model out-\nput during training rather than a sample from the\nmodel’s beam search. Update success improves by\n2729\nObjective Term Ablation Update Success Rate Retain Predictions ∆ Acc\nDataset Objective Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER Main 100 (0.0) - - - 98.27 (0.1) -0.15 (0.1)\n(no KL) 100 (0.0) - - - 40.42 (0.6) -27.19 (1.2)\nLeapOfThought Main 100 (0.0) - 76.43 (5.3) - 96.84 (0.3) -1.22 (0.8)\n+Ent 100 (0.0) - 71.87 (5.3) - 96.52 (0.3) -0.40 (0.8)\nzsRE Main 94.46 (0.4) 94.44 (0.7) - - 81.96 (0.4) -0.24 (0.1)\n+Para 93.75 (0.4) 94.41 (0.7) - - 75.24 (0.5) -0.42 (0.2)\nWikidata5m\nMain 88.67 (0.7) 64.12 (0.7) - 49.78 (1.0) 71.04 (0.5) -1.54 (0.3)\n+Para 87.46 (0.7) 81.06 (0.7) - 47.15 (1.0) 63.02 (0.6) -1.55 (0.3)\n+LN 87.73 (0.7) 59.75 (0.7) - 60.49 (1.0) 72.69 (0.6) -1.57 (0.3)\n+Para+LN 87.02 (0.7) 81.18 (0.7) - 56.86 (1.0) 68.42 (0.6) -1.65 (0.3)\nTable 17: Belief update results by the objective terms used for the learned optimizer. We do not bold any numbers\nbased on statistical significance. For tuning purposes we select whichever condition achieves the higher selection\ncriterion without testing for statistical significance.\n3.27 (±0.65; p<1e−4) points for the Main Input\nand 2.38 (±1.05; p<1e−4) for Paraphrases, while\n∆-Acc rises by 0.15 (±0.18; p=.09).\nWhich beliefs are hard to update?We hypothe-\nsize that beliefs will be easier to update when they\nare more belief-like to begin with. We principally\nmeasure this via the correlation between update suc-\ncess rate and a belief’s consistency on paraphrases\nbefore the update, for our learned optimizer in a\nsingle-update setting (r= 1). Surprisingly, we ob-\nserve no relationship between update success and\nthe belief consistency. The correlation between\nconsistency and update success is near 0 for both\nzsRE (ρ = −.027) and Wikidata5m ( ρ = .013);\nsee Fig. 6 for a plot of the relationship. So it ap-\npears that the learned optimizer can update model\nbeliefs independently of how belief-like they are to\nbegin with. We would also be interested in consid-\nering consistency under entailment, but the update\nsuccess rate on LeapOfThought is already 100%,\nso there is no variance to explain.\nLearning curve.In Fig. 7 we show the learning\ncurve of a learned optimizer trained with SLAG\non zsRE. The Main Input Update Success Rate\nsteadily rises as a function of the training set size.‘\nAblation by objective term.We give objective\nablation results in Table 17. Surprisingly, we do\nnot always see that the objective terms help for the\ndata they are intended to help with. First, we ob-\ntain mixed results for the paraphrase objective. On\nzsRE, the objective term seems to hinder perfor-\nmance, with update success dropping on Main In-\nputs by 0.71 (±0.60; p=.021) and ∆-Acc dropping\nby 0.18 (±0.19; p=.069), while the paraphrase Up-\ndate Success Rate itself is unaffected. With Wiki-\nZSRE\nWikidata5m\n0.00 0.25 0.50 0.75 1.00\n0.85\n0.90\n0.95\n1.00\n0.85\n0.90\n0.95\n1.00\nPre−Update Consistency\nUpdate Success Rate\nWhich Beliefs Are Hard to Update?\nFigure 6: Beliefs are neither easier nor harder to update\ndepending on their consistency beforehand.\n85\n90\n95\n100\n103 103.5 104 104.5 105\nn\nMain Input Update Success\nLearning Curve for zsRE\nFigure 7: Main Input Update Success Rate across train-\ning set sizes, using SLAG on zsRE.\ndata5m, however, the paraphrase term improves\nparaphrase update success by a large margin of\n16.94 (±1.03; p<1e−4) points. Adding the Local\nNeutral (LN) term with the paraphrase term greatly\nimproves the LN Retain Rate for Wikidata5m, by\n9.71 points (±1.44; p<1e−4), though both of these\nterms come at a cost to Main Input Update Success,\nsimilar to zsRE. Lastly, we do not find that the en-\ntailment objective improves Entailed Data Update\nSuccess; in fact, this metric falls by 4.56 ( ±7.22;\n2730\n0.90\n0.92\n0.94\n0.96\n1 2 4 6 8 10\nK test\nUpdate Success Rate\nTraining Obj. (K train)\n1\nMatches Test\nAblation by K\nFigure 8: Ablation across values of Kfor training and\ntesting, using SLAG on zsRE. It is useful to train the\noptimizer using the value of Kit will use at test time.\nAsylum Records is an English\n record label.\n[y: false]\nThe New Orleans Pelicans\n play in the Eastern Conference\n of the NBA.\n[y: false]\nTelemundo is a English-language\n television network.\n[y: false]\nNew Orleans Pelicans compete\n in the NBA.\n[y: true]\nJohn Deighton worked in California.\n[y: true]\nVictoria (Dance Exponents\n song) was released in the\n Southern Hemisphere in 1982.\n[y: true]\nCarlos Santana is a US president.\n[y: false]\nRichard Dawkins has yet to\n appear on the internet.\n[y: false]\nBermuda Triangle is in the\n western part of the Himalayas.\n[y: false]\nEmma Watson was born.\n[y: true]\nHarold Macmillan was born\n on February 20, 1894.\n[y: false]\nFilming for Boyhood was stopped\n between 2002 and 2013.\n[y: false]\nCHiPs is an American comedy\n film.\n[y: true]\nStarrcade was eventually\n broadcast via pay-per-view\n umbrella.\n[y: false]\nCroatia has a king.\n[y: false]\nSaturn Corporation is also\n known as Toyota LLC.\n[y: false]\nBasildon is far away from\n England.\n[y: false]\nThe Cincinnati Kid is a boy.\n[y: false]\nParamore formed in 2007.\n[y: false]\nXHamster produces online\n content.\n[y: true]\nFigure 9: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at least\none another node.\np=.213) points with the objective.\nAblation by num. update steps.Fig. 8 shows the\nresults of an ablation across values of K using a\nlearned optimizer trained using SLAG with r= 1\non zsRE. Main Input Update Success rises by over\nthree points by increasing Ktest from 1 to at least\n5. Using a value of Ktrain that matches Ktest gives\na further increase of about 0.5 points.\nHumphrey Bogart was ranked\n greatest male star of Classic\n American cinema.\n[y: true]\nRachel Green appeared in\n every episode of Friends\n until the final episode in\n 2002.\n[y: false]\nAngela Bassett is alive.\n[y: true]\nColin Kaepernick became\n a starter in the National\n Football League.\n[y: true]\n1978 is Ian Brennan's year\n of birth.\n[y: true]\nA Floppy disk is composed\n of a thin and flexible magnetic\n transmission medium.\n[y: true]\nSaturn is only an asteroid.\n[y: false]\nDan O'Bannon died on December\n 17th, 2009.\n[y: true]\nBeaverton, Oregon's city\n center is in decline.\n[y: false]\nMargaret Thatcher was the\n most senior politician within\n the Conservative Party in\n the UK in 1975.\n[y: true]\nStarrcade was originally\n broadcast via television.\n[y: true]\nTaylor Lautner appeared\n in The Bernie Mac Show in 2001.\n[y: false]\nI Kissed a Girl was only recorded\n by Donald Trump.\n[y: false]\nJulianne Moore created the\n television series As the\n World Turns.\n[y: false]\nHighway to Heaven is an American\n television series.\n[y: true]\nDan O'Bannon work was primarily\n science fiction and horror,\n serving as a screenwriter\n and director.\n[y: true]\nSidse Babett Knudsen graduated\n on November 22nd, 1968.\n[y: false]\nAleister Crowley was an English\n citizen.\n[y: true]\nMagic Johnson was a tap dancer.\n[y: false]\nQueen (band) is a Canadian\n rock band.\n[y: false]\nFigure 10: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at least\none another node.\nOn February 2, 2013, Chris\n Kyle died.\n[y: true]\nThe Mirny (sloop-of-war)\n was a ship without allegiance.\n[y: false]\nSt. Anger was released by\n Sub Pop Records.\n[y: false]\nKnocked Up is a work of art.\n[y: true]\nMel B had a career.\n[y: true]\nAustralia (2008 film) production\n took place in Bowen.\n[y: true]\nDaag is a home.\n[y: false]\nHarold Macmillan was born\n on February 20, 1894.\n[y: false]\nThe Chrysler Building has\n yet to be surpassed in height.\n[y: false]\nHeavy Metal music was developed\n in the early 1970's.\n[y: true]\nKuching is a city in Singapore.\n[y: false]\nJames VI and I was a major advocate\n of a single parliament for\n Scotland and England.\n[y: true]\nCamden, New Jersey is a large\n human settlement.\n[y: true]\nDerek Hough barely starred\n in Make Your Move.\n[y: false]\nChile is a country.\n[y: true]\nA River Runs Through It has\n lost every Academy Award.\n[y: false]\nNatural Born Killers was\n based upon Tarantino's original\n screenplay without revision.\n[y: false]\nThe Lincoln-Douglas debates\n happened in Quincy, Illinois.\n[y: true]\nCarlos Santana is a musician.\n[y: true]\nDespicable Me 2 was produced\n by a company.\n[y: true]\nFigure 11: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at least\none another node.\n2731",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5647451281547546
    },
    {
      "name": "Computational linguistics",
      "score": 0.5337074398994446
    },
    {
      "name": "Natural language processing",
      "score": 0.4424419403076172
    },
    {
      "name": "Linguistics",
      "score": 0.43447864055633545
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34385043382644653
    },
    {
      "name": "Data science",
      "score": 0.32610708475112915
    },
    {
      "name": "Philosophy",
      "score": 0.18092718720436096
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1333535994",
      "name": "University of North Carolina Health Care",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    }
  ]
}