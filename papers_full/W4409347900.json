{
    "title": "Importance Weighting Can Help Large Language Models Self-Improve",
    "url": "https://openalex.org/W4409347900",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2315833497",
            "name": "Chunyang Jiang",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4287895561",
            "name": "Chi-Min Chan",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2094192222",
            "name": "Wei Xue",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2122326491",
            "name": "Qifeng Liu",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2114993276",
            "name": "Yike Guo",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6847753483",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6803096969",
        "https://openalex.org/W3033035102",
        "https://openalex.org/W4378473757",
        "https://openalex.org/W3120376221",
        "https://openalex.org/W2610403318",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W6846076640",
        "https://openalex.org/W6676840641",
        "https://openalex.org/W4388585881",
        "https://openalex.org/W4386566682",
        "https://openalex.org/W2102689555",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W7064840731",
        "https://openalex.org/W6852875431",
        "https://openalex.org/W6855617100",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W6851275496",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2982295985",
        "https://openalex.org/W4353112996",
        "https://openalex.org/W4378765257",
        "https://openalex.org/W1493730910",
        "https://openalex.org/W189742998",
        "https://openalex.org/W2103851188",
        "https://openalex.org/W4378170349",
        "https://openalex.org/W4308672130",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W4382498654",
        "https://openalex.org/W4391272947"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capability in numerous tasks and applications. However, fine-tuning LLMs using high-quality datasets under external supervision remains prohibitively expensive. In response, LLM self-improvement approaches have been vibrantly developed recently. The typical paradigm of LLM self-improvement involves training LLM on self-generated data, part of which may be detrimental and should be filtered out due to the unstable data quality. While current works primarily employs filtering strategies based on answer correctness, in this paper, we demonstrate that filtering out correct but with high distribution shift extent (DSE) samples could also benefit the results of self-improvement. Given that the actual sample distribution is usually inaccessible, we propose a new metric called DS weight to approximate DSE, inspired by the Importance Weighting methods. Consequently, we integrate DS weight with self-consistency to comprehensively filter the self-generated samples and fine-tune the language model. Experiments show that with only a tiny valid set (up to 5% size of the training set) to compute DS weight, our approach can notably promote the reasoning ability of current LLM self-improvement methods. The resulting performance is on par with methods that rely on external supervision from pre-trained reward models.",
    "full_text": "Importance Weighting Can Help Large Language Models Self-Improve\nChunyang Jiang, Chi-Min Chan, Wei XueB, Qifeng Liu, Yike GuoB\nHong Kong University of Science and Technology\n{cjiangaq, cchanbc}@connect.ust.hk, {weixue, liuqifeng, yikeguo}@ust.hk\nAbstract\nLarge language models (LLMs) have shown remarkable ca-\npability in numerous tasks and applications. However, fine-\ntuning LLMs using high-quality datasets under external su-\npervision remains prohibitively expensive. In response, LLM\nself-improvement approaches have been vibrantly developed\nrecently. The typical paradigm of LLM self-improvement in-\nvolves training LLM on self-generated data, part of which\nmay be detrimental and should be filtered out due to the un-\nstable data quality. While current works primarily employs\nfiltering strategies based on answer correctness, in this pa-\nper, we demonstrate that filtering out correct but with high\ndistribution shift extent (DSE) samples could also benefit\nthe results of self-improvement. Given that the actual sam-\nple distribution is usually inaccessible, we propose a new\nmetric called DS weight to approximate DSE, inspired by\nthe Importance Weighting methods. Consequently, we inte-\ngrate DS weight with self-consistency to comprehensively\nfilter the self-generated samples and fine-tune the language\nmodel. Experiments show that with only a tiny valid set (up\nto 5% size of the training set) to compute DS weight, our ap-\nproach can notably promote the reasoning ability of current\nLLM self-improvement methods. The resulting performance\nis on par with methods that rely on external supervision from\npre-trained reward models.\nIntroduction\nRecently, Large Language Models (LLMs) have made im-\npressive achievements on a large amount of NLP tasks and\napplications (Li et al. 2023a; OpenAI 2023; Yang et al. 2023;\nLi et al. 2023b). Moreover, new capabilities emerge in LLMs\nwith the model size scaled to hundreds of billions of param-\neters, especially the general reasoning capabilities (Kojima\net al. 2022). Relevant techniques like in-context few-shot\nlearning (Brown et al. 2020a), Chain-of-Thought prompt-\ning (Wei et al. 2022), and self-consistency (Wang et al.\n2023a) were further proposed to get better performance.\nDespite the remarkable capabilities of LLMs pre-trained\non the large corpus, fundamentally improving the modelâ€™s\nperformance still necessitates fine-tuning on a great amount\nof high-quality supervised data (Huang et al. 2023a), which\nis usually costly. To alleviate this problem, many works are\ncommitted to investigating the self-improvement ability of\nCopyright Â© 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nLLMs (Shinn et al. 2023; Madaan et al. 2023; Vernikos et al.\n2024). Among them, fine-tuning the LLM on self-generated\ndata appears as one of the most promising way (G Â¨ulc Â¸ehre\net al. 2023; Huang et al. 2023a; Wang et al. 2023b; Xu\net al. 2023; Li et al. 2024). This formula typically includes\ngenerating reasoning thoughts and answers on unsupervised\ndatasets, filtering data, and fine-tuning models on the self-\ngenerated data (Huang et al. 2023a). It is regarded as an\nattractive approach for LLMs to self-supervise by utilizing\nunlabeled data without external supervision.\nThe primary challenge of utilizing self-generated data\nis the variability in data quality. While high-quality sam-\nples can enhance the modelâ€™s reasoning abilities, there are\nlow-quality samples that may detrimentally affect perfor-\nmance (Li and Qiu 2023). For example, an incorrectly gen-\nerated answer could mislead the model. Therefore, a good\nfiltering strategy is decisive for effective self-improvement.\nMany approaches have been proposed to address this is-\nsue. Inspired by Self-Consistency (Wang et al. 2023a),\nLMSI (Huang et al. 2023a) adopts majority voting to se-\nlect the most consistent answer, under the assumption that\nconsistency is positively related to the correctness. MoT (Li\nand Qiu 2023) further introduces uncertainty to the filter-\ning strategy, by utilizing entropy to exclude high-uncertainty\ndata points. Self-Alignment (Li et al. 2024) demonstrates\nthat prompting the LLM to self-filter is also feasible.\nHowever, present methods mostly emphasize assessing\nthe correctness of generated samples, yet ignore the dis-\ntribution shift problem. Specifically, the distribution of the\nLLM self-generated data may differ from that of real-world\ndata, and fine-tuning models on samples with high distri-\nbution shift extent (DSE) may defect the resulting perfor-\nmance (Shumailov et al. 2023a). In this paper, we demon-\nstrate that even self-generated samples with correct answers\ncan possess high DSE, potentially degrading model per-\nformance. Consequently, filtering out high DSE samples\nis essential to further promote the efficacy of LLM self-\nimprovement.\nTo exclude samples with high DSE, the primary question\nis how to estimate the DSE, since the actual data distribu-\ntion is usually inaccessible. We note Importance Weighting\n(IW) (Sugiyama, Krauledat, and M Â¨uller 2007) as a well-\nknown approach to address the traditional distribution shift\nproblem (Sugiyama and Kawanabe 2012), where the key\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n24257\nidea is deriving importance weights based on the distribution\nratio between test and training data, and using it to rebuild\nan unbiased training loss. IW usually contains two steps:\nweight estimation computes test-over-training density ratio\nand weighted classification utilizes the ratio to weight each\ndata point and train the model (Fang et al. 2020).\nInspired by IW, we propose Distribution Shift Weight\n(DS weight) as a new metric to measure the DSE of self-\ngenerated samples. Based on this, we build an LLM self-\nimprovement framework that incorporates both the correct-\nness and DSE in its filtering strategy. Specifically, given a\nquestion-only dataset, we first let a pre-trained LLM gen-\nerate multiple reasoning thoughts as well as answers. Then\nwe create a tiny valid set comprising a few human-written\ndemonstrations. With the pre-trained LLM and valid set, we\nleverages a simple approximation for importance weights\nto compute DS weight, as a measure of DSE, for each\ntraining data point. We subsequently combine the results\nfrom majority voting (for correctness) and DS weight (for\nDSE) to filter the dataset and fine-tune the LLM. We de-\nnote our framework as Importance Weighting-based Self-\nImprovement (IWSI). Experiments show that the perfor-\nmance of IWSI largely surpasses baseline self-improvement\nmethods and rivals the enhancements achieved with super-\nvision from the pre-trained reward model.\nOur contributions are threefold: (1) We propose a met-\nric called DS weight to approximate the DSE of LLM self-\ngenerated data, with help from a tiny valid set. (2) Leverag-\ning DS weight, we build a novel self-improvement frame-\nwork called IWSI where the filtering strategy considers both\nthe answer correctness and DSE. (3) We empirically exam-\nine the effectiveness of our proposed method, analyze the\nimpact of high DSE samples on LLM self-improvement, and\nexplore how DS weight interacts with other filtering criteria.\nRelated Work\nLLM Self-Improvement\nFundamentally improving LLMsâ€™ reasoning ability essen-\ntially requires fine-tuning on a large amount of high-\nquality supervised data. However, this methodology faces\nthe threat that the stock of high-quality language data will\nbe exhausted in some day (Villalobos et al. 2022). Self-\nimprovement emerges as a promising approach to utilize\nthe inherent knowledge to make supervision for self-training\nLLMs. While LLMs can easily generate extensive data, the\ndata quality is not always guaranteed (Huang et al. 2023b)\nand training on unfiltered data may even cause performance\ndegradation (Shumailov et al. 2023b). Therefore, an essen-\ntial requirement in LLM self-improvement is data filtering.\nPioneering works (Wang et al. 2023b; Bai et al. 2022;\nXu et al. 2023) use language models to generate diverse\ntypes of data such as feedback, instructions, and questions.\nThey filter data by heuristic rules as well as manual inspec-\ntion, which is challenging and costly. LMSI (Huang et al.\n2023a) proposed a framework including generating data for\na question-only dataset and using the majority voting (self-\nconsistency) (Wang et al. 2023a) to select the most consis-\ntent answers, which is empirically proven to be effective\namong various tasks. LMSI also demonstrates that the an-\nswer correctness is positively relevant to self-consistency.\nAlong with this work, MoT (Li and Qiu 2023) proposes fur-\nther filtering the consistent answers by entropy, which mea-\nsures the answer uncertainty. Self-Alignment (Li et al. 2024)\nshows it is feasible to prompt the LLM self-filtering the\ngenerated data. To comprehensively evaluate the generated\ndata, some works use external pre-trained LMs as the reward\nmodel to score the generated data, such as GENIE (Yehudai\net al. 2024) and ReST (G Â¨ulc Â¸ehre et al. 2023). With external\nsupervision from the reward model, their filtering strategies\nare typically more considered.\nImportance Weighting\nImportance weighting (IW) is a primary approach to miti-\ngate the influence of distribution shift problem (Sugiyama\nand Kawanabe 2012). The typical IW process includes two\nsteps: weight estimation and weighted classification. Weight\nestimation approximates the importance weights, which are\nsubsequently used in the weighted classification stage to\nbuild a weighted training loss (Fang et al. 2023).\nTraditional IW methods mainly estimate the importance\nweights by assessing the matching between training and test\ndistribution in different ways, such as maximum mean dis-\ncrepancy in a reproducing kernel Hilbert space (Huang et al.\n2006), KL divergence (Sugiyama et al. 2007), and squared\nloss (Kanamori, Hido, and Sugiyama 2009). While these\nmethods work well in linear models, their performances de-\ngrade largely in deep learning scenarios (Fang et al. 2020).\nTo overcome this, DIW (Fang et al. 2020) proposes an end-\nto-end dynamic solution, which uses a deep network to pre-\ndict the importance weights, and repeats weight estimation\nand weighted classification stages to iteratively converge on\nthe optimal solution.\nIn this paper, we use some lemmas and empirical results\nin DIW to build the DS weight for estimating the DSE of\nself-generated data.\nMethodology\nFig. 1 shows the overview of IWSI. Given an unsuper-\nvised (question-only) datasetDğ‘, we first use the pre-trained\nLLM Mğ¿ to generate multiple candidate answers as well\nas the reasoning thoughts for each question, using CoT\nprompts (Wei et al. 2022). Following LMSI (Huang et al.\n2023a), we adopt the majority voting to keep the most con-\nsistent answer and corresponding thoughts for each ques-\ntion, resulting in the consistency-filtered dataset Dğ‘. Then\nwe calculate DS weight for every data point in Dğ‘, with the\nhelp of a tiny valid setDğ‘£. Lastly, we filterDğ‘ into Dğ‘‘ğ‘  uti-\nlizing the DS weight and fine-tune the model Mğ¿. The fol-\nlowing sections elaborate on different components of IWSI.\nCandidate Answers Generation and\nSelf-Consistency Filtration\nIn this stage, we let the pre-trained LLM Mğ¿ generate can-\ndidate answers as well as reasoning thoughts for an unsu-\npervised dataset Dğ‘ which only contains unlabeled ques-\ntions. Given a question ğ‘ğ‘– âˆˆ Dğ‘, we concatenate Few-\n24258\nquestion\nCoT prompts\n120 pages will take\n5h. The answer is 5.\n8 pages take 20 min.\n... That is 600 / 60 =\n10h. The answer is 10.\n120 pages must be\n...... That is 300/60 =\n5 h. The answer is 5.\n...\nMajority\nVoting\nCandidate Answers Generation\nand Self-Consistency Filteration\nQ: Joy can read 8 pages in 20 min. How many\nhours will it take her to read 120 pages?\nA: 120 pages will take 5h. The answer is 5.\nQ: Weng earns $12 an hour. Yesterday, she\njust did 50 min. How much did she earn\nA: 12 / 6 * 5 = $10. The answer is 10.\nQ: John writes 20 pages a day.  How long will\nit take him to write 1200 pages?\nA: 1200/20=60 days. The answer is 60.\n...\nGenerate\nCandidate\nAnswers\nDS Weight\nComputation\nSelf-training Utilizing DS Weight\nto Improve LLM\nQ: 3 cars in the parking lot\nand 2 more cars arrive,\nhow many cars now?\nA: 3 + 2 = 5 cars. The\nanswer is 5.\nQ: ......\nQ: Joy can read 8 pages in\n20 min. How many hours\nwill it take her to read 120\npages?\nA:\n120 pages will take\n5h. The answer is 5.\n120 pages must be\n...... That is 300/60 =\n5 h. The answer is 5.\nUnsupervised\nDataset\nStore Q-A\npairs\nvalid set\nğŸ™ \n...\nFiltering out examples\nwith high DS weight\nFigure 1: The overview of IWSI. Given the unsupervised datasetDğ‘, the pre-trained LLM Mğ¿ is first used to generate multiple\ncandidate answers as well as the reasoning thoughts, prompted by CoT examples. Then IWSI uses majority voting to select the\nmost consistent answer and corresponding thoughts, stored in Dğ‘. With the help of Dğ‘£, IWSI calculates DS weight for each\ndata point in Dğ‘. IWSI filters Dğ‘ into Dğ‘‘ğ‘  by keeping samples with the ğ‘˜%-lowest DS weight and lastly self-trains Mğ¿.\nShot-CoT (Wei et al. 2022) prompts with ğ‘ğ‘– to form the\ninput text ğ‘¥ğ‘–. With temperature ğ‘‡ > 0, we let Mğ¿ sam-\nple ğ‘š candidate answers [ğ‘ğ‘–1 , ğ‘ğ‘–2 , . . . , ğ‘ğ‘–ğ‘š ] and their rea-\nsoning thoughts [ğ‘Ÿğ‘–1 , ğ‘Ÿğ‘–2 , . . . , ğ‘Ÿğ‘–ğ‘š ]. Then we select the most\nconsistent answer Ë†ğ‘ğ‘– by majority voting (Wang et al. 2023a),\nË†ğ‘ğ‘– = arg maxğ‘ğ‘–ğ‘—\nÃğ‘š\nğ‘˜=1 ğŸ™(ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘˜ ), and keep the correspond-\ning reasoning thoughts ğ‘…ğ‘– = {ğ‘Ÿğ‘–ğ‘— |ğ‘ğ‘–ğ‘— = Ë†ğ‘ğ‘–, 1 â‰¤ ğ‘— â‰¤ ğ‘š}. By\nrepeating over each question in Dğ‘, the consistency-filtered\ndataset Dğ‘ is built.\nDS Weight Computation\nTo elaborate DS Weight, we first introduce some important\npreliminaries in the distribution shift problem and impor-\ntance weighting methods.\nDistribution shift problem denotes that the training data\nand test data are drawn from two different distributions\nğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ , and ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› â‰  ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ (Sugiyama and Kawan-\nabe 2012). A common assumption for distribution shift is\nthat there exists a function ğ‘¤âˆ—(ğ‘¥), holding that:\nğ”¼ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡ (ğ‘¥) [ ğ‘“ (ğ‘¥)] = ğ”¼ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› (ğ‘¥) [ğ‘¤âˆ—(ğ‘¥) Â·ğ‘“ (ğ‘¥)] (1)\nfor any function ğ‘“ of ğ‘¥ (Fang et al. 2020). Based on Eq. 1,\nimportance weighting methods (Sugiyama, Krauledat, and\nMÂ¨uller 2007; Sugiyama et al. 2007) deal with distribution\nshift in two steps: weight estimation finds a proper solution\nfor ğ‘¤âˆ—(ğ‘¥); weighted classification trains the model with a\nweighted loss derived by substituting ğ‘“ in Eq. 1 with the\ntarget loss function.\nObviously, it plays a decisive role in importance weight-\ning that finding the appropriate importance weights W =\n{ğ‘¤ğ‘–}ğ‘ğ‘¡ , to approximateğ‘¤âˆ—(ğ‘¥) in Eq. 1. To simplify the ques-\ntion, DIW (Fang et al. 2020) provides an empirical surrogate\ngoal with the help of a valid set:\n1\nğ‘ğ‘£\nğ‘ğ‘£âˆ‘ï¸\nğ‘—=1\nL(M( ğ‘¥ğ‘£\nğ‘— )) â‰ˆ1\nğ‘ğ‘¡\nğ‘ğ‘¡âˆ‘ï¸\nğ‘–=1\nğ‘¤ğ‘– Â· L(M(ğ‘¥ğ‘¡\nğ‘– )). (2)\n24259\nHere ğ‘ğ‘£, ğ‘ğ‘¡, ğ‘¥ğ‘£, and ğ‘¥ğ‘¡ indicate the size of the valid set, the\nsize of the training set, data in the valid set, and data in the\ntraining set. M is the training model and L represents the\ntraining loss.\nWhile in DIW, Eq. 2 is used as a goal to train a deep\nmodel that predicts the desired W, we use Eq. 2 to design a\nnaive measurement for the distribution shift extent between\ntraining samples and valid set. Our intuition is that when\nthe training data distribution is identical to the valid data\ndistribution, ğ‘¤ğ‘– â‰¡ 1 would be a proper solution to Eq. 2.\nConversely, the larger the actual ğ‘¤ğ‘– differs from 1, the more\ndifferent the training distribution and valid distribution are.\nBased on this idea, we first design a naive estimation ğ‘¤â€²\nğ‘–\nfor ğ‘¥ğ‘¡\nğ‘– by regarding ğ‘ğ‘¡ as 1:\nğ‘¤â€²\nğ‘– =\nÃ\nğ‘¥ğ‘£\nğ‘— âˆˆD ğ‘£ L(M ğ¿ (ğ‘¥ğ‘£\nğ‘— ))\nğ‘ğ‘£ Â· L(Mğ¿ (ğ‘¥ğ‘¡\nğ‘– )) (3)\nwhere Mğ¿ is the pre-trained LLM, L denotes the sft\nloss (Brown et al. 2020b), Dğ‘£ is a tiny valid set and ğ‘¥ğ‘¡\nğ‘–\nis a self-generated training data point. Here we notice that\nthe value range of ğ‘¤â€²\nğ‘– is (0, +âˆ) while the ideal value is 1,\nwhich creates asymmetry between the two deviation direc-\ntions (lower than 1 and greater than 1) and makes filtering\ninconvenient. Therefore, to establish symmetry for both shift\ndirections, we define DS weight ğ‘¤ğ·ğ‘†\nğ‘– as:\nğ‘¤ğ·ğ‘†\nğ‘– =\n(\nğ‘¤â€²\nğ‘– if ğ‘¤â€²\nğ‘– â‰¥ 1\n1\nğ‘¤â€²\nğ‘–\nif ğ‘¤â€²\nğ‘– < 1 (4)\nUtilizing DS Weight to Improve LLM\nWith DS weight approximating DSE, we are able to further\nfilter the self-generated data in Dğ‘, excluding data points\nthat possibly possess higher DSE.\nFirst, all data points are ranked with respect to their DS\nweight ğ‘¤ğ·ğ‘†\nğ‘– , and the ğ‘˜-percentile ğœğ‘˜% is selected, s.t.\nÃ| Dğ‘ |\nğ‘– ğŸ™(ğ‘¤ğ·ğ‘†\nğ‘– â‰¤ ğœğ‘˜%)\n|Dğ‘| = ğ‘˜% (5)\nwhere | Â· |denotes the set size and ğ‘¤ğ·ğ‘†\nğ‘– is the corresponding\nDS weight of sample ğ‘¥ğ‘–. As a result, only samples whose\nğ‘¤ğ·ğ‘†\nğ‘– â‰¤ ğœğ‘˜% are kept to train the model Mğ¿. The training\nloss can be written as:\nLğ¹ = 1\n|Dğ‘| Â·ğ‘˜%\nDğ‘âˆ‘ï¸\nğ‘¥ğ‘–\nğŸ™ğ‘˜% (ğ‘¥ğ‘–) Â· L(Mğ¿ (ğ‘¥ğ‘–)) (6)\nwhere ğŸ™ğ‘˜% (ğ‘¥ğ‘–) equals to ğŸ™(ğ‘¤ğ·ğ‘†\nğ‘– â‰¤ ğœğ‘˜%) and L represents\nthe sft loss.\nAnother natural way to utilize DS weight is directly em-\nploying Eq. 3 to calculate a weighted loss, which is more\nanalogous to the standard IW procedure. We also imple-\nment this variant in our work and denote it as IWSI-w. The\nweighted loss is:\nLğ‘Š = 1\n|Dğ‘|\nDğ‘âˆ‘ï¸\nğ‘¥ğ‘–\nğ¶ğ‘™ğ‘–ğ‘ (ğ‘¤â€²\nğ‘–, ğ¶) Â· L(Mğ¿ (ğ‘¥ğ‘–)) (7)\nwhere ğ¶ is a constant. We clip ğ‘¤â€²\nğ‘– to (0, ğ¶] for stabilizing\nthe training process.\nHowever, we found that IWSI-w is much less effective\nthan IWSI. We believe this is mainly attributed to the inad-\nequacy of Eq. 3. Empirical results and details are discussed\nin the experiment section.\nExperiment\nSetup\nDatasets We conduct experiments on six datasets across\nthree types of tasks: Arithmetic Reasoning:gsm8k (Cobbe\net al. 2021) and SV AMP (Patel, Bhattamishra, and Goyal\n2021). Natural Language Inference:Adversarial NLI sub-\nsets (Nie et al. 2020). ANLI-A1 and ANLI-A2 subsets are\nused. Commonsense Reasoning:OpenBookQA (Mihaylov\net al. 2018) and StrategyQA (Geva et al. 2021).\nFor all datasets, only the questions are used to self-\ngenerate candidate answers. For gsm8k and SV AMP, we\nkeep the original question format, which is the open-ended\nquestion. For the other four datasets, we unify the ques-\ntion format to the multiple choice question. The LLM must\nchoose one option as its answer.\nTo build the valid set, we extract rationales from the orig-\ninal datasets apart from SV AMP, for which we manually\nwrite rationales. The size of valid sets varies among differ-\nent datasets, but none of them exceeds 5% size of the cor-\nresponding training set. Appendix A provides more details\nabout the split and statistics of all datasets.\nBaselines The goal of our experiments is to verify whether\nincorporating DS weight into the filtering strategy in our\nproposed approach can help LLMs self-improve. Therefore,\ngiven the same base model, we compare IWSI with the fun-\ndamental self-improvement framework LMSI (Huang et al.\n2023a), and some variants that we implement by adopting\ntrendy filtering strategies designed for training LLMs on\nmodel-generated data.\nLMSI (Huang et al. 2023a) is the first self-improvement\nframework that significantly improves LLMsâ€™ reasoning\nability without any external supervision. The core idea of\nLMSI is adopting majority voting to select answers that are\nmost likely correct, thus filtering the self-generated data.\nMoT (Li and Qiu 2023) uses entropy to measure the un-\ncertainty of the answers and further filters data. We combine\nthis technique with LMSI and denote it as Entropy-filter.\nSelf-Alignment (Li et al. 2024) shows that LLM self-\nevaluation could be helpful in filtering strategy. We imple-\nment this idea with LMSI and denote it as Self-filter.\nWorks like GENIE (Yehudai et al. 2024) and\nReST (G Â¨ulc Â¸ehre et al. 2023) use pre-trained models to\nevaluate the self-generated samples. Intervened by external\nsupervision, their filtering results are usually more compre-\nhensive and meticulous. Following that, we also implement\na variant of LMSI for reference, the RM-filter. RM-filter\nuses a pre-trained reward model to score the generated data,\nas GENIE (Yehudai et al. 2024) does. 1\n1https://huggingface.co/OpenAssistant/reward-model-deberta-\nv3-large-v2\n24260\ngsm8k SV\nAMP ANLI-A1 ANLI-A2 OpenBookQA StrategyQA Avg.\nbase 7.0\n14.7 16.4 14.6 31.8 48.3 22.1\nLMSI 27.9\n45.0 25.2 22.6 31.6 51.4 34.0\nEntropy-filter 22.7 56.0 25.2 22.8 33.4 51.2 35.2\nSelf-filter 35.6 62.7 22.8 25.6 35.0 50.4 38.7\nIWSI-w 37.0 43.3 21.8 21.8 31.8 49.2 34.2\nIWSI 37.6 62.7 27.2 23.4 37.0 54.6 40.4\nRM-filter 40.0\n66.3 25.6 25.0 34.2 51.4 40.4\nTable 1: Accuracy results on all datasets. Numbers in the table are the accuracy percent. The first part is the performance of the\nbase model. The second part is the performance of three baseline self-improvement methods, our proposed method IWSI, and\na variant IWSI-w. As RM-filter uses the external reward model, we list its performance separately at the bottom of the table.\nImplementation details We select Llama3-8B as our base\nmodel (Touvron et al. 2023). For each question, we gen-\nerate 15 candidates, with temperature ğ‘‡ = 1.1. All train-\ning process is performed on eight RTX-4090 GPUs. The\ntraining batch size per device is set to 1 and the gradient\naccumulation steps is 4. We use LoRA (Hu et al. 2022)\nto do fine-tuning. We use AdamW (Loshchilov and Hutter\n2019) optimizer and the learning rate is 3e-4. Few-Shot-CoT\nprompts are only applied in generating candidate answers\nand the evaluation stage. CoT examples for each dataset,\nprompts used for Self-filter, and details about how to de-\nrive the answer from output texts are given in Appendix D.\nThe source code and supplementary materials are available\nat https://github.com/rubickkcibur/IWSI.\nMain Results\nThe main comparison results are shown in Table 1. The eval-\nuation metric is accuracy percent and all results are derived\nby greedy decoding. The top part is the performance of the\nbase model. The middle part are self-improvement baselines\nand our proposed method IWSI. For reference, we list the\nperformance of RM-filter at the bottom of the table. For fair-\nness, we universally set the filtering percentage ğ‘˜ = 80 for\nIWSI, Entropy-filter, Self-filter, and RM-filter.\nAmong self-improvement methods (the middle part),\nIWSI is the only one that consistently outperforms LMSI,\nand it also achieves the best in almost all datasets. We further\nempirically demonstrate that the superiority of IWSI primar-\nily stems from excluding self-generated samples with higher\nDSE, rather than merely from access to part of the informa-\ntion of the valid set (the mean loss value of valid samples).\nDetails are in Appendix F.\nFor IWSI-w, the variant of IWSI that uses DS weight to\ncompute weighted loss other than filtering data, it generally\nperforms worse than IWSI, even though IWSI-w is more\ncompliant with the standard importance weighting formula.\nThe most possible reason is that unlike deep methods like\nDIW (Fang et al. 2020), which uses a deep neural network\nto learn the weights, our weight estimation (Eq. 3) is a pretty\nnaive approach. While it largely reduces computational cost,\nit also omits the semantic similarity among training samples,\npotentially compromising efficacy. Therefore, the weighted\nloss in IWSI-w might make the training process difficult and\nnoisy. In contrast, IWSI only uses the weight as an indicator\nto rank the samples with respect to DSE, without directly\nincorporating the weight into the training loss, which makes\nthe overall process more robust.\nAs for the RM-filter, we found that it does not always\nperform the best among all six datasets, even though it in-\ntroduces external supervision by using a pre-trained reward\nmodel. As Table 1 shows, after incorporating both the an-\nswer correctness and DSE of samples, the overall perfor-\nmance of IWSI is comparable to that achieved with external\nsupervision from a pre-trained reward model.\nHyperparameter Study\nWe investigate the effect of varying the filtering threshold ğ‘˜\nand corresponding percentile ğœğ‘˜% (in Eq. 5). Fig. 2 shows\nthe accuracy results on gsm8k, StrategyQA, and ANLI-A1.\nAs the figure shows, either a too-large or too-small ğ‘˜ value\nwill make the performance degrade. When ğ‘˜ is very large,\nmore samples with high DSE will be kept, thus potentially\nharming the performance. If the ğ‘˜ is pretty small, there will\nnot be sufficient samples kept to support the model training.\nThe optimal ğ‘˜ value range varies across different tasks. In\ngeneral, around 80% would be an appropriate choice.\nFig. 3 shows the varying ğ‘˜-percentile ğœğ‘˜% of DS weight.\nWhile ğœğ‘˜% of different datasets are similar when ğ‘˜ is very\nsmall, the difference becomes larger as ğ‘˜ increases. This\nphenomenon suggests that the boundary above which the\nDSE of samples can be regarded as â€highâ€ is relative ac-\ncording to different datasets.\nValid Set Analysis\nThe valid set Dğ‘£ plays a crucial role in IWSI. It determines\nthe calculation results of DS weight and subsequently steers\nthe filtering strategy. Therefore, variation in the composition\nof the valid set can introduce randomness and thus poten-\ntial instability. In this section, we take the gsm8k dataset as\nexample to discuss the impact of valid set.\nWe employ the loss value distribution as the analytical\ntool and, for simplicity, we assume all distributions of dif-\nferent sample sets conform to the normal distribution. For\nexample, the loss value distribution of valid set is denoted\nas Nğ‘£ (ğœ‡ğ‘£, ğœ2\nğ‘£ ), where ğœ‡ğ‘£ and ğœğ‘£ are the mean and standard\ndeviation respectively.\n24261\nFigure 2: Accuracy results with varying ğ‘˜ values.\nFigure 3: ğœğ‘˜% (in Eq. 5) with varying ğ‘˜ values.\nFig. 4 shows distributions of the valid set and self-\ngenerated samples before and after IWSI. Analogous to our\nintuition, the distributions differ significantly between valid\nset samples and self-generated samples before IWSI, and be-\ncome much closer after IWSI, illustrating the effectiveness\nof IWSI in handling the distribution shift problem. Further-\nmore, we provide quantitative analyses and a case study in\nAppendix E for a better understanding of how the LLM gen-\neration was affected by IWSI.\nThe next question is would the randomness of valid set\ncomposition cause great instability in IWSI, since ğœğ‘£ is ap-\nparently not small enough. The answer is â€Noâ€ as long as\nthere is an adequate valid set sizeğ‘ğ‘£. Theoretically, in Eq. 3,\nit is only the sample mean, denoted as Â¯Lğ‘£, that matters. Â¯Lğ‘£\nis also subject to the normal distribution, with its standard\ndeviation inversely proportional to the size ğ‘ğ‘£:\nÂ¯Lğ‘£ =\nÃ\nğ‘¥ğ‘£\nğ‘— âˆˆD ğ‘£ L(M ğ¿ (ğ‘¥ğ‘£\nğ‘— ))\nğ‘ğ‘£\nÂ¯Lğ‘£ âˆ¼ NÂ¯ğ‘£ (ğœ‡ğ‘£, ( ğœğ‘£\nğ‘ğ‘£\n)2)\n(8)\nEq. 8 implies that increasingğ‘ğ‘£ can scale down the variance\nof Â¯Lğ‘£, thus making the estimation more stable. More impor-\ntantly, it is completely irrelevant to the size of the training\nsamples. For instance, in gsm8k, if the valid set size is 100,\nthe standard deviation of Â¯Lğ‘£ is Â¯ğœğ‘£ = ğœğ‘£\n100 = 4.1Ã—10âˆ’3, which\nis small enough to mitigate the interference of randomness.\nğ‘ğ‘£\nComp.1 Comp.2\nComp.3\nÂ¯Lğ‘£ acc Â¯Lğ‘£ acc Â¯Lğ‘£ acc\n50 2.053\n36.77 2.083 36.54 2.091 36.47\n100 2.077 36.69 2.086 36.47 2.054 37.00\nTable 2: Results of different valid sets on gsm8k.\nFigure 4: Loss value distributions of the valid set sam-\nples, self-generated samples of base model (generated-base),\nand self-generated samples after IWSI (generated-IWSI), of\ngsm8k. ğœ‡ and ğœ denote the mean and standard deviation.\nTo empirically investigate the influence of different valid\nset compositions, we randomly constitute six subsets of the\nvalid set of gsm8k and test IWSI with them. Table 2 shows\nthe results. ğ‘ğ‘£ denotes the valid set size. Â¯Lğ‘£ is the sample\nmean of different composition. We use acc as the metric.\nAs we can see, the impact of different compositions on\nthe accuracy results is quite minimal. We believe this is pri-\nmarily attributed to thedouble-robustness of IWSI. First, the\nDS weight calculation is robust to the valid set composition,\nsince it only uses the sample mean which varies vary little.\nFurthermore, the filtering strategy is also robust to the DS\nweight, since the DS weight is used for ranking other than\nweighting. As a result, samples with extremely high DSE\nare probably always discarded even if DS weight changes.\nOrthogonality Analysis\nIn IWSI, two factors are considered in the filtering strategy,\nthe answer correctness (represented by self-consistency) and\nthe sample DSE (represented by DS weight). A natural ques-\ntion is what is the relationship between these two factors.\nAre they correlated to or independent of each other? To ex-\nplore this question, we counted the percentage of samples\nwith correct answers (using the ground truth labels) across\ndifferent DS weight intervals, as Fig 5 shows. Along ğ‘¥-\naxis are the selected intervals: [1, 1.1), [1.1, 1.3), [1.3, 1.5),\n[1.5, 2), and [2, âˆ). In each bar, the upper portion (yellow)\nindicates the ratio of correct answers, while the lower por-\ntion (blue) represents the ratio of wrong answers. For all\ndatasets, we observe a general downward trend in the ratio of\ncorrect answers, as DS weight increases. The highest ratios\nof correct answers is found either in the [1, 1.1) interval (for\ngsm8k and ANLI-A1) or in the [1.1, 1.3) interval (for Strat-\negyQA). However, both correct and wrong answers occupy a\n24262\nDensity\n1.0 2.0 3.0 1.0 2.0 3.0 1.0 2.0 3.0\ngsm8k StrategyQA ANLI-A1\nFigure 5: The first row shows relationship between answer correctness and DSE, where along ğ‘¥-axis are DSE intervals and the\nğ‘¦-axis indicates the proportion of correct answers and wrong answers. The second row are the DS weight probability density\nfunction curves with varying uncertainty threshold ğ‘¢âˆ— (Eq. 9).\nportion that can not be ignored in every interval, suggesting\na degree of independence between these two factors.\nWe delve deeper into the relationship between DSE\nand the answer uncertainty, which is first investigated by\nMoT (Li and Qiu 2023) regarding its impact on self-\nimprovement. MoT also suggested using entropy to repre-\nsent answer uncertainty. We briefly introduce the calcula-\ntion: given a certain question ğ‘, the self-generated candidate\nanswers [ğ‘1, ğ‘2, . . . , ğ‘ğ‘š], and the most consistent answer Ë†ğ‘,\nuncertainty ğ‘¢ is computed in the following steps:\nğ´âˆ— = ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ ({ğ‘ğ‘–}ğ‘š)\nğ‘(ğ‘âˆ—\nğ‘—) =\nğ‘šâˆ‘ï¸\nğ‘–\nğŸ™(ğ‘âˆ—\nğ‘— = ğ‘ğ‘–)/ğ‘š\nğ‘¢ = âˆ’\nğ´âˆ—\nâˆ‘ï¸\nğ‘âˆ—\nğ‘—\nğ‘(ğ‘âˆ—\nğ‘—) log ğ‘(ğ‘âˆ—\nğ‘—)\n(9)\nwhere ğ´âˆ— = {ğ‘âˆ—\n1, ğ‘âˆ—\n2, Â· Â· Â· }is the unique answer set. The\nhigher ğ‘¢ is, the more uncertain the answer is. In extreme\ncases, if ğ‘¢ = 0, all candidate answers are identical, and if\neach candidate answer has its unique value, ğ‘¢ will reach the\nmaximum log ğ‘š. For convenience, we normalize ğ‘¢ with a\ndivisor log ğ‘š and we denote the filter threshold as ğ‘¢âˆ—.\nWe draw the probability density function (PDF) of DS\nweight for various uncertainty thresholds ğ‘¢âˆ—. The second\nrow of Fig. 5 shows the results. For arithmetic reasoning\n(gsm8k), as ğ‘¢âˆ— increases, the peak of PDF falls and the PDF\ncurve becomes flatter, indicating a growth in the propor-\ntion of samples with high DSE. Conversely, for common-\nsense reasoning (StrategyQA) and natural language infer-\nence (ANLI-A1), the relationship between uncertainty and\nDSE appears much weaker. The PDF curves are almost iden-\ntical, with little variation at the peak, suggesting that DSE is\nnearly orthogonal to the uncertainty.\nPerception of DSE\nWe conducted a case study on gsm8k to provide an intuitive\nperception about what a correct but with high DSE sample\nlooks like. We compare the generated answers with the high-\nest and lowest DSE for the same question. We found that\ncases with the highest DSE are usually notably absurd that\nwe can easily tell them apart from human-written samples.\nWe categorize these samples into 3 types:\nâ€¢ Redundant samples.Redundant samples include irrele-\nvant or repeated information in the reasoning thoughts,\nmaking it confusing.\nâ€¢ Jumping samples.Jumping samples omit essential rea-\nsoning steps or even directly give the answer, making it\nless logically fluent.\nâ€¢ Spurious samples. The reasoning steps in a spurious\nsample (Guu et al. 2017; Jiang et al. 2023) are logically\nwrong. They get the correct answer just by coincidence.\nWe give more exact demonstrations in Appendix B.\nConclusion\nIn this paper, we investigate the impact of sample DSE on\nLLM self-improvement. We propose DS weight to approxi-\nmate the DSE inspired by importance weighting methods,\nand a novel framework IWSI where the filtering strategy\ncomprehensively considers DSE and answer correctness.\nEmpirical results demonstrate that the incorporation of DS\nweight significantly enhances the effectiveness of LLM self-\nimprovement. Further analysis reveals that DSE is nearly or-\nthogonal to other factors, suggesting a new direction to pro-\nmote LLM self-improvement for the future work.\n24263\nAcknowledgements\nThis research was supported by Theme-based Research\nScheme (T45-205/21-N) from Hong Kong RGC, and Gen-\nerative AI Research and Development Centre from InnoHK.\nThe corresponding authors are Wei Xue and Yike Guo.\nReferences\nBai, Y .; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;\nJones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon,\nC.; Chen, C.; Olsson, C.; Olah, C.; Hernandez, D.; Drain,\nD.; Ganguli, D.; Li, D.; Tran-Johnson, E.; Perez, E.; Kerr, J.;\nMueller, J.; Ladish, J.; Landau, J.; Ndousse, K.; Lukosiute,\nK.; Lovitt, L.; Sellitto, M.; Elhage, N.; Schiefer, N.; Mer-\ncado, N.; DasSarma, N.; Lasenby, R.; Larson, R.; Ringer,\nS.; Johnston, S.; Kravec, S.; Showk, S. E.; Fort, S.; Lanham,\nT.; Telleen-Lawton, T.; Conerly, T.; Henighan, T.; Hume, T.;\nBowman, S. R.; Hatfield-Dodds, Z.; Mann, B.; Amodei, D.;\nJoseph, N.; McCandlish, S.; Brown, T.; and Kaplan, J. 2022.\nConstitutional AI: Harmlessness from AI Feedback. CoRR,\nabs/2212.08073.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020a. Language Models\nare Few-Shot Learners. In NeurIPS.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020b. Language Models\nare Few-Shot Learners. In NeurIPS.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems. CoRR, abs/2110.14168.\nFang, T.; Lu, N.; Niu, G.; and Sugiyama, M. 2020. Rethink-\ning Importance Weighting for Deep Learning under Distri-\nbution Shift. In NeurIPS.\nFang, T.; Lu, N.; Niu, G.; and Sugiyama, M. 2023. Gen-\neralizing Importance Weighting to A Universal Solver for\nDistribution Shift Problems. In NeurIPS.\nGeva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and\nBerant, J. 2021. Did Aristotle Use a Laptop? A Question\nAnswering Benchmark with Implicit Reasoning Strategies.\nTrans. Assoc. Comput. Linguistics, 9: 346â€“361.\nGÂ¨ulc Â¸ehre, C Â¸ .; Paine, T. L.; Srinivasan, S.; Konyushkova, K.;\nWeerts, L.; Sharma, A.; Siddhant, A.; Ahern, A.; Wang, M.;\nGu, C.; Macherey, W.; Doucet, A.; Firat, O.; and de Freitas,\nN. 2023. Reinforced Self-Training (ReST) for Language\nModeling. CoRR, abs/2308.08998.\nGuu, K.; Pasupat, P.; Liu, E. Z.; and Liang, P. 2017. From\nLanguage to Programs: Bridging Reinforcement Learning\nand Maximum Marginal Likelihood. In ACL (1), 1051â€“\n1062. Association for Computational Linguistics.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adapta-\ntion of Large Language Models. In ICLR. OpenReview.net.\nHuang, J.; Gu, S.; Hou, L.; Wu, Y .; Wang, X.; Yu, H.; and\nHan, J. 2023a. Large Language Models Can Self-Improve.\nIn EMNLP, 1051â€“1068. Association for Computational Lin-\nguistics.\nHuang, J.; Smola, A. J.; Gretton, A.; Borgwardt, K. M.; and\nSchÂ¨olkopf, B. 2006. Correcting Sample Selection Bias by\nUnlabeled Data. In NIPS, 601â€“608. MIT Press.\nHuang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; Wang, H.;\nChen, Q.; Peng, W.; Feng, X.; Qin, B.; and Liu, T. 2023b. A\nSurvey on Hallucination in Large Language Models: Prin-\nciples, Taxonomy, Challenges, and Open Questions. CoRR,\nabs/2311.05232.\nJiang, C.; Zhu, T.; Zhou, H.; Liu, C.; Deng, T.; Hu, C.; and\nLi, J. 2023. Path Spuriousness-aware Reinforcement Learn-\ning for Multi-Hop Knowledge Graph Reasoning. In EACL,\n3173â€“3184. Association for Computational Linguistics.\nKanamori, T.; Hido, S.; and Sugiyama, M. 2009. A Least-\nsquares Approach to Direct Importance Estimation.J. Mach.\nLearn. Res., 10: 1391â€“1445.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nIn NeurIPS.\nLi, G.; Hammoud, H. A. A. K.; Itani, H.; Khizbullin, D.; and\nGhanem, B. 2023a. CAMEL: Communicative Agents for\nâ€Mindâ€ Exploration of Large Scale Language Model Soci-\nety. CoRR, abs/2303.17760.\nLi, R.; Allal, L. B.; Zi, Y .; Muennighoff, N.; Kocetkov, D.;\nMou, C.; Marone, M.; Akiki, C.; Li, J.; Chim, J.; Liu, Q.;\nZheltonozhskii, E.; Zhuo, T. Y .; Wang, T.; Dehaene, O.;\nDavaadorj, M.; Lamy-Poirier, J.; Monteiro, J.; Shliazhko,\nO.; Gontier, N.; Meade, N.; Zebaze, A.; Yee, M.; Uma-\npathi, L. K.; Zhu, J.; Lipkin, B.; Oblokulov, M.; Wang,\nZ.; V , R. M.; Stillerman, J.; Patel, S. S.; Abulkhanov, D.;\nZocca, M.; Dey, M.; Zhang, Z.; Moustafa-Fahmy, N.; Bhat-\ntacharyya, U.; Yu, W.; Singh, S.; Luccioni, S.; Villegas, P.;\nKunakov, M.; Zhdanov, F.; Romero, M.; Lee, T.; Timor, N.;\nDing, J.; Schlesinger, C.; Schoelkopf, H.; Ebert, J.; Dao, T.;\nMishra, M.; Gu, A.; Robinson, J.; Anderson, C. J.; Dolan-\nGavitt, B.; Contractor, D.; Reddy, S.; Fried, D.; Bahdanau,\nD.; Jernite, Y .; Ferrandis, C. M.; Hughes, S.; Wolf, T.; Guha,\nA.; von Werra, L.; and de Vries, H. 2023b. StarCoder: may\nthe source be with you! CoRR, abs/2305.06161.\nLi, X.; and Qiu, X. 2023. MoT: Memory-of-Thought En-\nables ChatGPT to Self-Improve. In EMNLP, 6354â€“6374.\nAssociation for Computational Linguistics.\nLi, X.; Yu, P.; Zhou, C.; Schick, T.; Levy, O.; Zettlemoyer,\nL.; Weston, J. E.; and Lewis, M. 2024. Self-Alignment with\nInstruction Backtranslation. In The Twelfth International\nConference on Learning Representations.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In ICLR (Poster). OpenReview.net.\n24264\nMadaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;\nWiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,\nY .; Gupta, S.; Majumder, B. P.; Hermann, K.; Welleck, S.;\nYazdanbakhsh, A.; and Clark, P. 2023. Self-Refine: Iterative\nRefinement with Self-Feedback. In NeurIPS.\nMihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018.\nCan a Suit of Armor Conduct Electricity? A New Dataset for\nOpen Book Question Answering. In EMNLP, 2381â€“2391.\nAssociation for Computational Linguistics.\nNie, Y .; Williams, A.; Dinan, E.; Bansal, M.; Weston, J.; and\nKiela, D. 2020. Adversarial NLI: A New Benchmark for\nNatural Language Understanding. In ACL, 4885â€“4901. As-\nsociation for Computational Linguistics.\nOpenAI. 2023. GPT-4 Technical Report. CoRR,\nabs/2303.08774.\nPatel, A.; Bhattamishra, S.; and Goyal, N. 2021. Are NLP\nModels really able to Solve Simple Math Word Problems?\nIn NAACL-HLT, 2080â€“2094. Association for Computational\nLinguistics.\nShinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K.; and\nYao, S. 2023. Reflexion: language agents with verbal rein-\nforcement learning. In NeurIPS.\nShumailov, I.; Shumaylov, Z.; Zhao, Y .; Gal, Y .; Papernot,\nN.; and Anderson, R. J. 2023a. The Curse of Recursion:\nTraining on Generated Data Makes Models Forget. CoRR,\nabs/2305.17493.\nShumailov, I.; Shumaylov, Z.; Zhao, Y .; Gal, Y .; Papernot,\nN.; and Anderson, R. J. 2023b. The Curse of Recursion:\nTraining on Generated Data Makes Models Forget. CoRR,\nabs/2305.17493.\nSugiyama, M.; and Kawanabe, M. 2012. Machine Learning\nin Non-Stationary Environments - Introduction to Covariate\nShift Adaptation. Adaptive computation and machine learn-\ning. MIT Press.\nSugiyama, M.; Krauledat, M.; and MÂ¨uller, K. 2007. Covari-\nate Shift Adaptation by Importance Weighted Cross Valida-\ntion. J. Mach. Learn. Res., 8: 985â€“1005.\nSugiyama, M.; Nakajima, S.; Kashima, H.; von B Â¨unau, P.;\nand Kawanabe, M. 2007. Direct Importance Estimation\nwith Model Selection and Its Application to Covariate Shift\nAdaptation. In NIPS, 1433â€“1440. Curran Associates, Inc.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR, abs/2302.13971.\nVernikos, G.; Brazinskas, A.; Ad Â´amek, J.; Mallinson, J.;\nSeveryn, A.; and Malmi, E. 2024. Small Language Mod-\nels Improve Giants by Rewriting Their Outputs. In EACL\n(1), 2703â€“2718. Association for Computational Linguistics.\nVillalobos, P.; Sevilla, J.; Heim, L.; Besiroglu, T.; Hobb-\nhahn, M.; and Ho, A. 2022. Will we run out of data? An\nanalysis of the limits of scaling datasets in Machine Learn-\ning. CoRR, abs/2211.04325.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023a. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. In ICLR. OpenReview.net.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023b. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nACL (1), 13484â€“13508. Association for Computational Lin-\nguistics.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain-\nof-Thought Prompting Elicits Reasoning in Large Language\nModels. In NeurIPS.\nXu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.;\nTao, C.; and Jiang, D. 2023. WizardLM: Empowering Large\nLanguage Models to Follow Complex Instructions. CoRR,\nabs/2304.12244.\nYang, K.; Swope, A. M.; Gu, A.; Chalamala, R.; Song,\nP.; Yu, S.; Godil, S.; Prenger, R. J.; and Anandkumar,\nA. 2023. LeanDojo: Theorem Proving with Retrieval-\nAugmented Language Models. In NeurIPS.\nYehudai, A.; Carmeli, B.; Mass, Y .; Arviv, O.; Mills, N.;\nToledo, A.; Shnarch, E.; and Choshen, L. 2024. Ge-\nnie: Achieving Human Parity in Content-Grounded Datasets\nGeneration. CoRR, abs/2401.14367.\n24265"
}