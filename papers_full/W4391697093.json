{
  "title": "Automatic Radiology Report Generator Using Transformer With Contrast-Based Image Enhancement",
  "url": "https://openalex.org/W4391697093",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4220490244",
      "name": "Hilya Tsaniya",
      "affiliations": [
        "Sepuluh Nopember Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2092694224",
      "name": "Chastine Fatichah",
      "affiliations": [
        "Sepuluh Nopember Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2255490229",
      "name": "Nanik Suciati",
      "affiliations": [
        "Sepuluh Nopember Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979159986",
    "https://openalex.org/W2777186991",
    "https://openalex.org/W3135057764",
    "https://openalex.org/W4296986640",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6746693533",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W2961944450",
    "https://openalex.org/W3197302148",
    "https://openalex.org/W2978540646",
    "https://openalex.org/W6788909264",
    "https://openalex.org/W6754228997",
    "https://openalex.org/W2942105358",
    "https://openalex.org/W4308714062",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W2963967185",
    "https://openalex.org/W4214539899",
    "https://openalex.org/W2508429489",
    "https://openalex.org/W3128640753",
    "https://openalex.org/W3202679357",
    "https://openalex.org/W3104371371",
    "https://openalex.org/W4309965800",
    "https://openalex.org/W2979956313",
    "https://openalex.org/W3173688449",
    "https://openalex.org/W3202260390",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W3151410070",
    "https://openalex.org/W4312605942",
    "https://openalex.org/W4285494545",
    "https://openalex.org/W4321483852",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W4289173402",
    "https://openalex.org/W4379116850",
    "https://openalex.org/W4388289870",
    "https://openalex.org/W4290996983",
    "https://openalex.org/W4290997035",
    "https://openalex.org/W3012084400",
    "https://openalex.org/W3098873896",
    "https://openalex.org/W3199942699",
    "https://openalex.org/W3116318880",
    "https://openalex.org/W3093354398",
    "https://openalex.org/W4286283129",
    "https://openalex.org/W4224031250",
    "https://openalex.org/W4200626141",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W3043290806",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4285288100",
    "https://openalex.org/W3126988965",
    "https://openalex.org/W2912250162",
    "https://openalex.org/W3097318481",
    "https://openalex.org/W3121229374",
    "https://openalex.org/W4300485340",
    "https://openalex.org/W3101156210"
  ],
  "abstract": "Writing radiology reports based on radiographic images is a time-consuming task that demands the expertise of skilled radiologists. Consequently, the integration of technology capable of automated report generation would be advantageous. Developing a coherent predictive text is the main challenge in automatic report generation. It is necessary to develop methods that can increase the relevance of features in producing predictive text. This study constructed a medical report generator model using the transformer approach and image enhancement implementation. To leverage the visual and semantic features, an approach to enhance the noise-prone nature of the medical image is explored in this study along with the transformers method to generate a radiology report based on Chest X-ray images. Four contrast-based image enhancement methods were used to investigate the effect of image enhancement techniques on the radiology report generator. The encoder-decoder model is used with text feature embedding using Bidirectional Encoder Representation from Transformer (BERT) and visual feature extraction utilizing a pre-trained model ChexNet and Multi-Head Attention (MHA) mechanism. The performance of the MHA model with gamma correction is 5&#x0025; in better with a 0.377 value using the Bilingual Assessment Understudy (BLEU) with 4 n-gram evaluation. MHA also produces 15&#x0025; better results with a 0.412 value than the baseline model. This method is able to outperform the baseline model and other previous works. It can be concluded that the use of transformer MHA encoder layer and BERT is effective in leveraging visual and text features. Additionally, the inclusion of an image enhancement approach has been found to have a positive impact on the model&#x2019;s performance.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000. \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nAutomatic radiology report generator using \ntransformer with contrast-based image \nenhancement \nHilya Tsaniya1, (Member, IEEE), Chastine Fatichah1, (Member, IEEE) and Nanik Suciati1, \n(Member, IEEE) \n1 Department of Informatics, Faculty of Intelligent Electrical and Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, 60111, Indonesia  \nCorresponding author: Fatichah Chastine (chastine@if.its.ac.id). \nThis work was funded by the Directorate General of Higher Education, Ministry of Education and Culture of the Republic of Indonesia for Masters to Doctoral \nEducation Scholarship Program for Excellent Undergraduates (PMDSU) 2021-2024 under Grant 084/E5/PG.02.00.PT/2022.  \nABSTRACT Writing radiology reports based on radiographic images is a time-consuming task that demands \nthe expertise of skilled radiologists. Consequently, the integration of technology capable of automated report \ngeneration would be advantageous. Developing a coherent predictive text is the main challenge in automatic \nreport generation. It is necessary to develop methods that can increase the relevance of features in producing \npredictive text. This study constructed a medical report generator model using the transformer approach and \nimage enhancement implementation. To leverage the visual and semantic features, an approach to enhance \nthe noise-prone nature of the medical image is explored in this study along with the transformers method to \ngenerate a radiology report based on Chest X-ray images. Four contrast-based image enhancement methods \nwere used to investigate the effect of image enhancement  techniques on the radiology report generator. The \nencoder-decoder model is used with text feature embedding using Bidirectional Encoder Representation from \nTransformer (BERT) and visual feature extraction utilizing a pre -trained model ChexNet and Multi -Head \nAttention (MHA) mechanism. The performance of the MHA model with gamma correction is 5% in better \nwith a 0.377 value using the Bilingual Assessment Understudy (BLEU) with 4 n-gram evaluation. MHA also \nproduces 15% better results with a 0.412 value than the baseline model. This method is able to outperform \nthe baseline model and other previous works. It can be concluded that the use of transformer MHA encoder \nlayer and BERT is effective in leveraging visual and text features. Additionally, the inclusion o f an image \nenhancement approach has been found to have a positive impact on the modelâ€™s performance. \nINDEX TERMS BERT Embedding, ChexNet, Image Enhancement, Medical Image Captioning, Multi-\nhead Attention  \nI. INTRODUCTION \nMedical image diagnoses are  getting more complicated as \nmedical imaging technology develops, increasing the need for \nmedical specialists. Data from the Medical Journal of \nRadiology publications in 2015 disclose a 26% increase in \nradiologists' workload compared to the previous decade. To \nget accurate and thorough diagnoses  based on medical \npictures, radiologists need to compare numerous information \nincluding a lot of factors than in the past due to medical \ndevelopments [1].  \nThis increased workload poses a significant risk of medical \nreport errors, particularly when radiologists work under \nadverse conditions or experience fatigue. These errors, in turn, \ncan compromise the accuracy of the patient's final diagnosis, \nespecially in cases when radiologists lack experience, leading \nto an increase in diagnostic inaccuracies. \nThe need to develop automated radiology generator \ntechnology that can provide interpretation and information \nfrom the medical image input would beneficially save time \nand reduce the workload of radiologists efficiently. \nRadiologists can produce reports with the aid of the \nautomated system, particularly in underdeveloped nations \nwith limited  professional resources. This system also can \nhelp radiologists by comparing  the generated report to \nmanual observation and use the information to make \ndiagnostic judgement .  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nOne of the challenge in the medical image processing is \nthe noise during image acquisition that leading to a \nreduction in image quality  [2]. In other computer vision \ncases, such as classification and segmentation, it has been \nproven that employing enhanced images significantly \nimpacts the performance of models compared to using the \noriginal images  without enhancement  [3]. Previous study \nalso proven  the effectiveness of enhanced images in \nradiology report generation using CNN-LSTM encoder -\ndecoder models  [4]. This observation motivated our \ninvestigation into the importance of image enhancement  \nand its effect in the  medical image cap tioning. Our \nexploration involved integrating the enhancement method \nwith transformer attention mechanisms, aiming to discern \nhow this combination could be utilized to elevate the \nperformance of automated systems in the context of \nmedical image interpretation.   \nThis research introduces an innovative architecture for \nan X-ray report generator  with the image enhancement in \nthe pre -processing process  and leveraging a Multi -Head \nAttention (MHA) transformer encoder layer inspired by \nVaswani [5]. Com bined by an additional pre -trained \ndensenet-121 for initializing visual features' weights and \nBERT embedding for textual features , the proposed model \nis trained on the Indiana University X -ray dataset . To \nfurther enhance the quality of X -ray images, a gamma \ncorrection is carried in the pre-processing step , as the best \nenhancement based in comparison with other methods . The \nmodel incorp orates a pre -trained ChexNet [6] encoder, \ninitially trained on the ChestX -ray14 dataset [7], that fine-\ntuned with the removal of the top classification layer to \nextract and provide initial weight to t he image features. For \nthe semantic aspect, pre-trained BERT is fine-tuned to \nextract text embedding from the medical reports. The \nextracted features are then utilize using Multi -Head \nAttention (MHA) to effectively exploit context information \nfrom both modalities. Subsequently, these features are \nforwarded in the LSTM decoder to generate diagnostic \nreports. Emphasizing integrated CNN and transformer \nembedding layers from BERT that fine -tuned with the \ndataset and  the MHA that allow the model to focus on \ndifferent parts of the input sequence simultaneously. \nComparative analysis with previous studies shown a better \nperformance from  our model, particularly evident in \nsurpassing previous benchmarks in average BLEU score \nmetrics. \nTo summarize, this paper has fo ur contributions:  \n1. To address the noise that occurred in the X -ray \nimage during the acquisition process, we conducted \na comprehensive exploration into the effects of \ncontrast-based image enhancement and its \nsubsequent influence on the report generator syst em. \nThe best result obtained from the comparison will be \nemployed in the pre -processing phase of the report \ngenerator. Different from previous works in medical \nimage captioning that mainly focus  on model \nmodifications to improve performance, we studied \nand compared the effect of several enhancement \nmethods to improve image input quality and its \neffect on the report generator model performance, \nour study underscores the significance of improving \nraw input quality over high -level complexity. This \napproach aim s to elevate feature quality by refining \nimage input quality to obtain better results on the \nmedical report generator model.  \n2. Encoding process using fine -tuned pre -trained \nmodel in both visual and semantic features  to fit the \nsmall dataset . The lower layer  of pre -trained \nChexNet is adjusted to effectively extract image \nfeatures. Simultaneously, the last layers of the pre -\ntrained BERT are used to extract the embedding \nfrom the report text. Usually, research in image \ncaptioning often concentrates on enhancing  the \nquality of text representation, our approach involves \nfine-tuning pre -trained models to extract features \nfrom both modalities â€”images and text. This \ncomprehensive approach contributes to an improved \nquality of features during the encoding process, \nfacilitating more effective context learning.  The use \nof the pre -trained model also optimizes the \ncomputation cost for the model training.   \n3. Aligning both features from different inputs is \nachieved through the use of multi -head attention \n(MHA). Instead of directly using the output of the \nencoder as the input for the LSTM decoder, we \nleverage MHA to align and match features from both \nimage and t ext. This facilitates the extraction of \ncontext information, as MHA allows the model to \nselectively focus on different parts of the input \nsequence simultaneously through multiple attention \nheads. This also helps  to reduce the vanishing \ngradient during the training process.  \nThis paper is organized in the following order:  Section 1 \nintroduces the background, motive and brief proposed \nsolution, Section 2 explains the method and model used, \nSection 3 describes and compares the obtained results, and \nSection 4 co ncludes this research.  \nII. Related Works \nImage captioning is an automated procedure aimed at \ninterpreting an image through one or more sentences in \nnatural language . Significant progress has been noted in \nimage captioning over time, moving from early template-\nbased models to more recent versions based on deep neural \nnetworks [8, 9] . Several approaches for image captioning \nhave been made from deep learning encoder -decoder based \nmodels with CNN  to extract the spatial and visual features \nand RNN to generate them in sequence [10, 11]. A spectrum \nof encoding models has been explored to enhance image \ncaptioning systems, encompassing diverse architectures such \nas Inception-v3, Visual Geometry Group Network \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n(VGGNet), Inception -v3 augmented with LSTM as a \ndecoder [12], Residual Network 152 layer (ResNet -152) \n[13], and VGG -16 [14]. Notably, employing transfer \nlearning through pre -trained encoders, commonly derived \nfrom ImageNet, has demonstrated superior outcomes [15]. \nThe strategic modification of weight embeddings has \nemerged as a pivotal factor in au gmenting the model's \nperformance, especially in the context of clinical image \nquality [16] some pre-trained such as ChexNet become the \nbenchmark of feature embedding in chest x -ray data [6].  \nThis wide range  of approaches signifies the dynamic \nlandscape within which image captioning research unfolds, \ncontinually exploring and improving methodologies to \nachieve optimal results.  The trajectory of employing deep \nlearning in the realm of medical report generation \ncommenced with the adoption of Deep Neural Network \n(DNN) based models, primarily focusing on generative tasks \nassociated with clinical interpretability of medical images \n[17], with one of the common baseline used are CNN-LSTM \n[18] that proven to be effective in generating clinical \ndiagnostic. Over time, this fundamental method has been \nimproved and refined. Notably, there has been a noteworthy \nevolution marked by the incorporation of reinf orcement \nlearning mechanisms, addressing challenges related to \ngradient propagation within generative models dealing with \ndiscrete outputs [19]. Concurrently, the utilization of latent \nvariables topics has been explored to further  enhance the \nvariety and specificity of generated reports [20]. Moreover, \nthe integration of classifiers into the generative process has \nemerged as a strategic avenue, aiming to ensure the clinical \naccuracy of the generated medical reports [21]. This \nchronological progression underscores the improvement of \nresearch in medical report generation, continually \nincorporating advanced technology to refine and optimize \nthe methodologies.  \nThe advent of the Transformer architecture in 2017 [5] \nmarked a transformative shift in computer vision and natural \nlanguage processing (NLP)  [22], setting a precedent for \nsurpassing state -of-the-art benchmarks in domain -specific \nchallenges. In the field of medica l image captioning , this \ntrend has taken off a bit later. One of the early research, such \nas the one undertaken in 2019 [23], focused on identifying \nthe regions of interest within medical images. In particular, \nthe prevailing approaches in medical image captioning \nleverage pre-trained models based on Convolutional Neural \nNetworks (CNN), with improvements to address several \nproblems such as data bias [24], feature alignment [25], and \ndescription coherence [26]. Specifically, in the chest X -ray, \nattention is also implemented to leverage semantic \ninformation [27, 28]  and visual information [29]. These \napproaches, have obtained competitive results in generating \ncomprehensive medical reports. Other researches, also put \nemphasizes in t he importance of feature information in \nmedical image data [30], with recent strides have been made \nthrough innovative techniques like meta -learning combined \nwith attention mechanisms to enhance feature interactions \nfor medical time -series classification [31]. Furthermore, a \nmemory-driven transformer approach has been deployed to \nensure the generation of extensive reports without omitting \ncrucial information [32]. The implementation of attention \nmechanisms has shown remarkable improvements in \nretrieving global information from medical images, a pivotal \naspect in facilitating clinical analysis  [33] [34]. To further \ndevelop the system, multi -level alignment approaches have \nbeen used in conjunctio n with self -attention processes, t his \nhas improved the performance of information extraction by \nefficiently bridging the gap between picture and text data  \n[35].  \nMedical images often face challenges related to noise \nintroduced during the acquisiti on process, with additional \nquality degradation when images are converted from \nDICOM format to jpg or png. Enhancing medical images \nbecomes crucial to not only mitigate noise but also improve \noverall image quality and spatial information. The objective \nof image enhancement is to achieve subjectively superior \ninput quality compared to the original image. Several \napproaches have been employed to enhance medical image \nquality, particularly in chest X -ray data. A histogram \nequalization-based model has shown goo d result in \nenhancing chest X -ray data, providing better visual results \nfor interpretability and noise reduction  [36, 37, 38] . The \nimplementation of enhancement method for specific cases \nwas also done such as  to boost deep learning model for \nclassification tasks in Covid -19 data using histogram \nequalization-based method  [39, 40, 41] , Siracusano et al. \nconducted a detailed exploration, utilizing advanced contrast \nenhancement and CLAHE to enhance Covid-19 images [42], \nwhile Kanjanasurat et al. incorporated gamma correction \nalongside histogram equalization to improve Covid -19 \nclassification model performance  [43]. In the context of \npneumonia classification, a comparison of histogram \nequalization-based techniques was carried out by Abin et al \nto enhance chest X -ray image and improve model \nperformance [44], the implementation of EFF was also \ncarried out for the same cases by Setiawan  [45]. Regarding \nthe medical report generator, in a similar dataset a \ncomparison of several enhancement methods also been \nconducted to obtain better model performance than using \noriginal image [4]. \nInspired by these advancements, our study introduces a \nmodel that employs a pre -trained model and incorporates a \ntransformer to synchronize features from both modalities, \nfostering a comprehensive contextual understanding. In \ncontrast to previous researches that primarily concentrate on \nmodifying models to enhance the quality of generated text, \nour approach diverges by integrating image enhancement in \nthe preprocessing stage, influenced from previous research \n[4], to improve model performance.  \nIII. Method \nA. OVERVIEW \nIn the Fig. 1 the proposed model schema is illustrated, the \ninput data will be through five process which contribution in \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nthe Preprocessing process specifically the image enhancement \nprocess and the Context Building process. The detailed \nproposed method is illustrated in Fig. 2. Whereas, the data will \nbe through pre-processing process in which the X-ray image \nwill be enhanced using best contrast-based methods while text \nreport are processed with basic preprocessing steps (cleaning \ncharacter, deconstructed, lowering case). Then both data will \nbe encoded, for the X -ray image using a pre-trained \nDensent121 ChexNet utilized as a feature extractor with an \noutput of image feature vector, and as for the report text BERT \nis used to generate embedding before the decoding process. \nBoth features then will be synchronized to get  the context \ninformation. To amplify the model's ability to recognize subtle \nfeatures in images, we use Multi-Head Attention (MHA) as an \naddition to build ing the context that aligns between image \nfeatures and text features combined with LSTM as a decoder. \nFig. 1 depicts the research methodology's schema, and the \nmodel architecture used in this research can be seen in Fig. 2. \nB. DATASET \nThe dataset used in this study is open data from the Indiana \nUniversity Hospital Link collection. The collected data \nconsists of 9199 chest X-ray images and 3973 XML format of \nmedical reports written in english by radiologists [46]. The \nmedical reports in the dataset include comparison and \nindication data, which are indication and comparative data \nbased on the patient's symptoms and medical background. The \ndiagnosis of the patient is  represented by the impression, \nwhereas the finding is a descriptive examination of the \nradiological image.   \nSeveral images may be linked to one medical report or no \nreport at all. Basic pre-processing will be applied to the image \nand text, then sampling will be done to the data frame before \nbeing split into train, validation, and test data. \nC. IMAGE ENHANCEMENT METHOD \nImage enhancement is an important image -processing \ntechnique which highlights key information in an image and \nreduces or removes certain secondary information to improve \nthe identification quality in the process. It aims to make the \nobjective images more suitable for a specific application than \nthe original image s. Based on previous research, \nimplementing an enhancement to the radiographic image has \na positive impact on feature extraction to clinical diagnoses \n[47], in another study on similar data radiograph x -ray, \ncomparison and analysis on the enhancement implementation \nalso showed good impact in segmentation and classification  \n[3]. In the medical report generator, our initial research with a \nconvolutional encoder and recurrent decoder has proven to \nhave a good impact on the generated report model \n \nFIGURE 1. Our Medical Image Captioning method scheme  \n \n \nFIGURE 2. Proposed radiology report generator architecture \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nperformance [4]. In this study, we continued the enhancement \nprocessing with an additional transformer as an improvement \nto build a context vector. \nFour different enhancement techniques are employed in this \nresearch, namely HE, CLAHE, EFF, and Gamma Correction. \nFirst method is Histogram Equalization which is a technique \ndesigned to enhance image quality by adjusting the contrast \nand brightness of low -contrast and dark images [48]. This \nmethod achieves improvement by redistributing the image's \ngrey levels, leading to enhanced image clarity and a more \nuniformly stretched histogram. The discrete function \nformulation of the image histogram is represented as â„(ğ‘Ÿğ’Œ) =\nğ‘›ğ’Œ, where â„ is the histogram function, ğ‘Ÿ is the intensity value \nin image pixel ğ‘˜, and ğ‘› is the number of pixels with intensity \nvalue ğ‘Ÿ in the image. Normalization of the histogram value is \naccomplished using the total number of pixels in the image. \nThe resulting even distribution of pixel values in the final \nimage enhances the clarity of the grayscale image. The second \nmethod is Contrast Limited Adaptive Histogram Equalization \n(CLAHE) which is an enhanced version of HE that improves \ncontrast in specific image sections. It differs from traditional \nhistogram equalization by enhancing local contrast and edges \nbased on the local distribution of pixel intensities. This method \noperates in the HSV color space, focusing solely on the value \ncomponent, and utilizes a threshold parameter to control \ncontrast enhancement within selected zones. The image \nundergoes HSV processing to create a color rendering more \nclosely aligned with human perception, with the final result \nconverted back to the RGB color space [4]. The third method \nis the Exposure Fusion Framework (EFF) which is an \nadvanced method for enhancing image contrast through \nexposure ratio adjustments, offering superior contrast \nimprovement compared to other techniques.  The algorithm, \nintelligently fuses pixel regions with varying exposure levels, \nconsidering pixel values, weight maps, and color channels to \nproduce a well-exposed result. Brightness Transform Function \n(BTF) is used to manage exposure differences, ensuring  a \nharmonious blend of exposures. The final enhancement \nprocess combines weighted pixel values, BTF application, and \nexposure ratios to achieve a visually appealing and \ncomputationally efficient contrast enhancement [4]. The last \nmethod is Gamma correction, a non -linear operation crucial \nfor adjusting exposure or tristimulus in images and videos \n[49], which involves altering pixel values based on the gamma \nconstant (Î³). The gamma function expressed as ğ‘”(ğ‘¥) = ğ‘¥ğ›¾, \ntransforms the pixel value ğ‘¥, yielding a new value in the \nimage. Applying gamma correction within a pixel range of 0-\n255 to adjust the image  gamma value. Gamma values > 1 \nlighten the image, while ğ›¾ = 1 shifts the image towards the \ndarker spectrum. \nD. IMAGE ENCODER \nA 121 -layer dense convolutional network (DenseNet) \ncalled ChexNet was trained using data from 14 chest X -ray \ndatasets. Existing networks can be optimized with DenseNet, \nwhich improves gradients and information throughout the \nnetwork [48]. On the pre -trained ChexNet, the DenseNet \nmodel has been trained on the data that consists of more than \n100,000 X-ray images [6].  \nIn this study, image features are extracted from the data as \nconvolutional features using ChexNet as an encoder. The pre-\ntrained ChexNet will transmit the weight to the image through \nthe unfroze last layer of the model. To customize the output \ninto 1024 as a dimensional image  initial features value, we \nremove the top layer on ChexNet. In this study, the parameters \nused were a batch size of 100, a dropout of 0.2, and learning \nrate of 10-2 and sigmoid activation. Then, the pooling of initial \nweight initiation will be forward with ReLU activation and \noutput vector size 512. This dimension will be adjusted with \nBERT embedding to get the context information along with \nthe text features in the multi-head attention. \nE. MULTI-HEAD ATTENTION \nThe initial weight from the encoder will forwarded as an \ninput to multi-head attention as a key and value to align the \ninformation with the semantic features as a query in a parallel \nprocess. Attention technically is mapping a query to the keys \nfrom data with a value as an output. It consists of three \ncomponents: query, key, and value. Multi -head attention is \nmultiple single attention that works repeatedly and \nsimultaneously.  \nThe multi-head attention function can be represented as (1). \nğ‘šğ‘¢ğ‘™ğ‘¡ğ‘–â„ğ‘’ğ‘ğ‘‘(ğ‘˜, ğ‘, ğ‘£) = ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â„ğ‘’ğ‘ğ‘‘1..ğ‘›)ğ‘Š (1) \nWhere ğ‘˜, ğ‘, and ğ‘£ are the three components, query from the \nreport embedding, key, and value  from the image encoder . \nEach head is a single attention process that can be defined as \n(2), (3), (4). \nattention(ğ‘˜, ğ‘, ğ‘£) = âˆ‘  ğ‘– ğ›¼ğª,ğ¤ğ‘– ğ¯ğ¤ğ‘–     (2) \nğ›¼ğª,ğ¤ğ‘– = softmax(ğ‘’ğª,ğ¤ğ‘– )    (3) \nğ‘’ğª,ğ¤ğ‘– = ğ‘ â‹… ğ‘˜ğ’Š    (4) \nWhere ğ›¼ in function (2 ) is the output of the soft max \nactivation from function (3). Meanwhile, ğ‘’ is the attention \nweight that has been aligned from both features as a new value \n(4). \nAttention is defined as mapping a query with key -value \npairs to get a new value. Where query, keys, and values are all \nvectors. The weights assigned to each value are determined by \nthe compatibility function between the query and keys. The \nfinal output is computed as the sum of the weighted values. \nThis mechanism is notably efficient for aligning features from \nboth images and text to establish co ntext. The resulting \nattention weight serves as the context vector, which is \nsubsequently fed into the decoder as input. \nF. BERT EMBEDDING \nThe semantic features are extracted from pre-trained BERT \nembedding. BERT is a neural network -based training \ntechnique released by Google known as Bidirectional Encoder \nRepresentations from Transformers (BERT)  [49]. This \nmethod is an open source after Google released it publicly in \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n2018. BERT can train a language model based on an entire set \nof words in a sentence or query in a two-way manner. BERT \nallows language models to study the context of words based \non the words around them, not just the words that precede or \nfollow them. \nBERT uses an attention mechanism in the data encoding \nprocess to study the contextual relation between words in the \nmedical report. The encoder will read the whole words, so it \ncan learn the context of the word. An example of input \nrepresentation in BERT can be seen in Fig. 3. \nFor the token in sequences, [CLS] is used as the first token \nin every input, and the [SEP] token is used to separate between \nsentences in the input. From the input, the token will be \nembedded based on the vocabulary id. Then, to distinguish \nbetween sentences, it will be embedded by the segment, and \nthe position embedding to indicate the precise position of the \ntoken. The input embedding is the sum of the three embedding \nprocesses. This embedding will be used as an input for context \nbuilding in multi-head attention with fixed dimensions. \nIn this research, we employed the \"bert -based-uncased\" \nmodel from Huggingface for BERT. Subsequently, we \nutilized BERT's tokenization to tokenize medical report data, \nand the BERT embeddings were loaded as initial weights for \nall words in the medical report data. The dimensions of BERT \nwere fine-tuned specifically for word embedding. \nG. DECODER \nLong Short-Term Memory (LSTM) is utilized as a decoder. \nThree gates make up the LSTM: an input gate, a forget gate, \nand an output gate. The gates perform several tasks related to \ndata collection, classification, and processing. An input \nmodulator is used by an LSTM cell to modify the input to the \nmemory as it continuously learns the weights from the input \ngate. This memory cell is also used to learn the weights for the \noutput gate. In each process step, the LSTM stores and deletes \nsome data, which is subsequently utilized in the process step. \nIn this study, the visual data from multi -head attention is \nutilized as input and preserved in the input mod ulator. \nAdditionally, the memory cell stores text embeddings through \nan embedding matrix, employing BERT embedding in text \nprediction to improve the selection of the next word. The \nhidden state's output is then employed in multi-head attention \nas a query to align with the visual features and get the context \ninformation from both modalities. \nH. EVALUATION \nBilingual Evaluation Understudy (BLEU) will be used to \nassess how well the model described the image. Using actual \ndata as a reference, the BLEU method approach counts word \noccurrences in the model-generated text [50]. \nThe method of determining the match between the words \nproduced by the model and the actual data is known as the n-\ngram calculation. To compare the predicted text quality value \nto the reference data, the value of the n -gram words in the \nsentence will be calculated. The equation to calculate t he \nBLEU score can be seen in (5). \nğµğ¿ğ¸ğ‘ˆ = BP. exp âˆ‘ ğ‘¤ğ‘› log ğ‘ğ‘›\nğ‘\nğ‘›=1     (5) \nWhere: \nğ‘ğ‘›: the predictive text precision per ğ‘› word, \nğ‘¤ğ‘›: the predictive text weight on the ğ‘› word, \nğ‘: gram value used, \nBP: penalty value of error prediction \n \nWhereas BP is a brevity penalty for predicted text errors and \nstands as a crucial factor, meticulously designed to evaluate \nand penalize error s in predicted text length. The precision \nvalue ğ‘ of the predicted text, the weight assigned to words ğ‘¤, \nand the chosen gram value ğ‘ collectively contribute to the \nnuanced calculation of BP. In the context of evaluating \ncandidate sentences against reference sentences, a standard \npractice involves employing a gram value of 4 for candidate \nsentences. This means that each of the prediction's four words \nis examined to see if it is similar to the reference sentence. The \ngram value, denoted by the letter ğ‘, plays a pivotal role in \ndetermining the extent of the comparison, thus adding a layer \nof complexity to the brevity penalty computation. In essence, \nthe n -gram value becomes a pivotal parameter in this \nevaluation, guiding the matching process and shaping the \nprecision assessment of predicted text with reference text. This \ndetailed understanding of the interplay between gram values, \nbrevity penalty, and precision offers a comprehensive view of \nthe intricacies involved in assessing the quality and \nconciseness of predicted text in natural language processing \ntasks. \nIV. RESULT AND DISCUSSION \nA. DATASET \nFrom the Indiana University dataset exploration in Open-I, \nthere was a disparity between the normal and abnormal \ndiagnoses of the X-ray images.  Because this can lead to model \nbias to normal diagnoses , several process es to reduce \nimbalance are done. In the normal diagnoses, the duplication \ndata for a similar diagnosis is reduced, and then the minor \ndiagnoses in normal and abnormal sampled to reduce the \nimbalances. The diagnoses as medical reports are subjected to \npre-processing before being embedded. \nThe pre-processing of the medical report includes word \ndeconstruction, character and number deletion, and letter \nconversion to lowercase form. Then, it is trained with BERT \nto get the embedding of the text and feature extraction from \n \nFIGURE 3. BERT Input Representation \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \ntext data. The medical report  is also filtered based on the \noccurrence in the data. The data split based on occurrence  \npercentage into training, validation, and test data. \nThe data are partitioned based on the number of reported \noccurrences, with the highest frequency group being under-\nsampled and the lowest frequency group being oversampled. \nThe data is then separated into train, validation, and test data \nbased on the percentage of each minor and major data. Table \n1 shows the quantity of data used for the training, validation, \nand test. \nTABLE I \nDATA SPLIT FOR MODELLING \nData Data Quantity \nTrain 3605 \nValidation 901 \nTest 566 \nB. IMAGE ENHANCEMENT \nRadiographic images tend to have noise during the \nacquisition process. The common uneven contrast values can \ncause damage to the extracted information from the image . \nThis could affect model learning in generating captions. In \naddressing this problem,  approaches to enhancing image \nquality and analyzing the effect of  improving the quality of \nlearning models in generating captions are carried out. Several \nenhancement methods were compared based on the effect of \nthe enhancement on the model improvement with the baseline \nmodel as an individual enhancement. The best result obtained \nwill be used as a preprocessing method before visual feature \nextraction. \nIn investigating this, X -ray images were enhanced using \nseveral contrast methods, namely HE, CLAHE, EFF, and \nGamma Correction. Each enhanced image will be used with \nbaseline model CNN as a feature extractor and LSTM as a \ndecoder. The effect in the generating learning model  is \nevaluated by the predicted diagnoses similarity with the \nground truth using BLEU.  \nHE method is used to distribute pixel value to enhance \nimage contrast. Calculating the histogram value in the range \n0-255, then creating  the cumulative distribution and re -\nassigning the pixel value to become a  linear function. For \nCLAHE, a default function from OpenCV with clip limit value \n2 to avoid amplification of extreme pixel value is used. The \nYing method, Exposure Fusion Framework (EFF), works by \ncombining multiple images of th e same scene taken with \ndifferent exposures to create a single image that retains the \nbest details and luminance from each input image. These \nframeworks analyze the pixel values in each image and choose \nthe most appropriate values for the final output. They prioritize \nwell-exposed regions and preserve details in both dark and \nbright areas, resulting in an image with a wider range of tones \nand details than a single exposure could capture. In this study, \nwe set the threshold for minimum exposure illumination with \nthe same value as the lambda which is 0.5, alpha -0.3, which \nis also the value used to calculate the max entropy of the image \nfeatures, and beta value 1 to get  the weight matrix of the \nimage. The beta and gamma values are also used to calculate \nthe exposure ratio between under-exposure images and over-\nexposure images based on the minimum illumination  \nthreshold. For the gamma method, adaptive gamma with a \nthreshold value of 0.3 was utilized to determine whether t he \nimage was too bright or too dimmed. The gamma value of 0.7 \nwas used to brighten the dimmed image and 1.5 was used for \nthe opposite. \nThe difference between the original image and the enhanced \nimage for each method can be seen in Fig. 4. The baseline \nmodel used in the comparison is ChexNet as a feature \nextractor and LSTM as a report generator, with the best result \nused as an enhancement method in the proposed model. The \nresult of the comparison can be seen in Table 2. \nTABLE 2 \nRESULT COMPARISON OF DIFFERENT ENHANCEMENT METHODS WITH THE \nBASELINE MODEL \nModel BLEU Evaluation \nBLEU-1 BLEU-2 BLEU-3 BLEU-4 \nChexNet-LSTM 0.25567 0.25576 0.29077 0.34157 \nChexNet-LSTM \n+ HE \n0.22783 0.28682 0.29190 0.33237 \nChexNet-LSTM \n+ CLAHE \n0.25893 0.29327 0.32329 0.33782 \nChexNet-LSTM \n+ EFF \n0.25892 0.30681 0.32879 0.34482 \nChexNet-LSTM \n+ Gamma \nCorrection \n0.30147 0.30698 0.32969 0.37792 \nFrom the results of the comparison using the baseline Table \n2 shows that the gamma correction gives the best result from \nthe BLEU evaluation number. In the radiographic image of the \nchest x -ray, the data was enhanced using the gamma \ncorrection method according to (2). The gamma value \nbetween 0.7 and 1.5 used the enhancement process to balance \nthe contrast in the image. \nC. IMPLEMENTATION DETAILS \nFrom the comparison result, gamma correctio n for the \nenhancement technique employs a threshold value of 0.3, \nstrategically determining whether an image leans towards \nexcessive brightness or undue dimness. The gamma \ncorrection process is guided by a nuanced choice of gamma \nvalues. In instances where  an image exhibits excessive \ndimness, a gamma value of 0.7 is applied to brighten and \nrestore visual clarity. Conversely, for images veering \ntowards excessive brightness, a gamma value of 1.5 is \nadeptly employed to achieve a harmonious balance and \nenhance overall quality. This methodical use of gamma \ncorrection, coupled with a discerning thresholding strategy, \ncontributes to a refined and adaptive enhancement pr ocess, \nensuring that each image receives a tailored treatment based \non its unique luminosity characteristics. \nFor the visual extraction process, the images that have been \nenhanced with Gamma Correction were resized to 224x224 \nfollowing the size of the pre-trained ChexNet model for initial \nweight initiation. The parameters were batch size of 10 0, \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \ndropout of 0.2, learning rate of 10-2 and sigmoid activation. \nThen, the pooling of initial weight initiation with ReLU \nactivation was done, and the output vector size was 512. \nThe vector generated by the weight initiation using \nChexNet becomes the input to the MHA as a key and value in \nparallel with the head attention value of 8, with an output \ntensor array size of 512x512 and dropout of 0.2. In the \ntransformer layer, the input image is positioned encoding with \nthe output in the form of a vector, which will be used as input \nin the form of a key and value for the query from the decoder \non the attention module. Attention weight is calculated using \nsoftmax activation on the module. \nExtracted diagnoses from radiology reports will be \nembedded using BERT. There are 3 combinations of \nembedding used as a final representation starting from token \nembedding for each specific word and with [CLS] token as a \ntag to indicate the beginning of the diagnosis, and [END] to \nindicate the end of the diagnosis, then segment embedding to \nprovide word position information in a sentence, and position \nembedding for detailed information of word position in the \ntext. \nThe model performance between the baseline and \ntransformer models was compared using BLEU as a natural \nlanguage generation evaluation. These comparison results can \nbe seen in Table 3.  \nTABLE 3 \nRESULT COMPARISON WITH DIFFERENT MODEL \nModel BLEU Evaluation \nBLEU-1 BLEU-2 BLEU-3 BLEU-4 \nCNN-LSTM 0.25567 0.25576 0.29077 0.34157 \nMulti-head \nAttention \n0.30681 0.30214 0.33182 0.39264 \nCNN-LSTM + \ngamma \nenhancement \n0.30147 0.30698 0.32969 0.37792 \nMulti-head \nAttention + \ngamma \nenhancement \n0.36343 0.37199 0.38846 0.41259 \n \nBased on the BLEU evaluation with n-gram values 1-4, the \nTransformer model is able to outperform the baseline model \nwith an increase of 15% from using the transformer MHA \nmechanism of the BLEU-4 score compared to the ChexNet-\nLSTM model. Furthermore, adding gamma enhancement \n \nFIGURE 4. Histogram comparison for original X-ray image and the image with different enhancement \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nboosts the BLEU-4 score by 11% for ChexNet-LSTM and 5% \nfor MHA models. The better result was obtained by the MHA \nmodel with gamma correction with an increase of 9% of the \nBLEU-4 score compared to ChexNet -LSTM with gamma \ncorrection. \nBased on the test data, the predicted text with a high BLEU \nscore effectively predicted that it would be nearly identical to \nthe average description on th e ground truth. Meanwhile, \ndescriptions of images can be found in data with low BLEU \nscores that do not match the reference word from ground truth. \nFig. 5 displays a few instances of predicted outcomes based on \nthe BLEU score. From the prediction results, in the prediction \ntext with a high BLEU value, the model successfully predicts \naccording to the ground truth per word with the same \ndefinition, especially data with a ground truth of more than one \nsentence. \nThe assumption on the influence of the low BLEU score is \ndue to the fact that ground truth descriptions on the dataset are \nwritten. Even though they have the same meaning, the words \nused differ, as in the descriptions of 'no acute', 'no evidence, \nand 'no abnormality' have the same meaning which is a healthy \nnormal chest condition based on x -ray results. Because the \nBLEU method calculates based on word similarity, the \nmodel's BLEU score is low as a result of the different word \nselections. \nIn the encoder -decoder model with MHA and gamma \nenhancement, the model performance is also seen based on the \nloss and accuracy values in the training process. Fig. 6 shows \nthat the loss and accuracy values in the validation data are not \ntoo far from the training data during the 10 -epoch training \niteration. This indicates that during the training period, the \nmodel has good performance, and there is no overfitting of the \ndata.  \nA comprehensive comparison of our approaches using \nenhanced image data was conducted in terms of computational \ncost. The first approach employs a Convolutional Neural \nNetwork (CNN) to extract image features, followed by a Long \nShort-Term Memory (LSTM) network to generate captions. \nWith the same batch size, the CNN-LSTM model took training \ntime for 10 epochs 4159s, and the inference time for testing \ndata is  436ms. The computational cost of this approach \nprimarily depends on the model's size, batch size, sequence \nlength, and the depth of the CNN -LSTM architecture. The \nsecond approach utilizes an encoder-decoder architecture with \nan attention mechanism applied to the image features. While \nthis attention mechanism enhances caption quality, it \nintroduces additional computational complexity. This model \napproach took training time in the same epoch 4284s and \ninference time for testing data 2340ms . The computational \ncost can be influenced by factors such as the choice of \nattention mechanism and the granularity of attention  as \nattention architecture has a more complex computational and \nlonger sequence in inference time. \nIt was observed that the CNN -LSTM encoder approach \ntends to have a lower computational cost, making it more \nsuitable for scenarios with limited computational resources. \nHowever, the encoder with attenti on, despite its higher \ncomputational demands, exhibits superior captioning quality \nand is preferred when high -quality captions are paramount, \nand ample computational resources are available. \nThe model undergoes a detailed comparative analysis with \ndiverse architectures from previous studies on the same \n \nFIGURE 5. Histogram comparison for original X-ray image and the image with different enhancement \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \ndataset. This assessment employs the BLEU evaluation metric \nacross various n-gram dimensions (1, 2, 3, and 4), as outlined \nin Table 4. The results offer insights into the model's \nproficiency in generating coherent medical image reports, \nserving as a benchmark against prior approaches.  \nTABLE 4 \nCOMPARISON WITH PREVIOUS RESEARCH USING BLEU \nModel BLEU Evaluation \nBLEU-1 BLEU-2 BLEU-3 BLEU-4 \nLRCN [18] 0.369 0.229 0.149 0.099 \nAtt-RK [27] 0.369 0.226 0.151 0.108 \nCDGPT2 [28] 0.387 0.245 0.166 0.111 \nVisualGPT [29] 0.388 0.333 0.231 0.226 \nProposed model \nwithout gamma \n0.307 0.302 0.332 0.393 \nProposed model 0.363 0.371 0.388 0.412 \nAs per the comparison results, the proposed model using \npre-trained ChexNet MHA and gamma correction \noutperformed other methods in the BLEU evaluation. The  \nincrease in image quality that is also conducted could be the \nreason for the result. \nThere are different mechanisms used in the previous model, \nfor the LRCN [18], the authors highlighted using the recurrent \nmodel as a way to d ecode the image interpretation for long-\nterm dependencies learning. Att-RK [27] combined top-down \nand bottom-up computation using the attention mechanism in \nfocus to leverage the semantic information by selectively \nattending to the features and using it in the recurrent hidden \nstate as the decoder. CDGPT2 [28] also tries to leverage the \nsemantic information using a self -attention mechanism and \ncombine the weighted visual embedding with the report \nembedding. VisualGPT [29] uses a masked attention \nmechanism with the pre -trained language model weight \ncombined before calculating the attention weight to balance \nthe visual information processed in the model. From the \nprevious works compared in this article, only CDGPT2 was \noriginally implemented on similar data from the author. The \nmodel proposed in this paper was also inspired by the LRCN \nto use the recurrent model as a decoder for long -term \ndependencies and use multi -head att ention instead of self -\nattention or masked attention to align the visual information \nand semantic information in parallel. \nIn comparison with LRCN  [18], Att-RK [27], CDGPT2 \n[28], and VisualGPT [29] our model both with and without the \naddition of image enhancement got averagely better results in \npredicting x-ray image interpretation. We investigate the test \nresult in BLEU-1 evaluation and BLEU-4, our model is able \nto outperform others from BLEU evaluation n-gram 2, 3, 4, \nwhile still behind VisualGPT [29] on BLEU n-gram 1 with a \ndifference of 0.02. We further investigate it by comparing the \npredicted diagnosis of VisualGPT in BLEU evaluation n -\ngrams 1 and 4. In the context of BLEU -1 evaluation, \nVisualGPT demonstrates superior proficiency in predicting \nindividual words from the Ground Truth compared to our \nmodel, particularly in succinct diagnoses. However, it is \nsusceptible to inaccuracies in predicting words for longer \ndiagnoses, sometimes leading to predictions longer than the \nground truth. This misalignment can result in penalties during \nBLEU evaluation.  On the other side, our model c an predict \naccording to ground truth better in longer diagnosis with \nBLEU-4. A sample of comparison can be seen in Fig. 7. \nD. LIMITATION AND FUTURE WORKS \nIn this study, while implementing our proposed model for a \nmedical image report generator, several challenges emerged. \nThe dataset used has a diverse report based on similar \nconditions, which caused difficulty in obtaining convergence \nin the model training process. These challenges underscore the \ncomplexity of predicting medical reports in order to get high \nsimilarity in BLEU score. \nThere are also several limitations that should be \nacknowledged. First, building a good report generator model \nis really dependent on the diversity of data. Even though we \ntried to curate and filter data to be as effective as possible, the \ndiversity in the dataset itself for a few abnormal conditions \nremains limited. Secondly, as our report generation model is \nbased on visual features, it may not capture subtle details \npresent in the images that an experienced radiologist  can \ninterpret, although enhancing the image has proven to help  \nincrease the similarity of predicted reports with ground truth.  \nFurthermore, there are several promising avenues for future \nwork, such as in the dataset enrichment for rare abnormalities \nto increase diversification. In clinical relevance, such as image \n \nFIGURE 6. Proposed model performance in loss and accuracy \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nenhancement and abnormality labelling enhancement also \nhave the potential to improve model report generators. There \nis also a potential to develop an evaluation method that is \ncomparable to radiologist clinical analysis. \nV. CONCLUSION \nIn this research, we introduce an automatic radiology \ndiagnosis generator leveraging the transformative capabilities \nof a transformer architecture. Our primary focus is to unravel \nthe intricate dynamics between image enhancement processes, \nmodel architecture, and evaluation metrics within the realm of \nmedical image captioning. The proposed model incorporates a \nMulti-Head Attention (MHA) mechanism and employs BERT \nembedding for extracting intricate text features. \nTo enhance the model's overall efficacy, we conducted an \nexploration of various enhancement processes and their \n \n(a) \n \n(b) \nFIGURE 7. Comparison VisualGPT and our proposed in BLEU-1 (a) and BLEU-4 (b) evaluation. Red color texts are the word that misspredicted by \nthe model, and the blue texts are the words that unable to be predicted by the model \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nimpact on radiograph images. Among the four implemented \nenhancement methods, all showcased  performance \nimprovements compared to the original images, with the \nGamma Correction method emerging as the most effective. \nThis nuanced understanding of image enhancement \ntechniques and their implications on improving the model \nperformance. \nRadiographic images often suffer from contrast noise, a \nchallenge that significantly influences the evaluation \noutcomes of generated captions. Our findings highlight the \npivotal role of enhancing image contrast, enabling the model \nto extract richer and more contextually relevant features. This \nnuanced insight aligns with contemporary discussions on the \nimportance of preprocessing steps in optimizing model \nperformance for medical image analysis. \nFollowing the encouraging results from the contrast method \ntests, we integrat ed Gamma Correction as a pre -processing \nstage for our proposed transformer model. This architecture, \naugmented with the MHA mechanism and BERT embedding, \nrepresents a cutting -edge approach to medical image \ncaptioning. Comparative experiments were conducted , both \nwith and without gamma correction, revealing a superior \nperformance when gamma correction was applied as a pre -\nprocessing step. The best result was with the addition of pre-\nprocessing gamma correction, which produced text prediction \nwith 9% better coherence in the BLEU evaluation result than \nthe conventional CNN-LSTM method. Similarly, the BLEU \nevaluation of the multi-head attention approach was also 15% \nbetter than the conventional CNN -LSTM method in \ncomparing models without gamma correction. This \nunderscores the significance of tailoring pre -processing \ntechniques to the specific characteristics of medical images, a \npractice that is gaining prominence in recent research \nendeavours. \nThe proposed model not only outperforms previous works \non the same da taset but also demonstrates superior BLEU \nevaluation results for n -grams 2, 3, and 4. Furthermore, it \nexcels in predicting more accurate diagnoses, providing longer \nground truth information. This comprehensive investigation \nadvances our understanding of the intricate interplay between \nimage enhancement, model architecture, and evaluation \nmetrics in the context of medical image captioning. Our \nproposed model is able to outperform other previous works \nwith the same dataset and BLEU evaluation n-gram 2, 3, 4. It \nalso predicts better diagnosis with longer ground truth. \nACKNOWLEDGMENT \nAuthor contribution. All authors contributed equally to the \nmain contributor to this paper. All authors read and approved \nthe final paper.  \nFunding statement. This research was funded by the Ministry \nof Education and Culture of the Republic of Indonesia with a \nPendidikan Magister menuju Doktor untuk Sarjana Unggul \n(PMDSU) scholarship program under the Penelitian Disertasi \nDoktor (PDD) scheme. \nConflict of interest.  The authors declare  no conflict of \ninterest. \nAdditional information.  No additional information is \navailable for this paper. \nREFERENCES \n \n[1]  L. I. T. Lee, S. Kanthasamy, R. S. Ayyalaraju and R. \nGanatra, \"The Current State of Artificial Intelligence \nin Medical Imaging and Nuclear Medicine,\" British \nInstitute of Radiology Journal, vol. 1, no. 1, 2019.  \n[2]  J. Ker, L. Wang, J. Rao and T. Lim, \"Deep Learning \nApplications in Medical Image Analysis,\" IEEE \nSpecial Section On Soft Computing Techniques For \nImage Analysis In The Medical Industry Current \nTrends, Challenges And Solution, pp. 9375 -9389, \n2018.  \n[3]  T. Rahman, A. Khandakar, Y. Qiblawey, A. Tahir, S. \nKiranyaz, S. B. A. Kashem, M. T. Islam, S. A. \nMaadeed, S. M. Zughaier, M. S. Khan and M. E. \nChowdhury, \"Exploring the effect of image \nenhancement techniques on COVID -19,\" Computers \nin Biology and Medicine, vol. 132, 2021.  \n[4]  H. Tsaniya, C. Fatichah and N. Suciati, \"Exposure \nFusion Framework in Deep Learning -Based \nRadiology Report Generator,\" IPTEK Journal of \nScience and Technology, vol. 33, 2022.  \n[5]  A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. \nJones, A. N. Gomez, L. Kaiser and L. Polosukhin, \n\"Attention Is All You Need,\" Neural Information \nProcessing System, 2017.  \n[6]  P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. \nDuan, D. Ding, A. Bagul, C. Langlotz, K. Shpanskaya, \nM. P. Lungren and A.  Y. Ng, \"Chexnet: radiologist -\nlevel pneumonia detection on chest x -rays with deep \nlearning,\" Stanford ML Group Project: CheXNet, \n2017. \n[7]  X. Wang, Y. Peng, L. Lu and Z. Lu, \"ChestX -ray8: \nHospital-scale Chest X-ray Database and Benchmarks \non Weakly-Supervised Classification and Localization \nof Common Thorax Diseases,\" in IEEE Conference on \nComputer Vision and Pattern Recognition (CVPR) , \nHonolulu, HI, USA, 2017.  \n[8]  M. IvaÅ¡iÄ‡-Kos and I. Hrga, \"Deep Image Captioning: \nAn Overview,\" in International Convention on \nInformation and Communication Technology, \nElectronics and Microelectronics (MIPRO), 2019.  \n[9]  Adriyendi, \"A Rapid Review of Image Captioning,\" \nJournal of Information Technology and Computer \nScience, vol. 6, pp. 158-169, 2021.  \n[10]  Z. Khong, Y. Cui, Z. Xia and H. Lv, \"Convolution and \nLong Short -Term Memory Hybrid Deep Neural \nNetworks for Remaining Useful Life Prognostics,\" \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nApplied Science, vol. 9, no. Machine Fault Diagnostics \nand Prognostics, 2019.  \n[11]  Hidayatullah, A. M. Nugroho and  A. Fathan, \n\"Keterangan Gambar Otomatis Berbahasa Indonesaia \ndengan CNN dan LSTM,\" Automata, vol. 2, 2021.  \n[12]  Y. Su, F. Liu and M. Rosen, \"UMass at ImageCLEF \ncaption prediction 2018 task,\" CEUR Workshop, p. \n215, 2018.  \n[13]  X. Zeng, L. Wen, B. Liu and X. Qi, \"Deep learning for \nultrasound image caption generation based on object \ndetection,\" Neurocomputing, p. 132, 2020.  \n[14]  Y. Azhar, M. R. Anugerah, M. A. R. Fahlopy and A. \nYusriansyah, \"Image Captioning using Hybrid of \nVGG16 and Bidirectional LSTM Model,\" KINETIK: \nGame Technology, Information System, Computer \nNetwork, Computing, Electronics, and Control, vol. 7, \nno. 4, 2022.  \n[15]  F. Shamshad, S. Khan, S. W. Zamir, M. H. Khan, M. \nHayat, F. S. Khan and H. Fu, \"Transformers in Medical \nImaging: A Survey,\" Medical Image Analysis, 2023.  \n[16]  X. Wang, Y. Peng, L. Lu, Z. Lu and R. Summers, \n\"TieNet: Text-image embedding network for common \nthorax disease classification and reporting in chest X -\nrays.,\" in IEEE/CVF Conference on Computer Vision \nand Pattern Recognition , Salt Lake City, UT, USA, \n2018.  \n[17]  R. Li, Z. Wang and L. Zhang, \"Image Caption and \nMedical Report Generation Bas ed on Deep Learning: \na Review and Algorithm Analysis,\" in International \nConference on Computer Information Science and \nArtificial Intelligence (CISAI), Kunming, China, 2021.  \n[18]  J. Donahue, L. A. Hendricks, M. Rohrbach, S. \nVenugopalan, S. Guadarrama, K . Saenko and T. \nDarrel, \"Long-term recurrent convolutional networks \nfor visual recognition and description,\" IEEE \nTransactions on Pattern Analysis and Machine \nIntelligence, vol. 4, no. 39, pp. 677-691, 2017.  \n[19]  D. Hou, Z. Zhao, Y. Liu, F. Chang and S.  Hu, \n\"Automatic Report Generation for Chest X -Ray \nImages via Adversarial Reinforcement Learning,\" \nIEEE Access, vol. 9, pp. 2169-3536, 2021.  \n[20]  I. Najdenkoska, X. Zhen, M. Worring and L. Shao, \n\"Variational topic inference for chest X -ray report \ngeneration,\" in International conference on medical \nimage computing and computer -assisted intervention, \n2021.  \n[21]  T. Cohn, Y. He and Y. Liu, \"Learning to Generate \nClinically Coherent Chest X -Ray Reports,\" Findings \nof the Association for Computational Linguistics: \nEMNLP, p. 1235â€“1243, 2020.  \n[22]  H. Tsaniya, C. Fatichah and N. Suciati, \"Transformer \nApproaches in Image Captioning: A Literature \nReview,\" in International Conference on Information \nTechnology and Electrical Engineering (ICITEE) , \nYogyakarta, 2022.  \n[23]  Y. Xiong, B. Du and P. Ya, \"Reinforced transformer \nfor medical image captioning,\" International \nWorkshop on Machine Learning in Medical Imag ing, \npp. 673-680, 2019.  \n[24]  P. Srinivasan, D. Thapa, A. Bhavsar and A. Nigam, \n\"Hierarchical x -ray report generation via pathology \ntags,\" Proceedings of the Asian Conference, 2020.  \n[25]  Z. Chen, Y. Shen, Y. Song and X. Wan, \"Cross-modal \nmemory networks for radiology report generation,\" in \nProceedings of the 59th Annual Meeting of the \nAssociation for Computational Linguistics and the \n11th International Joint Conference on Natural \nLanguage Processing, 2021.  \n[26]  B. Hou, G. Kaissis, R. M. Summers and B. Kainz, \n\"Ratchet: Medical transformer for chest x -ray \ndiagnosis and reporting,\" in International Conference \non Medical Image Computing and Computer-Assisted \nIntervention, 2021.  \n[27]  Q. You, H. Jin, Z. Wang, C. Fang and J. Luo, \"Image \ncaptioning with semantic attention,\" in IEEE \nConference on Computer Vision and Pattern \nRecognition (CVPR), Las Vegas, 2016.  \n[28]  O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal and \nA. Fahmy, \"Automated  radiology report generation \nusing conditioned transformers,\" Informatics in \nMedicine Unlocked, 2021.  \n[29]  J. Chen, H. Guo, K. Yi, B. Li and M. Elhoseiny, \n\"VisualGPT: Data -efficient Adaptation of Pretrained \nLanguage Models for Image Captioning,\" in \nProceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (CVPR) , \n2022.  \n[30]  Y. Zhuang, N. Jiang and Y. Xu, \"Progressive \nDistributed and Parallel Similarity Retrieval of Large \nCT Image Sequences in Mobile Telemedicine \nNetworks,\" Wireless Communication and Mobile \nComputing, 2022.  \n[31]  L. Sun, M. Zhang, B. Wang and P. Tiwari, \"Few -shot \nclass-incremental learning for Medical time series \nclassification,\" IEEE Journal Biomedical and Health \nInformatics, 2017.  \n[32]  Z. Chen, Y. Song, T. -H. Chang and X. Wan, \n\"Generating Radiology Reports via Memory -driven \nTransformer,\" in Proceedings of the 2020 Conference \non Empirical Methods in Natural Language \nProcessing (EMNLP), 2020.  \n[33]  Y. Zhuang, S. Chen, N. Jiang and H. Hu, \"An Effective \nWSSENet-Based Similarity Retrieval Method of \nLarge Lung CT Image Databases,\" KSII Transactions \non Internet and Information Systems, vol. 16, no. 7, \n2022.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n[34]  R. Cong, H. Sheng, D. Yang, Z. Cui and R . Chen, \n\"Exploiting Spatial and Angular Correlations With \nDeep Efficient Transformers for Light Field Image \nSuper-Resolution,\" IEEE Transaction on Multimedia, \n2022.  \n[35]  G. Zhao, Z. Zhao, W. Gong and F. Li, \"Radiology \nreport generation with medical know ledge and \nmultilevel image-report alignment: A new method and \nits verification,\" Artificial Intelligence in Medicine, \nvol. 146, 2023.  \n[36]  M. Sharma and D. Kumar, \"Comparative Analysis of \nImage Enhancement Techniques for Chest X -ray \nImages,\" in International Conference on \nComputational Intelligence and Sustainable \nEngineering Solutions (CISES), Greater Noida, 2022.  \n[37]  M. Sharma and D. Kumar, \"Contrast Enhancement and \nNoise Reduction of chest X -ray images,\" in \nInternational Conference on Computational \nIntelligence and Sustainable Engineering Solutions \n(CISES), Greater Noida, 2022.  \n[38]  R. Pardeshi, R. Patil, N. Ansingkar, P. D. Deshmukh \nand S. Biradar, \"DWT -LBP Descriptors for Chest X -\nRay View Classification,\" in Advances in Intelligent  \nSystems and Computing, Springer, 2020, pp. 381-389. \n[39]  T.-C. Lin and H. -C. Lee, \"Covid -19 Chest \nRadiography Images Analysis Based on Integration of \nImage Preprocess, Guided Grad -CAM, Machine \nLearning and Risk Management,\" in ICMHI '20: \nProceedings of the 4th International Conference on \nMedical and Health Informatics, 2020.  \n[40]  S. Goyal and R. Singh, \"Detection and classification of \nlung diseases for pneumonia and Covid -19 using \nmachine and deep learning techniques,\" Journal of \nAmbient Intelligence and Humanized Computing, vol. \n14, pp. 3239-3259, 2021.  \n[41]  R. Mostafiz, M. S. Uddin, N. A. Alam, M. M. Reza \nand M. M. Rahman, \"Covid -19 detection in chest X -\nray through random forest classifier using a \nhybridization of deep CNN and DWT optimized \nfeatures,\" Journal of King Saud University - Computer \nand Information Sciences, vol. 34, no. 6, pp. 3226 -\n3235, 2022.  \n[42]  G. Siracusano, A. L. Corte, M. Gaeta, G. Cicero, M. \nChiappini and G. Finocchio, \"Pipeline for Advanced \nContrast Enhancement (PACE) of Che st X -ray in \nEvaluating COVID -19 Patients by Combining \nBidimensional Empirical Mode Decomposition and \nContrast Limited Adaptive Histogram Equalization \n(CLAHE),\" Sustainability, vol. 12, no. 20, p. 8573, \n2020.  \n[43]  I. Kanjanasurat, N. Domepananakorn, T. \nArchevapanich and B. Purahong, \"Comparison of \nimage enhancement techniques and CNN models for \nCOVID-19 classification using chest x -rays images,\" \nin 8th International Conference on Engineering, \nApplied Sciences, and Technology (ICEAST) , Chiang \nMai, 2022.  \n[44]  D. Abin, S. D. Thepade, H. Mankar, S. Raut and A. \nYadav, \"Blending of Contrast Enhancement \nTechniques for Chest X -Ray Pneumonia Images,\" in \nInternational Conference on Electronics and \nRenewable Systems (ICEARS), Tuticorin, 2022.  \n[45]  A. W. Setiawa n, \"Effect of Chest X -Ray Contrast \nImage Enhancement on Pneumonia Detection using \nConvolutional Neural Networks,\" in IEEE \nInternational Biomedical Instrumentation and \nTechnology Conference (IBITeC), Yogyakarta, 2021.  \n[46]  D. Demner-Fushman, M. Kohli, M. Rosenman and S. \nSh, \"Preparing a collection of radiology examinations \nfor distribution and retrieval,\" Journal of the American \nMedical Informatics Association, vol. 23, pp. 304-310, \n2016.  \n[47]  S. F. M. Radzi, M. K. A. Karim, M. Saripan, M. A. A. \nRahman, N. Osman, E. Dalah and N. M. Noor, \"Impact \nof Image Contrast Enhancement on Stability of \nRadiomics Feature Quantification on a 2D \nMammogram Radiograph,\" IEEE Access, pp. 127720 \n- 127731, 2020.  \n[48]  G. Huang, Z. Liu, L. v . d. Maaten and K. Q. \nWeinberger, \"Densely Connected Convolutional \nNetworks,\" in IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR), 2017.  \n[49]  J. Devlin, M. -W. Chang, K. Lee and K. Toutanova, \n\"BERT: Pre -training of Deep Bidirectional \nTransformers for Language Understanding,\" in \nProceedings of the 2019 Conference of the North \nAmerican Chapter of the Association for \nComputational Linguistics: Human  Language \nTechnologies, Volume 1 (Long and Short Papers) , \n2019.  \n[50]  N. Fatima, A. S. Imran, Z. Kastrati, S. M. Daudpota \nand A. Soomro, \"A Systematic Literature Review on \nText Generation Using Deep Neural Network \nModels,\" IEEE Access, vol. 10, pp. 53490  - 53503, \n2022.  \n[51]  H. Ayesha, S. Iqbal, M. Tariq, M. Abrar, M. Sanaullah, \nI. Abbas, A. Rehman, M. F. K. Niazi and S. Hussain, \n\"Automatic medical image interpretation: State of the \nart and future directions,\" Pattern Recognition, vol. \n114, 2021.  \n[52]  B. Veluchamy and Subramani, \"Image contrast and \ncolor enhancement using adaptive gamma correction \nand histogram equalization,\" OPTIK, vol. 183, pp. \n329-337, 2019.  \n[53]  A. Naufal, C. Fatichah and N. Suciati, \"Preprocessed \nMask RCNN for Parking Spac e Detection in Smart \nParking Systems,\" International Journal of Intelligent \nEngineering and Systems, vol. 12, pp. 255-265, 2020.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n[54]  R. Kabilan and R. R. Ramaraj, \"Adaptive Gamma \nCorrection With Weighting Distribution Based \nContrast Enhancement For Ir Image Of Solid Object In \nDense Fire,\" Caribbean Journal of Science, vol. 53, \nno. 2, pp. 932-947, 2022.  \n \nHilya Tsaniya (Member, IEEE)  \nWas born in Padang, Indonesia in 1999. She received a Bachelor of \nComputer Science from Padjadjaran University, Master of Computer \nScience from Institut Teknologi Sepuluh Nopember, currently she \npursuing a doctoral program at  Institut Teknologi Sepuluh Nopember, \nSurabaya, Indonesia. She has 8 journal article s and conference paper in \ncomputer science, with special interest in computer vision, medical image \nprocessing, and natural language processing. \n \n \n \n  \nChastine Fatichah (Member, IEEE) earned her doctoral degree in 2012 \nfrom the Tokyo Institute of Technology in Japan. Currently, she is a \nProfessor in the Department of Informatics at Institut Teknologi Sepuluh \nNopember in Surabaya. She has published over 110 computer scien ce-\nrelated journal articles and conference papers. Among her areas of research \ninterest are artificial intelligence, image processing, and data mining. \n \n \n \n \nNanik Suciati (Member, IEEE) earned a master's degree in computer \nscience in 1998 from the Universi ty of Indonesia and a doctorate in \ninformation engineering from the University of Hiroshima in 2010. She is \na Professor in the Department of Informatics at Institut Teknologi Sepuluh \nNopember at present. She has published more than fifty journal articles \nand conference papers on computer science. Her areas of interest in \nresearch are computer vision, computer graphics, and artificial \nintelligence. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3364373\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.668213963508606
    },
    {
      "name": "Computer vision",
      "score": 0.5018596649169922
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49515500664711
    },
    {
      "name": "Contrast enhancement",
      "score": 0.4871620833873749
    },
    {
      "name": "Transformer",
      "score": 0.4809540808200836
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.43048611283302307
    },
    {
      "name": "Magnetic resonance imaging",
      "score": 0.22524449229240417
    },
    {
      "name": "Radiology",
      "score": 0.2209681272506714
    },
    {
      "name": "Electrical engineering",
      "score": 0.15304401516914368
    },
    {
      "name": "Medicine",
      "score": 0.1394755244255066
    },
    {
      "name": "Voltage",
      "score": 0.12227004766464233
    },
    {
      "name": "Engineering",
      "score": 0.10327285528182983
    },
    {
      "name": "Physics",
      "score": 0.09557867050170898
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166843116",
      "name": "Sepuluh Nopember Institute of Technology",
      "country": "ID"
    }
  ]
}