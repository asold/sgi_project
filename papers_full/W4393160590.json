{
    "title": "Generative Multi-Modal Knowledge Retrieval with Large Language Models",
    "url": "https://openalex.org/W4393160590",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2532862397",
            "name": "Xinwei Long",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2344356506",
            "name": "Jiali Zeng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2133392087",
            "name": "Fandong Meng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2108652900",
            "name": "Zhiyuan Ma",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2126740995",
            "name": "Kaiyan Zhang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2110030736",
            "name": "Bowen Zhou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2093278426",
            "name": "Jie Zhou",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2532862397",
            "name": "Xinwei Long",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2344356506",
            "name": "Jiali Zeng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2133392087",
            "name": "Fandong Meng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2108652900",
            "name": "Zhiyuan Ma",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2126740995",
            "name": "Kaiyan Zhang",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2110030736",
            "name": "Bowen Zhou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2093278426",
            "name": "Jie Zhou",
            "affiliations": [
                "Tencent (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4224438163",
        "https://openalex.org/W4367628274",
        "https://openalex.org/W4223512675",
        "https://openalex.org/W4292215729",
        "https://openalex.org/W6783498461",
        "https://openalex.org/W4380137047",
        "https://openalex.org/W6803842174",
        "https://openalex.org/W4322766882",
        "https://openalex.org/W2158322625",
        "https://openalex.org/W4312091824",
        "https://openalex.org/W6796475582",
        "https://openalex.org/W4296868462",
        "https://openalex.org/W6605174579",
        "https://openalex.org/W3015883388",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W4378715759",
        "https://openalex.org/W6852805313",
        "https://openalex.org/W3196798856",
        "https://openalex.org/W4285119160",
        "https://openalex.org/W4382202719",
        "https://openalex.org/W2947312908",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W2969862959",
        "https://openalex.org/W4378505343",
        "https://openalex.org/W6810139940",
        "https://openalex.org/W6850138286",
        "https://openalex.org/W6811138465",
        "https://openalex.org/W4387947178",
        "https://openalex.org/W4361229539",
        "https://openalex.org/W4389520393",
        "https://openalex.org/W4366850747",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4221166196",
        "https://openalex.org/W4385571319",
        "https://openalex.org/W4389519033",
        "https://openalex.org/W4385569751",
        "https://openalex.org/W3089997936",
        "https://openalex.org/W4375870261",
        "https://openalex.org/W2560674852",
        "https://openalex.org/W3091432621",
        "https://openalex.org/W4320465836",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4385565351",
        "https://openalex.org/W4309811444",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4320487943",
        "https://openalex.org/W4309444617",
        "https://openalex.org/W3189509741",
        "https://openalex.org/W4380993239",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4385567756",
        "https://openalex.org/W4226052861",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4221141581",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W4318239801",
        "https://openalex.org/W4385565470"
    ],
    "abstract": "Knowledge retrieval with multi-modal queries plays a crucial role in supporting knowledge-intensive multi-modal applications. However, existing methods face challenges in terms of their effectiveness and training efficiency, especially when it comes to training and integrating multiple retrievers to handle multi-modal queries. In this paper, we propose an innovative end-to-end generative framework for multi-modal knowledge retrieval. Our framework takes advantage of the fact that large language models (LLMs) can effectively serve as virtual knowledge bases, even when trained with limited data. We retrieve knowledge via a two-step process: 1) generating knowledge clues related to the queries, and 2) obtaining the relevant document by searching databases using the knowledge clue. In particular, we first introduce an object-aware prefix-tuning technique to guide multi-grained visual learning. Then, we align multi-grained visual features into the textual feature space of the LLM, employing the LLM to capture cross-modal interactions. Subsequently, we construct instruction data with a unified format for model training. Finally, we propose the knowledge-guided generation strategy to impose prior constraints in the decoding steps, thereby promoting the generation of distinctive knowledge clues. Through experiments conducted on three benchmarks, we demonstrate significant improvements ranging from 3.0% to 14.6% across all evaluation metrics when compared to strong baselines.",
    "full_text": "Generative Multi-Modal Knowledge Retrieval with Large Language Models\nXinwei Long*1, Jiali Zeng2, Fandong Meng2, Zhiyuan Ma1, Kaiyan Zhang1,\nBowen Zhou†1 , Jie Zhou2\n1Department of Electronic Engineering, Tsinghua University, Beijing, China\n2Pattern Recognition Center, WeChat AI, Tencent Inc, China\nlongxw22@mails.tsinghua.edu.cn, zhoubowen@tsinghua.edu.cn\nAbstract\nKnowledge retrieval with multi-modal queries plays a crucial\nrole in supporting knowledge-intensive multi-modal applica-\ntions. However, existing methods face challenges in terms of\ntheir effectiveness and training efficiency, especially when it\ncomes to training and integrating multiple retrievers to han-\ndle multi-modal queries. In this paper, we propose an innova-\ntive end-to-end generative framework for multi-modal knowl-\nedge retrieval. Our framework takes advantage of the fact that\nlarge language models (LLMs) can effectively serve as vir-\ntual knowledge bases, even when trained with limited data.\nWe retrieve knowledge via a two-step process: 1) generating\nknowledge clues related to the queries, and 2) obtaining the\nrelevant document by searching databases using the knowl-\nedge clue. In particular, we first introduce an object-aware\nprefix-tuning technique to guide multi-grained visual learn-\ning. Then, we align multi-grained visual features into the tex-\ntual feature space of the LLM, employing the LLM to capture\ncross-modal interactions. Subsequently, we construct instruc-\ntion data with a unified format for model training. Finally, we\npropose the knowledge-guided generation strategy to impose\nprior constraints in the decoding steps, thereby promoting the\ngeneration of distinctive knowledge clues. Through experi-\nments conducted on three benchmarks, we demonstrate sig-\nnificant improvements ranging from 3.0% to 14.6% across all\nevaluation metrics when compared to strong baselines.\nIntroduction\nKnowledge Retrieval (KR) is crucial in supporting\nknowledge-intensive multi-modal applications, such as vi-\nsual question answering (VQA) (Ma et al. 2023), multi-\nmodal entity linking (Huang et al. 2022) and multi-modal\ndialogue (Ma et al. 2022). In these applications, the infor-\nmation available within the multi-modal contexts may be\ninsufficient, necessitating the acquisition of external knowl-\nedge. As illustrated in Fig. 1, knowledge retrievers offer key\nevidence to assist VQA systems in identifying the motorcy-\ncle’s style as a “chopper”. In recent years, information re-\ntrieval (Chen et al. 2021; Wu et al. 2023; Tang et al. 2023)\nhas achieved remarkable success. However, challenges still\n*The work was done when Xinwei Long worked as intern at\nPattern Recognition Center, WeChat AI, Tencent Inc, China.\n†Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nthe yamaha raider really straddles two  the \npower cruiser and the custom chopper...\nText \nRetriever\nMultimodal \nRetriever\nQ34493\nroad \nvehiclet\nsingle-track \nvehicleMotorcycle\nText Query: What style of \nmotorcycles are pictured \nhere?\nA motorcycle (motorbike, or trike (if three-\nwheeled)) is a two motor vehicle steered....\nImage Query\n Our \nModel\nMulti-Modal Queries\nDoc: 69046: Motorcycles come in various \nstyles, such as cruiser, adventure, and ..\nEntity \nRetriever\nFigure 1: Multi-Modal Knowledge Retrieval. Prior studies\nuse multiple retrievers for separate purposes, while we re-\ntrieve knowledge through an end-to-end generative model.\npersist in terms of effectiveness and training efficiency when\napplying these methods to multi-modal scenes. Existing\nmethods (Luo et al. 2021a; Gao et al. 2022) handle multi-\nmodal queries by utilizing individual text-to-text and image-\nto-text retrievers, which struggle to capture cross-modal in-\nteractions and require abundant data to train each module\nin the pipelines. The question arises: Can we develop a re-\ntriever that effectively handles multi-modal queries while\navoiding the redundant pipeline?\nRecently, there has been a promising development in the\nfield of Ad-hoc retrieval called generative retrieval (Wang\net al. 2022; Bevilacqua et al. 2022). This approach aims to\nsimplify the retrieval pipeline by generating relevant docu-\nment identifiers instead of retrieving them from a large-scale\ncorpus. Instead of retrieving actual documents, these meth-\nods directly generate identifiers such as document titles or\nURLs that are relevant to the query.\nNevertheless, these generative retrieval methods have not\nbeen applied to multi-modal knowledge retrieval for two rea-\nsons. Firstly, knowledge-aware documents, which contain\ninformation from multiple aspects, cannot be effectively rep-\nresented by static identifiers, such as numeric IDs (Tay et al.\n2022) and titles (Chen et al. 2022b). This is because queries\nfrom different modalities attend to different aspects of doc-\numents. For example, as depicted in Fig. 1, the text query\nattends to keywords that are present in both query and docu-\nment (e.g. “motorcycles”), whereas the image query concen-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18733\ntrates on descriptive words about specific visual elements.\nSecondly, the identifiers (Tay et al. 2022) require additional\nmemory steps and are inefficient when dealing with large-\nscale corpora. This approach proves to be training-inefficient\nand struggles to perform well when encountering unseen\nknowledge, highlighting its lack of generalization capabil-\nities.\nTo address these challenges, we propose a generative\nframework for multi-modal knowledge retrieval, briefly de-\nnoted as GeMKR. This framework leverages the LLMs,\nLLaMA, as its core model, based on the premise that\nLLMs can effectively function as virtual knowledge bases\n(KB) (Pan et al. 2023) even then fine-tuning with limited\ndata. In GeMKR, we abandon the traditional pipeline that\ncalculates the similarity between queries and knowledge. In-\nstead, we retrieve knowledge via a two-step process: 1) gen-\nerating knowledge clues related to the queries, and 2) ob-\ntaining the relevant document by searching databases using\nthe knowledge clue. Please note that only the first step re-\nquires neural computation, while the second step is a defini-\ntive and efficient database operation. Here, knowledge clues\nare defined as any subsequences within a document that ap-\npear exclusively in that particular document. Unlike the one-\nto-one relationship between an identifier and a document,\neach knowledge clue in GeMKR uniquely corresponds to\na knowledge-aware document in the knowledge base, while\neach document can be mapped by multiple knowledge clues.\nWe carry out primary experiments on three bench-\nmarks, with knowledge base sizes ranging from 112,724 to\n21,015,324. The experimental results show significant im-\nprovements of 3.0-14.6% across all metrics compared to\nstrong baselines. Notably, GeMKR achieves improvements\nof 14.6% and 8.9% in P@5 and R@5 respectively, when re-\ntrieving information from a knowledge base comprising 21\nmillion documents. This outcome illustrates our model’s ca-\npacity to generalize well to large-scale knowledge sources.\nRelated Work\nMulti-Modal Knowledge Retrieval\nKnowledge-intensive multi-modal tasks require extensive\nknowledge access due to the insufficiency of vital informa-\ntion within their contexts. Existing methods ensemble var-\nious types of retrievers to acquire world knowledge. Rep-\nresentative retrieval methods include BM25 (Robertson and\nZaragoza 2009a) and DPR (Karpukhin et al. 2020a) for text\nretrieval, CLIP (Radford et al. 2021a) for image-to-text re-\ntrieval, and GENER (De Cao et al. 2021) for entity retrieval.\nHowever, the simple integration of multiple retrievers for\nindividual purposes is inadequate for knowledge-intensive\nmulti-modal tasks due to the following reasons. Firstly, it is\nimportant to consider the interaction between visual and tex-\ntual queries in order to understand the relationships between\nvisual objects and textual entities. Secondly, the pipeline in-\nvolves the integration of various external tools, resulting in\ninconvenient usage.\nIn contrast to the integration of multiple traditional re-\ntrievers, some studies have proposed new methods and\nbenchmarks to facilitate research on this task. (Luo et al.\n2021b) constructs a knowledge retrieval dataset using the\nOKVQA benchmark (Marino et al. 2019). This dataset ne-\ncessitates the retrieval of relevant evidence, using both the\nquestion and image as queries. This dataset comprises a\nsmall KB of 112K knowledge records and a large KB con-\nsisting of 21M records. This poses a tough challenge in\nobtaining accurate knowledge from such an extensive KB.\n(Luo et al. 2023a) introduces a high-quality multi-modal\nknowledge retrieval dataset, imposing higher demands on\ncross-modal understanding. To jointly encode visual and\ntextual queries, recent studies (Luo et al. 2023a) have ex-\nplored training a single-stream vision-language model to ob-\ntain cross-modal representation. Due to differences between\nthe modalities, they use millions of data to train their models\nin a contrastive framework. Despite achieving improvements\nover the above methods, multi-modal knowledge retrieval\nremains an under-explored task in terms of effectiveness and\ntraining efficiency.\nGenerative Retrieval\nRecently, some studies (Wang et al. 2022; Tay et al. 2022;\nLi et al. 2023; Zhou, Dou, and Wen 2023) have explored\nretrieving documents through generative language mod-\nels, e.g. BART. They simplify the pipelines of retrieval\nby directly generating identifiers of relevant documents for\nqueries rather than retrieving from a large-scale corpus.\n(Tay et al. 2022) proposes DSI (Differentiable Search In-\ndex) framework that builds the search index in Transformer\nmemories rather than in databases. It assigns each document\na numeric ID as the identifier, which requires extra mem-\nory steps and is inefficient and ineffective in the large-scale\ncorpus. Instead of numeric IDs, (Chen et al. 2022b,a) take\nWikipedia titles as identifiers to integrate semantic informa-\ntion about documents into identifiers, whereas (Bevilacqua\net al. 2022; Li et al. 2023; Chen et al. 2023) leverages n-\ngrams in documents as identifiers and introduces an effi-\ncient FM-Index (Ferragina and Manzini 2000) structure to\nguide the generation of identifiers. However, each n-gram\ncould correspond to several documents, as a short n-gram\nmight appear in several contexts. To address these issues, the\nabove methods generate numerous n-grams for each query\nand then re-rank these n-grams to obtain the final results.\nLarge Language Model and Efficient Fine-tuning\nEarly explorations leveraging the power of LLMs for infor-\nmation retrieval exist, such as using LLMs to understand\nqueries (Jagerman et al. 2023), generating training data for\ndownstream retrieval (Gao et al. 2023), and making deci-\nsions in re-ranking stages (Ferraretto et al. 2023), which\ndemonstrate the potential of LLMs in the retrieval task. De-\nspite the strong capabilities of LLMs, their enormous param-\neters pose challenges for computational resources when fine-\ntuning for downstream tasks. To mitigate these challenges,\nparameter-efficient fine-tuning methods (Han et al. 2022;\nDing et al. 2023; Zhang et al. 2023a) have been proposed\nwhich reduce costs by updating only a subset of parame-\nters. Typical efficient methods include prompt tuning (Ding\net al. 2022), prefix tuning (Li and Liang 2021; Yang and\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18734\nLiu 2022), adapter methods (Zhang et al. 2023b; Diao et al.\n2023), and the low-rank (LoRA) method (Hu et al. 2021).\nMethodology\nWe propose GeMKR, an end-to-end generative framework\nfor multi-modal knowledge retrieval. GeMKR consists of\nthree components, as depicted in Fig. 2: Object-aware\nprefix-tuning for fine-tuning the visual backbone, Multi-\nModal Alignment using LLMs to capture cross-modal in-\nteractions, and Knowledge-guided Constraint Decoding\nfor generating informative knowledge clues.\nProblem Formulation\nFormally, let D = {D1, D2, ..., Di}N\ni=1 denotes a knowl-\nedge base used for the multi-modal knowledge retrieval task.\nAs a generative retrieval model, our goal is to generate the\nrelevant knowledge clues {C}, which can be definitively\nmapped to documents in D. Our model takes the multi-\nmodal query Q = {T, V} as input and generate the relevant\nknowledge clue with an auto-regressive score, as Eq. 1.\nscore(C, Q) = FΘ(C|Q) =\nY\nj=1\nFΘ(cj|c<j, Q). (1)\nwhere FΘ is the generative retriever with parametersΘ, and\ncj is the jth token of the knowledge clue. During inference,\nthe model employs a constrained strategy to guide the de-\ncoder in generating content within a limited token space at\neach step, which ensures that each knowledge clue Cj can\nbe definitively mapped to a document Di as Eq. 2.\nφ : Cj → Di, where Di ∈ D. (2)\nFinally, we sort the document set {Di} based on auto-\nregressive scores score(Ci, Q) to obtain the final retrieval\nresults.\nObject-aware Prefix Tuning\nTo efficiently fine-tune the visual backbone, we present the\nObject-aware Prefix Tuning method that explicitly guides\nthe visual understanding using objects (i.e., visual entities)\nas the learnable prefix. As shown in the bottom of Fig. 2, we\nutilize the CLIP model with N Transformer layers as our\nvisual backbone, and we feed two groups of features, XP\nand XI, into CLIP using a prefix tuning approach as Eq. 3.\nHere, XP denotes the learnable prefix prompts, which are\nmixed with fine-grained visual object information, whileXI\nrepresents the embeddings of each visual token encoded by\nthe patch embedding layer. To mitigate catastrophic forget-\nting of the visual backbone, we freeze the parameters Θv of\nthe visual backbone and only make the prefix prompts XP\nlearnable.\nHv = Fvisual(XP , XI; Θv). (3)\nFor the preparation of XP , we first randomly initialize\nthe N prefix prompts XPr for N layers, with the parameter\nmatrix in a dimension of l × d, where l denotes the length\nof the prefix and d is the visual dimension. Note that the\nprefix prompts are not shared across layers. To obtain ob-\nject features, we crop the objects from images and trans-\nform them into a fixed resolution. Next, we extract their\nText: What style of \nmotorcycles are pictured \nhere?\nLarge Language Model\nProj. Layer\nProj. Layer Patch Linear Layer\nLayer N....\nPrefix Vector Visual Token\nLayer i-th\n... \nLLM\n FM-Index DB \nLookupDoc(*)\nDoc Id: 69046 ... of the \nhonda fury... a chopper, a...\nhonda\n... honda fury ...  a chopper\nhonda fury \nresemble\n......\nValidDistinct(*) & GetNext(*)\nFalse & {those, ...,chopper}\nValidDistinct(*) \nGenerate knowledge clues about... the Query: [What]\n[motor] [pictured] [CLS] [OBJ 1][style] IMG: Response:[OBJ 2]\nFigure 2: Overall architecture of our GeMKR.\nfeatures as XR ∈ ℜr×d using a frozen visual encoder,\nCLIP. Lastly, we feed XR into a learnable projection layer\nas HR = Linear(XR) and pad them with zero vectors to\nmaintain the same dimension as the prefix. Taking the ith\nlayer in the visual backbone as an example, we denote the\npredefined prefix vector as Xi\nPr ∈ ℜl×d and the visual fea-\ntures obtained from the (i − 1)th layer is Hi−1\nI ∈ ℜl×d, we\nacquire the object-aware prefix vector via simply addition as\nEq. 4,\nXi\nP = Xi\nPr + HR. (4)\nAfter that, we concatenate Xi\nP in front of Xi\nI, as X =\n[Xi\nP ; Hi\nI], and feed them into the self-attention module.\nThis method allows the fine-grained object features within\nXi\nP to better guide the visual backbone during the adapta-\ntion process.\nHowever, the distribution discrepancy between the object\nfeature HR and the immediate output Hi\nI may lead to a sig-\nnificant loss at the early training stages, potentially disturb-\ning the fine-tuning process. Similar to (Zhang et al. 2023b),\nwe introduce a dual-flow attention mechanism to address\nthis issue by independently computing attention weights for\nprompt vectors and hidden states. Specifically, We com-\npute the query vector on Hi\nI, by applying LinearQi (Hi\nI),\nwhile the key and value vectors are independently computed\nfor Xi\nP and Hi\nI using the linear layers LinearKi (·) and\nLinearVi (·), denoted as Ki\np, Ki\nh, V i\np , and V i\nh . We follow\nthe vanilla attention module, as Eq. 5, but individually ap-\nply attention maps for two components and multiply value\nvectors to obtain outputs, as Eq 6,\nAtt(Q,K, V ) = Softmax(Qi(Ki)T\n√\nd\n)V , (5)\nHi\nO = Att(Qi\nh, Ki\np, V i\np ) ∗ σ + Att(Qi\nh, Ki\nh, V i\nh ). (6)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18735\nHere, σ denotes a gate function to control the information\nflow from the prompt vectors to visual tokens. Dual-flow at-\ntention mechanism effectively alleviates the influence of un-\ncertainty from the prefix, while keeping the ability of the vi-\nsual backbone to obtain high-quality representation, thereby\nmaking a more stable fine-tuning process. After N Trans-\nformer Layers, we could obtain the final representation Hv\nfrom the visual backbone, where Hv contains a representa-\ntion of the [CLS] token, denoted as hcls and representations\nof other visual tokens.\nMulti-Modal Alignment\nTo effectively integrate visual features into pre-trained LLM,\nwe employ the simple projection scheme as illustrated in the\nmiddle part of Fig. 2, which demonstrates effectiveness in\nother vision-language studies (Zhu et al. 2023). Specifically,\nwe utilize the [CLS] token hcls to represent the image at a\nholistic level since the long sequence of visual tokens would\ndisturb the linguistic knowledge in LLMs. Besides, we also\nleverage object features HR as features to integrate object-\nlevel visual information. Then, a simple linear layer is ap-\nplied to map the visual representation to the text embedding\nspaces as Eq. 7,\nH\n′\nv = Linear([hcls; HR]). (7)\nAfter that, we utilize LLaMA as our textual backbone, with\ntext embedding and the visual representation H\n′\nv as inputs.\nBased on the multi-modal input, the LLM can predict the\nnext token step by step.\nInstruction Tuning\nSupervised Text Sampling. During training, knowledge\nclues are not explicitly provided in this task. Instead, the\nbenchmarks offer a set of relevant documents for each query.\nHowever, these documents tend to be excessively long, com-\nprising redundant information, whereas knowledge clues are\nideally concise text snippets directly pertinent to queries.\nTo address this issue, we first split the positive document\ninto individual sentences and evaluate each sentence’s rele-\nvance by counting the number of keyword hits between the\nsentence and the query, where the higher the hit rate, the\nmore relevant information the sentence contains. Despite se-\nlecting the sentences S with the most keyword hits, they are\nstill not ideal supervised texts, since these sentences have\nvarying lengths. Therefore, we calculate the count of hits\ncwi for each span wi:i+n as the relevant score πwi =\ncwi\ncwi +ρ\nand normalize the scores of all the spans in the sentence\nthrough a softmax function, where ρ is the smoothing fac-\ntor. We samplem start positions of snippets according to the\nnormalized distribution and cut out l tokens from the start\npositions to obtain m knowledge clues with the same length.\nInstruction Data Construction. Firstly, we create the in-\nstruction template in a unified format with predefined slots\nfor filling the multi-modal queries. As shown in Fig 2, the\ntemplate contains a task description, an instruction text de-\nrived from the textual query, and several predefined slots\nfor visual features. The instruction data is used to train the\nmodel, prompting the prediction of tokens after the “re-\nsponse:” token, and thus only the predicted tokens are used\nto compute the loss.\nModel Training. We perform instruction tuning for the\nwhole model on the predicted tokens. We freeze the param-\neters of LLaMA and use the low-rank adaptation (LoRA)\nmethod for efficient adaptation. We adopt the common auto-\nregressive training objective, as Eq 8,\nLgen = −\nlX\ni=1\nlogFΘ(ci|c<i; T; V ; Θ). (8)\nwhere Θ denotes unfrozen parameters.\nKnowledge-guided Constraint Decoding\nDuring inference, our model applies the knowledge-guided\nconstraint decoding strategy to guide the decoder in search-\ning within a limited token space at each step, so as to gen-\nerate a valid knowledge clue that can be mapped to one\nand only one document within the knowledge base. To fa-\ncilitate efficient search from the KB, we introduce the FM-\nIndex database (Ferragina and Manzini 2000) for its stor-\nage. The FM-Index offers three interfaces: GetNext, Valid-\nDistinct, and LookupDoc, enabling efficient lookup from a\nlarge-scale corpus at the millisecond level.\nIn each generation step, our model employs the previously\ngenerated tokens as prefix conditions to invoke the GetNext\ninterface. Subsequently, the interface searches for the strings\nthat match this prefix, obtaining the succeeding token as the\nnext allowable token. Lastly, the model constructs a mask\nmatrix derived from the set of allowable tokens, wherein to-\nkens in the set are assigned a value of 1, and others are set\nto 0. This matrix is employed to modify the predicted distri-\nbution, ensuring that the decoded knowledge clue appears at\nleast once in the knowledge base.\nTo make the generated knowledge clues more distinctive,\nwe force the model to generate at least lmin tokens. Then,\nwe use the ValidDistinct interface to validate whether the\ngenerated tokens can be uniquely mapped to a knowledge\nrecord in the KB. If the return is “True”, the generation\nprocess is stopped. Otherwise, the model continues to gen-\nerate the next token and validate every step until the max\nlength lmax is reached or the returned value is “True”. We\nadd the penalty term in the decoding process to encourage\nshort discriminable clues to be ranked in the front of the\nqueue whereas long ambiguous clues are in the tail. Based\non these strategies, most of the generated clues can corre-\nspond to a unique record in the knowledge base. We regard\nthe knowledge clues corresponding to several documents as\ninvalid outputs and drop them directly. We can obtain the\nwhole knowledge document by using the LookupDoc inter-\nface with a generated knowledge clue as input, which is a\ndefinitive and efficient operation.\nExperiments\nSettings\nWe conduct experiments on three benchmarks of multi-\nmodal knowledge retrieval: OKVQA-GS112K (Luo et al.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18736\nModel OKVQA-GS112K OKVQA-WK21M ReMuQ\nP@5 R@5 R@10 P@5 R@5 R@10 P@1 R@5 R@10\nBM25 (Robertson and Zaragoza 2009b) △ 27.5 51.4 63.0 27.9 50.2 60.9 5.6 8.8 10.8\nDPR (Karpukhin et al. 2020b) △ 27.7 55.6 66.4 28.1 59.4 71.1 35.8 43.4 48.8\nCorpusBrain (Chen et al. 2022b) ⋆ 28.2 58.6 66.9 - - - - - -\nSEAL (Bevilacqua et al. 2022) 30.4 62.9 73.9 - - - 56.7 66.4 74.1\nCLIP (Radford et al. 2021b) △ 11.1 34.5 50.5 9.7 29.8 43.0 19.4 40.2 49.3\nVRR (Luo et al. 2021a) △◦ 39.4 71.5 81.5 - - - - - -\nReViz(Luo et al. 2023a) △ 34.5 66.1 77.8 30.1 60.9 72.2 49.1 62.4 71.6\nReViz-ICT(Luo et al. 2023a) △⋆ 41.7 73.4 83.2 31.4 61.9 72.6 62.1 76.2 83.3\nGeMKR (Our Model) 49.1 78.6 86.2 46.0 70.8 79.1 75.2 90.3 92.7\nTable 1: Results on the benchmarks of multi-modal knowledge retrieval, where ⋆ represents the method uses external data, ◦\nindicates the method ensembles several retrievers, △ means results are reported in (Luo et al. 2023a).\nDataset Train/Val/ Test KB size\nOKVQA-GS112K 8,062/896/5,046 112,724\nOKVQA-WK21M 8,062/896/5,046 21,015,324\nReMuq 7,576/842/3,609 195,837\nTable 2: Dataset statistics.\n2021a), OKVQA-WK21M (Luo et al. 2023b) and Re-\nMuq (Luo et al. 2023b), which are derived from the VQA\ntask leveraging both the image and question as queries. The\ndataset statistics can be found in Tab. 2.\nEvaluation Metrics. We strictly follow the settings of\nthe original papers, using the corresponding metrics for\neach dataset. We evaluate model performance using Pseudo-\nrelevance Precision@K (P@K) and Pseudo-relevance Re-\ncall@K (R@K). Specifically, we use R@5, and R@10 for\nall datasets. For ReMuQ, which has exactly one correct doc-\nument per query, we use P@1. For the other datasets, we\nuse P@5. Please refer to the formalized definition in their\noriginal paper (Luo et al. 2021a, 2023a).\nBaselines. We adopt several baseline methods for com-\nparison: (1) BM25 (Robertson and Zaragoza 2009b) and\nDPR (Karpukhin et al. 2020b) are classical document\nretrieval models. (2) CorpusBrain (Chen et al. 2022b)\nand SEAL (Bevilacqua et al. 2022) are advanced gener-\native retrieval models. (3) CLIP (Radford et al. 2021b)\nis a typical image-to-text retriever. (4) VRR (Luo et al.\n2021a) integrates three retrievers, including BM25, DPR,\nand LXMERT (Tan and Bansal 2019). (5) ReViz and Re-\nViz+ICT (Luo et al. 2023a) are multi-modal retrievers that\nare designed for this task. Note that we use the image caption\nmodel to obtain the textual description of images and feed\nthe textual features to enhance the understanding of multi-\nmodal contexts for textual baselines.\nImplementation Details In our main experiments, we use\nViT-L/14 from pre-trained CLIP (Radford et al. 2021b) as\nthe image encoder and LLaMa-7b (Touvron et al. 2023) as\nthe text encoder. We use YOLOv7 (Wang, Bochkovskiy, and\nLiao 2022) to obtain bounding boxes, keeping the top 5\nmost confident objects for images with excessive objects.\nOur model is implemented by Pytorch and trained using\na learning rate of 6e-5, the Adam optimizer with a warm-\nup strategy, and batches of 12 instruction data. Training is\nperformed on an NVIDIA A6000 48G GPU and completed\nwithin three hours. Unlike other approaches, we train our\nmodel end-to-end without additional data. We construct in-\nstruction data from the original dataset as shown in Tab. 2,\nand sample two knowledge clues for each positive docu-\nment. For inference, knowledge sources are indexed using\nthe FM-Index (Ferragina and Manzini 2000) technique and\nstored in the Sdls-lite 1 database for efficient storage and\nlookup. To generate distinct knowledge clues, we use con-\nstrained beam search to decode clues over 10-15 timesteps\nwith 20 beams and 4 beam groups. 2\nMain Results\nAs shown in Tab 1, we conduct a comparative analysis of\nour model against baseline approaches across three bench-\nmarks, varying in KB sizes from 112K to 21M. Evidently,\nour proposed approach consistently outperforms the lead-\ning state-of-the-art baselines across all evaluated metrics.\nIn particular, the improvements, measured by P@K, sur-\npass a minimum of 13.1% on the ReMuQ and OKVQA-\nWK21M datasets, demonstrating our model’s capacity to re-\ntrieve more precise knowledge compared to alternative base-\nlines.\nBesides, ReViz-ICT, which utilizes a single-stream query\nencoder to capture cross-modal interactions, consistently\nachieves superior performance among other baselines. The\nlower performance of other baseline models, emphasizes the\nimportance of cross-modal interaction in this task. How-\never, training a multi-modal query encoder necessitates a\nlarge amount of multi-modal data, which is both resource-\nintensive and data-inefficient. In contrast, our model only re-\nquires 20K instruction data for lightly fine-tuning with only\n14M parameters of the total 7.3B. What’s more, we observe\nthat the textual generative baseline SEAL produces promis-\ning results when employing image captions as visual fea-\n1https://github.com/simongog/sdsl-lite\n2The code will be released in this repository.\nhttps://github.com/xinwei666/MMGenerativeIR\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18737\nDelete Module P@5 R@5 R@10\nFull Model 49.1 78.6 86.2\nw/o Obj. Feature 46.9 76.7 85.3\nw/o Dual-flow Att 47.5 76.5 84.5\nw/o Obj. Prefix 46.5 76.5 84.1\nw/o Lora 44.5 75.3 84.2\nw/o (Obj. Prefix & Lora) 39.5 72.2 82.1\nw/o Visual Queries 40.4 69.8 79.3\nTable 3: Ablation studies on the OKVQA-GS112K .\ntures. This observation indicates the effectiveness of gener-\native models in knowledge retrieval tasks.\nWhile ReViz-ICT demonstrates good performance in re-\ntrieving from smaller knowledge bases, the improvement\nis less evident when applied to the larger-scale knowledge\nbase. Conversely, our model exhibits superior performance,\nsurpassing ReViz-ICT by a margin of at least 6.4%, when re-\ntrieving information from a knowledge base comprising 21\nmillion entries. This outcome demonstrates that our model\ncan generalize well to varying scales of knowledge sources.\nIn a nutshell, our model is well-suited for multi-modal\nknowledge retrieval, supported by two potential reasons.\nFirstly, our model adeptly aligns visual representations with\nLLMs, thereby enhancing its capability to deeply understand\nmulti-modal queries. Secondly, our model introduces a con-\nstrained beam search guided by knowledge bases. This ap-\nproach takes advantage of the knowledge potential of LLMs\nwhile imposing constraints to mitigate unreliable outputs.\nAnalysis\nAblation Study\nIn this section, we conduct a series of ablation studies from\nthe bottom to the top layer by deleting each module respec-\ntively. Results are in Tab 3.\nIn the ablation study of the Object-aware Prefix Tuning\n(Obj. Prefix) module, we initiate the evaluation by omit-\nting the object feature, resulting in an evident drop of 2.2%\nand 1.9% in P@5 and R@5 respectively. This observation\nindicates the importance of object features in multi-modal\nknowledge retrieval. Next, we substitute the dual-flow atten-\ntion mechanism (as Eq 6) with the vanilla attention, leading\nto a performance decrease of 1.5% to 2.1%. Lastly, upon\ncomplete removal of the prefix tuning module, there is a sig-\nnificant decline of at least 2% across all metrics, thus demon-\nstrating the effectiveness of object-aware prefix-tuning in in-\ntegrating multi-grained visual features.\nFurthermore, we explore the effectiveness of each opera-\ntion on the LLMs. We first freeze all parameters of the LLM\n(i.e. removing the Lora adaptation), resulting in a perfor-\nmance decrease. Additionally, when both the Object-aware\nPrefix-tuning and Lora adaptation are removed (only updat-\ning the parameters of the projection layers), the results ex-\nhibit a sharp decline of over 10%, falling below even the\nbaseline performance levels. This outcome demonstrates the\nessential role of the LLM, which operates as a virtual knowl-\nedge base for generating precise knowledge clues. Finally,\nOPT-1.3B OPT-2.7B OPT-6.7B LLaMA-7B LLaMA-13B\n65\n70\n75\n80\n85 R@5\nR@10\nFigure 3: Results of scaling up the LLMs\nwe remove the visual module and use textual queries and im-\nage captions as inputs. The model’s performance decreases\nfurther, highlighting that image caption models cannot re-\nplace the role of visual modules in multi-modal tasks. Nev-\nertheless, the performance is still better than the best textual\nbaseline SEAL, which shows the effectiveness of other de-\nsigns in our model.\nEffect of Model Sizes\nAdditionally, we utilize diverse LLMs (Zhang et al. 2022;\nTouvron et al. 2023) at varying scales (i.e., 1.3B, 2.7B, 6.7B,\n7B, and 13B) to examine the impact of the LLM scale on\nperformance. Employing the same instruction data and train-\ning strategies, we fine-tune these models and present the\noutcomes in Fig. 3. The enhancements seen in LLaMA-\n13B in comparison to LLaMA-7B are minor. One possi-\nble explanation is that the LLaMA-7B has already achieved\nstrong performance. Despite achieving better outcomes with\nour model utilizing LLaMA-13B, we abstain from scaling\nup the model due to computational costs. Despite having a\ncomparable number of parameters, LLaMA-7B outperforms\nOPT-6.7B, thereby demonstrating the inherent strengths\nof LLaMA. Furthermore, Employing LLMs with smaller\nscales results in a decline in performance. When efficiently\nfine-tuning a small model with 20K instruction data, the re-\nsults reveal the restricted ability in knowledge retrieval ow-\ning to the insufficient scale of model parameters. Therefore,\nit is necessary to either employ large-scale language models\nor fully tune small models with more data.\nEffect of Constraint Strategies\nTo investigate the role of knowledge clues, we analyze the\neffect of constrained decoding on the recall metric. We pro-\npose four strategies with progressively relaxing constraints,\n1) directly generating sentences with constraints. 2) gener-\nating the first sentence in the document under constraints,\nthen using it as the identifier to look up the correspond-\ning document, similar to (Chen et al. 2022b). 3) generating\nknowledge clues as previously described. 4) generate uncon-\nstrained text.\nAs shown in Tab. 4, the first and last perform poor results\nfor two possible reasons. The one is that generating a long\ndocument from the first token is challenging due to insuffi-\ncient input. The other is that strong constraints may disrupt\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18738\nTextual Query Visual Query Predicted Knowledge Clues Relevant Documents\nWhat sport can you \nuse this for?\nmotocross is a form of off-road \nmotorcycle racing held on\nName the type of \nplant this is? \nWhat toy is this?\nDoc:19254.  \nmotocross is a form of off-road motorcycle \nracing held on enclosed off-road circuits.  the sport \nevolved from motorcycle trials competitions held in the \nUK. history 2 major competitions 2.1 fim motocross world \nchampionship 2.2 ama motocross championship......\nDoc:89997.  ... organic green living herbs gardentreespalm \ntrees palm trees by lovetoknow staff reviewed by sally \npainter \npalm trees are among the most exotic and \nrecognizable foliage on the planet.  there are nearly 3,000 \ndifferent species of palm trees throughout the world.  ...\npalm trees are among the most \nexotic and recognizable foliage\nDoc:11053.  ...  around the world, many children—and \nsome adults—have teddy bears, stuffed toys in the form \nof bears, named after the american statesman theodore \nroosevelt when in 1902 he had refused to shoot an \namerican black bear ...\nteddy bears, stuffed toys in the \nform of bears\nFigure 4: Case Study. Three cases from the OKVQA-GS112K dataset. Each predicted knowledge clue can be uniquely mapped\nto a document in the KB. The predicted knowledge clues that occur in corresponding documents are highlighted in yellow.\nGeneration Strategy R@5 R@10\nFull Document w/ Constraints 51.6 59.9\nFirst Sentence w/ Constraints 62.4 70.2\nKnowledge Clue w/ Constraints 78.6 86.2\nFree Text w/o Constraints 64.5 70.9\nTable 4: Results of different generation strategies.\nOnly Textual Query Only Visual Query\nUnited-States, racing, people, mountain,\nsport, American, building, black,\nSnowboarding, Olympics, white, statue,\nManhattan, Boeing, hand-painted, world,\nMcdonald’s, Baseball sunrise, lightning\nTable 5: Top 10 keywords in knowledge clues when using\nuni-modal queries.\nthe predicted distribution, whereas no constraint may lead to\nerroneous generation. Compared with the third, the second\nunderperforms by at least 14%, the possible explanation is\nthat the multi-modal query attends to the multiple aspects\nof knowledge, while the first sentence can not represent all\ninformation in the document. To verify the assumption, we\nrespectively count the occurrence of keywords in knowledge\nclues when using uni-modal queries as input. We sample 100\ndata points to visualize them in the Tab. 5\nAs shown in Tab 5, the generated keywords differ across\ndifferent modal inputs. Our model tends to produce knowl-\nedge clues that align with keywords in textual queries when\ntext alone is provided as input. Conversely, when images\nare the sole input, the generated knowledge clues encom-\npass more descriptive terms related to the images, such as at-\ntributes, colors, and objects. This observation highlights that\ndistinct modal queries focus on diverse aspects of knowl-\nedge, indicating why the static identifier yields unsatisfac-\ntory results in this task. Our model benefits from knowledge\nclues that can flexibly associate information from multiple\naspects and serve as dynamic identifiers.\nCase Study\nTo qualitatively illustrate why GeMKR works, we analyze\nthe prediction results on the OKVQA-GS112K dataset in\nFig. 4. We observe that (1) both textual and visual queries\nprovide useful features. As seen in the third example, the\nterm “toy” in the text is semantically correlated with the re-\ngion depicting a “teddy bear” in the image, indicating that\nfine-grained cross-modal correlations are important to un-\nderstand multi-modal queries. (2) Knowledge clues are free-\nformat text snippets with rich semantics that can appear at\nany position within a document. In contrast to static iden-\ntifiers (e.g. title and Docid), knowledge clues offer greater\nflexibility in representing a document, harnessing the gen-\nerative capabilities of LLMs without the need for addi-\ntional steps to memorize associations between knowledge\nand identifiers. This strategy enhances generalization for un-\nseen knowledge, potentially contributing to the effectiveness\nof our model.\nConclusion\nIn this paper, we are the first to introduce a generative\npipeline into multi-modal knowledge retrieval tasks, instead\nof discriminative retrievers, which ensemble multiple re-\ntrievers for separate modalities. Besides, we make use of in-\nherent knowledge within LLMs and design an efficient fine-\ntuning framework to align multi-grained visual features with\ntextual features and feed them into LLMs for efficient multi-\nmodal learning. Third, we propose a novel constraint decod-\ning strategy to utilize knowledge clues as dynamic identifiers\nfor generative decoding. Experiments on the three datasets\ndemonstrate the effectiveness of our model.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18739\nAcknowledgments\nThis work was supported by the National Key Research and\nDevelopment Program of China (2022ZD0160603).\nReferences\nBevilacqua, M.; Ottaviano, G.; Lewis, P. S. H.; Yih, S.;\nRiedel, S.; and Petroni, F. 2022. Autoregressive Search En-\ngines: Generating Substrings as Document Identifiers. In\nNeurIPS.\nChen, J.; Zhang, R.; Guo, J.; de Rijke, M.; Liu, Y .; Fan,\nY .; and Cheng, X. 2023. A Unified Generative Retriever\nfor Knowledge-Intensive Language Tasks via Prompt Learn-\ning. In Proceedings of the 46th International ACM SIGIR\nConference on Research and Development in Information\nRetrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023 ,\n1448–1457. ACM.\nChen, J.; Zhang, R.; Guo, J.; Fan, Y .; and Cheng, X. 2022a.\nGERE: Generative Evidence Retrieval for Fact Verification.\nIn SIGIR ’22: The 45th International ACM SIGIR Con-\nference on Research and Development in Information Re-\ntrieval, Madrid, Spain, July 11 - 15, 2022, 2184–2189.\nChen, J.; Zhang, R.; Guo, J.; Liu, Y .; Fan, Y .; and Cheng,\nX. 2022b. CorpusBrain: Pre-Train a Generative Retrieval\nModel for Knowledge-Intensive Language Tasks. In Pro-\nceedings of the 31st ACM International Conference on\nInformation Knowledge Management, CIKM ’22. ISBN\n9781450392365.\nChen, Z.; Cheng, X.; Dong, S.; Dou, Z.; Guo, J.; Huang,\nX.; Lan, Y .; Li, C.; Li, R.; Liu, T.; Liu, Y .; Ma, J.; Qin, B.;\nWang, M.; Wen, J.; Xu, J.; Zhang, M.; Zhang, P.; and Zhang,\nQ. 2021. Information retrieval: a view from the Chinese IR\ncommunity. Frontiers Comput. Sci., 15(1): 151601.\nDe Cao, N.; Izacard, G.; Riedel, S.; and Petroni, F. 2021.\nAutoregressive Entity Retrieval. In 9th International Con-\nference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nDiao, S.; Xu, T.; Xu, R.; Wang, J.; and Zhang, T.\n2023. Mixture-of-Domain-Adapters: Decoupling and In-\njecting Domain Knowledge to Pre-trained Language Mod-\nels’ Memories. In Rogers, A.; Boyd-Graber, J. L.; and\nOkazaki, N., eds., Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14, 2023,\n5113–5129. Association for Computational Linguistics.\nDing, N.; Hu, S.; Zhao, W.; Chen, Y .; Liu, Z.; Zheng, H.-T.;\nand Sun, M. 2022. OpenPrompt: An Open-source Frame-\nwork for Prompt-learning. In ACL System Demonstration.\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .; Hu, S.;\nChen, Y .; Chan, C.-M.; Chen, W.; Yi, J.; Zhao, W.; Liu, Z.;\nZheng, H.-T.; Chen, J.; Liu, Y .; Tang, J.; Li, J.; and Sun, M.\n2023. Parameter-efficient Fine-tuning of Large-scale Pre-\ntrained Language Models. Nature Machine Intelligence.\nFerragina, P.; and Manzini, G. 2000. Opportunistic Data\nStructures with Applications. In 41st Annual Symposium\non Foundations of Computer Science, FOCS 2000, 12-14\nNovember 2000, Redondo Beach, California, USA, 390–\n398. IEEE Computer Society.\nFerraretto, F.; Laitz, T.; Lotufo, R.; and Nogueira, R.\n2023. ExaRanker: Explanation-Augmented Neural Ranker.\narXiv:2301.10521.\nGao, F.; Ping, Q.; Thattai, G.; Reganti, A.; Wu, Y . N.;\nand Natarajan, P. 2022. A Thousand Words Are Worth\nMore Than a Picture: Natural Language-Centric Outside-\nKnowledge Visual Question Answering. arXiv:2201.05299.\nGao, L.; Ma, X.; Lin, J.; and Callan, J. 2023. Precise Zero-\nShot Dense Retrieval without Relevance Labels. InProceed-\nings of the 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers). Toronto,\nCanada.\nHan, X.; Zhao, W.; Ding, N.; Liu, Z.; and Sun, M. 2022.\nPTR: Prompt Tuning with Rules for Text Classification. In\nAI Open.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-\ntation of Large Language Models. arXiv:2106.09685.\nHuang, S.; Zhai, Y .; Long, X.; Jiang, Y .; Wang, X.; Zhang,\nY .; and Xie, P. 2022. DAMO-NLP at NLPCC-2022 Task 2:\nKnowledge Enhanced Robust NER for Speech Entity Link-\ning. In Natural Language Processing and Chinese Comput-\ning.\nJagerman, R.; Zhuang, H.; Qin, Z.; Wang, X.; and Bender-\nsky, M. 2023. Query Expansion by Prompting Large Lan-\nguage Models. arXiv:2305.03653.\nKarpukhin, V .; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,\nS.; Chen, D.; and Yih, W.-t. 2020a. Dense Passage Retrieval\nfor Open-Domain Question Answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 6769–6781. Online.\nKarpukhin, V .; Oguz, B.; Min, S.; Lewis, P. S. H.; Wu,\nL.; Edunov, S.; Chen, D.; and Yih, W. 2020b. Dense Pas-\nsage Retrieval for Open-Domain Question Answering. In\nProceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, 6769–6781. Association for Com-\nputational Linguistics.\nLi, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing\nContinuous Prompts for Generation. In Proceedings of the\n59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021, (Volume\n1: Long Papers), Virtual Event, August 1-6, 2021, 4582–\n4597. Association for Computational Linguistics.\nLi, Y .; Yang, N.; Wang, L.; Wei, F.; and Li, W. 2023. Multi-\nview Identifiers Enhanced Generative Retrieval. InProceed-\nings of the 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023, 6636–6648. Association\nfor Computational Linguistics.\nLuo, M.; Fang, Z.; Gokhale, T.; Yang, Y .; and Baral, C.\n2023a. End-to-end Knowledge Retrieval with Multi-modal\nQueries. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14, 2023,\n8573–8589. Association for Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18740\nLuo, M.; Fang, Z.; Gokhale, T.; Yang, Y .; and Baral, C.\n2023b. End-to-end Knowledge Retrieval with Multi-modal\nQueries. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), 8573–8589. Toronto, Canada: Association for\nComputational Linguistics.\nLuo, M.; Zeng, Y .; Banerjee, P.; and Baral, C.\n2021a. Weakly-Supervised Visual-Retriever-Reader\nfor Knowledge-based Question Answering. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November, 2021 ,\n6417–6431. Association for Computational Linguistics.\nLuo, M.; Zeng, Y .; Banerjee, P.; and Baral, C.\n2021b. Weakly-Supervised Visual-Retriever-Reader\nfor Knowledge-based Question Answering. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 6417–6431.\nMa, Z.; Li, J.; Li, G.; and Cheng, Y . 2022. UniTranSeR: A\nunified transformer semantic representation framework for\nmultimodal task-oriented dialog system. In Proceedings of\nthe 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 103–114.\nMa, Z.; Yu, Z.; Li, J.; and Li, G. 2023. HybridPrompt: bridg-\ning language models and human priors in prompt tuning for\nvisual question answering. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, 13371–13379.\nMarino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R.\n2019. OK-VQA: A Visual Question Answering Benchmark\nRequiring External Knowledge. InConference on Computer\nVision and Pattern Recognition (CVPR).\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu,\nX. 2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. arXiv:2306.08302.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021a. Learning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021b. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nMeila, M.; and Zhang, T., eds., Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML 2021,\n18-24 July 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, 8748–8763. PMLR.\nRobertson, S.; and Zaragoza, H. 2009a. The Probabilistic\nRelevance Framework: BM25 and Beyond. Found. Trends\nInf. Retr., 3(4): 333–389.\nRobertson, S. E.; and Zaragoza, H. 2009b. The Probabilistic\nRelevance Framework: BM25 and Beyond. Found. Trends\nInf. Retr., 3(4): 333–389.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing.\nTang, Y .; Zhang, R.; Guo, J.; Chen, J.; Zhu, Z.; Wang, S.;\nYin, D.; and Cheng, X. 2023. Semantic-Enhanced Differen-\ntiable Search Index Inspired by Learning Strategies. In Pro-\nceedings of the 29th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD 2023, Long Beach,\nCA, USA, August 6-10, 2023, 4904–4913. ACM.\nTay, Y .; Tran, V .; Dehghani, M.; Ni, J.; Bahri, D.; Mehta, H.;\nQin, Z.; Hui, K.; Zhao, Z.; Gupta, J. P.; Schuster, T.; Cohen,\nW. W.; and Metzler, D. 2022. Transformer Memory as a\nDifferentiable Search Index. In NeurIPS.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nWang, C.-Y .; Bochkovskiy, A.; and Liao, H.-Y . M. 2022.\nYOLOv7: Trainable bag-of-freebies sets new state-of-\nthe-art for real-time object detectors. arXiv preprint\narXiv:2207.02696.\nWang, Y .; Hou, Y .; Wang, H.; Miao, Z.; Wu, S.; Chen, Q.;\nXia, Y .; Chi, C.; Zhao, G.; Liu, Z.; Xie, X.; Sun, H.; Deng,\nW.; Zhang, Q.; and Yang, M. 2022. A Neural Corpus Indexer\nfor Document Retrieval. In NeurIPS.\nWu, C.; Zhang, R.; Guo, J.; Fan, Y .; and Cheng, X. 2023.\nAre Neural Ranking Models Robust? ACM Trans. Inf. Syst.,\n41(2): 29:1–29:36.\nYang, Z.; and Liu, Y . 2022. On Robust Prefix-Tuning for\nText Classification. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nZhang, K.; Ding, N.; Qi, B.; Zhu, X.; Long, X.; and Zhou,\nB. 2023a. CRaSh: Clustering, Removing, and Sharing En-\nhance Fine-tuning without Full Large Language Model. In\nProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing.\nZhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,\nH.; Gao, P.; and Qiao, Y . 2023b. LLaMA-Adapter: Efficient\nFine-tuning of Language Models with Zero-init Attention.\nCoRR, abs/2303.16199.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mi-\nhaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;\nKoura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.\n2022. OPT: Open Pre-trained Transformer Language Mod-\nels. arXiv:2205.01068.\nZhou, Y .; Dou, Z.; and Wen, J.-R. 2023. Enhancing Genera-\ntive Retrieval with Reinforcement Learning from Relevance\nFeedback. In Proceedings of the 2023 Conference on Em-\npirical Methods in Natural Language Processing.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. MiniGPT-4: Enhancing Vision-Language Under-\nstanding with Advanced Large Language Models. CoRR,\nabs/2304.10592.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18741"
}