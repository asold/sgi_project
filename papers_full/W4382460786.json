{
    "title": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification without Concrete Text Labels",
    "url": "https://openalex.org/W4382460786",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2115778751",
            "name": "Siyuan Li",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2089308352",
            "name": "Sun Li",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2168672653",
            "name": "Qingli Li",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2115778751",
            "name": "Siyuan Li",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2089308352",
            "name": "Sun Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2168672653",
            "name": "Qingli Li",
            "affiliations": [
                "East China Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4312825288",
        "https://openalex.org/W3112516115",
        "https://openalex.org/W2966094134",
        "https://openalex.org/W3081127035",
        "https://openalex.org/W2972012950",
        "https://openalex.org/W6602873311",
        "https://openalex.org/W2955854238",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W3194557739",
        "https://openalex.org/W2997351135",
        "https://openalex.org/W2947101691",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W2068042582",
        "https://openalex.org/W3156464220",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W3166362606",
        "https://openalex.org/W4207072681",
        "https://openalex.org/W1949591461",
        "https://openalex.org/W2470322391",
        "https://openalex.org/W2512434173",
        "https://openalex.org/W2922510913",
        "https://openalex.org/W4312749754",
        "https://openalex.org/W2433217581",
        "https://openalex.org/W3015218924",
        "https://openalex.org/W2981393440",
        "https://openalex.org/W2980046511",
        "https://openalex.org/W2924164130",
        "https://openalex.org/W3195399086",
        "https://openalex.org/W3215495159",
        "https://openalex.org/W2511791013",
        "https://openalex.org/W2783855081",
        "https://openalex.org/W3047929456",
        "https://openalex.org/W3011820993",
        "https://openalex.org/W3100506510",
        "https://openalex.org/W4285295127",
        "https://openalex.org/W2769994766",
        "https://openalex.org/W3088500069",
        "https://openalex.org/W6679857944",
        "https://openalex.org/W3112224994",
        "https://openalex.org/W4226053849",
        "https://openalex.org/W3093696348",
        "https://openalex.org/W3010766832",
        "https://openalex.org/W6687888618",
        "https://openalex.org/W6743440100",
        "https://openalex.org/W4226427271",
        "https://openalex.org/W6800895557",
        "https://openalex.org/W2943407549",
        "https://openalex.org/W4229066231",
        "https://openalex.org/W3045817950",
        "https://openalex.org/W4293177262",
        "https://openalex.org/W3217193091",
        "https://openalex.org/W2963047834",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3106883149",
        "https://openalex.org/W2204750386",
        "https://openalex.org/W4386790226",
        "https://openalex.org/W3143016713",
        "https://openalex.org/W3115484111",
        "https://openalex.org/W4312310776",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3206072662",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2990827756",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4226058394",
        "https://openalex.org/W2963842104",
        "https://openalex.org/W4312361652",
        "https://openalex.org/W2973183043",
        "https://openalex.org/W4287236877",
        "https://openalex.org/W3175823695",
        "https://openalex.org/W2984145721",
        "https://openalex.org/W71666767",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W2598634450",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W3109976024",
        "https://openalex.org/W2986093954",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2135442311",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W2984040540",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W3150864706",
        "https://openalex.org/W2997987796",
        "https://openalex.org/W4287554288",
        "https://openalex.org/W3034580371",
        "https://openalex.org/W3162540599",
        "https://openalex.org/W2967515867",
        "https://openalex.org/W4287330514"
    ],
    "abstract": "Pre-trained vision-language models like CLIP have recently shown superior performances on various downstream tasks, including image classification and segmentation. However, in fine-grained image re-identification (ReID), the labels are indexes, lacking concrete text descriptions. Therefore, it remains to be determined how such models could be applied to these tasks. This paper first finds out that simply fine-tuning the visual model initialized by the image encoder in CLIP, has already obtained competitive performances in various ReID tasks. Then we propose a two-stage strategy to facilitate a better visual representation. The key idea is to fully exploit the cross-modal description ability in CLIP through a set of learnable text tokens for each ID and give them to the text encoder to form ambiguous descriptions. In the first training stage, image and text encoders from CLIP keep fixed, and only the text tokens are optimized from scratch by the contrastive loss computed within a batch. In the second stage, the ID-specific text tokens and their encoder become static, providing constraints for fine-tuning the image encoder. With the help of the designed loss in the downstream task, the image encoder is able to represent data as vectors in the feature embedding accurately. The effectiveness of the proposed strategy is validated on several datasets for the person or vehicle ReID tasks. Code is available at https://github.com/Syliz517/CLIP-ReID.",
    "full_text": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-identification\nwithout Concrete Text Labels\nSiyuan Li1, Li Sun1,2, Qingli Li1\n1 Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University\n2 Key Laboratory of Advanced Theory and Application in Statistics and Data Science, East China Normal University\nsunli@ee.ecnu.edu.cn\nAbstract\nPre-trained vision-language models like CLIP have recently\nshown superior performances on various downstream tasks,\nincluding image classification and segmentation. However,\nin fine-grained image re-identification (ReID), the labels are\nindexes, lacking concrete text descriptions. Therefore, it re-\nmains to be determined how such models could be applied to\nthese tasks. This paper first finds out that simply fine-tuning\nthe visual model initialized by the image encoder in CLIP, has\nalready obtained competitive performances in various ReID\ntasks. Then we propose a two-stage strategy to facilitate a bet-\nter visual representation. The key idea is to fully exploit the\ncross-modal description ability in CLIP through a set of learn-\nable text tokens for each ID and give them to the text encoder\nto form ambiguous descriptions. In the first training stage,\nimage and text encoders from CLIP keep fixed, and only the\ntext tokens are optimized from scratch by the contrastive loss\ncomputed within a batch. In the second stage, the ID-specific\ntext tokens and their encoder become static, providing con-\nstraints for fine-tuning the image encoder. With the help of\nthe designed loss in the downstream task, the image encoder\nis able to represent data as vectors in the feature embedding\naccurately. The effectiveness of the proposed strategy is vali-\ndated on several datasets for the person or vehicle ReID tasks.\nCode is available at https://github.com/Syliz517/CLIP-ReID.\nIntroduction\nImage re-identification (ReID) aims to match the same ob-\nject across different and non-overlapping camera views. Par-\nticularly, it focuses on detecting the same person or vehicle\nin the surveillance camera networks. ReID is a challenging\ntask mainly due to the cluttered background, illumination\nvariations, huge pose changes, or even occlusions. Most re-\ncent ReID models depend on building and training a convo-\nlution neural network (CNN) so that each image is mapped\nto a feature vector in the embedding space before the classi-\nfier. Images of the same object tend to be close, while differ-\nent objects become far away in this space. The parameters of\nCNN can be effectively learned under the guidance of cross\nentropy loss together with the typical metric learning loss\nlike center or triplet loss (Hermans, Beyer, and Leibe 2017).\nAlthough CNN-based models for ReID have achieved\ngood performance on some well-known datasets, it is still far\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Stage1 (b) Stage2\nText features after stage1\nInitialized text features Original image features\nImage features after stage2\nFigure 1: t-SNE visualization on image and text features\n(Van der Maaten and Hinton 2008). Randomly selected 10\npersons in the MSMT17 are represented by different colors.\nThe dots and pentagons indicate the image and text features,\nrespectively. (a) and (b) show the data distributions after the\nfirst and second training stage.\nfrom being used in a real application. CNN is often blamed\nfor only focusing on a small irrelevant region in the image,\nwhich indicates that its feature is not robust and discrimina-\ntive enough. Recently, vision transformers like ViT (Doso-\nvitskiy et al. 2020) have become popular in many tasks, and\nthey have also shown better performances in ReID. Com-\npared to CNN, transformers can model the long-range de-\npendency in the whole image. However, due to a large num-\nber of model parameters, they require a big training set and\noften perform erratically during optimization. Since ReID\ndatasets are relatively small, the potential of these models is\nnot fully exploited yet.\nBoth CNN-based and ViT-based methods heavily rely on\npre-training. Almost all ReID methods need an initial model\ntrained on ImageNet, which contains images manually given\none-hot labels from a pre-defined set. Visual contents de-\nscribing rich semantics outside the set are completely ig-\nnored. Recently, cross-modal learning like CLIP (Radford\net al. 2021) connects the visual representation with its cor-\nresponding high-level language description. They not only\ntrain on a larger dataset but also change the pre-training\ntask, matching visual features to their language descriptions.\nTherefore, the image encoder can sense a variety of high-\nlevel semantics from the text and learn transferable features,\nwhich can be adapted to many different tasks. E.g., given\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1405\nText\nencoder\n…\n…\nStage1\nP P . . .P [class].\nA photo of a dog.\nlearnable\nText featuresImage features Similarity scores of unmatched samplesSimilarity scores of matched samples\n…\n… …\n(a) CLIP\n(b) CoOp (c) CLIP-ReID\n      A photo of a X X . . .X  person.\nStage2\nImage\nencoder\nText\nencoder\nImage\nencoder\nA photo of a person.X X . . .X\nlearnable\nText\nencoder\n…\n…\nImage\nencoder\n…\n… …\nText\nencoder\nImage\nencoder\nℒ ℒ ℒ ℒ\nℒ ℒℒ ℒ\nFigure 2: Overview of our approach compared to CLIP and CoOp. (a) describes the model of CLIP, using pairs of text and image\nto train the image encoder and text encoder. (b) shows the model of CoOp, which fixes the image encoder and text encoder\nand fine-tunes text prompt in the downstream dataset. (c) is our proposed CLIP-ReID method, which fixes the text encoder and\nimage encoder in the first training stage, optimizes a set of learnable text tokens to generate the text features, and then uses the\ntext features to optimize the image encoder in the second training stage.\na particular image classification task, the candidate text la-\nbels are concrete and can be combined with a prompt, such\nas “A photo of a”, to form the text descriptions. The classi-\nfication is then realized by comparing image features with\ntext features generated by the text encoder, which takes the\ntext description of categories as input. Note that it is a zero-\nshot solution without tuning any parameters for downstream\ntasks but still gives satisfactory results. Based on this, CoOp\n(Zhou et al. 2022b) incorporates a learnable prompt for dif-\nferent tasks. The optimized prompt further improves the per-\nformance.\nCLIP and CoOp need text labels to form text descrip-\ntions in downstream tasks. However, in most ReID tasks,\nthe labels are indexes, and there are no specific words to\ndescribe the images, so the vision-language model has not\nbeen widely adopted in ReID. In this paper, we intend to ex-\nploit CLIP fully. We first fine-tune the image encoder by di-\nrectly using the common losses in ReID, which has already\nobtained high metrics compared to existing works. We use\nthis model as our baseline and try to improve it by utiliz-\ning the text encoder in CLIP. A two-stage strategy is pro-\nposed, which aims to constrain the image encoder by gener-\nating language descriptions from the text encoder. A series\nof learnable text tokens are incorporated, and they are used\nto describe each ID ambiguously. In the first training stage,\nboth the image and text encoder are fixed, and only these to-\nkens are optimized. In the second stage, the description to-\nkens and text encoder keep static, and they together provide\nambiguous descriptions for each ID, which helps to build up\nthe cross-modality image to text cross-entropy loss. Since\nCLIP has CNN-based and ViT-based models, the proposed\nmethod is validated on both ResNet-50 and ViT-B/16. The\ntwo types of the model achieve the state-of-the-art on differ-\nent ReID datasets. Moreover, our method can also support\nthe input of camera ID and overlapped token settings in its\nViT-based version.\nFig. 1 simultaneously visualizes image and text features\nin 2D coordinates, which could help to understand our train-\ning strategy. In the first stage, the text feature of each ID is\nadapted to its corresponding image features, making it be-\ncome ambiguous descriptions. In the second stage, image\nfeatures gather around their text descriptions so that image\nfeatures from different IDs become distant.\nIn summary, the contributions of this paper lie in the fol-\nlowing aspects:\n• To our knowledge, we are the first to utilize CLIP for per-\nson/vehicle ReID. We provide competitive baseline mod-\nels on several ReID datasets, which are the result of fine-\ntuning the visual model initialized by the CLIP image\nencoder.\n• We propose the CLIP-ReID, which fully exploits the\ncross-modal describing ability of CLIP. In our model, the\nID-specific learnable tokens are incorporated to give am-\n1406\nbiguous text descriptions, and a two-stage training strat-\negy is designed to take full advantage of the text encoder\nduring training.\n• We demonstrate that CLIP-ReID has achieved state-of-\nthe-art performances on many ReID datasets, including\nboth person and vehicle.\nRelated Works\nImage ReID\nPrevious ReID works focus on learning discriminative fea-\ntures like foreground histograms (Das, Chakraborty, and\nRoy-Chowdhury 2014), local maximal occurrences (Liao\net al. 2015), bag-of-visual words (Zheng et al. 2015), or\nhierarchical Gaussian descriptors (Matsukawa et al. 2016).\nOn the other hand, it can also be solved as a metric learn-\ning problem, expecting a reasonable distance measurement\nfor inter- and intra-class samples (Koestinger et al. 2012).\nThese two aspects are naturally combined by the deep neu-\nral network (Yi et al. 2014), in which the parameters are op-\ntimized under an appropriate loss function with almost no\nintentional interference. Particularly, with the scale develop-\nment of CNN on ImageNet, ResNet-50 (He et al. 2016) has\nbeen regarded as the common model (Luo et al. 2019) for\nmost ReID datasets.\nDespite the powerful ability of CNN, it is blamed for\nits irrelevant highlighted regions, which is probably due to\nthe overfitting of limited training data. OSNet (Zhou et al.\n2019) gives a lightweight model to deal with it. Auto-ReID\n(Quan et al. 2019) and CDNet (Li, Wu, and Zheng 2021)\nemploy network architecture search for a compact model.\nOfM (Zhang et al. 2021a) proposes a data selection method\nfor learning a sampler to choose generalizable data during\ntraining. Although they obtain good results on some small\ndatasets, performances drop significantly on large ones like\nMSMT17.\nIntroducing prior knowledge into the network can also al-\nleviate overfitting. An intuitive idea is to use features from\ndifferent regions for identification. PCB (Sun et al. 2018)\nand SAN (Qian et al. 2020) divides the feature into hor-\nizontal stripes to enhance its ability to represent the local\nregion. MGN (Wang et al. 2018) utilizes a multiple granu-\nlarity scheme on feature division to enhance its expressive\ncapabilities further, and it has several branches to capture\nfeatures from different parts. Therefore, model complexity\nbecomes its major issue. BDB (Dai et al. 2019) has a sim-\nple structure with only two branches, one for global features\nand the other for local features, which employs a simple\nbatch feature drop strategy to randomly erase a horizontal\nstripe for all samples within a batch. CBDB-Net (Tan et al.\n2021) enhances BDB with more types of feature dropping.\nSimilar multi-branch approaches (Zhang et al. 2021c; Wang\net al. 2022; Zhang et al. 2021b; He et al. 2019; Zhang et al.\n2019; Sun et al. 2020) with the purpose of mining rich fea-\ntures from different locations are also proposed, and they can\nbe improved if the semantic parsing map participates during\ntraining (Jin et al. 2020b; Zhu et al. 2020; Meng et al. 2020;\nChen et al. 2020).\nAttention enlarges the receptive field, hence is another\nway to prevent the model from focusing on small areas. In\nRGA (Zhang et al. 2020), non-local attention is performed\nalong spatial and channel directions. ABDNet (Chen et al.\n2019) adopts a similar attention module and adds a reg-\nularization term to ensure feature orthogonality. HOReID\n(Wang et al. 2020) extends the traditional attention into\nhigh-order computation, giving more discriminative fea-\ntures. CAL (Rao et al. 2021) provides an attention scheme\nfor counterfactual learning, which filters out irrelevant ar-\neas and increases prediction accuracy. Recently, due to the\npower of the transformer, it has become popular in ReID.\nPAT (Li et al. 2021b) and DRL-Net (Jia et al. 2022) build on\nResNet-50, but they utilize a transformer decoder to exploit\nimage features from CNN. In the decoder attention block,\nlearnable queries first interact with keys from the image and\nthen are updated by weighted image values. They are ex-\npected to reflect local features for ReID. TransReID (He\net al. 2021), AAformer (Zhu et al. 2021) and DCAL (Zhu\net al. 2022) all use encoder attention blocks in ViT, and they\nobtain better performance, especially on the large dataset.\nThis paper implements both CNN and ViT models initial-\nized from CLIP. Benefiting from the two-stage training, both\nachieve SOTA on different datasets.\nVision-Language Learning\nCompared to supervised pre-training on ImageNet, vision-\nlanguage pre-training (VLP) has significantly improved the\nperformance of many downstream tasks by training to match\nimage and language. CLIP (Radford et al. 2021) and ALIGN\n(Jia et al. 2021) are good practices, which utilize a pair\nof image and text encoders, and two directional InfoNCE\nlosses computed between their outputs for training. Built on\nCLIP, several works (Li et al. 2022; Kim, Son, and Kim\n2021) have been proposed to incorporate more types of\nlearning tasks like image-to-text matching and mask image/-\ntext modeling. ALBEF (Li et al. 2021a) aligns the image and\ntext representation before fusing them through cross-model\nattention. SimVLM (Wang et al. 2021) uses a single prefix\nlanguage modeling objective for end-to-end training.\nInspired by the recent advances in NLP, prompt or\nadapter-based tuning becomes prevalent in vision domain\nCoOp (Zhou et al. 2022b) proposes to fit in a learn-\nable prompt for image classification. CoCoOp (Zhou et al.\n2022a) learns a light-weight visual network to give meta to-\nkens for each image, combined with a set of learnable con-\ntext vectors. CLIP-Adapter (Gao et al. 2021) adds a light-\nweight module on top of both image and text encoder.\nIn addition, researchers investigate different downstream\ntasks to apply CLIP. DenseCLIP (Rao et al. 2022) and\nMaskCLIP (Zhou, Loy, and Dai 2021) apply it for per-pixel\nprediction in segmentation. ViLD (Gu et al. 2021) adapts\nimage and text encoders in CLIP for object detection. EI-\nCLIP (Ma et al. 2022) and CLIP4CirDemo (Baldrati et al.\n2022) use CLIP to solve retrieval problems. However, as far\nas we know, no works deal with person/vehicle ReID based\non CLIP.\n1407\nAlgorithm 1: CLIP-ReID’s training process.\nInput: batch of images xi and their corresponding texts tyi .\nParameter: a set of learnable text tokens[X]m(m ∈ 1, ...M)\nfor all IDs existing in training set X, an image encoder I\nand a text encoder T , linear layers gV and gT .\n1: Initialize I, T , gV and gT from the pre-trained CLIP.\nInitialize [X]m(m ∈ 1, ...M) randomly.\n2: while in the 1st stage do\n3: s(Vi, Tyi ) =gV (I(xi)) · gT (T (tyi ))\n4: Optimize [X]m by Eq. (5).\n5: end while\n6: for yi = 1to N do\n7: textyi = gT (T (tyi ))\n8: end for\n9: while in the 2nd stage do\n10: s(Vi, Tyi ) =gT (I(xi)) · textyi\n11: Optimize I by Eq. (9).\n12: end while\nMethod\nPreliminaries: Overview of CLIP\nWe first briefly review CLIP. It consists of two encoders, an\nimage encoder I(·) and a text encoder T (·). The architec-\nture of I(·) has several alternatives. Basically, a transformer\nlike ViT-B/16 and a CNN like ResNet-50 are two models we\nwork on. Either of them is able to summarize the image into\na feature vector in the cross-modal embedding.\nOn the other hand, the text encoder T (·) is implemented\nas a transformer, which is used to generate a representation\nfrom a sentence. Specifically, given a description such as\n“A photo of a [class].” where [class] is generally replaced\nby concrete text labels. T (·) first converts each word into a\nunique numeric ID by lower-cased byte pair encoding (BPE)\nwith 49,152 vocab size (Sennrich, Haddow, and Birch 2015).\nThen, each ID is mapped to a 512-d word embedding. To\nachieve parallel computation, each text sequence has a fixed\nlength of 77, including the start [SOS] and end [EOS] to-\nkens. After a 12-layer model with 8 attention heads, the\n[EOS] token is considered as a feature representation of the\ntext, which is layer normalized and then linearly projected\ninto the cross-modal embedding space.\nSpecifically, i ∈ {1...B} denotes the index of the images\nwithin a batch. Let imgi be the [CLS] token embedding of\nimage feature, while texti is the corresponding [EOS] to-\nken embedding of text feature, then compute the similarity\nbetween imgi and texti:\ns(Vi, Ti) =Vi · Ti = gV (imgi) · gT (texti) (1)\nwhere gV (·) and gV (·) are linear layers projecting embed-\nding into a cross-modal embedding space. The image-to-text\ncontrastive loss Li2t is calculated as:\nLi2t(i) =−log exp(s(Vi, Ti))\nPB\na=1 exp(s(Vi, Ta))\n(2)\nand the text-to-image contrastive loss Lt2i:\nLt2i(i) =−log exp(s(Vi, Ti))PB\na=1 exp(s(Va, Ti))\n(3)\nwhere numerators in Eq. (2) and Eq. (3) are the similarities\nof two embeddings from matched pair, and the denominators\nare all similarities with respect to anchor Vi or Ti.\nFor regular classification tasks, CLIP converts the con-\ncrete labels of the dataset into text descriptions, then pro-\nduces embedding feature Ti , Vi and aligns them. CoOp in-\ncorporates a learnable prompt for different tasks while en-\ntire pre-trained parameters are kept fixed, as depicted in\nFig. 2(b). However, it is difficult to exploit CLIP in ReID\ntasks where the labels are indexes instead of specific text.\nCLIP-ReID\nTo deal with the above problem, we propose CLIP-ReID,\nwhich complements the lacking textual information by pre-\ntraining a set of learnable text tokens. As is shown in\nFig. 2(c), our scheme is built by pre-trained CLIP with the\ntwo stages of training, and its metrics exceed our baseline.\nThe first training stage. We first introduce ID-specific\nlearnable tokens to learn ambiguous text descriptions, which\nare independent for each ID. Specifically, the text de-\nscriptions fed into T (·) are designed as “A photo of a\n[X]1[X]2[X]3...[X]M person/vehicle”, where each [X]m(m ∈\n1, ...M) is a learnable text token with the same dimension as\nword embedding. M indicates the number of learnable text\ntokens. In this stage, we fix the parameters of I(·) and T (·),\nand only tokens [X]m are optimized.\nSimilar to CLIP, we use Li2t and Lt2i, but replace texti\nwith textyi in Eq. (1), since each ID shares the same text\ndescription. Moreover, for Lt2i, different images in a batch\nprobably belong to the same person, so Tyi may have more\nthan one positive, we change it to:\nLt2i(yi) = −1\n|P(yi)|\nX\np∈P(yi)\nlog exp(s(Vp, Tyi ))PB\na=1 exp(s(Va, Tyi ))\n(4)\nHere, P(yi) ={p ∈ 1...B : yp = yi} is the set of indices of\nall positives for Tyi in the batch, and | · |is its cardinality.\nBy minimizing the loss of Li2t and Lt2i, the gradients\nare back-propagated through the fixed T (·) to optimize\n[X]1[X]2[X]3...[X]M, taking full advantage of T (·).\nLstage1 = Li2t + Lt2i (5)\nTo improve the computation efficiency, we obtain all the\nimage features by feeding the whole training set into I(·) at\nthe beginning of the first training stage. For a dataset with\nN IDs, we save N different Tyi of all IDs at the end of this\nstage, preparing for the next stage of training.\nThe second training stage. In this stage, only parameters\nin I(·) are optimized. To boost the final performance, we\nfollow the general strong pipeline of object ReID (Luo et al.\n2019). We employ the ID loss Lid and triplet loss Ltri with\nlabel smoothing for optimization, they are calculated as:\nLid =\nNX\nk=1\n−qk log(pk) (6)\nLtri = max(dp − dn + α, 0) (7)\n1408\nwhere qk = (1−ϵ)δk,y +ϵ/N denotes value in the target dis-\ntribution, and pk represents ID prediction logits of class k,\ndp and dn are feature distances of positive pair and negative\npair, while α is the margin of Ltri.\nTo fully exploit CLIP, for each image, we can use the text\nfeatures obtained in the first training stage to calculate image\nto text cross-entropy Li2tce as is shown in Eq. (8). Note that\nfollowing Lid, we utilize label smoothing on qk in Li2tce.\nLi2tce(i) =\nNX\nk=1\n−qk log exp(s(Vi, Tyk ))PN\nya=1 exp(s(Vi, Tya ))\n(8)\nUltimately, the losses used in our second training stage\nare summarized as follows:\nLstage2 = Lid + Ltri + Li2tce (9)\nThe whole training process of the proposed CLIP-ReID,\nincluding both the first and second stages, is summarized\nin Algorithm 1. We use the learnable prompts to mine and\nstore the hidden states of the pre-trained image encoder and\ntext encoder, allowing CLIP to retain its own advantages.\nDuring the second stage, these prompts can regularize the\nimage encoder and thus increase its generalization ability.\nSIE and OLP. To make the model aware of the camera or\nviewpoint, we use Side Information Embeddings (SIE) (He\net al. 2021) to introduce relevant information. Unlike Tran-\nsReID, we only add camera information to the [CLS] to-\nken, rather than all tokens, to avoid disturbing image details.\nOverlapping Patches (OLP) can further enhance the model\nwith increased computational resources, which is realized\nsimply by changing the stride in the token embedding.\nExperiments\nDatasets and Evaluation Protocols\nWe evaluate our method on four person re-identification\ndatasets, including MSMT17 (Wei et al. 2018), Market-1501\n(Zheng et al. 2015), DukeMTMC-reID (Ristani et al. 2016),\nOccluded-Duke (Miao et al. 2019), and two vehicle ReID\ndatasets, VeRi-776 (Liu et al. 2016b) and VehicleID (Liu\net al. 2016a). The details of these datasets are summarized in\nTab. 1. Following common practices, we adapt the cumula-\ntive matching characteristics (CMC) at Rank-1 (R1) and the\nmean average precision (mAP) to evaluate the performance.\nImplementations\nModels. We adopt the visual encoder I(·) and the text en-\ncoder T (·) from CLIP as the backbone for our image and\ntext feature extractor. CLIP provides two alternatives I(·),\nnamely a transformer and a CNN with a global attention\npooling layer. For the transformer, we choose the ViT-B/16,\nwhich contains 12 transformer layers with the hidden size\nof 768 dimensions. To match the output of the T (·), the di-\nmension of the image feature vector is reduced from 768 to\n512 by a linear layer. For the CNN, we choose ResNet-50,\nwhere the last stride changes from 2 to 1, resulting in a larger\nfeature map to preserve spatial information. The global at-\ntention pooling layer after ResNet-50 reduces the dimension\nof the embedding vectors from 2048 to 1024, matching the\ndimensions of the text features converted from 512 to 1024.\nDataset Image ID Cam + View\nMSMT17 126,441 4,101 15\nMarket-1501 32,668 1,501 6\nDukeMTMC-reID 36,411 1,404 8\nOccluded-Duke 35,489 1,404 8\nVeRi-776 49,357 776 28\nVehicleID 221,763 26,267 -\nTable 1: Statistics of datasets used in the paper.\nTraining details. In the first training stage, we use the\nAdam optimizer for both the CNN-based and the ViT-based\nmodels, with a learning rate initialized at 3.5 × 10−4 and\ndecayed by a cosine schedule. At this stage, the batch size is\nset to 64 without using any augmentation methods. Only the\nlearnable text tokens [X]1[X]2[X]3...[X]M are optimizable.\nIn the second training stage (same as our baseline), Adam\noptimizer is also used to train the image encoder. Each mini-\nbatch consists of B = P×K images, where P is the number\nof randomly selected identities, and K is samples per iden-\ntity. We take P = 16and K = 4. Each image is augmented\nby random horizontal flipping, padding, cropping and eras-\ning (Zhong et al. 2020). For the CNN-based model, we\nspend 10 epochs linearly increasing the learning rate from\n3.5 × 10−6 to 3.5 × 10−4, and then the learning rate is de-\ncayed by 0.1 at the 40th and 70th epochs. For the ViT-based\nmodel, we warm up the model for 10 epochs with a linearly\ngrowing learning rate from 5 × 10−7 to 5 × 10−6. Then, it\nis decreased by a factor of 0.1 at the 30th and 50th epochs.\nWe train the CNN-based model for 120 epochs while the\nViT-based model for 60 epochs. For the CNN-based model,\nwe use Ltri and Lid pre and post the global attention pooling\nlayer, and α is set to 0.3. Similarly, we use them pre and post\nthe linear layer after the transformer. Note that we also em-\nploy Ltri after the 11th transformer layer of ViT-B/16 and\nthe 3rd residual layer of ResNet-50.\nComparison with State-of-the-Art Methods\nWe compare our method with the state-of-the-art methods on\nthree widely used person ReID benchmarks, one occluded\nReID benchmark in Tab. 2, and two vehicle ReID bench-\nmarks in Tab. 3. Despite being simple, CLIP-ReID achieves\na strikingly good result. Note that all results listed here are\nwithout re-ranking.\nPerson ReID. For both CNN-based and ViT-based meth-\nods, CLIP-ReID outperforms previous methods by a large\nmargin on the most challenging dataset, MSMT17. Our\nmethod achieves 63.0% mAP and 84.4% R1 on the CNN-\nbased backbone, and 73.4% mAP and 88.7% R1 (6.0% and\n3.4% higher than Transreid+SIE+OLP) on the ViT-based\nbackbone using only the CLIP-ReID method, in further use\nof SIE and OLP we can improve mAP and R1 to 75.8% and\n89.7%. On other smaller or occluded datasets, such as Mar-\nket1501, DukeMTMC-reID, and Occluded-Duke, we also\nincrease the mAP with the ViT-based backbone by 1.0%,\n0.5% and 1.1%, respectively.\n1409\nBackbone Methods References MSMT17 Market-1501 DukeMTMC Occluded-Duke\nmAP R1 mAP R1 mAP R1 mAP R1\nCNN\nPCB* ECCV (2018) - - 81.6 93.8 69.2 83.3 - -\nMGN* MM (2018) - - 86.9 95.7 78.4 88.7 - -\nOSNeT ICCV (2019) 52.9 78.7 84.9 94.8 73.5 88.6 - -\nABD-Net* ICCV (2019) 60.8 82.3 88.3 95.6 78.6 89.0 - -\nAuto-ReID* ICCV (2019) 52.5 78.2 85.1 94.5 - - - -\nHOReID CVPR (2020) - - 84.9 94.2 75.6 86.9 43.8 55.1\nISP ECCV (2020) - - 88.6 95.3 80.0 89.6 52.3 62.8\nSAN AAAI (2020b) 55.7 79.2 88.0 96.1 75.5 87.9 - -\nOfM AAAI (2021a) 54.7 78.4 87.9 94.9 78.6 89.0 - -\nCDNet CVPR (2021) 54.7 78.9 86.0 95.1 76.8 88.6 - -\nPAT CVPR (2021b) - - 88.0 95.4 78.2 88.8 53.6 64.5\nCAL* ICCV (2021) 56.2 79.5 87.0 94.5 76.4 87.2 - -\nCBDB-Net* TCSVT (2021) - - 85.0 94.4 74.3 87.7 38.9 50.9\nALDER* TIP (2021b) 59.1 82.5 88.9 95.6 78.9 89.9 - -\nLTReID* TMM (2022) 58.6 81.0 89.0 95.9 80.4 90.5 - -\nDRL-Net TMM (2022) 55.3 78.4 86.9 94.7 76.6 88.1 50.8 65.0\nbaseline 60.7 82.1 88.1 94.7 79.3 88.6 47.4 54.2\nCLIP-ReID 63.0 84.4 89.8 95.7 80.7 90.0 53.5 61.0\nViT\nAAformer* arxiv (2021) 63.2 83.6 87.7 95.4 80.0 90.1 58.2 67.0\nTransReID+SIE+OLP ICCV (2021) 67.4 85.3 88.9 95.2 82.0 90.7 59.2 66.4\nTransReID+SIE+OLP* 69.4 86.2 89.5 95.2 82.6 90.7 - -\nDCAL CVPR (2022) 64.0 83.1 87.5 94.7 80.1 89.0 - -\nbaseline 66.1 84.4 86.4 93.3 80.0 88.8 53.5 60.8\nCLIP-ReID 73.4 88.7 89.6 95.5 82.5 90.0 59.5 67.1\nCLIP-ReID+SIE+OLP 75.8 89.7 90.5 95.4 83.1 90.8 60.3 67.2\nTable 2: Comparison with state-of-the-art CNN- and ViT- based methods on person ReID datasets. DukeMTMC denotes the\nDukeMTMC-reID benchmark. The superscript star* means that the input image is resized to a resolution larger than 256x128.\nBack\n-bone Methods VeRi-776 VehicleID\nmAP R1 R1 R5\nCNN\nPRN (2019) 74.3 94.3 78.4 92.3\nPGAN (2019) 79.3 96.5 77.8 92.1\nSAN (2020) 72.5 93.3 79.7 94.3\nUMTS (2020a) 75.9 95.8 80.9 -\nSPAN (2020) 68.9 94.0 - -\nPVEN (2020) 79.5 95.6 84.7 97.0\nSA VER (2020) 79.6 96.4 79.9 95.2\nCFVMNet (2020) 77.1 95.3 81.4 94.1\nCAL (2021) 74.3 95.4 82.5 94.7\nEIA-Net (2018) 79.3 95.7 84.1 96.5\nFIDI (2021) 77.6 95.7 78.5 91.9\nbaseline 79.3 95.7 84.4 96.6\nCLIP-ReID 80.3 96.8 85.2 97.1\nViT\nTransReID (2021) 80.6 96.9 83.6 97.1\nTransReID! 82.0 97.1 85.2 97.5\nDCAL (2022) 80.2 96.9 - -\nbaseline 79.3 95.7 84.2 96.6\nCLIP-ReID 83.3 97.4 85.3 97.6\nCLIP-ReID! 84.5 97.3 85.5 97.2\nTable 3: Comparison with state-of-the-art CNN- and ViT-\nbased methods on vehicle ReID datasets. Only the small\nsubset of VehicleID is used in this paper. ! indicates that the\nmethod further uses SIE and OLP on VeRi-776 and OLP on\nVehicleID.\nVehicle ReID. Our method achieves competitive perfor-\nmance compared to the prior CNN-based and ViT-based\nmethods. With the ViT-based backbone, CLIP-ReID reaches\n85.3% mAP and 97.6% R1 on VehicleID, while CLIP-ReID!\nreaches 84.5% mAP and 97.3% R1 on VeRi-776.\nAblation Studies and Analysis\nWe conduct comprehensive ablation studies on MSMT17\ndataset to analyze the influences and sensitivity of some ma-\njor parameters.\nBaseline comparison. Many CNN-based works are based\non the strong baseline proposed by BoT (Luo et al. 2019).\nFor ViT-based methods, TransReID’s baseline is widely\nadopted, while AAformer also proposes a baseline. Al-\nthough slightly different, both of them are pre-trained on\nImageNet, which is different from ours. As shown in Tab. 4,\ndue to the effectiveness of CLIP pre-training, our baseline\nachieves superior performance compared to other baselines.\nNecessity of two-stage training. CLIP aligns embeddings\nfrom text and image domains, so it is important to exploit\nits text encoder. Since ReID has no specific text that distin-\nguishes different IDs, we aim to provide this by pre-training\na set of learnable text tokens. There are two ways to opti-\nmize them. One is one-stage training, in which we train the\nimage encoder I(·) while using contrastive loss to train the\ntext tokens at the same time. The other is the two-stage that\n1410\nBackbones Methods mAP Rank-1\nCNN BoT 51.3 75.3\nCLIP-ReID baseline 60.7 82.1\nViT\nAAformer baseline 58.5 79.4\nTransReID baseline 61.0 81.8\nCLIP-ReID baseline 66.1 84.4\nTable 4: Comparison of baselines on the MSMT17 dataset.\nBackbone Methods mAP Rank-1\nCNN\nbaseline 60.7 82.1\none stage 61.9 82.8\ntwo stage 63.0 84.4\nViT\nbaseline 66.1 84.4\none stage 68.9 85.9\ntwo stage 73.4 88.7\nTable 5: Comparison between one- and two-stage training.\nwe propose, in which we tune the learnable text tokens in\nthe first stage and use them to calculate theLi2tce in the sec-\nond stage. To verify which approach is more effective, we\nperform a comparison on MSMT17. As shown in Tab. 5, the\none-stage training is less effective because, in the early stage\nof training, learnable text tokens cannot describe the image\nwell but affects the optimization of I(·).\nConstraint from text encoder in the second stage. There\nare P different IDs in a batch, with K images per ID. When\ncomputing Li2tce, if we only consider text embeddings for\nthe IDs within a batch, like Li2t, the number of participat-\ning IDs is much less than the total number of IDs as in Lid.\nWe extend it to all IDs in the training set, like Li2tce. From\nTab. 6, we can conclude that comparing with all IDs in the\ntraining set is better than only comparing with the IDs of the\ncurrent batch. Another conclusion is that Lt2i is not neces-\nsary in the second stage. Finally, we combine the Lid, Ltri,\nLi2tce to form the total loss. For the ViT, the weights of the\nthree loss terms are 0.25, 1, and 1, respectively, while they\nare 1, 1, and 1 for the CNN.\nNumber of learnable tokens M. To be consistent with\nCLIP, we set the text description to “A photo of a\n[X]1[X]2[X]3...[X]M person/vehicle.”. We conduct analysis\non the parameter M and find that M = 1results in not learn-\ning sufficient text description, but whenM is added to 8, it is\nredundant and unhelpful. We finally choose M = 4, which\ngives the best result among different settings.\nSIE and OLP. In Tab. 7, we evaluate the effectiveness of\nSIE and OLP on MSMT17. Using SIE only for [CLS] to-\nkens works better than adding it for all global tokens. It gains\n1.1% mAP improvement on MSMT17 when the model uses\nonly SIE-cls and 1.2% improvement using only OLP. When\napplied together, mAP and R1 raise 2.4% and 1.0%, respec-\ntively.\nLi2tce Li2t Lt2i mAP Rank-1\n- - - 66.1 84.4\n- ✓ ✓ 71.3 87.5\n- ✓ - 71.7 87.6\n✓ - ✓ 73.2 88.6\n✓ - - 73.4 88.7\nTable 6: Loss terms from text encoder in the second stage.\nSIE-all SIE-cls OLP mAP Rank-1\n- - - 73.4 88.7\n✓ - - 74.3 88.6\n- ✓ - 74.5 88.8\n- - ✓ 74.6 89.5\n- ✓ ✓ 75.8 89.7\nTable 7: The validations on SIE-cls and OLP in ViT-based\nimage encoder.\n(a) (b) (c) (d) (a) (b) (c) (d)\n(a) (b) (c) (d) (a) (b) (c) (d)\nFigure 3: Visualization. (a) Input images, (b) TransReID\nbaseline, (c) our baseline (d) CLIP-ReID.\nVisualization of CLIP-ReID. Finally, we perform visual-\nization experiments using the (Chefer, Gur, and Wolf 2021)\nmethod to show the focused areas of the model. Both Tran-\nsReID’s and our baselines focus on local areas, ignoring\nother details about the human body, while CLIP-ReID will\nfocus on a more comprehensive area.\nConclusion\nThis paper investigates the way to apply the vision-language\npre-training model in image ReID. We find that fine-tuning\nthe visual model initialized by the CLIP image encoder, ei-\nther ResNet-50 or ViT-B/16, gives a good performance com-\npared to other baselines. To fully utilize the cross-modal de-\nscription ability in the pre-trained model, we propose CLIP-\nReID with a two-stage training strategy, in which the learn-\nable text tokens shared within each ID are incorporated and\naugmented to describe different instances. In the first stage,\nonly these tokens get optimized, forming ambiguous text de-\nscriptions. In the second stage, these tokens and text encoder\ntogether provide constraints for optimizing the parameters\nin the image encoder. We validate CLIP-ReID on several\ndatasets of persons and vehicles, and the results demonstrate\nthe effectiveness of text descriptions and the superiority of\nour model.\n1411\nAcknowledgments\nThis work is supported by the Science and Technology\nCommission of Shanghai Municipality under Grant No.\n22511105800, 19511120800 and 22DZ2229004.\nReferences\nBaldrati, A.; Bertini, M.; Uricchio, T.; and Del Bimbo, A. 2022.\nEffective Conditioned and Composed Image Retrieval Combining\nCLIP-Based Features. In Proceedings of the IEEE/CVF CVPR,\n21466–21474.\nChefer, H.; Gur, S.; and Wolf, L. 2021. Transformer interpretability\nbeyond attention visualization. In Proceedings of the IEEE/CVF\nCVPR, 782–791.\nChen, T.; Ding, S.; Xie, J.; Yuan, Y .; Chen, W.; Yang, Y .; Ren,\nZ.; and Wang, Z. 2019. Abd-net: Attentive but diverse person re-\nidentification. In Proceedings of the IEEE/CVF international con-\nference on computer vision, 8351–8361.\nChen, T.-S.; Liu, C.-T.; Wu, C.-W.; and Chien, S.-Y . 2020.\nOrientation-aware vehicle re-identification with semantics-guided\npart attention network. In European conference on computer vi-\nsion, 330–346. Springer.\nDai, Z.; Chen, M.; Gu, X.; Zhu, S.; and Tan, P. 2019. Batch drop-\nblock network for person re-identification and beyond. InProceed-\nings of the IEEE/CVF international conference on computer vision,\n3691–3701.\nDas, A.; Chakraborty, A.; and Roy-Chowdhury, A. K. 2014. Con-\nsistent re-identification in a camera network. In European confer-\nence on computer vision, 330–345. Springer.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.;\nZhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold,\nG.; Gelly, S.; et al. 2020. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929.\nGao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y .; Li, H.;\nand Qiao, Y . 2021. Clip-adapter: Better vision-language models\nwith feature adapters. arXiv preprint arXiv:2110.04544.\nGu, X.; Lin, T.-Y .; Kuo, W.; and Cui, Y . 2021. Open-vocabulary ob-\nject detection via vision and language knowledge distillation.arXiv\npreprint arXiv:2104.13921.\nHe, B.; Li, J.; Zhao, Y .; and Tian, Y . 2019. Part-regularized\nnear-duplicate vehicle re-identification. In Proceedings of the\nIEEE/CVF CVPR, 3997–4005.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 770–778.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W. 2021.\nTransreid: Transformer-based object re-identification. In Proceed-\nings of the IEEE/CVF international conference on computer vision,\n15013–15022.\nHermans, A.; Beyer, L.; and Leibe, B. 2017. In defense\nof the triplet loss for person re-identification. arXiv preprint\narXiv:1703.07737.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.; Le,\nQ.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling up visual\nand vision-language representation learning with noisy text super-\nvision. In International Conference on Machine Learning, 4904–\n4916. PMLR.\nJia, M.; Cheng, X.; Lu, S.; and Zhang, J. 2022. Learning disentan-\ngled representation implicitly via transformer for occluded person\nre-identification. IEEE Transactions on Multimedia.\nJin, X.; Lan, C.; Zeng, W.; and Chen, Z. 2020a. Uncertainty-\naware multi-shot knowledge distillation for image-based object re-\nidentification. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, 11165–11172.\nJin, X.; Lan, C.; Zeng, W.; Wei, G.; and Chen, Z. 2020b.\nSemantics-aligned representation learning for person re-\nidentification. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, 11173–11180.\nKhorramshahi, P.; Peri, N.; Chen, J.-c.; and Chellappa, R. 2020.\nThe devil is in the details: Self-supervised attention for vehicle re-\nidentification. In European Conference on Computer Vision, 369–\n386. Springer.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-language\ntransformer without convolution or region supervision. In Interna-\ntional Conference on Machine Learning, 5583–5594. PMLR.\nKoestinger, M.; Hirzer, M.; Wohlhart, P.; Roth, P. M.; and Bischof,\nH. 2012. Large scale metric learning from equivalence constraints.\nIn 2012 IEEE CVPR, 2288–2295. IEEE.\nLi, H.; Wu, G.; and Zheng, W.-S. 2021. Combined depth space\nbased architecture search for person re-identification. In Proceed-\nings of the IEEE/CVF CVPR, 6729–6738.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. Blip: Bootstrap-\nping language-image pre-training for unified vision-language un-\nderstanding and generation. arXiv preprint arXiv:2201.12086.\nLi, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and Hoi, S.\nC. H. 2021a. Align before fuse: Vision and language representation\nlearning with momentum distillation. Advances in neural informa-\ntion processing systems, 34: 9694–9705.\nLi, Y .; He, J.; Zhang, T.; Liu, X.; Zhang, Y .; and Wu, F. 2021b. Di-\nverse part discovery: Occluded person re-identification with part-\naware transformer. In Proceedings of the IEEE/CVF CVPR, 2898–\n2907.\nLiang, L.; Lang, C.; Li, Z.; Zhao, J.; Wang, T.; and Feng, S. 2018.\nSeeing Crucial Parts: Vehicle Model Verification via A Discrimi-\nnative Representation Model. Journal of the ACM (JACM), 22.\nLiao, S.; Hu, Y .; Zhu, X.; and Li, S. Z. 2015. Person re-\nidentification by local maximal occurrence representation and met-\nric learning. In Proceedings of the IEEE CVPR, 2197–2206.\nLiu, H.; Tian, Y .; Yang, Y .; Pang, L.; and Huang, T. 2016a. Deep\nrelative distance learning: Tell the difference between similar vehi-\ncles. In Proceedings of the IEEE CVPR, 2167–2175.\nLiu, X.; Liu, W.; Ma, H.; and Fu, H. 2016b. Large-scale vehicle\nre-identification in urban surveillance videos. In 2016 IEEE inter-\nnational conference on multimedia and expo (ICME), 1–6. IEEE.\nLuo, H.; Gu, Y .; Liao, X.; Lai, S.; and Jiang, W. 2019. Bag of\ntricks and a strong baseline for deep person re-identification. In\nProceedings of the IEEE/CVF CVPR workshops, 0–0.\nMa, H.; Zhao, H.; Lin, Z.; Kale, A.; Wang, Z.; Yu, T.; Gu, J.;\nChoudhary, S.; and Xie, X. 2022. EI-CLIP: Entity-Aware Inter-\nventional Contrastive Learning for E-Commerce Cross-Modal Re-\ntrieval. In Proceedings of the IEEE/CVF CVPR, 18051–18061.\nMatsukawa, T.; Okabe, T.; Suzuki, E.; and Sato, Y . 2016. Hierar-\nchical gaussian descriptor for person re-identification. In Proceed-\nings of the IEEE CVPR, 1363–1372.\nMeng, D.; Li, L.; Liu, X.; Li, Y .; Yang, S.; Zha, Z.-J.; Gao, X.;\nWang, S.; and Huang, Q. 2020. Parsing-based view-aware embed-\nding network for vehicle re-identification. In Proceedings of the\nIEEE/CVF CVPR, 7103–7112.\nMiao, J.; Wu, Y .; Liu, P.; Ding, Y .; and Yang, Y . 2019. Pose-guided\nfeature alignment for occluded person re-identification. In Pro-\nceedings of the IEEE/CVF international conference on computer\nvision, 542–551.\n1412\nQian, J.; Jiang, W.; Luo, H.; and Yu, H. 2020. Stripe-based and\nattribute-aware network: A two-branch deep model for vehicle\nre-identification. Measurement Science and Technology, 31(9):\n095401.\nQuan, R.; Dong, X.; Wu, Y .; Zhu, L.; and Yang, Y . 2019. Auto-reid:\nSearching for a part-aware convnet for person re-identification. In\nProceedings of the IEEE/CVF International Conference on Com-\nputer Vision, 3750–3759.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agar-\nwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021.\nLearning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning, 8748–\n8763. PMLR.\nRao, Y .; Chen, G.; Lu, J.; and Zhou, J. 2021. Counterfactual\nattention learning for fine-grained visual categorization and re-\nidentification. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 1025–1034.\nRao, Y .; Zhao, W.; Chen, G.; Tang, Y .; Zhu, Z.; Huang, G.; Zhou,\nJ.; and Lu, J. 2022. Denseclip: Language-guided dense prediction\nwith context-aware prompting. In Proceedings of the IEEE/CVF\nCVPR, 18082–18091.\nRistani, E.; Solera, F.; Zou, R.; Cucchiara, R.; and Tomasi, C.\n2016. Performance measures and a data set for multi-target, multi-\ncamera tracking. In European conference on computer vision, 17–\n35. Springer.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural machine\ntranslation of rare words with subword units. arXiv preprint\narXiv:1508.07909.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018. Be-\nyond part models: Person retrieval with refined part pooling (and\na strong convolutional baseline). In Proceedings of the European\nconference on computer vision (ECCV), 480–496.\nSun, Z.; Nie, X.; Xi, X.; and Yin, Y . 2020. Cfvmnet: A multi-\nbranch network for vehicle re-identification based on common field\nof view. In Proceedings of the 28th ACM international conference\non multimedia, 3523–3531.\nTan, H.; Liu, X.; Bian, Y .; Wang, H.; and Yin, B. 2021. Incomplete\ndescriptor mining with elastic loss for person re-identification.\nIEEE Transactions on Circuits and Systems for Video Technology,\n32(1): 160–171.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data using\nt-SNE. Journal of machine learning research, 9(11).\nWang, G.; Yang, S.; Liu, H.; Wang, Z.; Yang, Y .; Wang, S.; Yu, G.;\nZhou, E.; and Sun, J. 2020. High-order information matters: Learn-\ning relation and topology for occluded person re-identification. In\nProceedings of the IEEE/CVF CVPR, 6449–6458.\nWang, G.; Yuan, Y .; Chen, X.; Li, J.; and Zhou, X. 2018. Learn-\ning discriminative features with multiple granularities for person\nre-identification. In Proceedings of the 26th ACM international\nconference on Multimedia, 274–282.\nWang, P.; Zhao, Z.; Su, F.; and Meng, H. 2022. LTReID: Factoriz-\nable Feature Generation with Independent Components for Long-\nTailed Person Re-Identification.IEEE Transactions on Multimedia.\nWang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y .; and Cao, Y . 2021.\nSimvlm: Simple visual language model pretraining with weak su-\npervision. arXiv preprint arXiv:2108.10904.\nWei, L.; Zhang, S.; Gao, W.; and Tian, Q. 2018. Person transfer gan\nto bridge domain gap for person re-identification. In Proceedings\nof the IEEE CVPR, 79–88.\nYan, C.; Pang, G.; Bai, X.; Liu, C.; Ning, X.; Gu, L.; and Zhou,\nJ. 2021. Beyond triplet loss: person re-identification with fine-\ngrained difference-aware pairwise loss.IEEE Transactions on Mul-\ntimedia, 24: 1665–1677.\nYi, D.; Lei, Z.; Liao, S.; and Li, S. Z. 2014. Deep metric learning\nfor person re-identification. In 2014 22nd international conference\non pattern recognition, 34–39. IEEE.\nZhang, E.; Jiang, X.; Cheng, H.; Wu, A.; Yu, F.; Li, K.; Guo, X.;\nZheng, F.; Zheng, W.; and Sun, X. 2021a. One for More: Selecting\nGeneralizable Samples for Generalizable ReID Model. InProceed-\nings of the AAAI Conference on Artificial Intelligence, volume 35,\n3324–3332.\nZhang, Q.; Lai, J.; Feng, Z.; and Xie, X. 2021b. Seeing like a hu-\nman: Asynchronous learning with dynamic progressive refinement\nfor person re-identification. IEEE Transactions on Image Process-\ning, 31: 352–365.\nZhang, X.; Zhang, R.; Cao, J.; Gong, D.; You, M.; and Shen, C.\n2019. Part-guided attention learning for vehicle re-identification.\narXiv preprint arXiv:1909.06023, 2(8).\nZhang, Y .; He, B.; Sun, L.; and Li, Q. 2021c. Progressive Multi-\nStage Feature Mix for Person Re-Identification. In ICASSP 2021-\n2021 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2765–2769. IEEE.\nZhang, Z.; Lan, C.; Zeng, W.; Jin, X.; and Chen, Z. 2020. Relation-\naware global attention for person re-identification. In Proceedings\nof the ieee/cvf CVPR, 3186–3195.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian, Q.\n2015. Scalable person re-identification: A benchmark. In Pro-\nceedings of the IEEE international conference on computer vision,\n1116–1124.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020. Random\nerasing data augmentation. In Proceedings of the AAAI conference\non artificial intelligence, volume 34, 13001–13008.\nZhou, C.; Loy, C. C.; and Dai, B. 2021. Denseclip: Extract free\ndense labels from clip. arXiv preprint arXiv:2112.01071.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022a. Conditional\nprompt learning for vision-language models. In Proceedings of the\nIEEE/CVF CVPR, 16816–16825.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022b. Learning to\nprompt for vision-language models. International Journal of Com-\nputer Vision, 130(9): 2337–2348.\nZhou, K.; Yang, Y .; Cavallaro, A.; and Xiang, T. 2019. Omni-scale\nfeature learning for person re-identification. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 3702–\n3712.\nZhu, H.; Ke, W.; Li, D.; Liu, J.; Tian, L.; and Shan, Y . 2022. Dual\nCross-Attention Learning for Fine-Grained Visual Categorization\nand Object Re-Identification. In Proceedings of the IEEE/CVF\nCVPR, 4692–4702.\nZhu, K.; Guo, H.; Liu, Z.; Tang, M.; and Wang, J. 2020. Identity-\nguided human semantic parsing for person re-identification. In Eu-\nropean Conference on Computer Vision, 346–363. Springer.\nZhu, K.; Guo, H.; Zhang, S.; Wang, Y .; Huang, G.; Qiao, H.; Liu, J.;\nWang, J.; and Tang, M. 2021. Aaformer: Auto-aligned transformer\nfor person re-identification. arXiv preprint arXiv:2104.00921.\n1413"
}