{
  "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
  "url": "https://openalex.org/W4389518962",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099139823",
      "name": "Haoran Wang",
      "affiliations": [
        "Illinois Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099726507",
      "name": "Kai Shu",
      "affiliations": [
        "Illinois Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224247158",
    "https://openalex.org/W4327486059",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3176546569",
    "https://openalex.org/W3035317050",
    "https://openalex.org/W2949134692",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4290927845",
    "https://openalex.org/W4361193608",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221148719",
    "https://openalex.org/W3034808961",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W3196268181",
    "https://openalex.org/W4285254998",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4389523807",
    "https://openalex.org/W3116194654",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W2950336186",
    "https://openalex.org/W4379539320",
    "https://openalex.org/W4286978428",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4287854917",
    "https://openalex.org/W3169920290",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W3101682885",
    "https://openalex.org/W4289323723",
    "https://openalex.org/W4385574202",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W3214124416",
    "https://openalex.org/W2105505307",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4366460267",
    "https://openalex.org/W3101757358",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W4287121227",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2890801081",
    "https://openalex.org/W2061504941",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385570355"
  ],
  "abstract": "Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6288–6304\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExplainable Claim Verification via Knowledge-Grounded Reasoning with\nLarge Language Models\nHaoran Wang Kai Shu\nIllinois Institute of Technology, Chicago, IL, USA\nhwang219@hawk.iit.edu\nkshu@iit.edu\nAbstract\nClaim verification plays a crucial role in com-\nbating misinformation. While existing works\non claim verification have shown promising\nresults, a crucial piece of the puzzle that re-\nmains unsolved is to understand how to ver-\nify claims without relying on human-annotated\ndata, which is expensive to create at a large\nscale. Additionally, it is important for mod-\nels to provide comprehensive explanations that\ncan justify their decisions and assist human\nfact-checkers. This paper presents First-Order-\nLogic-Guided Knowledge-Grounded (FOLK)\nReasoning that can verify complex claims and\ngenerate explanations without the need for an-\nnotated evidence using Large Language Mod-\nels (LLMs). FOLK leverages the in-context\nlearning ability of LLMs to translate the claim\ninto a First-Order-Logic (FOL) clause consist-\ning of predicates, each corresponding to a sub-\nclaim that needs to be verified. Then, FOLK\nperforms FOL-Guided reasoning over a set\nof knowledge-grounded question-and-answer\npairs to make veracity predictions and gener-\nate explanations to justify its decision-making\nprocess. This process makes our model highly\nexplanatory, providing clear explanations of\nits reasoning process in human-readable form.\nOur experiment results indicate that FOLK out-\nperforms strong baselines on three datasets\nencompassing various claim verification chal-\nlenges. Our code and data are available. 1\n1 Introduction\nClaim verification (Guo et al., 2022) has become\nincreasingly important due to widespread online\nmisinformation (Tian et al., 2023; Jin et al., 2023).\nMost of the existing claim verification models\n(Zhou et al., 2019; Jin et al., 2022; Yang et al.,\n2022; Wadden et al., 2022b; Liu et al., 2020; Zhong\net al., 2020) use an automated pipeline that consists\n1https://github.com/wang2226/FOLK\nClaim:Lubabalo Kondlo won a silver medal in the 2012 SportAccord WorldMind Games inaugurated in July 2011 in Beijing.\nLabel:[NOT_SUPPORTED]\nPredicates:Won(Lubabalo Kondlo, a silver medal)::: Verify Lubabalo Kondlo won asilver medalInaugurated(the 2012 SportAccord World Mind Games, July 2011, Beijing)::: Verify the 2012 SportAccord World Mind Games was inaugurated in July2011 in Beijing.\nFollow-up Question:What did Lubabalo Kondlo win in the 2012 SportAccordWorld Mind Games?Grounded Answer:In 2012 he won the silver medal, ... in Beijing, China.\nFollow-up Question:When and where was the 2012 SportAccord World MindGames inaugurated?Grounded Answer:The International Mind Sports Association (IMSA) inau-gurated the SportAccord World Mind Games December 2011in Beijing ...\nPrediction:Won(Lubabalo Kondlo, a silver medal)isTruebecause In 2012 he won thesilver medal at the SportAccord World Mind Games in Beijing, China.Inaugurated(the 2012 SportAccord World Mind Games, July 2011, Beijing)isFalsebecause The International Mind Sports Association (IMSA) inauguratedthe SportAccord World Mind Games December 2011in Beijing.Won(Lubabalo Kondlo, a silver medal) && Inaugurated(the 2012\nSportAccord World Mind Games, July 2011, Beijing) isFalse.The claim is[NOT_SUPPORTED].\nExplanation:Lubabalo Kondlo won a silver medal in the 2012 SportAccord World MindGames. However, the event was inaugurated in December 2012, not July 2011,in Beijing.\nTable 1: An example from FOLK with GPT-3.5 on\nHoVER, a multi-hop claim verification dataset. We\nfirst use LLM to translate the claim into a First-Order-\nLogic clause (highlighted in orange), consisting of two\npredicates (highlighted in blue and purple). The LLM\nthen perform knowledge-grounded reasoning to predict\nlabel and generate explanation.\nof claim detection, evidence retrieval, verdict pre-\ndiction, and justification production. Despite some\nearly promising results, they rely on the availabil-\nity of large-scale human-annotated datasets, which\npose challenges due to labor-intensive annotation\nefforts and the need for annotators with specialized\ndomain knowledge. To address the issue of creat-\ning large-scale datasets, recent works (Pan et al.,\n2021; Wright et al., 2022; Lee et al., 2021) focus on\nclaim verification in zero-shot and few-shot scenar-\nios. However, these methods follow the traditional\nclaim verification pipeline, requiring both claim\n6288\nand annotated evidence for veracity prediction. Ad-\nditionally, these models often lack proper justifi-\ncations for their predictions, which are important\nfor human fact-checkers to make the final verdicts.\nTherefore, we ask the following question: Can we\ndevelop a model capable of performing claim verifi-\ncation without relying on annotated evidence, while\ngenerating natural language justifications for its\ndecision-making process?\nTo this end, we propose a novel framework\nFirst-Order-Logic-Guided Knowledge-Grounded\n(FOLK) to perform explainable claim verification\nby leveraging the reasoning capabilities of Large\nLanguage Models (LLMs) (Brown et al., 2020;\nTouvron et al., 2023; Zhang et al., 2022; Chowdh-\nery et al., 2022). Figure 1 illustrates a real-world\nexample from FOLK where it can provide veracity\nprediction based on logical reasoning and generate\nan explanation of its decision-making process in\na short paragraph. To ensure accurate prediction\nand provide high-quality explanations, FOLK first\ntranslates the input claim into a First-Order-Logic\n(FOL) (Enderton, 2001) clause consisting of a set\nof conjunctive predicates. Each predicate repre-\nsents a part of the claim that needs to be verified.\nNext, the generated FOL predicates guide LLMs\nto generate a set of questions and corresponding\nanswers. Although the generated answers may ap-\npear coherent and plausible, they often lack factual\naccuracy due to LLM’s hallucination problem (Ji\net al., 2023; Ouyang et al., 2022). To address this\nproblem, FOLK controls the knowledge source of\nthe LLMs by grounding the generated answers in\nreal-world truth via retrieving accurate informa-\ntion from trustworthy external knowledge sources\n(e.g. Google or Wikipedia). Finally, FOLK lever-\nages the reasoning ability of LLMs to evaluate the\nboolean value of the FOL clause and make a ve-\nracity prediction. Given the high stakes involved\nin claim verification, FOLK prompts the LLMs to\ngenerate justifications for their decision-making\nprocess in natural language. These justifications\nare intended to aid human fact-checkers in making\nthe final verdict, enhancing the transparency and\ninterpretability of the model’s predictions.\nWe evaluate our proposed methods on three fact-\nchecking datasets (Jiang et al., 2020; Aly et al.,\n2021; Wadden et al., 2022a) with the following dis-\ntinct challenges: multi-hop reasoning, numerical\nreasoning, combining text and table for reasoning,\nand open-domain scientific claim verification. Our\nexperiment results demonstrate that FOLK can ver-\nify complex claims while generating explanations\nto justify its decision-making process. Additionally,\nwe show the effectiveness of FOL-guided claim de-\ncomposition and knowledge-grounded reasoning\nfor claim verification.\nIn summary, our contributions are:\n• We introduce a new method to verify claims\nwithout the need for annotated evidence.\n• We demonstrate the importance of using sym-\nbolic language to help claim decomposition\nand provide knowledge-grounding for LLM\nto perform reasoning.\n• We show that FOLK can generate high-quality\nexplanations to assist human fact-checkers.\n2 Background\nClaim Verification. The task of claim verifica-\ntion aims to predict the veracity of a claim by re-\ntrieving related evidence documents, selecting the\nmost salient evidence sentences, and predicting\nthe veracity of the claim as SUPPORTS or RE-\nFUTES. Claim verification falls into the broader\ntask of Fact-checking, which includes all the steps\ndescribed in claim verification with the addition\nof claim detection, a step to determine the check-\nworthiness of a claim.\nWhile steady progress has been made in this\nfield, recent research focus has shifted to 1) deal-\ning with insufficient evidence (Atanasova et al.,\n2022) and 2) using explainable fact-checking mod-\nels to support decision-making (Kotonya and Toni,\n2020a). In the line of explainable fact-checking,\n(Popat et al., 2018) and (Shu et al., 2019) use visual-\nization of neural attention weights as explanations.\nAlthough attention-based explanation can provide\ninsights into the deep learning model’s decision\nprocess, it does not generate human-readable expla-\nnations and cannot be interpreted without any prior\nmachine learning knowledge. (Atanasova et al.,\n2020; Kotonya and Toni, 2020b) formulate the task\nof generating explanations as extractive summa-\nrizing of the ruling comments provided by profes-\nsional fact-checkers. While their work can generate\nhigh-quality explanations based on training data\nfrom professional fact-checkers, annotating such\ndatasets is expensive and not feasible at a large\nscale. Our work explores using reasoning steps\nas explanations of the model’s decision-making\n6289\nprocess while generating explanations in natural\nlanguage.\nLarge Language Models for Reasoning. Large\nlanguage models have demonstrated strong rea-\nsoning abilities through chain-of-thought (CoT)\nprompting, wherein LLM is prompted to gener-\nate its answer following a step-by-step explanation\nby using just a few examples as prompts. Recent\nworks have shown that CoT prompting can im-\nprove performance on reasoning-heavy tasks such\nas multi-hop question answering, multi-step com-\nputation, and common sense reasoning (Nye et al.,\n2021; Zhou et al., 2022; Kojima et al., 2022).\nVerifying complex claims often requires multi-\nstep (multi-hop) reasoning (Mavi et al., 2022),\nwhich requires combining information from multi-\nple pieces of evidence to predict the veracity of a\nclaim. Multi-step reasoning can be categorized into\nforward-reasoning and backward-reasoning (Yu\net al., 2023). Forward-reasoning (Creswell et al.,\n2022; Sanyal et al., 2022; Wei et al., 2022) em-\nploys a bottom-up approach that starts with existing\nknowledge and obtains new knowledge with infer-\nence until the goal is met. Backward-reasoning\n(Min et al., 2019; Press et al., 2022) on the other\nhand, is goal-driven, which starts from the goal and\nbreaks it down into sub-goals until all of them are\nsolved. Compared to forward reasoning, backward\nreasoning is more efficient, the divide-and-conquer\nsearch scheme can effectively reduce the problem\nsearch space. We propose FOLK, a FOL-guided\nbackward reasoning method for claim verification.\nDespite the recent progress in using LLMs for\nreasoning tasks, their capability in verifying claims\nhas not been extensively explored. (Yao et al.,\n2022) evaluate using LLMs to generate reasoning\ntraces and task-specific actions on fact verification\ntasks. Their reasoning and action steps are more\ncomplex than simple CoT and rely on prompting\nmuch larger models (PaLM-540B). Additionally,\nthey test their model’s performance on the FEVER\ndataset (Thorne et al., 2018), which lacks many-\nhop relations and specialized domain claims. In\ncontrast to their approach, our proposed method\ndemonstrates effectiveness on significantly smaller\nLLMs without requiring any training, and we test\nour method on scientific claims.\nContemporaneous to our work, (Peng et al.,\n2023) propose a set of plug-and-play modules that\naugment with LLMs to improve the factuality of\nLLM-generated responses for task-oriented dia-\nlogue and question answering. In contrast to their\napproach, our primary focus is on providing LLMs\nwith knowledge-grounded facts to enable FOL-\nGuided reasoning for claim verification, rather\nthan solely concentrating on enhancing the factual\naccuracy of LLMs’ responses. ProgramFC (Pan\net al., 2023) leverages LLMs to generate computer-\nprogram-like functions to guide the reasoning pro-\ncess. In contrast to their approach, which only\nuses LLMs for claim decomposition, we use LLMs\nfor both claim decomposition and veracity predic-\ntion. By using LLMs for veracity prediction, we\ncan not only obtain a comprehensive understand-\ning of LLMs’ decision process but also generate\nexplanations for their predictions. Furthermore,\nProgramFC is limited to closed-domain, as it needs\nto first retrieve evidence from a large textual corpus\nlike Wikipedia. FOLK on the other hand, can per-\nform open-domain claim verification since it does\nnot require a pre-defined evidence source.\n3 Method\nOur objective is to predict the veracity of a claim\nC without the need for annotated evidence while\ngenerating explanations to elucidate the decision-\nmaking process of LLMs. As shown in Figure 1,\nour framework contains three stages. In the FOL-\nGuided Claim Decomposition stage, we first trans-\nlate the input claim into a FOL clause P, then we\nuse P to guide LLM to generate a set of intermedi-\nate question-answer pairs (qi,ai). Each intermedi-\nate question qi represents a specific reasoning step\nrequired to verify the claim. In the Knowledge-\nGrounding stage, each ai represents the answer\ngenerated by LLMs that has been verified against\nground truth obtained from an external knowledge\nsource. Finally, in the Veracity Prediction and Ex-\nplanation Generation stage, we employ P to guide\nthe reasoning process of LLMs over the knowledge-\ngrounded question-and-answer pairs. This allows\nus to make veracity predictions and generate justi-\nfications for its underlying reasoning process.\n3.1 FOL-Guided Claim Decomposition\nAlthough LLMs have displayed decent perfor-\nmance in natural language reasoning tasks, they\nfall short when asked to directly solve complex\nreasoning problems. This limitation arises from\nthe lack of systematic generalization capabilities in\nlanguage models (Valmeekam et al., 2022; Elazar\net al., 2021). Recent works have discovered that\n6290\nTomáš Šmíd and Fabrice \nSantoro were both American \ntennis players.\nMulti-Hop Reasoning:\nNijō Michihira had two wives \nand three children during his \nshort lifetime of 47 years.\nHypertension can be accurately \ndiagnosed with ambulatory \nblood pressure monitoring.\nNumerical Reasoning: Open-Domain Scientific:\nClaim:\nF O L K\n1. First-Order-Logic (FOL) \nGuided Claim Decomposition\n2. Retrieve Knowledge-Grounded \nAnswers\n 3. Veracity Prediction and \nExplanation Generation\nREFUTE SUPPORT SUPPORT\nTomáš Šmíd is a former tennis player \nfrom Czechoslovakia, and Fabrice \nSantoro is a French retired tennis \nplayer. Therefore, they are not both \nAmerican tennis players.\nNijō Michihira married a daughter of Nijō \nMorotada and a daughter of Saionji Kin'aki, and \nfrom the later he had a son, Nijō Yoshimoto, and \na daughter … and another son who was adopted \nby the Tominokouji family. His lifetime was from \n1288 to February 27, 1335, which was 47 years.\nAmbulatory Blood Pressure Monitoring \n(ABPM) is an accurate way to diagnose \nhypertension, as it is able to diagnose \nmasked hypertension in those with \nnormal office blood pressure readings.\nPrediction:\nExplanation:\nFigure 1: Overview of our FOLK framework, which consists of three steps: (i) FOLK translates input claim into\na FOL clause and uses it to guide LLMs to generate a set of question-and-answer pairs; (ii) FOLK then retrieves\nknowledge-grounded answers from external knowledge-source; and (iii) FOLK performs FOL-Guided reasoning\nover knowledge-grounded answers to make veracity prediction and generate explanations. FOLK can perform a\nvariety of reasoning tasks for claim verification, such as multi-hop reasoning, numerical reasoning, and open-domain\nscientific claim verification.\nLLMs are capable of understanding and converting\ntextual input into symbolic languages, such as for-\nmal language (Kim, 2021), mathematical equations\n(He-Yueya et al., 2023), or Python codes (Gao et al.,\n2022). Inspired by these recent works, we harness\nthe ability of LLMs to translate textual claims into\nFOL clauses. This allows us to guide LLMs in\nbreaking down claims into various sub-claims.\nAt this stage, given the input claim C, the LLM\nfirst generates a set of predicates P = [p1,...,p n]\nthat correspond to the sub-claims C = [c1,...,c n].\nEach predicate pi ∈ P is a First-Order Logic (FOL)\npredicate that guides LLMs to prompt a question-\nand-answer pair that represents sub-claim ci. The\nclaim C can be represented as a conjunction of the\npredicates C = p1 ∧ p2 ∧ ...∧ pn. To classify the\nclaim Cas SUPPORTED, all predicates must eval-\nuate to True. If any of the predicates are False,\nthe claim is classified as REFUTED. By providing\nthe LLMs with symbolic languages such as pred-\nicates, alongside a few in-context examples, we\nobserve that LLMs can effectively identify the cru-\ncial entities, relations, and facts within the claim.\nConsequently, LLMs are capable of generating rel-\nevant question-and-answer pairs that align with the\nidentified elements.\n3.2 Retrieve Knowledge-Grounded Answers\nAlthough LLMs exhibit the ability to generate co-\nherent and well-written text, it is worth noting that\nthey can sometimes hallucinate (Ji et al., 2023), and\nproduce text that fails to be grounded in real-world\ntruth. To provide knowledge-grounded answers for\nthe generated intermediate questions, we employ\na retriever based on Google Search, via the Ser-\npAPI 2 service. Specifically, we return the top-1\nsearch result returned by Google. While it is im-\nportant to acknowledge that Google search results\nmay occasionally include inaccurate information, it\ngenerally serves as a more reliable source of knowl-\nedge compared to the internal knowledge of LLMs.\nAdditionally, in real-world scenarios, when human\nfact-checkers come across unfamiliar information,\nthey often rely on Google for assistance. There-\nfore, we consider the answers provided by Google\nsearch as knowledge-grounded answers.\n3.3 Veracity Prediction and Explanation\nGeneration\nAt this stage, the LLM is asked to make a verdict\nprediction V ∈ {SUPPORT,REFUTE} and pro-\nvide an explanation E to justify its decision.\nVeracity PredictionGiven the input claim C, the\n2https://serpapi.com/\n6291\npredicates [p1,...,p n], and knowledge-grounded\nquestion-and-answer pairs, FOLK first checks the\nveracity of each predicate against corresponding\nknowledge-grounded answers while giving reasons\nbehind its predictions. Once all predicates have\nbeen evaluated, FOLK makes a final veracity pre-\ndiction for the entire clause. In contrast to solely\nproviding LLMs with generated questions and their\ncorresponding grounded answers, we found that the\ninclusion of predicates assists LLMs in identifying\nthe specific components that require verification,\nallowing them to offer more targeted explanations.\nExplanation Generation We leverage LLMs’ ca-\npability to generate coherent language and prompt\nLLMs to generate a paragraph of human-readable\nexplanation. We evaluate the explanation gener-\nated by LLMs with manual evaluation. Further-\nmore, since claim verification is a high-stake task,\nit should involve human fact-checkers to make the\nfinal decision. Therefore, we provide URL links to\nthe relevant facts, allowing human fact-checkers to\nreference and validate the information.\n4 Experiments\nWe compare FOLK to existing methods on 7 claim\nverification challenges from three datasets. Our\nexperiment setting is described in Sections 4.1 &\n4.2 and we discuss our main results in Section 4.4.\n4.1 Datasets\nWe experiment with the challenging datasets listed\nbelow. Following existing works (Yoran et al.,\n2023; Kazemi et al., 2022; Trivedi et al., 2022),\nto limit the overall experiment costs, we use strat-\nified sampling to select 100 examples from each\ndataset to ensure a balanced label distribution.\nHoVER (Jiang et al., 2020) is a multi-hop fact\nverification dataset created to challenge models to\nverify complex claims against multiple informa-\ntion sources, or “hop”. We use the validation set\nfor evaluation since the test sets are not released\npublicly. We divide the claims in the validation\nset based on the number of hops: two-hop claims,\nthree-hop claims, and four-hop claims.\nFEVEROUS (Aly et al., 2021) is a benchmark\ndataset for complex claim verification over struc-\ntured and unstructured data. Each claim is anno-\ntated with evidence from sentences and forms in\nWikipedia. We selected claims in the validation set\nwith the following challenges to test the effective-\nness of our framework: numerical reasoning, multi-\nhop reasoning, and combining tables and text.\nSciFact-Open (Wadden et al., 2022a) is a test-\ning dataset for scientific claim verification. This\ndataset aims to test existing models’ claim verifica-\ntion performance in an open-domain setting. Since\nthe claims in SciFact-Open do not have a global\nlabel, we select claims with complete evidence that\neither support or refute the claim and utilize them\nas the global label. This dataset tests our model’s\nperformance on specialized domains that require\ndomain knowledge to verify.\n4.2 Baselines\nWe compare our proposed method against the fol-\nlowing four baselines.\nDirect This baseline simulates using LLM as\nstandalone fact-checkers. We directly ask LLMs to\ngive us veracity predictions and explanations given\nan input claim, relying solely on LLMs’ internal\nknowledge. It is important to note that we have\nno control over LLM’s knowledge source, and it is\npossible that LLMs may hallucinate.\nChain-of-Thought (Wei et al., 2022) is a popu-\nlar approach that demonstrates chains of inference\nto LLMs within an in-context prompt. We decom-\npose the claims by asking LLMs to generate the\nnecessary questions needed to verify the claim. We\nthen prompt LLMs to verify the claims step-by-step\ngiven the claims and knowledge-grounded answers.\nSelf-Ask (Press et al., 2022) is a structured\nprompting approach, where the prompt asks LLMs\nto decompose complex questions into easier sub-\nquestions that it answers before answering the\nmain question. It is shown to improve the perfor-\nmance of Chain-of-Thought on multi-hop question-\nanswering tasks. We use the same decomposition\nand knowledge-grounding processes as in CoT. For\nveracity prediction, we provide both questions and\nknowledge-grounded answers to LLMs to reason,\ninstead of just the knowledge-grounded answers.\nProgramFC (Pan et al., 2023) is a recently pro-\nposed baseline for verifying complex claims using\nLLMs. It contains three settings for knowledge-\nsource: gold-evidence, open-book, and closed-\nbook. To ensure that ProgramFC has the same prob-\nlem setting as FOLK, we use the open-book setting\nfor ProgramFC. Since we only use one reasoning\nchain, we select N=1 for ProgramFC. Since Pro-\ngramFC cannot perform open-domain claim verifi-\ncation, we exclude it from SciFact-Open dataset.\n6292\nHoVER FEVEROUS SciFact-Open\n2-Hop 3-Hop 4-Hop Numerical Multi-hop Text and Table\nDirect 57.11 44.95 55.91 48.52 50.18 59.07 49.70\nCoT 53.98 46.57 47.99 49.56 60.90 61.76 63.39\nSelf-Ask 54.23 48.87 51.76 55.33 61.16 54.23 60.94\nProgramFC 71.00 51.04 52.92 54.78 59.84 51.69 -\nFOLK 66.26 54.80 60.35 59.49 67.01 63.42 67.59\nTable 2: Macro F-1 score of Direct, Chain-of-Thought (CoT), Self-Ask, ProgramFC, and our method FOLK on\nthree challenging claim verification datasets. The best results within each dataset are highlighted.\n4.3 Experiment Settings\nThe baselines and FOLK use GPT-3.5,text-davinci-\n003 (175B) as the underlying LLM. We use SER-\nPAPI as our retrieval engine to obtain knowledge-\ngrounded answers. In addition to the results in\nTable 2, we perform experiments on smaller LLMs\n(Touvron et al., 2023): llama-7B, llama-13B, and\nllama-30B. The results are presented in Table 2.\nOur prompts are included in B. The number of\nprompts used varies between 4-6 between the\ndatasets. These prompts are based on random ex-\namples from the train and development sets.\n4.4 Main Results\nWe report the overall results for FOLK compared\nto the baselines for claim verification in Table 2.\nFOLK achieves the best performance on 6 out of\n7 evaluation tasks, demonstrating its effectiveness\non various reasoning tasks for claim verification.\nBased on the experiment results, we have the fol-\nlowing major observations:\nFOLK is more effective on complex claims. On\nHoVER dataset, FOLK outperforms the baselines\nby 7.37% and 7.94% on three-hop and four-hop\nclaims respectively. This suggests that FOLK be-\ncomes more effective on more complex claims as\nthe required reasoning depth increases. Among\nthe baselines, ProgramFC has comparable perfor-\nmance on three-hop claims, which indicates the\neffectiveness of using symbolic language, such\nas programming-like language to guide LLMs for\nclaim decomposition for complex claims. How-\never, programming-like language is less effective as\nclaims become more complex. Despite ProgramFC\nhaving a performance increase of 3.68% from three-\nhop to four-hop claims in HoVER, FOLK has a\nlarger performance increase of 10.13%. Suggest-\ning that FOL-guided claim decomposition is more\neffective on more complex claims.\nOn FEVEROUS dataset, FOLK outperforms the\nbaselines by 7.52%, 9.57%, and 2.69% on all three\ntasks respectively. This indicates that FOLK can\nperform well not only on multi-hop reasoning tasks\nbut also on numerical reasoning and reasoning over\ntext and table.\nFOL-guided Reasoning is more effective than\nCoT-like Reasoning. Our FOLK model, which\nuses FOL-guided decomposition reasoning ap-\nproach outperforms CoT and Self-Ask baselines on\nall three datasets. On average, there is an 11.30%\nimprovement. This suggests that FOL-like predi-\ncates help LLMs to better decompose claims, and\nresult in more accurate reasoning. This is particu-\nlarly evident when the claims become more com-\nplex: there is a 12.13% improvement in three-hop\nand a 16.6% improvement in the four-hop setting.\nKnowledge-grouding is more reliable than\nLLM’s internal knowledge. FOLK exhibits su-\nperior performance compared to Direct baseline\nacross all three datasets. This observation indicates\nthe critical role of knowledge-grounding in claim\nverification, as Direct solely relies on the internal\nknowledge of LLMs. It is also important to note\nthat the lack of control over the knowledge source\nin Direct can lead to hallucinations, where LLMs\nmake accurate predictions but for incorrect reasons.\nFor instance, when faced with a claim labeled as\nSUPPORT, LLMs may correctly predict the out-\ncome despite certain predicates being false.\n2-hop 3-hop 4-hop\nen.wikipedia.org 66.26 54.80 60.35\ngoogle.com 62.60 50.88 54.66\nTable 3: Ablation study on knowledge source.\n6293\nllama-13B llama-30B GPT-3.5\n(175B)\nLLM Size\n50.46\n55.35\n66.26\n35.06 36.28\n54.23\n48.52\n46.63\n53.98\nHoVER 2-hop\nFOLK\nSelf-Ask\nCoT\nllama-13B llama-30B GPT-3.5\n(175B)\nLLM Size\n42.23\n52.75\n54.80\n31.97\n37.43\n48.87\n45.78\n47.26 46.57\nHoVER 3-hop\nFOLK\nSelf-Ask\nCoT\nllama-13B llama-30B GPT-3.5\n(175B)\nLLM Size\n48.80\n54.82\n60.35\n33.56\n40.22\n51.76\n44.63\n42.07\n47.99\nHoVER 4-hop\nFOLK\nSelf-Ask\nCoT\nFigure 2: Macro-F1 score of running FOLK (brown line), Self-Ask (green dashed line), and CoT (blue dashed line)\non HoVER dataset for LLMs with increasing size: llama-13B, llama-30B, and GPT-3.5 (175B).\n4.5 The Impacts of FOL-Guided Reasoning\nTo gain more insights on prompting from FOL pred-\nicates, we perform an ablation study on the HoVER\ndataset. The goal is to see whether the perfor-\nmance difference in Table 2 primarily results from\nFOLK generating better follow-up questions or if\nthe predicates also play a role in constructing the\nveracity prediction. Specifically, we maintain the\nCoT prompt format but input knowledge-grounded\nanswers from FOLK. As for Self-Ask, we main-\ntain the Self-Ask prompt format while incorporat-\ning follow-up questions generated by FOLK along\nwith their associated knowledge-grounded answers.\nThis guarantees that both CoT and Self-Ask re-\ntain their reasoning capabilities while employing\nidentical factual information as provided by FOLK.\nThe results, presented in Table 4, show that FOLK\nconsistently outperforms CoT and Self-Ask in all\nthree tasks. This highlights that the FOL-guided\nreasoning process enhances the ability of language\nmodels to integrate knowledge in multi-hop reason-\ning scenarios effectively.\n4.6 The Impacts of Knowledge-Grounding\nTo better understand the role of knowledge-\ngrounding in LLM’s decision process, we perform\nan ablation study on four multi-hop reasoning tasks.\nWe use the FOLK prompt to generate predicates\nand decompose the claim, we then compare its\nperformance under two settings. In the first set-\nting, we let LLM reason over the answers it gener-\nated itself. In the second setting, we provide LLM\n2-hop 3-hop 4-hop\nCoT using FOLK questions57.78 41.20 44.57\nSelf-Ask using FOLK questions62.00 43.25 42.86\nFOLK 66.26 54.80 60.35\nTable 4: Ablation study on FOL-guided reasoning.\nwith knowledge-grounded answers. The results are\nshown in Figure 3, as we can see, FOLK performs\nbetter with knowledge-grounded answers. This\nsuggests that by providing knowledge-grounded\nanswers, we can improve LLM’s reasoning perfor-\nmance, and alleviate the hallucination problem by\nproviding it facts.\nNext, we investigate whether the knowledge\nsource can affect FOLK’s performance. Since both\nHoVER and FEVEROUS datasets are constructed\nupon Wikipedia pages. We add en.wikipedia.com\nin front of our query to let it search exclusively\nfrom Wikipedia. This is the same way as Pro-\ngramFC’s open-book setting. We record the per-\nformance in Table 3. As we can see, using a more\naccurate search can lead to better performance.\n4.7 The Generalization on Different-sized\nLLMs\nTo assess whether the performance of FOLK can\ngeneralize to smaller LLMs, we compare the per-\nformance of FOLK against cot and self-ask on\nHoVER dataset using two different-sized LLMs:\nllama-7B and llama-13B. Due to the inability of\nusing ProgramFC prompts to generate programs\n2-hop 3-hop 4-hop Multi-hop0\n20\n40\n60\n80\n100\n38.26\n44.96 46.9\n38.42\n66.26\n54.8\n60.35\n67.01\nFOLK GPT-3.5\nLLM Internal Knowledge\nKnowledge-grounded\nFigure 3: Ablation study on knowledge-grounding for\nmulti-hop reasoning task.\n6294\nusing the llama model, we exclude ProgramFC\nfrom our evaluation for this experiment. The re-\nsults are shown in Figure 2, FOLK can outperform\nCoT and Self-Ask regardless of the model size,\nexcept for 3-hop claims using llama-13B model.\nAs smaller models are less capable for complex\nreasoning, the performance of Self-Ask decreases\nsignificantly with decreasing model size. For CoT,\nits performance is less sensitive to LLM size com-\npared to Self-Ask. However, these trends are less\nnotable for FOLK. We believe it can attribute to\nthe predicates used to guide LLM to perform high-\nlevel reasoning. Our results show that FOLK using\nllama-30B model can achieve comparable perfor-\nmance to PrgramFC using 5.8x larger GPT-3.5 on\nthree-hop and four-hop claims. This further shows\nthat FOLK is effective on deeper claims and can\ngeneralize its performance to smaller LLMs.\n4.8 Assessing the Quality of Explanations\nTo measure the quality of the explanations gen-\nerated by FOLK, we conduct manual evaluations\nby three annotators. The annotators are graduate\nstudents with a background in computer science.\nFollowing previous work (Atanasova et al., 2020),\nwe ask annotators to rank explanations generated\nby CoT, Self-Ask, and FOLK. We choose the fol-\nlowing three properties for annotators to rank these\nexplanations:\nCoverage The explanation can identify and include\nall salient information and important points that\ncontribute to verifying the claim. We provide fact\ncheckers with annotated gold evidence and ask\nthem whether the generated explanation can ef-\nfectively encompass the key points present in the\ngold evidence.\nSoundness The explanation is logically sound and\ndoes not contain any information contradictory to\nthe claim or gold evidence. To prevent annotators\nfrom being influenced by the logic generated by\nFOLK, we do not provide annotators with the pred-\nicates generated by FOLK.\nReadability The explanation is presented in a clear\nand coherent manner, making it easily understand-\nable. The information is conveyed in a way that is\naccessible and comprehensible to readers.\nWe randomly sample 30 instances from the\nmulti-hop reasoning challenge from the FEVER-\nOUS dataset. For each instance, we collect verac-\nity explanations generated by CoT, Self-Ask, and\nFOLK. During the annotation process, we ask an-\nAnnotators CoT Self-Ask FOLK\nCoverage\n1st 1.90 1.95 1.75\n2nd 1.75 1.75 1.35\n3rd 1.55 1.70 1.60\nAvg 1.73 1.80 1.57\nSoundness\n1st 1.40 1.45 1.15\n2nd 1.40 1.25 1.00\n3rd 1.05 1.05 1.05\nAvg 1.28 1.25 1.07\nReadability\n1st 1.95 1.90 1.25\n2nd 1.75 1.60 1.20\n3rd 1.35 1.50 1.35\nAvg 1.68 1.67 1.27\nTable 5: Mean Average Ranks (MARs) of the explana-\ntions for each of the three evaluation criteria. The lower\nMAR indicates a higher ranking and represents a better\nquality of an explanation. For each row, the best results\nfrom each annotator are underlined, and the best overall\nresults are highlighted in blue.\nnotators to rank these explanations with the rank 1,\n2, and 3 representing first, second, and third place\nrespectively. We also allow ties, meaning that two\nveracity explanations can receive the same rank if\nthey appear the same. To mitigate potential posi-\ntion bias, we did not provide information about the\nthree different explanations and shuffled them ran-\ndomly. The annotators worked separately without\ndiscussing any details about the annotation task.\nFOLK can generate informative, accurate ex-\nplanations with great readability. Table 5 shows\nthe results from the manual evaluation mentioned\nabove. We use Mean Average Ranks (MARs)\nas our evaluation metrics, where a lower MAR\nsignifies a higher ranking and indicates a better\nquality of an explanation. To measure the inter-\nannotator agreement, we compute Krippendorf’s α\n(Hayes and Krippendorff, 2007). The correspond-\ning αvalues for FOLK are 0.52 for Coverage, 0.71\nfor Soundness, and 0.69 for Readability, where\nα >0.67 is considered good agreement. We as-\nsume the low agreement on coverage can be at-\ntributed to the inherent challenges of ranking tasks\nfor manual evaluation. Small variations in rank po-\nsitions and annotator bias towards ranking ties may\nimpact the agreement among annotators. We find\nthat explanations generated by FOLK are ranked\nthe best for all criteria, with 0.16 and 0.40 ranking\nimprovements on coverage and readability respec-\n6295\ntively. While Self-Ask has better prediction results\ncompared to CoT, as shown in Table 2, CoT has\na 0.17 MAR improvement compared to Self-Ask.\nThis implies that the inclusion of both questions\nand answers as context for Language Model-based\napproaches restricts their coverage in generating\nexplanations.\n5 Conclusion\nIn this paper, we propose a novel approach to\ntackle two major challenges in verifying real-world\nclaims: the scarcity of annotated datasets and the\nabsence of explanations. We introduce FOLK, a\nreasoning method that leverages First-Order Logic\nto guide LLMs in decomposing complex claims\ninto sub-claims that can be easily verified through\nknowledge-grounded reasoning with LLMs.\nOur experiment results show that FOLK demon-\nstrates promising performance on three challenging\ndatasets with only 4-6 in-context prompts provided\nand no additional training. Additionally, we investi-\ngate the impact of knowledge grounding and model\nsize on the performance of FOLK. The results indi-\ncate that FOLK can make accurate predictions and\ngenerate explanations when using a medium-sized\nLLM such as llama-30B. To evaluate the quality\nof the explanations generated by FOLK, we con-\nducted manual evaluations by three human annota-\ntors. The results of these evaluations demonstrate\nthat FOLK consistently outperforms the baselines\nin terms of explanation overall quality.\n6 Limitations\nWe identify two main limitations of FOLK. First,\nthe claims in our experiments are synthetic and\ncan be decomposed with explicit reasoning based\non the claims’ syntactic structure. However, real-\nworld claims often possess complex semantic struc-\ntures, which require implicit reasoning to verify.\nThus, bridging the gap between verifying synthetic\nclaims and real-world claims is an important direc-\ntion for future work. Second, FOLK has a much\nhigher computational cost than supervised claim\nverification methods. FOLK requires using large\nlanguage models for claim decomposition and ve-\nracity prediction. This results in around $20 per\n100 examples using OpenAI API or around 7.5\nhours on locally deployed llama-30B models on an\n8x A5000 cluster. Therefore, finding ways to infer\nLLMs more efficiently is urgently needed along-\nside this research direction.\n7 Ethical Statement\nBiases. We acknowledge the possibility of biases\nexisting within the data used for training the lan-\nguage models, as well as in certain factuality as-\nsessments. Unfortunately, these factors are beyond\nour control.\nIntended Use and Misuse Potential. Our models\nhave the potential to captivate the general public’s\ninterest and significantly reduce the workload of\nhuman fact-checkers. However, it is essential to rec-\nognize that they may also be susceptible to misuse\nby malicious individuals. Therefore, we strongly\nurge researchers to approach their utilization with\ncaution and prudence.\nEnvironmental Impact. We want to highlight\nthe environmental impact of using large language\nmodels, which demand substantial computational\ncosts and rely on GPUs/TPUs for training, which\ncontributes to global warming. However, it is worth\nnoting that our approach does not train such models\nfrom scratch. Instead, we use few-shot in-context\nlearning. Nevertheless, the large language models\nwe used in this paper are likely running on GPU(s).\nAcknowledgements\nThis material is based upon work supported by\nthe Defense Advanced Research Projects Agency\n(DARPA) under Agreement No. HR0011-22-\n9-0100, NSF SaTC-2241068, a Cisco Research\nAward, a Microsoft Accelerate Foundation Models\nResearch Award. The views, opinions and/or find-\nings expressed are those of the author and should\nnot be interpreted as representing the official views\nor policies of the Department of Defense or the U.S.\nGovernment.\nReferences\nRami Aly, Zhijiang Guo, Michael Schlichtkrull, James\nThorne, Andreas Vlachos, Christos Christodoulopou-\nlos, Oana Cocarascu, and Arpit Mittal. 2021. Fever-\nous: Fact extraction and verification over unstruc-\ntured and structured information. arXiv preprint\narXiv:2106.05707.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. Generating fact\nchecking explanations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7352–7364, Online. Association\nfor Computational Linguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2022. Fact checking\n6296\nwith insufficient evidence. Transactions of the Asso-\nciation for Computational Linguistics, 10:746–763.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nHerbert B Enderton. 2001. A mathematical introduction\nto logic. Elsevier.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics, 10:178–206.\nAndrew F Hayes and Klaus Krippendorff. 2007. An-\nswering the call for a standard reliability measure for\ncoding data. Communication methods and measures,\n1(1):77–89.\nJoy He-Yueya, Gabriel Poesia, Rose E Wang, and\nNoah D Goodman. 2023. Solving math word prob-\nlems by combining language models with symbolic\nsolvers. arXiv preprint arXiv:2304.09102.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3441–3460, Online. Association for Computational\nLinguistics.\nYiqiao Jin, Yeon-Chang Lee, Kartik Sharma, Meng\nYe, Karan Sikka, Ajay Divakaran, and Srijan Kumar.\n2023. Predicting information pathways across online\ncommunities. In KDD.\nYiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun,\nWei Wang, Hao Liao, and Xing Xie. 2022. Towards\nfine-grained reasoning for fake news detection. In\nAAAI.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,\nXin Xu, and Deepak Ramachandran. 2022. Lam-\nbada: Backward chaining for automated reasoning in\nnatural language. arXiv preprint arXiv:2212.13894.\nYoon Kim. 2021. Sequence-to-sequence learning with\nlatent neural grammars. Advances in Neural Infor-\nmation Processing Systems, 34:26302–26317.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nNeema Kotonya and Francesca Toni. 2020a. Explain-\nable automated fact-checking: A survey. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5430–5443, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nNeema Kotonya and Francesca Toni. 2020b. Explain-\nable automated fact-checking for public health claims.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7740–7754, Online. Association for Computa-\ntional Linguistics.\nNayeon Lee, Yejin Bang, Andrea Madotto, Madian\nKhabsa, and Pascale Fung. 2021. Towards few-\nshot fact-checking via perplexity. arXiv preprint\narXiv:2103.09535.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact verification\nwith kernel graph attention network. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7342–7351, On-\nline. Association for Computational Linguistics.\nVaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022.\nA survey on multi-hop question answering and gen-\neration. arXiv preprint arXiv:2204.09140.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097–6109, Florence, Italy. Association for Compu-\ntational Linguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\n6297\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-\nYen Kan, and William Yang Wang. 2021. Zero-shot\nfact verification by claim generation. arXiv preprint\narXiv:2105.14682.\nLiangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan\nLuu, William Yang Wang, Min-Yen Kan, and\nPreslav Nakov. 2023. Fact-checking complex claims\nwith program-guided reasoning. arXiv preprint\narXiv:2305.12744.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nKashyap Popat, Subhabrata Mukherjee, Andrew Yates,\nand Gerhard Weikum. 2018. DeClarE: Debunking\nfake news and false claims using evidence-aware\ndeep learning. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 22–32, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nSoumya Sanyal, Harman Singh, and Xiang Ren. 2022.\nFaiRR: Faithful and robust deductive reasoning over\nnatural language. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1075–1093,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. defend: Explainable fake news\ndetection. In Proceedings of the 25th ACM SIGKDD\ninternational conference on knowledge discovery &\ndata mining, pages 395–405.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nLin Tian, Xiuzhen Zhang, and Jey Han Lau. 2023. Meta-\ntroll: Few-shot detection of state-sponsored trolls\nwith transformer adapters. In Proceedings of the\nACM Web Conference 2023, pages 1743–1753.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498.\nDavid Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan,\nIz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi.\n2022a. SciFact-open: Towards open-domain scien-\ntific claim verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n4719–4734, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nDavid Wadden, Kyle Lo, Lucy Lu Wang, Arman Co-\nhan, Iz Beltagy, and Hannaneh Hajishirzi. 2022b.\nMultiVerS: Improving scientific claim verification\nwith weak supervision and full-document context. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 61–76, Seattle, United\nStates. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nDustin Wright, David Wadden, Kyle Lo, Bailey Kuehl,\nArman Cohan, Isabelle Augenstein, and Lucy Lu\nWang. 2022. Generating scientific claims for zero-\nshot scientific fact checking. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2448–2460, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nRuichao Yang, Xiting Wang, Yiqiao Jin, Chaozhuo Li,\nJianxun Lian, and Xing Xie. 2022. Reinforcement\nsubgraph reasoning for fake news detection. In Pro-\nceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 2253–\n2262.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\n6298\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. 2023. Answering\nquestions by meta-reasoning over multiple chains\nof thought. arXiv preprint arXiv:2304.13007.\nFei Yu, Hongbo Zhang, and Benyou Wang. 2023. Na-\nture language reasoning, a survey. arXiv preprint\narXiv:2303.14725.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan\nDuan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020.\nReasoning over semantic-level graph for fact check-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6170–6180, Online. Association for Computational\nLinguistics.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: Graph-based evidence aggregating and rea-\nsoning for fact verification. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 892–901, Florence, Italy.\nAssociation for Computational Linguistics.\nA Implementation Details about the\nBaselines\nIn this section, we give the implementation details\nfor the three baselines we used in our work.\nA.1 Large Language Models\nWe use llama models from decapoda-research on\nHugging Face. Note this is not the official model\nweight. Decapoda Research has converted orig-\ninal model weights to work with Transformers.\nFor llama-13B, we load the weight in its original\nfloat 16 precision. Due to limited GPU mem-\nory, we load llama-30B in int 8 precision.\nA.2 ProgramFC\nWe implement ProgramFC using the code provided\nby the authors and make necessary changes to ac-\ncept our custom data input.\n6299\nB Prompts\nListing 1: Chain-of-Thought (CoT) Decompose Prompt\nPlease tell me the necessary questions that need to be answered in order to verify the following claim:\nClaim: Howard University Hospital and Providence Hospital are both located in Washington, D.C.\n>>>>>>\nFollowup Question: Where is Howard Hospital located?\nFollowup Question: Where is Providence Hospital located?\n------\nClaim: An IndyCar race driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season.\n>>>>>>\nFollowup Question: Which Formula 1 car was designed by Peter McCool during the 2007 Formula One season?\nFollowup Question: Did an IndyCar driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season?\n------\nClaim: Sumo wrestler Toyozakura Toshiaki committed match-fixing, ending his career in 2011 that started in 1989.\n>>>>>>\nFollowup Question: When did Sumo wrestler Toyozakura Toshiaki ended his career?\nFollowup Question: What is Toyozakura Toshiaki’s occupation?\nFollowup Question: Did Sumo wrestler Toyozakura Toshiaki committed match-fixing?\n------\nClaim: In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight\ndivision at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United\nStates, and the world amateur welterweight title in Mexico City.\n>>>>>>\nFollowup Question: When was Alfredo Cornejo Cuevas born?\nFollowup Question: Did Alfredo Cornejo Cuevas win the gold metal in the welterweight division at the Pan American Games in\n1959?\nFollowup Question: Where was The Pan American Games in 1959 held?\nFollowup Question: Did Alfredo Cornejo Cuevas win the world amateur welterweight title in Mexico City?\n------\nClaim: %s\n>>>>>>\nListing 2: FOLK Decompose Prompt\nYou are given a problem description and a claim. The task is to:\n1) define all the predicates in the claim\n2) parse the predicates into followup questions\n3) answer the followup questions\nClaim: Howard University Hospital and Providence Hospital are both located in Washington, D.C.\n>>>>>>\nPredicates:\nLocation(Howard Hospital, Washington D.C.) ::: Verify Howard University Hospital is located in Washington, D.C.\nLocation(Providence Hospital, Washington D.C.) ::: Verify Providence Hospital is located in Washington, D.C.\nFollowup Question: Where is Howard Hospital located?\nFollowup Question: Where is Providence Hospital located?\n------\nClaim: An IndyCar race driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season.\n>>>>>>\nPredicates:\nDesigned(Peter McCool, a Formula 1 car) ::: Verify a Formula 1 car was designed by Peter McCool during the 2007 Formula One\nseason.\nDrive(An IndyCar race driver, a Formula 1 car) ::: Verify an IndyCar driver drove a Formula 1 car.\nFollowup Question: Which Formula 1 car was designed by Peter McCool during the 2007 Formula One season?\nFollowup Question: Did an IndyCar driver drove a Formula 1 car designed by Peter McCool during the 2007 Formula One season?\n------\nClaim: Thomas Loren Friedman has won more Pulitzer Prizes than Colson Whitehead\n>>>>>>\nPredicates:\nWon(Thomas Loren Friedman, Pulitzer Prize) ::: Verify the number of Pulitzer Prizes Thomas Loren Friedman has won.\nWon(Colson Whitehead, Pulitzer Prize) ::: Verify the number of Pulitzer Prizes Colson Whitehead has won.\nFollowup Question: How many Pulitzer Prize did Thomas Loren Friedman win?\nFollowup Question: How many Pulitzer Prize did Colson Whitehead win?\n------\nClaim: SkyHigh Mount Dandenong (formerly Mount Dandenong Observatory) is a restaurant located on top of Mount Dandenong,\nVictoria, Australia.\n>>>>>>\nPredicates:\nLocation(SkyHigh Mount Dandenong, top of Mount Dandenong, Victoria, Australia) ::: Verify that SkyHigh Mount Dandenong is\nlocated on top of Mount Dandenong, Victoria, Australia.\nKnown(SkyHigh Mount Dandenong, Mount Dandenong Observatory) ::: Verify that SkyHigh Mount Dandenong is formerly known as Mount\nDandenong Observatory.\nFollowup Question: Where is SkyHigh Mount Dandenong located?\nFollowup Question: Was SkyHigh Mount Dandenong formerly known as Mount Dandenong Observatory?\n------\nClaim: Shulin, a 33.1288 km (12.7911 sq mi) land located in New Taipei City, China, a country in East Asia, has a total\npopulation of 183,946 in December 2018.\n>>>>>>\nPredicates:\nLocation(Shulin, New Taipei City, Chian) ::: Verify that Shulin is located in New Taipei City, China.\nPopulation(Shulin, 183,946) ::: Verify that Shulin has a total population of 183,946 in December 2018.\nFollowup Question: Where is Shulin located?\nFollowup Question: What is the population of Shulin?\n------\nClaim: Sumo wrestler Toyozakura Toshiaki committed match-fixing, ending his career in 2011 that started in 1989.\n>>>>>>\nPredicates:\n6300\nEnding(Toyozakura Toshiaki, his career in 2011) ::: Verify that Toyozakura Toshiaki ended his career in 2011.\nOccupation(Toyozakura Toshiaki, sumo wrestler) ::: Verify that Toyozakura Toshiaki is a sumo wrestler.\nCommit(Toyozakura Toshiaki, match-fixing) ::: Verify that Toyozakura Toshiaki committed match-fixing.\nFollowup Question: When did Sumo wrestler Toyozakura Toshiaki ended his career?\nFollowup Question: What is Toyozakura Toshiaki’s occupation?\nFollowup Question: Did Sumo wrestler Toyozakura Toshiaki committed match-fixing?\n------\nClaim: In 1959, former Chilean boxer Alfredo Cornejo Cuevas (born June 6, 1933) won the gold medal in the welterweight\ndivision at the Pan American Games (held in Chicago, United States, from August 27 to September 7) in Chicago, United\nStates, and the world amateur welterweight title in Mexico City.\n>>>>>>\nPredicates:\nBorn(Alfredo Cornejo Cuevas, June 6 1933) ::: Verify that Alfredo Cornejo Cuevas was born June 6 1933.\nWon(Alfredo Cornejo Cuevas, the gold metal in the welterweight division at the Pan American Games in 1959) ::: Verify that\nAlfredo Cornejo Cuevas won the gold metal in the welterweight division at the Pan American Games in 1959.\nHeld(The Pan American Games in 1959, Chicago United States) ::: Verify that The Pan American Games in 1959 was held in Chicago\nUnited States.\nWon(Alfredo Cornejo Cuevas, the world amateur welterweight title in Mexico City).\nFollowup Question: When was Alfredo Cornejo Cuevas born?\nFollowup Question: Did Alfredo Cornejo Cuevas win the gold metal in the welterweight division at the Pan American Games in\n1959?\nFollowup Question: Where was The Pan American Games in 1959 held?\nFollowup Question: Did Alfredo Cornejo Cuevas win the world amateur welterweight title in Mexico City?\n------\nClaim: The birthplace of American engineer Alfred L.Rives is a plantation near Monticello, the primary residence of Thomas\nJefferson.\n>>>>>>\nPredicates:\nBirthplace(Alfred L. Rives, a plantation) ::: Verify The birthplace of American engineer Alfred L.Rives is a plantation\nPrimary residence(Thomas Jefferson, Monticello) ::: Verify Monticello, the primary residence of Thomas Jefferson.\nNear(a planation, Monticello) ::: Verify A plantation is near Monticello\nFollowup Question: Where is the birthplace of Alfred L. Rives?\nFollowup Question: Where is the primary residence of Thomas Jefferson?\nFollowup Question: Is the birthplace of Alfred L. Rives near the residence of Thomas Jefferson?\n------\nClaim: %s\n>>>>>>\nListing 3: Direct Reasoning Prompt\nPlease verify the following claim and provide explanations:\nClaim: The woman the story behind Girl Crazy is credited to is older than Ted Kotcheff.\n>>>>>>\nThis claim is: [NOT_SUPPORTED]\nHere are the reasons: The woman behind the story Girl Crazy is Hampton Del Ruth, who was born on September 7, 1879.\nTed Kotcheff was born on April 7, 1931. Hapmpton Del Ruth is not older than Ted Kotcheff.\n------\nClaim: A hockey team calls the 70,000 capacity Madison Square Garden it’s home. That team, along with the New York Islanders,\nand the New Jersey Devils NHL franchise, are popular in the New York metropolitan area.\n>>>>>>\nThis claim is: [NOT_SUPPORTED]\nHere are the reasons: Madison Square Garden is the home to New York Rangers and New York Islanders. Both are popular in the\nNew York metropolitan area.\nMadison Square Garden has a capacity of 19,500, not 70,0000.\n------\nClaim: The writer of the song Girl Talk and Park So-yeon have both been members of a girl group.\n>>>>>>\nThis claim is: [SUPPORTED]\nHere are the reasons: Tionne Watkins is the writer of the song Girl Talk. She was a member of the girl-group TLC.\nPark So-yeon is part of a girl group. Therefore, both Tioone Watkins and Park So-yeon have been members of a girl group.\n------\nClaim: Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt in the German state of Hesse and the fifth-largest city in\nGermany.\n>>>>>>\nThis claim is: [SUPPORTED]\nHere are the reasons: Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nFrankfurt is in the German state of Hesse and the fifth-largest city in Germany.\n------\nClaim: %s\n>>>>>>\nListing 4: CoT Reasoning Prompt\nAnswer the following SUPPORTED / NOT_SUPPORTED questions:\nIs it true that The woman the story behind Girl Crazy is credited to is older than Ted Kotcheff. ?\nLet’s think step by step.\nGirl Crazy ’s story is credited to Hampton Del Ruth.\nHampton Del Ruth was born on September 7 , 1879.\nTed Kotcheff was born on April 7 , 1931.\n>>>>>>\nTherefore , the answer is: [NOT_SUPPORTED]\nHere are the reasons: The woman behind the story Girl Crazy is Hampton Del Ruth, who was born on September 7, 1879.\nTed Kotcheff was born on April 7, 1931. Hapmpton Del Ruth is not older than Ted Kotcheff.\n------\nIs it true that A hockey team calls the 70,000 capacity Madison Square Garden it’s home. That team, along with the New York\nIslanders, and the New Jersey Devils NHL franchise, are popular in the New York metropolitan area. ?\nLet’s think step by step.\nMadison Square Garden hosts approximately 320 events a year. It is the home to the New York Rangers of the National Hockey\nLeague.\n6301\nMadison Square Garden has a capacity of 19.500.\nThe New York Islanders are a professional ice hockey team based in Elmont, New York. ...\n>>>>>>\nTherefore, the answer is: [NOT_SUPPORTED]\nHere are the reasons: Madison Square Garden is the home to New York Rangers and New York Islanders. Both are popular in the\nNew York metropolitan area.\nMadison Square Garden has a capacity of 19,500, not 70,0000.\n------\nIs it true that The writer of the song Girl Talk and Park So-yeon have both been members of a girl group. ?\nLet’s think step by step.\nTionne Watkins is the writer of the song Girl Talk.\nPark Soyeon is a South Korean singer. She is a former member of the kids girl group I& Girls.\nWatkins rose to fame in the early 1990s as a member of the girl-group TLC\n>>>>>>\nTherefore, the answer is: [SUPPORTED]\nHere are the reasons: Tionne Watkins is the writer of the song Girl Talk. She was a member of the girl-group TLC.\nPark So-yeon is part of a girl group. Therefore, both Tioone Watkins and Park So-yeon have been members of a girl group.\n------\nIs it true that Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt in the German state of Hesse and the fifth-largest city\nin Germany. ?\nLet’s think step by step.\nWerner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nFrankfurt is in the German state of Hesse.\nFrankfurt is the fifth-largest city in Germany.\n>>>>>>\nTherefore, the answer is: [SUPPORTED]\nHere are the reasons: Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nFrankfurt is in the German state of Hesse and the fifth-largest city in Germany.\n------\nIs it true that %s?\nLet’s think step by step:\n%s\n>>>>>>\nListing 5: Self-Ask Reasoning Prompt\nGiven a question and a context, provide a SUPPORTED or NOT_SUPPORTED and explain why.\nQuestion:\nIs it true that The writer of the song Girl Talk and Park So-yeon have both been members of a girl group. ?\nContext:\nWho is the writer of the song Girl Talk? Tionne Watkins is the writer of the song Girl Talk.\nIs Park So-yeon a member of a girl group? Park Soyeon is a South Korean singer. She is a former member of the kids girl group\nI& Girls.\nIs the writer of the song Girl Talk a member of a girl group? Watkins rose to fame in the early 1990s as a member of the girl-\ngroup TLC\n>>>>>>\nThe claim is [SUPPORTED]. Here are the reasons, Tionne Watkins is the writer of the song Girl Talk, and she fame in the early\n1990s as a member of the girl-group TLC.\nPark Soyeon is a South Korean singer. She is a former member of the kids girl group I& Girls.\n------\nQuestion:\nIs it true that A hockey team calls the 70,000 capacity Madison Square Garden it’s home. That team, along with the New York\nIslanders, and the New Jersey Devils NHL franchise, are popular in the New York metropolitan area. ?\nContext:\nWhich hocky team calls Madison Square Garden Home? Madison Square Garden hosts approximately 320 events a year. It is the home\nto the New York Rangers of the National Hockey League\nWhat is the capacity of Madison Square Garden? Madison Square Garden has a capacity of 19.500.\nIs New York Islanders popular in New York Metropolitan area? The New York Islanders are a professional ice hockey team based\nin Elmont, New York. ...\n>>>>>>\nThe claim is [NOT_SUPPORTED]. Here are the reasons, Madison Square Garden hosts approximately 320 events a year. It is the\nhome to the New York Rangers of the National Hockey League.\nand the New York Islanders are a professional ice hockey team based in Elmont, New York. Madison Square Garden has a capacity\nof 19.500, not 70,000.\n------\nQuestion:\nIs it true that Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt in the German state of Hesse and the fifth-largest city\nin Germany. ?\nContext:\nWhere was Werner Gunter Jaff\\u00e9 Fellner born? Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nWhich state is Frankfurt in? Frankfurt is in the German state of Hesse.\n>>>>>>\nThe claim is [SUPPORTED]. Here are the reasons, Werner Gunter JafFf\\u00e9 Fellner was born in Frankfurt and Frankfurt is in\nthe German state of Hesse.\n------\nQuestion:\nIs it true that The American lyricist Tom Jones, born in 1928, co-authored the screenplay for the musical film The Fantastics\n. ?\nContext:\nWhen was Tom Jones born? Thomas Jones Woodward was born in Pontypridd, South Wales, Great Britain on June 7, 1940\nWhat is Tome Jones nationality? Sir Thomas Jones Woodward OBE is a Welsh singer.\nWho co-author the musical film The Fantastics? Tome Jones co-authored the musical film The Fantastics.\n>>>>>>\nThe claim is [NOT_SUPPORTED]. Here are the reasons, Sir Thomas Jones Woodward OBE is a Welsh singer and Tome Jones co-authored\nthe musical film The Fantastics,\nbut Thomas Jones Woodward was born in Pontypridd, South Wales, Great Britain on June 7, 1940. Thomas Jones is British, not\nAmerican.\n------\nQuestion: Is it true that %s?\n6302\nContext:\n%s\n>>>>>>\nListing 6: FOLK Reasoning Prompt\nGiven a question and a context, provide a [SUPPORTED] or [NOT_SUPPORTED] answer and explain why.\nQuestion:\nIs it true that The writer of the song Girl Talk and Park So-yeon have both been members of a girl group.?\nContext:\nWrite(the writer, the song Girl Talk) ::: Verify that the writer of the song Girl Talk\nMember(Park So-yeon, a girl group) ::: Verify that Park So-yeon is a memeber of a girl group\nMember(the writer, a girl group) ::: Verify that the writer of the song Girl Talk is a member of a gril group\nWho is the writer of the song Girl Talk? Tionne Watkins is the writer of the song Girl Talk.\nIs Park So-yeon a member of a girl group? Park Soyeon is a South Korean singer. She is a former member of the kids girl group\nI& Girls.\nIs the writer of the song Girl Talk a member of a girl group? Watkins rose to fame in the early 1990s as a member of the girl-\ngroup TLC\n>>>>>>\nPrediction:\nWrite(Tionne Watkins, the song Girl Talk) is True because Tionne Watkins is the writer of the song Girl Talk.\nMember(Park So-yeon, a girl group) is True because Park Soyeon is a South Korean singer. She is a former member of the kids\ngirl group I& Girls.\nMember(Tionne Watkins, a girl group) is True because Watkins rose to fame in the early 1990s as a member of the girl-group TLC\nWrite(Tionne Watkins, the song Girl Talk) && Member(Park So-yeon, a girl group) && Member(Tionne Watkins, a girl group) is\nTrue.\nThe claim is [SUPPORTED].\nExplanation:\nTionne Watkins, a member of the girl group TLC in the 1990s, is the writer of the song \"Girl Talk.\"\nPark Soyeon, a South Korean singer, was formerly part of the girl group I& Girls.\nTherefore, both Watkins and Park Soyeon have been members of girl groups in their respective careers.\n------\nQuestion:\nIs it true that A hockey team calls the 70,000 capacity Madison Square Garden it’s home. That team, along with the New York\nIslanders, and the New Jersey Devils NHL franchise, are popular in the New York metropolitan area.?\nContext:\nHome(a hocky team, Madison Square Garden) ::: Verify that a hockey team calls Madison Square Garden its home.\nCapacity(Madison Square Garden, 70,000) ::: Verify that Madison Square Garden has capacity of 70,000.\nPopular(New York Islanders, New York Metropolitan area) ::: Verify that New York Islanders are popular in the New York\nmetropolitan area.\nWhich hocky team calls Madison Square Garden Home? Madison Square Garden hosts approximately 320 events a year. It is the home\nto the New York Rangers of the National Hockey League\nWhat is the capacity of Madison Square Garden? Madison Square Garden has a capacity of 19.500.\nIs New York Islanders popular in New York Metropolitan area? The New York Islanders are a professional ice hockey team based\nin Elmont, New York. ...\n>>>>>>\nPrediction:\nHome(New York Rangers, Madison Square Garden) is True because Madison Square Garden hosts approximately 320 events a year. It\nis the home to the New York Rangers of the National Hockey League\nCapacity(Madison Square Garden, 70,000) is False because Madison Square Garden has a capacity of 19.500.\nPopular(New York Islanders, New York Metropolitan area) is True because The New York Islanders are a professional ice hockey\nteam based in Elmont, New York. ...\nHome(New York Rangers, Madison Square Garden) && Capacity(Madison Square Garden, 70,000) && Popular(New York Islanders, New\nYork Metropolitan area) is False.\nThe claim is [NOT_SUPPORTED].\nExplanation:\nThe New York Rangers, along with the New York Islanders and the New Jersey Devils, are popular National Hockey League (NHL)\nteams in the New York metropolitan area.\nMadison Square Garden, a well-known venue in New York City, has a capacity of approximately 19,500, not 70,000.\n------\nQuestion:\nIs it true that Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt in the German state of Hesse and the fifth-largest city\nin Germany.?\nContext:\nBorn(Werner Gunter Jaff\\u00e9 Fellner, Frankfurt)\nState(Frankfurt, the German state of Hesse)\nWhere was Werner Gunter Jaff\\u00e9 Fellner born? Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nWhich state is Frankfurt in? Frankfurt is in the German state of Hesse.\n>>>>>>\nPrediction:\nBorn(Werner Gunter Jaff\\u00e9 Fellner, Frankfurt) is True because Werner Gunter Jaff\\u00e9 Fellner was born in Frankfurt.\nState(Frankfurt, the German state of Hesse) is True because Frankfurt is in the German state of Hesse.\nBorn(Werner Gunter Jaff\\u00e9 Fellner, Frankfurt) && State(Frankfurt, the German state of Hesse) is True.\nThe claim is [SUPPORTED].\nExplanation:\nWerner Gunter Jaffe Fellner was born in Frankfurt, which is both in the German state of Hesse and the fifth-largest city in\nGermany.\n------\nQuestion:\nIs it true that The American lyricist Tom Jones, born in 1928, co-authored the screenplay for the musical film The Fantastics\n.?\nContext:\nBorn(Tom Jones, 1928)\nNationality(Tom Jones, American)\nCo-author(Tome Jones, the musical film The Fantastics)\nWhen was Tom Jones born? Thomas Jones Woodward was born in Pontypridd, South Wales, Great Britain on June 7, 1940\n6303\nWhat is Tome Jones nationality? Sir Thomas Jones Woodward OBE is a Welsh singer.\nWho co-author the musical film The Fantastics? Tome Jones co-authored the musical film The Fantastics.\n>>>>>>\nPrediction:\nBorn(Tom Jones, 1928) is False because Thomas Jones Woodward was born in Pontypridd, South Wales, Great Britain on June 7,\n1940\nNationality(Tom Jones, American) is False because Thomas Jones Woodward is a British singer.\nCo-author(Tome Jones, the musical film The Fantastics) is True because Tome Jones co-authored the musical film The Fantastics.\nBorn(Tom Jones, 1928) && Nationality(Tom Jones, American) && Co-author(Tome Jones, the musical film The Fantastics) is False.\nThe claim is [NOT_SUPPORTED].\nExplanation:\nThomas Jones Woodward was born in Pontypridd, South Wales, Great Britain on June 7, 1940. He is a british singer.\nThomas Jones co-authored the musical film The Fantastics.\n------\nQuestion: Is it true that %s?\nContext:\n%s\n>>>>>>\n\"\"\"\nC Manual Evaluation Example\nPlease rank the following three explanations based on the following criteria,\nties are allowed:\n- Coverage: The explanation can identify and include all salient information and\nimportant points that contribute to verifying the claim.\n- Soundness: The explanation is logically sound and does not contain any information\ncontradictory to the claim.\n- Readability: The explanation is presented in a clear and coherent manner, making it\neasily understandable.\nClaim: Anat Berko was elected to the Knesset as Likud, an alliance of right-wing \nparties, had won 30 seats.\n    \nThe claim is [SUPPORTED]. Here are the reasons: Anat Berko was elected to the \nKnesset as Likud, an alliance of right-wing parties, had won 30 seats in the \nNovember 2015 legislative elections.\n    \nThe claim is: [SUPPORTED]. Here are the reasons: Anat Berko was a candidate of \nthe Likud party. Likud won the 2015 election with 30 seats. Therefore, Anat Berko \nwas elected to the Knesset as Likud had won 30 seats.\n    \nThe claim is [SUPPORTED]. Here are the reasons, Anat Berko was placed 23rd on the \nLikud list, and Likud won the 2015 election, winning 30 seats to the Zionist \nUnion's 24. She was elected to Knesset as Likud had won 30 seats.\n    \nCoverage: \nSoundness: \nReadability: \nS u b m i t \n6304",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.755186915397644
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5899893045425415
    },
    {
      "name": "Process (computing)",
      "score": 0.5479398965835571
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5074934363365173
    },
    {
      "name": "Natural language processing",
      "score": 0.5017204284667969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49906349182128906
    },
    {
      "name": "Order (exchange)",
      "score": 0.42901021242141724
    },
    {
      "name": "Data science",
      "score": 0.33486342430114746
    },
    {
      "name": "Programming language",
      "score": 0.19207808375358582
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    }
  ]
}