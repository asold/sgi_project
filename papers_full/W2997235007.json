{
  "title": "Graph-Based Transformer with Cross-Candidate Verification for Semantic Parsing",
  "url": "https://openalex.org/W2997235007",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2115292856",
      "name": "Bo Shao",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2104159869",
      "name": "Yeyun Gong",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1977137347",
      "name": "Weizhen Qi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2134697318",
      "name": "GuiHong Cao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2798666829",
      "name": "Jianshu Ji",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2157861970",
      "name": "Xiaola Lin",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2115292856",
      "name": "Bo Shao",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A1977137347",
      "name": "Weizhen Qi",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2134697318",
      "name": "GuiHong Cao",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2798666829",
      "name": "Jianshu Ji",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6674880660",
    "https://openalex.org/W2189089430",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W6669997747",
    "https://openalex.org/W6718734171",
    "https://openalex.org/W2227250678",
    "https://openalex.org/W2161002933",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W6674709584",
    "https://openalex.org/W2126170172",
    "https://openalex.org/W2964878476",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6712081154",
    "https://openalex.org/W4236265809",
    "https://openalex.org/W2393319904",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W2951278025",
    "https://openalex.org/W2963542836",
    "https://openalex.org/W2087165009",
    "https://openalex.org/W2892248135",
    "https://openalex.org/W2552839021",
    "https://openalex.org/W2096968458",
    "https://openalex.org/W2963357517",
    "https://openalex.org/W2888128175",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963655793",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2963898730",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2914081143",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2097647324",
    "https://openalex.org/W1947758080",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2414484917",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2782031709",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963794306",
    "https://openalex.org/W2798296074",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W62369917"
  ],
  "abstract": "In this paper, we present a graph-based Transformer for semantic parsing. We separate the semantic parsing task into two steps: 1) Use a sequence-to-sequence model to generate the logical form candidates. 2) Design a graph-based Transformer to rerank the candidates. To handle the structure of logical forms, we incorporate graph information to Transformer, and design a cross-candidate verification mechanism to consider all the candidates in the ranking process. Furthermore, we integrate BERT into our model and jointly train the graph-based Transformer and BERT. We conduct experiments on 3 semantic parsing benchmarks, ATIS, JOBS and Task Oriented semantic Parsing dataset (TOP). Experiments show that our graph-based reranking model achieves results comparable to state-of-the-art models on the ATIS and JOBS datasets. And on the TOP dataset, our model achieves a new state-of-the-art result.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nGraph-Based Transformer with\nCross-Candidate Veriﬁcation for Semantic Parsing\nBo Shao,∗1,2 Y eyun Gong,2 Weizhen Qi,2,3 Guihong Cao,4 Jianshu Ji,4 Xiaola Lin1\n1Sun Yat-sen University\n2Microsoft Research Asia\n3University of Science and Technology of China\n4Microsoft AI and Research, Redmond W A, USA\nshaobo2@mail2.sysu.edu.cn, {yegong, gucao, jianshuj}@microsoft.com,\nweizhen@mail.ustc.edu.cn, linxl@mail.sysu.edu.cn\nAbstract\nIn this paper, we present a graph-based Transformer for se-\nmantic parsing. We separate the semantic parsing task into\ntwo steps: 1) Use a sequence-to-sequence model to generate\nthe logical form candidates. 2) Design a graph-based Trans-\nformer to rerank the candidates. To handle the structure of\nlogical forms, we incorporate graph information to Trans-\nformer, and design a cross-candidate veriﬁcation mechanism\nto consider all the candidates in the ranking process. Further-\nmore, we integrate BERT into our model and jointly train\nthe graph-based Transformer and BERT. We conduct exper-\niments on 3 semantic parsing benchmarks, ATIS, JOBS and\nTask Oriented semantic Parsing dataset (TOP). Experiments\nshow that our graph-based reranking model achieves results\ncomparable to state-of-the-art models on the ATIS and JOBS\ndatasets. And on the TOP dataset, our model achieves a new\nstate-of-the-art result.\nIntroduction\nSemantic parsing is a classic NLP task that has attracted a\nhuge amount of attention recently. It aims at mapping a nat-\nural language sentence into a logical form. With the rapid\ndevelopment of natural language processing, semantic pars-\ning has been used in various applications, such as question\nanswering (Kwiatkowski et al. 2011), task-oriented dialog\nsystems (Yih et al. 2015) and interpreting instructions (Artzi\nand Zettlemoyer 2013).\nRecently, many approaches based on the sequence-to-\nsequence(S2S) model have been successfully used for se-\nmantic parsing (Dong and Lapata 2016; Yin and Neubig\n2017; Dong and Lapata 2018). In the analysis of (Gupta\net al. 2018), the accuracy of exact match on TOP dataset\nfor RNN based sequence-to-sequence model is 75.3%, but\nbeam search with the size of 5 can cover 88.76% golden\ntarget sequences. Since the exact match result is computed\nby the top1 prediction from beam search, it means that the\ncandidates from the beam search have a greater chance of\ncontaining the golden target.\n∗Work done while this author was visiting Microsoft Research.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nTo select the correct target from these candidates, it is\nnecessary to use a ranking model to rerank these candi-\ndates. There exist many applications that use sequence-\nto-sequence with reranking methods (Wen et al. 2015;\nEinolghozati et al. 2019). (Wen et al. 2015) uses recur-\nrent generation model with a convolutional ranker for a di-\nalogue system. In (Einolghozati et al. 2019), they propose\na SVM+language model to rerank the candidates generated\nby generation model. These methods achieve good perfor-\nmance, since the ranking model can capture the global in-\nformation in questions and decoding candidates simultane-\nously. However, the general ranking model can not capture\nthe strong hierarchical structures in the logical forms. For\nexample, Figure 1 shows an example of the TOP dataset.\nThe logical form in this dataset is a task oriented represen-\ntation of a dialog system. It contains a tree structure, which\nalso can be considered as a special graph.\nQuestion: How far is the coffee shop\nIN:GET_INSTANCE\nSL:DESTINATION\nI/g7:GE/g5_/g9E/g11/g5A/g12/g9A/g7/g5_LTCA/g5IT/g7\nIN:GET_INSTANCEthe shop\nHow far is\ncoffee\nLogical Form: [IN:GET_DISTANCE How far is [SL:DESTINATION [IN:GET_LOCATION \n[SL:CATEGORY_LOCATION the coffee shop ] ] ] ]\nFigure 1: An example of Task Oriented Parsing (TOP)\ndataset.\nIn this paper, we propose a ranking model for logical\nform reranking based on Transformer (Vaswani et al. 2017),\ncompared with other methods based on tree-structure, our\ngraph-based transformer is more scalable, which can han-\ndle different semantic graphs in an efﬁcient way. To capture\nthe tree structure information of logical forms, we extend\ntransformer with a Graph-based attention strategy. We in-\ncorporate the structure information of the logical forms to\nmulti-head attention in the transformer and generate graph-\naware interaction features. This method makes each token\n8807\ninteract with parent nodes in the semantic tree. Further-\nmore, to incorporate cross-candidate information, we enable\neach logical form candidate to interact with the other candi-\ndates based on their representations, which is inspired by V-\nNET (Wang et al. 2018). V-NET is applied on multi-passage\nreading comprehension dataset. During training, each sam-\nple in their task contains several extracted candidate answer\nspans which are extracted from the given passage. To better\nselect the correct one, V-NET computes the context vector\nusing the selected answers by the boundary model to help\nthe ranking score computation. It is a novel way to aggre-\ngate information from all candidates in the ranking model.\nIn our model, different from V-NET, using other answer can-\ndidates as a context, we use the similarity score between the\ncurrent candidate and all other candidates through an inter-\naction strategy to help the ranking score computation.\nThe main contributions of this paper are:\n• We extend the transformer model with a graph-based\nmatching strategy to enable it to capture the structure in-\nformation of logical forms.\n• We design a cross-candidate veriﬁcation method using the\nsimilarity between each candidate and the other candi-\ndates to boost the ranking performance.\n• We conduct experiments on 3 semantic parsing datasets,\nTOP, ATIS and JOBS. Our reranking model with a basic\nsequence-to-sequence generation model achieves a new\nstate-of-the-art result on the TOP dataset and compara-\nble results to the state-of-the-art models on the ATIS and\nJOBS datasets.\nModel\nSequence-to-sequence based models have been successfully\napplied to semantic parsing. They are trained by annotated\n<question, logical form > pairs without human designed\ntemplates and features. In recent semantic parsing meth-\nods (Dong and Lapata 2018), researchers decode the can-\ndidate logical forms using beam search method and select\ntop 1 candidate as the decoding result.\nThese methods have achieved good performance in var-\nious semantic parsing tasks. However, we ﬁnd that beam\nsearch can recall most correct logical forms, but some of\nthem can not be ranked in the ﬁrst place. In this work, we\nuse the reranking based semantic parsing method which ﬁrst\ngenerates candidates using the generation model, and then\nrerank the candidates by a ranking model.\nIn the following sections, we ﬁrst brieﬂy introduce the\ngeneration model we used in this work and then we intro-\nduce our model for reranking in detail.\nGeneration Model\nThe generation model in this paper is based on the general\nencoder-decoder framework, using a bidirectional RNN-\nGRU (Cho et al. 2014) layer as an encoder and a unidirec-\ntional RNN-GRU layer as a decoder, which has been applied\nin semantic parsing tasks in (Dong and Lapata 2016; Rabi-\nnovich, Stern, and Klein 2017; Dong and Lapata 2018). With\nthe input question of Q =[ x\n1,x2...x|Q|],xi ∈ Vq the model\ndecodes the target logical form L =[ l1,l2,...l|L|],li ∈ Vl,\nwhere Vq and Vl are the source and target vocabularies. Our\nbaseline model aims to estimate P(L|Q), and the condi-\ntional probability can be formulated as follows:\np(L|Q)=\n|L|∏\nt=1\np(lt|lt−1,...,l 1,Q) (1)\nWe will not introduce the implementation of the\nsequence-to-sequence model in detail because of its popu-\nlarity these years. Generally, the encoder uses embeddings\nof each token in question Q as input, and generates hidden\nstate r\ni at the ith position of Q. To decode the target se-\nquence with the hidden state ri, attention mechanism(Bah-\ndanau, Cho, and Bengio 2014) is used in the decoder:\net\ni = uT tanh(We[ri; st]+ be)\nat\ni = et\ni\n∑|Q|\nj=1 et\nj\n; ct =\n|Q|∑\ni=1\nat\niri\n(2)\nwhere ri is the encoder hidden state and st is the decoder\nhidden state, ct is the context vector at time stampt, uT , We\nbe are trainable parameters.\nSpeciﬁcally, to solve the out-of-vocabulary (OOV) words\nin the target dictionary Vl, we leverage the copy mechanism\nin (See, Liu, and Manning 2017). We use the attention distri-\nbution a\nt\ni as the copy probability from the source sequence\nand combine it with the distribution of generating probabil-\nity in the target dictionary by a gate mechanism.\nReranking Model\nIn this section, we introduce our Graph-based Transformer\nwith Cross-candidate Veriﬁcation (GTCV) model. To extract\nbetter sentence representations, we integrate BERT to our\nranking model with a graph-aware matching strategy and\ncross-candidate veriﬁcation.\nBERT Encoder We will ﬁrst brieﬂy introduce the BERT\nencoder in our model. Given a list of target logical form can-\ndidates LC =[ L\n1,L2,...,L c] from generation model,where\nLi is a candidate, and their labels Y =[ y1,y2,...,y c],yi ∈\n{0,1} indicating whether the candidate is correct, where c\nis the number of all candidates, and the input question Q.\nWe then make the question and each logical form with the\ncorresponding label as an input of the our model. The tokens\nof all pairs are packed into a token sequence S as “[CLS] Q\n[SEP] L\ni [SEP]”, where [CLS], [SEP] are special tokens to\nseparate the tokens of question Q and the ith logical form\ncandidate Li. The representation of each token is the sum of\nthree types of embedding, WordPiece embedding (Wu et al.\n2016), Position embedding indicating the position of input\ntokens, and Segment embedding, which is used to indicate\nthe question segment and logical form segment.\nThen, the representation of S is fed into the BERT\nencoder which is a multi-layer bidirectional Trans-\nformer(Vaswani et al. 2017). For the implementation of\nits architecture, readers can refer to (Vaswani et al. 2017;\nDevlin et al. 2018). BERT encodes the input token se-\nquence S into a context aware representation H\ni =\n8808\n+\u0015\n+RZ\u0003IDU\u0003LV\u0003WKH\u0003\nFRIIHH\u0003VKRS\nJ\u0014 J\u0015 J\u0015 JF\n3DLU\u0010ZLVH\u0003\nVLPLODULW\\\nY\u0014 Y\u0015 Y\u0016 YF\n)LQDO\n6FRUH\n4 *HQHUDWRU\u0003\n0RGHO\n/\u0014 /\u0015 /\u0016 /F\n%HDP\u00036HDUFK\n4\u001d+RZ\u0003IDU\u0003LV\u0003WKH\u0003FRIIHH\u0003VKRS\n/)\u001d>,1\u001d*(7B',67$1&(\u0003+RZ\u0003IDU\u0003LV\u0003>6/\u001d'(67,1$7,21\u0003>,1\u001d*(7B/2&$7,21\u0003>6/\u001d&$7(*25<B/2&$7,21\u0003WKH\u0003FRIIHH\u0003VKRS\u0003@\u0003@\u0003@\u0003@\n%(57\u0003(QFRGHU\n+\u0014\n+\u0016\n+\u0017\n+F\n*UDSK\u0003\n0DWFKLQJ\n&URVV\u0010\n&DQGLGDWH\u0003\n9HULILFDWLRQ\n*UDSK\u0010$ZDUHG\u0003\n0DWFKLQJ\n\u0011\u0011\u0011\n\u0011\u0011\u0011\n\u0011\u0011\u0011\nP\n,1\u001d*(7B,167$1&(\n6/\u001d'(67,1$7,21\n,1\u001d*(7B5(67$85$17B/2&$7,21\n,1\u001d*(7B,167$1&(WKH VKRS\n+RZ\u0003IDU\u0003LV\nFRIIHH0DWFKLQJ\n6FRUH\n9HULILFDWLRQ\n6FRUH\n+\u0014\nFigure 2: Overview of our method for semantic parsing.\n{h0,h1,...,h |S|−1} for the ith candidate. Then Hi is used\nin our graph-based transformer layer.\nTo directly use BERT in the ranking task, we extract the\nﬁnal hidden state of the ﬁrst token, h0, corresponding to the\nspecial token “[CLS]”. The label probabilities are computed\nwith h\n0 by a standard softmax layer. Finally, all of the pa-\nrameters of the model are ﬁne-tuned to maximize the log-\nprobability of the correct label in the ranking task.\nGraph Matching Although BERT has proven its effec-\ntiveness for text sequence matching tasks, there still exists a\nlimitation to matching questions and logical form candidates\nin our task. The two sequences can only interact through\na general attention mechanism in the transformer, which is\nnot enough to capture the structure information in complex\nmatching tasks, since it is based on transformer and only\nuses position embedding to encode the position informa-\ntion. To overcome the limitation, we leverage a hierarchical\ngraph-aware matching method to extend BERT for our task.\nTo better encode the structure information, we add extra n\nlayers of graph-based transformer for graph matching, incor-\nporated with a masked attention mechanism, which uses an\nadjacent matrix M as mask matrix. This mask matrix con-\ntains the structure information of the parsing tree.\nFor each transformer block, we ﬁrst apply a multi-head\nself-attention on input features. We setZ\n0 = Hi as the input\nof the ith candidate, and the attention matrix in each head is\ncomputed as follows:\nA = softmax((WQZ0,W K ZT\n0 )/\n√\ndk) (3)\nwhere WQ and WK are trainable parameters, dk is the di-\nmension of Z0.\nThis basic attention matrix in the transformer is a word-\nto-word attention that is not suitable in this task because it\nmakes every token attend to each other and can not incorpo-\nrate structure information in matching. Thus, we propose a\nmask attention method based on the adjacency matrix of the\nparsing tree.\nTo generate the adjacency matrix, we ﬁrst use the brack-\neting matching method to check the validation of candidate\nparsing trees. If it is an invalid parsing tree, we will dis-\ncard it. For each logical form, we make the nonterminal to-\nkens as nodes, the brackets to indicate each layer of tree,\nand the parameters in logical form as leaves. The procedure\nhas been used in previous methods like SEQ2TREE (Dong\nand Lapata 2016). After that, we ﬁrst initialize a zero matrix\nM ∈ R\n|L|∗|L|, then set Mi,j to 1, which represents that lj\nis the parent of the token li. It guarantees that each token\nin logical form will only interact with its corresponding par-\nents and not with nodes in different sub-trees. After process-\ning all tokens in question q, we will obtain the the adjacency\nmatrix M as a mask matrix.\nThen, we combine the mask adjacency matrix with the\nattention matrix:\nG\ni,j =\n{Ai,jMi−|Q|−2,j−|Q|−2 i,j > |Q|+2\nAi,j other\nAnd compute the interaction features:\nZ∗\ni = GWZ Zi −1 (4)\nwhere WZ are the trainable parameters andZ∗\ni are the inter-\naction features between the question in ith layer and logical\nform in one head of the multi-head attention mechanism. We\nconcatenate all features from each head as Z\ni. We apply the\noperation n times and acquire the graph aware features Zn.\nWe then apply the average pooling on Zn to obtain the ﬁnal\ngraph based representation Z.\n8809\nFinally, we combine the interaction feature with the ﬁrst\nhidden state h0 of the BERT encoder:\ng =[ Z : h0]\nm = sigmoid(Wgg + bg) (5)\nwhere Wg and bg are trainable parameters, and m is the\nmatching score of the graph matching.\nCross-candidate Veriﬁcation The BERT encoder and\nGraph matching method focus on encoding the question\nand logical form better and on extracting the interaction\nrepresentation. However, the candidates from beam search\nmethod are always similar, which is hard for the reranking\ntask. Furthermore, since the input contains a list of can-\ndidates, aggregating the information across all candidates\nshould be useful in this task.\nWe can observe that each subtree of the correct logical\nform usually appears in the candidates. If we assume that\none of the candidates is the correct one, then each part of\nthis candidates is likely to be covered by other candidates.\nThus the conﬁdence for one candidate to be the correct one\ncan be implied by the similarity to other candidates. We pro-\npose a method to enable our model to consider all candidates\nand to verify the candidate through their similarity with each\nother. To apply this operation, we feed all candidates with\nthe question into the model simultaneously as a list-wise in-\nput. For each candidate, we compute their representations\ng\ni for the ith candidate by Eq. 5. gi can be considered as\nthe representation of each candidate containing the structure,\nsemantic and interaction features. Each candidate computes\nthe similarity from each other with their representations as\nthe supportive information:\nr\ni\nj = uT\nd tanh(Wd[gi,gj,gigj]+ bd)\ndi\nj = ri\nj∑c\nk=1 ri\nk\n;\nvj = sigmoid(\nc∑\nk=1\ndk\nj )\n(6)\nwhere Wd and bd are trainable parameters, di\nj represents the\nsimilarity between the ith and jth candidates, and for thejth\ncandidate, we use the sum of the normalized similarity score\nv\nj as the ﬁnal veriﬁcation score.\nFinally, we combine the veriﬁcation score with the match-\ning score as the ﬁnal ranking score,\npi =( 1−α)mi + αvi (7)\nwhere mi and vi are the matching score and veriﬁcation\nscore of the ith candidate, and α ∈ [0,1] is used to bal-\nance the two parts. Then the loss function can be computed\nas follows,\nL =\nc∑\ni=1\n−yilog(pi) −(1 −yi)log(1 −pi) (8)\nwhere yi is the label of ith candidate.\nExperiment\nDatasets\nWe conduct our experiment on 3 semantic parsing datasets\nJOBS, ATIS and TOP.\nJOBS is a dataset containing 640 queries annotated from\na database of job listings. Questions are paired with Prolog-\nstyle queries. We follow the training and test split in (Zettle-\nmoyer and Collins 2012). It contains 500 training and 140\ntest instances. They have tagged the values for the variables\ncompany, degree, language, platform, location, job area, and\nnumber.\nATIS is a dataset containing5410 queries to a ﬂight book-\ning system (Hemphill, Godfrey, and Doddington 1990). The\ndata has been split into 4480 training instances, 480 vali-\ndation instances, and 450 test instances. Each pair contains\na question with the corresponding lambda-calculus expres-\nsions and identiﬁed values for the variables of date, time,\ncity, aircraft code, airport, airline and number.\nTOP\n1 is a large scale semantic parsing dataset (Gupta et\nal. 2018), containing 44,783 annotation question and parsing\ntree pairs, which are split into 31,279 for training, 4,462 for\nvalidation and 9,042 for test. The utterances in this dataset\nare focused on navigation, events, and navigation to events.\nFor the TOP dataset, (Einolghozati et al. 2019) remove\nthe samples with intent of “UNSUPPORTED” which repre-\nsents out-of-domain questions and get a subset of TOP, this\ndataset contains 28,276 training samples, 4,014 validation\nsamples and 8,191 test samples. We also conduct an experi-\nment on this dataset.\nSettings\nIn our generation model, Glove word embeddings (Pen-\nnington, Socher, and Manning 2014) are used as our pre-\ntrained word embeddings.Input sentences are lower-cased.\nThe beam size of the model is 10. We set the dropout rate to\n0.5. The dimension of all hidden vectors and word embed-\nding is set to 300. Word vocabulary is not shared between en-\ncoder and decoder. Parameters are randomly initialized from\na uniform distribution (-0.01, 0.01). We use Adagrad (Duchi,\nHazan, and Singer 2011) as optimizer during training, and\nan early stop strategy is used to decide the training epoch.\nIn our GTCV ranking model, we load the pre-trained model\nof BERT\nBASE with small parameters to reduce the training\ntime. The number of Transformer blocks is 12, and the di-\nmension of all hidden states is 768 in our model. The batch\nsize of the model is 32. Dropout is set to 0.1. We set the\nblock number n of our graph-based transformer to 3. The\nscore weight α in our model is set to be 0.1. We use all the\n10 candidates from beam search to train and evaluate our\nranking model. We optimize our model by an Adam opti-\nmizer with an initial learning rate of 3e − 4, β\n1 = 0.9, β2\n= 0.999 and γ= 10−9. Gradient accumulation is used in our\ntraining, and accumulate step is set to 12.\nFor TOP, we use the same metrics as used in (Gupta et\nal. 2018), including exact match accuracy(ACC), labeled\nbracketing F1 score (Black et al. 1991), and their proposed\n1http://fb.me/semanticparsingdialog\n8810\nModel ACC F1 P R TL-F1 TL-P TL-R TV\nRNNG (Dyer et al. 2016) 78.51 90.23 90.62 89.84 84.27 84.64 83.91 100.00\nFAIRSEQ (Gehring et al. 2016) 75.87 88.56 89.25 87.88 82.31 82.92 81.72 99.75\nS2S-LSTM (Wiseman and Rush 2016) 75.31 87.69 88.35 87.03 81.15 81.72 80.58 99.94\nTransformer (Vaswani et al. 2017) 72.20 86.60 87.09 86.11 78.54 78.99 78.19 99.55\nMatchLSTM* (Wang and Jiang 2015) 78.31 89.10 88.91 89.30 84.26 84.80 83.71 99.64\nOurs\nGeneration 77.40 88.51 88.80 88.23 83.80 84.17 83.44 99.67\nBERT* 80.15 89.52 89.70 89.34 85.02 85.43 84.61 99.63\nGTCV* 82.51 90.79 90.87 90.71 88.01 88.40 87.62 99.78\nTable 1: Performance (in percentage) of our proposed model and the state-of-the-art methods. * represents the reranking meth-\nods for semantic parsing.\nmetric Tree Label(TL) and Tree Validation(TV). The ﬁrst\ntwo metrics are commonly used in various semantic tasks.\nIn particular, TL is used to evaluate the subtree structures for\nnon-terminal tokens. TV represents the percentage of pre-\ndictions that is formed valid trees via bracket matching.\nFor ATIS and JOBS datasets, we identify the entities and\nnumbers in the input questions and replace them with their\ntype names and unique IDs, which is the same as the prepro-\ncessing in (Dong and Lapata 2016).\nResults and Analysis on TOP\nTable 1 shows the performance of our model and the state-\nof-the-art methods. RNNG (Dyer et al. 2016) is a top-down\ntransition-based parser and was originally proposed for pars-\ning syntax trees and language modeling. FAIRSEQ (Gehring\net al. 2016) and S2S-LSTM (Wiseman and Rush 2016)\nrepresent convolution based and LSTM based sequence to\nsequence model for natural language generation. Trans-\nformer (Vaswani et al. 2017) is a recent popular generation\nmodel based transformer blockers with multi-head attention\nmechanism and has been applied on different tasks.These\nresults are reported in (Dyer et al. 2016) as compared base-\nlines. “Generation” represents our generation model with-\nout reranking. “MatchLSTM” represents using MatchLSTM\nproposed in (Wang and Jiang 2015) as ranking model to\nrerank the candidates. “BERT” denotes using BERT as our\nranking model. “GTCV” represents the model proposed in\nthis paper.\nFrom the results, we observe that “GTCV” achieves state-\nof-the-art performance among most metrics. Compared with\n“Generation”, “GTCV” achieves about 5 points improve-\nment over accuracy which illustrates the effectiveness of\nour reranking method. From the results of “MatchLSTM”,\n“BERT” and “GTCV”, we see that “GTCV” achieves signif-\nicantly performance improvement which demonstrates the\nstructure information is important for the reranking model.\nTo evaluate the contribution of each part, we conduct\nan ablation experiment on TOP. Table 2 shows the results.\n“w/o Veriﬁcation” represents our model Without cross-\ncandidate veriﬁcation. “w/o Graph” denotes our model with-\nout graph information. Comparing the results of “GTCV”\n“w/o Graph” and “w/o Veriﬁcation”, we see that the graph\ninformation and cross candidates veriﬁcation proposed in\nMethods ACC F1\nGeneration 77.40 88.51\nBERT 80.15 89.52\nGTCV 82.51 90.79\nw/o Veriﬁcation 81.16 90.02\nw/o Graph Matching 81.26 90.06\nTable 2: Ablation Experiment\nthis paper are effectiveness for the performance improve-\nment.\nMethod ACC\nRNNG+Top 1 81.21\nLM 82.80\nSVM +LM ranker 84.26\nGeneration 79.82\nGTCV 85.84\nTable 3: In domain result.\nTable 3 shows the results of our model on the subset\nof TOP, which represents in-domain questions, proposed\nin (Gupta et al. 2018). The baseline methods are pro-\nposed in (Gupta et al. 2018), “RNNG+Top1” is a generation\nmethod. “LM” and “SVM+LM ranker” are re-ranking meth-\nods. From the results of “RNNG+Top1” and “Generation”,\nwe observe that the performance of our generation model is\nlower than “RNNG+Top1”, while “RNNG+Top1” requires\nmanually deﬁned grammars. Our generation model is more\nscalable which can be trained without any manually features.\nIn this work, we mainly focus on the reranking model. From\nthe results of “GTCV” and “SVM+LM ranker”, we observe\nthat even though the performance of our generation model\nis lower than RNNG, our model “GTCV” achieves better\nperformance than “SVM+LM ranker”.\nParameter Analysis We range the beam size among\n{1,2,3,5,10}, when the beam size is n, the model will gener-\nate n candidates, we compute the recall of these candidates.\nFigure 3 shows the recall of different beam sizes. From the\nresults, we observe that the recall increase fast when the\nbeam size ranges from 1 to 5, while from 5 to 10, the growth\nis slowed down, When the beam size equals to 10, We get\n8811\n77.4\n85.32\n87.81\n90.04\n92.14\n70\n75\n80\n85\n90\n95\n1235 1 0\nRecall(%)\nBeam Size\nFigure 3: Recall of our baseline sequence-to-sequence\nmodel in different beam sizes.\nabove 92% recall. However the recall for beam size equals\nto 1 is 77.4%. Thus, there exists a large improvement space\nusing reranking methods.\n79.51\n79.82\n80.1579.96\n80.74\n81.16\n80.01\n81.4\n82.51\n78\n79\n80\n81\n82\n83\n35 1 0\nAccuracy(%)\nBeam Size\nBERT\nBERT+GM\nGTCV\nFigure 4: Accuracy of various beam sizes.\nTo evaluate the robustness of our method, we run our\nranking model on different number of candidates. Figure 4\nshows the results when the candidate number ranges in\n{3,5,10}. We ﬁnd that our model achieves improvement for\nall these candidate numbers. Furthermore, when the candi-\ndate number is 3, since there are few candidates, the effec-\ntiveness of the cross candidate veriﬁcation mechanism is\nreduced, while the graph-based matching mechanism still\nachieves signiﬁcantly performance improvement. We also\nsee that the effectiveness of cross candidate veriﬁcation in-\ncreases with the number of candidate increasing.\nCase Study Figure 5 shows some cases of top 1 results\nranked by GTCV and BERT. In the ﬁrst case, we ob-\nserve that the logical form selected by BERT has a branch\n“SL:CATEGORY\nEVENT-the Franch market”, while “the\nFranch market” is not a “CATEGORYEVENT”. Our model\nGTCV considers the structure information and ﬁlters this\ncandidate. In the second case, we observe that the bold\nbranches selected by BERT are mismatch. The content of\nthe candidates selected by BERT and selected by our model\nare the same, the difference is the structure information. We\nuse this case to illustrate that our model can better distin-\nguish the candidate through structure information.\nFrom the cases, we see that the candidates generated by\nthe generation model are often similar except the structure,\nIN:GET_EVENT\nSL:LOCATION\nIN:GET_LOCATION\nthe Franch market\nDoes\nSL:POINT_ON_MAP\nhave SL:DATE_TIME events\nChristmas\nIN:GET_EVENT\nSL:CATEGORY_EVENT\nThe Franch market\nhave SL:DATE_TIME events\nChristmas\nDoes\nQuestion:Does the Franch marchet have Christmas events\nQuestion: What is the traffic driving from Nashville to Knoxville\nIN:GET_INFO_TRAFFIC\nSL:METHOD_TRAVEL\ndriving\nWhat is the traffic from SL:SOURCE\nNashville\nSL:DATE_TIME\ntonight\nGTCV\nBERT\nGTCV\nBERT\nĜ\n×\nĜ\n×\nto\nIN:GET_INFO_TRAFFIC\nSL:METHOD_TRAVEL\nthe traffic driving\nWhat is SL:SOURCE\nfrom Nashville\nSL:DATE_TIME\ntonight\nto\nFigure 5: Comparision of Top1 ranking examples in GTCV\nand BERT\nour graph based model captures the structure information,\nand reranks the candidates better.\nResults on ATIS and JOBS\nJOBS ATIS\nFUBL (Kwiatkowski et al. 2011) - 83.5\nGUSP++ (Poon 2013) - 83.5\nDCS+L (Liang, Jordan, and Klein 2013) 90.7 -\nTISP (Zhao and Huang 2014) 85.0 84.2\nPointer Network (See, Liu, and Manning 2017) 86.7 83.4\nseq2seq (Dong and Lapata 2016) 87.1 84.2\nseq2tree (Dong and Lapata 2016) 90.1 84.6\nASN (Rabinovich, Stern, and Klein 2017) 91.4 85.3\nASN+SUP (Rabinovich, Stern, and Klein 2017) 92.9 85.9\nSeq2Act (Chen, Sun, and Han 2018) - 85.5\nCoarse2ﬁne(Dong and Lapata 2018) - 87.7\nMatchLSTM* (Wang and Jiang 2015) 90.1 82.8\nGeneration 89.3 82.4\nBERT* 90.1 84.8\nGTCV* 92.9 87.3\nTable 4: Accuracy on JOBS and ATIS. * represents the\nreranking methods for semantic parsing.\nWe also evaluate our method on the datasets, ATIS and\nJOBS in Table 4. We observe that our GTCV model achieves\ncomparable result with the state-of-the-art methods, com-\npared with many existing methods. The accuracy of our\n8812\nGTCV model obtains absolute improvements of 3.6% and\n4.9% in both datasets, compared to the generation result.\nOur model also outperforms MatchLSTM and BERT rank-\ning models. Furthermore, we ﬁnd that our GTCV with the\nbasic sequence-to-sequence model achieves comparable re-\nsult to state-of-the-art methods. Specially, since the two\ndatasets are small and the size of test set in jobs only\n140, thus the slight improvement on the test set is not so\nconvinced. And the two datasets do not contain the com-\nplex structure information as TOP. And Results on differ-\nent datasets demonstrate the robustness and effectiveness\nof our ranking model, which is only based on the basic\nsequence-to-sequence model without speciﬁc designed gen-\neration model. The experiments show that our approaches\ncan be on various datasets based on general sequence to se-\nquence model and achieve good performance.\nRelated Work\nSemantic Parsing\nVarious models have been proposed over the years for se-\nmantic parsing (Kwiatkowski et al. 2011; Shao et al. 2019;\nReddy, Lapata, and Steedman 2014; Artzi and Zettlemoyer\n2011; Xiao, Dymetman, and Gardent 2017; Yin and Neubig\n2017). (Kwiatkowski et al. 2011) propose a combinatory\ncategorical grammar induction technique for semantic pars-\ning. (Xiao, Dymetman, and Gardent 2017; Yin and Neubig\n2017) use syntax information to improve semantic parsing\nmodels. (Reddy, Lapata, and Steedman 2014) try to build\nsemantic parsers without relying on logical form annotations\nthrough distant supervision. With the rapid development of\ndeep learning models. Most of these transitional methods re-\nquire experts to design features to represent and rank candi-\ndates. Recently, neural semantic parsing methods (Dong and\nLapata 2016; Jia and Liang 2016; Chen, Sun, and Han 2018;\nDong and Lapata 2018) are proposed to train semantic\nparsers in an end-to-end neural framework. (Dong and La-\npata 2016) proposes a SEQ2TREE model, which captures a\nhierarchical structure of logical forms. (Guo and Gao 2017)\nleverages a new attention mechanism to generate a more pre-\ncise SQL. (Dong and Lapata 2018) proposes a hierarchical\ndecoding process, from coarse sketches to ﬁne target logical\nforms. On the TOP dataset, various generation model includ-\ning FAIRSEQ (Gehring et al. 2016), S2S-LSTM (Wiseman\nand Rush 2016) and Transformer (Vaswani et al. 2017) are\nused and reported as the baselines in (Gupta et al. 2018).\nGraph-based Neural Network\nRecently, graph-based neural network has become a hot re-\nsearch topic. Researchers seek to capture more informa-\ntion in graph structure data. SDNE(Wang, Cui, and Zhu\n2016) uses ﬁnding proper node embedding to reconstruct its\nneighborhood as part of its objective function. GCN(Kipf\nand Welling 2016) iteratively aggregates neighbor informa-\ntion from previous layers with a deﬁned convolution opera-\ntor. GAN(Veliˇckovi´c et al. 2017) leverages attention mech-\nanisms to focus on the most relevant information by attend-\ning over nodes’ neighborhood. Structure information also\nhelps neural semantic parsing, SEQ2TREE (Dong and La-\npata 2016) uses a hierarchical tree decoder to model the\ncompositional nature of meaning representations in logical\nforms. In (Xu et al. 2018), researchers propose a graph-to-\nsequence model that uses the graph to represent syntactic\ninformation of word order, dependency and constituency.\nConclusion\nIn this paper, we propose a Graph-based Transformer model\nfor semantic parsing. The model captures the hierarchical\nstructure information of logical forms. To make each can-\ndidate interact with other candidates, we also incorporate a\ncross-candidate veriﬁcation mechanism into our model. Fur-\nther, with the Graph-based Transformer model, we build a\ngenerator + ranking pipeline. Experimental results on 3 se-\nmantic parsing datasets ATIS, JOBS and TOP show the ef-\nfectiveness and robustness of our model.\nAcknowledgements\nThis work is supported by the National Natural Science\nFoundation of China under Grants No.U1711263.\nReferences\nArtzi, Y ., and Zettlemoyer, L. 2011. Bootstrapping semantic\nparsers from conversations. In Proceedings of the confer-\nence on empirical methods in natural language processing,\n421–432. Association for Computational Linguistics.\nArtzi, Y ., and Zettlemoyer, L. 2013. Weakly supervised\nlearning of semantic parsers for mapping instructions to ac-\ntions. Transactions of the Association of Computational Lin-\nguistics 1:49–62.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBlack, E.; Abney, S.; Flickenger, D.; Gdaniec, C.; Grishman,\nR.; Harrison, P.; Hindle, D.; Ingria, R.; Jelinek, F.; Klavans,\nJ.; et al. 1991. A procedure for quantitatively comparing the\nsyntactic coverage of english grammars. In Speech and Nat-\nural Language: Proceedings of a Workshop Held at Paciﬁc\nGrove, California, February 19-22, 1991.\nChen, B.; Sun, L.; and Han, X. 2018. Sequence-to-action:\nEnd-to-end semantic graph generation for semantic parsing.\narXiv preprint arXiv:1809.00773.\nCho, K.; Van Merri¨enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning\nphrase representations using rnn encoder-decoder for statis-\ntical machine translation. arXiv preprint arXiv:1406.1078.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDong, L., and Lapata, M. 2016. Language to logical form\nwith neural attention. arXiv preprint arXiv:1601.01280.\nDong, L., and Lapata, M. 2018. Coarse-to-ﬁne decoding for\nneural semantic parsing. arXiv preprint arXiv:1805.04793.\n8813\nDuchi, J.; Hazan, E.; and Singer, Y . 2011. Adaptive subgra-\ndient methods for online learning and stochastic optimiza-\ntion. Journal of Machine Learning Research12(Jul):2121–\n2159.\nDyer, C.; Kuncoro, A.; Ballesteros, M.; and Smith, N. A.\n2016. Recurrent neural network grammars. arXiv preprint\narXiv:1602.07776.\nEinolghozati, A.; Pasupat, P.; Gupta, S.; Shah, R.; Mohit,\nM.; Lewis, M.; and Zettlemoyer, L. 2019. Improving\nsemantic parsing for task oriented dialog. arXiv preprint\narXiv:1902.06000.\nGehring, J.; Auli, M.; Grangier, D.; and Dauphin, Y . N.\n2016. A convolutional encoder model for neural machine\ntranslation. arXiv preprint arXiv:1611.02344.\nGuo, T., and Gao, H. 2017. Bidirectional attention for sql\ngeneration. arXiv preprint arXiv:1801.00076.\nGupta, S.; Shah, R.; Mohit, M.; Kumar, A.; and Lewis, M.\n2018. Semantic parsing for task oriented dialog using hier-\narchical representations. arXiv preprint arXiv:1810.07942.\nHemphill, C. T.; Godfrey, J. J.; and Doddington, G. R. 1990.\nThe atis spoken language systems pilot corpus. In Speech\nand Natural Language: Proceedings of a Workshop Held at\nHidden V alley, Pennsylvania, June 24-27, 1990.\nJia, R., and Liang, P. 2016. Data recombination for neural\nsemantic parsing. In Proceedings of ACL, 12–22.\nKipf, T. N., and Welling, M. 2016. Semi-supervised classi-\nﬁcation with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nKwiatkowski, T.; Zettlemoyer, L.; Goldwater, S.; and Steed-\nman, M. 2011. Lexical generalization in ccg grammar in-\nduction for semantic parsing. In Proceedings of the confer-\nence on empirical methods in natural language processing,\n1512–1523. Association for Computational Linguistics.\nLiang, P.; Jordan, M. I.; and Klein, D. 2013. Learning\ndependency-based compositional semantics. Computational\nLinguistics 39(2):389–446.\nPennington, J.; Socher, R.; and Manning, C. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of\nEMNLP, 1532–1543.\nPoon, H. 2013. Grounded unsupervised semantic parsing.\nIn Proceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics (V olume 1: Long Papers),\n933–943.\nRabinovich, M.; Stern, M.; and Klein, D. 2017. Abstract\nsyntax networks for code generation and semantic parsing.\narXiv preprint arXiv:1704.07535.\nReddy, S.; Lapata, M.; and Steedman, M. 2014. Large-scale\nsemantic parsing without question-answer pairs. Trans-\nactions of the Association of Computational Linguistics\n2(1):377–392.\nSee, A.; Liu, P. J.; and Manning, C. D. 2017. Get to\nthe point: Summarization with pointer-generator networks.\narXiv preprint arXiv:1704.04368.\nShao, B.; Gong, Y .; Bao, J.; Ji, J.; Cao, G.; Lin, X.; and\nDuan, N. 2019. Weakly supervised multi-task learning\nfor semantic parsing. In Proceedings of the 28th Inter-\nnational Joint Conference on Artiﬁcial Intelligence, 3375–\n3381. AAAI Press.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio,\nP.; and Bengio, Y . 2017. Graph attention networks. arXiv\npreprint arXiv:1710.10903.\nWang, S., and Jiang, J. 2015. Learning natural language\ninference with lstm. arXiv preprint arXiv:1512.08849.\nWang, Y .; Liu, K.; Liu, J.; He, W.; Lyu, Y .; Wu, H.; Li, S.;\nand Wang, H. 2018. Multi-passage machine reading com-\nprehension with cross-passage answer veriﬁcation. arXiv\npreprint arXiv:1805.02220.\nWang, D.; Cui, P.; and Zhu, W. 2016. Structural deep net-\nwork embedding. In Proceedings of the 22nd ACM SIGKDD\ninternational conference on Knowledge discovery and data\nmining. ACM.\nWen, T.-H.; Gasic, M.; Kim, D.; Mrksic, N.; Su, P.-H.;\nVandyke, D.; and Young, S. 2015. Stochastic lan-\nguage generation in dialogue using recurrent neural net-\nworks with convolutional sentence reranking.arXiv preprint\narXiv:1508.01755.\nWiseman, S., and Rush, A. M. 2016. Sequence-to-\nsequence learning as beam-search optimization. arXiv\npreprint arXiv:1606.02960.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google’s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144.\nXiao, C.; Dymetman, M.; and Gardent, C. 2017. Sequence-\nbased structured prediction for semantic parsing. US Patent\n9,830,315.\nXu, K.; Wu, L.; Wang, Z.; Yu, M.; Chen, L.; and Sheinin,\nV . 2018. Exploiting rich syntactic information for seman-\ntic parsing with graph-to-sequence model. arXiv preprint\narXiv:1808.07624.\nYih, S. W.-t.; Chang, M.-W.; He, X.; and Gao, J. 2015. Se-\nmantic parsing via staged query graph generation: Question\nanswering with knowledge base.\nYin, P., and Neubig, G. 2017. A syntactic neural\nmodel for general-purpose code generation. arXiv preprint\narXiv:1704.01696.\nZettlemoyer, L. S., and Collins, M. 2012. Learning\nto map sentences to logical form: Structured classiﬁca-\ntion with probabilistic categorial grammars. arXiv preprint\narXiv:1207.1420.\nZhao, K., and Huang, L. 2014. Type-driven incremen-\ntal semantic parsing with polymorphism. arXiv preprint\narXiv:1411.5379.\n8814",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8439394235610962
    },
    {
      "name": "Parsing",
      "score": 0.7812036275863647
    },
    {
      "name": "Transformer",
      "score": 0.6006747484207153
    },
    {
      "name": "Natural language processing",
      "score": 0.5258137583732605
    },
    {
      "name": "Graph",
      "score": 0.5210175514221191
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44278380274772644
    },
    {
      "name": "Programming language",
      "score": 0.3307666778564453
    },
    {
      "name": "Information retrieval",
      "score": 0.3206600844860077
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2829124629497528
    },
    {
      "name": "Voltage",
      "score": 0.07796865701675415
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}