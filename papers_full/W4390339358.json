{
  "title": "PersianLLaMA: Towards Building First Persian Large Language Model",
  "url": "https://openalex.org/W4390339358",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2189193503",
      "name": "Mohammad Amin Abbasi",
      "affiliations": [
        "Iran University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2633477338",
      "name": "Arash. Ghafouri",
      "affiliations": [
        "Imam Hossein University"
      ]
    },
    {
      "id": "https://openalex.org/A5046517153",
      "name": "Mahdi Firouzmandi",
      "affiliations": [
        "Imam Hossein University"
      ]
    },
    {
      "id": "https://openalex.org/A2297411994",
      "name": "Hassan Naderi",
      "affiliations": [
        "Iran University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2030491744",
      "name": "Behrouz Minaei Bidgoli",
      "affiliations": [
        "Iran University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6676965983",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6604963970",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W6826116265",
    "https://openalex.org/W6611333299",
    "https://openalex.org/W6600157417",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W6606084162",
    "https://openalex.org/W6816558821",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4309721248",
    "https://openalex.org/W6601449391",
    "https://openalex.org/W6605862995",
    "https://openalex.org/W6600686112",
    "https://openalex.org/W6600558321",
    "https://openalex.org/W6600662749",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Abstract Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization.",
  "full_text": "Page 1/23\nPersianLLaMA: Towards Building First Persian\nLarge Language Model\nMohammad Amin Abbasi \nIran University of Science and Technology\nArash Ghafouri  (  krghafouri@ihu.ac.ir )\nImam Hossein Comprehensive University\nMahdi Firouzmandi \nImam Hossein Comprehensive University\nHassan Naderi \nIran University of Science and Technology\nBehrouz Minaei Bidgoli \nIran University of Science and Technology\nResearch Article\nKeywords: Large language models, Persian language, Natural language generation, Natural language\nunderstanding\nPosted Date: December 28th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3789059/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/23\nAbstract\nDespite the widespread use of the Persian language by millions globally, limited efforts have been made\nin natural language processing for this language. The use of large language models as effective tools in\nvarious natural language processing tasks typically requires extensive textual data and robust hardware\nresources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware\nresources have hindered the development of large language models for Persian. This paper introduces\nthe \u0000rst large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and\ndatasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on\nformal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated\nfor natural language generation tasks based on the latest evaluation methods, namely using larger\nlanguage models, and for natural language understanding tasks based on automated machine metrics.\nThe results indicate that PersianLLaMA signi\u0000cantly outperforms its competitors in both understanding\nand generating Persian text. PersianLLaMA marks an important step in the development of Persian\nnatural language processing and can be a valuable resource for the Persian-speaking community. This\nlarge language model can be used for various natural language processing tasks, especially text\ngeneration like chatbots, question-answering, machine translation, and text summarization.\n1 Introduction\nIn recent decades, signi\u0000cant advances in arti\u0000cial intelligence have led to the emergence of large\nlanguage models as pivotal and transformative tools in the \u0000eld of arti\u0000cial intelligence and natural\nlanguage processing. These AI models, utilizing deep neural networks and learning from extensive\ndatasets, are capable of understanding and generating texts with structures similar to human language.\nLanguage modeling is a primary approach for advancing machine language intelligence. Generally, the\naim of language modeling is to model the probability of word sequences to predict the likelihood of\nfuture words. Research in language modeling has gained extensive attention in recent years, leading to\nthe emergence of pre-trained language models. Initially, the ELMo(Peters et al., 2018) model was\nintroduced to understand content in word embedding, which involved pre-training a bi-directional LSTM\n(biLSTM)(Staudemeyer & Morris, 2019) network and then \u0000ne-tuning the biLSTM network based on sub-\ntasks. Also, the BERT(Devlin et al., 2018) model, utilizing the structure of Transformers and the Encoder\npart of the Transformer model, was introduced. Pre-trained on large unlabelled text datasets, this model\nwas ready for use in sub-tasks, signi\u0000cantly enhancing the performance of natural language\nunderstanding tasks. These models often require the pre-trained language model to be \u0000nely tuned for\ncompatibility with sub-tasks.\nSubsequently, researchers discovered that increasing the number of parameters in pre-trained language\nmodels generally leads to better results in downstream tasks. Some studies in this \u0000eld have explored the\nperformance of increasingly large pre-trained language models, such as the GPT-3(Floridi & Chiriatti,\n2020) model with 175 billion parameters and the PaLM(Chowdhery et al., 2023) model with 540 billion\nPage 3/23\nparameters. Thus, the research community coined the term \"large language models\" for these types of\nmodels. A notable practical application of large language models is the ChatGPT(Ray, 2023) model,\nwhich uses the GPT series models for conversation, demonstrating remarkable ability in interactions with\nhumans.\nLLaMA(Touvron, Lavril, et al., 2023; Touvron, Martin, et al., 2023) is a large language model developed by\nFacebook. It is an arti\u0000cial neural network that has been trained on a vast dataset of text and code.\nLLaMA comprises a collection of open-source language models and is available in two versions. The \u0000rst\nand second versions have parameters ranging from 7 to 65 billion and from 7 to 70 billion, respectively,\nand are competitive with state-of-the-art language models. Notably, LLaMA-13B, a version of LLaMA with\n13 billion parameters, performs better in a multitude of tasks compared to GPT-3, while being\napproximately ten times smaller in size. Unlike most large language models, LLaMA has been trained on\nspeci\u0000c and open-source datasets and has achieved state-of-the-art performance without relying on\nproprietary data. Additionally, \u0000ne-tuning LLaMA on different data sets leads to very good results.\nHowever, large language models also face challenges and limitations; due to their numerous parameters,\nthese models require signi\u0000cant computational resources for training and usage. Additionally, concerns\nabout their capability to generate incorrect content or false news exist.\nDespite the remarkable capabilities of these large language models, they have been predominantly\ndeveloped in English and limiting their applicability to other languages. This language bias has created a\nsigni\u0000cant gap in the development and utilization of language models for non-English languages,\nincluding Persian. The Persian language, spoken by millions worldwide, lacks comprehensive language\nmodels that can effectively understand and generate Persian text.\nRecognizing this gap, our research aims to develop PersianLLaMA, the \u0000rst large language model\ndedicated to the Persian language. The development of this model is not only a technical challenge but\nalso a step towards linguistic inclusivity in the \u0000eld of AI and natural language processing. By focusing\non the Persian language, PersianLLaMA seeks to provide a robust tool for understanding and generating\nPersian text, which can be utilized in various applications, from chatbots to text summarization.\n2 Related works\nGiven the absence of a large language model in Persian, this section aims to examine the closest works\nrelated to our research. These include large Asian language models based on the LLaMA framework and\nboth single-language and multilingual pre-trained models that support Persian.\n2.1 LLaMA-Based Models\nSeveral studies have recently been conducted on developing large language models for Asian languages.\nFor instance, TAMIL-LLaMA(Balachandran, 2023), an Asian language model that leverages LLaMA and\nincorporates 16,000 Tamil tokens, has shown signi\u0000cant improvements. This model employs the\nPage 4/23\nLoRA(Hu et al., 2021) technique for e\u0000cient training on Tamil datasets, including translated Alpaca\ndatasets and a custom subset of the OpenOrca(Mukherjee et al., 2023) dataset for \u0000ne-tuning.\nPerformance evaluations demonstrate notable enhancements in Tamil text generation, with the 13B\nversion outperforming OpenAI's GPT-3.5-turbo in Tamil language tasks.\nSeaLLMs(Nguyen et al., 2023) is a suite of language models dedicated to Southeast Asian (SEA)\nlanguages, built on the LLaMA-2 model with extended vocabularies and speci\u0000c con\u0000gurations to cater to\nthe cultural nuances of SEA languages. These models excel in various linguistic tasks and comply with\nlocal cultural standards and legal considerations. They perform better than models like ChatGPT-3.5 in\nnon-Latin SEA languages, including Thai, Khmer, Lao, and Burmese. Additionally, SeaLLMs have\nsigni\u0000cantly advanced AI tools with cultural awareness.\nTAIWAN-LLM(Lin & Chen, 2023) is a large language model for Traditional Chinese, particularly used in\nTaiwan. Available in 7 and 13 billion parameter versions, it is developed using a comprehensive pre-\ntraining dataset and \u0000ne-tuning datasets that capture the linguistic and cultural essence of Taiwan.\nTAIWAN-LLM outperforms many models in understanding and generating Traditional Chinese text.\n2.2 Pre-Trained Persian Language Models\nPersian language models play a crucial role in advancing research related to Persian language\nprocessing. These models, pre-trained on extensive Persian text datasets, are adept at accurately\ncomprehending and interpreting Persian language structures and meanings.\nAriaBERT(Ghafouri et al., 2023) is recognized as the leading language model in understanding the\nPersian language. Utilizing the RoBERTa(Liu et al., 2019) architecture and the Byte-Pair\nEncoding(Sennrich et al., 2015) tokenizer, AriaBERT has been trained with over 32 gigabytes of various\nPersian textual data. This includes a mix of colloquial and formal texts comprising tweets, news, poetry,\nmedical texts, encyclopedia articles, user comments on websites, and other text types.\nParsBERT is a monolingual model based on Google's BERT architecture, trained on a large corpus of\nPersian texts covering diverse topics with more than 3.9 million documents and 16 gigabytes of data. It\noutperforms multilingual models like Multilingual BERT (M-BERT) in various tasks such as sentiment\nanalysis, text classi\u0000cation, named entity recognition, and named entity disambiguation in Persian texts.\nSINA-BERT(Taghizadeh et al., 2021) Introduced as a BERT-based language model, is published for high-\nquality Persian language coverage in the medical \u0000eld. It utilizes an extensive collection of medical\ncontent, including both formal and informal texts. The primary use of this model includes medical\nquestion classi\u0000cation, sentiment analysis in medicine, and medical question retrieval.\nThe mT5(Xue et al., 2020) model, known for its text-to-text transformation, is another prominent NLP\nmodel developed by Google. Based on the Transformer architecture, it frames all NLP tasks as converting\ninput text to output text. This versatile adaptability is evident during \u0000ne-tuning, where the model aligns\nPage 5/23\nwith speci\u0000c tasks using labeled data. The multilingual version, MT5, trained on a vast corpus of web-\ncrawled text in 101 languages including Persian, exempli\u0000es this adaptability.\nThe mGPT(Shliazhko et al., 2022) models, similar to GPT2(Radford et al., 2019), come in 1.3 and\n13 billion parameter versions and are trained on 60 languages. Their training dataset includes mc4(Raffel\net al., 2020) and Wikipedia, totaling approximately 600 gigabytes. mGPT, built on the decoder component\nof Transformer architecture, has been evaluated in various scenarios, including language modeling,\ndownstream task evaluation, and knowledge-based searches, showing remarkable performance in\nPersian among other supported languages.\nPage 6/23\nTable 1\ncomprehensive overview of recent language models\nModel ReleaseTime ParameterSize BaseModel Train DataSize Hardware Type\nGemini(Anil et al.,2023) Dec-2023 176B - 6.3T tokens - Causaldecoder\nSeaLLMs Dec-2023 13B LLaMA2 - - Causaldecoder\nTAMIL-LLaMA Nov-2023 13B LLaMA2 - - Causaldecoder\nTAIWAN-LLM Nov-2023 13B LLaMA2 35B tokens - Causaldecoder\nAriaBERT Nov − 2023 355M RoBERTa 102Mdocuments 4 A10040G MaskedLM\nFalcon-40B(Penedo et al.,2023)\nJul-2023 40B - 1T tokens 384*A10040GB Causaldecoder\nLLaMA 2 Jul-2023 70B - 2T tokens 2000*80GA100 Causaldecoder\nVicuna(Zheng etal., 2023) Jun-2023 13B LLaMA 125Kinstructions - auto-regressivelanguagemodel\nPaLM2 May-2023 16B - 100Btokens - Causaldecoder\nAlpaca May-2023 13B LLaMA 52Kinstructions - -\nWizardLM(Xu etal., 2023) Apr-2023 7B LLaMA 250kinstructions - -\nKoala Apr-2023 13B LLaMA 472instructions 8*A100 -\numT5(Chung et al.,2023) Apr-2023 13B T5 29Tcharacters - Encoder-decoder\nGPT-4(Koubaa,2023) Mar-2023 1.8T - - - Causaldecoder\nLLaMA Feb-2023 65B - 1.4T tokens 2048*A10080G Causaldecoder\nmGPT Apr-2022 13B GPT-3 440Btokens 256*NvidiaV100 Causaldecoder\nPage 7/23\nModel ReleaseTime ParameterSize BaseModel Train DataSize Hardware Type\nPaLM Apr-2022 540B - 780Btokens 6144 TPUv4 Causaldecoder\nSINA-BERT Apr-2021 110M BERT 2.8Mdocuments - MaskedLM\nmT5 Oct-2020 13B T5 1T tokens - Encoder-decoder\nParsBERT May-2020 110M BERT 3.9Mdocuments - MaskedLM\nGPT-3 May-2020 175B - 300Btokens - Causaldecoder\n3 proposed method\nIn this research, two distinct approach for developing a large Persian language model are introduced and\ncompared. The \u0000rst approach involves training a LLaMA 2 model from scratch using a Persian text\ndataset exceeding 90 gigabytes. A Persian tokenizer was trained using this dataset. Due to hardware\nresource limitations, a 7 billion parameter LLaMA 2 model was utilized, and the entire neural network was\ntrained from scratch. The second approach involves training a LLaMA model as an adapter on a pre-\ntrained English LLaMA 2 model. Initially, a Persian tokenizer was trained on Persian Wikipedia texts, then\ncombined with the tokenizer of the English LLaMA 2 model. Subsequently, an adapter neural network was\nplaced on the English LLaMA 2 model, and its weights were trained. The 13 billion parameter LLaMA 2\nmodel was used in this approach, as it requires fewer hardware resources, only necessitating training of\nthe adapter part while keeping the core neural network unchanged. Details of both approachs are\nthoroughly explained in the subsequent sections.\n3.1 LLaMA-Based Models\nTo train PersianLLaMA, we selected two datasets comprising various and diverse texts to ensure\ncomprehensive training across different topics. The training dataset includes:\nOSCAR Dataset (Open Super-large Crawled Aggregated Resource)(Suárez et al., 2020): A rich source\nof multilingual texts collected from the web, widely used in natural language processing and\nmachine learning research. It includes website texts in multiple languages, offering a broad variety of\ntopics and linguistic structures. The Persian section contains 23 million texts (93 GB, 9 billion\ntokens).\nPersian Wikipedia: A vast and diverse source of textual information, Persian Wikipedia serves as a\nvaluable resource due to its extensive coverage of topics across various \u0000elds. It provides a rich and\ncomprehensive linguistic collection for training our model. The dataset allows our model to develop\nPage 8/23\nan extensive vocabulary and a precise understanding of Persian language structures and semantics.\nUtilizing Persian Wikipedia as a foundational dataset ensures that the model is well-prepared to\ngenerate coherent and relevant Persian text, covering a wide range of domains. This dataset includes\n2.4 million texts (1.3 GB, 184 million tokens).\n3.2 Preprocessing\nIn developing the PersianLLaMA text generation model, effective preprocessing of training data is crucial\nto ensure the quality and coherence of the generated text. This section explains the preprocessing steps\nundertaken to prepare the training dataset for PersianLLaMA, utilizing a combination of techniques and\nlibraries suited for Persian text.\nThe process begins with text normalization using the Hazm library, a powerful tool for processing Persian\ntexts. This step involves correcting common issues related to spacing and normalizing characters in\nPersian texts. Subsequently, HTML tags present in texts are removed. Additional tasks include managing\nUnicode, normalizing spaces, and removing links, emails, and phone numbers. Persian texts may contain\nvarious punctuation marks that can impact analysis and underlying tasks. To address this, super\u0000uous\npunctuation marks are removed from the texts. Additionally, mentions starting with '@' in tweets are\ndeleted. Finally, multiple consecutive spaces are condensed into a single space. In summary, text\npreprocessing meticulously cleans and normalizes the training dataset, ensuring its suitability for training\nthe Persian text generation model.\n3.3 Tokenizer\nIn the development of natural language processing models for Persian, training an appropriate tokenizer\nis a fundamental and vital aspect. The tokenizer divides text into smaller units (tokens) to make it\nunderstandable and usable by neural models. PersianLLaMA models use the SentencePiece tokenizer, an\nunsupervised text tokenizer used for neural network-based text generation systems. SentencePiece(Kudo\n& Richardson, 2018) supports word separation using BPE (Byte Pair Encoding)(Wang et al., 2020)\nmethod. BPE is a popular text encoding and segmentation method used to reduce vocabulary size and\ncreate subword representations. Its implementation in Persian ensures that unknown words during\ntraining are not registered as unfamiliar tokens and can be effectively segmented. This is bene\u0000cial for\nthe Persian language with its complex words and diverse subword compositions.\nTwo distinct approaches were employed in developing the PersianLLaMA tokenizers, each serving a\nunique model architecture and objective. The details of these tokenizer implementations are discussed\nbelow:\n3.3.1 Monolingual Tokenizer\nFor the \u0000rst model, a Persian-speci\u0000c tokenizer was trained for a learning model developed from scratch\nin Persian. This tokenizer, focusing exclusively on Persian, covers 50,000 tokens and does not recognize\nor process English or other languages, aligning with the monolingual nature of the model.\nPage 9/23\n3.3.2 Multilingual Tokenizer With Expanding Vocabulary\nThe second model, built using LoRA architecture and based on an English foundational model, uses a\nbroader tokenizer combining English and Persian vocabularies. Initially, a Persian tokenizer with 32,000\ntokens was trained. Then, using the English tokenizer of LLaMA 2, a combined tokenizer with a total of\n64,000 tokens was created, allowing the model to understand and generate both Persian and English.\nThis tokenizer is suitable for bilingual applications like translation or combined content generation, given\nthe complexities of bilingual contexts\n3.4 Model Implementation\nThe PersianLLaMA models are trained on the Causal Language Modeling task to predict and generate the\nnext word. Training from scratch utilizes DeepSpeed (Rasley et al., 2020) and TencentPretrain(Zhao et al.,\n2022), two advanced frameworks for optimizing deep learning training. TencentPretrain offers\noptimizations for memory usage, mixed-precision training, and distributed training, enhancing e\u0000ciency\nand scalability. For the \u0000rst version of PersianLLaMA, with 7 billion parameters, a cosine scheduler with a\ndecay of 0.1 and a learning rate of 4e-3 was used. This model was trained on two A100 GPUs with 80 GB\nVRAM over 12 days on Wikipedia and Oscar datasets.\nThe conventional training pattern of large language models (LLMs) with full parameters is costly. Low-\nRank Adaptation (LoRA) is a parameter-e\u0000cient method that retains the weights of pre-trained models\nwhile introducing trainable rank-decomposition matrices. LoRA freezes the weights of the pre-trained\nmodel and injects trainable low-rank matrices into each layer. This approach signi\u0000cantly reduces the\ntrainable parameters, working with just 533 million parameters, making the training of LLMs feasible with\nmuch less computational resources. The second version, with 13 billion parameters, uses LoRA with\nattention modules and MLP layers applied to the adapter. This Persian LLaMA model was trained with\nthe original English LLaMA weights, using FP16, and supports a maximum text length of 2048, currently\nthe longest among Persian language models. This model was trained on an A100 GPU with 80 GB VRAM\nover 70 hours on Wikipedia data. Tables 2 and 3 details the training speci\u0000cations in two approchs, and\nFig. 2 illustrates the training process based on the LoRA method.\n \nTable 2\napproach-1 training con\u0000gs\nLearningrate Maxlength scheduler Decay Dropout params Trainableparams Torchdtype\n0.004 768 cosinescheduler 0.1 0.1 7B 7B (100%) Float32\n \n \nPage 10/23\nTable 3\napproach-2 training con\u0000gs\nLearningrate Maxlength LoRArank LoRAalpha LoRAweights params Trainableparams Torchdtype  \n0.0002 2048 8 32 QKVO,MLP 13B 533M(4.10%) Float16  \nFigure 3 shows the error rate during the training of the PersianLLaMA-Zero and PersianLLaMA-LoRA\nmodels. As evident, training with LoRA leads to a lower error rate in a much shorter time during training.\n4 Evaluation\nFor the evaluation of models, it is necessary to \u0000ne-tune them on datasets related to natural language\ngeneration and understanding, and compare their results with other Persian language generation models.\nTo evaluate the results in natural language understanding, it's crucial to de\u0000ne evaluation criteria for the\ntasks being compared:\nSentiment Analysis and Categorization: F1-score is used to assess and compare model outputs in\nsentiment analysis and topic categorization tasks.\nQuestion-Answering: Model e\u0000ciency is evaluated by checking if the response to each question is\nthe actual answer. Models sometimes include additional explanations in their text. After review,\naccuracy is used as the metric for comparison.\nText Summarization: While ROUGE and similar metrics like BLEU and METEOR provide quantitative\nmeasures, they often fall short in depicting the true nature of a summary and correlate less with\nhuman ratings. Given the advancements in LLMs in producing \u0000uent and coherent summaries,\ntraditional metrics like ROUGE might inadvertently undermine these models' performance, especially\nwhen summaries are expressed differently but still accurately encapsulate the main information.\nROUGE relies on the exact presence of words in both predicted and reference texts and fails in\nsemantic interpretation. To address this, inspired by the BertScore(Zhang et al., 2019) paper, ParsBert\nmodel's text embeddings are used to compare semantic similarities between the model-generated\ntext and the reference text.\nHowever, evaluating and comparing results in datasets related to natural language generation poses\ncertain challenges. In the past, the best method of evaluation was based on human assessment, but\nhuman evaluation comes with challenges that might make it di\u0000cult to operationalize. These challenges\ninclude high \u0000nancial costs for hiring human evaluators, time consumed for conducting evaluations, and\ndifferences of opinion among different individuals. Additionally, human evaluation might produce\ninaccurate results due to the inability to evaluate on a large scale and maintain consistency in operations.\nInspired by the paper E\u0000cient and Effective Text Encoding for Chinese LLaMA and Alpaca(Anil et al.,\n2023), ChatGPT was used for evaluating the models. ChatGPT, as an intelligent language model capable\nof understanding and generating text, has emerged as an effective tool in the evaluation process of\nPage 11/23\nlanguage models. ChatGPT can automatically evaluate generated texts and provide a credible score for\nthe quality of text production. Using ChatGPT signi\u0000cantly reduces time and costs, and can fairly score\ntexts generated by different language models. Similar to the approach in the cited paper, we also used\nprompt shown in Table 3 for evaluating the generated texts.\nTable 3:  training con\u0000gs\nThe followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten-pointscale for each and give explanations to justify your scores.\nPrompt:\n{prompt-input}\nsystem1:\n{system1-output}\nsystem2:\n{system2-output}\nsystem3:\n{system3-output}\nsystem4:\n{system4-output}\n4.1 Fine-Tuning\nFor \u0000ne-tuning the PersianLlama-Zero model, or the model trained from scratch, the TencentPretrain\nframework has been used to train the model on selected datasets. For the \u0000ne-tuning of PersianLlama-\nLora, after pre-training the model on LoRA with Persian Wikipedia data, the LoRA neural networks were\ncombined with the original model to form a uni\u0000ed llama model. In \u0000ne-tuning this model, the weights of\nPersianLlama were kept \u0000xed, and the LoRA technique was used for \u0000ne-tuning. For \u0000ne-tuning\nPersianLlama models on various datasets, mixed precision training (FP16) was utilized to expedite the\ntraining process. In the following sections, we introduce different datasets used to \u0000ne-tune\nPersianLLaMA and evaluate its performance in natural language understanding and generation.\n4.1.1 Natural Language Generation Datasets\nIn this part, datasets used for \u0000ne-tuning in the \u0000eld of text generation are introduced.\nMeDiaQA(Suri et al., 2021): This dataset includes 8,903 questions and answers from medical\ndialogues, based on physician-patient interactions on Persian medical websites. The dataset\ncontains responses from 150 experts.\nPage 12/23\nPerCQA(Jamali et al., 2021): The \u0000rst Persian dataset for Community Question Answering, it\ncontains questions and answers gathered from the most famous Persian forum. It includes 989\nquestions and 21,915 tagged answers. Questions are categorized as valid or invalid, with invalid\nones including advertisements, surveys, news, and collaboration announcements. Questions with\nless than 3 or more than 300 answers are also marked invalid. Answers are tagged as Bad, Potential,\nor Good. Bad answers include advertisements, non-Persian responses, greetings, empathy, thanks,\nstickers, and Persian typed in English characters. Potential answers refer to other sources, and Good\nanswers are partial or complete and relevant to the question. For \u0000ne-tuning PersianLLaMA, 10,468\nquestion-answer pairs were extracted, including valid questions and appropriate answers.\nAlpaca: This dataset is command-response style, created by OpenAI's Davinci-003 engine. The\noriginal dataset is in English, and its Persian-translated version is used for \u0000ne-tuning language\nmodels.\nOASST1 (OpenAssistant Conversations Dataset)(Köpf et al., 2023): Consists of 161,443 messages in\n35 different languages, styled as human assistant conversations. This dataset includes various\nconversation trees. Each conversation tree starts with an initial message as the root node, which can\nhave several child messages as responses, and these child messages can have multiple responses.\nAll messages have a role attribute, indicating whether the speaker is an assistant or a \"commander\".\nThe translated version of this dataset was used for \u0000ne-tuning the model.\n4.1.2 Natural Language Understanding Datasets\nIn this part, datasets used for \u0000ne-tuning in the \u0000eld of NLU are introduced.\nArmanEmo(Mirzaee et al., 2022): A Persian dataset for emotion detection in text. It includes a\ncollection of Persian sentences and texts, each labeled with an emotion such as sadness, anger, fear,\nsurprise, disgust, and others. The dataset comprises 6,125 records for training and 1,125 for testing.\nIt's used for evaluating models' sentiment analysis capabilities in Persian.\nPersian News1: This dataset is a structured summarization dataset for the Persian language,\nconsisting of 93,207 records. Each line of the dataset includes a title, main text, summary, and\ncategory. It's used for topic classi\u0000cation and summarization.\nSyntran-fa2: A Persian question-answering dataset offering a coherent and complete response for\neach question. It contains about 50,000 question-answer records. The purpose of evaluating models\non this dataset is to compare the general knowledge learned by the models. \nTo compare the PersianLLaMA model, other natural language generation models capable of producing\nPersian language were \u0000ne-tuned on the mentioned datasets. We selected 4 models: mGPT, mT5-XL,\nGPT2-Persian3, and Parsgpt4.  No article has been o\u0000cially published for the mGPT, mT5, GPT2-Persian,\nand parsgpt, and due to the lack of Persian text generation models, these models were used. The details\nof these models are described in Table 4.\nTable 4:   details of the models used for evaluation and comparison\nPage 13/23\nModel Infrastructures Main Application Base Model\nmT5 Encoder + Decoder NLG +NLU T5\nmGPT Decoder NLG GPT-2\nGPT2-Persian Decoder NLG GPT-2\nparsgpt Decoder NLG GPT-2\n       \nTo achieve the most accurate and best results, con\u0000gurations suggested by the creators of the mentioned\nmodels were utilized. Table 5 provides the con\u0000gurations applied for \u0000ne-tuning each model.\nTable 5:   Con\u0000gurations applied for \u0000ne-tuning each model.\nModel Max length epochs Batch size Lr Warmup steps\nPersianLLaMA-Lora 2048 1 2 1e-3 100\nPersianLLaMA-Zero 768 1 2 1e-3 100\nmT5 256 3 8 5e-5 500\nmGPT 512 3 8 1e-5 -\nGPT2-Persian 256 3 8 5e-4 100\nparsgpt 512 3 8 1e-3 -\n4.2 Results\nIn the Appendix section, examples and analyses of samples from each model are provided. Table 6\npresents the evaluation results of the outputs of each model, scored by ChatGPT, and the averages in the\n\u0000eld of text generation, depending on the dataset used for \u0000ne-tuning.\nTable 6:   The results of evaluating the outputs of each model based on different datasets in the \u0000eld of\nNLG.\nPage 14/23\n  Alpaca OASST1 PerCQA MeDiaQA\nPersianLLaMA-Lora 7.3 8.1 6.8 6.8\nPersianLLaMA-Zero 4.3 4.2 3.8 3.8\nGPT2-Persian 3.9 2.3 3.0 3.0\nmGPT 5.1 4.0 3.4 3.4\nparsgpt 1.4 1.1 0.2 0.2\nmT5 2.5 3.6 0.9 0.9\nThe experiments conducted on various models in the \u0000eld of natural language generation indicate a\nde\u0000nite superiority of PersianLLaMA-LoRA in all tests. PersianLLaMA-LoRA's outputs demonstrate its\ngood understanding of the input text, ability to maintain context, and produce logical, acceptable\nresponses. PersianLLaMA-Zero generates relevant, though sometimes brief, responses. It seems to\ncomprehend the context better than some models but lacks in attention to details. GPT2Persian generally\nshows poor performance in understanding and text generation. MGPT displays a good grasp of the\ncontext and provides relatively good responses, although sometimes it can be repetitive or overly\nextensive. Parsgpt's performance varies depending on the datasets; it generates relevant answers for\nsome questions and irrelevant outputs for others. mT5 often produces overly brief, irrelevant, and\ninappropriate responses lacking depth and logic, being suitable only in the context of summarization.\nOverall, the results demonstrate that, in understanding and responding to input text, Persian LLaMA-LoRA\nperforms signi\u0000cantly better than the other models.\nTable 7 shows the evaluation results of each model based on the type of natural language understanding\ntasks.\nTable 7:   The results of evaluating the outputs of each model based on different datasets in the \u0000eld of\nNLU.\n  ArmanEmo PersianQA Persian NewsClassi\u0000cation Persian NewsSummary\nPersianLLaMA-Lora 65 8.26 82.38 83.47\nPersianLLaMA-Zero 42.13 2.82 14.21 49.65\nGPT2-Persian 8 0 11.23 41.41\nmGPT 45.6 5.47 19.76 81.54\nparsgpt 59.92 2.91 38.46 60.35\nmT5 06.85 4.58 12.74 64.02\nPage 15/23\nAs the results indicate, PersianLLaMA-Lora performs well in natural language understanding tasks,\nmaking it a viable option for Persian language understanding and generation tasks, yielding good results.\nIt's important to note that some labels in datasets may be ambiguous. For example, in the ArmanEmo\ndataset, the text \"\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000 \u0000\u0000 \u0000\u0000\" (peaceful coexistence) is labeled as having unde\u0000ned emotions,\nbut PersianLLaMA interprets the emotion of this text as happiness, which seems more accurate than the\ndataset's labeling.\n5 Limitations\nThe PersianLLaMA large language model represents the start of an ongoing journey in improving\nunderstanding and generation of the Persian language. In this section, we discuss the current limitations\nnot only as challenges but also as opportunities for research and development. Our aim is to ensure that\nPersianLLaMA is a signi\u0000cant step towards the growth and evolution of researchers in the \u0000eld of Persian\nlanguage processing.\nTraining Data Scope Limitation: Given that our training dataset lacks diversity, our models'\nknowledge is limited. Future works aim to use a more diverse training dataset to achieve a more\ncomprehensive model knowledge.\nLack of Ethical Safeguards: During this research, no measures were taken to cleanse or prevent the\ngeneration of unethical texts. This means that the models might unintentionally produce biased,\ntoxic, offensive, or harmful content and fail to \u0000lter such content.\nVariability in Logical and Numerical Processing: The models may not perform well in logical\nreasoning and mathematical calculations, a challenge common to all large language models. They\nmight handle some scenarios well but struggle in others due to current training methodologies. This\npresents an important area for improving numerical problem-solving capabilities in models.\nAbsence of a Speci\u0000c Persian Evaluation Dataset: The lack of a specialized dataset for evaluating\nPersian text generation signi\u0000cantly limits our ability to precisely evaluate and compare the\nperformance of models in Persian language tasks. This absence hinders comprehensive\ncomparative analysis with other models in the same language.\nHardware Limitations: Our training was limited due to restricted access to computational resources,\nspeci\u0000cally two A100 GPUs with 80 GB capacity for a short duration. This not only reduced the depth\nof our training process but also limited the scope of experiments and extensive improvements of the\nmodels.\n6 Conclusion\nThis study marks a signi\u0000cant milestone in Persian natural language processing by introducing\nPersianLLaMA, the \u0000rst large-scale language model for Persian. PersianLLaMA demonstrates remarkable\npro\u0000ciency in understanding and generating Persian text, outperforming existing models. Its development\nopens new avenues for advanced NLP applications tailored to the Persian language, from enhanced\nPage 16/23\nprompt-base tasks to sophisticated text analysis tools. This achievement not only showcases the\npotential of language-speci\u0000c models but also highlights the importance of linguistic diversity in AI\nresearch. Despite the achievements reported in this research, PersianLLaMa faces challenges such as\nlimited knowledge base and hardware limitations, underscoring the need for continuous development.\nThese limitations provide directions for future improvements. Looking forward, the success of\nPersianLLaMA paves the way for future innovations in language technology, especially for\nunderrepresented languages, encouraging similar endeavors worldwide.\n7 Future work\nIn future research, the team plans to expand the model's capabilities by training it on a larger and more\ndiverse dataset. This approach aims to enhance the model's understanding and generation of Persian\ntext, making it more versatile and effective across a wider range of applications. By incorporating a\nbroader variety of text sources, the model can achieve a more comprehensive understanding and\ngenerating of the Persian language, covering both formal and colloquial styles in greater depth.\nAdditionally, the team intends to explore the development of models with an increased number of\nparameters. This advancement is expected to signi\u0000cantly improve the model's performance by enabling\nit to capture more complex language patterns and nuances. The enhanced model will be subjected to\nrigorous human evaluation to compare its performance against current models. This step is crucial for\nassessing the model's practical effectiveness in real-world applications and for guiding further\nre\u0000nements. The aim is to develop a state-of-the-art Persian language model that sets a new benchmark\nin natural language processing for Persian.\nDeclarations\nEthical Approval\n\"Not Applicable\"\nAvailability of supporting data\nThe datasets generated and analysed during the current study are not publicly available because they\nconstitute an excerpt of research in progress, but are available from the corresponding author upon\nreasonable request.\nCompeting interests\nThe authors have no relevant \u0000nancial or non-\u0000nancial interests to disclose. The authors have no\ncon\u0000icts of interest to declare that are relevant to the content of this article. All authors certify that they\nhave no a\u0000liations with or involvement in any organization or entity with any \u0000nancial interest or non-\n\u0000nancial interest in the subject or materials discussed in this manuscript. The authors have no \u0000nancial\nor proprietary interests in any material discussed in this article.\nPage 17/23\nFunding\nThe authors did not receive support from any organization for the submitted work. No funding was\nreceived to assist with the preparation of this manuscript. No funding was received for conducting this\nstudy. No funds, grants, or other support were received.\nAuthors' contributions\nAll authors contributed to the study's conception and design. Material preparation, data collection, and\nanalysis were performed by Abbasi, Ghafouri and Firouzmandi. The \u0000rst draft of the manuscript was\nwritten by Abbasi and Ghafouri, and all authors commented on previous versions of the manuscript. All\nauthors read and approved the \u0000nal manuscript.\nAcknowledgments\nThe authors wish to express their sincere gratitude to Vira Intelligent Data Mining Company for\ngenerously providing access to their GPU resources. This support was instrumental in facilitating the\ncomputational aspects of our research. Their contribution not only enhanced the e\u0000ciency of our model\ntraining processes but also enabled us to push the boundaries of our study, leading to more robust and\ncomprehensive outcomes. We acknowledge and appreciate their valuable contribution to the\nadvancement of our research.\nReferences\n1. Anil, G. T. G. R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth,\nA., Millican, K., Silver, D., Petrov, S., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J.,\nPitler, E., Lillicrap, T., . . . Vinyals, O. (2023). Gemini: A Family of Highly Capable Multimodal Models. \n2. Balachandran, A. (2023). Tamil-Llama: A New Tamil Language Model Based on Llama 2. arXiv\npreprint arXiv:2311.05845. \n3. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W.,\nSutton, C., & Gehrmann, S. (2023). Palm: Scaling language modeling with pathways. Journal of\nMachine Learning Research, 24(240), 1-113. \n4. Chung, H. W., Constant, N., Garcia, X., Roberts, A., Tay, Y., Narang, S., & Firat, O. (2023). Unimax: Fairer\nand more effective language sampling for large-scale multilingual pretraining. arXiv preprint\narXiv:2304.09151. \n5. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805. \n\u0000. Floridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. Minds and\nMachines, 30, 681-694. \n7. Ghafouri, A., Abbasi, M. A., & Naderi, H. (2023). AriaBERT: A Pre-trained Persian BERT Model for\nNatural Language Understanding. \nPage 18/23\n\u0000. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). Lora: Low-rank\nadaptation of large language models. arXiv preprint arXiv:2106.09685. \n9. Jamali, N., Yaghoobzadeh, Y., & Faili, H. (2021). Percqa: Persian community question answering\ndataset. arXiv preprint arXiv:2112.13238. \n10. Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M.,\nStanley, O., & Nagy\u0000, R. (2023). OpenAssistant Conversations--Democratizing Large Language Model\nAlignment. arXiv preprint arXiv:2304.07327. \n11. Koubaa, A. (2023). GPT-4 vs. GPT-3.5: A concise showdown. \n12. Kudo, T., & Richardson, J. (2018). Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226. \n13. Lin, Y.-T., & Chen, Y.-N. (2023). Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned\nLanguage Model. arXiv preprint arXiv:2311.17487. \n14. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V.\n(2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. \n15. Mirzaee, H., Peymanfard, J., Moshtaghin, H. H., & Zeinali, H. (2022). ArmanEmo: A Persian Dataset\nfor Text-based Emotion Detection. arXiv preprint arXiv:2207.11808. \n1\u0000. Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., & Awadallah, A. (2023). Orca:\nProgressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. \n17. Nguyen, X.-P., Zhang, W., Li, X., Aljunied, M., Tan, Q., Cheng, L., Chen, G., Deng, Y., Yang, S., & Liu, C.\n(2023). SeaLLMs--Large Language Models for Southeast Asia. arXiv preprint arXiv:2312.00738. \n1\u0000. Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E.,\n& Launay, J. (2023). The Re\u0000nedWeb dataset for Falcon LLM: outperforming curated corpora with\nweb data, and web data only. arXiv preprint arXiv:2306.01116. \n19. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018, June).\nDeep Contextualized Word Representations. In M. Walker, H. Ji, & A. Stent, Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) New Orleans, Louisiana.\n20. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are\nunsupervised multitask learners. OpenAI blog, 1(8), 9. \n21. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020).\nExploring the limits of transfer learning with a uni\u0000ed text-to-text transformer. The Journal of\nMachine Learning Research, 21(1), 5485-5551. \n22. Rasley, J., Rajbhandari, S., Ruwase, O., & He, Y. (2020). Deepspeed: System optimizations enable\ntraining deep learning models with over 100 billion parameters. Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, \n23. Ray, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges,\nbias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems. \nPage 19/23\n24. Sennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909. \n25. Shliazhko, O., Fenogenova, A., Tikhonova, M., Mikhailov, V., Kozlova, A., & Shavrina, T. (2022). mgpt:\nFew-shot learners go multilingual. arXiv preprint arXiv:2204.07580. \n2\u0000. Staudemeyer, R. C., & Morris, E. R. (2019). Understanding LSTM--a tutorial into long short-term\nmemory recurrent neural networks. arXiv preprint arXiv:1909.09586. \n27. Suárez, P. J. O., Romary, L., & Sagot, B. (2020). A monolingual approach to contextualized word\nembeddings for mid-resource languages. arXiv preprint arXiv:2006.06202. \n2\u0000. Suri, H., Zhang, Q., Huo, W., Liu, Y., & Guan, C. (2021). MeDiaQA: A Question Answering Dataset on\nMedical Dialogues. arXiv preprint arXiv:2108.08074. \n29. Taghizadeh, N., Doostmohammadi, E., Seifossadat, E., Rabiee, H. R., & Tahaei, M. S. (2021). SINA-\nBERT: a pre-trained language model for analysis of medical texts in Persian. arXiv preprint\narXiv:2104.07613. \n30. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N.,\nHambro, E., & Azhar, F. (2023). Llama: Open and e\u0000cient foundation language models. arXiv preprint\narXiv:2302.13971. \n31. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava,\nP., & Bhosale, S. (2023). Llama 2: Open foundation and \u0000ne-tuned chat models. arXiv preprint\narXiv:2307.09288. \n32. Wang, C., Cho, K., & Gu, J. (2020). Neural machine translation with byte-level subwords. Proceedings\nof the AAAI conference on arti\u0000cial intelligence, \n33. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., & Jiang, D. (2023). Wizardlm:\nEmpowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. \n34. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., & Raffel, C. (2020). mT5:\nA massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934. \n35. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). Bertscore: Evaluating text generation\nwith bert. arXiv preprint arXiv:1904.09675. \n3\u0000. Zhao, Z., Li, Y., Hou, C., Zhao, J., Tian, R., Liu, W., Chen, Y., Sun, N., Liu, H., & Mao, W. (2022).\nTencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities. arXiv\npreprint arXiv:2212.06385. \n37. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., & Xing, E. (2023).\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685. \nFootnotes\n1. https://huggingface.co/datasets/pn_summary\n2. https://huggingface.co/datasets/SLPL/syntran-fa\n3. https://huggingface.co/bolbolzaban/gpt2-persian\nPage 20/23\n4. https://huggingface.co/HooshvareLab/gpt2-fa\nFigures\nFigure 1\nA timeline of existing large language models in recent years. The timeline was established mainly\naccording to the release date (e.g., the submission date to arXiv) of the technical paper for a model\nPage 21/23\nFigure 2\nThe training pipeline of PersianLLaMA model with two approachs. Our training \u0000ow can be separated\ninto two approachs, namely model training model from scratch on Persian data from OSCAR dataset and\ntraining with LoRA on Persian Wikipedia data. In the \u0000rst approach, we give LLaMA 23 million Persian\ntexts to train the model. In the second approach, we train the LLaMA model with LoRA on all Persian\nWikipedia articles, which contain 2.4 million documents\nPage 22/23\nFigure 3\nFigure 2: train the second PersianLLaMA model on wikipedia using the LoRA\nFigure 4\nFigure 2: train the second PersianLLaMA model on wikipedia using the LoRA\nPage 23/23\nFigure 5\nFigure 2: \u0000ne-tune the second PersianLLaMA model using the LoRA\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nAPPENDICES.docx",
  "topic": "Persian",
  "concepts": [
    {
      "name": "Persian",
      "score": 0.8234587907791138
    },
    {
      "name": "Computer science",
      "score": 0.7925121784210205
    },
    {
      "name": "Natural language processing",
      "score": 0.7488973140716553
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6398978233337402
    },
    {
      "name": "Automatic summarization",
      "score": 0.5734328031539917
    },
    {
      "name": "Natural language",
      "score": 0.5564054846763611
    },
    {
      "name": "Machine translation",
      "score": 0.5416929721832275
    },
    {
      "name": "Language identification",
      "score": 0.5180666446685791
    },
    {
      "name": "Universal Networking Language",
      "score": 0.42621710896492004
    },
    {
      "name": "Natural language programming",
      "score": 0.4183875322341919
    },
    {
      "name": "Comprehension approach",
      "score": 0.2203100025653839
    },
    {
      "name": "Linguistics",
      "score": 0.2172580361366272
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I120234788",
      "name": "Imam Hossein University",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I67009956",
      "name": "Iran University of Science and Technology",
      "country": "IR"
    }
  ],
  "cited_by": 4
}