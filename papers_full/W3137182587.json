{
  "title": "Can Vision Transformers Learn without Natural Images?",
  "url": "https://openalex.org/W3137182587",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2252485255",
      "name": "Kodai NAKASHIMA",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116606213",
      "name": "Hirokatsu Kataoka",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2976390251",
      "name": "Asato Matsumoto",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2065213542",
      "name": "Kenji Iwata",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101229142",
      "name": "Nakamasa Inoue",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2172850644",
      "name": "Yutaka Satoh",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2252485255",
      "name": "Kodai NAKASHIMA",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology",
        "University of Tsukuba"
      ]
    },
    {
      "id": "https://openalex.org/A2116606213",
      "name": "Hirokatsu Kataoka",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2976390251",
      "name": "Asato Matsumoto",
      "affiliations": [
        "University of Tsukuba",
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2065213542",
      "name": "Kenji Iwata",
      "affiliations": [
        "National Institute of Advanced Industrial Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101229142",
      "name": "Nakamasa Inoue",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2172850644",
      "name": "Yutaka Satoh",
      "affiliations": [
        "University of Tsukuba",
        "National Institute of Advanced Industrial Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6703787047",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3036982689",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2950187998",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2785325870",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W6756427901",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6787774617",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W6766120893",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W6743135768",
    "https://openalex.org/W2799113232",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2734663976",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2145607950",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2996535895",
    "https://openalex.org/W6701655646",
    "https://openalex.org/W2339378878",
    "https://openalex.org/W2750549109",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W343636949",
    "https://openalex.org/W3122542623",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963465221",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4394645241",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3014034447",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3112788634",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2078206416",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2800860906",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W3101298150",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963703197",
    "https://openalex.org/W2732026016"
  ],
  "abstract": "Is it possible to complete Vision Transformer (ViT) pre-training without natural images and human-annotated labels? This question has become increasingly relevant in recent months because while current ViT pre-training tends to rely heavily on a large number of natural images and human-annotated labels, the recent use of natural images has resulted in problems related to privacy violation, inadequate fairness protection, and the need for labor-intensive annotations. In this paper, we experimentally verify that the results of formula-driven supervised learning (FDSL) framework are comparable with, and can even partially outperform, sophisticated self-supervised learning (SSL) methods like SimCLRv2 and MoCov2 without using any natural images in the pre-training phase. We also consider ways to reorganize FractalDB generation based on our tentative conclusion that there is room for configuration improvements in the iterated function system (IFS) parameter settings of such databases. Moreover, we show that while ViTs pre-trained without natural images produce visualizations that are somewhat different from ImageNet pre-trained ViTs, they can still interpret natural image datasets to a large extent. Finally, in experiments using the CIFAR-10 dataset, we show that our model achieved a performance rate of 97.8, which is comparable to the rate of 97.4 achieved with SimCLRv2 and 98.0 achieved with ImageNet.",
  "full_text": "Can Vision Transformers Learn without Natural Images?\nKodai Nakashima1,2*, Hirokatsu Kataoka1*, Asato Matsumoto1,2,\nKenji Iwata1, Nakamasa Inoue3 and Yutaka Satoh1,2\n1National Institute of Advanced Industrial Science and Technology (AIST), Japan\n2University of Tsukuba, Japan\n3Tokyo Institute of Technology, Japan\nfnakashima.kodai, hirokatsu.kataoka, matsumoto-a, kenji.iwata, yu.satoug@aist.go.jp, inoue@c.titech.ac.jp\nAbstract\nIs it possible to complete Vision Transformer (ViT) pre-\ntraining without natural images and human-annotated labels?\nThis question has become increasingly relevant in recent\nmonths because while current ViT pre-training tends to rely\nheavily on a large number of natural images and human-\nannotated labels, the recent use of natural images has re-\nsulted in problems related to privacy violation, inadequate\nfairness protection, and the need for labor-intensive anno-\ntations. In this paper, we experimentally verify that the re-\nsults of formula-driven supervised learning (FDSL) frame-\nwork are comparable with, and can even partially outper-\nform, sophisticated self-supervised learning (SSL) methods\nlike SimCLRv2 and MoCov2 without using any natural im-\nages in the pre-training phase. We also consider ways to re-\norganize FractalDB generation based on our tentative con-\nclusion that there is room for conﬁguration improvements in\nthe iterated function system (IFS) parameter settings of such\ndatabases. Moreover, we show that while ViTs pre-trained\nwithout natural images produce visualizations that are some-\nwhat different from ImageNet pre-trained ViTs, they can still\ninterpret natural image datasets to a large extent. Finally, in\nexperiments using the CIFAR-10 dataset, we show that our\nmodel achieved a performance rate of 97.8, which is compa-\nrable to the rate of 97.4 achieved with SimCLRv2 and 98.0\nachieved with ImageNet.\n1 Introduction\nIn contemporary visual recognition, transformer architec-\ntures (Vaswani et al. 2017) are gradually replacing the usage\nof convolutional neural networks (CNNs), which has long\nbeen dominated in the ﬁeld of computer vision (CV).\nTransformer architectures, which are based on self-\nattention mechanisms, were initially employed in natural\nlanguage processing (NLP) tasks such as machine trans-\nlation and semantic analysis. Recently, however, we have\nwitnessed the development of epoch-making methods that\nuse transformer modules such as Bidirectional Encoder\nRepresentations from Transformers (BERT) (Devlin et al.\n2019) and Generative pre-trained Transformers (GPT)-f1,\n2, 3g(Radford et al. 2018a,b; Brown and et al. 2020). The\ntrend is now gradually shifting from NLP to CV tasks, where\n*These authors contributed equally.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Val. accuracy transition during ﬁne-tuning on CIFAR-10.\n(b) Attention maps in fractal images. The brighter areas are\nthose receiving more attention.\nFigure 1: The impact of FDSL ViT pre-training on a Frac-\ntalDB. These results show that (a) the pre-trained model fa-\ncilitates ﬁne-tuning accuracy improvements to a level that\nis much closer to an ImageNet pre-trained ViT, and (b) the\nFractalDB pre-trained ViT recognizes fractal images while\nidentifying complex contours. We believe that this property\ncontributes to the pre-training effect even though fractal im-\nages do not have background areas.\none of the most active topics is the use of Vision Transform-\ners (ViTs) for image classiﬁcation (Dosovitskiy et al. 2021),\nprimarily because ViTs can effectively process and recog-\nnize an image based on Transformers with minimum mod-\niﬁcations. Additionally, even though the reimplementation\nprocess is reasonably straightforward, it has been shown that\nViTs often perform at least as well as state-of-the-art trans-\nfer learning. However, it is also true that ViT tends to require\na large amount of data in the pre-training phase. For ex-\nample, Dosovitskiy et al.(Dosovitskiy et al. 2021) reported\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n1990\nFigure 2: Following (Dosovitskiy et al. 2021), we list the (a) RGB embedding ﬁlters, (b) position embedding similarity, and\n(c) mean attention distance in the frameworks of formula-driven supervised learning (FDSL) with FractalDB and supervised\nlearning (SL) with human-annotated ImageNet. When compared to SL, we found that the FDSL pre-trained ViT enables the\nacquisition of slightly different ﬁlters, the same position embedding, and a wider receptive ﬁeld. Of particular note, in the mean\nattention comparison with SL, we found that FDSL enables us to acquire a wider receptive ﬁeld from the early layers.\nthat unless a ViT architecture is pre-trained with more than\n100 million images, its accuracy is inferior to a CNN. This\npre-training problem can be somewhat alleviated by Data-\nefﬁcient image Transformer (DeiT) (Touvron et al. 2021).\nOn the other hand, the use of large-scale image datasets\nmay be problematic from the perspective of privacy preser-\nvation, annotation labor, and artiﬁcial intelligence (AI)\nethics. In fact, the use of representative datasets that in-\nclude natural images taken by camera is currently restricted\nto academic or educational usage. This problem cannot be\nresolved by using self-supervised learning (SSL) for auto-\nmatic natural image labeling because training on natural im-\nage datasets will still raise privacy-violation and fairness-\nprotection concerns. In relation to this, other large-scale\ndatasets (e.g., human-related images in ImageNet (Yang\net al. 2020) and 80M Tiny Images (Torralba, Fergus, and\nFreeman 2008)1) are now being deleted due to AI ethic is-\nsues. To date, other huge datasets such as JFT-300M (Sun\net al. 2017) and Instagram-3.5B (Mahajan et al. 2018) are\nnot publicly available. Since these dataset-related problems\nsigniﬁcantly limit research opportunities in this ﬁeld, the re-\nsearch community must carefully consider the use of large-\nscale datasets in terms of availability and reliability while\nworking to overcome them. However, if a method could be\ndeveloped to train a ViT without using natural images, we\nbelieve that the result would have an impact similar to previ-\nously proposed models such as BERT, GPT, and ViT. This is\nbecause, even though a transformer is used to compose the\ncore modules in each of these models, the resulting insights\nobtained to date have led to signiﬁcant research community\n1https://groups.csail.mit.edu/vision/TinyImages/\nadvancements.\nIn this context, the formula-driven supervised learning\n(FDSL) concept, which involves automatically generating\nimage patterns and their labels based on a mathematical for-\nmula that includes rendering functions and real-world rules,\nwas ﬁrst proposed in late 2020 (Kataoka et al. 2020). In that\npaper, Kataoka et al. showed why fractal geometry (Man-\ndelbrot 1983; Barnsley 1988) was the best way to construct\na dataset in the FDSL framework, while the present paper\nexamines the question of whether or not ViTs can be pre-\ntrained with FractalDB in FDSL. We contend that the ViTs\ncan successfully be pre-trained with the FDSL framework\nbecause the self-attention mechanism enables background\neffects between fractal and natural images to be eliminated\nand because the mechanism can understand wholly fractal\nshapes consisting of iteratively recursive patterns. Herein,\nwe also report on our rethink of the fractal rendering func-\ntion in relation to an iterated function system (IFS) (Barnsley\n1988). In our careful ‘sanity check’, we reveal that the pro-\ncess of reconsidering the IFS relationship has shown there is\nstill room for improving the pre-training effect and enhanc-\ning natural image classiﬁcations. Figure 1, 2 shows both the\nimpact of FractalDB pre-trained ViT and the characteristics\nof its training properties.\nThe contributions of this paper are as follows: We clar-\nify that the pre-training of ViT can be completed without\nany natural images and human annotations. Our reconsid-\neration of the fractal rendering function show that the per-\nformance of a FractalDB-10k pre-trained ViT is similar to\nSL approaches (see Table 4) and slightly surpasses the self-\nsupervised ImageNet with a SimCLRv2 pre-trained ViT (see\n1991\n‘Average’ in Table 5). More speciﬁcally, in experiments us-\ning the CIFAR-10 dataset, we show that our model achieved\na performance rate of 97.8, which is comparable to the rate\nof 97.4 achieved with SimCLRv2 and the 98.0 rate achieved\nwith ImageNet. Additionally, we found that FractalDB pre-\ntraining allows us to acquire slightly different ﬁlters, sim-\nilar position embeddings, and a wider range of receptive\nﬁelds (see Figure 2(c)). Furthermore, the FractalDB pre-\ntrained model appears to pay more attention to the contour\nareas (see Figure 1b), which helps to promote effective pre-\ntraining that facilitates natural image understanding.\n2 Related Work\n2.1 Network Architectures for Image Recognition\nAs mentioned previously, CNNs are currently popular in the\nvisual recognition ﬁelds, and several well-deﬁned structures\nhave emerged through a large number of trials over the last\ndecade (Krizhevsky, Sutskever, and Hinton 2012; Simonyan\nand Zisserman 2015; Szegedy et al. 2015; He et al. 2016;\nXie et al. 2017; Huang et al. 2017; Tan and Le 2019). Very\nrecently, at the end of 2020, the architecture began shifting\nto Transformers (Vaswani et al. 2017) originating from NLP.\nSince this mechanism has enabled the construction of revo-\nlutionary models (e.g., BERT (Devlin et al. 2019), GPT-f1,\n2, 3g(Radford et al. 2018a,b; Brown and et al. 2020)), the\nCV community is now in the process of replacing the de-\nfacto-standard CNN-based architectures with Transformer.\nOf these, one of the most insightful ViT architectures\nis (Dosovitskiy et al. 2021), which has been found to per-\nform comparably to state-of-the-art alternative approaches,\neven though it is a basic architecture in terms of image\ninput. Separately, experiments have shown that the JFT-\n300M/ImageNet-21k pre-trained ViT performed well in\nterms of accuracy but is hampered by the fact that ViTs re-\nquire over ten-million-order labeled images in representa-\ntion learning. Furthermore, even though the issue of learn-\ning with large-scale datasets has been somewhat alleviated\nwith the introduction of DeiTs (Laplan et al. 2020), the pre-\ntraining problem still exists in image classiﬁcation tasks.\n2.2 Image Dataset and Training Framework\nIt has been said that the deep learning era started with the\nILSVRC (Russakovsky et al. 2015), and it is undeniable that\ntransfer learning with large-scale image datasets has con-\ntributed to accelerating visual training (He, Girshick, and\nDoll´ar 2019). Initially, the ImageNet (Deng et al. 2009)\npre-trained model were widely used for diverse tasks not\nlimited to image classiﬁcation. However, even in million-\nscale datasets, there are concerns regarding issues such as AI\nethics and copyrights, e.g., fairness protection, privacy viola-\ntions, and offensive labeling. Due to these sensitive issues, as\nmentioned above, human-related labels in ImageNet (Yang\net al. 2020) and 80M Tiny Images (Torralba, Fergus, and\nFreeman 2008) are in the process of being deleted, and it\nhas become necessary for community members to consider\nterms-of-use in large-scale image datasets in order to create\npre-trained models responsibly.\nHere, it should be noted that efforts to reduce the image\nlabeling labor required by human annotators in SSL meth-\nods have progressed signiﬁcantly in recent years. The ﬁrst\nsuch methods created pseudo labels based on semantic con-\ncepts (Doersch, Gupta, and Efros 2015; Noroozi and Favaro\n2016; Noroozi et al. 2018; Noroozi, Pirsiavash, and Favaro\n2017; Gidaris, Singh, and Komodakis 2018) and trained fea-\nture representations through image reconstruction (Zhang,\nIsola, and Efros 2016). However, in terms of performance\nrates, current SSL methods are closer to SL with human an-\nnotations (e.g., MoCo (He et al. 2020; Chen et al. 2020c),\nSimCLR (Chen et al. 2020a,b), SwA V (Caron et al. 2020)).\nIn this context, in addition to alleviating the annotation\nlabor issue, FDSL (Kataoka et al. 2020) was proposed to\novercome the problems of AI ethics and copyrights. The\nFDSL framework is similar to SSL, but FDSL methods do\nnot require any natural images taken by a camera. Instead,\nthe framework simultaneously and automatically generates\nimage patterns and paired labels for pre-training image rep-\nresentations. With this point in mind, we investigate whether\na formula-driven image dataset could sufﬁciently optimize a\nViT in the pre-training phase. At the same time, we com-\npare a pre-trained ViT created through the FDSL framework\nwith supervised and self-supervised pre-training. It is our\ncontention that if the supervised/self-supervised pre-training\ncan be replaced by FDSL, it could be possible to pre-train\nViTs in the future without using any natural images.\n3 FDSL for ViTs\nAt the beginning of the section, we describe a brief review of\nFractalDB (Kataoka et al. 2020), followed by how to apply\nthis auto-generated pre-training dataset to ViT (Section 3.1).\nThen we consider how to re-organize FractalDB in order to\nimprove from the original one (Section 3.2).\n3.1 FractalDB\nThe most successful approach in FDSL relies on FractalDB,\na dataset consists of fractal images generated with the IFS.\nOn the 2D Euclidian space R2, IFS is deﬁned by\nIFS = fR2; w1; w2; \u0001\u0001\u0001 ; wN ; p1; p2; \u0001\u0001\u0001 ; pN g; (1)\nwhere wi : R2 !R2 is an afﬁne transformation, pi is a\nprobability, and N is the number of transformations. With\nIFS, a fractal S = fxtg1\nt=1 is constructed by the random it-\neration algorithm, which repeats the following two steps: (1)\nselect an afﬁne transformation w\u0003from fw1; w2; \u0001\u0001\u0001 ; wN g\nunder pre-deﬁned probabilities pi = p(w\u0003 = wi), and (2)\nproduce the next point by xt+1 = w\u0003(xt). Here, the initial\npoint x1 = (0; 0) corresponds to the center of an image, and\nt is incremented by 1 at each iteration.\nThe original FractalDB (Kataoka et al. 2020) consisted\nof 1,000 or 10,000 different fractal categories with 1,000\ninstances per category. In their experiments, the Frac-\ntalDB pre-trained ResNet-50 partially outperformed the\nsame model pre-trained with human-annotated datasets such\nas ImageNet and Places, but they did not show whether or\nnot ViTs could learn without natural images because their\nsuccess rates rely on convolutional architectures. Neverthe-\nless, even though a FractalDB image does not contain a\n1992\n(a) FractalDB\n (b) Random colorization\n (c) Number of transformations\n (d) Range of parameters\nFigure 3: Reconsidering item of FractalDB. (a) Original FractalDB in (Kataoka et al. 2020). (b) Randomly colorized FractalDB\non the foreground fractal areas, since natural images are not grayscale. (c) FractalDB by considering #transformations (N ).\nAlthough the conventional FractalDB consists of combined N = f2; 3; 4; 5; 6; 7gtransformations (see also (a) in the ﬁgure),\nthe ﬁrst row shows the images with N = 2, the second row illustrates the images with N = 6. The lower N tends to be a\n‘vivid’ shape in a fractal image.Here, the range of the transformation parameters is ﬁxed at\u00061:0. (d) In IFS, the transformation\nparameters basically take values in the range\u00061.0. Since changing the range to\u00060:8 (ﬁrst row) or \u00061:5 (second row) produces\ndifferent shapes even though the other parameters are ﬁxed.\nbackground area, we believe that the self-attention mecha-\nnism can effectively focus on the fractal patterns while fol-\nlowing complex contours and ignoring background areas.\n3.2 Reconsidering FractalDB\nWe introduce several modiﬁcations to FractalDB that were\nnot considered in (Kataoka et al. 2020). Speciﬁcally: (i)\ngrayscale vs. color, (ii) the number of transformations (N in\nequation (1)), (iii) the range of transformation parameters,\nand (iv) training epochs.\nGrayscale vs. color.Conventional FractalDB images are\ndrawn by moving dots or patches in grayscale. However,\nthe natural images used for common pre-training include not\nonly grayscale, but also images with various color combina-\ntions. In the color generation process, color points or patches\nwere drawn randomly at each iteration time (see Figure 3b).\nConducting pre-training with a dataset consisting of colored\nfractal images makes it possible for the model to acquires\nfeature representations related to color.\nNumber of transformations.FractalDB consisted of N =\nf2; 3; 4; 5; 6; 7g transformations. However, in Figure 3c,\nfractal shapes can change drastically due to the N param-\neter. According to this ﬁgure, smaller values tend to render\n‘vivid’ shapes while larger values tend to generate ‘vague’\nshapes. Therefore, to determine what shapes are effective in\na pre-trained ViT, we explore the FractalDB conﬁguration\nwith a ﬁxed N = f2; 4; 6; 8; 10g.\nRange of transformation parameters.The parameters in\nIFS take values in the range \u00061.0. We found that the trans-\nformation parameters also make a signiﬁcant difference in\nfractal images, as can be seen in Figure 3d. In this study,\nfour different parameters \u0006f0.8, 1.0, 1.2, 1.5gwere set.\nTraining epoch.Recent SSL methods have begun to con-\nsider longer training epochs. For example, SimCLR tried\nto lengthen training epochs up to 1k [epoch] (Chen et al.\n2020a). Therefore, although the ﬁrst FDSL study (Kataoka\net al. 2020) was conducted with just 90 [epochs], we\nconducted further tests to determine. We also investigated\nlonger training epochs in light of the more recent SSL meth-\nods. We evaluate up to 300 epochs to determine if they pro-\nvide additional improvements to FractalDB pre-training.\n4 Experiments\nThis section reports on experiments conducted to verify the\neffectiveness of FractalDB pre-trained ViTs from various as-\npects. We begin by attempting to identify a better FractalDB\nconﬁguration for ViT. Then, we evaluate the best conﬁgu-\nration obtained on several image datasets. Speciﬁcally, by\nfollowing the procedure outlined in (Touvron et al. 2021),\nwe evaluate the CIFAR-10/100 (C10/C100), Stanford Cars\n(Cars), and Flowers-102 (Flowers) datasets. We also quan-\ntitatively compare the FractalDB pre-trained ViT with the\npre-training performed with representative large-scale im-\nage datasets (e.g., ImageNet-1k and Places-365) and archi-\ntectures (e.g., ResNet-50). For simplicity, this study uses the\nViT tiny model and DeiT (Touvron et al. 2021) settings to\nconﬁrm the properties of the FractalDB pre-trained model\n(hereafter, this is called ViT/ViT-Ti).\n4.1 Exploration Study\nFollowing the reconsideration discussed in Section 3.2, we\nexplored an effective FractalDB conﬁguration for ViT using\nthe process described in (Kataoka et al. 2020). We began by\ncarrying out experiments related to the FDSL-family (Frac-\ntalDB, BezierCurveDB, and PerlinNoiseDB; see Table 1),\narchitectures (ViT, gMLP, and CNN; see Table 2), and #cat-\negory/#instance (see Figure 4).\nComparison with other FDSL methods (see Table 1).In\naddition to FractalDB, Kataoka et al. (Kataoka et al. 2020)\nproposed datasets based on Perlin noise (PerlinNoiseDB)\nand Bezier curves (BezierCurveDB). Accordingly, we con-\nducted pre-training and ﬁne-tuning experiments on the ViT\n1993\n0 500 1000\nCategory/Instance (#)\n70\n80\n90\n100Accuracy (%)\n #Category\n#Instance\n(a) C10\n0 250 500 750 1000\nCategory/Instance (#)\n60\n70\n80Accuracy (%)\n #Category\n#Instance (b) C100\n0 250 500 750 1000\nCategory/Instance (#)\n0\n20\n40\n60\n80Accuracy (%)\n #Category\n#Instance (c) Cars\n0 500 1000\nCategory/Instance (#)\n60\n70\n80\n90\n100Accuracy (%)\n #Category\n#Instance (d) Flowers\nFigure 4: Effects of #category and #instance. The other parameter is ﬁxed at 1,000, e.g., #Category is ﬁxed at 1,000 while\n#Instance varies among f16, 32, 64, 128, 256, 512, 1,000g.\nC10 C100 Cars Flowers\nScratch 78.3 57.7 11.6 77.1\nPerlinNoiseDB 94.5 77.8 62.3 96.1\nBezierCurveDB 96.7 80.3 82.8 98.5\nFractalDB-1k 97.1 82.6 87.1 98.3\nTable 1: Comparisons of ViT pre-training among FractalDB\nin (Kataoka et al. 2020), other FDSL with BezierCurvesDB,\nand PerlinNoiseDB.\nArch. Params C10 C100 Cars Flowers(M)\nResNet-50 25 96.1 80.0 82.5 98.2\ngMLP-Ti/16 6 95.4 77.4 78.7 94.2\nViT-Ti/16 5 97.1 82.6 87.1 98.3\nTable 2: Comparison among ViT, gMLP, and ResNet with\nconventional FractalDB-1k pre-training.\nwith those FDSL methods as well.\nFrom Table 1, we can conﬁrm the existence of higher\naccuracy levels compared to scratch training for all of the\nFDSL methods. The improvements are up to f+18.8, +24.9,\n+75.5, +21.2g higher accuracy with FractalDB-1k on the\nfC10, C100, Cars, Flowersgdatasets, respectively. Note that\nthe conﬁguration used here is based on the original and\nstandard FractalDB-1k, which contains 1,000 [categories]\n\u00021,000 [instances]. In FDSL methods, the FractalDB pre-\ntrained ViT outperforms the other pre-trained models. The\naccuracy levels from BezierCurveDB pre-trained ViT are\nf+0.4, +2.3, +4.3, -0.2g on the fC10, C100, Cars, Flow-\nersgdatasets, respectively. Although the results showed that\nthe BezierCurveDB pre-trained model recorded a better ac-\ncuracy level on Flowers, we decided to use the FractalDB\npre-trained model in consideration of the other performance\nrates.\nViT vs. gMLP/ResNet (see Table 2).We additionally ver-\nify ResNet-50, gMLP-Ti/16 (Liu et al. 2021), and ViT-Ti/16.\nWe assigned the same data augmentation methods as DeiT\nto other architecture since the investigation was comprehen-\nsively executed. Practically, the FractalDB-1k pre-trained\nResNet and gMLP were improved by using the data aug-\nmentation methods. Especially, the accuracy with ResNet-\n50 on C10 dataset was increased from 94.1 (reported by\n(Kataoka et al. 2020)) to 96.1 (+2.0 pt). The performance\nrates are listed in Table 2. For all ﬁne-tuning datasets, the\naccuracy of ViT is higher than others. Therefore the self-\nattention module based architecture is more compatible with\nFractalDB than CNN or MLP based architecture.\n#category/#instance (see Figures 4a, 4b, 4c, 4d).Figure 4\nshows the effects of category and instance increases on Frac-\ntalDB pre-training. We set category and instance as vari-\nables, ﬁxing the former at 1000 and varying the others to\nvalues of f16, 32, 64, 128, 256, 512, 1000g. From the ex-\nperimental results, we determined that larger category and\ninstance values tend to lead to higher accuracy levels on\na ﬁne-tuning dataset. Particularly in the case of FractalDB\npre-training, we found that the category increase is more ef-\nfective for transfer learning on an image dataset. This result\nis intuitive because the task is easier for datasets with fewer\ncategories and more instances. Hereafter, we set the category\nat 1,000 \u00021,000 [instance] as the basic FractalDB setting.\nGrayscale vs. color (see Table 3a).The table shows that\npre-training on FractalDB works better with grayscale im-\nages than colored images. In particular, it can be seen that\nin C100 and Cars datasets, the improvement gaps are +1.0\npt and +1.1 improved over the pre-training performed with\ncolored fractal images. These results conﬁrm that colored\nrepresentation is not required in ViT architecture.\nNumber of transformations (see Table 3b).Reducing the\n#transformation tends to improve the pre-training effect.\nThis is particularly true when N is 2, which results in the\nhighest accuracy for all the ﬁne-tuning datasets. This in-\ndicates that it is important to input images with complex\nshapes for ViT pre-training.\nRange of transformation parameters (see Table 3c).Ac-\ncording to the results shown in the table, the original param-\neter [-1.0, 1.0] provided the best level of accuracy. However,\nit is necessary to carefully consider the range since the per-\nformance gap may be up to three points, for example, on\nCars, between 88.4 in [-1.0, 1.0] and 85.4 in [-1.2, 1.2].\nTraining epochs.In FractalDB pre-training, as in the SSL,\na longer training epoch tends to achieve better performance\nrates. The accuracy levels obtained in 300 epoch pre-training\nrecorded the best scores in three out of the four different\n1994\nC10 C100 Cars Flowers\nGrayscale 97.1 82.6 87.1 98.3\nColor 96.8 81.6 86.0 98.3\n(a) Grayscale vs. color. The conﬁguration of\nIFS follows the conventional FractalDB.\nN C10 C100 Cars Flowers\n2 97.4 84.3 88.4 96.5\n4 97.3 82.4 87.7 98.0\n6 97.1 82.7 86.3 98.7\n8 97.2 82.3 88.1 97.4\n10 97.1 82.0 86.1 98.3\n(b) Number of transformations. The param-\neter range is ﬁxed at \u00061:0.\nRange C10 C100 Cars Flowers\n\u00060:8 96.9 82.3 88.1 96.7\n\u00061:0 97.4 84.3 88.4 96.5\n\u00061:2 97.1 81.9 85.4 98.4\n\u00061:5 97.1 82.5 87.4 98.4\n(c) Range of transformation parameters. N\nis ﬁxed at 2.\nTable 3: Effects of colorization and IFS parameters.\nPT PT Img PT Type C10 C100 Cars Flowers VOC12 P30 IN100\nScratch – – 78.3 57.7 11.6 77.1 64.8 75.7 73.2\nPlaces-30 Natural Supervision 95.2 78.5 69.4 96.7 77.6 – 86.5\nPlaces-365 Natural Supervision 97.6 83.9 89.2 99.3 84.6 – 89.4\nImageNet-100 Natural Supervision 94.7 77.8 67.4 97.2 78.8 78.1 –\nImageNet-1k Natural Supervision 98.0 85.5 89.9 99.4 88.7 80.0 –\nFractalDB-1k Formula Formula-supervision 97.4 84.3 88.4 96.5 81.4 79.3 88.2\nFractalDB-10k Formula Formula-supervision 97.8 83.1 89.1 98.8 82.6 80.8 88.0\nTable 4: Comparison of pre-training ViT on various datasets. The pre-trained image (PT img) types shown are fnatural image\n(Natural) and formula-driven image dataset (Formula)g; while the pre-training types (PT Types) shown are fSL (supervision)\nand FDSL (formula-supervision)g. The Underlined bold and bold scores show the best and second-best values.\ndatasets. By comparing the pre-trained model at 100 epochs,\nwe can see that, in terms of accuracy, the 300 epoch model\nincreased f+0.7, +0.5, +4.0, +1.8g points on the fC10,\nC100, Cars, Flowersgdatasets, respectively.\n4.2 Comparisons\nBased on the results of Section 4.1, we construct a\nnew FractalDB-f1k, 10kg and experiment with the new\nFractalDB-f1k, 10kgin this subsection.\nIn addition to performing training from scratch with\nadditional ﬁne-tuning datasets. We compared the perfor-\nmance of FractalDB pre-trained ViT with fImageNet-1k,\nImageNet-100, Places-365, Places-30gpre-trained ViT. The\nImageNet-100 (IN100) and Places-30 (P30) categories were\nrandomly selected from ImageNet-1k and Places-365 pre-\nsented in (Kataoka et al. 2020). We also evaluate the mod-\nels on IN100, P30, and Pascal VOC 2012 (VOC12). Addi-\ntionally, we also evaluated SSL withfJigsaw, Rotation, Mo-\nCov2, SimCLRv2g on ImageNet-1k. The results obtained\nshow the effectiveness of the FDSL in the compared proper-\nties of human supervision with natural images (Table 4) and\nself-supervision with natural images (Table 5).\nLarger categories.Table 4 indicates that a larger FractalDB\npre-training enhances the transformer in image classiﬁca-\ntion. In fact, the accuracy levels are improved tof97.4, 97.8g\nby FractalDB-f1k, 10kgpre-training on C10.\nFDSL vs. SL (see Table 4).Next, natural image datasets\nand the FractalDB in the pre-training phase were compared.\nTable 4 describes the pre-training (PT), pre-training im-\nages (PT img), and their performance levels in terms of\naccuracy. Initially, the FractalDB-1k/10k pre-trained ViTs\noutperformed the pre-trained models on 100k-order la-\nbeled datasets (ImageNet-100 and Places-30). Furthermore,\neven though the FractalDB-10k pre-trained ViT did not ex-\nceed the performance with million-order labeled datasets\n(ImageNet-1k and Places-365), the scores obtained were\nsimilar to the ImageNet-1k pre-trained model.\nFDSL vs. SSL (see Table 5).Through comparisons with\nSimCLRv2, we clariﬁed that the FractalDB-10k pre-trained\nViT performs slightly higher (FractalDB-10k 88.7 vs. Sim-\nCLRv2 88.5), in terms of average accuracy, on the represen-\ntative datasets. Additionally, we found that FractalDB-10k\npre-training outperformed SimCLRv2 pre-training on C10\n(97.8 vs. 97.4), Cars (89.1 vs. 84.9), and P30 (80.8 vs. 80.0).\nHowever, the accuracy obtained was lower on C100 (83.5 vs.\n84.1), Flowers (98.8 vs. 98.9), and VOC12 (82.6 vs. 86.2). In\naddition to SimCLRv2, we implemented Jigsaw, Rotation,\nand MoCov2 in order to compare their results with those of\nFractalDB-10k. The results show that while the FractalDB-\n10k pre-trained model tends to higher accuracy levels than\nother SSL methods.\nVisualization (see Figures 1b, 2 and 5).As for ViT, the\nﬁlters of the ﬁrst linear embedding, similarity of positional\nembedding, and mean attention distance can be visualized\nby following the process described in (Dosovitskiy et al.\n2021). Figure 2(a) shows ﬁlters trained with ImageNet-\n1k and FractalDB-1k. Although ViT pre-trained on both\nImageNet-1K and FractalDB-1K acquire similar ﬁlters, the\nFractalDB-1k pre-trained ViT ﬁlters tend to spread over\nwide areas, while the ImageNet-1k pre-trained ViT ﬁlters\nseem to concentrate on the center areas. Figure 2(b) shows\nthe cosine similarity of positional embedding corresponding\n1995\nMethod Use Natural Images? C10 C100 Cars Flowers VOC12 P30 Average\nJigsaw YES 96.4 82.3 55.7 98.2 82.1 80.6 82.5\nRotation YES 95.8 81.2 70.0 96.8 81.1 79.8 84.1\nMoCov2 YES 96.9 83.2 78.0 98.5 85.3 80.8 87.1\nSimCLRv2 YES 97.4 84.1 84.9 98.9 86.2 80.0 88.5\nFractalDB-10k NO 97.8 83.1 89.1 98.8 82.6 80.8 88.7\nTable 5: Detailed results with FDSL vs. SSL. The additional column labeled ‘Use Natural Images?’ indicates whether or not\nnatural images were used in the pre-training phase. ‘Average’ is the average accuracy of all datasets in the table. Note that\nImageNet-100 has been eliminated from this table because the listed SSL methods are trained by images on ImageNet-1k. The\nUnderlined bold and bold scores show the best and second-best values.\n(a) ImageNet-1k\n (b) FractalDB-1k\n (c) FractalDB-10k\nFigure 5: Attention maps.\nto the input patch at each row and column. From the visual-\nized ﬁgures, it can be seen that the FractalDB-1k pre-trained\nViT acquired position embeddings at each row and column\nthat are similar to ImageNet-1k pre-trained ViT.\nFigure 2(c) shows mean attention distance in the same\nmanner as the original ViT (Dosovitskiy et al. 2021). In com-\nparison to the ImageNet-1k pre-training, the FractalDB-1k\npre-trained ViT appears to look at widespread areas in an\nimage.\nFigure 5 shows attention maps with different pre-training.\nHere, it can be seen that, similar to ImageNet pre-training\n(Figure 5a), the FractalDB-1k pre-trained ViT focuses on\nthe object areas (Figure 5b). Additionally, we can see that\nthe FractalDB-10k pre-trained ViT looks at more speciﬁc ar-\neas (Figure 5c) compared to FractalDB-1k pre-training. Fig-\nure 1b shows attention maps in fractal images. From these\nﬁgures, it can be seen that the FractalDB pre-training seems\nto perform recognition by observing contour lines. In rela-\ntion to Figure 2(c), we believe that the ability to recognize\ncomplex and distant contour lines enabled the extraction of\nfeatures from a widespread area.\n5 Conclusion and Discussion\nThis study demonstrated that the FDSL framework makes\nit possible to train ViTs without the use of natural images\nor human-annotated labels. Additionally, we showed how\nour FractalDB pre-trained ViT achieved similar performance\nrates to the human-annotated ImageNet pre-trained model,\nand partially outperformed SimCLRv2 self-supervised Ima-\ngeNet pre-trained model. Furthermore, based on the results\nin this study, the following ﬁndings are presented:\nFeature representation with FractalDB pre-trained ViT.\nWe see that the FractalDB pre-trained ViT acquired different\nfeature representations in the ﬁrst linear embeddings (Fig-\nure 2(a)) and similarly arranged position embeddings (Fig-\nure 2(b)) when compared to the ImageNet-1k pre-training.\nMoreover, Figure 1b shows that the ViT tends to pay at-\ntention to contour areas in the pre-training. We believe that\nthis pre-trained model enabled feature acquisition in a wider\nrange of areas than the ImageNet-1k pre-trained model (Fig-\nure 2(c)). We also identiﬁed the complex contour lines used\nto classify fractal categories in the pre-training phase.\nThe effect of FractalDB reconsideration.Extending the\nprocess outlined in (Kataoka et al. 2020), we noticed the po-\ntential for improvement and attempted to implement several\nnovel conﬁgurations such as using colored FractalDB, in-\ncreasing the number of transformations, using a wider range\nof transformation parameters, and applying longer training\nepochs. As shown in the experimental section, the highest\nscores occurred when the following parameter combination\nwas used: grayscale fractal images, N = 2for #transforma-\ntion, range of \u00061.0, and 300 epochs in the pre-training.\nIs it possible to complete pre-training of ViT without nat-\nural images and human-annotated labels?In Table 5, we\nshowed that the performance of FractalDB-10k was compa-\nrable in terms of accuracy to the SimCLRv2 pre-trained ViT.\nAlthough 10M images are used in FractalDB-10k, no natural\nimages were used in the pre-training. Therefore, an FDSL-\nbased pre-training dataset can be safely and effectively used\nto train a ViT in terms of AI ethics and image copyright, pro-\nviding we can exceed the SL accuracy achieved via human\nannotation (Table 4). In relation to this point, we showed\nthat the FDSL pre-training recorded similar accuracy levels\nto those achieved via SL, which means that it is possible that\nthe ImageNet pre-trained model may be replaced by a model\nwithout any natural images in the near future.\n1996\nAcknowledgement\nThis paper is based on results obtained from a project,\nJPNP20006, commissioned by the New Energy and In-\ndustrial Technology Development Organization (NEDO).\nThis work was supported by JSPS KAKENHI Grant Num-\nber JP19H01134. Computational resource of AI Bridging\nCloud Infrastructure (ABCI) provided by National Institute\nof Advanced Industrial Science and Technology (AIST) was\nused. We want to thank Yoshihiro Fukuhara and Munetaka\nMinoguchi for their helpful comments in research discus-\nsions.\nReferences\nBarnsley, M. F. 1988. Fractals Everywhere.Academic Press.\nNew York.\nBrown, T. B.; and et al. 2020. Language Models are Few-\nShot Learners. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nCaron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.;\nand Joulin, A. 2020. Unsupervised Learning of Visual Fea-\ntures by Contrasting Cluster Assignments. In Advances in\nNeural Information Processing Systems (NeurIPS).\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.\nA Simple Framework for Contrastive Learning of Visual\nRepresentations. In International Conference on Machine\nLearning (ICML).\nChen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-\nton, G. 2020b. Big Self-Supervised Models are Strong Semi-\nSupervised Learners. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nChen, X.; Fan, H.; Girshick, R.; and He, K. 2020c. Im-\nproved Baselines with Momentum Contrastive Learning.\narXiv:2003.04297.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR).\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL).\nDoersch, C.; Gupta, A.; and Efros, A. 2015. Unsupervised\nVisual Representation Learning by Context Prediction. In\nIEEE International Conference on Computer Vision (ICCV).\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representation (ICLR).\nGidaris, S.; Singh, P.; and Komodakis, N. 2018. Unsuper-\nvised Representation Learning by Predicting Image Rota-\ntions. In International Conference on Learning Representa-\ntion (ICLR).\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020. Mo-\nmentum Contrast for Unsupervised Visual Representation\nLearning. In IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR).\nHe, K.; Girshick, R.; and Doll ´ar, P. 2019. Rethinking Im-\nageNet Pre-training. In IEEE International Conference on\nComputer Vision (ICCV).\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In IEEE International\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nHuang, G.; Liu, Z.; Maaten, L. v. d.; and Weinberger, K. Q.\n2017. Densely Connected Convolutional Networks. InIEEE\nInternational Conference on Computer Vision and Pattern\nRecognition (CVPR).\nKataoka, H.; Okayasu, K.; Matsumoto, A.; Yamagata, E.;\nYamada, R.; Inoue, N.; Nakamura, A.; and Satoh, Y . 2020.\nPre-training without Natural Images. In Asian Conference\non Computer Vision (ACCV).\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nageNet Classiﬁcation with Deep Convolutional Neural Net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NeurIPS).\nLaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling Laws for Neural Language Mod-\nels. arXiv:2001.08361.\nLiu, H.; Z., D.; So, D. R.; and Le, Q. V . 2021. Pay Attention\nto MLPs. arXiv:2105.08050.\nMahajan, D.; Girshick, R.; Ranathan, V .; He, K.; Paluri, M.;\nLi, Y .; Bharambe, A.; and Maaten, L. v. d. 2018. Exploring\nthe Limits of Weakly Supervised Pretraining. In European\nConference on Computer Vision (ECCV).\nMandelbrot, B. 1983. The fractal geometry of nature. Amer-\nican Journal of Physics, 51(3).\nNoroozi, M.; and Favaro, P. 2016. Unsupervised Learning\nof Visual Representations by Solving Jigsaw Puzzles. In\nEuropean Conference on Computer Vision (ECCV).\nNoroozi, M.; Pirsiavash, H.; and Favaro, P. 2017. Represen-\ntation Learning by Learning to Count. InIEEE International\nConference on Computer Vision (ICCV).\nNoroozi, M.; Vinjimoor, A.; Favaro, P.; and Pirsiavash, H.\n2018. Boosting Self-Supervised Learning via Knowledge\nTransfer. In IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR).\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018a. Improving language understanding by generative\npre-training. In Technical Report, OpenAI.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2018b. Language Models are Unsupervised\nMultitask Learners. InInternational Conference on Machine\nLearning (ICML).\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\nBerg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Vi-\nsual Recognition Challenge. International Journal of Com-\nputer Vision (IJCV), 115(3).\n1997\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Con-\nvolutional Networks for Large-Scale Image Recognition.\nIn International Conference on Learning Representations\n(ICLR).\nSun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Re-\nvisiting Unreasonable Effectiveness of Data in Deep Learn-\ning Era. In International Conference on Computer Vision\n(ICCV).\nSzegedy, C.; Liu, W.; Jia, Y .; Sermanet, P.; Reed, S.;\nAnguelov, D.; Erhan, D.; Vanhoucke, V .; and Rabinovich, A.\n2015. Going Deeper with Convolutions. In IEEE Interna-\ntional Conference on Computer Vision and Pattern Recogni-\ntion (CVPR).\nTan, M.; and Le, Q. V . 2019. EfﬁcientNet: Rethinking Model\nScaling for Convolutional Neural Networks. In Interna-\ntional Conference on Machine Learning (ICML).\nTorralba, A.; Fergus, R.; and Freeman, W. T. 2008. 80 Mil-\nlion Tiny Images: A Large Data Set for Nonparametric Ob-\nject and Scene Recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI).\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning (ICML).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nXie, S.; Girshick, R.; Doll ´ar, P.; Tu, Z.; and He, K. 2017.\nAggregated Residual Transformations for Deep Neural Net-\nworks. In IEEE International Conference on Computer Vi-\nsion and Pattern Recognition (CVPR).\nYang, K.; Qinami, K.; Fei-Fei, L.; Deng, J.; and Rus-\nsakovsky, O. 2020. Towards Fairer Datasets: Filtering and\nBalancing the Distribution of the People Subtree in the Ima-\ngeNet Hierarchy. In Conference on Fairness, Accountability\nand Transparency (FAT).\nZhang, R.; Isola, P.; and Efros, A. A. 2016. Colorful Image\nColorization. In European Conference on Computer Vision\n(ECCV).\n1998",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7630876302719116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6764514446258545
    },
    {
      "name": "Transformer",
      "score": 0.6699209213256836
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5550925731658936
    },
    {
      "name": "Training set",
      "score": 0.5305331945419312
    },
    {
      "name": "Machine learning",
      "score": 0.46658116579055786
    },
    {
      "name": "Image (mathematics)",
      "score": 0.46070021390914917
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39084723591804504
    },
    {
      "name": "Engineering",
      "score": 0.06886601448059082
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I73613424",
      "name": "National Institute of Advanced Industrial Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    }
  ]
}