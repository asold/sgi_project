{
  "title": "Under what circumstances do Large Language Models (LLMs) outperform humans, and who judges? A systematic review of task types and benchmarking criteria in social science literature",
  "url": "https://openalex.org/W4388345017",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2118869142",
      "name": "Xinzhi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2510823276",
      "name": "Yuner Zhu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6629545505",
    "https://openalex.org/W6601096871",
    "https://openalex.org/W4385459268",
    "https://openalex.org/W4312915403",
    "https://openalex.org/W2889463086",
    "https://openalex.org/W1484251804",
    "https://openalex.org/W4366150698",
    "https://openalex.org/W2164506448",
    "https://openalex.org/W2471445381",
    "https://openalex.org/W2111469023",
    "https://openalex.org/W6764956643",
    "https://openalex.org/W4362585490",
    "https://openalex.org/W3006679461",
    "https://openalex.org/W4309926814",
    "https://openalex.org/W4388583174",
    "https://openalex.org/W4313334409",
    "https://openalex.org/W4308481882",
    "https://openalex.org/W2955088691",
    "https://openalex.org/W2278432463",
    "https://openalex.org/W3198419608",
    "https://openalex.org/W4381549348",
    "https://openalex.org/W38170266",
    "https://openalex.org/W4389380302",
    "https://openalex.org/W4302231525",
    "https://openalex.org/W2209024180"
  ],
  "abstract": "Humanities and social sciences scholars have conducted several experiments to test the competence of large language models (LLMs) against biological humans. These ardent efforts generated a mix of optimism and pessimism in applying LLMs for scientific inquiry or inviting LLMs as co-workers. Unfortunately, less investigation interrogates the task type (“what are the LLMs prompted to do”) and the geographical distribution of the data utilized to validate and benchmark LLMs in these experiments (“who judges”). In this commentary, we reviewed 72 empirical articles published in SSCI-indexed journals that employed LLMs in their method design. We examined the task type (i.e., prediction, classification, or content production), the country of origin of the benchmark datasets, and researchers’ subjective interpretation of LLM’s effectiveness in completing human communication tasks. Results reveal varying conclusions across social science disciplines. LLMs excelled in applied social sciences but underperformed in philosophy, politics, communication, and ethics. LLMs are less effective in content production but better in classification. A substantial geopolitical bias was evident in the origins of benchmark datasets, with dominance from the US, UK, and developed European nations. We also found that researchers demonstrated a negativity bias, holding LLMs to a higher standard and being more critical when evaluating LLMs’ performance in addressing issues within their own cultural context. We proposed two recommendations, including developing a framework to calibrate LLM’s tasks and a call for more inclusive benchmarking criteria in the age of generative AI.",
  "full_text": " 1  Under what circumstances do Large Language Models (LLMs) outperform humans, and who judges? A systematic review of task types and benchmarking criteria in social science literature  Xinzhi Zhang, Yuner Zhu   Updated on 30-Jan-2024 (version 3)    Xinzhi Zhang (PhD, City University of Hong Kong) is an Associate Professor in the Department of Media and Communication at the City University of Hong Kong. Previously, he was an Associate Professor in the Department of Interactive Media at the School of Communication at Hong Kong Baptist University when this paper was initially completed and uploaded to OSF in Nov 2023. Email: xzzhang2@gmail.com, ORCID number: 0000-0003-3479-9327.   Yuner Zhu (PhD, University of Hong Kong) is a Research Assistant Professor in the Department of Interactive Media, School of Communication, Hong Kong Baptist University. Email: yunerzhu@gmail.com, ORCID number: 0000-0003-2772-2188.   Funding: This project is supported by the Interdisciplinary Research Clusters Matching Scheme (project number: RC-IRCMS/19-20/D05) by the Hong Kong Baptist University.    Conflict of Interest: The authors have no conflict of interest.   Pre-print Versions: version 1&2: 3-Nov-2023 | version 3: 30-Jan-2024 (In this version, we updated some results and also considered the review comments from the ICA and other journals. We are grateful for the reviewers and readers who offered their valuable suggestions to improve our humble exploration).   This combined PDF file contains:  1. The full text  2. The Supplementary materials    \n 2  Abstract Humanities and social sciences scholars have conducted several experiments to test the competence of large language models (LLMs) against biological humans. These ardent efforts generated a mix of optimism and pessimism in applying LLMs for scientific inquiry or inviting LLMs as co-workers. Unfortunately, less investigation interrogates the task type (“what are the LLMs prompted to do”) and the geographical distribution of the data utilized to validate and benchmark LLMs in these experiments (“who judges”). In this commentary, we reviewed 72 empirical articles published in SSCI-indexed journals that employed LLMs in their method design. We examined the task type (i.e., prediction, classification, or content production), the country of origin of the benchmark datasets, and researchers’ subjective interpretation of LLM’s effectiveness in completing human communication tasks. Results reveal varying conclusions across social science disciplines. LLMs excelled in applied social sciences but underperformed in philosophy, politics, communication, and ethics. LLMs are less effective in content production but better in classification. A substantial geopolitical bias was evident in the origins of benchmark datasets, with dominance from the US, UK, and developed European nations. We also found that researchers demonstrated a negativity bias, holding LLMs to a higher standard and being more critical when evaluating LLMs’ performance in addressing issues within their own cultural context. We proposed two recommendations, including developing a framework to calibrate LLM’s tasks and a call for more inclusive benchmarking criteria in the age of generative AI. Keywords: Large Language Models, generative AI, ChatGPT, computational social science, world system theory, geopolitics of knowledge production, validation   \n 3 Under what circumstances do Large Language Models (LLMs) outperform humans, and who judges? A commentary on the task types and validation criteria in social science literature   Humanities and social sciences scholars have conducted a flurry of experiments on Large Language Models (LLMs) since the launch of ChatGPT in late 2022, such as conducting interviews with LLMs as if they were biological humans (Leippold, 2023), tasking them with surveys and aptitude tests that were originally designed for humans (Argyle, 2023), or simulating LLMs to impersonate professional communicators, research assistants, or artists (Yan, 2023). These efforts have generated a mix of optimism and pessimism regarding LLMs’ capabilities in social science research and real-world communication practices.  Regrettably, limited attention has been given to two crucial factors that may influence a fair and reasonable interpretation of the effectiveness of LLMs, namely, the types of tasks LLMs are employed for (such as content production, prediction, and classification) and the countries from which the benchmarks are derived. Simply put, when there is a claim that LLMs can convince judges into thinking they achieve comparable or superior performance in social science tasks to human benchmarks, we ask: what tasks, who judges, and which humans served as the benchmarks?  The present commentary, based on a systematic review, draws insights from data science to distinguish task types and the world system theory to scrutinize geopolitical biases in benchmarking criteria. We analyze empirical studies published in SSCI-indexed journals that involve LLMs in their research design. The results offer insights on the geopolitics of knowledge production, aiming to promote a more diverse and inclusive research agenda of LLMs-augmented social research. Interpreted Valence: Good or Bad?  \n 4  Valence is defined as the author’s explicit and conclusive interpretation of LLM’s performance. The history has repeatedly shown that when a novel and disruptive technology emerges, people tend to cluster into two groups. One group sees it as a driving force for social progress while the other perceives it as a threat to humanity, authenticity, and integrity (Lane, Overbye-Thompson, & Gagrčin, 2024). We apply this optimism-pessimism dichotomy to analyze the valence of the conclusions that researchers have drawn on LLMs. We distinguish between studies where researchers perceive LLMs as capable and beneficial for social science research tasks, and those that is concluded with negative and pessimistic viewpoints about LLMs through empirical investigation. Task Type: What the LLMs are Prompted for?  The performance of LLMs varies across different tasks (Ziems et al., 2023). As suggested by related studies reviewing the broadly defined AI and human communication (Gil de Zúñiga, Goyanes, & Durotoye, 2023; Wing, 2019), we distinguish three task types of LLMs, namely, classification, prediction, and content production, which are corresponding to, and have the potential to replace or assist, conventional manual coding, survey, and open-ended interview methods.  Classification involves assigning pre-defined categories to input data based on the observable patterns. LLM classifier provides an alternative to traditional manual coding, allowing automated models to fulfill the role of human annotators. Prediction involves estimating a specific outcome that is not directly observable but can be inferred from input data. The outputs can be of various forms, such as categorical labels of sentiments (Huang, Wang, & Yang, 2023)or continuous ratings of item desirability (Hommel, 2023). Before the invention of LLMs, this type of work was typically done by closed-ended surveys.  Lastly, content production is enabled by the generative AI such as text-based ChatGPT and image-based Midjourney. The tasks involve creating new text, images, or \n 5 audio, based on a given prompt. It is the most creative type of task. Prior to the invent of LLMs, this type of work is typically done by interview or open-ended surveys. Our explication is illustrated in Table 1.  | Table 1 is here. | Who Judges? The Validation and its Geopolitics  In judging LLMs’ performance in the social science research, valence can be derived from two distinct and complementary approaches: (1) benchmarking against pre-existing ground truth and (2) benchmarking against human annotators specifically recruited for the focal research. In the first approach, researchers instructed LLMs to answer a series of questions, often extracted from well-known tests that are originally designed for humans, such as IQ tests and professional aptitude exams. Then, LLMs’ answers were validated against the ground truth, such as the answer keys of these exams. We documented the countries where the ground truth was constructed. For example, if the researchers assess LLMs’ abilities of providing correct answers to the Scholastic Aptitude Test (SAT) questions as designated by the college admission based in the U.S., we considered the U.S. as the country of origin of the ground truth. In situations where standard answers are absent, human judges may rely on their scholarly expertise, subjective interpretation, or common sense to formulate ground truth. For example, when LLMs are prompted to provide course-specific assistance to students, researchers utilize their existing knowledge to evaluate the quality and usefulness of LLM-generated messages. In this case, the researchers’ country of affiliation serves as the reference point for determining the ground truth. When multiple countries were involved in a certain article, we calculated their fractional representation in the benchmark dataset. Human values are implicitly embedded in the process. The question central to our inquiry here is: where does the ground truth come from and who assumes the role of judges?   \n 6 However, while LLMs may have imperfections, humans themselves can be susceptible to the same errors observed in LLMs. Human performance serves as a fairer baseline than the ground truth. Therefore, in addition to ground-truth benchmarking, some researchers also applied human benchmarking, where human annotators are recruited to undertake the same tasks as LLMs, allowing for a comparison of their performance metrics with those of LLMs. The objective is to gauge the extent to which LLMs are able to replicate or surpass human-level understanding and proficiency. Against this backdrop, an important question arises regarding the specific human counterparts to which the LLMs are compared to.  Several empirical studies have revealed that a handful of large organizations in the WEIRD (western, educated, industrialized, rich, and democratic) nations wield substantial influence over the digital public sphere in the production of public knowledge, with limited presence from the developing countries (Chang, Himelboim, & Dong, 2009; Koch et al., 2021; Wallerstein, 1979). The present study aims to examine this proposition in the context of LLM-driven research by analysing the geographical distribution of benchmark sources at the national level.  Meanwhile, several literatures on the human-technology interaction propose that individual acceptance of technology are affected by the cultural values and configured along the cultural lines (Guzman & Lewis, 2020; Shin, Chotiyaputta, & Zaid, 2022). To explore this argument, we further categorize countries into eight civilization groups as per Huntington’s (2011) typology. By doing so, we aim to examine the performance of LLMs across different populations and investigate whether LLMs can be effectively steered to specific demographic groups beyond the western context where they were developed.  Furthermore, according to Zakour (2004) and Baptista & Oliveir (2015), acceptance of technology is deeply intertwined with individuals’ self-identification. A researcher’s \n 7 interpretation of benchmarking outcomes might be swayed by their personal affiliations. In light of this, we compare the valence of works where scholars employ LLMs to simulate populations within their own cultural groups (referred as “in-group evaluation”) with those focused on populations outside their cultural groups (referred as “out-group evaluation”).  Methods  We retrieved empirical research articles retrieved from the Social Science Citation Index (SSCI) of the Web of Science on 29 Aug 2023 and reached 722 articles, after the screening (Appendix Figure 1A), the final number of reviewed articles was 72. A list of the reviewed articles is reported in the Supplementary Information (SI). We coded the valence, task type, the country of origin of the validation. We reported the detailed coding protocol in the SI.  Two authors coded 20 articles and the inter-coder reliability was satisfactory (>.85 in the Kripendroff Alpha values).  Observation 1: The Valance Varies across Task Types and Disciplines  Figure 1 reports that the majority of studies expressed a positive assessment of their experimental results (58.57%). The proportions of studies with neutral and negative evaluations are similar. | Figure 1 is here. |  Figure 2 maps different social science disciplines (the scatterplots) based on the valence interpretations (the X-axis) and the number of studies within a particular discipline (the Y-axis, as a feasible proxy measure to examine how actively scholars are engaged with the LLMs). We found that in most applied social sciences, such as business, social work, education, green and sustainable sciences, and public health, scholars tend to hold a more positive view (higher than the grand mean). Conversely, fields like nursing, ethics, and psychiatry tend to reach a more conservative perspective regarding the performance of \n 8 LLMs. Political science, communication studies, and linguistics fall in the middle, and the framing of the valence for these three subjects are quite similar.  | Figure 2 is here. | Regarding task types, two-thirds of the reviewed studies employed LLMs for content production tasks, followed by prediction and classification (Figure 3(a)). When considering the valence, we found that LLMs excelled in classification tasks, with over 80% of the studies reporting positive conclusions. The proportions of negative perspectives were similar in generation and prediction tasks, each comprising a fifth of the reviewed studies. Generation tasks yielded a slightly higher proportion of mixed conclusions compared to prediction tasks (Figure 3(b)).  | Figure 3(a) and Figure 3(b) are here. | Recommendation 1: Reconsidering the nature of the task when deploying the LLMs Considering the above findings, we suggest reconsidering the nature of the LLM tasks from two alternative dimensions: (a) the extent to which the LLMs’ outputs are likely to be globally standardized for mass production, and (b) the extent to which the LLMs’ outputs are normative. The first dimension proposes a spectrum from Ritzer’s (2000) idea of McDonaldization, which represents the standardized mass production, to Benjamin’s idea of “aura” (Benjamin, 1936), (p. 4), which refers to a highly contextualized creation and viewing experience that confined to particular time and space. The second dimension assesses the extent to which the generated content contains opinion-based and open-ended deliberation, rather than close-end, factual, and accuracy-driven information.  The cross-over of these two dimensions yields four different revised task types, as presented in Table 2.  | Table 2 is here. |  As shown in Table 2, the upper left quadrant includes topics domains like artistic creation and philosophical comments. Research tasks falling into this category include \n 9 assessing the conversations between ChatGPT and social science scholars or prompting ChatGPT to impersonate philosophers. On the one hand, these tasks are yielding different outputs across different cultures and political systems. On the other hand, they are also normative, devoid of factual ground truth to validate model outputs. These tasks are not “factcheckable”, nor do they possess definitive or universally applicable answers.  We recommend reevaluating the suitability of current validation in these domains.  The upper right quadrant includes tasks related to areas like news production, where “fact-checkability”, accountability, and reliability are crucial; yet, these tasks are also highly contextualized as hardly are there one-size-fits-all solutions around the globe. The contextual nature of tasks in this category highlights the limitations of LLMs or any other AI-enabled automated tools in the current stage. For example, a 38-country cross-polity investigation of news chatbots (i.e., conversational agents for news delivery and audience engagement) reveals a considerate variation in the performance across different media systems (Zhang et al., 2022). We assert that deploying LLMs in these fields demands quality assurance protocols and reliability standards to mitigate potential harm.  The lower left quadrant includes tasks whose solutions exhibit minimal variation across contexts. For instance, a second-language tutoring materials consist of standardized and decontextualized elements, which can transcend the boundaries between countries and languages. This comprises a promising arena where LLMs can demonstrate their capabilities across various contexts. Lastly, when applied to standardized tasks in the applied social sciences and the science discipline (the lower right quadrant), LLMs can achieve satisfactory performance after rigorous fine-tuning.  Observation 2: The Geopolitics of the Generative AI Figure 4(a) presents the country of origin of the ground truth benchmark. A lion’s share of validation datasets originates from English-speaking and European countries. In \n 10 other words, when scholars claim that LLMs can generate human-like responses, they are mostly referring to populations from WEIRD countries.  The same holds true for the human benchmark. Figure 4(b) demonstrates a considerably unbalanced and centralized geographical distribution of country of origin of human annotators who serve as the benchmarks. The United States takes the lead prominently, and other countries share the remaining divisions.  | Figure 4(a) and Figure 4(b) are here. |  Then, we grouped the countries into eight civilizations, as per Huntington’s typology. As illustrated in Figure 5, LLMs show comparable performance across both Western and non-Western contexts. In Western contexts and beyond, LLMs exhibit promising results more than 53% of the time. Despite being developed primarily in Western countries, LLMs do not possess an enhanced capacity to simulate Western populations according to WoS authors’ diagnoses.  | Figure 5 is here. | Last but not least, in terms of the in-group versus out-group comparison, we found that the researchers might hold LLMs to a higher standard and be more critical of their outputs when they are applied to issues within their own cultural context. For example, Western scholars evaluating LLMs in Western context might induce an inherent negative bias. To account for the potential bias, we conducted a Chi-square test to compare the valence distribution arising from in-group evaluation and out-group evaluation. Results displayed in Table 3 corroborate that the in-group evaluation is significantly more likely to result in negative or pessimistic conclusions. | Table 3 is here. | Recommendation 2: Improving the diversity of benchmark datasets \n 11 Our review reveals geographical imbalance in benchmarking, i.e., the overrepresentation of specific countries where pre-existing benchmark datasets are more available and human validation tasks are more easily standardized, neglecting the rich diversity of cultures and contexts worldwide. Such pattern well echoes a classic framework, the world system theory (Chang, et al., 2009; Chase-Dunn & Grimes, 1995; Wallerstein, 1979) in describing the dominating power of countries with the greatest political, economic, and cultural impacts. These countries have accumulated a wealth of cultural capital and nurtured a more advanced higher education. They are affluent of clearly labeled benchmark datasets as well as well-trained human annotators, which make them an expedient for benchmarking the emerging technology of LLMs.  Our review also suggests future research involving the human factors in the LLMs may consider the structural factors, such as the media system, political system, and the cultural values. We advocate for the establishment of a more balanced, inclusive, and globally representative benchmarking process.   Admittedly, the current review is an initial exploration based on the published studies. In the next step, we will extend our search to more databases (even science and engineering databases such as ACM and IEEE). We will also perform the analysis with a higher resolution to synergize the detailed findings (such as the actual model performances) for more actionable recommendations.    \n 12 References Argyle, L.P. et al. (2023). Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis, 1–15. Available at: https://doi.org/10.1017/pan.2023.2. Baptista, G., & Oliveira, T. (2015). Understanding mobile banking: The unified theory of acceptance and use of technology combined with cultural moderators. Computers in Human Behavior, 50, 418-430.  Benjamin, W. (1936). The Work of Art in the Age of Mechanical Reproduction. Online fulltext maintained by MIT: https://web.mit.edu/allanmc/www/benjamin.pdf  Chang, T.-K., Himelboim, I., & Dong, D. (2009). Open global networks, closed international flows: World system and political economy of hyperlinks in cyberspace. International Communication Gazette, 71(3), 137-159. Elyoseph, Z., & Levkovich, I. (2023). Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment. Frontiers in psychiatry, 14.  Gil de Zúñiga, H., Goyanes, M., & Durotoye, T. (2023). A scholarly definition of artificial intelligence (AI): advancing AI as a conceptual framework in communication research. Political Communication, 1-18. Guzman, A. L., & Lewis, S. C. (2020). Artificial intelligence and communication: A human–machine communication research agenda. New media & society, 22(1), 70-86.  Hanitzsch, T., Hanusch, F., Mellado, C., Anikina, M., Berganza, R., Cangoz, I., ... & Kee Wang Yuen, E. (2011). Mapping journalism cultures across nations: A comparative study of 18 countries. Journalism studies, 12(3), 273-293. Hommel, B. E. (2023). Expanding the methodological toolbox: Machine-based item desirability ratings as an alternative to human-based ratings. Personality and Individual Differences, 213, 112307. \n 13 Huang, A. H., Wang, H., & Yang, Y . (2023). FinBERT: A large language model for extracting information from financial text. Contemporary Accounting Research, 40(2), 806-841.  Koch, B., Denton, E., Hanna, A., & Foster, J. G. (2021). Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research. arXiv preprint arXiv:2112.01716.  Lane, D. S., Overbye-Thompson, H., & Gagrčin, E. (2024). The story of social media: evolving news coverage of social media in American politics, 2006–2021. Journal of Computer-Mediated Communication, 29(1), zmad039.  Leippold, M. (2023). Thus spoke GPT-3: Interviewing a large-language model on climate finance. Finance Research Letters, 53, 103617. Ritzer, G. (2000). The McDonaldization of society (New Century ed.). Thousand Oaks, CA: Pine Forge.  Shin, D., Chotiyaputta, V ., & Zaid, B. (2022). The effects of cultural dimensions on algorithmic news: How do cultural value orientations affect how people perceive algorithms?. Computers in Human Behavior, 126, 107007.  Wallerstein, I. (1974) The Modern World System. New York: Academic Press. Wing, J. M. (2019). The data life cycle. Harvard Data Science Review, 1(1), 6. URL: https://assets.pubpub.org/hzllsv80/e3005d85-fea9-4b60-8ba0-f5409ca0905e.pdf  Yan, D. (2023). Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation. Education and Information Technologies, 1-25. Zakour, Amel B., \"Cultural Differences and Information Technology Acceptance\" (2004). SAIS 2004 Proceedings. 26. URL: https://aisel.aisnet.org/sais2004/26 Zhang, X., Zhu, R., Chen, L., Zhang, Z., & Chen, M. (2022). News from Messenger? A Cross-National Comparative Study of News Media's Audience Engagement Strategies via Facebook Messenger Chatbots. Digital Journalism, 1-20.  \n 14 Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can Large Language Models Transform Computational Social Science?. arXiv preprint arXiv:2305.03514.     \n 15 Table 1.  Task type (of data generation) of LLMs for social science research    Classification Prediction Content Production  Output type Categorical variable Categorical or continuous variable Any type (text, number, audio, image) Options Predefined Predefined  or Fixed range No Traditional Counterpart Manual coding Closed-ended survey Open-ended survey or interview Example Sentiment analysis  Survey response Prediction Syllabus generation, translation    \n 16 Table 2.  The revised task type of LLMs for humanities and social science research and communication practice    Opinion-based  Facts-based  The “Aura”  (highly contextualized) Ethics, communication, linguistics, psychiatry Communication (journalism) \nThe McDonaldization (subject to mass production,  standardization) Education Business, green science, information science, computer science \n    \n 17 Table 3.  Valence by Validation Across Different Cultures   Valence Negative / Pessimistic Neutral / Mixed Positive / Optimistic Total In-group Count  2 (-1.10) 9 (-2.54) 32 (3.07) 43  % within row 4.65 % 20.93 % 74.42 % 100 %  Out-group Count  9 (1.10) 38 (2.54) 40 (-3.07) 87  % within row 10.35 % 43.68 % 45.98 % 100 %  Total Count 11 47 72 130  % within row 8.46 % 36.15 % 55.39 % 100 %     \n 18  \n Figure 1. Distribution of the valance as interpreted by the researchers (n = 72)     \n4158%1623%\n1319%\nPositiveNeutral/MixedNegative\n 19  \n Figure 2. Scatter plot of involved disciplines. X axis represents the average positivity towards LLMs expressed by research articles within a given discipline. Y axis represents the number of research articles published within a given discipline.       \n\n 20  \n Figure 3(a). Distribution of task types (n = 72)     \n\n 21  \n Figure 3(b). Valence (percentages within the bar) by different task types (n = 72)      \n\n 22  \n Figure 4(a). Geographical distribution of ground-truth benchmarks. For articles that involves cross-national validation, countries/regions are weighted by the proportions of ground truth they contribute. For instance, if article A involves two human judges from US and UK, these two countries will be assigned with an equal weight of 0.5. Only countries whose aggregate weights exceed 1 are displayed.   \n Figure 4(b). Geographical distribution of human benchmarks.    \n0\n5\n10\n15\n20\n25\nUSUKCanadaChinaGermanySwitzerlandTaiwanNew ZealandIsraelJapanAustraliaBrazilSwedenTurkeyBelgiumSpainAustria\n0123456789\nUSGermanyChinaSwedenJapanFranceIsraelGreece\n 23 \n Figure 5. Civilization of validation sources by valence        - The end of the main text –    \n\n 24 References References Argyle, L.P. et al. (2023). Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis, 1–15. Available at: https://doi.org/10.1017/pan.2023.2. Baptista, G., & Oliveira, T. (2015). Understanding mobile banking: The unified theory of acceptance and use of technology combined with cultural moderators. Computers in Human Behavior, 50, 418-430.  Benjamin, W. (1936). The Work of Art in the Age of Mechanical Reproduction. Online fulltext maintained by MIT: https://web.mit.edu/allanmc/www/benjamin.pdf  Chang, T.-K., Himelboim, I., & Dong, D. (2009). Open global networks, closed international flows: World system and political economy of hyperlinks in cyberspace. International Communication Gazette, 71(3), 137-159. Elyoseph, Z., & Levkovich, I. (2023). Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment. Frontiers in psychiatry, 14.  Gil de Zúñiga, H., Goyanes, M., & Durotoye, T. (2023). A scholarly definition of artificial intelligence (AI): advancing AI as a conceptual framework in communication research. Political Communication, 1-18. Guzman, A. L., & Lewis, S. C. (2020). Artificial intelligence and communication: A human–machine communication research agenda. New media & society, 22(1), 70-86.  Hanitzsch, T., Hanusch, F., Mellado, C., Anikina, M., Berganza, R., Cangoz, I., ... & Kee Wang Yuen, E. (2011). Mapping journalism cultures across nations: A comparative study of 18 countries. Journalism studies, 12(3), 273-293. Hommel, B. E. (2023). Expanding the methodological toolbox: Machine-based item desirability ratings as an alternative to human-based ratings. Personality and Individual Differences, 213, 112307. \n 25 Huang, A. H., Wang, H., & Yang, Y . (2023). FinBERT: A large language model for extracting information from financial text. Contemporary Accounting Research, 40(2), 806-841.  Koch, B., Denton, E., Hanna, A., & Foster, J. G. (2021). Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research. arXiv preprint arXiv:2112.01716.  Lane, D. S., Overbye-Thompson, H., & Gagrčin, E. (2024). The story of social media: evolving news coverage of social media in American politics, 2006–2021. Journal of Computer-Mediated Communication, 29(1), zmad039.  Leippold, M. (2023). Thus spoke GPT-3: Interviewing a large-language model on climate finance. Finance Research Letters, 53, 103617. Ritzer, G. (2000). The McDonaldization of society (New Century ed.). Thousand Oaks, CA: Pine Forge.  Shin, D., Chotiyaputta, V ., & Zaid, B. (2022). The effects of cultural dimensions on algorithmic news: How do cultural value orientations affect how people perceive algorithms?. Computers in Human Behavior, 126, 107007.  Wallerstein, I. (1974) The Modern World System. New York: Academic Press. Wing, J. M. (2019). The data life cycle. Harvard Data Science Review, 1(1), 6. URL: https://assets.pubpub.org/hzllsv80/e3005d85-fea9-4b60-8ba0-f5409ca0905e.pdf  Yan, D. (2023). Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation. Education and Information Technologies, 1-25. Zakour, Amel B., \"Cultural Differences and Information Technology Acceptance\" (2004). SAIS 2004 Proceedings. 26. URL: https://aisel.aisnet.org/sais2004/26 Zhang, X., Zhu, R., Chen, L., Zhang, Z., & Chen, M. (2022). News from Messenger? A Cross-National Comparative Study of News Media's Audience Engagement Strategies via Facebook Messenger Chatbots. Digital Journalism, 1-20.  \n 26 Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can Large Language Models Transform Computational Social Science?. arXiv preprint arXiv:2305.03514.   \n 27  Supplementary Information for the Pre-Print Manuscript:  Under what circumstances do Large Language Models (LLMs) outperform humans, and who judges? A systematic review of task types and benchmarking criteria in social science literature  Xinzhi Zhang, Yuner Zhu     Version: 30-Jan-2024 (version 3 in the OSF pre-print repository)   Contents  1. The PRISMA flow diagram  2. Coding scheme  3. A list of the reviewed publications    \n 28 1. The PRISMA flow diagram  \n Figure 1A. The PRISMA flow diagram indicating the screening process. The keywords include: \"large language model*\" OR \"LLM*\" OR \"*GPT*\" in the “Topic” (title and abstract, and keywords) of the WoS-SSCI database. The initial search obtained 722 articles. We removed non-empirical articles, limited the publication date in 2022 and 2023 (since GPT was released and gained societal attention), and removed irrelevant articles.     \n\n 29  2. Coding Scheme Relevance  In this study, we will systematically review existing literature on large language models (LLMs) and annotate their characteristics in the following six aspects: 1. Relevance: relevant = 1; irrelevant = 0 Code 0 (irrelevant) if the paper:  a. does not involve any empirical investigation, like review or position papers; or b. does not incorporate human factors, such as STEM papers that fully focus on non-human processes; or  c. mismatches with the search query, such as Glutamic-pyruvic transaminase (abbr. GPT).  For all other papers, please assign a code of 1 (relevant).  Continue to code the remaining columns only if the paper is relevant to our investigation.  Valence We coded the valence explicitly argued by the researchers when they subjectively review their results. We took the overall stance of the paper so each article would only have one single value (positive, negative, or neutral). An example of positive interpretation is (we underscored the positive framing section):  “These results demonstrate the feasibility of relying on machine-based item desirability ratings as a viable alternative to human-based ratings and contribute to the field of personality psychology by expanding the methodological toolbox available to researchers, scale developers, and practitioners” (Hommel, 2023, p. 213).   Alternatively, an example of negative interpretation is like (we underscored the negative framing section):  “These results imply that gatekeepers, patients or even mental health professionals who rely on ChatGPT for evaluating suicidal risk or as a complementary tool to improve decision-making may receive an inaccurate assessment that underestimates the actual suicide risk” (Elyoseph & Levkovich, 2023, p. 14).  Task Types Choose the types of research tasks achieved by LLMs in the focal paper from: classification, prediction, and generation. Below is a brief explanation of each: a. Classification: Classification involves assigning pre-defined categories to input data based on the patterns that can be directly captured by LLM without extrapolation. For example, the LLM model may classify news articles into pre-defined topic groups or classify social media posts into pre-defined sentiment categories. Topics and sentiments are the latent features that are endogenous to the input. They can be determined by observing and summarizing the patterns of word usage within the focal texts. Prior to the invent of LLM, this type of work is typically done by manual coding in communication studies. • Requisites: 1) Model outputs are pre-defined categories (categorical variable); 2) Outputs are latent features endogenous to and homologous with inputs, e.g. linguistic features predict linguistic outputs. \n 30 • Equivalent in the pre-LLM era: Manual Coding • Example: Using apple to predict banana. Topic Classification, Sentiment Analysis, Frame Analysis b. Prediction: Prediction involves estimating or forecasting a specific outcome that is exogenous to and non-homologous with input data. It is methodologically related to statistical extrapolation. The outputs can be of various forms, e.g. categorical labels or continuous numbers. Regardless of the form, however, they should fall within a fixed range of values that are predefined by scholars. Prior to the invent of LLM, this type of work is typically done by closed-ended survey in communication studies. • Requisites: 1) Model outputs should fall within a pre-defined fixed range of values; 2) Outputs are exogenous to and non-homologous with inputs, e.g. using one’s languages to predict his/her vote or using stock market trends to predict election results. • Equivalent in the pre-LLM era: Closed-Ended Survey • Example: Using apple to predict weather. V ote Prediction, Survey Response Prediction, IQ Test c. Generation: Generation involves creating new content, such as text, images, or audio, based on a given prompt. It is the most creative type of tasks. Generation can result in an arbitrary number of outputs in arbitrary forms. Outputs can be either exogenous or endogenous to inputs. Prior to the invent of LLM, this type of work is typically done by interview or open-ended survey in communication studies. • Requisites: 1) Model outputs are not subject to a fixed range of answers. • Equivalent in the pre-LLM era: Open-Ended Survey • Example: Using apple to make an apple pie. Syllabus Generation, Text Generation, Translation. Ground Truth & Validation Source What types of data are used as the ground truth to validate the LLM’s performance? From which humans do they draw response to form a benchmark dataset to evaluate the relative strength of LLM’s performance? Code 0 if the authors do not validate nor evaluate model outputs at all. Otherwise, please specify the type of data/source that is drawn on to validate or evaluate model outputs and also the geolocation of data/source. If the authors implicitly that “postgraduate students are recruited” or “two psychologists are enlisted,” we assume they are recruiting students or psychologists from the corresponding author’s country.  Human Benchmark If authors draw comparisons between the performance of LLM and that of human, we should also record the identities and geolocation of human benchmark. Code 0 if the authors do not conduct such comparison at all.   \n 31 3. A list of the studies reviewed in the present study   Authors Title Publication title  Subjects Leippold, M Thus spoke GPT-3: Interviewing a large-language model on climate finance FINANCE RESEARCH LETTERS Business Webb, T; Holyoak, KJ; Lu, HJ Emergent analogical reasoning in large language models NATURE HUMAN BEHAVIOUR Psychology Trott, S; Jones, C; Chang, T; Michaelov, J; Bergen, B Do Large Language Models Know What Humans Know? COGNITIVE SCIENCE Psychology \nSchwitzgebel, E; Schwitzgebel, D; Strasser, A Creating a large language model of a philosopher MIND & LANGUAGE Linguistics \nLiu, SR; Wright, AP; Patterson, BL; Wanderer, JP; Turer, RW; Nelson, SD; McCoy, AB; Sittig, DF; Wright, A \nUsing AI-generated suggestions from ChatGPT to optimize clinical decision support JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION \nComputer Science \nHuang, AH; Wang, H; Yang, Y FinBERT: A Large Language Model for Extracting Information from Financial Text CONTEMPORARY ACCOUNTING RESEARCH Business \nKumah-Crystal, Y; Mankowitz, S; Embi, P; Lehmann, CU ChatGPT and the clinical informatics board examination: the end of unproctored maintenance of certification? JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION \nComputer Science \nKhan, S; Fazil, M; Imoize, AL; Alabduallah, BI; Albahlal, BM; Alajlan, SA; Almjally, A; Siddiqui, T \nTransformer Architecture-Based Transfer Learning for Politeness Prediction in Conversation SUSTAINABILITY Green & Sustainable Science & Technology \nHommel, BE Expanding the methodological toolbox: Machine-based item desirability ratings as an alternative to human-based ratings PERSONALITY AND INDIVIDUAL DIFFERENCES Psychology \nMcGowan, A; Gui, YL; Dobbs, M; Shuster, S; Cotter, M; Selloni, A; Goodman, M; Srivastava, A; Cecchi, GA; Corcoran, CM \nChatGPT and Bard exhibit spontaneous citation fabrication during psychiatry literature search PSYCHIATRY RESEARCH Psychiatry \nMann, SP; Earp, BD; Moller, N; Vynn, S; Savulescu, J AUTOGEN: A Personalized Large Language Model for Academic Enhancement-Ethics and Proof of Principle AMERICAN JOURNAL OF BIOETHICS Ethics \nFurukawa, TA; Iwata, S; Horikoshi, M; Sakata, M; Toyomoto, R; Luo, Y; Tajika, A; Kudo, N; Aramaki, E \nHarnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study COGNITIVE THERAPY AND RESEARCH Psychology \nMotoki, F; Neto, VP; Rodrigues, V More human than human: measuring ChatGPT political bias PUBLIC CHOICE Political Science Wu, TT; Lee, HY; Li, PH; Huang, CN; Huang, YM Promoting Self-Regulation Progress and Knowledge Construction in Blended Learning via ChatGPT-Based Learning Aid JOURNAL OF EDUCATIONAL COMPUTING RESEARCH Education & Educational Research Smith, A; Hachen, S; Schleifer, R; Bhugra, D; Buadze, A; Liebrenz, M \nOld dog, new tricks? Exploring the potential functionalities of ChatGPT in supporting educational methods in social psychiatry INTERNATIONAL JOURNAL OF SOCIAL PSYCHIATRY Psychiatry \nda Silva, JAT; Tsigaris, P Human- and AI-based authorship: Principles and ethics LEARNED PUBLISHING Information Science & Library Science Dziri, N; Rashkin, H; Linzen, T; Reitter, D Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark TRANSACTIONS OF THE ASSOCIATION FOR Computer Science \n 32 COMPUTATIONAL LINGUISTICS Mayer, CWF; Ludwig, S; Brandt, S Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models \nJOURNAL OF RESEARCH ON TECHNOLOGY IN EDUCATION \nEducation & Educational Research Lau, C; Zhu, XD; Chan, WY Automatic depression severity assessment with deep learning using parameter-efficient tuning FRONTIERS IN PSYCHIATRY Psychiatry \nGrinbaum, A; Adomaitis, L Moral Equivalence in the Metaverse NANOETHICS Ethics Chatterjee, P; Mishra, H; Mishra, A Does the First Letter of One's Name Affect Life Decisions? A Natural Language Processing Examination of Nominative Determinism \nJOURNAL OF PERSONALITY AND SOCIAL PSYCHOLOGY Psychology \nFriederichs, H; Friederichs, WJ; Marz, M ChatGPT in medical school: how successful is AI in progress testing? MEDICAL EDUCATION ONLINE Education & Educational Research Kim, J; Kim, JH; Kim, C; Park, J Decisions with ChatGPT: Reexamining choice overload in ChatGPT recommendations JOURNAL OF RETAILING AND CONSUMER SERVICES Business \nWei, T; Wu, H; Chu, G Is ChatGPT competent? Heterogeneity in the cognitive schemas of financial auditors and robots INTERNATIONAL REVIEW OF ECONOMICS & FINANCE \nBusiness \nElyoseph, Z; Levkovich, I Beyond human expertise: the promise and limitations of ChatGPT in suicide risk assessment FRONTIERS IN PSYCHIATRY Psychiatry \nHowell, BE; Potgieter, PH What do telecommunications policy academics have to fear from GPT-3? TELECOMMUNICATIONS POLICY Communication Koc, E; Hatipoglu, S; Kivrak, O; Celik, C; Koc, K Houston, we have a problem!: The use of ChatGPT in responding to customer complaints TECHNOLOGY IN SOCIETY Social Issues \nRoumeliotis, KI; Tselikas, ND; Nasiopoulos, DK Unveiling Sustainability in Ecommerce: GPT-Powered Software for Identifying Sustainable Product Features SUSTAINABILITY Green & Sustainable Science & Technology Farazouli, A; Cerratto-Pargman, T; Bolander-Laksov, K; McGrath, C Hello GPT! Goodbye home examination? An exploratory study of AI chatbots impact on university teachers' assessment practices ASSESSMENT & EVALUATION IN HIGHER EDUCATION Education & Educational Research Sajja, R; Sermet, Y; Cwiertny, D; Demir, I Platform-independent and curriculum-oriented intelligent assistant for higher education INTERNATIONAL JOURNAL OF EDUCATIONAL TECHNOLOGY IN HIGHER EDUCATION \nEducation & Educational Research \nAmin, S; Kawamoto, CT; Pokhrel, P Exploring the ChatGPT platform with scenario-specific prompts for vaping cessation TOBACCO CONTROL Public, Environmental & Occupational Health Lopes, APL Artificial history? Inquiring ChatGPT on historiography RETHINKING HISTORY History Victor, BG; McNally, K; Qi, Z; Perron, BE Construct-Irrelevant Variance on the ASWB Clinical Social Work Licensing Exam: A Replication of Prior Validity Concerns RESEARCH ON SOCIAL WORK PRACTICE Social Work \nMohammed, M; Kumar, N; Zawiah, M; Al-Ashwal, FY; Bala, AA; Lawal, BK; Wada, AS; Halboup, A; Muhammad, S; Ahmad, R; Sha'aban, A \nPsychometric Properties and Assessment of Knowledge, Attitude, and Practice Towards ChatGPT in Pharmacy Practice and Education: a Study Protocol \nJOURNAL OF RACIAL AND ETHNIC HEALTH DISPARITIES Public, Environmental & Occupational Health \nKreps, S; Jakesch, M Can AI communication tools increase legislative responsiveness and trust in democratic institutions? GOVERNMENT INFORMATION QUARTERLY Information Science & Library Science \n 33 Kikalishvili, S Unlocking the potential of GPT-3 in education: opportunities, limitations, and recommendations for effective integration INTERACTIVE LEARNING ENVIRONMENTS Education & Educational Research Zhu, ZL; Ying, YQ; Zhu, JM; Wu, HM ChatGPT's potential role in non-English-speaking outpatient clinic settings DIGITAL HEALTH Public, Environmental & Occupational Health Dalalah, D; Dalalah, OMA The false positives and false negatives of generative AI detection tools in education and academic research: The case of ChatGPT \nINTERNATIONAL JOURNAL OF MANAGEMENT EDUCATION \nBusiness \nElyoseph, Z; Hadar-Shoval, D; Asraf, K; Lvovsky, M ChatGPT outperforms humans in emotional awareness evaluations FRONTIERS IN PSYCHOLOGY Psychology \nSpreafico, C; Sutrisno, A Artificial Intelligence Assisted Social Failure Mode and Effect Analysis (FMEA) for Sustainable Product Design SUSTAINABILITY Green & Sustainable Science & Technology Alvarez-alvarez, C; Falcon, S Students' preferences with university teaching practices: analysis of testimonials with artificial intelligence ETR&D-EDUCATIONAL TECHNOLOGY RESEARCH AND DEVELOPMENT \nEducation & Educational Research Van Bulck, L; Moons, P What if your patient switches from Dr. Google to Dr. ChatGPT? A vignette-based survey of the trustworthiness, value, and danger of ChatGPT-generated responses to health questions \nEUROPEAN JOURNAL OF CARDIOVASCULAR NURSING Nursing \nYan, D Impact of ChatGPT on learners in a L2 writing practicum: An exploratory investigation EDUCATION AND INFORMATION TECHNOLOGIES Education & Educational Research Oh, BD; Schuler, W Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times? \nTRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS \nComputer Science \nVictor, BG; Kubiak, S; Angell, B; Perron, BE Time to Move Beyond the ASWB Licensing Exams: Can Generative Artificial Intelligence Offer a Way Forward for Social Work? \nRESEARCH ON SOCIAL WORK PRACTICE Social Work \nDay, T A Preliminary Investigation of Fake Peer-Reviewed Citations and References Generated by ChatGPT PROFESSIONAL GEOGRAPHER Geography \nKreps, S; Kriner, DL The potential impact of emerging technologies on democratic representation: Evidence from a field experiment NEW MEDIA & SOCIETY Communication Dowling, M; Lucey, B ChatGPT for (Finance) research: The Bananarama Conjecture FINANCE RESEARCH LETTERS Business Haluza, D; Jungwirth, D Artificial Intelligence and Ten Societal Megatrends: An Exploratory Study Using GPT-3 SYSTEMS Social Sciences \nKang, KE; Lee, K Collaboration and Public Participation for Municipal Growth in Land Economic Development Projects ECONOMIC DEVELOPMENT QUARTERLY Economics \nArgyle, LP; Busby, EC; Fulda, N; Gubler, JR; Rytting, C; Wingate, D \nOut of One, Many: Using Language Models to Simulate Human Samples POLITICAL ANALYSIS Political Science \nGotz, F; Maertens, R; Loomba, S; van der Linden, S Let the Algorithm Speak: How to Use Neural Networks for Automatic Item Generation in Psychological Scale Development \nPSYCHOLOGICAL METHODS Psychology \nHenrickson, L Chatting with the dead: the hermeneutics of thanabots MEDIA CULTURE & SOCIETY Communication Lee, PL; Fyffe, S; Son, M; Jia, ZH; Yao, ZY A Paradigm Shift from Human Writing to Machine Generation in Personality Test JOURNAL OF BUSINESS AND PSYCHOLOGY Business \n 34 Development: an Application of State-of-the-Art Natural Language Processing Querubin, NS; Niederer, S Climate futures: Machine learning from cli-fi CONVERGENCE-THE INTERNATIONAL JOURNAL OF RESEARCH INTO NEW MEDIA TECHNOLOGIES \nCommunication \nHernandez, I; Nie, WW The AI-IP: Minimizing the guesswork of personality scale item development through artificial intelligence PERSONNEL PSYCHOLOGY Psychology \nKomatsu, H; Maeno, A; Watanabe, E Origin of the ease of association of color names: Comparison between humans and AI I-PERCEPTION Psychology Szewczyk, JM; Federmeier, KD Context-based facilitation of semantic access follows both logarithmic and linear functions of stimulus probability JOURNAL OF MEMORY AND LANGUAGE Linguistics \nPerrotta, C; Selwyn, N; Ewin, C Artificial intelligence and the affective labour of understanding: The intimate moderation of a language model NEW MEDIA & SOCIETY Communication Chen, P; Poeppel, D; Zuanazzi, A Meaning creation in novel noun-noun compounds: humans and language models LANGUAGE COGNITION AND NEUROSCIENCE Linguistics Markowitz, DM; Hancock, JT; Bailenson, JN Linguistic Markers of Inherently False AI Communication and Intentionally False Human Communication: Evidence From Hotel Reviews \nJOURNAL OF LANGUAGE AND SOCIAL PSYCHOLOGY Communication \nSareen, K Assessing the ethical capabilities of Chat GPT in healthcare: A study on its proficiency in situational judgement test INNOVATIONS IN EDUCATION AND TEACHING INTERNATIONAL \nEducation & Educational Research Guo, K; Wang, DL To resist it or to embrace it? Examining ChatGPT's potential to support teacher feedback in EFL writing EDUCATION AND INFORMATION TECHNOLOGIES Education & Educational Research Chow, JCL; Wong, V; Sanders, L; Li, K Developing an AI-Assisted Educational Chatbot for Radiotherapy Using the IBM Watson Assistant Platform HEALTHCARE Health Care Sciences & Services Chubb, LA Me and the Machines: Possibilities and Pitfalls of Using Artificial Intelligence for Qualitative Data Analysis INTERNATIONAL JOURNAL OF QUALITATIVE METHODS \nSocial Sciences \nHamilton, L; Elliott, D; Quick, A; Smith, S; Choplin, V Exploring the Use of AI in Qualitative Analysis: A Comparative Study of Guaranteed Income Data INTERNATIONAL JOURNAL OF QUALITATIVE METHODS \nSocial Sciences \nSong, BW; Luan, CJ; Liang, DN Identification of emerging technology topics (ETTs) using BERT-based model and sematic analysis: a perspective of multiple-field characteristics of patented inventions (MFCOPIs) \nSCIENTOMETRICS Computer Science \nOrganisciak, P; Acar, S; Dumas, D; Berthiaume, K Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models THINKING SKILLS AND CREATIVITY Education & Educational Research Hsu, MH Mastering medical terminology with ChatGPT and Termbot HEALTH EDUCATION JOURNAL Education & Educational Research Branum, C; Schiavenato, M Can ChatGPT Accurately Answer a PICOT Question? Assessing AI Response to a Clinical Question NURSE EDUCATOR Nursing \nChang, TA; Bergen, BK Word Acquisition in Neural Language Models TRANSACTIONS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS \nComputer Science \n  Updated: 30-Jan-2024   ",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.5430940389633179
    },
    {
      "name": "Geopolitics",
      "score": 0.5216444134712219
    },
    {
      "name": "Psychology",
      "score": 0.3830806314945221
    },
    {
      "name": "Political science",
      "score": 0.37686073780059814
    },
    {
      "name": "Social science",
      "score": 0.36762118339538574
    },
    {
      "name": "Social psychology",
      "score": 0.34893912076950073
    },
    {
      "name": "Sociology",
      "score": 0.28236716985702515
    },
    {
      "name": "Politics",
      "score": 0.2346743941307068
    },
    {
      "name": "Law",
      "score": 0.18101564049720764
    },
    {
      "name": "Economics",
      "score": 0.17649909853935242
    },
    {
      "name": "Management",
      "score": 0.13244342803955078
    }
  ]
}