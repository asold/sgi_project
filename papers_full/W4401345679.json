{
    "title": "A comparison of the diagnostic ability of large language models in challenging clinical cases",
    "url": "https://openalex.org/W4401345679",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2319577608",
            "name": "Maria Palwasha Khan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5106321095",
            "name": "Eoin Daniel O’Sullivan",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A2319577608",
            "name": "Maria Palwasha Khan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5106321095",
            "name": "Eoin Daniel O’Sullivan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4378464713",
        "https://openalex.org/W4377013694",
        "https://openalex.org/W4385900159",
        "https://openalex.org/W4388823522",
        "https://openalex.org/W4399489002",
        "https://openalex.org/W4385346108",
        "https://openalex.org/W4365458667",
        "https://openalex.org/W4386033569",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4393128858",
        "https://openalex.org/W4396732548",
        "https://openalex.org/W4386110374"
    ],
    "abstract": "Introduction The rise of accessible, consumer facing large language models (LLM) provides an opportunity for immediate diagnostic support for clinicians. Objectives To compare the different performance characteristics of common LLMS utility in solving complex clinical cases and assess the utility of a novel tool to grade LLM output. Methods Using a newly developed rubric to assess the models’ diagnostic utility, we measured to models’ ability to answer cases according to accuracy, readability, clinical interpretability, and an assessment of safety. Here we present a comparative analysis of three LLM models—Bing, Chat GPT, and Gemini—across a diverse set of clinical cases as presented in the New England Journal of Medicines case series. Results Our results suggest that models performed differently when presented with identical clinical information, with Gemini performing best. Our grading tool had low interobserver variability and proved a reliable tool to grade LLM clinical output. Conclusion This research underscores the variation in model performance in clinical scenarios and highlights the importance of considering diagnostic model performance in diverse clinical scenarios prior to deployment. Furthermore, we provide a new tool to assess LLM output.",
    "full_text": null
}