{
  "title": "B2T Connection: Serving Stability and Performance in Deep Transformers",
  "url": "https://openalex.org/W4385569967",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5075222372",
      "name": "Sho Takase",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5066002127",
      "name": "Shun Kiyono",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5112876488",
      "name": "Sosuke Kobayashi",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5002182453",
      "name": "Jun Suzuki",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W4226102207",
    "https://openalex.org/W2941599692",
    "https://openalex.org/W2962931466",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2950621961",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W3118578889",
    "https://openalex.org/W4288621368",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3167739156",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W3176948526",
    "https://openalex.org/W2970290486"
  ],
  "abstract": "In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.Recent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, resulting in useless models.However, in contrast, Post-LN has also consistently achieved better performance than Pre-LN in relatively shallow Transformers, e.g., six or fewer layers.This study first investigates the reason for these discrepant observations empirically and theoretically and discovers 1, the LN in Post-LN is the source of the vanishing gradient problem that mainly leads the unstable training whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation that may lead an effective training.Exploiting the new findings, we propose a method that can equip both higher stability and effective training by a simple modification from Post-LN.We conduct experiments on a wide range of text generation tasks and demonstrate that our method outperforms Pre-LN, and stable training regardless of the shallow or deep layer settings.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3078–3095\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nB2T Connection: Serving Stability and Performance in Deep Transformers\nSho Takase†∗ Shun Kiyono† Sosuke Kobayashi‡ Jun Suzuki‡\n†LINE Corporation ‡Tohoku University\n{sho.takase, shun.kiyono}@linecorp.com\nsosk@preferred.jp\njun.suzuki@tohoku.ac.jp\nAbstract\nFrom the perspective of the layer normalization\n(LN) positions, the architectures of Transform-\ners can be categorized into two types: Post-LN\nand Pre-LN. Recent Transformers tend to be\nPre-LN because, in Post-LN with deep Trans-\nformers (e.g., those with ten or more layers),\nthe training is often unstable, resulting in use-\nless models. However, Post-LN has consis-\ntently achieved better performance than Pre-LN\nin relatively shallow Transformers (e.g., those\nwith six or fewer layers). This study first inves-\ntigates the reason for these discrepant observa-\ntions empirically and theoretically and made\nthe following discoveries: 1, the LN in Post-\nLN is the main source of the vanishing gradient\nproblem that leads to unstable training, whereas\nPre-LN prevents it, and 2, Post-LN tends to pre-\nserve larger gradient norms in higher layers dur-\ning the back-propagation, which may lead to\neffective training. Exploiting the new findings,\nwe propose a method that can provide both\nhigh stability and effective training by a simple\nmodification of Post-LN. We conduct experi-\nments on a wide range of text generation tasks.\nThe experimental results demonstrate that our\nmethod outperforms Pre-LN, and enables sta-\nble training regardless of the shallow or deep\nlayer settings. Our code is publicly available at\nhttps://github.com/takase/b2t_connection.\n1 Introduction\nTo prevent the vanishing (or exploding) gradient\nproblem in the training of a deep neural network\n(DNN), various techniques, such as batch nor-\nmalization (Ioffe and Szegedy, 2015) and resid-\nual connection (Srivastava et al., 2015; He et al.,\n2016a), have been proposed and widely used in\nalmost all recent DNNs. Transformer (Vaswani\net al., 2017) employs the layer normalization (Ba\net al., 2016) for this purpose. Transformer is\ncurrently the most successful model architecture\n∗ A part of this work was done when the author was at\nTokyo Institute of Technology.\n15(a) Trainloss (b) Validloss\nFigure 1: Loss values of 18 layered Transformer-based\nencoder-decoder architectures.\nin DNNs. It was firstly developed for apply-\ning sequence-to-sequence tasks, such as machine\ntranslation (Vaswani et al., 2017), summariza-\ntion (Takase and Okazaki, 2019), and automatic\nspeech recognition (ASR) (Wang et al., 2020), and\nis currently used in speech, vision, and many other\ninformation processing research fields.\nAs reported in the batch normalization litera-\nture (He et al., 2016b), the position of the nor-\nmalization layers primarily affects both the stabil-\nity and resultant performance of a trained model.\nIn Transformers, some previous studies have in-\nvestigated the impact of the layer normalization\npositions (Wang et al., 2019; Xiong et al., 2020).\nThere are currently two major layer normalization\npositions in Transformers: Pre-Layer Normaliza-\ntion (Pre-LN) and Post-Layer Normalization (Post-\nLN). Pre-LN applies the layer normalization to an\ninput for each sub-layer, and Post-LN places the\nlayer normalization after each residual connection.\nThe original Transformer (Vaswani et al., 2017)\nemploys Post-LN. However, many recent studies\nhave suggested using Pre-LN (Wang et al., 2019;\nBaevski and Auli, 2019; Brown et al., 2020) be-\ncause the training of deep Transformers (e.g., those\nwith ten or more layers) using Post-LN is often\nunstable, resulting in useless models. Figure 1\n3078\nshows loss curves for an actual example; the train-\ning of 18 layered Transformer encoder-decoders\n(18L-18L) on a widely used WMT English-to-\nGerman machine translation dataset. These fig-\nures clearly show that the Post-LN Transformer\nencoder-decoders fail to train the model. However,\nin contrast, Liu et al. (2020) reported that Post-LN\nconsistently achieved better performance than Pre-\nLN on a machine translation task when they used 6\nlayered (relatively shallow, 6L-6L) Transformers.\nThis paper focuses specifically on such discrep-\nancies between Pre-LN and Post-LN in configura-\ntions with various number of layers. We investigate\nthe sources of the instability of training in deep con-\nfigurations and the superior performance in shallow\nconfigurations for Post-LN, compared with that for\nPre-LN, to understand the essentials of the differ-\nences between Pre-LN and Post-LN. We discover\nthat the layer normalization in Post-LN is the main\nsource of the vanishing gradient problem that leads\nto unstable training, whereas Pre-LN prevents it,\nas shown in Figure 1. In particular, we clarify\nthat the layer normalization is a significant factor\nof the vanishing gradient problem by comparing\nthe input/output vector norms of gradient flows for\neach layer normalization during back-propagation.\nThese analyses bring us a novel idea that can satisfy\nhigher stability by skipping over layer normaliza-\ntions and provide better performance than Pre-LN\nregardless of their layer sizes. Consequently, we\npropose a method that is based on Post-LN Trans-\nformers but has additional residual connections to\nenable stable training.\nWe conduct experiments on a wide range of text\ngeneration tasks, namely machine translation, sum-\nmarization, language modeling, and ASR. The ex-\nperimental results lead to the following three new\nmajor findings:\n1. Post-LN Transformers achieve better perfor-\nmance than Pre-LN Transformers on text\ngeneration tasks (not only machine transla-\ntion (Liu et al., 2020) but also other tasks).\nThus, Post-LN is superior to Pre-LN if the\nproblem of its unstable training can be solved.\n2. Our modification enables Post-LN Transform-\ners to stack many layers.\n3. Our method can maintain the performance ad-\nvantage of Post-LN and mitigate its unstable\ntraining property, thus providing better perfor-\nmance than Pre-LN.\n2 Post-LN and Pre-LN Transformers\nWe briefly describe Post-LN and Pre-LN Trans-\nformers in this section. The original Trans-\nformer (Vaswani et al., 2017) uses Post-LN, in\nwhich layer normalizations are located after each\nresidual connection. Let x be an input of a sub-\nlayer, and F(·) be a sub-layer of a Transformer,\nsuch as a feed-forward network or multi-head at-\ntention. Post-LN is defined as follows:\nPostLN(x) = LN(x+ F(x)), (1)\nwhere LN(·) is the layer normalization function.\nIn contrast, Pre-LN places the layer normaliza-\ntion before an input of each sub-layer:\nPreLN(x) =x+ F(LN(x)). (2)\nFigure 2 (a) and (b) illustrate Post-LN and Pre-LN\nTransformer architectures, respectively.\n3 Gradients of Transformer Layers\nAs described in Liu et al. (2020), the vanishing\ngradient problem often occurs in Post-LN Trans-\nformers. Figure 3 shows the gradient norms of\neach layer for the (a) encoder-side and (b) decoder-\nside at the beginning of training, when 18L-18L\nTransformer encoder-decoders are trained on a\nwidely used machine translation dataset (the WMT\nEnglish-to-German dataset). Focus on the decoder-\nside of Post-LN as illustrated in Figure 3 (b). This\nfigure shows that shallower layers have smaller gra-\ndient norms. In other words, the vanishing gradient\noccurs in the decoder-side of Post-LN because its\ngradient norms exponentially decay as they are\nback-propagated to shallower layers. This result is\nconsistent with the previous study (Liu et al., 2020).\nWe consider that this vanishing gradient causes the\ndifficulty of stacking many layers with the Post-LN\nsetting, as shown in Figure 1.\nTo investigate the vanishing gradient empirically\nin more detail, we measure the gradient norms of\nparts (1) - (5) of Figure 2 (a). Figure 4 shows\nthe gradient norms of each part in the 18th layer1.\nThis figure shows that the gradient norms decrease\ndrastically from (4) to (3) and (2) to (1). These\nparts correspond to layer normalizations, as shown\nin Figure 4. This suggests that layer normalizations\nin Post-LN Transformers are probably the cause of\nthe vanishing gradient problem.\n1Appendix B shows the gradient norms of each part in the\n1st and 9th decoders as additional examples.\n3079\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\n×N\n×N\nAttentionLayer Norm\nLayer Norm\nAttentionLayer Norm\nLayer NormFFN\nAttentionLayer Norm\n×N\n×N\nFFN\nLayer Norm\nLayer Norm\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\n×N\n×N\n(a) Post-LN (b) Pre-LN (c) Post-LN with B2T connection\n(1)\n(3)\n(2)\n(4)\n(5)\nFigure 2: Transformer-based encoder-decoder architectures for (a) Post-LN, (b) Pre-LN, and (c) Post-LN with our\nproposed B2T connection.\n13\n100\n101\n101\n10-1\n100\n(a) Encoder side(b) Decoder side\n100\n101\n101\n10-1\n100\nFigure 3: Gradient norms of 18 layered Transformer-\nbased encoder-decoder architectures.\nTo investigate the difference between the gradi-\nent flows of Post-LN and those of Pre-LN theoreti-\ncally, we calculate the derivatives of Equations (1)\nand (2), as follows:\n∂PostLN(x)\n∂x = ∂LN(x+ F(x))\n∂(x+ F(x))\n(\nI+ ∂F(x)\n∂x\n)\n,\n(3)\n∂PreLN(x)\n∂x = I+ ∂F(LN(x))\n∂LN(x)\n∂LN(x)\n∂x , (4)\nwhere I is the identity matrix. As Equation (3),\nthe derivative of Post-LN is equal to the product of\ntwo derivatives: one is the layer normalization, and\nthe other consists of the residual connection and\nsub-layer F. In contrast, in Pre-LN, the derivative\nof the residual connection is isolated from the term\nrelated to the derivative of the layer normalization.\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\nFigure 4: Gradient norms of each location in the 18th de-\ncoder for the 18 layered Post-LN Transformer encoder-\ndecoder on WMT English-to-German translation train-\ning data.\nThe difference between these equations implies\nthat the residual connection in Pre-LN prevents the\nvanishing gradient because it retains the gradients\nof upper layers even if the derivative of the layer\nnormalization decreases gradients drastically.\n4 Transformations by Each Layer\nAs described, it is difficult to stack many layers in\nPost-LN Transformers because the vanishing gra-\n3080\n10\n(a) Post-LN (b) Pre-LN (c) B2T connection\nEncoderDecoder\nFigure 5: Cosine similarities between the outputs of\neach pair of layers.\ndient problem occurs. Although Pre-LN is more\nstable in training, Post-LN can achieve better per-\nformance if training succeeds (see Section 6). In\nthis section, we explore the reason for this differ-\nence in performance.\nFocus Pre-LN in Figure 3. In contrast to Post-\nLN, in Pre-LN, a deeper (higher) layer has a smaller\ngradient norm. Thus, the parameters of higher lay-\ners are not required to change dramatically from\ntheir initial values. This implies that higher layers\nin Pre-LN are not sufficiently effective.\nTo investigate the effectiveness of higher lay-\ners, we focus on the transformations by each layer.\nFigure 5 shows the average cosine similarities be-\ntween the outputs of each pair of layers for 6L-\n6L Transformer encoder-decoders trained on the\nWMT dataset when several sequences are input.\nThis figure indicates that the lower-left similarities\nof Pre-LN are higher than those of Post-LN. This\nresult means that the outputs of shallow layers are\nsimilar to the output of the final layer in Pre-LN,\nbut not in Post-LN. Consequently, higher layers in\nPre-LN are less effective than those in Post-LN if\ntraining succeeds.\nWe consider that the residual connection in Pre-\nLN causes this phenomenon. As Equation (2)\nshows, in Pre-LN, an input xskips over the sub-\nlayer F(·) by the residual connection. Thus, the\ninput xis directly connected to the final layer out-\nput. This property makes the training stable, as de-\nscribed in Section 3, but causes high similarities be-\ntween the outputs of the various layers. Therefore,\nwe consider that Pre-LN underperforms Post-LN\nbecause the residual connection in Pre-LN reduces\nthe effectiveness of its higher layers. In contrast,\nin Post-LN, larger gradient norms in higher layers\n(as shown in Figure 3) make higher layers more ef-\nfective (as shown in Figure 5) but it is necessary to\nprevent the vanishing gradient problem in shallow\nlayers when we stack many layers.\n5 Modification for Stable Training in\nPost-LN: Bottom-to-Top Connection\nThis section introduces a modification that makes\nthe training of Post-LN more stable while preserv-\ning its high performance. This modification com-\nprises an additional residual connection to miti-\ngate the vanishing gradient in Post-LN by enabling\nmany layers to be stacked.\nAs discussed in the previous sections, we need\na term that retains gradients in the derivatives, as\nin Equation (4), to prevent the vanishing gradient.\nTo satisfy this requirement, we propose a residual\nconnection that skips over all layer normalizations\nexcept the final one in each layer. Our introduced\nconnection ties an input of a layer to the result of\nthe feed-forward network (FFN), as illustrated by\nthe red arrows in Figure 2 (c). We call this connec-\ntion Bottom-to-Top (B2T) connection, which is\nformalized in the following equation:\nxinp + xffn + FFN(xffn ), (5)\nwhere xinp is an input of a layer, FFN(·) is an\nFFN, and xffn is an input of the FFN. In short,\nxinp skips the layer normalizations after the self-\nattention and encoder-decoder cross-attention. Be-\ncause the derivative of xinp is isolated from the\nterms related to the derivatives of the layer nor-\nmalizations just behind the attention sub-layers, it\nretains gradients, as in Pre-LN. For example, in an\nencoder-side, xffn is as follows:\nxffn = LN(SelfAttn(xinp) +xinp), (6)\nwhere SelfAttn(·) is a self-attention network.\nThus, Equation (5) can be written as follows:\nxinp + LN(SelfAttn(xinp) +xinp)\n+ FFN(LN(SelfAttn(xinp) +xinp)), (7)\nThe derivative of this equation is the following\nequation:\nI+ ∂(LN(SelfAttn(xinp) +xinp))\n∂xinp\n+ ∂(FFN(LN(SelfAttn(xinp) +xinp)))\n∂xinp\n, (8)\n3081\nBecause this derivative contains I, which is unre-\nlated to the derivatives of internal layer normal-\nizations, our B2T connection (i.e., xinp) helps to\npropagate gradients. For a decoder-side, we can\nprove this property in the same manner.\nFigure 3 (b) indicates that B2T connection miti-\ngates the vanishing gradient of 18L-18L encoder-\ndecoders. Moreover, we locate B2T connection\nbefore the final layer normalization in each layer to\navoid a direct connection to the final layer output\nbased on the discussion in Section 4. Thus, B2T\nconnection preserves the property of Post-LN with\nrespect to the transformations performed by each\nlayer, as illustrated in Figure 5 (c)2.\n6 Experiments\nThrough experiments, we indicate following three\nfindings.\n• Post-LN Transformers achieve better perfor-\nmance than Pre-LN Transformers if their train-\ning succeeds.\n• B2T connection enables the training of deep\nTransformers with the Post-LN configuration.\n• Our modification preserves the performance\nadvantage of Post-LN Transformers, which\ntherefore outperform Pre-LN Transformers.\nWe describe the essential experimental configura-\ntions in this section. Appendix A presents more\ndetails, such as the hyper-parameters and computa-\ntional budgets.\n6.1 Machine Translation\n6.1.1 Dataset\nThe machine translation task has been\nwidely used to investigate the performance\nof Transformer-based methods since the original\nTransformer (Vaswani et al., 2017; Ott et al.,\n2018; Wang et al., 2019; Xiong et al., 2020; Liu\net al., 2020). We adopted the widely used WMT\nEnglish-to-German training dataset (Vaswani et al.,\n2017; Ott et al., 2018), which contains 4.5M\n2We also tried a connection that skips over all layer nor-\nmalizations including the final one in each layer but it signif-\nicantly impaired the performance. When we prepare such a\nconnection, the connection ties an input to the output directly.\nBecause this connection inhibits transformations performed\nby each layer as described in Section 4, it is reasonable that\nthe performance is impaired. Therefore, we avoid skipping the\nfinal layer normalization in each layer to take the advantage\nof Post-LN.\nsentence pairs. We applied the byte-pair-encoding\n(BPE) algorithm (Sennrich et al., 2016) to\nconstruct a vocabulary set in the same manner\nas previous studies. We set the number of BPE\nmerge operations to 32K and shared the vocabulary\nbetween the source and target languages. We used\nnewstest2010-2016 to investigate the performance,\nfollowing Takase and Kiyono (2021).\n6.1.2 Methods\nWe compare Post-LN, Pre-LN, and Post-LN with\nour B2T connection (B2T connection) Transform-\ners. We used fairseq3 (Ott et al., 2019) as an\nimplementation of Transformers. We stacked 6\nand 18 layers for the encoders and decoders (6L-\n6L and 18L-18L) as the widely used configuration\nand deep configuration, respectively. We used the\nTransformer (base) setting for dimension sizes of\ninternal layers. In addition to the above methods,\nwe evaluate the following five methods, which are\nrecent approaches that enable the training of deep\nTransformers. We used the same hyper-parameters\nfor all methods except T-Fixup. For T-Fixup, we\nused the hyper-parameters reported in Huang et al.\n(2020) to prevent divergence.\nDLCL To make Transformers deep, Wang et al.\n(2019) proposed dynamic linear combination of\nlayers (DLCL), which uses the weighted sum of the\nlower layers as an input of a layer. In contrast to our\nB2T connection, which is an additional connection\nwithin each layer, DLCL uses a connection among\nlayers. We apply DLCL to Post-LN Transformers.\nWe used the official implementation4.\nAdmin Liu et al. (2020) proposed adaptive model\ninitialization (Admin), which uses additional pa-\nrameters to stabilize the training of Post-LN Trans-\nformers. This method requires the variances of in-\nternal layers to initialize the additional parameters.\nThus, this method first processes several forward\nsteps for the initialization, and then conducts the\nactual training. In a nutshell, this method incurs ad-\nditional computational costs. We used the official\nimplementation5.\nT-Fixup Huang et al. (2020) proposed an initializa-\ntion scheme for Transformers, T-Fixup, to perform\nstable training without the learning rate warm-up\nand layer normalizations. Because this method can\nremove the cause of the vanishing gradient, we can\n3https://github.com/pytorch/fairseq\n4https://github.com/wangqiangneu/dlcl\n5https://github.com/LiyuanLucasLiu/Transformer-Clinic\n3082\nMethod 2010 2011 2012 2013 2014 2015 2016 Average\nEnc-Dec: 6L-6L\nPost-LN 24.27 22.06 22.43 26.11 27.13 29.70 34.40 26.59\nPre-LN 24.03 21.77 22.08 25.63 26.27 29.07 33.84 26.10\nDLCL (Wang et al., 2019) 23.94 22.00 22.24 26.11 27.37 29.71 34.26 26.52\nAdmin (Liu et al., 2020) 24.32 21.79 22.17 26.26 27.14 29.61 34.12 26.49\nT-Fixup (Huang et al., 2020) 24.09 21.98 22.04 25.96 26.92 29.45 34.56 26.43\nRealFormer (He et al., 2021) 24.18 22.02 22.17 26.02 26.98 29.36 34.15 26.41\nDeepNet (Wang et al., 2022) 24.08 21.76 22.09 25.90 26.85 29.62 34.39 26.38\nB2T connection 24.12 21.93 22.29 26.31 26.84 29.48 34.73 26.53\nEnc-Dec: 18L-18L\nPost-LN Training failed (See Figure 1) N/A\nPre-LN 24.07 21.98 22.40 26.28 27.36 29.74 34.16 26.57\nDLCL (Wang et al., 2019) 24.20 22.51 22.83 26.59 27.97 30.24 33.98 26.90\nAdmin (Liu et al., 2020) 24.56 22.17 22.62 26.48 27.99 30.35 33.88 26.86\nT-Fixup (Huang et al., 2020) 24.45 22.29 22.76 26.57 27.71 30.13 34.69 26.94\nRealFormer (He et al., 2021) 24.32 22.42 22.68 26.59 28.58 30.36 33.71 26.95\nDeepNet (Wang et al., 2022) 24.70 22.40 22.92 26.85 28.21 30.60 34.25 27.13\nB2T connection 24.62 22.51 22.86 26.74 28.48 30.99 34.93 27.30\nTable 1: BLEU scores of each method on WMT newstest2010-2016 and their averages.\nstack many layers. We used the official implemen-\ntation6.\nRealFormer To improve the performance of Trans-\nformers, He et al. (2021) proposed RealFormer,\nwhich introduces additional connections into atten-\ntion sub-layers. Although their motivation is not\naddressing the vanishing gradient problem, their\nmethod is similar to ours with respect to the use of\nadditional connections.\nDeepNet Wang et al. (2022) proposed DeepNorm,\nwhich uses a weight that corresponds to the num-\nber of layers in a residual connection before layer\nnormalizations to stabilize Post-LN based Trans-\nformers. They also provided the combination of the\ninitialization scheme and DeepNorm as DeepNet.\n6.1.3 Results\nWe measured case-sensitive detokenized BLEU\nscores with SacreBLEU (Post, 2018) 7. Ta-\nble 1 shows BLEU scores 8 of each method on\nnewstest2010-2016 and the averaged scores of\nthem. Since the BLEU score is precision-based\nn-gram overlapping between the model output and\ncorrect examples, a higher score represents better\nperformance.\n6https://github.com/layer6ai-labs/T-Fixup\n7The BLEU scores calculated by SacreBLEU are often\nlower than those calculated by the procedure of Vaswani et al.\n(2017) as reported in Ott et al. (2018). In fact, Pre-LN and\nB2T connection in the 18L-18L configuration achieved scores\nof 28.94 and 29.91, respectively, on newstest2014 when we\nused the same procedure of Vaswani et al. (2017). However,\nwe used SacreBLEU to ensure the compatibility of results, as\ndescribed in Post (2018).\n8The signature of SacreBLEU is BLEU+nrefs:1+\ncase:mixed+eff:no+tok:13a+smooth:exp+version:2.0.0.\n0 20 40 60 80 100 120 140\nEpoch\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5Valid loss (NLL)\nPre-LN\nDLCL\nAdmin\nT-Fixup\nRealFormer\nDeepNet\nB2T connection\nFigure 6: Negative Log-Likelihood (NLL) on validation\ndata (newstest2013) when we stack 18 layers.\nThe upper part of Table 1 shows results in the\n6L-6L configuration. This part indicates that Post-\nLN achieved better scores than Pre-LN on all test\nsets. In addition, B2T connection outperformed\nPre-LN on all test sets. Thus, these methods are\nsuperior to Pre-LN when the total number of layers\nis small.\nThe lower part of Table 1 shows results in the\n18L-18L configuration. This part shows that the\ntraining of Post-LN failed, and thus we cannot suc-\ncessfully stack 18L-18L in the vanilla Post-LN.\nWith the B2T connection, its training succeeded\nand it outperformed Pre-LN in the 18L-18L config-\nuration. Figure 6 shows the negative log-likelihood\n(NLL) values of all methods when we regard new-\nstest2013 as validation data. This figure indicates\nthat the NLLs of Pre-LN are worse than those of the\nother methods. These results demonstrate that our\nmodification enabled the stacking of many layers\n3083\nMethod 2010 2011 2012 2013 2014 2015 2016 Average\nEnc-Dec: 100L-100L\nPost-LN Training failed N/A\nPre-LN 24.81 22.67 23.15 26.98 28.42 30.50 34.53 27.29\nB2T connection 25.26 23.27 23.72 27.50 29.33 31.57 35.37 28.00\nTable 2: BLEU scores on WMT newstest2010-2016 and their averages in the 100L-100L configuration.\nwithout harm to its performance such as Pre-LN.\nIn the comparison with the recent methods, B2T\nconnection outperformed them with respect to the\naveraged BLEU score. This result implies that our\nmodification is superior to the recent methods. To\nmake our findings more reliable, we also conduct\na comparison with the recent methods on the sum-\nmarization task.\nTable 2 shows results in a much deeper config-\nuration: 100L-100L. This table also indicates that\nB2T connection stabilized the training and outper-\nformed Pre-LN. Appendix C describes the details\nof this 100L-100L configuration and shows a com-\nparison with the latest method, DeepNet (Wang\net al., 2022).\n6.2 Abstractive Summarization\n6.2.1 Dataset\nThe abstractive summarization task is one of the\nmost famous sequence-to-sequence problems in\nNLP. In this study, we conduct the experiment\non the headline generation task, which is the\ntask of generating a headline from a given sen-\ntence (Rush et al., 2015). We used headline-\nsentence pairs extracted from Annotated English\nGigaword (Napoles et al., 2012) by Rush et al.\n(2015). This dataset contains 3.8M headline-\nsentence pairs as the training set and 1951 pairs\nas the test set. In addition, we used 13M additional\nheadline-sentence pairs extracted from REAL-\nNEWS (Zellers et al., 2019) and NewsCrawl (Bar-\nrault et al., 2019) for training deep Transformers,\nfollowing Takase and Kiyono (2021). We applied\nBPE (Sennrich et al., 2016) to construct a vocabu-\nlary set. As in the machine translation experiments,\nwe set the number of BPE merge operations to 32K\nand shared the vocabulary between the encoder and\ndecoder sides.\n6.2.2 Methods\nWe compare Post-LN, Pre-LN, and B2T connec-\ntion Transformers in the same manner as in Sec-\ntion 6.1. In addition, we compare DLCL, Admin,\nT-Fixup, RealFormer, and DeepNet because it\nMethod R-1 R-2 R-L\nEnc-Dec: 6L-6L\nPost-LN 38.57 19.37 35.79\nPre-LN 38.27 19.29 35.39\nDLCL (Wang et al., 2019) 38.13 18.49 35.00\nAdmin (Liu et al., 2020) 37.96 18.93 35.05\nT-Fixup (Huang et al., 2020) 38.11 19.13 35.32\nRealFormer (He et al., 2021) 38.30 19.32 35.46\nDeepNet (Wang et al., 2022) 38.27 18.89 35.34\nB2T connection 38.43 19.37 35.72\nEnc-Dec: 18L-18L\nPost-LN Training failed\nPre-LN 38.97 19.94 35.99\nDLCL (Wang et al., 2019) 38.25 19.44 35.57\nAdmin (Liu et al., 2020) 39.10 20.08 36.30\nT-Fixup (Huang et al., 2020) 39.15 19.97 36.34\nRealFormer (He et al., 2021) 39.22 20.12 36.49\nDeepNet (Wang et al., 2022) 39.27 19.97 36.41\nB2T connection 39.61 20.28 36.66\nTable 3: F1 based ROUGE-1, 2, and L scores (columns\nheaded R-1, R-2, and R-L, respectively) on headline\ngeneration (Rush et al., 2015).\nwould be premature to conclude that our modifi-\ncation is more effective than those methods from\nthe results of experiments on the machine transla-\ntion task alone. We set the numbers of layers of\nencoders and decoders to 6L-6L and 18L-18L as\nthe base and deep configurations, respectively.\n6.2.3 Results\nTable 3 shows the ROUGE-1, 2, and L scores\nachieved by each method on the test set. Since\nthese scores are computed by n-gram overlapping\nbetween the generated and correct headlines, a\nhigher score represents better performance.\nIn the 6L-6L configuration, Post-LN achieved\nbetter performance than Pre-LN. Thus, Post-LN\noutperformed Pre-LN on the headline generation\ntask if training succeeded. Moreover, B2T con-\nnection achieved scores comparable to those of\nPost-LN.\nIn the 18L-18L configuration, the training of\nPost-LN failed. In contrast, the training of B2T con-\nnection succeeded, and this method outperformed\nPre-LN. Thus, our modification is more suitable\nthan Pre-LN for training deep Transformers to per-\nform the headline generation task.\n3084\nB2T connection outperformed the recent meth-\nods in the 6L-6L configuration and achieved the\nbest ROUGE scores in the 18L-18L configuration.\nAccording to the results on both the machine trans-\nlation and headline generation tasks, B2T connec-\ntion achieved performance that was better than, or\ncomparable to, that of previous methods. It is worth\nemphasizing that, in addition to the performance,\nour modification does not incur additional compu-\ntational costs, such as those incurred by DLCL and\nAdmin.\n6.3 Language Model\nIn addition to encoder-decoders, we investigate the\neffect of our B2T connection when used in the\ndecoder side only, i.e., a neural language model.\nBecause recent pre-trained models, such as the\nGPT series, are language models trained on a large\namount of training data, experimental results in this\nsection give an insight for pre-trained models.\n6.3.1 Dataset\nWe used WikiText-103 (Merity et al., 2017), which\nconsists of a large number of tokens. The train-\ning, validation, and test sets contain 103M, 0.2M,\nand 0.2M tokens, respectively. The vocabulary set\ncontains 0.3M words.\n6.3.2 Methods\nWe used a Transformer with adaptive input rep-\nresentations (Baevski and Auli, 2019), which is\nimplemented in fairseq, as the base architecture\nin this experiment. For the base configuration, we\nstacked 6 layers, in the same manner as in the ma-\nchine translation and summarization experiments.\nFor the deep configuration, we used 16 layers, fol-\nlowing Baevski and Auli (2019). For the dimen-\nsions of internal layers, we used the same values\nas those used by Baevski and Auli (2019). We\ncompare Post-LN, Pre-LN, and B2T connection.\n6.3.3 Results\nTable 4 shows perplexities of each method on the\nvalidation and test sets of WikiText-103. Since the\nperplexity is computed based on the negative log-\nlikelihood, a smaller value corresponds to better\nperformance. The upper part of this table indi-\ncates that, with 6 layers, Post-LN and our B2T con-\nnection outperformed Pre-LN. When we stacked\n16 layers, the training of Post-LN failed, but B2T\nconnection achieved better performance than Pre-\nLN. These results are consistent with results on\nMethod Valid Test\nDec: 6L\nPost-LN 20.24 21.22\nPre-LN 20.98 21.93\nB2T connection 20.50 21.47\nDec: 16L\nPost-LN Training failed\nPre-LN 18.53 19.24\nB2T connection 18.38 19.20\nTable 4: Perplexities on WikiText-103 (Merity et al.,\n2017).\nDev Test\nMethod Clean Other Clean Other\nEnc-Dec: 6L-6L\nPost-LN 3.78 8.76 4.19 8.74\nPre-LN 3.89 9.69 4.22 9.65\nB2T connection 3.69 8.97 3.86 8.94\nEnc-Dec: 12L-6L\nPost-LN Training failed\nPre-LN 3.21 7.91 3.49 8.22\nB2T connection 3.26 7.74 3.48 7.68\nTable 5: Word error rates of each method on Lib-\nriSpeech.\nthe machine translation and summarization tasks.\nThus, our modification enables the training of deep\nTransformers for language modeling, and it is more\neffective than Transformers with Pre-LN.\n6.4 Automatic Speech Recognition\nIn addition to experiments on natural language pro-\ncessing tasks, we conduct an experiment on another\nmodality, ASR.\n6.4.1 Dataset\nWe used LibriSpeech (Panayotov et al., 2015),\nwhich is the standard English ASR benchmark\ndataset. The dataset contains 1,000 hours of En-\nglish speech extracted from audiobooks. We used\nthe standard splits of LibriSpeech: we used all\navailable training data for training and two config-\nurations (‘clean’ and ‘other’) of development sets\nand test sets for evaluation. We applied the same\npre-processing as that used by Wang et al. (2020).\nWe constructed a vocabulary set for the decoder-\nside with SentencePiece (Kudo and Richardson,\n2018) by setting the vocabulary size to 10,000. To\nobtain speech features, we used torchaudio9.\n6.4.2 Methods\nWe used the Transformer-based speech-to-text\nmodel described in Wang et al. (2020) as the base\n9https://github.com/pytorch/audio\n3085\narchitecture in this experiment. This model con-\ntains a convolutional layer to construct an embed-\nding for the encoder-side but the other parts are\nidentical to the Transformers used on the machine\ntranslation and summarization tasks. We used the\nsame dimensions as those of T-Md, described in\nWang et al. (2020). We set the numbers of layers\nto 6L-6L and 12L-6L as the base and deep config-\nurations, respectively, because Wang et al. (2020)\nstacked many layers on the encoder-side only. We\ncompare Post-LN, Pre-LN, and B2T connection.\n6.4.3 Results\nTable 5 shows the word error rates (WERs) of each\nmethod on each set. A smaller value of WER cor-\nresponds to better performance. The upper part\nof this table indicates that Post-LN and B2T con-\nnection outperformed Pre-LN on all sets in the\n6L-6L configuration. The lower part of the table\nshows that B2T connection succeeded in training\nand achieved performance that was better than (or\ncomparable to) that of Pre-LN in the 12L-6L con-\nfiguration10. These results are consistent with those\nof the other experiments in this study.\n7 Related Work\nLayer normalization (Ba et al., 2016) is a useful\ntechnique for training neural networks but its mech-\nanism has been unclear (Xu et al., 2019). The\nTransformer, which is the standard architecture for\nvarious tasks, also contains layer normalizations.\nThe original Transformer architecture adopted the\nPost-LN configuration (Vaswani et al., 2017). How-\never, recent Transformer implementations have\nadopted Pre-LN configurations (Klein et al., 2017;\nVaswani et al., 2018; Ott et al., 2019; Baevski and\nAuli, 2019).\nTo construct deep Transformers that achieve bet-\nter performance, recent studies have focused on\nthe behavior of layer normalizations. Wang et al.\n(2019) indicated the difficulty of training deep\nTransformers with Post-LN due to the vanishing\ngradient problem, and demonstrated that Pre-LN\nenables the stacking of many layers through ma-\nchine translation experiments. In addition, they\nproposed a method to connect all layers to increase\nthe effectiveness of deep Transformers. Bapna\n10Wang et al. (2020) reported that the improvement was\nsmall even if they increased the number of parameters. Thus,\nwe emphasize that B2T connection achieved better WERs on\ndev-other and test-other even though the number of parameters\nof B2T connection is (almost) equal to that of Pre-LN.\net al. (2018) and Dou et al. (2018) also proposed\nsuch connection methods to stack many layers. He\net al. (2021) introduced additional connections into\nattention sub-layers to improve the performance.\nXiong et al. (2020) explored the relation between\nthe warm-up strategy and layer normalizations in\nTransformers. Through theoretical and empirical\nanalyses, they indicated that Post-LN requires the\nwarm-up strategy to stabilize the training.\nLiu et al. (2020) analyzed the training dynam-\nics of Post-LN and Pre-LN Transformers. They\nthen proposed Admin, which consists of additional\nweight parameters to control the variances of out-\nputs from each sub-layer. In contrast, we indicated\nthat we can stabilize the training of Post-LN Trans-\nformers by adding only a residual connection that\nskips over layer normalizations that cause the van-\nishing gradient.\nSome studies have proposed initialization meth-\nods to make the training of deep neural networks\nstable (Zhang et al., 2019a,b; Huang et al., 2020).\nZhang et al. (2019a) proposed the depth-scaled ini-\ntialization to prevent the vanishing gradient prob-\nlem in Transformers. Zhang et al. (2019b) pro-\nposed the fixed-update initialization to remove nor-\nmalizations in neural networks. Inspired by these\nstudies, Huang et al. (2020) proposed T-Fixup,\nwhich enables both warm-up and layer normaliza-\ntions to be removed from Transformers. In addition\nto the initialization scheme, Wang et al. (2022) in-\ntroduced weights into residual connections before\nlayer normalizations, following Liu et al. (2020).\n8 Conclusion\nIn this study, we addressed the stability of train-\ning Post-LN Transformers. Through theoretical\nand empirical analyses, we indicated that layer nor-\nmalizations cause the unstable training when many\nlayers are stacked. In addition, we investigated the\nreason for the different performance of Pre-LN and\nPost-LN by transformations of each layer. We in-\ntroduced B2T connection to prevent the vanishing\ngradient while preserving the advantage of Post-\nLN. We conducted experiments on various tasks.\nThe experimental results led to the following three\nfindings; 1, Post-LN achieved better performance\nthan Pre-LN if its training succeeded. 2, Our mod-\nification enabled the training of deep Transform-\ners (e.g., those with ten or more layers). 3, Our\nmodification preserved the benefit of Post-LN, and\ntherefore outperformed Pre-LN.\n3086\nLimitations\nIn this paper, we indicated that the vanishing gra-\ndient problem, caused by layer normalizations,\nmakes the training of deep Post-LN Transform-\ners unstable. We proposed the B2T connection to\nmitigate this vanishing gradient problem. However,\nthe proposed B2T connection does not perfectly\nprevent the vanishing gradient, as shown in Figure\n3. Therefore, the vanishing gradient might harm\nthe training in extremely deep Transformers even\nif our B2T connection is used.\nIn addition, this study depends on empirical ob-\nservations. In particular, we provided little theoret-\nical justification of the reason for Post-LN outper-\nforming Pre-LN when training succeeds. However,\nas discussed in Appendix C, the method with a the-\noretical justification often collapses in some situa-\ntions. Because the behavior of deep Transformers\nin various situations is not fully understood, we\nbelieve that it is important to provide empirical\nfindings for our research field to progress.\nAlthough Appendix C includes a comparison be-\ntween our B2T connection and the latest method,\nDeepNet (Wang et al., 2022), we could not investi-\ngate the behavior of all methods in the 100L-100L\nconfiguration because of our limited computational\nbudgets. However, we are confident that we con-\nducted sufficient experiments to verify our contri-\nbutions.\nEthics Statement\nThe proposed method helps to construct deep Trans-\nformers. As discussed in Strubell et al. (2019) and\nSchwartz et al. (2019), such deep neural networks\nconsume substantial amounts of energy. In fact,\nas discussed in Appendix A.2, we spent a large\namount of computational resources on our experi-\nments. Therefore, we also need to explore methods\nof improving energy efficiency while maintaining\nthe good performance achieved by stacking many\nlayers.\nWith respect to ethical considerations, the\ndatasets used in our experiments are publicly avail-\nable. LibriSpeech (Panayotov et al., 2015) is\nderived from audiobooks. The other datasets\nare mainly constructed from newswire texts and\nWikipedia. Thus, in our understanding, our used\ndatasets do not contain any personally identifiable\ninformation or offensive contents.\nAcknowledgements\nWe thank the anonymous reviewers for their useful\nsuggestions. A part of this work was supported\nby JSPS KAKENHI Grant Number JP21K17800\nand JST ACT-X Grant Number JPMJAX200I. The\nwork of Jun Suzuki was partly supported by JST\nMoonshot R&D Grant Number JPMJMS2011 (fun-\ndamental research). We thank Edanz for editing a\ndraft of this manuscript.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In\nProceedings of the 7th International Conference on\nLearning Representations (ICLR).\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and\nYonghui Wu. 2018. Training deeper neural machine\ntranslation models with transparent attention. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3028–3033.\nLoïc Barrault, Ond ˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Confer-\nence on Machine Translation (WMT), pages 1–61.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems\n33 (NeurIPS), pages 1877–1901.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and\nTong Zhang. 2018. Exploiting deep representations\nfor neural machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4253–4262.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770–778.\n3087\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016b. Identity mappings in deep residual net-\nworks. In 14th European Conference on Computer\nVision, pages 630–645.\nRuining He, Anirudh Ravula, Bhargav Kanagal, and\nJoshua Ainslie. 2021. RealFormer: Transformer likes\nresidual attention. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 929–943.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims\nV olkovs. 2020. Improving transformer optimization\nthrough better initialization. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119, pages 4475–4483.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by re-\nducing internal covariate shift. In Proceedings of the\n32nd International Conference on Machine Learning\n(ICML), volume 37, pages 448–456.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In Pro-\nceedings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics (ACL) , pages\n67–72.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 66–71.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen,\nand Jiawei Han. 2020. Understanding the difficulty\nof training transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5747–5763.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer Sentinel Mixture Mod-\nels. In Proceedings of the 5th International Confer-\nence on Learning Representations (ICLR).\nCourtney Napoles, Matthew Gormley, and Benjamin\nVan Durme. 2012. Annotated Gigaword. In Proceed-\nings of the Joint Workshop on Automatic Knowledge\nBase Construction and Web-scale Knowledge Extrac-\ntion (AKBC-WEKEX), pages 95–100.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associ-\nation for Computational Linguistics (NAACL), pages\n48–53.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation (WMT), pages 1–9.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur. 2015. Librispeech: An asr corpus\nbased on public domain audio books. In 2015 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5206–5210.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation (WMT), pages 186–191.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A Neural Attention Model for Abstractive Sen-\ntence Summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 379–389.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI. CoRR, abs/1907.10597.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 1715–1725.\nRupesh K Srivastava, Klaus Greff, and Jürgen Schmid-\nhuber. 2015. Training very deep networks. In Ad-\nvances in Neural Information Processing Systems 28\n(NIPS), pages 2377—-2385.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL), pages 3645–3650.\nSho Takase and Shun Kiyono. 2021. Rethinking per-\nturbations in encoder-decoders for fast training. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT), pages 5767–5780.\nSho Takase and Naoaki Okazaki. 2019. Positional en-\ncoding to control output sequence length. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL),\npages 3999–4004.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar,\nRyan Sepassi, Noam Shazeer, and Jakob Uszkoreit.\n2018. Tensor2Tensor for neural machine translation.\nIn Proceedings of the 13th Conference of the Associ-\nation for Machine Translation in the Americas, pages\n193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 (NIPS), pages 5998–6008.\n3088\nChanghan Wang, Yun Tang, Xutai Ma, Anne Wu,\nDmytro Okhonko, and Juan Pino. 2020. Fairseq\nS2T: Fast speech-to-text modeling with fairseq. In\nProceedings of the 1st Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 10th International Joint Conference\non Natural Language Processing (AACL-IJCNLP),\npages 33–39.\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,\nDongdong Zhang, and Furu Wei. 2022. Deepnet:\nScaling transformers to 1,000 layers.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 1810–1822.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. In Pro-\nceedings of the 37th International Conference on\nMachine Learning (ICML), pages 10524–10533.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao,\nand Junyang Lin. 2019. Understanding and improv-\ning layer normalization. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), volume 32.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Processing\nSystems 32 (NeurIPS), pages 9054–9065.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019a. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 898–909.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma.\n2019b. Fixup initialization: Residual learning with-\nout normalization. In Proceedings of the 7th Inter-\nnational Conference on Learning Representations\n(ICLR).\n3089\nA Details of Experimental Settings\nA.1 Hyper-parameters\nAs described in Section 6, our hyper-parameters\nfollow those used in previous studies. Table 6\nshows hyper-parameters used for each experiment.\nFor fair comparisons, we used the same hyper-\nparameters for all methods except T-Fixup. For\nT-Fixup, we used hyper-parameters reported in\nHuang et al. (2020) to prevent divergence.\nA.2 Computational Resources\nWe mainly used NVIDIA Tesla P100 GPUs for\nmost of our experiments. Table 7 shows the num-\nber of GPUs and the computational time used to\nconstruct one model in our experiments. For the\n100L-100L configuration, described in Section 6.1,\nwe used 24 Tesla V100 GPUs and spent approxi-\nmately 120 hours to train one model.\nB Supplementary of Gradient Norms of\nEach Location\nFor gradient norms of each part in a layer, we check\n1st and 9th decoders in addition to the 18th decoder\nfor the 18L-18L Post-LN Transformer encoder-\ndecoder as shown in Figure 4. Figure 7 shows\nthe gradient norms of each part. This figure shows\nthat the gradient norms decrease drastically through\nlayer normalizations in the same manner as they do\nin the 18th decoder (Figure 4). Therefore, the van-\nishing gradient problem in Post-LN Transformers\nis probably caused by layer normalizations.\nC Details of the 100L-100L Configuration\nC.1 Regularizations during the Training\nAs reported in Section 1, we constructed 100L-\n100L Transformers with widely-used WMT\nEnglish-to-German dataset. In the preliminary ex-\nperiments, we found that regularization is the key\nto preventing overfitting and achieving high perfor-\nmance in this situation. Figure 8 shows the NLL\nvalues of Pre-LN and B2T connection on validation\ndata in the 36L-36L configuration when we used\nthe same hyper-parameters as those used in 6L-6L\nand 18L-18L configurations. As this figure shows,\nthe NLL values began to increase from the middle\nof training, and thus the overfitting occurred. In\naddition, the use of the same hyper-parameters as\n6L-6L and 18L-18L makes it difficult to improve\nthe performance of deeper configurations. Figure 9\nshows the best NLL values on validation data when\nwe varied the number of layers: 6L-6L, 12L-12L,\n18L-18L, 36L-36L, and 50L-50L11. This figure in-\ndicates that adding more layers to the 18L-18L\nconfiguration did not improve the performance.\nTo prevent overfitting during the training of\n100L-100L Transformers, we increased the dropout\nrate from 0.3 to 0.5. In addition, we used word\ndropout, as described in Takase and Kiyono (2021).\nWe set the word dropout rate to 0.1 for the encoder\nand decoder. We multiplied the initial parameter\nvalues, except those for embeddings, by0.1. We set\nthe gradient clipping to 0.1. Finally, we decreased\nthe number of updates from 50K to 25K. These\nregularization techniques prevented overfitting and\nachieved better performance than 18L-18L, as de-\nscribed in Section 6.1.\nC.2 Comparison with DeepNet\nAs described in Section 7, various studies have\nattempted to stabilize the training of deep Trans-\nformers. Each study indicated the effectiveness\nof their proposed method empirically, and some\nhave provided theoretical justifications. However,\nWang et al. (2022) demonstrated that the training\nof previous methods except DeepNet failed in a\nmuch deeper configuration than normally used, i.e.,\n100L-100L. Then, can we conclude that DeepNet\nis a silver bullet for deep Transformers? It is diffi-\ncult to reach this conclusion because the training of\nDeepNet also fails in some configurations. For ex-\nample, when we train deep Transformers, we might\ndecrease the batch size because the trainable param-\neters occupy most of the GPU memories. When\nwe tried this, the NLL value of DeepNet on val-\nidation data diverged, as shown in Figure 10. In\nother words, the training of DeepNet failed. In con-\ntrast, the training of our B2T connection succeeded\nin this situation. This result implies that there are\nproblems in the training of deep Transformers that\nhave not been solved in previous studies. There-\nfore, we believe that we should continue to add the\nempirical findings about new techniques, including\nB2T connection, to those of previous studies.\nD B2T Connection without Layer\nNormalization\nIn addition to B2T connection, we also consider a\nfurther modification to prevent the vanishing gra-\n11The horizontal axis of Figure 9 represents the total number\nof layers, which are divided equally between the encoder and\ndecoder. For example, 100 on the horizontal axis represents\n50L-50L Transformers.\n3090\nParams Machine Translation Abstractive Summarization Language Model ASR\nHidden dim size 512 512 1024 512\nFFN dim size 2048 2048 4096 2048\nAttention heads 8 8 8 8\nLearning rate 0.001 0.001 0.001 0.001\nScheduler inverse sqrt inverse sqrt inverse sqrt inverse sqrt\nAdam β (0.9, 0.98) (0.9, 0.98) (0.9, 0.98) (0.9, 0.98)\nWarmup updates 4K 4K 2K 4K\nMax updates 50K 50K 50K 150K\nMax tokens / GPU 3584 3584 1024 40K\nTable 6: Hyper-parameters used in our experiments.\nMachine Translation Abstractive Summarization Language Model ASR\n6L-6L 18L-18L 6L-6L 18L-18L 6L 16L 6L-6L 12L-6L\n#GPU 128 128 64 144 128 192 32 32\nTime (hour) 5 13 4 17 4 7 22 34\nTable 7: The number of GPUs and computational time used to construct one model in our experiments.\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\nLayer Norm\nAttention\nFFN\nLayer Norm\nLayer Norm\nAttention\n(a) 1stdecoder (b) 9th decoder\nFigure 7: Gradient norms of each part in the (a) 1st decoder and (b) 9th decoder of the 18L-18L Post-LN Transformer\nencoder-decoder on WMT English-to-German translation training data.\n0 20 40 60 80 100 120 140\nEpoch\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5Valid loss (NLL)\nPre-LN\nB2T connection\nFigure 8: Negative Log-Likelihood (NLL) values of Pre-\nLN and our proposed B2T connection on validation data\n(newstest2013) in 36L-36L. We used the same hyper-\nparameters as those used in 6L-6L and 18L-18L.\ndient problem. Because layer normalizations de-\ncrease gradients drastically, as described in Section\n20 40 60 80 100\nThe total number of layers\n2.00\n2.05\n2.10\n2.15\n2.20\n2.25\n2.30Valid Loss\nPost-LN\nPre-LN\nB2T connection\nFigure 9: The best Negative Log-Likelihood (NLL)\nvalues on validation data (newstest2013) when the total\nnumber of layers is varied. The total number of layers\nis divided equally between the encoder and decoder.\n3, removing layer normalizations may provide sta-\nble gradients during back-propagation. However,\n3091\n0 1000 2000 3000 4000 5000 6000 7000 8000\nThe number of updates\n2\n4\n6\n8\n10\n12\n14\n16Valid loss (NLL)\nDeepNet\nB2T connection\nFigure 10: Negative Log-Likelihood (NLL) values of\nDeepNet and our proposed B2T connection on valida-\ntion data (newstest2013) in 100L-100L with a small\nbatch size.\nthe values in the forward pass increase exponen-\ntially if layer normalizations are removed. There-\nfore, we introduce weights that prevent the explo-\nsive increase in the forward pass while mitigating\nthe decreasing gradients in back-propagation, as\nan alternative to the layer normalization. To use\nthis alternative, we replace Equation (5) with the\nfollowing equation:\nαxinp + β(xffn + FFN(xffn )) . (9)\nThrough several experiments12, we found that the\nfollowing values of αand βare suitable:\nα= min\n(N\n12,N−0.15\n)\n, (10)\nβ = d−0.2, (11)\nwhere N is the number of layers and dis the di-\nmension of the input vectors xinp. For example, N\nis set to 12 and 6 in the encoder and decoder, re-\nspectively, in the 12L-6L configuration. Therefore,\nas the number of layers increases, the value of α\nincreases while N remains small (until N = 9),\nand then αstarts to decrease. In short, αprevents\nan explosive increase in the forward pass when we\nstack many layers. βdecreases as the dimension d\nincreases, and thus it prevents an explosive increase\nwhen a large dimension is used. By using Equa-\ntion (9), we can remove all layer normalizations in\ninternal layers. This solves the vanishing gradient\nproblem caused by layer normalizations.\nTables 8 and 9 shows the results of B2T connec-\ntion without layer normalizations (“w/o LN”) on\n12We could instead tuneα and β to improve performance on\neach task but here we define values that are useful for various\ntasks.\nthe machine translation and summarization tasks.\nThese results indicate that B2T connection without\nlayer normalizations achieved scores comparable\nto those of B2T connection with layer normaliza-\ntions. However, because the results of B2T con-\nnection without layer normalizations are slightly\nworse than those with layer normalizations, we\nrecommend the use of B2T connection with layer\nnormalizations.\n3092\nMethod 2010 2011 2012 2013 2014 2015 2016 Average\nEnc-Dec: 6L-6L\nB2T connection 24.12 21.93 22.29 26.31 26.84 29.48 34.73 26.53\n+ w/o LN 24.17 22.07 22.24 25.83 26.96 29.70 34.42 26.48\nEnc-Dec: 18L-18L\nB2T connection 24.75 22.88 23.09 27.12 28.82 30.99 33.64 27.33\n+ w/o LN 24.47 22.37 22.58 27.04 28.34 30.49 34.38 27.10\nTable 8: BLEU scores of our modifications on WMT newstest2010-2016 and their averages.\nMethod R-1 R-2 R-L\nEnc-Dec: 6L-6L\nB2T connection 38.43 19.37 35.72\n+ w/o LN 38.63 19.75 35.77\nEnc-Dec: 18L-18L\nB2T connection 39.61 20.28 36.66\n+ w/o LN 39.29 20.01 36.48\nTable 9: F1 based ROUGE scores of our modifications\non headline generation.\n3093\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section, after Conclusion section\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthics Statement section, after Conclusion section\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSections 1 and 8\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 5\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSections 6 and 7\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nEthics Statement section\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nEthics Statement section\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 6\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 6\nC □\u0013 Did you run computational experiments?\nSection 6 and Appendices A, B, C, and D\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendices A and C\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3094\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix A\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 6 and Appendices A\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n3095",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6563270688056946
    },
    {
      "name": "Computer science",
      "score": 0.5487563610076904
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.489391952753067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4300253391265869
    },
    {
      "name": "Electrical engineering",
      "score": 0.224252849817276
    },
    {
      "name": "Voltage",
      "score": 0.18831610679626465
    },
    {
      "name": "Engineering",
      "score": 0.17882466316223145
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I114531698",
      "name": "Tokyo Institute of Technology",
      "country": "JP"
    }
  ]
}