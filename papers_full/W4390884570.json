{
    "title": "YOLOV5-CBAM-C3TR: an optimized model based on transformer module and attention mechanism for apple leaf disease detection",
    "url": "https://openalex.org/W4390884570",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2111300621",
            "name": "Meng Lv",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A2116702738",
            "name": "Wen Hao Su",
            "affiliations": [
                "China Agricultural University"
            ]
        },
        {
            "id": "https://openalex.org/A2111300621",
            "name": "Meng Lv",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116702738",
            "name": "Wen Hao Su",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3158308652",
        "https://openalex.org/W4229439833",
        "https://openalex.org/W3018757597",
        "https://openalex.org/W4210261474",
        "https://openalex.org/W2607056758",
        "https://openalex.org/W3178340391",
        "https://openalex.org/W6788137759",
        "https://openalex.org/W2008056655",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W3043046815",
        "https://openalex.org/W6803118227",
        "https://openalex.org/W4281694288",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3190789542",
        "https://openalex.org/W4312195514",
        "https://openalex.org/W4296218720",
        "https://openalex.org/W4361030063",
        "https://openalex.org/W4304128474",
        "https://openalex.org/W6742348326",
        "https://openalex.org/W4303579401",
        "https://openalex.org/W2193145675",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W2956675317",
        "https://openalex.org/W3209833194",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6986403248",
        "https://openalex.org/W4220675870",
        "https://openalex.org/W6768952390",
        "https://openalex.org/W4292583675",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W3217005998",
        "https://openalex.org/W4385692867",
        "https://openalex.org/W4283770853",
        "https://openalex.org/W2995504058",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W2924160059",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W4293584584",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W3208826636",
        "https://openalex.org/W2993737916",
        "https://openalex.org/W2559655401",
        "https://openalex.org/W3106250896"
    ],
    "abstract": "Apple trees face various challenges during cultivation. Apple leaves, as the key part of the apple tree for photosynthesis, occupy most of the area of the tree. Diseases of the leaves can hinder the healthy growth of trees and cause huge economic losses to fruit growers. The prerequisite for precise control of apple leaf diseases is the timely and accurate detection of different diseases on apple leaves. Traditional methods relying on manual detection have problems such as limited accuracy and slow speed. In this study, both the attention mechanism and the module containing the transformer encoder were innovatively introduced into YOLOV5, resulting in YOLOV5-CBAM-C3TR for apple leaf disease detection. The datasets used in this experiment were uniformly RGB images. To better evaluate the effectiveness of YOLOV5-CBAM-C3TR, the model was compared with different target detection models such as SSD, YOLOV3, YOLOV4, and YOLOV5. The results showed that YOLOV5-CBAM-C3TR achieved mAP@0.5, precision, and recall of 73.4%, 70.9%, and 69.5% for three apple leaf diseases including Alternaria blotch, Grey spot, and Rust. Compared with the original model YOLOV5, the mAP 0.5increased by 8.25% with a small change in the number of parameters. In addition, YOLOV5-CBAM-C3TR can achieve an average accuracy of 92.4% in detecting 208 randomly selected apple leaf disease samples. Notably, YOLOV5-CBAM-C3TR achieved 93.1% and 89.6% accuracy in detecting two very similar diseases including Alternaria Blotch and Grey Spot, respectively. The YOLOV5-CBAM-C3TR model proposed in this paper has been applied to the detection of apple leaf diseases for the first time, and also showed strong recognition ability in identifying similar diseases, which is expected to promote the further development of disease detection technology.",
    "full_text": "YOLOV5-CBAM-C3TR: an\noptimized model based on\ntransformer module and\nattention mechanism for apple\nleaf disease detection\nMeng Lv and Wen-HaoSu*\nCollege of Engineering, China Agricultural University, Beijing, China\nApple trees face various challenges during cultivation. Apple leaves, as the key\npart of the apple tree for photosynthesis, occupy most of the area of the tree.\nDiseases of the leaves can hinder the healthy growth of trees and cause huge\neconomic losses to fruit growers. The prerequisite for precise control of apple\nleaf diseases is the timely and accurate detection of different diseases on apple\nleaves. Traditional methods relying on manual detection have problems such as\nlimited accuracy and slow speed. In this study, both the attention mechanism and\nthe module containing the transformer encoder were innovatively introduced\ninto YOLOV5, resulting in YOLOV5-CBAM-C3TR for apple leaf disease detection.\nThe datasets used in this experiment were uniformly RGB images. To better\nevaluate the effectiveness of YOLOV5-CBAM-C3TR, the model was compared\nwith different target detection models such as SSD, YOLOV3, YOLOV4, and\nYOLOV5. The results showed that YOLOV5-CBAM-C3TR achieved mAP@0.5,\nprecision, and recall of 73.4%, 70.9%, and 69.5% for three apple leaf diseases\nincluding Alternaria blotch, Grey spot, and Rust. Compared with the original\nmodel YOLOV5, the mAP 0.5increased by 8.25% with a small change in the\nnumber of parameters. In addition, YOLOV5-CBAM-C3TR can achieve an\naverage accuracy of 92.4% in detecting 208 randomly selected apple leaf\ndisease samples. Notably, YOLOV5-CBAM-C3TR achieved 93.1% and 89.6%\naccuracy in detecting two very similar diseases including Alternaria Blotch and\nGrey Spot, respectively. The YOLOV5-CBAM-C3TR model proposed in this paper\nhas been applied to the detection of apple leaf diseases for theﬁrst time, and also\nshowed strong recognition ability in identifying similar diseases, which is\nexpected to promote the further development of disease detection technology.\nKEYWORDS\ndeep learning, apple leaf, disease detec tion, YOLOv5, attention mechanism,\ntransformer encoder\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nLei Shu,\nNanjing Agricultural University, China\nREVIEWED BY\nMarcin Wozniak,\nSilesian University of Technology, Poland\nJakub Nalepa,\nSilesian University of Technology, Poland\n*CORRESPONDENCE\nWen-Hao Su\nwenhao.su@cau.edu.cn\nRECEIVED 17 October 2023\nACCEPTED 26 December 2023\nPUBLISHED 15 January 2024\nCITATION\nLv M andSu W-H (2024) YOLOV5-CBAM-\nC3TR: an optimized model based on\ntransformer module and attention\nmechanism for apple leaf disease detection.\nFront. Plant Sci.14:1323301.\ndoi: 10.3389/fpls.2023.1323301\nCOPYRIGHT\n©2 0 2 4L va n dS u .T h i si sa no p e n - a c c e s s\narticle distributed under the terms of the\nCreative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 15 January 2024\nDOI 10.3389/fpls.2023.1323301\n1 Introduction\nApples are highly prized for their nutritional richness and rank\namong the world ’s most economically signi ﬁcant fruits ( Shu et al.,\n2019). However, due to environmental, bacterial, and insect pests,\nthe growth of apple fruits can be attacked by a variety of diseases,\nwhich can lead to a decrease in fruit yield and quality, resulting in\nhuge economic losses. Timely detection and accurate classi ﬁcation\nof the type of disease is the ﬁrst step to early control of the disease.\nThe leaves of apple trees occupy most of the area of the tree and are\nthe easiest part to observe. Most apple diseases can be identi ﬁed by\nobserving diseased leaves ( Wang et al., 2009 ; Vishnu and Rajanith,\n2015). Therefore, the research in this study focuses on diseases of\napple leaf parts.\nTraditionally, the identi ﬁcation of apple leaf disease mostly\nrelied on experienced farmers to identify the disease. However, due\nto the similarity of diseases or the complexity of symptoms, relying\non human eye detection can easily lead to misjudgment of diseases,\nwhich can not only solve the problem of diseases but also cause\nenvironmental pollution ( Liu et al., 2022 ). The combination of\nmachine learning and image p rocessing replaced human eye\ndetection and provided a new direction for disease detection. For\nexample, Dubey and Jalal (2012) used K-means clustering for the\nsegmentation of apple fruit diseases, then global color histogram,\ncolor coherence vector, local binary pattern, and complete local\nbinary pattern were used for feature extraction, the support vector\nmachine (SVM) ( Hearst et al., 1998 )w a su s e df o rd i s e a s e\nclassiﬁcation, which can achieve an accuracy of 93%. Chuanlei\net al. (2017) introduced a method for apple leaf disease detection.\nTo improve the detection accuracy, a region-growing algorithm is\nused to segment the disease image, a genetic algorithm combined\nwith correlation feature selection is used to select the important\nfeatures, and ﬁnally SVM classi ﬁer is used to identify the disease,\nwhich was tested on a dataset containing 90 images on a dataset\nwith an accuracy of 90%. Shi et al. (2017) proposed an apple disease\nrecognition method based on two-dimensional subspace learning\ndimensionality reduction, with recognition accuracy above 90% on\nthe apple leaf disease dataset. Gargade and Khandekar (2021) used\nK-NN and SVM algorithms to classify apple leaf defects with 99.5%\naccuracy. Jan and Ahmad (2020) used 11 apple leaf image features\nand a multilayer perceptron (MLP) pattern classi ﬁer to detect apple\nAlternaria leaf blotch with 99.1% accuracy. However, segmentation\nbased on image processing and feature extraction based on\ntraditional machine learning are extremely complex, leading to\ninefﬁcient disease diagnosis.\nIn recent years, convolutional neural network (CNN)-based\nmodel avoids complex preprocessing work on images and\nautomatically extracts features through an end-to-end approach\n(Abade et al., 2021 ; Dhaka et al., 2021 ), which is more suitable for\nsolving problems in the ﬁeld of computer vision. Apple leaf disease\ndetection tasks can be classi ﬁed into three main categories according\nto the type of computer vision tasks: 1) image classi ﬁ\ncation, which\nclassiﬁes the detected images into various types of diseases, 2) target\ndetection, which detects and locates the diseases in the images, and\n3) image segmentation, which segments the images into semantic\ndisease maps. In general, image classi ﬁcation studies using CNN\nmodels are the most common. Based on Densenet-121, Zhong and\nZhao (2020) proposed regression, multi-label classi ﬁcation and focal\nloss function recognition methods for three apple leaf diseases with\naccuracies of 93.51%, 93.31% and 93.71%, respectively. Yu and Son\n(2019) used the ROI-aware DCNN model to classify Marssonia\nblotch and Alternaria leaf spots, which was shown to outperform\ntraditional methods. Singh et al. (2021) improved the classical CNN\nmodel to implement Marssonia Coronaria, Rust, and Scab for\naccurate classi ﬁcation with up to the accuracy of 99.2%. Babu and\nRam (2022) proposed a deep residual convolutional neural network\n(DRCNN) with contrast limited adaptive histogram equalization for\nweed and soybean crop classi ﬁcation with an accuracy of 97.25%.\nKundu et al. (2021) proposed the use of deep learning in conjunction\nwith IoT for automatic detection of pearl millet diseases, and the\naccuracy of the proposed custom network model is comparable to\nthat of the current state-of-the-art image classi ﬁcation model, with\nan accuracy of up to 98.78%, while greatly reducing the training\ntime. Image classi ﬁcation methods are excellent at accurately\nclassifying diseases, but their utility is limited by failing to provide\ninformation about the location of the disease. In contrast, target\ndetection methods can locate the target object in real-time and\nobtain more detailed information, which is more conducive to\npractical application. Currently, target detection methods can be\nclassiﬁed into single-stage and two-stage algorithms. Two-stage\nalgorithms such as Faster-RCNN ( Ren et al., 2015 ) and Mask-\nRCNN ( Kaiming et al., 2017 ) have higher detection accuracy but\nlose detection speed. In comparison, single-stage algorithms are\ncharacterized by a small number of model parameters and fast\ninference speed, which better meet the needs of practical production\nenvironments. The single-stage algorithms are best known as you\nonly look once (YOLO) ( Redmon et al., 2016 ), which turns the\ndetection task into a simple regression problem and has a simple\nnetwork model that is easy for researchers to learn and train.\nAlthough there have been many iterations of YOLO, YOLOV5\nremains the most widely used version across all domains ( Lang\net al., 2022 ). For example, Chen et al. (2022) added the SE module to\nYOLOV5 and replaced the original loss function GIOU with EIOU\nto automatically identify diseases on rubber trees, ﬁnally the average\naccuracy was improved by 5.4% compared to the original YOLOV5.\nWith the aim of improving the accuracy of vegetable disease\ndetection in natural environments, Li (2022) improved the CSP,\nFPN, and NMS modules in YOLOV5s, and ﬁnally achieved a\nmAP of up to 93.1% on a dataset containing a total of 1,000\nimages of ﬁve diseases. In order to accu rately identify and locate\ntomatoes, Li et al. (2023) optimized YOLOV5 by adding the\nCARAFE module to obtain a larger sensory ﬁeld while\nmaintaining lightness, introducing EIOU and quality focal loss\nto solve the problem of uneven samples, and ﬁ\nnally proposing\nYOLOv5s-CQE. The mAP 0.5of YOLOv5s-CQE on the dataset\ncontaining 3,820 tomato images ﬁnally reaches 98.68%.\nTherefore, YOLOV5 shows excellent detection accuracy and fast\nprocessing speed in a series of target detection tasks and shows\ngreat potential in the automatic identi ﬁcation and classi ﬁcation of\napple leaf diseases.\nIn this study, aiming to achieve accurate detection of three\ncommon apple leaf diseases in the natural environment, YOLOV5\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org02\nwas selected as the baseline model, and YOLOV5-CBAM-C3TR\nwas ﬁnally proposed by adding different attention mechanisms and\nC3TR modules and transformer encoders individually or jointly.\nThe speciﬁc objectives of this study are as follows: (1) A proposal for\nan improved YOLOV5 method based on CBAM and C3TR\nmodules for the identi ﬁcation of three apple leaf diseases\nincluding Alternaria blotch, Grey spot, and Rust; (2) Comparison\nof the performance of YOLOV5-CBAM-C3TR, SSD, YOLOV3,\nYOLOV4, YOLOV5 and other different target detection models\non the same dataset containing three diseases; (3) Comparison of\nthe performance improvement of YOLOV5 with the addition of\nCBAM, SE, ECA and C3TR modules, individually or in\ncombination; (4) development of a model for the effective\nclassiﬁcation of similar apple leaf diseases. As far as we know, this\nis the ﬁrst time that the YOLOV5-CBAM-C3TR model has been\nused for the identi ﬁcation and localization of apple leaf diseases.\n2 Materials and methods\n2.1 Datasets\nIn this study, the images were collected from the publicly\navailable apple leaf pathology image dataset ( https://aistudio.\nbaidu.com/datasetdetail/11591 ). Disease images in natural\nenvironments in the dataset were obtained from a real apple\norchard in Yantai, Shandong Province, China. A total of 390\nhigh-quality images of three common apple leaf diseases were\nselected for study in this dataset. However, the original images\ncannot be trained, validated, and tested directly. Images used for\ntarget detection need to determine the location of the target in the\ndataset image, which requires the researcher to label the observed\ntargets before starting training, validation, and testing ( Wang and\nZhao, 2022 ). The dataset used in this study was in YOLO format\nand manually labeled for apple leaf diseases using image annotation\nsoftware. In order to facilitate model training, the labeled images\nwere divided into training, validation, and test sets in a ratio of 8:1:1.\nIn addition, to better adapt the model to different environments and\nto reduce the negative effects of photometric distortion during\ntraining ( Zhu et al., 2021 ), data enhancements such as random\ncropping, panning, changing luminance, adding noise, rotating, and\nmirroring were chosen to extend the dataset. Finally, the dataset\nrequired for the experiment consisted of 3900 images of apple leaves\ncontaining the three diseases, 1680 from the laboratory background\nand 2220 from the orchard background, as shown in Table 1 .\n2.2 Methods\n2.2.1 YOLOV5\nThe aim of this study is to achieve real-time detection and\naccurate identiﬁcation of apple leaf diseases. Considering the type of\ndisease detected including early-stage disease, the shape of the\ninfestation is small. Therefore, target detection methods are\nchosen for identi ﬁcation. Classical single-stage target detection\nalgorithms such as SSD ( Liu et al., 2016 ), YOLOV3 ( Redmon and\nFarhadi, 2018 ), YOLOV4 ( Bochkovskiy et al., 2020 ), YOLOV5\n(Jocher et al., 2021 ), RetinaNet ( Lin et al., 2017 ) can obtain the\npositional information of the target object for identi ﬁcation and\nlocalization. In this study, YOLOV5 was selected for the detection of\napple leaf diseases. As shown in Figure 1 , the YOLOV5 model\nconsists of four parts: Input, Backbone, Neck, and Head. The main\nwork of each part is as follows:\n1. Input. The Input part of YOLOV5s is preprocessed by\nadding mosaic data enhancement, adaptive anchor frames,\nand adaptive image scaling. The model can extract the\nfeatures better during training and thus shows better results\non the dataset.\n2. Backbone. The Backbone part mainly relies on the Focus,\nC3, and SPP modules to extract features from the input\nimages. The Focus module performs slicing operations on\nthe image before it enters the backbone, thus reducing the\nfeature dimensionality. The C3 module, which consists of\nthree convolutional modules and a bottleneck structure,\nbrings the dual advantages of increased computational\nspeed and reduced parameter complexity. The SPP\nmodule is a pooling module that passes the input features\nin parallel to the Maxpool pooling layer to obtain a set of\nfeature maps of different sizes, and ﬁnally joins these\nfeature maps together so that feature information at\ndifferent scales can be captured. The backbone is\nresponsible for passing the extracted position and\ncategory information to the Neck layer.\n3. Neck. The Neck part of the YOLOV5s combines up-\nsampling and down-sampling to generate a feature\npyramid that improves the detection accuracy of the target\nobject, which on one hand needs to reprocess the extracted\nfeatures in the backbone network and on the other hand\nplays an important role in the subsequent detection.\n4. Head. The Head part is to classify and predict the results of\nthe neck layer by using a 1 /C2 1 convolutional layer to\nTABLE 1 Apple leaf disease dataset.\nDiseases Training Set Validation Set Test Set Total\nAlternaria blotch 1040 130 130 1300\nGrey spot 1040 130 130 1300\nRust 1040 130 130 1300\nTotal 3120 390 390 3900\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org03\ngenerate batch size different three results for ﬁnal\ntarget detection.\n2.2.2 CBAM module\nWhen detecting apple leaf disea ses, intricate background\nenvironments can cause interference, which can affect the\naccuracy of disease recognition. To address this challenge,\nintegrating the attention mechanism becomes a promising\nsolution that enhances the model ’s ability to selectively focus on\nrelevant features while ﬁltering out irrelevant information. As\nshown in Figure 2 , the convolutional block attention mechanism\n(CBAM) ( Woo et al., 2018 ) consists of two key components: the\nchannel attention module (CAM) and the spatial attention module\n(SAM). The CAM emphasizes the key features, while the SAM\nemphasizes the spatial localization of these key features. The\noperation of the CAM consists of extracting features through\naverage pooling and maximum pooling respectively. These\nfeatures are then processed separately through a MLP network,\nand ﬁnally summed and output the feature vector. The\nmathematical formulation of the Channel Attention Module was\nshown in Equation 1 .\nMc(F)= s(MLP(AvgPool(F)) + MLP(MaxPool( F))) (1)\nwhere s is a nonlinear sigmoid function used to map inputs to\ncontinuous outputs between 0 and 1. F is the input feature map, the\nMLP consists of two linear layers and a ReLU activation function.\nSAM generates the spatial attention map by splicing the features\nthat are average pooled and maximum pooled in the channel\ndimension. The formulation of the Spatial Attention Module was\nFIGURE 2\nSpeciﬁc structure of the CBAM module.\nFIGURE 1\nOverall structure of YOLOV5-CBAM-C3TR.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org04\nshown in Equation 2 .\nMs(F)= s f 7/C2 7 Concat AvɡPool(F), Maxpool(F)ðÞðÞ\n/C0/C1\n(2)\nWhere s denotes the sigmoid function, f 7/C2 7 represents a\nconvolutional kernel size of 7 /C2 7, and Concat denotes the\nconnection operation.\n2.2.3 C3TR module\nIn recent years, the transformer ( Vaswani et al., 2017 )\narchitecture has been widely used in the ﬁeld of natural language\nprocessing (NLP) with resounding success. As with NLP, where\nlarge amounts of textual data are key to training, the ﬁeld of\ncomputer vision also relies on large image libraries for effective\nmodel learning. The transformer module can acquire complex\nrelationships between different locations in the image. The\nmultiple attention mechanism in the transformer module helps to\nextract multi-scale information, which can focus on both location\nand feature information and has great research potential. In order to\nrealize the application of transformer in the ﬁeld of computer\nvision, researchers endeavor to replace certain convolutional\nstructures with transformer. For example, in target detection\ninvolving a drone capture scene, Zhu et al. (2021) innovatively\nintegrated the transformer block into the C3 module of the\nYOLOV5 architecture, resulting in the C3TR module. As shown\nin Figure 1 , richer image information extraction is achieved by\nreplacing the bottleneck module in the C3 module.\nThe transformer block serves as the fundamental constituent\nwithin the C3TR framework, adopting the classical transformer\nencoder architecture. Illustrated in Figure 3, this block is comprised\nof three primary layers: Flatten, Multi-head attention, and\nfeedforward neural network (FFN).\n1. Flatten\nThe Flatten operation is to ﬂatten the two-dimensional feature\nvectors obtained by the model based on the image into one-\ndimensional vectors, which can preserve the positional\ninformation of the image. If an input feature map X ∈ RH/C2 W/C2 C\nis given, it will become X1 ∈ RH/C2 C after the spreading operation,\nwhere H×W=H.\n2. Multi-head attention\nThe multiple attention operation is responsible for different\nlinear mappings through the Flatten and LayerNorm, allowing\nsimultaneous attention to feature information at different scales.\nAfter converting the feature maps into Q, K, V ∈ RN/C2 C as inputs\nfor multi-head attention, each single head performs one feature\nmapping for Q, k, V. The output formula after the completion of the\nsingle-head attention operation was shown in Equation 3 and\nEquation 4 .\nOutputi = SiVi (3)\nSi = softmax( QiKT\ni ) (4)\nwhere Qi, Ki, Vi denote the multiplication of Q, K, V with the\nweight matrix of the single-head attention mechanism, Si represents\nthe single-head attention matrix, and Outputi refers to the\nintegration of global information. The Outputi generated after\nfeature mapping for each single-head attention will eventually be\nuniﬁed through the connectivity layer to produce the ﬁnal output\nwith expression was shown in Equation 5 .\nOutput = Conca t( Output1, Output2, :::, Outputn) (5)\nwhere n represents the number of multi-head attention.\n3. FFN\nThe FFN layer is a feed forward neural network, which is\ncomposed of two fully connected layers, one of which contains\nthe Relu activation function and the Dropout function between the\ntwo layers. The expression of FFN was shown in Equation 6 .\nFIGURE 3\nDetailed architecture of the transformer block.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org05\nFFN(x)= max(0, xW1 + b1)W2 + b2 (6)\nwhere x represents the feature sequence of the input FFN layer,\nW1 and b1 represent the weights and offsets of the ﬁrst fully\nconnected layer, and W2 and b2 represent the weights and offsets\nof the second fully connected layer, respectively.\n2.2.4 Proposed model\nOrchard environments are extremely complex. Common\nproblems in target object detect ion such as similar texture\nbetween target object and background, target occlusion, and\nsimilarity between target object types. The focus on improving\ndetection accuracy led us to optimize the YOLOV5 framework. This\nwas done by trying to add CBAM, SE ( Hu et al., 2018 ), ECA ( Wang\net al., 2020 ), and C3TR modules to improve the performance of the\nmodel. Finally, by adding CBAM module before SPP module and\nC3TR module at the last layer of backbone network, the optimized\nYOLOV5-CBAM-C3TR model was proposed. Figure 1 shows the\noverall structure of the optimized model YOLOV5-CBAM-C3TR.\nBefore starting the training, the optimal runtime environment was\ncreated, the input images were resized to 640 /C2 640. After training,\nthe ﬁnal three different dimensions of the detection header\neffectively outputted important information related to the type\nand location of the apple leaf disease.\n2.3 Model training environment\nparameter conﬁguration\nIn this study, the model training environment was built using\nPytorch and GPUs with the parameters shown in Table 2 . The\nadaptive moment estimation (Adam) ( Kingma and Ba, 2014 ) was\nused as the optimizer in the experiments. The input image input size\nwas set to 640×640, obtained by ﬁlling the original image. After\nrepeated experiments, the ﬁnal hyperparameters were set as follows:\nthe initial learning rate was set to 0.0005, the epoch number was set\nto 100, and the batch size was 8. To ensure the fairness of model\ncomparison, the parameters used in this study were consistent.\n2.4 Model evaluation\nIn order to comprehensively assess the performance of the\nmodel in apple leaf disease detection, a set of evaluation metrics\nincluding precision, recall, mAP@0.5, mAP@[0.5:0.95], F1 Score,\nand parameters were chosen. Among them, mAP is the mean\naverage precision, which is the evaluation metric of the main\nmodel in target detection. mAP 0.5and mAP@[0.5:0.95] are\ndistinguished by the difference in the size of the intersection over\nunion (IOU), which determines that mAP@[0.5:0.95] is more\nstringent. In addition, the loss value is used to assess the error\nbetween the predicted and the ground truth. The training loss\nreﬂects the model ’s ability to ﬁt on that dataset, and the validation\nloss reﬂects the model ’s ability to generalize. The loss value contains\nthree parameters: obj_loss (object loss), cls_loss (classi ﬁcation loss),\nbox_loss (bounding loss). The above metrics were calculated in\nEquation 7 , Equation 8 , Equation 9 , Equation 10 , and Equation 11 .\nPrecision = TP\nTP + FP (7)\nRecall = TP\nTP + FN (8)\nF1 Score = 2 /C2 Precision /C2 Recall\nPrecision + Recall (9)\nmAP = o\nC\ni=1APi\nC (10)\nloss = box_loss + obj _ loss + cls _ loss (11)\nwhere TP, FP , FN , and TN stand for true positive, false\npositive, false negative, and true negative, respectively. The APi  i s\nthe average precision value at the i-th species. C is the total number\nof species.\n3 Results\n3.1 Model optimization\nTo further improve the detection precision of YOLOV5 for\napple leaf diseases, different modules including SE, CBAM, ECA,\nand C3TR were added to improve the detection capability of\nYOLOV5. As can be seen from Table 3 ,c o m p a r e dw i t ht h e\noriginal YOLOV5 model, the improved YOLOV5-CBAM-C3TR\nachieved 73.4%, 40.9%, 70.9%, and 69.5% of mAP@0.5, mAP@\n[0.5:0.95], precision and recall, which was a signi ﬁcant\nimprovement in detection performance. In addition, the\nexperimental results also showed that the improved YOLOV5-\nCBAM-C3TR is more suitable for the detection of apple leaf\ndiseases in real and complex environments.\n3.1.1 Model performance optimization by adding\nan individual module\nAs can be seen from Table 4 , the low accuracy of YOLOV5 in\ndetecting apple leaf diseases may be caused by the fact that the two\ndiseases including Alternaria blotch and Grey spot, which were too\nsimilar. Therefore, to make the model more focused on extracting\nthe characteristics of apple leaf disease, different attention\nTABLE 2 Software and hardware environment resource conﬁguration.\nConﬁguration Parameter\nOperating system Ubuntu 20.04\nGPU NVIDIA GeForce RTX 3090\nCPU Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz\nMemory 100 GB\nLanguage Python 3.8\nFramework Pytorch 1.10.0\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org06\nmechanisms were tried to be added to the backbone network of\nYOLOV5 for experiments. As shown in Table 4, the addition of SE,\nECA, CBAM and C3TR all improved the accuracy of the YOLOV5\nmodel for detecting apple leaf diseases while keeping the number of\nmodel parameters relatively constant. The SE module allows the\nmodel to better focus on feature channels that are effective for apple\nleaf disease identi ﬁcation. Compared to YOLOV5, the addition of\nthe SE model resulted in an improvement of 3.39%, 5.73%, -4.75%,\n-5.3%, and -5.1% in mAP@0.5, mAP@[0.5:0.95], precision, recall,\nand F1 score, respectively. The ECA module calculates the\ncorrelation of the feature channels so that the model focuses\nmore on the desired feature channels. Compared to YOLOV5, the\naddition of the ECA model resulted in an improvement of 3.69%,\n6.88%, 5.65%, -2.12%, and 1.5% in mAP@0.5, mAP@[0.5:0.95],\nprecision, recall, and F1 score, respectively. Unlike the SE and ECA\nmodules, The CBAM module extracts features by focusing on the\nchannel and spatial information of the image. Compared to\nYOLOV5, the addition of the CBAM model resulted in an\nimprovement of 7.08%, 12.03%, -7.43%, -7.72%, and -7.65% in\nmAP@0.5, mAP@[0.5:0.95], precision, recall, and F1 score,\nrespectively. The transform er module in the C3TR module\ncaptures global contextual information, which improved the\nmAP@0.5, mAP@[0.5:0.95], precision, recall, and F1 score of\nthe C3TR module by 6.34%, 15.2%, 8.9%, 2.7%, and 5.7% over\nthe YOLOV5 model, respectively. Adding modules can improve the\naccuracy of the model in detecting target objects, but it also\nincreases the number of parameters of the model, which is not\nconducive to the lightweight deployment of the model. By observing\nthe change in the number of model parameters when each module\nacts alone. It was found that the addition of the SE module severely\nincreased the number of model parameters, while the addition of\nthe other four modules had little effect on the number of\nmodel parameters.\n3.1.2 Model performance optimization by adding\nmultiple modules\nAs can be seen from Table 5 , adding the modules individually\nall improved the detection accuracy of the YOLOV5 model. In\norder to further improve the feature extraction ability of the model,\nthe attention mechanism was combined with the C3TR module.\nThe combination experiments of CBAM+C3TR, ECA+C3TR, and\nSE+C3TR were conducted respectively. Table 3 shows that\ncombining two modules improved the detection accuracy of the\nYOLOV5 model better than adding a single module. Compared\nwith the addition of SE and C3TR alone, the mAP@0.5, mAP@\n[0.5:0.95], precision, recall, and F1 score of the YOLOV5-SE -C3TR\nwere improved by 3.7% and 0.8%, 10.8% and 1.7%, 6.2% and -7.1%,\nand 8.6% and 0.1%, 7.4% and -3.5%, respectively. Compared with\nthe addition of the ECA module and C3TR module alone, the\nmAP@0.5, mAP@[0.5:0.95], precision, recall, and F1 score of\nYOLOV5-ECA -C3TR were improved by 3.0% and 0.4%, 7.8%,\nand 0.0%, -1.1% and -4.1%, 5.4% and 0.4%, and 2.2% and -1.8%,\nrespectively. Compared with the addition of the CBAM module and\nC3TR module alone, the mAP@0.5, mAP@[0.5:0.95], precision,\nrecall, and F1 score of the YOLOV5-CBAM-C3TR were improved\nby 1.1% and 1.8%, 4.6% and 1.7%, 13.8% and -3.2%, 13.9%, and\nTABLE 4 Comparison of model performance improvement by adding a single module.\nMethods mAP@0.5\n(%)\nmAP@ [0.5:0.95] (%) Precision\n(%)\nRecall\n(%)\nF1 Score Parameters\nYOLOV5 67.8 34.9 67.3 66.1 66.7 7018216\nYOLOV5-SE 70.1 36.9 64.1 62.6 63.3 7542504\nYOLOV5-ECA 70.3 37.3 71.1 64.7 67.7 7018217\nYOLOV5-CBAM 72.6 39.1 62.3 61.0 61.6 7051627\nYOLOV5-C3TR 72.1 40.2 73.3 67.9 70.5 7060072\nThe performance of the indicators is best shown in bold.\nTABLE 3 Comparison with different target detection models.\nMethods mAP@0.5\n(%)\nmAP@ [0.5:0.95]\n(%)\nPrecision\n(%)\nRecall\n(%)\nSSD 66.2 46.9 96.8 36.9\nYOLOV3 65.1 33.3 64.3 63.4\nYOLOV4 52.4 18.6 87.9 25.1\nYOLOV5 67.8 34.9 67.3 66.1\nMGA-YOLOV5 69.0 34.2 74.4 64.3\nBTC-YOLOV5 72.0 39.8 70.7 67.9\nYOLOV5-CBAM-C3TR 73.4 40.9 70.9 69.5\nThe performance of the indicators is best shown in bold.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org07\n2.4%, and 14% and -0.4%, respectively. Overall, combining SE,\nECA, and CBAM with C3TR all further improved mAP@0.5,\nmAP@[0.5:0.95], and recall with little parameter change\ncompared to adding each module individually. Although the\naddition of multiple modules resulted in a decrease in accuracy\nand F1 score metrics compared to the addition of the C3TR module\nalone, YOLOV5-CBAM-C3TR had the smallest decrease and the\nlargest increase, achieving almost positive growth and being the best\nperforming model. The added SE or ECA modules need to capture\nchannel information, while the transformer module in C3TR needs\nto capture context information. The reason for the accuracy\ndegradation may be the mutual interference between multiple\nmodules leading to inadequate feature extraction. On the other\nhand, the CBAM module, which focuses on both channel and\nspatial dimension information, interoperates with the C3TR\nmodule to better ensure that suf ﬁcient feature information is\nprovided to the model.\n3.2 Model training\nSeven target detection models including YOLOV3, YOLOV4,\nYOLOV5, SSD, MGA-YOLOV5, BTC-YOLOV5, and optimized\nYOLOV5-CBAM-C3TR were established based on labeled apple\nleaf disease images. Figure 4A shows the graph of training loss\nvalues for each model with increasing epoch values in apple leaf\ndisease detection. In general, the loss functions of each model\ndecreased with increasing epochs and eventually stabilized. The\nSSD model had the fastest convergence of the training loss curve,\nbut also had the largest loss value after stabilization, which reached\nfull convergence after 10 epochs. The loss functions of the other six\nmodels gradually stabilized after 60 epochs of training. Among\nthem, MGA-YOLOV5 had the second-highest training loss value\nafter the training loss function gradually stabilized. The loss\nfunctions of YOLOV5-CBAM-C3TR, BTC-YOLOV5, YOLOV3,\nand YOLOV4 were very similar, with slightly higher stabilized\nloss values than those of YOLOV5. YOLOV5 has the lowest training\nloss value of all the models.\nFigure 4B shows a plot of the validation loss function with\nincreasing epoch values. As with the training loss function curve,\nthe SSD model still had the fastest convergence rate and stabilized\nafter 10 epochs, while the validation loss value was the highest. The\nother six target detection models all stabilized around the 30th\nepoch. Speci ﬁcally, YOLOV5 and MGA-YOLOV5 had the similar\nloss function curves after stabilization, with the second highest loss\nfunction value. The YOLOV3, YOLOV4, BTC-YOLOV5, and\nYOLOV5-CBAM-C3TR had also the similar loss curves after\nstabilization, with slightly higher stabilized loss values than that\nof YOLOV5, which had the lowest training loss value among\nall models.\n3.3 Comparative analysis with different\ndetection models\nThe objective of this study is to propose a target detection model\ncapable of accurately identifying and locating apple leaf diseases,\nwhich can assist the disease precision spraying device for automatic\nspraying. To verify the effectiveness of YOLOV5-CBAM-C3TR in\ndetecting apple leaf diseases, it was compared with SSD, YOLOV3,\nYOLOV4, YOLOV5, MGA-YOLOV5 and BTC-YOLOV5 models\non the same dataset. The results in Table 3 showed that the mAP\n0.5and mAP@[0.5:0.95] of YOLOV4 were the lowest with 52.4%\nand 18.6% respectively. While the mAP 0.5and mAP@[0.5:0.95] of\nYOLOV5-CBAM-C3TR were the highest with 73.4% and 40.9%\nrespectively. The precision of SSD was up to 96.8% and the recall\nwas only 36.9%, indicating that SSD was accurate in detecting apple\nleaf diseases, but there were omissions in disease identi ﬁcation.\nSimilarly, YOLOV4 had large variations in precision and recall,\nresulting in a poor mAP 0.5 In contrast, YOLOV3, YOLOV5,\nMGA-YOLOV5, and BTC-YOLOV5 can balance the precision\nand recall metrics better, with mAP 0.5of 65.1%, 67.8%, 69%, and\n72%, respectively. Overall, compared with YOLOV5, the optimized\nYOLOV5-CBAM-C3TR showed a signi ﬁcant improvement in\ndetection precision, with an 8.25% improvement in mAP@0.5\nand a 17.2% improvement in mAP@[0.5:0.95]. In addition, the\nexperimental results also con ﬁrms that the optimized YOLOV5-\nCBAM-C3TR has a high detection accuracy, which is suf ﬁcient for\npractical needs.\n3.4 Performance of the improved model in\napple leaf disease detection\nTo further validate the effectiveness of the improved model, the\noriginal YOLOV5 model and the optimized YOLOV5-CBAM-\nC3TR model were selected for the comparison of detection results\nin real environments. A total of 208 sample images with natural\nenvironment backgrounds were selected in the test set to examine\nthe detection effect of YOLOV5-CBAM-C3TR in real scenes.\nTable 6 shows that YOLOV5-CBAM-C3TR improves the correct\nrecognition rate of the three apple leaf diseases compared to\nTABLE 5 Comparison of model performance improvement by adding combinations of models.\nMethods mAP@0.5\n(%)\nmAP@ [0.5:0.95] (%) Precision (%) Recall (%) F1 Score Parameters\nYOLOV5-SE -C3TR 72.7 40.9 68.1 68.0 68.0 7092840\nYOLOV5-ECA -C3TR 72.4 40.2 70.3 68.2 69.2 7060073\nYOLOV5-CBAM-C3TR 73.4 40.9 70.9 69.5 70.2 7093483\nThe performance of the indicators is best shown in bold.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org08\nYOLOV5, with a signi ﬁcant increase of 18.9% in the average\naccuracy. Figure 5 shows a comparison of typical detection results\nfor the three apple leaf diseases. The results show that the YOLOV5\nalgorithm has errors in detecting the three apple leaf diseases in a\nnatural environment with a complex background, and the main\nreason for the unsatisfactory detection results is its inaccurate\nfeature extraction of the diseases. As can be seen in Figure 6 ,\nYOLOV5-CBAM-C3TR is able to extract the features of various\ndiseases better, but there is still a risk of misjudging Alternaria\nblotch and Grey spot, which are two similar diseases. The possible\nreason for this is that these two diseases are very similar after data\nenhancement in the simulated natural environment. In this study,\nB\nA\nFIGURE 4\n(A) Curve of training loss values with epoch values,(B) Curve of validation loss values with epoch values. Due to the different scales of change in loss\nfunction values for SSD and other models, a double Y-axis is used to represent the change in loss function for each model. The left axis represents\nthe scale of variation of the loss function values for the YOLOV3, YOLOV4, YOLOV5, BTC-YOLOV5, MGA-YOLOV5, and YOLOV5-CBAM-C3TR\nmodels, and the right axis represents the scale of variation of the loss function values for the SSD model. the loss function curves for the SSD model\nare shown in bold red.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org09\nthe CBAM module and the C3TR module were added to YOLOV5,\nand the two modules work together to enable YOLOV5 to better\nextract disease features.\n4 Discussion\nIn this study, attention mechanism and module with the\ntransformer encoder were added to optimize YOLOV5, and\nﬁnally proposed YOLOV5-CBAM-C3TR to accurately classify\nthree common diseases of apple leaves. Comparing with the\ntarget detection algorithms such as SSD, YOLOv3, YOLOv4, and\nYOLOv5, YOLOV5-CBAM-C3TR had the highest mAP@0.5 and\nmAP@[0.5:0.95], which reached 73.4% and 40.9%, respectively. An\naverage accuracy of 92.4% was achieved on a randomly selected\nsample of 208 images containing the three apple leaf diseases.\nEmpirical results showed that adding CBAM, SE, ECA, and C3TR\nindividually or in combination can signi ﬁcantly improve the\ndetection accuracy of YOLOV5. In contrast, combining each\nattention mechanism with the C3TR module has a higher\nB\nC\nA\nFIGURE 5\n(A) Original image,(B) YOLOV5 detection results,(C) YOLOV5-CBAM-C3TR detection results. Different colored bounding boxes are used in the\nimages to distinguish the types of apple leaf diseases, with Alternaria blotch in red, Grey spot in pink and Rust in orange. The names of apple leaf\ndiseases from theﬁrst to the third column in the image are: Alternaria blotch, Grey spot, and Rust.\nTABLE 6 Test results of improved models in detecting apple leaf diseases.\nModel\nNumber of\nAlternaria\nBlotch Samples\nNumber of\nCorrectly\nDetected\nNumber of\nGrey\nSpot Samples\nNumber of\nCorrectly\nDetected\nNumber of\nRust\nSamples\nNumber of\nCorrectly\nDetected\nAverage\nAccuracy\n(%)\nYOLOV5 87 52 67 54 54 50 77.7\nYOLOV5-\nCBAM-\nC3TR\n87 81 67 60 54 51 92.4\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org10\ndetection accuracy than adding each module separately. Among\nthem, the combination of CBAM and C3TR provided the most\nsigniﬁcant performance enhancement for YOLOv5. Different from\nSE or ECA modules, CBAM module pays attention to both channel\ninformation and spatial information, and can better cooperate with\nC3TR for global information extr action. Certainly, Attention\nmechanisms have been shown to be effective in many tasks ( Xue\net al., 2021 ; Wang et al., 2022 ; Zhao et al., 2022 ). However, the task\nrequirements in different scenarios should be carefully considered\nwhen choosing the appropriate attention module, which suggests\nthat the selection of modules requires extensive experimentation.\nAlthough adding modules can improve the detection accuracy of\ntarget objects, it also increases the number of parameters of the\nmodel, which is not conducive to the actual deployment of the\nmodel. The addition of CBAM, ECA, SE, and C3TR in this study\nincreased the number of parameters in YOLOV5. Future research\nwill consider methods to reduce the number of parameters while\nmaintaining model detection accuracy, such as pruning ( Liang et al.,\n2022) and distillation, in order to achieve a good balance between\nmodel detection accuracy and the number of parameters.\nTimely detection and control of apple leaf diseases is extremely\nimportant. Since different apple leaf diseases may have similar\ncharacteristics, even the human eye cannot distinguish them\naccurately after exposure and other treatments that simulate the\nnatural environment. The experimental data in Table 6 showed that\nYOLOV5-CBAM-C3TR improved the two types of diseases\nincluding Alternaria blotch and Grey spot, by 33.33% and 8.95%,\nrespectively. The average accuracy achieved 92.1% for the three\ntypes of diseases. The experimental data af ﬁrmed the ability of the\noptimized model to accurately identify similar diseases. However,\nfactors such as the number of disease types in the data set, the\ncomplexity of the environment of the objects to be detected in the\nimage, and the difference in categories of the objects to be detected\nwill affect the accuracy of the network model detection to some\nextent. Therefore, the selection of detection accuracy to evaluate the\nmodel performance should be combined with speci ﬁc application\nscenarios. For example, Khan et al. (2022) developed an automated\napple leaf disease detection system based on deep learning. The\nexperimental results show that on a dataset containing more than\n9000 images, Faster-RCNN can reach 42.01% mAP at 6FPS,\nshowing a good detection accuracy for 9 common apple leaf\ndiseases. The model is tested on data set images that are less\ndisturbed by the real background environment, and its robustness\nis low in the real environment. The experimental results also\nindicate that the model is not effective in detecting diseased\nleaves. When using a target detection model to detect apple leaf\ndiseases, Zhang et al. (2023) found that the MFaster R-CNN model\ncould achieve 97.23% mAP for eight kinds of corn leaf diseases, but\nonly 80.69% mAP on a self-built data set of apple leaf diseases. The\nabove examples show that the accuracy evaluation of the model\nshould be combined with the speci ﬁc application scenarios of the\nmodel. In different tasks and different data sets, the model will show\ndifferent detection performance. Only 3 kinds of apple leaf diseases\nwere considered in this experiment, while there are more than 200\nkinds of apple leaf diseases. Therefore, although YOLOV5-CBAM-\nC3TR can accurately identify apple leaf diseases similar to those in\nthe classi ﬁcation data set, it may not be universally applicable to\nother similar diseases. It is necessary to expand the data set of apple\nleaf disease and collect more comprehensive types of leaf disease for\nresearch. In addition, the model proposed in this study needs to be\ncompared with more advanced object detection algorithms such as\nYOLOV7 and YOLOV8. These questions will be further explored in\nFIGURE 6\nConfusion matrix for the detection of three apple leaf diseases.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org11\nthe future to improve the accuracy of the model ’s detection of\ndifferent apple leaf diseases so that each class of similar diseases can\nbe accurately classi ﬁed.\n5 Conclusions\nYOLOV5-CBAM-C3TR algorithm was proposed to improve\nthe accuracy of detection of three apple leaf diseases including\nAlternaria blotch, Grey spot, and Rust. The model was obtained by\noptimizing YOLOV5 with the addition of an attention mechanism\nand a module with a transformer encoder. Compared with different\ntarget detection models, the optimized YOLOV5-CBAM-C3TR\nalgorithm achieved the highest detection accuracy than other\nmodels, with mAP@0.5, mAP@[0.5:0.95], precision, recall of\n73.4%, 40.9%, 70.9%, 69.5%, respectively. In randomly selected\napple leaf disease samples, the average accuracy based on the\nYOLOV5-CBAM-C3TR model can reach 92.4%, which was\n18.9% higher than that of the original YOLOV5. Moreover, the\nYOLOV5-CBAM-C3TR model also showed a strong ability to\nidentify similar diseases, and could accurately identify Alternaria\nblotch and grey spot, which are almost indistinguishable from the\nnaked eye. In the future, YOLOV5-CBAM-C3TR can also be\nextended to detect similar diseases in other crops.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nML: Data curation, Formal analysis, Investigation,\nMethodology, Software, Validation, Visualization, Writing –\noriginal draft. W-HS: Conceptua lization, Funding acquisition,\nMethodology, Project administration, Resources, Supervision,\nWriting – review & editing.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This study\nwas supported by the National Natural Science Foundation of\nChina (Grant No. 32101610; 32371991).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAbade, A., Ferreira, P. A., and de Barros Vidal, F. (2021). Plant diseases recognition\non images using convolutional neural networks: A systematic review. Comput. Electron.\nAgric. 185, 106125. doi: 10.1016/j.compag.2021.106125\nBabu, V. S., and Ram, N. V. (2022). Deep residual CNN with contrast limited\nadaptive histogram equalization for weed detection in soybean crops. Traitement du\nSignal 39, 717. doi: 10.18280/ts.390236\nBochkovskiy, A., Wang, C.-Y., and Liao, H.-Y. M. (2020). Yolov4: Optimal speed and\naccuracy of object detection. arXiv preprint arXiv 2004, 10934. doi: 10.48550/\narXiv.2004.10934\nChen, Z., Wu, R., Lin, Y., Li, C., Chen, S., Yuan, Z., et al. (2022). Plant disease\nrecognition model based on improved YOLOv5. Agronomy 12, 365. doi: 10.3390/\nagronomy12020365\nChuanlei, Z., Shanwen, Z., Jucheng, Y., Yancui, S., and Jia, C. (2017). Apple leaf\ndisease identi ﬁcation using genetic algorithm and correlation based feature selection\nmethod. Int. J. Agric. Biol. Eng.10, 74 – 83. doi: 10.25165/ijabe.v10i2.2166\nDhaka, V. S., Meena, S. V., Rani, G., Sinwar, D., Kavita,, Ijaz, M. F., et al. (2021). A\nsurvey of deep convolutional neural networks applied for prediction of plant leaf\ndiseases. Sensors 21, 4749. doi: 10.3390/s21144749\nDubey, S. R., and Jalal, A. S. (2012). “Detection and classi ﬁcation of apple fruit\ndiseases using complete local binary patterns, in: 2012 third international conference\non computer and communication technology, ” in Presented at the 2012 3rd\nInternational Conference on Computer and Communication Technology (ICCCT\n2012), (Allahabad, India: IEEE). 346 – 351.\nGargade, A., and Khandekar, S. (2021). “Custard apple leaf parameter analysis, leaf\ndiseases, and nutritional de ﬁciencies detection using machine learning, ” in Advances in\nSignal and Data Processing, Lecture Notes in Electrical Engineering, vol. pp . Eds. S. N.\nMerchant, K. Warhade and D. Adhikari (Singapore: Springer Singapore), 57 – 74.\nHearst, M. A., Dumais, S. T., Osuna, E., Platt, J., and Scholkopf, B. (1998). Support\nvector machines. IEEE Intelligent Syst. their Appl.13, 18 – 28. doi: 10.1109/5254.708428\nHu, J., Shen, L., and Sun, G. (2018). Squeeze-and-excitation networks. Proc. IEEE Conf.\nComput. Vision Pattern Recognition1709, 7132– 7141. doi: 10.1109/CVPR.2018.00745\nJan, M., and Ahmad, H. (2020). Image features based intelligent apple disease\nprediction system: machine learning based apple disease prediction system. IJAEIS 11,\n31– 47. doi: 10.4018/IJAEIS.2020070103\nJocher, G., Stoken, A., Borovec, J., Chaurasia, A., Changyu, L., Hogan, A., et al.\n(2021). ultralytics/yolov5: v5. 0-YOLOv5-P6 1280 models, AWS, Supervise. ly and\nYouTube integrations. Zenodo.\nKaiming, H., Georgia, G., Piotr, D., and Ross, G. (2017). “Mask R-CNN, ” in\nProceedings of the IEEE International C onference on Computer Vision (ICCV)\n(Hawaii, US: Computer Vision Foundation), 2961\n– 2969.\nKhan, A. I., Quadri, S. M. K., Banday, S., and Latief Shah, J. (2022). Deep diagnosis: A\nreal-time apple leaf disease detection system based on deep learning. Comput. Electron.\nAgric. 198, 107093. doi: 10.1016/j.compag.2022.107093\nKingma, D. P., and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv\npreprint arXiv1412, 6980. doi: 10.48550/arXiv.1412.6980\nKundu, N., Rani, G., Dhaka, V. S., Gupta, K., Nayak, S. C., Verma, S., et al. (2021).\nIoT and interpretable machine learning based framework for disease prediction in pearl\nmillet. Sensors 21, 5386. doi: 10.3390/s21165386\nLang, X., Ren, Z., Wan, D., Zhang, Y., and Shu, S. (2022). MR-YOLO: an improved\nYOLOv5 network for detecting magnetic ring surface defects. Sensors 22, 9897. doi:\n10.3390/s22249897\nLi, J. (2022). An improved YOLOv5-based vegetable disease detection method.\nComput. Electron. Agric202. doi: 10.1016/j.compag.2022.107345\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org12\nLi, T., Sun, M., He, Q., Zhang, G., Shi, G., Ding, X., et al. (2023). Tomato recognition\nand location algorithm based on improved YOLOv5. Comput. Electron. Agric. 208,\n107759. doi: 10.1016/j.compag.2023.107759\nLiang, X., Jia, X., Huang, W., He, X., Li, L., Fan, S., et al. (2022). Real-time grading of\ndefect apples using semantic segmentation combination with a pruned YOLO V4\nnetwork. Foods 11, 3150. doi: 10.3390/foods11193150\nLin, T.-Y., Goyal, P., Girshick, R., He, K., and Dolla ́ r, P. (2017). “Focal loss for dense\nobject detection, ” in Proceedings of the IEEE International Conference on Computer\nVision (Hawaii, US: Computer Vision Foundation), 2980 – 2988.\nLiu, S., Qiao, Y., Li, J., Zhang, H., Zhang, M., and Wang, M. (2022). An improved\nlightweight network for real-time detection of apple leaf diseases in natural scenes.\nAgronomy 12, 2363. doi: 10.3390/agronomy12102363\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., et al. (2016). Ssd:\nSingle shot multibox detector, in: Computer Vision – ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11 – 14, 2016, Proceedings, Part I\n14. Springer 9905, 21 – 37. doi: 10.1007/978-3-319-46448-0_2\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). “You only look once:\nuniﬁed, real-time object detection, in: 2016 IEEE conference on computer vision and\npattern recognition (CVPR), ” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR)(Computer Vision Foundation), Vol. pp. 779 –\n788(Las Vegas, NV, USA).\nRedmon, J., and Farhadi, A. (2018). YOLOv3: an incremental improvement\n(arxiv:1804.02767). doi: 10.48550/arXiv.1804.02767\nRen, S., He, K., Girshick, R., and Sun, J. (2015). “Faster R-CNN: Towards Real-Time\nObject Detection with Region Proposal Networks, ” in Advances in Neural Information\nProcessing Systems(NIPS 2015): Curran Associates, Inc, 28-2015.\nShi, Y., Huang, W., and Zhang, S. (2017). Apple disease recognition based on two-\ndimensionality subspace learning. Comput. Eng. Appl.53 (22), 180 – 184.).\nShu, C., Zhao, H., Jiao, W., Liu, B., Cao, J., and Jiang, W. (2019). Antifungal ef ﬁcacy\nof ursolic acid in control of Alternaria alternata causing black spot rot on apple fruit\nand possible mechanisms involved. Scientia Hortic. 256, 108636. doi: 10.1016/\nj.scienta.2019.108636\nSingh, S., Gupta, I., Gupta, S., Koundal, D., Mahajan, S., and Pandit, A. (2021). Deep\nlearning based automated detection of diseases from apple leaf images. Computers\nMaterials Continua71, 1849 – 1866. doi: 10.32604/cmc.2022.021875\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you Need, ” in Advances in Neural Information Processing\nSystems (Curran Associates, Inc).\nVishnu, S., and Ranjith, R. A. (2015). Plant disease detection using leaf pattern: A\nreview.\nInt. J. Innovative Science Eng. Technol.2, 774 – 780.\nWang, S., He, D., Li, W., and Wang, Y. (2009). Plant leaf disease recognition based on\nkernel K-means clustering algorithm. Nongye Jixie Xuebao= Trans. Chin. Soc. Agric.\nMachinery 40, 152 – 155.\nWang, H., Li, Y., Dang, L. M., and Moon, H. (2022). An ef ﬁcient attention module for\ninstance segmentation network in pest monitoring. Comput. Electron. Agric. 195,\n106853. doi: 10.1016/j.compag.2022.106853\nWang, Q., Wu, B., Zhu, P., Li, P., Zuo, W., and Hu, Q. (2020). “ECA-Net: Efﬁcient\nchannel attention for deep convolutional neural networks, ” in Proceedings of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition(Seattle, USA: Computer\nVision Foundation). 11534 – 11542.\nWang, Y., and Zhao, J. (2022). MGA-YOLO: A lightweight one-stage network for\napple leaf disease detection. Front. Plant Sci.(Technical Advances in Plant Science), 13-\n022. doi: 10.3389/fpls.2022.927424\nWoo, S., Park, J., Lee, J.-Y., and Kweon, I. S. (2018). “CBAM: Convolutional Block\nAttention Module, ” in Computer Vision – ECCV 2018, Lecture Notes in Computer\nScience. Eds. V. Ferrari, M. Hebert, C. Sminchisescu and Y. Weiss (Cham: Springer\nInternational Publishing), 3 – 19.\nXue, M., Chen, M., Peng, D., Guo, Y., and Chen, H. (2021). One spatio-temporal\nsharpening attention mechanism for light-weight YOLO models based on sharpening\nspatial attention. Sensors 21, 7949. doi: 10.3390/s21237949\nYu, H.-J., and Son, C.-H. (2019). Apple Leaf Disease Identiﬁcation through Region-of-\nInterest-Aware Deep Convolutional Neural Network(arxiv) 64 (2), pp. 20507-1-20507-10.\nZhang, Y., Zhou, G., Chen, A., He, M., Li, J., and Hu, Y. (2023). A precise apple leaf\ndiseases detection using BCTNet under unconstrained environments. Comput.\nElectron. Agric.212, 108132. doi: 10.1016/j.compag.2023.108132\nZhao, S., Liu, J., and Wu, S. (2022). Multiple disease detection method for\ngreenhouse-cultivated strawberry based on multiscale feature fusion Faster R_CNN.\nComput. Electron. Agric.199, 107176. doi: 10.1016/j.compag.2022.107176\nZhong, Y., and Zhao, M. (2020). Research on deep learning in apple leaf disease\nrecognition. Comput. Electron. Agric.168, 105146. doi: 10.1016/j.compag.2019.105146\nZhu, X., Lyu, S., Wang, X., and Zhao, Q. (2021). “TPH-YOLOv5: improved YOLOv5\nbased on transformer prediction head for object detection on drone-captured scenarios,\nin: 2021 IEEE/CVF international conf erence on computer vision workshops\n(ICCVW), ” in Presented at the 2021 IEEE/CVF International Conference on\nComputer Vision Workshops (ICCVW), IEEE ,M o n t r e a l ,B C ,C a n a d a( S o u t h e r n\nCalifornia, USA: Computer Vision Foundation). 2778 – 2788.\nLv and Su 10.3389/fpls.2023.1323301\nFrontiers inPlant Science frontiersin.org13"
}