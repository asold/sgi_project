{
    "title": "Dict-BERT: Enhancing Language Model Pre-training with Dictionary",
    "url": "https://openalex.org/W3207539211",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2124365211",
            "name": "Wenhao Yu",
            "affiliations": [
                "Microsoft (United States)",
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2110288568",
            "name": "Chen-guang Zhu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2125393909",
            "name": "Yu-Wei Fang",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2313445170",
            "name": "Dong-han Yu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2127805350",
            "name": "Shuohang Wang",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2155579404",
            "name": "Yichong Xu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2806698144",
            "name": "Michael Zeng",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2098004101",
            "name": "Meng Jiang",
            "affiliations": [
                "Microsoft (United States)",
                "University of Notre Dame"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034978746",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W2946006146",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3176750236",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W3047312186",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4309417034",
        "https://openalex.org/W2891958973",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2998554035",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2962883166",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2995480165",
        "https://openalex.org/W3090656107",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W2964303159",
        "https://openalex.org/W3202807130",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2966610483",
        "https://openalex.org/W2890560993",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3092288641",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W4287888426",
        "https://openalex.org/W2949759968",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3197849207",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3123123873",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3213435953",
        "https://openalex.org/W2981861606",
        "https://openalex.org/W2964420626",
        "https://openalex.org/W3014521650",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2250539671"
    ],
    "abstract": "Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus. Therefore, the embeddings of rare words on the tail are usually poorly optimized. In this work, we focus on enhancing language model pre-training by leveraging definitions of the rare words in dictionaries (e.g., Wiktionary). To incorporate a rare word definition as a part of input, we fetch its definition from the dictionary and append it to the end of the input text sequence. In addition to training with the masked language modeling objective, we propose two novel self-supervised pre-training tasks on word and sentence-level alignment between input text sequence and rare word definitions to enhance language modeling representation with dictionary. We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets. Extensive experiments demonstrate that Dict-BERT can significantly improve the understanding of rare words and boost model performance on various NLP downstream tasks.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1907 - 1918\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDict-BERT: Enhancing Language Model Pre-training with Dictionary\nWenhao Yu1∗, Chenguang Zhu2, Yuwei Fang2, Donghan Yu3∗,\nShuohang Wang2, Yichong Xu2, Michael Zeng2, Meng Jiang1\n1University of Notre Dame, Notre Dame, IN\n2Microsoft Cognitive Services Research, Redmond, W A\n3Carnegie Mellon University, Pittsburgh, PA\n1{wyu1, mjiang2}@nd.edu; 2chezhu@microsoft.com\nAbstract\nPre-trained language models (PLMs) aim to\nlearn universal language representations by con-\nducting self-supervised training tasks on large-\nscale corpus. Since PLMs capture word seman-\ntics in different contexts, the quality of word\nrepresentations highly depends on word fre-\nquency, which usually follows a heavy-tailed\ndistribution in the pre-training corpus. Thus,\nthe embeddings of rare words on the tail are\nusually poorly optimized. In this work, we fo-\ncus on enhancing language model pre-training\nby leveraging definitions of the rare words in\ndictionary. To incorporate a rare word defini-\ntion as a part of input, we fetch it from the\ndictionary and append it to the end of the in-\nput text sequence. In addition to training with\nthe masked language modeling objective, we\npropose two novel self-supervised pre-training\ntasks on word-level and sentence-level align-\nment between the input text and rare word defi-\nnition to enhance language representations. We\nevaluate the proposed model named Dict-BERT\non the GLUE benchmark and eight specialized\ndomain datasets. Extensive experiments show\nthat Dict-BERT significantly improves the un-\nderstanding of rare words and boosts model per-\nformance on various NLP downstream tasks.\n1 Introduction\nRecently pre-trained language models (PLMs) such\nas BERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019) have revolutionized the field of natural\nlanguage processing (NLP), yielding remarkable\nperformance on various downstream tasks (Qiu\net al., 2020). However, these PLMs suffer from\nlacking knowledge when completing real-world\ntasks. To address this issue, some methods have\nincorporated the knowledge to enrich language rep-\nresentations, ranging from linguistic (Wang et al.,\n* This work was done when Wenhao Yu and Donghan Yu\ninterned at Microsoft Cognitive Services Research group.\n2021a), commonsense (Guan et al., 2020; Liu et al.,\n2020), factual (Wang et al., 2021b), to domain\nknowledge (Liu et al., 2020; Yu et al., 2022b).\nNevertheless, rare words (Schick and Schütze,\n2020) and unseen words (Cui et al., 2021) are\nstill blind spots of pre-trained language models\nwhen they are fine-tuned on downstream tasks. For\ninstance, in a dialogue system, users often talk\nto chatbots about recent hot topics, e.g., “Covid-\n19”, which may not appear in the pre-training cor-\npus (Cui et al., 2021). Since PLMs capture word\nsemantics in different contexts to address the is-\nsue of polysemous and the context-dependent na-\nture of words, consequently they usually perform\npoorly when a user mentions such novel words (Wu\net al., 2021; Ruzzetti et al., 2021). As indicated\nby Wu et al. (2021), the quality of word represen-\ntations highly depends on the word frequency in\nthe pre-training corpus, which typically follows a\nheavy-tail distribution. Thus, a large proportion of\nwords appear very few times and the embeddings of\nthese rare words are poorly optimized (Gong et al.,\n2018; Schick and Schütze, 2020). Such embed-\ndings usually carry inadequate semantic meaning,\nwhich complicate the understanding of input text,\nand even hurt the pre-training of the entire model.\nIn this work, we focus on enhancing language\nmodel pre-training by leveraging rare word defi-\nnitions in English dictionaries (e.g., Wiktionary).\nDefinitions in dictionaries are intended to describe\nthe meaning of a word to a human reader. We\nappend the definitions of rare words to the end\nof the input text and encode the whole sequence\nwith Transformer encoder. The pre-training tasks\nare mainly based on the alignment between input\ntext and the appended word definitions, some of\nwhich are randomly sampled polluted words and\ndon’t explain the input. We propose two types of\npre-training objectives: 1) a word-level contrastive\nobjective aims to maximize the mutual information\nbetween Transformer representations of a rare word\n1907\nappeared in the input text and its dictionary defini-\ntion. 2) a sentence-level discriminative objective\naims at learning to differentiate between correct\nand polluted word definitions. During downstream\nfine-tuning, in order to avoid the appended rare\nword definitions diverting the sentence from its\noriginal meaning, we employ a knowledge atten-\ntion mechanism that makes word definitions only\nvisible to the corresponding words in the input text\nsequence. We name our method Dict-BERT. No-\ntably, Dict-BERT is general and model-agnostic, in\nthe sense that any pre-trained language model (e.g.,\nBERT, RoBERTa) suffices and can be used.\nOverall, our main contributions in this work can\nbe summarized as follows:\n1. We are the first work to enhance language\nmodel pre-training with rare word definitions from\ndictionaries (e.g., Wiktionary).\n2. We propose two novel pre-training tasks on\nword-level and sentence-level alignment between\ninput text sequence and rare word definitions to\nenhance language modeling with dictionary.\n3. We evaluate Dict-BERT on the GLUE (Wang\net al., 2019) benchmark, in which our model pre-\ntrained from scratch can improve accuracy by\n+1.15% on average over the vanilla BERT.\n4. We follow the domain adaptive pre-training\n(DAPT) setting (Gururangan et al., 2020), where\nlanguage models are continuously pre-trained with\nin-domain data. We evaluate Dict-BERT on eight\nspecialized domain datasets. Our method can im-\nprove F1 score by +0.5%/+0.7% on average over\nthe BERT-DAPT/RoBERTa-DAPT settings.\n2 Related Work\nRare word representation in language models.\nThe quality of word representations highly depends\non word frequency creating a heavy-tail distribu-\ntion (Wu et al., 2021). Recent works have shown\nrare words that are not frequently covered in the\ncorpus can hinder the understanding of specific yet\nimportant sentences (Noraset et al., 2017; Bosc and\nVincent, 2018; Schick and Schütze, 2020; Ruzzetti\net al., 2021). Due to the poor quality of rare word\nrepresentations, the pre-training model built on\ntop of it suffers from noisy input semantic signals\nwhich lead to inefficient training. Gao et al. (2019)\nprovided a theoretical understanding of the rare\nword problem, which illustrates that the problem\nlies in the sparse stochastic optimization of neu-\nral networks. Schick and Schütze (2020) adapted\nattentive mimicking to explicitly learn rare word\nembeddings to language models. Wu et al. (2021)\nproposed to maintain a note dictionary and saves a\nrare word’s contextual information as notes. When\nthe same rare word occurs again during language\nmodel pre-training, the note information saved be-\nforehand can be employed to enhance the seman-\ntics of the current sentence. Different from afore-\nmentioned works that keep a fixed vocabulary of\nrare words during pre-training and fine-tuning, our\nmethod can dynamically adjust the vocabulary of\nrare words, obtain and represent their definitions in\na dictionary in a plug-and-play manner.\nLanguage model pre-training and knowledge-\nenhanced methods Recent years have seen sub-\nstantial pre-trained language models (PLMs) such\nas BERT (Devlin et al., 2019) and T5 (Raffel et al.,\n2020) have achieved remarkable performance in\nvarious NLP downstream tasks. However, these\nPLMs suffer from lacking domain-specific knowl-\nedge when completing many real-world tasks (Yu\net al., 2022c). For example, BERT cannot give full\nplay to its value when dealing with electronic med-\nical record analysis tasks in the medical field (Liu\net al., 2020). A lot of efforts have been made\non investigating how to integrate knowledge into\nPLMs (Yu et al., 2022b; Liu et al., 2021; Xiong\net al., 2020; Guan et al., 2020; Zhou et al., 2021;\nYu et al., 2022a,d). Overall, these approaches can\nbe grouped into two categories: The first one is\nto explicitly inject knowledge representation into\nPLMs, where the representations are pre-computed\nfrom external sources (Zhang et al., 2019; Liu et al.,\n2021). However, it has been argued that the em-\nbedding vectors of input words and knowledge are\nobtained in separate ways, making their vector-\nspace inconsistent (Liu et al., 2020). The sec-\nond one is to implicitly model knowledge informa-\ntion into PLMs by performing knowledge-related\ntasks, such as concept order recovering (Zhou et al.,\n2021), entity category prediction (Yu et al., 2022b).\nHowever, none of existing work has explored using\ndictionary to enhance language model pre-training.\n3 Proposed Method\nIn this section, we introduce the details of our\nmodel Dict-BERT. We first describe the notations\nand how to incorporate rare word definitions as a\npart of input. Then we detail the two novel self-\nsupervised pre-training objectives. Finally, we in-\ntroduce the knowledge attention during fine-tuning.\n1908\n3.1 Notation and Problem Definition\nGiven the input text sequence X =\n[CLS, x1, x2, ··· , xL, SEP] with L tokens,\na language model fLM produces the con-\ntextual word representation fLM (X) =\n[hCLS, h1, h2, ··· , hL, hSEP]. For a specific\ndownstream task, a header function fH further\nuses fLM (X) and generates the prediction as\nfH(hCLS) for sequence classification tasks.\nThe goal of our work is to learn better contex-\ntual word representation fLM (x) by leveraging\ndefinitions of the rare words in dictionaries (e.g.,\nWiktionary). Suppose S = [ s1, ··· , sK] and\nC = [c(1), ··· , c(K)] are the sets of rare words in\nthe input text sequence X and their definitions\nin the dictionary. When a rare word si appears\nin the input text sequence, we fetch its definition\nfrom the dictionary as c(i) = [c(i)\n1 , ··· , c(i)\nNi] with\nNi tokens, and append it to the end of the input\ntext sequence. If a word has multiple definitions,\nwe use the definition of their first etymology (i.e.,\nthe most commonly used meaning). Therefore,\nan input sequence X with appended definitions\nof K rare words can be written as: [X; C] =\n[CLS, x1, x2, ..., xL, SEP(1), c(1)\n1 , c(1)\n2 , ..., c(1)\nN1 ; ...;\nSEP(K), c(K)\n1 , c(K)\n2 , ..., c(K)\nNK , SEP], and the cor-\nresponding contextual representation generated\nfrom the language model fLM as: fLM (X, C) =\n[hCLS, h1, h2, ··· , hL, h(1)\nSEP, h(1)\n1 , ··· , h(1)\nN1 ; ······\n; h(K)\nSEP , h(K)\n1 , ··· , h(K)\nNK , hSEP]. For a specific down-\nstream task, a header function fH still uses\nfLM (X, C) to generate the prediction as fH(hCLS)\nfor sequence classification tasks.\n3.2 Choosing the Rare Words\nThere are different ways to choose the rare word\nset S in a pre-training corpus. One way is to use a\npre-defined absolute frequency value as the thresh-\nold. Wu et al. (2021) used 500 as the threshold to\ndivide frequent words and rare words, and main-\ntained a fixed vocabulary of rare words during pre-\ntraining and fine-tuning. However, rare words can\nvary greatly in different corpora. For example, rare\nwords in the medical domain are very different\nfrom those in general domain (Lee et al., 2020).\nBesides, keeping a large threshold for a small down-\nstream datasets makes the vocabulary of rare words\ntoo large. For example, only 51 words in the RTE\ndataset have a frequency of more than 500.\nTherefore, we propose to choose specialized rare\nwords for each pre-training corpus and downstream\ntasks. Specifically, we ranked all word frequency\nfrom smallest to largest, and add them to the list one\nby one until the word frequency of the added word\nreaches 10% of the total word frequency. Com-\npared with Wu et al. (2021) which maintained a\nfixed vocabulary, our method can dynamically ad-\njust the vocabulary of rare words, obtain and rep-\nresent their definitions in dictionary in a plug-and-\nplay manner. To fetch the definition of rare words,\nwe leveraged the largest online dictionary, i.e., Wik-\ntionary, and collected a dump of Wiktionary which\nincludes definitions of 999,614 concepts.\nWe noted that when choosing the rare words, we\nused a word tokenizer (i.e., NLTK) instead of using\nany subword tokenizer (e.g., WordPiece). This is\nmainly because quite a few rare subwords, either\ngenerated by BPE or in WordPiece, do not have spe-\ncific understandable semantic meanings to humans,\nsuch as “123@@”, “elids”, “al”, “ch”, “di”. For\nsuch subwords, their contexts can be very diverse\ndue to their vague semantic meanings. As most\nrare words have their own concrete semantics, the\nsubword meanings cannot act as effective auxiliary\nsemantics to enhance the current input.\n3.3 Dict-BERT: Language Model Pre-training\nwith Dictionary\nDict-BERT is based on the BERT architecture,\nwhich can be initialized either randomly or from a\npre-trained checkpoint with the same structure. It\nis worth noting that we slightly modified the type\nembedding, in which the type embedding of the\ninput text is set as 0, and the type embedding of the\ndictionary definitions is set as 1. In addition, we\nused the absolute positional embedding.\nWe represent each input text sequence and dic-\ntionary definitions pair as a tuple (X, C). The se-\nmantics of a word in the input text depends on the\ncurrent context, while the semantics of a word in\nthe dictionary is standardized by linguistic experts.\nIn order to better align the representations between\nthem, we propose two novel pre-training tasks on\nword-level and sentence-level alignment between\ninput text sequence and rare word definitions to en-\nhance pre-trained language models with dictionary.\n3.3.1 Word-level Mutual Information\nMaximization\nRecently, there has been a revival of approaches in-\nspired by the InfoMax principle (Oord et al., 2018;\nTschannen et al., 2020): maximizing the mutual\n1909\nmax(       )\ninput text (with masked tokens)definition of Covid-19 definition of SARS[CLS] Covid-19 has become a global epidemic [SEP] Covid-19 is the disease caused by severe acute respiratory[SEP] SARS … [SEP]\n[CLS]   [MASK]   has   become    a    [MASK] epidemic\nCovid-19\nToken EmbPos. EmbType Emb\nInput text\nBERT architecture\nPre-trainingtasks\n[SEP] Covid-19    is    disease [MASK][SEP]    SARS      is0           1          2          3          4         5            6   7            8           9         10         1121        22         23     0           0          0          0          0         0            0   1            1           1          1           1… 1          1           1                \nTask 1: masked language modelTask 3: definition discrimination\n[MASK] \nCovid-19\nSARSglobal\n[SEP] for Covid-19\nTask 2: mutual information maximization\nTransformer Encoder\n…\ncausedviral\n… ………\n[SEP] for SARSmin(       )\nFigure 1: The overall architecture of Dict-BERT. The definitions of rare words are appended to the end of input text.\nIn additional to training with masked language modeling, Dict-BERT performs two novel self-supervised learning\ntasks: word-level mutual information maximization (§3.3.1) and sentence-level definition discrimination (§3.3.2).\ninformation (MI) between the input and its repre-\nsentation. MI measures the amount of informa-\ntion obtained about a random variable by observ-\ning another random variable. As the input text\nsequence and rare word definitions are obtained\nfrom different sources, in order to better align their\nsemantic representations, we proposed to maxi-\nmize the MI between a rare word xi in the input\nsequence and its well-defined meaning in the dictio-\nnary c(i), with joint densityp(xi, c(i)) and marginal\ndensities p(xi) and p(c(i)), is defined as the Kull-\nback–Leibler (KL) divergence between the joint\nand the product of the marginals,\nI(xi; c(i)) = DKL\n\u0000\np(xi, c(i))||p(xi)p(c(i))\n\u0001\n(1)\nThe intuition of maximizing mutual information\nbetween a rare word appeared in the input text se-\nquence and its definitions in the dictionary is to en-\ncode the underlying shared information and align\nthe semantic representation between the contex-\ntual meaning and well-defined meaning of a word.\nNevertheless, estimating MI in high-dimensional\nspaces is a notoriously difficult task, and in prac-\ntice one often maximizes a tractable lower bound\non this quantity (Poole et al., 2019). Intuitively,\nif a classifier can accurately distinguish between\nsamples drawn from the joint p(xi, c(i)) and those\ndrawn from the product of marginals p(xi)p(c(i)),\nthen xi and c(i) have a high mutual information.\nIn order to approximate the mutual information,\nwe adopted InfoNCE (Oord et al., 2018), which is\none of the most commonly used estimators in the\nrepresentation learning literature, defined as\nI(xi; c(i)) ≥ E [\nKX\ni=1\nlog efMI(hi,h(i))\nPK\nj=1 1 [j̸=i]efMI(hi,h(j)) ]\n≜ INCE (xi; c(i)), (2)\nwhere the expectation is over K independent sam-\nples {(hi, h(i))}K\ni=1 from the joint distribution\np(xi, c(i)) (Poole et al., 2019). Intuitively, the critic\nfunction fMI(·) measures the similarity (e.g., inner\nproduct) between two word representations. The\nmodel should assign high values to the positive\npair (hi, h(i)), and low values to all negative pairs.\nWe compute InfoNCE using Monte Carlo estima-\ntion by averaging over multiple batches of sam-\nples (Chen et al., 2020). By maximizing the mutual\ninformation between the encoded representations,\nwe extract the underlying latent variables that the\nrare words in the input text sequence and their dic-\ntionary definitions have in common.\n3.3.2 Sentence-level Definition Discrimination\nInstead of locally aligning the semantic representa-\ntion, learning to differentiate between correct and\npolluted word definitions helps the language model\ncapture global information of input text and dictio-\nnary definitions. We denote the set of definitions\nof rare words in the input text as C. We then create\na set of “polluted” word that are randomly sampled\nfrom the entire vocabulary together with its defini-\ntion. The number of sampled “polluted” words is\nequal to the number of rare words appeared in the\n1910\n12345678910\nInput textDef 1Def 2\n12345678910\nFigure 2: An illustration of knowledge-visible attention\nmatrix. “Def 1” is the dictionary definition of the second\nword in the input text, and “Def 2” is the definition of\nthe third word in the input text. Colored circle means\ntoken i can attend information from tokenj, while white\ncircle means no attention from token i to token j.\ninput text sequence.\nLDD = −E\nKX\ni=1\nlog p(y|fMLP(h(i)\nSEP). (3)\n3.3.3 Overall objective.\nNow we present the overall training objective of\nDict-BERT. To avoid catastrophic forgetting (Mc-\nCloskey and Cohen, 1989) of general language un-\nderstanding ability, we train the masked language\nmodeling together with word-level mutual informa-\ntion maximization (MIM) and definition discrim-\nination (DD) tasks. We denote LMIM as the loss\nfunction of the MIM task which is the opposite\nof expectation in Equation 2. Hence, the overall\nlearning objective is formulated as:\nL = LMLM + λ1LMIM + λ2LDD (4)\nwhere λ1, λ2 are introduced as hyperparameters to\ncontrol the importance of each task.\n3.4 Dict-BERT: Fine-tuning with\nKnowledge-visible Attention\nMost existing work uses the final hidden state of the\nfirst token (i.e., the [CLS] token) as the sequence\nrepresentation (Devlin et al., 2019; Liu et al., 2019;\nYang et al., 2019). For a sequence classification\ntask, a multi-layer perception network function fH\ntakes the output of fLM as input and generates the\nprediction as fH(hCLS). Notably, when fine-tuning\na language model on downstream tasks, there could\nbe many rare/unseen words in the dataset. So, in the\nfine-tuning stage, when encountering a rare word\nin the input text, we append its definition to the end\nof input text, just like what we did in pre-training.\nHowever, the appended dictionary definitions\nmay change the meaning of the original sentence\nsince the [CLS] token attend information from both\ninput text and dictionary description. As pointed\nin Liu et al. (2020) and Xu et al. (2021), too much\nknowledge incorporation may divert the sentence\nfrom its original meaning by introducing a lot of\nnoise. This is more likely to happen if there are mul-\ntiple rare words in the input text. To address this is-\nsue, we adopt the visibility matrix (Liu et al., 2020)\nto limit the impact of definitions on the original\ntext. In BERT, an attention mask matrix is added\nwith the self-attention weights before softmax. If\ntoken j is not supposed to be visible to token i, we\nadd a -∞ value in the attention matrix (i, j).\nAs shown in Figure 2, we modify the attention\nmask matrix such that a token i can attend to an-\nother token j only if: (1) both tokens belong to the\ninput text sequence, or (2) both tokens belong to\nthe definition of the same rare word, or (3) i is a\nrare word in the input text sequence and j is from\nits definition in the dictionary.\n4 Experiments\n4.1 Tasks and Datasets\nTo show the wide adaptability of our Dict-BERT,\nwe conducted experiments on 16 NLP benchmark\ndatasets. We use BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) as the backbone pre-\ntrained language methods. First, we followed Liu\net al. (2019) and Wu et al. (2021) to use 8 natu-\nral language understanding tasks in GLUE, includ-\ning CoLA, RTE, MRPC, STS, SST, QNLI, QQP,\nand MNLI. Second, we followed Gururangan et al.\n(2020) to use 8 specialized domain tasks, including\nChemprot, RCT-20k, ACL-ARC, SciERC, Hyper-\nPartisan, AGNews, Helpfulness, IMDB.\n4.2 Rare Word Collection\nHere, we briefly introduce the statistic of rare words\nin BERT pre-training corpus: English Wikipedia\nand BookCorpus. By concatenating these two\ndatasets, we obtained a corpus with roughly 16GB\nin size. The total number of unique words in the\npre-training corpus is 504,812, of which 112,750\n(22.33%) words are defined as frequent words.\nIn other words, the sum of the occurrences of\n1911\nTable 1: Performance of different models on GLUE tasks. Each configuration is run five times with different random\nseeds, and the average of these five results on the validation set is reported in the table. We note that our code\nis implemented on Huggingface Transformer (Wolf et al., 2020). The performance of our implemented BERT is\nconsistent with the official performance, but it is slightly lower than the performance reported by Wu et al. (2021).\nWe reported the relative improvement (∆) of BERT-TNF and Dict-BERT compared with the original BERT.\nMethods Dict in MNLI QNLI QQP SST CoLA MRPC RTE STS-B Avg ∆\nPT FT Acc. Acc. Acc. Acc. Matthews Acc. Acc. Pearson\nBERT (Wu’s) × × 85.00 91.50 91.20 93.30 58.30 88.30 69.00 88.50 83.10 -\nBERT-TNF √ √ 85.00 91.00 91.20 93.20 59.50 89.30 73.20 88.50 83.90 +0.80\nBERT (ours) × × 84.12 90.69 90.75 92.52 58.89 86.17 68.67 89.39 82.65 -\nDict-BERT-F × √ 84.19 90.94 90.68 92.59 59.16 85.75 68.10 88.72 82.51 -0.14\nDict-BERT-P √ × 84.33 91.02 90.69 92.62 60.44 86.81 73.86 89.81 83.70 +1.05\n⊢w/o MIM √ × 84.24 90.79 90.24 92.22 60.14 87.03 73.79 89.67 83.52 +0.87\n⊢w/o DD √ × 84.18 90.54 90.30 92.39 61.49 86.49 71.89 89.60 83.36 +0.71\nDict-BERT-PF √ √ 84.34 91.20 90.81 92.65 61.68 87.21 72.89 89.68 83.80 +1.15\n⊢w/o MIM √ √ 84.22 90.67 90.66 92.53 61.58 87.20 71.58 89.37 83.47 +0.82\n⊢w/o DD √ √ 84.16 90.21 90.78 92.39 61.14 87.19 71.84 89.24 83.37 +0.72\nTable 2: Performance of different models on eight specialized domain datasets under the domain adaptive pre-\ntraining (DAPT) setting. Each configuration is run five times with different random seeds, and the average of these\nfive results on the test set is calculated as the final performance.\nMethods ChemProt RCT ACL-ARC SciERC HP AGNews Helpful IMDB Avg\nMi-F1 Mi-F1 Ma-F1 Ma-F1 Ma-F1 Ma-F1 Ma-F1 Ma-F1\nBERT 81.16 86.91 64.20 80.40 91.17 94.48 69.39 93.67 82.67\nBERT-DAPT 83.10 86.85 71.45 81.62 93.52 94.58 70.73 94.78 84.57\nDict-BERT-DAPT 83.49 87.46 74.18 83.01 94.70 94.58 70.04 94.80 85.25\n⊢w/o MIM 83.33 87.38 72.26 82.70 94.72 94.58 70.33 94.73 85.06\n⊢w/o DD 84.09 87.23 72.78 82.54 94.69 94.57 70.43 94.70 85.01\nRoBERTa 82.03 87.14 66.20 79.55 90.15 94.43 68.35 95.16 83.15\nRoBERTa-DAPT 84.02 87.62 73.56 81.85 90.22 94.51 69.06 95.18 84.51\nDict-RoBERTa-DAPT 84.41 87.42 75.33 82.53 92.51 94.80 70.57 95.51 85.32\n⊢w/o MIM 84.49 87.51 74.83 81.58 93.27 94.75 70.67 95.40 85.31\n⊢w/o DD 84.09 87.39 74.04 81.18 90.91 94.64 70.81 95.51 84.82\nthese 112,750 words in the corpus accounts for\n90% of the occurrences of all words in the cor-\npus. We look up definitions of the remaining\n392,062 (77.67%) words in the Wiktionary, of\nwhich 252,581 (64.42%) can be found. The av-\nerage length of definition is 11.51±6.84 words.\n4.3 Pre-training Corpus and Tasks\nExperiments on the GLUE benchmark. The\nlanguage model is first pre-trained on the general\ndomain corpus, and then fine-tuned on the training\nset of different GLUE tasks. Following BERT (De-\nvlin et al., 2019), we used the English Wikipedia\nand BookCorpus as the pre-training corpus. We\nremoved the next sentence prediction (NSP) as sug-\ngested in RoBERTa (Liu et al., 2019), and kept\nmasked language modeling (MLM) as the objec-\ntive for pre-training a vanilla BERT.\nExperiments on specialized domain datasets.\nThe language model is not only pre-trained on\nthe general domain corpus, but also pre-trained\non domain specific corpus before fine-tuned on do-\nmain specific tasks. We initialized our model with\nthe checkpoint from pre-trained BERT/RoBERTa\nand continue to pre-train on domain-specific cor-\npus (Gururangan et al., 2020). The four domains\nwe focus on are biomedical science (BIOMED),\ncomputer science (S2ORC-CS), news text (REAL-\nNEWS), and e-commerce reviews (AMAZON).\n1912\n4.4 Baseline Methods\nVanilla BERT/RoBERTa.We use the off-the-shelf\nBERT-base (Devlin et al., 2019) and RoBERTa-\nbase (Liu et al., 2019) model and perform super-\nvised fine-tuning for each downstream tasks.\nBERT-DAPT/RoBERTa-DAPT. It continues pre-\ntraining BERT/RoBERTa on a large unlabeled\ndomain-specific corpus (e.g., BioMed, RealNews)\nby MLM objective (Gururangan et al., 2020).\nBERT-TNF. It takes notes for rare words on the fly\nduring pre-training to help the model understand\nthem when they occur next time. Specifically, it\nmaintains a note dictionary and saves a rare word’s\ncontextual information in it as notes when the rare\nword occurs in a sentence (Wu et al., 2021).\n4.5 Implementation Details\nWe introduce our pre-training and fine-tuning de-\ntails and hyperparameter choices in Appendix A.2\nto A.4. We also listed several detail discussions\nabout using Wiktioanry in Appendix A.6.\n4.6 Ablation Settings\nDict-BERT-Fmeans that we load the vanilla BERT\ncheckpoint and fine-tune on the downstream tasks\nby using knowledge attention for dictionary.\nDict-BERT-Pmeans that we only leverage dictio-\nnary in the pre-training stage and fine-tune Dict-\nBERT on downstream tasks without dictionary.\nDict-BERT-PFindicates that we use dictionary in\nboth pre-training and fine-tuning stages.\nFurthermore, Dict-BERT w/o MIM removes the\nword-level mutual information maximization task\nand Dict-BERT w/o DD removes the sentence-level\ndefinition discriminative task during pre-training.\n4.7 Experimental Results\nDict-BERT-F v.s. BERT. As shown in Table 1,\ncomparing the vanilla BERT with Dict-BERT-F,\nwe observed that only using dictionary during fine-\ntuning could even hurt the model performance on\nthe GLUE benchmark, especially on those small\ndatasets (e.g., RTE, MRPC). This indicated the ex-\nisting pre-trained language models cannot better\nunderstand the input sequence by using word defi-\nnitions when not pre-trained with dictionary. They\nmight be even misled by the noisy explanations in\nthe dictionary. Thus, it is important to incorporate\ndictionary into language model pre-training so the\ndictionary definitions can be better utilized.\nDict-BERT-PF v.s. BERT. As shown in Table\n1, Dict-BERT-PF outperformed the vanilla BERT\non the GLUE benchmark by improving +1.15%\naccuracy on average. This indicated leveraging\nword definitions in dictionary can improve lan-\nguage model pre-training and boost performance\non various NLP downstream tasks. On RTE, Dict-\nBERT-P obtained the biggest performance improve-\nment compared with the vanilla BERT. On an-\nother small-data sub-tasks CoLA, Dict-BERT-PF\nalso outperformed the baseline with considerable\nmargins. This indicated when Dict-BERT was\nfine-tuned on a small downstream dataset, the im-\nprovement was particularly significant. Besides, as\nshown in Table 2, Dict-BERT-DAPT outperformed\nBERT-DAPT on the specialized domain datasets\nby improving +0.68% F1 on average. The same ob-\nservation was obtained from the RoBERTa setting.\nDict-BERT-PF v.s. Dict-BERT-P. As shown in\nTable 1, we compared model performance between\nusing dictionary in fine-tuning (i.e., Dict-BERT-\nPF) and not using dictionary in fine-tuning (i.e.,\nDict-BERT-P). First, after pre-training the language\nmodel with dictionary, even without using dic-\ntionary in fine-tuning, the performance has been\ngreatly improved. This indicated pre-training lan-\nguage model with dictionary generally improved\nthe language representation and provided better ini-\ntiation before fine-tuning the language model on the\ndownstream tasks. Besides, we also observed the\nperformance of Dict-BERT-PF performed slightly\nbetter than Dict-BERT-P. We hypothesized the rea-\nson behind can be the distribution discrepancy of\nthe pre-training and fine-tuning data.\nAblation study. As shown in Table 1 and Table 2,\nwe conducted ablation study on both GLUE bench-\nmark and specialized domain datasets. First, both\nMIM and DD helped learn knowledge from dic-\ntionary and improve language model pre-training.\nSpecifically, DD demonstrated larger average im-\nprovement than MIM. The average improvements\non GLUE benchmark brought by DD and MIM\nare +0.63% and +0.52%. Second, combining MIM\nand DD together achieved the highest performance\non the GLUE benchmark, in which the average\ngain enlarges to +1.15%. For specialized domain\ndatasets, we had the same observations as above.\nKnowledge attention v.s. Full attention. As we\nmentioned in the Section 3.4, too much knowledge\nincorporation may divert the sentence from its orig-\ninal meaning by introducing some noise. This is\n1913\n59.568.7\n89.090.2\n60.569.6\n88.390.1\n61.7\n73.2\n89.291.2\n5060708090100\nCo LARTESTSBMRPC\nVanilla BERTDict-BERT FTDict-BERTKT\n(a) Full attn. (FT) v.s. Knowledge attn. (KT)\n60.6\n73.1\n89.186.9\n61.4\n72.9\n89.287.2\n60.7\n72.3\n89.587.0\n5060708090100\nCo LARTESTSBMRPC\n5%  tail10%tail15% tail (b) Rare word ratios (5% v.s. 10% v.s. 15%)\nFigure 3: Model performance on CoLA, RTE, STSB and MRPC when (a) using two different attention mechanisms\nand (b) selecting different rare word ratios on the downstream task datasets during fine-tuning.\nmore likely to happen if there are multiple rare\nwords appeared in the input text. Therefore, we\ncompared the model performance between using\nknowledge attention and full attention. As shown\nin Figure 3(a), we observed that using knowledge\nattention can consistently perform better than us-\ning full attention mechanism during the fine-tuning\nstage on CoLA, RTE, STSB and MRPC datasets.\nBesides, Dict-BERT with full attention even under-\nperformed than the vanilla BERT without using\nany dictionary definition, which indicates the ap-\npended description in the dictionary may change\nthe meaning of the original sentence. For example,\nSTSB compares similarity between two sentence.\nUsing full attention includes semantic meanings of\ndefinitions into the sentence representation, which\nmight reduce the sentence similarity score and hurt\nthe model performance.\nLearning with different rare word ratios. As we\nmentioned in Section 3.2, we select rare words for\neach downstream tasks by truncating the tail distri-\nbution of the word frequency. In order to verify the\nimpact of using different tail proportions of rare\nwords on the downstream tasks, we selected three\ndifferent ratios (i.e., 5%, 10%, and 15%) and exper-\nimented on CoLA, RTE, STSB and MRPC datasets.\nAs shown in Figure 3(b), on the CoLA and STSB\ndatasets, the model achieves the best performance\nwhen using 10% words at the tail as rare words. On\nthe MRPC data, there is no significant difference of\nmodel performance in using different proportions\nof rare words. However, the performance on RTE\ndata demonstrates a trend, that is, the more rare\nwords selected, the worse the performance of the\nmodel. This is consistent with the conclusion of\nwhether the dictionary is used in fine-tuning in Ta-\nble 1, i.e., the performance of not using dictionary\nis better than using dictionary on the RTE dataset.\nTable 3: Performance of different models on WNLaM-\nPro test set, subdivided by word frequency.\nMethods RARE(0, 10) FREQUENT(100, +∞)\nMRR P@3 p@10 MRR P@3 p@10\nBERT (base)0.117 0.053 0.036 0.356 0.179 0.116\nDict-BERT 0.145 0.068 0.041 0.359 0.181 0.117\n⊢w/o MIM 0.144 0.067 0.041 0.357 0.180 0.115\n⊢w/o DD 0.141 0.065 0.040 0.355 0.179 0.116\nThus, the selection of rare words with different tails\nhas no obvious correlation with the performance of\nthe model on downstream tasks.\nUnsupervised language model probing. In order\nto assess the ability of language models to under-\nstand words as a function of their frequency, we\nused WordNet Language Model Probing (WNLaM-\nPro) dataset (Schick and Schütze, 2020) to test how\nwell a language model understands a given word:\nwe can ask it for properties of that word using nat-\nural language. For example, a language model that\nunderstands the concept of “guilt”, should be able\nto correctly complete the sentence “Guilt is the\nopposite of ___” with the word “innocence”. WN-\nLaMPro contains four different kinds of relations:\nantonym, hypernym, cohyponym+, and corruption.\nBased on the word frequency in English Wikipedia,\nWNLaMPro defines three subsets based on key-\nword counts: RARE (0, 10), MEDIUM (10, 100),\nand FREQUENT (100, +∞). As shown in Table\n3, Dict-BERT can greatly improve the word repre-\nsentation compared with the vanilla BERT without\nusing a dictionary during pre-training. Based on\nthe word frequency, we observe Dict-BERT can\nsignificantly help learn rare word representations.\nCompared to the vanilla BERT, Dict-BERT im-\nproves MRR and P@3 by relatively +23.93% and\n+28.30%, respectively. In addition, Dict-BERT is\nalso able to learn better frequent word representa-\ntions. Although we did not directly take frequent\n1914\nword definitions as part of the input, Dict-BERT\nspends less memory on rare words, because it is\neasier to predict rare words than the vanilla BERT,\nso the saved memory power could be used to mem-\norize the facts involving popular words and interac-\ntions between popular words.\n5 Conclusions\nIn this work, we leveraged rare word definitions\nin English dictionary to improve language model\npre-training. When encountering a rare word in the\ninput text during pre-training, we fetched its defini-\ntion from Wiktionary and appended it to the end of\nthe input text. In order to make better interactions\nbetween the input text and rare word definitions, we\nproposed two novel self-supervised training tasks\nto help language model learn better representations\nfor rare words. Experimental on the GLUE bench-\nmark and eight specialized domain datasets demon-\nstrated that our method significantly improved the\nunderstanding of rare words and boosted model\nperformance on various downstream tasks.\nAcknowledgements\nWenhao Yu and Meng Jiang are supported in part\nby the National Science Foundation IIS-1849816,\nCCF-1901059, IIS-2119531 and IIS-2142827.\nReferences\nTom Bosc and Pascal Vincent. 2018. Auto-encoding dic-\ntionary definitions into consistent word embeddings.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning (ICML).\nLeyang Cui, Yu Wu, Shujie Liu, and Yue Zhang. 2021.\nKnowledge enhanced fine-tuning for better handling\nunseen entities in dialogue generation. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Conference of North American Chapter of the\nAssociation for Computational Linguistics (NAACL).\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019. Representation degeneration problem\nin training natural language generation models. In In-\nternational Conference for Learning Representation\n(ICLR).\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: frequency-agnostic\nword representation. In Conference on Neural Infor-\nmation Processing Systems (Neruips).\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nIn Transactions of the Association for Computational\nLinguistics (TACL).\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference for Learning Representation (ICLR).\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining. In\nBioinformatics. Oxford University Press.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert: En-\nabling language representation with knowledge graph.\nIn AAAI Conference on Artificial Intelligence (AAAI).\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S Yu.\n2021. Kg-bart: Knowledge graph-augmented bart for\ngenerative commonsense reasoning. In Conference\non Artificial Intelligence (AAAI).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In arXiv preprint arXiv:1907.11692.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation. Elsevier.\nThanapon Noraset, Chen Liang, Larry Birnbaum, and\nDoug Downey. 2017. Definition modeling: Learning\nto define word embeddings in natural language. In\nAAAI Conference on Artificial Intelligence (AAAI).\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. In arXiv preprint arXiv:1807.03748.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In conference on empirical methods\nin natural language processing (EMNLP).\n1915\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex\nAlemi, and George Tucker. 2019. On variational\nbounds of mutual information. In International Con-\nference on Machine Learning (ICML).\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey. In\nScience China Technological Sciences. Springer.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. In Journal of Machine Learning Research\n(JMLR).\nElena Sofia Ruzzetti, Leonardo Ranaldi, Michele Mas-\ntromattei, Francesca Fallucchi, and Fabio Massimo\nZanzotto. 2021. Lacking the embedding of a word?\nlook it up into a traditional dictionary. In arXiv\npreprint arXiv:2109.11763.\nTimo Schick and Hinrich Schütze. 2020. Rare words:\nA major problem for contextualized embeddings and\nhow to fix it by attentive mimicking. In AAAI Con-\nference on Artificial Intelligence (AAAI).\nMichael Tschannen, Josip Djolonga, Paul K Rubenstein,\nSylvain Gelly, and Mario Lucic. 2020. On mutual in-\nformation maximization for representation learning.\nIn International Conference for Learning Represen-\ntation(ICLR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems (Neruips).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In International\nConference for Learning Representation (ICLR).\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Guihong Cao, Daxin Jiang, Ming\nZhou, et al. 2021a. K-adapter: Infusing knowledge\ninto pre-trained models with adapters. In Proceed-\nings of Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. In Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-art\nnatural language processing. In 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations (EMNLP).\nQiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, and\nTie-Yan Liu. 2021. Taking notes on the fly helps\nbert pre-training. In International Conference for\nLearning Representation (ICLR).\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference of Learning Rep-\nresentation (ICLR).\nRuochen Xu, Yuwei Fang, Chenguang Zhu, and Michael\nZeng. 2021. Does knowledge help general nlu? an\nempirical study. In arXiv preprint arXiv:2109.00563.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in neural informa-\ntion processing systems (Neruips).\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022a. Kg-fid: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Annual Meeting of\nthe Association for Computational Linguistics (ACL).\nDonghan Yu, Chenguang Zhu, Yiming Yang, and\nMichael Zeng. 2022b. Jaket: Joint pre-training of\nknowledge graph and language understanding. In\nAAAI Conference on Artificial Intelligence (AAAI).\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022c. A\nsurvey of knowledge-enhanced text generation. In\nACM Computing Survey (CSUR).\nWenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan\nZhang, Tong Zhao, and Meng Jiang. 2022d. Diversi-\nfying content generation for commonsense reasoning\nwith mixture of knowledge graph experts. In An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nAnnual Meeting of the Association for Computational\nLinguistics (ACL).\nWangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Selvam,\nSeyeon Lee, Bill Yuchen Lin, and Xiang Ren. 2021.\nPre-training text-to-text transformers for concept-\ncentric common sense. In International Conference\nfor Learning Representation (ICLR).\n1916\nA Appendix\nA.1 Preliminary: BERT Pre-training\nWe use the BERT (Devlin et al., 2019) model as\nan example to introduce the basics of the model\narchitecture and training objective of PLMs. BERT\nis developed on a multi-layer bidirectional Trans-\nformer (Vaswani et al., 2017) encoder. The Trans-\nformer encoder is a stack of multiple identical\nlayers, where each layer has two sub-layers: a\nself-attention sub-layer and a position-wise feed-\nforward sub-layer. The self-attention sub-layer pro-\nduces outputs by calculating the scaled dot products\nof queries and keys as the coefficients of the values,\nAttention(Q, K, V) = Softmax(QKT\n√\nd\n)V. (5)\nQ(Query), K(Key), V (Value) are the hidden repre-\nsentations produced by the previous self-attention\nlayer and d is the dimension of the hiddens.\nTransformer also extends the aforementioned self-\nattention layer to a multi-head self-attention layer\nversion in order to jointly attend to information\nfrom different representation subspaces.\nBERT uses the Transformer model as its back-\nbone neural network architecture and trains the\nmodel parameters with the masked language mod-\neling (MLM) objective on large text corpora. In\nthe masked language modeling task, a random sam-\nple of the words in the input text sequence is se-\nlected. The selected positions will be either re-\nplaced by special token [MASK], replaced by ran-\ndomly picked tokens or remain the same. The ob-\njective of masked language modeling is to predict\nwords at the masked positions correctly given the\nmasked sentences. RoBERTa (robustly optimized\nBERT approach) is a retraining of BERT with im-\nproved training methodologies, 1000% more data\n(i.e., 160 GB) and computation power (i.e., 1024\nV100 GPUs). To improve the training procedure,\nRoBERTa introduces dynamic masking so that the\nmasked token changes during the training epochs.\nLarger batch-training sizes were also found to be\nmore useful in the training procedure.\nA.2 BERT Pre-training Details\nWe conducted experiments on pre-training BERT-\nbase with 110M parameters (Devlin et al., 2019).\nBERT-base consists of 12 Transformer layers. For\neach layer, the hidden size is set to 768 and the\nnumber of attention head is set to 12. All mod-\nels (including BERT-base and Dict-BERT-base)\nare pre-trained for 300k steps with batch size\n2,000 and maximum sequence length 512. We\nuse Adam (Kingma and Ba, 2015) as the optimizer,\nand set its hyperparameter ϵ to 1e-6 and (β1, β2)\nto (0.9, 0.98). The peak learning rate is set to 7e-4\nwith a 10k-step warm-up stage. We set the dropout\nprobability to 0.1 and weight decay to 0.01. All\nconfigurations are reported in Table 4.\nA.3 Domain Adaptive Pre-training Details\nWe conducted experiments on domain adaptive\npre-training (DAPT) of BERT-base and RoBERTa-\nbase. RoBERTa (Liu et al., 2019) is a retraining\nof BERT with improved training methodologies,\n1000% more data (i.e., 160 GB) and computation\npower (i.e., 1024 V100 GPUs). To improve the\ntraining procedure, RoBERTa removes the next sen-\ntence prediction task from BERT’s pre-training and\nintroduces dynamic masking so that the masked\ntoken changes during the training epochs. To train\nthe models, we followed (Gururangan et al., 2020)\nand domain adaptive pre-training for 12.5k steps\nwith batch size 2,000. All other configurations are\nreported in Table 4.\nA.4 Fine-tuning Details\nFollowing previous work, we search the learning\nrates during the fine-tuning for each downstream\ntask. The details are listed in Table 5. Each con-\nfiguration is run five times with different random\nseeds, and the average of these five results on the\nvalidation set is calculated as the final performance\nof one configuration. We report the best number\nover all configurations for each task.\nA.5 Evaluation Metrics\nFor GLUE, we followed RoBERTa (Liu et al.,\n2019) and reported Matthews correlation for CoLA,\nPearson correlation for STS-B, and Accuracy for\nother tasks. For specialized tasks, we followed (Gu-\nrurangan et al., 2020) and reported Micro-F1 for\nChemprot and RCT-20k, and Macro-F1 for other\ntasks. For WNLaMPro, we followed (Schick and\nSchütze, 2020) and reported MRR and P@K.\nA.6 Usage of Wiktionary\nPolysemy in Wiktionary. There are plenty of\nEnglish words having multiple meanings (aka. pol-\nysemy). If multiple meanings of a word are ap-\npended to the input text sequence simultaneously,\n1917\nTable 4: Hyperparameters for model pre-training and domain-adaptive pre-training (DAPT).\nHyperparameter Assignments\nPre-training setting BERT pre-training Domain adaptive pre-training\nnumber of steps 300K 12.5K\nbatch size 2,000 2,000\nmaximum learning rate 7e-4 1e-4\nlearning rate optimizer Adam Adam\nAdam epsilon 1e-6 1e-6\nAdam beta weights 0.9, 0.98 0.9, 0.98\nWeight decay 0.01 0.01\nWarmup proportion 0.06 0.06\nlearning rate decay linear linear\nTable 5: Hyperparameters for model fine-tuning on GLUE and specialized domain benchmarks.\nHyperparameter Assignments\nFine-tuning setting GLUE benchmark Specialized domain\nnumber of epochs 5 or 10 10\nbatch size 24 or 168 168\nlearning rate 2e-5 2e-5 or 3e-5\nlearning rate optimizer Adam Adam\nAdam epsilon 1e-6 1e-6\nAdam beta weights 0.9, 0.98 0.9, 0.98\nDropout 0.1 0.1\nWeight decay 0.01 0.01\nlearning rate decay linear linear\nit may bring noisy information and disrupt the train-\ning of the entire language model.\nIn this work, we did not pay particular attention\nto the polysemy issue, because for most rare words,\nthey often only have one meaning in the dictionary.\nFor example, as mentioned in Section 4.2, there\nare a total of 252,581 rare words in the BERT pre-\ntraining corpus (i.e., Bookcorpus and Wikipedia).\nAmong them, 228,658 (90.52%) words only have\none meaning, and 21,721 (8.6%) words have two\nmeanings. So, words less than three meanings\naccount for more than 99% of words. To deal with\nthe words having more than one meaning, we use\nthe definition of their first etymology, i.e., the most\ncommonly used meaning, in the Wiktionary.\nRare words during fine-tuning. One important\nadvantage of Dict-BERT is that it can dynamically\nadjust the vocabulary of rare words, obtain and\nrepresent their definitions in a dictionary in a plug-\nand-play manner. As different domain datasets\nusually follow different word distributions, the pre-\ntrained language model may still encounter many\nrare words when fine-tuned on the downstream\ntasks. To enhance the rare word representations\nduring fine-tuning process, when encountering a\nrare word in the input text sequence, we append\nits definition to the end of input text sequence, just\nlike what we did in pre-training.\nNegative words sampling. The sentence-level\ndefinition discrimination task samples negative\nword for each input text sequence with rare words.\nIn order to select harder negative words, we\nexplored using the negative words with similar\nGloVe (Pennington et al., 2014) embeddings or\ntaking the synonyms of rare words provided in the\ndictionary as negative samples. However, either of\nthe two methods has the problem of extremely low\ncoverage. Most of the rare words are not appeared\nin GloVe, and only about 10% of the words have\nsynonyms in the Wiktionary. Thus, we chose to\nuse the random negative sampling strategy.\n1918"
}