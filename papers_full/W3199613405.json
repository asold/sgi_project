{
  "title": "MISSFormer: An Effective Medical Image Segmentation Transformer",
  "url": "https://openalex.org/W3199613405",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1918101254",
      "name": "Huang Xiao-hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2188546050",
      "name": "Deng Zhi-fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112903538",
      "name": "Li Dandan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227011276",
      "name": "Yuan Xueguang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3116570699",
    "https://openalex.org/W3135385363",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2951839332",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3013198566",
    "https://openalex.org/W2980190282",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2884555738",
    "https://openalex.org/W3017153481",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2980185997",
    "https://openalex.org/W3001591165",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2962914239"
  ],
  "abstract": "The CNN-based methods have achieved impressive results in medical image segmentation, but they failed to capture the long-range dependencies due to the inherent locality of the convolution operation. Transformer-based methods are recently popular in vision tasks because of their capacity for long-range dependencies and promising performance. However, it lacks in modeling local context. In this paper, taking medical image segmentation as an example, we present MISSFormer, an effective and powerful Medical Image Segmentation tranSFormer. MISSFormer is a hierarchical encoder-decoder network with two appealing designs: 1) A feed-forward network is redesigned with the proposed Enhanced Transformer Block, which enhances the long-range dependencies and supplements the local context, making the feature more discriminative. 2) We proposed Enhanced Transformer Context Bridge, different from previous methods of modeling only global information, the proposed context bridge with the enhanced transformer block extracts the long-range dependencies and local context of multi-scale features generated by our hierarchical transformer encoder. Driven by these two designs, the MISSFormer shows a solid capacity to capture more discriminative dependencies and context in medical image segmentation. The experiments on multi-organ and cardiac segmentation tasks demonstrate the superiority, effectiveness and robustness of our MISSFormer, the experimental results of MISSFormer trained from scratch even outperform state-of-the-art methods pre-trained on ImageNet. The core designs can be generalized to other visual segmentation tasks. The code has been released on Github: https://github.com/ZhifangDeng/MISSFormer",
  "full_text": "MISSFormer: An Effective Medical Image Segmentation Transformer\nXiaohong Huang,1 Zhifang Deng,1 Dandan Li,1* Xueguang Yuan1\n1 Beijing University of Posts and Telecommunications\nhuangxh@bupt.edu.cn, dengzfong@bupt.edu.cn, dandl@bupt.edu.cn, yuanxg@bupt.edu.cn\nAbstract\nThe CNN-based methods have achieved impressive results\nin medical image segmentation, but they failed to capture\nthe long-range dependencies due to the inherent locality\nof the convolution operation. Transformer-based methods\nare recently popular in vision tasks because of their ca-\npacity for long-range dependencies and promising perfor-\nmance. However, it lacks in modeling local context. In this\npaper, taking medical image segmentation as an example,\nwe present MISSFormer, an effective and powerful Medi-\ncal Image Segmentation tranSFormer. MISSFormer is a hi-\nerarchical encoder-decoder network with two appealing de-\nsigns: 1) A feed-forward network is redesigned with the\nproposed Enhanced Transformer Block, which enhances the\nlong-range dependencies and supplements the local context,\nmaking the feature more discriminative. 2) We proposed En-\nhanced Transformer Context Bridge, different from previous\nmethods of modeling only global information, the proposed\ncontext bridge with the enhanced transformer block extracts\nthe long-range dependencies and local context of multi-scale\nfeatures generated by our hierarchical transformer encoder.\nDriven by these two designs, the MISSFormer shows a solid\ncapacity to capture more discriminative dependencies and\ncontext in medical image segmentation. The experiments on\nmulti-organ and cardiac segmentation tasks demonstrate the\nsuperiority, effectiveness and robustness of our MISSFormer,\nthe experimental results of MISSFormer trained from scratch\neven outperform state-of-the-art methods pre-trained on Im-\nageNet. The core designs can be generalized to other visual\nsegmentation tasks. The code has been released on Github:\nhttps://github.com/ZhifangDeng/MISSFormer\n1 Introduction\nWith the improvement of medical treatment and the people’s\nhealth awareness, the requirements of accurate medical im-\nage analysis (such as preoperative evaluation, auxiliary di-\nagnosis) have become more critical. The medical image seg-\nmentation, as a crucial step of them, the precise and robust\nsegmentation results will provide a sound basis for subse-\nquent analysis and treatment.\nSince the fully convolutional networks (FCNs)(Long,\nShelhamer, and Darrell 2015) opened a door for seman-\ntic segmentation, one of its variants, the U-shaped net-\nworks(Ronneberger, Fischer, and Brox 2015; iek et al. 2016)\n*Corresponding author\ngot a promising performance in medical image segmenta-\ntion by the improvement of skip connection, which provided\nmore detailed information. According to this elegant archi-\ntecture, the variants of U-Net(Isensee et al. 2021; Zhou et al.\n2018; Huang et al. 2020) have been achieved excellent per-\nformance and impressive results. Although their superb per-\nformance and prevalence, the CNN-based methods suffer\nfrom a limitation in modeling the long-range dependencies\nbecause of the locality of convolution operation(Cao et al.\n2021; Xie et al. 2021b), and they failed to achieve the goal\nof precise medical image analysis. To overcome the limi-\ntation, some works proposed dilated convolution(Gu et al.\n2019; Feng et al. 2020) and pyramid pooling(Zhao et al.\n2017) to enlarge the receptive ﬁeld as much as possible. And\nsome recent works(Xie et al. 2021b; Mou et al. 2019; Chen\net al. 2021; Sinha and Dolz 2020) tried to employ few self-\nattention layers or transformer layers(Vaswani et al. 2017)\nin high-level semantic feature maps due to the quadratic re-\nlationship between self-attention computational complexity\nand feature map size, which makes these methods insufﬁ-\ncient to capture the abundant long-range dependencies.\nRecently, the success of transformers that capture long-\nrange dependencies makes it possible to solve the above\nproblems. Especially, the researches on visual trans-\nformer(Liu et al. 2021; Dosovitskiy et al. 2020; Wang et al.\n2021c; Graham et al. 2021; Chu et al. 2021a; Xie et al.\n2021a; Zheng et al. 2021) are in full swing and have got a\npromising performance in vision tasks, encouraged by the\ngreat success of transformer in natural language process-\ning (NLP). Corresponding to the transformer in NLP, vi-\nsion transformer(Dosovitskiy et al. 2020) fed the image into\na standard transformer with positional embeddings by di-\nviding an image into non-overlapping patches and achieved\ncomparable performance with CNN-based methods. Pyra-\nmid vision transformer (PVT)(Wang et al. 2021c) and Swin\ntransformer(Liu et al. 2021) proposed hierarchical trans-\nformer to explore the vision transformer with spatial re-\nduction attention (SRA) and window-based attention, re-\nspectively, which are responsible for reducing computa-\ntional complexity. Besides, the attempts of SETR(Zheng\net al. 2021) in semantic segmentation proved the potential\nof transformer in visual tasks once again.\nHowever, some recent works(Islam, Jia, and Bruce 2020;\nChu et al. 2021b; Li et al. 2021) showed the limita-\narXiv:2109.07162v2  [cs.CV]  19 Dec 2021\ntion of self-attention on local context, inspired by this,\nUformer(Wang et al. 2021d), SegFormer(Xie et al. 2021a)\nand PVTv2(Wang et al. 2021b) tried to embed convolutional\nlayer between fully-connected layers of feed-forward net-\nwork in transformer block to overcome this problem. De-\nspite it captured local context to some extent, but there are\nsome limitations in medical image segmentation: 1) the con-\nvolutional layer is embedded between fully-connected lay-\ners of the feed-forward network directly, which limits the\ndiscrimination of features for our task, although some local\ncontext is supplemented, it will be conﬁrmed in Section 4.2;\n2) it did not consider the integration of multi-scale informa-\ntion generated by the hierarchical encoder. Both limitations\nlead to the inferior learning of networks.\nIn this paper, MISSFormer, an effective and powerful\nMedical Image Segmentation tranSFormer, is proposed to\nleverage the powerful long-ranged dependencies capability\nof self-attention to produce accurate medical image seg-\nmentation. MISSFormer is based on the U-shaped archi-\ntecture, whose redesigned transformer block, named En-\nhanced Transformer Block, enhances the feature represen-\ntations. The MISSFormer consists of encoder, bridge, de-\ncoder and skip-connection. These components are all based\non the enhanced transformer block. The encoder extracts hi-\nerarchical features through the overlapped image patches.\nLocal and global dependencies between different scale fea-\ntures are modeled via the bridge. The decoder is responsible\nfor pixel-wise segmentation prediction with skip connection.\nThe main contributions of this paper can be summarized as\nfollows:\n• We propose MISSFormer, a position-free and hierarchi-\ncal U-shaped transformer for medical image segmenta-\ntion.\n• We redesign a powerful feed-forward network, Enhanced\nMix-FFN, with better feature discrimination, long-range\ndependencies and local context. Based on this, we ex-\npand it and get an Enhanced Transformer Block to make\na strong feature representation.\n• We propose an Enhanced Transformer Context Bridge\nbased on the Enhanced Transformer Block to capture the\nlocal and global correlations of hierarchical multi-scale\nfeatures.\n• The superior experimental results on medical image seg-\nmentation datasets demonstrate the effectiveness, superi-\nority and robustness of the proposed MISSFormer.\n2 Related Work\nMedical image segmentation. Medical image segmenta-\ntion is a pixel-level task of separating the pixels of le-\nsions or organs in a given medical image. U-shaped net-\nwork(Ronneberger, Fischer, and Brox 2015) played a cor-\nnerstone role in medical image segmentation tasks because\nof its superior performance and elegant structure. Due to\nthe rapid development of computer vision tasks(He et al.\n2016; Chen et al. 2017), the medical image segmentation\ndrew lessons from its key insight. For example, resnet archi-\ntecture became a general encoder backbone for medical im-\nage segmentation network, the dilated convolution and pyra-\nmid pooling were utilized to enlarge the receptive ﬁeld for\nlesion and organ segmentation(Gu et al. 2019; Feng et al.\n2020). Besides, various attention mechanisms were effec-\ntive to promote segmentation performance, reverse atten-\ntion(Chen et al. 2018) was applied to accurate polyp seg-\nmentation(Fan et al. 2020), squeeze-and-excitation atten-\ntion(Hu, Shen, and Sun 2018) was integrated into module\nto reﬁne the channel information to segment vessel in retina\nimages(Zhang et al. 2019), and some works(Mou et al. 2019;\nSinha and Dolz 2020) employed self-attention mechanism to\nsupplement the long-range dependencies for segmentation\ntasks.\nVision transformers. ViT(Dosovitskiy et al. 2020) in-\ntroduced transformer(Vaswani et al. 2017) into visual tasks\nfor the ﬁrst time and achieved impressive performance be-\ncause of the capacity for global dependencies of the trans-\nformer. Vision tasks developed a new stage inspired by ViT.\nFor example, DeiT(Touvron et al. 2021) explored the efﬁ-\ncient training strategies for ViT, PVT(Wang et al. 2021c)\nproposed a pyramid transformer with SRA to reduce the\ncomputational complexity, and Swin transformer(Liu et al.\n2021) was an efﬁcient and effective hierarchical vision\ntransformer, whose window-based mechanism enhances the\nlocality of features, which was also the improvement of\nsome excellent transformer works(Islam, Jia, and Bruce\n2020; Chu et al. 2021b; Li et al. 2021). For other spe-\nciﬁc tasks, SETR( ?) was a semantic segmentation network\nbased on the transformer and made ViT as a backbone, Seg-\nFormer(Xie et al. 2021a) introduced a simple and efﬁcient\ndesign for semantic segmentation powered by transformer,\nDETR(Carion et al. 2020) proposed an end-to-end object\ndetection framework with transformer, Uformer(Wang et al.\n2021d) built a general U-shaped transformer for image\nrestoration.\nTransformers for medical image segmentation. Re-\nsearchers borrowed the transformer to medical image seg-\nmentation inspired by the rapid development of vision\ntransformers. Transunet(Chen et al. 2021) employed some\ntransformer layers into the low-resolution encoder fea-\nture maps to capture the long-range dependencies, UN-\nETR(Hatamizadeh et al. 2021) applied transformer to make\na powerful encoder for 3d medical image segmentation with\nCNN decoder, CoTr(Xie et al. 2021b) and TransBTS(Wang\net al. 2021a) bridged the CNN-based encoder and decoder\nwith the transformer to improve the segmentation perfor-\nmance in low-resolution stage. Besides these methods which\nare the combination of CNN and transformer, (Cao et al.\n2021) proposed Swin-Unet, based on Swin transformer(Liu\net al. 2021), to demonstrate the application potential of\npure transformer in medical image segmentation. However,\nSwin-Unet, whose encoder backbone is Swin transformer\npre-trained on ImageNet, requires pre-training on large-\nscale datasets. Different from it, the proposed MISSFormer\nis trained on the medical image datasets from scratch and\nachieves better performance because of the discriminative\nfeature representations by Enhanced Transformer Block.\nOverlap Patch \nEmbedding\nEnhanced Transformer \nBlock x2\nOverlap Patch Merging\nEnhanced Transformer \nBlock x2\nOverlap Patch Merging\nEnhanced Transformer \nBlock x2\nOverlap Patch Merging\nEnhanced Transformer \nBlock x2\nEnhanced Transformer \nContext Bridge\nEnhanced Transformer \nBlock x2\nPatch Expanding\nEnhanced Transformer \nBlock x2\nPatch Expanding\nEnhanced Transformer \nBlock x2\nPatch Expanding\nPatch Expanding\n44\nHW C\n288\nHW C\n416 16\nHW C\n832 32\nHW C\nNC NC\n44\nHW C\n288\nHW C\n416 16\nHW C\n832 32\nHW C\nLinear Projection\nHWC\nHWc l a s s\nLayer Norm\nEfficient Self-Attention\nLayer Norm\nEMix_FFN\n\n\n(a) (b)\nx4\nFigure 1: The overall structure of the proposed MISSFormer. (a) The proposed MISSFormer framework. (b) The structure of\nEnhanced Transformer Block.\n3 Method\nThis section describes the overall pipeline and the speciﬁc\nstructure of MISSFormer ﬁrst, and then we show the details\nof the improved transformer block, Enhanced Transformer\nBlock, which is the basic unit of MISSFormer. After that,\nwe introduce the proposed Enhanced Transformer Context\nBridge, which models the local and global correlations of\nhierarchical multi-scale information.\n3.1 Overall Pipeline\nThe proposed MISSFormer is shown in Fig.1(a), a hierar-\nchical encoder-decoder architecture with an enhanced trans-\nformer context bridge module appended between encoder\nand decoder. Speciﬁcally, given an input image, MISS-\nFormer ﬁrst divides it into overlapping patches of size 4*4 to\npreserve its local continuity with convolutional layers. Then,\nthe overlapping patches are fed into the encoder to produce\nthe multi-scale features. Here, the encoder is hierarchical,\nand each stage includes enhanced transformer blocks and\npatch merging layer. The enhanced transformer block learns\nthe long-range dependencies and local context with limited\ncomputational complexity, patch merging layer is applied to\ngenerate the downsampling features.\nAfter that, MISSFormer makes the generated multi-scale\nfeatures pass through the Enhanced Transformer Context\nBridge to capture the local and global correlations of dif-\nferent scale features. In practice, different level features are\nﬂattened in spatial dimension and reshaped to make consis-\ntent in channel dimension, then concatenate them in ﬂat-\ntened spatial dimension and feed into the enhanced trans-\nformer context bridge with d-depth. After that, we split and\nrestore them to their original shape and obtain the discrimi-\nnative hierarchical multi-scale features.\nFor the segmentation prediction, MISSFormer takes the\ndiscriminative features and skip connections as inputs of de-\ncoders. Each decoder stage includes Enhanced Transformer\nBlocks and patch expanding layer(Cao et al. 2021). Contrary\nto the patch merging layer, the patch expanding layer upsam-\nple the adjacent feature maps to twice the original resolution\nexcept that the last one is four times. Last, the pixel-wise\nsegmentation prediction is output by a linear projection.\n3.2 Enhanced Transformer Block\nLong-range dependencies and local context are effective for\naccurate medical image segmentation. Transformer and con-\nvolution are good choices for long-range dependencies and\nlocality at present, respectively. At the same time, the com-\nputational complexity of the original transformer block is\nquadratic with the feature map resolution, making it un-\nsuitable for high-resolution feature maps. Second, the trans-\nformer lacks the ability to extract the local context(Islam,\nJia, and Bruce 2020; Chu et al. 2021b; Li et al. 2021), al-\nthough Uformer, SegFormer and PVTv2 tried to overcome\nthe limitation by embedding a convolutional layer in feed-\nforward network directly, we argue that this approach limits\nthe discrimination of features, even some improved perfor-\nmance is achieved by them.\nTo solve the above problems, we proposed Enhanced\n(a)\n(b)\n1x1 Conv\n1x1 Conv\n3x3 Depth-wise conv\nFC_in\nFC_out\nDepth-wise conv\nGELU\nFC_in\nFC_out\nLayer Norm\nDepth-wise conv\nGELU\n\nFC_in\nFC_out\nLayer Norm\nDepth-wise conv\nGELU\n\nFC_in\nFC_out\nEnhanced Mix Block\nGELU\nDepth-wise conv\nLayer Norm\n\nLayer Norm\n\nLayer Norm\n\n(b) (c) (d)\nGELU\nSeq2Img\nImg2Seq\n\nFigure 2: The various exploration of locality in feed-forward neural network, from left to right: (a)Residual Block in LocalViT,\n(b)LeFF in Uformer, Mix-FFN in SegFormer and PVTv2, (c) proposed Simple Enhanced Mix-FFN, (d) proposed Enhanced\nMix-FFN\nTransformer Block. As is shown in Fig.1(b), the Enhanced\nTransformer Block is composed of LayerNorm, Efﬁcient\nSelf-Attention and Enhanced Mix-FFN.\nEfﬁcient Self-Attention. Efﬁcient self-attention is a spa-\ntial reduction self-attention(Wang et al. 2021c), which can\nbe applied to high-resolution feature map. Given a fea-\nture map F∈RH×W×C, and H, W, Cis the height, width\nand channel depth respectively. For the original standard\nmulti-head self-attention, it makesQ, K, Vhave same shape\nN ×C, where N = H ×W, which can be formulated as:\nAttention(Q, K, V) =SoftMax ( QKT\n√dhead\n)V, (1)\nand its computational complexity is O(N2). While for the\nefﬁcient self-attention, it applied a spatial reduction ratio R\nto reduce the spatial resolution as follows:\nnew K = Reshape(N\nR , C·R)W(C ·R, C), (2)\nit ﬁrst reshapes K and V to N\nR ×(C ·R), and then a lin-\near projection W is used to make channel depth restore to\nC. After that, the computational complexity of self-attention\nreduces to O(N2\nR ), and can be applied to high-resolution fea-\nture maps. The spatial reduction operation is convolution or\npooling in common.\nEnhanced Mix-FFN. Different from previous methods in\nFig.2(a) and (b), we redesigned the structure of Mix-FFN to\nalign features and make discriminative representations. As\nshown in Fig.2(c), First we add a skip connection before the\ndepth-wise convolution, and then, we applied layer norm af-\nter the skip connection, which can be denoted as:\ny1 = LN(Conv3×3(FC (xin)) +FC(xin)),\nxout = FC (GELU (y1)) +xin, (3)\nwhere, xin is the output of efﬁcient self-attention, Conv3×3\nis convolution with kernel3×3, we applied depth-wise con-\nvolution for efﬁciency in this paper. We will show that these\nimprovements are essential for Mix-FFN in Section 4.2.\n(a)\nLN-ESA\nsplit\nSeq2Img\nEmix_FFN\nConcatenate\nImg2Seq\nglobal local\nEmix_FFN\nEmix_FFN\nEmix_FFN\nLN\nFigure 3: The Enhanced Transformer Context Bridge\nInspired by(Liu et al. 2020), we extend our design to a\ngeneral form with the help of layer norm, which facilitates\nthe optimization of skip connection(Vaswani et al. 2017).\nAs shown in Fig.2(d), we make an Enhanced Mix block em-\nbedded in the original feed-forward network. We introduced\nrecursive skip connection in Enhanced Mix block, given an\ninput feature map xin, a depth-wise convolution layer is ap-\nplied to capture the local context, and then a recursive skip\nconnection followed, and it can be deﬁned as:\nyi = LN(xin + yi−1),\nxout = FC (GELU (yi)) +xin, (4)\nwhere y1 = LN(xin + F(xin, W)). After that, the model\nmakes more expressive power due to the construction of dif-\nferent feature distribution and consistency by each recursive\nstep.\n3.3 Enhanced Transformer Context Bridge\nMulti-scale information fusion has been proved to be cru-\ncial for accurate semantic segmentation in the CNN-based\nmethod(Sinha and Dolz 2020; Chen et al. 2017). In this\npart, we explore the multi-scale feature fusion for the\nTransformer-based method with the aid of the hierarchical\nstructure of MISSFormer. As is shown in Fig.3, the multiple\nstage feature maps are obtained after feeding the patches into\nthe encoder, whose settings of patch merging and channel\ndepth in each stage keep the same with SegFormer. Given\nmulti-level features F1, F2, F3, F4, which are generated by\nhierarchical encoder, we ﬂatten them in spatial dimension\nand reshape them to keep the same channel depth with each\nother, then we concatenate them in the ﬂattened spatial di-\nmension, after that, the concatenated token is fed into en-\nhanced transformer block to construct the long-range de-\npendencies and local context correlation. The process can\nbe summarized as formula (5).\ntoken Fi = Reshape(Fi, [B, −1, C])\nmerge token = Concatenate(token Fi, dim= 1)\nAtten token = Efficient Atten(LN(merge token))\nres token = LN(Atten token + merge token)\nsplit token = Split(res token, dim= 1)\nFFN i = EnhancedMix −FFN (split token)\noutput = Concatenate(FFN i, dim= 1) +res token\n(5)\nAfter the feature passes through d enhanced transformer\nblock, we split tokens, restore them to the original shape\nof features in each stage, and feed them into a transformer-\nbased decoder with a corresponding skip connection to pre-\ndict the pixel-wise segmentation map. The depth of Context\nBridge is set to 4 in this paper.\n4 Experiments\nIn this section, we ﬁrst conduct the experiment of ablation\nstudies to validate the effectiveness of each component in\nMISSFormer, and then the comparison results with previous\nstate-of-the-art methods are reported to demonstrate the su-\nperiority of the proposed MISSFormer.\n4.1 Experiments Settings\nDatasets. We perform experiments on two different for-\nmats of datasets: Synapse multi-organ segmentation dataset\n(Synapse) and Automated cardiac diagnosis challenge\ndataset (ACDC). The Synapse dataset includes 30 abdom-\ninal CT scans with 3779 axial abdominal clinical CT im-\nages, and the dataset is divided into 18 scans for training and\n12 for testing randomly, follow the (Cao et al. 2021; Chen\net al. 2021). We evaluate our method with the average Dice-\nSørensen Coefﬁcient (DSC) and average Hausdorff Distance\n(HD) on 8 abdominal organs (aorta, gallbladder, spleen, left\nkidney, right kidney, liver, pancreas, spleen, stomach). The\nACDC dataset includes 100 MRI scans collected from dif-\nferent patients, and each scan labeled three organs, left ven-\ntricle (LV), right ventricle (RV) and myocardium (MYO).\nConsistent with the previous method(Cao et al. 2021; Chen\net al. 2021), 70 cases are used for training, 10 for valida-\ntion and 20 for testing, and the average DSC is applied to\nevaluate the method.\nImplementation details. The MISSFormer is imple-\nmented based on PyTorch and trained on Nvidia GeForce\nArchitecture DSC↑ HD↓\nSegFormer B1 (Xie et al. 2021) 75.24 25.07\nSegFormer B5 75.74 21.62\nU-mlpFormer 75.88 27.22\nU-SegFormer 76.10 26.97\nTable 1: Accuracy on Synapse dataset of SegFormer and U-\nSegFormer\nArchitecture skip LN DSC ↑ HD↓\nU-SegFormer – – 76.10 26.97\nU-SegFormer w/skip cat – 78.14 28.77\nU-SegFormer w/skip add – 78.74 20.20\nSimple MISSFormer ✓ ✓ 79.73 20.14\nTable 2: Effectiveness of Simple Enhanced Mix-FFN, cat\ndenotes concatenation operation for skip connection, add de-\nnotes summation.\nRTX 3090 GPU with 24 GB memory. Different from previ-\nous work(Cao et al. 2021; Chen et al. 2021), whose model is\ninitialized by the pre-trained model on ImageNet, the MISS-\nFormer is initialized randomly and trained from scratch, so\nthe moderate data augmentation is conducted for all datasets.\nWe set the input image size as 224×224, the initial learning\nrate is 0.05 and poly learning rate policy is used, the max\ntraining epoch is 400 with a batch size of 24. SGD optimizer\nwith momentum 0.9 and weight decay 1e-4 is adopted for\nMISSFormer.\n4.2 Ablation Studies\nWe conduct ablation studies on the Synapse dataset to varify\nthe effectiveness of the essential component in our approach.\nWe set the SegFormer B1 as the baseline method, and the\nnumber of transformer blocks in every stage of encoder and\ndecoder is set to 2 to keep the same with other methods for\na fair comparison. All experiments are performed with the\nsame super parameter settings and trained from scratch.\nArchitecture selection. We replace the SegFormer B1\nMLP decoder with its transformer block and patch expand-\ning to make it U-shaped SegFormer, called “U-SegFormer”,\nand the results are shown in Table 1. As we can see, the\nU-SegFormer achieved better performance than SegFormer\nbecause the U-shaped model can fuse more corresponding\ndetails information with skip connection in each stage, al-\nthough the SegFormer integrates multi-level information.\nIn order to verify the priority of the U-shaped structure,\nthe naive U-shaped transformer with original mlp FFN and\nSegFormer B5 are also used as comparison results. Seg-\nFormer B5 has not achieved breakthrough results because of\nthe limitation of a huge number of parameters and medical\ndataset size. The results of Table 1 demonstrate the priority\nof U-shaped network structure based on the transformer for\nmedical image segmentation as before.\nAblation studies of Simple Enhanced Mix-FFN. Based\non the U-shaped transformer, we further perform the ex-\nperiments to validate the impact of the proposed Simple\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30\n35\n40grad norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7\n(a) U-mlpFormer.\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30grad norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7 (b) U-SegFormer.\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45grad norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7 (c) U-SegFormer w/skip.\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50grad norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7\n(d) U-SegFormer w/LN.\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55grad norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7 (e) Simple MISSFormer.\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45grad_norm\nlayer 0\nlayer 1\nlayer 3\nlayer 6\nlayer 7 (f) MISSFormer.\nFigure 4: The average L1 norm of gradients to the second fully connected weight in FFN for layer 0,1,3,6,7\n0 0.5 1 1.5 2 2.5 3 3.5 4\nstep 104\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6total_loss\nU_mlpFormer\nU_SegFormer\nSimple_MISSFormer\n(a) total loss.\n0 50 100 150 200 250 300 350 400\nepoch\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9dice\nU_mlpFormer\nU_SegFormer\nSimple_MISSFormer\n(b) dice.\nFigure 5: The convergence and evaluation results of different\nmethods.\nEnhanced Mix-FFN components. Table 2 reports the com-\nparison results. We ﬁrst design different skip connections:\nconcatenation and summation. Table 2 shows that both skip\nconnections boost the model performance considerably, and\nthe summation skip connection improved more than 2.6%.\nThen, we explore the gap caused by the direct embedding of\nconvolution. A layer norm is applied to align the feature and\ndistribution. We integrate it after the skip connection, which\nis called Simple MISSFormer, and it has 1% improvements\nbased on U-SegFormer w/skip. Finally, with the help of the\nredesigned feed-forward network, we improved feature dis-\ntributions and enhanced feature representations to generate\nan increasing promotion of 3.63 DSC, compared with the\nU-SegFormer baseline.\nAnalysis of Simple Enhanced Mix-FFN. In order to ex-\nplore the reasons for the effectiveness of the above improve-\nments, we observe the tendency of gradients to the second\nfully connected weight in FFN for different models with 8\nencoder layers. Fig.4 shows the average L1 norm of gradi-\nents of different layers of models. We observe that the gra-\ndients to the middle layer in U-SegFormer become 1/3 com-\npared with U-mlpFormer, which indicates that direct em-\nbedding of 3 ×3 convolution between fully connected lay-\ners makes the update of the middle layer slow and may not\nget better weights, although it supplements local informa-\ntion and makes slight improvements. In comparison, the two\nproposed components can solve this problem and make bet-\nter convergence and evaluation results, as shown in Fig.5.\nComparison of different methods to supplement local\nMethods DSC↑ HD ↓ Aorta Gallbladdr Kidney(L) Kidney(R) Liver Pancreas Spleen Stomach\nV-Net 68.81 - 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR 69.77 - 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50 U-Net 74.68 36.87 87.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16\nU-Net 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nR50 Att-Unet 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nAtt-UNet 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nR50 ViT 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransunet 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwin-Unet 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nMISSFormer S 80.74 19.65 85.31 66.47 83.37 81.65 94.52 63.49 91.51 79.63\nMISSFormer 81.96 18.20 86.99 68.65 85.21 82.00 94.41 65.67 91.92 80.81\nTable 8: Comparison with state-of-the-art methods on Synapse dataset. The results of other experiments are original from\nSwin-Unet(Cao et al. 2021).\nArchitecture DSC↑ HD↓\nU-mlpFormer 75.88 27.22\nU-SegFormer 76.10 26.97\nU-LocalViT 76.92 23.62\nSimple MISSFormer 79.73 20.14\nTable 3: Comparison of different methods to supplement lo-\ncal information\ninformation. In order to prove the necessity of supplement-\ning local information and the effectiveness of the proposed\nmethod, we compare it with other methods of supplement-\ning local information. Keeping the U-shaped structure un-\nchanged, the experiment is carried out by replacing the FFN\nin the transformer block with different modules, such as\nMix-FFN in SegFormer, residual blocks in LocalViT and the\nproposed Enhanced Mix-FFN in Simple MISSFormer. The\nresult is shown in Table 3.\nAchitecture step DSC ↑ HD ↓\nMISSFormer S\n1 79.73 20.14\n2 79.91 21.33\n3 80.74 19.65\nTable 4: Impact of recursive skip connection in Enhanced\nMix-FFN, step means recursive step.\nImpact of further feature consistency in Enhanced\nMix-FFN. Inspired by the above exploration and (Liu\net al. 2020), we extend the redesigned FFN of Sim-\nple MISSFormer to make it more general. We call it MISS-\nFormer S due to the absence of multi-scale feature integra-\ntion. We design experiments to assess the inﬂuence of fur-\nther consistency and distribution caused by different recur-\nsive steps, and its results are recorded in Table 4. The results\nimprove with the increase of the recursive step, which fur-\nther improved the insufﬁcient feature discrimination when\nconvolution is embedded directly in FFN.\nInﬂuence of Enhanced Transformer Context Bridge.\nWe conducted experiments to explore the role of multi-scale\ninformation in transformer-based methods on account of the\nAchitecture step bridge 4 DSC ↑ HD ↓\nMISSFormer S\n1 – 79.73 20.14\n2 – 79.91 21.33\n3 – 80.74 19.65\nMISSFormer\n1 ✓ 81.96 18.20\n2 ✓ 80.91 19.48\n3 ✓ 80.72 23.43\nTable 5: Impact of Enhance Transformer Context Bridge on\nrecursive skip connection of MISSFormer.\nAchitecture depth stage DSC ↑ HD ↓\nMISSFormer\n2 4/3/2/1 80.19 18.88\n4 4/3/2/1 81.96 18.20\n6 4/3/2/1 81.03 21.36\n4 4/3/2 80.65 18.39\n4 4/3 79.86 20.33\n4 4 79.56 20.95\nTable 6: Exploration of the bridge depth and multi-scale in-\nformation in MISSFormer.\nhierarchical features generated by the MISSFormer encoder.\nAs Table 5 shows, we list the results of MISSFormer S for\nintuitionistic comparison, and the performance of the model\nhas been improved to varying degrees except step equals 3\nafter embedding the Enhanced Transformer Context Bridge\ninto MISSFormer S, we call it as MISSFormer. We observe\nthat the model achieved the best performance to have a\n2.26% DSC improvement when the step is 1 and the growth\nrate gradually decreases with the increase of recursive step,\neven negative. We guess there is a balance between the\nrecursive step and Enhanced Transformer Context Bridge\nor between the number of layer norm and model capacity,\nwhich will be discussed in our future work. Besides, we also\ninvestigated how the bridge depth and multi-scale informa-\ntion integration affect model performance, and the results\nare saved in Table 6. For the exploration of bridge depth, 4\nis a suitable depth in MISSFormer because of the limited\nmedical data. For transformer-based hierarchical features,\nFigure 6: The visual comparison with previous state-of-the-art methods on Synapse dataset. Above the red line is good cases,\nand below it is a failed case, Our MISSFormer shows a better performance than other methods\n.\nthe more scale features are fed into the enhanced transformer\ncontext bridge, the more comprehensive the model can be\nlearned for long-range dependencies and local context.\nAchitecture Context Bridge DSC↑ HD ↓\nMISSFormer\nno 79.73 20.14\nmlp 79.54 17.26\nMix FNN 80.18 20.17\nEnhanced Mix FFN 81.96 18.20\nTable 7: Comparison of different modules in Transformer\nContext Bridge.\nThe necessity of global-local information in Trans-\nformer Context Bridge. To further explore the effective-\nness of the proposed module and the impact of each feature\ncomponent in multi-scale information aggregation, we take\nMISSFormer with the depth of 4 as a basis and replace the\nFFN module in Transformer Context Bridge as mlp FFN,\nMix FFN and Enhanced Mix FFN, respectively. The results\nare recorded in Table 7. We observe that mlp Context Bridge\nhas more accurate edge predictions, Mix FFN has more ac-\ncurate segmentation results due to the supplement of local\ninformation, while our Enhanced Mix FFN gets better seg-\nmentation performance and moderate edge prediction be-\ncause of the discriminative global and local features.\n4.3 Comparison with state-of-the-art methods\nThis section reports the comparison results of MISSFormer\nand previous state-of-the-art methods on the Synapse dataset\nand ACDC dataset.\nExperiment results on Synapse dataset. Table 8\npresents the comparison results of proposed MISSFormer\nand previous state-of-the-art methods. As shown in Table 8,\nthe proposed method achieved state-of-the-art performance\nin almost all measures, and it is worth mentioning that the\nMethods DSC↑ RV Myo LV\nR50 U-Net 87.55 87.10 80.63 94.92\nR50 Att-UNet 86.75 87.58 79.20 93.47\nR50 ViT 87.57 86.07 81.88 94.75\nTranUnet 89.71 88.86 84.53 95.73\nSwinUnet 90.00 88.55 85.62 95.83\nMISSFormer 90.86 89.55 88.04 94.99\nTable 9: Comparison to state-of-the-art methods on ACDC\ndataset. The results of other experiments are original from\nSwin-Unet(Cao et al. 2021).\nencoder of Transunet and Swin-Unet is pre-trained on Im-\nageNet, while the MISSFormer trained on Synapse dataset\nfrom scratch, which indicates that MISSFormer capture the\nbetter long-range dependencies and local context to make\nstrong feature representations. The visualization results are\nshown in Figure 6. It can be seen that our MISSFormer\nachieves better edge predictions and hard example seg-\nmentations compared to Tranunet and Swin-Unet, even in\nthe bad case. Comparing MISSFormer and MISSFormer S,\nMISSFormer has precise results and less false segmentation\nbecause of the integration of multi-scale information.\nExperiment results on ACDC dataset. We evaluate our\nmethod on the ACDC dataset in the form of MRI. Table 9\npresents the segmentation accuracy. MISSFormer maintains\nthe ﬁrst position because of the powerful feature extraction,\nwhich indicates the outstanding generalization and robust-\nness of MISSFormer.\n5 Conclusion\nIn this paper, we presented MISSFormer, a position-free and\nhierarchical U-shaped medical image segmentation trans-\nformer, which explored the global dependencies and local\ncontext capture. The proposed Enhanced Mix Block can\novercome the problem of feature discrimination limitation\ncaused by the direct embedding of convolution in feed-\nforward neural network effectively and make discrimina-\ntive feature representations. Based on these core designs, we\nfurther investigated the integration of multi-scale features\ngenerated by our hierarchical transformer encoder, which\nis essential for accurate segmentation. We evaluated our\nmethod on two different forms of datasets, the superior re-\nsults demonstrate the effectiveness and robustness of MISS-\nFormer.\nReferences\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian,\nQ.; and Wang, M. 2021. Swin-Unet: Unet-like Pure Trans-\nformer for Medical Image Segmentation. arXiv preprint\narXiv:2105.05537.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y . 2021. Transunet: Transform-\ners make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306.\nChen, L.-C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; and\nYuille, A. L. 2017. Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully\nconnected crfs. IEEE transactions on pattern analysis and\nmachine intelligence, 40(4): 834–848.\nChen, S.; Tan, X.; Wang, B.; and Hu, X. 2018. Reverse at-\ntention for salient object detection. InProceedings of the Eu-\nropean Conference on Computer Vision (ECCV), 234–250.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021a. Twins: Revisiting the design\nof spatial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 1(2): 3.\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.;\nand Shen, C. 2021b. Conditional positional encodings for\nvision transformers. arXiv preprint arXiv:2102.10882.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, D.-P.; Ji, G.-P.; Zhou, T.; Chen, G.; Fu, H.; Shen, J.; and\nShao, L. 2020. Pranet: Parallel reverse attention network for\npolyp segmentation. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention,\n263–273. Springer.\nFeng, S.; Zhao, H.; Shi, F.; Cheng, X.; Wang, M.; Ma, Y .; Xi-\nang, D.; Zhu, W.; and Chen, X. 2020. CPFNet: Context pyra-\nmid fusion network for medical image segmentation. IEEE\ntransactions on medical imaging, 39(10): 3008–3018.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J´egou, H.; and Douze, M. 2021. LeViT: a Vision Trans-\nformer in ConvNet’s Clothing for Faster Inference. arXiv\npreprint arXiv:2104.01136.\nGu, Z.; Cheng, J.; Fu, H.; Zhou, K.; Hao, H.; Zhao, Y .;\nZhang, T.; Gao, S.; and Liu, J. 2019. Ce-net: Context en-\ncoder network for 2d medical image segmentation. IEEE\ntransactions on medical imaging, 38(10): 2281–2292.\nHatamizadeh, A.; Yang, D.; Roth, H.; and Xu, D. 2021. Un-\netr: Transformers for 3d medical image segmentation. arXiv\npreprint arXiv:2103.10504.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7132–7141.\nHuang, H.; Lin, L.; Tong, R.; Hu, H.; Zhang, Q.; Iwamoto,\nY .; Han, X.; Chen, Y .-W.; and Wu, J. 2020. Unet 3+: A\nfull-scale connected unet for medical image segmentation.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 1055–\n1059. IEEE.\niek, z.; Abdulkadir, A.; Lienkamp, S. S.; Brox, T.; and Ron-\nneberger, O. 2016. 3D U-Net: Learning Dense V olumetric\nSegmentation from Sparse Annotation. In Springer, Cham.\nIsensee, F.; Jaeger, P. F.; Kohl, S. A.; Petersen, J.; and Maier-\nHein, K. H. 2021. nnU-Net: a self-conﬁguring method for\ndeep learning-based biomedical image segmentation. Na-\nture methods, 18(2): 203–211.\nIslam, M. A.; Jia, S.; and Bruce, N. D. 2020. How much\nposition information do convolutional neural networks en-\ncode? arXiv preprint arXiv:2001.08248.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707.\nLiu, F.; Ren, X.; Zhang, Z.; Sun, X.; and Zou, Y . 2020. Re-\nthinking skip connection with layer normalization. In Pro-\nceedings of the 28th International Conference on Computa-\ntional Linguistics, 3586–3598.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-\nlutional networks for semantic segmentation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 3431–3440.\nMou, L.; Zhao, Y .; Chen, L.; Cheng, J.; Gu, Z.; Hao, H.; Qi,\nH.; Zheng, Y .; Frangi, A.; and Liu, J. 2019. CS-Net: channel\nand spatial attention network for curvilinear structure seg-\nmentation. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, 721–730.\nSpringer.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention.\nSinha, A.; and Dolz, J. 2020. Multi-scale self-guided at-\ntention for medical image segmentation. IEEE journal of\nbiomedical and health informatics, 25(1): 121–130.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, W.; Chen, C.; Ding, M.; Li, J.; Yu, H.; and Zha, S.\n2021a. TransBTS: Multimodal Brain Tumor Segmentation\nUsing Transformer. arXiv preprint arXiv:2103.04430.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang,\nD.; Lu, T.; Luo, P.; and Shao, L. 2021b. Pvtv2: Improved\nbaselines with pyramid vision transformer. arXiv preprint\narXiv:2106.13797.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021c. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2021d. Uformer:\nA General U-Shaped Transformer for Image Restoration.\narXiv preprint arXiv:2106.03106.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021a. SegFormer: Simple and Efﬁcient De-\nsign for Semantic Segmentation with Transformers. arXiv\npreprint arXiv:2105.15203.\nXie, Y .; Zhang, J.; Shen, C.; and Xia, Y . 2021b. CoTr: Efﬁ-\nciently Bridging CNN and Transformer for 3D Medical Im-\nage Segmentation. arXiv preprint arXiv:2103.03024.\nZhang, Z.; Fu, H.; Dai, H.; Shen, J.; Pang, Y .; and Shao, L.\n2019. Et-net: A generic edge-attention guidance network\nfor medical image segmentation. In International Confer-\nence on Medical Image Computing and Computer-Assisted\nIntervention, 442–450. Springer.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyra-\nmid scene parsing network. InProceedings of the IEEE con-\nference on computer vision and pattern recognition, 2881–\n2890.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n6881–6890.\nZhou, Z.; Siddiquee, M. M. R.; Tajbakhsh, N.; and Liang, J.\n2018. Unet++: A nested u-net architecture for medical im-\nage segmentation. In Deep learning in medical image anal-\nysis and multimodal learning for clinical decision support,\n3–11. Springer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7532951235771179
    },
    {
      "name": "Encoder",
      "score": 0.6422545909881592
    },
    {
      "name": "Segmentation",
      "score": 0.633033037185669
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6188377737998962
    },
    {
      "name": "Transformer",
      "score": 0.5637675523757935
    },
    {
      "name": "Discriminative model",
      "score": 0.5586002469062805
    },
    {
      "name": "Image segmentation",
      "score": 0.5190120935440063
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42974650859832764
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.42580389976501465
    },
    {
      "name": "Computer vision",
      "score": 0.3645842671394348
    },
    {
      "name": "Engineering",
      "score": 0.11372599005699158
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}