{
  "title": "Affective Prompt-Tuning-Based Language Model for Semantic-Based Emotional Text Generation",
  "url": "https://openalex.org/W4392562716",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101314232",
      "name": "Zhaodong Gu",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2160740659",
      "name": "Kejing He",
      "affiliations": [
        "South China University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1969826591",
    "https://openalex.org/W2015712988",
    "https://openalex.org/W2096945473",
    "https://openalex.org/W2814750830",
    "https://openalex.org/W1968836297",
    "https://openalex.org/W4213223968",
    "https://openalex.org/W3217618723",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4221103790",
    "https://openalex.org/W4313143594",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4285219276",
    "https://openalex.org/W2993398598",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W4361298176",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W2608438166",
    "https://openalex.org/W3116170960",
    "https://openalex.org/W4309076649",
    "https://openalex.org/W2913946806",
    "https://openalex.org/W3212095079",
    "https://openalex.org/W4213065755",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W4315498239",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2798357113",
    "https://openalex.org/W2806227953",
    "https://openalex.org/W4323642664",
    "https://openalex.org/W2053782908",
    "https://openalex.org/W2616957565",
    "https://openalex.org/W1975238145",
    "https://openalex.org/W2134031328",
    "https://openalex.org/W3213225290",
    "https://openalex.org/W4296976275",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2914306653",
    "https://openalex.org/W3134112078",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W4321792464",
    "https://openalex.org/W2842348042",
    "https://openalex.org/W2962796276",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2929134750",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2997195635"
  ],
  "abstract": "The large language models based on transformers have shown strong text generation ability. However, due to the need for significant computing resources, little work has been done to generate emotional text using language models such as GPT-2. To address this issue, the authors proposed an affective prompt-tuning-based language model (APT-LM) equipped with an affective decoding (AD) method, aiming to enhance emotional text generation with limited computing resources. In detail, the proposed model incorporates the emotional attributes into the soft prompt by using the NRC emotion intensity lexicon and updates the additional parameters while freezing the language model. Then, it steers the generation toward a given emotion by calculating the cosine distance between the affective soft prompt and the candidate tokens generated by the language model. Experimental results show that the proposed APT-LM model significantly improves emotional text generation and achieves competitive performance on sentence fluency compared to baseline models across automatic evaluation and human evaluation.",
  "full_text": "DOI: 10.4018/IJSWIS.339187\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1 \nThis article published as an Open Access article distributed under the terms of the Creative Commons Attribution License\n(http://creativecommons.org/licenses/by/4.0/) which permits unrestricted use, distribution, and production in any medium,\nprovided the author of the original work and original publication source are properly credited.\n*Corresponding Author\n1\nAffective Prompt-Tuning-Based \nLanguage Model for Semantic-\nBased Emotional Text Generation\nZhaodong Gu, South China University of Technology, China\nKejing He, South China University of Technology, China*\n https://orcid.org/0000-0003-4116-037X\nABSTRACT\nThe large language models based on transformers have shown strong text generation ability. However, \ndue to the need for significant computing resources, little work has been done to generate emotional \ntext using language models such as GPT-2. To address this issue, the authors proposed an affective \nprompt-tuning-based language model (APT-LM) equipped with an affective decoding (AD) method, \naiming to enhance emotional text generation with limited computing resources. In detail, the proposed \nmodel incorporates the emotional attributes into the soft prompt by using the NRC emotion intensity \nlexicon and updates the additional parameters while freezing the language model. Then, it steers \nthe generation toward a given emotion by calculating the cosine distance between the affective soft \nprompt and the candidate tokens generated by the language model. Experimental results show that the \nproposed APT-LM model significantly improves emotional text generation and achieves competitive \nperformance on sentence fluency compared to baseline models across automatic evaluation and \nhuman evaluation.\nKEyWoRdS\nAffective Decoding, Discrete Emotion, Emotional Text Generation, Language Model, Prompt-Tuning\nINTR odUCTI oN\nArtificial intelligence (AI) has numerous applications in various fields, including cloud computing \n(Bisht & Vampugani, 2022; Ilyas et al., 2022), intelligent systems (Casillo et al., 2022; Deveci et al., \n2023), digital transformation (Gupta et al., 2023; Li et al., 2023), text detection (Yen et al., 2021; \nZhang et al., 2023), and more. However, AI generally lacks the ability to express human emotions. \nEmotional intelligence is an important branch of artificial intelligence, which has been widely studied \nand explored in the field of natural language processing (NLP) (Barbosa et al., 2022; Chopra et al., \n2022; Ismail et al., 2022). Emotional text generation, in particular, holds great potential for a variety \nof applications. Research shows that systems that can express emotions significantly improve user \nsatisfaction (Prendinger & Ishizuka, 2005; Abo-Hammour et al., 2013; Arqub & Abo-Hammour, \nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n2\n2014). In the field of dialogue systems, some studies have improved the generated responses by \nendowing the dialogue system with increased empathy towards human users (Colombo et al., 2019; \nAbo-Hammour et al., 2014; Abu Arqub et al., 2012). The controlled emotional text generation \nmodel not only enables a more meaningful dialogue between AI agents and humans, but also aims to \nestablish emotional connections with readers. This model proves beneficial for conversation therapy \nrobots, as it can produce suitable emotional responses according to the user’s psychological state \n(Sarivougioukas & Vagelatos, 2022).\nDespite the diverse range of applications for emotional text generation, how to better \nintegrate emotions into the generation model is still a difficult problem. Conventional methods \nfor emotional text generation primarily rely on discourse templates and manual rules, frequently \ndemonstrating limitations in addressing complex situations. Deep learning based methods have \ngained widespread usage in NLP tasks since the proposal of the recurrent neural network (RNN) \nEncoder-Decoder model by Cho et al. (2014). Vaswani et al. (2017) proposed the transformer \narchitecture, which has significantly advanced NLP tasks by enabling large-scale language model \ntraining on massive datasets (Radford et al., 2019). Although deep learning-based methods \nhave made significant progress in classification tasks (Chen et al., 2022), generating emotional \ntext remains a challenging task, in contrast to the success achieved in sentiment classification \n(Yang et al., 2019). In particular, generation models based on transformer architecture can \nleverage a significant amount of unlabeled data for training. Nevertheless, this approach makes \nit challenging to change the attributes of the generated text without either modifying the model’s \narchitecture or utilizing specific attribute data for fine-tuning (Keskar et al., 2019; Zhang et \nal., 2018). Therefore, it becomes more difficult to incorporate emotional attributes into the \npretrained model. However, most of the emotional text generation models today are still based \non the Seq2Seq model of the RNN architecture, failing to take advantage of the latest pretrained \ntransformer models such as GPT.\nTherefore, we propose an affective prompt-tuning based language model (denoted as APT-\nLM) in this paper. Different from fine-tuning, affective prompt-tuning only needs to fine-tune \nthe extra emotional attributes parameters freezing the whole model parameters. In detail, we first \nselect discrete emotional attributes from the NRC Emotion Intensity Lexicon and map them to \ncontinuous soft prompts through embedding layers of autoregressive language models such as \nGPT2. Then, during the training process, we only update these custom parameters. To enhance \nthe emotional expression of generated text, we further propose a simple and effective decoding \nmethod called affective decoding (denoted as AD) based on APT. This method can first obtain the \npossible candidate sets of the token generated by the model in each step of the decoding process. \nThen it calculates the cosine distance between the candidate sets and the soft prompt. Finally, we \nobtain the generated text that can maintain semantic coherence and enhance emotional expression. \nExperiment results show that our model significantly outperforms other models in automatic and \nhuman evaluation, not only in terms of accuracy in generating corresponding emotional texts, \nbut also in terms of emotional richness of texts and can maintain strong competitiveness in other \nmetrics such as perplexity, distinct-1-2-3, grammar, and fluency. The case study demonstrates \nthe flexibility and scalability of APT-LM, allowing the model to incorporate other attributes into \nthe model and generate specific attribute texts by adjusting custom parameters. The proposed \nmodel holds significant industrial relevance and offers a wide range of applications, seamlessly \nintegrating with diverse intelligent systems. It proves valuable in emotion analysis across social \nmedia platforms, enabling businesses and organizations to assess public sentiment regarding their \nproducts, services, or campaigns. Moreover, its utility extends to virtual therapy applications, where \nit delivers emotionally sensitive responses to users seeking mental health support or counseling \n(Zhang et al., 2023). The model’s impact also extends to entertainment and gaming, interactive \neducational content, healthcare chatbots, smart assistants imbued with emotional intelligence, and \npersonalized recommendation systems.\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n3\nThe main contributions of this paper can be summarized as follows:\n(1)  Proposing the affective prompt-tuning based language model (APT-LM) to generate emotional \ntext using state-of-the-art language model under limited computational resources.\n(2)  Designing the affective decoding (AD) method to further enhance the emotional expression of \nthe generated text while maintaining sentence fluency.\n(3)  Conducting experiments on two public datasets has demonstrated that our model outperforms \nother baseline models significantly in emotional expression, as observed through both automatic \nevaluation and human evaluation.\nRELATEd WoRKS\nEfficient Training Models\nFine-tuning has been the prevalent technique for using pretrained language models since the release \nof GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). However, it requires updating and \nstoring all the model’s parameters. Given the vast number of parameters in pretrained models, \nresearchers are beginning to study how to give full play to the ability of the large pretrained models \nunder the condition of limited hardware resources. These studies generally tend to freeze most of the \npretrained parameters and only perform gradient updates in the customized parameters or decoding \nprocess. These techniques can be broadly categorized into two groups: lightweight fine-tuning and \nplug-and-play decoding.\nOn lightweight fine-tuning, Rebuffi et al. (2017) propose the concept of an “adapter,” which \ninserts task-specific layers with a small number of parameters between each layer of the pretrained \nmodel. Houlsby et al. (2019) freeze the pretrained BERT-large model and add only 2-4% extra \nparameters with the “adapter” to achieve close performance on GLUE compared with the fine-tuning \nmethod. Li and Liang (2021) propose “prefix-tuning” to train pretrained models for natural language \ngeneration tasks instead of fine-tuning. First, each transformer layer begins with the addition of a \nsmall, continuously updated task-specific vector known as the prefix. Then, the model only optimizes \nthe prefix during training. Based on “prefix-tuning,” Lester et al. (2021) propose “prompt-tuning,” \nwhich still uses task-specific parameters but only adds and tunes them on the input layer. It is noted \nthat “prompt-tuning” uses the soft prompt (continuous vector) instead of the hard prompt, which is \nused in GPT-3 (Brown et al., 2020) for in-context learning or prompting with some discrete tokens. In \ncomparison, “adapter” emphasizes minimizing architectural modifications and achieving task-specific \nadaptation through a constrained set of parameters in additional adapter layers. On the other hand, \n“prefix-tuning” prioritizes updating a task-specific vector without altering the model’s architecture, \nand “prompt-tuning” goes a step further by incorporating task-specific parameters exclusively on \nthe input layer using a soft prompt. Unlike Lester et al. (2021) using a masked language model, we \nexplore using an autoregressive language model (Radford et al., 2019) for prompt-tuning in this paper.\nOn plug-and-play decoding, Dathathri et al. (2019) propose a model called PPLM, which \nintegrates the pretrained LM with several simple attribute discriminators and iteratively updates the \npast activation functions (hidden states) to steer the generation without training the LM. Pascual et al. \n(2021) propose a model-agnostic method called Keyword2Text (K2T). It works with any autoregressive \nlanguage model and doesn’t need any discriminators. Specifically, when considering a topic or \nkeyword as a hard constraint, K2T introduces an adjustment to the probability distribution across \nthe vocabulary, favoring semantically similar words. It allows the model to control text generation \nwithout training the LM. PPLM’s strength lies in its iterative refinement process, enabling fine-tuned \ncontrol, but it requires training additional attribute discriminators for controlling text generation. In \ncontrast, K2T does not necessitate extra training and only updates the probability distribution across \nthe vocabulary rather than modifying the hidden states of the model.\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n4\nEmotional Text Generation Models\nMany methods have been introduced to model the relationships between distinct emotions. We can \ngenerally classify these methods into two categories: discrete and dimensional emotion models. In \nthe dimensional or continuous emotion model, emotions are mapped to several dimensions (Russell, \n2003), including valence, arousal, and dominance. Valence stands for pleasure, with negative as \nunpleasant and positive as pleasant. Arousal evaluates the strength of emotion. Dominance stands for \nthe extent to which a participant is in control of the emotional state. Colombo et al. (2019) introduce \nan emotion-driven dialog system, designed to produce controlled emotional responses through the \nutilization of a continuous representation of emotions, employing a valence-arousal-dominance (VAD) \nlexicon (Mohammad, 2018). In this work, although we follow the discrete emotion models, emotions \nare projected to a continuous embedding layer instead of the VAD space.\nIn discrete emotion models, basic emotions can be classified into various categories (Ekman, \n1992; Plutchik, 2001), such as joy, sadness, and anger. Although discrete emotion models have \nbeen criticized for their different numbers of basic emotions in different models (Russell, 1994), the \nmajority of emotional text generation models are still based on the discrete emotion model. Zhou et \nal. (2018) propose an emotional chatbot model (ECM), which generates emotional texts by employing \nemotional category embeddings, internal emotional memory, and external emotional memory. Ghosh \net al. (2017) propose an affect-LM model that generates emotional dialogue texts across four specific \nemotion categories, each characterized by varying influence strengths. ECM employs gated recurrent \nunit (GRU) and attention to implement various modules for controlling the content and emotions \nof generated text. However, its complex mechanism can make model training and interpretation \nrelatively challenging. On the other hand, affect-LM utilizes long short-term memory (LSTM) to \neasily and efficiently control the emotional attributes and intensity of generated text, with the aid of \nan additional design parameter. However, the above methods need to fine-tune all parameters and do \nnot use the state-of-the-art pretrained language model, while our proposed method freezes the model \nparameters and only needs to update a small number of custom parameters to obtain competitive \nemotional text generation capability.\nIn order to better demonstrate the limitations of the previous work in this paper, we have \nsummarized them as shown in the Table 1.\nTable 1. Limitations of the Previous Work\nModel Method Limitation\nEfficent \nTraining \nModels\nFine-tuning (Radford et al., 2018) Need to train all parameters from scratch.\nAdapter (Rebuffi et al., 2017) Change the architecture of the model and need add \nadditional adapter layer.\nPrefix-tuning (Li & Liang, 2021) Need to add prefix to every single Transformer layer.\nPrompt-tuning (Lester et al., 2021) No explore the emotional text generation and nor the use \nautoregressive models for generation.\nPPLM (Dathathri et al., 2019) Require training additional attribute discriminators for \ncontrolling text generation.\nK2T (Pascual et al., 2021) Cannot be applied in emotional text generation.\nEmotional \nText \nGeneration \nModels\nColombo et al. (2019) Do not follow the Ekman’ s emotion theory.\nECM (Zhou et al., 2018) Do not use the language model like GPT-2\nAffect-LM (Ghosh et al., 2017) Do not use the language model like GPT-2\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n5\nProposed Method\nOur proposed model for emotional text generation based on prompt-tuning is illustrated in Figure 1, \nwhich mainly consists of two modules: affective prompt-tuning and affective decoding. The affective \nprompt-tuning first converts the discrete emotional tokens into a continuous space and integrates them \ninto the soft prompt initialization process. Then, it updates the extra emotional attributes parameters \nthrough the soft prompt while freezing the whole model parameters during training. The affective \ndecoding method enhances the emotional expression of the text by calculating the cosine distance \nbetween the candidate tokens and the soft prompt after affective prompt-tuning. With these modules, \nAPT-LM can easily integrate with autoregressive language models, such as GPT-2, as it does not \nrequire changing the architecture of the model, only defining the emotion parameters to be trained \nthrough the embedding layer of the language model. APT-LM’s architecture is designed to efficiently \nscale across diverse hardware configurations, making it adaptable to both smaller setups and more \nextensive computing environments. The model’s flexibility in terms of fine-tuning and customization \nallows developers to tailor APT-LM to their specific use cases by designing different custom soft \nprompts. This adaptability enhances user experience by providing a more personalized and efficient \nsolution that aligns with individual project requirements.\nAffective Prompt-Tuning\nAssume we have an autoregressive language model based on transformer architecture, such as GPT. \nWith a input sequence of tokens X x x x\nn={ , ,..., }1 2 , LM is trained to calculate the unconditional \nprobability of the sequence P X( )  and updates the model parameters q. The probability can be \nexpressed as the product of conditional probabilities through the recursive application of the chain \nrule, represented as:\nP X P x x\ni\nn\ni iθ θ( ) ( | )= ∏ <  (1)\nFigure 1. Overview of the Proposed Model for Emotional Text Generation\n\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n6\nUsually, prompting is done by adding a sequence of tokens Pr  in the front of the input sequence \nX . In GPT-3, the prompt can be represented by a sequence of tokens Pr pr pr prn= { }1 2, , ..., , which \nare all part of the model embedding table and are still affected by the model parameters q.\nHowever, our method enables the prompt to have an independent emotional parameter qPr  while \nfreezing the parameter q of the original model. Then, our new condition generation model can be \nformulated as follows:\nP X P x Pr x\nPr Pr\ni\nn\ni iθ θ( ) ( | [ ; ])= ∏ <  (2)\nIn the autoregressive transformer model, it is assumed that the activation at time step i  is \nh h hi i i\nn=[ ; ...; ]( ) ( )1 , where hi\nj( )  is the activation of the j -th transformer layer at time step i . Then, \nthe model computes h i<  as a function of xi  and the past activations in its left context, as follows:\nh L M x hi i i= <( , )  (3)\nwhere h i<  is generally used to calculate the distribution of the next token. The distribution can \nbe formulated as P x h s oftmaxW h\nPr Pri i i\nn\nθ θ( | ) ( )( )\n< = ⋅ . Therefore, we can transform discrete tokens \ninto continuous word embedding space, and its effect will propagate from bottom to top to all \ntransformer activation layers. Eventually, it affects the generated text from left to right.\nWe propose an efficient and simple method of initializing soft prompts with emotional attributes. \nSpecifically, we apply the NRC Emotion Intensity Lexicon (Mohammad & Kiritchenko, 2018) for \nselecting words to initialize the affective soft prompts based on the descending emotional intensity \nof each word. This is because NRC Emotion Intensity Lexicon provides emotion classification and \ncorresponding emotional intensity for each word, covering the six basic emotions of Ekman’s emotion \ntheory. In this paper, we focus on generating emotional text based on Ekman’s emotion theory. As \nshown in Figure 1, we use the words with the emotion of joy in the NRC lexicon as an example. First, \nwe map the discrete sequence of tokens \nPr  to the continuous embedding space through the embedding \nlayer of the LM, and get the soft prompt Prd\npr d∈ × , where pr  is the length of the prompt and d  \nis the dimension of the embedding space. Second, we concatenate the soft prompt to the embedding \nof the input sequence X . Finally, we only update the parameters qPr  when training the LM to generate \noutput sequence Y , as follows:\nmax log ( | ) m ax log |P Y P x h\nPr Pr\ni\nn\ni iθ θX = ( )∑ <  (4)\nIt is noted that APT-LM utilizes the generalization and knowledge obtained from pre-training \nlanguage model for emotional text generation. If traditional fine-tuning methods are used to control \nLM for emotional text generation, a large amount of emotion annotated data needs to be used to update \nall parameters of LM. On one hand, the datasets are difficult to obtain due to labor costs, and on the \nother hand, training LM from scratch requires a large amount of computing resources. APT-LM can \nutilize the emotional information provided by the NRC lexicon and the embedding layer of LM to \ngenerate custom training parameters. During the training process, LM is frozen, thus maintaining the \nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n7\nadvantage of generating text using LM. Updating the custom parameters ensures that LM is guided \nto generate emotional text.\nAffective decoding\nAfter prompt-tuning the LM with emotional attributes, the generation can be guided by an affective \nsoft prompt. To enhance the emotional expression of the generated text, we propose affective decoding \n(AD). For each decoding step, AD aims to: (1) generate output from the most likely candidate set \npredicted by the LM and (2) generate output with a meaningful emotional association with the soft \nprompt. As a result, the generated text can increase the emotional expression while maintaining the \nsemantic coherence associated with the prefix. Formally, our method can be formulated as follows:\nsim m ax cos h h j len Prw p rj\n= ( ) ≤ ≤{ , : ( )}0  (5)\nC k P w x s im\nw W\ni\nk Pr\n( ) ( | )\n( )∈\n<= + ×θ λ  (6)\nx C ki = arg max ( ) (7)\nwhere W k( )  is the set of the most likely k  tokens generated by the LM after affective prompt-\ntuning and P w x\nPr iθ ( | )< is the probability of candidate w predicted by the LM. cos h hw p rj\n( , )  measures \nthe cosine similarity between the candidate token w and the affective soft prompt. This is due to the \ninclusion of emotional information in the affective soft prompt, which guides the language model \n(LM) in generating emotional texts. The cosine distance serves as a metric to gauge the similarity \nbetween the distribution of the next token and the affective soft prompt. Intuitively, the closer \nw is \nto the affective soft prompt, the more emotional information it contains, and thus the more likely it \nis to generate an emotionally rich response. The candidate representation hw  is computed by the LM \ngiven the concatenation of x i<  and w. As depicted in Figure 1, we assume k  is 5, l is 0.6, and \ninput is “I think love is.” First, APT-LM will generate 5 most likely candidate words as the next \ngeneration token, with generation weights of {(a, 0.7), (the, 0.6), (beautiful, 0.5), (painful, 0.4), (gift, \n0.3)}. Second, by calculating the similarity between candidate words and emotional soft cues using \nequation (5), we can obtain: {(beautiful, 0.8), (gift, 0.6), (painful, 0.3), (a, 0.2), (the,0.1)}. Finally, \nthe final generated word weights can be obtained through equation (6), which are {(beautiful, 0.98), \n(a, 0.82), (gift, 0.66), (the, 0.66), (painful,0.52)}. So, the next generation token is “beautiful,” resulting \nin the output text “I think love is beautiful.” The generation process will continue until the end \ncondition is reached.\nExperimental Setup\nDatasets\nWe conduct experiments on two public datasets that include text and corresponding emotional labels. \nGoEmotions (Demszky et al., 2020) is a fine-grained multi-label English emotion dataset that consists \nof 58,000 Reddit comments with artificial emotion category tags. It includes 27 emotion categories, \nincluding one neutral category, and provides the mapping relationship of 27 emotion categories to \nEkman’s six basic emotion categories. ISEAR (Scherer & Wallbott, 1994) is a single-label categorical \nemotion corpus that contains 7,666 sentences but covers only five basic emotions of Ekman’s emotion \ntheory. We use only the train dataset covered with Ekman’s emotion theory for training. The statistics \nof datasets are shown in Table 2.\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n8\nBaseline Models\nTo the best of our knowledge, there is little research to address emotional text generation with prompt-\ntuning, and we focus on exploring the ability of the LM rather than RNN-based models for emotional \ntext generation. Thus, we compare our method with the following baseline models:\nGPT2-Medium (Radford et al., 2019): It is the 355M parameter version of GPT-2. To evaluate the \neffectiveness of the proposed method and model, we compare the vanilla GPT2-Medium with \nour proposed model.\nAffective-PPLM (Goswamy et al., 2020): It serves as a plug-and-play extension of the PPLM \nframework proposed by Dathathri et al. (2019). It affords flexibility in selecting the base text \ngeneration model, specifying the emotion category from a range of eight basic emotions and \noffering fine-grained control over the intensity of emotion within each category.\nCTRL (Keskar et al., 2019): It provides a conditional transformer language model with control \ncodes. We add extra new codes for generating emotional text based on Ekman’s emotion theory \nby fine-tuning CTRL on GoEmotions and ISEAR datasets.\nEvaluation Metrics\nIn automatic evaluation, the evaluation of emotional text generation is still a challenging problem \nbecause there is no unified standard to evaluate the emotional factors well. Therefore, we intend to \nuse the proportion of emotional tokens in the generated text and the external text emotion classifier \nfor emotional evaluation. Then, we combine it with human evaluation to verify the effectiveness of \nour model.\nAutomatic Evaluation\nPerplexity (denoted as PPL) is commonly employed to assess the fluency of generated text. Lower \nperplexity is crucial in applications like machine translation, chatbots, and text generation, where \nnatural and coherent language output is essential. A model with lower perplexity is more likely to \ngenerate text that is contextually appropriate and coherent. Distinct-N (denoted as Dist-n) quantifies the \ndiversity of text in passages by assessing the count of distinct n-grams across all samples, divided by \nthe total number of words (Li et al., 2016). In applications where variety in language is important, such \nas creative writing, content generation, or dialogue systems, a model with high Distinct-N is preferred. \nIt ensures that the model can produce diverse and interesting output. Grammaticality (denoted as \nGrammar) verifies the grammatical correctness of the generated text (Warstadt et al., 2019). In \napplications where grammatical accuracy is crucial, such as content generation for professional use, \neducational tools, or formal communication, a model with high grammaticality is desired to ensure \nthe quality of the generated content. Emotion intensity (denoted as EmoInt) measures the emotional \nexpression of the generated text at the word level. In applications where emotional tone matters, like \nsentiment analysis, creative writing, or dialogue systems, a model with accurate emotion intensity \ncan generate text that conveys the desired emotional tone effectively. Specifically, we calculate the \nemotion intensity of each word in the output based on the NRC lexicon and average the result to \nTable 2. Statistics of Datasets\nDataset Types of emotion overall\nanger disgust fear joy sadness surprise\nGoEmotions 7,022 1,013 929 21,733 4,032 6,668 41,397\nISEAR 1,096 1,096 1,095 1,094 1,096 - 5,477\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n9\nobtain the emotion intensity of the output. It is noted that not every word exists in the NRC lexicon. \nFor words not in the lexicon, we use the emotion word embeddings (EWE) (Agrawal et al., 2018) to \ncalculate the cosine distance between the word and the corresponding emotion category. It is because \nthe EWE has the capability to map emotionally similar words into proximate spaces while positioning \nemotionally dissimilar words at greater distances from each other. Accuracy (denoted as Acc) measures \nthe emotional expression of the generated text at the sentence level. In emotion-sensitive applications, \nsuch as chatbots, virtual assistants, or customer support systems, accurate emotional expression is \ncrucial for effective communication. A model with high accuracy in emotion expression is desirable \nfor such applications. It evaluates whether the generated text accurately expresses the corresponding \nemotion. We use an external emotion classifier\n1 on hugging face to verify whether the model is capable \nof generating text that corresponds to the given emotion. Its evaluation accuracy is 66%.\nHuman Evaluation\nFollowing Dathathri et al. (2019), we randomly select 20 prefixes to generate the continuation with \ndifferent models and decoding methods (normal decoding and affective decoding). Then, we ask three \nhuman annotators to compare the generated text against two criteria: Fluency and AffectInt. AffectInt \nmeasures the emotional expression of a continuation, while Fluency evaluates the grammatical \nproblem. According to the 5-point Likert scale theory, the scores assigned to all these evaluation \nmetrics span from 1 to 5, corresponding to strongly disagree, disagree, not necessarily, agree, and \nstrongly agree, respectively. During the evaluation process, all annotators were kept unaware of the \nmodel responsible for generating the continuation to ensure the integrity and validity of the results.\nImplementation details\nIn this paper, we fine-tune CTRL on an open platform\n2 that provides free computing resources with \na Tesla V100. We adopt GPT2-Medium to generate emotional text and conduct other experiments \non a RTX3060. For all models, we use the same 35 prefixes following Dathathri et al. (2019) to \ngenerate 10 continuations for each prefix, a total of 350 output and use both the top-k samples and \nthe top-p samples during the decoding. The top-k is set to 10 and the top-p is set to 0.9. On affective \nprompt-tuning, the maximum length of the text is set to 64. Following the transformer fine-tuning \nconfigure\n3, the other parameters are the same as the default configure.\nEXPERIMENTAL RESULTS\nAutomatic Evaluation Results\nThe evaluation results of different emotional generation models are reported in Table 3. The best \nresult in each column is highlighted in bold. The vanilla GPT2-Medium performs the worst in \nemotional expression, with results of 0.152 and 0.274 for Acc and EmoInt metrics, respectively. \nTable 3. Automatic Evaluation Results of Different Emotional Generation Models\nModel PPL Dist-1 Dist-2 Dist-3 Grammar Acc EmoInt\nGPT2-Medium 41.87 0.288 0.737 0.906 0.797 0.152 0.274\nAffective-PPLM 34.86 0.413 0.769 0.863 0.789 0.333 0.560\nCTRL GoEmotions 36.25 0.214 0.542 0.663 0.692 0.336 0.397\nISEAR 37.71 0.326 0.658 0.676 0.723 0.286 0.368\nAPT-LM GoEmotions 38.79 0.209 0.637 0.836 0.746 0.556 0.646\nISEAR 40.07 0.237 0.684 0.883 0.785 0.528 0.654\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n10\nThis indicates that a language model without fine-tuning or prompts struggles to leverage the \nemotional knowledge acquired during pre-training. Consequently, it faces difficulty accurately \ngenerating text corresponding to the required emotional categories, and the generated text contains \nless emotional information. Affective-PPLM shows superior performance in PPL, Dist-1, and Dist-2 \nmetrics, which can be attributed to its use of a new loss term defined by Gaussian functions. This \nguides the model’s generation towards the given emotion during the inference stage, improving \nthe diversity and fluency of the generated text. However, it cannot accurately generate text \ncorresponding to the given emotion, as reflected in its score of 0.333 for Acc metric. CTRL faces \nchallenges in accurately and effectively generating emotional text as well. Its approach involves \nconditioning text generation on a control code, serving as an attribute variable that signifies a data \nsource. Nonetheless, reliance on a specific control code may decrease sample diversity and hinder \nemotional text generation, as the generated samples tend to resemble the data source associated with \nthe control code. Furthermore, CTRL provides each emotional control code for a large language \nmodel trained from scratch, which can be expensive. In contrast, our model requires updating only \na small number of custom parameters to utilize the knowledge acquired by the language model \nduring pre-training for emotional text generation.\nAs shown in Figure 2, we observe that APT-LM outperforms other benchmarks in terms of \nemotional metrics. APT-LM achieves the best average results on Acc and EmoInt metrics, with \nvalues of 0.542 and 0.650, respectively. Table 3 shows that, compared to the vanilla GPT2-Medium, \nAPT-LM significantly improves emotional text generation while keeping competitive results in \nterms of PPL, Dist-1-2-3, and Grammar. This is because it leaves the parameters of the original \nlanguage model unchanged, preserving the emotional knowledge acquired during its pre-training \nstage. Meanwhile, the affective soft prompt is appended at the beginning to the embedding of the \ninput sentence, guiding the model to generate emotional text in a left-to-right fashion. Although \nAffective-PPLM shows improvement in emotion expression at the word level with an EmoInt metric \nof 0.560, it falls short in effectively conveying corresponding emotions at the sentence level. In \ncontrast, APT-LM not only utilizes affective soft prompts to guide emotion expression at the sentence \nlevel but also strengthens the model’s emotional expression at the word level through affective \ndecoding. Moreover, CTRL requires expensive computation resource to train the model from scratch \nfor emotional text generation, although it can improve sentence fluency to some extent. However, \nFigure 2. Average Results of Emotional Metrics for Different Models\n\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n11\nthis undermines the generalization and knowledge that LM obtained during the pre-training stage. In \ncontrast, in the case of limited computational resources, APT-LM can outperform CTRL by freezing \nthe model parameters and updating additional affective parameters with affective prompt-tuning. \nThis illustrates that APT-LM effectively employs custom parameters, specifically the affective \nsoft prompt, to accurately convey emotions. Additionally, APT-LM employs affective decoding \nto assess the emotional similarity between the distribution of the next token and the affective soft \nprompt, thereby guiding the model to generate emotionally rich text. Also, we find that our model \ndoes not achieve the best performance on the Dist-N metrics, which is because the affective soft \nprompt steers the generation and the affective decoding reduces the probability distribution of other \ntokens that are not related to the corresponding emotion, resulting in the generation of less diverse \nbut more emotional text. It is worth noting that, as shown in Table 3, APT-LM performs better in \nthe PPL metric on the GoEmotions dataset compared to GPT2-Medium, with a score of 38.79, and \nstill maintains strong competitiveness in text fluency. In conclusion, APT-LM focuses on assessing \nlanguage models’ proficiency in generating emotional texts. In comparison to alternative models, \nAPT-LM excels not only in producing precise emotional texts, as indicated by the Acc metric, but \nalso in incorporating a higher degree of emotional information, as reflected in the EmoInt metric. \nThis underscores the effectiveness of our approach.\nHuman Evaluation Results\nIn human evaluation, we focus on the fluency and affective intensity of the generated text. As \nshown in Table 4 and Figure 3, we observe that APT-LM outperforms GPT2-Medium, Affective-\nPPLM, and CTRL on AffectInt, with improvements of +0.92, +0.41, and +0.74, respectively. \nThe observed improvements in AffectInt scores for APT-LM can be attributed to its specialized \ndesign for emotional text generation, the effective use of affective soft prompts and decoding \nmechanisms in comparison to other models. Furthermore, as shown in Table 4, APT-LM still \noutperforms other models in terms of Fluency, with a score of 3.63. Human annotators perceive \nthat the text generated by our model conveys more emotional information and is closer to human \nnatural language, contributing to APT-LM’s superiority in the Fluency metric compared to other \nmodels. In conclusion, APT-LM is capable of generating coherent and emotional text, as it benefits \nfrom steering generation toward a given emotion by affective prompt-tuning (APT) and affective \ndecoding (AD).\nParameter Analysis\nTo investigate the impact of affective coefficient l in equation (6) and the length of the soft prompt \npr  on the emotional expression capability of the model, we conduct parameter analysis experiments \nsimilar to automatic evaluation. l can take values from {0, 0.2, 0.4, 0.6, 0.8, 1} and pr  can be taken \nfrom {1, 10, 20}. We average the results of the two datasets and present the visual outcomes in terms \nof sentence fluency and emotional text generation.\nAs shown in Figure 4 (a), the PPL metric increases with the parameter l increasing when the \naffective coefficient is greater than 0.6. When the length of the soft prompt is equal to 20, PPL is \nTable 4. Human Evaluation Results of Different Emotional Generation Models\nModel Fluency AffectInt\nGPT2-Medium 3.52 2.93\nAffective-PPLM 3.61 3.44\nCTRL 3.59 3.11\nAPT-LM 3.63 3.85\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n12\nrelatively low. From Figure 4 (b) to (d), we observe that Distinct-N shows a downward trend with \nthe increase of the affective coefficient (Distinct-N always reaches its maximum value when l is \n0). Similarly, the same phenomena can be found in Figure 4 (e) and the reason for this is the same \nas we mentioned in the analysis of automatic evaluation results, which is affective decoding makes \nthe text generations less diverse but more emotional.\nFor the results in Figure 5, APT-LM presents better performance on both Acc and EmoInt metrics \nwhen \npr = 20 . Furthermore, when l is equal to 0.6, it effectively enhances the model’s emotional \nexpression capability without excessively shifting the tokens distribution over the vocabulary. Hence, \nthe parameter \nl is set to 0.6 and pr  is set to 20 for a better generation in this paper.\nAblation Study\nTo further verify our proposed decoding method, we conduct ablation experiments for affective \ndecoding while maintaining the parameters we discussed above. Table 5 shows the results of the \nablation study, where w/o AD means we do not adopt affective decoding for the model. It is not \nsurprising that APT-LM shows poorer performance in terms of text fluency and diversity, but stronger \nability in emotional expression. This is because AD can further strengthen the ability of emotional text \ngeneration by shifting the token distribution over the vocabulary toward a given emotion. However, \nit is worth noting that the APT-LM achieves the best performance in terms of Fluency and AffectInt. \nThis might be due to the fact that the texts generated by the APT-LM give more information about \nemotions, which is more in line with the language of humans.\nCase Study\nTable 6 presents the emotional text generation results of different models. The prefix is underlined, \nand the words related to the given emotion are highlighted in bold. We set a maximum text length \nof 64. If the generated text exceeds this limit or if the model prematurely inserts a sentence-\nending symbol, we use ellipses to indicate the truncation. For the joy emotion, the vanilla GPT2-\nMedium can generate a fluent but not emotional sentence. Affective-PPLM and CTRL generate \nweak emotional text compared to our model. Although the text generated by our model contains \nthe emotional word “love” several times, it still maintains a high level of linguistic fluency. For \nthe sadness emotion, we use the same prefix for comparison. It is clear that our model tends to \ngenerate more emotional expression and the Affective-PPLM fails to maintain good language \nFigure 3. Human Evaluation Results of Different Emotional Generation Models\n\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n13\nfluency with the same emotional word “prison”. The CTRL model produces a higher number of \nemotional statements compared to the vanilla GPT2-Medium, although it does not reach the level \nof emotional expressiveness demonstrated by our model. In conclusion, by using affective prompt-\ntuning and affective decoding, APT-LM produces responses that not only convey information \nbut also resonate with the emotional undertones present in the input. Furthermore, APT-LM \nexhibits the capability to generate emotional text by adjusting the emotional information within \nsoft prompt, encompassing emotions like joy and sadness. Theoretically, APT-LM can extend \nFigure 4. Effect of the Parameter l and pr  on Sentence Fluency\n\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n14\nthis proficiency to effectively incorporate other additional attribute information such as topic, \nsentiment, gender, and personality. This greatly enhances the scalability and generalization of \nthe model, allowing APT-LM to be applied to more fields, such as recommendation systems \nand personalized AI. This methodology empowers the model to make efficient use of its potent \nlanguage generation capabilities. Notably, APT-LM accomplishes this with minimal reliance on \nlabeled data, underscoring its prowess in guiding controlled text generation while fully harnessing \nthe robust text generation capabilities inherent in language models. This advancement contributes \nto a deeper comprehension of the emotional context inherent in human communication within the \nfield of natural language understanding and processing.\nCoNCLUSI oN\nIn this paper, we proposed an affective prompt-tuning based language model (APT-LM) and an \naffective decoding (AD) method for steering generation toward a given emotion. Due to the need for \nsignificant computing resources, little work has been done to generate emotional text using language \nmodel like GPT-2. With limited computational resources, APT-LM only updates the additional \nparameters while freezing the language model during training. Compared with the baseline model, \nour model shows significant enhancement in emotional text generation in both automatic and human \nevaluation, while maintaining competitiveness in sentence fluency and diversity. Moreover, we \nvalidated the effectiveness of AD through ablation experiments and demonstrated the powerful \nFigure 5. Effect of the Parameter l and pr on Emotional Text Generation\nTable 5. Ablation Study on Different Datasets\nDataset Model Automatic Evaluation Human Evaluation\nPPL Dist-1 Dist-2 Dist-3 Grammar Acc EmoInt Fluency AffectInt\nGoEmotions APT-LM 38.79 0.209 0.637 0.836 0.746 0.556 0.646 3.62 3.88\nw/o AD 37.87 0.22 0.654 0.853 0.801 0.469 0.537 3.55 3.65\nISEAR APT-LM 40.07 0.237 0.684 0.883 0.785 0.528 0.654 3.64 3.82\nw/o AD 38.21 0.240 0.697 0.891 0.821 0.488 0.564 3.58 3.71\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n15\nemotional text generation ability of APT-LM through case study. Additionally, APT-LM has strong \nscalability and can be used in multiple applications by adjusting custom training parameters, \nsuch as chatbots, dialogue systems, creative writing, educational tools, sensory analysis, virtual \nassistants, and customer support systems. Although APT-LM demonstrates strong emotional text \ngeneration capabilities, it can only generate single attribute text at a time and may generate harmful \nor misleading information. For future work, we can enhance the generated text of APT-LM by \nemploying multi-task learning, integrating additional attributes such as topic. Additionally, through \njoint training, we can train an auxiliary detoxification classifier to guide APT-LM in generating text \nthat is harmless. Furthermore, delving into the interpretability of the proposed model represents \na promising direction for future research.\nAUTH oR NoTE\nCorrespondence concerning this article should be addressed to Kejing He, School of Computer Science \nand Engineering, South China University of Technology. E-mail: kejinghe@ieee.org\ndATA AVAILABILITy\nThe data used to support the findings of this study are included within the article.\nTable 6. Case Study Results\nEmotion Model Generation\nJoy GPT2-\nMedium\nIn brief:\\n\\nI will be making a statement on Wednesday and I will post it here on the site.\\n\\\nnYou can also find me in Twitter, Face- book, YouTube and here on reddit.\\n\\nPlease let me \nknow if there is anything I’ve missed or can add.\\n\\nThanks for reading...\nAffective-\nPPLM\nIn brief, the new law will allow people convicted of drug offenses to receive credit for any \ncommunity service they have served. That could help to keep some drug offenders off welfare. \nThe law’s critics are hoping that the Supreme Court’s landmark ruling will make a victory \ncelebration more common...\nCTRL\nIn brief, I have to say that it is a very good book. It has some great tips on how to get the most \nout of your trip in an efficient way. The only thing missing from my opinion was more detail \nabout what you can do when there are no other options for getting around...\nAPT-LM\nIn brief, you can see this kind of love is one of the best ways to bring people to your life. Love \nis a gift that cannot be faked and is always genuine. So, why not use this love to build your life \nin the most beautiful way possible, like loving to learn! Here are 6 Love Skills that...\nSadness GPT2-\nMedium\nIn brief, one can understand the necessity of a state that is willing to pay to protect a population \nagainst non-state violence. It is understandable why the state pays for it.\\n\\nA state is willing to \naccept a certain amount of income as payment for protection from violence, if that is the only \nmeans available to pay for...\nAffective-\nPPLM\nIn brief, when you are in the midst of a legal matter, the state attorney general is the ultimate \nlaw judge. If you’re accused of a crime, the prosecutor is your trial lawyer and the state attorney \nis your jail.\\nThe criminal court is a judicial prison, jail prison prison prison...\nCTRL\nIn brief, the relationship was over . Sad but true. I’m so sorry for your loss and sadness you \nmust feel right now. It is a very difficult thing to deal with, it’s hard to understand what he went \nthrough the pain of losing his wife...\nAPT-LM\nIn brief \\n\\nI am a doctor with about ten years experience in treating cancer, I also have some \nexperience working on people suffering from depression and anxiety, and other health related \nanxiety as well. I have also worked on patients suffering from the effects of chronic pain and \nother medical conditions including cancer...\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n16\nCoNFLICT oF INTEREST\nThe authors declares no conflicts of interest.\nFUNdING STATEMENT\nThis work was supported by the Fundamental Research Funds for the Central Universities \n(2022ZYGXZR004).\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n17\nREFERENCES\nAbo-Hammour, Z., Abu Arqub, O., Momani, S., & Shawagfeh, N. (2014). Optimization solution of Troesch’s \nand Bratu’s problems of ordinary type using novel continuous genetic algorithm. Discrete Dynamics in Nature \nand Society, 2014, 1–15. doi:10.1155/2014/401696\nAbo-Hammour, Z., Alsmadi, O., Momani, S., & Abu Arqub, O. (2013). A genetic algorithm approach for prediction \nof linear dynamical systems. Mathematical Problems in Engineering, 2013, 1–12. doi:10.1155/2013/831657\nAbu Arqub, O., Abo-Hammour, Z., Momani, S., & Shawagfeh, N. (2012). Solving singular two-point \nboundary value problems using continuous genetic algorithm. Abstract and Applied Analysis , 2012, 1–25. \ndoi:10.1155/2012/205391\nAgrawal, A., An, A., & Papagelis, M. (2018). Learning emotion-enriched word representations. Proceedings of \nthe 27th international conference on computational linguistics, 950–961.\nArqub, O. A., & Abo-Hammour, Z. (2014). Numerical solution of systems of second-order boundary value \nproblems using continuous genetic algorithm. Information Sciences, 279, 396–415. doi:10.1016/j.ins.2014.03.128\nBarbosa, A., Bittencourt, I. I., Siqueira, S. W., Dermeval, D., & Cruz, N. J. (2022). A context-independent \nontological linked data alignment approach to instance matching. International Journal on Semantic Web and \nInformation Systems, 18(1), 1–29. doi:10.4018/IJSWIS.295977\nBisht, J., & Vampugani, V. S. (2022). Load and cost-aware min-min workflow scheduling algorithm for \nheterogeneous resources in fog, cloud, and edge scenarios. International Journal of Cloud Applications and \nComputing, 12(1), 1–20. doi:10.4018/IJCAC.2022010105\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, \nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. \nM., Wu, J., Winter, C., & Amodei, D. et al. (2020). Language models are few-shot learners. Advances in Neural \nInformation Processing Systems, 33, 1877–1901.\nCasillo, M., Gupta, B. B., Lombardi, M., Lorusso, A., Santaniello, D., & Valentino, C. (2022). Context aware \nrecommender systems: A novel approach based on matrix factorization and contextual bias. Electronics (Basel), \n11(7), 1003. doi:10.3390/electronics11071003\nChen, X., Li, J., & Zhang, Y. F. (2022). Multidirectional gradient feature with shape index for effective texture \nclassification. International Journal on Semantic Web and Information Systems , 18(1), 1–19. doi:10.4018/\nIJSWIS.312183\nCho, K., van Merrienboer, B., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). \nLearning phrase representations using RNN encoder-decoder for statistical machine translation. Proceedings of \nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1724-1734. doi:10.3115/\nv1/D14-1179\nChopra, M., Singh, S. K., Sharma, A., & Gill, S. S. (2022). A comparative study of generative adversarial \nnetworks for text-to-image synthesis. International Journal of Software Science and Computational Intelligence, \n14(1), 1–12. doi:10.4018/IJSSCI.300364\nColombo, P., Witon, W., Modi, A., Kennedy, J., & Kapadia, M. (2019). Affect-driven dialog generation. \nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, 3734-3743.\nDathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., & Liu, R. (2019). Plug and \nplay language models: A simple approach to controlled text generation. International Conference on Learning \nRepresentations.\nDemszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., & Ravi, S. (2020). Goemotions: A dataset of \nfine-grained emotions. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, \n4040-4054. doi:10.18653/v1/2020.acl-main.372\nDeveci, M., Gokasar, I., Pamucar, D., Zaidan, A. A., Wen, X., & Gupta, B. B. (2023). Evaluation of cooperative \nintelligent transportation system scenarios for resilience in transportation using type-2 neutrosophic fuzzy \nVIKOR. Transportation Research Part A, Policy and Practice, 172, 103666. doi:10.1016/j.tra.2023.103666\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n18\nDevlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for \nlanguage understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, 4171-4186.\nEkman, P. (1992). An argument for basic emotions. Cognition and Emotion , 6(3-4), 169–200. \ndoi:10.1080/02699939208411068\nGhosh, S., Chollet, M., Laksana, E., Morency, L.-P., & Scherer, S. (2017). Affect-lm: A neural language model \nfor customizable affective text generation. Proceedings of the 55th Annual Meeting of the Association for \nComputational Linguistics, 1, 634-642. doi:10.18653/v1/P17-1059\nGoswamy, T., Singh, I., Barkati, A., & Modi, A. (2020). Adapting a language model for controlled affective \ntext generation. Proceedings of the 28th International Conference on Computational Linguistics, 2787-2801. \ndoi:10.18653/v1/2020.coling-main.251\nGupta, B. B., Gaurav, A., Panigrahi, P. K., & Arya, V. (2023). Analysis of artificial intelligence-based technologies \nand approaches on sustainable entrepreneurship. Technological Forecasting and Social Change, 186, 122152. \ndoi:10.1016/j.techfore.2022.122152\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., & \nGelly, S. (2019). Parameter-efficient transfer learning for NLP. International Conference on Machine Learning, \n2790-2799.\nIlyas, Q. M., Ahmad, M., Rauf, S., & Irfan, D. (2022). RDF query path optimization using hybrid genetic \nalgorithms: Semantic web vs. data-intensive cloud computing. International Journal of Cloud Applications and \nComputing, 12(1), 1–16. doi:10.4018/IJCAC.2022010101\nIsmail, S., Shishtawy, T. E., & Alsammak, A. K. (2022). A new alignment word-space approach for measuring \nsemantic similarity for Arabic text. International Journal on Semantic Web and Information Systems, 18(1), \n1–18. doi:10.4018/IJSWIS.297036\nLester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efficient prompt tuning. \nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , 3045-3059. \ndoi:10.18653/v1/2021.emnlp-main.243\nLi, J., Galley, M., Brockett, C., Gao, J., & Dolan, B. (2016). A diversity-promoting objective function for neural \nconversation models. Proceedings of the 2016 Conference of the North American Chapter of the Association for \nComputational Linguistics: Human Language Technologies, 110-119. doi:10.18653/v1/N16-1014\nLi, S., Gao, L., Han, C., Gupta, B., Alhalabi, W., & Almakdi, S. (2023). Exploring the effect of digital \ntransformation on firms’ innovation performance. Journal of Innovation & Knowledge , 8(1), 100317. \ndoi:10.1016/j.jik.2023.100317\nLi, X. L., & Liang, P. (2021). Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the \n59th annual meeting of the association for computational linguistics and the 11th international joint conference \non natural language processing, 1, 4582-4597.\nMohammad, S. (2018). Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English \nwords. Proceedings of the 56th annual meeting of the association for computational linguistics, 1, 174-184. \ndoi:10.18653/v1/P18-1017\nMohammad, S., & Kiritchenko, S. (2018). Understanding emotions: A dataset of tweets to study interactions \nbetween affect categories. Proceedings of the 11th international conference on language resources and evaluation.\nPascual, D., Egressy, B., Meister, C., Cotterell, R., & Wattenhofer, R. (2021). A plug-and-play method for \ncontrolled text generation. Findings of the Association for Computational Linguistics: Proceedings of the 2021 \nConference on Empirical Methods in Natural Language Processing (EMNLP), 3973-3997.\nPlutchik, R. (2001). The nature of emotions: Human emotions have deep evolutionary roots, a fact that \nmay explain their complexity and provide tools for clinical practice. American Scientist , 89(4), 344–350. \ndoi:10.1511/2001.28.344\nPrendinger, H., & Ishizuka, M. (2005). The empathic companion: A character-based interface that addresses \nusers’ affective states. Applied Artificial Intelligence, 19(3-4), 267–285. doi:10.1080/08839510590910174\nInternational Journal on Semantic Web and Information Systems\nVolume 20 • Issue 1\n19\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding by \ngenerative pre-training. OpenAI, 1(6), 5.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised \nmultitask learners. OpenAI, 1(8), 9.\nRebuffi, S.-A., Bilen, H., & Vedaldi, A. (2017). Learning multiple visual domains with residual adapters. \nProceedings of the 31st international conference on neural information processing systems, 506-516.\nRussell, J. A. (1994). Is there universal recognition of emotion from facial expression? A review of the cross-\ncultural studies. Psychological Bulletin, 115(1), 102–141. doi:10.1037/0033-2909.115.1.102 PMID:8202574\nRussell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review, 110(1), \n145–172. doi:10.1037/0033-295X.110.1.145 PMID:12529060\nSarivougioukas, J., & Vagelatos, A. (2022). Fused contextual data with threading technology to accelerate \nprocessing in home UbiHealth. International Journal of Software Science and Computational Intelligence, \n14(1), 1–14. doi:10.4018/IJSSCI.285590\nScherer, K. R., & Wallbott, H. G. (1994). Evidence for universality and cultural variation of differential \nemotion response patterning. Journal of Personality and Social Psychology, 66(2), 310–328. doi:10.1037/0022-\n3514.66.2.310 PMID:8195988\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). \nAttention is all you need. Advances in Neural Information Processing Systems, 30.\nWarstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments. Transactions of the \nAssociation for Computational Linguistics, 7, 625–641. doi:10.1162/tacl_a_00290\nYang, M., Yin, W., Qu, Q., Tu, W., Shen, Y., & Chen, X. (2019). Neural attentive network for cross-domain \naspect-level sentiment classification. IEEE Transactions on Affective Computing, 12(3), 761–775. doi:10.1109/\nTAFFC.2019.2897093\nYen, S., Moh, M., & Moh, T.-S. (2021). Detecting compromised social network accounts using deep learning \nfor behavior and text analyses. International Journal of Cloud Applications and Computing, 11(2), 97–109. \ndoi:10.4018/IJCAC.2021040106\nZhang, H., Song, H., Li, S., Zhou, M., & Song, D. (2023). A survey of controllable text generation using \ntransformer-based pre-trained language models. ACM Computing Surveys, 56(3), 1–37. doi:10.1145/3617680\nZhang, Q., Guo, Z., Zhu, Y., Vijayakumar, P., Castiglione, A., & Gupta, B. B. (2023). A deep learning-based \nfast fake news detection model for cyber-physical social services. Pattern Recognition Letters, 168, 31–38. \ndoi:10.1016/j.patrec.2023.02.026\nZhang, Z., Li, C., Gupta, B. B., & Niu, D. (2018). Efficient compressed ciphertext length scheme using \nmulti-authority CP-ABE for hierarchical attributes. IEEE Access : Practical Innovations, Open Solutions, 6, \n38273–38284. doi:10.1109/ACCESS.2018.2854600\nZhou, H., Huang, M., Zhang, T., Zhu, X., & Liu, B. (2018). Emotional chatting machine: Emotional conversation \ngeneration with internal and external memory. Proceedings of the 32nd AAAI conference on artificial intelligence \nand 30th innovative applications of artificial intelligence conference and 8th AAAI symposium on educational \nadvances in artificial intelligence, 730-738. doi:10.1609/aaai.v32i1.11325\nENdNoTES\n1  https://huggingface.co/j-hartmann/emotion-english-distilroberta-base\n2  https://openi.pcl.ac.cn/\n3 . https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_\nno_trainer.py",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8132756352424622
    },
    {
      "name": "Natural language processing",
      "score": 0.6897614598274231
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4989767074584961
    },
    {
      "name": "Linguistics",
      "score": 0.32536250352859497
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 12
}