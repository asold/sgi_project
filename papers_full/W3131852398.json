{
    "title": "Training Microsoft News Recommenders with Pretrained Language Models in the Loop.",
    "url": "https://openalex.org/W3131852398",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3132230269",
            "name": "Shitao Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2014768895",
            "name": "Zheng Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2122925794",
            "name": "Yingxia Shao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140780764",
            "name": "Tao Di",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105409468",
            "name": "Xing Xie",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2950416834",
        "https://openalex.org/W2970793364",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2742272831",
        "https://openalex.org/W2969574947",
        "https://openalex.org/W2963869731",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2512971201",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W3034503922",
        "https://openalex.org/W3021397474",
        "https://openalex.org/W2123427850",
        "https://openalex.org/W3094444847"
    ],
    "abstract": "News recommendation calls for deep insights of news articles' underlying semantics. Therefore, pretrained language models (PLMs), like BERT and RoBERTa, may substantially contribute to the recommendation quality. However, it's extremely challenging to have news recommenders trained together with such big models: the learning of news recommenders requires intensive news encoding operations, whose cost is prohibitive if PLMs are used as the news encoder. In this paper, we propose a novel framework, SpeedyFeed, which efficiently trains PLMs-based news recommenders of superior quality. SpeedyFeed is highlighted for its light-weighted encoding pipeline, which gives rise to three major advantages. Firstly, it makes the intermedia results fully reusable for the training workflow, which removes most of the repetitive but redundant encoding operations. Secondly, it improves the data efficiency of the training workflow, where non-informative data can be eliminated from encoding. Thirdly, it further saves the cost by leveraging simplified news encoding and compact news representation. \r\nSpeedyFeed leads to more than 100$\\times$ acceleration of the training process, which enables big models to be trained efficiently and effectively over massive user data. The well-trained PLMs-based model significantly outperforms the state-of-the-art news recommenders in comprehensive offline experiments. It is applied to Microsoft News to empower the training of large-scale production models, which demonstrate highly competitive online performances. SpeedyFeed is also a model-agnostic framework, thus being potentially applicable to a wide spectrum of content-based recommender systems. We've made the source code open to the public so as to facilitate research and applications in related areas.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nTraining Large-Scale News Recommenders with\nPretrained Language Models in the Loop\nShitao Xiao, Zheng Liu, Yingxia Shao, Member, IEEE, Tao Di, Xing Xie, Member, IEEE\nAbstract‚ÄîNews recommendation calls for deep insights of news articles‚Äô underlying semantics. Therefore, pretrained language\nmodels (PLMs), like BERT and RoBERTa, may substantially contribute to the recommendation quality. However, it‚Äôs extremely\nchallenging to have news recommenders trained together with such big models: the learning of news recommenders requires intensive\nnews encoding operations, whose cost is prohibitive if PLMs are used as the news encoder. In this paper, we propose a novel\nframework, SpeedyFeed, which efÔ¨Åciently trains PLMs-based news recommenders of superior quality. SpeedyFeed is highlighted for its\nlight-weighted encoding pipeline, which gives rise to three major advantages. Firstly, it makes the intermedia results fully reusable for\nthe training workÔ¨Çow, which removes most of the repetitive but redundant encoding operations. Secondly, it improves the data\nefÔ¨Åciency of the training workÔ¨Çow, where non-informative data can be eliminated from encoding. Thirdly, it further saves the cost by\nleveraging simpliÔ¨Åed news encoding and compact news representation. Extensive experiments show that SpeedyFeed leads to more\nthan 100√óacceleration of the training process, which enables big models to be trained efÔ¨Åciently and effectively over massive user\ndata. The well-trained PLMs-based model from SpeedyFeed demonstrates highly competitive performance, where it outperforms the\nstate-of-the-art news recommenders with signiÔ¨Åcant margins. SpeedyFeed is also a model-agnostic framework, which is potentially\napplicable to a wide spectrum of content-based recommender systems; therefore, the whole framework is open-sourced to facilitate the\nprogress in related areas.\nIndex Terms‚ÄîNews Recommendation, Pretrained Language Models, Training Framework, EfÔ¨Åciency and Effectiveness\n!\n1 I NTRODUCTION\nOnline news platforms have been important media of\ninformation acquisition. Given the huge volumes of online\nnews articles, personalized news feed [1], [2], [3] become\nimperative, with which users may get the news articles they\nfeel interested in. The high-quality news recommendation\nis built upon the precise understanding of news articles‚Äô\nunderlying semantics. Therefore, the pretrained language\nmodels (PLMs), e.g., BERT and RoBERTa [4], [5], which\nachieve remarkable performances on general text under-\nstanding tasks, are desirable of being applied as the news\nencoder. However, the PLMs are not quite friendly to the\nend-to-end training of news recommenders. On the one\nhand, it is expensive to work with PLMs: the encoding speed\nwill be relatively slow and the GPU RAM consumption will\nbe huge given the considerable sizes of PLMs. On the other\nhand, the training of news recommenders requires intensive\nnews encoding operations: to learn from every click signal\nof a user, the user‚Äôs entire historical news clicks need to\nbe encoded, whose computation cost will be prohibitive if\nPLMs are utilized. As a result, the development of PLMs-\nbased news recommenders is severely limited by the efÔ¨Å-\nciency bottleneck.\nTo overcome the above challenge, a novel framework\nSpeedyFeed is proposed in this work, which trains PMLs-\nbased news recommenders with high efÔ¨Åciency and high\nquality. SpeedyFeed is highlighted for its light-weighted\n‚Ä¢ Shitao Xiao and Yingxia Shao are with BUPT\nE-mail: {stxiao,shaoyx}@bupt.edu.cn.\n‚Ä¢ Zheng Liu and Xing Xie are with Microsoft Research Asia\nE-mail: {zhengliu,xingx}@microsoft.com\n‚Ä¢ Tao Di is with Microsoft\nE-mail: Tao.Di@microsoft.com\nencoding pipeline, which leads to the following advantages.\n‚Ä¢The intermedia results are made highly reusable . Instead\nof having training instances encoded for one-shot use and\ndiscard afterwards, our framework saves the cost by making\nthe following intermedia results fully reusable. Firstly, there\nare a small fraction of ‚Äúbreaking news articles‚Äù, which are\nhighly popular and widely exist in the majority of users‚Äô his-\ntories. Such news articles may frequently appear throughout\nthe training process, and need to be re-encoded everytime.\nKnowing that the news recommenders are trained with\nsmall learning rates, usually in the magnitude of 1e ‚àí5 (es-\npecially when PLMs are Ô¨Åne-tuned), the same news article‚Äôs\nembedding will merely be slightly changed in every training\nstep. As such, a caching mechanism is developed, which\nenables freshly generated news embeddings to be reused for\nmultiple steps. Secondly, it is also wasteful of simply having\nuser history encoded for the prediction of one single click\nsignal. In our framework, the autoregressive user modeling\nis proposed, where an encoded preÔ¨Åx of user history can\nbe reused for the calculation of all its subsequent user\nembeddings.\n‚Ä¢The data efÔ¨Åciency is signiÔ¨Åcantly improved. The typical\ntraining workÔ¨Çow is prone to poor data efÔ¨Åciency: given\nthat the lengths of news articles and user histories are\nhighly diversiÔ¨Åed, plenty of padded elements have to be\nintroduced so that raw data can be batched as input tensors.\nThe padded data is not only non-informative, but also\nseverely slows down the training speed. In our framework,\na centralized news encoding workÔ¨Çow is designed, which\ncompletely eliminates the padded data in user history. Be-\nsides, the data loader is designed to adaptively group the\ntraining instances, so that less padded data is needed for\nthe news articles.\narXiv:2102.09268v2  [cs.IR]  5 Mar 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\n‚Ä¢The news encoding cost is reduced with well preserved\nencoding quality. The PLMs are limited by their quadratic\nencoding complexity, which makes the news encoding cost\ngrow dramatically when the news article‚Äôs length becomes\nlonger. In our framework, two techniques are utilized to\nmitigate this problem. Firstly, the bus language modeling\n(BusLM) is introduced for news encoding: on the one hand,\nit partitions each news article into small segments, which\nresults in the linear reduction of encoding complexity; on\nthe other hand, it establishes the bus connection between\nthe segments, which makes them jointly encoded for a high-\nquality news embedding. Secondly, the content reÔ¨Ånement\nis performed for each news article before it is encoded by\nPLMs: the useful part of a news article is identiÔ¨Åed from the\nraw content, based on which the news article is transformed\ninto a more compact representation.\nIt is worth noting that SpeedyFeed is not solely for\ntraining speedup. But because of the high training speed,\nit is now made feasible of training large-scale PLMs-based\nnews recommenders over a huge amount of user data. The\nenlarged model scale, together with the enriched training\ndata, ultimately make our recommender superior in gen-\nerating high-quality news recommendation. SpeedyFeed is\nveriÔ¨Åed with the production data of Microsoft News, where\nit leads to more than 100√óacceleration of the training speed,\ncompared with its conventional workÔ¨Çow. Besides, our well\ntrained PLMs-based recommender also demonstrates highly\ncompetitive performance, where it signiÔ¨Åcantly outperforms\nthe state-of-the-art approaches in comprehensive evalua-\ntions.\nFinally, SpeedyFeed is a model-agnostic framework. In\nfact, it can be helpful to a wide variety of content-based\nrecommender systems where user behaviors are associated\nwith rich textual information, such as commodity and ad-\nvertisement recommendation.\nWe summarize the major contributions of this work with\nthe following points:\n‚Ä¢ We propose a novel framework SpeedyFeed to facilitate\nthe training of large-scale PLMs-based news recom-\nmenders. With highly improved reusability, data ef-\nÔ¨Åciency and reduced news encoding complexity, our\nframework achieves signiÔ¨Åcant acceleration for the\ntraining process.\n‚Ä¢ Our framework also fully preserves the PLMs‚Äô expres-\nsiveness. The PLMs-based news recommender, trained\nvia SpeedyFeed, signiÔ¨Åcantly outperforms the state-\nof-the-art news recommendation baselines with even\nsmaller training cost.\n‚Ä¢ We‚Äôve made SpeedyFeed1 open to public, which can be\nadapted for a wide spectrum of content-based recom-\nmendation systems with rich textual information.\n2 R ELATED WORKS\nIn this section, we brieÔ¨Çy review the related works from two\nperspectives: the deep news recommendation systems, and\nthe pretrained language models.\n1. https://github.com/Microsoft/SpeedyRec\n2.1 Deep News Recommendation Systems\nNews recommendation systems are designed to identify\nusers‚Äô interested news articles with intensive exploitation of\ntheir historical news browsing behaviors [1], [3], [6]. As a re-\nsult, two inherent problems need to be resolved within this\nprocess. One problem is the modeling of users‚Äô behaviors.\nWith the progress of deep learning based recommendation\nsystems, plenty of techniques have been proposed for user\nmodeling. In Youtube-DNN [7], users are represented as the\naverages of their interacted items‚Äô embeddings; in GRU4Rec\n[8], users‚Äô historical behaviors are aggregated with GRUs\nfor sequential awareness; in DIN [9] and DEIN [10], users‚Äô\nhistorical behaviors are attentively aggregated to establish\ncandidate dependency; and in RUM [11], memory networks\nare utilized to capture the diversity about users‚Äô behaviors.\nSuch technical advancement also inspires the development\nof news recommenders. In DKN [12], users‚Äô historical news\nclicks are attended by the candidate news for more precise\nmodeling of user interest; and in LSTUR [13], recurrent\nneural networks are leveraged to capture users‚Äô short-term\ninterests.\nThe other problem, which is more speciÔ¨Åc to news\nrecommendation, is the modeling of news content. In re-\ncent years, the prosperity of natural language process-\ning pushes forward the progress of news modeling. For\nexample, the hierarchical attention networks (HAN) [14],\nwhich was originally proposed for document classiÔ¨Åcation,\nis adapted for the multi-view representation of news articles\n[15]; meanwhile, the Deep Attention Matching Networks\n(DAMN) [16], which was designed for response selection\nin chatbots, is applied to perform Ô¨Åne-grained matching\nbetween the news content and user history. The remarkable\nprogress of the pretrained language models brings huge\npotentials for the further enhancement of news modeling.\nHowever, the efÔ¨Åciency issue becomes one of the major\nobstacles of applying PLMs for news recommendation:\ncompared with the conventional small-scale text encoders,\nmaking use of PLMs in news recommenders is expensive in\nterms of both training time and computation resources. It\nusually requires the models to be trained on powerful GPU\nclusters, and still takes tremendously more running time. As\na result, the research progress and real-world applications of\nPLMs-based news recommenders are comparatively limited\nfor the current stage.\n2.2 Pretrained Language Models\nThe pretrained language models are proposed to learn uni-\nversal representation/generation models with neural net-\nworks trained on large-scale corpus. The early works were\nstarted with some shallow structures, e.g, Skip-Gram [17]\nand GloVe [18]; in recent years, the network structures are\nbeing quickly scaled up: from EMLo [19], GPT [20], to BERT\n[5], RoBERTa [5], UniLM [21], till today‚Äôs GPT-3 [22]. The\nlarge-scale models, which get fully trained with massive\ncorpus, demonstrate superior capabilities on general NLP\ntasks, e.g., semantic matching, question-answering, machine\ntranslation, and response generation.\nThe pretrained language models are also being inten-\nsively applied for the retrieval or information-Ô¨Åltering re-\nlated scenarios [23], [24], [25]; e.g., in [26], PLMs are trained\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\n‚Ä¢Data Efficiency‚Ä¢Reusability‚Ä¢Light-weighted Encoding‚Ä¢Centralized News Encoding‚Ä¢Cache Acceleration‚Ä¢Bus Language Modeling‚Ä¢Autoregressive User Modeling‚Ä¢Content Refinement‚Ä¢Dynamic BatchingSpeedyFeedNewsEncoderbiden takes the wheel[pad]Candidate News\nNews & UserEncoderUser History‚Ä¶\nNews EmbeddingsUser EmbeddingsPrediction Loss\nNews TensorUser Tensorpad‚Ä¢Encoding Cost\nFig. 1. Left: the typical training workÔ¨Çow of news recommendation. Middle: underlying problems within the typical workÔ¨Çow. Right: the constitution\nof SpeedyFeed, and how it contributes to the training workÔ¨Çow.\nfor knowledge retrieval, and in [27], PLMs are Ô¨Åne-tuned for\nadvertisement keyword matching. In these scenarios, PLMs\nare required to represent a query and a keyword into their\nlatent embeddings, where the query-keyword relationship\ncan be reÔ¨Çected by their embedding similarity. Apparently,\nnews recommenders turn out to be similar applications.\nHowever, the PLMs-based news recommenders can be rela-\ntively more expensive: to match a user towards a candidate\nnews, it needs to encode all of the user‚Äôs historical news\nclicks with PLMs, which will lead to huge encoding costs.\n3 P RELIMINARIES\n3.1 Typical WorkÔ¨Çow of Training News Recommenders\nThe news recommender is to predict user‚Äôs future news\npreference given their news clicks in history. Therefore, a\ntypical training workÔ¨Çow consists of three steps (shown\non the left side of Figure 1), as implemented by Microsoft\nRecommenders [28].\n1) Input Processing . The trainer needs to transfer the\nraw data, i.e., user‚Äôs historical interactions with news, into\nthe required format, such that it can be loaded for training.\nTwo operations are involved in this stage. On the one hand,\nthe news articles are tokenized, and then padded/truncated\ninto token sequences of a uniÔ¨Åed length. On the other\nhand, all users‚Äô histories are padded/truncated into news\nsequences of a uniÔ¨Åed length.\n2) News & User Encoding . The input tensors are en-\ncoded via two steps [1]. Firstly, the news encoding, which\nmaps user‚Äôs historical news clicks and all of the candidate\nnews into news embeddings. Secondly, the user encoding,\nwhich generates user embeddings based on the encoded\nhistorical news and other auxiliary features.\n3) Learning from Prediction . Finally, the prediction is\nmade about which candidate news is clicked given the\nuser and news embeddings. The prediction loss (e.g., cross\nentropy or BPR) will be back-propagated for the model\nparameters‚Äô update.\n3.2 What‚Äôs wrong with the typical WorkÔ¨Çow\nOne of the most notable things about training a news recom-\nmender is its huge text encoding cost: to make a prediction\nfor one single user click, the trainer needs to encode 1) all\nof the news articles in user history and 2) the candidate news .\nConsidering the large magnitudes of pretrained language\nmodels, the text encoding related computations will almost\ndominate the entire training cost. However, because of the\nfollowing issues (shown in the middle of Figure 1), the typi-\ncal training workÔ¨Çow becomes severely limited in efÔ¨Åciency,\nwhich makes it prohibitive to train a PLMs-based large-scale\nnews recommender.\n‚Ä¢High Encoding Cost . First of all, the PLMs are con-\nsiderably larger in scale than the text encoders used in\nconventional text-related recommendations, e.g., bi-LSTM,\nCNN, and shallow transformers. What is worse, the PLMs\nare highly unfavorable to the processing of long texts.\nParticularly, the encoding cost is vulnerable to the length\nof input news ( N), whose time complexity is O(N2), given\nthat the mainstream PLMs are all based on transformer ar-\nchitectures [29]. Considering that many news articles require\nlong textual descriptions to fully express their underlying\ninformation, it results in a huge computation overhead\nwhile encoding such news articles.\n‚Ä¢Low Reusability . Secondly, the reusability is seldom\nemphasized before: every time a training instance is given,\nit is processed for the calculation of its own loss; once\nthe loss is back-propagated, all related intermediate results,\nespecially the news embeddings, will all be discarded after\nbeing used just one time. Considering that it is quite an\nexpensive operation to encode a news article with PLMs,\nsuch a defect severely slows down the training progress.\n‚Ä¢Low Data EfÔ¨Åciency . Lastly, due to the existence of\nsubstantial padded data (the padded elements in news arti-\ncle and user history), the meaningful computation through-\nput can be severely limited. Particularly, given that a token\nis the atomic data unit for both user tensor and candidate\ntensor, we deÔ¨Åne data efÔ¨Åciency as the ratio of valid (i.e.,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nAlgorithm 1: Light-weighted Encoding Pipeline\nInput: a mini-batch: user tensor U, news tensor N\nOutput: Overall prediction loss Lauto\n1 begin\n2 Merged set M: gather(U.news ‚à™N.news);\n3 Cached set MC: {m: m in cache}M;\n4 Get lookup rate pt from scheduler (Eq. 2);\n5 Lookup Set ML: sample from MC with pt;\n6 News embeddings set Œò1 ‚ÜêCacheLookup(ML);\n7 News embeddings set Œò2 ‚ÜêBusLM(M \\ML);\n8 Dispatch Œò1 ‚à™Œò2;\n9 Refresh cache with Œò2;\n10 Get Lauto from autoregressive user modeling.\nnon-padded) tokens within the input tensors:\nDE = |valid tokens|\n|valid tokens|+ |padded tokens|√ó100%. (1)\nDue to the highly diversiÔ¨Åed lengths of user histories and\nnews articles, a huge amount of padded elements will\nprobably be introduced. We empirically Ô¨Ånd that the data\nefÔ¨Åciency is usually lower than 50% in practice, which\nleads to a big waste of computation capacity and further\njeopardizes the training efÔ¨Åciency.\n4 M ETHODOLOGY\nWe develop an efÔ¨Åcient training framework SpeedyFeed,\nwhich enables news recommenders built upon large-scale\nPLMs to be trained with both high speed and high quality.\nWith SpeedyFeed, the news and user encoding are carried\nout through a light-weighted encoding pipeline, which is\ncharacterized by the following techniques: 1) the central-\nized news encoding for high data efÔ¨Åciency, 2) the cache\nacceleration and the autoregressive user modeling for high\nreusability, 3) the bus language modeling for economic\nencoding complexity (as the rightmost of Figure 1). Besides,\ntwo auxiliary techniques: content reÔ¨Ånement and dynamic\nbatching, are introduced, which give rise to a more compact\nrepresentation of news content and further reduction of\npadded data, respectively.\n4.1 Light-weighted Encoding Pipeline\nAlgorithm 1 presents the main logics of the light-weighted\nencoding pipeline. For each mini-batch, we apply centralized\nnews encoding (Section 4.1.1) to gather all the involved news\narticles from both user tensor and news tensor into the\nmerged set M. Then the lookup set ML is sampled from\nthe cached news embeddings MC with the lookup rate pt,\nand the news articles within the lookup set directly use their\ncached embeddings, denoted by Œò1. The news articles out\nof the lookup set are encoded with BusLM (Section 4.1.3),\nwhich gives Œò2. The whole news embeddings Œò1 ‚à™Œò2 are\ndispatched to their original positions (in either user history\nor candidate news); then, the cache is refreshed with the\nnewly generated news embeddings Œò2. Finally, the overall\nprediction loss is calculated with the autoregressive user\nmodeling (Section 4.1.4), as in Eq. 5.\nIn the following sections, we elobrate the details of tech-\nnical contributions in the light-weighted encoding pipeline.\nInstance 1 {News}Instance ùëñ {News}Instance M {News}News EncoderInstance 1 {Embeddings}Instance ùëñ {Embeddings}Instance M {Embeddings}gatherdispatch‚Ä¶‚Ä¶‚Ä¶‚Ä¶MergedSetEncodingResultMergedSet{   ‚Ä¶    }EncodingResult{    ‚Ä¶    }EncoderGathering of raw news Dispatching of news embeddings\nFig. 2. Illustration of centralized news encoding: the news articles are\ngathered from the training instances, with all padding (grey) and dupli-\ncated ones removed from the merged set; the encoding results, i.e., the\nnews embeddings (green), are dispatched to the training instances.\n4.1.1 Centralized News Encoding\nThe overall news encoding workÔ¨Çow is discussed in the Ô¨Årst\nplace. In the typical training workÔ¨Çow, the news encoder\nwill directly work on the input tensors (i.e., user tensor,\nnews tensor) for the news embeddings. During this process,\nthe padded news articles are encoded together with the\nvalid news, which results in low data efÔ¨Åciency.\nUnlike the typical method, all of the news articles within\na common mini-batch are jointly encoded in SpeedyFeed\n(as Figure 2). The centralized news encoding takes 3 steps:\ngathering, encoding and dispatching. Once a mini-batch is\ngiven, it gathers the news articles from all users and candi-\ndates into a merged set. The padded news and duplicated\nnews are all removed. Then, the new embeddings are gen-\nerated for all remaining news in the merged set. Finally, the\nnews embeddings are dispatched to their original training\ninstances. Note that the padded news articles also require\ntheir embeddings so as to infer the user embeddings; in this\nplace, a dummy vector is plugged into the positions taken\nby the padded news, whereby no additional encoding cost\nis needed.\n4.1.2 Cache-accelerated News Encoding\nTABLE 1\nThe long-tail property about news click. The Top-1% popular news\nyields almost 60% of the overall clicks.\nTop-Œ± 1% 3% 5% 10% 20% 30%\nClick Ratio (%) 59.53 77.65 84.82 92.47 97.36 98.97\nCache Controller\nReadWrite WriteReadWrite ReadWriteReadWrite-in with ùíëExpired after ùú∏Cache LookupNews Encoder\nNYIn Cache?News Embedding\nFig. 3. Illustration of Cache-accelerated News Encoding: the in-cache\nnews article‚Äôs embedding is obtained by looking up the cache; the non-\ncached news article‚Äôs embedding needs to be encoded from scratch.\nThe cache mechanism is developed to fully reuse the\nintermedia news encoding results. Particularly, one notable\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nobservation about Microsoft News is its long-tail property\nof the news click distribution. As shown in Table 1, the\ntop-1% popular news articles yield almost 60% of the\noverall news clicks. Therefore, such popular news articles\nmay widely exist in the majority of users‚Äô histories, mak-\ning them frequently re-encoded across different training\nbatches. Knowing that the model parameters are updated\nwith a fairly small learning rate, usually in the magnitude\nof 1e‚àí5, one news article‚Äôs recent embedding can be reused\nin the current mini-batch for approximation. Based on this\nintuition, we propose Cache-accelerated News Encoding,\nwhere a cache is maintained in memory for the storage of\nfresh news embeddings. The news encoding workÔ¨Çow is\nchanged accordingly as Figure 3.\n‚Ä¢News Encoding with Cache . For each news article in\na mini-batch, the trainer will check the cache in the Ô¨Årst\nplace: if there is a copy of news embedding in cache, it will\nbe directly reused without encoding; otherwise, the news\narticle will be encoded from scratch.\nCache LookupNews Encoder\nNYIn Cache?News Embedding\nReadWrite\nCache Manager\nLookup: ùíëùíïExpired: ùú∏Device 1Device 2Device 3Device 4Master Memory\nFig. 4. Illustration of Cache Management: in the initial stage, the cached\nnews embeddings are looked up with probability pt; the cached news\nembeddings will be expired after Œ≥ steps; all devices will share a com-\nmon cache maintained in the master node‚Äôs memory.\n‚Ä¢Cache Management Policy . The cache is managed\nwith the following principles. Firstly, all of the embeddings\nin cache must be newly generated in the past few steps; oth-\nerwise, it will be incompatible with the current model. Sec-\nondly, the cache lookup should be dynamically scheduled:\nin the initial stage, the cached news embeddings should be\nused with a relatively low probability, because the step-wise\nchange of model parameters is sharp; as the training goes\non, the lookup rate should be gradually increased, because\nthe change of model parameters becomes mild.\nBased on the above principles, the cache management\npolicy is made (as Figure 4), which is subject to two decisive\nvariables: the stepwise lookup rate pt, and the expiration\nstep Œ≥. 1) An exponential scheduler is used to control the\nprobability of whether to lookup the cache: the cache is\nlooked up with probability 0 when the training gets started;\nthe lookup probability will gradually grow to pt at the t-th\nstep w.r.t. the following relationship:\npt = 1.0 ‚àíexp(‚àíŒ≤t). (2)\nŒ≤is the hyper parameter for growth rate, which lets pt grow\nto 1.0 after the initial stage of the training process. 2) A\ncached news embedding is expired after Œ≥steps, and then it\nis removed from the cache.\nFinally, instead of maintaining a private cache for each\ntraining thread, we establish a global cache in the master\nnode. As a result, the newly generated news embeddings\nAlgorithm 2: Cache-accelerated News Encoding\nInput: The merged news set M in mini-batch,\ncurrent training step t\nOutput: News embeddings Œò for all news in M\n1 begin\n2 initialize Œò1 and ML;\n3 pt = 1 ‚àíexp(‚àíŒ≤t);\n4 if random<p t then\n5 for nin M do\n6 if nin cache then\n7 en ‚ÜêRead(cache, n);\n8 if t‚àíten ‚â§Œ≥then\n9 Œò1.add(en);\n10 ML.add(n);\n11 Œò2 ‚ÜêBusLM(M \\ML);\n12 Write(cache, Œò2);\n13 Œò = Œò1 ‚à™Œò2.\nin one node can be shared across all devices, which ac-\ncommodates the distributed training of news recommender.\nBesides, the cache is maintained in memory, instead of GPU\nRAM; therefore, it is almost free of cost, and the storage\ncapacity can be large enough to host all needed embeddings.\nSummary. We summarize the cache mechanism in Al-\ngorithm 2. In the training step t, we input a merged news\nset M by centralized news encoding. Firstly, the lookup\nprobability pt is generated according to the current step t\nand hyper-parameter Œ≤(Lines 3). We use a random value to\ndetermine whether to read embeddings from the cache. If\ntrue, for all news embeddings in the cache, we only load the\nones which are encoded less than Œ≥training steps before the\ncurrent step (Line 8). These loaded embeddings are denoted\nby Œò1, and the corresponding news are denoted by ML.\nFor news which are not cached, i.e., M \\ML, we encode\nthem into embeddings Œò2 with BusLM, which is introduced\nin the next subsetction, and write Œò2 into cache. Finally,\nŒò1 ‚à™Œò2 is the whole new embeddings in step t.\n4.1.3 Bus Language Modeling\nWe make further analysis of how we conduct news encoding\nin an economic way. The news encoding complexity is to\nthe square of news length O(N2). A straightforward way\nof time reduction is to split the news into several sub-\ncomponents, e.g., the title, abstract, and body, as done in\n[15]; the text segments are processed independently, whose\nencoding results will be aggregated for the Ô¨Ånal news em-\nbedding. The operation may cut down the time complexity\nto O(N2/K), if the text can be partitioned into K ‚Äúalmost\nequal-length‚Äù segments. Yet, the naive split of text harms the\nnews embedding quality, as the text segments cannot make\nreference to each other during the encoding process.\nInspired by recent progress on efÔ¨Åcient transformers [30],\nwe propose BusLM (Figure 5) to encode the news articles,\nwhere the acceleration is achieved with fully preserved\nembedding quality. In BusLM, the input news is uniformly\npartitioned into K text segments, such that the encoding\ncomplexity is reduced to O(N2/K). The segments are still\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nNews: Title\\#Abstract\\#Bodies >Aggregated as News Embedding\nùíä-th\nseg-1ùíä‚àíth\n‚Ä¶‚Ä¶seg-2ùíä‚àíth\nseg-K\nBusi Busi Busi(ùíä-1)-th(ùíä‚àí1)‚àíth (ùíä‚àí1)‚àíth(ùíä+1)-th(ùíä+1)‚àíth(ùíä+1)‚àíth‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶\nFig. 5. Illustration of BusLM (using thei-th layer for demonstration): the Ô¨Årst tokens from all segments are gathered as the bus of thei-th layer, Busi;\nBusi is broadcasted to all segments, such that for each segment, the transformer attends to both the in-segment elements and the bus elements.\n12345\n‚Ä¶\n‚Ä¶‚Ä¶\nNews EncoderNewsNewsEmbUserEmb1 2 3 4 51 2 3 4 5ùíäùíä+1\nùíä+1\nùíä+1\nùíä+1\nùíä+1Pos NegsNext Click PredictionUserEmbNewsEmb\nFig. 6. Illustration of Autoregressive User Modeling: the user embed-\ndings are generated for all timestamps, each of which is based on its\ncurrent and preceding news embeddings (Left); each user embedding\nis used to predict its next news click (Right).\nencoded by transformers; however, a layer-wise ‚Äúbus con-\nnection‚Äù is established between the transformers, which\nenables information to be exchanged across the segments.\nIn each layer of the transformers, a ‚Äúproxy embedding‚Äù\nis chosen for each segment, which serves as the sketch of its\nunderlying information. To avoid additional computation as\nmuch as possible, we directly select the Ô¨Årst embedding of\neach segment as its proxy; e.g., for the i-th layer of segment\nj, Hi\nj[0] is chosen as the proxy (let Hi\nj be the j-th segment‚Äôs\nembedding sequence on the i-th layer). The i-th layer‚Äôs\nproxy embeddings from all of the segments are gathered\nas the i-th Bus:\nBusi = {Hi\nj[0]}K\nj=1. (3)\nThe bus is used as a medium of information exchange,\nwhich all the segments may attend to directly. Particularly,\nthe i-th to i+ 1-th transformation of the j-th segment will\nbecome:\nHi+1\nj = Transformeri(\n[\nHi\nj, Busi]\n), (4)\nin which ‚Äú[]‚Äù denotes concatenation, and ‚ÄúTransformeri(¬∑)‚Äù\nis the i-th layer of the transformers. The Ô¨Ånal news embed-\nding is acquired by aggregating all of the hidden states in\nthe last layer, i.e., H‚àí1\n‚àó . In Appendix A, we present the im-\nplementation details of applying BusLM to different types\nof layers in transformer. It is empirically veriÔ¨Åed that both\ntime-efÔ¨Åciency and memory-efÔ¨Åciency beneÔ¨Åt from BusLM;\nmeanwhile, the information loss due to the split of text is\nfully mitigated with the Bus connection.\n4.1.4 Autoregressive User Modeling\nThe computation cost is further saved by reusing the en-\ncoded preÔ¨Åx of user history. As discussed, the news rec-\nommender is trained based on the prediction loss of user‚Äôs\nnews click. Therefore, a typical training instance consists of\nuser‚Äôs news click at one timestamptand its preceding news\nclicks: ‚ü®click=t,clicks‚â§t‚àí1‚ü©. However, when another train-\ning instance ‚ü®click=t+1,clicks‚â§t‚ü© is presented, clicks‚â§t‚àí1\nbecomes the preÔ¨Åx of user history, which requires to be re-\nencoded.\nWe propose autoregressive user modeling for more efÔ¨Å-\ncient utilization of user history (Figure 6), where all of the\nnews clicks about a user can be predicted at one-shot of\nnews encoding. Instead of processing each training instance\n‚ü®clicks=t,click‚â§t‚àí1‚ü© case-by-case, the whole user history\nclicks‚â§L will be treated as one uniÔ¨Åed training instance\n(let L be the max length of user history). The trainer will\nencode all of the historical news clicks, which gives the news\nembedding set: {Œ∏l}‚â§L. The trainer will then calculate the\nuser embedding set {¬µt}‚â§L, where ¬µt is conditioned on the\npreceding news embeddings {Œ∏l}‚â§t. Each user embedding\nis used to predict the news click for the next timestamp,\nwhere the overall prediction loss Lauto w.r.t. one sample\nuser is computed as:\nLauto = ‚àí\n‚àë\nt<L\nlog exp(‚ü®Œ∏t+1,¬µt‚ü©)\nexp(‚ü®Œ∏t+1,¬µt‚ü©) +‚àë\nŒ∏‚Ä≤\nt+1\nexp(‚ü®Œ∏\n‚Ä≤\nt+1,¬µt‚ü©)\n.\n(5)\n‚Äú‚ü®¬∑‚ü©‚Äù calculates the relevance of the user and news embed-\ndings, e.g., inner product; and Œ∏\n‚Ä≤\nt+1 is the embedding of a\nnegative sample.\n4.2 Further Enhancement\n4.2.1 Content ReÔ¨Ånement\nAlthough the news encoding cost is reduced with BusLM,\nit is still prohibitive to process extremely long news arti-\ncles. Therefore, moderate truncation of the new article is\ninevitable in practice. Instead of simply taking the head\nof a news article, the truncation is treated as a Ô¨Åltering\noperation in SpeedyFeed: we will try to remove the redun-\ndant or non-informative data, and preserve the important\ninformation as much as possible. The Ordered Bag-of-Words\n(OBoW) model is proposed for this purpose (Figure 7),\nwhich compactly represents the informative data distilled\nfrom the raw content. In OBoW, all special characters and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nNovak Djokovic has voiced his fears for lower-ranked players as the coronavirus-ravaged 2020 season draws to a close. The world number one, in London for the elite eight-man ATP Finals, said players ranked outside the world's top 500 were struggling to make ends meet. Djokovic said the biggest concern was the unpredictability of the 2021 calendar, with decisions taken out of the hands of the sport's administrators as a result of Covid-19.Novak:1;Djokovic:2;voiced:2; his:1;fears:1;lower:1;ranked:2;players:2;coronavirus:1;ravaged:1;2020:1; season:1; draws:1; close:1; ‚Ä¶Novak:1;Djokovic:2;voiced:2;fears:1;lower:1;ranked:2;players:2;coronavirus:1;ravaged:1;2020:1; ‚Ä¶Raw TextOrdered Bag-of-WordsRefined by BM25\nFig. 7. Illustration of Content ReÔ¨Ånement: the raw text is represented\nas an ‚Äúordered-bag-of-words‚Äù in the Ô¨Årst place, with all stopping words\nand special characters removed; then, the top- k important words are\npreserved based on their BM25 scores.\nstopping words are discarded; besides, the news article is\nrepresented as a sequence of ‚Äúword: count‚Äù tuples ordered\nby the words‚Äô Ô¨Årst-appearances in the original content. The\nremaining words are labeled with their BM25 scores. The\nwords with the top- k BM25 importance are believed to be\nthe most informative, which will be preserved by the Ô¨Ånal\nOBoW model.\nOne minor modiÔ¨Åcation is required to encode the OBoW\nwith PLMs: apart from the original token embedding, po-\nsition embedding, and segment embedding, an additional\nfrequency embedding is added to each input token, which\nbrings in the information about each word‚Äôs times of ap-\npearance in the original content.\n4.2.2 Dynamic Batching\nDynamic batching is an asynchronous data loading process,\nwhich runs in parallel to the model training process. It\nuses multiple threads to read the user log and generate\ntraining instances (user history and candidate news) from\nit. The training instances are gathered as mini-batches and\nconsumed by the training process. To reduce the number\nof padded tokens and maximize the GPU utilization, the\nfollowing treatments are adopted.\nFirstly, the training instances are grouped based on the\nlengths of their included news articles. As each training\ninstance is virtually a collection of news articles, they can\nbe marked by the ‚Äúmax-length‚Äù of their included news; e.g.,\nan instance with 3 news, whose lengths are (32, 48 , 36), will\nbe marked with 48. Each training instance is routed to a\nbucket based on its max-length; e.g., the above instance may\nbe routed to the bucket which hosts training instances of\nmax lengths 40‚àº50. All the news within one bucket will be\npadded to the same length based on the currently longest\nnews in the bucket. Secondly, the bucket is checked after\nevery Ô¨Åll-in: whether the total number of tokens reaches the\nthreshold determined by the GPU RAM‚Äôs capacity. Once a\nbucket is full, all of its training instances will be dump into\na mini-batch and appended to the mini-batch queue, which\nis consumed by the training process.\nThe above generation of mini-batch is ‚Äúdynamic‚Äù, as the\npadded length and the batch size are determined case-by-\ncase. Based on the grouping operation, the overall padded\nlength can be minimized; and with the dynamic batch size,\nthe GPU capacity can be used as much as possible.\nTABLE 2\nStatistics of the experimental dataset\n#User #News #Impressions #Clicks\n4,720,192 1,202,576 41,481,252 72,093,567\n5 E XPERIMENTS\n5.1 Settings\n5.1.1 Dataset description\nWe use a large real-world dataset for evaluation. The dataset\ncontains Microsoft News 2 users‚Äô news reading behaviors\nfrom 2020-05-01 to 2020-08-31. The data in the Ô¨Årst 3\nmonths (2020-05-01 to 2020-07-31) is used for training; the\nlast month‚Äôs data (2020-08-*) is used for testing. There are\n1,202,576 news articles and 4,720,192 users, and it yields\n72,093,576 news clicks in total (summarized as Table 2). The\nfollowing features are utilized for ofÔ¨Çine experiments. The\nnews articles consist of their titles, abstracts, and bodies;\nthe users are represented with their historical news clicks.\nOther features, like news categories, user demography and\ncontexts, are omitted here, but exploited in production.\nMore speciÔ¨Åcations of the data related to reproducibility are\nincluded in Appendix B.\n5.1.2 SpeedyFeed Recommenders\nA default news recommender is trained by SpeedyFeed,\nwhose conÔ¨Ågurations are listed as follows.\n‚Ä¢News Encoder . Our news encoder is initialized with\nthe pretrained checkpoint of UniLMv2-base [21] ( UniLM\nfor short), which is a 12-layer & 768-hidden-dimension\nlanguage model trained by Microsoft. Leveraging the state-\nof-the-art pretraining techniques, it outperforms other well-\nknown PLMs of the same scale, e.g., BERT-base and\nRoBERTa-base, on GLUE benchmark and many other gen-\neral NLP tasks.\n‚Ä¢User Encoder . A highly simpliÔ¨Åed user encoder is\nadopted by the default news recommender, as we target on\nthe impact brought by the news encoder. Particularly, a sim-\nple adaptation of the YouTube-DNN [7], namely, Attentive\nYouTube-DNN, is utilized: it makes use of the weighted-\nsum of user‚Äôs historical news embeddings for user repre-\nsentation, where a learnable attention vector is introduced\nto generate the aggregation weights.\nBesides, we also combine the default user encoder with\nthe following alternative news encoders, which are certain\nsorts of simpliÔ¨Åcations of the original UniLM.\n‚Ä¢MiniLM [31], a high-quality distillation of UniLM;\nboth its depth and width are reduced to 50% of the original\nmodel (i.e., with 6 layers and 384 hidden-dimensions).\n‚Ä¢UniLM-Half, where the model scale is the same as\nMiniLM, but the model weights are directly inherited from\nthe original UniLM.\n‚Ä¢UniLM-Last, which uses the whole UniLM but simply\nÔ¨Ånetunes the last layer in the training stage. (It is different\nfrom the default one where UniLM is trained end-to-end.)\nAlthough it does not contribute to the feed-forward speed, it\nreduces the training cost as most of the layers are frozen: the\n2. https://microsoftnews.msn.com/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE 3\nUpper: baselines w.o. SpeedyFeed acceleration. Lower: PLMs-based news recommenders accelerated with SpeedyFeed.\nAUC MRR NDCG@5 NDCG@10 Recall@50 Recall@100 Recall@200 Time (hour)\nNPA 65.01 24.66 26.06 30.90 2.24% 4.04% 7.14% 23.6\nNAML 67.57 26.90 28.73 33.75 2.29% 5.22% 8.67% 26.6\nLSTUR 64.37 24.35 25.58 30.63 2.28% 3.96% 6.84% 30.4\nNRMS 68.62 27.30 29.09 34.15 3.10% 5.81% 9.15% 27.8\nUniLM ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì 2497.5\nSpeed-Mini 72.06 30.16 32.63 37.74 6.93% 10.75% 16.13% 3.1\nSpeed-Half 69.47 28.06 30.04 35.11 4.78% 7.47% 11.84% 3.3\nSpeed-Last 70.98 28.92 31.09 36.26 4.69% 8.07% 12.94% 6.5\nSpeed-UniLM 73.74 31.70 34.40 39.53 8.32% 13.17% 19.40% 19.4\nGPU RAM usage will be lower, so that we may use larger\nbatch sizes for acceleration. Besides, it also saves the cost as\nmuch fewer model parameters call for update.\nAll the above approaches are trained with SpeedyFeed,\nthus referred to as Speed-UniLM, Speed-Mini, Speed-Half,\nand Speed-Last in the experiments.\n5.1.3 Baselines\nThe following representative news recommender baselines\nare utilized in our experiments.\n‚Ä¢NPA [32], which leverages personalized attention to\nselect and aggregate useful information in user history.\n‚Ä¢NAML [15], which uses multi-view attention to aggre-\ngate user‚Äôs historical news clicks.\n‚Ä¢LSTUR [13], which relies on multiple neural network\nstructures to jointly capture user‚Äôs long-term and short-term\ninterest.\n‚Ä¢NRMS [33], which makes use of multi-head self atten-\ntion to improve the quality of user representation.\nThe above approaches make trial of various user mod-\neling strategies for news recommendation. But one thing\nin common is that all of them make use of compara-\ntively small-scale text encoders to generate news embed-\ndings, such as 1D-CNN or self-attention. These methods are\ntrained following the default workÔ¨Çow as demonstrated in\nMicrosoft Recommenders [28].\n5.1.4 Evaluations\nThe experiment results are comprehensively evaluated from\nthree perspectives.\n1) Ranking: The evaluation is made for the ranking\nperformance: given a testing impression, the recommender\nis required to generate the ranking orders for the impressed\nnews articles; the ranking orders are compared with the\nground-truth (i.e., the clicked news within the impression),\nwhose performance is measured with the typical ranking\nmetrics: AUC, NDCG, MRR.\n2) Recall: The evaluation is made for the recall per-\nformance: based on the user embedding generated by the\nrecommender, the relevant news articles are retrieved from\nthe whole production index. Since the relevance between\nuser embedding and news embedding is measured by inner-\nproduct, it turns out to be a Max-Inner-Product-Search\nproblem, where HNSW [34] is used as the backbone of\nANN index. The performance is measured with Recall@K,\nwhere the ground-truth is still the clicked news of the testing\nimpression.\n3) Training EfÔ¨Åciency: We also evaluate the training ef-\nÔ¨Åciency, where the time cost is measured with the following\nconÔ¨Ågurations.\n5.1.5 Training conÔ¨Ågurations\nAll the training jobs are performed on an Azure 3 machine,\nwith 4√óNvidia-V100-32G GPUs, 40 √óIntel(R) Xeon(R) Plat-\ninum 8168 CPU @ 2.70GHz processors, run on Ubuntu\n16.04.6. The models are implemented with PyTorch 1.7.0.\nMore speciÔ¨Åcations about the training process are included\nin Appendix C.\n5.2 Experiment Analysis\n5.2.1 Overall Performance\nThe main experiments are performed to clarify the following\nissues: 1) the effect on recommendation quality when PLMs\nare utilized as the news encoders, and 2) the effect on\nefÔ¨Åciency when SpeedyFeed is leveraged for recommender\ntraining. The following conclusions can be drawn based on\nthe experiments results reported in Table 3.\nFirstly, our default recommender, which is equipped\nwith a full-scale and end-to-end trained UniLM, beats all\nmodels with simpliÔ¨Åed (MiniLM, UniLM-half) or insufÔ¨Å-\nciently trained (UniLM-last) UniLM. Besides, it outperforms\nall of the baseline recommenders by huge margins: all the\nranking metrics are signiÔ¨Åcantly improved, and the recall\nmetrics go above the baselines by several times. Therefore,\nit validates 1) the recommendation quality can be greatly\nimproved by large-scale PLMs, and 2) the PLMs need to be\nfully trained within the recommender so as to achieve the\nbest performance.\nNotice that the above comparisons do not deprecate the\nimportance of user encoder. Instead, it points out the neces-\nsity of collaborative utilization of large-scale news encoders\nand expressive user encoders; otherwise, the recommenda-\ntion quality will be severely limited due to the insufÔ¨Åcient\nmodeling of news semantic.\nSecondly, our default recommender (which is larger\nthan all baseline recommenders by several orders) can\n3. https://azure.microsoft.com/en-us/services/machine-learning/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 4\nLeft: default recommender‚Äôs training time w./w.o. being accelerated by\nSpeedyFeed. Right: the overall speedup effect, and the speedup effect\nfrom each basic module.\nTraining Time Speedup\nw.o. SpeedyFeed: 2497.5 hours Overall Speedup: 128.7 √ó\nw. SpeedyFeed: 19.4 hours Central & Batch: 3.0 √ó\nCache: 1.98√ó\nAutoregressive: 17.0√ó\nBusLM: 1.27√ó\n0.30840.37650.37130.41570.63320.70600.73840.769600.20.40.60.811-Bucket 3-Bucket 5-Bucket 10-BucketData EfficiencyImpact of Centralized News Encoding (CNE) and Dynamic Batching (DB)w. DB, w.o. CNEw. CNE & DB26.2720.3920.6623.76051015202530#seg: 1 #seg: 3 #seg: 5 #seg: 10Time Cost per 100-Batch (s)Effect of BusLM on Training Time31.04520.21118.28718.66505101520253035#seg: 1 #seg: 3 #seg: 5 #seg: 10Memory Cost per Batch (GB)Effect of BusLM on G-RAM Cost\nFig. 8. Effect of Dynamic Batching (DB) and Centralized News Encoding\n(CNE) on data efÔ¨Åciency.\nbe efÔ¨Åciently trained with even less time compared with\nthose small-scale recommenders trained by the conven-\ntional workÔ¨Çow. Besides, the recommenders with simpli-\nÔ¨Åed UniLMs can be trained by SpeedyFeed with further\nless time, as the news encoding costs become relatively\nsmaller. We also let the default recommender trained by\nthe conventional workÔ¨Çow; and the training time becomes\n2497.5 hours (estimated from a faction of training progress,\ni.e., average-time-per-step √óthe-total-required-steps). In other\nwords, the training speed is accelerated by over 100 √ówith\nSpeedyFeed.\nFinally, one additional issue is that whether we should\nresort to the distilled PLMs for further training speedup.\nGiven the results in Table 3, we incline to stay with the full-\nscale PLMs (i.e., UniLM v.s. MiniLM), as the recommenda-\ntion quality can be signiÔ¨Åcantly improved with an accept-\nable increment of training cost. Besides, although MiniLM is\nfaster, the online inference speed is hardly a bottleneck for\nthe recommendation of Microsoft News: there are merely\ntens of thousands of fresh news articles generated each day,\nwhich can be encoded and cached for the recommender\nwith very little cost. However, we do not exclude distilled\nPLMs‚Äô necessity for other scenarios, like search and ads,\nwhere fresh contents are generated rapidly and need to be\nprocessed in realtime.\n5.2.2 Further Analysis\nExperiments are performed to further clarify the following\nissues: 1) SpeedyFeed‚Äôs impacts on training speedup, 2)\nSpeedyFeed‚Äôs impact on data efÔ¨Åciency, 3) SpeedyFeed‚Äôs\nimpact on recommendation quality, 4) detailed analysis of\ncache-accelerated news encoding, and 5) detailed analysis\nof BusLM.\n‚Ä¢Impact on Speedup We study the overall speedup\neffect of SpeedyFeed (Table 4): the training time is reduced\nTABLE 5\nAblation studies, where BusLM, Cache-accelerated news encoding,\nand content reÔ¨Ånement are disabled, respectively.\nAUC Recall@50 Recall@100\nw.o. Bus 73.20 6.72 10.81\nw.o. Cache 73.70 8.11 12.73\nw.o. ReÔ¨Åne 73.70 7.88 12.45\nDefault 73.74 8.32 13.17\nTABLE 6\nEffect of cache-accelerated news encoding. The cache is disabled\nwhen Œ≥ = 0.\nAUC Recall@50 Recall@100 Time (h)\nŒ≥ = 0 73.70 8.11 12.73 38.5\nŒ≥ = 10 73.94 8.40 13.09 26.2\nŒ≥ = 20 73.74 8.32 13.17 19.4\nŒ≥ = 30 73.69 7.99 12.37 15.4\nfrom 2497.5 hours (estimated) to 19.4 hours, which means\na 128.7√óspeedup. We further analyze the detailed speedup\neffect of each module. We Ô¨Ånd that the autoregressive user\nmodeling leads to the biggest gain, where the training\nspeed is accelerated by 17 √ó. This observation is natural to\ninterpret, as the encoding cost for the entire preÔ¨Åx of user\nhistory can now be saved by reusing the preceding encod-\ning results (as discussed in Section 4.1.4). The centralized\nnews encoding (Central) and the dynamic batching (Batch)\njointly improve the data efÔ¨Åciency, which results in another\n3√óspeedup. Besides, the cache-accelerated news encoding\n(Cache) and BusLM generate 2 √óand 1.27 √óspeedup, re-\nspectively.\n‚Ä¢Impact on Data EfÔ¨Åciency . The data efÔ¨Åciency (Eq. 1)\nis jointly increased with the centralized news encoding and\nthe dynamic batching (Figure 8). The original data efÔ¨Åciency\n(1-Bucket, w.o. CNE) is merely around 30%, which means\nroughly 70% of the computation is wasted on the padded\ndata. With the adoption of both techniques, a large portion\nof the padded data is removed, and the data efÔ¨Åciency can\nbe easily improved to more than 70%.\n‚Ä¢Impact on Recommendation Quality . Shown as Table\n5, the recommendation quality is reduced when bus and\ncontent reÔ¨Ånement are disabled. Particularly, for ‚Äúw.o. Bus‚Äù,\nthe bus connection is removed, where the partitioned news\nsegments become independently encoded. Without making\nreference to the whole context, it‚Äôs hard to fully express each\nsegment‚Äôs underlying semantic, which harms the quality\nof news embedding. For ‚Äúw.o. reÔ¨Åne‚Äù, the front part of\neach news article is truncated for input; therefore, it will\nresult in more severely information loss, which reduces the\nrecommendation quality.\n‚Ä¢More on Cache . The effect of cache-accelerated news\nencoding is tested with different values of Œ≥ with Table 6\n(the ‚Äúexpiration step‚Äù deÔ¨Åned in Section 4.1.2). With the in-\ncrement of Œ≥, the training speed is accelerated tremendously.\nWe choose ‚ÄúŒ≥ = 20‚Äù as our default setting for the trade-off\nof training efÔ¨Åciency and quality. Besides, it is more inter-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n0.30840.37650.37130.41570.63320.70600.73840.769600.20.40.60.811-Bucket 3-Bucket 5-Bucket 10-BucketData EfficiencyEffect of Dynamic Batching (DB) and Centralized News Encoding (CNE)w. DBw. CNE & DB26.2720.3920.6623.76051015202530#seg: 1 #seg: 3 #seg: 5 #seg: 10Time Cost per 100-Batch (s)Effect of BusLM on Training Time31.04520.21118.28718.66505101520253035#seg: 1 #seg: 3 #seg: 5 #seg: 10Memory Cost per Batch (GB)Effect of BusLM on G-RAM Cost\nFig. 9. BusLM‚Äôs effect on training speed and GPU RAM usage, with #seg\nincreased from 1 to 10.\nesting to see that the cache-accelerated news encoding also\ncontributes to the recommendation quality. This is probably\nbecause the cache-accelerated news encoding helps to put\nmore emphasis on the long-tailed news articles, which could\nsuffer from insufÔ¨Åcient training due to the dominance of the\nmost popular news. In other words, most of the training\nopportunities would be taken by a small number of news\narticles (if cached is disabled), given that a large portion of\nthe news clicks are resulted from the hottest minority (as\nreÔ¨Çected by Table 1).\n‚Ä¢More on BusLM. Shown as Figure 9, the training speed\nis improved and GPU RAM consumption is reduced thanks\nto BusLM (‚Äú#seg:1‚Äù means the input news remains one\nsingle sequence). However, it is also found that the training\nspeed will no longer increase when #seg is beyond 5. This\nis because the over-partition of the news will result in too\nmany bus elements (discussed in Section 4.1.3), which mag-\nniÔ¨Åes the cost of information exchange across the segments.\nSpeciÔ¨Åcally, it will slow down the layer-wise transformer\nencoding in Eq. 4. By default, the #seg is set to 3 for the best\nperformance.\n6 C ONCLUSION\nIn this paper, we propose a novel framework, SpeedyFeed,\nfor the efÔ¨Åcient training of PLMs-based news recom-\nmender. SpeedyFeed enjoys three technical advantages:\nhigh reusability, high data efÔ¨Åciency, and economic news\nencoding complexity, which jointly lead to a huge speedup\nof the training workÔ¨Çow. The proposed framework is ver-\niÔ¨Åed in Microsoft News, where signiÔ¨Åcant improvements\nare achieved in comprehensive evaluations. The proposed\nframework is made public-available so as to facilitate the\ndevelopment in related areas. In the future, we‚Äôll proactively\nextend this framework to support more real-world applica-\ntions, such as commodity and advertisement recommenda-\ntion.\nAPPENDIX A\nBUS LANGUAGE MODELING\nThe news article is split into K segments which are inter-\nconnected by bus technology. For the i-th layer of segment\nj, the Ô¨Årst tokens (i.e., [CLS]) is chosen to form the bus\nfor information exchange, then the bus is spliced into each\nsegment as the input of transformer layer:\nBusi = {Hi\nj[0]}K\nj=1, (6)\nHi+1\nj = Transformeri(\n[\nHi\nj, Busi]\n). (7)\nIn particular, for the self-attention layer of transformer, the\nbus is applied in key and value, and the query still only\nadopt the original embedding sequence of segments:\nQi\nj = Hi\njWi\nQ, (8)\nKi\nj = (\n[\nHi\nj, Busi]\n)Wi\nK, (9)\nVi\nj = (\n[\nHi\nj, Busi]\n)Wi\nV . (10)\nIn this way, Hi+1\nj has the same shape as Hi\nj. We repeat the\nEqns 6 and 7 for each layer.\nAfter all transformer layers, we aggregate all of the\nhidden states in the last layer (i.e., H‚àí1\n‚àó ) as news embed-\ndings by two additional attention layers. SpeciÔ¨Åcally, the\nÔ¨Årst attention layer is proposed to learn more informative\nrepresentations of segments. The attention weight Œ±j,n of\nthe n-th tokens in j-th segment is computed as:\nŒ±j,n = qT\n1 tanh(W1H‚àí1\nj,n + b1) (11)\nŒ±j,n = exp(Œ±j,n)‚àëL\nn=1 exp(Œ±j,n)\n(12)\nwhere W1 and b1 are projection parameters, and qT\n1 is the\nquery vector. The representation of segment is the weighted\nsummation of the contextual tokens representations, formu-\nlated as:\nvj =\nL‚àë\nn=1\nŒ±j,nH‚àí1\nj,n (13)\nThe second attention layer is to aggregate the segment\nembedding vj. Similarly, denote the attention weight of the\nj-th segment as Œ±j, which is calculated by:\nŒ±j = qT\n2 tanh(W2vj + b2) (14)\nŒ±j = exp(Œ±j)‚àëK\nj=1 exp(Œ±j)\n(15)\nwhere qT\n2 , W2, and b2 are learnable parameters. The Ô¨Ånal\nrepresentation of a news article is the summation of the seg-\nment representations weighted by their attention weights:\ne =\nK‚àë\nj=1\nŒ±jvj (16)\nAPPENDIX B\nDETAILS OF DATASET\n‚Ä¢News. Each news article is composed of its title, abstract\nand body. The average text length is 659.64, which is even\nlonger than the default maximum length (512) of ordinary\nPLMs. It is also challenging to load a sufÔ¨Åcient amount\nof training instances into a mini-batch given such long\ntexts. However, knowing that the title, abstract and the 1st\nparagraph in the body are the most informative parts for the\nmajority of Microsoft news, we‚Äôll take a text segment from\neach of them, whose length is no more than 32. Finally, the\noverall text length is conÔ¨Åned within 96 for the trade-off of\nquality and feasibility.\n‚Ä¢User. The users are characterized by their historical\ninteraction with the platform. As a result, each user is\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nassociated with one record, containing all of the user‚Äôs\nimpressions ordered by time:\n‚ÄúUser-ID # Impression-0 ... Impression-N‚Äù.\nEach impression refers to user‚Äôs news browsing behaviors\nwithin one interaction, which is in the format of:\n‚ÄúImpression-ID # Time # Clicked-list # Impressed-list‚Äù.\nThe clicked list refers to all of the news articles clicked\nby the user in the impression, the impressed list includes\nthe news articles shown to user but not clicked. The user‚Äôs\nactiveness follows long-tail distribution: each user has 15.27\nnews clicks on average; however, 3.26% users have more\nthan 100 news clicks, and 1.44% users have more than 150\nnews clicks. In our experiments, the user history is truncated\nto 100 uniformly.\nAPPENDIX C\nDETAILS OF TRAINING CONFIGURATIONS\nAll the training jobs are performed on an Azure 4 machine,\nwith 4√óNvidia-V100-32G GPUs, 40 √óIntel(R) Xeon(R) Plat-\ninum 8168 CPU @ 2.70GHz processors, run on Ubuntu\n16.04.6. The models are implemented with PyTorch 1.7.0.\nWe optimize the parameters with the Adam optimizer. The\nlearning rate is 8e-6 for pretrained model and 1e-4 for other\nlayers (e.g., user encoder). The negative sampling ratio is 1.\nThe max length of user click history is 100. The default max\nlength of news article is 96 and we split the text into three\nsegments according to the structure of title, abstract and\nbody. The default pretrained model is the complete UniLM.\nWe use 2 buckets and push the train data in buckets to the\nmini-batch queue once a basket is Ô¨Ålled with 39800 tokens.\nFor content reÔ¨Ånement, the k1 of BM25 is 2 and we reserve\nthe words of top-32 BM25 scores for each segment. For cache\nmanagement policy, the hyper-parameter Œ≤ is 2e-3, and the\ndefault expiration-step Œ≥of cache is 20.\nREFERENCES\n[1] F. Wu, Y. Qiao, J.-H. Chen, C. Wu, T. Qi, J. Lian, D. Liu, X. Xie,\nJ. Gao, W. Wu et al. , ‚ÄúMind: A large-scale dataset for news\nrecommendation,‚Äù in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 2020, pp. 3597‚Äì3606.\n[2] S. Okura, Y. Tagami, S. Ono, and A. Tajima, ‚ÄúEmbedding-based\nnews recommendation for millions of users,‚Äù in Proceedings of the\n23rd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, 2017, pp. 1933‚Äì1942.\n[3] L. Li, W. Chu, J. Langford, and R. E. Schapire, ‚ÄúA contextual-\nbandit approach to personalized news article recommendation,‚Äù\nin Proceedings of the 19th international conference on World wide web ,\n2010, pp. 661‚Äì670.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,‚Äù arXiv preprint arXiv:1810.04805, 2018.\n[5] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A ro-\nbustly optimized bert pretraining approach,‚Äù arXiv preprint\narXiv:1907.11692, 2019.\n[6] A. S. Das, M. Datar, A. Garg, and S. Rajaram, ‚ÄúGoogle news per-\nsonalization: scalable online collaborative Ô¨Åltering,‚Äù in Proceedings\nof the 16th international conference on World Wide Web, 2007, pp. 271‚Äì\n280.\n4. https://azure.microsoft.com/en-us/services/machine-learning/\n[7] P . Covington, J. Adams, and E. Sargin, ‚ÄúDeep neural networks\nfor youtube recommendations,‚Äù in Proceedings of the 10th ACM\nconference on recommender systems, 2016, pp. 191‚Äì198.\n[8] B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, ‚ÄúSession-\nbased recommendations with recurrent neural networks,‚Äù arXiv\npreprint arXiv:1511.06939, 2015.\n[9] G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin,\nH. Li, and K. Gai, ‚ÄúDeep interest network for click-through rate\nprediction,‚Äù in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining , 2018, pp. 1059‚Äì\n1068.\n[10] G. Zhou, N. Mou, Y. Fan, Q. Pi, W. Bian, C. Zhou, X. Zhu,\nand K. Gai, ‚ÄúDeep interest evolution network for click-through\nrate prediction,‚Äù in Proceedings of the AAAI conference on artiÔ¨Åcial\nintelligence, vol. 33, no. 01, 2019, pp. 5941‚Äì5948.\n[11] X. Chen, H. Xu, Y. Zhang, J. Tang, Y. Cao, Z. Qin, and H. Zha,\n‚ÄúSequential recommendation with user memory networks,‚Äù in\nProceedings of the eleventh ACM international conference on web search\nand data mining, 2018, pp. 108‚Äì116.\n[12] H. Wang, F. Zhang, X. Xie, and M. Guo, ‚ÄúDkn: Deep knowledge-\naware network for news recommendation,‚Äù in Proceedings of the\n2018 world wide web conference, 2018, pp. 1835‚Äì1844.\n[13] M. An, F. Wu, C. Wu, K. Zhang, Z. Liu, and X. Xie, ‚ÄúNeural news\nrecommendation with long-and short-term user representations,‚Äù\nin Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019, pp. 336‚Äì345.\n[14] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy,\n‚ÄúHierarchical attention networks for document classiÔ¨Åcation,‚Äù in\nProceedings of the 2016 conference of the North American chapter of the\nassociation for computational linguistics: human language technologies ,\n2016, pp. 1480‚Äì1489.\n[15] C. Wu, F. Wu, M. An, J. Huang, Y. Huang, and X. Xie, ‚ÄúNeural\nnews recommendation with attentive multi-view learning,‚Äù arXiv\npreprint arXiv:1907.05576, 2019.\n[16] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu,\nand H. Wu, ‚ÄúMulti-turn response selection for chatbots with deep\nattention matching network,‚Äù in Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), 2018, pp. 1118‚Äì1127.\n[17] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‚ÄúDis-\ntributed representations of words and phrases and their composi-\ntionality,‚Äù arXiv preprint arXiv:1310.4546, 2013.\n[18] J. Pennington, R. Socher, and C. D. Manning, ‚ÄúGlove: Global vec-\ntors for word representation,‚Äù in Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP) , 2014,\npp. 1532‚Äì1543.\n[19] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‚ÄúDeep contextualized word representations,‚Äù\narXiv preprint arXiv:1802.05365, 2018.\n[20] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, ‚ÄúIm-\nproving language understanding by generative pre-training,‚Äù\nTechnical report, OpenAI, 2018.\n[21] H. Bao, L. Dong, F. Wei, W. Wang, N. Yang, X. Liu, Y. Wang,\nJ. Gao, S. Piao, M. Zhou et al., ‚ÄúUnilmv2: Pseudo-masked language\nmodels for uniÔ¨Åed language model pre-training,‚Äù in International\nConference on Machine Learning. PMLR, 2020, pp. 642‚Äì652.\n[22] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP . Dhariwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell\net al. , ‚ÄúLanguage models are few-shot learners,‚Äù arXiv preprint\narXiv:2005.14165, 2020.\n[23] W.-C. Chang, F. X. Yu, Y.-W. Chang, Y. Yang, and S. Kumar, ‚ÄúPre-\ntraining tasks for embedding-based large-scale retrieval,‚Äù arXiv\npreprint arXiv:2002.03932, 2020.\n[24] S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston, ‚ÄúPoly-\nencoders: Transformer architectures and pre-training strategies\nfor fast and accurate multi-sentence scoring,‚Äù arXiv preprint\narXiv:1905.01969, 2019.\n[25] O. Khattab and M. Zaharia, ‚ÄúColbert: EfÔ¨Åcient and effective pas-\nsage search via contextualized late interaction over bert,‚Äù in Pro-\nceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, 2020, pp. 39‚Äì48.\n[26] K. Guu, K. Lee, Z. Tung, P . Pasupat, and M.-W. Chang, ‚ÄúRealm:\nRetrieval-augmented language model pre-training,‚Äù arXiv preprint\narXiv:2002.08909, 2020.\n[27] W. Lu, J. Jiao, and R. Zhang, ‚ÄúTwinbert: Distilling knowledge to\ntwin-structured compressed bert models for large-scale retrieval,‚Äù\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nin Proceedings of the 29th ACM International Conference on Informa-\ntion & Knowledge Management, 2020, pp. 2645‚Äì2652.\n[28] Microsoft Recommenders, 2020. [Online]. Available: https://github.\ncom/microsoft/recommenders/\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in neural information processing systems, 2017, pp. 5998‚Äì\n6008.\n[30] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, ‚ÄúEfÔ¨Åcient trans-\nformers: A survey,‚Äù arXiv preprint arXiv:2009.06732, 2020.\n[31] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, ‚ÄúMinilm:\nDeep self-attention distillation for task-agnostic compression of\npre-trained transformers,‚Äù arXiv preprint arXiv:2002.10957, 2020.\n[32] C. Wu, F. Wu, M. An, J. Huang, Y. Huang, and X. Xie, ‚ÄúNpa:\nneural news recommendation with personalized attention,‚Äù in\nProceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2019, pp. 2576‚Äì2584.\n[33] C. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, ‚ÄúNeural news\nrecommendation with multi-head self-attention,‚Äù in Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , Hong Kong, China, Nov. 2019, pp.\n6389‚Äì6394.\n[34] Y. A. Malkov and D. A. Yashunin, ‚ÄúEfÔ¨Åcient and robust approx-\nimate nearest neighbor search using hierarchical navigable small\nworld graphs,‚Äù IEEE transactions on pattern analysis and machine\nintelligence, vol. 42, no. 4, pp. 824‚Äì836, 2018."
}