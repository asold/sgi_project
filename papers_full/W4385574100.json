{
  "title": "Graph Hawkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs",
  "url": "https://openalex.org/W4385574100",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100229198",
      "name": "Haohai Sun",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101230946",
      "name": "Shangyi Geng",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2580167594",
      "name": "Jialun Zhong",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116723555",
      "name": "Han Hu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2116048628",
      "name": "Kun He",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3113177135",
    "https://openalex.org/W138372711",
    "https://openalex.org/W2798864014",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2962948632",
    "https://openalex.org/W3187578449",
    "https://openalex.org/W3176757281",
    "https://openalex.org/W2569260160",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W3035580113",
    "https://openalex.org/W3182741322",
    "https://openalex.org/W2300469216",
    "https://openalex.org/W2889782235",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W3097986917",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2996371683",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3007404067",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W2101645017",
    "https://openalex.org/W4302769929",
    "https://openalex.org/W2907192581",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3174368915",
    "https://openalex.org/W3121980643",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4289548669",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W3035591463",
    "https://openalex.org/W2792839191",
    "https://openalex.org/W3155883059",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3211666987",
    "https://openalex.org/W2998528434",
    "https://openalex.org/W3196669501",
    "https://openalex.org/W3187289530",
    "https://openalex.org/W3012871709"
  ],
  "abstract": "Temporal Knowledge Graph (TKG) reasoning has attracted increasing attention due to its enormous potential value, and the critical issue is how to model the complex temporal structure information effectively. Recent studies use the method of encoding graph snapshots into hidden vector space and then performing heuristic deductions, which perform well on the task of entity prediction. However, these approaches cannot predict when an event will occur and have the following limitations: 1) there are many facts not related to the query that can confuse the model; 2) there exists information forgetting caused by long-term evolutionary processes. To this end, we propose a Graph Hawkes Transformer (GHT) for both TKG entity prediction and time prediction tasks in the future time. In GHT, there are two variants of Transformer, which capture the instantaneous structural information and temporal evolution information, respectively, and a new relational continuous-time encoding function to facilitate feature evolution with the Hawkes process. Extensive experiments on four public datasets demonstrate its superior performance, especially on long-term evolutionary tasks.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7481â€“7493\nDecember 7-11, 2022 Â©2022 Association for Computational Linguistics\nGraph Hawkes Transformer for\nExtrapolated Reasoning on Temporal Knowledge Graphs\nHaohai Sun1âˆ— Shangyi Geng1âˆ— Jialun Zhong1 Han Hu2 Kun He1â€ \n1 School of Computer Science and Technology,\nHuazhong University of Science and Technology\n2 Microsoft Research Asia\n{haohais, shangyigeng, zhongjl}@hust.edu.cn\nhanhu@microsoft.com brooklet60@hust.edu.cn\nAbstract\nTemporal Knowledge Graph (TKG) reasoning\nhas attracted increasing attention due to its enor-\nmous potential value, and the critical issue is\nhow to model the complex temporal structure\ninformation effectively. Recent studies use the\nmethod of encoding graph snapshots into hid-\nden vector space and then performing heuristic\ndeductions, which perform well on the task of\nentity prediction. However, these approaches\ncannot predict when an event will occur, and\nhave the following limitations: 1) there are\nmany facts not related to the query that can\nconfuse the model; 2) there exists information\nforgetting caused by long-term evolutionary\nprocesses. To this end, we propose a Graph\nHawkes Transformer (GHT) for both TKG en-\ntity prediction and time prediction tasks in the\nfuture time. In GHT, there are two variants of\nTransformer, which capture the instantaneous\nstructural information and temporal evolution\ninformation, respectively, and a new relational\ncontinuous-time encoding function to facili-\ntate feature evolution with the Hawkes pro-\ncess. Extensive experiments on four public\ndatasets demonstrate its superior performance,\nespecially on long-term evolutionary tasks.\n1 Introduction\nKnowledge Graph (KG), a multi-relational di-\nrected graph database that stores human knowl-\nedge and facts, is widely used in downstream ap-\nplications such as recommendation systems (Guo\net al., 2020; Wang et al., 2018), web search (Paul-\nheim, 2017) and question answering (Saxena et al.,\n2021). Conventionally, KGs store each fact in the\nform of a triplet (ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡, ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘ğ‘¡ğ‘’, ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ).\nHowever, many facts may change over time\nand may contain event-based interaction data.\nTo encode the temporal information, Temporal\nKnowledge Graph (TKG) is proposed so that\n*Equal Contribution.\nâ€ Corresponding author.\neach temporal fact is stored as a quadruple\n(ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡, ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘ğ‘¡ğ‘’, ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡, ğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘ ). Typ-\nically, we represent TKG as a sequence of static\nKG snapshots associated with timestamps.\nThere are two types of reasoning for TKG. In-\nterpolated reasoning is designed to complete the\nmissing facts on known historical snapshots, while\nextrapolated reasoning predicts future events or\nfacts. This work focuses on extrapolated reason-\ning, a problematic but meaningful task, which can\nbe used for crisis warning, user behavior predic-\ntion, supply chain management, etc. While most\nexisting works (Han et al., 2021a; Sun et al., 2021)\nperform link predictions on snapshots at the next\ntimestamp, we consider predicting events that may\noccur over a long period in the future, which are\nmore appropriate in the real-world setting.\nThe task requires the assistance of known histor-\nical facts. A key feature required by the model is\nto retrieve key information from complex tempo-\nral structured data that can help answer the query\nand make correct judgments. Recent methods\nof RE-NET (Jin et al., 2020) and RE-GCN (Li\net al., 2021b) employ R-GCN (Schlichtkrull et al.,\n2018) to capture structural information from his-\ntorical snapshots and then use Recurrent Neural\nNetworks (RNNs) to model the latent vector se-\nquences. While the two methods work well on en-\ntity prediction task, they also meet some limitations.\n(1) Query information cannot be fully utilized by\nR-GCN. (2) RNN-based models assume that se-\nquences are equidistant, which is inconsistent with\nreal-life event sequences. (3) Step-by-step infer-\nence methods accumulate errors during training.\n(4) They cannot predict the timestamp that an event\nwill occur in the future.\nThere are some early methods (Trivedi et al.,\n2017, 2019) that could do both tasks simultane-\nously by using a temporal point process to repre-\nsent the evolution of facts, with the key concept\nof conditional intensity function denoting the con-\n7481\nditional probability of an event occurring over a\nperiod. Nevertheless, these methods could not per-\nform well on the entity prediction task as they\ndid not capture the structural information from\nthe graph snapshots. On the other hand, Trans-\nformer (Vaswani et al., 2017) has recently been\nwidely used in various fields (Lin et al., 2021) ow-\ning to its powerful modeling capabilities. However,\nto our knowledge, there exists no work that uses\nTransformer or its variants to solve the TKG evolu-\ntion problem.\nIn this work, we propose a new model termed\nGraph Hawkes Transformer (GHT), which intro-\nduces Transformer into the TKG evolution model-\ning and further integrates the neural temporal point\nprocess. Specifically, we design two Transformer\nvariants to capture the structural and temporal infor-\nmation in TKGs by building conditional intensity\nfunction. One variant is used to aggregate multi-\nrelational graphs, capturing structural information\nfor each timestamp and generating feature vectors.\nThe model can learn which interactions are more\ncritical in query with the attention mechanism. The\nother variant captures temporal information based\non a sequence of feature vectors and simultane-\nously outputs a hidden state for each timestamp in\nthe future. Finally, the model uses the hidden state\nto calculate the conditional intensity and then gets\nthe candidate entity score or the time probability\ndensity of the next event.\nOur main contributions are as follows:\nâ€¢ We propose a new Transformer-based model\nfor TKG extrapolated reasoning, which cap-\ntures both structural and temporal information.\nNot only can it simultaneously predict events\nof multiple timestamps in the future, but it can\nalso predict when an event will occur. To our\nknowledge, this is the first Transformer-based\ntemporal point process model for the TKG\nevolutionary representation learning.\nâ€¢ We design a new relational continuous-time\nencoding function that can handle unseen\ntimestamps and provide personalized re-\nsponses to different queries. It can be used to\nconstruct conditional intensity in the Hawkes\nprocess.\nâ€¢ State-of-the-art performance has been\nachieved on four popular benchmarks,\nindicating the effectiveness of our model,\nespecially on long-term evolution tasks.\n2 Related Work\n2.1 Neural Hawkes Process\nThe Hawkes process is a self-exciting temporal\npoint process applied to model sequential dis-\ncrete events occurring in continuous time (Hawkes,\n1971). It assumes that past events can temporarily\nexcite future events, characterized via an intensity\nfunction. The intensity function ğœ†âˆ—\nğ‘˜(ğ‘¡)represents\nthe expected number of events happened in interval\n(ğ‘¡,ğ‘¡ +ğ‘‘ğ‘¡]defined as follows, whereâˆ—is a shorthand\nfor conditioning on history Hğ‘¡:\nğœ†âˆ—\nğ‘˜(ğ‘¡)= lim\nÎ”ğ‘¡â†’0\nğ‘ƒ(ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡ ğ‘œğ‘“ ğ‘¡ğ‘¦ğ‘ğ‘’ ğ‘˜ğ‘–ğ‘›[ğ‘¡,ğ‘¡ +Î”ğ‘¡)|Hğ‘¡)\nÎ”ğ‘¡ .\n(1)\nThe traditional Hawkes process can only cap-\nture simple patterns of the events. In contrast, the\nneural Hawkes process (Shchur et al., 2021) that\nintroduces neural networks to parameterize the in-\ntensity function exhibits high model capacity in\ncomplex real-life scenarios. Researchers have re-\ncently started adapting neural networks, especially\nRNN models, to the temporal point process to build\nmore flexible and efficient models. RMTPP (Zhou\net al., 2013) embeds the sequence data into RNN\nand models the conditional intensity function con-\nsidering the historical non-linear dependence. Mei\nand Eisner (2016) develop a neural Hawkes pro-\ncess based on LSTM to model the asynchronous\nevent sequence. Zhang et al. (2020) apply the at-\ntention mechanism to the neural Hawkes process.\nMoreover, Zuo et al. (2020) propose a Transformer\nHawkes process, which utilizes Transformer to cap-\nture the complicated short-term and long-term tem-\nporal dependencies. However, these works do not\nmodel structured information.\n2.2 Temporal Knowledge Graph Reasoning\nEmbedding-based methods (Bordes et al., 2013;\nTrouillon et al., 2016) achieve excellent results\non static KGs, and have been extended to tempo-\nral KGs (Leblay and Chekol, 2018; GarcÃ­a-DurÃ¡n\net al., 2018; Goel et al., 2020; Lacroix et al., 2020).\nHowever, these methods cannot handle extrapo-\nlated reasoning since the timestamps in the test\ndataset do not exist in the training dataset. For\nextrapolation, Know-Evolve (Trivedi et al., 2017)\nand GHNN (Han et al., 2020) use temporal point\nprocesses to estimate conditional probability. How-\never, they fail to model the structural information\nin historical graphs, leading to low performance.\n7482\nRecently, TITer (Sun et al., 2021) uses rein-\nforcement learning to search answer in the his-\ntory. xERTE (Han et al., 2021a) finds answers\non subgraphs through subgraph sampling and at-\ntention flow. Both methods restrict the answer\ndomain to N-order neighbors and do not capture\nevolutionary representations. CyGNet (Zhu et al.,\n2021) uses a generate-copy mechanism to let the\nmodel remember recurring historical events. RE-\nNET (Jin et al., 2020) and RE-GCN (Li et al.,\n2021b) use RGCN (Schlichtkrull et al., 2018) to\ncapture structural information and then use RNN\nto perform representational evolution. Addition-\nally, TANGO (Han et al., 2021b) uses a neural\nordinary differential equation to model this task in\nthe continuous-time domain. CluSTeR (Li et al.,\n2021a) uses a two-stage approach for clue search-\ning and candidate ranking. These methods achieve\ngood performance on the entity prediction task but\ncannot predict time.\n3 The Proposed Model\nOur method first obtains concurrent structural in-\nformation from subgraphs and generates feature\nvectors for each source node. Then it captures the\ntemporal evolution information from the vector se-\nquences and finally outputs the hidden state vectors\nto participate in the construction of the conditional\nstrength function to complete the prediction task.\nCompared with previous evolutionary representa-\ntion learning methods, e.g., RE-NET (Jin et al.,\n2020) and RE-GCN (Li et al., 2021b), we expect\nthe model to make up for the four limitations intro-\nduced in Section 1. Figure 1 illustrates the overall\narchitecture of our model.\n3.1 Notations and Task Definition\nLet Eand Rdenote the sets of entities and relations,\nrespectively. Let Fğ‘¡ denote the set of facts at time ğ‘¡.\nA TKG can be represented as a sequence of static\nKG snapshots, i.e., a known TKG from time 1 to\ntime ğ‘¡can be described asG(1,ğ‘¡) ={G1,G2,..., Gğ‘¡},\nwhere Gğ‘¡ =(E,R,Fğ‘¡)is the KG snapshot at timeğ‘¡,\na directed multi-relational graph. Each fact in Fğ‘¡ is\ndescribed in the form of a quadruple (ğ‘’ğ‘ ,ğ‘Ÿ,ğ‘’ ğ‘œ,ğ‘¡),\nwhere ğ‘’ğ‘ ,ğ‘’ğ‘œ âˆˆE and ğ‘Ÿ âˆˆR. The quadruple can be\nseen as an edge from ğ‘’ğ‘  to ğ‘’ğ‘œ of type ğ‘Ÿat graph Gğ‘¡.\nBased on the historical TKG G(1,ğ‘¡), the model\nneeds to predict the facts in time ğ‘¡ğ‘. We further\nconsider another task of predicting the time of an\nevent that will occur in the future. For a concrete\nfact, the two tasks can be defined as follows.\nTask 1. Entity Prediction. Given a query\n(ğ‘’ğ‘ ,ğ‘Ÿğ‘,?,ğ‘¡ğ‘), the model needs to predict the miss-\ning entity 1.\nTask 2. Time Prediction. Given a query\n(ğ‘’ğ‘ ,ğ‘Ÿğ‘,ğ‘’ğ‘œ,?), the model needs to predict the times-\ntamp ğ‘¡ğ‘ that this event will occur next time.\nIn the framework of the neural Hawkes pro-\ncess (Shchur et al., 2021; Han et al., 2020), Hğ‘\nğ‘¡ =\n(ğ‘’ğ‘ ,ğ‘Ÿğ‘,G(1,ğ‘¡))denotes the historical information\nrelated to the query until time ğ‘¡. Let ğœ†ğ‘’(ğ‘¡|Hğ‘\nğ‘¡ )be\nthe conditional intensity function of a candidate ob-\nject entity ğ‘’, abbreviated as ğœ†ğ‘’\nğ‘¡, we can accomplish\nboth tasks with the conditional intensity function:\nğ‘’ğ‘œ =ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥\nğ‘’âˆˆE\n{ğœ†ğ‘’\nğ‘¡ğ‘ }, (2)\nğ‘(ğ‘¡â€²|ğ‘’ğ‘œ,Hğ‘\nğ‘¡ )=ğœ†ğ‘’ğ‘œ\nğ‘¡â€² ğ‘’ğ‘¥ğ‘\n(\nâˆ’\nâˆ« ğ‘¡â€²\nğ‘¡\nğœğœ†ğ‘’ğ‘œ\nğœ ğ‘‘ğœ\n)\n, (3)\nğ‘¡ğ‘ =\nâˆ« +âˆ\nğ‘¡\nğœğ‘(ğœ|ğ‘’ğ‘œ,Hğ‘\nğ‘¡ )ğ‘‘ğœ. (4)\nSince all candidate entities share the same sur-\nvival term (Han et al., 2020), we can directly com-\npare the intensity value to get the answer entity, as\nshown in Eq. 2. Eq. 3 is the corresponding condi-\ntional time density function, according to which we\ncan estimate the time through the integral formula\nof Eq. 4. Then, we introduce how to model the\nintensity function.\n3.2 Relational Graph Transformer\nCapturing the structural information on historical\nsnapshots is the first key to answer the query. Pre-\nvious works (Jin et al., 2020; Li et al., 2021b) use\nR-GCN (Schlichtkrull et al., 2018) for informa-\ntion aggregation to store interaction information\nbetween nodes in the form of hidden vectors.\nHowever, in a historical snapshot, many events\nrelate to an entity simultaneously, yet only a tiny\nfraction of the relational information is helpful to\nanswer the query. R-GCN cannot handle this issue\nas it treats every message equally important. There-\nfore, we design a Relational Graph Transformer\n(RGT) to let our model know which concurrent\nevents are more critical in a snapshot. There are\ntwo forms of graph representation: global recep-\ntive field (Ying et al., 2021) and local receptive\nfield (Velickovic et al., 2018; Dwivedi and Bresson,\n1Without loss of generality, we can transform\n(?,ğ‘Ÿğ‘,ğ‘’ğ‘œ,ğ‘¡ğ‘) into (ğ‘’ğ‘œ,ğ‘Ÿâˆ’1ğ‘ ,?,ğ‘¡ğ‘) by changing subject\nprediction to object prediction, where ğ‘Ÿâˆ’1ğ‘ is the reciprocal\nrelation of ğ‘Ÿğ‘.\n7483\nFigure 1: An illustration of the proposed GHT model. The Relational Graph Transformer encodes the graph\nstructure for each historical snapshot. The Temporal Transformer updates the hidden state to any future timestamp\nbased on the historical state sequence. The conditional intensity and probability density are calculated based on the\nhidden vector sequences.\n2020; Hu et al., 2020). The global form cannot\ncope with graphs with too many nodes. Thus, to\nmaximize the preservation of structural informa-\ntion of the graph, RGT operates in local graph\nneighborhoods.\nLet E âˆˆR|E|Ã—ğ‘‘ and R âˆˆR|R|Ã—ğ‘‘ be the initial\nembedding matrices of the entities and relations,\nrespectively, where ğ‘‘represents the dimension of\nthe embedding2. eğ‘– = E[ğ‘–]is the embedding of\nentity ğ‘’ğ‘–, and rğ‘– =R[ğ‘–]is the embedding of relation\nğ‘Ÿğ‘–. RGT aggregates structural information from\neach incoming edge for each entity in the snapshots\nsequence.\nSpecifically, for Gğ‘\nğ‘¡ , we initialize the hidden\nstates of the nodes as the initial embeddings of the\ncorresponding entities, and each node will update\nits hidden state via the message-passing framework.\nE.g., for an entity ğ‘’ğ‘–, we concatenate the hidden\nstate of the source node and the corresponding rela-\ntion embedding as the message for each incoming\nedge ğ‘’ğ‘–. Then we pack them together as the key ma-\ntrix ğ¾ âˆˆR|ğ‘€|Ã—2ğ‘‘ and value matrix ğ‘‰ âˆˆR|ğ‘€|Ã—2ğ‘‘,\nwhere |ğ‘€|is the number of messages. For each\nmessage, we use rğ‘ as the query, and the query\nmatrix is ğ‘„ âˆˆR|ğ‘€|Ã—ğ‘‘. Finally, the hidden state of\nan entity at layer ğ‘™ âˆˆ[1,ğ‘]is defined as follows:\nhğ‘™\nğ‘¡ =ğ¿ğ‘(ğ¹ğ¹ğ‘(ğ‘€ğ»ğ´(ğ‘„,ğ¾,ğ‘‰ )+hğ‘™âˆ’1\nğ‘¡ ), (5)\n2For brevity, we define all definable parameterâ€™s dimension\nas ğ‘‘.\nwhere ğ¿ğ‘ is the layer normalization, ğ¹ğ¹ğ‘ is the\nfeed-forward blocks, and ğ‘€ğ»ğ´ is Multi-Head At-\ntention (Vaswani et al., 2017). Finally, we out-\nput the hidden state sequence of the query entity\nğ»ğ‘’ ={hğ‘\nğ‘¡âˆ’ğ‘š+1,..., hğ‘\nğ‘¡ }. We do not introduce query\nentity information eğ‘ in the query matrix ğ‘„, be-\ncause as a TKG develops, new entities could appear\non the graph, and the model has not learned their\ninitial embeddings. In contrast, the semantics of\nrelations are stable and context-independent , so all\nlayers share the relational embedding matrix.\n3.3 Temporal Transformer\nWe design the Temporal Transformer (TT) to model\nthe temporal evolution of entity representations in\nthe continuous-time domain. Transformer learns\nlocation information through a position encoding\nfunction. Although researchers have designed a\nvariety of position encoding methods (Devlin et al.,\n2019; Shaw et al., 2018; Dai et al., 2019; Ke et al.,\n2021), most of them do not apply to our scenario\ndue to the following two reasons: (1) These posi-\ntion encoding methods are performed in the dis-\ncrete domain and are not suitable for the Hawkes\nprocess. (2) Different from discrete tokens, our\ninput is a continuous vector after the graph infor-\nmation aggregation, and the distribution shift be-\ntween the training set and the test set will lead\nthem (Vaswani et al., 2017; Devlin et al., 2019) to\nhave poor generalization performance.\n7484\nMoreover, the temporal distribution of attention\ncould be different for different query event types.\nFor example, diarrhea is more likely to be caused\nby eating spoiled food the day before, while long-\nterm eating habits may cause obesity. Thus, we re-\ndesign a relational continuous-time encoding func-\ntion to assist the attention calculation. We need\nto ensure the inductive ability of the function be-\ncause future timestamps cannot be seen during the\nmodel training. Based on the calculation princi-\nple of Transformer attention, we need to ensure\nthat (Xu et al., 2020), âˆ€ğ‘âˆˆR:\nâŸ¨ğ‘‡ğ¸(ğ‘¡ğ‘˜ +ğ‘),ğ‘‡ğ¸ (ğ‘¡ğ‘ +ğ‘)âŸ©=âŸ¨ğ‘‡ğ¸(ğ‘¡ğ‘˜),ğ‘‡ğ¸ (ğ‘¡ğ‘)âŸ©,\nwhere ğ‘‡ğ¸ denotes the time encoding function, âŸ¨Â·,Â·âŸ©\nindicates inner product, ğ‘¡ğ‘˜ and ğ‘¡ğ‘ represent the time\nof key and time of query in the attention calculation,\nrespectively. We can use absolute time encoding to\nrepresent relative time information.\nWe design a learnable sinusoid function that\nmeets the above condition, and use the query rela-\ntion rğ‘ to control the amplitude of the function:\nğ‘‡ğ¸ğ‘Ÿ(ğ‘¡)=[ğ›¼ğ‘Ÿ\n1 ğ‘ğ‘œğ‘ (ğ‘¤1ğ‘¡),ğ›¼ğ‘Ÿ\n1 ğ‘ ğ‘–ğ‘›(ğ‘¤1ğ‘¡),...,\nğ›¼ğ‘Ÿ\nğ‘‘ğ‘ğ‘œğ‘ (ğ‘¤ğ‘‘ğ‘¡),ğ›¼ğ‘Ÿ\nğ‘‘ğ‘ ğ‘–ğ‘›(ğ‘¤ğ‘‘ğ‘¡)], (6)\nwhere [ğ›¼ğ‘Ÿ\n1,...,ğ›¼ ğ‘Ÿ\nğ‘‘]is the linear projection of the\nrelation embedding r, and [ğ‘¤1,...,ğ‘¤ ğ‘‘] is a ğ‘‘-\ndimensional learnable vector. We use the time\ninformation to calculate a bias term for the atten-\ntion matrix ğ´. Let ğ‘‡ğ‘ =[ğ‘‡ğ¸ğ‘Ÿğ‘ (ğ‘¡ğ‘1 ); ...;ğ‘‡ğ¸ğ‘Ÿğ‘ (ğ‘¡ğ‘ğ‘  )]\ndenote the queriesâ€™ time encoding matrix, and\nğ‘‡ğ‘˜ = [ğ‘‡ğ¸ğ‘Ÿğ‘ (ğ‘¡ğ‘˜1 ); ...;ğ‘‡ğ¸ğ‘Ÿğ‘ (ğ‘¡ğ‘˜ğ‘š )]denotes the keysâ€™\ntime encoding matrix, we have the attention matrix\nğ´as follows:\nğ´= (ğ»ğ‘ğ‘Šğ‘„)(ğ»ğ‘’ğ‘Šğ¾)ğ‘‡ +(ğ‘‡ğ‘)(ğ‘‡ğ‘˜)ğ‘‡\nâˆš\n2ğ‘‘\n, (7)\nwhere ğ‘Šğ‘„,ğ‘Šğ¾ are weight matrices. ğ»ğ‘ is the em-\nbedding matrix of the query relation and ğ»ğ‘’ is the\npacked matrix of the query entity embedding se-\nquence obtained from RGT.\n3.4 Conditional Intensity\nAfter the encoding described in Section 3.3, we\ncan generate a hidden representation at any time\nin the future. Then, we can use it to construct a\ncontinuous-time conditional intensity function ğœ†ğ‘’ğ‘–\nğ‘¡ğ‘\nfor all candidate entities:\nğœ†ğ‘’ğ‘–\nğ‘¡ğ‘ = ğ‘“(âŸ¨[eğ‘ ,rğ‘,hğ‘¡ğ‘ ]ğ‘Šğœ†, eğ‘–âŸ©), (8)\nwhere ğ‘Šğœ† âˆˆR3ğ‘‘Ã—ğ‘‘ is the projection matrix, hğ‘¡ğ‘\nis the hidden state at time ğ‘¡ğ‘ obtained from TT,\nğ‘“(ğ‘¥)= ğ›½Â·ğ‘™ğ‘œğ‘”(1 +ğ‘’ğ‘¥ğ‘(ğ‘¥\nğ›½))is softplus function\nwith parameter ğ›½that guarantees a positive inten-\nsity, and eğ‘ , rğ‘, and eğ‘– are the embeddings of the\nquery entity, query relation and candidate entity,\nrespectively.\nFinally, we can predict the entity or time based\non Eq. 2, Eq. 3 and Eq. 4. For the integral oper-\nation, we use the trapezoidal rule (Stoer and Bu-\nlirsch, 2013) for approximation:\nğ‘¡ğ‘ =\nğ¿âˆ‘ï¸\nğ‘—=1\nğ‘¡ğ‘— âˆ’ğ‘¡ğ‘—âˆ’1\n2\n(ğ‘¡ğ‘—ğ‘(ğ‘¡ğ‘—|ğ‘’ğ‘œ,Hğ‘\nğ‘¡ )+\nğ‘¡ğ‘—âˆ’1 ğ‘(ğ‘¡ğ‘—âˆ’1|ğ‘’ğ‘œ,Hğ‘\nğ‘¡ )),\n(9)\nwhere ğ‘¡ğ‘— âˆˆ[ğ‘¡,+âˆ), and ğ¿ is the number of sam-\nples. Other estimation methods, such as Simpsonâ€™s\nrule (Stoer and Bulirsch, 2013) and Monte Carlo\nintegration (Stoer and Bulirsch, 2013) can also be\nused.\n3.5 Training\nWe view the entity prediction task as a multi-class\nclassification task and the time prediction task as\na regression task. Then, we use the cross-entropy\nloss for the entity prediction task and the mean\nsquare error (MSE) loss for the time prediction\ntask. Let Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› denote the training set, ğ¿ğ‘’ be the\nloss of the entity prediction and ğ¿ğ‘¡ be the loss of\ntime prediction. Then,\nğ¿ğ‘’ =âˆ’\n|Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› |âˆ‘ï¸\nğ‘–=1\n|E|âˆ’1âˆ‘ï¸\nğ‘=0\nğ‘¦ğ‘ğ‘™ğ‘œğ‘”(ğ‘(ğ‘’ğ‘œğ‘– =ğ‘|Hğ‘\nğ‘¡ğ‘– )), (10)\nğ¿ğ‘¡ =\n|Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› |âˆ‘ï¸\nğ‘–=1\n(ğ‘¡ğ‘– âˆ’Ë†ğ‘¡ğ‘–)2. (11)\nHere Ë†ğ‘¡ğ‘– is the estimated time. We jointly train the\ntwo tasks, and the final loss ğ¿ =ğ¿ğ‘’ +ğœ‡ğ¿ğ‘¡, where\nğœ‡is a hyperparameter.\n4 Experiments\n4.1 Experimental Setup\nDatasets We evaluate our model on four public\nTKG datasets, ICEWS14, ICEWS18 ICEWS05-\n15 and GDELT. Integrated Crisis Early Warning\nSystem (ICEWS) (Boschee et al., 2015) is an inter-\nnational event dataset. Its three subsets, ICEWS14,\nICEWS18 and ICEWS05-15, are usually used to\nevaluate the performance of TKG reasoning mod-\nels, which contains events occurring in 2014, 2018,\n7485\nand 2005 to 2015 respectively. Global Database\nof Events, Language and Tone (GDELT) is a large\ncomprehensive event dataset that records data ev-\nery 15 minutes. Following the previous work (Jin\net al., 2020), we split the dataset into train/valid/test\nby timestamps, and ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘¡ğ‘–ğ‘šğ‘’ < ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ ğ‘¡ğ‘–ğ‘šğ‘’ <\nğ‘¡ğ‘’ğ‘ ğ‘¡ğ‘¡ğ‘–ğ‘šğ‘’ . Statistics of these datasets are shown in\nAppendix A.1.\nEvaluation Metrics The MRR and Hits@ğ‘˜(ğ‘˜ âˆˆ\n{1,3,10}) are standard metrics for the entity pre-\ndiction task. MRR is the average reciprocal of the\ncorrect query answer rank. Hits@ ğ‘˜ indicates the\nproportion of correct answers among the top ğ‘˜can-\ndidates. As mentioned in (Han et al., 2020; Sun\net al., 2021), the static filtered setting (Jin et al.,\n2020; Zhu et al., 2021), which removes entities\nfrom candidates according to the triples without\nconsidering the time, is unsuitable for TKG rea-\nsoning. Thus, we adopt the time-aware filtered\nsetting (Han et al., 2020; Sun et al., 2021; Han\net al., 2021a).\nMoreover, most previous works only evaluate\nthe performance of their models for the entity pre-\ndiction at the next timestamp. Such evaluation\ncannot adequately reflect the modelâ€™s performance\nfor future predictions. Therefore, we evaluate the\nmodelâ€™s short-term and long-term evolution by set-\nting different forecasting time window size of Î”ğ‘¡.\nFor the time prediction task, we use the mean\nabsolute error (MAE) between the predicted time\nand ground truth time as the metric.\nBaselines For the entity prediction task, we\ncompare our model with three types of KG rea-\nsoning models: (1) static KG reasoning mod-\nels, including TransE (Bordes et al., 2013), Dist-\nMult (Yang et al., 2014) and ComplEx (Trouillon\net al., 2016). Ignoring the time information, we\ncan get a static knowledge graph, and then ap-\nply these model. (2) TKG interpolated reasoning\nmodels, including TTransE (Leblay and Chekol,\n2018), TA-DistMult (GarcÃ­a-DurÃ¡n et al., 2018),\nDE-SimplE (Goel et al., 2020), and TNTCom-\nplEx (Lacroix et al., 2020); (3) TKG extrapolated\nreasoning models, including RE-NET (Jin et al.,\n2020), CyGNet (Zhu et al., 2021), RE-GCN (Li\net al., 2021b) and TITer (Sun et al., 2021). It is\nworth noting that RE-NET and RE-GCN are the\nmost relevant model, which also learn evolutionary\nrepresentation.\nFor the time prediction task, we compare our\nmodel with the previous temporal point process\nmodel for TKG, GHNN (Han et al., 2020). As for\nKnow-evolve (Trivedi et al., 2017), it has a prob-\nlematic formulation that has already been discussed\nin (Jin et al., 2020; Han et al., 2020). Thus, we do\nnot choose it as a baseline.\n4.2 Implementation Details\nBaseline details During the inference, static rea-\nsoning models, interpolated reasoning models, and\nCyGNet do not encode the historical information.\nTherefore, they perform the same in the short-\nterm as well as the long-term evolution task, so\nwe directly use the results reported in previous\nworks (Sun et al., 2021; Han et al., 2021a). For\nTITer3, RE-NET4, RE-GCN5, and GHNN6, using\ntheir released source code with default hyperparam-\neters, we rerun these models on four benchmarks.\nTo ensure fairness, for RE-GCN, we do not use the\nmodule that encodes the node type information.\nGHT details We implement our model in Py-\nTorch. We set the dimension of all embeddings and\nhidden states to 100. The history length is limited\nto 6. The layer number of the Relational Graph\nTransformer is set to 2, and the attention head num-\nber is set to 1. For Temporal Transformer, the layer\nnumber is 2, and the attention head number is 2.\nWe apply dropout with dropout rate of 50% to each\nlayer and use label smoothing to suppress overfit-\nting. The hyperparameter ğœ‡used for loss function\nis set to 0.2. We use Adam to optimize the param-\neters, with a learning rate of 0.003 and a weight\ndecay of 0.0001. The batch size is set to 256. All\nexperiments were done on Tesla P100.\n4.3 Results and Discussion\n4.3.1 Entity Prediction\nTable 1 and Figure 2 report the experimental results\non the entity prediction task. We report results for\nÎ”ğ‘¡ = 10 in details, other cases can be found in\nAppendix A.2. To explore the effect of evolution\ntime on our modelâ€™s performance, we report MRR\nover four datasets as the evolution timespan grows,\nwhere ğ‘‘ğ‘¡ indicates that the model predicts events\nafter ğ‘‘ğ‘¡ interval. Due to layout limitations, the\nexperimental results for GDELT are in Appendix\nA.2.\n3https://github.com/JHL-HUST/TITer\n4https://github.com/INK-USC/RE-Net\n5https://github.com/Lee-zix/RE-GCN\n6https://github.com/Jeff20100601/GHNN_clean\n7486\nModel ICEWS14 ICEWS18 ICEWS0515 GDELT\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nTransE 22.48 13.36 25.63 41.2312.24 5.84 12.81 25.1022.55 13.05 25.61 42.05- - - -\nDistmult 27.67 18.16 31.15 46.9610.17 4.52 10.33 21.2528.73 19.33 32.19 47.548.61 3.91 8.27 17.04\nComplEx 30.84 21.51 34.48 49.5921.01 11.87 23.47 39.9731.69 21.44 35.74 52.049.84 5.17 9.58 18.23\nTTransE 13.43 3.11 17.32 34.55 8.31 1.92 8.56 21.89 15.71 5.00 19.72 38.025.50 0.49 4.99 15.18\nTA-DistMult26.47 17.09 30.22 45.4116.75 8.61 18.41 33.5924.31 14.58 27.92 44.2111.17 5.09 11.58 22.65\nDE-SimplE32.67 24.43 35.69 49.1119.30 11.53 21.86 34.8035.02 25.91 38.99 52.75- - - -\nTNTComplEx32.12 23.35 36.03 49.1321.23 13.28 24.02 36.9127.54 19.52 30.80 42.86- - - -\nCyGNet 32.73 23.69 36.31 50.6724.93 15.90 28.28 42.6134.97 25.67 39.09 52.9418.05 11.13 19.11 31.50\nTITer 36.06 27.5140.16 52.0525.34 18.0928.17 38.9538.11 26.83 44.43 59.4415.75 10.94 15.74 25.37\nRE-NET 35.71 26.79 39.57 52.8126.88 17.70 30.35 45.0439.11 29.04 44.10 58.90- - - -\nRE-GCN 36.08 27.30 39.89 53.0127.3418.4430.4844.8539.8930.1044.6356.9819.0712.1620.2032.39\nGHT 37.40\nÂ±0.13\n27.77\nÂ±0.10\n41.66\nÂ±0.09\n56.19\nÂ±0.07\n27.40\nÂ±0.12\n18.08\nÂ±0.09\n30.76\nÂ±0.12\n45.76\nÂ±0.14\n41.5\nÂ±0.15\n30.79\nÂ±0.17\n46.85\nÂ±0.19\n62.73\nÂ±0.15\n20.04\nÂ±0.18\n12.68\nÂ±0.19\n21.37\nÂ±0.10\n34.42\nÂ±0.14\nTable 1: Entity prediction results (forecasting time window size is 10 units of timeÎ”ğ‘¡ =10). Evaluation metrics are\ntime-aware filtered MRR and Hits@1/3/10. All results are multiplied by 100. The last row of the table illustrates\nthe bias of GHT during experiments.\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000047/uni00000057\n/uni00000016/uni00000017\n/uni00000016/uni00000018\n/uni00000016/uni00000019\n/uni00000016/uni0000001a\n/uni00000016/uni0000001b\n/uni00000016/uni0000001c\n/uni00000017/uni00000013\n/uni00000017/uni00000014\n/uni00000017/uni00000015/uni00000030/uni00000035/uni00000035\n/uni00000037/uni0000002c/uni00000037/uni00000048/uni00000055\n/uni00000035/uni00000028/uni00000010/uni00000031/uni00000028/uni00000037\n/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031\n/uni0000002a/uni0000002b/uni00000037\n(a) ICEWS14\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000047/uni00000057\n/uni00000015/uni00000016\n/uni00000015/uni00000017\n/uni00000015/uni00000018\n/uni00000015/uni00000019\n/uni00000015/uni0000001a\n/uni00000015/uni0000001b\n/uni00000015/uni0000001c\n/uni00000016/uni00000013\n/uni00000016/uni00000014/uni00000030/uni00000035/uni00000035\n/uni00000037/uni0000002c/uni00000037/uni00000048/uni00000055\n/uni00000035/uni00000028/uni00000010/uni00000031/uni00000028/uni00000037\n/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031\n/uni0000002a/uni0000002b/uni00000037 (b) ICEWS18\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000047/uni00000057\n/uni00000016/uni00000017\n/uni00000016/uni00000019\n/uni00000016/uni0000001b\n/uni00000017/uni00000013\n/uni00000017/uni00000015\n/uni00000017/uni00000017\n/uni00000017/uni00000019/uni00000030/uni00000035/uni00000035\n/uni00000037/uni0000002c/uni00000037/uni00000048/uni00000055\n/uni00000035/uni00000028/uni00000010/uni00000031/uni00000028/uni00000037\n/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031\n/uni0000002a/uni0000002b/uni00000037 (c) ICEWS05-15\nFigure 2: MRR on ICEWS datasets with regard to various ğ‘‘ğ‘¡, which denote the modelsâ€™ performance at future ğ‘‘ğ‘¡\nmoments.\nThe extrapolated reasoning methods can deal\nwith unseen timestamps. GHT outperforms all\nthese methods on metrics MRR, Hits@3, and\nHits@10. As for Hits@1, GHT also achieves the\nstate-of-the-art (SOTA) performance. We further\nanalyze and compare these models through the\ncurve diagrams in Figure 2. As can be seen, al-\nthough TITer outperforms the other models when\nğ‘‘ğ‘¡ = 1, it decays faster on long-term future pre-\ndictions. We think this is because it searches the\nanswer in local subgraphs, focusing more on the\nexplicit cues. As the evolution timespan grows, the\nexplicit cues will gradually decrease, and the im-\nplicit cues will become more important. Moreover,\nretrieving answers from neighbor subgraphs limits\nthe candidate answer space, which is more pro-\nnounced on long-term evolution tasks. Our model\noutperforms RE-NET and RE-GCN in predicting\nlong-term events because they use a heuristic evolu-\ntionary method to predict long-term events through\ngradual evolution, which forgets previous knowl-\nedge during the evolution. By contrast, we intro-\nduce an attention mechanism when encoding struc-\nFigure 3: Time prediction results. Lower is better.\ntural information, which helps the model capture\nmore helpful information.\nOverall, our model has excellent performance,\nproviding more accurate answers when perform-\ning entity prediction tasks, especially over long\nintervals.\n4.3.2 Time Prediction\nFigure 3 shows the results of the time prediction\ntask. The result indicates the superiority of our\nmodel utilizing Transformer for conditional in-\ntensity function construction. GHNN aggregates\n7487\nFigure 4: Time prediction ablation study on ICEWS14.\nModel MRR H@1 H@3 H@10\nGHT 37.40 27.77 41.66 56.19\nGHT w.o. TE 36.21 26.52 40.31 55.69\nGHT w. APE 36.42 26.91 40.46 55.20\nGHT w. RPE 36.49 26.88 40.73 55.24\nGHT w. RGCN 33.86 24.25 37.87 52.75\nTable 2: Entity prediction ablation study on ICEWS14.\nneighbor information through simple mean pooling\nand only focuses on the most relevant first-order\nneighbors. In comparison, our RGT has a better\nstructural information extraction ability. Moreover,\nGHNN uses continuous-time LSTM (Mei and Eis-\nner, 2016) to estimate the intensity function, which\nis not as good as Transformer in capturing complex\nlong-term and short-term dependencies(Zuo et al.,\n2020). Therefore, GHT outpeforms GHNN on the\ntime prediction task.\nDuring the experiment, our model also shows\ngood efficiency. The discussion on efficiency anal-\nysis can be found in Appendix A.3.\n4.4 Ablation Study\nIn this subsection, we study the effect of each mod-\nule separately, and the results are reported in Table\n2 and Figure 4. We do all ablation experiments on\nthe ICEWS14 dataset. Specially, we try various\nposition encoding functions to show the effective-\nness of the relational time encoding function, and\nuse other relational GNNs instead of RGT to verify\nthe performance of RGT. We report the entity pre-\ndiction results in Table 2, and the time prediction\nresults in Figure 4. We also study the hyperpa-\nrameterâ€™s sensitivity of GHT, which can be seen in\nAppendix A.4\n4.4.1 Different Graph Aggregator\nR-GCN, the most common relational graph aggre-\ngator, is used in RE-NET and RE-GCN. We replace\nRGT with R-GCN and denote the model as GHT\nw. RGCN. Results in Table 2 and Figure 4 demon-\nstrate that GHT significantly outperforms GHT w.\nRGCN on both entity and time prediction tasks. It\nindicates that RGT is more suitable for TKG ex-\ntrapolated reasoning than R-GCN. Compared with\nR-GCN, RGT can extract more useful information\nfor query answering from complex structural infor-\nmation.\n4.4.2 Different Position Encoding Functions\nTo verify whether our proposed relational\ncontinuous-time encoding function is effective, we\ncompare GHT with several variants, which use\ndifferent position encoding methods: (1) Do not\nuse the relational continuous-time encoding func-\ntion, denoted as GHT w.o. TE; (2) Replace it with\nabsolute position encoding (Devlin et al., 2019),\ndenoted as GHT w. APE; (3) Replace it with rela-\ntive position encoding (Ke et al., 2021), denoted as\nGHT w. RPE. In Table 2 and Figure 4, we observe\nthat GHT outperforms all other variants on both\nentity and time prediction tasks. Note that GHT w.\nAPE and GHT w. RPE perform almost the same\nas not using the position encoding function. This\nindicates that they did not learn the location infor-\nmation. By contrast, relational continuous-time\nencoding can effectively capture temporal informa-\ntion and help the model achieve good performance.\n5 Conclusion\nWe propose a new TKG reasoning model, called\nGraph Hawkes Transformer (GHT), a neural tempo-\nral point process model based on Transformer. We\nfirst analyze four limitations of the previous state-\nof-the-art methods, RE-GCN (Li et al., 2021b) and\nRE-NET (Jin et al., 2020). Then we design two\nTransformer blocks, which are used to capture the\nstructural and temporal information, respectively.\nBased on Hawkes process, the model can learn a\nconditional intensity function to solve the above\nissues with the attention mechanism and the pro-\nposed relational continuous-time encoding func-\ntion. Moreover, most previous works only evalu-\nate their methods for entity prediction at the next\nfuture timestamp, while we evaluate the models\ncomprehensively by setting different forecasting\ntime window sizes. Experimental results on four\npopular datasets demonstrate the superior perfor-\nmance of our model on both entity prediction and\ntime prediction tasks. Notably, our model performs\nmuch better under long-term evolution scenarios.\n7488\n6 Limitations\nThe inference of GHT relies entirely on the learned\nentity representation, and new entities that emerge\nas TKG evolves will get a random initialization,\nwhich limits the modelâ€™s inductive reasoning abil-\nity.\nWhen making time predictions, the integral op-\neration is implemented by approximate estimation.\nThe more accurate the estimation, the more calcu-\nlations are required. The attention computation in\nTransformer is also of quadratic complexity, which\nrequires much calculation. All of these factors limit\nthe capability of GHT to handle large-scale graphs.\nAcknowledgements\nThis work is supported by National Natural Sci-\nence Foundation (62076105) and International Co-\noperation Foundation of Hubei Province, China\n(2021EHB011).\nReferences\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Neural Information Processing\nSystems, pages 2787â€“2795.\nElizabeth Boschee, Jennifer Lautenschlager, Sean\nOâ€™Brien, Steve Shellman, James Starz, and Michael\nWard. 2015. ICEWS Coded Event Data.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Computa-\ntional Linguistics, pages 2978â€“2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171â€“4186.\nVijay Prakash Dwivedi and Xavier Bresson. 2020. A\ngeneralization of transformer networks to graphs.\narXiv preprint arXiv:2012.09699.\nAlberto GarcÃ­a-DurÃ¡n, Sebastijan Dumancic, and Math-\nias Niepert. 2018. Learning sequence encoders for\ntemporal knowledge graph completion. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4816â€“4821.\nAlberto GarcÃ­a-DurÃ¡n, Sebastijan DumanË‡ciÂ´c, and Math-\nias Niepert. 2018. Learning sequence encoders\nfor temporal knowledge graph completion. arXiv\npreprint arXiv:1809.03202.\nRishab Goel, Seyed Mehran Kazemi, Marcus Brubaker,\nand Pascal Poupart. 2020. Diachronic embedding\nfor temporal knowledge graph completion. In Thirty-\nFourth AAAI Conference on Artificial Intelligence ,\npages 3988â€“3995.\nQingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu\nZhu, Xing Xie, Hui Xiong, and Qing He. 2020. A\nsurvey on knowledge graph-based recommender sys-\ntems. IEEE Transactions on Knowledge and Data\nEngineering.\nZhen Han, Peng Chen, Yunpu Ma, and V olker Tresp.\n2021a. Explainable subgraph reasoning for forecast-\ning on temporal knowledge graphs. In International\nConference on Learning Representations.\nZhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, and\nV olker Tresp. 2021b. Learning neural ordinary equa-\ntions for forecasting future links on temporal knowl-\nedge graphs. In Empirical Methods in Natural Lan-\nguage Processing, pages 8352â€“8364.\nZhen Han, Yunpu Ma, Yuyi Wang, Stephan GÃ¼nnemann,\nand V olker Tresp. 2020. Graph hawkes neural net-\nwork for forecasting on temporal knowledge graphs.\nIn Conference on Automated Knowledge Base Con-\nstruction.\nAlan G Hawkes. 1971. Point spectra of some mutually\nexciting point processes. Journal of the Royal Statis-\ntical Society: Series B (Methodological), 33(3):438â€“\n443.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020. Heterogeneous graph transformer. In\nWWW â€™20: The Web Conference 2020, pages 2704â€“\n2710.\nWoojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren.\n2020. Recurrent event network: Autoregressive\nstructure inference over temporal knowledge graphs.\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6669â€“6683.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking\npositional encoding in language pre-training. In 9th\nInternational Conference on Learning Representa-\ntions.\nTimothÃ©e Lacroix, Guillaume Obozinski, and Nicolas\nUsunier. 2020. Tensor decompositions for temporal\nknowledge base completion. In International Con-\nference on Learning Representations.\nJulien Leblay and Melisachew Wudage Chekol. 2018.\nDeriving validity time in knowledge graph. In Com-\npanion Proceedings of the The Web Conference ,\npages 1771â€“1776.\nZixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng\nGuo, Yuanzhuo Wang, and Xueqi Cheng. 2021a.\nSearch from history and reason for future: Two-stage\n7489\nreasoning on temporal knowledge graphs. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, pages 4732â€“4743.\nZixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng\nGuo, Huawei Shen, Yuanzhuo Wang, and Xueqi\nCheng. 2021b. Temporal knowledge graph reason-\ning based on evolutional representation learning. In\nSIGIR â€™21: The 44th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, pages 408â€“417.\nTianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng\nQiu. 2021. A survey of transformers.\nHongyuan Mei and Jason Eisner. 2016. The neu-\nral hawkes process: A neurally self-modulating\nmultivariate point process. arXiv preprint\narXiv:1612.09328.\nHeiko Paulheim. 2017. Knowledge graph refinement:\nA survey of approaches and evaluation methods. Se-\nmantic web, 8(3):489â€“508.\nApoorv Saxena, Soumen Chakrabarti, and Partha P.\nTalukdar. 2021. Question answering over temporal\nknowledge graphs. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European Semantic Web Confer-\nence, pages 593â€“607.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn NAACL-HLT, pages 464â€“468.\nOleksandr Shchur, Ali Caner TÃ¼rkmen, Tim\nJanuschowski, and Stephan GÃ¼nnemann. 2021.\nNeural temporal point processes: A review. In\nProceedings of the Thirtieth International Joint\nConference on Artificial Intelligence , pages\n4585â€“4593.\nJosef Stoer and Roland Bulirsch. 2013. Introduction to\nNumerical Analysis, volume 12. Springer Science &\nBusiness Media.\nHaohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and\nKun He. 2021. Timetraveler: Reinforcement learn-\ning for temporal knowledge graph forecasting. In\nEmpirical Methods in Natural Language Processing,\npages 8306â€“8319.\nRakshit Trivedi, Hanjun Dai, Yichen Wang, and\nLe Song. 2017. Know-Evolve: Deep temporal rea-\nsoning for dynamic knowledge graphs. In Proceed-\nings of the 34th International Conference on Machine\nLearning, pages 3462â€“3471.\nRakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal,\nand Hongyuan Zha. 2019. Dyrep: Learning repre-\nsentations over dynamic graphs. In 7th International\nConference on Learning Representations.\nThÃ©o Trouillon, Johannes Welbl, Sebastian Riedel, Ã‰ric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Proceed-\nings of the 33nd International Conference on Ma-\nchine Learning, pages 2071â€“2080.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems, pages 5998â€“6008.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro LiÃ², and Yoshua Bengio.\n2018. Graph attention networks. In 6th International\nConference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings.\nHongwei Wang, Fuzheng Zhang, Jialin Wang, Miao\nZhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018.\nRipplenet: Propagating user preferences on the\nknowledge graph for recommender systems. In Pro-\nceedings of the 27th ACM International Conference\non Information and Knowledge Management, pages\n417â€“426.\nDa Xu, Chuanwei Ruan, Evren KÃ¶rpeoglu, Sushant\nKumar, and Kannan Achan. 2020. Inductive repre-\nsentation learning on temporal graphs. In 8th Inter-\nnational Conference on Learning Representations.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2014. Embedding entities and relations\nfor learning and inference in knowledge bases. arXiv\npreprint arXiv:1412.6575.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and\nTie-Yan Liu. 2021. Do transformers really per-\nform bad for graph representation? arXiv preprint\narXiv:2106.05234.\nQiang Zhang, Aldo Lipani, Omer Kirnap, and Emine\nYilmaz. 2020. Self-attentive hawkes process. In In-\nternational Conference on Machine Learning, pages\n11183â€“11193.\nKe Zhou, Hongyuan Zha, and Le Song. 2013. Learning\nsocial infectivity in sparse low-rank networks using\nmulti-dimensional hawkes processes. In Artificial\nIntelligence and Statistics, pages 641â€“649.\nCunchao Zhu, Muhao Chen, Changjun Fan, Guangquan\nCheng, and Yan Zhang. 2021. Learning from history:\nModeling temporal knowledge graphs with sequen-\ntial copy-generation networks. In Thirty-Fifth AAAI\nConference on Artificial Intelligence , pages 4732â€“\n4740.\n7490\nSimiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and\nHongyuan Zha. 2020. Transformer hawkes process.\nIn International Conference on Machine Learning,\npages 11692â€“11702.\n7491\nA Appendix\nA.1 Dataset Statistics\nTable 3 details the statistics of ICEWS14,\nICEWS18, ICEWS05-15 and GDELT.\nA.2 Supplementary Results\nIn order to illustrate the performance of our model\nmore comprehensively, we conduct experiments\nunder different Î”ğ‘¡. Table 1 reports the experimental\nresults when Î”ğ‘¡ =5. Since the experiments on the\nGDELT dataset are very time-consuming, Table 1\nonly provides the experimental results on the three\nICEWS datasets.\nFigure 2 presents the results of MRR for the\nGDELT dataset as ğ‘‘ğ‘¡ changes. Since the training\nof RE-NET and TITer on the GDELT dataset are\ntoo time-consuming, we use CyGNet instead for\ncomparison. Because the time span of GDELT is\nrelatively small, the change is not obvious.\nA.3 Efficiency Analysis\nWe analyzed the computational complexity of GHT\nfor each module. The computational complexity of\nRGT is ğ‘‚(ğ‘ğ‘€2ğ‘‘), where ğ‘ is the layer number,\nğ‘€is the message number(in-degree of the graph),\nand ğ‘‘is the dimension. For TT, the computational\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013\n/uni00000047/uni00000057\n/uni00000014/uni0000001a\n/uni00000014/uni0000001b\n/uni00000014/uni0000001c\n/uni00000015/uni00000013\n/uni00000015/uni00000014\n/uni00000015/uni00000015/uni00000030/uni00000035/uni00000035\n/uni00000026/uni0000005c/uni0000002a/uni00000031/uni00000048/uni00000057\n/uni00000035/uni00000028/uni00000010/uni0000002a/uni00000026/uni00000031\n/uni0000002a/uni0000002b/uni00000037\nFigure 5: MRR on GDELT dataset with regard to vari-\nous ğ‘‘ğ‘¡.\ncomplexity is ğ‘‚(Ë†ğ‘ğ¿2ğ‘‘), and ğ¿ is the sequence\nlength. Thus, the computational complexity of\nGHT is ğ‘‚(ğ‘ğ‘€2ğ‘‘+ Ë†ğ‘ğ¿2ğ‘‘).\nFigure 6 illustrates the inference time of RE-\nNET, RE-GCN, and GHT under the setting ofÎ”ğ‘¡ =\n5 on ICEWS14. We can see that RE-NET inference\nis the slowest because it processes the query for\neach timestamp separately and can only predict the\nevent that occurs at the next timestamp. RE-GCN\nis also a heuristic model, which only predicts the\nnext timestamp step by step. In contrast, our model\ncan parallelly predict events that occur at multiple\ndifferent timestamps. Therefore, compared with\nRE-NET and RE-GCN, the longer the evolution\nDataset # entity # relation # train # valid # test # timestamp Time granularity\nICEWS14 7128 230 63685 13823 13222 365 24 hours\nICEWS18 23033 256 373018 45995 49545 304 24 hours\nICEWS0515 10488 251 322958 69224 69147 4017 24 hours\nGDELT 7691 240 1734399 238765 305241 2751 15 mins\nTable 3: Statistics of the datasets. The columns include the dataset name, number of entities, number of relationships,\nnumber of quadruples in the training set, number of quadruples in the validation set, number of quadruples in the\ntest set, number of timestamps, and the time granularity.\nModel ICEWS14 ICEWS18 ICEWS0515 GDELT\nMRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10\nTransE 22.48 13.36 25.63 41.2312.24 5.84 12.81 25.1022.55 13.05 25.61 42.05- - - -\nDistmult 27.67 18.16 31.15 46.9610.17 4.52 10.33 21.2528.73 19.33 32.19 47.548.61 3.91 8.27 17.04\nComplEx 30.84 21.51 34.48 49.5921.01 11.87 23.47 39.9731.69 21.44 35.74 52.049.84 5.17 9.58 18.23\nTTransE 13.43 3.11 17.32 34.55 8.31 1.92 8.56 21.89 15.71 5.00 19.72 38.025.50 0.49 4.99 15.18\nTA-DistMult26.47 17.09 30.22 45.4116.75 8.61 18.41 33.5924.31 14.58 27.92 44.2111.17 5.09 11.58 22.65\nDE-SimplE32.67 24.43 35.69 49.1119.30 11.53 21.86 34.8035.02 25.91 38.99 52.75- - - -\nTNTComplEx32.12 23.35 36.03 49.1321.23 13.28 24.02 36.9127.54 19.52 30.80 42.86- - - -\nCyGNet 32.73 23.69 36.31 50.6724.93 15.90 28.28 42.6134.97 25.67 39.09 52.9418.05 11.13 19.11 31.50\nTITer 37.49 28.69 41.88 53.9326.5118.9829.59 40.5940.15 29.55 45.75 60.4915.75 10.94 15.74 25.37\nRE-NET 36.28 27.19 40.16 53.5127.38 17.93 31.04 45.6540.28 30.11 45.54 60.67- - - -\nRE-GCN 36.95 28.04 40.92 54.0728.0118.9731.2945.8441.1931.3246.0860.3319.1112.1920.2532.43\nGHT 38.28\nÂ±0.25\n28.43\nÂ±0.21\n42.85\nÂ±0.18\n57.47\nÂ±0.09\n28.38\nÂ±0.18\n18.78\nÂ±0.15\n32.01\nÂ±0.17\n47.27\nÂ±0.11\n42.90\nÂ±0.20\n31.76\nÂ±0.19\n48.77\nÂ±0.21\n64.64\nÂ±0.17\n20.14\nÂ±0.18\n12.75\nÂ±0.19\n21.50\nÂ±0.10\n34.60\nÂ±0.14\nTable 4: Entity prediction results (Î”ğ‘¡ =5).\n7492\nFigure 6: Inference time of RE-NET, RE-GCN and\nGHT on ICEWS14.\n(a) History length\n (b) # att heads in\nTT\n(c) # att heads in\nRGT\n(d) # layers in TT\n (e) # layers in RGT\nFigure 7: Hyperparameter study on ICEWS14.\nprocess is, the less time our model inference takes\nrelatively.\nA.4 Sensitivity Analysis\nWe also study the hyperparameterâ€™s sensitivity of\nGHT, including the layer number and the attention\nhead number of RGT, the layer number and the\nattention head number of TT, and the history length.\nWe report the MRR results of entity prediction on\nICEWS14 (ğ‘‘ğ‘¡ =1) in Figure 7.\nResults in Table 1 demonstrate that GHT out-\nperforms all static KG reasoning models and TKG\ninterpolated reasoning models because these base-\nlines fail to utilize the time information. For static\nmethods, it is effortless to understand that it has\nno ability to model the time. The interpolated rea-\nsoning methods learn an embedding for each times-\ntamp. However, in our experimental setting, the\ntimestamps of inference are not seen during train-\ning, causing the models to use randomly initialized\ntimestamp embeddings.\nRGT Analysis The layer number of RGT corre-\nsponds to the aggregated neighbor order. In Figure\n7e, the model performs best when the layer number\nis 2, aggregating more structural information than\n1-layer. However, the modelâ€™s performance will\ndeteriorate as the number of layers increases. This\nmay be because, in ICEWS, neighbors above the\nthird order do not provide more information but\ninstead introduce noise. Figure 7c shows that the\nnumber of attention heads has a small impact.\nTT Analysis Figure 7b and 7d show the perfor-\nmance of GHT with different number of TT layers\nand attention heads. We notice that 2-layer TT and\n2-head TT perform better than others. Compared\nwith the number of RGT layers, the model is less\nsensitive to the number of TTâ€™s layers.\nHistory Length Analysis GHT needs to model\nthe historical sequence information, and we fix a\nhyperparameter to limit the maximum sequence\nlength. Figure 7a shows the performance with dif-\nferent history length. If the length is no greater\nthan 6, the longer the history, the better the GHT\nperformance. However, continuing to extend the\nsequence will make the MRR fall. It shows that the\neffective information density is inversely propor-\ntional to the history length.\n7493",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7376822233200073
    },
    {
      "name": "Forgetting",
      "score": 0.6345379948616028
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5176470875740051
    },
    {
      "name": "ENCODE",
      "score": 0.4954918622970581
    },
    {
      "name": "Graph",
      "score": 0.48761263489723206
    },
    {
      "name": "Knowledge graph",
      "score": 0.44936856627464294
    },
    {
      "name": "Transformer",
      "score": 0.4423038363456726
    },
    {
      "name": "Encoding (memory)",
      "score": 0.43247321248054504
    },
    {
      "name": "Machine learning",
      "score": 0.3953888416290283
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33912643790245056
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 40
}