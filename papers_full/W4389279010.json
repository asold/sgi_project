{
    "title": "Can Large Language Models Provide Security &amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions",
    "url": "https://openalex.org/W4389279010",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2108550554",
            "name": "Yufan Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093013828",
            "name": "Arjun Arunasalam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2250297608",
            "name": "Z. Berkay Celik",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385187279",
        "https://openalex.org/W4225108562",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4299301436",
        "https://openalex.org/W4210320858",
        "https://openalex.org/W4298112463",
        "https://openalex.org/W2797893620"
    ],
    "abstract": "Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored.",
    "full_text": null
}