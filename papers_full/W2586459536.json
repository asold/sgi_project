{
  "title": "Language Models with Pre-Trained (GloVe) Word Embeddings",
  "url": "https://openalex.org/W2586459536",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Makarenkov, Victor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742549429",
      "name": "Shapira, Bracha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742716361",
      "name": "Rokach, Lior",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2008467960",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1591801644"
  ],
  "abstract": "In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].",
  "full_text": "Language Models with Pre-Trained (GloVe)\nWord Embeddings\nVictor Makarenkov, Lior Rokach, Bracha Shapira\nmakarenk@post.bgu.ac.il, liorrk@bgu.ac.il, bshapira@bgu.ac.il\nDepartment of Software and Information Systems Engineering\nBen-Gurion University of the Negev\n1 Introduction\nIn this work we present a step-by-step implementation of training a Language\nModel (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe\nword embeddings, introduced by Pennigton et al. in [1]. The implementation is\nfollowing the general idea of training RNNs for LM tasks presented in [2] , but\nis rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the\nmore commonly used LSTM [4]. The implementation presented is based on using\nkeras1 [5].\n2 Motivation\nLanguage Modeling is an important task in many Natural Language Processing\n(NLP) applications. These application include clustering, information retrieval,\nmachine translation, spelling and grammatical errors correction. In general, a\nlanguage model deﬁned as a function that puts a probability measure over strings\ndrawn from some vocabulary. In this work we consider a RNN based language\nmodel task, which aims at predicting the nextn-th word in a sequence, given\nthe previousn−1 words. Put otherwise, ﬁnding the word with maximum value\nfor P(wn|w1,...,w n−1) . Thenparameter is theContextWindowSize argument\nin the algorithm described further.\nTo maximize the eﬀectiveness and performance of the model we use word em-\nbeddings into a continuous vector space. The model of embedding we use is\nthe GloVe [1] model, with dimensionality size equal to 300 or 50. We use both\npre-trained2 on 42 billion tokens and 1.9 million vocabulary size, and speciﬁ-\ncally trained for this work vector models, which we trained on SIGIR-2015 and\nICTIR-2015 conferences’ proceedings.\nThemodelitselfistrainedasaRNN,withinternalGRUformemorizingtheprior\nsequence of words. It was shown lately, that RNNs outperform most language\nmodeling based tasks [2, 3] when tuned and trained correctly.\n1 https://keras.io/\n2 downloaded from: http://nlp.stanford.edu/projects/glove/\narXiv:1610.03759v2  [cs.CL]  5 Feb 2017\n3 Short Description\nIn this work we use 300-dimensional and 50-dimensional, GloVe word embed-\ndings. In order to embed the words in a vector space, GloVe model is trained\nby examining word co-occurrence matrixXij within a huge text corpus. Despite\nthe huge size of the Common Crawl corpus, some words may not exist with the\nembeddings, so we set these words to random vectors, and use the same embed-\ndings consistently if we encounter the same unseen word again in the text. The\nRNN is further trained to predict the next word in its embedding form, that is,\npredicts the next n-dimensional vector, given theContextWindowSize previ-\nous words. We divide theTextFile into 70% and 30% for training and testing\npurposes.\n4 Pseudo Code\ninput : Input: glove-vectors : Pre-Trained-Word-Embeddings, Text-File,\nContextWindowSize=10, hidden-unites=300\noutput: A Language Model trained on Text-File with word-embeddings\nrepresentation\nfor w∈Text-File do\nif w∈OOV-ﬁle then\ntokenized-ﬁle.append(OOV-ﬁle.get-vector(w))\nend\nif w∈glove-vectors then\ntokenized-ﬁle.append(glove-vectors.get-vector(w))\nend\nelse\nvector ←random-vector()\ntokenized-ﬁle.append(vector)\nOOV-ﬁle.append(vector)\nend\nend\nNN ←CreateSingleHiddenLayerDenseRNN(unit=GRU, inputs=300,\noutputs=300, hidden-unites)\nNN.setDropout(0.8)\nNN.setActivationFunction(Linear)\nNN.setLossFunction(MSE)\nNN.setOptimizer(rmsprop)\nXtrain ←tokenized-ﬁle.getInterval(0.0,0.7)\nXtest ←tokenized-ﬁle.getInterval(0.7,1.0)\nYtrain ←Xtrain.Shift(ContextWindowSize )\nYtest ←Xtest.Shift(ContextWindowSize )\nNN.Fit(Xtrain,Ytrain)\nNN.Predict(Xtest,Ytest)\nAlgorithm 1:Training a language model on word embeddings\n5 Detailed Explanation\nAs stated earlier, GloVe model is trained by examining word co-occurrence\nmatrix of two words i and j: Xij within a huge text corpus. While training\nthe main idea is stating that wiTwj + bi + bj = log(Xij) where wi and wj\nare the trained vectors, bi and bj are the scalar bias terms associated with\nwords i and j. The most important parts of the training process in GloVe\nare: 1) A weighting function f for elimination of very common words (like\nstop words) which add noise and not overweighted, 2) rare words are not over-\nweighted 3) the co-occurrence strength, when modeled as a distance, should be\nsmoothed with alog function. Thus, the ﬁnal loss function for a GloVe model is\nJ = Σi,j∈Vf(Xij)(wiTwj+bi+bj−log(Xij))2 where V is a complete vocabulary,\nand f(x) = (x/xmax)α if x<x max, andf(x) = 1otherwise. The model that is\nused in this work was trained withxmax = 100 and α= 0.75.\nFig. 1.A general architecture of training an unfolded RNN with 1-sized window based\nshifted labeled data\nTraining of the RNN is somewhat blurred between supervised and unsuper-\nvised techniques. That is, no extra labeled data is given, but part of the input is\nused as labels. In thisunfolded training paradigm, which is illustrated on Figure\n1, the output isshifted in a way to create a labels for the input train dataset.\nIn this way the RNN can actually learn to predict a next word (vector) in a\nsequence.\n6 Evaluation\n6.1 Pre-trained Vector Models\nIn order to evaluate our implementation of the language model, we train several\ndiﬀerent language models and compare the predicted error distribution with a\nrandom word prediction. The error is measured with a cosine distance3 between\ntwo vectors:1 − xxx·yyy\n||xxx||·||yyy||. On ﬁgure 2 we see the error distribution of the RNN\nwith 30 hidden units. The training was performed on 5000 tokens long text ﬁle,\nthe ﬁrst entry at English wikipedia,Anarchism.\nFig. 2.Two distributions of the predicted next word vector errors. The left is the\nresult of predicted by RNN errors, and the righ is predicted by random. The RNN in\nthe model was trained with 30 hidden GRU units. It took 300 iterations (epochs) on\nthe data to achieve these results.\nThe machine that was used to run the evaluation had the following charac-\nteristics: 1.7 GHz, Core i7 with 8 GB memory, OS X version 10.11.13.\nThe time it took to train the model with 30 epochs was 125 seconds . The\ntime took to make the predictions on a test set is 0.5 seconds.\nOn ﬁgure 3 we see the error distribution of the RNN with 300 hidden units\nThe time it took to train the model with300 epochs was 1298 seconds . The\ntime took to make the predictions on a test set is 0.49 seconds.\n6.2 Self Trained Vector Model\nIn addition, in order to further evaluate the current approach, we speciﬁcally\ntrainedanarrowdomain-speciﬁc,vectormodel.WeusedICTIR-2015andSIGIR-\n2015 conferences proceedings as a corpora, and produced 50-dimensional vectors.\nThe vector modelis based on 1,500,000 tokens total, and resulted in 17,000 long\nvocabulary. Thelanguage model for the evaluation was built on a paper pub-\nlished in the ICTIR-2015 proceedings [7]. Consider ﬁgure 4. The predicted words’\nerrors distribution diﬀers even more than in the general case, where the vectors\nwere trained on the general Common-Crawl corpora. That is, the performance\nof the language model, for the task of word prediction is higher.\nThe time took to train this model is 10 seconds. The time took to compute\nthe predictions for evaluations is 0.04 seconds.\n3 implemented on python, with scipy package\nFig. 3.Two distributions of the predicted next word vector errors. The left is the\nresult of predicted by RNN errors, and the righ is predicted by random. The RNN in\nthe model was trained with 300 hidden GRU units. It took 300 iterations (epochs) on\nthe data to achieve these results.\n7 Instructions for running the code\nThe implementation of the model training was written in this work in the Python\nlanguage, version 2.7. The library that was used is Keras, which in the course\nof this implementation was based onTheano framework. Instead of Theano, the\nGoogle’sTensorﬂow can be also used behind the scenes of the Keras in this\nimplementation. In order to train the model yourself, you need to follow the\nnext steps:\n1. Downloadpre-trainedGloVevectorsfrom http://nlp.stanford.edu/projects/glove/\n2. Obtain a text to train the model on. In our example we use a Wikipdia\nAnarchism entry.\n3. Open and adjust the LM_RNN_GloVe.py ﬁle parameters inside the main\nfunction:\n(a) ﬁle_2_tokenize_name(example=\"/Users/macbook/corpora/text2tokenize.txt\")\n(b) tokenized_ﬁle_name(example=\"/Users/macbook/corpora/tokenized2vectors.txt\")\n(c) glove_vectors_ﬁle_name(example=\"/Users/macbook/corpora/glove.42B.300d.txt\")\n(d) extra_vocab_ﬁlename(example=\"/Users/macbook/corpora/extra_vocab.txt\").\nThis argument has also a default value in theget_vector function\n4. Run the following methods:\n(a) tokenize_ﬁle_to_vectors(glove_vectors_ﬁle_name,ﬁle_2_tokenize_name,\ntokenized_ﬁle_name)\n(b) run_experiment(tokenized_ﬁle_name)\n8 Discussion\nIn this work we implemented and tested the training of a LM based on RNN.\nTo emphasize the strength of such an approach, we have chosen one of the most\nFig. 4.Two distributions of the predicted next word vector errors. The left is the\nresult of predicted by RNN errors, and the righ is predicted by random. The RNN in\nthe model was trained with 50 hidden GRU units. It took 50 iterations (epochs) on\nthe data to achieve these results.\npowerful and prominent techniques for word embeddings - the GloVe embed-\ndings. Although there are other approaches, such as the popularword2vec [8]\ntechnique, the GloVe embeddings was shown to outperform it on several tasks\n[1], partially because of the reasons described in section 5. By training the model\nwithtwodiﬀerentsettings,oneofwhichisorderofmagnitudemorecomplexthan\nthe other we show the power of such LM. The distributions shown on ﬁgures 2\nand 3 clearly indicate much smaller error on the task of next word prediction.\nThe main limitation of this implementation is the ﬁxed window size, of the\npreﬁx in the LM. This approach does not fully show the full power of RNN-\nbased LM. For dynamic size preﬁx LM please consider the DyNet [6] package for\nexample. DyNet supports a dynamic computation graph and shares the learned\nparameters across multiple variable length instances during the training.\n9 The source at GitHub\nThe code was submitted publicly to the GitHub repository of the author, and is\navailable undervicmak username, proofseer project4.\n4 https://github.com/vicmak/ProofSeer\nReferences\n[1] Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word\nrepresentation. In: Empirical Methods in Natural Language Processing\n(EMNLP). (2014) 1532–1543\n[2] Zaremba, W., Sutskever, I., Vinyals, O.: Recurrent neural network regular-\nization. CoRR abs/1409.2329 (2014)\n[3] Cho, K., van Merrienboer, B., Gülçehre, Ç., Bougares, F., Schwenk, H., Ben-\ngio, Y.: Learning phrase representations using RNN encoder-decoder for\nstatistical machine translation. CoRRabs/1406.1078 (2014)\n[4] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput.\n9 (1997) 1735–1780\n[5] FranÃ§ois Chollet : keras GitHub repository. 2015.\n[6] Graham Neubig and Chris Dyer and Yoav Goldberg and Austin Matthews\nand Waleed Ammar and Antonios Anastasopoulos and Miguel Ballesteros\nand David Chiang and Daniel Clothiaux and Trevor Cohn and Kevin Duh\nandManaalFaruquiandCynthiaGanandDanGarretteandYangfengJiand\nLingpeng Kong and Adhiguna Kuncoro and Gaurav Kumar and Chaitanya\nMalaviya and Paul Michel and Yusuke Oda and Matthew Richardson and\nNaomi Saphra and Swabha Swayamdipta and Pengcheng Yin: DyNet: The\nDynamic Neural Network Toolkit arXiv preprint arXiv:1701.03980. 2017.\n[7] Makarenkov, V., Shapira, B., Rokach, L.: Theoretical categorization of query\nperformance predictors. In: Proceedings of the 2015 International Conference\non The Theory of Information Retrieval. ICTIR ’15, New York, NY, USA,\nACM (2015) 369–372\n[8] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word\nrepresentations in vector space. CoRRabs/1301.3781 (2013)",
  "topic": "Word (group theory)",
  "concepts": [
    {
      "name": "Word (group theory)",
      "score": 0.7361643314361572
    },
    {
      "name": "Computer science",
      "score": 0.6090110540390015
    },
    {
      "name": "Natural language processing",
      "score": 0.49487775564193726
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37818971276283264
    },
    {
      "name": "Speech recognition",
      "score": 0.3451341986656189
    },
    {
      "name": "Linguistics",
      "score": 0.268006831407547
    },
    {
      "name": "Philosophy",
      "score": 0.04241189360618591
    }
  ],
  "institutions": []
}