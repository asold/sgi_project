{
    "title": "Large language models provide discordant information compared to ophthalmology guidelines",
    "url": "https://openalex.org/W4411878633",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2254448405",
            "name": "Andrea Taloni",
            "affiliations": [
                "Scuola Superiore Internazionale di Studi Universitari di Ricerca e Formazione",
                "Gruppo Italiano per lo Studio dei Carcinomi dell’Apparato Digerente",
                "University of Ferrara",
                "Magna Graecia University"
            ]
        },
        {
            "id": "https://openalex.org/A5118751069",
            "name": "Antonia Carmen Sangregorio",
            "affiliations": [
                "Magna Graecia University"
            ]
        },
        {
            "id": "https://openalex.org/A5099487406",
            "name": "Giuseppe Alessio",
            "affiliations": [
                "Magna Graecia University"
            ]
        },
        {
            "id": "https://openalex.org/A3157891347",
            "name": "Maria Angela Romeo",
            "affiliations": [
                "Magna Graecia University"
            ]
        },
        {
            "id": "https://openalex.org/A2546302630",
            "name": "Giulia Coco",
            "affiliations": [
                "University of Rome Tor Vergata"
            ]
        },
        {
            "id": "https://openalex.org/A5116854554",
            "name": "Linda Marie Louise Busin",
            "affiliations": [
                "Fatebenefratelli Hospital",
                "Gruppo Italiano per lo Studio dei Carcinomi dell’Apparato Digerente"
            ]
        },
        {
            "id": "https://openalex.org/A2952927840",
            "name": "Andrea Sollazzo",
            "affiliations": [
                "Gruppo Italiano per lo Studio dei Carcinomi dell’Apparato Digerente",
                "University of Ferrara",
                "Scuola Superiore Internazionale di Studi Universitari di Ricerca e Formazione"
            ]
        },
        {
            "id": "https://openalex.org/A188511083",
            "name": "Vincenzo Scorcia",
            "affiliations": [
                "Magna Graecia University"
            ]
        },
        {
            "id": "https://openalex.org/A2063938695",
            "name": "Giuseppe Giannaccare",
            "affiliations": [
                "University of Cagliari"
            ]
        },
        {
            "id": "https://openalex.org/A2254448405",
            "name": "Andrea Taloni",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5118751069",
            "name": "Antonia Carmen Sangregorio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5099487406",
            "name": "Giuseppe Alessio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3157891347",
            "name": "Maria Angela Romeo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2546302630",
            "name": "Giulia Coco",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5116854554",
            "name": "Linda Marie Louise Busin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2952927840",
            "name": "Andrea Sollazzo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A188511083",
            "name": "Vincenzo Scorcia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2063938695",
            "name": "Giuseppe Giannaccare",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4388007786",
        "https://openalex.org/W4390985458",
        "https://openalex.org/W4389305650",
        "https://openalex.org/W4402567360",
        "https://openalex.org/W4404442740",
        "https://openalex.org/W4393093628",
        "https://openalex.org/W4386046428",
        "https://openalex.org/W4401575877",
        "https://openalex.org/W4321351832",
        "https://openalex.org/W4385658354",
        "https://openalex.org/W4392710953",
        "https://openalex.org/W4383500999",
        "https://openalex.org/W4367834585",
        "https://openalex.org/W4379094566",
        "https://openalex.org/W4401852727",
        "https://openalex.org/W4392034542",
        "https://openalex.org/W4405921096",
        "https://openalex.org/W4405365363",
        "https://openalex.org/W4385774833",
        "https://openalex.org/W4400775727",
        "https://openalex.org/W4400727220",
        "https://openalex.org/W4401735946",
        "https://openalex.org/W4405321560"
    ],
    "abstract": "To evaluate the agreement of LLMs with the Preferred Practice Patterns<sup>®</sup> (PPP) guidelines developed by the American Academy of Ophthalmology (AAO). Open questions based on the AAO PPP were submitted to five LLMs: GPT-o1 and GPT-4o by OpenAI, Claude 3.5 Sonnet by Anthropic, Gemini 1.5 Pro by Google, and DeepSeek-R1-Lite-Preview. Questions were classified as \"open\" or \"confirmatory with positive/negative ground-truth answer\". Three blinded investigators classified responses as \"concordant\", \"undetermined\", or \"discordant\" compared to the AAO PPP. Undetermined and discordant answers were analyzed to assess harming potential for patients. Responses referencing peer-reviewed articles were reported. In total, 147 questions were submitted to the LLMs. Concordant answers were 135 (91.8%) for GPT-o1, 133 (90.5%) for GPT-4o, 136 (92.5%) for Claude 3.5 Sonnet, 124 (84.4%) for Gemini 1.5 Pro, and 119 (81.0%) for DeepSeek-R1-Lite-Preview (P = 0.006). The highest number of harmful answers was reported for Gemini 1.5 Pro (n = 6, 4.1%), followed by DeepSeek-R1-Lite-Preview (n = 5, 3.4%). Gemini 1.5 Pro was the most transparent model (86 references, 58.5%). Other LLMs referenced papers in 9.5-15.6% of their responses. LLMs can provide discordant answers compared to ophthalmology guidelines, potentially harming patients by delaying diagnosis or recommending suboptimal treatments.",
    "full_text": "Large language models provide \ndiscordant information compared \nto ophthalmology guidelines\nAndrea Taloni1,2,3,4, Antonia Carmen Sangregorio4, Giuseppe Alessio4, Maria Angela Romeo4, \nGiulia Coco5, Linda Marie Louise Busin2,6, Andrea Sollazzo1,2,3, Vincenzo Scorcia4 & \nGiuseppe Giannaccare 7\nTo evaluate the agreement of LLMs with the Preferred Practice Patterns® (PPP) guidelines developed \nby the American Academy of Ophthalmology (AAO). Open questions based on the AAO PPP were \nsubmitted to five LLMs: GPT-o1 and GPT-4o by OpenAI, Claude 3.5 Sonnet by Anthropic, Gemini 1.5 \nPro by Google, and DeepSeek-R1-Lite-Preview. Questions were classified as “open” or “confirmatory \nwith positive/negative ground-truth answer”. Three blinded investigators classified responses as \n“concordant”, “undetermined”, or “discordant” compared to the AAO PPP . Undetermined and \ndiscordant answers were analyzed to assess harming potential for patients. Responses referencing \npeer-reviewed articles were reported. In total, 147 questions were submitted to the LLMs. Concordant \nanswers were 135 (91.8%) for GPT-o1, 133 (90.5%) for GPT-4o, 136 (92.5%) for Claude 3.5 Sonnet, 124 \n(84.4%) for Gemini 1.5 Pro, and 119 (81.0%) for DeepSeek-R1-Lite-Preview (P = 0.006). The highest \nnumber of harmful answers was reported for Gemini 1.5 Pro (n = 6, 4.1%), followed by DeepSeek-R1-\nLite-Preview (n = 5, 3.4%). Gemini 1.5 Pro was the most transparent model (86 references, 58.5%). \nOther LLMs referenced papers in 9.5–15.6% of their responses. LLMs can provide discordant answers \ncompared to ophthalmology guidelines, potentially harming patients by delaying diagnosis or \nrecommending suboptimal treatments.\nKeywords Large language model, Artificial intelligence, Guidelines, Preferred practice patterns, American \nAcademy of Ophthalmology, AAO\nLarge language models (LLMs) are dramatically changing the current scenarios of education and productivity, \nand the application of generative artificial intelligence (AI) in the medical field aroused significant interest in the \nscientific community. Several authors have investigated the proficiency of LLMs in answering ophthalmology \nquestions, both from patients and examination boards, reporting encouraging results1–8. However, the reliability \nof LLM use in clinical practice is still a controversial topic. The main offenders are safety, transparency, and \naccountability concerns. Despite rapid technological advancements, LLMs can still provide inaccurate, wrong \nor non-sensical answers, often referred to as “hallucinations” 9. In addition, chatbots do not always provide \nreferences for their outputs, because answers are based on mixed information from training data sets, which \ncan include unreliable sources10. Even if references are specifically requested by the user, they can be inaccurate \nor completely made up 11,12. Finally, medical practitioners have no control on the way patients interact with \ngenerative AI. In case of discrepancies between physician and LLM, doctor-patients relationship may become \nchallenging.\nGiven the growing influence of LLMs on patient care and clinical practice, it is essential that these tools \nundergo rigorous, systematic evaluations to determine their alignment with established medical guidelines \nand good practice standards. The purpose of this study is to evaluate the agreement of publicly available LLMs \nwith the Preferred Practice Patterns® (PPP) guidelines developed by the American Academy of Ophthalmology \n(AAO) and to identify cases in which AI outputs may be harmful to patients13.\n1Department of Translational Medicine, University of Ferrara, Ferrara, Italy. 2Department of Ophthalmology, \nOspedali Privati Forlì “Villa Igea”, Forlì, Italy. 3Istituto Internazionale di Ricerca e Formazione in Oftalmologia, \nForlì, Italy. 4Department of Ophthalmology, University Magna Graecia of Catanzaro, Catanzaro, Italy. 5Department \nof Clinical Sciences and Translational Medicine, University of Rome Tor Vergata, Rome, Italy. 6Department of \nOphthalmology, Fatebenefratelli Hospital, Milan, Italy. 7Department of Surgical Sciences, Eye Clinic, University of \nCagliari, Via Università 40, 09124 Cagliari, Italy. email: giuseppe.giannaccare@unica.it\nOPEN\nScientific Reports |        (2025) 15:20556 1| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports\n\nMethods\nIn this study, five publicly available LLMs were selected to investigate their agreement with the AAO PPP: \nGPT-o1 and GPT-4o by OpenAI (San Francisco, CA, USA), Claude 3.5 Sonnet by Anthropic (San Francisco, \nCA, USA), Gemini 1.5 Pro by Google (Mountain View, CA, USA), and DeepSeek-R1-Lite-Preview (DeepSeek, \nHangzhou, Zhejiang, China). Open-source LLMs which require local deployment were excluded from the study \nbecause they necessitate substantial computational resources, coding expertise, and technical customization, \nwhich would have limited the generalizability of our findings.\nOpen questions based on the “Highlighted findings and recommendations for care” summaries in each \nAAO PPP document were prepared by an investigator (A.T.) and reviewed by a Fellow of the European Board \nof Ophthalmology (G.G.). For each finding reported in the guidelines, one question was crafted. Questions \nwere classified as “open” or “confirmatory with positive ground truth answer” , and “confirmatory with negative \nground-truth answer” . Prompts were submitted to the LLMs in separate chat sessions, to prevent any potential \ninfluence of previous queries on subsequent ones. All questions were submitted between December 27th, 2024 \nand January 2nd, 2025.\nThree blinded investigators (Giulia Coco, MD, PhD, researcher, and corneal surgeon consultant; Andrea \nSollazzo, MD, researcher, and Andrea Taloni, MD and PhD student) collaboratively reviewed responses to \nclassify them as “concordant” , “undetermined” , or “discordant” with respect to the AAO PPP documents. Any \ndisagreement between investigators was resolved through discussion until a consensus was reached. Simple \nrandomization was used to ensure variation in the order of exposure of answers by the five LLM models. \nAgreement was considered to be achieved if the chatbot’s response exactly matched the AAO recommendations \nor provided conclusions that aligned with them. Answers were defined as undetermined if they failed to mention \nspecific aspects of the findings reported in the AAO PPP , if they were too vague, or presented minor numerical \ndiscrepancies for quantitative measurements (e.g. slightly different percentages for the incidence of a certain \ndisease). Finally, discordant responses presented opposite statements compared to the AAO guidelines or were \nseverely lacking in addressing key aspects of the topic. An example of questions with concordant, undetermined, \nand discordant answers is reported in Table 1.\nFurthermore, investigators collaboratively analyzed undetermined and discordant answers to assess whether \ntheir content had the potential to harm patients. This evaluation focused on identifying unclear, misleading \nor incorrect information that could lead to inappropriate clinical decisions, delayed diagnosis, or suboptimal \ntreatment protocols. Examples of potentially harmful answers have been included in the Supplementary \nMaterials.\nFinally, instances in which LLM responses referenced peer-reviewed articles were recorded, while citations to \nnon-peer-reviewed sources, as well as any tangential or hallucinated references, were excluded from the analysis.\nStatistical analysis\nThe Chi-square test of independence or Fisher’s exact test were performed to compare categorical variables \nas appropriate. The word count for answers by LLMs was calculated. The Shapiro–Wilk test was performed \nto determine the normality of data. Wilcoxon-Mann-Whitney U test for non-parametric variables was used \nto compare word count between LLMs. Values for continuous variables were expressed as mean ± standard \ndeviation (95% confidence intervals [CI]). Bonferroni adjustment was applied to control for multiple \ncomparisons. A P value < 0.01 (0.05/5) was considered statistically significant when comparing the outcomes of \nthe 5 different LLMs, while a P value < 0.016 (0.05/3) was considered statistically significant when comparing \noutcomes between concordant, undetermined, and discordant answers. All tests were two-sided. All answers \nwere recorded in Microsoft Excel (Microsoft Corp., Redmond, W A, USA). Statistical analysis was performed \nusing GraphPad Prism (version 10.3.1; GraphPad Software, Boston, MA, USA).\nResults\nIn total, 147 questions across the 25 AAO PPP documents were submitted to the selected LLMs. Table 2 includes \nresults about answers agreement, harming potential, and inclusion of references. The number of answers classified \nas concordant with the AAO PPP was significantly different across the five LLMs (P = 0.006). GPT-o1, GPT-4o, \nand Claude 3.5 Sonnet showed very similar performance, respectively scoring 135 (91.8%), 133 (90.5%), and \n136 (92.5%) concordant answers (P = 0.81); Gemini 1.5 Pro scored 124 concordant answers (84.4%), while the \nworst performing model was DeepSeek-R1-Lite-Preview, with 119 concordant answers (81.0%). GPT-o1 was \nthe LLM with the least number of discordant answers ( n = 3; 2.0%), while the highest amount was recorded \nfor DeepSeek-R1-Lite-Preview ( n = 10; 6.8%). Figure  1 shows the number of concordant, undetermined, and \ndiscordant answers for each LLM.\nSome of the undetermined and discordant answers were deemed to be potentially harmful to patients. The \nhighest number of harmful answers was reported for Gemini 1.5 Pro, with 6 instances (4.1%), followed by \nDeepSeek-R1-Lite-Preview, with 5 instances (3.4%). The other models were safer, with 3 harmful answers (2.0%) \nfor Claude 3.5 Sonnet, and 2 (1.4%) from both GPT-o1 and GPT-4o (Fig.  2). Concerning references, Gemini \n1.5 Pro was the most transparent model, with 86 answers citing peer-reviewed articles (58.5%). Surprisingly, \nincluding references did not improve agreement with the AAO PPP guidelines (84.88% concordant answers with \nreferences vs. 83.61% concordant answers without references; P = 0.82). Other models provided significantly \nfewer references ( P < 0.001). GPT-o1 cited studies in 23 answers (15.6%), DeepSeek-R1-Lite-Preview in 15 \n(10.2%), while both GPT-4o and Claude 3.5 Sonnet in 14 (9.5%) (Fig. 3). Similarly to Gemini 1.5 Pro, references \ndid not correlate with significantly higher numbers of concordant responses, in any LLM.\nOverall, 112 questions required an open answer, while the remaining 35 were confirmatory by design (14 with \nan expected positive answer and 21 with a negative one). Three LLMs had better performance in confirmatory \nScientific Reports |        (2025) 15:20556 2| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nQuestion Is aspirin associated with an increased risk of age-related macular degeneration?\nAAO PPP A meta-analysis of 10 studies found that the use of aspirin was not associated with an increased risk of AMD. Therefore, patients \nwho have been instructed by a physician to use aspirin should continue to use it as prescribed\nAdherence \nto AAO \nPPP\nGPT o1\nNo definitive causal link has been established between aspirin use and an increased risk of age-related macular degeneration \n(AMD). While some observational studies have raised questions about a possible association, the overall body of evidence and large, \ncontrolled studies have not confirmed that aspirin use leads to a higher incidence of AMD or its progression.\nKey Points:\n1. Conflicting Observational Findings: A few observational studies suggested a slight association between aspirin use and AMD. \nHowever, these findings have not been consistently replicated and may be influenced by factors like age, general health status, or \nconcurrent conditions for which aspirin is prescribed.\n2. Lack of Conclusive Evidence: Larger studies and meta-analyses have not found a definitive, dose-dependent, or temporal \nrelationship that would indicate aspirin use directly contributes to AMD. The discrepancies across various studies highlight the \ncomplexity of determining a true causal relationship.\n3. Clinical Recommendations: Aspirin is often used for cardiovascular protection. Given the lack of strong evidence linking aspirin \nto increased AMD risk, most eye care professionals do not recommend stopping aspirin therapy for fear of AMD progression. \nDecisions about aspirin use should continue to be based on cardiovascular risk assessments rather than concerns about AMD.\nConclusion: Current evidence does not support changing aspirin use solely on the basis of potential AMD risk. Patients should \nfollow medical advice for aspirin therapy based on their individual health profiles rather than concerns about AMD.\nC\nGPT4o\nY es, some studies have suggested that regular aspirin use may be associated with an increased risk of age-related macular \ndegeneration (AMD), particularly the “wet” form of the condition. AMD is a leading cause of vision loss in older adults, and it \ncomes in two forms: dry AMD and wet AMD. The “wet” form is more severe and involves the growth of abnormal blood vessels in \nthe retina. Here are the key points from research on the relationship between aspirin use and AMD:\n1. Wet AMD Risk: A few studies, including a well-known study from the European Eye Study and the Beaver Dam Eye Study, found \nthat regular aspirin users, especially those using it long-term, were at a higher risk of developing wet AMD compared to non-users. \nFor example, one study found that individuals who took aspirin daily for over 10 years had a slightly higher incidence of wet AMD.\n2. Dry AMD Risk: The evidence connecting aspirin use to dry AMD is less clear. Some studies haven’t found a significant association \nbetween aspirin and an increased risk of the dry form.\n3. Conflicting Evidence: Despite these findings, other research has shown no strong connection between aspirin use and AMD. This \nhas led to some debate in the medical community about the exact nature of this potential risk.\n4. Potential Mechanisms: It’s not entirely clear how aspirin might increase the risk of wet AMD. Aspirin is an anti-inflammatory \nand blood-thinning medication, and some researchers have speculated that its effect on blood vessels could contribute to the \ndevelopment of abnormal vessels in the retina.\n5. Weighing the Risks: It’s important to note that aspirin is often prescribed to reduce the risk of heart attack and stroke. For people \nwith cardiovascular conditions, the benefits of aspirin may outweigh the risks related to AMD. \nGiven the mixed results from different studies, it’s crucial for individuals, especially those with a higher risk of AMD, to consult with \ntheir doctors before making any changes to aspirin use based on concerns about eye health.\nU\nClaude 3.5-Sonnet\nBased on several epidemiological studies, there is evidence suggesting that regular aspirin use may be associated with an increased \nrisk of age-related macular degeneration (AMD), particularly the neovascular (wet) form. The strongest evidence comes from:\n1. The Beaver Dam Eye Study found that regular aspirin use (at least twice weekly for more than 3 months) was associated with \nincreased risk of developing neovascular AMD over a 10-year period.\n2. The European Eye Study showed a nearly doubled risk of neovascular AMD among regular aspirin users.\n3. The Blue Mountains Eye Study demonstrated an independent association between regular aspirin use and incident neovascular \nAMD over a 15-year period.\nHowever, it’s important to note that:\n- The absolute risk increase appears to be relatively small\n- The benefits of aspirin for cardiovascular prevention may outweigh the potential increased AMD risk for many patients\n- The association doesn’t definitively prove causation\n- Patients shouldn’t stop prescribed aspirin without consulting their healthcare provider\nFor patients with cardiovascular conditions requiring aspirin therapy, the established benefits generally outweigh the potential small \nincrease in AMD risk. Individual risk-benefit discussions with healthcare providers are recommended. Since you’re asking about \nresearch findings, I should mention that while I aim to accurately represent the scientific literature, you should verify these specific \nstudies with current medical sources, as I may not have perfect recall of the exact details.\nD\nContinued\nScientific Reports |        (2025) 15:20556 3| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nAnswers GPT-o1 GPT-4o Claude 3.5 Sonnet Gemini 1.5 Pro DeepSeek R1-Lite-Preview P value\nConcordant\nTotal 135 (91.8%) 133 (90.5%) 136 (92.5%) 124 (84.4%) 119 (81%) 0.006*\nR+ 23 (17%) 13 (9.8%) 10 (7.4%) 73 (58.9%) 13 (10.9%) < 0.001*\nUndetermined\nTotal 9 (6.1%) 8 (5.4%) 4 (2.7%) 16 (10.9%) 18 (12.2%) 0.096\nR+ 0 (0%) 1 (0.8%) 1 (0.7%) 10 (8.1%) 2 (1.7%) 0.002*\nH+ 2 (22.2%) 0 (0%) 1 (25.0%) 3 (18.8%) 0 (0%) 0.102\nDiscordant\nTotal 3 (2%) 6 (4.1%) 7 (4.8%) 7 (4.8%) 10 (6.8%) 0.406\nR+ 0 (0%) 0 (0%) 3 (42.9%) 3 (42.9%) 0 (0%) 0.028\nH+ 1 (33.3%) 2 (33.3%) 2 (28.6%) 3 (42.9%) 5 (50%) 0.932\nAll\nR+ 23 (15.6%) 14 (9.5%) 14 (9.5%) 86 (58.5%) 15 (10.2%) < 0.001*\nH+ 3 (2.0%) 2 (1.4%) 3 (2.0%) 6 (4.1%) 5 (3.4%) 0.471\nTable 2. Number of concordant, undetermined, and discordant answers compared to the American academy \nof ophthalmology preferred practice patterns. Each category includes the number of references (“R+”) and \npotentially harmful answers (“H+”). The P values indicate the statistical significance of differences across all \nfive llms, calculated using the Chi-square test of independence or fisher’s exact test, as appropriate. * A P value \n< 0.016 was considered statistically significant.\n \nQuestion Is aspirin associated with an increased risk of age-related macular degeneration?\nGemini 1.5 Pro\nThe association between aspirin and age-related macular degeneration (AMD) has been a topic of research and some debate, but the \ncurrent consensus leans towards no significant increase in risk. Here’s a breakdown of the evidence:\nEarly Studies: Some initial observational studies suggested a potential link between aspirin use and AMD, particularly wet AMD. \nHowever, these studies had limitations, as they couldn’t fully account for other factors that might influence both aspirin use and \nAMD development.\nLarge-Scale Clinical Trials: More recent and robust studies, including the Age-Related Eye Disease Studies (AREDS and AREDS2), \nhave provided stronger evidence. These large-scale trials found no significant association between aspirin use and progression to late \nAMD or its subtypes.\nRandomized Controlled Trials: These trials, considered the gold standard in research, have also indicated that aspirin does not \nincrease the risk of developing AMD.\nImportant Considerations:\nMedical Advice: It’s crucial to consult your doctor before making any decisions about aspirin use, especially if you have AMD or are \nat risk for it. They can assess your individual situation and advise on the best course of action.\nBenefits of Aspirin: Aspirin has proven benefits in preventing cardiovascular disease, which is a major cause of morbidity and \nmortality. \nRisk Factors for AMD: While aspirin may not be a significant risk factor, other factors like age, smoking, genetics, and diet play a \nmore substantial role in AMD development.\nIn conclusion: The current evidence suggests that aspirin use does not significantly increase the risk of age-related macular \ndegeneration. If your doctor recommends aspirin for other health reasons, you generally don’t need to avoid it due to concerns \nabout AMD. However, always discuss your individual situation with your healthcare provider. Here are some resources for further \nreading: AREDS: Aspirin Safe for Patients With AMD - American Academy of Ophthalmology: [ h t t p s :  / / w w w .  a a o . o r  g / e y e n  e t / a r  t i c l e /  a r \ne d s -  a s p i r i  n - s a f  e - f o r -  p a t i e n  t s - w i t  h - a m dDoes Aspirin Increase the Risk of Permanent Vision Loss from Macular Degeneration?: [ h t t p s :  / / \nw w w .  b r i g h t  f o c u s .  o r g / m  a c u l a r  / a r t i c  l e / a s p  i r i n - a n d - m a c u l a rDoes aspirin increase risk of AMD? - Macular Society: [ h t t p s :   /  / w w  w . m a c u l \na r s o c i  e t  y . o   r g / r e s  e a r  c h  / f e a t u   r e s / a s p i  r i n - r i s k /\nC\nDeepSeek R1-Lite-Preview\nThe question of whether aspirin is associated with an increased risk of age-related macular degeneration (AMD) is complex and \nrequires a nuanced answer based on current research. Here is a summary of the key points:\n1. Aspirin’s Role and Uses: Aspirin is a nonsteroidal anti-inflammatory drug (NSAID) commonly used for pain relief, fever reduction, \nand as an antiplatelet agent to prevent heart attacks and strokes.\n2. AMD Overview: AMD is a degenerative eye disease affecting the macula, leading to central vision loss. It has two forms: dry AMD \nand the more severe wet AMD, characterized by abnormal blood vessel growth.\n3. Potential Association: Some studies suggest a possible link between regular aspirin use and an increased risk of wet AMD. \nHowever, this association is not definitively established, and further research is needed to confirm causation.\n4. Mechanisms and Considerations: The exact mechanisms by which aspirin might influence AMD development are unclear. \nAspirin’s effects on blood clotting and inflammation could potentially impact eye health, but this is speculative.\n5. Risk Factors and Confounders: Other risk factors for AMD, such as age, smoking, genetics, and diet, may confound the \nrelationship between aspirin use and AMD risk.\n6. Dosage and Frequency: The impact of aspirin on AMD risk may depend on dosage and frequency of use, though this is not well-\ndefined.\n7. Risk-Benefit Balance: Aspirin has established benefits for heart health, so any potential increased risk of AMD must be weighed \nagainst these benefits on an individual basis.\n8. Consistency of Evidence: The evidence on aspirin and AMD risk is not entirely consistent across studies, indicating that the \nrelationship may not be straightforward.\n9. Guidelines and Recommendations: Medical organizations such as the American Academy of Ophthalmology or the American \nHeart Association may provide guidance on aspirin use in the context of AMD risk, though specific recommendations may not be \navailable.\nIn conclusion, while some studies suggest a potential association between aspirin use and increased risk of AMD, the evidence is \nnot conclusive. Individuals should consult with their healthcare providers to discuss their specific risks and benefits before making \ndecisions about aspirin use.\nU\nTable 1. Example of a question with concordant (“C”), undetermined (“U”), and discordant answers (“D”) \ncompared to the American Academy of Ophthalmology Preferred Practice Patterns (AAO PPP).\n \nScientific Reports |        (2025) 15:20556 4| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nquestions (GPT-o1, 90.2% vs. 97.1%, P = 0.30; Gemini 1.5 Pro, 81.3% vs. 94.3%, P = 0.11; DeepSeek-R1-Lite-\nPreview, 79.5% vs. 85.7%, P = 0.47), while two showed worse results (GPT-4o, 92.0% vs. 85.7%, P = 0.32; Claude \n3.5 Sonnet, 92.9% vs. 91.4%, P = 0.72); however, the differences were not statistically significant. Interestingly, in \ncase of confirmatory questions with positive ground truth, all five LLMs were always concordant with the AAO \nPPP , while performance worsened in confirmatory questions with negative ground truth (Table 3).\nThe word count of each answer was significantly different across LLMs (P < 0.001). The most verbose chatbot \nwas GPT-o1 (321.7 ± 164.2 words [95% CI, 295.0-348.5]), closely followed by GPT-4o (296.4 ± 97.4 words \n[95% CI, 280.6-312.3]) and Gemini 1.5 Pro (291.7 ± 87.71 [95% CI, 277.4–306.0]). DeepSeek-R1-Lite-Preview \npresented a lower count of words (245.1 ± 116.5 [95% CI, 226.2-264.1]), while the most concise model was \nClaude 3.5 Sonnet (180.9 ± 45.01 [95% CI, 173.6-188.2]). GPT-4o was the only model to present a significant \ndifference in word count between concordant and non-concordant answers (respectively, 286.7 ± 91.5 words vs. \n405.9 ± 98.7, P < 0.001).\nDiscussion\nGenerative AI has the potential to support healthcare systems by providing quick access to information and \nassisting clinicians in both administrative and clinical-related tasks 14–17. LLMs will also increasingly influence \nmedical education, answering patients’ queries and enhancing their understanding of ocular diseases and \navailable treatment options 18,19. However, there are still several barriers to the widespread adoption of this \ntechnology in daily clinical practice and telemedicine. Key priorities include eliminating or greatly reducing \nhallucinations, while improving the overall accuracy of the models and their adherence to official medical \nguidelines20.\nIn this study five publicly available LLMs have been prompted to answer 147 questions based on the AAO PPP \ndocuments. Answers were classified as concordant, undetermined, or discordant compared to the “Highlighted \nfindings and recommendations for care” of the AAO PPP . The best performing models were GPT-o1, GPT-4o, \nFig. 1. Number of concordant (green), undetermined (yellow), and discordant answers (red) compared to the \nAmerican Academy of Ophthalmology Preferred Practice Patterns®.\n \nScientific Reports |        (2025) 15:20556 5| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nand Claude 3.5 Sonnet, with approximately 90% concordant answers. This might be considered a promising \nresult; however, it is still arguably insufficient to consistently rely on LLMs for clinical decision-making. Gemini \n1.5 Pro and DeepSeek-R1-Lite-Preview obtained even lower percentages, respectively 84.4% and 81.6%. While \nachieving higher agreement with guidelines is crucial, a greater concern is the impact of discordant, incomplete, \nor inaccurate answers, which may directly or indirectly harm patients. The percentage of potentially harmful \nanswers across all LLMs ranged from 1.4 to 4.1%. LLMs yielded potential for harm due to omitted clinical signs \nand/or inadequate treatment recommendations.\nNotably, confirmatory questions with positive ground truth were always answered correctly by all LLMs, \nwhile performance was worse for confirmatory questions with negative ground truth. This may suggest a possible \nconfirmation bias of LLMs; however, our sample size of confirmatory questions was limited, and the study might \nnot have been adequately powered to detect significant differences.\nAnother key aspect in evaluating answers from LLMs is the inclusion of references to peer-reviewed articles. \nAll queried LLMs were severely lacking in this regard. Gemini 1.5 Pro provided the highest percentage of \nanswers with references (58.5%). Citations were even less common in other LLMs, appearing in approximately \n10–15% of the responses. Furthermore, the inclusion of references did not correlate with higher agreement \nrate with the AAO PPP . Arguably, current LLMs may be still limited by their core principles of function. LLMs \nprimarily operate by predicting the most likely next word in a sentence based on the submitted prompt and prior \ncontext. They are trained on a huge quantity of texts to recognize and reproduce natural language; however, \ngiven the sheer volume of data, it is not feasible to perform a comprehensive quality check on the entire training \ndataset, which may end up including inaccurate or inappropriate information. In addition, GPT-4o and Gemini \n1.5 Pro can also browse the internet to gather more data. Currently, it is not clearly documented how LLMs deal \nwith conflicting information from their training dataset or from websites they browse, and what are the specific \ncriteria used to prioritize one source over another.\nAnalyzing the word count of LLM responses may offer insights into how models manage uncertainty or \nknowledge gaps. Short answers may be incomplete or unclear. On the other hand, models may attempt to \ncompensate for uncertainty or lack of accurate information through excessive verbosity. While longer responses \nmay provide detailed explanations, they may also introduce ambiguity or irrelevant data. In our study, Claude 3.5 \nSonnet was the most concise model, whereas models by OpenAI generated the longest answers. Notably, GPT-\n4o generated significantly longer answers in case of non-concordant responses. Other LLMs did not display \nsignificant differences in word count between concordant and non-concordant answers.\nFig. 2. Number of answers potentially harmful to the patients.\n \nScientific Reports |        (2025) 15:20556 6| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nAnswers GPT-o1 GPT-4o Claude 3.5 Sonnet Gemini 1.5 Pro DeepSeek R1-Lite-Preview\nOpen\nC 101 (90.2%) 103 (92%) 104 (92.9%) 91 (81.3%) 89 (79.5%)\nU 9 (8%) 6 (5.4%) 4 (3.6%) 16 (14.3%) 14 (12.5%)\nD 2 (1.8%) 3 (2.7%) 4 (3.6%) 5 (4.5%) 9 (8%)\nConfirmatory (All)\nC 34 (97.1%) 30 (85.7%) 32 (91.4%) 33 (94.3%) 30 (85.7%)\nU 0 (0%) 2 (5.7%) 0 (0%) 0 (0%) 4 (11.4%)\nD 1 (2.9%) 3 (8.6%) 3 (8.6%) 2 (5.7%) 1 (2.9%)\nP value 0.30 0.32 0.72 0.11 0.47\nConfirmatory (+)\nC 14 (100%) 14 (100%) 14 (100%) 14 (100%) 14 (100%)\nU 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%)\nD 0 (0%) 0 (0%) 0 (0%) 0 (0%) 0 (0%)\nConfirmatory (-)\nC 20 (95.2%) 16 (76.2%) 18 (85.7%) 19 (90.5%) 16 (76.2%)\nU 0 (0%) 2 (9.5%) 0 (0%) 0 (0%) 4 (19%)\nD 1 (4.8%) 3 (14.3%) 3 (14.3%) 2 (9.5%) 1 (4.8%)\nP value 1.00 0.07 0.26 0.51 0.07\nTable 3. Number of concordant, undetermined, and discordant answers compared to the American Academy \nof Pphthalmology Preferred Practice Patterns. Results are presented separately for “open” and “confirmatory” \nquestions (positive and negative ground truth). On top, P values indicate the statistical significance of \nthe difference between concordant answers for open and confirmatory questions, in each of the five llms. \nOn bottom, P values indicate the statistical significance of the difference between concordant answers for \nconfirmatory questions with positive ground truth (+) and negative ground truth (-), in each of the five llms. \nAll P values were calculated using the Chi-square test of independence or fisher’s exact test, as appropriate. * A \nP value < 0.01 was considered statistically significant.\n \nFig. 3. Number of answers containing references to peer-reviewed articles.\n \nScientific Reports |        (2025) 15:20556 7| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nA new feature of some LLMs is the enhancement of logical reasoning thanks to reinforcement learning. \nGPT-o1 generates a chain of thought before each answer, optimizing performance in complex reasoning tasks21. \nSimilarly, DeepSeek provides a dedicated tool called “DeepThink” (R1 model) which is based on reinforcement \nlearning22. Despite these advancements, our results did not reveal significant differences in the performance \nof GPT-o1, GPT-4o, and Claude 3.5 Sonnet. In addition, DeepSeek was the worst performer in our study. \nPublishers’ benchmarks highlight the capabilities of GPT-o1 and DeepSeek in logical reasoning and math-\nrelated problems23,24; however, the medical questions submitted in our study likely did not offer opportunities \nfor these strengths to emerge.\nA possible solution to improve the reliability of generative AI is the development of specialized LLMs \ntailored towards medical practice. Luo et al. developed ChatZOC, a retrieval-augmented LLM framework for \nophthalmology based on the Chinese model Baichuan-13B 25. This LLM was fine-tuned on a comprehensive \nophthalmic dataset including over 30,000 pieces of ophthalmic knowledge 25. ChatZOC outperformed the \nbaseline Baichuan-13B model in answering ophthalmology questions; however, results were not superior to \nthe ChatGPT-4 model. Zheng et al. developed another LLM called MOPH by adopting the open-source LLM \nChatGLM2-6B. Prompt engineering was used to enhance MOPH’s semantic understanding by creating domain-\nspecific prompts, leveraging Chinese ophthalmic databases, peer-reviewed articles, and AAO EyeWiki. MOPH \ndemonstrated an adherence of 83.3% in respect to Chinese guidelines 26. Training and running such models \nrequires information technology expertise, as well as high computational resources and economical investments. \nOn the other hand, local deployment of LLMs minimizes potential privacy concerns, and allows better control \nover the model.\nAn alternative and simpler approach may be the creation of “custom GPTs” . These are versions of the default \nGPT-4o model which include user-defined “Instructions” and extra “Knowledge” consisting of uploaded \ndocuments that the LLM can use as reference to answer the user’s prompts. Similar custom models called “Gems” \ncan be designed in Gemini. In this case the LLM retrieves data from documents uploaded in Google Drive.\nThis study presents several limitations that deserve mentioning. Firstly, we did not include a control group of \nexperienced ophthalmologists to answer the questions submitted to the LLMs. In addition, the classification of \nanswers as concordant, undetermined, discordant, and harmful, although collaboratively performed by multiple \ninvestigators, is still partially subjective. Moreover, while we compared agreement between answers to “open” \nand “confirmatory” questions, we did not systematically formulate each question in both formats. This approach \ncould have provided valuable insight into the potential impact of different prompting strategies. Finally, a key \nlimitation of studies in the field of generative AI is that LLM performance may change rapidly due to frequent \nmodel updates and changes in training data cutoffs.\nTo our knowledge this is the first study to assess the agreement of LLMs with ophthalmology guidelines \n(PubMed, search query: “ophthalmology” AND (“agreement” OR “concordance” OR “adherence”) AND \n(“guidelines” or “practice patterns”) AND “large language model” , 2022–2025). However, other authors have \nalready investigated LLMs’ adherence to guidelines in other medical fields. Nwachukwu et al. have evaluated \nwhether commercially available LLMs (ChatGPT-4, Gemini, Mistral-7B, and Claude-3 Sonnet) provide \ntreatment recommendations concordant with the clinical guidelines published by the American Academy of \nOrthopaedic Surgeons for the management of rotator cuff tears and anterior cruciate ligament injuries. In their \nstudy, agreement with guidelines was most frequently observed with ChatGPT-4 (79.2%) and least frequently \nwith Mistral7B (58.3%). Conversely, discordant recommendations were most frequently recorded with Gemini \n(12.5%) and least frequently with ChatGPT-4 (2.1%)27. More recently, Fast et al. developed a benchmark called \n“ Autonomous Medical Evaluation for Guideline Adherence” designed to evaluate the adherence of 17 LLMs to \nmedical guidelines across 20 diagnostic scenarios in 13 specialties. The benchmark included 135 questions and \nmore than one thousand weighted scoring elements. ChatGPT-4 scored highest (41.9/50), followed closely by \nLlama-3 70B and WizardLM-2-8 × 22B 28.\nAssessing and optimizing the reliability of LLMs in medicine should be a priority to expand the applicability \nof generative AI. Physicians and developers should combine efforts to create domain-specific LLMs, tested \nand approved by international medical associations; however, significant challenges remain for their full-\nfledged implementation in clinical practice. Medical questions used for testing purposes often provide a clear \nand unequivocal clinical scenario. In contrast, real-life diagnostic and therapeutic workflows often rely on the \nidentification of subtle signs or specifics symptoms which can be ambiguously described by the patients. The \ncomplexity of such clinical scenarios cannot be fully captured in an online conversation with a chatbot. LLMs \nmay serve best as supportive tools rather than standalone solutions in medical practice.\nData availability\nData are available upon reasonable request by contacting Andrea Taloni (taloni.ophthalmology@gmail.com).\nReceived: 9 February 2025; Accepted: 9 June 2025\nReferences\n 1. Taloni, A. et al. Comparative performance of humans versus GPT-4.0 and GPT-3.5 in the self-assessment program of American \nAcademy of Ophthalmology. Sci. Rep. 13, 18562. https://doi.org/10.1038/s41598-023-45837-2 (2023).\n 2. Haddad, F . & Saade, J. S. Performance of ChatGPT on ophthalmology-related questions across various examination levels: \nObservational study. JMIR Med. Educ. 10, e50842. https://doi.org/10.2196/50842 (2024).\n 3. Sakai, D. et al. Performance of ChatGPT in board examinations for specialists in the Japanese Ophthalmology Society. Cureus 15, \ne49903. https://doi.org/10.7759/cureus.49903 (2023).\nScientific Reports |        (2025) 15:20556 8| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\n 4. Gill, G. S. et al. Comparison of Gemini advanced and ChatGPT 4.0’s performances on the ophthalmology resident ophthalmic \nknowledge assessment program (OKAP) examination review question banks. Cureus 16, e69612.  h t t p s : / / d o i . o r g / 1 0 . 7 7 5 9 / c u r e u s . 6 \n9 6 1 2     (2024).\n 5. Gill, G. S., Blair, J. & Litinsky, S. Evaluating the performance of ChatGPT 3.5 and 4.0 on StatPearls oculoplastic surgery text- and \nimage-based exam questions. Cureus 16, e73812. https://doi.org/10.7759/cureus.73812 (2024).\n 6. Cohen, S. A. et al. Dr. Google vs. Dr. ChatGPT: Exploring the use of artificial intelligence in ophthalmology by comparing the \naccuracy, safety, and readability of responses to frequently asked patient questions regarding cataracts and cataract surgery. Semin. \nOphthalmol. 39, 472–479. https://doi.org/10.1080/08820538.2024.2326058 (2024).\n 7. Bernstein, I. A. et al. Comparison of ophthalmologist and large language model Chatbot responses to online patient eye care \nquestions. JAMA Netw. Open 6, e2330320. https://doi.org/10.1001/jamanetworkopen.2023.30320 (2023).\n 8. Carlà, M. M., Giannuzzi, F ., Boselli, F . & Rizzo, S. Testing the power of Google DeepMind: Gemini versus ChatGPT 4 facing a \nEuropean ophthalmology examination. AJO Int. 1, 100063. https://doi.org/10.1016/j.ajoint.2024.100063 (2024).\n 9. Alkaissi, H. & McFarlane, S. I. Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus  h t t p s : / / d o i . o r g / 1 0 . 7 \n7 5 9 / C U R E U S . 3 5 1 7 9     (2023).\n 10. Chen, A. & Chen, D. O. Accuracy of Chatbots in citing journal articles. JAMA Netw. Open 6, e2327647.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 1 / j a \nm a n e t w o r k o p e n . 2 0 2 3 . 2 7 6 4 7     (2023).\n 11. Mugaanyi, J. et al. Evaluation of large language model performance and reliability for citations and references in scholarly writing: \nCross-disciplinary study. J. Med. Internet Res. 26, e52935. https://doi.org/10.2196/52935 (2024).\n 12. McGowan, A. et al. ChatGPT and Bard exhibit spontaneous citation fabrication during psychiatry literature search. Psychiatry Res. \n326, 115334. https://doi.org/10.1016/j.psychres.2023.115334 (2023).\n 13. About Preferred Practice Patterns (PPPs). American Academy of Ophthalmology.\n 14. Singh, S., Djalilian, A. & Ali, M. J. ChatGPT and ophthalmology: Exploring its potential with discharge summaries and operative \nnotes. Semin. Ophthalmol. 38, 503–507. https://doi.org/10.1080/08820538.2023.2209166 (2023).\n 15. GPT-4 and Ophthalmology Operative Notes | Annals of Biomedical Engineering. https://doi.org/10.1007/s10439-023-03263-5. \nAccessed 13 Jan 2025.\n 16. Chotcomwongse, P ., Ruamviboonsuk, P . & Grzybowski, A. Utilizing large language models in ophthalmology: The current \nlandscape and challenges. Ophthalmol. Ther. 13, 2543–2558. https://doi.org/10.1007/s40123-024-01018-6 (2024).\n 17. Huang, A. S. et al. Assessment of a large language model’s responses to questions and cases about glaucoma and retina management. \nJAMA Ophthalmol. 142, 371–375. https://doi.org/10.1001/jamaophthalmol.2023.6917 (2024).\n 18. Özer Özcan, Z., Doğan, L. & Yilmaz, I. E. Artificial doctors: Performance of Chatbots as a tool for patient education on keratoconus. \nEye Contact Lens 51, e112. https://doi.org/10.1097/ICL.0000000000001160 (2025).\n 19. Doğan, L., Özer Özcan, Z. & Edhem Yılmaz, I. The promising role of chatbots in keratorefractive surgery patient education. J. Fr. \nOphtalmol. 48, 104381. https://doi.org/10.1016/j.jfo.2024.104381 (2025).\n 20. Liu, Y ., Y ao, Y ., Ton, J.-F . et al. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment \n(2024).\n 21. Learning to reason with LLMs.  h t t p s :  / / o p e n  a i . c o m  / i n d e x  / l e a r  n i n g - t  o - r e a s  o n - w i t  h - l l m s /. Accessed 10 Jan 2025.\n 22. DeepSeek-AI, Liu, A., Feng, B. et al. DeepSeek-V3 Technical Report (2024).\n 23. Learning to reason with LLMs.  h t t p s :  / / o p e n  a i . c o m  / i n d e x  / l e a r  n i n g - t  o - r e a s  o n - w i t  h - l l m s /. Accessed 14 Jan 2025.\n 24. Introducing DeepSeek-V3 | DeepSeek API Docs. https://api-docs.deepseek.com/news/news1226. Accessed 14 Jan 2025.\n 25. Luo, M.-J. et al. Development and evaluation of a retrieval-augmented large language model framework for ophthalmology. JAMA \nOphthalmol. 142, 798–805. https://doi.org/10.1001/jamaophthalmol.2024.2513 (2024).\n 26. Zheng, C. et al. Development and evaluation of a large language model of ophthalmology in Chinese. Br. J. Ophthalmol. 108, \ne324526. https://doi.org/10.1136/bjo-2023-324526 (2024).\n 27. Nwachukwu, B. U. et al. Currently available large language models do not provide musculoskeletal treatment recommendations \nthat are concordant with evidence-based clinical practice guidelines. Arthroscopy https://doi.org/10.1016/j.arthro.2024.07.040 \n(2024).\n 28. Fast, D. et al. Autonomous medical evaluation for guideline adherence of large language models. NPJ Digit. Med. 7, 358.  h t t p s : / / d \no i . o r g / 1 0 . 1 0 3 8 / s 4 1 7 4 6 - 0 2 4 - 0 1 3 5 6 - 6     (2024).\nAcknowledgements\nThe American Academy of Ophthalmology Preferred Practice Pattern® is a registered trademark of the Ameri -\ncan Academy of Ophthalmology (AAO). This research was neither funded nor promoted by the AAO, but was \nconceived independently by the authors.\nAuthor contributions\nConceptualization, A.T., G.C., V .S., and G.G.; Methodology, A.T., AC.S., G.A., G.C., LML.B., MA.R., and G.G.; \nValidation, A.T., G.A., MA.R., G.C., LML.B., A.S., V .S., and G.G.; Formal Analysis, A.T., AC.S., G.C., LML.B., \nA.S., MA.R., and G.G.; Investigation, A.T., AC.S., G.A., LML.B., A.S., and MA.R.; Data Curation, A.T., AC.S., \nG.A., LML.B., A.S., and MA.R.; Writing – Original Draft Preparation, A.T., AC.S., G.C., and MA.R.; Writing – \nReview & Editing, A.S., G.C., V .S., and G.G.; Visualization, A.T., AC.S., G.A., MA.R., G.C., LML.B., A.S., V .S., \nand G.G.; Supervision, A.T., V .S., and G.G.; Project Administration, A.T., V .S., and G.G. All authors have read \nand agreed to the published version of the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nEthical approval\nThe research did not involve humans or animals.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 6 4 0 4 - z     .  \nCorrespondence and requests for materials should be addressed to G.G.\nScientific Reports |        (2025) 15:20556 9| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:20556 10| https://doi.org/10.1038/s41598-025-06404-z\nwww.nature.com/scientificreports/"
}