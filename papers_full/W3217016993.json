{
  "title": "Mixed Precision Low-Bit Quantization of Neural Network Language Models for Speech Recognition",
  "url": "https://openalex.org/W3217016993",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2156602468",
      "name": "Junhao Xu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2119302463",
      "name": "Jianwei Yu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2889079203",
      "name": "Shoukang Hu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2127361652",
      "name": "Xunying Liu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2011510006",
      "name": "Helen Meng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2156602468",
      "name": "Junhao Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119302463",
      "name": "Jianwei Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889079203",
      "name": "Shoukang Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127361652",
      "name": "Xunying Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011510006",
      "name": "Helen Meng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6677580257",
    "https://openalex.org/W6638632666",
    "https://openalex.org/W6727208969",
    "https://openalex.org/W6639703010",
    "https://openalex.org/W6684089856",
    "https://openalex.org/W3002595344",
    "https://openalex.org/W6770425567",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W6698200048",
    "https://openalex.org/W6638749077",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3008191852",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W6685891324",
    "https://openalex.org/W6686067075",
    "https://openalex.org/W2962760690",
    "https://openalex.org/W3048889251",
    "https://openalex.org/W2803431233",
    "https://openalex.org/W1735150698",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W6753767121",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W6695314431",
    "https://openalex.org/W6738642365",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6634534149",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W6679667936",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2496955520",
    "https://openalex.org/W6637551013",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W1996901117",
    "https://openalex.org/W2508418541",
    "https://openalex.org/W6725543821",
    "https://openalex.org/W6640289440",
    "https://openalex.org/W6694260854",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3034887213",
    "https://openalex.org/W6756887525",
    "https://openalex.org/W6757204547",
    "https://openalex.org/W3015720739",
    "https://openalex.org/W3016230677",
    "https://openalex.org/W6741553980",
    "https://openalex.org/W3008011454",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2408021097",
    "https://openalex.org/W2972736078",
    "https://openalex.org/W2952265507",
    "https://openalex.org/W2973051376",
    "https://openalex.org/W2111935653",
    "https://openalex.org/W6748587240",
    "https://openalex.org/W6729956949",
    "https://openalex.org/W6729972426",
    "https://openalex.org/W6746693648",
    "https://openalex.org/W2796265726",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W6752515464",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W6631362777",
    "https://openalex.org/W3196364802",
    "https://openalex.org/W6766225098",
    "https://openalex.org/W3034309359",
    "https://openalex.org/W6790647312",
    "https://openalex.org/W6753278433",
    "https://openalex.org/W2963821229",
    "https://openalex.org/W6730091202",
    "https://openalex.org/W3113244915",
    "https://openalex.org/W3095311338",
    "https://openalex.org/W2952613254",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W3015484572",
    "https://openalex.org/W3094852745",
    "https://openalex.org/W3095714920",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1495076553",
    "https://openalex.org/W3163368926",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W6680230698",
    "https://openalex.org/W2094147890",
    "https://openalex.org/W2057450058",
    "https://openalex.org/W6714058667",
    "https://openalex.org/W6638218882",
    "https://openalex.org/W1903115690",
    "https://openalex.org/W3097747488",
    "https://openalex.org/W2963048316",
    "https://openalex.org/W3163570686",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963225922",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4288026258",
    "https://openalex.org/W2962847160",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2949964376",
    "https://openalex.org/W2119144962",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W1797288984",
    "https://openalex.org/W1935978687",
    "https://openalex.org/W2161758346",
    "https://openalex.org/W4295185264",
    "https://openalex.org/W2405920868",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W2962746461",
    "https://openalex.org/W2885311373",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2619096655",
    "https://openalex.org/W2136939460",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W4292363360",
    "https://openalex.org/W2788853733",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W3007328579",
    "https://openalex.org/W2773706593",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W2962833442",
    "https://openalex.org/W2279098554",
    "https://openalex.org/W1902934009"
  ],
  "abstract": "State-of-the-art language models (LMs) represented by long-short term memory\\nrecurrent neural networks (LSTM-RNNs) and Transformers are becoming\\nincreasingly complex and expensive for practical applications. Low-bit neural\\nnetwork quantization provides a powerful solution to dramatically reduce their\\nmodel size. Current quantization methods are based on uniform precision and\\nfail to account for the varying performance sensitivity at different parts of\\nLMs to quantization errors. To this end, novel mixed precision neural network\\nLM quantization methods are proposed in this paper. The optimal local precision\\nchoices for LSTM-RNN and Transformer based neural LMs are automatically learned\\nusing three techniques. The first two approaches are based on quantization\\nsensitivity metrics in the form of either the KL-divergence measured between\\nfull precision and quantized LMs, or Hessian trace weighted quantization\\nperturbation that can be approximated efficiently using matrix free techniques.\\nThe third approach is based on mixed precision neural architecture search. In\\norder to overcome the difficulty in using gradient descent methods to directly\\nestimate discrete quantized weights, alternating direction methods of\\nmultipliers (ADMM) are used to efficiently train quantized LMs. Experiments\\nwere conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed\\nperturbation, i-Vector and learning hidden unit contribution (LHUC) based\\nspeaker adaptation on two tasks: Switchboard telephone speech and AMI meeting\\ntranscription. The proposed mixed precision quantization techniques achieved\\n\"lossless\" quantization on both tasks, by producing model size compression\\nratios of up to approximately 16 times over the full precision LSTM and\\nTransformer baseline LMs, while incurring no statistically significant word\\nerror rate increase.\\n",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nMixed Precision Low-bit Quantization of Neural\nNetwork Language Models for Speech Recognition\nJunhao Xu, Jianwei Yu, Shoukang Hu, Xunying Liu, Member, IEEE, Helen Meng, Fellow, IEEE\nAbstract—State-of-the-art language models (LMs) represented\nby long-short term memory recurrent neural networks (LSTM-\nRNNs) and Transformers are becoming increasingly complex\nand expensive for practical applications. Low-bit neural network\nquantization provides a powerful solution to dramatically reduce\ntheir model size. Current quantization methods are based on\nuniform precision and fail to account for the varying perfor-\nmance sensitivity at different parts of LMs to quantization\nerrors. To this end, novel mixed precision neural network LM\nquantization methods are proposed in this paper. The optimal\nlocal precision choices for LSTM-RNN and Transformer based\nneural LMs are automatically learned using three techniques.\nThe ﬁrst two approaches are based on quantization sensitivity\nmetrics in the form of either the KL-divergence measured\nbetween full precision and quantized LMs, or Hessian trace\nweighted quantization perturbation that can be approximated\nefﬁciently using matrix free techniques. The third approach is\nbased on mixed precision neural architecture search. In order\nto overcome the difﬁculty in using gradient descent methods to\ndirectly estimate discrete quantized weights, alternating direction\nmethods of multipliers (ADMM) are used to efﬁciently train\nquantized LMs. Experiments were conducted on state-of-the-\nart LF-MMI CNN-TDNN systems featuring speed perturbation,\ni-Vector and learning hidden unit contribution (LHUC) based\nspeaker adaptation on two tasks: Switchboard telephone speech\nand AMI meeting transcription. The proposed mixed precision\nquantization techniques achieved “lossless” quantization on both\ntasks, by producing model size compression ratios of up to\napproximately 16 times over the full precision LSTM and Trans-\nformer baseline LMs, while incurring no statistically signiﬁcant\nword error rate increase.\nIndex Terms—Language models, Speech recognition, LSTM-\nRNN, Transformer, Low-bit Quantization, ADMM\nI. I NTRODUCTION\nL\nANGUAGE models (LMs) are important components in\nautomatic speech recognition (ASR) systems and many\nother applications. A key part of the statistical language\nmodelling problem is to derive the suitable representation\nof long-range history contexts. Directly modelling long-span\nword histories using conventional back-off n-gram models [1]\ngenerally leads to a severe data sparsity issue [2]. To this end,\nover the past few decades there have been signiﬁcant efforts\nof developing artiﬁcial neural network based language mod-\nelling techniques in the speech technology community [3]–\n[14]. Neural network language models (NNLMs) representing\nlonger span history contexts in a continuous and lower dimen-\nJunhao Xu, Jianwei Yu, Shoukang Hu, Xunying Liu, and Helen Meng\nare with the Chinese University of Hong Kong, Hong Kong 999077, China\n(e-mail: jhxu@se.cuhk.edu.hk; jwyu@se.cuhk.edu.hk; skhu@se.cuhk.edu.hk;\nxyliu@se.cuhk.edu.hk; hmmeng@se.cuhk.edu.hk. Corresponding author:\nXunying Liu.\nsional vector space, are used to improve the generalization\nperformance.\nWith the rapid progress of deep neural network (DNN)\nbased ASR technologies in recent decades, the underlying net-\nwork architectures of NNLMs have evolved from feedforward\nstructures [3]–[7] to more advanced variants represented by\nlong-short term memory recurrent neural networks (LSTM-\nRNNs) [8]–[10], [15] and more recently neural Transform-\ners [11]–[14], [16] that are designed for modelling longer\nrange contexts. In particular, Transformer based language\nmodels in recent years have deﬁned state-of-the-art perfor-\nmance across a range of ASR task domains [11]–[14], [17].\nThese models [11]–[13], [17] are often constructed using a\ndeep stacking of multiple self-attention based neural building\nblocks [18]–[20], each of which also includes residual con-\nnections [21] and layer normalization modules [22]. Additional\npositional encoding layers [16], [23] are employed to augment\nthe self-attention structures with word sequence order infor-\nmation. Performance improvements over conventional LSTM-\nRNN language models have been widely reported [11], [24].\nHowever, the increasingly deeper and more complex ar-\nchitecture designs featuring in LSTM-RNN and Transformer\nmodels present many challenges for current ASR technologies.\nThese not only lead to a large increase in their overall memory\nfootprint and computational cost when operating on the cloud,\nbut also creates difﬁculty when deployed on edge devices\nto enhance privacy and reduce latency. In a wider context\nwithin the speech technology community, such dramatically\nincreasing demand for computational resources is consistent\nwith the recent trend of moving towards a data and com-\nputational intensive all neural end-to-end (E2E) modelling\nparadigm represented by, for example, Transformers [25]–\n[27], RNN transducers (RNN-T) [28], and listen, attend and\nspell (LAS) [29]. State-of-the-art ASR systems featuring these\nend-to-end approaches often contain a very large number\nof parameters, for example, up to 280 million [30]. Hence,\nthere is a pressing need of developing ultra-compact, low\nfootprint language modelling methods, and ASR technologies\nin general, to facilitate more aggressive reduction in memory\nfootprint, model training and evaluation time while maintain-\ning competitive accuracy performance.\nTo this end, signiﬁcant efforts have been made in both the\nmachine learning and speech technology communities to de-\nvelop DNN compression techniques [31]–[74]. Pruning based\nmethods exploiting the structural and parameter sparsity were\nused to reduce DNN model size in [38]–[44]. They are particu-\nlarly useful for models containing large fully connected layers\nsuch as the ResNet systems [43], [44]. Knowledge distilla-\narXiv:2112.11438v1  [cs.CL]  29 Nov 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\ntion and teacher-student learning [45]–[47] based approaches\nextract information from a pre-trained, larger model into a\nsmaller one. Low rank matrix factorization [48]–[51], [55],\nand neural architecture search (NAS) based methods [63]–[72]\nhave also been proposed.\nAnother powerful family of techniques recently drawing in-\ncreasing interest across the machine learning, computer vision\nand speech technology communities to solve this problem is\nto use low-bit DNN quantization techniques [31]–[37], [52],\n[57], [58], [62], [74], [75]. By replacing ﬂoating point based\nDNN parameters with low precision values, for example,\nbinary numbers, model sizes can be dramatically reduced\nwithout changing the DNN architecture [32], [57], [73]. Fur-\nther DNN size reduction can be obtained when low-precision\nquantization is used in combination with neural architecture\nsearch (NAS) techniques, for example, in the SqueezeNet\nsystem designed for computer vision tasks [52]. In contrast\nto the extensive prior research works on low-bit quantization\nmethods primarily targeting computer vision tasks [31]–[37],\n[52], only limited previous research in this direction has been\nconducted in the context of language modelling [57], [58] and\nASR systems [56], [59].\nTwo issues are associated with current low-bit DNN quan-\ntization methods. First, these approaches are predominantly\nbased on uniform precision, where an identical bit-width is\napplied to all weight parameters for quantization [76]. Within\nsuch framework, the varying local performance sensitivity\nexhibited at different parts of the underlying DNN system\nto quantization errors is not taken into account. In practice,\nthis often leads to large performance degradation against full\nprecision models. Second, when DNN weights are restricted\nto discrete values, the conventional back-propagation (BP)\nalgorithm based on gradient descent methods cannot be di-\nrectly applied to estimate the quantized model parameters.\nExisting approaches for training quantized DNNs often use\na modiﬁed BP algorithm [31], [32]. In this approach, low\nprecision quantized parameters are ﬁrst used in a forward pass\nto compute the error loss, before full precision parameters are\nthen used in a backward pass to propagate the gradients for\nsubsequent model update. However, the inconsistency between\nquantized, discrete weights and the SGD algorithm assuming\ncontinuous and differentiable error cost functions leads to not\nonly very slow convergence in training, but also performance\ndegradation against full precision models.\nIn order to address the ﬁrst issue discussed above re-\ngarding performance sensitivity, and motivated by the recent\ndevelopment of mixed precision DNN acceleration hardware\nthat allows multiple locally selected precision settings to\nbe used [37], novel mixed precision DNN quantization ap-\nproaches are proposed in this paper by utilizing locally vari-\nable bit-widths at different layer components of LSTM-RNN\nand Transformer LMs. The optimal local precision settings\nare automatically learned using three techniques. The ﬁrst\ntwo approaches are based on quantization sensitivity metrics\nin the form of either Hessian trace weighted quantization\nperturbation that can be approximated efﬁciently via matrix\nfree techniques, or the KL-divergence measured between full\nprecision and quantized language models. The third approach\nis based on mixed precision neural architecture search.\nIn order to address the second issue over the difﬁculty in\nusing gradient descent methods to directly estimate NNLMs of\ndiscrete weights, the general problem of estimating quantized\nDNN model parameters is reformulated as an optimization\ntask. For any form of quantized NNLMs using uniform or\nmixed precision settings, alternating direction methods of mul-\ntipliers (ADMM) [17], [73], [77] are proposed to efﬁciently\ntrain their discrete parameters. Two sets of model parameters\nrespectively associated with a full precision neural network\nLM, and the corresponding optimal quantized model with\na particular precision setting, are iteratively learned via a\ndecomposed dual ascent scheme in an alternating fashion. This\nnovel quantized NNLM estimation algorithm draws strength\nfrom both the decomposability of classic dual ascent schemes\nand the stable convergence of multiplier methods.\nExperiments were conducted on state-of-the-art LF-MMI\nCNN-TDNN and TDNN systems [78] featuring speed per-\nturbation, i-Vector [79] and learning hidden unit contribution\n(LHUC) based speaker adaptation [80] on two tasks, Switch-\nboard telephone speech [81] and AMI meeting transcrip-\ntion [82]. Experimental results suggest the proposed mixed\nprecision LM quantization techniques achieved model size\ncompression ratios of about 16 times over the full precision\nTransformer LM baselines with no statistically signiﬁcant\nrecognition performance degradation.\nThe main contributions of this paper are summarized as\nfollows.\n1) To the best of our knowledge, this paper presents the\nﬁrst work in the speech technology community to apply mixed\nprecision DNN quantization techniques to both LSTM-RNN\nand Transformer based NNLMs. In contrast, prior researches\nwithin the speech community in this direction largely focused\non uniform precision based quantization of convolutional\nneural networks (CNNs) acoustic models [62] and LSTM-\nRNN language models [57], [58], [75].\n2) To the best of our knowledge, this paper is the ﬁrst\nwork to introduce ADMM based neural network quantization\ntechniques for speech recognition tasks. In contrast, prior\nresearches with the speech technology community on low-\nbit quantization of CNNs [62] and LSTM-RNN LMs [57],\n[58], [75] used the modiﬁed BP algorithm [31], [32] while\nthe inconsistency between discrete, quantized parameters and\ngradient based SGD update remains unaddressed.\n3) Transformer LM model size compression ratios of up to\napproximately 16 times over the full precision Transformer\nLM baselines with no statistically signiﬁcant recognition per-\nformance degradation were obtained using an average 2-bit\nmixed precision conﬁguration. To the best of our knowledge,\nthis is the best low-bit Transformer language model compres-\nsion ratio published so far in the speech technology community\nwhile incurring no recognition accuracy loss.\nThe rest of the paper is organized as follows. LSTM-RNN\nand Transformer based NNLMs are reviewed in section II. A\ngeneral neural network quantization scheme based on uniform\nor locally varying mixed quantization precisions are presented\nin section III. ADMM based training of quantized NNLMs are\npresented in Section IV . Section V presents three novel mixed\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nprecision quantization methods. Experiments and results are\nshown in Section VI. Finally, conclusions and possible future\nwork are discussed in section VII.\nII. NN L ANGUAGE MODELS\nThis section reviews two types of neural network language\nmodels that are widely used in state-of-the-art speech recog-\nnition systems: long-short term memory recurrent neural net-\nwork (LSTM-RNN) and Transformer based language models.\nA. Recurrent Neural Network LMs\nThe form of LSTM-RNN language models considered in\nthis paper computes the word probability wt given the pre-\nceding history context of t−1 words w1,..., wt−1 as\nP(wt|wt−1\n1 ) ≈P(wt|wt−1,ht−1) , (1)\nwhere ht−1 is the D-dimensional vector hidden state encoding\npart of the history information (w1,..., wt−2) up to word\nwt−2, where D is the number of RNNLM hidden layer nodes.\nThe most recent word history wt−1 is represented by a N-\ndimensional one-hot vector ˜ wt, where N is the vocabulary\nsize. This one-hot word input vector is ﬁrst projected into a\nM-dimensional (M ≪N) linear vector embedding as\nxt = ΘU˜ w⊤\nt , (2)\nwhere ΘU is a projection matrix to be learned, before being\nfurther fed into the hidden layers where non-linear transforma-\ntions are applied. The hidden state ht is calculated within the\nLSTM cells [83] where the previous hidden state ht−1 and the\ncurrent word input embedding ˜ wt is combined. Each LSTM\nmemory cell consists of a set of Sigmoidal gating activations,\nthe input gate it, forget gate ft, cell gate ˜ ct and output gate\not. These are used to control the information ﬂow within the\ncells in order to trap longer range history contexts and address\nthe vanishing gradient issue. The respective outputs from these\ngates are given by\nft = σ\n(\nΘf [xt−1,ht−1,1]⊤\n)\n(3)\nit = σ\n(\nΘi [xt−1,ht−1,1]⊤\n)\n(4)\n˜ ct = tanh\n(\nΘc [xt−1,ht−1,1]⊤\n)\n(5)\not = σ\n(\nΘo [xt−1,ht−1,1]⊤\n)\n(6)\nwhere tanh(u) = [tanh(u1),..., tanh(uD)] for any u ∈RD.\nThe ﬁnal hidden state is normally computed recursively in\nan auto-regressive fashion, for example, from left to right in\ncase of standard uni-directional LSTM-RNN LMs modelling\nhistory contexts only [5], [8], [10]. Using the above four gating\nfunctions outputs, the LSTM-RNN cell contents ct and hidden\nstate representation ht are ﬁnally computed as\nct = ft ⊗ct−1 + it ⊗˜ ct (7)\nht = ot ⊗tanh(ct), (8)\nwhere ⊗is the Hadamard product.\nB. Transformer LMs\nThe Transformer model architecture considered in this paper\nfeatures a deep stacking of multiple Transformer decoder\nblocks. Unless otherwise stated, 6 Transformer decoder blocks\nare used in all the experiments of this paper. As shown in\nthe top part of Figure 1, each Transformer decoder block\nconsists of a multi-head self-attention [18]–[20] module and\na feed forward module. Residual connections [21] and layer\nnormalization operations [22] are also inserted between these\ntwo modules. Let xl−1\nt denotes the output of the (l−1)-th\nTransformer block at input word position t. The multi-head\nself-attention module in the succeeding l-th block transforms\nxl−1\nt to zl\nt as follows:\nql\nt,kl\nt,vl\nt = Θl\nQxl−1\nt ,Θl\nKxl−1\nt ,Θl\nVxl−1\nt (9)\nyl\nt = Attention(kl\n1,..., kl\nt,vl\n1,..., vl\nt,ql\nt) (10)\nzl\nt = Θl\nhyl\nt + xl−1\nt (11)\nol\nt = LayerNorm(zl\nt) (12)\nwhere Θl\nQ,Θl\nK,Θl\nV denote the learnable query, key, value\nprojection matrices which map xl−1\nt into the corresponding\nvector representations of query ql\nt, key kl\nt and value vl\nt\nrespectively. zl\nt is the sequence of of cached key-value vector\npairs up to time t, which only contains the history context\ninformation and can prevent the model from using any future\ncontext. Attention(·) denotes the scaled multi-head dot product\nself-attention [20]. LayerNorm(·) represents the layer normal-\nization operation [22]. Θl\nh denotes the learnable projection\nmatrix applied to the outputs of the Attention operation prior\nto layer normalization. The normalized output ol\nt is then fed\ninto the feed forward module:\nsl\nt = Θl\n2GELU(Θl\n1ol\nt) + ol\nt (13)\nxl\nt = LayerNorm(sl\nt) (14)\nwhere Θl\n1 and Θl\n2 are the weight matrices that are applied\nto the normalized output ol\nt before Gaussian error linear unit\n(GELU) [84] activation functions and after. For simplicity the\nbias vectors optionally used in the feed forward modules are\nomitted in the above Equation (13). In addition, positional\nembedding layers are also used in all the transformer LMs\nconsidered in this paper.\nIII. N EURAL NETWORK QUANTIZATION\nThe standard n-bit quantization problem for deep neural\nnetworks (DNNs) considers the task of for any full precision\nweight parameter, Θ, ﬁnding its closest discrete approximation\nfrom the following quantization table with a global scaling\nfactor α and quantization parameter V,\nq= αV ∈{0,±α,..., ±α·(2n−1 −1)} (15)\nas the one that incurs the minimum quantization error\nf(Θ) = arg min\nq\n|Θ −q| (16)\nFurther simpliﬁcation to the above quantization table of\nEquation (15) leads to extremely low bit quantization based\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\non, for example, binary values {−1,1}[34], [74], or tertiary\nvalues {−1,0,1}[85].\nIt is assumed in the above standard quantization process that\na global quantization table is applied to all weight parameters.\nIn order to account for the ﬁne-grained local quantization\nsensitivity, the following more general form of quantization is\nconsidered for a particular model parameter Θ(l) within any\nof the l-th weight cluster, for example, all parameters of the\nsame LSTM or Transformer LM layer,\nf(Θ(l)) = arg min\nQ(l)\n|Θ(l) −Q(l)| (17)\ncan be used. The locally shared l-th quantization table is\nQ(l) = α(l)V(l) ∈{0,±α(l),..., ±α(l)(2nl−1 −1)} (18)\nwhere the full precision scaling factor α(l) is used to adjust\nthe dynamic range of all the quantized weights in the l-\nth cluster. It is shared locally within each individual DNN\nparameter clusters. The locally variable quantization bit length\nnl can be set to be 1,2,4,8 etc. depending on the optimal\nprecision settings to be used. A special case, when the local\nquantization table in Equation (18) is shared across all the\nlayers in the system, this leads to the traditional uniform\nprecision quantization. The above tying of quantization tables\nmay be ﬂexibly performed at either layer, or node level, or\nin the extreme case at individual parameter level (this is\nequivalent to no quantization being applied). Intuitively, the\nlonger the local quantization precision bit widths {nl}are used\nat each part of the underlying neural language model, a smaller\ncompression ratio after quantization and reduced performance\ndegradation is expected.\nIV. ADMM BASED TRAINING OF QUANTIZED DNN\nOne major challenge faced by both uniform and mixed\nprecision quantization is that the gradient descent methods\nand back-propagation (BP) algorithm cannot be directly used\nwhen weights are quantized to discrete values. To this end,\nmixed precision BP was proposed in [31], [32] where low\nprecision binarized parameters were ﬁrst used in the forward\npass to compute the error loss before full precision parameters\nare used in the backward pass to propagate the gradients.\nHowever, there is an inconsistency between quantized, discrete\nweights and the assumption over continuous and differentiable\nerror cost functions in SGD update. Hence, directly training\nquantized system using mixed precision BP leads to very slow\nconvergence and the performance gap between full precision\nand quantized systems remains large. An alternative solution\nto this problem is to reformulate quantization as a constrained\noptimization problem solved by the alternating direction meth-\nods of multipliers (ADMM) [73].\nThe ADMM based optimization decomposes a dual as-\ncent problem into alternating updates of two variables. In\nthe context of the neural network LM quantization problem\nconsidered here, these correspond to the full precision model\nweights update and the discrete quantization tables estimation.\nThe overall ADMM Lagrange function is given as\nL= Fce(Θ) + (γλ)⊤·(Θ −f(Θ)) + γ\n2 ||Θ −f(Θ)||2\n2 (19)\nwhere Fce is the cross entropy (CE) loss, Θ are the full pre-\ncision model parameters. f(Θ) represents the quantization of\nthe parameters calculated from the quantization using Equation\n(17). γ >0 is the penalty parameter which is empirically set\nas 10−3 throughout this paper and λdenotes the Lagrangian\nmultiplier. The standard Lagrangian term expressed in the form\nof a dot product between the multiplier variable λ and the\nquantization error, Θ −f(Θ), is shown as the second term\nin Equation (19). In order to further improve the robustness\nand convergence speed of the ADMM algorithm, an additional\nterm related to the quantization error squared norm, shown as\nthe third term in Equation (19), is also introduced to form\nan augmented Lagrangian [73]. Further rearranging Equation\n(19) leads to the following loss function.\nL= Fce(Θ) + γ\n2 ||Θ −f(Θ) + λ||2\n2 −γ\n2 ||λ||2 (20)\nGiven a particular uniform or mixed precision quantization\nconﬁguration, the ADMM algorithm is iteratively performed\nto ﬁnd the optimal scaling factors, {α(l)}, in the quantization\ntable(s) of Equation (18). For simplicity, in the following\ndetailed description of the algorithm, we assume a globally\nshared quantization table {0,±α,..., ±α(2n−1 −1)}is ap-\nplied to all parameters and a single scaling factor α is to be\nlearned. The following iterative update can be extended when\nmultiple shared quantization tables of different bit-widths nl\nin Equation (18) are used. When the ADMM algorithm is\nperformed at the (k+ 1)th iteration, the overall update can be\nsplit into three stages presented in the following subsections.\nA. Full precision model parameter update\nThe following equation is used to update the full precision\nweight parameters Θ(k+1).\nΘ(k+1) = arg min\nΘ\nL(Θ,f(Θ(k)),λ(k)) (21)\nwhere f(Θ(k)),λ(k) are the quantized weights and error\nvariable at the kth iteration. The gradient of the loss function\nin Equation (20) w.r.t Θ is calculated as the following.\n∇L= ∇Fce + γ(Θ −f(Θ(k)) + λ(k)) (22)\nIt is found in practice that the quadratic term of the\naugmented Lagrangian of Equation (20) can dominate the loss\nfunction computation and lead to a local optimum. Following\nthe extra-gradient method suggested in [86], the solution to\naddress this issue in this paper is to perform the gradient\ncalculation by one additional step ahead to improve the\nconvergence.\n¯Θ ←Θ(k) −η1∇L(Θ)\nΘ(k+1) ←Θ(k) −η2∇L( ¯Θ)\n(23)\nHere ¯Θ represents the temporary variable used to store the\nintermediate updated parameters, and η1 and η2 are separate\nlearning rates that are empirically set as 0.02 and 0.001\nthroughout the experiments of this paper.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nB. Quantization variables update\nThe quantization variables including the scaling factor αin\na globally shared quantization table {0,±α,..., ±α(2n−1 −\n1)}, and the corresponding quantized parameters derived using\nEquation (17) can be solved by minimizing the following:\nmin\nf\n||Θ(k+1) −f(Θ(k)) + λ(k)||2\n2\n⇒min\nα,V\n||Θ(k+1) + λ(k) −α(k)V(k)\nα ||2 (24)\nwhere alternating updates of the scaling factor estimate α(k)\nand the associated quantized model parameters V(k+1)\nα are\nperformed via an inner-loop within the current (k+1)th outer\niteration.\nThe following algorithm shows the details of such inner-\nloop update. In all the experiments of this paper, the maximum\nnumber of inner-loops is set as 20 and practically found\nsufﬁcient to ensure convergence.\nAlgorithm 1: Quantization variables update inner loop\nResult: α(k+1), V(k+1)\nα\nj = 0;\nα(k+1)\n0 = α(k);\nV(k+1)\n0,α = arg min\nV\n|Θ(k+1) −α(k+1)\n0 V|;\nwhile j <20 && convergence is not reached do\nα(k+1)\nj+1 =\n(Θ(k+1)+λ(k))⊤V(k+1)\nj,α\nV(k+1)⊤\nj,α V(k+1)\nj,α\n;\nV(k+1)\nj+1,α = arg min\nV\n|Θ(k+1) −α(k+1)\nj+1 V|;\nj = j+ 1;\nend\nα(k+1) = α(k+1)\nj ;\nV(k+1)\nα = V(k+1)\nj,α\nThe minimization operation of Equation (24) aims to ﬁnd\nthe corresponding quantized model parameters given the full\nprecision parameter updates in Section IV-A. As it is non-\ntrivial to update both the scaling parameters α(l) of the\nquantization table of Equation (18) and the quantized pa-\nrameters derived using Equation (17) at the same time, an\nalternating estimation procedure is used here in the inner-loop\nof Algorithm 1, to produce interleaving updates of the scaling\nparameters α(l) when ﬁxing the current quantized parameters\nV(k)\nα , and vice versa when updating the quantized parameters\nV(k)\nα , while keeping α(l) unchanged. Intuitively the resulting\nupdate of α(l) accounts for the change in the dynamic range\nof model parameters before and after quantization.\nC. Quantization error update\nThe Lagrange multiplier variable λ, now encoding the\naccumulated quantization errors computed at each iteration,\nis updated as\nλ(k+1) = λ(k) + Θ(k+1) −f(Θ(k+1)) (25)\nIn all experiments of this paper the scaling factors α(l) in\nEquation (18) are initialized to 1.0 before the ADMM update\nis performed.\nThe above ADMM estimation of quantized neural network\nLMs can be executed iteratively until convergence measured\nin terms of validation data perplexity. In practice, a maximum\nnumber of 20 ADMM iterations was used throughout all the\nexperiments of this paper to obtain convergence for all quan-\ntized LSTM-RNN and transformer LMs, as will be shown later\nin the convergence speed analysis of the following Section VI\nof experiments.\nV. M IXED PRECISION QUANTIZATION\nThis section presents three approaches to automatically\nlearn the optimal local precision settings previously introduced\nin Equation (18) for DNN quantization. The ﬁrst two mini-\nmizes the performance sensitivity to low-bit quantization. They\nare measured using either the KL divergence between full\nprecision and quantized LMs, or the log-likelihood curvature\nwith respect to quantization error. The third approach uses\na mixed precision differentiable neural architecture search\ntechnique. Examples of their application to Transformer and\nLSTM-RNN based LMs are also given.\nA. KL Divergence Based Mixed Precision Quantization\nThe ultimate goal for any DNN quantization task, including\nthe neural network LMs considered in this paper, is to obtain a\n“lossless” model compression such that the distance between\nthe distribution embodied by the original full precision LM\nand that of the quantized model must be minimized. This\nrequires the relative entropy, or equivalently the Kullback-\nLeibler (KL) divergence between full precision and quantized\nneural network LMs to be minimized. As a special case,\nthe KL divergence is zero when a lossless compression is\nachieved, so that the same LM distribution is preserved after\nquantization. The use of KL divergence based distance metrics\nin early researches led to widely adopted back-off n-gram LM\npruning techniques [87], [88].\nIn this paper, the KL divergence between the probability\ndistributions obtained from a full precision NNLM and its\nquantized counterpart using a particular mixed precision set-\nting is used to measure the resulting performance sensitivity.\nTaking a L-layer Transformer LM for example, for any quan-\ntization f(·) being applied to the full precision parameters Θ,\nthe KL divergence based quantization sensitivity measure is\ncomputed over the training data of Nw words as,\nΩKL =\nL∑\ni=1\nΩKL\ni =\nL∑\ni=1\nDKL(P(Θi)||P(fni(Θi))) (26)\n=\nL∑\ni=1\nNw∑\nt=1\nP(wt|wt−1,ht−1,Θi) ln P(wt|wt−1,ht−1,Θi)\nP(wt|wt−1,ht−1,fni(Θi)\nwhere Θi denote the full precision parameters of the ith layer,\nand fni(Θi) its associated ni-bit quantized parameters given\na particular local precision bit width ni.\nGiven a target average quantization precision such as 2-bit,\nthe local quantization bit widths used in each layer should\nbe selected such that the total performance sensitivity in\nEquation (26) is minimized while satisfying the target model\nsize constraint. However, directly evaluate the combined KL\ndivergence metric for all possible mixed precision local quanti-\nzation settings leads to a very large number of possible systems\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nto be considered. For example, choosing among 4 different\nprecision settings, 1-bit, 2-bit, 4-bit and 8-bit, across all 6\nlayers of a Transformer LM, produces a total of 46 = 4096\nmixed precision quantized Transformer LMs to be evaluated\nin terms of KL divergence against the full precision model.\nIn order to address this scalability issue, a practical im-\nplementation adopted in this paper is based on a divide and\nconquer approach. The key information required to compute\nthe sensitivity measure in Equation (26) is the local KL\ndivergence metric, ΩKL\ni , for example, associated with the\nith Transformer LM layer. In our implementation, a set of\nprototype transformer LMs quantized using uniform precision,\nfor example 1-bit, 2-bit, 4-bit and 8-bit are trained off-line\nﬁrst via ADMM optimization of Section IV . The performance\nsensitivity in Equation (26) can then be computed ﬁrst locally\nfor each layer by replacing the full precision parameters using\neach of the above four possible quantization choices at that\nlayer only, before taking the sum to produce the combined\nnet KL divergence metric when a particular mixed precision\nbased local quantization conﬁguration is applied across the\nentire Transformer LM.\nIn order to further improve the efﬁciency when computing\nthe KL divergence based sensitivity measure in Equation (26),\na very small number of training data samples, as little as the\ndata of one single mini-batch (batch size set as 32 throughout\nthis paper) can be used, and adopted in all experiments of this\npaper. In practice this was found to produce quantized LM\nperplexity comparable to that obtained by computing the KL\nmetric over the entire training data set. An ablation study on\nthe relationship between the amount of Switchboard training\ndata used to compute the KL divergence metric and determine\nthe resulting mixed precision for Transformer LM quantization\nand the resulting LM’s perplexity performance is shown in\nTable I. In order to further ensure efﬁciency, the resulting\nmixed precision quantized LMs using varying automatically\nlearned local precision settings together with quantized pa-\nrameters inherited from uniform precision models of different\nbit-widths are ﬁne-tuned, rather than retrained from scratch. In\npractice, this was found to produce performance comparable to\nre-training them from scratch after determining the precision\nsettings, as is illustrated in the example contrast of Table II\nfor 2-bit and 4-bit KL mixed precision quantized Transformer\nLMs on the Switchboard data.\nAn example application of KL divergence based mixed\nprecision quantization of Transformer and LSTM-RNN LMs\nare shown in Figure 1 and 2 respectively.\nTABLE I\nPerplexity performance of Switchboard data trained average 2-bit quantized\nTransformer LMs with their local precision setting learned using the KL\ndivergence metric of Equation (26), or the curvature based sensitivity metric\nof Equation (27). These two metrics were computed using either one single\nrandomly drawn mini-batch (batch size 32), a randomly drawn 50% subset,\nor all the training the data.\nLM quant. quant. #bit PPL\nestim. method 1 50% 100%\nTransformer\n- - 32 - - 40.7\nADMM KL 1.9 46.0 46.3 45.9\nHes 1.9 46.4 46.0 46.2\nFig. 1. An example of mixed precision quantization of a Transformer LM\nusing KL-divergence based mixed precision quantization. For the ﬁrst Trans-\nformer module positioned right after the embedding and position encoding\nlayer, its multi-head attention layer (green) uses 2-bit quantization while its\nfeed forward layer (orange) uses binary quantization precision, as determined\nby the KL-divergence based sensitivity measure.\nFig. 2. An example of auto-conﬁgured mixed precision quantization of a\n2-layer LSTM-RNN LM with its forget, input and output Sigmoid gates\nσ marked in orange, grey and dark blue, and cell gate tanh in brown\nrespectively. The forget gate (orange) parameters use 1-bit quantization, while\nthose of the output gate (dark blue) use 2-bit quantization, as determined by\nthe KL-divergence based sensitivity measure.\nB. Curvature Based Mixed Precision Quantization\nThe second approach to measure performance sensitivity\nto quantization examines the local training data log-likelihood\ncurvature. Under mild assumptions such that the parameters of\na DNN is twice differentiable and have converged to a local\noptimum, it has been shown in previous researches [35] that\nthe performance sensitivity to quantization, when using a given\nprecision setting, can be expressed as the squared quantization\nerror further weighted by the parameter Hessian matrix trace.\nFor any quantization f(·) being applied to the parameters Θ of\na Llayer Transformer LM, the total performance sensitivity is\ngiven by the following sum of Hessian trace weighted squared\nquantization error.\nΩHes =\nL∑\ni=1\nΩHes\ni =\nL∑\ni=1\nTr(Hi) ·||f(Θi) −Θi||2\n2 (27)\nIntuitively for each cluster of weight parameters (for ex-\nample of the same layer) to be quantized using a particular\nprecision bit width, given the same amount of model parameter\nchanges resulted from quantization, a smaller Hessian matrix\ntrace indicates a lower performance sensitivity to quantization.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE II\nPerformace contrast between 2-bit and 4-bit KL mixed precision quantized Transformer LMs on the Switchboard data constructed using either post\nprecision learning model ﬁne-tuning (LM 2, 4), or retraining from scratch (LM 1, 3).\nID LM quant. train #bit PPL eval2000 rt02 rt03 WER\nmethod method swbd callhm swbd1 swbd2 swbd3 fsh swbd avg.\n1\nTransformer KL\nre-train from scratch 1.9 46.2 7.2 13.4 8.4 10.6 14.0 8.3 14.3 10.9\n2 post-train ﬁne-tuning 45.9 7.2 13.4 8.3 10.7 14.0 8.3 14.3 10.9\n3 re-train from scratch 4 43.8 7.1 13.3 8.2 10.5 14.0 8.3 14.3 10.8\n4 post-train ﬁne-tuning 44.0 7.1 13.3 8.2 10.5 14.0 8.3 14.2 10.8\nGiven a target average quantization precision such as 2-\nbit, the local quantization setting, for example, used in each\nTransformer LM layer, should be selected such that the total\nperformance sensitivity in Equation (27) is minimized while\nsatisfying the target model size constraint. Similar to the\nKL divergence based mixed precision quantization, a set of\nprototype Transformer or LSTM-RNN LMs quantized using\nuniform precision, for example 1-bit, 2-bit, 4-bit and 8-bit, are\ntrained off-line ﬁrst using the ADMM optimization in Section\nIV . The log-likelihood curvature based performance sensitivity\nin Equation (27) can then be computed locally for each\nlayer using each quantization choice before taking the sum to\nproduce the combined net sensitivity measure for any mixed\nprecision setting being considered. For larger LSTM-RNN or\nTransformer LMs containing up to hundreds of millions of\nparameters, and large DNNs in general, directly computing\nthe Hessian matrix and its trace is computationally infeasible.\nIn order to address this issue, an efﬁcient stochastic linear\nalgebra approach based on the Huchinson’s Algorithm [89] is\nused to approximate the Hessian trace,\nTr(H) ≈ 1\nm\nm∑\ni=1\nz⊤\ni Hzi (28)\nwhere the expensive matrix multiplication between H and\nzi can be avoided, and efﬁciently computed using Hessian-\nfree approaches [35]. zi is a random vector sampled from\na Gaussian Distribution N(0,1). Following the previous re-\nsearch reported in [37], the maximum number of Hutchinson\nsteps set as m= 50 is found sufﬁcient to obtain an accurate\nHessian approximation for computing the curvature based\nquantization sensitivity of Equation (27), and used throughout\nthe experiments of this paper. Again for efﬁciency, a small\nsubset of the training data from a randomly drawn mini-batch\n(batch size 32) is used, in common with the previous KL\ndivergence metric. Further analysis on the relationship between\nthe sampled training data size and the resulting quantized\nTransformer LMs perplexity performance is shown in the\nlast line of Table I for the Switchboard data. An example\napplication of the above curvature performance sensitivity\nbased mixed precision quantization of a Transformer LM is\nshown in Figure 3.\nC. Architecture Search Based Mixed Precision Quantization\nThe third solution to automatically learn the optimal local\nquantization precision settings is to use mixed precision based\nneural architecture search (NAS) approaches. Neural architec-\nture search (NAS) techniques [90] can efﬁciently automate\nFig. 3. An example of auto-conﬁgured mixed precision quantization of a\nTransformer LM using curvature based sensitivity measure. For the ﬁrst Trans-\nformer module positioned right after the embedding and position encoding\nlayer, its multi-head attention layer (green) uses binary quantization while its\nfeed forward layer (orange) uses 4-bit quantization precision, as determined\nby the Hessian-trace weighted quantization sensitivity measure.\nEmbedding\nPositional Encoding\n+ …\nMultiheadAttention/FeedForwardMultiheadAttention/FeedForward\nFC/softmax\n…\n+\nMultiheadAttention1-bit2-bit4-bit8-bit\n + 1-bit2-bit4-bit8-bit\n +\nFeedForward\n…!\"#=0.1!$#=0.6!%#=0.1!&#=0.1\n'()*=ℱ-./+1234,#67#28!\"#9\"=0.1!$#9\"=0.2!%#9\"=0.5!&#9\"=0.2\nFig. 4. An example of auto-conﬁgured mixed precision quantization of\na Transformer LM using mixed precision architecture search. For the ﬁrst\nTransformer module, its multi-head attention layer is uses 2-bit quantization\n(green) given the associated selection weight of 0.6 while its feed forward\nlayer uses 4-bit quantization precision (orange) given the associated selection\nweight of 0.5, as the 1-best choice selected from the mixed precision NAS\nsuper-network.\nneural network structure designs that have been largely based\non expert knowledge or empirical choice to date. Among ex-\nisting NAS methods, differentiable neural architecture search\n(DARTS) [69]–[71], [91], [92] beneﬁts from a distinct advan-\ntage of being able to simultaneously compare a very large\nnumber of candidate architectures during search time. This\nis contrast to earlier and more expensive forms of NAS\ntechniques based on, for example, genetic algorithms [72] and\nReinforcement learning (RL) [65], [93], where explicit system\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\ntraining and evaluation are required for a large number of\ncandidate structures under consideration.\nNeural architecture search using DARTS is performed over\nan over-parameterized super-network containing paths con-\nnecting all candidate DNN structures to be considered. The\nsearch is transformed into the estimation of the weights as-\nsigned to each candidate neural architecture within the super-\nnetwork. The optimal architecture is obtained by pruning lower\nweighted paths. This allows both architecture selection and\ncandidate DNN parameters to be consistently optimized within\nthe same super-network model.\nThe key difference from conventional NAS tasks is that\ninstead of selecting over heterogeneous neural building struc-\ntures, for example, varying hidden layer left and right context\noffsets or projection layer dimensionality in LF-MMI trained\ntime delay neural networks (TDNNs) [91], different quantized\nneural building blocks, for example, LSTM or Transformer\nLM modules of different bit-widths are considered. This cru-\ncial difference requires the associated mixed precision quan-\ntization super-network to be specially designed. Such super-\nnetwork is constructed by ﬁrst separately training transformer\nLMs using uniform precision, for example 1-bit, 2-bit, 4-bit\nand 8-bit, using ADMM optimization, before connecting these\nuniform precision quantized Transformer LMs at each layer,\nwhere the system speciﬁc activation outputs are linearly com-\nbined using a set of quantization precision selection weights\nas in the following equation:\nol =\n∑\nn∈N\nexp(al\nn)∑exp(aln)F(fn(Θl),ol−1) (29)\nwhere al\nn is the architecture weights using n-bit quantization\nfor the l-th cluster of weight parameters. fn(·) represents\nn-bit quantization of the weight parameter Θl and F(·) is\nthe underlying layer’s activation function. The set of possi-\nble quantization precision setting, N = {1,2,4,8}, is used\nthroughout this paper. An example of such mixed precision\nTransformer super-network is shown in Figure 4.\nIn order to avoid the trivial selection of the longest, most\ngenerous quantization bit width, these precision selection\nweights learning can be further constrained by a model\ncomplexity penalty term with respect to the number of bits\nretained after quantization, in order to obtain a target average\nquantization precision, for example, 2-bit,\nΩNAS = Fce(Θ) + β\n∑\n(n,l)\nal\nn ·√n (30)\nwhere Fce(Θ) is the standard cross-entropy loss.\nVI. E XPERIMENTS\nIn this section the performance of mixed precision quan-\ntized LSTM-RNN and Transformer LMs are evaluated on\ntwo speech recognition systems both of which use state-of-\nthe-art LF-MMI sequence trained hybrid time delay neural\nnetworks (TDNNs) acoustic models with factored weights and\nadditional convolutional layers [78]. Speech perturbation based\ndata augmentation and i-Vector based speaker adaptation are\nalso employed. Modiﬁed KN smoothed 4-gram back-off LMs\nare used during the initial N-best list generation pass before\nvarious NNLMs are then applied in the following rescoring\nstage. In Section VI-A, the ﬁrst set of experiments conducted\non the Switchboard I (SWBD) corpus [81] are presented.\nIn Section VI-B another comparable set of experiments are\ncarried out on the AMI meeting room data [82].\nAll the LSTM-RMM LMs investigated in this paper consist\nof 2 LSTM layers while both the input word embedding and\nhidden layer sizes were set as 1024. All the Transformer\nLMs used in this paper contain 6 Transformer layers. The\ndimensionality of all query, key and value embedding and\nhidden vectors are set as 512 for each batch of data. All the\nmixed precision quantized LSTM-RNN LMs and Transformer\nLMs of this paper use layer or node level precision settings that\nare set either manually as equal bit-widths (1-bit, 2-bit, 4-bit\nor 8-bit), or automatically learned using the KL, curvature or\nNAS based mixed precision quantization methods of Sections\nV . All NNLMs were tained using a single NVIDIA Tesla V100\nV olta GPU card. Statistical signiﬁcance test was conducted at\nlevel α= 0.05 based on matched pairs sentence segment word\nerror (MAPSSWE) for recognition performance analysis. This\nis used to identify various “lossless” neural LM quantization\nconﬁgurations that incur no recognition performance degrada-\ntion, or statistically signiﬁcant word error rate (WER) increase.\nThe implementation used to evaluate the mixed precision\nquantization methods of this paper is exclusively based on the\nexisting low-bit quantized precisions that are already natively\nsupported by the NVidia Tesla V100 GPU. These include the\nuse of the Boolean and masking operators to implement 1-bit\nquantization, and the INT8 data type used to implement 2, 4\nand 8-bit quantization. In case of 2-bit and 4-bit quantization,\nextra padded bits of zero were also included.\nA. Experiments on Conversational Telephone Speech\nThe Switchboard I telephone speech corpus we use consists\nof approximately 300 hours of audio data released by LDC\n(LDC97S62). Following the Kaldi toolkit [95] and its CNN-\nTDNN recipe 1, LF-MMI trained CNN-TDNN acoustic mod-\nels [78] with data augmentation and i-Vector adaptation [79]\nwere then built. The CNN-TDNN network consisted of 6\nconvolutional layers followed by 8 context-splicing TDNN\nlayers with 1536 nodes per layer. A 160-dimensional factored\nlinear projection was employed prior to afﬁne transformation\nin each context-splicing layer other than the ﬁrst one. ReLU\nactivation functions were used, followed by batch normaliza-\ntion and dropout operations. Left and right context offsets\nof {−3,3} were also used in the context-splicing layers.\nMore detailed description fo the baseline system’s acoustic\nmodelling conﬁguration can be found in [96]. The Switchboard\nNIST Hub5’00, RT02 and RT03 evaluation sets were used.\nA 30K word recognition lexicon was used. Various LSTM\nand Transformer LMs trained on the Switchboard and Fisher\ntranscripts (LDC2004T19, LDC2005T19) were used to rescore\nthe 4-gram LM produced N-best lists ( N = 20).\n1) Experiments on LSTM-RNN LMs using baseline full\nprecision, various uniform and mixed quantization settings\nare shown in Table III. All WER changes as the result of\n1egs/swbd/s5c/local/chain/run cnn tdnn 1a.sh\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE III\nPerformance of the baseline full precision (LM 1), uniform precision quantized (LM 2-11) and mixed precision quantized RNNLMs with local layer or node\nlevel precision set either manually with equal bit-width (LM 12-15, 16-19) or automatically learned (LM 20-25) using KL, curvature (Hes) or NAS based\nmixed precision quantization methods of Section V on Switchboard NIST Hub5’00, RT02 and RT03. Equal weight interpolation of n-gram and neural LMs\nused in N-best restoring. All WER changes of no statistical signiﬁcance (MAPSSWE, α= 0.05) over the full precision baseline (LM 1) are marked with\n”∗”. Ofﬂine, post-training quantized LMs’ performance are also shown (LM 6-9).\nID quant. param. quant. #bit PPL eval2000 rt02 rt03 WER(%) model comp. eval. time\nprec. estim. method swbd callhm swbd1 swbd2 swbd3 fsh. swbd avg. size(MB) ratio (ms./word)\n0 4-gram - 85.3 9.5 17.8 11.4 15.1 19.5 12.4 19.3 15.0 - - -\n1 baseline (full precision) 32 40.7 7.1 13.2 8.2 10.7 13.8 8.4 14.1 10.8 183 - 0.226\n2 1 52.4 8.3 15.9 9.6 12.0 15.8 9.2 15.6 12.3 6.1 30.0 0.086\n3 BP [31], [32] 2 50.1 8.0 15.1 9.3 11.6 15.4 9.1 15.4 12.0 11.8 15.5 0.140\n4 (modiﬁed) 4 49.7 7.8 14.5 8.7 11.3 15.1 8.9 15.0 11.6 23.5 7.9 0.142\n5 8 44.2 7.4 13.9 8.6 11.1 14.6 8.7 14.8 11.3 46.2 3.9 0.141\n6 uniform 1 50.3 8.2 15.4 9.6 12.0 15.7 9.1 15.6 12.2 6.1 30.0 0.085\n7 prec. Post-training [94] 2 47.6 7.9 14.6 9.2 11.3 15.2 9.0 15.2 11.8 11.8 15.5 0.141\n8 Ofﬂine Quant 4 47.1 7.8 14.3 8.7 11.2 14.9 8.8 15.0 11.5 23.5 7.9 0.142\n9 8 44.8 7.3 13.6 8.4 11.1 14.4 8.7 14.6 11.2 46.2 3.9 0.141\n10 ADMM 1 49.9 8.0 14.8 9.6 11.8 15.7 9.1 15.4 12.1 6.1 30.0 0.083\n11 (global) 8 43.0 7.2 13.6 8.4 11.0 14.3 8.6 14.6 11.1 46.2 3.9 0.144\n12 ADMM manual 1 49.4 7.8 14.2 9.4 11.7 15.5 8.8 15.1 11.8 7.6 24.1 0.102\n13 (multiple deﬁne 2 45.3 7.6 14.0 9.1 11.4 15.2 8.7 14.9 11.6 13.3 13.6 0.155\n14 eql. #bit 4 44.7 7.3* 13.5* 8.6 11.0 14.5 8.6* 14.7 11.2 24.8 7.4 0.152\n15 node) 8 42.1 7.2* 13.3* 8.3* 10.9* 14.0* 8.4* 14.3* 10.9* 47.5 3.8 0.158\n16 mixed ADMM 1 50.3 8.0 15.3 9.5 11.8 15.6 9.1 15.4 12.1 6.1 30.0 0.088\n17 prec. (multiple 2 48.2 7.7 14.5 9.2 11.4 15.3 8.9 15.2 11.7 11.8 15.5 0.148\n18 eql. #bit 4 46.3 7.6 14.0 8.6 11.2 14.7 8.8 14.9 11.4 23.5 7.9 0.148\n19 layer) 8 43.9 7.3* 13.5* 8.4* 11.0* 14.3 8.6* 14.5 11.1* 46.2 3.9 0.147\n20 Hes 1.9 46.2 7.5 14.1 8.8 11.1 14.5 8.9 15.0 11.4 11.7 15.6 0.130\n21 ADMM 4 43.1 7.4 13.6 8.4 10.9 14.1 8.6 14.5 11.0 23.8 7.7 0.140\n22 (layer level KL 1.9 45.9 7.4* 13.9 8.5* 10.9* 14.2 8.7* 14.8 11.2 11.7 15.6 0.132\n23 variable) 4 41.9 7.3 13.4 8.3 10.8 14.0 8.5 14.4 10.9 23.7 7.7 0.139\n24 NAS 2.3 48.3 7.7 14.3 9.0 11.4 14.9 9.0 15.2 11.6 13.1 13.9 0.136\n25 4 46.8 7.6 14.1 8.5 11.2 14.6 8.7 14.7 11.3 23.4 7.8 0.141\nquantization that are of no statistical signiﬁcance (MAPSSWE,\nα = 0 .05) are marked by “ ∗”. Several trends can be found\nhere:\n• Among the uniform precision quantized LMs, the LSTM-\nRNN LMs of 1-bit and 8-bit precisions trained using the\nADMM optimization of Section IV consistently outper-\nform those of comparable bit-widths but built using the\nmodiﬁed BP algorithm (LM 10 and 11 vs. LM 2 and\n5 in Table III). In particular, for 1-bit quantization, the\nADMM optimization method produced WER reductions\nup to 1.1% absolute on the callhm data of the Hub5’00\nset over the traditional modiﬁed BP algorithm (LM 10\nvs. LM 2 in Table III). The advantage of the ADMM\nalgorithm in terms of convergence speed and training\nefﬁciency is further illustrated in left part of Figure 5.\n• The mixed precision quantized LSTM LMs (LM 12-\n25 in Table III) consistently outperform the uniform\nprecision quantized models (LM 2-11). For example,\ngiven the same quantization precision at approximately\n2-bit (compression ratio of 16 times over 32-bit full\nprecision), a wide range of mixed precision quantized\nLMs (LM 13, 17, 20, 22, 24 in Table III) produced\nstatistically signiﬁcant 0.3-0.8% average WER reduction\nacross three test data sets over with the comparable 2-bit\nuniform precision quantization baseline (LM 3).\n• Note that the ﬁrst two of these mixed precision quantized\nLSTM LMs (LM 13, 17 in Table III) used multiple layer\nor node level locally applied quantization tables of the\nsame bit-width manually set as 2-bit. As expected, a ﬁner\nmodelling granularity provided by the node level local\nquantization (LM 17) marginally outperformed the use of\nlayer level (LM 13) by 0.1% on average in WER, albeit\nwith a smaller compression ratio of 13.6 vs. 15.5. Similar\ntrends can be found when increasing the quantization\nprecision from 2-bit to 8-bit 2.\n• Among the three mixed precision quantization LSTM\nLMs with approximately 2-bit precision (LM 20, 22, 24\nin Table III), the KL quantized LM (LM 22) with a 1.9-\nbit average precision gives the lowest average WER of\n11.2%. These three mixed precision LMs’ respective local\nselection of quantization bit-widths at different gates of\nvarious LSTM layers are shown in Figure 6(a)-6(c). Note\nthat KL and curvature based quantization (LM 20, 22\nin Table III) both selected 8-bit precision for the 2nd\nLSTM layer’s output gate parameters, indicating a larger\nperformance sensitivity to quantization measured at the\nLSTM cells’ output gate.\n• Among all the quantized LSTM LMs marked with “ ∗”\nin Table III that incur no statistically signiﬁcant WER\nincrease against the full precision baseline (LM 1), the\n1.9-bit KL mixed precision quantized model (LM 22) also\nproduced the largest ”lossless” compression ratio of 15.6.\n2) Experiments on Transformer LMsdesigned using a com-\nparable set of contrasts previously used in Table III for LSTM-\nRNN LMs are shown in Table IV, where a large number\nof quantized Transformer LMs featuring different forms of\nmixed quantization schemes (LM 12 to 25 in Table IV, marked\nwith “∗”) produced lossless compression with no statistically\nsigniﬁcant WER changes relative to the baseline full precision\nmodel (LM 1). Several other trends that are similar to those\nfound in Table III on LSTM-RNN LMs are summarized below:\n2Considering the trade-off between WER and compression ratio for node\nand layer level local quantization precision settings, the subsequent experi-\nments will focus on using layer level mixed precision quantization.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nTABLE IV\nPerformance of the baseline full precision (LM 1), uniform precision quantized (LM 2-11) and mixed precision quantized Transformer LMs with local layer\nor node level precision set either manually with equal bit-width (LM 12-15, 16-19) or automatically learned (LM 20-25) using KL, curvature (Hes) or NAS\nbased mixed precision quantization methods of Section V on SWBD NIST Hub5’00, RT02 and RT03. Equal weight interpolation of n-gram and neural LMs\nused in N-best restoring. All WER changes of no statistical signiﬁcance (MAPSSWE, α= 0.05) over the full precision baseline (LM 1) are marked with\n”∗”. Ofﬂine, post-training quantized LMs’ performance are also shown (LM 6-9).\nID quant. param. quant. #bit PPL eval2000 rt02 rt03 WER(%) model comp. eval. time\nprec. estim. method swbd callhm swbd1 swbd2 swbd3 fsh. swbd avg. size(MB) ratio (ms./word)\n0 4-gram - 85.3 9.5 17.8 11.4 15.1 19.5 12.4 19.3 15.0 - - -\n1 baseline (full precision) 32 40.7 7.1 13.4 8.2 10.5 14.0 8.3 14.3 10.8 106 - 0.127\n2 1 52.4 7.5 14.2 8.5 11.0 14.7 9.0 14.9 11.4 3.6 29.5 0.043\n3 BP [31], [32] 2 50.1 7.5 14.1 8.5 11.0 14.6 8.9 14.8 11.3 7.2 14.7 0.059\n4 (modiﬁed) 4 49.7 7.4* 13.9* 8.4* 10.8* 14.4* 8.7* 14.6* 11.0 13.8 7.7 0.063\n5 8 44.2 7.3* 13.7* 8.4* 10.8* 14.3* 8.6* 14.5* 10.9 27.0 3.9 0.065\n6 uniform 1 50.7 7.4 14.1 8.3 10.9 14.4 8.8 14.6 11.2 3.6 29.5 0.044\n7 prec. Post-training [94] 2 49.2 7.3 13.8 8.3 10.7 14.1 8.5 14.5 11.1 7.2 14.7 0.061\n8 Ofﬂine Quant 4 47.3 7.2 13.7 8.3 10.6 14.0 8.4 14.4 10.9 13.8 7.7 0.059\n9 8 45.0 7.2 13.5 8.2 10.6 14.0 8.3 14.4 10.9 27.0 3.9 0.062\n10 ADMM 1 51.2 7.5 14.1 8.5 10.9 14.5 8.8 14.7 11.3 3.6 29.5 0.046\n11 (global) 8 44.0 7.2 13.6 8.4 10.8 14.2 8.5 14.5 10.9 27.0 3.9 0.062\n12 ADMM manual 1 49.4 7.3* 13.8* 8.3* 10.8* 14.2* 8.6* 14.5* 11.0* 4.3 24.7 0.059\n13 (multiple deﬁne 2 45.3 7.2* 13.5* 8.3* 10.6* 14.0* 8.4* 14.4* 10.9* 7.7 13.8 0.076\n14 eql. #bit 4 44.7 7.1* 13.5* 8.2* 10.5* 14.0* 8.3* 14.3* 10.8* 15.3 7.0 0.075\n15 node) 8 42.1 7.2* 13.5* 8.3* 10.6* 14.0* 8.3* 14.4* 10.9* 28.5 3.7 0.076\n16 mixed ADMM 1 50.3 7.3 14.0 8.3* 10.8* 14.3* 8.7* 14.6* 11.2* 3.7 29.5 0.048\n17 prec. (multiple 2 48.2 7.2* 13.6* 8.3* 10.6* 14.0* 8.5* 14.4* 11.0* 7.2 14.7 0.070\n18 eql. #bit 4 46.3 7.2* 13.6* 8.2* 10.7* 14.0* 8.3* 14.3* 10.9* 13.8 7.7 0.072\n19 layer) 8 43.9 7.2* 13.5* 8.3* 10.7* 14.0* 8.3* 14.4* 10.9* 27.0 3.9 0.072\n20 Hes 1.9 46.2 7.2* 13.5* 8.3* 10.7* 14.0* 8.3* 14.4* 10.9* 7.0 15.1 0.062\n21 4 44.2 7.2 13.4 8.2 10.6 14.0 8.3 14.3 10.8 13.7 7.7 0.069\n22 ADMM KL 1.9 45.9 7.2* 13.4* 8.3* 10.7* 14.0* 8.3* 14.3* 10.9* 7.0 15.1 0.064\n23 (layer level 4 44.0 7.1 13.3 8.2 10.5 14.0 8.3 14.2 10.8 13.7 7.7 0.070\n24 variable) NAS 2.3 48.3 7.3* 13.7* 8.4* 10.8* 14.0* 8.4* 14.5* 11.1* 7.5 14.2 0.065\n25 4 45.8 7.2 13.5 8.2 10.6 14.1 8.3 14.3 10.9 13.4 7.9 0.068\n41465156616671\n1 112131415161718191101111\nLSTM-RNNLMPPL Baseline: 15 min/epoch, 13 epochsBin-ADMM: 21 min/epoch, 23 epochsBin-modBP: 14 min/epoch, 114 epochs\n(a) LSTM-RNN LMs\n41465156616671768186\n1 11 21 31 41 51\nTransformerLMPPL\nBaseline: 21min/epoch, 6 epochsBin-ADMM: 35 min/epoch, 16 epochsBin-modBP: 19min/epoch, 52 epochs (b) Transformer LMs\nFig. 5. Perplexity and convergence speed comparison on LSTM-RNN LMs (ﬁgure (a), showing LM 1, 2, 6 in Table III) and Transformer LMs (ﬁgure (b),\nshowing LM 1,2,6 in Table IV) between baseline full precision models ((Baseline), binarized models trained using modiﬁed back-propagation (Bin-modBP) [31],\n[32] and ADMM trained binarized models ( Bin-ADMM) on Switchboard validation data.\n• Among the uniform precision quantized LMs, the Trans-\nformer LMs of 1-bit and 8-bit precision trained using\nADMM optimization of Section IV marginally outper-\nform, or are comparable in performance to those two of\nsame precisions using the modiﬁed BP algorithm (LM 10\nand 11 vs. LM 2 and 5 in Table IV). The advantage of\nthe ADMM based Transformer LM estimation in terms\nof convergence speed and training efﬁciency is shown in\nright part of Figure 5.\n• The mixed precision quantized Transformer LMs (LM\n8-18 in Table IV) again consistently outperform the\nuniform precision quantized baselines (LM 2-11) across\ndifferent lower bit-widths ranging from 1-bit, 2-bit to 4-\nbit. For example, given the same quantization precision\nat approximately 2-bit (compression ratio of 16 times),\na wide range of mixed precision quantized Transformer\nLMs (LM 13, 20, 22 in Table IV) produced statistically\nsigniﬁcant 0.4% absolute WER reduction averaged across\nall data sets over with the comparable 2-bit uniform\nprecision quantization LM (LM 3 in Table IV).\n• Among the three mixed precision quantized Transformer\nLMs with approximately 1.9-bit precision (LM 20, 22,\n24 in Table IV), the KL and curvature based mixed pre-\ncision quantized Transformer LMs (LM 22) with a 1.9-\nbit average precision produced the lowest average WER\nof 10.9%, as well as the largest “lossless” compression\nratio of 15.1 among all the quantized Transformer LMs\nmarked with “ ∗” in Table IV that incur no statistically\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nTABLE V\nPerformance of the baseline full precision (LM 1), uniform precision quantized (LM 2-7) and mixed precision quantized RNNLMs with local layer or node\nlevel precision set either manually with equal bit-width (LM 8-11, 12-15) or automatically learned (LM 16-21) using KL, Hes or NAS based mixed precision\nquantization methods of Section V on AMI dev and eval sets of ihm, mdm8 and sdm1 conditions. Equal weight interpolation of n-gram and neural LMs used\nin N-best restoring. All WER changes of no statistical signiﬁcance (MAPSSWE, α= 0.05) over the full precision baseline (LM 1) are marked with ” ∗”\nID param. quant. quant. #bit PPL ihm mdm8 sdm1 model comp. eval. time\nprec. estim. method dev eval dev eval dev eval size(MB) ratio (ms./word)\n0 4-gram - 93.6 18.1 18.4 33.5 35.3 34.2 40.5 - - -\n1 baseline (full precision) 32 57.6 16.4 16.2 27.7 30.8 29.9 34.7 248 - 0.290\n2 1 78.6 17.2 16.9 32.5 34.1 33.9 40.1 8.2 30.2 0.108\n3 BP [31], [32] 2 67.9 16.8 16.6 30.1 33.2 31.6 37.9 16.2 15.3 0.177\n4 uniform (modiﬁed) 4 62.6 16.7 16.5 29.3 32.1 30.7 35.7 31.5 7.9 0.179\n5 prec. 8 60.3 16.5* 16.3* 28.6 31.8 30.4 35.3 62.8 3.9 0.175\n6 ADMM 1 74.2 17.0 16.8 31.2 33.7 32.9 39.6 8.2 30.2 0.110\n7 (global) 8 59.3 16.6* 16.3* 28.5 31.6 30.4 35.2 62.8 3.9 0.176\n8 ADMM manual 1 68.2 16.9 16.7 30.0 32.6 32.4 38.6 9.5 26.1 0.145\n9 (multiple deﬁne 2 61.9 16.6* 16.5* 29.1 31.9 30.9 35.9 17.6 14.1 0.205\n10 eql. #bit 4 58.5 16.4* 16.3* 28.6 31.2 30.5 35.2 33.0 7.5 0.207\n11 node) 8 57.9 16.4* 16.3* 28.4 31.1* 30.1* 34.9* 64.5 3.8 0.204\n12 mixed ADMM 1 70.3 17.0 16.8 30.9 33.2 32.7 39.1 8.4 30.1 0.116\n13 prec. (multiple 2 63.1 16.7 16.6 29.4 32.1 31.2 36.2 16.2 15.3 0.189\n14 eql. #bit 4 59.2 16.6* 16.4* 28.9 31.7 30.9 35.4 31.5 7.9 0.190\n15 layer) 8 58.4 16.5* 16.3* 28.5 31.4 30.4 35.2 62.8 3.9 0.192\n16 Hes 1.9 60.7 16.5* 16.4* 28.6 31.8 30.8 35.3 16.0 15.5 0.165\n17 ADMM 4.0 59.0 16.5* 16.4* 28.5 31.2 30.3* 35.1 31.3 7.9 0.185\n18 (layer KL 1.8 59.2 16.5* 16.3* 28.5 31.6 30.6 35.2 15.8 15.7 0.167\n19 level 3.8 58.8 16.5* 16.3* 28.4 31.3 30.2* 35.0* 30.8 8.1 0.183\n20 variable) NAS 2.0 63.6 16.8 16.5 28.9 32.0 31.1 35.9 16.2 15.3 0.171\n21 3.8 61.7 16.5* 16.4* 28.8 31.6 30.7 35.5 30.8 8.1 0.186\nsigniﬁcant WER increase over the full precision baseline\n(LM 1 in Table IV). Their respective local selection\nof quantization bit-widths at different sublayers within\nvarious Transformer layers are shown in Figure 6(e)-\n6(f), where an expected general trend is found that both\nthe lower Transformer layers heavily tasked with de-\nnoising the data, and the top Transformer layer used\nto immediately predict detailed word probabilities over\na large vocabulary, require longer quantization precision\nsettings (2-bit to 8-bit) than those middle Transformer\nlayers (1-bit to 4-bit).\n• It is also worth noting the 4-bit ADMM node level\nlocally quantized Transformer LM (LM 14 in Table IV),\nand the 4-bit KL or curvature mixed precision quantized\nTransformer LM (LM 21, 23 in Table IV), both produced\nno WER degradation against the full precision model\n(LM 1), albeit with a smaller compression ratio of 7.0.\nB. Experiments on AMI Meeting Room Data\nThe Augmented Multi-party Interaction (AMI) speech cor-\npus consists of approximately 100 hours of audio data col-\nlected using both headset microphone and distant microphone\narrays from the meeting environment. Following the Kaldi\nrecipe3, three LF-MMI trained [78] acoustic models with\nspeech perturbation based data augmentation and i-Vector\nbased speaker adaptation [79] were then constructed. The\nAMI 8.9-hour dev and 8.7-hour eval sets recorded under\nclose talking microphone ( ihm), single distant microphone\n(sdm) and multiple distant microphones ( mdm) were used.\nA 47K word recognition lexicon was used. Various LSTM\nand Transformer LMs based on the same conﬁgurations as\nthose on the Switchboard data in Table III and IV were trained\n3egs/ami/s5c/local/chain/run tdnn.sh\nusing a mixture of text sources of 15M words including Fisher\ntranscripts and 3 times of AMI transcriptions before being used\nto rescore the 3-gram LM produced N-best lists ( N = 20 ).\nAll other experimental conﬁgurations remain the same as the\nSwitchboard experiments of Section VI-A.\nThe mixed precision quantization experiments for RNNLMs\nand Transformer LMs on AMI meeting room data are shown\nin Table V and VI . The following trends similar to those\npreviously found on the Switchboard data are again observed\nin Table V and VI for LSTM-RNN and Transformer LMs.\n• The mixed precision quantized LSTM and Transformer\nLMs (LM 8-21 in Table V and Table VI) consistently\noutperform the uniform precision quantized models (LM\n2-7 in Table V and Table VI). For example, given the\nsame quantization precision at approximately 4-bit (com-\npression ratio of 8 times over 32-bit full precision), a wide\nrange of mixed precision quantized LMs (LM 10, 14, 17,\n19, 21 in Table V and Table VI) produced statistically\nsigniﬁcant WER reductions up to 0.9% absolute (LM 19\nvs. LM 4 on mdm dev in Table V) for LSTM LMs, and\nup to 0.7% absolute WER reduction (LM 19 vs. LM 4\non mdm dev in Table VI) for Transformer LMs over the\ncomparable 4-bit uniform precision quantization (LM 4).\n• Among the six mixed precision quantization LSTM and\nTransformer LMs with approximately 4-bit precision (LM\n17, 19, 21 in Table V and Table VI), both the KL quan-\ntized LSTM and Transformer LMs (LM 19 in Table V\nand Table VI) with their corresponding 3.8-bit and 4.0-bit\naverage precision give the lowest WERs. Both the KL and\ncurvature based mixed precision quantized Transformer\nLMs (LM 17, 19 in Table VI) with 4-bit average precision\nproduced no statistically signiﬁcant recognition error rate\nincrease relative to the full precision Transformer LM\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE VI\nPerformance of the baseline full precision (LM 1), uniform precision quantized (LM 2-7) and mixed precision quantized Transformer LMs with local layer\nor node level precision set either manually with equal bit-width (LM 8-11, 12-15) or automatically learned (LM 16-21) using KL, Hes or NAS based mixed\nprecision quantization methods of Section V on AMI dev, eval sets of ihm, mdm8 and sdm1 conditions. Equal weight interpolation of n-gram and neural LMs\nused in N-best restoring. All WER changes of no statistical signiﬁcance (MAPSSWE, α= 0.05) over the full precision baseline (LM 1) are marked with ” ∗”\nID quant. quant. quant. #bit PPL ihm mdm8 sdm1 model comp. eval. time\nprec. estim. method dev eval dev eval dev eval size(MB) ratio (ms.word)\n0 4-gram - 93.6 18.1 18.4 33.5 35.3 34.2 40.5 - - -\n1 baseline (full precision) 32 46.9 16.1 15.8 27.5 30.6 29.8 34.5 138 - 0.172\n2 1 64.3 16.7 16.5 29.8 33.0 31.3 37.2 4.7 29.4 0.058\n3 BP [31], [32] 2 58.4 16.4 16.3 28.7 31.2 30.4 35.3 9.0 15.3 0.082\n4 uniform (modiﬁed) 4 54.1 16.3* 16.1* 28.3 31.0 30.3 35.1 17.9 7.7 0.080\n5 prec. 8 52.7 16.2* 16.0* 28.1 30.8 30.1 34.8 35.0 3.9 0.083\n6 ADMM 1 59.9 16.6 16.5 29.7 32.6 31.1 37.0 4.7 29.5 0.062\n7 (global) 8 49.7 16.2* 16.0* 28.0 30.7* 30.0* 34.8* 35.0 3.9 0.086\n8 ADMM manual 1 55.3 16.5 16.4 29.2 31.9 30.8 36.4 5.9 23.4 0.070\n9 (multiple deﬁne 2 48.8 16.3* 16.2* 28.4 30.9 30.4 35.0 10.3 13.4 0.098\n10 eql. #bit 4 49.0 16.1* 15.9* 27.9 30.7 30.0* 34.7* 19.4 7.1 0.098\n11 node) 8 48.3 16.1* 15.8* 27.8* 30.7* 29.9* 34.6* 36.3 3.8 0.096\n12 mixed ADMM 1 57.4 16.6 16.5 29.4 32.1 31.0 36.5 4.8 28.8 0.066\n13 prec. (multiple 2 52.9 16.3 16.3 28.5 31.1 30.4 35.1 9.1 15.2 0.092\n14 eql. #bit 4 49.4 16.1* 16.0* 27.8* 30.8* 30.0* 34.6* 17.9 7.7 0.093\n15 layer) 8 48.3 16.1* 15.9* 27.9 30.7 30.0* 34.7* 35.0 3.9 0.092\n16 Hes 1.8 48.1 16.1* 16.0* 28.0 30.8 30.1* 34.7* 8.7 15.9 0.075\n17 ADMM 4.0 49.3 16.1* 15.9* 27.8* 30.7* 29.9* 34.6* 17.8 7.8 0.086\n18 (layer KL 1.8 47.6 16.2* 16.1* 28.1 30.9 30.1* 34.8* 8.7 15.9 0.074\n19 level 4.0 48.9 16.1* 15.8* 27.6* 30.7* 29.9* 34.6* 17.8 7.8 0.085\n20 variable) NAS 1.9 49.8 16.3* 16.2* 28.3 31.0 30.3 35.0 9.0 15.3 0.077\n21 3.9 49.7 16.2* 16.1* 28.1 30.7* 30.0* 34.9 17.7 7.8 0.087\n(LM 1 in Table VI) across all test sets.\nVII. C ONCLUSION\nThis paper presents a set of novel mixed precision based\nneural network LM quantization techniques for LSTM-RNNs\nand Transformers. In order to account for the locally varying\nperformance sensitivity to low-bit quantization, the optimal\nlocal precision settings are automatically learned by either\nminimizing the KL-divergence or log-likelihood curvature\nbased performance sensitivity measures, or derived using\nmixed precision neural architecture search. Quantized LSTM-\nRNN and Transformer LM parameters are estimated efﬁciently\nusing alternating direction methods of multipliers based op-\ntimization, to address the low convergence speed issue when\ndirectly applying gradient descent methods to estimate discrete\nquantized neural network parameters. Experimental results\nconducted on two state-of-the-art speech recognition tasks\nsuggest the proposed mixed precision neural network LM\nquantization methods outperform traditional uniform precision\nbased quantization approaches, and can produce “lossless”\nquantization and large model size compression ratios of up\nto around 16 times over the full precision LSTM-RNN and\nTransformer LM baselines while incurring no statistically\nsigniﬁcant recognition accuracy degradation.\nAll of the three automatic mixed precision conﬁguration\napproaches proposed in this paper aim to minimize the per-\nformance sensitivity to quantization errors. To the best of our\nknowledge, they are the ﬁrst research work to apply mixed\nprecision quantization to neural language models for speech\nrecognition tasks. For all the three approaches such perfor-\nmance sensitivity measure is related to the empirical training\ndata log-likelihood degradation given a particular quantization\nprecision bit-width setting relative to the full precision model.\nThe precise manner with which such measure is computed\ndiffers among the three. This leads to the measures of infor-\nmation loss via KL-divergence, rate of log-likelihood degra-\ndation using curvature, and penalized log-likelihood based\nmixed precision architecture search approaches respectively.\nExperimental results obtained both the Switchboard and AMI\ntasks shown in Tables II to V on LSTM-RNN and Transformer\nLMs suggest the performance between the KL-divergence and\nlog-likelihood curvature methods is consistently close. This is\nexpected as both are measuring similar forms of log-likelihood\nbased quantization loss, while an extra precision penalty term\nis introduced in the mixed precision NAS cost function. The\nlarger performance difference between the mixed precision\narchitecture search approach and the other two methods, for\nexample, shown in Tables II and III, may be attributed to\nthe use of Softmax function based architecture weights in\nthe DARTS super-network. When similar architecture weights\nare obtained, the confusion over different candidate precision\nsettings increases and search errors may occur.\nFuture research will focus on improving mixed precision\nquantization methods and their application to other neural\nnetwork components of speech recognition systems and end-\nto-end based neural architectures.\nACKNOWLEDGMENT\nThis research is supported by Hong Kong Research Grants\nCouncil GRF grant No. 14200218, 14200220, 14200021,\nInnovation and Technology Fund grant No. ITS/254/19, and\nShun Hing Institute of Advanced Engineering grant No. MMT-\np1-19.\nREFERENCES\n[1] S. Katz, “Estimation of probabilities from sparse data for the language\nmodel component of a speech recognizer,” IEEE transactions on acous-\ntics, speech, and signal processing , vol. 35, no. 3, pp. 400–401, 1987.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n0123456789\nInputLSTM-1LSTM2Output\nPRECISIONSETTING#BIT\nΘf: forget gateΘi: input gateΘc: cell gateΘo: output gateΘ of Input/output linear layer\n(a) KL quantized LSTM LM (avg. 2-bit)\n0123456789\nInputLSTM-1LSTM2Output\nPRECISIONSETTING#BIT\nΘf: forget gateΘi: input gateΘc: cell gateΘo: output gateΘ of Input/output linear layer (b) Curvature quantized LSTM LM (avg. 2-bit)\n0123456789\nInputLSTM-1LSTM2Output\nPRECISIONSETTING#BIT\nΘf: forget gateΘi: input gateΘc: cell gateΘo: output gateΘ of Input/output linear layer (c) NAS quantized LSTM LM (avg. 2-bit)\n0123456789\nInputLayer1Layer2Layer3Layer4Layer5Layer6Output\nPRECISIONSETTING#BIT\n[Θq Θk Θv]: query/key/value embeddingΘh: multihead attentionΘ1: feedforward Θ2: feedforward Θ of Input/output linear layer\n(d) KL quantized Transformer LM (avg. 2-bit)\n0123456789\nInputLayer1Layer2Layer3Layer4Layer5Layer6Output\nPRECISIONSETTING#BIT\n[Θq Θk Θv]: query/key/value embeddingΘh: multihead attentionΘ1: feedforwardΘ2: feedforwardΘ of Input/output linear layer (e) Curvature quantized Transformer LM (avg. 2-\nbit)\n0123456789\nInputLayer1Layer2Layer3Layer4Layer5Layer6Output\nPRECISIONSETTING#BIT\n[Θq Θk Θv]: query/key/value embeddingΘh: multihead attentionΘ1: feedforwardΘ2: feedforwardΘ of Input/output linear layer\n(f) NAS quantized Transformer LM (avg. 2-bit)\nFig. 6. Number of bits used in local quantization of SWBD data trained neural LMs automatically derived using KL, curvature and NAS based mixed\nprecision quantization of Section V for: 1) LSTM-RNN LMs ((a)-(c), also shown as LM 16-18 in Table III) at their input/output linear layers (light blue),\nand individual forget (dark blue), input (orange), cell (grey) and output (yellow) gates inside LSTM layers; 2) Transformer LMs ((d)-(f), also shown as LM\n16-18 in Table IV) at their input/output linear layers (light blue), and 1st/2nd multihead attention sublayers (dark blue/orange), following 1st/2nd feedforward\nsublayers (grey/yellow) within each of 6 Transformer layers.\n[2] S. F. Chen and J. Goodman, “An empirical study of smoothing tech-\nniques for language modeling,” Computer Speech & Language , vol. 13,\nno. 4, pp. 359–394, 1999.\n[3] Y . Bengio et al., “A neural probabilistic language model,” The journal\nof machine learning research , vol. 3, pp. 1137–1155, 2003.\n[4] H. Schwenk, “Continuous space language models,” Computer Speech &\nLanguage, vol. 21, no. 3, pp. 492–518, 2007.\n[5] T. Mikolov et al., “Recurrent neural network based language model,” in\nProc. Interspeech, 2010.\n[6] E. Arisoy et al. , “Deep neural network language models,” in NAACL,\npp. 20–28, 2012.\n[7] H.-S. Le et al., “Structured output layer neural network language models\nfor speech recognition,” TASLP, vol. 21, no. 1, pp. 197–206, 2012.\n[8] M. Sundermeyer et al. , “From feedforward to recurrent lstm neural\nnetworks for language modeling,” TASLP, vol. 23, no. 3, pp. 517–529,\n2015.\n[9] X. Chen et al. , “Efﬁcient training and evaluation of recurrent neural\nnetwork language models for automatic speech recognition,” TASLP,\nvol. 24, no. 11, pp. 2146–2157, 2016.\n[10] X. Chen et al., “Exploiting future word contexts in neural network lan-\nguage models for speech recognition,” TASLP, vol. 27, no. 9, pp. 1444–\n1454, 2019.\n[11] K. Irie et al. , “Language modeling with deep transformers,” Proc.\nInterspeech, 2019.\n[12] K. Li et al., “An empirical study of transformer-based neural language\nmodel adaptation,” in ICASSP, pp. 7934–7938, IEEE, 2020.\n[13] E. Beck et al., “Lvcsr with transformer language models,” Proc. Inter-\nspeech, pp. 1798–1802, 2020.\n[14] P. Baquero-Arnal et al. , “Improved hybrid streaming asr with trans-\nformer language models,” Proc. Interspeech, pp. 2127–2131, 2020.\n[15] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[16] A. Vaswani et al., “Attention is all you need,” in NIPS, 2017.\n[17] J. Xu et al. , “Mixed precision quantization of transformer language\nmodels for speech recognition,” in ICASSP, pp. 7383–7387, IEEE, 2021.\n[18] J. Cheng et al., “Long short-term memory-networks for machine read-\ning,” in EMNLP, pp. 551–561, 2016.\n[19] Z. Lin et al., “A structured self-attentive sentence embedding,” in ICLR,\n2017.\n[20] A. Parikh et al., “A decomposable attention model for natural language\ninference,” in EMNLP, pp. 2249–2255, 2016.\n[21] K. He et al., “Deep residual learning for image recognition,” in CVPR,\npp. 770–778, 2016.\n[22] J. L. Ba et al., “Layer normalization,” arXiv preprint arXiv:1607.06450,\n2016.\n[23] J. Gehring et al. , “Convolutional sequence to sequence learning,” in\nICML, pp. 1243–1252, PMLR, 2017.\n[24] A. Zeyer et al., “A comparison of transformer and lstm encoder decoder\nmodels for asr,” in ASRU, pp. 8–15, IEEE, 2019.\n[25] Y . Wang et al., “Transformer-based acoustic modeling for hybrid speech\nrecognition,” in ICASSP, pp. 6874–6878, IEEE, 2020.\n[26] Q. Zhang et al. , “Transformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,” in ICASSP,\npp. 7829–7833, IEEE, 2020.\n[27] S. Karita et al., “A comparative study on transformer vs rnn in speech\napplications,” in ASRU, pp. 449–456, IEEE, 2019.\n[28] A. Graves, “Sequence transduction with recurrent neural networks,” in\nICML, 2012.\n[29] W. Chan et al. , “Listen, attend and spell: A neural network for large\nvocabulary conversational speech recognition,” in ICASSP, pp. 4960–\n4964, IEEE, 2016.\n[30] Z. T ¨uske et al. , “Single headed attention based sequence-to-sequence\nmodel for state-of-the-art results on switchboard,” Proc. Interspeech ,\npp. 551–555, 2020.\n[31] D. Soudry et al., “Expectation backpropagation: Parameter-free training\nof multilayer neural networks with continuous or discrete weights.,” in\nNIPS, vol. 1, p. 2, 2014.\n[32] M. Courbariaux et al. , “Binaryconnect: training deep neural networks\nwith binary weights during propagations,” in NIPS, pp. 3123–3131,\n2015.\n[33] I. Hubara et al., “Quantized neural networks: Training neural networks\nwith low precision weights and activations,” The Journal of Machine\nLearning Research, vol. 18, no. 1, pp. 6869–6898, 2017.\n[34] M. Rastegari et al. , “Xnor-net: Imagenet classiﬁcation using binary\nconvolutional neural networks,” in ECCV, pp. 525–542, Springer, 2016.\n[35] Z. Dong et al., “Hawq: Hessian aware quantization of neural networks\nwith mixed-precision,” in ICCV, pp. 293–302, 2019.\n[36] K. Wang et al. , “Haq: Hardware-aware automated quantization with\nmixed precision,” in CVPR, pp. 8612–8620, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[37] Z. Dong et al. , “Hawq-v2: Hessian aware trace-weighted quantization\nof neural networks,” NeurIPS, 2019.\n[38] S. Han et al., “Learning both weights and connections for efﬁcient neural\nnetworks,” in NIPS, pp. 1135–1143, 2015.\n[39] S. Han et al. , “Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding,” ICLR, 2015.\n[40] H. Mao et al., “Exploring the regularity of sparse structure in convolu-\ntional neural networks,” CVPR, 2017.\n[41] B. Liu et al., “Sparse convolutional neural networks,” inCVPR, pp. 806–\n814, 2015.\n[42] W. Wen et al., “Learning structured sparsity in deep neural networks,”\nin NIPS, 2016.\n[43] C. Szegedy et al. , “Rethinking the inception architecture for computer\nvision,” in CVPR, pp. 2818–2826, 2016.\n[44] C. Szegedy et al. , “Inception-v4, inception-resnet and the impact of\nresidual connections on learning,” in AAAI, vol. 31, 2017.\n[45] G. Hinton et al., “Distilling the knowledge in a neural network,” NIPS,\n2014.\n[46] A. Romero et al., “Fitnets: Hints for thin deep nets,” ICLR, 2015.\n[47] Y . Chebotar and A. Waters, “Distilling knowledge from ensembles of\nneural networks for speech recognition.,” inProc. Interspeech, pp. 3439–\n3443, 2016.\n[48] M. Jaderberg et al. , “Speeding up convolutional neural networks with\nlow rank expansions,” BMVC, 2014.\n[49] V . Lebedev et al. , “Speeding-up convolutional neural networks using\nﬁne-tuned cp-decomposition,” in ICLR, 2015.\n[50] C. Tai et al., “Convolutional neural networks with low-rank regulariza-\ntion,” ICLR, 2016.\n[51] V . Sindhwani et al. , “Structured transforms for small-footprint deep\nlearning,” NIPS, 2015.\n[52] F. N. Iandola et al., “Squeezenet: Alexnet-level accuracy with 50x fewer\nparameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360 ,\n2016.\n[53] X. Zhang et al., “Shufﬂenet: An extremely efﬁcient convolutional neural\nnetwork for mobile devices,” in CVPR, pp. 6848–6856, 2018.\n[54] N. Ma et al. , “Shufﬂenet v2: Practical guidelines for efﬁcient cnn\narchitecture design,” in ECCV, pp. 116–131, 2018.\n[55] T. N. Sainath et al. , “Low-rank matrix factorization for deep neural\nnetwork training with high-dimensional output targets,” in 2013 IEEE\ninternational conference on acoustics, speech and signal processing ,\npp. 6655–6659, IEEE, 2013.\n[56] Y . Wang et al., “Small-footprint high-performance deep neural network-\nbased speech recognition using split-vq,” in ICASSP, pp. 4984–4988,\nIEEE, 2015.\n[57] X. Liu et al. , “Binarized lstm language model,” in Proceedings of\nthe Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pp. 2113–2121, 2018.\n[58] K. Yu et al., “Neural network language model compression with product\nquantization and soft binarization,” TASLP, vol. 28, pp. 2438–2449,\n2020.\n[59] Y . He et al. , “Streaming end-to-end speech recognition for mobile\ndevices,” in ICASSP, pp. 6381–6385, IEEE, 2019.\n[60] Ł. Dudziak et al., “Shrinkml: End-to-end asr model compression using\nreinforcement learning,” Proc. Interspeech, pp. 2235–2239, 2019.\n[61] K. C. Sim et al. , “An investigation into on-device personalization of\nend-to-end automatic speech recognition models,” Proc. Interspeech ,\npp. 774–778, 2019.\n[62] Y .-m. Qian and X. Xiang, “Binary neural networks for speech recog-\nnition,” Frontiers of Information Technology & Electronic Engineering ,\nvol. 20, no. 5, pp. 701–715, 2019.\n[63] K. O. Stanley and R. Miikkulainen, “Evolving neural networks through\naugmenting topologies,” Evolutionary computation , vol. 10, no. 2,\npp. 99–127, 2002.\n[64] K. Kandasamy et al., “Neural architecture search with bayesian optimi-\nsation and optimal transport,” in NeurIPS, 2018.\n[65] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement\nlearning,” ICLR, 2017.\n[66] B. Baker et al., “Designing neural network architectures using reinforce-\nment learning,” ICLR, 2017.\n[67] H. Cai et al., “Efﬁcient architecture search by network transformation,”\nin AAAI, vol. 32, 2018.\n[68] Z. Zhong et al. , “Practical block-wise neural network architecture\ngeneration,” in CVPR, pp. 2423–2432, 2018.\n[69] H. Liu et al., “Darts: Differentiable architecture search,” in ICLR, 2018.\n[70] S. Xie et al., “Snas: stochastic neural architecture search,” ICLR, 2019.\n[71] H. Cai et al., “Proxylessnas: Direct neural architecture search on target\ntask and hardware,” ICLR, 2019.\n[72] Z. Cai and N. Vasconcelos, “Rethinking differentiable search for mixed-\nprecision neural networks,” in CVPR, pp. 2349–2358, 2020.\n[73] S. Boyd et al., Distributed optimization and statistical learning via the\nalternating direction method of multipliers . Now Publishers Inc, 2011.\n[74] C. Leng et al., “Extremely low bit neural network: Squeeze the last bit\nout with admm,” in AAAI, vol. 32, 2018.\n[75] R. Ma et al. , “Highly efﬁcient neural network language model com-\npression using soft binarization training,” in ASRU, pp. 62–69, IEEE,\n2019.\n[76] Y . Boo and W. Sung, “Fixed-point optimization of transformer neural\nnetwork,” in ICASSP, pp. 1753–1757, IEEE, 2020.\n[77] J. Xu et al., “Low-bit quantization of recurrent neural network language\nmodels using alternating direction methods of multipliers,” in ICASSP,\npp. 7939–7943, IEEE, 2020.\n[78] D. Povey et al., “Purely sequence-trained neural networks for asr based\non lattice-free mmi.,” in Proc. Interspeech, pp. 2751–2755, 2016.\n[79] N. Dehak et al., “Language recognition via i-vectors and dimensionality\nreduction,” in Proc. Interspeech, 2011.\n[80] P. Swietojanski and S. Renals, “Learning hidden unit contributions for\nunsupervised speaker adaptation of neural network acoustic models,”\nin IEEE Spoken Language Technology Workshop (SLT) , pp. 171–176,\nIEEE, 2014.\n[81] J. J. Godfrey et al., “Switchboard: Telephone speech corpus for research\nand development,” in Acoustics, Speech, and Signal Processing , vol. 1,\npp. 517–520, IEEE Computer Society, 1992.\n[82] T. Hain et al. , “The ami meeting transcription system: Progress and\nperformance,” in International Workshop on Machine Learning for\nMultimodal Interaction, pp. 419–431, Springer, 2006.\n[83] S. Hochreiter and J. Schmidhuber, “Lstm can solve hard long time lag\nproblems,” NIPS, pp. 473–479, 1997.\n[84] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[85] F. Li et al. , “Ternary weight networks,” arXiv preprint\narXiv:1605.04711, 2016.\n[86] G. Korpelevi, “An extragradient method for ﬁnding saddle points and\nfor other problems, konom. i mat,” 1976.\n[87] A. STOLCKE, “Entropy-based pruning of backoff language models,”\nin Proc. of DARPA Broadcast News Transcription and Understanding\nWorkshop, 1998.\n[88] K. Seymore and R. Rosenfeld, “Scalable backoff language models,” in\nICSLP, vol. 1, pp. 232–235, IEEE, 1996.\n[89] H. Avron and S. Toledo, “Randomized algorithms for estimating the\ntrace of an implicit symmetric positive semi-deﬁnite matrix,” JACM,\nvol. 58, no. 2, pp. 1–34, 2011.\n[90] T. Elsken et al., “Neural architecture search: A survey.,”J. Mach. Learn.\nRes., vol. 20, no. 55, pp. 1–21, 2019.\n[91] S. Hu et al., “Neural architecture search for lf-mmi trained time delay\nneural networks,” ICASSP, 2020.\n[92] S. Hu et al., “Dsnas: Direct neural architecture search without parameter\nretraining,” in CVPR, June 2020.\n[93] H. Pham et al. , “Efﬁcient neural architecture search via parameters\nsharing,” in ICML, pp. 4095–4104, PMLR, 2018.\n[94] A. Fasoli, C.-Y . Chen, M. Serrano, X. Sun, N. Wang, S. Venkataramani,\nG. Saon, X. Cui, B. Kingsbury, W. Zhang, Z. T¨uske, and K. Gopalakrish-\nnan, “4-Bit Quantization of LSTM-Based Speech Recognition Models,”\nin Proc. Interspeech 2021 , pp. 2586–2590, 2021.\n[95] D. Povey et al. , “The kaldi speech recognition toolkit,” in ASRU,\nno. CONF, IEEE Signal Processing Society, 2011.\n[96] X. Xie et al. , “Bayesian learning for deep neural network adaptation,”\nTASLP, 2021.\n[97] J. Li et al., “On the comparison of popular end-to-end models for large\nscale speech recognition,” Proc. Interspeech, 2020.\n[98] C. Liu et al., “Progressive neural architecture search,” in ECCV, pp. 19–\n34, 2018.\n[99] A. M. Maddison, Chris J and Y . W. Teh, “The concrete distribution: A\ncontinuous relaxation of discrete random variables,” in ICLR, 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.728410542011261
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.712195634841919
    },
    {
      "name": "Artificial neural network",
      "score": 0.5443927049636841
    },
    {
      "name": "Transformer",
      "score": 0.5356336236000061
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5242679119110107
    },
    {
      "name": "Speech recognition",
      "score": 0.45567119121551514
    },
    {
      "name": "Algorithm",
      "score": 0.450411319732666
    },
    {
      "name": "Language model",
      "score": 0.44117119908332825
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32836011052131653
    },
    {
      "name": "Engineering",
      "score": 0.07998695969581604
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}