{
    "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
    "url": "https://openalex.org/W4385570641",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3119901949",
            "name": "Lennart Wachowiak",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2080814899",
            "name": "Dagmar Gromann",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2807251626",
        "https://openalex.org/W4244900429",
        "https://openalex.org/W4308245305",
        "https://openalex.org/W4283802865",
        "https://openalex.org/W2578317340",
        "https://openalex.org/W2105508801",
        "https://openalex.org/W2251883304",
        "https://openalex.org/W2803972085",
        "https://openalex.org/W3211499990",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4225090118",
        "https://openalex.org/W1493697127",
        "https://openalex.org/W3045533700",
        "https://openalex.org/W3011373955",
        "https://openalex.org/W3173441389",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2164777277",
        "https://openalex.org/W2560778841",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385573902",
        "https://openalex.org/W4285302754",
        "https://openalex.org/W160005803",
        "https://openalex.org/W4287828172",
        "https://openalex.org/W3144965234",
        "https://openalex.org/W1968310932",
        "https://openalex.org/W2279625306",
        "https://openalex.org/W4213193789",
        "https://openalex.org/W4205183187",
        "https://openalex.org/W1974991592",
        "https://openalex.org/W168564468",
        "https://openalex.org/W2798651446",
        "https://openalex.org/W3171213613",
        "https://openalex.org/W2150375089"
    ],
    "abstract": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor’s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1018–1032\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDoes GPT-3 Grasp Metaphors?\nIdentifying Metaphor Mappings with Generative Language Models\nLennart Wachowiak\nKing’s College London\nlennart.wachowiak@gmail.com\nDagmar Gromann\nUniversity of Vienna\ndagmar.gromann@gmail.com\nAbstract\nConceptual metaphors present a powerful cog-\nnitive vehicle to transfer knowledge structures\nfrom a source to a target domain. Prior neural\napproaches focus on detecting whether natu-\nral language sequences are metaphoric or lit-\neral. We believe that to truly probe metaphoric\nknowledge in pre-trained language models,\ntheir capability to detect this transfer should be\ninvestigated. To this end, this paper proposes to\nprobe the ability of GPT-3 to detect metaphoric\nlanguage and predict the metaphor’s source do-\nmain without any pre-set domains. We experi-\nment with different training sample conﬁgura-\ntions for ﬁne-tuning and few-shot prompting on\ntwo distinct datasets. When provided 12 few-\nshot samples in the prompt, GPT-3 generates\nthe correct source domain for a new sample\nwith an accuracy of 65.15% in English and\n34.65% in Spanish. GPT’s most common er-\nror is a hallucinated source domain for which\nno indicator is present in the sentence. Other\ncommon errors include identifying a sequence\nas literal even though a metaphor is present\nand predicting the wrong source domain based\non speciﬁc words in the sequence that are not\nmetaphorically related to the target domain.\n1 Introduction\nMetaphor processing with pre-trained language\nmodels (e.g.Conneau et al., 2020; Brown et al.,\n2020) has been dominated by metaphor detec-\ntion, that is, the classiﬁcation of expressions into\nmetaphoric or literal (e.g.Aghazadeh et al., 2022;\nLeong et al., 2020). In metaphor interpretation, a\ncommon approach is to paraphrase metaphoric ex-\npressions into literal ones (e.g.Stowe et al., 2021a).\nFew approaches target metaphor identiﬁcation, e.g.\npredicting the source domain of a metaphor in a lin-\nguistic sequence. For instance,Rosen(2018) relies\non grammatical constructs and pre-deﬁned labels.\nInstead, in this paper, we test a generative language\nmodel’s ability to predict the source domain given\na target domain and sequence without grammatical\nassumptions or ﬁxed source domain labels.\nConceptual metaphor theory (CMT) (Lakoff and\nJohnson, 1980) starts from the assumption that\nmetaphors represent a powerful cognitive mech-\nanism to transfer physical knowledge structures to\nabstract domains. In natural language,He was\nbombarded by insultsor Your words pierce my\nheart transfers the concrete domain of weapons\nto the abstract domain of words in the metaphor\nWORDS ARE WEAPONS. On the assumption that\nour cognitive organization relies on metaphors, au-\ntomatically identifying metaphoric transfer holds\nthe promise of contributing to more human-like\ncomputational models. From the overall success of\npre-trained language models in metaphor detection,\na certain degree of metaphoric knowledge in these\nmodels can be assumed (Aghazadeh et al., 2022).\nThis paper aims to evaluate whether this inherent\nknowledge extends beyond contextual clues to pre-\ndict the concrete domains in the metaphoric trans-\nfer. Detecting a metaphor entails contrasting the\nphysical with the abstract meaning of a sequence.\nHowever, the source domain is frequently a non-\ncontextual attribute (Aghazadeh et al., 2022), while\nthe target domain can be found directly using con-\ntextual clues. For instance, in the above example,\npierceis more implicitly related to WEAPONS\nthan the explicitwordsis to WORDS. To deter-\nmine the accuracy of the predicted source domains\nfrom ﬁne-tuning and few-shot prompting GPT-3\n(Brown et al., 2020), we manually evaluate the re-\nsults. To this end, we propose a classiﬁcation of\nerror types from too generic domains to relying\non words in the sentence that are not connected to\nthe metaphor, which we call trigger words. This\nprovides further intuition on the nature and extent\nof metaphoric knowledge encoded in pre-trained\nlanguage models. We compare methods to elicit\nmetaphoric knowledge without any assumptions\non grammar or source domains and test if it ex-\n1018\ntends across languages, i.e., Spanish in addition to\nEnglish. Finally, we evaluate its generalization by\ntesting on two distinct datasets and a set of non-\nmetaphoric sentences.\n2 Preliminaries\nTwo major pillars that build the foundation for this\napproach are conceptual metaphors and generative\nlanguage models, which we brieﬂy introduce here.\n2.1 Conceptual Metaphors\nThe idea of metaphoric projection from a physi-\ncal source domain to an abstract target domain is\ndeeply rooted in the tradition of embodied cogni-\ntion, which assumes that higher-level cognition is\nshaped by physical experiences (Barsalou, 1999).\nFor instance, actual physical movement recruits\nsimilar areas in the brain as communicating with\naction verbs (Durand et al., 2018; Gibbs, 2006).\nConceptual metaphors are deeply entrenched in\nour knowledge organization system and utilized in\neveryday communication to convey thoughts more\nprecisely. In a large-scale study,Prabhakaran et al.\n(2021) evaluate the persuasiveness of metaphors\nand show that metaphoricity in political posts in-\ncreases social media engagement.Citron and Gold-\nberg(2014) show that metaphoric emotional lan-\nguage elicits a higher emotional response by recip-\nients than literal use. To provide complex analy-\nses of metaphoricity in language and analyze the\nmetaphoric knowledge of generative language mod-\nels, we believe that identifying concrete metaphoric\nprojections in natural language is required.\n2.2 Generative Language Models\nLarge generative language models are trained with\nthe objective of predicting the next token in a se-\nquence. During inference, this allows them to be\nprompted with some text by a user and then gen-\nerate what they predict to be most likely to come\nnext. Scaled to large training corpora based on\nweb-data and multi-billion parameter architectures,\nthis simple objective resulted in models such as\nGPT-3 (Brown et al., 2020) or its open-source vari-\nants BLOOM (Luccioni et al., 2022) and OPT-175\n(Zhang et al., 2022). For a speciﬁc task, these\nmodels can be used either in a zero-shot, few-shot,\nor ﬁne-tuning manner. For zero-shot text comple-\ntion, the model is prompted with an instance of\na task without being provided any example solu-\ntion of other task instances. In comparison, for\nfew-shot completions, the prompt already contains\nsome samples of the task and the respective solu-\ntions. In both variants, the model weights are not\nchanged anymore, only the prompt differs. In con-\ntrast, when ﬁne-tuning the model, its weights are\noptimized to predict the task-speciﬁc output given\nsome input/output task samples.\n3 Related Work\nTong et al.(2021) provide a recent overview of\narchitectures used for metaphor detection, avail-\nable datasets, and further metaphor-related tasks.\nAn overview byRai and Chakraverty(2020)\ntakes many different approaches to computational\nmetaphor processing into account, additionally re-\nﬂecting on the different theoretical and linguistic\nviews on the deﬁnition of metaphors. While there\nare many metaphor-related tasks, the closest to ours\nare presented in the sections on paraphrasing and\nconnecting source and target domains.\nDetection. Metaphor detection, the simplest form\nof computational metaphor processing, is a binary\nclassiﬁcation task in which each word of a sentence\nis labeled as being used metaphorically or literally.\nIn a 2020 shared task on metaphor detection, ﬁne-\ntuning pre-trained language models led to the best\nresults (Leong et al., 2020). To achieve small im-\nprovements in accuracy, different approaches en-\nrich the model input by, for instance, providing\ndictionary deﬁnitions of the words being classiﬁed\n(Babieno et al., 2022) or concreteness measures\nthat indicate to what extent something can be ex-\nperienced via the senses (Brysbaert et al., 2014).\nCommonly used datasets for this task are the VU\nAmsterdam Metaphor (VUA) Corpus (Steen et al.,\n2010) and the TOEFL corpus (Klebanov et al.,\n2018), both human-annotated based on different\nprotocols.\nModel Insights.Other research explores the em-\nbeddings generated by language models and how\nthey relate to metaphoricity.Pedinotti et al.(2021)\nshow that BERT’s likelihood scores show a de-\ncreasing likelihood from literal sentences to con-\nventional and novel metaphors and, lastly, to non-\nsense sentences; thus, BERT’s scores correlate with\nhuman-annotated plausibility scores. Moreover,\nfor different layers, they explore cosine similari-\nties between words used metaphorically, e.g.,the\nﬂowersnoddedin the wind, and their metaphori-\ncal paraphrases and literal synonyms. Similarly,\n1019\nAghazadeh et al.(2022) investigate which layers\nof different language models encode metaphoric\nknowledge across different languages and datasets\nvia probing.\nParaphrasing. One common approach to\nmetaphor interpretation is paraphrasing the\nmetaphorical expression using only literal words.\nFor example, the phraseto devour a novelcould\nbe rephrased asto enjoy a novel. An example\nof metaphor interpretation is the work byMao\net al. (2018), who propose to query WordNet\nfor possible candidate translations, from which\nthe best is selected based on similarities in the\nembedding space. On the other hand, there is also\nresearch on generating metaphoric paraphrases\ngiven a literal sentence as input. Recent work in\nmetaphoric paraphrasing uses text-to-text models,\nsuch as T5 or BART (Stowe et al., 2021b,a). Most\nrecently,Liu et al.(2022) proposed a new task for\nwhich they created a dataset of novel metaphors\nin the form of similes, for exampleThe meteor\nwas as bright as (New York City | coal), which\nthe language model has then to interpret asvery\nbrightor not bright at all. A ﬁne-tuned RoBERTa\nmodel outperforms various GPT variants on the\ntask and comes close to human performance. The\nauthors also show that the reverse of the tasks,\ni.e., predicting the metaphoric language given the\nliteral answer, is more difﬁcult.\nConnecting Source and Target Domains.Try-\ning to automate the process of identifying metaphor\nmappings is not a new endeavor. For instance,\ngiven manually collected metaphoric phrases of a\nspeciﬁc target domain,Chung et al.(2004) propose\nto facilitate the identiﬁcation of source domains by\nquerying WordNet senses and the ontology SUMO.\nMore recent research makes use of syntactical pat-\nterns metaphoric language often occurs in (Sullivan,\n2013), thereby narrowing down the pool of sen-\ntences considered as metaphoric candidates.Dodge\net al.(2015) use such patterns to ﬁnd metaphor\ncandidates that are further analyzed by identifying\nevoked frames and checking for whether the frames\nrelate in MetaNet. Given a target domain and a cor-\npus, they can use this system to see which source\ndomains are frequently used to metaphorically talk\nabout a target domain. This system, however, is\nlimited by existing frame resources and relies on\npre-deﬁned grammatical structures. Also querying\nan existing database,Ge et al.(2022) use hypernym\nrelations from WordNet to identify the source and\ntarget domains for pairs of literally used nouns and\nliterally or metaphorically used verbs or adjectives.\nWhile the target domain identiﬁcation reaches an\naccuracy of 87.3%, the source domain identiﬁca-\ntion only reaches 67.3% based on the manual eval-\nuation of a small subset of the data.Shutova et al.\n(2017) explore unsupervised methods for identify-\ning clusters of source and target concepts as well\nas the connections between them. They limit their\napproach to verb–noun constructions, from which\nthe verbs constitute the source domain clusters and\nthe nouns the target domain clusters.\nMohler et al.(2016) provide a dataset with\nsentences from government discourse annotated\nwith scores from -1 to 3 to indicate the level of\nmetaphoricity. More importantly, 7,941 sentences\nare annotated for source–target domain mappings\nwith 108 different source domains.Rosen(2018)\nuses this dataset to build a model to predict the\nsource domain of a metaphor given a contextual\nsentence and a target domain referent. Compared\nto our approach, this work presupposes that a given\nsentence is metaphoric while also depending on\nspeciﬁc grammatical dependencies when construct-\ning the model input. Most importantly, it is limited\nto the 77 labels sub-sampled from the overall avail-\nable 108 domains as experiments are done using\nfeed-forward neural networks and LSTMs instead\nof text-to-text networks. Rosen also shows that the\ninter-annotator agreement for the original source\ndomain annotations is rather low with a Cohen’s\nkappa of 0.544, which indicates the difﬁculty and\npotential ambiguity of the task.\nIn contrast to the existing work on computational\nextraction of source and target domains, our ap-\nproach does not rely on any assumptions about\ngrammatical structure or word types that suppos-\nedly indicate metaphorical language. Moreover,\nwe are not limited to a pre-deﬁned set of source or\ntarget domains due to the text-to-text approach.\n4 Method\n4.1 Task\nIn our experiments, we use GPT-3 to predict a\nmetaphor’s source domain given a sentence and\na target domain. For example, a prompt to identify\nthe conceptual metaphor underlying the sentence\nYou are wasting my timecould look like this:\nExtract the conceptual metaphor from the\nfollowing sentence :\n1020\nSentence : Our relationship is at\ncrossroads\nTarget Domain: Relationship\nSource Domain: Journey\nSentence : You are wasting m y time\nTarget Domain: Time\nSource Domain: <<model completion >>\nIn this prompt, the model is provided with one\nexample of a metaphor mapping, which is RE-\nLATIONSHIP IS A JOURNEY . Afterwards, it is\nprovided with the sentence and target domain for\nwhich we want to know the source domain. A cor-\nrect prediction, in this case, would be TIME IS\nMONEY or TIME IS A RESOURCE.\n4.2 Dataset\nThe main dataset was gathered by retrieving all\nnatural language examples annotated with source\nand target domain from Lakoff’s Master Metaphor\nList1, called Metaphor List in the following. For\nthis task, we randomly selected 446 sentences, with\na maximum of three per metaphor, i.e., per unique\ncombination of source and target domain. To en-\nsure that the model does not simply assume all sen-\ntences to be metaphoric, we use non-metaphoric\nEnglish sentences from the VUA corpus (Steen\net al., 2010) by extracting sentences for which each\nword is labeled as literal by the annotators. For in-\nstance,He did not even see an English newspaper\nis an example of a non-metaphoric sentence. From\nthe extracted non-metaphoric sentences, we manu-\nally chose 50 to be added to our dataset as many of\nthe sentences were wrongly labeled or extremely\nshort.\nThe resulting dataset is split into a train, vali-\ndation, and test set detailed in Table1. A unique\ncombination of source and target domain, for ex-\nample, BELIEFS (target) ARE PLANTS (source),\ndoes not appear in the validation or test set if it\nalready appeared in the training set. This allows\nus to test whether the model can generalize to new,\nunseen metaphors. As the Metaphor List data only\ncontains a limited number of domain combinations,\nthe validation and test set contain the same com-\nbinations of source and target domains, however,\nwith different unique sentences. Entirely new do-\nmain combinations in the test set are evaluated via\nsentences from additional datasets.\nTo test the ability to generalize across datasets,\n1http://www.lang.osaka-u.ac.jp/~sugi\nmoto/MasterMetaphorList/metaphors/index.\nhtmlcopyright (c) 1994 by George Lakoff, University of\nCalifornia, Berkeley\nwe use sentences from the LCC dataset (Mohler\net al., 2016) (CC BY-NC-SA v4.0), where we use\nthe provided source and target domains and the\nraw sentences without indication of the precise\nmetaphor location. From the English and Span-\nish sentences, we use a subset of maximally 10\nsentences per target domain, resulting in a set of\n284 (EN) and 110 (ES) sentences. In comparison\nto the Metaphor List samples, the LCC dataset con-\nsists of much longer sentences using complicated,\nexpert language from the political domain.\nAll multilingual samples, as well as sentences\nfrom the LCC corpus, are solely used as hold-out\ntest set and do not play a role in the model and\nprompt selection process. These sentences, thus,\ntest the model’s generalization ability to new source\ndomains, a different language, i.e., Spanish, and\nmore complex sentences. Model and prompt se-\nlection is based on the validation set created from\nthe Metaphor List samples and the non-metaphoric\nVUA sentences. The number of samples from the\ntraining set that are actually used depends on the\nprompting type.\n4.3 Experiments and Evaluation\nUsing two automated evaluation metrics (Sec.\n4.3.1), we compare few-shot prompts and ﬁne-\ntuned models on the validation set (Sec.4.3.2). The\ntest set evaluation is done manually (Sec.4.3.3)\n4.3.1 Evaluation Metric\nWhile we manually evaluate the model on the test\nset, we use two automatically computed scores to\nevaluate on the validation set. The validation per-\nformance is used to select the best way to prompt\nor ﬁne-tune the model. As the ﬁrst score, we com-\npute the embedding similarity of the gold standard\nsource domain and the GPT-3-generated domain.\nWe compute the similarity using the Gensim library\n(ˇReh˚ uˇrek and Sojka, 2010) with 300-dimensional\nGloVe vectors (Pennington et al., 2014).\nTo provide more context to the automated eval-\nuation, we also use knowledge graph embeddings.\nWe rely on the KGvec2go Web API (Portisch et al.,\n2020) created from the resources WordNet, Wik-\ntionary, DBpedia, and WebIsALOD. We average\nthe four returned similarity scores based on the dif-\nferent resources, called KB score in the following.\n4.3.2 Prompt Selection\nTo see with what prompts the model returns the\nbest source domains, we vary the number of labeled\n1021\nDataset Train\nSentences\nVal. Sen-\ntences\nTest Sen-\ntences\nTarget\nDomains\nSource\nDomains\nMetaphor List 117 105 224 91 94\nVUA non-metaphoric 15 15 15 47 -\nLCC EN 0 0 284 30 90\nLCC ES 0 0 110 11 67\nTotal 132 120 633 179 251\nTable 1: Number of sentences and unique target and source domains in the different datasets.\nfew-shot samples provided at the beginning of each\nprompt. We compute the scores described in Sec-\ntion4.3.1for generations obtained through prompts\ncontaining 2, 4, 6, 8, and 12 labeled samples. That\nmeans, in each few-shot setting, the model has at\nleast 2 examples of correct domain mappings for\norientation. For each of these ﬁve prompt varia-\ntions, we choose three distinct sets of training sam-\nples. Thus, we generate three solutions to evaluate.\nThis allows us to observe how much the model\ndepends on speciﬁc training samples, and we can\ncompute average scores and standard deviations.\nMoreover, we also ﬁne-tune GPT-3 by using our\nsamples to train the model for 4 epochs, during\nwhich the model’s weights are adapted, instead of\njust providing the samples as few-shot samples in\nthe prompt. After ﬁne-tuning, the model does not\nrequire any few-shot samples in the prompt, but can\ndirectly classify a sample from the validation set.\nWe ﬁne-tune two variants: (1) a model ﬁne-tuned\nwith all 132 sentences from the training set; and\n(2) a model ﬁne-tuned with 34 sentences from the\ntraining set, one per unique source domain. The\nexperiments of comparing different prompts with\neach other and the ﬁne-tuned variants use the vali-\ndation set. With GPT-3 being proprietary licensed\nby the OpenAI, L.L.C Terms of Use, the text gen-\nerations with its API cost 42.73$. Our code is\navailable online2.\nFor all generations, we set the temperature pa-\nrameter to 0, which means that the text generation\nmodel samples words in a greedy fashion, i.e., it al-\nways generates the most likely next word. Increas-\ning the temperature changes the likelihood with\nwhich words are sampled. For now, a temperature\nof 0 allows us to generate words in a deterministic,\nrepeatable fashion. However, future experiments\ncould include the temperature as a hyperparame-\nter to be optimized. The GPT-3 architectures we\n2https://github.com/lwachowiak/Metaphor-Extraction-\nWith-GPT-3\nused are davinci-002, the most powerful available\nmodel variant at the time of the experiments3, and\ncurie-001, the second most powerful variant.\n4.3.3 Manual Evaluation\nIssues with the gold standard source domains, as\nwell as the fact that the source domains can be\nphrased with different expressions and differ in\ntheir level of precision, make it difﬁcult for the\nautomated scores to be reliable enough to directly\nderive an accuracy score from them. Thus, to com-\npute the ﬁnal accuracy on the test set, we manu-\nally check the model’s output. After experimenting\nwith the different prompting styles on the validation\nset, we choose the model with the best combined\nKB score and embedding similarity for manual\nevaluation on the hold-out test set. Two annota-\ntors, the authors of this paper, manually evaluate\nthe correctness of the generated answers for En-\nglish. Both annotators independently evaluated the\nmodel output and then discussed disagreements.\nOne annotator evaluates the answers for Spanish.\nThe source domain was considered correct if it\ncorresponded to the gold standard or was deemed\ncorrect by the annotator(s).\nWhile hard to automate, for humans it is often\neasy to detect a close correspondence between a\ngold domain, e.g. “musical harmony”, and a pre-\ndicted domain, e.g. “music”. In difﬁcult cases,\nannotators, following the Metaphor Identiﬁcation\nProcedure (MIP) (Group, 2007), analyze words for\ntheir more basic, physical meaning and see if these\nare in concordance with the predicted source do-\nmain. For instance, the gold standard forYou make\nme sick!is “nausea”, whereassickis also deﬁned\nas physically ill and thus related to the predicted\n“disease” domain. To gather more insights into\nthe type of issues that can be observed from the\npredicted source domains, all predictions deemed\n3In November 2022, OpenAI released davinci-003, an\nInstructGPT variant (Ouyang et al., 2022).\n1022\nFigure 1: Automatically computed scores on the valida-\ntion set in relation to the number of examples provided\nin the prompt\nincorrect are classiﬁed by type of error as detailed\nin Section5.3.\n5 Results\nThis section describes the results from the experi-\nments that determine the manually evaluated test\nset predictions, their accuracy, and types of errors.\n5.1 Prompt Selection Results\nFigure1 shows the automatically computed scores\non the validation set achieved by davinci-002 and\ncurie-001 with different numbers of few-shot sam-\nples. We can see that davinci-002 outperforms the\nsmaller architecture by about 0.15 to 0.2 points.\nThe highest embedding similarity and highest KB\nscore are achieved by davinci-002 when prompted\nwith 12 different few-shot training samples, achiev-\ning an embedding similarity of 0.505 and a KB\nscore of 0.553. However, the standard deviation is\nvery high for the models prompted with 12 samples,\nthus, showing the importance of the quality of those\nsamples. Due to this ﬂuctuation in performance,\nthe average KB score over all three runs is highest\nfor davinci-002 models prompted with 8 samples,\nand the average embedding score is highest for\ndavinci-002 models prompted with 4 samples. The\nprompt based on 12 few-shot samples that led to\nthe overall best results is available in the appendix.\nIn comparison, the ﬁne-tuned models perform\nbetter than the curie-001 models but worse than\nthe davinci-002 models. Fine-tuning a model with\n36 samples, each with a unique source domain,\nleads to an embedding similarity of 0.303 and a\nKB score of 0.386. Fine-tuning GPT-3 on all avail-\nable training samples results in improved scores of\n0.413 and 0.513. Examining the completions of the\nmodel ﬁne-tuned on all samples, we can see that it\nsticks more to the source domains already present\nin the training data while also predicting fewer\ndistinct source domains overall: the completions\nfrom the best performing few-shot variant contain\n74 unique source domains, from which 7 are also\npresent in the training data; the completions of the\nmodel ﬁne-tuned on 36 contain 78 unique source\ndomains, from which 13 are present in the training\ndata; and the completions of the model ﬁne-tuned\non all data contain only 50 unique source domains,\nfrom which 18 are present in the training data.\n5.2 Manual Evaluation Results\nWe used the best prompt identiﬁed in the previous\nsection to generate the source domains for the test\nset samples. The correctness of the generations\nwas manually veriﬁed by two annotators. We used\nCohen’s Kappa, a chance-corrected coefﬁcient of\nagreement, to compute the inter-annotator agree-\nment. Across all test data points, we obtained a Co-\nhen’s Kappa of 0.51, corresponding to a moderate\nagreement according toLandis and Koch(1977).\nAfter disagreements were resolved through discus-\nsion, we computed the model’s accuracy, which is\nreported by dataset in Table2. The model achieved\nan accuracy of 81.33% on the Metaphor List cor-\npus, 53.74% on the English part of the LCC corpus,\nand 34.65% on the Spanish part of the LCC cor-\npus. In addition, the model was able to achieve an\naccuracy of 42.11% in predicting a sentence is non-\nmetaphoric instead of predicting a source domain.\nAveraged by sample, this results in an accuracy of\n60.22%. The decrease in performance on the LCC\ntest set is not surprising as the sentences are on av-\nerage much longer and often use domain-speciﬁc\nlanguage. Moreover, the target domains speciﬁed\nby the LCC gold standard are often much harder to\nidentify in the sentence as they are less precisely\nmatched to the sentence’s words.\nTo provide insights into the adequacy of the eval-\nuation metrics, we evaluate their correlation with\nthe manual annotation decisions. As we have an\nordinal variable (correctly classiﬁed, wrongly clas-\nsiﬁed) and a continuous variable (KB score and\nembedding similarity), we used Spearman’s rank\ncorrelation coefﬁcient. We achieve a correlation of\n0.43 for the KB score and 0.40 for the embedding\nsimilarity. Both scores are statistically signiﬁcant\nwithp< 0.05, and can be interpreted as a moderate\ncorrelation (Dancey and Reidy, 2007).\n1023\nDataset Accuracy Inter-annotator Agreement\nCohen’s Kappa Agreement in %\nMetaphor List 81.33% 0.55 (Moderate) 87.6%\nVUA non-metaphoric 42.11% 0.89 (Almost perfect) 94.7%\nLCC EN 53.74% 0.45 (Moderate) 72.5%\nLCC ES 34.65% - -\nAverage (weighted by samples) 60.22% 0.51 (Moderate) 79.7%\nAverage (unweighted) 52.96% 0.63 (Substantial) 84.9%\nTable 2: Manually evaluated test performance\n5.3 Type of Errors\nWe manually classiﬁed all errors on the English\ntest sets based on the typology presented in Ta-\nble 3: wrong with trigger, wrong without trigger,\ntoo literal, should be non-metaphoric, should be\nmetaphoric, too speciﬁc, too general, wrong subele-\nment mapping. Trigger here refers to words in\nthe input that are clearly related to the predicted\nsource domain. For instance, any mentions of\nanimal-related terms, e.g.bullish mindsetortrough\nof poverty, led the model to predict “animals” as\nsource domain. The most common error class is be-\ning wrong without any trigger in the sentence, fol-\nlowed by erroneous predictions of non-metaphoric\nand being wrong with trigger. Some instances in-\ndicate a misinterpretation of words, e.g.dumb-\nfoundedlikely leads to the entertaining prediction\nof “being_stupid”. Furthermore, interesting errors\ncan be found in the category of wrong subelement\nmappings, where the model identiﬁes the general\nsource domain but fails to pick the correct element\nof that domain for its prediction. For instance,\nin the sentenceChina is a fertile ground for re-\nvolt, the gold standard refers to “plants”, and the\nmodel predicts “land”, which is in the same domain\nof cultivation but not entirely the correct domain.\nSimilarly, when a metaphor involved movement\nand locations and the true source domain referred\nto only one of them, the model regularly picked\nthe wrong subelement. For instance, the model\nwrongly predicts EXISTANCE IS MOTION for\nthe sentenceIt came into existence, where the true\nsource domain would have been “location”.\nFor the Spanish LCC data, one annotator classi-\nﬁed erroneous predictions according to our error\ntypology. A vast majority of 62.12% of errors were\npredictions of non-metaphoric sequences which\nshould be metaphoric, followed by 19.70% wrong\nwithout trigger. A trend to predict “family” without\nany trigger in the sentence for the target domain\n“government” in half of its occurrences could be\nobserved. In the 13.64% cases of wrong with trig-\nger, the model’s predictions mostly represented lit-\neral English translations of context words from the\nSpanish sentence. All source domain predictions\nwere made in English, which was expected given\nthat the source and target domains in the prompt\nwere also in English. In total, 12 LCC sentences\nwere disregarded since the gold standard was faulty.\n6 Discussion\nWe experimented with different GPT-3 variants and\nprompts containing varying numbers of few-shot\nsamples to see whether GPT-3 can generate the\nsource domain of a conceptual metaphor mapping\ngiven a context and a target domain. The best re-\nsults were achieved with a long few-shot prompt\ncontaining 12 example completions. The largest\nmodel variant davinci-002 strongly outperformed\nthe next biggest variant and a ﬁne-tuned GPT-3.\nWe also saw that ﬁne-tuning the model can\nlead to a decrease in expressiveness, that is, fewer\nunique source domains being generated. In our\ncase, this might be because the model ﬁne-tuned\non all data sees each source domain around three\ntimes per training. It might be possible to counter-\nact the decrease in expressiveness by increasing the\ntemperature parameter, thus, making less probable\ngenerations more likely.\nManually coding the errors made by the model,\nwe saw that the model often fabricates source do-\nmains for which no related words are present in the\nsentence. Other common errors included predicting\na literal meaning although a metaphor was present,\nand generating wrong source domains based on\ntrigger words that were not metaphorically related\n1024\nError Code Deﬁnition Example % of All\nErrors\nSentence Wrong Prediction\nWrong with trig-\nger\nThe model predicts a\nwrong source domain due\nto words in the sentence re-\nlated to that domain\nThe arms race COMPETITION IS\nWAR\n21.31\nWrong without\ntrigger\nThe model predicts a\nwrong source domain\nwithout any noticeable\ntriggers for that domain in\nthe sentence\nSally gave the idea\nto Sam\nIDEAS ARE CHIL-\nDREN\n27.32\nToo literal The model predicts a lit-\neral relationship instead of\na metaphoric mapping\nI’m down to my bot-\ntom dollar\nMONEY IS IN-\nVESTMENT\n7.10\nShould be non-\nmetaphoric\nThe model predicted a\nmetaphoric source domain\ninstead of non-metaphoric\nThey saw him ad-\nvancing\nMOVING IS COM-\nING\n7.65\nShould be\nmetaphoric\nThe model wrongly pre-\ndicted non-metaphoric\nUnder the cover of\ndarkness\nDARKNESS IS\nnon-metaphoric\n25.14\nToo speciﬁc The predicted metaphor is\nmore speciﬁc than what\nthe sentence implies.\nHe ﬁnally caught\nup to schedule\nSCHEDULE IS\nPEOPLE\n2.73\nToo general The predicted source do-\nmain is too unspeciﬁc\nThe idea slipped\nthrough my ﬁngers\nMIND IS SPACE 1.09\nWrong subele-\nment mapping\nThe model predicts an as-\npect of the correct source\ndomain, however, it is not\nthe exact element\nLet’s strip away the\nunimportant details\nIMPORTANCE IS\nCLOTHING\n7.65\nTable 3: The different types of errors made by the model\nto the target domain. Discerning whether to predict\na source domain for a given sentence or to label it\nas non-metaphoric seems to be quite challenging\nfor the model as well. Analyzing the errors of large\nlanguage models as done here is essential to build\nappropriate trust or distrust in the model and allow\nfor the use of error-correction methods in the fu-\nture, for instance, the selection of better prompts or\ntraining samples.\nIn the context of analyzing the model’s misclas-\nsiﬁcations, we also experienced issues with the\ndataset, e.g. unintuitive metaphor mappings or\nlack of contextual clues for the provided target\ndomain. The dataset’s quality strongly affected\nthe Spanish test results and clearly indicated that\nmore multilingual resources for metaphor identi-\nﬁcation are needed. The difference in the nature\nand quality of the datasets is also the main reason\nfor the strong variation in accuracy results. The\nMetaphor List dataset provides prototypical, gen-\neral language examples, while the LCC dataset\nannotated real-world, domain-speciﬁc expert lan-\nguage. This affects the complexity as well as the\nlength of sentences, both contributing to the differ-\nence in accuracy across datasets.\nApplication. Using GPT-3 to analyze metaphors\nused in an unlabeled corpus comes with two prob-\nlems: (1) we do not know what target domains are\nthe right ones to provide to the model, (2) there will\nbe an overwhelming amount of output given that\n1025\nmost sentences contain at least subtle metaphoric\nlanguage that will largely not even be relevant to the\ndomain we are interested in. Therefore, it would be\nuseful to ﬁrst ﬁlter sentences based on seed words\nwhose usage interests us or that belong to a spe-\nciﬁc target domain we want to analyze (Wachowiak\net al., 2022). As such an approach already narrows\ndown the candidate sentences to a pre-speciﬁed\ntarget domain, we can include that target domain\nin the prompt for the language model. Lastly, it\nmight help to restrict the context window around\nthe words of interest so that the model is not dis-\ntracted by other metaphors in the sentence. How-\never, to conﬁrm this, further research is needed.\nConsidering precise element mappings.As the\ncapabilities of large neural language models con-\ntinue to grow, it will be interesting to see if they can\nidentify not only the correct source domains but\nalso precise element-wise mappings between the\nconcepts of the target and source domain. For ex-\nample, the conceptual metaphor LOVE IS A JOUR-\nNEY involves mapping lovers to travelers, difﬁcul-\nties to roadblocks, and progress to distance traveled\nforward. Querying such an element-wise mapping\ncould be facilitated through a set of the target do-\nmain’s core elements being provided to the model.\nOpenAI Transparency Issues.An issue with re-\nsearching the capabilities of large language models,\nsuch as GPT-3, is the accessibility and transparency.\nWhile GPT-3 variants are easily accessible via an\nAPI, the model stays a black-box, and researchers\ncan not investigate the speciﬁc model weights.\nMoreover, there is no explicit mapping available of\nhow the models advertised on the website relate to\nthose described in OpenAI’s papers (Leike, 2022).\nLastly, the model variant accessible for ﬁne-tuning\ndiffers from the one accessible for direct zero- and\nfew-shot text generation, which might also explain\nthe drop in performance observed in our metaphor\nextraction task. On the other hand, comparable\nmodels for which the weights are publicly released,\nsuch as BLOOM (Luccioni et al., 2022) or OPT-\n175 (Zhang et al., 2022), have the issue that they\nare not hosted anywhere. Thus, researchers must\nprovide the infrastructure to run them, which is\nonly possible for very few academic institutions.\n7 Conclusion\nWe analyzed how well GPT-3 can identify source\ndomains of metaphors in natural language. Across\nthree different datasets in English and Spanish,\nGPT-3 predicts the source domain with an accuracy\nof 60.22%. The best performance was achieved\ngiven 12 few-shot examples in the prompt, al-\nthough the average performance was highest with\n4 to 8 few-shot examples. However, the model\nstill suffers from speciﬁc error types, such as hal-\nlucinating domains without any indicators being\npresent. We believe future iterations of large lan-\nguage models like GPT-3 will become important\ntools in computational metaphor analysis, where\none investigates conceptual metaphors in different\ndomains, for instance, literature or political dis-\ncourse. In the future, we want to experiment with\nusing large language models to generate complete\nmetaphors, i.e., generate both, source domain and\ntarget domain, given a sentence. We also plan to\nuse the developed techniques in corpus analyses.\nLimitations\nThe approach of identifying source domains relies\non having a contextual sentence but also a target\ndomain available. The datasets available for evalu-\nation do not always provide precise target domains.\nFor example, the LCC dataset provides the target\ndomaingun ownershipfor the sentenceI just don’t\nknow what it will take for people in this country to\nembrace gun safety, or the target domainclimate\nchangefor the sentenceThe event is billed as the\nlargest meeting of inﬂuential ﬁgures within the re-\nnewable energy ﬁeld. This mismatch often makes\nit difﬁcult to provide precise source domains. A\nsimilar problem also exists when wanting to use\nour source domain prediction approach in the wild\nas we have to somehow provide the model with\na target domain. While we can provide a target\ndomain by selecting sentences based on seed-word\nlists designed for speciﬁc domains, we do not know\nhow precisely this matches the target domain occur-\nring in the sentence. In a multilingual setting, the\nissue becomes more pressing since there are very\nfew multilingual metaphor datasets and for semi-\nautomated approaches the seed-word lists would\nhave to be provided for each language.\nAnother challenge is connected to the fact that\nthe model output requires time-consuming man-\nual evaluation to obtain a precise accuracy score.\nHowever, deciding what counts as a correct source\ndomain can be difﬁcult and might change depend-\ning on how strictly the annotators apply certain\nrules. For instance, whether an annotator sees a pre-\n1026\ndicted source domain as too general or too speciﬁc\nis a matter of degree. Overall, this makes it hard\nto benchmark different approaches across papers,\nwhich is why further investigation of automated\nmetrics, as presented in this paper, is crucial.\nLastly, there are issues regarding the accessibil-\nity of large neural language models, such as GPT-3,\nand the transparency of OpenAI’s API as described\nin the discussion section.\nEthics Statement\nMetaphor identiﬁcation represents an analysis of\npeople’s usage of language in communication as\nwell as its grounding in the physical world. Using\nmetaphoric language has been shown to increase\nthe speaker’s persuasiveness and the listener’s emo-\ntional response. On the one hand, people might\nunconsciously use metaphors and might not appre-\nciate their language being automatically analyzed\nin this regard. On the other hand, a model able to\nidentify metaphors can be trained to actively uti-\nlize metaphoric language and thus become more\npersuasive and elicit a higher emotional response.\nIn the long run, this could be viewed as a means\nto train language models to become more manip-\nulative in their interaction with humans, e.g. in\nspeech assistance or chat applications. The pro-\nposed approach served the purpose of probing the\nextent of metaphoric knowledge in a pre-trained\nlanguage model and not to train it to manipulate\nusers. As a matter of fact, the proposed method can\nalso be utilized to detect the extent of metaphoric\nlanguage produced by a language model and, thus,\ncounteract this development. Nevertheless, we pro-\npose that the aspect of metaphoricity in language\nmodels might be worth including in discussions on\nethics in AI.\nThe nature of the datasets utilized herein might\nalso represent a number of biases. The Metaphor\nList has been introspectively curated by a white\nmale Western person, i.e., George Lakoff, while\nthe LCC dataset stems from online websites and\npolitical debates in American English respectively\nMexican Spanish where the proﬁle of the annota-\ntors remains unclear. Thus, the ﬁrst bias is that not\nall genders, communities of speakers, and language\nvarieties have been represented in this experiment.\nSecond, the domains are limited to political and\ngeneral language domains and the results might\ndiffer when applied to other domains. Third, the\ncoverage of languages is limited to two due to the\nlack of datasets and annotators, i.e., for Russian\nin the case of the LCC dataset. Thus, it would\nbe interesting and important to extend the scope\nof the experiment to investigate the utilization of\nmetaphoric language by different speaker proﬁles\nof different languages and language varieties in the\nfuture.\nReferences\nEhsan Aghazadeh, Mohsen Fayyaz, and Yadollah\nYaghoobzadeh. 2022.Metaphors in pre-trained lan-\nguage models: Probing and generalization across\ndatasets and languages. InProceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2037–\n2050, Dublin, Ireland. Association for Computational\nLinguistics.\nMateusz Babieno, Masashi Takeshita, Dusan Radisavl-\njevic, Rafal Rzepka, and Kenji Araki. 2022. MIss\nRoBERTa WiLDe: Metaphor identiﬁcation using\nmasked language model with wiktionary lexical deﬁ-\nnitions.Applied Sciences, 12(4):2081.\nLawrence W Barsalou. 1999. Perceptual symbol sys-\ntems.Behavioral and brain sciences, 22(4):577–660.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners.Advances in neural information processing\nsystems, 33:1877–1901.\nMarc Brysbaert, Amy Beth Warriner, and Victor Ku-\nperman. 2014. Concreteness ratings for 40 thousand\ngenerally known english word lemmas.Behavior\nresearch methods, 46(3):904–911.\nSiaw-Fong Chung, Kathleen Ahrens, and Chu-Ren\nHuang. 2004. Using WordNet and SUMO to de-\ntermine source domains of conceptual metaphors. In\nRecent Advancement in Chinese Lexical Semantics:\nProceedings of 5th Chinese Lexical Semantics Work-\nshop (CLSW-5). Singapore: COLIPS, pages 91–98.\nFrancesca MM Citron and Adele E Goldberg. 2014.\nMetaphorical sentences are more emotionally engag-\ning than their literal counterparts.Journal of cogni-\ntive neuroscience, 26(11):2585–2595.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.Unsupervised\ncross-lingual representation learning at scale. InPro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\n1027\nChristine P Dancey and John Reidy. 2007.Statistics\nwithout maths for psychology. Pearson education,\nEssex.\nEllen Dodge, Jisup Hong, and Elise Stickles. 2015.\nMetaNet: Deep semantic automatic metaphor anal-\nysis. In Proceedings of the Third Workshop on\nMetaphor in NLP, pages 40–49, Denver, Colorado.\nAssociation for Computational Linguistics.\nEdith Durand, Pierre Berroir, and Ana Ines Ansaldo.\n2018.The neural and behavioral correlates of anomia\nrecovery following poem – personalized observation,\nexecution, and mental imagery therapy: A proof of\nconcept. Neural Plasticity.\nMengshi Ge, Rui Mao, and Erik Cambria. 2022.Ex-\nplainable metaphor identiﬁcation inspired by concep-\ntual metaphor theory. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 36 (10),\npages 10681–10689.\nRaymond W Gibbs. 2006. Metaphor interpretation as\nembodied simulation.Mind & Language, 21(3):434–\n458.\nPragglejaz Group. 2007. MIP: A method for identifying\nmetaphorically used words in discourse.Metaphor\nand symbol, 22(1):1–39.\nBeata Beigman Klebanov, Chee Wee Leong, and\nMichael Flor. 2018. A corpus of non-native writ-\nten english annotated for metaphor. InProceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 86–91.\nGeorge Lakoff and Mark Johnson. 1980.Metaphors we\nlive by. University of Chicago press.\nJ. Richard Landis and Gary G. Koch. 1977.The Mea-\nsurement of Observer Agreement for Categorical\nData. Biometrics, 33(1).\nJan Leike. 2022.Psa: If you want to compare Instruct-\nGPT to a base model in your research, the closest\ncomparison is \"text-davinciplus-002\" with \"davinci\"\n(you might need to request access to the former). it’s\nnot a super clean comparison, because we haven’t\ndeployed the exact paper models.Twitter post on\nJune 29, 2022.\nChee Wee (Ben) Leong, Beata Beigman Klebanov,\nChris Hamill, Egon Stemle, Rutuja Ubale, and Xi-\nanyang Chen. 2020.A report on the 2020 VUA and\nTOEFL metaphor detection shared task. InProceed-\nings of the Second Workshop on Figurative Language\nProcessing, pages 18–29, Online. Association for\nComputational Linguistics.\nEmmy Liu, Chenxuan Cui, Kenneth Zheng, and Graham\nNeubig. 2022.Testing the ability of language models\nto interpret ﬁgurative language. InProceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4437–4452,\nSeattle, United States. Association for Computational\nLinguistics.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat. 2022.Estimating the carbon footprint\nof bloom, a 176b parameter language model. CoRR,\nabs/2211.02001.\nRui Mao, Chenghua Lin, and Frank Guerin. 2018.Word\nembedding and WordNet based metaphor identiﬁca-\ntion and interpretation. InProceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1222–\n1231, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nMichael Mohler, Mary Brunson, Bryan Rink, and Marc\nTomlinson. 2016.Introducing the LCC metaphor\ndatasets. InProceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 4221–4227, Portorož, Slovenia.\nEuropean Language Resources Association (ELRA).\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. InAdvances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nPaolo Pedinotti, Eliana Di Palma, Ludovica Cerini,\nand Alessandro Lenci. 2021.A howling success or\na working sea? testing what BERT knows about\nmetaphors. In Proceedings of the Fourth Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP, pages 192–204, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.GloVe: Global vectors for word\nrepresentation. InProceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nJan Portisch, Michael Hladik, and Heiko Paulheim.\n2020. KGvec2go – knowledge graph embeddings\nas a service. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n5641–5647, Marseille, France. European Language\nResources Association.\nVinodkumar Prabhakaran, Marek Rei, and Ekaterina\nShutova. 2021.How metaphors impact political dis-\ncourse: A large-scale topic-agnostic study using neu-\nral metaphor detection. In Proceedings of the Fif-\nteenth International AAAI Conference on Web and\nSocial Media, ICWSM 2021, held virtually, June 7-10,\n2021, pages 503–512. AAAI Press.\n1028\nSunny Rai and Shampa Chakraverty. 2020.A survey on\ncomputational metaphor processing. ACM Comput.\nSurv., 53(2).\nRadimˇReh˚ uˇrek and Petr Sojka. 2010.Software frame-\nwork for topic modelling with large corpora. InPro-\nceedings of the LREC 2010 Workshop on New Chal-\nlenges for NLP Frameworks, pages 45–50, Valletta,\nMalta. ELRA.\nZachary Rosen. 2018.Computationally constructed\nconcepts: A machine learning approach to metaphor\ninterpretation using usage-based construction gram-\nmatical cues. InProceedings of the Workshop on Fig-\nurative Language Processing, pages 102–109, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nEkaterina Shutova, Lin Sun, Elkin Darío Gutiérrez, Pa-\ntricia Lichtenstein, and Srini Narayanan. 2017.Mul-\ntilingual Metaphor Processing: Experiments with\nSemi-Supervised and Unsupervised Learning. Com-\nputational Linguistics, 43(1):71–123.\nGerard Steen, Lettie Dorst, Berenike Herrmann, Anna\nKaal, Tina Krennmayr, and Trijntje Pasma. 2010.A\nmethod for linguistic metaphor identiﬁcation: From\nMIP to MIPVU, volume 14. John Benjamins Publish-\ning, Amsterdam.\nKevin Stowe, Nils Beck, and Iryna Gurevych. 2021a.\nExploring metaphoric paraphrase generation. InPro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 323–336, Online.\nAssociation for Computational Linguistics.\nKevin Stowe, Tuhin Chakrabarty, Nanyun Peng,\nSmaranda Muresan, and Iryna Gurevych. 2021b.\nMetaphor generation with conceptual mappings. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 6724–\n6736, Online. Association for Computational Lin-\nguistics.\nKaren Sullivan. 2013.Frames and constructions in\nmetaphoric language, volume 14. John Benjamins\nPublishing, Amsterdam.\nXiaoyu Tong, Ekaterina Shutova, and Martha Lewis.\n2021.Recent advances in neural metaphor process-\ning: A linguistic, cognitive and social perspective.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4673–4686, Online. Association for Computa-\ntional Linguistics.\nLennart Wachowiak, Dagmar Gromann, and Chao Xu.\n2022. Drum up SUPPORT: Systematic analysis of\nimage-schematic conceptual metaphors. InProceed-\nings of the 3rd Workshop on Figurative Language\nProcessing (FLP), pages 44–53, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nAppendix\nThe 12 few-shot samples included in the best iden-\ntiﬁed prompt and used for the generation of the\ncompletions on the test set:\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : I ’ve lost all hope of a\nsolution .\nTarget Domain: hope\nSource Domain: possessions\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : Even in backruptcy he managed\nto hang onto his car collection .\nTarget Domain: possession\nSource Domain: holding\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : A tigress in bed .\nTarget Domain: lust\nSource Domain: animal\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : H e’s really high .\nTarget Domain: euphoria\nSource Domain: up\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : W e were made for each other .\nTarget Domain: love\nSource Domain: part −whole\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : Many theories sprang up out of\nthe fertile soil of his discoveries\n.\nTarget Domain: theories\nSource Domain: beings\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : Her blood ran cold\nTarget Domain: fear\nSource Domain: cold\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : the contagion of democratic\nideas\nTarget Domain: belief\nSource Domain: disease\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : She is made of tougher stuff .\nTarget Domain: personality\nSource Domain: substance\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : Things are at a standstill .\nTarget Domain: progress\n1029\nSource Domain: motion\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : She took inventory of her\nbeliefs .\nTarget Domain: beliefs\nSource Domain: commodities\nExtract the conceptual metaphor from the\nfollowing sentence :\nSentence : But he he said , don’ t wash it\nIw a n n aw e a ri t .\nTarget Domain: washing\nSource Domain: not metaphoric\n1030\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6 & Limitations section\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 6 & Ethics section\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract & Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nWe did not use any AI writing assistants and all contents of the paper were written exclusively by the\nauthors.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4 & 5\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 1 & 2 & 3\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection 4.3.2\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 4.3.3\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4 & 5\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.2\nC □\u0013 Did you run computational experiments?\nSection 4 & 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.3.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1031\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 & 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4.3.3 & 5.2 & 5.3\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nThe authors of this paper performed the manual evaluation themselves.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nThe authors of this paper performed the manual evaluation themselves.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nThe authors of this papers were the evaluators so no consent form was needed.\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nThere were no ethical concerns with the evaluation method.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nWe re-used two already published datasets and only manually evaluated the model’s predictions.\n1032"
}