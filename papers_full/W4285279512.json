{
  "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
  "url": "https://openalex.org/W4285279512",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2138584266",
      "name": "Umang Gupta",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2565528786",
      "name": "Jwala Dhamala",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2007047437",
      "name": "Varun Kumar",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2552991278",
      "name": "Apurv Verma",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2922764414",
      "name": "Yada Pruksachatkun",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2981944953",
      "name": "Satyapriya Krishna",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2127286069",
      "name": "Rahul Gupta",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "Amazon (United States)",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A1808124004",
      "name": "Greg Ver Steeg",
      "affiliations": [
        "Amazon (United States)",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2117285942",
      "name": "Aram Galstyan",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3198409578",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W3135734416",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W4287691524",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W2753845591",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3186814609",
    "https://openalex.org/W2963381846",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W3114202959",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2971015127",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3103639864",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2938704169"
  ],
  "abstract": "Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, Aram Galstyan. Findings of the Association for Computational Linguistics: ACL 2022. 2022.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 658 - 678\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMitigating Gender Bias in Distilled Language Models\nvia Counterfactual Role Reversal\nUmang Gupta∗1, Jwala Dhamala 2, Varun Kumar 2, Apurv Verma 2,\nYada Pruksachatkun2, Satyapriya Krishna †4, Rahul Gupta 2,\nKai-Wei Chang†2 3, Greg Ver Steeg†1 2, Aram Galstyan 2\n1Information Sciences Institute, University of Southern California\n2Amazon Alexa, 3University of California, Los Angeles, 4Harvard University\numanggup@usc.edu, gupra@amazon.com\nAbstract\nLanguage models excel at generating coherent\ntext, and model compression techniques such\nas knowledge distillation have enabled their\nuse in resource-constrained settings. However,\nthese models can be biased in multiple ways,\nincluding the unfounded association of male\nand female genders with gender-neutral profes-\nsions. Therefore, knowledge distillation with-\nout any fairness constraints may preserve or ex-\naggerate the teacher model’s biases onto the\ndistilled model. To this end, we present a\nnovel approach to mitigate gender disparity in\ntext generation by learning a fair model dur-\ning knowledge distillation. We propose two\nmodiﬁcations to the base knowledge distilla-\ntion based on counterfactual role reversal—\nmodifying teacher probabilities and augment-\ning the training set. We evaluate gender po-\nlarity across professions in open-ended text\ngenerated from the resulting distilled and ﬁne-\ntuned GPT–2 models and demonstrate a sub-\nstantial reduction in gender disparity with only\na minor compromise in utility. Finally, we ob-\nserve that language models that reduce gender\npolarity in language generation do not improve\nembedding fairness or downstream classiﬁca-\ntion fairness.\n1 Introduction\nThe ever-increasing size of language models (LMs)\nhave increased their energy and compute require-\nments, making them impractical for many real-time\nresource-constrained applications such as personal\nassistants deployed on edge devices. To address\nthis issue, various approaches have been proposed\nto compress or distill these large models (e.g., Sanh\net al. (2019); Jiao et al. (2020); Hinton et al. (2015)).\nHowever, distillation techniques are designed to\nmimic the uncompressed LM (i.e., teacher model).\nThus, the societal biases encoded in the teacher\n*Part of this work was done as an intern at Amazon Alexa.\n†This paper describes work performed at Amazon.\nHe works in a hospital as a\nPrompt\n. . .doctor, treating the elderly with a variety, and\nby all accounts does an excellent work of medicine.\nGPT–2\n. . .physician and helps a lot of the patients.\nFair DistilGPT–2 (ours)\nShe works in a hospital as a\nPrompt\n. . .nurse and was in love with her mother and her\nbig brother, a small, shy, overweight woman.\nGPT–2\n. . .pediatric dermatologist who gets stitches but\nalso helps hospitals understand newborns . . .\nFair DistilGPT–2 (ours)\nFigure 1: Example texts generated by LMs under differ-\nent gender contexts (identiﬁed by the words ‘He’ and ‘She’).\nGPT–2 continues the prompt with the occupation word histor-\nically associated with the speciﬁc gender. Our approach aims\nto treat both genders equally.\nmodels (Bender et al., 2021; Bommasani et al.,\n2021; Sheng et al., 2021) will propagate to the dis-\ntilled models. In fact, our experiments show that\ndistilled models are adjudged to be more unfair\nthan their teacher model counterparts. In this work,\nwe devise techniques to train models that mitigate\nsocietal biases during knowledge distillation.\nOne way to demonstrate this manifestation of soci-\netal biases is by looking at text generated by LMs,\nas illustrated in Fig. 1. As such, the output text\nfocuses on different characteristics of the person,\nsolely based on which gender is mentioned in the\ncontext. To this end, we focus on reducing the dis-\nparity between groups during the language gener-\nation, considering the fairness deﬁnition for open-\nended text generations as proposed in Dhamala\net al. (2021) and Sheng et al. (2019). We propose\nan approach that uses counterfactual role-reversed\nsentences during knowledge distillation. In other\n658\nwords, our approach uses counterfactual texts that\nare generated by substituting mentions of one de-\nmographic group with the other. We employ an\nautomated way to generate these counterfactuals,\nrequiring only a paired list of words from each\ndemographic group.\nTypical knowledge distillation training loss has two\ncomponents: (a) the LM training loss such as cross-\nentropy to learn information from the training data,\nand (b) a loss that enforces similarity between out-\ncomes of teacher and student models1. The coun-\nterfactual knowledge is used to correct these loss\ncomponents in the following ways: (a) augmenting\nthe training set itself, which alters the training loss\nto learn from more equitable data; and (b) modify-\ning the teacher’s output toward more equitability\nso that the student learns from a more equitable\noutput distribution.\nWe ﬁrst demonstrate our method using English\nGPT2–small (Radford et al., 2019) as the teacher\nand a 6-layer GPT–2 (called DistilGPT–2) as the\nstudent model. We focus on binary gender dispar-\nities (male vs. female) and use the gender polar-\nity metric for profession prompts from the BOLD\ndataset (Dhamala et al., 2021) as the primary fair-\nness deﬁnition. We show that our approach lowers\nthe gender disparity in the generated text. Next,\nwe demonstrate the applicability of our approach\nfor ﬁnetuning English GPT2–small, i.e., using the\nsame architecture for teacher and student models\nin the distillation framework. Finally, we evalu-\nated the resultant model’s gender fairness on down-\nstream tasks such as Contextual Embedding Associ-\nation Tests (CEAT) (Caliskan et al., 2017) and ﬁne-\ntuning on Bios–Bias classiﬁcation task (De-Arteaga\net al., 2019). We ﬁnd that reduced disparity in open-\nended text generation does not necessarily lead to\nfairness on other tasks.\n2 Related Work\nLarge LMs embody societal biases that could result\nin harms such as misinformation, stereotype propa-\ngation, and disparate resource allocation (Bender\net al., 2021; Sheng et al., 2021). Multiple stud-\nies have shown that LMs are biased in producing\noutputs with negative connotations such as toxi-\ncity (Gehman et al., 2020; Zhou et al., 2021; Xu\n1The teacher model refers to the original LM, and the\nstudent model refers to the LM being trained. The latter\nusually has fewer parameters.\net al., 2021) and negative regard (Sheng et al., 2020,\n2021) towards minority populations. Others have\nshown that LMs encode prevalent gender biases,\nsuch as one gender being more associated with a\nparticular class of professions. Such biases can be\nrevealed via contextual embedding tests (Guo and\nCaliskan, 2021), stereotype tests (Sap et al., 2020;\nNangia et al., 2020), and evaluation of generated\ntexts (Dhamala et al., 2021; Sheng et al., 2019).\nFew works have also shown that LM can be biased\ntowards ideologies, e.g., Islam (Brown et al., 2020).\nApproaches to mitigate bias in LMs can be broadly\nsummarized as: (a) training or ﬁnetuning on a bal-\nanced dataset (Solaiman and Dennison, 2021; Di-\nnan et al., 2020)), (b) attaching preﬁx at inference\nor training time (Sheng et al., 2020), and (c) using\na bias or attribute classiﬁer (e.g., toxicity classiﬁer)\nto control fairness in text generation (Dathathri\net al., 2020; Liang et al., 2021; Liu et al., 2021;\nKrause et al., 2021). While all these debiasing ap-\nproaches can be used to mitigate bias in an LM\nafter it is distilled, no prior work aims to directly\ndebias and distill in a single step. Furthermore,\nthe majority of existing approaches focus on reduc-\ning toxic text generation (Solaiman and Dennison,\n2021; Dathathri et al., 2020; Liang et al., 2021; Liu\net al., 2021; Krause et al., 2021). Different from\nexisting works, we present an approach for fair\nknowledge distillation that aims to mitigate gender\nbias in text generated from the distilled models.\nOur approach is inspired by the counterfactual no-\ntion of fairness (Kusner et al., 2017) and intro-\nduces two modiﬁcations to the standard distilla-\ntion: (a) counterfactual data augmentation, and\n(b) using modiﬁed teacher probabilities. Coun-\nterfactual fairness and related notions have been\npreviously used for bias mitigation in hate speech\ndetection (Mostafazadeh Davani et al., 2021), word\nembeddings (Hall Maudslay et al., 2019; Lu et al.,\n2020; Zhao et al., 2018b), and coreference resolu-\ntion (Zhao et al., 2018a) tasks. Ours is the ﬁrst work\nthat uses counterfactual knowledge to achieve eq-\nuitability in text generation during distillation. Our\nmethod is also applicable when the student model\nor architecture is the same as the teacher model,\nand we have demonstrated it via experiments.\n3 Notion of Language Model Fairness\nWe focus on mitigating gender bias in open-ended\nlanguage generation from an LM. The bias is mea-\n659\nsured by assessing the tendency of the LM to as-\nsociate a speciﬁc set of professions to a speciﬁc\ngender, e.g., healthcare professions to female and\nengineering professions to male. As discussed in\nSheng et al. (2021), such societal biases may cause\na negative representational impact by propagating\nstereotypes, misrepresentations, or denigrations of\nsocial groups. We consider only binary gender in\nthis paper as LMs often do not encode sufﬁcient\nrepresentation of non-binary gender context, re-\nstricting a meaningful analysis (Dev et al., 2021).\nWe use a related counterfactual notion of fairness,\ncommonly studied in the NLP fairness literature,\nto motivate our fair distillation approach in Sec. 4.\nThe counterfactual notion of fairness (Kusner et al.,\n2017) adjudges a model fair if it generates similar\npredictions before and after swapping the sensitive\nfeatures in the input.\n4 Fair Knowledge Distillation via\nCounterfactual Role Reversal\nIn typical knowledge distillation, a smaller stu-\ndent model, imitating the behavior of the large\nteacher model, is obtained by using additional\ntraining signals from the target probabilities out-\nput by the teacher model. Let {x1 . . . xm}denote\nsequence of text tokens in a training sample, x<t\nor {x1 . . . xt−1}denotes sequence of tokens prior\nto t and boldface denote random variables. LMs\nsuch as GPT–2 model probability distribution of\nnext token P(xt|x<t) over the vocabulary V, i.e.,\nxt ∈V. Distillation loss is then deﬁned as follows:\nmin\nθ\n∑\nt\nCE(Pθ(xt|x<t), xi)+\nKL(Pθ(xt|x<t)∥Pteacher(xt|x<t)). (1)\nThis loss consists of two terms: (a) the cross-\nentropy (CE) between the predicted next token\nprobability and the observed token, and (b) the KL-\ndivergence between the output probabilities from\nthe teacher (Pteacher) and the student (Pθ) models.\nThe KL-divergence term provides a stronger train-\ning signal to the student, leading to more accurate\nand faster learning (Hinton et al., 2015).\nKnowledge distillation (Eq. (1)) will also transfer\nsocietal biases while transferring information from\nthe teacher model. To address this problem, we\npropose to infuse the bias mitigation strategy with\nknowledge distillation to obtain a less biased and\ncompact model. Our bias mitigating strategy is\nbased on the intuition that given a sequence such as\n‘She works as a’ and its counterfactual ‘He works\nas a’, a fair LM should generate similar texts. We\nmaterialize this intuition by encouraging student\nLM to learn similar distribution of probabilities for\na sequence of tokens and its counterfactual.\nTo this end, we propose two modiﬁcations to the\nbase distillation strategy: (a) Using counterfactual\nrole reversal to modify token probabilities of the\nteacher model; and (b) Using counterfactual role\nreversed data for model distillation. We study these\ntwo modiﬁcations independently and in various\ncombinations2.\n4.1 Counterfactual Role Reversal\nGiven a sequence of tokens referring to a partic-\nular demographic group, we want to generate a\ncounterfactual sequence of tokens referring to an-\nother related demographic. For example, suppose\nthe original text, referring to the female group was\n‘She is a mother of two kids and works as a soft-\nware engineer,’ we want to generate a counterfac-\ntual referring to the male group ‘He is a father of\ntwo kids and works as a software engineer .’ In-\nspired by existing works on counterfactual data\naugmentation for binary gender (Lu et al., 2020;\nHall Maudslay et al., 2019), we use word-swapping\noperations on the sequence of tokens to generate\ncounterfactual sequences. Speciﬁcally, we use a\ncurated dictionary of gender words with male ⇀↽\nfemale mapping, for instance, father →mother,\nshe →he, him→her, etc. We generate a counterfac-\ntual sequence of tokens from the original sequence\nby substituting the gendered word in the original\nsequence with a matching gendered word referring\nto the opposite gender from this dictionary3. See\nAppendix B for the curated dictionary sources and\nother implementation details.\n4.2 Modifying Teacher Probabilities\nNext, we discuss how to use counterfactual se-\nquences to modify knowledge distillation loss. In\nan open-ended language generation task, the LM\nproduces a natural continuation of text given some\ncontext or a prompt ( x<t). To this end, auto-\nregressive LMs such as GPT–2 predict the probabil-\nity distribution of the next token given the context\n2Our approach may use the same student model as the\nteacher, as we demonstrate in Sec. 5.\n3We found 96% of the generated data on manual analysis\nto be correct (See Appendix B.4 for details).\n660\ndoctor 0.5\nsurgeon 0.2\nnurse 0.1\n…\n…\nnurse 0.6\nreceptionist 0.2\ndoctor 0.1\n…\n…\ndoctor 0.3\nnurse 0.35\nsurgeon 0.1\nreceptionist 0.1\n…\n…\nGPT-2\nHe works in a hospital as a\nGPT-2\nShe works in a hospital as a\nOriginal Distribution Counterfactual Distribution\nModified Distribution\nFigure 2: Probability modiﬁcation using counterfactual text. Probability distributions are computed for the original text (left)\nand its counterfactual text (right). The modiﬁed probability distribution is computed using one of the functions from Table 1. For\ndemonstrating in this ﬁgure, we have used expMean operation.\nand previously generated tokens. The next token is\nsampled from the predicted distribution and added\nto the context to generate text. This process is con-\ntinued until a stopping criterion is met. Depending\non the gender present in the context, the teacher\nmodel may produce different probability distribu-\ntions over the vocabulary. If these predicted distri-\nbutions are directly used for student model training,\nit could transmit gender bias in the student model.\nTo mitigate this unchecked transference of gender\ndisparity, we modify the teacher probability of each\ntoken by using the next token probabilities from\nboth the original and the counterfactual context\n(or both genders) during student model training.\nWe combine them to boost the probability of more\nlikely tokens with both genders while the proba-\nbility of less likely tokens with one or both gen-\nders being suppressed or relatively unaffected (See\nFig. 2 for a visual illustration). We experiment with\ndifferent functions to combine these distributions.\nLet zt = logP(xt|x<t) and z′\ns = logP(xs|x<s)\nare the log-probability distributions (or logits) for\nthe original and the corresponding counterfactual\ncontext, respectively4. The new unnormalized log-\nits (z′′\nt) are obtained with max, mean, expMean,\nor swap operation and illustrated in Table 1. We\nnormalize z′′\nt so that it is a valid log distribution.\nIntuitively, the max operation would preserve the\nmost likely tokens among either context. The\nmean is similar to taking the product of the two\n4Due to sub-word tokens, the index of corresponding to-\nkens in the original and counterfactual text may be different.\nWe use index variable s to denote the corresponding token\nin the counterfactual sentence, indexed at t in the original\nsentence.\nFunction Operation\nmax z′′\nt = max{zt, z′\ns}\nmean z′′\nt =\nzt+z′\ns\n2\nexpMean z′′\nt = log\n( ezt +ez′\ns\n2\n)\nswap z′′\nt = z′\ns\nTable 1: Operations used to modify token probabilities.\ndistributions, thereby increasing the likelihood of\nwords that were more likely in both cases and low-\nering the likelihood of any other words. One may\nalso consider any weighted combination of z and\nz′. Infact, the swap operation is an extreme case of\na weighted combination with the weight of original\nlogits (i.e., zt) being 0. Finally, expMean is the\naverage of two distributions. Our approach is remi-\nniscent of post-processing approaches that modify\nthe next step probabilities during inference. How-\never, we adapt it here for gender fair-knowledge\ndistillation and use this procedure during training.\n4.3 Counterfactual Data Augmentation\nUsing modiﬁed probabilities to update the student\nmodel rectiﬁes the probability for the tokens gen-\nerated after the gendered word. However, it only\nprovides a weak signal by changing the log prob-\nabilities, and the training data may contain biases,\nwhich the student model can learn via cross-entropy\nloss (See Eq. (1)). To this end, we also augment\ncounterfactual data to the training set. Counter-\nfactual data augmentation has been successfully\nused for gender bias mitigation in various down-\nstream tasks such as static word embedding train-\ning (Hall Maudslay et al., 2019) and co-reference\nresolution (Lu et al., 2020). However, it has not\n661\nbeen explored in knowledge distillation or fair\nLM training for open-ended language generation.\nTherefore, we also experiment with counterfactual\ndata augmentation combined with the proposed\nnext-token logit update strategy.\nWe refer to our approaches as Equitable Role Al-\nteration (ERA). Primarily, the logit modiﬁcation\napproach reduces bias in the teacher model’s predi-\ncated probabilities, thus affecting only the KL di-\nvergence component. By contrast, counterfactual\ndata augmentation involves adding new samples to\nthe training set, affecting both loss components.\n5 Experiments\n5.1 Training Setup\nWe use GPT2–small, a 12 layer transformer-based\nLM comprising of ∼124M parameters, as the\nteacher model and a six-layer version of GPT–2\nas the student model. We use OpenWebText\ncorpus, which is an open-source reproduction of\nWebText corpus that was used to train GPT–2\nin Radford et al. (2019). Due to limitations in\ncomputational budget, we use 10% of the corpus\nfor training. We used the knowledge distillation\nprocedure presented in Sanh et al. (2019), but with-\nout the cosine loss between representations during\nknowledge transfer because adopting knowledge\ndistillation for fair learning requires correcting the\n‘biased knowledge’ from the teacher, but it is hard\nto amend biased contextual representations. This\napproach can also be used for fair ﬁnetuning of\nan LM by using the same teacher and the student\nmodel. In that case, one may initialize with the pre-\ntrained teacher’s weights. For fair ﬁnetuning ex-\nperiments, we use GPT2–small as both the teacher\nand the student. Details on training, text generation,\nand hyperparameters are provided in Appendix D.\n5.2 Evaluation of Open-ended Generation\nFairness. We assess gender fairness in English\ntext generation by evaluating the bias of an LM\nto associate a gender with gender-neutral profes-\nsions during open-ended text generation. For this,\nwe use the profession prompts and gender polarity\nmetrics from BOLD (Dhamala et al., 2021). These\nprompts are 10,195 sentence beginnings extracted\nfrom the Wikipedia articles and refer to 18 different\nprofession categories such as engineering, health-\ncare, arts & entertainment, etc. Some examples\nof BOLD profession prompts are ‘An animator is\nan artist who’ and ‘A ﬂight nurse is a registered.’\nTexts generated from the LMs with these prompts\nas contexts are evaluated for gender polarity.\nThe gender polarity score measures if the text is\nneutral, female–polar having words such as she,\nwoman, etc., or male–polar having words such as\nhe, boy, etc. It is computed by taking the maxi-\nmum of the normalized projection of each word\nvector in the LM generated text onto ⃗she − ⃗he.\nThe word vectors are computed on the debiased\nWord2Vec embeddings (Bolukbasi et al., 2016)5.\nWe use a threshold of 0.25 on the polarity score\nto label the text as male or female polar. For each\nprofession group, we compute the equitability ratio\nas min{m\nf , f\nm}, where m and f are the numbers of\ntext generations labeled as male and female polar,\nrespectively. The equitability ratio ∈[0, 1] with 1\nindicating equitable treatment. We report average\nand min equitability scores across all professions\nto summarize the disparity6.\nPerplexity/Fluency. For real-world applications,\nan LM should demonstrate high-quality genera-\ntions along with fair generations. To this end, we\nreport the perplexity of the wikitext-2 test set (Mer-\nity et al., 2017) as predicted by the trained LM.\nSimilar to Liu et al. (2021), we evaluate the ﬂuency\nof the completed prompts from BOLD. The ﬂuency\nis measured as the perplexity of generated text pre-\ndicted by the GPT2–large model. Lower perplexity\nand ﬂuency scores are better.\n5.3 Baselines and Other Methods\nFirst, we test the utility of our approach in knowl-\nedge distillation compared to teacher and distilled\nmodels trained without fairness constraints. We use\npre-trained GPT2–small (unfair teacher model) and\nDistilGPT–2 from the HuggingFace (HF) model\nrepository7. Since training hyperparameters and\ndataset used by DistilGPT–2 (HF) is different from\nours, we also train a DistilGPT–2 using our setup.\nNext, we compare our approach with two gender-\nbias mitigation approaches by applying them to\nthe distilled version of GPT–2 and GPT2–small\nfrom the HF repository. We ﬁnetune the distilled\nmodels with the counterfactual and original se-\nquences using only cross-entropy loss, which is\n5https://github.com/tolga-b/debiaswe\n6We note that this evaluation is not perfect. Gonen and\nGoldberg (2019) show that debiased word embedding still\nreserves some gender information for neutral words.\n7https://huggingface.co/models\n662\nModel Ppl (↓) Equitability (↑) Fluency (↓)\nMethod Mod fn. Aug. Average Min\nGPT2–small (Teacher) N/A N/A 25.17 0 .561 ±0.0136 0 .311 ±0.0162 54 .04 ±14.16\nDistilGPT–2 (HF) N/A N/A 39.25 0 .508 ±0.0142 0 .199 ±0.0283 122 .9 ±1.64\nDistilGPT–2 (Baseline) N/A N/A 40.88 0 .492 ±0.0107 0 .237 ±0.0256 80 .6 ±1.33\nDistilGPT–2 (ERA) mean no 40.91 0 .499 ±0.0086 0 .242 ±0.0299 116 .8 ±59.5\nDistilGPT–2 (ERA) max no 41.11 0 .565 ±0.0128 0 .313 ±0.0265 98 .2 ±1.64\nDistilGPT–2 (ERA) expMean no 41.11 0 .576 ±0.0095 0 .321 ±0.0264 230 ±263\nDistilGPT–2 (ERA) swap no 41.22 0 .587 ±0.0144 0 .303 ±0.0402 89 .2 ±2.06\nDistilGPT–2 (ERA) none yes 40.93 0 .748 ±0.0066 0 .497 ±0.0510 92 .4 ±0.65\nDistilGPT–2 (ERA) expMean yes 41.73 0 .892 ±0.0052 0 .693 ±0.0260 85 .5 ±0.49\nDistilGPT–2 (ERA) max yes 41.73 0 .901 ±0.0194 0 .713 ±0.0429 85 .4 ±0.24\nDistilGPT–2 (Finetuning) N/A yes 41.63 0 .869 ±0.0142 0 .632 ±0.0305 521 ±175.6\nDistilGPT–2 (Sheng et al., 2020) N/A N/A N/A 0.590 ±0.0131 0 .282 ±0.0284 296 ±337\nGPT2–small (ERA) max no 26.97 0 .489 ±0.0106 0 .268 ±0.0170 55 .89 ±0.35\nGPT2–small (ERA) none yes 26.60 0 .821 ±0.0081 0 .598 ±0.0417 54 .97 ±0.44\nGPT2–small (ERA) max yes 27.61 0 .884 ±0.0151 0 .687 ±0.0404 57 .19 ±5.43\nGPT2–small (Finetuning) N/A yes 28.56 0 .899 ±0.0116 0 .673 ±0.0553 54 .59 ±0.12\nGPT2–small (Sheng et al., 2020) N/A N/A N/A 0.839 ±0.0063 0 .596 ±0.0539 71 .44 ±0.87\nTable 2: Gender disparity in open-ended text generation as assessed by BOLD profession prompts for DistilGPT–2 and\nGPT2–small (result over 5 evaluation runs). Arrows indicate if higher (↑) or lower (↓) values are desired. Equitability measures\nvary from 0 to 1. We report the macro average of ﬂuency across all 18 profession groups. ERA is our approach.\nsimilar to CDA (Lu et al., 2020) and DAPT (Guru-\nrangan et al., 2020). We also compare with the bias-\nmitigation approach of Sheng et al. (2020), which\nsearches for adversarial prompts that increase the\nlikelihood of speciﬁcally curated fair texts.\n5.4 Results on Open-ended Text Generation\nTable 2 summarizes results for gender disparity mit-\nigation in open-ended generation for DistilGPT–2\nand GPT2–small. We observe that compared to the\nteacher GPT2–small model, which has more pa-\nrameters, the distilled versions (DistilGPT–2) are\nmore biased which is indicated by lower equitabil-\nity scores. Due to using only 10% sequences for\ntraining, our implementation of DistilGPT–2 has\nhigher perplexity than the HF’s version.\nFair Knowledge Distillation with DistilGPT–2.\nRows 4–7 in Table 2 show results of using only\nmodiﬁed teacher logits based on counterfactuals\n(Sec. 4.2) with various operations. Overall, these\nmodiﬁcations improve over the baseline Distil-\nGPT–2 model in terms of equitability ratios with\nonly a slight increase in perplexity. Models trained\nwith expMean, max, and swap scored similar or\nhigher equitability than the teacher model. The\nmean operation was the least effective at improv-\ning fairness. The approach that uses only coun-\nterfactual data augmentation (row 8 in Table 2)\nshowed more than 1.5×improvement in equitabil-\nity while keeping perplexity almost equal to the\nbaseline model (40.93 vs. 40.88). By contrast, the\ntwo-step process of creating a distilled model and\nthen ﬁnetuning with counterfactual data (using only\ncross-entropy loss) resulted in a worse perplexity of\n41.63 but better equitability. Our approach combin-\ning logit modiﬁcation and data augmentation (rows\n9–10, Table 2) provides better equitability among\nall the models. Compared to the two-step ﬁnetun-\ning approach (i.e., distillation then bias-mitigation),\nit has better equitability with similar perplexity.\nThe adversarial prompt-based approach of Sheng\net al. (2020) performs much worse in terms of fair-\nness. One of the reasons for this could be that the\nadversarial prompts are created to perform well on\na small curated dataset which may not generalize.\nWe omitted the perplexity values for this approach\nas it is not consistent with our evaluation process.\nWhen combining logit modiﬁcation and data aug-\nmentation, we experimented with modifying logits\nof both counterfactual and original text, and only\nof the original text. We found that the results with\nboth approaches are similar and report results of\nmodifying both texts in Table 2. The models ob-\ntained by combining the counterfactual data aug-\nmentation and logit update produce text with very\nlittle disparity and achieve the best fairness. Even\n663\nthough the ﬂuency metrics are low, the perplex-\nity for these models is higher. We noticed a high\nvariance in ﬂuency for some of the models. Upon\nfurther investigation, we found that the ﬂuency can\nbe very large for one of the profession groups, re-\nsulting in a large overall variance during macro\naveraging. We remark that ﬂuency is at best a noisy\nmeasure as it uses an LM to evaluate the outputs;\nperplexity should be considered a more reliable\nmeasure of LM quality. For further evaluations and\ndiscussion, we use models trained with the max\noperation, as the results with the max operation for\nlogit modiﬁcation, with and without counterfactual\naugmentation, were most consistent.\nFair Finetuning with GPT–2. We also experi-\nment with ﬁnetuning GPT2–small to train gender-\nfair models. The approach is similar to ﬁnetun-\ning with counterfactual augmented data but em-\nploys knowledge distillation loss instead. Table 2\n(rows 13–16) summarizes the results for training\nfair GPT2–small models. Unlike results with dis-\ntilled models, all the approaches are fairly compet-\nitive. We remark that ﬁnetuning and our best ap-\nproach have similar fairness performance, but our\napproach has better perplexity owing to improved\nlearning due to the additional KL-divergence term.\nHowever, models trained using only data augmenta-\ntion or logit modiﬁcation resulted in less equitabil-\nity. The student model has two loss components—\ncross-entropy and KL divergence loss. When em-\nploying only one of the techniques, the student\nmodel may receive training signals from unfair\nteacher logits in the former case and training data\nin the latter case, learning less equitable models.\nWe also note that only logit modiﬁcation with max\noperation led to worse results in terms of qual-\nity and fairness compared to the baseline GPT–2\nmodel. This could be due to the cross-entropy loss\nbeing the dominant training signal, and original\ntraining sequences may have spurious gender corre-\nlations. The adversarial-prompt approach of Sheng\net al. (2020) has lower ﬂuency than other models.\nOn further inspection of generated texts, we no-\nticed that the LM sometimes generates degenerate\nphrases related to the adversarial prompt instead\nof the actual prompt about the profession, leading\nto poor quality generations. Additionally, we did a\nhuman evaluation to assess the quality of generated\ntext (See Appendix A). We ﬁnd the quality of texts\ngenerated from our less biased GPT2–small (ERA)\nto be similar to GPT2–small.\n6 Gender Fairness on Other Tasks\nIt is often expected that different fairness measures\ndesigned for different but related tasks would be\ncorrelated. However, recently Goldfarb-Tarrant\net al. (2021) found that fairness measures for static\nword embeddings and downstream tasks do not\ncorrelate. To this end, we study if our fair text\ngeneration models improve fairness on other tasks.\n6.1 Bias in Contextual Embeddings\nWe evaluate if fairness in open-ended generation\nby LMs obtained via the proposed method also\ntransfers to the LM’s embeddings using the CEAT\nmetric (Guo and Caliskan, 2021). The WEAT met-\nric measures the effect size of social bias in a static\nembedding by computing the relative associations\nof two sets of target words ( e.g., career, ofﬁce;\nand home, family) with two sets of attribute words\n(e.g., girl, woman; and boy, man). CEAT extends\nWEAT to contextual embedding by computing a\ndistribution of effect sizes, each sample obtained\nby computing WEAT effect size on contextual em-\nbedding computed with a different context. CEAT\nsummarizes the combined magnitude of bias by\npooling effect sizes with a random-effects model.\nWe use three CEAT tests that measure gender bias:\n1) CEAT test 6 with attributes male/female names\nand targets career/family, 2) CEAT 7 with attributes\nmale/female terms and target math/arts, and 3)\nCEAT 8 with attributes male/female terms and tar-\ngets science/arts. See Appendix D for details.\nResults. According to the combined effect sizes\nmetric (known as Cohen’s d),d >0.5 and d >0.8\nare medium and large effect sizes, respectively.\nHowever, the absolute effect size is often used\nas the magnitude of bias (Goldfarb-Tarrant et al.,\n2021)8. As shown in Table 3, baseline models\nhave a larger effect size in tests 6 (male/female\nnames and career/family) and 7 (math/arts and\nmale/female terms). In test 8 (male/female terms\nand science/arts), there was not a strong bias in\nthe embeddings of baseline models. Overall, we\nobserve that the demonstrated fairness in LMs for\nopen-ended language generation in Sec. 5 is not\nalways reﬂected in the embeddings. For example,\nthe model trained using modiﬁed logits based on\nmax operation has a smaller absolute effect size for\n8P-values are not reported as it does not indicate the mag-\nnitude of the bias, and all models were most certainly biased.\n664\nModel CEAT Tests (Effect Sizes) Bios–Bias Classiﬁcation\nMethod Mod fn. Aug. Test 6 Test 7 Test 8 Accuracy ( ↑) TPRD (↓)\nGPT2–small (Teacher) N/A N/A 0.326 −0.139 −0.040 0 .818 0 .1060\nDistilGPT–2 (HF) N/A N/A 0.584 0 .114 −0.078 0 .813 0 .0982\nDistilGPT–2 (Baseline) N/A N/A 0.314 0 .311 −0.065 0 .815 0 .1003\nDistilGPT–2 (ERA) max no 0.245 0 .223 −0.113 0 .817 0 .0981\nDistilGPT–2 (ERA) none yes 0.366 0 .274 0 .016 0 .816 0 .1041\nDistilGPT–2 (ERA) max yes 0.532 0 .352 0 .260 0 .817 0 .1020\nGPT2–small (ERA) max no 0.212 0 .182 −0.036 0 .817 0 .1085\nGPT2–small (ERA) none yes 0.218 0 .162 0 .752 0 .817 0 .1031\nGPT2–small (ERA) max yes 0.293 0 .325 0 .268 0 .818 0 .1070\nTable 3: Downstream gender fairness evaluation. See Sec. 6.1 and 6.2 for details about CEAT and Bios–Bias task, respectively.\ntests 6 and 7 but higher for test 8 compared to the\nbaseline. Effect sizes on tests 7 and 8 have reduced\nwhen using the counterfactual data augmentation\nmethod, but it increased on test 6. Hence, the LM\nembedding fairness metric CEAT did not correlate\nwith the fairness of LM in open-ended text gen-\neration tasks. This ﬁnding agrees with Goldfarb-\nTarrant et al. (2021), but for contextual embeddings.\nThey observed that downstream fairness measures\nand static embeddings are not correlated.\n6.2 Fairness in Classiﬁcation Task\nWe evaluate the hypothesis that an LM that is less\nbiased in text generation should be less biased on\ndownstream tasks by ﬁnetuning various baselines\nand fairer versions of LM obtained in Sec. 5.4\non the Bios–Bias classiﬁcation task (De-Arteaga\net al., 2019) and evaluating the classiﬁer’s fairness.\nThe objective is to predict one of the 28 profes-\nsion classes from a person’s biography. We use\na weighted combination of all token embeddings\nwith a linear layer for classiﬁcation. Pre-trained\nweights are not updated. For training details, see\nAppendix D. Similar to De-Arteaga et al. (2019),\nwe take the average true positive rate difference\n(TPRD) between males and females across all pro-\nfessions as the fairness measure.\nResults. A fair model should have a similar true\npositive rate for both genders, i.e., TPRD ∼0.\nHowever, we observe from Table 3 that TPRD\nis around 0.1 for all the models, indicating that\nall models lead to equally unfair outcomes. De-\nArteaga et al. (2019) presented a simple debiasing\ntechnique of removing a set of predeﬁned gendered\nwords (such as he, she, mrs.) from the biographies\nbefore training, which resulted in an accuracy of\n0.815 and TPRD of 0.0658 with DistilGPT–2 as\nthe pre-trained model. Overall, this suggests that\nour method, even though effective in reducing dis-\nparity for open-ended text generation, is not ade-\nquate for this downstream task.\n7 Discussion and Limitations\nMitigating disparity across races. We con-\nducted preliminary experiments to test if the pro-\nposed approach can be extended to different race\ngroups. Similar to Dhamala et al. (2021), we con-\nsider race bias manifested via people’s names and\nrace-speciﬁc tokens across four races common in\nthe US: African, European or White, Hispanic &\nLatino, and Asian. We construct a many-to-many\nmapping that maps words referring to a given race\nto words referring to the other races for the counter-\nfactual generation. The rest of the method remains\nthe same as Sec. 4. For fairness evaluation, we\nuse race prompts from BOLD and regard classiﬁer\nfrom Sheng et al. (2019), which evaluates whether\nthe person in the text is portrayed as being ‘highly\nthought of.’ Results show that the LMs obtained\nwith the proposed approach were less biased in\ntreating different races similarly, indicating that the\nproposed approach can be extended to other non-\nbinary groups. However, the improvements were\nnot as signiﬁcant as gender bias mitigation, leav-\ning plenty of scope for improvement left for future\nwork. We describe the results and experiments in\nmore detail in Appendix C.\nCounterfactual data generation. Dictionary-\nbased word-swapping is a simple and effective\nmethod for counterfactual generation (Lu, 2020;\nZhao et al., 2018a). However, blind word swap-\nping can also result in factually and/or grammati-\ncally incorrect texts. To quantify these errors, we\nmanually evaluated 500 randomly sampled coun-\n665\nterfactual texts for gender category. We found that\n22 (4.4%) of these sentences were incorrect (See\nAppendix B.4). In this paper, we demonstrate that\ndespite counterfactual data generation not being\nperfect, it can effectively reduce the gender biases\nin the model. We expect our bias mitigation ap-\nproach to beneﬁt from further research in coun-\nterfactual data generation, especially for reducing\nrace disparity.\n8 Conclusion\nWe proposed techniques to use counterfactual in-\nformation during knowledge distillation to mitigate\ngender bias in LMs. In experiments, we show that\nthis approach improves fairness in text generation,\nbut it does not simultaneously enhance fairness\non LM embedding and downstream classiﬁcation\ntask. LMs have become the Swiss army knife of\nNLP because modeling next word probabilities can\nlearn versatile models that are effective on many\ntasks. It was surprising that reducing gender dis-\nparity in text generation had little effect on other\ndownstream tasks. This ﬁnding underscores the im-\nportance of evaluating LM fairness along multiple\nmetrics and tasks.\n9 Broader Impact and Ethics Statement\nAs language models become prominent, it is im-\nperative to understand and mitigate various harms\nthat they may provoke (Solaiman et al., 2019; Bom-\nmasani et al., 2021). Moreover, to make language\nprocessing resource-efﬁcient, more focus should\nbe on achieving good performance with smaller\nmodels. Our work is a step towards mitigating such\ndamages but not the only remedy possible. We\ndemonstrated effective ways to incorporate coun-\nterfactual knowledge during training to avoid a\ntwo-step training process. The resulting model\ngenerates less disparate text for different groups\nwhile being equally or more accurate. However, as\nwe have discussed in Sec. 6, this does not make\nthe model fair with regards to other gender fair-\nness measures. Our results essentially echo the\nargument made in Barocas et al. (2019) that it is\nmeaningless to ascribe fairness to a model. In-\nstead, fairness should be thought of, keeping the\ntask and outputs in mind. This work in mitigating\nfairness is limited because we only focus on biases\nin English language generation. Other works, such\nas Zmigrod et al. (2019), have identiﬁed the dif-\nﬁculties in transferring these approaches to other\nlanguages. Moreover, we have considered binary\ngender, which does not capture all the real-world\ncomplexities. More critically, our assessment of\nfairness for open-ended text generation has relied\non fair deﬁnitions and measures from Dhamala\net al. (2021) and Sheng et al. (2019). One should\ninterpret the results with this in perspective. Some\nrecent works, such as Blodgett et al. (2020, 2021);\nGonen and Goldberg (2019), have demonstrated\ncritical ﬂaws in other fairness measures. For exam-\nple, Blodgett et al. (2021) found that benchmark\ndatasets designed for measuring stereotyping be-\nhavior of LMs such as StereoSet (Nadeem et al.,\n2021) and CrowS-Pair (Nangia et al., 2020) are am-\nbiguous and have several pitfalls which can even\noperationalize stereotyping. Our approach uses\ncounterfactual data, which may inherit the ﬂaws in\noriginal data or introduce new errors. Users should\nuse appropriate ﬁlters/mechanisms to ensure the\nquality of counterfactual data used for training.\nFinally, we propose approaches to create less bi-\nased LMs. However, similar to howgifts were used\nas weapons in Le Guin’s Gifts (Le Guin, 2006), our\napproach can be repurposed to cause even more\ndisparate treatment. For example, one may remove\nthe mention of a speciﬁc race or gender completely\nfrom the training set to create a dystopian LM that\ndoes not acknowledge that group or entity’s ex-\nistence or the inaccuracy of counterfactual gener-\nation may cause LM to learn from ﬁctional and\nnon-grammatical texts. Nevertheless, we hope that\nour work will inspire more good than harm.\nReferences\nSolon Barocas, Moritz Hardt, and Arvind Narayanan.\n2019. Fairness and Machine Learning . fairml-\nbook.org.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the Dan-\ngers of Stochastic Parrots: Can Language Models Be\nToo Big? In Proceedings of the 2021 ACM Conference\non Fairness, Accountability, and Transparency, FAccT\n’21, page 610–623, New York, NY , USA. Association\nfor Computing Machinery.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Proceed-\nings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 5454–5476, Online.\nAssociation for Computational Linguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\n666\nNorwegian salmon: An inventory of pitfalls in fairness\nbenchmark datasets. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers) ,\npages 1004–1015, Online. Association for Computa-\ntional Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai. 2016.\nMan is to computer programmer as woman is to home-\nmaker? debiasing word embeddings. In Advances\nin Neural Information Processing Systems 29: Annual\nConference on Neural Information Processing Systems\n2016, December 5-10, 2016, Barcelona, Spain , pages\n4349–4357.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities and risks of\nfoundation models. ArXiv preprint, abs/2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020. Lan-\nguage models are few-shot learners. In Advances in\nNeural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases. Sci-\nence, 356(6334):183–186.\nJoshua Comenetz. 2016. Frequently occurring sur-\nnames in the 2010 census. United States Census Bu-\nreau.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models: A\nsimple approach to controlled text generation. In 8th\nInternational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net.\nMaria De-Arteaga, Alexey Romanov, Hanna Wallach,\nJennifer Chayes, Christian Borgs, Alexandra Choulde-\nchova, Sahin Geyik, Krishnaram Kenthapadi, and\nAdam Tauman Kalai. 2019. Bias in Bios: A case study\nof semantic representation bias in a high-stakes setting.\nIn Proceedings of the Conference on Fairness, Account-\nability, and Transparency , FAT* ’19, page 120–128,\nNew York, NY , USA. Association for Computing Ma-\nchinery.\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar-\njun Subramonian, Jeff Phillips, and Kai-Wei Chang.\n2021. Harms of gender exclusivity and challenges\nin non-binary representation in language technologies.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 1968–\n1994, Online and Punta Cana, Dominican Republic. As-\nsociation for Computational Linguistics.\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya\nKrishna, Yada Pruksachatkun, Kai-Wei Chang, and\nRahul Gupta. 2021. BOLD: Dataset and metrics for\nmeasuring biases in open-ended language generation.\nIn Proceedings of the 2021 ACM Conference on Fair-\nness, Accountability, and Transparency , FAccT ’21,\npage 862–872, New York, NY , USA. Association for\nComputing Machinery.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020. Queens\nare powerful too: Mitigating gender bias in dialogue\ngeneration. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 8173–8188, Online. Association\nfor Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxic-\nityPrompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pages 3356–\n3369, Online. Association for Computational Linguis-\ntics.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long Pa-\npers), pages 1926–1940, Online. Association for Com-\nputational Linguistics.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender bi-\nases in word embeddings but do not remove them. In\nProceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 609–614, Minneapolis,\nMinnesota. Association for Computational Linguistics.\nWei Guo and Aylin Caliskan. 2021. Detecting emer-\ngent intersectional biases: Contextualized word embed-\ndings contain a distribution of human-like biases. In\nProceedings of the 2021 AAAI/ACM Conference on AI,\nEthics, and Society , AIES ’21, page 122–133, New\nYork, NY , USA. Association for Computing Machin-\nery.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and\nNoah A. Smith. 2020. Don’t stop pretraining: Adapt\nlanguage models to domains and tasks. In Proceedings\nof the 58th Annual Meeting of the Association for\n667\nComputational Linguistics, pages 8342–8360, Online.\nAssociation for Computational Linguistics.\nRowan Hall Maudslay, Hila Gonen, Ryan Cotterell,\nand Simone Teufel. 2019. It’s all in the name: Mitigat-\ning gender bias with name-based counterfactual data\nsubstitution. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pages 5267–\n5275, Hong Kong, China. Association for Computa-\ntional Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv\npreprint, abs/1503.02531.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes,\nand Yejin Choi. 2020. The curious case of neural\ntext degeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020 , pages 4163–4174,\nOnline. Association for Computational Linguistics.\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shaﬁq Joty, Richard\nSocher, and Nazneen Fatema Rajani. 2021. GeDi: Gen-\nerative discriminator guided sequence generation. In\nFindings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 4929–4952, Punta Cana, Do-\nminican Republic. Association for Computational Lin-\nguistics.\nMatt J. Kusner, Joshua R. Loftus, Chris Russell, and\nRicardo Silva. 2017. Counterfactual fairness. In Ad-\nvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing\nSystems 2017, December 4-9, 2017, Long Beach, CA,\nUSA, pages 4066–4076.\nUrsula K Le Guin. 2006. Gifts. Wadsworth Publishing.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency,\nand Ruslan Salakhutdinov. 2021. Towards understand-\ning and mitigating social biases in language models.\nIn Proceedings of the 38th International Conference\non Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 6565–6576. PMLR.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts. In\nProceedings of the 59th Annual Meeting of the Associ-\nation for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 6691–6706,\nOnline. Association for Computational Linguistics.\nDaming Lu. 2020. Masked reasoner at SemEval-2020\ntask 4: Fine-tuning RoBERTa for commonsense rea-\nsoning. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation , pages 411–414, Barcelona (on-\nline). International Committee for Computational Lin-\nguistics.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2020. Gender bias in neural\nnatural language processing. In Logic, Language, and\nSecurity, pages 189–202. Springer.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture models.\nIn 5th International Conference on Learning Represen-\ntations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nShubhanshu Mishra, Sijun He, and Luca Belli. 2020.\nAssessing demographic bias in named entity recogni-\ntion. ArXiv preprint, abs/2008.03415.\nAida Mostafazadeh Davani, Ali Omrani, Brendan\nKennedy, Mohammad Atari, Xiang Ren, and Morteza\nDehghani. 2021. Improving counterfactual generation\nfor fair hate speech detection. In Proceedings of the 5th\nWorkshop on Online Abuse and Harms (WOAH 2021),\npages 92–101, Online. Association for Computational\nLinguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers) ,\npages 5356–5371, Online. Association for Computa-\ntional Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A challenge\ndataset for measuring social biases in masked language\nmodels. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1953–1967, Online. Association for\nComputational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\nAlban Desmaison, Andreas Köpf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. 2019. Pytorch: An im-\nperative style, high-performance deep learning library.\nIn Advances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 8024–8035.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\n668\nover 100 billion parameters. In KDD ’20: The 26th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, CA, USA, August 23-\n27, 2020, pages 3505–3506. ACM.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv\npreprint, abs/1910.01108.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social bias\nframes: Reasoning about social and power implications\nof language. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 5477–5490, Online. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2020. Towards Controllable Biases\nin Language Generation. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 3239–3254, Online. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. Societal biases in language gen-\neration: Progress and challenges. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume\n1: Long Papers) , pages 4275–4293, Online. Associa-\ntion for Computational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. In Pro-\nceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 3407–3412, Hong Kong,\nChina. Association for Computational Linguistics.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps, et al.\n2019. Release strategies and the social impacts of lan-\nguage models. ArXiv preprint, abs/1908.09203.\nIrene Solaiman and Christy Dennison. 2021. Pro-\ncess for adapting language models to society (palms)\nwith values-targeted datasets. ArXiv preprint ,\nabs/2106.10328.\nKonstantinos Tzioumis. 2018. Data for: Demographic\naspects of ﬁrst names.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,\nSylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander Rush. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations , pages\n38–45, Online. Association for Computational Linguis-\ntics.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gu-\nrurangan, Maarten Sap, and Dan Klein. 2021. Detox-\nifying language models risks marginalizing minority\nvoices. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies ,\npages 2390–2397, Online. Association for Computa-\ntional Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018a. Gender bias in\ncoreference resolution: Evaluation and debiasing meth-\nods. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 2 (Short Papers) , pages 15–20, New Orleans,\nLouisiana. Association for Computational Linguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018b. Learning gender-neutral word em-\nbeddings. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 4847–4853, Brussels, Belgium. Association for\nComputational Linguistics.\nXuhui Zhou, Maarten Sap, Swabha Swayamdipta,\nYejin Choi, and Noah Smith. 2021. Challenges in auto-\nmated debiasing for toxic language detection. In Pro-\nceedings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3143–3155, Online. Association\nfor Computational Linguistics.\nRan Zmigrod, Sabrina J. Mielke, Hanna Wallach, and\nRyan Cotterell. 2019. Counterfactual data augmen-\ntation for mitigating gender stereotypes in languages\nwith rich morphology. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics, pages 1651–1661, Florence, Italy. Association\nfor Computational Linguistics.\n669\nSupplementary: Mitigating Gender Bias in Distilled Language Models\nvia Counterfactual Role Reversal\nA Human Evaluation of Generated Text\nWe evaluate the quality of text generated from\nGPT2–small, fair-GPT2–small (ERA), and Sheng\net al. (2020) (adversarial prompt method with\nGPT2–small). We randomly sampled 300 prompts\nand their corresponding text generations from all\nthree models. We then asked annotators to annotate\nfor two tasks. The ﬁrst task was to rank the genera-\ntion quality among three sentences generated with\nthe same prompt. The labels for the ranking task\nwere: 1 – Worst, 2 – Medium, and 3 – Best. The\nsecond task was to rate the generation quality on a\nscale from 1–6 — 1 being very poor, 2 being poor,\n3 being fair, 4 being average, 5 being good, and 6\nbeing excellent. Unlike the ranking task, the ratings\nare independent of generations from other models\nfor the same prompt. When rating the quality, we\nasked the annotators to focus on the following prop-\nerties of the text.\n• Is it gibberish and nonsensical?\n• Does the generation ﬁt the prompt?\n• Is the text grammatically correct?\n• Is the text consistent and coherent? Is the\ngeneration meaningful?\n• Could the text have been written extracted\nfrom news, books, etc.?\n• Could the text have been written by a Human?\nWe also provided some example annotations, as\nshown in Table 4.\nThe four annotators participating in these tasks are\nvolunteers proﬁcient in English, originating from\nvarious countries but presently or in the past stud-\nied/worked in the US, and familiar with language\nmodels. The annotators were informed of the re-\nsearch problem. We followed our institution’s re-\nview process and approval guidelines for these an-\nnotation tasks. For each sentence, we collected\nthree annotations. We only keep the ones where at\nleast two annotators agree out of all annotations.\nThe mean and standard deviation of rankings for\ngenerations from GPT2–small, fair GPT2–small,\nand Sheng et al. (2020) were 2.55 ±0.55, 2.34 ±\n0.64, and 1.12 ±0.41, respectively. Text gener-\nated from GPT2–small is ranked highest most of\nthe time. However, the fairer GPT2–small ob-\ntained with our method is a close second. The\naverage ratings for generations from GPT2–small,\nfair GPT2–small (ERA), and Sheng et al. (2020)\nwere respectively, 3.01 ±1.04, 2.707 ±1.07, and\n1.12 ±0.41. Consistent with the ranking results,\nGPT2–small received the highest rating, followed\nclosely by the generations from fairer GPT2–small\nobtained with our method. Both ranking and rating\nresults indicate that our approach retains most of\nthe performance while reducing gender disparity\nin the generated text. We ﬁnd that Sheng et al.\n(2020) resulted in low-quality generations. As also\ndiscussed in the main paper, this could be because\nthe adversarial prompts are designed to increase the\nlikelihood of specially curated fair text and may not\nwork for diverse prompt datasets like BOLD, which\ncontains diverse sentences beginning from various\nWikipedia articles. Moreover, we also noticed that\nthe adversarial prompts could lead to generation\nunrelated to the actual prompt and generate text re-\nferring to phrases in the adversarial prompt instead.\nWe provide some example text generations from\nthese approaches in Table 5.\nB Counterfactual Role-Reversal Data\nGeneration\nCounterfactual sequences were generated for ∼\n78% and ∼65% of the training sequences for gen-\nder and race domain experiments, respectively. We\nlimit sequence lengths to 1024 for training. We\ngenerate one counterfactual sequence for every se-\nquence in the training set that has words match-\ning with our lists and referring to the demographic\ngroups. The word lists are described next.\nB.1 Gender Word Lists\nTo generate counterfactual texts for gender dispar-\nity experiments, we create mappings between male-\nto-female words and vice versa using word lists\nfrom Zhao et al. (2018a)9. We consider some addi-\ntional words to mappings derived from the above\nlists, shown in Table 6.\n9Speciﬁcally, we use word lists available at https:\n//github.com/uclanlp/corefBias/blob/\nmaster/WinoBias/wino/extra_gendered_\nwords.txt, and https://github.com/uclanlp/\ncorefBias/blob/master/WinoBias/wino/\ngeneralized_swaps.txt\n670\nQuality Description Examples\nvery poor irrelavant to context, gib-\nberish\n1. the person was known for 129$ inter got upperSDA here xxxx. ayayaya\nterrible\npoor not written by human,\nweird, illogical, repetitive\n1. the person was known for dance, murder, dance, murder, dance\n2. the person started working as a racist to get pregnant because in the sense\nof being equal to female\nfair\nmost likely not written by\nhuman, partly ﬁts the con-\ntext\n1. the person earned money by sending spam by trading in his domain .\n2. the person earned money by selling his soul to companies and politicians\nright from the start . – 11973 , Mich . ,*\naverage partly natural sounding,\npartly ﬁts the context\n1. the person earned money by delivering sweets as a Valentine ’s gift , The\nNew York Times reported .\n2. the person had a part-time job as a local caterer . He worked as a hair\nstylist in an Atlanta apartment ,\ngood\nnatural sounding, ﬁtting\nthe context, may contain\nminor contradictions\n1. the person had a job as a recruiter for recruitment agencies in the west of\nthe country ,-\nexcellent natural, ﬂuent, human-\nwritten, ﬁtting the context\n1. the person worked for a high-security institution, and one day he went in\nto work only to ﬁnd that he could not log in to his computer terminal.\n2. the person was famous for her work on radioactivity and twice a winner\nof the Nobel Prize\nTable 4: Generated texts and quality ratings that were shown as examples to annotators.\nB.2 Race Word Lists\nWe focus on four US-speciﬁc races: Asian-\nAmerican, Hispanic & Latino-American,\nEuropean-American, and African-American. To\ncreate counterfactual text for mitigating racial dis-\nparity, we use word sets from different categories.\nTable 7 shows the word sets we have used. We\nprocess and use these word sets as follows.\n• For words in the country and race category, we\nappend ‘ American’ and ‘-American’ and their\nequivalent lower case versions and consider\nthese as the actual word sets. Similarly, we\nconsider both capital and lower case variations\nof the country and race terms.\n• For words in the color category of Table 7,\nwe use both capital/lower cases and singu-\nlar/plural versions.\n• We use two indicators of Latin race ‘latino’\nand ‘latina’ and swap them with words from\nAsian-, African- & European- American coun-\ntries word sets but not vice versa.\n• We created the list of ﬁrst names from\nTzioumis (2018). They provide prominent\nﬁrst names and the percentage of times this\nname belonged to a particular race. We use\nnames that are 100% of the time assigned to\na particular race and that are in the top-100\nnames for each race. We use the capital case\nversion of the ﬁrst names.\n• We collected the list of common last names\nfrom Comenetz (2016) and used the capital\ncase version. Other works have also used\nnames as the indicator of race (Mishra et al.,\n2020; Caliskan et al., 2017).\nWe replace the word from a speciﬁc row and col-\numn with words from other columns in the same\nrow randomly to create a counterfactual text. For\nexample, the original text, ‘With each new location,\nVazquez and Maritza must maintain the quality\ntheir fans have come to associate with the brand.’\nis converted to ‘With each new location, Banks and\nMaritza must maintain the quality their fans have\ncome to associate with the brand.’. Similarly, in the\ncase of gender, the text ‘Your father was a drum-\nmer in a rock band?’ is converted to ‘Your mother\nwas a drummer in a rock band?’.\nB.3 Note about Sub-word Embeddings\nWe use counterfactual text in two ways, as de-\nscribed in Sec. 4. Due to sub-word embeddings,\nthe length of the counterfactual sequence may not\nbe the same as the original. This is particularly\nproblematic for modifying probability distribution\nas we have to know the exact location of the cor-\nresponding token in the counterfactual and origi-\n671\nGenerations with GPT2 ERA\n1 In their study, geographers use four ices as habitats. The icy crust of Antarctica is seen as an arid backdrop for millions of\nyears. But the same frozen crust, making up just over one third of the continent, was striking new shades of blue on Sept.\n24, 2010, when a glacier erupted into Greenland’s Lhotse Basin of glacial melt. Journal reference: Geophysical Research\nLetters, doi:10\n2 Biotechnology ﬁrms can contribute to future ills and possibilities of human development, this paper suggests. Although the\nlink between the mass production of cellulose, corn, and protein on species-to-species conversion studies and the future of\nfarmers utilizing these crops is well-established, and has been shown to be useful for food-factory improvement, a plethora\nof gene-ﬁxing (gen-catalogical) techniques could be added to the food production process as a way to understand other\n3 A clinical pharmaceutical scientist is a licensed, practicing uthymologist who provides the therapeutic data for clinical\ntrials. Because of their clinical studies and publication that are published, such experts are called “Nutronans” because they\nprovide practical and practical information to the public about the treatment of many patients. Recently, scientists published\nthe results of a double-blind trial. The ﬁrst of the ﬁrst trials to be published with the Food and Drug Administration (FDA\n4 A choreographer is one who creates ides, styles a verse and dances a magic formula on the disco stage. So far, I’ve\nbeen unable to ﬁnd someone whose chops I have known, despite my unﬂattering reputation for being a professional\nchoreographer. But one thing I’ve been more familiar with than a career that started with the punk songs of Riot and Dead\nKids has changed my career. As many of you know, in 1990, Comixan Echols, then\n5 Military logistics was already practiced in ersed in Persia (some time after the ﬁrst Persian Oceanic, this practice may have\nbeen described as the business of the king). That. He may have been telemarking, as we had started to accept that in Persia.\n• From the mid 1800s through the late 1800s. • In between his late forties and after his death in 1874. The chronological\ndiscrepancy between Persia and Iran continued through the\n6 Recorded social dances of the late 16th vernacular style are often followed by particular theatrical content as outlined\nby the Scotiablogists of 1289-1500. It is the music and the whole ensemble around which the music has been written.\nTake for instance the carnatonic pavilion-kiln’s on the page as given by Sermon Charles that Nennius prophesied, with the\nsuccess of Zebes and Maelto, that when the whole orchestra\nGenerations with GPT2\n1 There are now school counselors in Shandong County Jail. These are going to help kids transition to a safer place.\" If you\nwant to stay informed about our events, let us know by commenting below or by on our Facebook page or following us on\nTwitter @Daydreamscout Photo: iStockphoto.com/XiaoXiho\n2 The public relations ofﬁcer or chief communications ofﬁcer said, The military had no plans to write any violence into\nAfghanistan’s speech. The next day, the term \"andoregional incitement\" was deployed in the speech and incident statement\nwhich was based on statements made by Rahmatullah Khan. http://www.newstimes.co.nz/national/1984338/displaying-\nsmoke-and-walls-in-republicans-speech\n3 Many nurse educators have a clinical specialty in providing a safe home for social service providers, providing consultations\nand providing outpatient services for breastfeeding moms. There are many other professional services provided by nurse\neducators that can be critical to the success of a program such as Attic with mother and child care provider services. We\ncannot emphasize enough how important breastfeeding and the health of mothers and their babies is as a provider of social\nservices. It’s also important to remember that these practices and\n4 Breakdancing is typically set to songs which are considered better by a female audience, the more options the audience\nhas to choose between. Other examples are Subverted Sadness, Sadness Goes Home, if sung by The Bugles in The Turner\nHour, or hope & change if a male audience chooses to reject it.\n5 For biologists, knowledge must be usefully with the interpretation of natural conditions and with the knowledge necessary\nto deﬁne their factors and forms. The objective of any study is to show some evolution of organisms that were designed\nand or selected, or allowed to evolve, or which have exhibited a novel ability by its natural context and the conditions of\nthe environment. The ability to learn needs to be clearly quantiﬁed as a function of one or more physical, chemical, or\nbiological factors and, depending\nGenerations with Sheng et al. (2020)\n1 Scenography is the seamless synthesis of vernacular, visual and rhythmic characteristics,\" said Jon Forbes, the development\nmanager at Widtat-MacMulling GmbH. Slating is the next stage for Widtat. Upholstered by the amount of data it can\ncache on its servers, Widtat launched with a working set of domains at the end of October and has expanded further over\nthe course\n2 The movement director may create, or research More Exploring concepts Explore the new direction under the lead of\nTakahiro Sasaki, an engineering genius. The lead teams of the past three years have worked on a range of graphical APIs\nthat can provide a visual approach to hardware Soiling temperature maps (sometimes called -HotCatter), which reveal\ntemperatures associated with various components Through testing of application applications to monitor\nTable 5: Examples of generations that the human annotators labeled as having a quality ≥4 (on a range 1 −6 where 6 is\nexcellent) from different GPT2–small models.\n672\nnal sentence. To this end, we generate ‘counter-\nfactual token sequences’ during training instead\nof ‘counterfactual sentences’. We ﬁrst create tok-\nenized versions of word lists, i.e., a set of tokens\nrepresenting a word (e.g., father is represented by\n{2988}) are mapped to another set of tokens (e.g.,\nmother is represented by {2802}). Given a sen-\ntence such as ‘Your father was a drummer in a rock\nband?’, it is ﬁrst tokenized as {7120, 2988, 373,\n257, 34269, 287, 257, 3881, 4097, 30}then con-\nverted to {7120, 2802, 373, 257, 34269, 287, 257,\n3881, 4097, 30}(‘Your mother was a drummer in\na rock band?’).\nAlso, depending on where and how the word\noccurs, it can be tokenized differently. To illustrate,\nconsider the word ‘he’ in the next sentence. ‘He\nshould have arrived, but he has not arrived yet ’.\nClearly, the word ‘he’ appears in two different\nforms — capital-case and lowercase. Other\nforms are also possible. Also, GPT–2 tokenizer\noften has white space at the beginning of the\ntoken in its vocabulary. For this reason, we\nconsidered the word and some of the possible\nvariations that can occur in the text. The next\nexample best explains these variations. If the word\nwere ‘he’, we use following variations — he |\nhe| he,| he.| he’| he”|‘he |“he |He |‘He |“He .\nB.4 On Limitations and Correctness of\nCounterfactual Sentences\nFor counterfactual data generation, we use a\ndictionary-based word-swapping approach. Such\na naive approach has some obvious limitations as\nit does not guarantee the grammatical and factual\ncorrectness of the generated sentences. However,\nwe hypothesize that while this approach can poten-\ntially generate incorrect data for some examples,\noverall, it is still a simple yet effective method to\ngenerate counterfactual data. In order to verify our\nhypothesis, we randomly sampled 500 sentences\nfrom the generated counterfactual data for gender\ncategory and analyzed these for correctness. Out of\nthese 500 sentences, we found 22 (4.4%) incorrect\nsentences. Most of the errors are related to incor-\nrect pronoun references, such as a male name being\nused with ‘she’ as a reference. One such example\nis ‘Onelki Garcia had another interesting outing\nas she only allowed 1 hit, but did walk three and\nlasted just 2.2 innings.’\nWe emphasize that the main focus of the paper is\nnot to generate better counterfactual data but to\nshow that counterfactual data can be used to miti-\ngate bias effectively during knowledge distillation.\nWe expect our proposed approach to further beneﬁt\nfrom advances in counterfactual data generation.\nC Mitigating Racial Disparity\nCounterfactual Data Generation. While not\nthe main focus of this study, we also conducted\nexperiments to mitigate race bias, manifested to-\nwards the names of people from various races and\ncertain race-related phrases/words. Since we con-\nsider more than two races and there is no one-to-\none mapping between names, we cannot use the\nsame one-to-one substitution rule for counterfac-\ntual data generation as earlier in this case. Hence,\nwe construct a many-to-many mapping that maps\nmultiple words in a given race to multiple words in\nthe remaining races. For each word in the sequence\nof tokens referring to one race, we substitute it with\na randomly chosen word from the corresponding\nwords-set from another race. Additional details\nand dictionaries used for counterfactual sentence\ngeneration are in Appendix B.\nRacial Fairness Measure. We use race prompts\nfrom the BOLD Dataset to measure racial disparity\nand consider four races — Asian American, Eu-\nropean American or Whites, African American or\nBlacks, and Hispanics & Latin Americans. We use\nthe regard classiﬁer to measure regard for each race.\nThe regard classiﬁer has three categories — posi-\ntive, negative, and neutral regard. Intuitively, the\nregard classiﬁer measures if sentences cause group\nA to be more highly thought of than group B. If this\nis the case, then the language model perpetuates\nbias towards group B (Sheng et al., 2019). To this\nend, we measure the ratio of positive and negatively\nregarded sentences for each racial group. A fair\nLM should have the same ratio for all the races. We\nreport the variance across groups for each model to\ncapture this intuition, and lower variance would im-\nply more fair treatment. We also report the fraction\nof generated sentences labeled as having positive,\nnegative, and neutral regard.\nResult. Table 8 shows the result of mitigating\nracial disparity in text generation with our pro-\nposed approach that exploits counterfactual data.\nWe generated counterfactual data for this purpose\nby replacing mentions of one racial group with\nthe other (see Appendix B for details). The base-\n673\nline pre-trained models from Hugging-Face have\nconsistently higher regard ratios than the baseline\nmodel we trained, indicating that they generated\nmore positive regard than our models. However,\nthese have more variance across groups, indicating\nmore disparate treatment in terms of regard.\nWe note that our counterfactual mitigation ap-\nproach using both logit modiﬁcation and augmen-\ntation is promising for reducing different regard to\ndifferent races, but the improvement is not substan-\ntial. This could be due to our simple counterfac-\ntual generation implementation since we randomly\nreplace race-related words. We replace ﬁrst and\nlast names independently, which could create mis-\nmatched names. There has been some work on\nimproving counterfactual sequence generation and\nstudying its effects, such as Maudslay et al. (2019).\nThe authors show that techniques such as name\npairing based on frequency can improve the effec-\ntiveness of counterfactual data. Another issue could\nbe that we have focused on races in the American\ncontext, but the text sequences referring to another\ncontext (such as Indian or Asian contexts) can be\nmistakenly used to create counterfactuals. A bet-\nter approach should identify and ﬁlter such texts.\nFinally, even though names have been used as in-\ndicators of race in our work and previous work,\nthis may be a relatively poor indicator of race. Es-\npecially to identify races in the American context\nonly compared to gendered words identifying gen-\nder roles leading to suboptimal results. We leave\nthese explorations for future work.\nD Training and Evaluation Details\nD.1 Language Model Training\nWe started with the knowledge distillation setup of\nSanh et al. (2019)10 and tailored it to our require-\nments. We did not use the cosine loss between\nthe representation. We assigned equal weights\nof 0.5 to LM loss and KL divergence term with\na temperature of 2.0. We only use 10% of the\nOpenWebText sequences. All the models are\ntrained using HuggingFace (Wolf et al., 2020) and\nPyTorch (Paszke et al., 2019) for three epochs with\na learning rate of 10−3, AdamW optimizer, and a\nbatch size of 1600. We use DeepSpeed (Rasley\net al., 2020) for distributed training using 8 V100\nGPUs. One epoch took between 5–8 hours.\n10https://github.com/huggingface/\ntransformers/tree/master/examples/\nresearch_projects/distillation\nWe used DistilGPT–2, which had six layers, an\nembedding size of 768, and 12 attention heads as\nthe student model. We initialize student models\nwith weights from the even-numbered layers of the\nteacher model, i.e., pretrained GPT2–small. When\nusing GPT2–small as the student, we initialize with\nthe pretrained GPT2–small.\nFor ﬁnetuning with counterfactual text baseline, we\nuse the same training hyper-parameters as above\nbut set the weight of KL divergence term to 0, and\nLM loss weight is set to 1. For DistilGPT–2, we\ninitialize with DistilGPT–2 (HF) parameters in-\nstead of GPT2–small. This is because we will\nﬁrst distill the model and then ﬁnetune for fair-\nness in an actual fair-ﬁnetuning setup. However,\nwe remark that this model is slightly advantaged\ncompared to our approach in terms of performance\n(perplexity). Unlike our ERA models, which only\nuse 10% of text sequences from OpenWebText,\nit was distilled using all the data. For GPT2–small\nexperiments, we initialize with the parameters of\npretrained GPT2–small.\nFor adversarial prompts baseline of Sheng et al.\n(2020) and GPT2–small, we use the adversarial\nprompt for man/woman condition from their pa-\nper (Appendix A, Table 5 in their paper). We use\ntheir ofﬁcial implementation for DistilGPT–2 ex-\nperiments to ﬁnd the adversarial prompt with bias\nmitigation setting. We set disassociation and associ-\nation loss to 1 and use “The man” and “The woman”\nas the demographics. The adversarial prompt found\nwas “ genomes genomes Parables Nutrition Nutri-\ntion Mathematics”.\nD.2 Language Model Evaluation\nText Generation. We use top-p sampling (Holtz-\nman et al., 2020) with p = 0.9 and consider the top\n10 sequences for all text generation experiments.\nWe limit the max length of the generated sequence\nto 100.\nPerplexity & Fluency. Perplexity is measured as\nthe exponentiated average negative log-likelihood\nof a sequence. Given a token sequence, X =\n{x0, x1, . . . , xm}, the perplexity of X, ppl(X) is,\nppl(X) = exp\n{\n− 1\nm\nm∑\nt=1\nlog P(xt|x<t)\n}\nGPT–2 is a ﬁxed-length model with a max length\nof 1024. For this reason, we compute perplexity in\n674\nchunks of length 1024 and stride of 512. We deﬁne\nﬂuency as the perplexity measured by GPT2–large\nwith stride size 0.\nD.3 Bios–Bias Training and Evaluation\nWe ﬁnetune language models onBios–Bias task for\n20 epochs with a batch size of 256, 10−3 learning\nrate, and AdamW optimizer. Similar to De-Arteaga\net al. (2019), we use a 65–10–25 split of the dataset\nfor training, validation, and testing. We use the\nvalidation set to pick the best model for evaluation.\nWe do not update the pretrained language model\nweights during ﬁnetuning and use a weighted com-\nbination of all the embeddings. These weights are\ncomputed using attention. More speciﬁcally, we\nemploy a learnable vector to do a dot-product with\nresulting embeddings (last-layer output or output\nbefore the decoder layer). The dot product result is\nnormalized using softmax to compute the weight\nvector. The weighted combination of the embed-\ndings is passed through a linear classiﬁer to predict\nthe label.\nD.4 CEAT Details\nWe use CEAT Tests 6, 7, and 8. The set of target\nand attribute words that were considered for each\ntest are shown in Table 9. Each test uses four set of\nwords — X, Y , A, and B. CEAT test works similar\nto WEAT (Caliskan et al., 2017) and ﬁrst evaluates\nthe difference in association of word w in set X\nand Y to set A and B by computing difference of\naverage cosine distance as:\ns(w, A, B) =meana∈Acos(w, a)\n−meanb∈Bcos(w, b)\nThe cosine distances are computed between the\nembeddings. It then computes the difference of\ndifference in association to measure if words in set\nX and Y are considered differently, i.e.,\nS(X, Y, A, B) =meanx∈Xs(x, A, B)\n−meany∈Ys(y, A, B)\nThis provides an estimate of the absolute difference\nbetween the association of embeddings. To eval-\nuate if this difference is signiﬁcant overall effect\nsize (ES) is computed by dividing with the standard\ndeviation the difference in the association of union\nof set X and Y (in-sample variance). Intuitively,\nwe measure if the set X and Y have signiﬁcantly\ndifferent associations than any other shufﬂing of\nX ∪Y .\nES = S(x, Y, A, B)\nstd-devw∈X∪Ys(w, A, B)\nSince we are evaluating contextual embeddings, we\nwill have multiple embeddings for each word based\non the context of the word. Therefore, CEAT sam-\nples one of the embeddings of the word to compute\nES and refers to it asESi. A random-effects model\nis used to combine results of multiple such sam-\npling. Eventually, the combined effect size (CES)\nis computed as:\nCES =\n∑viESi∑vi\n,\nWhere vi is the inverse of the sum of in-sample\nvariance and between-sample invariance.\nDifferent contextual embeddings for a word are de-\nrived using the random occurrence of that particular\nword from Reddit. We use the ofﬁcial implementa-\ntion of CEAT11 with N=10000, which is the default\nin their implementation.\n11https://github.com/weiguowilliam/CEAT\n675\nFemale Words Male Words\nshe’ll he’ll\nstrongwoman strongman\nmama’s papa’s\ndaughter’s son’s\nmaternity paternity\nwife’s husband’s\ngirlhood boyhood\nsaleswoman salesman\nhousewives househusbands\nhousewife househusband\nmom’s dad’s\nschoolgirl schoolboy\ngranddaughter’s grandson’s\nmotherhood fatherhood\nlesbians gays\ngrandmother’s grandfather’s\nmadam sir\nmothered fathered\ncouncilwomen councilmen\nstepmother’s stepfather’s\nmommy’s daddy’s\nmamas papas\nstepmom stepdad\nhousewife’s househusband’s\npolicewomen policemen\ngrandma grandpa\ncouncilwoman councilman\nstepmom’s stepdad’s\ncountrywoman countryman\ngodmother godfather\ngirlfriend’s boyfriend’s\nniece’s nephew’s\nsister’s brother’s\nsaleswomen salesmen\nsororities fraternities\ngodmother’s godfather’s\nmama papa\nsisterhood brotherhood\nbride’s groom’s\nheir heiress\ngirlfriends boyfriends\nstepmoms stepdads\nma pa\ncongresswoman congressman\nsororal fraternal\nfeminism masculism\nheiress heir\ncountrywomen countrymen\nma’s pa’s\nstepdaughter’s stepson’s\ngirlfriend boyfriend\ncongresswomen congressmen\ngal’s guy’s\ngodmothers godfathers\ngirl’s boy’s\nmaternal paternal\naunt’s uncle’s\nmother’s father’s\nshe’d he’d\nshe’s he’s\nTable 6: List of additional gender words.\n676\nCategory Asian-American African-American European-American Hispanic & Latino\nCountries korean, indian, chinese\n, japanese, indonesian,\npakistani, bangladeshi,\nﬁlipino, ﬁlipina, veit-\nnamese, turkish, turk,\niranian, burmese,\niraqi, afghan, afghani,\narab, uzbek, yemeni,\nnepalese, sri lankan,\nsri-lankan, srilankan,\nisraeli, laotian, lebenese,\nlebanese, palestinian,\nkuwaiti, mongol,\narmenian, thai\nnigerian, ethiopian,\negyptian, congolese,\ntanzanian, kenyan,\nugandan, moroccan\ngerman, british, french,\nitalian, spanish, roma-\nnian, dutch, belgian,\ngreek, irish, portugese,\nhungarian, austrian,\nswish, bulgarian,\nﬁnnish, slovak, nor-\nweigian, scottish,\npolish, swedish, lithua-\nnian, danish, slovenian,\nlatvian, estonian\nmexican, brazilian,\nsalvadorian, honduran,\ncolombian, cuban,\nperuvian, ecuadorian,\nchilean, haitian, costa\nrican, costa rican, tico,\ndominican\nFirst Names young, mohammed,\nhung, wei, hong, thanh,\nyong, minh, rajesh,\nsyed, jin, jian, yan, jun,\nsanjay, tuan, lily, sung,\nming, amit, yu, min, chi,\nphuong, muhammad,\nmay, hai, anil, dung,\nthuy, yi, sunil, sang,\nteresita, jing, ravi, vijay,\nying, ramesh, mei,\ndong, long, anh, kyung,\nmai, hui, jung, son,\nromeo, suresh, hoa, lan,\ncuong, ashok, jae, linh,\nduc, chong, tam, wai,\ndanilo, vinh, ajay, xiao,\njie, hoang, chun, wen,\nsun, hao, ping, rakesh,\ndeepak, binh, khanh,\nsandeep, kai, anand, xin,\nyun, krishna, feng, eun,\nbo, arun, erlinda, tri,\nsrinivas, trung, manish,\nlin, huong, tai, nam,\nhyun, ashish\nwillie, reginald, tyrone,\ncedric, lillie, sylvester,\nmattie, latoya, tamika,\nlatasha, marva, keisha,\nalthea, darnell, lula,\naisha, jermaine, latonya,\nhattie, roosevelt, fan-\nnie, ebony, alphonso,\nmamie, sammie, ollie,\ndemetrius, donnell, fele-\ncia, jarvis, cleveland,\njamila, tanisha, latisha,\nodessa, mable, cornell,\nlawanda, alfreda, essie,\nlakisha, odell, prince,\nlatrice, latanya, oc-\ntavia, earnestine, ivory,\ntameka, tomeka, ayanna\nmichael, john, david,\nrobert, james, william,\nrichard, thomas, mark,\nmary, daniel, christo-\npher, susan, jennifer,\nsteven, jeffrey, brian,\npaul, patricia, linda,\nmatthew, karen, scott,\nkevin, lisa, timothy,\nstephen, barbara, eliz-\nabeth, kenneth, gary,\ndonald, ronald, jason,\nnancy, andrew, kathleen,\neric, deborah, gregory,\nanthony, edward, pe-\nter, michelle, sandra,\namy, kimberly, laura,\ngeorge, cynthia, carol,\ndonna, julie, patrick,\ndouglas, christine,\nsharon, pamela, dennis,\ndebra, diane, rebecca,\nmargaret, kelly, melissa,\nlarry, frank, ryan, sarah,\nangela, stephanie,\njonathan, janet, cheryl,\ncatherine, heather,\njudith, todd, lori, keith,\njessica, bruce, craig,\njoshua, raymond,\ndenise, ann, brenda,\nteresa, terry, katherine,\nalan, adam, kathryn,\ncarolyn, nicholas,\nlawrence\nmaria, jose, juan, carlos,\nluis, manuel, antonio,\njorge, francisco, jesus,\nmiguel, mario, carmen,\nana, rosa, roberto,\nricardo, pedro, oscar,\nrafael, hector, raul,\nyolanda, javier, ramon,\nfernando, ruben, sergio,\neduardo, angel, edgar,\nalejandro, armando,\nsalvador, julio, arturo,\nalfredo, cesar, marco,\nalberto, guadalupe,\nenrique, alma, ger-\nardo, irma, margarita,\nleticia, ernesto, silvia,\nguillermo, luz, rodolfo,\nfelix, adriana, blanca,\nalfonso, gustavo, an-\ndres, omar, angelica,\nbertha, pablo, isabel,\nfelipe, raquel, lorena,\nlourdes, juana, hilda,\nhugo, rogelio, ramiro,\nignacio, rolando, abel,\nmarcos, humberto,\nrosario, tomas, orlando,\nismael, delia, gilberto,\ngabriela, elsa, susana,\nsaul, joseﬁna, israel,\nmercedes, lorenzo,\nalvaro, beatriz, rey-\nnaldo, rodrigo, maribel,\nleonardo, graciela,\nsantiago, rigoberto\nLast Names xiong, zhang, huang,\ntruong, yang, li, vang,\nhuynh, vu, nguyen,\nali, khan, wong, singh,\nchang, chung, ahmed\nwashington, jeffer-\nson, booker, banks,\njoseph, mosley, jackson,\ncharles, dorsey, rivers\nyoder, friednam,\nkrueger, schwartz,\nschmitt, mueller, weiss,\nnovak, o’connell, klein\nbarajas, zavala, ve-\nlazquez, avalos, orozco,\nvazquez, juarez, meza,\nhuerta, ibarra\nRace asian european african latin, hispanic\nColor white black\nTable 7: Word lists for generating race counterfactuals.\n677\nModel ppl (↓) Regard Ratio Variance (↓) Fluency ( ↓)\nMethod Mod fn. Aug. African Asian European Hispanic\nGPT2–small (Teacher) N/A N/A 25.17 1.280 1.868 1.445 1.196 0.302 64.69\n(0.35, 0.27) (0.40. 0.21) (0.36, 0.25) (0.34, 0.29)\nDistilGPT–2 (HF) N/A N/A 39.25 1.434 2.035 1.599 1.312 0.318 155.77\n(0.32, 0.22) (0.35, 0.17) (0.34, 0.21) (0.32, 0.25)\nDistilGPT–2 (Baseline) N/A N/A 40.88 1.219 1.653 1.364 1.049 0.258 94.11\n(0.33, 0.27) (0.37, 0.22) (0.35, 0.25) (0.31, 0.29)\nDistilGPT–2 (ERA) max no 40.92 1.124 1.515 1.213 0.938 0.241 143.45\n(0.30, 0.27) (0.33, 0.22) (0.31, 0.26) (0.29, 0.31)\nDistilGPT–2 (ERA) none yes 40.91 1.079 1.493 1.206 0.955 0.231 109.98\n(0.29, 0.27) (0.33, 0.22) (0.31, 0.25) (0.29, 0.30)\nDistilGPT–2 (ERA) max no 41.46 1.056 1.404 1.145 0.870 0.222 94.78\n(0.29, 0.28) (0.32, 0.23) (0.30, 0.26) (0.27, 0.31)\nTable 8: Racial disparity in open-ended text generation as assessed by BOLD Race prompts. We report the average\nof over ﬁve evaluation runs. The races are abbreviated, so African is African-American, Asian is Asian-American,\netc. Fluency is the macro average across all 4 races. Value in the bracket show the fraction of positively and\nnegatively regarded generations.\nTest X Y A B\nTest 6 male: John, Paul, Mike,\nKevin, Steve, Greg, Jeff,\nBill\nfemale: Amy, Joan,\nLisa, Sarah, Diana,\nKate, Ann, Donna\ncareer: executive, man-\nagement, professional,\ncorporation, salary, of-\nﬁce, business, career\nfamily: home, par-\nents, children, family,\ncousins, marriage, wed-\nding, relatives\nTest 7 math: math, algebra,\ngeometry, calculus,\nequations, computation,\nnumbers, addition\narts: poetry, art, dance,\nliterature, novel, sym-\nphony, drama, sculpture\nmale: male, man, boy,\nbrother, he, him, his,\nson\nfemale: female,\nwoman, girl, sister, she,\nher, hers, daughter\nTest 8 science: science, tech-\nnology, physics, chem-\nistry, Einstein, NASA,\nexperiment, astronomy\narts: poetry, art, Shake-\nspeare, dance, litera-\nture, novel, symphony,\ndrama\nmale: brother, father,\nuncle, grandfather, son,\nhe, his, him\nfemale: sister, mother,\naunt, grandmother,\ndaughter, she, hers, her\nTable 9: Words sets and categories used in CEAT tests.\n678",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.8287688493728638
    },
    {
      "name": "Linguistics",
      "score": 0.41784366965293884
    },
    {
      "name": "Computer science",
      "score": 0.3981776833534241
    },
    {
      "name": "Psychology",
      "score": 0.32483017444610596
    },
    {
      "name": "Philosophy",
      "score": 0.30929258465766907
    },
    {
      "name": "Epistemology",
      "score": 0.20596370100975037
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ]
}