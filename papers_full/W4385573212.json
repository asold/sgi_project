{
  "title": "Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling",
  "url": "https://openalex.org/W4385573212",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2224351680",
      "name": "Peijie Jiang",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2119947217",
      "name": "Dingkun Long",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111322234",
      "name": "Yanzhao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2944937469",
      "name": "Pengjun Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127176296",
      "name": "Meishan Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3104235802",
    "https://openalex.org/W3035516800",
    "https://openalex.org/W2945864679",
    "https://openalex.org/W2803437449",
    "https://openalex.org/W2574077427",
    "https://openalex.org/W2983180560",
    "https://openalex.org/W2096204319",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2952054097",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3176023514",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W4206121183",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W2036516910",
    "https://openalex.org/W2912473624",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2142259548",
    "https://openalex.org/W2158049734",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W2400932086",
    "https://openalex.org/W2746222821",
    "https://openalex.org/W3035594424",
    "https://openalex.org/W3035193825",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2899395607",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2785522575",
    "https://openalex.org/W2251584595",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2020073413",
    "https://openalex.org/W3171291687",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2971871542"
  ],
  "abstract": "Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary information. However, to ensure the quality of the lexicon, great human effort is always necessary, which has been generally ignored. In this work, we suggest unsupervised statistical boundary information instead, and propose an architecture to encode the information directly into pre-trained language models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature induction of Chinese sequence labeling tasks. Experimental results on ten benchmarks of Chinese sequence labeling demonstrate that BABERT can provide consistent improvements on all datasets. In addition, our method can complement previous supervised lexicon exploration, where further improvements can be achieved when integrated with external lexicon information.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 526â€“537\nDecember 7-11, 2022 Â©2022 Association for Computational Linguistics\nUnsupervised Boundary-Aware Language Model Pretraining for Chinese\nSequence Labeling\nPeijie Jiang1 Dingkun Long Yanzhao Zhang Pengjun Xie\nMeishan Zhang2âˆ— Min Zhang2\n1School of New Media and Communication, Tianjin University, China\n2Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)\njzx555@tju.edu.cn, {zhangmeishan,zhangmin2021}@hit.edu.cn\n{longdingkun1993,zhangyanzhao00,xpjandy}@gmail.com\nAbstract\nBoundary information is critical for various\nChinese language processing tasks, such as\nword segmentation, part-of-speech tagging,\nand named entity recognition. Previous stud-\nies usually resorted to the use of a high-quality\nexternal lexicon, where lexicon items can of-\nfer explicit boundary information. However,\nto ensure the quality of the lexicon, great hu-\nman effort is always necessary, which has been\ngenerally ignored. In this work, we suggest un-\nsupervised statistical boundary information in-\nstead, and propose an architecture to encode\nthe information directly into pre-trained lan-\nguage models, resulting in Boundary-Aware\nBERT (BABERT). We apply BABERT for\nfeature induction of Chinese sequence label-\ning tasks. Experimental results on ten bench-\nmarks of Chinese sequence labeling demon-\nstrate that BABERT can provide consistent im-\nprovements on all datasets. In addition, our\nmethod can complement previous supervised\nlexicon exploration, where further improve-\nments can be achieved when integrated with\nexternal lexicon information.\n1 Introduction\nThe representative sequence labeling tasks for the\nChinese language, such as word segmentation, part-\nof-speech (POS) tagging and named entity recogni-\ntion (NER) (Emerson, 2005; Jin and Chen, 2008),\nhave been inclined to be performed at the character-\nlevel in an end-to-end manner (Shen et al., 2016).\nThe paradigm, naturally, is standard to Chinese\nword segmentation (CWS), while for Chinese POS\ntagging and NER, it can better help reduce the error\npropagation (Sun and Uszkoreit, 2012; Yang et al.,\n2016; Liu et al., 2019a) compared with word-based\ncounterparts by straightforward modeling.\nRecently, all the above tasks have reached state-\nof-the-art performances with the help of BERT-\nlike pre-trained language models (Yan et al., 2019;\nâˆ—Corresponding author.\nMeng et al., 2019). The BERT variants, such as\nBERT-wwm (Cui et al., 2021), ERNIE (Sun et al.,\n2019), ZEN (Diao et al., 2020), NEZHA (Wei et al.,\n2019), etc., further improve the vanilla BERT by ei-\nther using external knowledge or larger-scale train-\ning corpus. The improvements can also beneï¬t\ncharacter-level Chinese sequence labeling tasks.\nNotably, since the output tags of all these\ncharacter-level Chinese sequence labeling tasks in-\nvolve identifying Chinese words or entities (Zhang\nand Yang, 2018; Yang et al., 2019), prior bound-\nary knowledge could be highly helpful for them.\nA number of studies propose the integration of an\nexternal lexicon to enhance their baseline models\nby feature representation learning (Jia et al., 2020;\nTian et al., 2020a; Liu et al., 2021). Moreover,\nsome works suggest injecting similar resources into\nthe pre-trained BERT weights. BERT-wwm (Cui\net al., 2021) and ERNIE (Sun et al., 2019) are the\nrepresentatives, which leverage an external lexicon\nfor masked word prediction in Chinese BERT.\nThe lexicon-based methods have indeed\nachieved great success for boundary integration.\nHowever, there are two major drawbacks. First,\nthe lexicon resources are always constructed\nmanually (Zhang and Yang, 2018; Diao et al.,\n2020; Jia et al., 2020; Liu et al., 2021), which\nis expensive and time-consuming. The quality\nof the lexicon is critical to our tasks. Second,\ndifferent tasks as well as different domains require\ndifferent lexicons (Jia et al., 2020; Liu et al., 2021).\nA well-studied lexicon for word segmentation\nmight be inappropriate for NER, and a lexicon\nfor news NER might also be problematic for\nï¬nance NER. The two drawbacks can be due to the\nsupervised characteristic of these lexicon-based\nenhancements. Thus, it is more desirable to offer\nboundary information in an unsupervised manner.\nIn this paper, we propose an unsupervised\nBoundary-Aware BERT (BABERT), which is\nachieved by fully exploring the potential of statisti-\n526\ncal features mined from a large-scale raw corpus.\nWe extract a set of N-grams (a predeï¬ned ï¬xed N)\nno matter they are valid words or entities, and then\ncalculate their corresponding unsupervised statisti-\ncal features, which are mostly related to boundary\ninformation. We inject the boundary information\ninto the internal layer of a pre-trained BERT, so\nthat our ï¬nal BABERT model can approximate the\nboundary knowledge softly by using inside repre-\nsentations. The BABERT model has no difference\nfrom the original BERT, so that we can use it in the\nsame way as the standard BERT exploration.\nWe conduct experiments on three Chinese se-\nquence labeling tasks to demonstrate the effec-\ntiveness of our proposed method. Experimental\nresults show that our approach can signiï¬cantly\noutperform other Chinese pre-trained language\nmodels. In addition, compared with supervised\nlexicon-based methods, BABERT obtains com-\npetitive results on all tasks and achieves further\nimprovements when integrated with external lex-\nicon knowledge. We also conduct extensive anal-\nyses to understand our method comprehensively.\nThe pre-trained model and code are publicly avail-\nable at http://github.com/modelscope/\nadaseq/examples/babert.\nOur contributions in this paper include the fol-\nlowing: 1) We design a method to encode un-\nsupervised statistical boundary information into\nboundary-aware representation, 2) propose a new\npre-trained language model called BABERT as\na boundary-aware extension for BERT, 3) verify\nBABERT on ten benchmark datasets of three Chi-\nnese sequence labeling tasks.\n2 Related Work\nIn the past decades, machine learning has achieved\ngood performance on sequence labeling tasks\nwith statistical information (Bellegarda, 2004; Low\net al., 2005; Bouma, 2009). Recently, neural mod-\nels have led to state-of-the-art results for Chinese\nsequence labeling (Lample et al., 2016; Ma and\nHovy, 2016; Chiu and Nichols, 2016). In addi-\ntion, the presence of language representation mod-\nels such as BERT (Devlin et al., 2019) has led\nto impressive improvements. In particular, many\nvariants of BERT are devoted to integrating bound-\nary information into BERT to improve Chinese se-\nquence labeling (Diao et al., 2020; Jia et al., 2020;\nLiu et al., 2021).\nStatistical Machine Learning Statistical infor-\nmation is critical for sequence labeling. Previous\nworks attempt to count such information from large\ncorpora in order to combine it with machine learn-\ning methods for sequence labeling (Bellegarda,\n2004; Liang, 2005; Bouma, 2009). Peng et al.\n(2004) attempts to conduct sequence labeling by\nCRF and a statistical-based new word discovery\nmethod. Low et al. (2005) introduce a maximum\nentropy approach for sequence labeling. Liang\n(2005) utilizes unsupervised statistical information\nin Markov models, and gets a boost on Chinese\nNER and CWS.\nPre-trained Language Model Pre-trained lan-\nguage model is a hot topic in natural language pro-\ncessing (NLP) communities (Devlin et al., 2019;\nLiu et al., 2019b; Wei et al., 2019; Clark et al.,\n2020; Diao et al., 2020; Zhang et al., 2021) and\nhas been extensively studied for Chinese sequence\nlabeling. For instance, TENER (Yan et al., 2019)\nadopts Transformer encoder to model character-\nlevel features for Chinese NER. Glyce (Meng et al.,\n2019) uses BERT to capture the contextual rep-\nresentation combined with glyph embeddings for\nChinese sequence labeling.\nLexicon-based Methods In recent studies, lexi-\ncon knowledge has been applied to improve model\nperformance. There are two mainstream categories\nto the work of lexicon enhancement. The ï¬rst aims\nto enhance the original BERT with implicit bound-\nary information by using the multi-granularity\nword masking mechanism. BERT-wwm (Cui et al.,\n2021) and ERNIE (Sun et al., 2019) are represen-\ntatives of this category, which propose to mask\ntokens, entities, and phrases as the mask units\nin the masked language modeling (MLM) task to\nlearn the coarse-grained lexicon information dur-\ning pre-training. ERNIE-Gram (Xiao et al., 2021),\nan extension of ERNIE, utilizes statistical bound-\nary information for unsupervised word extraction\nto support masked word prediction, The second\ncategory, which includes ZEN (Diao et al., 2020),\nEEBERT (Jia et al., 2020), and LEBERT (Liu et al.,\n2021), exploits the potential of directly injecting\nlexicon information into BERT via extra modules,\nleading to better performance but is limited in pre-\ndeï¬ned external knowledge. Our work follows the\nï¬rst line of work, most similar to ERNIE-Gram.\nHowever, different from ERNIE-Gram, we do not\ndiscretize the real-valued statistical information ex-\n527\nPMI\nLRE\nğ‘1ğ‘2 â€¦ğ‘ğ‘– â€¦ğ‘ğ‘›âˆ’1ğ‘ğ‘›\nMLM Loss\nâ„’MLM\nMSE Loss\nâ„’BA\n(c). Boundary-Aware BERT Learning\nInput Sentence\nRaw Corpus\nN-gram Statistical Dictionary\nContextual N-gram Sets\nÂ·Â·Â·Â·Â·Â·\nÂ·Â·Â·Â·Â·Â·\nN-gram Set ğ‘†ğ‘›ğ‘ of ğ‘ğ‘›\nN-gram Set ğ‘†ğ‘–\nğ‘ of ğ’„ğ’Š\nğ’„ğ’Šğ‘ğ‘–+1ğ’„ğ’Š\nğ’„ğ’Š â€¦ğ‘ğ‘–+ğ‘âˆ’1\nÂ·Â·Â·Â·Â·Â·\nğ‘ğ‘–âˆ’1ğ’„ğ’Š\nğ‘ğ‘–âˆ’ğ‘/2 â€¦ğ‘ğ‘–+ğ‘/2Â·Â·Â· Â·Â·Â·\nğ’„ğ’Šğ‘ğ‘–+1ğ‘ğ‘–+2\nğ‘ğ‘–âˆ’ğ‘+1 â€¦ğ’„ğ’Š\nğ‘ğ‘–âˆ’1ğ’„ğ’Šğ‘ğ‘–+1\nN-gram Set ğ‘†1\nğ‘ of ğ‘1\nPre-Trained Language Model\nRepresentation Composition\n(b). Boundary-Aware BERT Representation(a). Boundary Information Extractor\nGram1 PMI1;LE1;RE1\nGram2 PMI2;LE2;RE2\nÂ·Â·Â·Â·Â·Â·\nGramğ‘› PMIğ‘›;LEğ‘›;REğ‘›\n+\nÂ·Â·Â·Â·Â·Â·\nÂ·Â·Â·Â·Â·Â·\nLE Rep PMI Rep RE Rep\nUnsupervised\nInformation Mining\nğ¿-th BERT Layer\nğ‘™-th BERT Layer\n1-th BERT Layer\nâŠ• âŠ•\nFigure 1: The overall architecture of the boundary-aware pre-trained language model, which consists of three parts:\n(a) boundary information extractor, (b) boundary-aware representation, and (c) boundary-aware BERT Learning.\nThe boundary-aware objective LBA is deï¬ned in Equation 7.\ntracted from corpus, but adopt a regression manner\nto leverage the information fully.\n3 Method\nFigure 1 shows the overall architecture of our un-\nsupervised boundary-aware pre-trained language\nmodel, which mainly consists of three compo-\nnents: 1) boundary information extractor for un-\nsupervised statistical boundary information min-\ning, 2) boundary-aware representation to integrate\nstatistical information at the character-level, and\n3) boundary-aware BERT learning which injects\nboundary knowledge into the internal layer of\nBERT. In this section, we ï¬rst focus on the details\nof the above components, and then introduce the\nï¬ne-tuning method for Chinese sequence labeling.\n3.1 Boundary Information Extractor\nStatistical boundary information has been shown\nwith a positive inï¬‚uence on a variety of Chinese\nNLP tasks (Song and Xia, 2012; Higashiyama et al.,\n2019; Ding et al., 2020; Xiao et al., 2021). We\nfollow this line of work, designing a boundary in-\nformation extractor to mine statistical information\nfrom a large raw corpus in an unsupervised way.\nThe overall ï¬‚ow of the extractor includes two\nsteps: I) First, we collect all N-grams from the\nraw corpus to build a dictionary N, in which we\ncount the frequencies of each N-gram and ï¬lter out\nthe low frequencies items; II) second, considering\nthat word frequency is insufï¬cient for represent-\ning the ï¬‚exible boundary relation in the Chinese\ncontext, we further compute two unsupervised in-\ndicators which can capture most of the boundary\ninformation in the corpus. In the following, we will\ndescribe these two indicators in detail.\nPointwise Mutual Information (PMI) Given\nan N-gram, we split it into two sub-strings and com-\npute the mutual information (MI) between them as\na candidate. Then, we enumerate all sub-string\npairs and choose the minimum MI as the overall\nPMI to estimate the tightness of the N-gram. Let\ng = {c1...cm}be an N-gram that consists of m\ncharacters, we calculate PMI using this formula:\nPMI(g) = min\niâˆˆ[1:mâˆ’1]\n{ p(g)\np(c1...ci) Â·p(ci+1...cm)}, (1)\nwhere p(Â·) denotes the probability over the corpus.\nNote that, when m = 1, the corresponding PMI is\nconstantly equal to 1. The higher PMI indicates that\nthe N-gram (e.g., \" è´å…‹æ±‰å§† (Beckham)\") has a\nsimilar occurrence probability to the sub-string pair\n(e.g., \"è´å…‹ (Beck)\" and \"æ±‰å§† (Ham)\"), leading\nto a higher association between internal sub-string\npairs, which makes the N-gram more likely to be a\nword/entity. In contrast, a lower PMI means the N-\ngram (e.g., \"å…‹æ±‰(Kehan)\") is possibly an invalid\nword/entity.\nLeft and Right Entropy (LRE) Given an N-\ngram g, we ï¬rst collect a left-adjacent character\nset Sl\nm = {cl\n1, ..., cl\nnl}with nl characters. Then,\nwe utilize the conditional probability between g\nand its left adjacent characters in Sl\nm to compute\n528\nthe left entropy (LE), which measures sufï¬cient\nboundary information. LE can be deï¬ned as:\nLE(g) =âˆ’\nâˆ‘nl\ni\np(cl\nig|g) logp(cl\nig|g). (2)\nSimilar to LE, we further collect a right adjacent set\nSr\nm = {cr\n1, ..., cr\nnr }with nr characters to calculate\nthe right entropy (RE) for the N-gram g:\nRE(g) =âˆ’\nâˆ‘nr\ni\np(gcr\ni |g) logp(gcr\ni |g). (3)\nIntuitively, LRE represents the abundance of\nneighboring characters for the N-gram. With a\nlower LRE, the N-gram (e.g., \"æ±‰å§†\") has a more\nï¬xed context, indicating it is more likely to be a\npart of a phrase or entity. Conversely, the N-gram\nwith a higher LRE (e.g., \"è´å…‹æ±‰å§†\") will interact\nmore with context, which prefers to be an indepen-\ndent word or phrase.\nFinally, we utilize PMI and LRE to measure the\nï¬‚exible boundary relations in the Chinese context,\nand then update each N-gram in Nwith the unsu-\npervised statistical indicators above.\n3.2 Boundary-Aware Representation\nBy using the boundary information extractor, we\ncan obtain an N-gram dictionary Nwith unsuper-\nvised statistical boundary information. Unfortu-\nnately, since the context independence and the high\nrelevance to N-gram, previous works (Ding et al.,\n2020; Xiao et al., 2021) use such statistical features\nfor word extraction only, which ignore the potential\nof statistical boundary information in representa-\ntion learning. To alleviate this problem, we propose\nboundary-aware representation, a highly extensible\nmethod, to fully beneï¬t from the statistical bound-\nary information for representation learning.\nTo achieve boundary-aware representation, we\nï¬rst build contextual N-gram sets from the sen-\ntence. As shown in Figure 1 (b), given a sen-\ntence x = {c1, c2, ..., cn}with n characters and\nthe maximum N-gram length N, we extract all\nN-grams that include ci as the contextual N-gram\nset Sc\ni = {ci, cici+1, Â·Â·Â· , ciâˆ’N+1...ci}for char-\nacter ci. Then, we design a composition method\nto integrate the statistical features of N-grams in\nSc\ni by using speciï¬c conditions and rules, aiming\nto avoid the sparsity and contextual independence\nlimitations of statistical information.\nConcretely, we divide the information composi-\ntion method into PMI and entropy representation.\nFirst, we concatenate the PMI of all N-grams in Sc\ni\nto generate PMI representation:\nep\ni =PMI(ci )\nâŠ•PMI(ci ci+1) âŠ•PMI(ciâˆ’1 ci )\nâŠ•PMI(ci ci+1ci+2) âŠ•Â·Â·Â·âŠ• PMI(ciâˆ’2ciâˆ’1 ci )\nÂ·Â·Â·Â·Â·Â·\nâŠ•PMI(ci ...ci+Nâˆ’1) âŠ•Â·Â·Â·âŠ• PMI(ciâˆ’N+1...ci ),\n(4)\nwhere ep\ni âˆˆRa, and a = 1+2+Â·Â·Â·+N is the num-\nber of the N-grams that contain ci. Note that the\nposition of each N-gram is ï¬xed in PMI representa-\ntion. We strictly follow the order of N-gram length\nand the position of ci in N-gram to concatenate\ntheir corresponding PMI, ensuring that the position\nand context information can be encoded into ep\ni .\nEntropy representation focuses on the contextual\ninteractions of each character. When ci is the bor-\nder of N-grams in Sc\ni , we separately aggregate the\nLE and RE as left and right entropy representation:\nele\ni =LE(ci ) âŠ•LE(ci ci+1) âŠ•Â·Â·Â·âŠ• LE(ci ...ci+Nâˆ’1),\nere\ni =RE(ci ) âŠ•RE(ciâˆ’1 ci ) âŠ•Â·Â·Â·âŠ• RE(ciâˆ’N+1...ci ),\n(5)\nwhere ele\ni âˆˆRb, ere\ni âˆˆRb, and b = N1 is the\nnumber of integrated N-grams. Similar to PMI\nrepresentation, the position of each N-gram in ele\ni\nand ele\ni is ï¬xed and symmetric. Therefore, the\nboundary-aware representation ei of ci can be for-\nmalized as:\nei = ele\ni âŠ•ep\ni âŠ•ere\ni , (6)\nwhere ei âˆˆRa+2b. Finally, by composing multi-\ngranularity statistical boundary information in a\nspeciï¬c order, we are able to obtain the boundary-\naware representation, which explicitly contains the\nboundary and context information.\nFigure 2 shows an example of the boundary-\naware representation. Given a sentence \" å—äº¬\nå¸‚é•¿æ±Ÿå¤§æ¡¥ (Nanjing Yangtze River Bridge)\"\nand a maximum N-gram length N = 3, we ï¬rst\nbuild a contextual N-gram set for the character \"é•¿\n(Long)\". Then, we integrate the PMI of all N-grams\nin a speciï¬c order (from N-gram \"é•¿\" to \"äº¬å¸‚é•¿\n(Mayor of Jing)\") to compute PMI representation.\nFurthermore, left and right entropy representations\nare also calculated in a particular order (from N-\ngram \"é•¿\" to \"é•¿æ±Ÿå¤§ (Yangtze River Big)\" and\n\"äº¬å¸‚é•¿\", respectively). Finally, we concatenate\nthe above features to produce the overall boundary-\naware representation of the character \"é•¿\".\n1We only consider the N-grams where ci is on the bound-\nary, which means that for each direction the number of N-\ngrams is equal to the maximum N-gram length.\n529\nContextual N-grams Set of â€œé•¿â€\né•¿\nLong\nğ’„ğ’Š\né•¿æ±Ÿ\nYangtze River\nğ’„ğ’Šğ‘ğ‘–+1\nå¸‚é•¿\nMayor\nğ‘ğ‘–âˆ’1ğ’„ğ’Š\näº¬å¸‚é•¿\nMayor of Jing\nğ‘ğ‘–âˆ’2ğ‘ğ‘–âˆ’1ğ’„ğ’Š\né•¿æ±Ÿå¤§\nYangtze River Big\nğ’„ğ’Šğ‘ğ‘–+1ğ‘ğ‘–+2\nPMI Representation\nå¸‚é•¿æ±Ÿ\nMayor Jiang\nğ‘ğ‘–âˆ’1ğ’„ğ’Šğ‘ğ‘–+1\nRE RepresentationLE Representation\nâ€¢â€¢â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢â€¢â€¢\nâŠ•\nBoundary-Aware Representation\nFigure 2: The illustration of boundary-aware represen-\ntation for character \"é•¿\" in text \"å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥\".\n3.3 Boundary-Aware BERT Learning\nBoundary-aware BERT is a variant of BERT, en-\nhanced with boundary information simply and ef-\nfectively. In this subsection, we describe how the\nboundary information can be integrated into BERT\nduring pre-training by boundary-aware learning.\nBoundary-Aware Objective As mentioned in\nSection 3.2, given a sentence x with character-\nlength n, we can compute the corresponding\nboundary-aware representation E = {e1, ...,en}.\nThen, we transfer the BERT feature into the bound-\nary information space and approximate it to E\nfor boundary-aware learning. Moreover, Liu et al.\n(2021) shows that encoding basic lexical knowl-\nedge in the shallow BERT layers is a more effec-\ntive approach. Hence, we use the hidden features\nHl = {hl\n1, ...,hl\nn}of the l-th shallow layer to\nachieve the boundary-aware objective:\nLBA =\nâˆ‘n\ni\nMSE(WBhl\ni, ei), (7)\nwhere MSE(Â·) denotes the mean square error loss.\nWB is a trainable matrix used to project BERT\nrepresentation into boundary information space.\nPrevious classiï¬cation-based word-level mask-\ning methods use statistical information as thresh-\nolds to ï¬lter valid words for masked word predic-\ntion. Unlike the above works, we softly utilize such\ninformation in a regression manner, avoiding possi-\nble errors in empirically ï¬ltering valid tags, thereby\nfully exploring the potential of this information.\nPre-training Following Jia et al. (2020) and\nGao and Callan (2021), we opt to initialize our\nmodel with a pre-trained BERT model released\nby Google2 and randomly initialize the other pa-\nrameters, alleviating the enormous cost of train-\n2https://github.com/google-research/bert\ning BABERT from scratch. In particular, we dis-\ncard the next sentence prediction task during pre-\ntraining, which is conï¬rmed to be not essential for\nthe pre-trained language models (Lan et al., 2020;\nLiu et al., 2019b). The total pre-training loss of\nBABERT can be formalized as:\nLpre = LMLM + LBA, (8)\nwhere LMLM is the standard objective of MLM task.\n3.4 Fine-tuning for Sequence Labeling\nStraightforward Fine-tuning As shown in Fig-\nure 1 (c), because BABERT has the same archi-\ntecture as BERT, we can adopt the identical proce-\ndure that BERT uses for ï¬ne-tuning, where the\noutput of BABERT can be used as the contex-\ntual character representation for sequence label-\ning. Concretely, given a sequence labeling dataset\nD= {(xj, yj)}N\nj=1, where yj is the label sequence\nof xj, we utilize the output of BABERT and a CRF\nlayer to calculate the sentence-level output proba-\nbility p(yj|xj), which is exactly the same as Liu\net al. (2021). The negative log-likelihood loss for\ntraining can be deï¬ned as:\nLsq = âˆ’\nâˆ‘N\nj\nlog p(yj|xj), (9)\nAt the inference stage, we use the Viterbi algorithm\n(Viterbi, 1967) to generate the ï¬nal label sequence.\nCombining with Supervised Lexicon Features\nWe can naturally combine BABERT with other su-\npervised lexicon-based methods because of the un-\nsupervised setting of BABERT. To this end, we pro-\npose a lexicon-enhanced BABERT (BABERT-LE)\nfor the ï¬ne-tuning stage, which utilizes the lexicon\nadapter proposed by Liu et al. (2021) to incorporate\nexternal lexicon knowledge into BABERT feature:\nË†hi = LA(hi, Slex\ni ), (10)\nwhere LA(Â·) is the lexicon adapter, Slex\ni is a set of\nrelated N-gram embeddings of character ci, and Ë†hi\nis the lexicon-enhanced version of original BERT\nfeature hi. We apply the lexicon adapter after the\nl-th layer to be consistent with boundary-aware\nlearning. Finally, BABERT-LE performs a similar\nï¬ne-tuning procedure as BABERT for training:\nLlex = âˆ’\nâˆ‘N\nj\nlog p(yj|xj, [Slex\n1 , ..., Slex\nnj ]). (11)\n530\n4 Experiments\n4.1 Datasets\nFollowing previous works (Devlin et al., 2019;\nXiao et al., 2021), we draw the mixed corpus of\nChinese Wikipedia3 and Baidu Baike4 as our pre-\ntraining corpus, which contains 3B tokens and 62M\nsentences. To further conï¬rm the effectiveness\nof our proposed method for Chinese sequence la-\nbeling, we evaluate BABERT on ten benchmark\ndatasets of three representative tasks:\nChinese Word Segmentation We use three\nCWS benchmarks to evaluate our BABERT. Penn\nChinese TreeBank version 6.0 (CTB6) is from\nXue et al. (2005), and MSRA and PKU are from\nSIGHAN 2005 Bakeoff (Emerson, 2005).\nPart-Of-Speech Tagging For Chinese POS tag-\nging, we conduct experiments on CTB6 (Xue et al.,\n2005) and the Chinese part of Universal Dependen-\ncies (UD) (Nivre et al., 2016). The UD dataset uses\ntwo different POS tagsets, which are universal and\nlanguage-speciï¬c tagsets. We follow Shao et al.\n(2017), referring to the corpus with the two tagsets\nas UD1 and UD2, respectively.\nNamed Entity Recognition For the Chinese\nNER task, we conduct experiments on OntoNotes\n4.0 (Onto4) (Weischedel et al., 2011) and News\ndatasets (Jia et al., 2020), both of which are from\nthe standard newswire domain. Moreover, we eval-\nuate BABERT in the internet novel (Book) and\nï¬nancial report (Finance) domains (Jia et al., 2020)\nto further verify the robustness of our method.\nThe statistics of the benchmark datasets are\nshown in Table 1. For a fair comparison, we split\nthese datasets into training, development, and test\nsections following previous works (Jia et al., 2020;\nLiu et al., 2021). Note that MSRA, PKU, and Fi-\nnance do not have development sections. Therefore,\nwe randomly select 10% instances from the training\nset as the development set for these datasets.\n4.2 Experimental Settings\nHyperparameters During pre-training, we use\nthe hyperparameters of BERT BASE to initialize\nBABERT and Adam (Kingma and Ba, 2014) for\noptimizing. The number of BERT layers L is 12,\nwith 12 self-attention heads, 768 dimensions for\nhidden states, and 64 dimensions for each head.\n3https://zh.wikipedia.org/wiki/\n4https://baike.baidu.com/\nTrain Dev Test\nCWS\nCTB6 23401 2078 2795\nMSRA 86924 - 3985\nPKU 19056 - 1944\nPOS CTB6 23401 2078 2795\nUD1/2 3997 500 500\nNER\nOnto4 15724 4303 4346\nBook 6675 2551 2551\nNews 5179 610 708\nFinance 46364 - 2000\nTable 1: The statistics of sentence number for the\nbenchmark datasets. For the datasets without develop-\nment sections, we randomly select 10% sentences from\nthe corresponding training set as the development set.\nThe batch size is set to 32, the learning rate is 1e-4\nwith a warmup ratio of 0.1, and the max length\nof the input sequence is 512. To extract unsuper-\nvised boundary information, we set the maximum\nN-gram length N to 4 5 and the frequency ï¬ltering\nthreshold to 50. Then we use the 3-th BERT layer\nto compute boundary-aware objective. BABERT\nhas no extra modules, which is why the parameter\nsize and model architecture are the same as those\nof BERTBASE. Finally, we train the BABERT on 8\nNVIDIA Tesla V100 GPUs with 32GB memory.\nFor Chinese sequence labeling, we empirically\nset hyperparameters based on previous studies (Jia\net al., 2020; Liu et al., 2021) and preliminary ex-\nperiments. The batch size is 32, the max sequence\nlength is 256, and the learning rate is ï¬xed to 2e-5.\nBaselines To verify the effectiveness of our pro-\nposed BABERT, we build systems on the following\nmethods to conduct fair comparisons:\nâ€¢ BERT is the Chinese version BERT BASE\nmodel released by Google.\nâ€¢ BERT-wwm performs segmentation on the\ncorpus and further conduct word-level mask-\ning in pre-training (Cui et al., 2021).\nâ€¢ ERNIE is an extension of BERT, which lever-\nages external lexicons for word-level masking\n(Sun et al., 2019).\nâ€¢ ERNIE-Gram is an extension of ERNIE,\nwhich alleviates the limitations of external lex-\n5According to our preliminary experiments, whenN is less\nthan 4, the performance will be signiï¬cant decrease. When\nN is set to 5, the improvement is limited, while the time\ncomplexity will increase. Thus, we set N to 4.\n531\nCWS POS NER AvgCTB6 MSRA PKU CTB6 UD1 UD2 Onto4 Book News Finance\n(I) Pre-Trained Language Model\nBERT 97.35 98.22 96.26 94.72 95.04 94.89 80.98 76.11 79.15 85.31 89.80\nBABERT 97.45 98.44 96.70 95.05 95.65 95.54 81.90 76.84 80.27 86.89 90.47\nBERT-wwm 97.39 98.31 96.51 94.84 95.50 95.41 80.87 76.21 79.26 84.97 89.93\nERNIE 97.37 98.25 96.30 94.90 95.28 95.12 80.38 76.56 80.36 86.03 90.06\nERNIE-Gram 97.28 98.27 96.36 94.93 95.26 95.16 80.96 77.19 79.96 85.31 90.07\nZENâ€¡ 97.33 98.28 96.55 94.76 95.55 95.54 80.06 75.67 80.17 85.05 89.90\nNEZHAâ€  97.53 98.61 96.67 94.98 95.57 95.52 81.74 77.03 79.81 85.15 90.26\n(II) Supervised Lexicon Enhanced Model\nBERT-LEâ€¡â™  97.44 98.41 96.70 94.92 95.49 95.42 81.59 77.05 80.29 86.47 90.38\nBABERT-LEâ€¡ 97.56 98.63 96.84 95.24 95.74 95.70 82.35 78.36 80.86 87.25 90.85\n(âˆ†BABERT) +0.11 +0.19 +0.14 +0.21 +0.09 +0.16 +0.45 +1.52 +0.59 +0.36 +0.38\nLatticeâ€¡ 96.10 97.80 95.80 - - - 73.88 - - - -\nMEM-BERTâ€¡ 97.16 98.28 96.51 94.82 95.60 95.46 - - - - -\nMEM-ZENâ€¡ 97.25 98.40 96.53 94.87 95.69 95.49 - - - - -\nEEBERTâ€¡ - - - - - - - 77.58 76.66 87.05 -\nTable 2: The overall results on three Chinese sequence labeling tasks, where we report the F1-score on the test set.\nâ€¡denotes external knowledge is used. â€ denotes that large-scale pre-training corpus is used. â™ indicates that we\nreproduce LEBERT in a similar way for fair comparisons.\nicons by using statistical information for entity\nand phrase extraction (Xiao et al., 2021).\nâ€¢ ZEN uses an extra N-gram encoder to inte-\ngrate external lexicon knowledge into BERT\nduring pre-training (Diao et al., 2020).\nâ€¢ NEZHA leverages functional relative posi-\ntional encoding, supervised word-level mask-\ning strategy, and enormous training data6 to\nenhance vanilla BERT (Wei et al., 2019).\nâ€¢ BERT-LEis a lexicon-enhanced BERT (Liu\net al., 2021), which introduces a lexicon\nadapter between BERT layers to incorporate\nexternal lexicon embeddings. We strictly fol-\nlow.Liu et al. (2021) to reimplement it with\nopen-source word embeddings7\n4.3 Main Results\nThe overall Chinese sequence labeling results are\nshown in Table 2. We report the F1-score of the test\ndatasets on CWS, POS, and NER tasks. Here, we\nï¬rst compare our BABERT with various Chinese\npre-trained language models to evaluate its effec-\ntiveness. Then, we compare BABERT-LE with\nother supervised lexicon-based methods to show\nthe potential of BABERT in combining with exter-\nnal lexicon knowledge.\n6NEZHA uses three large corpora, including Chinese\nWikipedia, Baidu Baike, and Chinese News, which contain\n11B tokens and are four times more than us.\n7https://ai.tencent.com/ailab/nlp/en/embedding.html\nFirst, we examine the F1 values of the BERT\nbaseline. As shown, BERT obtains comparable re-\nsults on all Chinese sequence labeling tasks, which\nis similar to that of Diao et al. (2020), Tian et al.\n(2020a) and Liu et al. (2021). BABERT signiï¬-\ncantly outperforms BERT, resulting in an increase\nof 90.47 âˆ’89.80 = 0.67 on average. This observa-\ntion clearly indicates the advantage of introducing\nboundary information into BERT pre-training.\nCompared with various BERT extensions, our\nBABERT can achieve competitive performances\nas a whole. First, in comparison with BERT-wwm,\nERNIE, ERNIE-gram, and ZEN, which lever-\nage external lexicons that include high-frequency\nwords for pre-training, BABERT outperforms all\nof them by averaging 0.54+0.41+0.40+0.57\n4 = 0.48\npoint, and achieves top scores on eight of the ten\nbenchmarks. This result is consistent with our in-\ntuition that directly exploiting a supervised lexi-\ncon can only achieve good performance in speciï¬c\ntasks, indicating the limitation of these methods\nwhen the chosen lexicon is incompatible with the\ntarget tasks. Second, we ï¬nd that BABERT sur-\npasses NEZHA in the average F1 values, indicating\nthat the boundary information is more critical than\nthe data scale for Chinese sequence labeling.\nThen, we compare our method with supervised\nlexicon-based methods. Lattice-based methods\n(Zhang and Yang, 2018; Yang et al., 2019) are the\nï¬rst to integrate word features into neural networks\nfor Chinese sequence labeling. MEM (Tian et al.,\n532\nCWS-PKU NER-Onto4\n10 50 100 10 50 100\nBERT 83.97 87.93 88.25 14.87 42.37 57.95\nBABERT 84.73 89.45 90.0232.07 46.55 60.58\nBERT-wwm84.70 88.05 88.84 12.75 43.09 59.44\nERNIE 84.31 87.04 88.20 19.86 42.96 50.81\nERNIE-Gram84.01 86.62 87.99 28.39 45.88 60.01\nNEZHA 84.40 88.70 89.73 14.45 44.10 59.20\nTable 3: Few-shot results on PKU and Onto4, using 10,\n50, and 100 instances of the training data.\n2020a,b) designs an external memory network after\nthe BERT encoder to incorporate lexicon knowl-\nedge. EEBERT (Jia et al., 2020) builds entity em-\nbeddings from the corpus and further utilizes them\nin the multi-head attention mechanism. The re-\nsults are shown in Table 2 (II). All the above meth-\nods lead to signiï¬cant improvements over the base\nBERT model, which shows the effectiveness of ex-\nternal lexicon knowledge. Moreover, BABERT can\nachieve comparable performance with the above\nmethods, which further demonstrates the potential\nof our unsupervised manner.\nBABERT learns boundary information from un-\nsupervised statistical features with vanilla BERT,\nwhich means it has excellent scalability to fuse\nwith other BERT-based supervised lexicon mod-\nels. As shown, we can see that our BABERT-LE\nachieves further improvements and state-of-the-art\nperformances on all tasks, showing the advantages\nof our unsupervised setting and boundary-aware\nlearning. Interestingly, compared with MEM-ZEN,\nBABERT-LE has larger improvements over their\ncorresponding baselines. One reason might be that\nboth ZEN and the memory network module ex-\nploits supervised lexicons, which leads to a dupli-\ncation of introduced knowledge.\n4.4 Analysis\nIn this subsection, we conduct detailed experimen-\ntal analyses for an in-depth comprehensive under-\nstanding of our method.\nFew-Shot Setting To further verify the effective-\nness of BABERT, we conduct experiments under\nthe few-shot setting, where we randomly sample\n10, 50, and 100 instances of the original training\ndata from PKU (CWS) and Onto4 (NER). For fair\ncomparisons, we compare BABERT with the pre-\ntrained language models without external super-\nvised knowledge. The results are presented in Ta-\nble 3. As the size of training data is reduced, the\nOnto4 Book News Finance\nBABERT 81.90 76.84 80.27 86.89\n+ T-test 81.77 76.65 80.12 86.23\n- PMI 81.12 76.28 79.51 85.56\n- LRE 81.51 76.47 79.67 85.94\n- LE 81.58 76.62 79.88 85.94\n- RE 81.53 76.59 79.75 85.90\nTable 4: The results of different feature combining set-\ntings on the four NER benchmark datasets.\nLayer CWS POS NER\nPKU CTB6 Onto4 News\n12 96.38 94.68 81.02 79.19\n6 96.53 94.84 81.45 79.91\n3 96.70 95.03 81.90 80.27\n1 96.62 94.90 81.59 79.52\nTable 5: The inï¬‚uence of boundary information learn\nat different layers of BERT model.\nperformance drops signiï¬cantly, indicating that the\nperformances of such models rely on the scale of\nthe labeled training data. Nevertheless, BABERT\nachieves top scores under each setting and signif-\nicantly outperforms vanilla BERT, demonstrating\nthe potential of injecting unsupervised boundary\ninformation by using regression-based boundary-\naware learning, which effectively alleviates the low-\nresource problem.\nChoice of Statistical FeaturesAs mentioned in\nSection 3.2, we use PMI and LRE to model unsu-\npervised boundary-aware representation. In addi-\ntion to these features, T-test (Xiao et al., 2021) is\nanother popular choice that can be utilized. Thus,\nwe conduct ablation experiments to check the ef-\nfectiveness of these features. We analyze ï¬ve fea-\nture combining settings, including PMI (-LRE) and\nLRE (-PMI) alone, the ablation study of LRE (-LE\nand -RE), and our two indicators with additional\nT-test features being concatenated at the end of the\noriginal boundary-aware representation (+T-test).\nAs shown in Table 4, the combination of PMI and\nLRE can achieve the best results, discarding ei-\nther of them will result in decreased performance.\nBesides, right entropy is more important than left\nentropy according to our results, which may be\nthat the right entropy is more compatible with the\nreading characteristics of Chinese. Interestingly,\n533\nENT GAM LOT FIN\nBERT 89.50 74.35 83.62 79.78\nBABERT 89.86 75.65 83.74 80.22\nBERT-wwm 83.82 75.39 81.18 81.38\nERNIE 89.75 74.76 86.69 80.20\nERNIE-Gram 88.95 74.58 84.21 79.83\nNEZHA 89.47 72.78 87.12 79.75\nTable 6: The results of different feature combining set-\ntings on the four NER benchmark datasets.\nadding T-test does not bring further improvements.\nOne possible reason is that the T-test is essentially\nsimilar to the entropy measure of 2-grams, which\nhas already been injected into our BABERT model.\nBoundary Information Encoding LayerPrevi-\nous works (Jawahar et al., 2019; Liu et al., 2021)\nexploit the fact that different BERT layers would\ngenerate different concept representations. The\nshallow BERT layers are more likely to capture ba-\nsic lexicon information, while the top layers focus\non the semantic representation. We empirically set\nl in {1, 3, 6, 12}to explore the effect of computing\nboundary-aware loss by the hidden features Hl of\ndifferent BERT layers on Chinese sequence label-\ning tasks. Table 5 shows the results. We can see\nthat the best F1-score can be achieved when l = 3\non all datasets, which indicates that the BABERT\nstill needs sufï¬cient parameters to learn the basic\nboundary information. Interestingly, the BABERT\nperforms poorly when l = 12, which might be\ndue to a conï¬‚ict between the MLM loss and our\nboundary-aware regression loss during pretraining.\nQualitative Analysis To explore how BABERT\nimproves the performance for Chinese sequence\nlabeling, we conduct qualitative analysis on the\nNews test dataset, which consists of four different\nsubdomains, namely game (GAM), entertainment\n(ENT), lottery (LOT) and ï¬nance (FIN). The results\nare shown in Table 6. We can see that compared\nwith other pre-trained language models, BABERT\ncan obtain consistent improvement in all domains\nwith unsupervised statistical boundary information,\nwhile the other models only improve performance\non speciï¬c domains. Moreover, as shown in Ta-\nble 7, we also give an example from the game do-\nmain to further demonstrate the effectiveness of our\nmethod. BABERT is the only model that correctly\nrecognizes all entities. In particular, the prediction\nof BABERT for the entity \"WCG2011 org\" indi-\ncates the potential of boundary information.\nSentence å¦‚æœWCG2011orgæœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nIf WCG2021 has W AR3 and StarCraft2 these two projects\nBERT å¦‚æœWCGorg2011æœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nBABERT å¦‚æœWCG2011orgæœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nBERT-wwm å¦‚æœWCG2org011æœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nERNIE å¦‚æœWCG20org11æœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nERNIE-Gramå¦‚æœWCGorg2011æœ‰W AR3gameå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nNEZHA å¦‚æœWCG2011æœ‰W AR3orgå’Œæ˜Ÿé™…2gameä¸¤ä¸ªé¡¹ç›®\nTable 7: Example from the game domain. Red (Blue)\nrepresents correct (incorrect) entities.\n5 Conclusion\nIn this paper, we proposed BABERT, a novel\nunsupervised boundary-aware pre-training model\nfor Chinese sequence labeling. In BABERT,\ngiven a Chinese sentence, we calculated boundary-\naware representation with unsupervised statisti-\ncal information to capture boundary information,\nand directly injected such information into BERT\nweights during pre-training. Unlike previous works,\nBABERT exploited an effective way to utilize\nboundary information in an unsupervised manner,\nthereby alleviating the limitations of supervised\nlexicon-based approaches. Experimental results\non ten benchmark datasets of three different tasks\nillustrated that our method was highly effective\nand better than other Chinese pre-trained models.\nMoreover, the combination with supervised lexicon\nextensions could achieve further improvements and\nstate-of-the-art results on most tasks.\nLimitations\nBABERT suffers from three major limitations. The\nï¬rst limitation is that in the boundary information\nextractor, we empirically chose PMI and LRE. In\naddition to these indicators and the T-test measure\nwe veriï¬ed in the experimental analyses, some al-\nternatives that contain boundary information could\nbe used to compute boundary-aware representation.\nThus, we plan to explore more unsupervised sta-\ntistical features. The second limitation is that we\nfocused only on Chinese sequence labeling tasks in\nthis work, ignoring the potential of boundary infor-\nmation and BABERT in other Chinese NLP tasks.\nThe third one is that we only consider BABERT\nfor Chinese. For other languages which do not use\nspaces between words such as Japanese and Thai,\nwe can also attempt to inject boundary information,\nand the effectiveness in these languages should be\nveriï¬ed by experiments. Further research is needed\nto evaluate our BABERT in future studies.\n534\nAcknowledgements\nThis work is supported by grants from the National\nKey Research and Development Program of China\n(No. 2018YFC0832101) and the National Natural\nScience Foundation of China (No. 62176180).\nReferences\nJerome R Bellegarda. 2004. Statistical language model\nadaptation: review and perspectives. Speech com-\nmunication, 42(1):93â€“108.\nGerlof Bouma. 2009. Normalized (pointwise) mutual\ninformation in collocation extraction. Proceedings\nof GSCL, 30:31â€“40.\nJason Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional lstm-cnns. Transac-\ntions of the Association for Computational Linguis-\ntics, 4(0):357â€“370.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and\nZiqing Yang. 2021. Pre-training with whole word\nmasking for chinese bert. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing,\n29:3504â€“3514.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of 2019 Conference of the\nNAACL, pages 4171â€“4186.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\ntext encoder enhanced by n-gram representations. In\nFindings of the EMNLP 2020, pages 4729â€“4740. As-\nsociation for Computational Linguistics.\nNing Ding, Dingkun Long, Guangwei Xu, Muhua Zhu,\nPengjun Xie, Xiaobin Wang, and Haitao Zheng.\n2020. Coupling distant annotation and adversarial\ntraining for cross-domain Chinese word segmenta-\ntion. In Proceedings of the 58th ACL, pages 6662â€“\n6671. Association for Computational Linguistics.\nThomas Emerson. 2005. The second international Chi-\nnese word segmentation bakeoff. In Proceedings of\nthe Fourth SIGHAN Workshop on Chinese Language\nProcessing.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 981â€“993.\nAssociation for Computational Linguistics.\nShohei Higashiyama, Masao Utiyama, Eiichiro Sumita,\nMasao Ideuchi, Yoshiaki Oida, Yohei Sakamoto,\nand Isaac Okada. 2019. Incorporating word atten-\ntion into character-based word segmentation. In\nProceedings of the 2019 Conference of the NAACL,\npages 2699â€“2709. Association for Computational\nLinguistics.\nGanesh Jawahar, BenoÃ®t Sagot, and DjamÃ© Seddah.\n2019. What does bert learn about the structure of\nlanguage? In Proceedings of the 57th ACL.\nChen Jia, Yuefeng Shi, Qinrong Yang, and Yue Zhang.\n2020. Entity enhanced BERT pre-training for Chi-\nnese NER. In Proceedings of the 2020 Conference\non EMNLP, pages 6384â€“6396. Association for Com-\nputational Linguistics.\nGuangjin Jin and Xiao Chen. 2008. The fourth inter-\nnational Chinese language processing bakeoff: Chi-\nnese word segmentation, named entity recognition\nand Chinese POS tagging. In Proceedings of the\nSixth SIGHAN Workshop on Chinese Language Pro-\ncessing.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of NAACL,\npages 260â€“270. Association for Computational Lin-\nguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nPercy Liang. 2005. Semi-supervised learning for natu-\nral language. Ph.D. thesis, Massachusetts Institute\nof Technology.\nWei Liu, Xiyan Fu, Yue Zhang, and Wenming Xiao.\n2021. Lexicon enhanced Chinese sequence label-\ning using BERT adapter. In Proceedings of the 59th\nACL and the 11th IJCNLP, pages 5847â€“5858. Asso-\nciation for Computational Linguistics.\nWei Liu, Tongge Xu, Qinghua Xu, Jiayu Song, and\nYueran Zu. 2019a. An encoding strategy based\nword-character LSTM for Chinese NER. In Pro-\nceedings of the 2019 Conference of the NAACL,\npages 2379â€“2389. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n535\nJin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.\nA maximum entropy approach to chinese word seg-\nmentation. In Proceedings of the fourth SIGHAN\nworkshop on Chinese language processing.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th ACL, pages 1064â€“\n1074. Association for Computational Linguistics.\nYuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie,\nFan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and\nJiwei Li. 2019. Glyce: Glyph-vectors for chinese\ncharacter representations. Advances in Neural Infor-\nmation Processing Systems, 32.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajic, Christopher D Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, et al. 2016. Universal dependencies\nv1: A multilingual treebank collection. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LRECâ€™16), pages\n1659â€“1666.\nFuchun Peng, Fangfang Feng, and Andrew McCallum.\n2004. Chinese segmentation and new word detec-\ntion using conditional random ï¬elds. In COLING\n2004: Proceedings of the 20th International Confer-\nence on Computational Linguistics, pages 562â€“568.\nYan Shao, Christian Hardmeier, JÃ¶rg Tiedemann, and\nJoakim Nivre. 2017. Character-based joint segmen-\ntation and POS tagging for Chinese using bidirec-\ntional RNN-CRF. In Proceedings of the 8-th IJC-\nNLP, pages 173â€“183. Asian Federation of Natural\nLanguage Processing.\nMo Shen, Wingmui Li, HyunJeong Choe, Chenhui\nChu, Daisuke Kawahara, and Sadao Kurohashi.\n2016. Consistent word segmentation, part-of-speech\ntagging and dependency labelling annotation for\nChinese language. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 298â€“308. The\nCOLING 2016 Organizing Committee.\nYan Song and Fei Xia. 2012. Using a goodness mea-\nsurement for domain adaptation: A case study on\nchinese word segmentation. In In Proceedings of\nthe Eight International Conference on Language Re-\nsources and Evaluation (LRECâ€™12.\nWeiwei Sun and Hans Uszkoreit. 2012. Capturing\nparadigmatic and syntagmatic lexical relations: To-\nwards accurate Chinese part-of-speech tagging. In\nProceedings of the 50th ACL, pages 242â€“252. Asso-\nciation for Computational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nYuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiao-\njun Quan, Tong Zhang, and Yonggang Wang. 2020a.\nJoint Chinese word segmentation and part-of-speech\ntagging via two-way attentions of auto-analyzed\nknowledge. In Proceedings of the 58th ACL, pages\n8286â€“8296. Association for Computational Linguis-\ntics.\nYuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and\nYonggang Wang. 2020b. Improving Chinese word\nsegmentation with wordhood memory networks. In\nProceedings of the 58th ACL, pages 8274â€“8285. As-\nsociation for Computational Linguistics.\nAndrew Viterbi. 1967. Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE transactions on Information Theory,\n13(2):260â€“269.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNezha: Neural contextualized representation for\nchinese language understanding. arXiv preprint\narXiv:1909.00204.\nRalph Weischedel, Sameer Pradhan, Lance Ramshaw,\nMartha Palmer, Nianwen Xue, Mitchell Marcus,\nAnn Taylor, Craig Greenberg, Eduard Hovy, Robert\nBelvin, et al. 2011. Ontonotes release 4.0.\nLDC2011T03, Philadelphia, Penn.: Linguistic Data\nConsortium.\nDongling Xiao, Yu-Kun Li, Han Zhang, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. ERNIE-\ngram: Pre-training with explicitly n-gram masked\nlanguage modeling for natural language understand-\ning. In Proceedings of the 2021 Conference of\nNAACL, pages 1702â€“1715. Association for Compu-\ntational Linguistics.\nNaiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta\nPalmer. 2005. The penn chinese treebank: Phrase\nstructure annotation of a large corpus. Natural lan-\nguage engineering, 11(2):207.\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng\nQiu. 2019. Tener: adapting transformer encoder\nfor named entity recognition. arXiv preprint\narXiv:1911.04474.\nJie Yang, Zhiyang Teng, Meishan Zhang, and Yue\nZhang. 2016. Combining discrete and neural fea-\ntures for sequence labeling. In The 17th Interna-\ntional Conference on Intelligent Text Processing and\nComputational Linguistics (CICLing).\nJie Yang, Yue Zhang, and Shuailong Liang. 2019. Sub-\nword encoding in lattice LSTM for Chinese word\nsegmentation. In Proceedings of the 2019 Confer-\nence of the NAACL, pages 2720â€“2725. Association\nfor Computational Linguistics.\n536\nXinsong Zhang, Pengshuai Li, and Hang Li. 2021.\nAMBERT: A pre-trained language model with multi-\ngrained tokenization. In Findings of the ACL-\nIJCNLP 2021, pages 421â€“435. Association for Com-\nputational Linguistics.\nYue Zhang and Jie Yang. 2018. Chinese NER using lat-\ntice LSTM. In Proceedings of the 56th ACL, pages\n1554â€“1564. Association for Computational Linguis-\ntics.\n537",
  "topic": "Lexicon",
  "concepts": [
    {
      "name": "Lexicon",
      "score": 0.9259440898895264
    },
    {
      "name": "Computer science",
      "score": 0.8365384936332703
    },
    {
      "name": "Sequence labeling",
      "score": 0.7261660695075989
    },
    {
      "name": "Natural language processing",
      "score": 0.6952295303344727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6648404002189636
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5626259446144104
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5469378232955933
    },
    {
      "name": "ENCODE",
      "score": 0.5360335111618042
    },
    {
      "name": "Text segmentation",
      "score": 0.52855384349823
    },
    {
      "name": "Boundary (topology)",
      "score": 0.5214663743972778
    },
    {
      "name": "Complement (music)",
      "score": 0.5059358477592468
    },
    {
      "name": "Segmentation",
      "score": 0.4840070903301239
    },
    {
      "name": "Word (group theory)",
      "score": 0.46621188521385193
    },
    {
      "name": "Language model",
      "score": 0.46390506625175476
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.42576587200164795
    },
    {
      "name": "Hidden Markov model",
      "score": 0.418876588344574
    },
    {
      "name": "Speech recognition",
      "score": 0.3425421118736267
    },
    {
      "name": "Linguistics",
      "score": 0.12338674068450928
    },
    {
      "name": "Complementation",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 9
}