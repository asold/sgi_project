{
  "title": "Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning",
  "url": "https://openalex.org/W3103920202",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A2988554339",
      "name": "Shijing Si",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A2109369421",
      "name": "Guoyin Wang",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2082114532",
      "name": "Lei Zhang",
      "affiliations": [
        "Fidelity Investments (United States)",
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A657437189",
      "name": "Lawrence Carin",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A2790424367",
      "name": "Ricardo Henao",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963912736",
    "https://openalex.org/W4298077097",
    "https://openalex.org/W4254298996",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W4234117503",
    "https://openalex.org/W114517082",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W1972306304",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3129401410",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2964061809",
    "https://openalex.org/W2091526670",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W2086573335",
    "https://openalex.org/W2577250310",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2171061940",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4394666973"
  ],
  "abstract": "Pretrained Language Models (PLMs) have improved the performance of natural language understanding in recent years. Such models are pretrained on large corpora, which encode the general prior knowledge of natural languages but are agnostic to information characteristic of downstream tasks. This often results in overfitting when fine-tuned with low resource datasets where task-specific information is limited. In this paper, we integrate label information as a task-specific prior into the self-attention component of pretrained BERT models. Experiments on several benchmarks and real-word datasets suggest that the proposed approach can largely improve the performance of pretrained models when fine-tuning with small datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3181–3186\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3181\nIntegrating Task Speciﬁc Information into Pretrained Language Models\nfor Low Resource Fine Tuning\nRui Wang1* Shijing Si 1* Guoyin Wang 1,2 Lei Zhang3 Lawrence Carin1 Ricardo Henao1\n1Duke University 2Amazon Alexa AI 3Fidelity Investments\nrui.wang16@duke.edu\nAbstract\nPretrained Language Models (PLMs) have im-\nproved the performance of natural language\nunderstanding in recent years. Such mod-\nels are pretrained on large corpora, which\nencode the general prior knowledge of natu-\nral languages but are agnostic to information\ncharacteristic of downstream tasks. This of-\nten results in overﬁtting when ﬁne-tuned with\nlow resource datasets where task-speciﬁc in-\nformation is limited. In this paper, we inte-\ngrate label information as a task-speciﬁc prior\ninto the self-attention component of pretrained\nBERT models. Experiments on several bench-\nmarks and real-word datasets suggest that the\nproposed approach can largely improve the\nperformance of pretrained models when ﬁne-\ntuning with small datasets. The code repos-\nitory is released in https://github.com/\nRayWangWR/BERT_label_embedding.\n1 Introduction\nRecently, Pretrained Language Models (PLMs)\n(Devlin et al., 2018; Radford et al., 2019) have yield\nsigniﬁcant progress on various natural language\nprocessing (NLP) tasks, e.g., neural language un-\nderstanding, text generation, etc. Existing PLMs\nare usually pretrained in a task-agnostic manner, in\nwhich the model is expected to capture the general\nknowledge of natural language from a large corpus,\nindependent of downstream-speciﬁc information.\nThis is not a problem when data is abundant in the\ndownstream dataset, in which case, the model can\neffectively extract task-speciﬁc information during\nﬁne-tuning. However, in real scenarios, data may\nbe difﬁcult to collect and labeling is usually expen-\nsive. We show that PLMs pretrained with general\nknowledge can overﬁt without enough guidance\nfrom the task-speciﬁc information, resulting in de-\ngraded performance during testing.\n*These authors contributed equally to this work\nA clear-cut solution to this problem is to focus\nmore on samples that are more relevant to the tar-\nget task during pretraining. However, this requires\na task-speciﬁc pretraining, which in most cases\nis computational or time prohibitive. Another ap-\nproach is to pretrain on an auxiliary dataset before\nﬁne-tuning on the target task (Phang et al., 2018).\nSuch method requires the availability of an appro-\npriate auxiliary datasets. Unfortunately, in some\ncases it may negatively impact the downstream\ntransfer (Wang et al., 2018a). Label embeddings\n(Akata et al., 2015) can be regarded as a feature-\nbased deﬁnition of a classiﬁcation task, in which\ndetailed information of the task is encoded. One\nnatural question is whether we can combine the\ngeneral knowledge in a PLM and the task-speciﬁc\ncharacterization contained within label embeddings\nfor better ﬁne-tuning on low-resource tasks.\nIn this paper, we propose to utilize the label em-\nbeddings as a task-speciﬁc prior, complementary to\nthe general prior already encoded during pretrain-\ning. We learn and integrate these label embeddings\ninto BERT models (Devlin et al., 2018) to regular-\nize its self-attention modules, so the task-irrelevant\ntokens or patterns can be readily ﬁltered out, while\nthe task-speciﬁc information can be enhanced dur-\ning ﬁne-tuning. Such a modiﬁcation is compatible\nwith any PLM built upon self-attention and will not\ndegrade the original pretrained structure.\nIn order to validate the performance of our ap-\nproach in a real-world setting, we collected two text\nclassiﬁcation datasets from the online patient portal\nof a large academic health system, each with a few\nthousand sequences. These are the ﬁrst datasets\nfor automatic patient message triage, which consti-\ntute an important problem in the ﬁeld of clinical\ndata analysis. Experimental results show that our\napproach signiﬁcantly improves the performance\nof ﬁne-tuning on low-resource datasets, e.g., those\nconsisting of only several thousand data samples.\n3182\n2 Related Work\nLabel embeddings have been previously leveraged\nfor image classiﬁcation (Akata et al., 2015), multi-\nmodal learning between images and text (Kiros\net al., 2014), text recognition in images (Rodriguez-\nSerrano and Perronnin, 2015), zero-shot learning\n(Li et al., 2015; Ma et al., 2016) and text classiﬁca-\ntion (Zhang et al., 2017). Notably, LEAM (Wang\net al., 2018b) jointly embeds words (tokens) and\nlabels in a common latent space as a means to im-\nprove the performance on general text classiﬁcation\ntasks. Further, Moreo et al. (2019) concatenates\nlabel embedding with word embeddings. However,\nthis approach cannot be directly implemented into\nPLMs since the new (concatenated) embedding is\nnot compatible with the pretrained parameters. We\nintegrate label embeddings into the self-attention of\nBERT models, so the attention can be regularized\nto better focus on task-relevant information.\n3 Methods\n3.1 The BERT Model\nThe encoder of BERT and other popular PLMs\nare built upon the transformer architecture, which\nis composed of multiple layers of multi-head self-\nattention and position-wise feed-forward layers.\nMulti-head Self-attention The multi-head self-\nattention is an ensemble of multiple single-head\nself-attention modules. Let X ∈RL×D be the em-\nbedding matrix of the input sequence with length\nL. For each single head, the input sequence is\nﬁrst mapped into the key, query and value triplet,\ndenoted as,\nK = XWK, Q= XWQ, V = XWV , (1)\nwhere {WK,WQ,WV } ∈RD×d are projection\nmatrices. The self-attention can be formalized as\nA= QKT\n√\nd\n∈RL×L, (2)\nHi = softmax(A)V ∈RL×D, (3)\nwhere i = 1,...,h , h is the number of heads,\nsoftmax(·) is the row-wise softmax function and\ndis the head dimension. Ais the attention score\nmatrix representing the compatibility between Q\nand K. The multi-head self-attention is deﬁned by\nconcatenating and projecting {Hi}h\ni=1, the repre-\nsentation of each head, into ˆH ∈RL×D.\nPositional-wise Feed Forward Layer After self-\nattention, a fully connected network is applied on\neach token representation xusing\nFFN(x) = max(0,max(0,xW1 + b1)W2 + b2),\nwhich consists of two linear transformations and\nReLU activations.\nIn BERT, the input sequence starts with a[CLS]\ntoken, whose hidden state will be extracted as\nthe sequence representation for classiﬁcation. Let\nCE(·,·) be the cross-entropy loss,C(·) be the ﬁnal\nclassiﬁer and enc(·) be the encoder consisting of a\nstack of transformer layers. The classiﬁcation loss\ncan be written as,\nLc = E(X,y)∼D[CE(C(enc(X)[CLS]),y)] (4)\nwhere enc(X)[CLS] is the representation of [CLS]\nafter encoding, yis the classiﬁcation label and D\nis a dataset.\nIn the context of graph embeddings (Kipf and\nWelling, 2016), the [CLS] token acts as a super\nnode that connects to all other tokens (nodes) and\naggregates global information during self-attention\n(convolution). After training, the embedding of\nthe [CLS] token should contain the task-speciﬁc\ninformation, so that it can mostly attend to task rel-\nevant information in self-attention during inference.\nHowever, embeddings of the PLMs are pretrained\nagnostic to downstream tasks. When ﬁne-tuning\nwith low-resource datasets where label informa-\ntion is scarce, a single [CLS] token may not cap-\nture enough task speciﬁc information, resulting in\nmodel overﬁtting to task irrelevant tokens or pat-\nterns in the input sequences.\n3.2 Integrating Label Embedding into\nSelf-Attention\nIn this paper, we propose to leverage label embed-\ndings to optimize the self-attention modules, so the\nmodel can better focus on task-relevant information\nwhen ﬁne-tuned with small datasets.\nWe reformulate the representations in (1) as\n{Kw,Qw,Vw}by replacing X with block ma-\ntrix Xw = [XCLS ,X], where XCLS ∈ R1×D\nand X ∈R(L−1)×D represent the embeddings of\n[CLS] and the other tokens in the sequence, respec-\ntively. The attention score matrix can be rewritten\nas,\nA= 1√\nd\n[\nQ[CLS]KT\n[CLS] Q[CLS]KT\nQKT\n[CLS] QKT\n]\n. (5)\n3183\n(a)\n (b)\nFigure 1: (a) Incorporating label embeddings into multi-head self-attention. C(·) is the classiﬁer for the BERT\nmodel. (b) Modifying self-attention scores with label embeddings. ⨁ indicates row concatenation.\nWe denote the cross-attention between the [CLS]\ntoken and all the other input tokens as S ≜\nQT\n[CLS]KT ∈R1×(L−1). Let Xl ∈RM×D be the\nlabel embedding matrix, where M is the number\nof classes. We ﬁrst compute the cross attention\nbetween Xl and X as\nAl = QlKT\n√\nd\n, Ql = XlWQ, (6)\nwhere Xl is encoded in to Ql with the same map-\nping matrix WQ as in (1). Then, we compute a\nmodiﬁed cross-attention row vector S′by concate-\nnating Sand Al by row and keeping the maximum\nvalue of each column,\nS′= max([S; Al]) ∈R1×L. (7)\nAs a result, S′represents the maximum attention\nscore of a input token with both [CLS] and the\nlabel embeddings. A new attention score matrix A′\ncan be obtained by replacing Swith S′in (5),\nA′= 1√\nd\n[\nQ[CLS]KT\n[CLS] S′\nQKT\n[CLS] QKT\n]\n. (8)\nIn (8), when a token is highly relevant to one of the\nlabels, it will result in a larger attention score in\nS′, thus the [CLS] embedding will be less affected\nby irrelevant information in the sequence, unlike\n(2) where only attention from the current [CLS]\nembedding is considered. The proposed attention\nlayer is shown in Figure 1(b). The attention score\nmatrix Ain (2) is replaced as A′in (8). All other\ncomponents are the same as the original layers in\nBERT as in (1)–(3.1).\nWe share the same label embedding Xl for all\nthe layers. The label embedding is adapted on each\nlayer via WQ in the multi-head attention module.\nAs shown in Figure 1a, we also feed Xl into the\nﬁnal classiﬁer C(·), so the label embeddings can\nbe classiﬁed into their corresponding classes. The\nﬁnal loss for classiﬁcation is then\nLlabel =\nM∑\ni=1\nCE(C(Xi\nl ),i), (9)\nLfinal = Lc + λLlabel. (10)\nwhere Xi\nl is the i-th label embedding, λis a trade-\noff parameter between the regularization on label\nembeddings and the original classiﬁcation loss.\nThe label embeddings can be initialized ran-\ndomly or by the pretrained embeddings of rele-\nvant keywords. When the label is not identiﬁed by\nkeywords, e.g., in sentence entailment tasks, their\nembeddings can be initialized with the represen-\ntations of [CLS], averaged over samples from the\nsame class. All other parameters can be initialized\nfrom the pretrained BERT. This modiﬁcation can\nbe adapted to any PLM with self-attention modules.\n4 Experiments\nWe focus on ﬁne-tuning with small datasets. We\nintegrate label embeddings into the pretrained\n(Bio)BERT models, and ﬁne-tune on various clas-\nsiﬁcation benchmarks as well as two real-world\nclinical datasets that we collected from the online\npatient portal of a large academic health system.\n4.1 Public Benchmarks\nTable 1 shows the results of integrating label em-\nbedding into the pretrained bert-based-uncased\nmodel on 9 public classiﬁcation benchmarks of var-\nious sizes. We ﬁnd that our method improves the\nresults from BERT on small datasets, e.g, WNLI,\nMRPC, CoLA, etc, which typically have only sev-\neral thousand data samples available for ﬁne-tuning.\nThis shows that the BERT model, which is pre-\ntrained with task-agnostic objectives, is more likely\n3184\nTable 1: Results on public benchmarks.\nMethod TREC(5.5k)WNLI(0.6k)RTE(2.5k)MRPC(3.7k)CoLA(8.5k)IMDB(25k)SST-2(67k)MNLI-M/MM(393k) QQP(364k)Avg\nBERT(Devlin et al., 2018) 97.00 55.11 63.90 87.29 54.47 92.36 92.3284.38/84.87 87.53 79.92\nOur Method 97.40 57.75 66.43 89.48 56.26 92.43 92.58 84.12/ 84.6287.84 80.89\n(a) Attention from the BioBERT.\n(b) Attention from our method.\nFigure 2: Examples of the attention from the [CLS]\ntoken in the ﬁnal attention layer. The sequences are\nsampled from the Message-urgency dataset. Red color\nindicates higher attention score. It can be shown that\nour method can better focus on keywords, e.g., ’chest’,\n’bad’ and ’stairs’, which are more likely to ocurr on\nurgent requests. Alternatively, BioBERT ﬁne-tuned on\nsuch a small dataset tends to overﬁt to task-irrelevant\nwords, such as ’holiday’, ’school’, ’tests’,etc.\nto overﬁt when there is limited task-speciﬁc infor-\nmation during ﬁne-tuning. However, our method\nproduces comparable results on larger datasets such\nas MNLI and QQP. This is consistent with the study\nin Lazar (2003) where additional priors are less use-\nful when the size of dataset grows larger. These\nresults suggest that our method is more suitable\nfor ﬁne-tuning with smaller amounts of data, and\nthat our approach to injecting the label informa-\ntion is at least not detrimental to the original pre-\ntrained model. This supports the intuition of com-\nbining the pretrained general knowledge and the\ntask-speciﬁc information for better ﬁne-tuning with\nsmall datasets.\nWe note that label information can improve the\nresults on many tasks of neural language inference,\ne.g., WMLI and QQP, where classes are not iden-\ntiﬁed by keywords, but rather certain patterns in\nthe input sentence pair. This may be because the\nself-attention will encode these input patterns into\nintermediate tokens, which act as pseudo keywords\nTable 2: Results on our healthcare datasets. Values are\nshown as F1/Precision/Recall.\nDataset Message-urgency(1.7k) Acknowledgment(1.6k)\nBERT(Devlin et al., 2018) 0.761/0.762/0.761 0.980/0.976/0.984\nBioBERT(Lee et al., 2020) 0.764/0.774/0.758 0.985/0.990/0.980\nOur Method 0.789/0.784/0.7970.990/0.993/0.987\nthat can be emphasized by the attention from label\nembeddings.\n4.2 Patient Message Triage\nWe further evaluate the proposed approach in real-\nworld scenarios of patient message classiﬁcation.\nThis is a task motivated by the increasing popular-\nity of online patient portals. Most of the patient\nmessages generated from the portal are non-urgent,\nwhile the doctors are expected to focus on the ur-\ngent requests, which amount to only a small por-\ntion (about 10%) of all messages. As a result, the\nheath providers will have to spend considerable\ntime just identifying urgent messages, thus being\nless efﬁcient at emergency responses. We obtain\ntwo healthcare datasets –Message-urgency and Ac-\nknowledgment– from a large academic health sys-\ntem online portal. Detailed description of these two\ndatasets can be found in Appendix A.\nWe employ our method on the BioBERT pre-\ntrained model (Lee et al., 2020), which has the\nsame architecture as BERT but further pretrained\non the clinical corpora. Results are shown in Ta-\nble 2. Our model improves on all the baselines in\nterms of F1 score, which validates the usefulness of\nthe proposed method for low-resource ﬁne-tuning\nin the real scenarios.\n5 Conclusion\nWe propose to integrate task speciﬁc information\ninto PLMs that are pretrained with task-agnostic ob-\njectives. To do this, we leverage label embeddings\nto regularize the self-attention in PLMs. Results on\npublic benchmarks and real-world datasets suggest\nthat our method can effectively improve the results\nfor low resource ﬁne-tuning.\n3185\nReferences\nZeynep Akata, Florent Perronnin, Zaid Harchaoui, and\nCordelia Schmid. 2015. Label-embedding for image\nclassiﬁcation. IEEE transactions on pattern analy-\nsis and machine intelligence, 38(7):1425–1438.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nL´eon Bottou. 2010. Large-scale machine learning\nwith stochastic gradient descent. In Proceedings of\nCOMPSTAT’2010, pages 177–186. Springer.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation.\nIn Proceedings of the IEEE international conference\non computer vision, pages 1026–1034.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nDiederik P Kingma and Max Welling. 2013. Auto-\nencoding variational bayes. arXiv preprint\narXiv:1312.6114.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nRyan Kiros, Ruslan Salakhutdinov, and Rich Zemel.\n2014. Multimodal neural language models. In In-\nternational conference on machine learning, pages\n595–603.\nNicole A Lazar. 2003. Bayesian empirical likelihood.\nBiometrika, 90(2):319–326.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nXirong Li, Shuai Liao, Weiyu Lan, Xiaoyong Du, and\nGang Yang. 2015. Zero-shot image tagging by hi-\nerarchical semantic embedding. In Proceedings of\nthe 38th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 879–882.\nYukun Ma, Erik Cambria, and Sa Gao. 2016. Label\nembedding for zero-shot ﬁne-grained named entity\ntyping. In Proceedings of COLING 2016, the 26th\nInternational Conference on Computational Linguis-\ntics: Technical Papers, pages 171–180.\nAlejandro Moreo, Andrea Esuli, and Fabrizio Se-\nbastiani. 2019. Word-class embeddings for\nmulticlass text classiﬁcation. arXiv preprint\narXiv:1911.11506.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJose Antonio Rodriguez-Serrano and Florent C Per-\nronnin. 2015. Label-embedding for text recognition.\nUS Patent 9,008,429.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-\npagari, R Thomas McCoy, Roma Patel, Najoung\nKim, Ian Tenney, Yinghui Huang, Katherin Yu, et al.\n2018a. Can you tell me how to get past sesame\nstreet? sentence-level pretraining beyond language\nmodeling. arXiv preprint arXiv:1812.10860.\nGuoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe\nZhang, Dinghan Shen, Xinyuan Zhang, Ricardo\nHenao, and Lawrence Carin. 2018b. Joint embed-\nding of words and labels for text classiﬁcation. In\nACL.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nHonglun Zhang, Liqiang Xiao, Wenqing Chen,\nYongkun Wang, and Yaohui Jin. 2017. Multi-task la-\nbel embedding for text classiﬁcation. arXiv preprint\narXiv:1710.07210.\nA Description of healthcare datasets\nIn this work, we utilized 1,756 web portal mes-\nsages generated from 10/2014 to 08/2018 by adult\npatients (>18 years old) of a large academic med-\nical center. The Electronic Health Record (EHR)\nsystem (Epic Verona, WI, USA) with associated pa-\ntient portal (MyChart) was the source of all patient\nmessages. A custom-built Application Program-\nming Interface (API) securely made available the\nportal messages from the EHR enterprise data ware-\nhouse into a highly protected virtual network space\noffered by the medical center. Approved users\nwere allowed access to work with the identiﬁable\n3186\nLabel Count Typical Example\nNon-urgent 631 That would be awesome... thank you.\nMedium 955 Dr. [name]. All seems well now. I am at home resting.\nMy wife and I have a trip planned to Maryland this week\nbeginning on Wednesday. We can ﬂy, drive or stay home\nif I should not travel. Are there any reasons that I should\nnot ﬂy.\nUrgent 170 I have continued having chest pain shortness of breath\nsince waking. Please tell me what to do. I have tried in\nhailers am going to try nebulizers. I just feel extremely\ntight in my chest.\nTable 3: Typical examples of patient messages to providers grouped by urgency. These are examples of the message\nurgency dataset used in the experiments.\nLabel Count Typical Example\n1 1123 Thank you. Have a\ngood day.\n0 566 I have continued hav-\ning chest pain short-\nness of breath since\nwaking. Please let me\nknow what to do.\nTable 4: Typical examples of patient messages to\nproviders. Label 1 for messages being pure acknowl-\nedgment, while 0 for non-trivial messages.\nprotected health information. These messages in-\ncluded free, unstructured plain text sent by patients\nto their healthcare team. Responses and messages\nsent from the clinician or health system to the pa-\ntient were excluded from the analysis.\nA.1 Message-urgency dataset\nIn message-urgency dataset, portal messages were\nmanually labeled by experienced sub-specialty (car-\ndiology) clinicians into three levels of priority:\nnon-urgent, medium and urgent. Non-urgent la-\nbels include notes of appreciation (e.g., thank you).\nThe Medium urgency class contains messages that\ncould be reasonably responded to in 1-3 days. Ur-\ngent messages are those requiring an immediate\nphone call to the patient by the clinician. Condi-\ntions suggesting acute myocardial infarction, ex-\nacerbation of heart failure respiratory distress or\npossible stroke were labeled as urgent and would\nbe inappropriate for an asynchronous patient portal.\nA.2 Acknowledgment dataset\nThis acknowledgment dataset is randomly selected\nfrom patient’s responses to the hospital. A signiﬁ-\ncant portion of these messages is purely acknowl-\nedgment, like ’Thank you’. It would be helpful if\nthis type of messages can be ﬁltered out, so that\nhospital staff can focus on non-trivial messages.\nA doctor and a nurse labelled and validated this\ndataset.\nB Implementation Details\nFor all the experiments, we use ﬁnetune the pre-\ntrained model for 3 epoches with learning rate 2e-5\nand batch size 32. We use the Adam training al-\ngorithm. λis generally set to 3. We set warm up\nsteps as 10 percent of the total training steps. We\ndo not apply weight decay and the norm of all the\ngradients are clipped by 1. Experiments on the\npublic benchmarks are run on a TITAN X (Pascal)\n1080 gpu. The healthcare experiment are run on\nthe CPU in a secured virtual machine system.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8653867244720459
    },
    {
      "name": "Overfitting",
      "score": 0.8270084857940674
    },
    {
      "name": "Task (project management)",
      "score": 0.709175705909729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6564576029777527
    },
    {
      "name": "ENCODE",
      "score": 0.6284347772598267
    },
    {
      "name": "Language model",
      "score": 0.6046711206436157
    },
    {
      "name": "Natural language processing",
      "score": 0.555769145488739
    },
    {
      "name": "Natural language understanding",
      "score": 0.5465107560157776
    },
    {
      "name": "Word (group theory)",
      "score": 0.5106785893440247
    },
    {
      "name": "Natural language",
      "score": 0.495674729347229
    },
    {
      "name": "Question answering",
      "score": 0.43465837836265564
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4265936017036438
    },
    {
      "name": "Machine learning",
      "score": 0.4231051206588745
    },
    {
      "name": "Artificial neural network",
      "score": 0.12816864252090454
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1318611468",
      "name": "Fidelity Investments (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}