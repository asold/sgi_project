{
  "title": "Learning the Visualness of Text Using Large Vision-Language Models",
  "url": "https://openalex.org/W4389524503",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1983308573",
      "name": "Gaurav Verma",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2321607291",
      "name": "Ryan Rossi",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4287037429",
      "name": "Christopher Tensmeyer",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2634692963",
      "name": "Jiuxiang Gu",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A114841185",
      "name": "Ani Nenkova",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1985449126",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4385573039",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2073679637",
    "https://openalex.org/W4385571588",
    "https://openalex.org/W3162741707",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W4299518610",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2184410296",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2141000440",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4225432580",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4312697098",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W3173610337",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2028891871",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4303444943",
    "https://openalex.org/W3190434222",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W4230405732",
    "https://openalex.org/W2476034201",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W2970726176"
  ],
  "abstract": "Visual text evokes an image in a person’s mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model’s contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2394–2408\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLearning the Visualness of Text Using Large Vision-Language Models\nGaurav Verma\nGeorgia Institute of Technology\ngverma@gatech.edu\nRyan A. Rossi\nAdobe Research\nryrossi@adobe.com\nChristopher Tensmeyer\nAdobe Research\ntensmeye@adobe.com\nJiuxiang Gu\nAdobe Research\njigu@adobe.com\nAni Nenkova\nAdobe Research\nnenkova@adobe.com\nAbstract\nVisual text evokes an image in a person’s mind,\nwhile non-visual text fails to do so. A method\nto automatically detect visualness in text will\nenable text-to-image retrieval and generation\nmodels to augment text with relevant images.\nThis is particularly challenging with long-form\ntext as text-to-image generation and retrieval\nmodels are often triggered for text that is de-\nsigned to be explicitly visual in nature, whereas\nlong-form text could contain many non-visual\nsentences. To this end, we curate a dataset\nof 3,620 English sentences and their visual-\nness scores provided by multiple human an-\nnotators. We also propose a fine-tuning strat-\negy that adapts large vision-language models\nlike CLIP by modifying the model’s contrastive\nlearning objective to map text identified as non-\nvisual to a common NULL image while match-\ning visual text to their corresponding images\nin the document. We evaluate the proposed ap-\nproach on its ability to (i) classify visual and\nnon-visual text accurately, and (ii) attend over\nwords that are identified as visual in psycholin-\nguistic studies. Empirical evaluation indicates\nthat our approach performs better than several\nheuristics and baseline models for the proposed\ntask. Furthermore, to highlight the importance\nof modeling the visualness of text, we conduct\nqualitative analyses of text-to-image generation\nsystems like DALL-E.\n1 Introduction\nPeople typically communicate knowledge and\ninformation textually, but most prefer to con-\nsume visually rich content. Text-to-image genera-\ntion/retrieval models could augment text with ap-\npropriate images, aiding the creation of appealing\nand easy-to-understand documents. Models like\nDALL-E (Ramesh et al., 2022) and Stable Diffu-\nsion (Rombach et al., 2022) work phenomenally\nwell for input text that is carefully constructed to\nelicit images. However, they cannot handle long-\nform text with a mix of sentences that may or may\nWhen t he gar dens open, just \naft er dawn, t he first t o appear \nar e t he joggers and t he silent \nfigur es per f orming t he \nintricat e maneuv ers of tai chi.\nIn case of y our f ailur e t o \nansw er , judgment will be \ntak en against y ou b y def ault \nf or t he r elief demanded in t he \ncomplaint.\nhuman-annotat ed \nvisualness scor e = 6 .44\nhuman-annotat ed \nvisualness scor e = 1 . 67\nCLIP model \nfine-tuned f or \nidentifying \nvisual t e xt\nt e xt input\n“visual t e xt”\n“non-visual t e xt”\nt e xt input\nt e xt -t o-image ( e.g., D ALL -E)\nOur contributions\nFigure 1: Overview of the sentence visualness identifi-\ncation task, along with a motivating downstream appli-\ncation (passive generation of relevant images).\nnot evoke a visual image. To this end, we introduce\nthe task of identifying sentence visualness—a term\nwe use interchangeably with imageability—as a\nnecessary first step toward connecting long-form\ntextual documents with relevant visual assets, with-\nout having to manually find visual sentences. In\nother words, to work effectively with long-form\ntext without relying on manual input, text-to-image\ngeneration models like Stable Diffusion, DALL-E,\nand Imagen (Saharia et al., 2022) would benefit\nfrom inferring text visualness before they can gen-\nerate images to embellish textual documents. In\nFigure 1, we demonstrate the need with some ex-\namples: text identified to have low visualness leads\nto irrelevant generations from DALL-E, while text\nidentified to have high visualness leads to the gen-\neration of relevant images.\nPrior approaches for quantifying the visu-\nalness of text operate on a word or phrase\nlevel (Deschacht and Moens, 2007; Jeong et al.,\n2012) and leverage lexicons that contain human-\nassigned world-level imageability scores (Louis\nand Nenkova, 2013). However, besides being lim-\nited in their coverage, our experiments also show\nthat word or phrase-level visualness cannot be ag-\ngregated to quantify sentence-level visualness.\nTo this end, in this work, we curate a corpus of\n3,260 sentences in English paired with their human\n2394\nratings for visualness, as well as a noisy but large\ncorpus of 48,077 automatic alignments between\ntext and visual assets in long-form documents. The\ntextual part of the resulting alignment pairs can\nbe used as examples of visual and non-visual sen-\ntences. We propose a strategy to fine-tune vision-\nlanguage models like CLIP, allowing classification\ninferences over text-only inputs. Our objective also\nensures that the learned embeddings remain usable\nfor downstream text-to-image retrieval.\nWe compare the performance of our proposed\napproach against several heuristic and model-based\nbaselines. Our extensive evaluation suggests that\nour fine-tuning strategy leads to the most accu-\nrate visual and non-visual text classifier. Finally,\nwe conduct several analyses to glean insights into\nthe model’s learned attention mechanism, text-to-\nimage retrieval abilities, and downstream text-to-\nimage generation capabilities.1\nIn sum, our key contributions are:\n•\nWe propose the task of identifying the visualness\nof a sentence and curate a dataset by crowdsourc-\ning annotations for English sentences.\n•\nWe develop a training objective that fine-tunes\nlarge vision-language models for the task of text\nvisualness identification.\n•Quantitative and qualitative experiments demon-\nstrate the effectiveness of our fine-tuning approach\nin identifying visual text over several competitive\nbaselines, while preserving downstream text-to-\nimage retrieval performance.\n2 Related Work\nFine-tuning vision-language models for down-\nstream tasks: Large vision-language models like\nCLIP (Radford et al., 2021), UNITER (Chen et al.,\n2020), and ALIGN (Jia et al., 2021) have demon-\nstrated remarkable performance on downstream\ntasks via transfer learning or fine-tuning. How-\never, such downstream tasks assume both text and\nimage as input to determine similarity or gener-\nate/retrieve the other modality for every instance\nof the corresponding modality; for instance, vi-\nsual question answering (Antol et al., 2015), cap-\ntion generation (Xu et al., 2015), and cross-modal\nretrieval (Wang et al., 2016). Fine-tuning large\nvision-language models on such downstream tasks\ninvolves adding components to the encoders’ ar-\nchitecture and training additional parameters on\n1Project webpage: https://gaurav22verma.github.\nio/text-visualness/\nthe task-specific dataset (Mittal et al., 2022; Sarto\net al., 2022). Our work differs from existing work\nin that the input is only text, requiring us to adapt\nlarge vision-language models to not rely on both\nmodalities during inference. We propose a fine-\ntuning strategy that does not involve additional ar-\nchitectural components (and parameters) on top of\na pre-trained CLIP architecture and yet effectively\nadapts CLIP for learning text visualness. Our task\ncan be considered a precursor to tasks like text-to-\nimage retrieval and generation, where images are\nonly retrieved or generated for visual text. Further,\nsince reusability of representation is a desirable\nproperty (Yosinski et al.,2014; Long et al., 2015)\nwe aim to preserve the reusability of text embed-\ndings learned for the visualness categorization task\nfor downstream tasks like text-to-image retrieval.\nVisualness of words: The visualness of text has\nbeen studied in multiple prior works but at a word\nor phrase level. Coltheart (1981) curated the MRC\nPsycholinguistic Database comprising human rat-\nings for imageability of 3769 words, which were\nlater expanded using automated methods by Louis\nand Nenkova (2013). Beyond word-level visual-\nness, some studies have focused on automated\nquantification of phrase-level visualness (Jeong\net al., 2012; Deschacht and Moens, 2007). Our\nwork focuses on learning sentence-level visualness\ninstead of word or phrase-level visualness. While it\nis possible to aggregate word-level and phrase-level\nvisualness scores to obtain sentence-level scores,\nit is unclear how accurate and generalizable these\ntechniques are. We design multiple baselines that\naggregate word-level scores to obtain sentence-\nlevel visualness and contrast the performance of\nsuch approaches with our proposed approach.\n3 Text Imageability Dataset (TImeD)\nOur proposed fine-tuning approach follows multi-\nstage training of a large vision-language model\nCLIP (Radford et al., 2021). In the first stage, we\nconduct large-scale fine-tuning, followed by fine-\ntuning on a relatively smaller annotated corpus in\nthe second stage. We first discuss the curation of\na large-scale corpus that comprises automatically-\nassigned and distant labels and then describe the\ncuration of the human-labeled corpus of visual &\nnon-visual sentences.\n3.1 Fine-tuning with automatic labels\nThe formulation of the training objective (discussed\nlater) requires positive examples comprising vi-\n2395\nCategory\nExample text from TIMED µ/ σ\nV\nisual\n·now the snow has melted and the grass not only looks dreary, but it is soggy. µ= 6.88\n·The operation left a six-inch zipper scar on his chest. µ= 6.55\n·When the gardens open, just after dawn, the first to appear are the joggers and the silent figures performing the intricate maneuvers of tai chi. µ= 6.44\n·He removed the box, placed it next to the garbage can, and put his garbage inside the can. µ= 5.88\n·But, after running only the first 500 meters, he realized that the injury that seemed so insignificant would not only prevent him from winning the race,\nbut also from finishing it.\nµ= 5.00\nNon-visual\n·There’\ns only one way to prove them wrong. µ= 1.22\n·For more information or to schedule an outreach, please call (999) 123-4567 or email email@website.com. µ= 1.55\n·In case of your failure to answer, judgment will be taken against you by default for the relief demanded in the complaint. µ= 1.67\n·A 25% quorum of member votes in each district is needed to conduct district delegate elections in October. µ= 1.77\n·Colliers International makes no guarantees, representations or warranties of any kind, expressed or implied, regarding the information including, but\nnot limited to, warranties of content, accuracy and reliability.\nµ= 2.00\nAmbiguous\n·J. Roman discusses his book Ohio State Football: The Forgotten Dawn which draws on extensive archival research to tell the untold story of the early\ndays of football at Ohio as flagship public university.\nσ= 2.34\n·Remember to be sure to set your clocks back 1 hour before you go to bed on Saturday, November 3rd. σ= 2.23\n·That is the most important thing in my life today: Jesus. σ= 2.20\n·Children & parents will get to hear author George McClements read his book Ridin’ Dinos with Buck Bronco. σ= 2.14\n·Financial Peace University is a nine-lesson class taught by financial expert Dave Ramsey through entertaining videos with an in-depth workbook, that\nwill teach you how to take control of your money.\nσ= 2.16\nTable 1: Qualitative examples of visual and non-visual text from the human-annotated subset of theText Imageability\nDataset (based on the average of annotator ratings), and text with high ambiguity (based on the standard deviation\nof annotator ratings).\nsual text and paired images as well as negative\nexamples that comprise non-visual text. To cre-\nate a corpus like this, we: (i) leverage image-text\nco-occurrences in documents to develop a self-\nsupervised approach, and (ii) use image-text simi-\nlarity scores obtained using CLIP as priors to con-\nstruct a large training corpus. We start with 450,000\npublicly available PDFs referenced in the Common\nCrawl corpus and identify pages within those PDFs\nthat include images. 2 We use a proprietary doc-\nument object detection tool like Fitz 3 to extract\nparagraphs and images from the document pages.\nWe do sentence segmentation for the identified\nparagraphs using NLTK Tokenizer (Loper and\nBird, 2002). To map the images in the page to\nsentences, we compute CLIP similarity scores be-\ntween each image-sentence pair in a given page.\nBased on the distribution of image-sentence simi-\nlarity scores across all the pages in our corpus, we\nset two thresholds, Tpos and Tneg. A sentence in a\npage is considered a positive example (visual text)\nif its similarity with any of the images in the page\nis greater than Tpos. Similarly, chosen negative ex-\namples have similarity values less than Tneg with\nall images within the same page. Sentences with\nan image similarity value greater than Tpos are as-\nsociated with the most similar image in the same\npage, while the negative examples are associated\n2We choose to work with PDF documents rather than web-\npages because (i) PDFs have natural demarcations in the form\nof pages (whereas webpages often contain long-running text\nwith complex image-text interactions), and (ii) images within\na page are likely to be related to selected text fragments within\nthe same page.\n3https://github.com/pymupdf/PyMuPDF\nwith a common NULL image. The thresholds Tpos\nand Tneg are chosen conservatively to only include\ntop or bottom k% sentences from the entire corpus,\nrespectively. This limits the noise in our training\ncorpus for adapting the CLIP model for scoring\ntext visualness. In our experiments, we set Tpos to\nbe 0.35 to consider top 1% sentences as visual and\nTneg to be 0.18 to consider bottom 5% sentences as\nnon-visual. Our automatically-labeled corpus com-\nprises 15,359 visual sentences, the corresponding\nimages, and 32,718 non-visual sentences.\n3.2 Human-annotated dataset\nFor the human-annotated visual and non-visual ex-\namples, we start with another 200,000 PDFs dis-\ntinct from those used for the automated assignment\nof labels. To focus on natural images rather than in-\nfographics and academic figures, we filtered these\ndocuments to only include brochures, flyers, and\nmagazines. For the resulting 35,432 documents,\nwe adopted the same policy as that for curating\nthe automatically-labeled dataset (selecting top 1%\nand bottom 5% sentences based on similarity val-\nues). We then recruited annotators to rate the visual-\nness of the resulting 3,620 sentences after manually\nanonymizing any personal information.\nWe recruited annotators on Amazon Mechani-\ncal Turk (AMT). We randomly ordered the 3,620\nexamples and, for each example, we asked nine an-\nnotators to provide a response on a 7-point Likert\nscale for the following question: “Do you agree\nthat the sentence below evokes an image or picture\nin your mind?” A response of1 indicated strong\ndisagreement, while 7 indicated strong agreement.\n2396\nWe also inserted some attention-check examples\n(5%; n = 181) to ensure the annotators read the\ntext carefully before responding. These checks ex-\nplicitly asked the annotators to mark a randomly\nchosen score on the Likert scale regardless of the\nactual content. We discarded the annotations from\nannotators who did not correctly respond to all the\nattention-check examples and re-collected more re-\nsponses iteratively. Appendix A.3 provides more\ndetails about the filters used for recruiting the an-\nnotators and the annotation interface.\nIf a majority of annotations (i.e., at least 5 out of\n9) were 1, 2, or 3, we considered the example to\nbe non-visual (n= 2108). Similarly, visual ex-\namples had a majority of 5, 6, or 7 responses (n=\n1132). We considered examples that did not have\na clear majority or majority of responses of 4 (i.e.,\n‘Neutral’ on the Likert scale) as ambiguous and\nneutral, respectively. Table 1 shows illustrative\nexamples of visual, non-visual, and ambiguous\ntext from our human-annotated corpus.\nFor 27.1% of the examples only at most 1 of\nthe 9 annotators disagreed with the labels decided\nbased on the process described above. 10.5% of\nthe sentences were assigned a neutral or ambigu-\nous class. Inter-annotator agreement measured by\nKrippendorff’s αwas 0.446. This inter-annotator\nagreement value is in a similar range to what is\nobserved for other language-related tasks that in-\nvolve assessment of text by experts on dimensions\nlike coherence, likability, relevance, and even gram-\nmar (Karpinska et al., 2021). For brevity, we refer\nto the curated dataset as TI MED, short for Text\nImageability Dataset.\n4 TIP-CLIP for Scoring Text Visualness\nBackground: The CLIP model (Radford et al.,\n2021) jointly trains image and text encoders to pre-\ndict the correct pairing between images and textual\ndescriptions. In a batch size of N images and N\ntexts (N2 possible image-text pairings), the objec-\ntive function ensures that the cosine similarity be-\ntween the embeddings of correct image-text pairs is\nmaximized while the cosine similarity between the\n(N2 −N) incorrect image-text pairs is minimized.\nThe encoders are trained over a large multimodal\ndataset of ∼400 million image-text pairs.\nUpdated training objective: When predicting text\nvisualness, the goal is to assign a higher score to\ntext that is visual (evokes a concrete image for the\nperson reading it) and a lower score for non-visual\nt he common loon, minnesota ‘s stat e \nbir d, usually nest s on islands or on \nshor e lines of our nor t hern lak es\ncolliers Int ernational mak es no \nguarant ees, r epr esentations or \nwarranties of an y kind, e xpr essed or \nimplied, r egar ding t he inf o...\nt her e’ s only one wa y t o pr o v e t hem \nwr ong.\njim sent along t he f ollo wing images \nof t hese successful anglers and \none of r ed drum t he y caught.\nT e xt encoder\nImage encoder\nT1\n. T1\nT2 T NT N - 1\nI1 I1 . T2I1 . T N - 1I1 . T NI1\n. T1Inull . T2Inull . T N - 1Inull . T NInull\n. T1I N - 1 . T2I N - 1 . T N - 1I N - 1 . T NI N - 1\n. T1Inull . T2Inull . T N - 1Inull . T NInull\nInull\nvisual t e xt\nvisual t e xt\nnon-visual t e xt\nnon-visual t e xt\nInull\nI N - 1\nNULL image\nNULL image\nFigure 2: Our approach to predicting sentence visual-\nness, with a fine-tuning strategy where visual text is\nmatched with its corresponding image while non-visual\ntext is matched with a fixed NULL image.\ntext (text that does not evoke an image). In line\nwith the original training objective, we further train\nthe CLIP model to match text that is identified as\nvisual with the corresponding image. We adapt\nthe CLIP training to match text that is identified\nas non-visual with a single NULL image (see Fig.\n2). Matching visual text with the corresponding\nimage while non-visual text to a NULL image not\nonly encourages the model to distinguish between\nvisual and non-visual text, but also allows it to\nanchor non-visual text in the common NULL image\nthat can be used during inference without having\naccess to a potentially paired image. Formally, the\nadapted training objective is given as,\nL= − 1\n2N\nN∑\nj=1\nlog\n(\nexp(⟨Ie\nj ,Te\nj ⟩/τ)\n∑N\nk=1 exp(⟨Ie\nj ,Te\nk ⟩/τ)\n)\n−\n1\n2N\nN∑\nk=1\nlog\n(\nexp(⟨Ie\nk,Te\nk ⟩/τ)∑N\nj=1 exp(⟨Ie\nj ,Te\nk ⟩/τ)\n)\nst. Ie\nm =\n{\nIe\nnull, if m∈¯V (i.e., non-visual)\nIe\nm, if m∈V (i.e., visual).\n(1)\nHere, N denotes the number of examples in\na batch, Ie\nm and Te\nm denote the embeddings of\nthe m-th pair of image and text that are normal-\nized to have unit ℓ2-norm, respectively, such that\nm ∈{1,...,N }. ⟨...⟩represents the inner prod-\nuct, and τ is the trainable temperature parameter. ¯V\nand Vare the set of examples in the current batch\n2397\nthat belong to non-visual and visual categories, re-\nspectively. Finally, Ie\nnull denotes the embedding\nof the NULL image. During inference, we compute\nthe cosine similarity between the representation of\na given text with the representation of the NULL\nimage; non-visual texts will have a high similarity\nwith the NULL image. Conversely, the visualness\nscore Sof any text with embedding Te can be ob-\ntained using\nS= 1 −⟨Ie\nNULL,Te⟩. (2)\nFor the NULL image, we create an RGB image\nof size (224,224,3) in which each pixel value is\nchosen randomly (see Figure 2). However, experi-\nments with different types of NULL images indicate\nthat the choice of null image does not affect the\nmodel’s performance; see Appendix A.1.\nAn alternative formulation for adapting the CLIP\ntraining objective could have been to match visual\ntext with a single image while matching non-visual\ntext with a singleNULL image. However, this formu-\nlation of the training objective is similar to binary\nclassification and does not enforce a contrastive\nobjective for the positive examples. Matching vi-\nsual text with its corresponding image instead of a\ncommon image for all visual text affords text em-\nbeddings that can be used for downstream tasks\nlike text-to-image retrieval; we provide empirical\nevidence for worse text-to-image retrieval perfor-\nmance with the alternative formulation in Results.\n5 Training details and Baselines\nTrain, test, & validation splits: Recall that our\nfine-tuning approach requires paired images for vi-\nsual sentences only during training time and not\nduring inference time; the model needs only text\nas input during inference. Of the 1132 visual sen-\ntences in the human-annotated set of TIMED, we\nassign 515 examples that had an automatically de-\ntermined corresponding image to the training set,\nand the remaining were randomly assigned to the\ntest set ( n = 517) and validation set ( n = 100).\nThe 2108 non-visual sentences were randomly split\ninto the training ( n = 980 ), test ( n = 928 ),\nand validation set ( 200). All three sets maintain\npositive:negative class ratio of ∼0.5.\nFor the first stage of training, we fine-tune the\nCLIP model (ViT/B-32) on the proposed objec-\ntive (see Eq. 1) using the 48,077 examples with\nautomatic labels. This training is done on Tesla\nT4 GPUs, for 5 epochs, and a learning rate ini-\ntialized at 5 ×10−5 and optimized using Adam\noptimizer (Kingma and Ba, 2014). Following\nthis, for the second stage, we further fine-tune\nthe same model for 2 epochs using the same ob-\njective and hyper-parameters, but this time using\nthe train set of human-annotated TI MED.\n4 The\nhyper-parameters are selected by performing a grid\nsearch while observing performance on the vali-\ndation set of TI MED. Based on the performance\non the validation set of TIMED, we set the thresh-\nold of S(Eq. 2) to be 0.79 to categorize text as\nvisual or non-visual. We refer to the model\ntrained using our fine-tuning strategy as TIP-CLIP\n— Text Imageability Predictor CLIP, and report\nperformance on the test set of TIMED.\n5.1 Baselines\nWe investigate the performance of TIP-CLIP\nagainst several heuristics and baseline models.\nRandom: The random baseline generates predic-\ntions via prior class probabilities in the training set.\nAverage MRC-I score: We consider the image-\nability scores of 3,769 words in the MRC lexicon\nand normalize them to be ∈[0,1]. For each exam-\nple, we take the average of the imageability scores\nof the unique words; out-of-vocabulary words are\nassigned a score of 0. We lowercase the words in\nthe MRC lexicon as well as the input text. Based\non this average score, we categorize an example\nas\nvisual or non-visual by setting the decision\nboundary as 0.17. The threshold is chosen to opti-\nmize performance on the validation set of TIMED.\nConcentration of Visual Genome objects (VG-\nObjects): The Visual Genome dataset comprises\n75,729 objects, along with annotations for their at-\ntributes and object-object relations (Krishna et al.,\n2017). Based on the heuristic that a mention of a\nvisual object in the text can trigger imageability,\nwe quantify the concentration of Visual Genome\nobjects by computing the fraction of unique object\nmentions in tokenized text with respect to the num-\nber of total unique words within the input text. We\nset the threshold to 0.5 based on the performance\non the validation set.\nExpanding the MRC lexicon using word embed-\ndings: The coverage of the MRC lexicon is poor be-\ncause it contains only 3,769 words. We expand this\n4The CLIP model has a maximum context length of 77\ntokens (about 50 words). Fewer than 1% of the training exam-\nples are truncated to fit this context length.\n2398\nlist using semantic similarity between distributed\nrepresentations of words (300-dim word2vec vec-\ntors trained on Google News corpus). For each\nword win the word2vec (Mikolov et al., 2013) vo-\ncabulary of pre-trained representations that does\nnot occur in the MRC lexicon, we compute its co-\nsine similarities with all the words in the MRC lex-\nicon to identify the most semantically similar word\nthat exists in MRC, given by wMRC and its similar-\nity with wgiven as (simmax). We assign the word\nwan imageability score of simmax ×scorewMRC ,\nwhere scorewMRC is the normalized imageability\nscore of w’s most similar word wMRC. Based on\nthe performance on the validation set, the decision\nboundary for average imageability score of input\ntext is set as 0.17. This baseline propagation ap-\nproach is highly effective in quantifying word-level\nimageability as the Pearson’s correlation coefficient\nbetween the assigned visualness score and the aver-\nage AMT rating of humans is 0.735 (p< 0.001);\nsee Appendix A.2 for details.\nFine-tuned BERT classifier: We fine-tune a BERT\nmodel (\nbert-base-uncased on HuggingFace (De-\nvlin et al., 2018; Wolf et al., 2020)) for the clas-\nsification task of visual versus non-visual text\ndetection. Similar to our proposed model, we adopt\na two-stage fine-tuning approach with the BERT\nclassifier (adding a classification layer to BERT for\nthe first input token’s ([CLS]) representation). We\nfirst fine-tune the model using the automatically\nlabeled dataset followed by fine-tuning on the train-\ning set of the human-curated TIMED. For the first\nstage, we fine-tune the model for 7 epochs with a\nlearning rate initialized at 5 ×10−5 using a batch\nsize of 32 while setting other hyper-parameters to\ndefault. We fine-tune the model for 3 epochs for\nthe second stage with the same hyperparameters.\nPre-trained CLIP model: We use the pre-trained\nCLIP model (ViT/B-32) to obtain similarity scores\nbetween the embeddings of the NULL image (used\nfor the fine-tuning of our model) and the input text.\nWe then use 1 −⟨Ie\nNULL,Te⟩as an estimate of the\nvisual score of text (see Eq. 2). Based on the per-\nformance on the TIMED validation set, we set the\nthreshold for Sto be 0.83.\n6 Results and Analyses\nEvaluation on held-out test set of TIMED: We\nfirst evaluate the baselines and our approach on\nthe test set of the human-annotated TIMED, com-\nputing macro-averaged F1, precision, recall scores,\nMO\nDELS F1 ↑ PRECISION ↑ RECALL ↑ ACC. ↑\nRandom\n0.531 0.531 0.531 0.577\nMRC-I\n0.584 0.599 0.583 0.644\nVG-Objects 0.606 0.610 0.605 0.646\nMRC-I\n+ w2v 0.638 0.637 0.639 0.667\nBER\nT 0.753 0.766 0.789 0.756\nCLIP\n0.694 0.695 0.701 0.712\nTIP-CLIP (Ours) 0.865 0.858 0.873 0.871\nTable 2: Evaluation on human-annotated test set of\nTIMED. Reported F1, Precision, and Recall values are\nmacro-averages across the two classes ( visual and\nnon-visual).\nand classification accuracy. Table 2 show the re-\nsults for this evaluation. We observe that our pro-\nposed two-stage fine-tuning strategy leads to the\nbest-performing model (TIP-CLIP). In comparison,\nthe pre-trained CLIP model demonstrates notably\nweaker performance on the task of distinguishing\nvisual text from non-visual text. Interestingly, fine-\ntuned BERT performs reasonably well on the task,\nconsiderably better than the CLIP model. Using the\naverage imageability scores from MRC provides\nbetter-than-random performance but is severely\nsubpar to models like CLIP, BERT, and TIP-CLIP.\nUsing word2vec embeddings to expand the cover-\nage of the MRC lexicon (i.e., MRC-I + w2v) leads\nto a boost in performance. However, collectively,\nthe lacking performance of MRC-I and MRC-I\n+ w2v demonstrates that word-level imageability\ndoes not translate to sentence-level imageability\nto a great extent. Notably, in terms of baselines\nthat aggregate word-level attributes, VG-Objects\nprovides the best estimate of sentence-level image-\nability by quantifying the concentrations of visual\nobjects in the input sentence.\nCorrelation of attention Weights with MRC im-\nageability scores: Attention mechanisms could be\ntaken as proxies for explainability (Wiegreffe and\nPinter, 2019; Chefer et al., 2021). Since the fine-\ntuned BERT, pre-trained CLIP, and our TIP-CLIP\nare attention-based models, we compute the corre-\nlation between average word-level attention scores\n(obtained from the last layer) on a given dataset\nwith the imageability scores assigned by humans\nin the MRC lexicon. We compute these values for\ntwo datasets—the MSCOCO dataset (Vinyals et al.,\n2016) and the test set of TI MED. We only consider\nwords that occur more than once in the specific cor-\npus. Table 3 shows that TIP-CLIP attention scores\ncorrelate the most with MRC imageability scores,\n2399\nMO\nDELS MSCOCO TI MED\nBER\nT 0.461*** (n = 344) 0.326*** (n = 294)\nCLIP 0.448*** (n = 344) 0.283*** (n = 294)\nTIP-CLIP (Ours) 0.497*** (n = 344) 0.367*** (n = 294)\nTable 3: Correlation between MRC Imageability scores\nand model attention-scores for BERT, CLIP, and TIP-\nCLIP.ndenotes the number of overlapping words across\nvocabularies; *** denotes p <10−3.\nMO\nDELS F1 ↑ PRECISION ↑ RECALL ↑ ACC. ↑\nBER\nT (auto-labeled) 0.714 0.704 0.716 0.710\nBERT (human-labeled) 0.753 0.766 0.789 0.756\nBERT (auto + human-labeled) 0.774 0.783 0.797 0.771\nCLIP\n0.694 0.695 0.701 0.712\nTIP-CLIP (auto-labeled) 0.751 0.763 0.791 0.748\nTIP-CLIP (human-labeled) 0.810 0.807 0.815 0.820\nTIP-CLIP (auto + human-labeled) 0.865 0.858 0.873 0.871\nTable 4: Ablation studies to understand the benefits of\ntwo-stage fine-tuning. The presented results are on the\nhuman-annotated test set of TI MED. Reported values\nare macro-averages of class-wise F1, precision, and\nrecall, and overall classification accuracy.\nfollowed by the fine-tuned BERT’s attention scores.\nThe trends are consistent across both datasets. The\nrelative ordering of models in terms of the corre-\nlation of their attention scores with MRC image-\nability scores follows the same order as their per-\nformance on the test set of TI MED. However, all\ncorrelation scores are in the low range, indicating\na non-trivial relationship between sentence- and\nword-level imageability. The same trends hold for\npropagated visualness scores; see App. A.4. We\nalso analyze the reason behind higher correlation\nscores on MSCOCO with respect to the TI MED\ncorpus in Appendix A.4.\nEffect of multi-stage training: We conduct abla-\ntions to isolate the effect of two-stage training. In\nTable 4, we show that BERT and TIP-CLIP can\nlearn to distinguish visual and non-visual text\neven when fine-tuned only using the automatically\nlabeled data. However, for both models, the gains\nfrom fine-tuning only on smaller, human-labeled\ndata are notably higher. Furthermore, we find\nthe proposed two-stage fine-tuning (i.e., training\non automatically labeled data followed by human-\nlabeled data) to be most effective, leading to a gain\nof over 2 and 5 absolute F1 points over training\nonly on human-labeled data for BERT and TIP-\nCLIP models, respectively. Additionally, for a\ngiven training strategy, our proposed fine-tuning\nof TIP-CLIP demonstrates better performance than\nthe corresponding fine-tuned BERT model as well\nas the standard pre-trained CLIP model.\nEffect on text-to-image retrieval: We aim to ana-\nlyze the re-usability of learned embeddings by the\nTIP-CLIP model for the text-to-image retrieval task.\nTo this end, we consider the 515 visual examples\nfrom the test set of TI MED and, for each visual\nexample, we rank the 515 corresponding images\nbased on the cosine similarity between the image\nand text embeddings obtained from the TIP-CLIP\nmodel. We compute the Mean Reciprocal Rank\n(MRR) and contrast it with the MRR obtained us-\ning the pre-trained CLIP embeddings. As expected,\nCLIP achieves a near-perfect MRR of 0.989. The\nproposed fine-tuning objective does not severely\nimpact the reusability of embeddings obtained from\nTIP-CLIP for retrieval, and results in an MRR of\n0.937. This comparison evaluates the retrieval ca-\npabilities of TIP-CLIP against that of the CLIP\nmodel because the correspondence between visual\ntext and images was established using similarities\nbetween CLIP embeddings.5\nThe downside of an alternate training objec-\ntive: Recall that our fine-tuning strategy involves\nmatching visual text with its corresponding im-\nage and matching non-visual text with the NULL\nimage. With only the classification of visual and\nnon-visual text in mind, an alternate fine-tuning\nstrategy would have been to match all the visual\nexamples with one common image while match-\ning all the non-visual text with the common NULL\nimage. The major downside of this approach is\nthat while it leads to an effective classifier after\ntwo-stage fine-tuning, demonstrating a compara-\nble\nF1 score of 0.842 as the TIP-CLIP model, it\nperforms poorly on the text-to-image retrieval task\nwith an MRR of 0.014. Overall, while the alternate\nentirely classification-based training objective per-\nforms at par with the proposed TIP-CLIP model\non the classification task, the resultant embeddings\ndemonstrate poor reusability for downstream tasks\nlike text-to-image retrieval.\nProperties of the new embedding space: In\nFigure 3 we visualize the embedding space of\nthe learned embeddings using t-SNE (Van der\nMaaten and Hinton, 2008). Alongside visual and\nnon-visual sentences from the test set of TIMED,\n5To automatically establish a correspondence between\nvisual text and images, we enforce that the most similar\nimage for a text should exist on the same page of the PDF.\nTherefore, it is possible that the CLIP similarity of text may\nbe higher for a different image, resulting in an MRR slightly\nless than 1.0 (i.e., 0.989).\n2400\nNULL image\nImages for \nvisual text\nNon-visual text\nVisual text\n(a) CLIP embeddings\nNULL imageImages for \nvisual text\nNon-visual text\nVisual text (b) TIP-CLIP embeddings\nNULL image 1\nNULL image 2\nImages for \nvisual text\nNon-visual text\nVisual text (c) Alt. formulation embeddings\nFigure 3: t-SNE visualization of embeddings learned by (a) CLIP, (b) TIP-CLIP — using contrastive and adapted\ncontrastive learning objective, respectively, & (c) model trained using alternative formulation solely focusing on\nclassification. The plotted data points are from the TI MED test set. The observed “gap” in image & text spaces has\nbeen studided by Liang et al. (2022).\nOriginal imageInput t e xt\nCLIP : I’ll be or dering our christmas plant s t hat ar e in\n 6 1 / 2 pot s at a price of $ 5 . 0 0 each . \nTIP -CLIP : I’ll be or dering our christmas plant s t hat ar e in\n 6 1 / 2 pot s at a price of $ 5 . 0 0 each .\nCLIP : t he common loon , minnesota ‘s stat e bir d , usually nest s\non islands or on shor e lines of our nor t hern lak es\nTIP -CLIP : t he common loom , minnesota ‘s stat e bir d , usually nest s\non islands or on shor e lines of our nor t hern lak es .\nCLIP : jim sent along t he f ollo wing images of t hese successful\nanglers and one of r ed drum t he y caught .\nTIP -CLIP : jim sent along t he f ollo wing images of t hese successful\nanglers and one of r ed drum t he y caught .\nCLIP att ention TIP -CLIP att ention Original imageInput t e xt CLIP att ention TIP -CLIP att ention\nCLIP : dog - friendly pubs ar e a k e y ingr edient of t he charm and \nuni que atmospher e in man y places in nsw .\nTIP -CLIP : dog - friendly pubs ar e a k e y ingr edient of t he charm and\nuni que atmospher e in man y places in nsw .\nFigure 4: Comparing the attention maps over input text and images for CLIP and TIP-CLIP. For text, a darker shade\nof green demonstrates greater attention by the model. For images, red demonstrates the greatest attention in the\nheatmap. Image best viewed with zoom.\nFigure 5: Examples of DALL-E generations for non-visual and visual text.\nwe also plot the embeddings of images correspond-\ning to the visual sentences, and the embedding(s)\nof the NULL image(s). First off, we observe that the\nembeddings in Figure 3a and 3b from CLIP and\nTIP-CLIP are different in that the TIP-CLIP em-\nbeddings demonstrate better distinguishability be-\ntween visual and non-visual text. In Figure 3c\nwe observe that the alternative formulation pushes\nthe NULL embeddings to the periphery of the image\nembeddings’ cluster from a near-center location in\nFigures 3a and 3b. The text embeddings demon-\nstrate notable distinguishability in Figure 3c too.\nWe believe that the alternative classification-only\nformulation causes distortion in the latent space\nthat causes drastic modification of text-only embed-\ndings, making them useless for downstream text-to-\nimage retrieval, as demonstrated empirically earlier.\nHowever, our proposed objective in TIP-CLIP pre-\nserves reusability for downstream tasks by main-\ntaining semantic relevance between learned image\nand text embeddings.\n6.1 Qualitative Analysis\nIn this section, we conduct two qualitative analy-\nses: (i) contrasting the attention mechanisms for\nCLIP and TIP-CLIP, and(ii) the role of distinguish-\ning visual and non-visual text in downstream\ntext-to-image generation using systems like DALL-\nE (Ramesh et al., 2021).\nAttention map visualization: To contrast the\nmechanism by which CLIP and TIP-CLIP mod-\nels match input text with their corresponding im-\nage, we visualize and contrast the attention maps\nfor both models. We adopt the state-of-the-art ap-\nproach to explain multimodal Transformers (Chefer\n2401\net al., 2021). In Fig. 4 we show 4 illustrative\nvisual sentences from the test set of TIMED along\nwith their corresponding images. Focusing on text,\nwe observe that TIP-CLIP has a greater tendency\nto attend to visual aspects in the text; for instance,\nwords like ‘islands,’ ‘lakes,’ ‘anglers’ are attended\nto a greater extent by TIP-CLIP than CLIP. In im-\nages, we observe small changes in attention maps\nacross CLIP and TIP-CLIP; for instance, while the\nCLIP attention is focused on the Common Loon,\nTIP-CLIP also attends to the ‘lake.’ The qualitative\nanalysis of visualization maps reinforces that the\nmatching process for text and images undergoes\nsmall changes to accommodate greater attention to\nvisual aspects in the text.\nDownstream text-to-image generation: In Fig.\n5 we show the generations obtained using DALL-\nE for text that is categorized as non-visual and\nvisual in our dataset. We observe that for\nnon-visual text, the images produced by DALL-\nE show poor relevance to the text. However, for\nvisual text the generated images demonstrate\ngreat relevance to the input text.\nTriggering text-to-image generation models like\nDALL-E for visual text is crucial to effectively use\nsuch systems in a passive setting. For instance,\nthe authors should only be recommended to add\nvisual assets in relevant places (i.e., for visual sen-\ntences) while working with long-form documents;\ntriggering image generations for non-visual sen-\ntences could cause sub-optimal experiences. Thus,\nour contributions focus on distinguishing visual\ntext from non-visual text as the necessary first step.\n7 Conclusion and Future Work\nWe propose the task of predicting the visualness\nof text and curate a human-annotated dataset of\nsentence-level visualness scores. Additionally, we\npropose a two-stage fine-tuning objective for the\ntask that involves training on a distantly supervised\ncorpus followed by a smaller human-annotated cor-\npus. Comparisons with several baselines demon-\nstrate the effectiveness of our approach in distin-\nguishing visual and non-visual text. We analyze the\nattention weights and downstream text-to-image re-\ntrieval capabilities of the model. Qualitative analy-\nsis of attention weights over textual input reinforces\nthat our model attends to visual words to a greater\nextent. In closing, we show qualitative examples\nof how predicting text visualness can make text-to-\nimage generation more effective.\nIn the future, we will study alternate objectives\nfor learning text visualness while ensuring that the\nlearned representations are transferable to related\ndownstream tasks. We are also interested in us-\ning measures relating to the quality of the images\ngenerated from text-to-image generation systems\nto decipher signals about the visualness of input\ntext, enabling the creation of auto-labeled exam-\nples. As the aggregation of word-level visualness\nscores leads to poor predictability of sentence-level\nvisualness, future work could aim to understand\nwhat linguistic factors (like compositionality) pre-\ncipitate sentence-level visualness.\n8 Limitations and Broader Perspective\nLimitations: As the first study on predicting\nsentence-level visualness, we focus on fine-tuning\nrepresentative vision-and-language (CLIP) and\nlanguage-only (BERT) encoders. Future studies\ncan extend our experiments to explore the bene-\nfits of using other encoders to model text visual-\nness. Our curated TI MED dataset only covers the\nEnglish language. The notion of visualness can\nvary across languages and we encourage future re-\nsearch to contrast visualness in the context of the\nEnglish language with that in other non-English\nlanguages. Additionally, since US-based crowd\nworkers provided our ground-truth annotations for\nvisualness, the dataset reflects a predominantly\nWestern-centric view of text visualness. It is un-\nclear how visualness in the text is perceived across\ndifferent cultures. To this end, we acknowledge that\nour work and artifacts reflect West-centric views of\nvisualness in the English language and encourage\ncross-lingual and cross-cultural extensions.\nBroader Social Impact, Annotations, and\nDatasets: The authors do not foresee any nega-\ntive social impacts of this work. However, our\nmodel can inherit the known biases in underly-\ning models like CLIP and BERT (Agarwal et al.,\n2021; Garimella et al., 2021). The documents from\nwhich our datasets are curated are publicly avail-\nable and are mentioned in The Common Crawl\ncorpus (\nhttps://commoncrawl.org/); we abide\nby their terms of use. We manually anonymize\ninstances of PII in the sentences that are annotated\nusing Amazon Mechanical Turk and check for po-\ntentially offensive content. The recruited annota-\ntors are from the United States and are paid at an\nhourly rate of 12 USD.\n2402\nReferences\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec\nRadford, Jong Wook Kim, and Miles Brundage.\n2021. Evaluating CLIP: Towards Characterization of\nBroader Capabilities and Downstream Implications.\narXiv preprint arXiv:2108.02818.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. VQA: Visual question answering.\nIn Proceedings of the IEEE International Conference\non Computer Vision, pages 2425–2433.\nHila Chefer, Shir Gur, and Lior Wolf. 2021. Generic\nattention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 397–406.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104–120. Springer.\nMax Coltheart. 1981. The MRC psycholinguistic\ndatabase. The Quarterly Journal of Experimental\nPsychology Section A, 33(4):497–505.\nKoen Deschacht and Marie Francine Moens. 2007. Text\nanalysis for automatic image annotation. In Proceed-\nings of the 45th Annual Meeting of the Association\nof Computational Linguistics, pages 1000–1007.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nAparna Garimella, Akhash Amarnath, Kiran Kumar,\nAkash Pramod Yalla, N Anandhavelu, Niyati Chhaya,\nand Balaji Vasan Srinivasan. 2021. He is very intel-\nligent, she is very beautiful? on mitigating social\nbiases in language modelling and generation. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4534–4545.\nJin-Woo Jeong, Xin-Jing Wang, and Dong-Ho Lee.\n2012. Towards measuring the visualness of a concept.\nIn Proceedings of the 21st ACM International Con-\nference on Information and Knowledge Management,\npages 2415–2418.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using mechanical turk to evaluate\nopen-ended text generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1265–1285.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision, 123:32–\n73.\nWeibin Li, Qiwei Zhong, Qingyang Zhao, Hongchun\nZhang, and Xiaonan Meng. 2021. Multimodal\nand contrastive learning for click fraud detection.\narXiv:2105.03567.\nVictor Weixin Liang, Yuhui Zhang, Yongchan Kwon,\nSerena Yeung, and James Y Zou. 2022. Mind the gap:\nUnderstanding the modality gap in multi-modal con-\ntrastive representation learning. Advances in Neural\nInformation Processing Systems, 35:17612–17625.\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael\nJordan. 2015. Learning transferable features with\ndeep adaptation networks. In International Confer-\nence on Machine Learning, pages 97–105. PMLR.\nEdward Loper and Steven Bird. 2002. NLTK: the natu-\nral language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Compu-\ntational Linguistics-Volume 1, pages 63–70.\nAnnie Louis and Ani Nenkova. 2013. What makes\nwriting great? first experiments on article quality\nprediction in the science journalism domain. Trans-\nactions of the Association for Computational Linguis-\ntics, 1:341–352.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in Neural Information Processing Systems,\n26.\nAnshul Mittal, Kunal Dahiya, Shreya Malani, Janani\nRamaswamy, Seba Kuruvilla, Jitendra Ajmera, Keng-\nhao Chang, Sumeet Agarwal, Purushottam Kar, and\nManik Varma. 2022. Multi-modal extreme classifi-\ncation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n12393–12402.\nAllan Paivio, John C Yuille, and Stephen A Madigan.\n1968. Concreteness, imagery, and meaningfulness\nvalues for 925 nouns. Journal of Experimental Psy-\nchology, 76(1p2):1.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1532–1543.\n2403\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with clip latents. arXiv\npreprint: 2204.06125.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821–8831. PMLR.\nShivaen Ramshetty, Gaurav Verma, and Srijan Kumar.\n2023. Cross-modal attribute insertions for assess-\ning the robustness of vision-and-language learning.\narXiv preprint arXiv:2306.11065.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n10684–10695.\nChitwan Saharia, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily L Denton, Kam-\nyar Ghasemipour, Raphael Gontijo Lopes, Burcu\nKaragol Ayan, Tim Salimans, et al. 2022. Photo-\nrealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 35:36479–36494.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\nCucchiara. 2022. Retrieval-augmented transformer\nfor image captioning. In Proceedings of the 19th\nInternational Conference on Content-based Multime-\ndia Indexing, pages 1–7.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2443–2449.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9(11).\nGaurav Verma, Vishwa Vinay, Ryan Rossi, and Sri-\njan Kumar. 2022. Robustness of fusion-based mul-\ntimodal classifiers to cross-modal content dilutions.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n360–374.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. 2016. Show and tell: Lessons learned\nfrom the 2015 mscoco image captioning challenge.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39(4):652–663.\nKaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang\nWang. 2016. A comprehensive survey on cross-\nmodal retrieval. arXiv preprint arXiv:1607.06215.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n38–45.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual attention.\nIn International Conference on Machine Learning,\npages 2048–2057. PMLR.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? Advances in Neural Information\nProcessing Systems, 27.\nA Appendix\nA.1 Effect of the NULL Image\nSince all the non-visual sentences in the training\ncorpus are mapped to a common NULL image, we\naim to see the effect of the chosen NULL image\non the results. Recall that the NULL image used\nfor our main experiments was obtained by creat-\ning an RGB image in which each pixel value is\nchosen randomly. We perform the same process\nwith a different random seed to generate another\nNULL image. Additionally, we use a natural image\nas another alternative for the NULL image. These\nimages are shown in Figure 6. We then evaluate\nthe resulting models on the human-annotated test\nset of TI MED. Table 5 shows that the performance\nof the models is not dependent on the choice of the\nNULL image. We also find no dependence between\nthe choice of the NULL image and the performance\non downstream text-to-image retrieval.\n2404\n(a) Original NULL image\n (b) NULL image with diff. seed\n (c) Natural NULL image\nFigure 6: Various NULL images used to study the effect of the chosen image on the text visualness identification task\nand the downstream text-to-image retrieval task.\nVA\nRIANTS F1 ↑ PRECISION ↑ RECALL ↑ ACC. ↑ M\nRR ↑\nTIP-CLIP\n(Original – Fig. 6a) 0.865 0.858 0.873 0.871 0.937\nTIP-CLIP\n(w/ diff. seed – Fig. 6b) 0.867 0.854 0.875 0.872 0.934\nTIP-CLIP\n(natural image - Fig. 6c) 0.861 0.855 0.876 0.872 0.939\nTable 5: Effect of the choice of the NULL image on categorizing the human-annotated test set of TI MED and\ndownstream text-to-image retrieval. Reported F1, Precision, and Recall values are macro-averages across the two\nclasses (visual and non-visual).\nCategory\nExample words (assigned score)\nHigh\nimageability\nmartini, crabmeat, teeth, oysters, mosquitos, bracelets, motorboat, dia-\nmonds, squirrels, cigarettes, beaches, trumpets, dolphin, caramel, cattle,\nportobello, libraries, chimpanzee, snorkeling, sailboat, harmonica\nMedium\nimageability\nreassure, militancy, inhumanly, catalyses, industrial, peacefulness, hand-\nwoven, neurosurgery, overwashed, whooper, snails, preeminence, recluse,\nentrepreneur, character, insufficient, paladin, impersonal, deviously, re-\ncover\nLo\nw imageability\npolitologist, psycholinguistic, requirements, confirmatory, terseness, pre-\nformulation, offender, controversial, unhealable, monoculturalism, mis-\nerable, reprogrammability, this, participate, attractive, determinant, dises-\ntablishment\nTable 6: Qualitative examples of words that are assigned scores in the high (≥0.7), medium (∈(0.3,0.7)), and\nlow (≤0.3) range using the word2vec embedding-based propagation methodology.\nA.2 Assessment of word-level imageability\nscore propagation\nWe randomly selected 500 words from the MRC\nlexicon and 500 words from the word2vec vocabu-\nlary that did not occur in the MRC lexicon. Each\nword was shown to 9 annotators using Amazon\nMechanical Turk to seek responses to the follow-\ning question: “Do you agree that the word below\nevokes an image or picture in your mind?” The\nannotators were instructed to respond on a 7-point\nLikert scale, where 1 denoted strong disagreement\nand 7 denoted strong agreement. Please see Ap-\npendix A.3 for details about the instructions, demo-\ngraphic filters, and compensation.\nWe average the ratings for all the annotated\nwords and normalized them to be∈[0,1]. We com-\npute the Pearson’s correlation coefficient between\n(a) the average ratings for MRC words and the nor-\nmalized imageability scores, and (b) the average\nratings for word2vec words and the imageability\nscores assigned via embedding-based propagation.\nThe correlation between MRC imageability scores\nand average annotators’ ratings is 0.870 (p <\n0.001) and the correlation between scores assigned\nvia our propagation method and average annota-\ntors’ ratings is 0.735 (p< 0.001). This high posi-\ntive correlation coefficient between assigned image-\nability scores and human-perceived ratings demon-\nstrates the effectiveness of our adopted propagation\nmethod. We also note that the inter-annotator agree-\n2405\nments for the ratings for MRC words and word2vec\nwords, as computed using Krippendorf’sα(ordinal\nmeasure), were 0.626 and 0.584, respectively.\nOverall, this assessment illustrates the validity\nof propagating word-level imageability scores us-\ning embedding-based semantic similarities. More\nbroadly, the aim of adopting this approach is to\nexpand the coverage of MRC lexicon. Qualita-\ntively, we observe that words like ‘gotcha’ (0.33)\nand ‘presbyterian’ ( 0.61) are assigned meaning-\nful imageability scores, demonstrating expansion\nalong time and domains. As a point of difference\nbetween human ratings and assigned scores, we\nnotice that the propagation approach assigned a\nhigh imageability score to words like ‘qawwali’\n(\n0.60) while the human annotators did not, possi-\nbly due to a lack of sociocultural context. In Table\n6 we show illustrative words that are assigned high\n(≥0.7), medium (∈(0.3,0.7)), and low (≤0.3)\nimageability scores using our propagation method.\nA.3 Details about MTurk Experiments\nFor all our annotation tasks, we recruited annota-\ntors using Amazon Mechanical Turk. We set the\ncriteria to ‘Master’ annotators with at least a 99%\napproval rate and were located in the United States.\nTo further ensure the quality of annotations, we re-\nquired the annotators to have at least5000 accepted\nannotations in the past. The rewards were set by\nassuming an hourly rate of 12 USD for all the anno-\ntators. We show the annotation interfaces in Figure\n7. In addition, the annotators were informed that\nthe aggregate statistics of their annotations would\nbe used and shared as part of academic research.\nWe also inserted some “attention-check” exam-\nples during the annotation tasks to ensure the an-\nnotators read the text carefully before responding.\nThis was done by asking the annotators to mark\na randomly chosen score on the Likert scale re-\ngardless of the actual content. We discard the an-\nnotations from annotators who did not correctly\nrespond to all the attention-check examples and\nre-collect annotations for the affected samples.\nA.4 Further analyses on the correlation\nbetween attention scores and word-level\nvisualness scores\nWe compute the Pearson’s correlation coefficient\nbetween a model’s average attention scores over\nwords and the visualness score assigned using our\npropagation method. However, unlike Table 3,\nthis time, we consider the propagated imageability\nscores which lead to broader coverage in terms of\nvocabulary. As seen in Table 7, we observe the\nsame trends as with MRC imageability scores, al-\nbeit with slightly lower values of correlation scores.\nTo analyze the alignment between learned atten-\ntion scores for various models, we compute the\ncorrelation between average attention scores across\ndifferent models. Pearson’s correlation coefficients\nin Table 8 show that all the model attention scores\nhave a moderate correlation with each other.\nWhy are correlation scores higher for MSCOCO\nthan for TIMED?: An interesting trend across Ta-\nble 3 and 7 is that the correlation scores are consis-\ntently higher, across all the models under consid-\neration, for the MSCOCO dataset than the test set\nof TI MED. We note that, on average, MSCOCO\nhas a caption length of 11.4 whereas the TI MED\ndataset has an average sentence length of 20.6,\nwith a greater concentration of objects from the\nVisual Genome objects—6.7 (58.7%) objects per\nexample versus 8.4 (40.7%) objects per example).\nFor our TIP-CLIP model, these objects acquire an\naverage of 63.2% attention scores across all the\nMSCOCO examples, whereas they only acquire\n37.1% of attention scores, on average, across the\nexamples in the TI MED test set. Overall, these\nresults demonstrate that the TIP-CLIP model at-\ntends over words in the MSCOCO corpus in an\nobject-targeted manner but the attention is rela-\ntively diffused in the TI MED corpus. Combined\nwith the observation that MRC imageability scores\nare higher for concrete objects (Paivio et al., 1968),\nthis explains why the correlation scores are consis-\ntently higher on MSCOCO than on TIMED.\nMO\nDELS MSCOCO TI MED\nBER\nT 0.434*** 0.301***\nCLIP 0.429*** 0.262***\nTIP-CLIP (Ours) 0.465*** 0.338***\nTable 7: Pearson’s correlation coefficient between prop-\nagated imageability scores (using word2vec) and model\nattention-scores. *** denotes p <0.001\nEffect of length on the correlation between at-\ntention and MRC-I scores: We categorize the sen-\ntences in the test set of TIM ED into short (≤10;\nn = 304 ), medium ( ∈(10,20); n = 505 ), and\nlong (≥20; n = 606 ) sentences based on word\ncounts. However, we did not find a notable vari-\nation in the correlation scores between the atten-\ntion weights of the TIP-CLIP model and MRC\nImageability scores. Pearson’s correlation coeffi-\n2406\n(b ) Int er f ace t o e v aluat e w or d-le v el visualness scor es assigned b y t he pr opagation met hod\n(a) Int er f ace t o collect sent ence-le v el visualness scor es\nFigure 7: Interface for our annotation tasks on Amazon Mechanical Turk. For each of the annotations task, we also\nshow the instructions provided to the annotators.\nMO\nDELS BERT CLIP TIP-CLIP\nBER\nT — – –\nCLIP 0.552*** – –\nTIP-CLIP (Ours) 0.631*** 0.571*** –\nTable 8: Pearson’s correlation coefficient between word-\nlevel attention scores of various models for the TIMED\ntest set. *** denotes p <0.001\nMO\nDELS F1 ↑ PRECISION ↑ RECALL ↑ ACC. ↑\nRandom\n0.503 0.503 0.503 0.505\nMRC-I\n0.470 0.472 0.472 0.470\nVG-Objects 0.536 0.541 0.539 0.548\nMRC-I\n+ w2v 0.501 0.502 0.504 0.502\nMRC-I + GloVe (Twitter) 0.516 0.518 0.520 0.519\nBER\nT 0.612 0.634 0.624 0.618\nCLIP\n0.644 0.645 0.645 0.644\nTIP-CLIP (Ours) 0.696 0.693 0.691 0.694\nTable 9: Out of domain evaluation on the Twitter\ndataset. Reported F1, Precision, and Recall values\nare macro-averages across the two classes (visual and\nnon-visual).\ncient was 0.33, 0.35, and 0.37 for short, medium,\nand long sentences, respectively. We observed the\nsame trend for the fine-tuned BERT model and the\npre-trained CLIP model.\nA.5 Out-of-Domain Generalization\nRobustness of vision-language models has been\nthe subject of investigation in several prior\nworks (Verma et al.,2022; Ramshetty et al., 2023;\nLi et al., 2021). A critical assessment of the ro-\nbustness and generalizability of the models trained\nusing our proposed approach is to conduct evalu-\nations on out-of-domain (OOD) datasets. To this\nend, we curate a social media dataset by scraping\nTwitter. We start with the Wikipedia-based Image\nText Dataset (WIT) (Srinivasan et al., 2021) and\nquery Twitter using the Wikipedia page title to re-\ntrieve posts in English that are with and without\nimages. We require that the retrieved post con-\ntains the page title string to ensure topical simi-\nlarity between posts with and without images. To\nremove examples with irrelevant images, we dis-\ncard posts with a CLIP-similarity lower than 0.70\nbetween the Twitter post’s image and the corre-\nsponding image on Wikipedia. Consequently, we\nobtain a dataset of Twitter posts containing men-\ntions of 1185 Wikipedia topics, 7844 Twitter posts\nwith images, and 7248 Twitter posts without im-\nages. The posts with and without images are tied\nby common Wikipedia topics.\nWe hypothesize that the text in Twitter posts that\nmention a certain topic and contain an image is\nmore visual than text in Twitter posts that men-\ntion the same topic and do not contain any images.\nTo test this hypothesis, we randomly sample 40\nWikipedia topics and present the associated text\nwith (n = 264 ) and without images ( n = 241 )\nto human annotators. In an AMT survey that fol-\nlows the design for curating TIMED, we find that\nthe average annotator rating for the text from Twit-\nter posts without images is 2.306 (±1.369) while\nthat for text from Twitter posts with images is\n2407\n4.304 (±1.273). We observe the inter-annotator\nagreement of 0.413, which is similar to that ob-\nserved while curating TIMED. For 34 out of the 40\nWikipedia topics, the annotators provided a higher\nimageability rating to text originally associated\nwith an image on Twitter than text not associated\nwith an image. Overall, the AMT survey validates\nour hypothesis by demonstrating that text in Twitter\nposts with images is perceived as more visual than\ntext in Twitter posts without images, modulo the\ntopic is common across the posts.\nWe now ask the question: how well the models\nconsidered in our work categorize Twitter text with\nimages as visual and Twitter text without images\nas non-visual? We first adapt the thresholds used\nto classify text using various methods by running\nan evaluation on a randomly sampled validation set\nof 100 Twitter examples, 50 from each category.\nThe thresholds are set as follows: MRC-I: 0.19;\nVG-Objects: 0.52; MRC-I + w2v: 0.17; MRC-I +\nGloVe: 0.326; CLIP: 0.87; TIP-CLIP: 0.74. Using\nthese threshold values, we categorize the rest of\nthe Twitter dataset (n = 14,992) into visual and\nnon-visual categories. The random baseline uses\nuniform sampling.\nTable 9 shows the results for this out-of-domain\nevaluation. First, we note that all models undergo\na severe drop in performance on the OOD dataset,\nindicating that the notion of sentence-level image-\nability is strongly tied to the domain. Our proposed\nTIP-CLIP model demonstrates better OOD gener-\nalization capabilities than all the considered base-\nlines. It is noteworthy that the fine-tuned BERT\nmodel performs poorly on the OOD dataset than\nthe standard pre-trained CLIP model. The aggre-\ngation of word-level imageability scores provides\na worse-than-random estimate of sentence-level\nimageability on the OOD dataset.\nA.6 Predictions on Ambiguous Sentences\nRecall that while curating TI MED, we combined\nexamples without a clear majority from the anno-\ntators (n = 378 ) and those with majority votes\nfor the ‘Neutral’ category ( n = 2 ) into a single\ncategory called ambiguous. We revisit these exam-\nples to analyze how the most competitive baselines\n6Since we are operating with the Twitter domain, we de-\nsign a version of the propagation method where MRC Im-\nageability scores are propagated in the GloVe-embedding\nspace, where the GloVe embeddings are learned on Twitter cor-\npus (Pennington et al., 2014). We use 200-dimensional GloVe\nvectors trained on 2 billion Twitter posts with a vocabulary\nsize of 1.2 million.\n3\n 2\n 1\n 0 1 2 3 4 5\nstandardized 'visual' score\n0\n25\n50\n75\n100\n125\n150\n175\n200Number of examples\nCLIP scores\nBERT scores\nTIP-CLIP scores\nFigure 8: Distribution of standardized visualness scores\nfor ambiguous examples (i.e., (v−µ)/σ, where v is\nthe original visualness score, µand σare the mean and\nstandard deviation of the distributions, respectively). We\ncontrast the predicted visualness scores by fine-tuned\nBERT, pre-trained CLIP, and our TIP-CLIP models.\nand our proposed TIP-CLIP model score them on\nimageability. We compute the imageability score\nusing Equation 2 for CLIP and TIP-CLIP, while\ntreating fine-tuned BERT’s prediction probability\nscore as its imageability score for a given exam-\nple. To appropriately compare the distribution of\nimageability scores across these three models, we\nstandardize the values by computing z-scores (i.e.,\nxi is transformed into zi = (xi −µ)/σ; where xi\nis the original value, µand σ are mean and stan-\ndard deviation of the distribution that xi belongs\nto). In Figure 8, we show that while CLIP and TIP-\nCLIP imageability scores are distributed normally\naround their respective means, BERT imageability\nscores are bimodal with peaks close to one stan-\ndard deviation away from their mean. This demon-\nstrates that if the models were to be used forscoring\ntext imageability, as opposed to categorizing text\ninto visual and non-visual categories, CLIP and\nTIP-CLIP models will provide more reasonable\nmiddle-level scores for ambiguous text, whereas\nscores from BERT would either be higher or lower.\nWe attribute this to how the underlying models are\ntrained and how the consequent imageability scores\nare computed. While the BERT model is trained\nsolely for the classification task that emphasizes\ndiscriminative encoding and the predicted proba-\nbility score is used as the imageability score, the\ndistribution is bimodal. However, CLIP and TIP-\nCLIP are trained using image-text matching (the\nformer, entirely; the latter, to some extent), and\nimageability scores are computed as the distance\nbetween the NULL image and input text.\n2408",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7689958810806274
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6743627786636353
    },
    {
      "name": "Heuristics",
      "score": 0.6601366400718689
    },
    {
      "name": "Natural language processing",
      "score": 0.6410638689994812
    },
    {
      "name": "Text generation",
      "score": 0.5934343934059143
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5255947113037109
    },
    {
      "name": "Task (project management)",
      "score": 0.5186083316802979
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4311951696872711
    },
    {
      "name": "Visualization",
      "score": 0.4237838089466095
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3385942280292511
    },
    {
      "name": "Mathematics",
      "score": 0.07714816927909851
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1306409833",
      "name": "Adobe Systems (United States)",
      "country": "US"
    }
  ]
}