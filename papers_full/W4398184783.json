{
  "title": "On Replacing Humans with Large Language Models in Voice-Based Human-in-the-Loop Systems",
  "url": "https://openalex.org/W4398184783",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2556116543",
      "name": "Shih-Hong Huang",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A4226032672",
      "name": "Ting-Hao `Kenneth' Huang",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2556116543",
      "name": "Shih-Hong Huang",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A4226032672",
      "name": "Ting-Hao `Kenneth' Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W7064952927",
    "https://openalex.org/W6987405165",
    "https://openalex.org/W4377864954",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W2794424581",
    "https://openalex.org/W6910945018",
    "https://openalex.org/W6642317481",
    "https://openalex.org/W2767057583",
    "https://openalex.org/W4389519553",
    "https://openalex.org/W4387075911",
    "https://openalex.org/W4310997258",
    "https://openalex.org/W4385014863",
    "https://openalex.org/W4390091358",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4386875608",
    "https://openalex.org/W1968326021"
  ],
  "abstract": "It is easy to assume that Large Language Models (LLMs) will seamlessly take over applications, especially those that are largely automated. In the case of conversational voice assistants, commercial systems have been widely deployed and used over the past decade. However, are we indeed on the cusp of the future we envisioned? There exists a social-technical gap between what people want to accomplish and the actual capability of technology. In this paper, we present a case study comparing two voice assistants built on Amazon Alexa: one employing a human-in-the-loop workflow, the other utilizes LLM to engage in conversations with users. In our comparison, we discovered that the issues arising in current human-in-the-loop and LLM systems are not identical. However, the presence of a set of similar issues in both systems leads us to believe that focusing on the interaction between users and systems is crucial, perhaps even more so than focusing solely on the underlying technology itself. Merely enhancing the performance of the workers or the models may not adequately address these issues. This observation prompts our research question: What are the overlooked contributing factors in the effort to improve the capabilities of voice assistants, which might not have been emphasized in prior research?",
  "full_text": "On Replacing Humans with Large Language Models in\nVoice-Based Human-in-the-Loop Systems\nShih-Hong Huang, Ting-Hao ‘Kenneth’ Huang\nCollege of Information Sciences and Technology, The Pennsylvania State University\n201 Old Main, University Park, PA 16802, USA\n{szh277,txh710}@psu.edu\nAbstract\nIt is easy to assume that Large Language Models (LLMs)\nwill seamlessly take over applications, especially those that\nare largely automated. In the case of conversational voice\nassistants, commercial systems have been widely deployed\nand used over the past decade. However, are we indeed on\nthe cusp of the future we envisioned? There exists a social-\ntechnical gap between what people want to accomplish and\nthe actual capability of technology. In this paper, we present\na case study comparing two voice assistants built on Ama-\nzon Alexa: one employing a human-in-the-loop workflow, the\nother utilizes LLM to engage in conversations with users. In\nour comparison, we discovered that the issues arising in cur-\nrent human-in-the-loop and LLM systems are not identical.\nHowever, the presence of a set of similar issues in both sys-\ntems leads us to believe that focusing on the interaction be-\ntween users and systems is crucial, perhaps even more so than\nfocusing solely on the underlying technology itself. Merely\nenhancing the performance of the workers or the models may\nnot adequately address these issues. This observation prompts\nour research question: What are the overlooked contributing\nfactors in the effort to improve the capabilities of voice as-\nsistants, which might not have been emphasized in prior re-\nsearch?\nIntroduction\nFor a long time, the argument for (nearly) real-time crowd-\npowered systems—employing online workers to operate\ncomputer components for quick judgments or predictions,\nmimicking actual computer components—was to fill the\nsocial-technical gaps of current automatic technologies, en-\nabling researchers to study interaction problems or user\nneeds that can only be studied when particular systems or\nservices exist. For example, Huang, Chang, and Bigham\nsuggested in Evorus (Huang, Chang, and Bigham 2018), a\ncrowd-powered conversational assistant:\nSuch automated [general conversational assistant]\nsystems only become useful once they can com-\npletely take over from the crowd-powered system.\nSuch abrupt transition points mean substantial up-\nfront costs must be paid for collecting training exam-\nples before any automation can be tested in an on-\nline system. ... we explore an alternative approach\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nof a crowdpowered system architecture that supports\ngradual automation over time. (Huang, Chang, and\nBigham 2018, p. 1-2)\nThis was essentially based on the stance: “AI is not there\n(yet); let’s use humans for now”. A classic example is\nVizWiz (Bigham et al. 2010), a nearly real-time crowd-\npowered system that uses online crowd workers to quickly\nrespond to visual questions sent from blind users or those\nwith visual impairments. Way before AI could reliably\nanswer arbitrary visual questions, VizWiz enabled large-\nscale research into the visual challenges that blind users\nencounter daily. Other impactful examples include Cho-\nrus (Lasecki et al. 2013), Soylent (Bernstein et al. 2010), and\nScribe (Lasecki et al. 2012). A series of techniques and tools\nwere created to make these systems possible, such as the re-\ntainer model (Bernstein et al. 2011) and quikturkit (Bigham\net al. 2010), or even scalable (Huang and Bigham 2017).\nSome projects, such as Evorus (Huang, Chang, and Bigham\n2018) and VizWiz, took a step further to argue that the in-\nsights gained from studying these imminent problems could,\nin turn, improve today’s computer technology.\nSuch arguments were, unspokenly and probably subtly,\nchallenged by the recent rise of large language models\n(LLMs). Evidence has shown that ChatGPT exceeds the per-\nformance of online crowd workers in labeling and writing\ntasks (T ¨ornberg 2023; Gilardi, Alizadeh, and Kubli 2023;\nCegin, Simko, and Brusilovsky 2023); many crowd work-\ners use ChatGPT for their tasks. A recent study showed that\naggregating 20 crowdsourced labels from a well-executed,\ncarefully designed MTurk pipeline still falls short of GPT-\n4’s labeling accuracy (He et al. 2024). Some even view\nLLMs as a precursor to Artificial General Intelligence\n(AGI), which can achieve or surpass human performance in\nmillions of tasks (Bubeck et al. 2023). All of these, collec-\ntively, hint that LLMs might be closing the social-technical\ngaps that justified the creation of real-time crowd-powered\nsystems more effectively than human crowd workers. If\ntrue, researchers could replace crowd workers with LLMs\nin these systems, significantly reducing engineering efforts\nand financial costs while still being able to study the same\nset of interactive problems and user needs. In fact, some\nstudies have directly compared crowdsourcing workflows\nwith LLM workflows, seeking to identify what elements of\ncrowdsourcing can be adapted to LLM workflows and what\nAAAI Spring Symposium Series (SSS-24)\n45\ncannot (Wu et al. 2023; Grunde-McLaughlin et al. 2023).\nThis paper presents an interesting case study to offer our\nviewpoint within this discussion. We compared two systems\nbuilt on top of Amazon’s Echo, a commercial smart speaker\nintegrated with Amazon Alexa. One, created by us, involved\nhumans in the loop to hold the conversation (Huang et al.\n2022), and the other, created by a team of researchers from\nJohns Hopkins University and Northeastern University, used\nan LLM to hold the conversation behind the scenes (Mah-\nmood et al. 2023). Through this comparison, we argue that\nwhile last-mile interaction issues such as conversation cut-\noffs and speech recognition problems persist across both\nhuman-powered and LLM workflows, each approach re-\nquires distinct considerations. Specifically, certain accom-\nmodations must be made when replacing human workflows\nwith LLMs. LLMs may introduce new challenges that are\nrarely encountered by human workers, and vice versa.\nA Tale of Two Systems\nIn this paper, we observe and compare two similar systems:\none powered by humans and the other by an LLM.\nECHO PAL: A Human-in-the-Loop Voice Assistant\nECHO PAL is a human-in-the-loop voice assistant based on\nAmazon Echo introduced by Huang et al. in 2022 (Huang\net al. 2022). When a user talks to E CHO PAL, Echo records\nthe audio and turns the speech into text through the built-\nin automatic speech recognition (ASR) system. The tran-\nscribed text is then sent to the backend of E CHO PAL and\npresented to the human worker as a message in the worker\ninterface, where the worker can see the transcribed mes-\nsage and a set of possible alternative transcriptions gener-\nated by the system. Furthermore, E CHO PAL also automati-\ncally generates and presents a list of suggested responses to\nthe worker. As each alternative transcription has its own sug-\ngested responses, the worker can click on the transcription to\nswitch between them. Search support sites, such as Google\nSearch or Google Weather, are also provided in the interface.\nWith the support of these technological features, the worker\nwas required to produce each response within 25 seconds.\nThe worker’s response is then sent back to the Echo device,\nwhere a built-in text-to-speech (TTS) system reads out the\nmessage to the user. Users were allowed to chat freely with\nECHO PAL without constraints on the conversation topic.\nECHO PAL inherited the spirit of Chorus and Evorus,\nwhich aimed to use human intelligence to fill the gaps be-\ntween what users want to use conversational assistants for\nand what conversational assistants are capable of, allowing\nresearchers to study interaction questions that could not oth-\nerwise be studied (Lasecki et al. 2013; Huang, Chang, and\nBigham 2018). In the user study, Huang et al. used partic-\nipants as both users and workers (instead of hiring online\nworkers to be the workers.)\nCHATGPT-IN-ECHO : An LLM-in-the-Loop Voice\nAssistant\nCHATGPT-IN-ECHO was presented by Mahmood et al. in\n2023, with the aim of exploring the constraints and oppor-\ntunities of LLM-powered voice assistants (Mahmood et al.\n2023).1 In the same period of time, many other projects also\nappeared to connect LLMs, especially ChatGPT provided\nby OpenAI via API, with voice assistants such as Alexa\nor Google Assistant (Yang et al. 2024; AndroidAuthority\n2023).\nWhen a user talks to C HATGPT-IN-ECHO , Echo records\nthe audio and turns the speech into text through the built-\nin automatic speech recognition (ASR) system. The tran-\nscribed text is then sent to a primary middleman API; this\nmiddleman API is used to handle the time-out constraint\nset by Alexa Skill. If it does not receive a message from\nChatGPT within a certain amount of time, it will initi-\nate filler/small talk to maintain Alexa Skill activity. A sec-\nondary middleman API is used to communicate with Chat-\nGPT. Upon receiving the relayed user transcription from the\nfirst middleman API, it will request ChatGPT via API call\nwith the latest user message. A shared database of conversa-\ntion history and progress achieved communication between\nthe primary and secondary middleman API. Once the sec-\nondary middleman API receives the ChatGPT response, it\nwill update the conversation history. The primary middle-\nman API will monitor the database for conversation updates\nand send the ChatGPT message to the user through Alexa\nTTS. Three tasks from different scenarios were tested: med-\nical self-diagnosis, creative planning, and discussion with\nopposing stances.\nWhat Makes Them Comparable\nECHO PAL and CHATGPT-IN-ECHO were both run on Ama-\nzon’s Echo devices, a commercial voice-enabled smart\nspeaker designed to assist users via voice. In the origi-\nnal study, E CHO PAL ran on Echo (2nd generation), and\nCHATGPT-IN-ECHO ran on Echo Dot. Both devices had no\nmonitors and used voice as the only communication chan-\nnel with users, aided with a ring light to show the state of\nthe device, e.g., listening, processing, or deactivated. Note\nthat Alexa Skill did not (and still does not) support au-\ndio streaming and only provided the transcribed text of the\nusers’ speech to the backend for developers. The transcribed\ntexts were fed directly to the backend without utilizing the\nintent architecture of Alexa Skill. The conversation logs be-\ntween the user and the voice assistants were stored in the\ndatabase hosted outside the Alexa Skill.\nECHO PAL and C HATGPT-IN-ECHO are similar also in\nterms of schematic. Both utilized behind-the-scenes archi-\ntecture to handle the conversation history, and Alexa Skills\ninteracted with the backend responder regarding the user\nmessage. E CHO PAL had human workers—hence the need\nfor a worker interface—and helper functions to help the\nworkers. C HATGPT-IN-ECHO utilized LLM and therefore\nneeded to handle the API call.\nAlexa API Time-Out. To our knowledge, the current de-\nfault time limit for Alexa Skills to process responses is about\n1Mahmood et al. did not name their system. We used\nCHATGPT-IN-ECHO as a placeholder name for communication\npurposes.\n46\n8 seconds, which is fixed for most cases. C HATGPT-IN-\nECHO employed middleman APIs to handle the time-outs\nby inserting fillers and small talks before the 8-second time-\nout occurred and continuing the conversation after responses\nwere received from ChatGPT. ECHO PAL, on the other hand,\ndoes not face the same issue because it was developed under\nan earlier version of Alexa Skill, and extending the time-out\nvalue to 25 seconds was possible. However, we believe that\nit is possible to modify E CHO PAL’s architecture to fit the\ncurrent Alexa Skill configuration and adopt techniques used\nby CHATGPT-IN-ECHO .\nHumans vs. LLMs Comparison\nWe are the authors of E CHO PAL (Huang et al. 2022), and\nthe following comparison was made by us reading through\nthe paper of CHATGPT-IN-ECHO in detail (Mahmood et al.\n2023). We also looked into our experimental records to com-\npare the details, which might include some nuanced infor-\nmation that we did not mention in our original paper.\nHumans’ Problems That LLMs Do Not Have\nLong Response Latency. One of the main drawbacks for\nECHO PAL is the long latency for the users, where they\nhave to wait for an average of 17.68 seconds to get qual-\nity responses from Alexa. However, the same response\ntime was considered too short for the workers at the back-\nend to provide quality responses. Most of this latency\ncomes from human workers needing to type; some latency\ncomes from ASR and internet transmission time. Mean-\nwhile, CHATGPT-IN-ECHO is powered by GPT-3.5, which\ncan produce thousands of words in seconds.\nTo some extent, integration with LLMs might make future\nvoice assistants that aim to hold longer, open-ended conver-\nsations one step closer to reality: it reduces the unreasonably\nlong latency of human-powered voice assistants, it makes\nsuch systems more scalable and affordable to build than re-\ncruiting human workers, and it bypasses the speed limits set\nby human typings.\nLLMs’ Problems That Humans Do Not Have\nOversharing and Repetitiveness. CHATGPT-IN-ECHO\nraised concerns regarding voice assistants providing repeti-\ntive information and oversharing, which was not observed in\nECHO PAL’s study. Repetitive information provided by Chat-\nGPT was reported in the scenarios tested by CHATGPT-IN-\nECHO . Since voice users cannot review previous conversa-\ntions compared to text conversations, repetitive information\ncauses further inconvenience during the conversation. The\nphenomenon was particularly true in medical-related con-\nversations; repetitive information can occur even if Chat-\nGPT was prompted not to repeat certain warning infor-\nmation. OpenAI’s policy likely causes this, and it might\nnot be easily fixable by outside developers. Additionally,\nCHATGPT-IN-ECHO encountered a challenge with Chat-\nGPT’s oversharing, where the responses provided by Chat-\nGPT was too overwhelming for users to absorb. Even when\nprompted to give brief responses under 100 words, the\nhigher information density in text interaction, which Chat-\nGPT is based on, can overwhelm users in a voice setup.\nProblems That Both LLMs and Humans Have\nCut-Offs. We define cut-offs as Alexa stopping listening\nto the users’ speech while users are in the middle of their\nspeech or intend to keep speaking. In E CHO PAL, cut-offs\nwere attributed to (1) users having longer pauses between\nwords, causing Alexa to think the user’s speech ended, and\n(2) Alexa entered listening mode after its own speech and\nstopped listening if users did not speak in time. C HATGPT-\nIN-ECHO faced similar problems, such as (1) partial listen-\ning, where partial speech was captured in users’ speech, (2)\ninterruption, where Alexa interrupts the users’ speech mid-\nsentence, and (3) pauses between words when the users or-\nganize thoughts.\nASR Errors. ASR errors were one of the main obstacles\nin E CHO PAL, since the audio of users’ speech was not ob-\ntainable and only the transcribed text by Alexa was received.\nDifferent alternative transcriptions were provided to help hu-\nman workers to tackle this problem. C HATGPT-IN-ECHO\nalso faced ASR problems but had the inherent ChatGPT re-\ncovery mechanism, such as apologizing or asking the user to\nclarify. We suspect the typo fixing of the text-based system,\nwhich ChatGPT is good at, does not work as well for voice-\nbased communication. This is likely because similar sounds\ndo not translate to good guessing by ChatGPT.\nConversation Breakdowns. Both Cut-offs and ASR er-\nrors can cause the interaction to break down, meaning the\ncurrent interaction session will either be (1) terminated by\nAlexa without the users’ acknowledgment or (2) require ini-\ntiating a recovery mechanism in order to continue the inter-\naction. The only solution to address the first type of break-\ndown is for the user to re-initiate the Alexa Skill, effectively\nrestarting the interaction. One way to make the re-initiation\neasier is by better maintaining the previous interaction his-\ntory, but it still requires the user to restart the interaction\nactively. For the second scenario, recovery within the same\ninteraction session is possible. Recovery can be initiated by\nthe users or by the voice assistants. In the first category of\nbreakdown, users will need to restart the interaction with the\nvoice assistants actively. In the second category, the voice\nassistants can assist in the recovery process within the same\nsession. However, most recovery happens when users repeat\nthe previous sentence that failed to be correctly processed or\nmove on to the next sentence and discard the failed sentence.\nOne feature of echo devices is the light ring that indicates\nthe state that Alexa is in. When the device is in “listening”\nmode, the light ring will illuminate and shimmer. ECHO PAL\nreported that users’ familiarity level with Echo devices’ in-\nteraction pattern significantly impacts their ability to handle\ncut-offs. Those who are more familiar with the Echo inter-\naction patterns encountered fewer cut-offs.\nCHATGPT-IN-ECHO described users’ mental model\nwhile interacting with voice assistants in multi-turn scenar-\nios. They suggested that actions users take after conversation\nbreakdowns indicated the gap between users’ mental mod-\nels and voice assistants’ capabilities. The light ring on the\ndevice was not discussed in CHATGPT-IN-ECHO ’s paper.\n47\nLong responses are not suitable. ECHO PAL suggested\nthat long text responses are not suitable for voice applica-\ntion since it takes a lot of time to read out the full paragraph\nand interrupt the flow of voice conversation. CHATGPT-IN-\nECHO on the other hand limited the response of ChatGPT\nto 100 words in prompt but still face the problem of voice\nassistant oversharing and overwhelming the users.\nDiscussion\nTime vs. Quality Trade-offs\nWe identified a trade-off between the quality of responses\nprovided by E CHO PAL and the time required to generate\nthe response. In order to provide higher-quality responses,\nworkers needed more time. Participants who were assigned\nto be the workers that operated E CHO PAL suggested that\nthe 25-second response time was not enough. Enforcing a\nshorter response time would likely lead to a decrease in re-\nsponse quality. Grunde-McLaughlin et al. proposed the de-\nsign space for LLM chaining by adapting crowdsourcing\nworkflows (Grunde-McLaughlin et al. 2023). The trade-off\nbetween time and quality we observed in ECHO PAL also ex-\nists in this design space.\nFor LLM-based systems such as C HATGPT-IN-ECHO ,\nwhich utilized API calls instead of waiting for human work-\ners to provide responses, we speculate such trade-offs to be\nless prevalent.\nEffort Put in Prompt Engineering\nWhile LLM workflow can offer lower latency and cost com-\npared to crowdsourcing workflow, efforts need to be di-\nrected toward prompting LLMs to generate more diverse re-\nsponses. While it is possible to get diversified responses by\nquerying crowd workers multiple times, simply prompting\nLLMs multiple times can generate responses with less di-\nversity. Additionally, instructions and subtasks suitable for\ncrowdsourcing workflows may be overly complicated and\nmay not suit LLMs. Therefore, simpler subtasks and instruc-\ntions are required to utilize LLM capabilities effectively.\nDesign Implication\nWe propose that theinteraction aspect for voice assistants’\nworkflows should be emphasizedwhile the workflow is be-\ning designed. How users interact with the system and their\nsituations play important roles. For example, ECHO PAL and\nCHATGPT-IN-ECHO both had users sit in front of the echo\ndevice alone in a quiet room. Considering real-life scenar-\nios, it is likely that users will interact with the voice assis-\ntants in less than ideal situations, namely (1) where back-\nground noise and external interruption of the conversation\nare present compared to in-lab studies and (2) visual cues\nsuch as on Echo devices (light ring) can be less obvious or\ndoes not apply, especially in voice only interaction such as\nphone calls.\nEcho is Not Designed for Long Conversations. The en-\nforced eight-second timeout for Alexa Skills makes per-\nforming tasks with longer processing time hard and requires\nworkarounds. Alexa Skills are structured with specific tasks\nin mind. Freestyle speech without a predefined topic or\nspeech structure does not integrate well with the current\nAlexa infrastructure. Longer speeches are also not favored\nby Alexa, partly due to its strictly turn-based nature, where\nonly one side can speak at a time. From the users’ perspec-\ntive, it is unclear when the device will stop “listening”; thus,\nmaking longer speeches feels less secure and more prone to\ncut-offs. There is also no means for users to edit their speech\nafter Alexa stops listening; all edits can only happen after\nAlexa replies to the message and begins listening again in\nthe next turn. Combined with the lack of indication for users\nregarding the actual transcribed text, it becomes challeng-\ning to backtrack and edit previous speech, especially if it is\nlengthy and difficult to pinpoint where the edit is needed.\nFrom Alexa’s standpoint, the only way to deliver a message\nis through TTS. However, not all messages are suited to be\ndelivered in one long speech, as users may lose focus in such\nscenarios.\nConclusion\nWe presented a comparison between two Amazon Alexa-\nbased voice assistants, ECHO PAL and CHATGPT-IN-ECHO ,\nwhere E CHO PAL utilized human-in-the-loop workflow and\nCHATGPT-IN-ECHO utilized LLM to generate responses\nto user inquiries. Prior research suggested that crowdsourc-\ning workflow could be adapted by LLM chaining workflow,\nwhich supported our comparison. Regarding the use case,\nECHO PAL let users chat with it freely, and C HATGPT-IN-\nECHO predefined a set of scenarios and tasks that needed\nto be accomplished. We concluded that there are problems\ntroublesome for E CHO PAL, such as latency, and problems\npresent for CHATGPT-IN-ECHO , such as repetitiveness and\noversharing. At the same time, there is also a set of prob-\nlems that were present for both workflows, namely the cut-\noffs, breakdowns, and ASR errors. We consider the above\nshared problems for human-in-the-loop and LLM workflow\nto be interaction-related, and simply treating them as an\nLLM model problem or crowdsourcing problem does not\nhelp solve them.\nReferences\nAndroidAuthority. 2023. Supercharge your Google\nNest Mini with AI by swapping out its brains.\nhttps://www.androidauthority.com/google-nest-mini-\nunofficial-ai-experiment-3346448/. Accessed: 2024-04-10.\nBernstein, M. S.; Brandt, J.; Miller, R. C.; and Karger, D. R.\n2011. Crowds in two seconds: Enabling realtime crowd-\npowered interfaces. In Proceedings of the 24th annual ACM\nsymposium on User interface software and technology, 33–\n42. ACM.\nBernstein, M. S.; Little, G.; Miller, R. C.; Hartmann, B.;\nAckerman, M. S.; Karger, D. R.; Crowell, D.; and Panovich,\nK. 2010. Soylent: a word processor with a crowd inside. In\nProceedings of the 23nd annual ACM symposium on User\ninterface software and technology, 313–322. ACM.\nBigham, J. P.; Jayant, C.; Ji, H.; Little, G.; Miller, A.; Miller,\nR. C.; Miller, R.; Tatarowicz, A.; White, B.; White, S.; et al.\n2010. VizWiz: nearly real-time answers to visual questions.\n48\nIn Proceedings of the 23nd annual ACM symposium on User\ninterface software and technology, 333–342. ACM.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nCegin, J.; Simko, J.; and Brusilovsky, P. 2023. ChatGPT to\nReplace Crowdsourcing of Paraphrases for Intent Classifica-\ntion: Higher Diversity and Comparable Model Robustness.\nIn Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, 1889–1905. Singapore: Association for\nComputational Linguistics.\nGilardi, F.; Alizadeh, M.; and Kubli, M. 2023. ChatGPT\noutperforms crowd workers for text-annotation tasks. Pro-\nceedings of the National Academy of Sciences, 120(30).\nGrunde-McLaughlin, M.; Lam, M. S.; Krishna, R.; Weld,\nD. S.; and Heer, J. 2023. Designing LLM Chains\nby Adapting Techniques from Crowdsourcing Workflows.\narXiv:2312.11681.\nHe, Z.; Huang, C.-Y .; Ding, C.-K. C.; Rohatgi, S.; and\nHuang, T.-H. 2024. If in a Crowdsourced Data Annotation\nPipeline, a GPT-4. arXiv preprint arXiv:2402.16795.\nHuang, S.-H.; Huang, C.-Y .; Deng, Y .; Shen, H.; Kuan, S.-\nC.; and Huang, T.-H. K. 2022. Too Slow to Be Useful?\nOn Incorporating Humans in the Loop of Smart Speakers.\narXiv:2212.03969.\nHuang, T.-H.; and Bigham, J. 2017. A 10-month-long de-\nployment study of on-demand recruiting for low-latency\ncrowdsourcing. In Proceedings of the AAAI Conference on\nHuman Computation and Crowdsourcing, volume 5, 61–70.\nHuang, T.-H.; Chang, J. C.; and Bigham, J. P. 2018. Evorus:\nA crowd-powered conversational assistant built to automate\nitself over time. In Proceedings of the 2018 CHI conference\non human factors in computing systems, 1–13.\nLasecki, W.; Miller, C.; Sadilek, A.; Abumoussa, A.; Bor-\nrello, D.; Kushalnagar, R.; and Bigham, J. 2012. Real-time\ncaptioning by groups of non-experts. In Proceedings of the\n25th annual ACM symposium on User interface software\nand technology, 23–34.\nLasecki, W. S.; Wesley, R.; Nichols, J.; Kulkarni, A.; Allen,\nJ. F.; and Bigham, J. P. 2013. Chorus: a crowd-powered\nconversational assistant. In Proceedings of the 26th annual\nACM symposium on User interface software and technology,\n151–162. ACM.\nMahmood, A.; Wang, J.; Yao, B.; Wang, D.; and Huang, C.-\nM. 2023. LLM-Powered Conversational V oice Assistants:\nInteraction Patterns, Opportunities, Challenges, and Design\nGuidelines. arXiv:2309.13879.\nT¨ornberg, P. 2023. ChatGPT-4 Outperforms Experts and\nCrowd Workers in Annotating Political Twitter Messages\nwith Zero-Shot Learning. arXiv:2304.06588.\nWu, T.; Zhu, H.; Albayrak, M.; Axon, A.; Bertsch, A.; Deng,\nW.; Ding, Z.; Guo, B.; Gururaja, S.; Kuo, T.-S.; Liang, J. T.;\nLiu, R.; Mandal, I.; Milbauer, J.; Ni, X.; Padmanabhan, N.;\nRamkumar, S.; Sudjianto, A.; Taylor, J.; Tseng, Y .-J.; Vai-\ndos, P.; Wu, Z.; Wu, W.; and Yang, C. 2023. LLMs as\nWorkers in Human-Computational Algorithms? Replicating\nCrowdsourcing Pipelines with LLMs. arXiv:2307.10168.\nYang, Z.; Xu, X.; Yao, B.; Zhang, S.; Rogers, E.; Intille,\nS.; Shara, N.; Gao, G. G.; and Wang, D. 2024. Talk2Care:\nFacilitating Asynchronous Patient-Provider Communication\nwith Large-Language-Model. arXiv:2309.09357.\n49",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7153950333595276
    },
    {
      "name": "Human-in-the-loop",
      "score": 0.7028472423553467
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6863199472427368
    },
    {
      "name": "Workflow",
      "score": 0.6848464012145996
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5162072777748108
    },
    {
      "name": "Loop (graph theory)",
      "score": 0.46920934319496155
    },
    {
      "name": "Voice command device",
      "score": 0.4269244074821472
    },
    {
      "name": "Data science",
      "score": 0.36221277713775635
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}