{
    "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation",
    "url": "https://openalex.org/W3175182975",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221940329",
            "name": "Hansen, Nicklas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2127821928",
            "name": "Su, Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1806612777",
            "name": "Wang Xiao-long",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2100677568",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2963680188",
        "https://openalex.org/W3041890730",
        "https://openalex.org/W2605102758",
        "https://openalex.org/W2781726626",
        "https://openalex.org/W2949517790",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2326925005",
        "https://openalex.org/W2950872548",
        "https://openalex.org/W2964342357",
        "https://openalex.org/W3205321526",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2995298643",
        "https://openalex.org/W2964161785",
        "https://openalex.org/W3119908121",
        "https://openalex.org/W2766614170",
        "https://openalex.org/W2968116426",
        "https://openalex.org/W2201912979",
        "https://openalex.org/W2962887844",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W2076337359",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2141559645",
        "https://openalex.org/W3149814450",
        "https://openalex.org/W3208466670",
        "https://openalex.org/W3126676912",
        "https://openalex.org/W2902125520",
        "https://openalex.org/W2168359464",
        "https://openalex.org/W3036670859",
        "https://openalex.org/W1992008267",
        "https://openalex.org/W3157052873",
        "https://openalex.org/W3094044532",
        "https://openalex.org/W2963420272",
        "https://openalex.org/W2963864421",
        "https://openalex.org/W2962938178",
        "https://openalex.org/W2963211300",
        "https://openalex.org/W2781585732",
        "https://openalex.org/W2962742544",
        "https://openalex.org/W2996283175",
        "https://openalex.org/W1757796397",
        "https://openalex.org/W2321533354",
        "https://openalex.org/W3085605093",
        "https://openalex.org/W2121863487",
        "https://openalex.org/W2158782408",
        "https://openalex.org/W2787938642",
        "https://openalex.org/W2155968351",
        "https://openalex.org/W2964291307",
        "https://openalex.org/W2997359900",
        "https://openalex.org/W2058735307",
        "https://openalex.org/W2977481643",
        "https://openalex.org/W2893662673",
        "https://openalex.org/W3101442004",
        "https://openalex.org/W3115293622",
        "https://openalex.org/W2991420892",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W3036185205",
        "https://openalex.org/W3132674603",
        "https://openalex.org/W2953772919",
        "https://openalex.org/W3021708257",
        "https://openalex.org/W3128475431",
        "https://openalex.org/W2954996726",
        "https://openalex.org/W2963906246"
    ],
    "abstract": "While agents trained by Reinforcement Learning (RL) can solve increasingly challenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efficiency and can even lead to divergence. In this paper, we investigate causes of instability when using data augmentation in common off-policy RL algorithms. We identify two problems, both rooted in high-variance Q-targets. Based on our findings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efficiency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL in environments with unseen visuals. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.",
    "full_text": "Stabilizing Deep Q-Learning with ConvNets and\nVision Transformers under Data Augmentation\nNicklas Hansen1 Hao Su1 Xiaolong Wang1\n1University of California, San Diego\nnihansen@ucsd.edu {haosu,xiw012}@eng.ucsd.edu\nAbstract\nWhile agents trained by Reinforcement Learning (RL) can solve increasingly chal-\nlenging tasks directly from visual observations, generalizing learned skills to novel\nenvironments remains very challenging. Extensive use of data augmentation is a\npromising technique for improving generalization in RL, but it is often found to\ndecrease sample efﬁciency and can even lead to divergence. In this paper, we in-\nvestigate causes of instability when using data augmentation in common off-policy\nRL algorithms. We identify two problems, both rooted in high-variance Q-targets.\nBased on our ﬁndings, we propose a simple yet effective technique for stabilizing\nthis class of algorithms under augmentation. We perform extensive empirical\nevaluation of image-based RL using both ConvNets and Vision Transformers (ViT)\non a family of benchmarks based on DeepMind Control Suite, as well as in robotic\nmanipulation tasks. Our method greatly improves stability and sample efﬁciency\nof ConvNets under augmentation, and achieves generalization results competitive\nwith state-of-the-art methods for image-based RL in environments with unseen\nvisuals. We further show that our method scales to RL with ViT-based architectures,\nand that data augmentation may be especially important in this setting.†\n1 Introduction\nReinforcement Learning (RL) from visual observations has achieved tremendous success in various\napplications such as video-games [43, 4, 70], robotic manipulation [37], and autonomous navigation\n[42, 83]. However, it is still very challenging for current methods to generalize the learned skills to\nnovel environments, and policies trained by RL can easily overﬁt to the training environment [81, 13],\nespecially for high-dimensional observation spaces such as images [8, 58].\nIncreasing the variability in training data via domain randomization [ 66, 50] and data augmenta-\ntion [57, 35, 33, 51] has demonstrated encouraging results for learning policies invariant to changes\nin environment observations. Speciﬁcally, recent works on data augmentation [35, 33] both show im-\nprovements in sample efﬁciency from simple cropping and translation augmentations, but the studies\nalso conclude that additional data augmentation in fact decrease sample efﬁciency and even cause\ndivergence. While these augmentations have the potential to improve generalization, the increasingly\nvaried data makes the optimization more challenging and risks instability. Unlike supervised learning,\nbalancing the trade-off between stability and generalization in RL requires substantial trial and error.\nIn this paper, we illuminate causes of instability when applying data augmentation to common\noff-policy RL algorithms [43, 38, 15, 18]. Based on our ﬁndings, we provide an intuitive method for\nstabilizing this class of algorithms under use of strong data augmentation. Speciﬁcally, we ﬁnd two\nmain causes of instability in previous work’s application of data augmentation: (i) indiscriminate\napplication of data augmentation resulting in high-varianceQ-targets; and (ii) that Q-value estimation\nstrictly from augmented data results in over-regularization.\n†Website and code is available at: https://nicklashansen.github.io/SVEA.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021)\narXiv:2107.00644v2  [cs.LG]  9 Dec 2021\nTo address these problems, we propose SVEA: Stabilized Q-Value Estimation under Augmentation,\na simple yet effective framework for data augmentation in off-policy RL that greatly improves stability\nof Q-value estimation. Our method consists of the following three components: Firstly, by only\napplying augmentation in Q-value estimation of the current state, without augmenting Q-targets\nused for bootstrapping, SVEA circumvents erroneous bootstrapping caused by data augmentation;\nSecondly, we formulate a modiﬁed Q-objective that optimizes Q-value estimation jointly over both\naugmented and unaugmented copies of the observations; Lastly, for SVEA implemented with an actor-\ncritic algorithm, we optimize the actor strictly on unaugmented data, and instead learn a generalizable\npolicy indirectly through parameter-sharing. Our framework can be implemented efﬁciently without\nadditional forward passes nor introducing additional learnable parameters.\nWe perform extensive empirical evaluation on the DeepMind Control Suite [64] and extensions of\nit, including the DMControl Generalization Benchmark [21] and the Distracting Control Suite [60],\nas well as a set of robotic manipulation tasks. Our method greatly improve Q-value estimation\nwith ConvNets under a set of strong data augmentations, and achieves sample efﬁciency, asymptotic\nperformance, and generalization that is competitive or better than previous state-of-the-art methods in\nall tasks considered, at a lower computational cost. Finally, we show that our method scales to RL\nwith Vision Transformers (ViT) [10]. We ﬁnd that ViT-based architectures are especially prone to\noverﬁtting, and data augmentation may therefore be a key component for large-scale RL.\n2 Related Work\nRepresentation Learning. Learning visual invariances using data augmentation and self-supervised\nobjectives has proven highly successful in computer vision [46, 45, 82, 74, 68, 65, 75, 27, 7]. For\nexample, Chen et al. [7] perform an extensive study on data augmentation (e.g. random cropping\nand image distortions) for contrastive learning, and show that representations pre-trained with such\ntransformations transfer effectively to downstream tasks. While our work also uses data augmentation\nfor learning visual invariances, we leverage the Q-objective of deep Q-learning algorithms instead of\nauxiliary representation learning tasks.\nVisual Learning for RL.Numerous methods have been proposed with the goal of improving sample\nefﬁciency [29, 56, 68, 76, 40, 59, 61, 54, 77] of image-based RL. Recently, using self-supervision\nto improve generalization in RL has also gained interest [80, 47, 55, 1, 22, 21, 72]. Notably, Zhang\net al. [80] and Agarwal et al. [ 1] propose to learn behavioral similarity embeddings via auxiliary\ntasks (bisimulation metrics and contrastive learning, respectively), and Hansen et al. [21] learn visual\ninvariances through an auxiliary prediction task. While these results are encouraging, it has also been\nshown in [29, 40, 22, 79, 41] that the best choice of auxiliary tasks depends on the particular RL\ntask, and that joint optimization with sub-optimally chosen tasks can lead to gradient interference.\nWe achieve competitive sample-efﬁciency and generalization resultswithout the need for carefully\nchosen auxiliary tasks, and our method is therefore applicable to a larger variety of RL tasks.\nData Augmentation and Randomization for RL. Our work is directly inspired by previous work\non generalization in RL by domain randomization [66, 50, 48, 52, 6] and data augmentation [36, 9,\n71, 35, 33, 51, 61, 21]. For example, Tobin et al. [66] show that a neural network trained for object\nlocalization in a simulation with randomized visual augmentations improves real world generalization.\nSimilarly, Lee et al.[36] show that application of a random convolutional layer to observations during\ntraining improve generalization in 3D navigation tasks. More recently, extensive studies on data\naugmentation [35, 33] have been conducted with RL, and conclude that, while small random crops\nand translations can improve sample efﬁciency, most data augmentations decrease sample efﬁciency\nand cause divergence. We illuminate main causes of instability, and propose a framework for data\naugmentation in deep Q-learning algorithms that drastically improves stability and generalization.\nImproving Deep Q-Learning. While deep Q-learning algorithms such as Deep Q-Networks (DQN)\n[43] have achieved impressive results in image-based RL, the temporal difference objective is known\nto have inherent instabilities when used in conjunction with function approximation and off-policy data\n[63]. Therefore, a variety of algorithmic improvements have been proposed to improve convergence\n[24, 73, 25, 23, 53, 38, 15, 14, 28]. For example, Hasselt et al. [ 24] reduce overestimation of\nQ-values by decomposing the target Q-value estimation into action selection and action evaluation\nusing separate networks. Lillicrap et al. [ 38] reduce target variance by deﬁning the target Q-network\nas a slow-moving average of the online Q-network. Our method also improves Q-value estimation,\nbut we speciﬁcally address the instability of deep Q-learning algorithms on augmented data.\n2\n3 Preliminaries\nProblem formulation. We formulate the interaction between environment and policy as a Markov\nDecision Process (MDP) [2] M= ⟨S,A,P,r,γ ⟩, where Sis the state space, Ais the action space,\nP: S×A↦→S is the state transition function that deﬁnes a conditional probability distribution\nP(·|st,at) over all possible next states given a state st ∈S and action at ∈A taken at time t,\nr: S×A↦→ R is a reward function, andγ ∈[0,1) is the discount factor. Because image observations\nonly offer partial state observability [30], we deﬁne a state st as a sequence of k+ 1 consecutive\nframes (ot,ot−1,..., ot−k), o ∈O, where Ois the high-dimensional image space, as proposed\nin Mnih et al. [ 43]. The goal is then to learn a policy π: S ↦→ Athat maximizes discounted\nreturn Rt = EΓ∼π[∑T\nt=1 γtr(st,at)] along a trajectory Γ = (s0,s1,..., sT) obtained by following\npolicy πfrom an initial state s0 ∈S to a state sT with state transitions sampled from P, and πis\nparameterized by a collection of learnable parameters θ. For clarity, we hereon generically denote\nparameterization with subscript, e.g. πθ. We further aim to learn parameters θs.t. πθ generalizes well\n(i.e., obtains high discounted return) to unseen MDPs, which is generally unfeasible without further\nassumptions about the structure of the space of MDPs. In this work, we focus on generalization to\nMDPs M= ⟨S,A,P,r,γ ⟩, where states st ∈Sare constructed from observations ot ∈O, O⊆ O\nof a perturbed observation space O(e.g. unseen visuals), and M∼ M for a space of MDPs M.\nDeep Q-Learning. Common model-free off-policy RL algorithms aim to estimate an optimal state-\naction value function Q∗: S×A↦→ R as Qθ(s,a) ≈Q∗(s,a) = max πθ E[Rt|st = s,at = a]\nusing function approximation. In practice, this is achieved by means of the single-step Bellman\nresidual\n(\nr(st,at) + γmaxa′\nt Qtgt\nψ(st+1,a′\nt)\n)\n−Qθ(st,at) [62], where ψ parameterizes a target\nstate-action value function Qtgt. We can choose to minimize this residual (also known as thetemporal\ndifference error) directly wrt θusing a mean squared error loss, which gives us the objective\nLQ(θ,ψ) = Est,at,st+1∼B\n[\n1\n2\n[(\nr(st,at) + γmax\na′\nt\nQtgt\nψ(st+1,a′\nt)\n)\n−Qθ(st,at)\n]2]\n, (1)\nwhere Bis a replay buffer with transitions collected by a behavioral policy [39]. From here, we can\nderive a greedy policy directly by selecting actions at = arg maxat Qθ(st,at). While Qtgt = Qand\nperiodically setting ψ←−θexactly recovers the objective of DQN [43], several improvements have\nbeen proposed to improve stability of Eq. 1, such as Double Q-learning [24], Dueling Q-networks\n[73], updating target parameters using a slow-moving average of the online Q-network [38]:\nψn+1 ←−(1 −ζ)ψn + ζθn (2)\nfor an iteration step nand a momentum coefﬁcient ζ ∈(0,1], and others [25, 23, 53, 14, 28]. As com-\nputing maxa′\nt Qtgt\nψ(st+1,a′\nt) in Eq. 1 is intractable for large and continuous action spaces, a number\nof prominent actor-critic algorithms that additionally learn a policy πθ(st) ≈arg maxat Qθ(st,at)\nhave therefore been proposed [38, 15, 18].\nSoft Actor-Critic (SAC) [18] is an off-policy actor-critic algorithm that learns a state-action value\nfunction Qθ and a stochastic policy πθ (and optionally a temperature parameter), where Qθ is\noptimized using a variant of the objective in Eq. 1 and πθ is optimized using a γ-discounted\nmaximum-entropy objective [84]. To improve stability, SAC is also commonly implemented using\nDouble Q-learning and the slow-moving target parameters from Eq. 2. We will in the remainder\nof this work describe our method in the context of a generic off-policy RL algorithm that learns a\nparameterized state-action value function Qθ, while we in our experiments discussed in Section 6\nevaluate of our method using SAC as base algorithm.\n4 Pitfalls of Data Augmentation in Deep Q-Learning\nIn this section, we aim to illuminate the main causes of instability from naïve application of data\naugmentation in Q-value estimation. Our goal is to learn a Q-function Qθ for an MDP Mthat\ngeneralizes to novel MDPs M∼ M with unseen visuals, and we leverage data augmentation as an\noptimality-invariant state transformation τ to induce a bisimulation relation [34, 17] between a state\ns and its transformed (augmented) counterpart saug = τ(s,ν) with parameters ν ∼V.\nDeﬁnition 1 (Optimality-Invariant State Transformation [33]). Given an MDPM, a state transforma-\ntion τ: S×V↦→S is an optimality-invariant state transformation if Q(s,a) = Q(τ(s,ν),a) ∀s ∈\nS, a ∈A, ν∈V, where ν ∈V parameterizes the transformation τ.\n3\nFollowing our deﬁnitions of M,Mfrom Section 3, we can further extend the concept of optimality-\ninvariant transformations to MDPs, noting that a change of state space (e.g. perturbed visuals) itself\ncan be described as a transformation τ: S× V↦→ Swith unknown parameters ν ∈V. If we choose\nthe set of parameters Vof a state transformation τ to be sufﬁciently large such that it intersects with\nVwith high probability, we can therefore expect to improve generalization to state and observation\nspaces not seen during training. However, while naïve application of data augmentation as in previous\nwork [35, 33, 61, 54] may potentially improve generalization, it can be harmful toQ-value estimation.\nWe hypothesize that this is primarily because it dramatically increases the size of the observed state\nspace, and consequently also increases variance Var [Q(τ(s,ν))] ≥Var [Q(s)] , ν∼V when Vis\nlarge. Concretely, we identify the following two issues:\nPitfall 1: Non-deterministic Q-target. For deep Q-learning algorithms, previous work [35, 33, 61,\n54] applies augmentation to both state saug\nt ≜ τ(st,ν) and successor state saug\nt+1 ≜ τ(st+1,ν′) where\nν,ν′∼V. Compared with DQN [43] that uses a deterministic (more precisely, periodically updated)\nQ-target, this practice introduces a non-deterministic Q-target r(st,at) + γmaxa′\nt Qtgt\nψ(saug\nt+1,a′\nt)\ndepending on the augmentation parameters ν′. As observed in the original DQN paper, high-variance\ntarget values are detrimental to Q-learning algorithms, and may cause divergence due to the “deadly\ntriad” of function approximation, bootstrapping, and off-policy learning [ 63]. This motivates the\nwork to introduce a slowly changing target network, and several other works have reﬁned theQ-target\nupdate rule [38, 15] to further reduce volatility. However, because data augmentation is inherently\nnon-deterministic, it can greatly increase variance in Q-target estimation and exacerbates the issue\nof volatility, as shown in Figure 1 (top). This is particularly troubling in actor-critic algorithms\nsuch as DDPG [38] and SAC [18], where the Q-target is estimated from (st+1,a′), a′∼π(·|st+1),\nwhich introduces an additional source of error from πthat is non-negligible especially when st+1 is\naugmented.\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n2\n4\n6\n8\n10Q-target variance\nDrQ + conv SVEA w/ conv\naffine-jitter blur conv cutout overlay rotation shift\n0\n5\n10\n15\n20Mean Q-error\nFigure 1. (Top)Mean Q-target variance of DrQ [33]\nand SVEA (ours), both trained with conv augmenta-\ntion [36]. (Bottom) Mean difference in Q-value es-\ntimation on augmented vs. non-augmented data.\nWe measure mean absolute error in Q-value estima-\ntion from converged DrQ agents (trained with shift\naugmentation) on the same observations before and\nafter augmentation. Both ﬁgures are averages across\n5 seeds for each of the 5 tasks from DMControl-GB.\nPitfall 2: Over-regularization. Data aug-\nmentation was originally introduced in the\nsupervised learning regime as a regularizer to\nprevent overﬁtting of high-capacity models.\nHowever, for RL, even learning a policy in\nthe training environment is hard. While data\naugmentation may improve generalization, it\ngreatly increases the difﬁculty of policy learn-\ning, i.e., optimizing θfor Qθ and potentially\na behavior networkπθ. Particularly, when the\ntemporal difference loss from Eq. 1 cannot\nbe well minimized, the large amount of aug-\nmented states dominate the gradient, which\nsigniﬁcantly impacts Q-value estimation of\nboth augmented and unaugmented states. We\nrefer to this issue as over-regularization by\ndata augmentation. Figure 1 (bottom) shows\nthe mean difference in Q-predictions made\nwith augmented vs. unaugmented data in\nfully converged DrQ [33] agents trained with\nshift augmentation. Augmentations such as\nafﬁne-jitter, random convolution, and random\noverlay incur large differences in estimated\nQ-values. While such difference can be re-\nduced by regularizing the optimization with\neach individual augmentation, we empha-\nsize that even the minimal shift augmentation\nused throughout training incurs non-zero dif-\nference. Since ψis commonly chosen to be a\nmoving average of θas in Eq. 2, such differences caused by over-regularization affect Qθ and Qtgt\nψ\nequally, and optimization may therefore still diverge depending on the choice of data augmentation.\nAs such, there is an inherent trade-off between accurate Q-value estimation and generalization when\nusing data augmentation. In the following section, we address these pitfalls.\n4\nTarget Q-function\nQ-function\nQ-target\nTarget encoder\nQ-prediction\n= stop-grad\n= data-mix\nEncoder\naug\nFigure 2. Overview. An observation st is transformed by data augmentation τ(·,ν), ν ∼ V\nto produce a view saug\nt . The Q-function Qθ is then jointly optimized on both augmented and\nunaugmented data wrt the objective in Eq. 7, with the Q-target of the Bellman equation computed\nfrom an unaugmented observation st+1. We illustrate our data-mixing strategy by the ⊗operator.\n5 Method\nWe propose SVEA: Stabilized Q-Value Estimation under Augmentation, a general framework for\nvisual generalization in RL by use of data augmentation. SVEA applies data augmentation in a novel\nlearning framework leveraging two data streams – with and without augmented data, respectively. Our\nmethod is compatible with any standard off-policy RL algorithm without changes to the underlying\nneural network that parameterizes the policy, and it requires no additional forward passes, auxiliary\ntasks, nor learnable parameters. While SVEA in principle does not make any assumptions about the\nstructure of states st ∈S, we here describe our method in the context of image-based RL.\n5.1 Architectural Overview\nAn overview of the SVEA architecture is provided in Figure 2. Our method leverages properties of\ncommon neural network architectures used in off-policy RL without introducing additional learnable\nparameters. We subdivide the neural network layers and corresponding learnable parameters of\na state-action value function into sub-networks fθ (denoted the state encoder) and Qθ (denoted\nthe Q-function) s.t qt ≜ Qθ(fθ(st),at) is the predicted Q-value corresponding to a given state-\naction pair (st,at). We similarly deﬁne the target state-action value function s.t. qtgt\nt ≜ r(st,at) +\nγmaxa′\nt Qtgt\nψ(f\ntgt\nψ(st+1),a′) is the target Q-value for (st,at), and we deﬁne parameters ψ as an\nexponential moving average of θas in Eq. 2. Depending on the choice of underlying algorithm, we\nmay choose to additionally learn a parameterized policy πθ that shares encoder parameters with Qθ\nand selects actions at ∼πθ(·|fθ(st)).\nTo circumvent erroneous bootstrapping from augmented data (as discussed in Section 4), we strictly\napply data augmentation in Q-value estimation of the current state st, without applying data augmen-\ntation to the successor state st+1 used in Eq. 1 for bootstrapping with Qtgt\nψ (and πθ if applicable),\nwhich addresses Pitfall 1. If πθ is learned (i.e., SVEA is implemented with an actor-critic algorithm),\nwe also optimize it strictly from unaugmented data. To mitigate over-regularization in optimization\nof fθ and Qθ (Pitfall 2), we further employ a modiﬁed Q-objective that leverages both augmented\nand unaugmented data, which we introduce in the following section.\n5.2 Learning Objective\nOur method redeﬁnes the temporal difference objective from Eq. 1 to better leverage data augmenta-\ntion. First, recall that qtgt\nt = r(st,at) + γmaxa′\nt Qtgt\nψ(f\ntgt\nψ(st+1),a′). Instead of learning to predict\nqtgt\nt only from state st, we propose to minimize a nonnegative linear combination of LQ over two\nindividual data streams, st and saug\nt = τ(st,ν), ν∼V, which we deﬁne as the objective\nLSVEA\nQ (θ,ψ) ≜ αLQ\n(\nst,qtgt\nt ; θ,ψ\n)\n+ βLQ\n(\nsaug\nt ,qtgt\nt ; θ,ψ\n)\n(3)\n= Est,at,st+1∼B\n[\nα\nQθ(fθ(st),at) −qtgt\nt\n2\n2 + β\nQθ(fθ(saug\nt ),at) −qtgt\nt\n2\n2\n]\n, (4)\nwhere α,β are constant coefﬁcients that balance the ratio of the unaugmented and augmented data\nstreams, respectively, and qtgt\nt is computed strictly from unaugmented data. LSVEA\nQ (θ,ψ) serves as a\ndata-mixing strategy that oversamples unaugmented data as an implicit variance reduction technique.\n5\nAs we will verify empirically in Section 6, data-mixing is a simple and effective technique for\nvariance reduction that works well in tandem with our proposed modiﬁcations to bootstrapping. For\nα= β, the objective in Eq. 4 can be evaluated in a single, batched forward-pass by rewriting it as:\ngt = [st,τ(st,ν)]N (5)\nht =\n[\nqtgt\nt ,qtgt\nt\n]\nN (6)\nLSVEA\nQ (θ,ψ) = Est,at,st+1∼B,ν ∼V\n[\n(α+ β) ∥Qθ(fθ(gt),at) −ht∥2\n2\n]\n, (7)\nwhere [·]N is a concatenation operator along the batch dimension N for st,saug\nt ∈RN×C×H×W\nand qtgt\nt ∈RN×1, which is illustrated as ⊗in Figure 2. Empirically, we ﬁnd α = 0.5,β = 0.5 to\nbe both effective and practical to implement, which we adopt in the majority of our experiments.\nHowever, more sophisticated schemes for selecting α,β and/or varying them as training progresses\ncould be interesting directions for future research. If the base algorithm of choice learns a policy πθ,\nits objective Lπ(θ) is optimized solely on unaugmented states st without changes to the objective,\nand a stop-grad operation is applied after fθ to prevent non-stationary gradients of Lπ(θ) from\ninterfering with Q-value estimation, i.e., only the objective from Eq. 4 or optionally Eq. 7 updates fθ\nusing stochastic gradient descent. As described in Section 5.1, parameters ψare updated using an\nexponential moving average of θand a stop-grad operation is therefore similarly applied after Qtgt\nψ.\nWe summarize our method for α= βapplied to a generic off-policy algorithm in Algorithm 1.\nAlgorithm 1 Generic SVEA off-policy algorithm ( ▶ naïve augmentation, ▶ our modiﬁcations)\nθ,θπ,ψ: randomly initialized network parameters, ψ←−θ ⊿ Initialize ψto be equal to θ\nη,ζ: learning rate and momentum coefﬁcient\nα,β: loss coefﬁcients, default: (α= 0.5,β = 0.5)\n1: for timestep t= 1...T do\nact:\n2: at ∼πθ(·|fθ(st)) ⊿ Sample action from policy\n3: s′\nt ∼P(·|st,at) ⊿ Sample transition from environment\n4: B←B∪ (st,at,r(st,at),s′\nt) ⊿ Add transition to replay buffer\nupdate:\n5: {si,ai,r(si,ai),s′\ni |i= 1...N}∼B ⊿ Sample batch of transitions\n6: si = τ(si,νi), s′\ni = τ(s′\ni,ν′\ni), νi,ν′\ni ∼V ▶ Naïve application of data augmentation\n7: for transition i= 1..N do\n8: θπ ←−θπ −η∇θπLπ(si; θπ) (if applicable) ⊿ Optimize πθ with SGD\n9: qtgt\ni = r(si,ai) + γmaxa′\ni\nQtgt\nψ(ftgt\nψ (s′\ni),a′\ni) ⊿ Compute Q-target\n10: saug\ni = τ(si,νi), νi ∼V ▶ Apply stochastic data augmentation\n11: gi =\n[\nsi,saug\ni\n]\nN , hi =\n[\nqtgt\ni ,qtgt\ni\n]\nN ▶ Pack data streams\n12: θ←−θ−η∇θLSVEA\nQ (gi,hi; θ,ψ) ▶ Optimize fθ and Qθ with SGD\n13: ψ←−(1 −ζ)ψ+ ζθ ⊿ Update ψusing EMA of θ\n6 Experiments\ntest\ntraining colors natural videos camera poses\nFigure 3. Experimental setup. Agents are trained\nin a ﬁxed environment and are expected to general-\nize to novel environments with e.g. random colors,\nbackgrounds, and camera poses.\nWe evaluate both sample efﬁciency, asymptotic\nperformance, and generalization of our method\nand a set of strong baselines using both Con-\nvNets and Vision Transformers (ViT) [ 10] in\ntasks from DeepMind Control Suite (DMCon-\ntrol) [ 64] as well as a set of robotic manip-\nulation tasks. DMControl offers challenging\nand diverse continuous control tasks and is\nwidely used as a benchmark for image-based\nRL [ 19, 20, 76, 59, 35, 33]. To evaluate generalization of our method and baselines, we test\nmethods under challenging distribution shifts (as illustrated in Figure 3) from the DMControl Gen-\neralization Benchmark (DMControl-GB) [21], the Distracting Control Suite (DistractingCS) [60],\nas well as distribution shifts unique to the robotic manipulation environment. Code is available at\nhttps://github.com/nicklashansen/dmcontrol-generalization-benchmark.\n6\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (SVEA) Walker, stand (SVEA) Cartpole, swingup (SVEA) Cartpole, balance (SVEA) Finger, spin (SVEA)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (DrQ)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nWalker, stand (DrQ)\n0.0 0.1 0.2\nNumber of frames (×106)\nCartpole, swingup (DrQ)\n0.0 0.1 0.2\nNumber of frames (×106)\nCartpole, balance (DrQ)\n0.00 0.05 0.10 0.15 0.20\nNumber of frames (×106)\nFinger, spin (DrQ)\nconv\noverlay\nblur\nrotation\ncutout\naffine-jitter\nFigure 4. Data augmentations. Training performance of SVEA (top) and DrQ (bottom) under 6\ncommon data augmentations. Mean of 5 seeds. Red line at 800 return is for visual guidance only. We\nomit visualization of std. deviations for clarity, but provide per-augmentation comparisons to DrQ\n(including std. deviations) across all tasks in Appendix B, and test performances in Appendix C.\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (training) Walker, stand (training) Cartpole, swingup (training) Ball in cup, catch (training) Finger, spin (training)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nWalker, stand (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nCartpole, swingup (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nBall in cup, catch (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nFinger, spin (evaluation)\nDrQ DrQ + conv SVEA w/ conv (data-mixing only) SVEA w/ conv ( = 0, = 1)\n SVEA w/ conv\nFigure 5. Training and test performance. We compare SVEA to DrQ with and without random\nconvolution augmentation, as well as a set of ablations. Data-mixing only indiscriminately applies\nour data-mixing strategy to all data streams, and (α= 0,β = 1) only augments Q-predictions but\nwithout data-mixing. We ﬁnd both components to contribute to SVEA’s success.Top: episode return\non the training environment during training. Bottom: generalization measured by episode return on\nthe color_hard benchmark of DMControl-GB. Mean of 5 seeds, shaded area is ±1 std. deviation.\nSetup. We implement our method and baselines using SAC [18] as base algorithm, and we apply\nrandom shift augmentation to all methods by default. This makes our base algorithm equivalent to\nDrQ [33] when K=1,M=1; we refer to the base algorithm asunaugmented and consider stability under\nadditional data augmentation. We use the same network architecture and hyperparameters for all\nmethods (whenever applicable), and adopt the setup from Hansen and Wang[21]. Observations are\nstacks of 3 RGB frames of size 84 ×84 ×3 (and 96 ×96 ×3 in ViT experiments). In the DMControl-\nGB and DistractingCS benchmarks, all methods are trained for 500k frames and evaluated on all 5\ntasks from DMControl-GB used in prior work, and we adopt the same experimental setup for robotic\nmanipulation. See Appendix H for hyperparameters and further details on our experimental setup.\nBaselines and data augmentations. We benchmark our method against the following strong base-\nlines: (1) CURL [59], a contrastive learning method for RL; (2) RAD that applies a random crop; (3)\nDrQ that applies a random shift; (4) PAD [22] that adapts to test environments using self-supervision;\n(5) SODA [21] that applies data augmentation in auxiliary learning; as well as a number of ablations.\nWe compare to the K=1,M=1 setting of DrQ by default, but also provide comparison to varying\nK,M. We experiment with a diverse set of data augmentations proposed in previous work on RL\nand computer vision, namely random shift [33], random convolution (denoted conv) [36], random\noverlay [21], random cutout [9], Gaussian blur, random afﬁne-jitter, and random rotation [35, 16].\nWe provide samples for all data augmentations in Appendix C and test environments in Appendix E.\n7\nTable 1. Comparison to state-of-the-art. Test performance (episode return) of methods trained in a\nsingle, ﬁxed environment and evaluated on (i) randomized colors, and (ii) natural video backgrounds\nfrom DMControl-GB. Results for CURL, RAD, PAD, and SODA are obtained from [ 21] and we\nreport mean and std. deviation over 5 seeds. DrQ corresponds to our SAC base algorithm using\nrandom shift augmentation. SVEA matches or outperforms prior methods in all tasks considered.\nDMControl-GB CURL RAD DrQ PAD SODA SODA SVEA SVEA\n(random colors) (conv) (overlay) (conv) (overlay)\nwalker, 445 400 520 468 697 692 760 749walk ±99 ±61 ±91 ±47 ±66 ±68 ±145 ±61\nwalker, 662 644 770 797 930 893 942 933stand ±54 ±88 ±71 ±46 ±12 ±12 ±26 ±24\ncartpole, 454 590 586 630 831 805 837 832swingup ±110 ±53 ±52 ±63 ±21 ±28 ±23 ±23\nball_in_cup, 231 541 365 563 892 949 961 959catch ±92 ±29 ±210 ±50 ±37 ±19 ±7 ±5\nfinger, 691 667 776 803 901 793 977 972spin ±12 ±154 ±134 ±72 ±51 ±128 ±5 ±6\nDMControl-GB CURL RAD DrQ PAD SODA SODA SVEA SVEA\n(natural videos) (conv) (overlay) (conv) (overlay)\nwalker, 556 606 682 717 635 768 612 819walk ±133 ±63 ±89 ±79 ±48 ±38 ±144 ±71\nwalker, 852 745 873 935 903 955 795 961stand ±75 ±146 ±83 ±20 ±56 ±13 ±70 ±8\ncartpole, 404 373 485 521 474 758 606 782swingup ±67 ±72 ±105 ±76 ±143 ±62 ±85 ±27\nball_in_cup, 316 481 318 436 539 875 659 871catch ±119 ±26 ±157 ±55 ±111 ±56 ±110 ±106\nfinger, 502 400 533 691 363 695 764 808spin ±19 ±64 ±119 ±80 ±185 ±97 ±86 ±33\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\n0 24 48 72 96 120 144 168\nWall-time (hours)\nDeepMind Control Suite (training)\nDrQ + conv [K=1,M=1]\nDrQ + conv [K=2,M=2]\nDrQ + conv [K=4,M=4]\nDrQ + conv [K=8,M=8]\nSVEA w/ conv\n0.1 0.2 0.3 0.4 0.5\nIntensity\n100\n150\n200\n250\n300\n350\n400\n450\n500Episode return\nDistracting Control Suite\nDrQ SVEA w/ conv SVEA w/ overlay\nFigure 6. (Left) Comparison with additional DrQ baselines. We compare SVEA implemented\nwith DrQ [K=1,M=1] as base algorithm to DrQ with varying values of its K,M hyperparameters.\nAll methods use the conv augmentation (in addition to shift augmentation used by DrQ). Results are\naveraged over 5 seeds for each of the 5 tasks from DMControl-GB [21] and shaded area is ±1 std.\ndeviation across seeds. Increasing values of K,M improve sample efﬁciency of DrQ, but at a high\ncomputational cost; DrQ uses approx. 6x wall-time to match the sample efﬁciency of SVEA. (Right)\nDistractingCS. Episode return as a function of randomization intensity at test-time, aggregated\nacross 5 seeds for each of the 5 tasks from DMControl-GB. See Appendix E for per-task comparison.\n6.1 Stability and Generalization on DMControl\nStability. We evaluate the stability of SVEA and DrQ under 6 common data augmentations; results are\nshown in Figure 4. While the sample efﬁciency of DrQ degrades substantially for most augmentations,\nSVEA is relatively unaffected by the choice of data augmentation and improves sample efﬁciency in\n27 out of 30 instances. While the sample efﬁciency of DrQ can be improved by increasing its K,M\nparameters, we ﬁnd that DrQ requires approx. 6x wall-time to match the sample efﬁciency of SVEA;\nsee Figure 6 (left). We further ablate each component of SVEA and report both training and test\ncurves in Figure 5; we ﬁnd that both components are key to SVEA’s success. Because we empirically\nﬁnd the conv augmentation to be particularly difﬁcult to optimize, we provide additional stability\nexperiments in Section 6.2 and 6.3 using this augmentation. See Appendix A for additional ablations.\n8\nGeneralization. We compare the test performance of SVEA to 5 recent state-of-the-art methods for\nimage-based RL on the color_hard and video_easy benchmarks from DMControl-GB (results\nin Table 1), as well as the extremely challenging DistractingCS benchmark, where camera pose,\nbackground, and colors are continually changing throughout an episode (results in Figure 6 (right)).\nWe here use conv and overlay augmentations for fair comparison to SODA, and we report additional\nresults on the video_hard benchmark in Appendix F. SVEA outperforms all methods considered\nin 12 out of 15 instances on DMControl-GB, and at a lower computational cost than CURL, PAD,\nand SODA that all learn auxiliary tasks. On DistractingCS, we observe that SVEA improves\ngeneralization by 42% at low intensity, and its generalization degrades signiﬁcantly slower than DrQ\nfor high intensities. While generalization depends on the particular choice of data augmentation\nand test environments, this is an encouraging result considering that SVEA enables efﬁcient policy\nlearning with stronger augmentations than previous methods.\n6.2 RL with Vision Transformers\n… \n8x8 patches\n ViT encoder Features\n… *0 1 2 n\nTransformer encoder\nLinear embedding\n… \n4x\nViT encoder Transformer encoder\nLayerNorm\nLayerNorm\nMulti-Head Attention\nMLP\naug\n… \n= data-mix\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (training) Cartpole, swingup (training) Cartpole, balance (training)\n0.0 0.1 0.2 0.3\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (evaluation)\n0.0 0.1 0.2 0.3\nNumber of frames (×106)\nCartpole, swingup (evaluation)\n0.0 0.1 0.2 0.3\nNumber of frames (×106)\nCartpole, balance (evaluation)\nDrQ DrQ + conv SVEA w/ conv (CNN) SVEA w/ conv\nFigure 7. (Top) ViT architecture. Observations are\ndivided into 144 non-overlapping space-time patches\nand linearly projected into tokens. Each token uses a\nlearned positional encoding and we also use a learnable\nclass token as in [ 10]. The ViT encoder consists of\n4 stacked Transformer encoders [ 69]. (Bottom) RL\nwith a ViT encoder. Training and test performance of\nSVEA and DrQ using ViT encoders. We report results\nfor three tasks and test performance is evaluated on the\ncolor_hard benchmark of DMControl-GB. Mean of 5\nseeds, shaded area is ±1 std. deviation.\nVision Transformers (ViT) [ 10] have re-\ncently achieved impressive results on down-\nstream tasks in computer vision. We re-\nplace all convolutional layers from the\nprevious experiments with a 4-layer ViT\nencoder that operates on raw pixels in\n8 ×8 space-time patches, and evaluate our\nmethod using data augmentation in con-\njunction with ViT encoders. Importantly,\nwe design the ViT architecture such that it\nroughly matches our CNN encoder in terms\nof learnable parameters. The ViT encoder\nis trained from scratch using RL, and we\nuse the same experimental setup as in our\nConvNet experiments. In particular, it is\nworth emphasizing that both our ViT and\nCNN encoders are trained using Adam [32]\nas optimizer and without weight decay. See\nFigure 7 (top) for an architectural overview,\nand refer to Appendix H for additional im-\nplementation details.\nOur training and test results are shown in\nFigure 7 (bottom). We are, to the best\nof our knowledge, the ﬁrst to success-\nfully solve image-based RL tasks without\nCNNs. We observe that DrQ overﬁts sig-\nniﬁcantly to the training environment com-\npared to its CNN counterpart ( 94 test re-\nturn on color_hard for DrQ with ViT vs.\n569 with a ConvNet on the Walker, walk\ntask). SVEA achieves comparable sam-\nple efﬁciency and improves generalization\nby 706% and 233% on Walker, walkand\nCartpole, swingup, respectively, over DrQ,\nwhile DrQ + conv remains unstable. In-\nterestingly, we observe that our ViT-based\nimplementation of SVEA achieves a mean\nepisode return of 877 on the color_hard\nbenchmark of the challenging Walker, walk\ntask (vs. 760 using CNNs). SVEA might\ntherefore be a promising technique for fu-\nture research on RL with CNN-free architectures, where data augmentation appears to be especially\nimportant for generalization. We provide additional experiments with ViT encoders in Section 6.3\nand make further comparison to ConvNet encoders in Appendix A.\n9\nTable 2. Generalization in robotic manipulation. Task success rates of SVEA and DrQ with CNN\nand ViT encoders in the training environment, as well as aggregated success rates across 25 different\ntest environments with randomized camera pose, colors, lighting, and background. Mean of 5 seeds.\nRobotic Arch. reach reach mv.tgt. mv.tgt. push push\nmanipulation (encoder) (train) (test) (train) (test) (train) (test)\nDrQ CNN 1.00 0.60 1.00 0.69 0.76 0.26\nDrQ + conv CNN 0.59 0 .77 0.60 0 .89 0.13 0 .12\nSVEAw/ conv CNN 1.00 0 .89 1.00 0 .96 0.72 0.48\nDrQ ViT 0.93 0 .14 1.00 0.16 0.73 0 .05\nDrQ + conv ViT 0.26 0 .67 0.48 0.82 0.08 0 .07\nSVEAw/ conv ViT 0.98 0 .71 1.00 0.81 0.82 0 .17\n6.3 Robotic Manipulation\ntest\ntraining randomized environments\nFigure 8. Robotic manipulation. Agents are trained in\na ﬁxed environment and evaluated on challenging environ-\nments with randomized colors, lighting, background, and\ncamera pose.\n0 10 20 30 40 50\nNumber of frames (×103)\n10\n0\n10\n20\n30\n40\n50\nEpisode return\nRobot, reach (training)\n0 10 20 30 40 50\nNumber of frames (×103)\nRobot, reach mv. tgt. (training)\n0 100 200 300\nNumber of frames (×103)\nRobot, push (training)\nDrQ DrQ + conv SVEA w/ conv\nFigure 9. Stability with a CNN encoder. Training per-\nformance (episode return) of SVEA and DrQ in 3 robotic\nmanipulation tasks. Mean and std. deviation of 5 seeds.\nSuccess rates are shown in Table 2.\n0 10 20 30 40 50\nNumber of frames (×103)\n10\n0\n10\n20\n30\n40\n50\nEpisode return\nRobot, reach (training)\n0 10 20 30 40 50\nNumber of frames (×103)\nRobot, reach mv. tgt. (training)\n0 100 200 300\nNumber of frames (×103)\nRobot, push (training)\nDrQ DrQ + conv SVEA w/ conv\nFigure 10. Stability with a ViT encoder. Training per-\nformance (episode return) of SVEA and DrQ in 3 robotic\nmanipulation tasks. Mean and std. deviation of 5 seeds. Suc-\ncess rates are shown in Table 2. DrQ is especially unstable\nunder augmentation when using a ViT encoder.\nWe additionally consider a set of\ngoal-conditioned robotic manipula-\ntion tasks using a simulated Kinova\nGen3 arm: (i) reach, a task in which\nthe robot needs to position its gripper\nabove a goal indicated by a red mark;\n(ii) reach moving target, a task sim-\nilar to (i) but where the robot needs\nto follow a red mark moving contin-\nuously in a zig-zag pattern at a ran-\ndom velocity; and (iii) push, a task\nin which the robot needs to push a\ncube to a red mark. The initial conﬁg-\nuration of gripper, object, and goal is\nrandomized, the agent uses 2D posi-\ntional control, and policies are trained\nusing dense rewards. Observations\nare stacks of RGB frames with no ac-\ncess to state information. Training and\ntest environments are shown in Figure\n8. See Appendix G for further details\nand environment samples.\nResults are shown in Figure 9 and\nFigure 10. For both CNN and ViT\nencoders, SVEA trained with conv\naugmentation has similar sample ef-\nﬁciency and training performance as\nDrQ trained without augmentation,\nwhile DrQ + conv exhibits poor sam-\nple efﬁciency and fails to solve the\npush task. Generalization results are\nshown in Table 2. We ﬁnd that naïve\napplication of data augmentation has\na higher success rate in test environ-\nments than DrQ, despite being less\nsuccessful in the training environment, which we conjecture is because it is optimized only from\naugmented data. Conversely, SVEA achieves high success rates during both training and testing.\nConclusion. SVEA is found to greatly improve both stability and sample efﬁciency under augmenta-\ntion, while achieving competitive generalization results. Our experiments indicate that our method\nscales to ViT-based architectures, and it may therefore be a promising technique for large-scale RL\nexperiments where data augmentation is expected to play an increasingly important role.\nBroader Impact. While our contribution aims to reduce computational cost of image-based RL, we\nremain concerned about the growing ecological and economical footprint of deep learning – and RL\nin particular – with increasingly large models such as ViT; see Appendix J for further discussion.\n10\nAcknowledgments and Funding Transparency. This work was supported by grants from DARPA\nLwLL, NSF CCF-2112665 (TILOS), NSF 1730158 CI-New: Cognitive Hardware and Software\nEcosystem Community Infrastructure (CHASE-CI), NSF ACI-1541349 CC*DNI Paciﬁc Research\nPlatform, NSF grant IIS-1763278, NSF CCF-2112665 (TILOS), as well as gifts from Qualcomm,\nTuSimple and Picsart.\nReferences\n[1] Rishabh Agarwal, Marlos C. Machado, P. S. Castro, and Marc G. Bellemare. Contrastive behavioral\nsimilarity embeddings for generalization in reinforcement learning. ArXiv, abs/2101.05265, 2021.\n[2] Richard Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5):679–684,\n1957.\n[3] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers\nof stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency. Association for Computing Machinery, 2021.\n[4] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Den-\nnison, David Farhi, Quirin Fischer, et al. Dota 2 with large scale deep reinforcement learning. ArXiv,\nabs/1912.06680, 2019.\n[5] T. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, et al. Language models are few-shot learners. arXiv, abs/2005.14165, 2020.\n[6] Yevgen Chebotar, A. Handa, Viktor Makoviychuk, M. Macklin, J. Issac, Nathan D. Ratliff, and D. Fox.\nClosing the sim-to-real loop: Adapting simulation randomization with real world experience. 2019\nInternational Conference on Robotics and Automation (ICRA), pages 8973–8979, 2019.\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations, 2020.\n[8] K. Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quantifying generalization\nin reinforcement learning. In Icml, 2019.\n[9] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in\nreinforcement learning, 2018.\n[10] A. Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nM. Dehghani, Matthias Minderer, et al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. ArXiv, abs/2010.11929, 2020.\n[11] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight:\nModel-based deep reinforcement learning for vision-based robotic control, 2018.\n[12] Lasse Espeholt, Hubert Soyer, R. Munos, K. Simonyan, V . Mnih, Tom Ward, Yotam Doron, Vlad Firoiu,\net al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. ArXiv,\nabs/1802.01561, 2018.\n[13] Jesse Farebrother, Marlos C. Machado, and Michael H. Bowling. Generalization and regularization in dqn.\nArXiv, abs/1810.00123, 2018.\n[14] Meire Fortunato, M. G. Azar, Bilal Piot, Jacob Menick, Ian Osband, A. Graves, Vlad Mnih, R. Munos,\net al. Noisy networks for exploration. ArXiv, abs/1706.10295, 2018.\n[15] Scott Fujimoto, H. V . Hoof, and D. Meger. Addressing function approximation error in actor-critic methods.\nArXiv, abs/1802.09477, 2018.\n[16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting\nimage rotations, 2018.\n[17] R. Givan, T. Dean, and M. Greig. Equivalence notions and model minimization in markov decision\nprocesses. Artiﬁcial Intelligence, 147:163–223, 2003.\n[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.\n[19] Danijar Hafner, T. Lillicrap, Ian S. Fischer, R. Villegas, David R Ha, H. Lee, and James Davidson. Learning\nlatent dynamics for planning from pixels. ArXiv, abs/1811.04551, 2019.\n[20] Danijar Hafner, T. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors\nby latent imagination. ArXiv, abs/1912.01603, 2020.\n[21] Nicklas Hansen and Xiaolong Wang. Generalization in reinforcement learning by soft data augmentation.\nIn International Conference on Robotics and Automation, 2021.\n11\n[22] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto,\nand Xiaolong Wang. Self-supervised policy adaptation during deployment. In International Conference on\nLearning Representations, 2021.\n[23] H. V . Hasselt, A. Guez, Matteo Hessel, V . Mnih, and D. Silver. Learning values across many orders of\nmagnitude. In Nips, 2016.\n[24] H. V . Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. InAaai, 2016.\n[25] M. Hausknecht and P. Stone. Deep recurrent q-learning for partially observable mdps. In AAAI Fall\nSymposia, 2015.\n[26] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.\n[27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsu-\npervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 9726–9735, 2020.\n[28] Matteo Hessel, Joseph Modayil, H. V . Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,\nBilal Piot, et al. Rainbow: Combining improvements in deep reinforcement learning. In Aaai, 2018.\n[29] Max Jaderberg, V olodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver,\nand Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks, 2016.\n[30] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially\nobservable stochastic domains. Artiﬁcial Intelligence, 1998.\n[31] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, et al. Qt-\nopt: Scalable deep reinforcement learning for vision-based robotic manipulation. ArXiv, abs/1806.10293,\n2018.\n[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,\n2015.\n[33] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. International Conference on Learning Representations, 2020.\n[34] K. G. Larsen and A. Skou. Bisimulation through probabilistic testing (preliminary report). In Proceedings\nof the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, 1989.\n[35] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforce-\nment learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.\n[36] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. A simple randomization technique for generaliza-\ntion in deep reinforcement learning. ArXiv, abs/1910.05396, 2019.\n[37] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor\npolicies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\n[38] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa, D. Silver, and Daan Wierstra. Continuous\ncontrol with deep reinforcement learning. CoRR, abs/1509.02971, 2016.\n[39] L. J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine\nLearning, 8:293–321, 2004.\n[40] Xingyu Lin, Harjatin Singh Baweja, George Kantor, and David Held. Adaptive auxiliary task weighting\nfor reinforcement learning. Advances in neural information processing systems, 32, 2019.\n[41] Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the effect of auxiliary tasks on\nrepresentation dynamics. In Aistats, 2021.\n[42] P. Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil,\nR. Goroshin, et al. Learning to navigate in complex environments. ArXiv, abs/1611.03673, 2017.\n[43] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,\nand Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602,\n2013.\n[44] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual\nreinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, pages\n9191–9200, 2018.\n[45] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\npuzzles. In European Conference on Computer Vision, pages 69–84. Springer, 2016.\n[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:\nFeature learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2536–2544, 2016.\n12\n[47] Deepak Pathak, Dhiraj Gandhi, and A. Gupta. Self-supervised exploration via disagreement. ArXiv,\nabs/1906.04161, 2019.\n[48] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of\nrobotic control with dynamics randomization. 2018 IEEE International Conference on Robotics and\nAutomation (ICRA), May 2018.\n[49] Lerrel Pinto and Abhinav Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700\nrobot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages 3406–3413.\nIeee, 2016.\n[50] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric\nactor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017.\n[51] Roberta Raileanu, M. Goldstein, Denis Yarats, Ilya Kostrikov, and R. Fergus. Automatic data augmentation\nfor generalization in deep reinforcement learning. ArXiv, abs/2006.12862, 2020.\n[52] Fabio Ramos, Rafael Possas, and Dieter Fox. Bayessim: Adaptive domain randomization via probabilistic\ninference for robotics simulators. Robotics: Science and Systems XV, Jun 2019.\n[53] Tom Schaul, John Quan, Ioannis Antonoglou, and D. Silver. Prioritized experience replay. CoRR,\nabs/1511.05952, 2016.\n[54] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman.\nData-efﬁcient reinforcement learning with self-predictive representations.arXiv preprint arXiv:2007.05929,\n2020.\n[55] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.\nPlanning to explore via self-supervised world models, 2020.\n[56] Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-\nsupervision for reinforcement learning. ArXiv, abs/1612.07307, 2017.\n[57] Connor Shorten and T. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of\nBig Data, 6:1–48, 2019.\n[58] Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overﬁtting in\nreinforcement learning. ArXiv, abs/1912.02975, 2020.\n[59] Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representations for\nreinforcement learning. arXiv preprint arXiv:2004.04136, 2020.\n[60] Austin Stone, Oscar Ramirez, K. Konolige, and Rico Jonschkowski. The distracting control suite - a\nchallenging benchmark for reinforcement learning from pixels. ArXiv, abs/2101.02722, 2021.\n[61] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from\nreinforcement learning. ArXiv, abs/2004.1499, 2020.\n[62] R. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 2005.\n[63] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second\nedition, 2018.\n[64] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,\nAbbas Abdolmaleki, et al. Deepmind control suite. Technical report, DeepMind, 2018.\n[65] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\narXiv:1906.05849, 2019.\n[66] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain\nrandomization for transferring deep neural networks from simulation to the real world. 2017 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), Sep 2017.\n[67] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033, 2012. doi:\n10.1109/iros.2012.6386109.\n[68] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding, 2018.\n[69] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv, abs/1706.03762, 2017.\n[70] Oriol Vinyals, I. Babuschkin, Wojciech Czarnecki, Micha\n\"el Mathieu, Andrew Dudzik, J. Chung, D. Choi, Richard Powell, et al. Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, pages 1–5, 2019.\n[71] K. Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforcement learning\nwith mixture regularization. ArXiv, abs/2010.10814, 2020.\n13\n[72] Xudong Wang, Long Lian, and Stella X. Yu. Unsupervised visual attention and invariance for reinforcement\nlearning. ArXiv, abs/2104.02921, 2021.\n[73] Ziyu Wang, Tom Schaul, Matteo Hessel, H. V . Hasselt, Marc Lanctot, and N. D. Freitas. Dueling network\narchitectures for deep reinforcement learning. ArXiv, abs/1511.06581, 2016.\n[74] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-\nparametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3733–3742, 2018.\n[75] Zhenlin Xu, Deyi Liu, Junlin Yang, and M. Niethammer. Robust and generalizable visual representation\nlearning via random convolutions. ArXiv, abs/2007.13003, 2020.\n[76] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving\nsample efﬁciency in model-free reinforcement learning from images, 2019.\n[77] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical\nrepresentations. arXiv preprint arXiv:2102.11271, 2021.\n[78] Sarah Young, Dhiraj Gandhi, Shubham Tulsiani, Abhinav Gupta, Pieter Abbeel, and Lerrel Pinto. Visual\nimitation made easy. CoRL, 2020.\n[79] Tianhe Yu, Saurabh Kumar, A. Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery\nfor multi-task learning. ArXiv, abs/2001.06782, 2020.\n[80] A. Zhang, Rowan McAllister, R. Calandra, Y . Gal, and Sergey Levine. Learning invariant representations\nfor reinforcement learning without reconstruction. ArXiv, abs/2006.10742, 2020.\n[81] C. Zhang, Oriol Vinyals, R. Munos, and S. Bengio. A study on overﬁtting in deep reinforcement learning.\nArXiv, abs/1804.06893, 2018.\n[82] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on\ncomputer vision, pages 649–666. Springer, 2016.\n[83] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi.\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. In Icra, pages 3357–\n3364. Ieee, 2017.\n[84] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse\nreinforcement learning. In Proceedings of the 23rd National Conference on Artiﬁcial Intelligence, volume 3,\n2008.\n14\nA Ablations\nWe ablate the design choices of SVEA and compare both training and test performance to DrQ and\nRAD. Results are shown in Table 3. We ﬁnd that our proposed formulation of SVEA outperforms\nthe test performance of all other variants, and by a large margin (method 2). Using a ViT encoder\n(method 1) instead of a CNN further improves both the training and test performance of SVEA,\nwhereas the test performance of DrQ decreases by a factor of 5 when using a ViT encoder (method\n7). This indicates that ViT encoders overﬁt heavily to the training environment without the strong\naugmentation of SVEA. We observe that both DrQ and RAD are unstable under strong augmentation\n(method 10 and 12). While the test performance of DrQ does not beneﬁt from using a ViT encoder,\nwe observe a slight improvement in training performance (method 7), similar to that of SVEA.\nTable 3. Ablations. We vary the following choices: (i) architecture of the encoder; (ii) our proposed\nobjective LSVEA\nQ as opposed to LQ or a mix-all objective that uses two data-streams for both Q-\npredictions, Q-targets, and π; (iii) using strong augmentation (conv) in addition to the random shift\naugmentation used by default (abbreviated as Str. aug.); and (iv) whether the target is augmented or\nnot (abbreviated as Aug. tgt.). We report mean episode return in the training and test environments\n(color_hard) of the Walker, walktask. Method 1 and 2 are the default formulations of SVEA using\nViT and CNN encoders, respectively, method 7 and 8 are the default formulations of DrQ using Vit\nand CNN encoders, respectively, and method 11 is the default formulation of RAD that uses a random\ncrop augmentation and is implemented using a CNN encoder. Mean and std. deviation. of 5 seeds.\nMethod Arch. Objective Str. aug. Aug. tgt. Train return Test return\n1 SVEA ViT SVEA ! % 918±57 877±54\n2 − CNN SVEA ! % 833±91 760±145\n3 − CNN SVEA ! ! 872±53 605±148\n4 − CNN mix-all ! ! 927±24 599±214\n5 − CNN Q ! % 596±55 569±139\n6 − CNN Q % % 771±317 498±196\n7 DrQ ViT Q % ! 920±36 94±18\n8 − CNN Q % ! 892±65 520±91\n9 − ViT Q ! ! 286±225 470±67\n10 − CNN Q ! ! 560±158 569±139\n11 RAD CNN Q % ! 883±23 400±61\n12 − CNN Q ! ! 260±201 246±184\nB Stability under Data Augmentation\nFigure 11 compares the sample efﬁciency and stability of SVEA and DrQ under each of the 6\nconsidered data augmentations for 5 tasks from DMControl. We observe that SVEA improves\nstability in all 27 instances where DrQ is impaired by data augmentation. Stability of DrQ under data\naugmentation is found to be highly sensitive to both the choice of augmentation and the particular task.\nFor example, the DrQ + aug baseline is relatively unaffected by a majority of data augmentations in\nthe Walker, standtask, while we observe signiﬁcant instability across all data augmentations in the\nCartpole, swingup task. Our results therefore indicate that SVEA can be a highly effective method\nfor eliminating the need for costly trial-and-error associated with application of data augmentation.\nC Data Augmentation in RL\nApplication of data augmentation in image-based RL has proven highly successful [9, 36, 35, 33, 61,\n21, 51] in improving generalization by regularizing the network parameterizing the Q-function and\npolicy π. However, not all augmentations are equally effective. Laskin et al. [35] and Kostrikov et\nal. [33] ﬁnd small random crops and random shifts (image translations) to greatly improve sample\nefﬁciency of image-based RL, but they empirically offer no signiﬁcant improvement in generalization\nto other environments [ 21, 60]. On the other hand, augmentations such as random convolution\n[36] have shown great potential in improving generalization, but is simultaneously found to cause\ninstability and poor sample efﬁciency [35, 21]. In this context, it is useful to distinguish between\n15\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk\nWalker, stand\nCartpole, swingup\nRandom convolution\nCartpole, balance\nFinger, spin\n0\n200\n400\n600\n800\n1000Episode return\nRandom blur\n0\n200\n400\n600\n800\n1000Episode return\nRandom cutout\n0\n200\n400\n600\n800\n1000Episode return\nRandom overlay\n0\n200\n400\n600\n800\n1000Episode return\nRandom rotation\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0.0 0.1 0.2\nNumber of frames (×106)\nRandom affine-jitter\n0.0 0.1 0.2\nNumber of frames (×106)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nDrQ DrQ + aug SVEA w/ aug\nFigure 11. Stability under data augmentation. Training performance measured by episode return\nof SVEA and DrQ under 6 common data augmentations (using ConvNets). We additionally provide\nreference curves for DrQ without additional augmentation. Mean of 5 seeds, shaded area is ±1 std.\ndeviation. SVEA obtains similar sample efﬁciency to DrQ without augmentation, while the sample\nefﬁciency of DrQ + aug is highly dependent on the task and choice of augmentation.\n16\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (training) Cartpole, swingup (training) Cartpole, balance (training)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nCartpole, swingup (evaluation)\n0.0 0.1 0.2\nNumber of frames (×106)\nCartpole, balance (evaluation)\nconv\noverlay\nblur\nrotation\ncutout\naffine-jitter\nFigure 12. Generalization depends on the choice of data augmentation. A comparison of SVEA\nimplemented using each of the 6 data augmentations considered in this work (using ConvNets). SVEA\nexhibits comparable stability and sample efﬁciency for all augmentations, but generalization ability is\nhighly dependent on the choice of augmentation. Top: episode return on the training environment\nduring training. Bottom: generalization measured by episode return on the color_hard benchmark\nof DMControl-GB. Mean of 5 seeds, shaded area is ±1 std. deviation.\nweak augmentations such as small random translations that improve sample efﬁciency due to their\nregularization, and strong augmentations such as random convolution that improve generalization at\nthe expense of sample efﬁciency. In this work, we focus on stabilizing deep Q-learning under strong\ndata augmentation with the goal of improving generalization.\nFigure 12 shows training and test performance of SVEA implemented using each of the 6 data\naugmentations considered in this work. SVEA exhibits comparable stability and sample efﬁciency\nfor all augmentations, but we ﬁnd that generalization ability on the color_hard benchmark of\nDMControl-GB is highly dependent on the choice of augmentation. Generally, we observe that\naugmentations such as conv, overlay, and afﬁne-jitter achieve the best generalization, but they\nempirically also cause the most instability in our DrQ + aug baseline as shown in Figure 11.\nFigure 13 provides a comprehensive set of samples for each of the data augmentations considered\nin this study: random shift [33], random convolution (denoted conv) [36], random overlay [21],\nrandom cutout [9], Gaussian blur, random afﬁne-jitter, and random rotation [35]. We emphasize\nthat the random convolution augmentation is not a convolution operation, but rather application of\na randomly initialized convolutional layer as in the original proposal [ 36]. As in previous work\n[35, 33, 21] that applies data augmentation to image-based RL, we either clip values or apply a\nlogistic function, whichever is more appropriate, to ensure that output values remain within the [0,1)\ninterval that unaugmented observations are normalized to. Each of the considered data augmentations\nare applied to the walker and cartpole environments and are representative of theWalker, walk, Walker,\nstand, Cartpole, swingup, and Cartpole, balance tasks. To illustrate the diversity of augmentation\nparameters associated with a given transformation, we provide a total of 6 samples for each data\naugmentation in each of the two environments. Note that, while random shift has been shown to\nimprove sample efﬁciency in previous work, it provides very subtle randomization. Stronger and\nmore varied augmentations such as random convolution, random overlay, and afﬁne-jitter can be\nexpected to improve generalization to a larger set of MDPs, but naïve application of these data\naugmentations empirically results in optimization difﬁculties and poor sample efﬁciency.\nD Choice of Base Algorithm\nIn the majority of our results, we implement SVEA using SAC as base algorithm, implemented\nwith a random shift augmentation as proposed by DrQ [33]. We now consider an additional set of\nexperiments where we instead implement SVEA using RAD [35] as base algorithm, which proposes\nto add a random cropping to SAC (in place of random shift). Training and test performances on\ncolor_hard are shown in Figure 15. We ﬁnd SVEA to provide similar performance and beneﬁts in\nterms of sample efﬁciency and generalization as when using DrQ as base algorithm. RAD likewise\n17\n(a) No augmentation (walker).\n (b) No augmentation (cartpole).\n(c) Random shift (walker).\n (d) Random shift (cartpole).\n(e) Random convolution (walker).\n (f) Random convolution (cartpole).\n(g) Random overlay (walker).\n (h) Random overlay (cartpole).\n(i) Random cutout (walker).\n (j) Random cutout (cartpole).\n(k) Random blur (walker).\n (l) Random blur (cartpole).\n(m) Random afﬁne-jitter (walker).\n (n) Random afﬁne-jitter (cartpole).\n(o) Random rotation (walker).\n (p) Random rotation (cartpole).\nFigure 13. Data augmentation. Visualizations of all data augmentations considered in this study.\nLeft column contains samples from the Walker, walk and Walker, stand tasks, and right column\ncontains samples from the Cartpole, swingup and Cartpole, balance tasks.\n0.1 0.2 0.3 0.4 0.5\nIntensity\n0\n100\n200\n300\n400\n500\n600\n700Episode return\nWalker, walk\n0.1 0.2 0.3 0.4 0.5\nIntensity\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900Episode return\nWalker, stand\n0.1 0.2 0.3 0.4 0.5\nIntensity\n0\n50\n100\n150\n200\n250\n300Episode return\nCartpole, swingup\n0.1 0.2 0.3 0.4 0.5\nIntensity\n0\n50\n100\n150\n200\n250\n300Episode return\nBall in cup, catch\n0.1 0.2 0.3 0.4 0.5\nIntensity\n0\n100\n200\n300\n400\n500Episode return\nFinger, spin\nDrQ SVEA w/ conv SVEA w/ overlay\nFigure 14. DistractingCS. Episode return as a function of randomization intensity, for each of the\n5 tasks from DMControl-GB (using ConvNets). Mean of 5 seeds. We ﬁnd that the difﬁculty of\nDistractingCS varies greatly between tasks, but SVEA consistently outperforms DrQ in terms of\ngeneralization across all intensities and tasks, except for Ball in cup, catch at the highest intensity.\n18\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (training) Walker, stand (training) Cartpole, swingup (training) Ball in cup, catch (training) Finger, spin (training)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\n0\n200\n400\n600\n800\n1000Episode return\nWalker, walk (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nWalker, stand (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nCartpole, swingup (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nBall in cup, catch (evaluation)\n0.0 0.1 0.2 0.3 0.4 0.5\nNumber of frames (×106)\nFinger, spin (evaluation)\nRAD RAD + conv SVEA w/ conv\nFigure 15. Choice of base algorithm. We compare SVEA implemented with RAD as base algorithm\nto instances of RAD with and without random convolution augmentation (using ConvNets). Top:\nepisode return on the training environment during training. Bottom: generalization measured by\nepisode return on the color_hard benchmark of DMControl-GB. Mean of 5 seeds, shaded area is\n±1 std. deviation. SVEA improves generalization in all instances.\nhas similar performance to DrQ without use of strong augmentation, however, we observe that RAD\ngenerally is more unstable than DrQ when additionally using conv augmentation, and the relative\nimprovement of SVEA is therefore comparably larger in our RAD experiments.\nE Test Environments\nFigure 16 provides visualizations for each of the two generalization benchmarks, DMControl Gener-\nalization Benchmark [21] and Distracting Control Suite [60], used in our experiments. Agents are\ntrained in a ﬁxed training environment with no visual variation, and are expected to generalize to\nnovel environments of varying difﬁculty and factors of variation. The color_hard, video_easy,\nand video_hard benchmarks are from DMControl Generalization Benchmark, and we further\nprovide samples from the Distracting Control Suite (DistractingCS) benchmark for intensities\nI = {0.1,0.2,0.5}. While methods are evaluated on a larger set of intensities, we here provide\nsamples deemed representative of the intensity scale. We note that the DistractingCS benchmark\nhas been modiﬁed to account for action repeat (frame-skip). Dynamically changing the environment\nat each simulation step makes the benchmark disproportionally harder for tasks that use a large\naction repeat, e.g. Cartpole tasks. Therefore, we choose to modify the DistractingCS benchmark\nand instead update the distractors every second simulation step, corresponding to the lowest action\nrepeat used (2, in Finger, spin). This change affects both SVEA and baselines equally. Figure 14\nshows generalization results on DistractingCS for each task individually. We ﬁnd that the difﬁculty\nof DistractingCS varies greatly between tasks, but SVEA consistently outperforms DrQ in terms of\ngeneralization across all intensities and tasks.\nF Additional Results on DMControl-GB\nTable 1 contains results for the color_hard and video_easy generalization benchmarks from\nDMControl-GB. We here provide additional results for thevideo_hard benchmark, and note that\nwe leave out the color_easy benchmark because it is already considered solved by previous work\n[21, 72]. Results are shown in Table 4. SVEA also achieves competitive performance across all 5\ntasks in the video_hard benchmark.\nG Robotic Manipulation Tasks\nWe conduct experiments with a set of three goal-conditioned robotic manipulation tasks: (i) reach,\na task in which the robot needs to position its gripper above a goal indicated by a red mark; (ii)\nreach moving target, a task similar to (i) but where the robot needs to follow a red mark moving\ncontinuously in a zig-zag pattern at a random velocity; and (iii) push, a task in which the robot\nneeds to push a cube to a red mark. We implement the tasks using MuJoCo [67] for simulation, and\n19\n(a) Training environment (walker).\n (b) Training environment (cartpole).\n(c) color_hard environment (walker).\n (d) color_hard environment (cartpole).\n(e) video_easy environment (walker).\n (f) video_easy environment (cartpole).\n(g) video_hard environment (walker).\n (h) video_hard environment (cartpole).\n(i) DistractingCS for intensity 0.1 (walker).\n (j) DistractingCS for intensity 0.1 (cartpole).\n(k) DistractingCS for intensity 0.2 (walker).\n (l) DistractingCS for intensity 0.2 (cartpole).\n(m) DistractingCS for intensity 0.5 (walker).\n (n) DistractingCS for intensity 0.5 (cartpole).\nFigure 16. Test environments. Samples from each of the two generalization benchmarks, DMControl\nGeneralization Benchmark [21] and Distracting Control Suite [60], considered in this study. In our\nexperiments, agents are trained in a ﬁxed training environment with no visual variation, and are\nexpected to generalize to novel environments of varying difﬁculty and factors of variation.\nwe use a simulated Kinova Gen3 robotic arm. The initial conﬁguration of gripper, cube, and goal\nis randomized, the agent uses 2D positional control, and policies are trained using dense rewards.\nFor reach and reach moving target, at each time step there is a reward penalty proportional to the\neuclidean distance between the gripper and the goal, and there is a reward bonus of +1 when the\ndistance is within a ﬁxed threshold corresponding to the radius of the red mark. We use the same\nreward structure for the push task, but use the euclidean distance between the cube and the goal.\nEach episode consists of 50 time steps, which makes 50 an upper bound on episode return, while\nthere is no strict lower bound. All observations are stacks of three RGB frames of size 84 ×84 ×3,\nand the agent has no access to state information. During training, the camera position is ﬁxed, and the\ncamera orientation follows the gripper. During testing, the camera orientation still follows the gripper,\nbut we additionally randomize the camera position, as well as colors, lighting, and background of the\nenvironment. Samples for the robotic manipulation tasks – both in the training environment and the\nrandomized test environments – are shown in Figure 17. We use a binary threshold for measuring\ntask success: the episode is considered a success if the environment is in success state (i.e. either the\ngripper or the cube is within a ﬁxed distance to the center of the red mark) for at least 50% of all time\nsteps in the two reaching tasks, and 25% for the push task. This is to ensure that the success rate of a\nrandom policy does not get inﬂated by trajectories that coincidentally visit a success state for a small\nnumber of time steps, e.g. passing through the goal with the gripper.\n20\nTable 4. Comparison to state-of-the-art. Test performance (episode return) of methods trained in a\nﬁxed environment and evaluated on thevideo_hard benchmark from DMControl-GB. In this setting,\nthe entirety of the ﬂoor and background is replaced by natural videos; see Figure 16 for samples.\nResults for CURL, RAD, PAD, and SODA are obtained from Hansen and Wang[21] and we report\nmean and std. deviation of 5 runs. We compare SVEA and SODA using the overlay augmentation,\nsince this is the augmentation for which the strongest results are reported for SODA in Hansen and\nWang [21]. SVEA achieves competitive results in all tasks considered, though there is still some\nroom for improvement before the benchmark is saturated.\nDMControl-GB CURL RAD DrQ PAD SODA SVEA\n(video_hard) (overlay) (overlay)\nwalker, 58 56 104 93 381 377\nwalk ±18 ±9 ±22 ±29 ±72 ±93\nwalker, 45 231 289 278 771 834\nstand ±5 ±39 ±49 ±72 ±83 ±46\ncartpole, 114 110 138 123 429 393\nswingup ±15 ±16 ±9 ±24 ±64 ±45\nball_in_cup, 115 97 92 66 327 403\ncatch ±33 ±29 ±23 ±61 ±100 ±174\nfinger, 27 34 71 56 302 335\nspin ±21 ±11 ±45 ±18 ±41 ±58\n(a) Training environment (reach).\n (b) Training environment (push).\n(c) Test environments (reach).\n (d) Test environments (push).\nFigure 17. Environments for robotic manipulation. Training and test environments for our robotic\nmanipulation experiments. Agents are trained in a ﬁxed environment, and are expected to generalize\nto the unseen test environments with randomized camera pose, colors, lighting, and backgrounds.\nH Implementation Details\nIn this section, we provide extensive implementation details for our experimental setup, including\nnetwork architecture, hyperparameters, as well as design choices speciﬁc to our ViT experiments.\nNetwork architecture. For experiments in DMControl [64], we adopt our network architecture from\nHansen and Wang [21], without any changes to the architecture nor hyperparameters to ensure a fair\ncomparison. The shared encoder fθ is implemented as an 11-layer CNN encoder that takes a stack of\nRGB frames rendered at 84×84×3 and outputs features of size32×21×21, where 32 is the number\nof channels and 21 ×21 are the dimensions of the spatial feature maps. All convolutional layers\nuse 32 ﬁlters and 3 ×3 kernels. The ﬁrst convolutional layer uses a stride of 2, while the remaining\nconvolutional layers use a stride of 1. Following previous work on image-based RL for DMControl\ntasks [76, 59, 35, 33, 21], the shared encoder is followed by independent linear projections for the\nactor and critic of the Soft Actor-Critic [18] base algorithm used in our experiments, and the actor\nand critic modules each consist of three fully connected layers with hidden dimension 1024. Training\ntakes approximately 24 hours on a single NVIDIA V100 GPU. For simplicity, we choose to apply the\nsame experimental setup for robotic manipulation.\nHyperparameters. Whenever applicable, we adopt hyperparameters from Hansen and Wang[21].\nWe detail hyperparameters relevant to our experiments in Table 5; ViT hyperparameters are discussed\nin the following. We use the default SVEA loss coefﬁcients α = 0.5,β = 0.5 in all experiments\nusing a CNN encoder.\n21\nTable 5. Hyperparameters used in experiments on DMControl and robotic manipulation.\nHyperparameter Value\nFrame rendering 84×84×3\nStacked frames 3\nRandom shift Up to ±4pixels\nAction repeat 2 (ﬁnger)\n8 (cartpole)\n4 (otherwise)\nDiscount factorγ 0.99\nEpisode length 1,000\nLearning algorithm Soft Actor-Critic (SAC)\nNumber of frames 500,000\nReplay buffer size 500,000\nOptimizer (θ) Adam ( β1 = 0.9,β2 = 0.999)\nOptimizer (αof SAC) Adam ( β1 = 0.5,β2 = 0.999)\nLearning rate (θ) 1e-3\nLearning rate (αof SAC) 1e-4\nBatch size 128\nSVEA coefﬁcients α= 0.5,β= 0.5\nψupdate frequency 2\nψmomentum coefﬁcient 0.05 (encoder)\n0.01 (critic)\nTable 6. Hyperparameters used in our ViT experiments.\nHyperparameter Value\nFrame rendering 96×96×3\nRandom shift Up to ±6pixels\nPatch size 8×8×3k\nNumber of patches 144\nEmbedding dimensionality128\nNumber of layers 4\nNumber of attention heads8\nScore function Scaled dot-product\nNumber of frames 300,000\nReplay buffer size 300,000\nBatch size 512\nSVEA coefﬁcients α= 1,β= 1(walker)\nα= 1,β= 0.5(push)\nα= 0.5,β= 0.5(otherwise)\nNumber of ViT parameters489,600\nRL with Vision Transformers. We adopt a similar experimental setup for our experiments with\nVision Transformers (ViT) [10] as replacement for the CNN encoder used in the rest of our experi-\nments, but with minimal changes to accommodate the new encoder. Our ViT encoder takes as input a\nstack of RGB frames rendered at 96 ×96 ×3 (versus 84 ×84 ×3 for CNN) and uses a total of 144\nnon-overlapping 8 ×8 ×3k(where kis the number of frames in a frame stacked observation) image\npatches from a spatial grid evenly placed across the image observation. All patches are projected\ninto 128-dimensional embeddings, and all embeddings are then forwarded as tokens for the ViT\nencoder. Following the original ViT implementation, we use learned positional encodings as well as\na learnable class token. Our encoder consists of 4 stacked Transformer [69] encoders, each using\nMulti-Head Attention with 8 heads. We use the ViT encoder as a drop-in replacement to CNN and\noptimize it jointly together with the Q-function using the Adam [32] optimizer and with no changes\nto the learning rate. We do not pre-train the parameters of the ViT encoder, and we do not use weight\ndecay. Similar to the CNN encoder, we ﬁnd ViT to beneﬁt from random shift augmentation and we\ntherefore apply it by default in all methods. Training takes approximately 6 days on a single NVIDIA\nV100 GPU. See Table 6 for an overview of hyperparameters speciﬁc to the ViT encoder experiments.\nImplementation of data augmentation in SVEA vs. previous work. Previous work [35, 33, 61,\n54] applies augmentation to both state saug\nt = τ(st,ν) and successor state saug\nt+1 = τ(st+1,ν′) where\nν,ν′∼V. As discussed in Section C, this is empirically not found to be an issue in application of\nweak augmentation such as random shift [33]. However, previous work ﬁnds that strong augmentation,\ne.g. random convolution [36, 35], can cause instability and poor sample efﬁciency. We apply random\n22\nshifts in SVEA and all baselines by default, and aim to stabilize learning under strong augmentation.\nAs such, we generically refer to observations both with and without the random shift operation\nas unaugmented, and instead refer to observations as augmented after application of one of the 6\naugmentations considered in our study. While we provide the full SVEA algorithm in Algorithm 1, we\nhere provide supplementary Python-like pseudo-code for the update rules of SVEA as well as generic\noff-policy actor-critic algorithms both with and without naïve application of data augmentation.\nGeneric off-policy actor-critic algorithm.We here provide a reference implementation for a generic\nbase algorithm from which we will implement our proposed SVEA framework for data augmentation.\ndef update(state, action, reward, next_state):\n\"\"\"Generic off-policy actor-critic RL algorithm\"\"\"\nnext_action = actor(next_state)\nq_target = reward + critic_target(next_state, next_action)\nq_prediction = critic(state, action)\nupdate_critic(q_prediction, q_target)\nupdate_actor(state)\nupdate_critic_target()\nNaïve application of data augmentation. A natural way to apply data augmentation in off-policy\nRL algorithms is to augment both st and st+1. Previous work on data augmentation in off-policy\nRL [35, 33, 61, 54] follow this approach. However, this is empirically found to be detrimental to\nsample efﬁciency and stability under strong data augmentation. In the following, we generically refer\nto application of data augmentation as an aug operation.\ndef update_aug(state, action, reward, next_state):\n\"\"\"Generic off-policy actor-critic RL algorithm that uses data augmentation\"\"\"\nstate = aug(state)\nnext_state = aug(next_state)\nnext_action = actor(next_state)\nq_target = reward + critic_target(next_state, next_action)\nq_prediction = critic(state, action)\nupdate_critic(q_prediction, q_target)\nupdate_actor(state)\nupdate_critic_target()\nSVEA. Our proposed method, SVEA, does not apply strong augmentation to st+1 nor st when used\nfor policy learning. SVEA jointly optimizes the Q-function over two data streams with augmented\nand unaugmented data, respectively, which can be implemented efﬁciently for α= βas in Algorithm\n1. The following pseudo-code assumes α= 0.5,β = 0.5.\ndef update_svea(state, action, reward, next_state):\n\"\"\"SVEA update for a generic off-policy actor-critic RL algorithm\"\"\"\nupdate_actor(state)\nnext_action = actor(next_state) # [B, A]\nq_target = reward + critic_target(next_state, next_action) # [B, 1]\nsvea_state = concatenate(state, aug(state), dim=0) # [2*B, C, H, W]\nsvea_action = concatenate(action, action, dim=0) # [2*B, A]\nsvea_q_target = concatenate(q_target, q_target, dim=0) # [2*B, 1]\nsvea_q_prediction = critic(svea_state, svea_action) # [2*B, 1]\nupdate_critic(svea_q_prediction, svea_q_target)\nupdate_critic_target()\nwhere B is the batch size, C is the number of input channels, H and W are the dimensions of\nobservations, and Ais the dimensionality of the action space. For clarity, we omit hyperparameters\nsuch as the discount factor γ, learning rates, and update frequencies.\n23\nI Task Descriptions\nWe experiment on tasks from DMControl [64] as well as a set of robotic manipulation tasks that we\nimplement using MuJoCo [67]. DMControl tasks are selected based on previous work on both sample\nefﬁciency [19, 76, 59, 20] and generalization [21, 60, 72], and represent a diverse and challenging\nskill set in the context of image-based RL. Our set of robotic manipulation tasks are designed to\nrepresent fundamental visuomotor skills that are widely applied in related work on robot learning\n[37, 50, 11, 44, 22, 78]. We here provide a uniﬁed overview of the tasks considered in our study and\ntheir properties; see Section G for a detailed discussion of the robotic manipulation environment. All\ntasks emit observations o ∈R84×84×3 that are stacked as states s ∈R84×84×9.\n• Walker, walk(a ∈R6). A planar walker that is rewarded for walking forward at a target\nvelocity. Dense rewards.\n• Walker, stand(a ∈R6). A planar walker that is rewarded for standing with an upright torso\nat a constant minimum height. Dense rewards.\n• Cartpole, swingup (a ∈R). Swing up and balance an unactuated pole by applying forces to\na cart at its base. The agent is rewarded for balancing the pole within a ﬁxed threshold angle.\nDense rewards.\n• Cartpole, balance (a ∈R). Balance an unactuated pole by applying forces to a cart at its\nbase. The agent is rewarded for balancing the pole within a ﬁxed threshold angle. Dense\nrewards.\n• Ball in cup, catch (a ∈R2). An actuated planar receptacle is to swing and catch a ball\nattached by a string to its bottom. Sparse rewards.\n• Finger, spin(a ∈R2). A manipulation problem with a planar 3 DoF ﬁnger. The task is to\ncontinually spin a free body. Sparse rewards.\n• Robot, reach (a ∈R2). A manipulation problem with a simulated Kinova Gen3 robotic arm.\nThe task is to move the gripper to a randomly initialized goal position. Dense rewards.\n• Robot, reach moving target (a ∈R2). A manipulation problem with a simulated Kinova\nGen3 robotic arm. The task is to continuously track a randomly initialized goal with the\ngripper. The goal moves in a zig-zag pattern at a random constant speed. Dense rewards.\n• Robot, push (a ∈R2). A manipulation problem with a simulated Kinova Gen3 robotic arm.\nThe task is to push a cube to a goal position. All positions are randomly initialized. Dense\nrewards.\nJ Broader Impact\nThe discipline of deep learning – and reinforcement learning in particular – is rapidly evolving, which\ncan in part be attributed to better algorithms [43, 38, 18], neural network architectures [26, 69, 12], and\navailability of data [49, 31], but advances are also highly driven by increased computational resources\nand larger models such as GPT-3 [5] in natural language processing and ViT [10] in computer vision.\nAs a result, both computational and economic requirements for training and deploying start-of-the-art\nmodels are increasing at an unprecedented rate [3]. While we are concerned by this trend, we remain\nexcited about the possibility of reusing and re-purposing large learned models (in the context of\nRL: policies and value functions) that learn and generalize far beyond the scope of their training\nenvironment. A greater reuse of learned policies can ultimately decrease overall computational costs,\nsince new models may need to be trained less frequently. As researchers, we are committed to pursue\nresearch that is to the beneﬁt of society. We strive to enable reuse of RL policies through extensive\nuse of data augmentation, and we ﬁrmly believe that our contribution is an important step towards that\ngoal. Our method is empirically found to reduce the computational cost (in terms of both stability,\nsample efﬁciency, and total number of gradient steps) of training RL policies under augmentation,\nwhich is an encouraging step towards learning policies that generalize to unseen environments. By\nextension, this promotes policy reuse, and may therefore be a promising component both for reducing\ncosts and for improving generalization of large-scale RL that the ﬁeld appears to be trending towards.\n24"
}