{
    "title": "CLTR: An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering",
    "url": "https://openalex.org/W3176069778",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2102503046",
            "name": "Feifei Pan",
            "affiliations": [
                "Rensselaer Polytechnic Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2266279181",
            "name": "Mustafa Canim",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2175883443",
            "name": "Michael Glass",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2080393478",
            "name": "Alfio Gliozzo",
            "affiliations": [
                "IBM (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1981374407",
            "name": "Peter Fox",
            "affiliations": [
                "Rensselaer Polytechnic Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980741705",
        "https://openalex.org/W2751448157",
        "https://openalex.org/W2101964891",
        "https://openalex.org/W2132083030",
        "https://openalex.org/W3034944976",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2788550262",
        "https://openalex.org/W2342096063",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3012644370",
        "https://openalex.org/W2162020046",
        "https://openalex.org/W2140116426",
        "https://openalex.org/W3013003636",
        "https://openalex.org/W2325227998",
        "https://openalex.org/W2270766381",
        "https://openalex.org/W3166417463",
        "https://openalex.org/W2768409085",
        "https://openalex.org/W2899286282",
        "https://openalex.org/W1956559956",
        "https://openalex.org/W2398606196",
        "https://openalex.org/W135190683",
        "https://openalex.org/W1967830139",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W3035231859",
        "https://openalex.org/W3102264439",
        "https://openalex.org/W1988217119",
        "https://openalex.org/W2108223890",
        "https://openalex.org/W3124424060",
        "https://openalex.org/W2963759819",
        "https://openalex.org/W2963899988",
        "https://openalex.org/W2947354947",
        "https://openalex.org/W2890431379"
    ],
    "abstract": "Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, Peter Fox. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. 2021.",
    "full_text": "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: System Demonstrations, pages 202‚Äì209, August 1st - August 6th, 2021.\n¬©2021 Association for Computational Linguistics\n202\nCLTR: An End-to-End, Transformer-Based System for Cell Level Table\nRetrieval and Table Question Answering\nFeifei Pan1, Mustafa Canim 2, Michael Glass 2, AlÔ¨Åo Gliozzo2, Peter Fox1\npanf2@rpi.edu, mustafa@us.ibm.com,\nmrglass@us.ibm.com, gliozzo@us.ibm.com\npfox@cs.rpi.edu\n1 Rensselaer Polytechnic Institute\n2 IBM TJ Watson Research Center\nAbstract\nWe present the Ô¨Årst end-to-end, transformer-\nbased table question answering (QA) system\nthat takes natural language questions and mas-\nsive table corpus as inputs to retrieve the most\nrelevant tables and locate the correct table cells\nto answer the question 1. Our system, CLTR,\nextends the current state-of-the-art QA over ta-\nbles model to build an end-to-end table QA ar-\nchitecture. This system has successfully tack-\nled many real-world table QA problems with\na simple, uniÔ¨Åed pipeline. Our proposed sys-\ntem can also generate a heatmap of candi-\ndate columns and rows over complex tables\nand allow users to quickly identify the cor-\nrect cells to answer questions. In addition, we\nintroduce two new open-domain benchmarks,\nE2E WTQ and E2E GNQ, consisting of 2,005\nnatural language questions over 76,242 ta-\nbles. The benchmarks are designed to validate\nCLTR as well as accommodate future table re-\ntrieval and end-to-end table QA research and\nexperiments. Our experiments demonstrate\nthat our system is the current state-of-the-art\nmodel on the table retrieval task and produces\npromising results for end-to-end table QA.\n1 Introduction\nTables are widely used in digital documents across\nmany domains, ranging from open-domain knowl-\nedge bases to domain-speciÔ¨Åc scientiÔ¨Åc journals,\nenterprise reports, to store structured information in\ntabular format. Many algorithms have been devel-\noped to retrieve tables based on given queries (Ca-\nfarella et al., 2008, 2009; Sun et al., 2019; Bhaga-\nvatula et al., 2013; Shraga et al., 2020a; Chen et al.,\n2021). The majority of these solutions exploit tradi-\ntional information retrieval (IR) techniques where\ntables are treated as documents without consider-\ning the tabular structure. However, these retrieval\n1System page: https://github.com/IBM/row-column-\nintersection\nmethods often result in an inferior quality due to\na major limitation that most of these approaches\nhighly rely on lexical matching between keyword\nqueries and table contents. Recently, there is a\ngrowing demand to support natural language ques-\ntions (NLQs) over tables and answer the NLQs\ndirectly, rather than simply retrieving top- k rel-\nevant tables for keyword-based queries. Shraga\net al. (2020c) introduce the Ô¨Årst NLQ-based table\nretrieval system, which leverages an advanced deep\nlearning model. Although it is a practical approach\nto better understand the structure of NLQs and ta-\nble content, it only focuses on table retrieval rather\nthan answering NLQs. Lately, transformer-based\npre-training approaches have been introduced in\nTABERT (Yin et al., 2020), TAPAS (Herzig et al.,\n2020), and the Row-Column Intersection model\n(RCI ) (Glass et al., 2020). These algorithms are\nvery powerful at answering questions on given ta-\nbles; however, one cannot apply them over all ta-\nbles in a corpus due to the computationally expen-\nsive nature of transformers. An end-to-end table\nQA system that accomplishes both tasks is in need\nas it has the following advantages over separated\nsystems: (1) It reduces error accumulations caused\nby inconsistent, separated models; (2) It is easier\nto Ô¨Åne-tune, optimize, and perform error analysis\nand reasoning on an end-to-end system; and (3)\nIt better accommodates user needs with a single,\nuniÔ¨Åed pipeline. Hence, we propose a table re-\ntrieval and QA over tables system in this paper,\ncalled Cell Level Table Retrieval (CLTR). It Ô¨Årst\nretrieves a pool of tables from a large table corpus\nwith a coarse-grained but inexpensive IR method.\nIt then applies a transformer-based QA over tables\nmodel to re-rank the table pool and Ô¨Ånally Ô¨Ånds the\ntable cells as answers. To the best of our knowl-\nedge, this is the Ô¨Årst end-to-end framework where a\ntransformer-based, Ô¨Åne-grained QA model is used\nalong with efÔ¨Åcient coarse-grained IR methods to\n203\nFigure 1: The overview of the end-to-end table QA architecture of CLTR.\nretrieve tables and answer questions over them. Our\nexperiments demonstrate that CLTR outperforms\ncurrent state-of-the-art models on the table retrieval\ntask while further helping customers Ô¨Ånd answers\nover returned tables.\nTo build such a Table QA system, an end-to-\nend benchmark is needed to evaluate alternative\napproaches. Current benchmarks, however, are\nnot designed for such tasks, as they either focus\non the retrieval task over multiple tables or QA\ntask on a single table. To address the problems,\nwe propose two new benchmarks: E2E WTQ and\nE2E GNQ. The details of these benchmarks and\nmore discussions are provided in Section 4.1.\nThe speciÔ¨Åc contributions of this paper are sum-\nmarized as follows:\n‚Ä¢ A transformer-based end-to-end table QA\nsystem: We build a novel end-to-end table\nQA pipeline by utilizing a transfer learning\napproach to retrieve tables from a massive\ntable corpus and answer questions over them.\nThe end system outperforms the state-of-the-\nart approaches on the table retrieval task.\n‚Ä¢ Creating heatmaps over complex tables:\nTo highlight all relevant table columns, rows,\nand cells, CLTR generates heatmaps on tables.\nFollowing a pre-deÔ¨Åned color code, the high-\nlighted columns, rows, and cells are ranked ac-\ncording to their relevance to the questions. Us-\ning the heatmap, users can efÔ¨Åciently glance\nthrough complex tables and accurately locate\nthe answers to the questions.\n‚Ä¢ Two new benchmarks for the end-to-end\ntable QA evaluation: We propose and re-\nlease two new benchmarks, E2E WTQ and\nE2E GNQ, extending two existing bench-\nmarks, WikiTableQuestions and GNQtables,\nrespectively. The benchmarks can be used to\nevaluate systems for table retrieval and end-\nto-end table QA.\n2 Overview\nThe Architecture The architecture of our end-\nto-end table QA system, CLTR, is illustrated in\nFigure 1. This system aims to solve the end-to-\nend table QA task by generating a reasonable-sized\nsubset of relevant tables from a massive table cor-\npus, and employs the transformer-based approach\nto re-rank them based on their relevance to the user\ngiven NLQs, and Ô¨Ånally answer the given NLQs\nwith cells from these tables.\nCLTR possess an abundant number of tables\ngenerated from documents of various knowledge\nsources to form a large table corpus. The system\nhas two components: an inexpensive tf-idf (Salton\nand McGill, 1986) based coarse-grained table re-\ntrieval component and a Ô¨Åne-grained RCI-based\ntable QA component. CLTR Ô¨Årst takes as input\nany user given NLQs and processes the questions\nand the table corpus with the inexpensive BM25\nalgorithm to generate a set of relevant tables, which\nis relatively large and contains noise (i.e., irrelevant\ntables). Here we use BM25 to efÔ¨Åciently narrow\ndown the table candidates from a massive table cor-\npus and highly reduce the execution time and com-\nputational cost for CLTR. The output of this coarse-\ngrained table retrieval component is later fed into\nthe more expensive but accurate, transformer-based\nRCI to learn probability scores for table columns\nand rows, respectively. The scores produced by\nRCI indicate how likely the given question‚Äôs Ô¨Å-\nnal answer exists within a table column or row.\n204\nWith the probability scores, CLTR re-ranks the ta-\nbles and produces two outputs to the users: (1) a\nheatmap over top-ranked tables that highlights the\nmost relevant columns and rows with a color code;\n(2) the table cells that contain the answers to the\nNLQs.\nThe applications Figure 2 presents the user in-\nterface of an application of the CLTR system. In\nthis example, we apply the system to table QA\nover an aviation-related dataset, a domain-speciÔ¨Åc\ndataset on tables in aviation companies‚Äô annual re-\nports. This user interface consists of two major\nsections, with Tag Aand Tag Bpoint to the user\ninput and the system output sections, respectively.\nUnder Tag Aand B, the CLTR pipeline is employed\nto support multiple functionalities. Users can input\nany NLQs, such as ‚ÄúWhen is the purchase agree-\nment signed between Airbus and Virgin America?‚Äù\nin this example, into the text box at Tag Dand\nclick the Search button at Tag Cto query the pre-\nloaded table corpus. Users may select to reset the\nsystem for new queries or re-train a new model\nwith a new corpus. In the system output sections,\na list of tables similar to the table at Tag Fis gen-\nerated and presented to users. For each table, the\nsystem output includes: (a) the surrounding text\nof the table from the original PDF ( Tag E); (b)\nthe pre-processed table in a clean, human-readable\nformat with a heatmap on it, indicating the most\nrelevant rows, columns, and cells (Tag F); (c) an\nannotation option, where the users can contribute\nto reÔ¨Åning the system with feedback ( Tag G). In\naddition, the CLTR architecture has been widely ap-\nplied to datasets from many other domains, varying\nfrom Ô¨Ånance to medical. The system is also vali-\ndated with open-domain benchmarks, with more\ndetails discussed in Section 4.\n3 The RCI-based Table QA\nTraditional approaches solve the table QA problem\nwith two consecutive steps: retrieval of the most\nrelevant tables for a given NLQ and locating the\ncorrect answers out of the cells with the help of\na QA over tables model. These steps are usually\nstudied separately. Our proposed system, CLTR,\nuniÔ¨Åes the two-step table QA with a single pipeline\nby leveraging the novel RCI model. RCI is the\nstate-of-the-art approach for locating answers over\ntables (Glass et al., 2020); however, it is not de-\nsigned to retrieve tables out of large table corpus.\nIn this section, we describe how we build an end-\nto-end table QA system combining the strength of\ninexpensive IR methods and the RCI model.\n3.1 The Row-Column Intersection Model\nWe Ô¨Årst brieÔ¨Çy introduce the Row-Column Intersec-\ntion model (RCI), which supports the Ô¨Åne-grained\ntable retrieval component of our system. The RCI\nmodel decomposes table QA into its two com-\nponents: projection, corresponding to identifying\ncolumns, and selection, identifying rows. Every\nrow and column identiÔ¨Åcation is a binary sequence-\npair classiÔ¨Åcation. The Ô¨Årst sequence is the ques-\ntion and the second sequence is the row or column\ntextual sequence representation. We use the in-\nteraction model of RCI that concatenates the two\nsequences, with standard separator tokens, as the\ninput to a transformer.\nThe RCI interaction model uses the sequence\nrepresentation which is later appended to the ques-\ntion with standard [CLS] and [SEP ] tokens to de-\nlimit the two sequences. This sequence pair is fed\ninto a transformer encoder, ALBERT (Lan et al.,\n2020). The Ô¨Ånal hidden state for the[CLS] token is\nused in a linear layer followed by a softmax to clas-\nsify if the column or row containing the answer or\nnot. Each row and column is assigned with a prob-\nability of containing the answer. The RCI model\noutputs the top-ranked cell as the intersection of the\nmost probable row and the most probable column.\nFigure 3 gives a sample question fed into the\ntransformer architecture along with the column and\nrow representation of a table.\n3.2 The End-to-End Table QA with RCI\nTo tackle the table retrieval problem, we exploit an\ninexpensive IR method together with the state-of-\nthe-art RCI model. Unlike the traditional methods\ntreating tables as free text, a set of features, or\nmulti-modal objects, CLTR treats tables as a set of\ncolumns and rows and re-rank the tables based on\ncell-level RCI scores.\nAs we previously mentioned in Section 2, CLTR\nÔ¨Årst processes the question and table corpus with\nthe inexpensive BM25 algorithm to generate a pool\nof highly relevant tables. Later, the RCI model\nis used to produce probability scores for every\ncolumn and row for tables in the pool. There-\nfore, for every table t with n columns and m rows\nin the table pool T, we have two set of scores,\nPcolumn = {pc1 , pc2 , pc3 , ..., pcn } for columns and\nProw = {pr1 , pr2 , pr2 , ..., prm } for rows. We cal-\nculate the overall probability score for each ta-\n205\nF\nG\nBE\nADC\nFigure 2: The application of CLTR on an aviation corpus\n‚Ä¶\n‚Ä¶\nE[CLS] E1 EN E[SEP]E‚Äô1 E‚ÄôM‚Ä¶‚Ä¶\nC T1 TN T[SEP] T‚Äô1 T‚ÄôM‚Ä¶‚Ä¶\n[CLS]\nNL Question[SEP]\nTable Column or Row\nALBERT\nWhat party was William Pinkney and Uriah Forrest a part of ?\n[SEP]\nT[SEP]\nE[SEP]‚Ä¶\nLinear Layer and Softmaxùëù(ùë¶ùëíùë†)ùëù(ùëõùëú) ùêø!\"Weak Supervision Label\nFigure 3: The RCI Table QA Model\nble by taking the maximum cell-level score, us-\ning Pt = max(Pcol) +max(Prow). Our experi-\nments prove the advantages of this method over\nthe other algorithms (e.g., taking the averaged cell-\nlevel scores).\nCLTR re-ranks the tables within the table pool\nT using the maximum cell-level scores. Once the\nre-ranking is done, the top- k tables out of T are\nreturned to the users. The correct cells on the top-k\ntables are later identiÔ¨Åed by locating the intersec-\ntion of the most relevant columns and rows discov-\nered by the RCI model.\n4 Experiments\n4.1 Data\nProposed Benchmarks: Existing table retrieval\nand QA benchmarks focus on either answering\nNLQs on a single table or the retrieval of mul-\ntiple tables for a keyword query. A comprehen-\nsive comparison of existing benchmarks with their\nlimitations is listed in Table 1. WikiSQL (Zhong\net al., 2017) and WikiTableQuestions (Pasupat and\nLiang, 2015) are widely used to evaluate table QA\nsystems. More recently, they have been used by\nTAPAS (Herzig et al., 2020) and TABERT (Yin\net al., 2020) where transformer-based models for\nQA over tables have been introduced. However,\nthese benchmarks are not created to be used as part\nof an end-to-end table retrieval and QA pipeline.\nOn the other hand, WikiTables was created based\non the corpus introduced by Bhagavatula et al.\n(2015) and used in many recent table retrieval stud-\nies (Zhang and Balog, 2018a; Deng et al., 2019;\nShraga et al., 2020b,c). Despite its popularity, the\nWikiTables benchmark has two major limitations.\nFirst, the query set is fairly limited, containing only\n100 keyword-based queries. Many recent studies\nuse this small set of queries for a learning-to-rank\n(LTR) task with 5-fold cross-validation, potentially\ncausing overÔ¨Åtting issues for the proposed table\nretrieval models. Second, the query set includes\nonly keyword-based queries, which do not repre-\nsent the NLQs customers are expected to ask to get\nanswers over tables. To solve the aforementioned\nissues and create an end-to-end table QA bench-\nmark with NLQs, we introduce two new bench-\nmarks, E2E WTQ and E2E GNQ, inspired by Wik-\niTableQuestions and GNQtables.\nThe WikiTableQuestions (Pasupat and Liang,\n2015) benchmark is originally designed for Ô¨Ånd-\ning answer to questions from given tables. It con-\nsists of complex NLQs and tables extracted from\nWikipedia. We Ô¨Ålter the benchmark following\nGlass et al. (2020) to generate a subset of 1,216\nquestions with 2,108 tables.\nThe GNQtables dataset, introduced in Shraga\net al. (2020c), extends the Google Natural Ques-\ntions (NQ) benchmark (Kwiatkowski et al., 2019).\nIt contains 789 NLQs and a large table corpus of\n74,224 tables. For each question, the ground truth\n206\n# of tables # of queries Retrieval task QA task Reference\nWikiSQL 24,241 80,654 \u0017 \u0013 (Zhong et al., 2017)\nTabMCQ 68 9,092 \u0017 \u0013 (Jauhar et al., 2016)\nWikiTableQuestions 2,108 22,033 \u0017 \u0013 (Pasupat and Liang, 2015)\nWikiTables 1.6M 100 \u0013 \u0017 (Bhagavatula et al., 2015)\nGNQtables 74,224 789 \u0013 \u0017 (Shraga et al., 2020c)\nE2E WTQ 2,108 1,216 \u0013 \u0013\nE2E GNQ 74,224 789 \u0013 \u0013\nTable 1: Comparison of table QA and retrieval benchmarks\nonly points to the most relevant table (with a binary\ngrade 1 indicates relevant), while all other tables in\nthe table corpus are considered irrelevant (grade 0).\nGNQtables is the only table retrieval benchmark\nusing NLQs, which makes it possible to adapt it to\nend-to-end table QA. To create the E2E GNQ, we\nmanually annotate and enhance GNQtables with\nadditional ground truth data for each question: (1)\nthe table cells containing the correct answers; (2)\nthe index of the target columns; (3) the index of the\ntarget rows.\nExperimental Data: We experiment with\nE2E WTQ to test the portability of CLTR, in\nwhich we Ô¨Åne-tune the RCI model with two other\ntable QA benchmarks. We utilize an open-domain\nbenchmark, WikiSQL (Zhong et al., 2017), and a\ndomain-speciÔ¨Åc benchmark, TabMCQ (Jauhar\net al., 2016). The WikiSQL dataset has 80,654\nquestions on 24,241 Wikipedia tables, while the\nTabMCQ is a much smaller dataset, with only\n68 hand-crafted tables and 9,092 multiple-choice\nquestions.\n4.2 Experimental Setup\nOverall Setup: We test our system under two ex-\nperimental settings for table retrieval: (1) We test\nCLTR without task-speciÔ¨Åc training on E2E WTQ\nand Ô¨Åne-tune the RCI model with WikiSQL and\nTabMCQ; (2) To fairly compare against the state-\nof-the-art, we follow the experimental setup in\nShraga et al. (2020c) and Ô¨Åne-tune CLTR with\nE2E GNQ. We implement 5-fold cross-validation\non E2E GNQ, where 80% of data is used for Ô¨Åne-\ntuning and 20% is used for validation. For both\nE2E GNQ and E2E WTQ, we use BM25 as our\nbaseline model, which is widely used in industry-\nscale IR systems. We test the end-to-end table QA\ncapability of CLTR with our newly proposed bench-\nmarks. Since we are the Ô¨Årst publicly accessible\nend-to-end table QA system, we do not have a base-\nline to fairly compare to for our end-to-end table\nQA experiments.\nWe implement the coarse-grained table retrieval\nwith the BM25 algorithm embedded in the Elas-\nticSearch python API for all of our experiments.\nThis API can be accessed at https://elasticsearch-\npy.readthedocs.io/en/master/. Each table is indexed\nas a single text document with the embedded En-\nglish analyzer. For each question, we generate a\npool of 300 tables with the highest BM25 similar-\nity scores. Following the current state-of-the-art\nmodel in Shraga et al. (2020c), we set k1 = 1.2\nand b = 0.7. The tables in the pool are later pro-\ncessed with the RCI model.\nOur experiments employ the RCI model with\nALBERT XXL version (Lan et al., 2020). The RCI\nmodel is Ô¨Åne-tuned for different benchmarks with\nthe following conÔ¨Ågurations: (1) training batch size\n= 128; (2) Number of epochs = 2; (3) Learning rate\n= 2.5e-5; and (4) maximum sequence length = 512.\nThe model and data for the experiments with\nCLTR are available athttps://github.com/IBM/row-\ncolumn-intersection.\nEvaluation metrics: For table retrieval evalua-\ntion, we use the three metrics from previous work\n(Zhang and Balog, 2018b; Shraga et al., 2020c)\nfor the top- k retrieved tables, namely precision\n(P) with k ‚àà {5, 10}, normalized discounted gain\n(NDCG) with k ‚àà {5, 10, 20}, and the mean av-\nerage precision (MAP). For the end-to-end table\nQA tasks, we evaluate our proposed model follow-\ning Glass et al. (2020) with two commonly used\nmetrics in the IR community, accuracy at top 1 re-\ntrieved answer (Hit@1) and the mean reciprocal\nrank (MRR).\nAll experimental results are evaluated with\nthe TREC standard evaluation tool (V oorhees\nand Harman, 2005). The source code of\nthe TREC evaluation tool can be found at\nhttps://trec.nist.gov/trec eval/.\n4.3 Experimental Results\nWe experimentally compare CLTR against the\nBM25 baseline and the current state-of-the-art\nmodel on table retrieval in this section. Further-\nmore, we test CLTR with our proposed benchmarks\non the end-to-end table QA task.\n207\nP@5 P@10 N@5 N@10 N@20 MAP\nBM25 0.5938 0.6587 0.5228 0.5356 0.5359 0.4704\nCLTR 0.7437 0.8735 0.6915 0.7119 0.7321 0.5971\n(a) E2E WTQ\nP@5 P@10 N@5 N@10 N@20 MAP\nBM25 0.0413 0.0242 0.1650 0.1764 0.1852 0.1601\nMTR point 0.1460 0.0767 0.6227 0.6349 0.6359 0.5920\nMTR pair 0.1826‚àó 0.0990‚àó 0.6945‚àó 0.7198‚àó 0.7220‚àó 0.6328‚àó\nCLTR 0.2203 0.1660 0.7235 0.7402 0.7458 0.7176\n(b) E2E GNQ\nTable 2: A comparison of CLTR and the baselines (* indicates the current state-of-the-art numbers).\nTable Retrieval: We present the experimental\nresults for table retrieval without task speciÔ¨Åc train-\ning on E2E WTQ in Table 2a. Since the MTR\nmodel (Shraga et al., 2020c) is not available to us\nand this dataset has never been used in any pub-\nlished table retrieval work, we only compare our\nresults to the coarse-grained BM25 baseline. The\nresults indicate our proposed model outperforms\nthe BM25 baseline with average improvements of\n29.12%, 33.94% and 26.93% on precision, NDCG,\nand MAP, respectively. The results on E2E WTQ\nalso indicate that pre-trained CLTR can be adapted\nto new datasets without task-speciÔ¨Åc training.\nThe experimental results for E2E GNQ are\nshown in Table 2b, comparing against BM25 and\nthe current state-of-the-art, the two MTR mod-\nels, MTR point (with point-wise training) and\nMTR pair (with pair-wise training) in Shraga et al.\n(2020c). The comparison shows that our proposed\nmodel outperforms the current best MTR pair\nmodel on all metrics, with an average improve-\nment of 28.73% on precision, 3.43% on NDCG,\nand 13.40% on MAP. The experimental results in-\ndicates CLTR is the new state-of-the-art system for\ntable retrieval. Moreover, CLTR can further locate\ncell values to answer NLQs after table retrieval.\nMRR Hit@1\nE2E WTQ 0.5503 0.4675\nE2E GNQ 0.4067 0.2699\nTable 3: Model evaluation for end-to-end table QA\nEnd-to-End Table QA: To further validate\nCLTR, we implement the end-to-end Table QA\nevaluation with E2E WTQ and E2E GNQ. The\nonly existing end-to-end table QA model, Sun et al.\n(2016), and its dataset are not publicly available.\nTherefore, we do not have any baseline models to\ncompare to. Our experimental results are reported\nin Table 3. As the Ô¨Årst attempt for an end-to-end ta-\nble QA system with transformer-based architecture\non complex table benchmarks, we show that our ap-\nproach is able to achieve promising and consistent\nperformance. Our results indicate CLTR performs\nbetter for the Ô¨Årst benchmark, E2E WTQ, where\nthe table corpus mainly contains well-structured\ntables. On the other hand, we expect the results for\nE2E GNQ to be worse due to the amount of poorly\nformatted tables in the table corpus.\nQualitative Analysis: The experiments indicate\nCLTR outperforms all baselines, as well as the\ncurrent state-of-the-art models on table retrieval. It\nalso produces promising results for the end-to-end\ntable QA task. We further demonstrate the high-\nportability of CLTR with pre-trained models using\nunseen benchmarks.\nThe system performance is much better for\nE2E WTQ based on the experimental results. After\na thorough investigation, we notice that the original\nGNQtables contains a large amount of noisy tables\nwhich do not have tabular structures. A consider-\nable amount of tables in GNQtables are Wikipedia\nInfoBoxes, which may have multiple column/row\nheaders and are difÔ¨Åcult to process by machines\naccurately. Although table quality is crucial for\ntable QA models, CLTR proves its advantageous\nby producing state-of-the-art results with noisy ta-\nble corpus. Furthermore, the example shown in\nFigure 2 demonstrates the effectiveness of CLTR\nwhen applied to real-world data.\n5 Related Work\nTable Retrieval A majority of the table retrieval\nmethods proposed in the literature treat tables as in-\ndividual documents without taking the tabular struc-\nture into consideration (Pyreddy and Croft, 1997;\nWang and Hu, 2002; Liu et al., 2007; Cafarella\net al., 2008, 2009). More recent approaches utilize\nfeatures generated from queries, tables, or query-\ntable pairs. For example, Zhang and Balog (2018b)\nintroduces an ad-hoc table retrieval method, re-\ntrieving tables with features such as #query term,\n#columns, #null values, etc. Similar work includes\n208\nSun et al. (2019), Bhagavatula et al. (2013), and\nShraga et al. (2020a). The current state-of-the-art\nmodel is introduced in Shraga et al. (2020c), where\ntables are treated as multi-modal objects and re-\ntrieved with a neural ranking model. We compare\nCLTR with this approach in Section 4.\nTable QA Models Early table QA systems typi-\ncally convert natural language questions into SQL\nformat to answer questions over tables (Yu et al.,\n2018; Guo and Gao, 2020; Lin et al., 2019; Xu\net al., 2018). In Jim ¬¥enez-Ruiz et al. (2020),\nthe authors promote the idea of matching tabu-\nlar data to knowledge graphs and create the Se-\nmantic Web Challenge on Tabular Data to Knowl-\nedge Graph Matching (SemTab), which provide\na new solution for table understanding and QA\nrelated tasks. Recently, TAPAS (Herzig et al.,\n2020) and TABERT (Yin et al., 2020) introduce the\ntransformer-based approaches for this task. The\nRCI (Glass et al., 2020) model is the state-of-the-\nart model for QA over tables. It utilizes a transfer\nlearning based framework to independently clas-\nsify the most relevant columns and rows for a given\nquestion and further identify the most relevant cells\nas the intersections of top-ranked columns and\nrows.\nEnd-to-End Table QA Models To the best of\nour knowledge, the table cell search framework\npublished in Sun et al. (2016) is the only existing\nend-to-end Table QA system. This work leverages\nthe semantic relations between table cells and uses\nrelational chains to connect queries to table cells.\nHowever, the proposed model only works for well-\nformatted questions containing at least one highly\nrelevant entity to link tables to the questions. In\naddition, the model and the data are not publicly\navailable for comparison.\n6 Conclusion\nThis paper proposes an end-to-end solution for ta-\nble retrieval and Ô¨Ånding answers for NLQs over\ntables. To the best of our knowledge, this is the Ô¨Årst\nsystem built where a transformer-based QA model\nis used for locating answers over tables while im-\nproving the ranking of tables out of a table pool\nformed by inexpensive IR methods. To evaluate\nthe efÔ¨Åcacy of this system, we introduce two bench-\nmarks, namely E2E WTQ and E2E GNQ.\nThe experimental results indicates that the pro-\nposed system, CLTR, outperforms the baselines\nand the current state-of-the-art model on the ta-\nble retrieval task. Furthermore, CLTR produces\npromising results on the end-to-end table QA task.\nIn real-world applications, CLTR can be applied\nto create a heatmap over tables to assist users in\nquickly identifying the correct cells on tables.\nReferences\nChandra Bhagavatula, Thanapon Noraset, and Doug\nDowney. 2013. Methods for exploring and min-\ning tables on wikipedia. Proceedings of the ACM\nSIGKDD Workshop on Interactive Data Exploration\nand Analytics.\nChandra Sekhar Bhagavatula, Thanapon Noraset, and\nDouglas C Downey. 2015. Tabel: Entity linking in\nweb tables. In The Semantic Web ‚Äì ISWC 2015 -\n14th International Semantic Web Conference, Pro-\nceedings, Lecture Notes in Computer Science (in-\ncluding subseries Lecture Notes in ArtiÔ¨Åcial Intel-\nligence and Lecture Notes in Bioinformatics), pages\n425‚Äì441. Springer Verlag.\nMichael J Cafarella, Alon Halevy, and Nodira Khous-\nsainova. 2009. Data integration for the rela-\ntional web. Proceedings of the VLDB Endowment,\n2(1):1090‚Äì1101.\nMichael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eu-\ngene Wu, and Yang Zhang. 2008. Webtables: ex-\nploring the power of tables on the web. Proceedings\nof the VLDB Endowment, 1(1):538‚Äì549.\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\nWilliam Yang Wang, and William W. Cohen. 2021.\nOpen question answering over tables and text. In\nInternational Conference on Learning Representa-\ntions.\nL. Deng, Shuo Zhang, and K. Balog. 2019. Table2vec:\nNeural word and entity embeddings for table popu-\nlation and retrieval. Proceedings of the 42nd Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval.\nMichael Glass, Mustafa Canim, AlÔ¨Åo Gliozzo, Saneem\nChemmengath, Rishav Chakravarti, Avi Sil, Feifei\nPan, Samarth Bharadwaj, and Nicolas Rodolfo\nFauceglia. 2020. Capturing row and column seman-\ntics in transformer based question answering over ta-\nbles. Proceedings of the Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL-HLT2020).\nTong Guo and Huilin Gao. 2020. Content enhanced\nbert-based text-to-sql generation.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nM¬®uller, Francesco Piccinno, and Julian Eisensch-\nlos. 2020. TaPas: Weakly supervised table pars-\ning via pre-training. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\n209\nLinguistics, pages 4320‚Äì4333, Seattle, Washington,\nUnited States. Association for Computational Lin-\nguistics.\nSujay Kumar Jauhar, Peter Turney, and Eduard Hovy.\n2016. Tabmcq: A dataset of general knowledge ta-\nbles and multiple-choice questions.\nErnesto Jim ¬¥enez-Ruiz, Oktie Hassanzadeh, Vasilis\nEfthymiou, Jiaoyan Chen, and Kavitha Srinivas.\n2020. Semtab 2019: Resources to benchmark tab-\nular data to knowledge graph matching systems. In\nESWC, pages 514‚Äì530.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nÔ¨Åeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\nrant, and Matt Gardner. 2019. Grammar-based neu-\nral text-to-sql generation.\nYing Liu, Kun Bai, Prasenjit Mitra, and C Lee Giles.\n2007. Tableseer: automatic table metadata extrac-\ntion and searching in digital libraries. In Proceed-\nings of the 7th ACM/IEEE-CS joint conference on\nDigital libraries, pages 91‚Äì100.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables.\nPallavi Pyreddy and W Bruce Croft. 1997. Tintin: A\nsystem for retrieval in text tables. In Proceedings of\nthe second ACM international conference on Digital\nlibraries, pages 193‚Äì200.\nGerard Salton and Michael J McGill. 1986. Introduc-\ntion to modern information retrieval.\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\nMustafa Canim. 2020a. Ad hoc table retrieval using\nintrinsic and extrinsic similarities. In Proceedings of\nThe Web Conference 2020, pages 2479‚Äì2485.\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\nMustafa Canim. 2020b. Ad hoc table retrieval us-\ning intrinsic and extrinsic similarities. In WWW ‚Äô20:\nThe Web Conference 2020, Taipei, Taiwan, April 20-\n24, 2020, pages 2479‚Äì2485. ACM / IW3C2.\nRoee Shraga, Haggai Roitman, Guy Feigenblat, and\nMustafa Cannim. 2020c. Web table retrieval using\nmultimodal deep learning. In Proceedings of the\n43rd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nSIGIR ‚Äô20, page 1399‚Äì1408, New York, NY , USA.\nAssociation for Computing Machinery.\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\nand Xifeng Yan. 2016. Table cell search for question\nanswering. In Proceedings of the 25th International\nConference on World Wide Web, pages 771‚Äì782.\nYibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing\nQin. 2019. Content-based table retrieval for web\nqueries. Neurocomputing, 349:183‚Äì189.\nEllen M. V oorhees and Donna K. Harman. 2005.\nTREC: Experiment and Evaluation in Information\nRetrieval (Digital Libraries and Electronic Publish-\ning). The MIT Press.\nYalin Wang and Jianying Hu. 2002. A machine learn-\ning based approach for table detection on the web.\nIn Proceedings of the 11th International Conference\non World Wide Web, WWW ‚Äô02, page 242‚Äì250, New\nYork, NY , USA. Association for Computing Machin-\nery.\nXiaojun Xu, Chang Liu, and Dawn Song. 2018. SQL-\nNet: Generating structured queries from natural lan-\nguage without reinforcement learning.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8413‚Äì\n8426, Online. Association for Computational Lin-\nguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, Zilin Zhang,\nand Dragomir Radev. 2018. Spider: A large-\nscale human-labeled dataset for complex and cross-\ndomain semantic parsing and text-to-SQL task. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n3911‚Äì3921, Brussels, Belgium. Association for\nComputational Linguistics.\nShuo Zhang and K. Balog. 2018a. Ad hoc table re-\ntrieval using semantic similarity. Proceedings of the\n2018 World Wide Web Conference.\nShuo Zhang and Krisztian Balog. 2018b. Ad hoc ta-\nble retrieval using semantic similarity. In Proceed-\nings of the 2018 World Wide Web Conference, pages\n1553‚Äì1562.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.\nCoRR, abs/1709.00103."
}