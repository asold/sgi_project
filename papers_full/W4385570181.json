{
  "title": "Prompt Discriminative Language Models for Domain Adaptation",
  "url": "https://openalex.org/W4385570181",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2329123625",
      "name": "Ke-ming Lu",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2250905348",
      "name": "Peter Potash",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2122813587",
      "name": "Xihui Lin",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2097923710",
      "name": "Yuwen SUN",
      "affiliations": [
        "Xi’an Jiaotong-Liverpool University"
      ]
    },
    {
      "id": "https://openalex.org/A3154302785",
      "name": "Zihan Qian",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2032612703",
      "name": "Zheng Yuan",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2134067980",
      "name": "Tristan Naumann",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1969234209",
      "name": "Tianxi Cai",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2098876567",
      "name": "Junwei Lu",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385574140",
    "https://openalex.org/W4285169731",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W4309523728",
    "https://openalex.org/W3166204619",
    "https://openalex.org/W4281955959",
    "https://openalex.org/W4285249657",
    "https://openalex.org/W4224088803",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4285214521",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W4226474962",
    "https://openalex.org/W4223492536",
    "https://openalex.org/W4285158728",
    "https://openalex.org/W3004877964",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4221154592",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W2523586319",
    "https://openalex.org/W4224863589",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4286589315",
    "https://openalex.org/W4309589733",
    "https://openalex.org/W4297030216",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4281479158",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W4297253404"
  ],
  "abstract": "Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining.In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.",
  "full_text": "Proceedings of the 5th Clinical Natural Language Processing Workshop, pages 247–258\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nPrompt Discriminative Language Models for Domain Adaptation\nKeming Lu1, Peter Potash2, Xihui Lin2, Yuwen Sun4, Zihan Qian3, Zheng Yuan5\nTristan Naumann2, Tianxi Cai3 and Junwei Lu3\n1University of Southern California 2Microsoft Research 3Harvard University\n4Xi’an Jiaotong-Liverpool University 5Alibaba Damo Academy\n1keminglu@usc.edu 2{Peter.Potash,xihlin,tristan}@microsoft.com\n3{zihanqian,tcai,junweilu}@hsph.harvard.edu\n4Yuwen.Sun19@student.xjtlu.edu.cn 5yuanzheng.yuanzhen@alibaba-inc.com\nAbstract\nPrompt tuning offers an efficient approach\nto domain adaptation for pretrained language\nmodels, which predominantly focus on masked\nlanguage modeling or generative objectives.\nHowever, the potential of discriminative lan-\nguage models in biomedical tasks remains un-\nderexplored. To bridge this gap, we develop\nBIODLM , a method tailored for biomedical\ndomain adaptation of discriminative language\nmodels that incorporates prompt-based contin-\nual pretraining and prompt tuning for down-\nstream tasks. BIODLM aims to maximize\nthe potential of discriminative language mod-\nels in low-resource scenarios by reformulating\nthese tasks as span-level corruption detection,\nthereby enhancing performance on domain-\nspecific tasks and improving the efficiency of\ncontinual pertaining. In this way, BIODLM\nprovides a data-efficient domain adaptation\nmethod for discriminative language models, ef-\nfectively enhancing performance on discrimi-\nnative tasks within the biomedical domain.\n1 Introduction\nRecent years witnessed the development of biomed-\nical pretrained language models (PLMs) (Kalyan\net al., 2022). These domain-specific PLMs con-\ntribute to a large number of downstream tasks in\nthe biomedical domain, such as named entity recog-\nnition (Yuan et al., 2021; Khandelwal et al., 2022;\nWatanabe et al., 2022), entity linking (Zhang et al.,\n2022; Liu et al., 2020), relation extraction (Li et al.,\n2022a; Sarrouti et al., 2022), and question answer-\ning (Jin et al., 2019a; Pappas et al., 2022).\nMost existing domain-specific PLMs rely on\ntremendous in-domain corpus and computing re-\nsources for continual pretraining (Lee et al., 2020;\nRasmy et al., 2021; Yuan et al., 2022; Alsentzer\net al., 2019) or pretraining from scratch (Gu et al.,\n2021; Yasunaga et al., 2022), which could be in-\nfeasible with limited resources. Meanwhile, PLMs\n[Question]Which of the following is the most likely cause of this patient's decreased sensation?[Prompt]Theansweris+[Option]\n[Context]A 67-year-old woman comes to the physician for a follow-up examination. …Examination of the skin shows no abnormalities. Muscle strength is normal. …\n[Opt.A]Cerebral infarction during the hospitalization[Opt.B]Complication of the IVC filter placement[Opt.C]Compression of the lateral femoral cutaneous nerve[Opt.D]Hematoma of the left thighConcat\nScoreFromRTD\nFigure 1: A case for prompting discriminative pretrained\nlanguage models (DLMs) on multi-choice biomedical\nquestion answering. Each option is first concatenated\nwith a predefined hard prompt: “The answer is”. They\nare separately concatenated with the context and ques-\ntion as input. We rank the score from the head of re-\nplaced token detection (RTD) in DLMs to determine the\nbest option.\nfor general purposes usually fails to achieve com-\nparable performance on biomedical tasks with fine-\ntuning compared with in-domain PLMs at the same\nmodel scale (Gu et al., 2021). To combat these is-\nsues, exploring a prompt-based domain adaptation\nmethod that better leverages existing knowledge\nlearned in pretaining is necessary. Recent research\ndemonstrates that prompts or instructions can acti-\nvate the hidden abilities of PLMs (Liu et al., 2022;\nRadford et al., 2019; Brown et al., 2020), includ-\ning cross-domain inference (Yeh et al., 2022; Fries\net al., 2022; Yao et al., 2022b). Therefore, prompt\ntuning on general PLMs can be a data-efficient do-\nmain adaptation method as they are proven promis-\ning on various downstream tasks (Wang et al., 2018,\n2019).\nExisting explorations about prompt-based do-\nmain adaptation mainly focus on PLMs with\nmasked language modeling (Lai et al., 2022; Sung\net al., 2021) or generative objectives (Luo et al.,\n2022). However, we identify that discrimina-\ntive pretrained language models (DLMs) also hold\n247\ngreat potential for prompt-based domain adaptation\nbut remains understudied. DLMs are pretrained to\ndistinguish between alternatives and proved to be\nstronger few-short learners than PLMs with other\ntraining objectives (Xia et al., 2022). Therefore,\nDLMs are better choices for domain adaptation\nsince many downstream tasks in the biomedical do-\nmain focus on discriminative objectives (Gu et al.,\n2021). However, complex model architecture and\ntraining recipes hinder DLMs from efficient adap-\ntation to other domains.\nTo shed light on this topic, we developBIODLM\n(Prompt-based Biomedical Domain Adaptation for\nDiscriminative Language Models), which can effi-\nciently take advantage of the state-of-the-artDLMs\nin the general domain. BIODLM is a prompt-based\nbiomedical domain adaptation method designed ex-\nplicitly for DLMs, including prompt-based contin-\nual pertaining and prompt tuning for downstream\ntasks. Inspired by Xia et al. (2022), we first formu-\nlate discriminative downstream tasks in the biomed-\nical domain, such as multi-choice question answer-\ning, as span-level corruption detection.\nAs shown in Fig. 1, this prompt tuning reformu-\nlation allows general-domain DLMs to be used as\nzero-shot or few-shot learners in biomedical tasks,\nwhich is also supported by our probing experiments\nin §4.2. We develop an efficient prompt-based con-\ntinual pretraining method to further enhance the\nperformance of DLMs on biomedical tasks. As Ba-\njaj et al. (2022) revealed, the selection of corrupted\ntokens and the corruption methods play a vital role\nin pretraining DLMs and is highly related to the\nperformance on downstream tasks. BIODLM se-\nlects domain-specific words, defined as different\nvocabulary between in-domain and general models,\nas corrupted tokens to lead the continual pretraining\nfocusing on new domain knowledge and improve\npretraining efficiency. For corruption, BIODLM\nemploys fixed in-domain PLMs as encoders to cor-\nrupt selected tokens instead of co-training encoders\nand decoders in DLMs. BIODLM is a flexible do-\nmain adaptation method that can be applied to any\nexisting DLMs.\nThe contributions of this work are mainly two-\nfold. First, we explore prompt tuning general-\ndomain DLMs on various biomedical downstream\ntasks, showing prompting DLMs has significant\npotential on these tasks under low-resource scenar-\nios. Second, we develop a data-efficient continual\npretraining method based on replaced token detec-\ntion, which employs in-domainPLMs as generators\nto corrupt domain-specific words in the biomedi-\ncal corpus. In summary, BIODLM efficiently im-\nproves low-resource performance on discriminative\ntasks in the biomedical domain.\n2 Related Works\nDiscriminative PLMs. Discriminative PLMs\n(DLMs) incorporate replaced token detection\n(RTD) or other discriminative objectives during\npretraining. Clark et al. (2020) first propose a dis-\ncriminative pretraining method, which trains a gen-\nerator to create replaced tokens and a discriminator\nto distinguish between real and replaced tokens.\nThis approach increases the pretraining efficiency\nby reducing the computation required in the head\ncompared with previous masked language mod-\neling. Meng et al. (2021) further improves the\nRTD to corrective language modeling, which re-\nquires both RTD and language modeling for cor-\nrecting the replaced tokens. Bajaj et al. (2022)\nproposes a more stable and efficient training recipe\nfor DLMs. In this work, we explore domain adap-\ntation for these methods in the biomedical domain.\nWe use METRO-LM (Bajaj et al., 2022) in our ex-\nperiments of BIODLM since it demonstrates the\nbest performance on general benchmarks, such as\nGLUE (Wang et al., 2018) and SuperGLUE (Wang\net al., 2019).\nPrompt tuning for DLMs. Prompt tuning for\nDLMs is an emerging topic in general and biomedi-\ncal domains. Ni and Kao (2022) presents empirical\nevidence showing that ELECTRA can perform\nwell on downstream tasks without fine-tuning or\nadditional training. Xia et al. (2022) introduces a\nprompt-based fine-tuning approach that leverages\ndiscriminative prompts to guide the model towards\nlearning specific downstream tasks with only a few\nexamples. Li et al. (2022b) proposes a few-shot\nlearning approach with pre-trained token-replaced\ndetection models to transform traditional classi-\nfication and regression tasks into token-replaced\ndetection problems. Yao et al. (2022a) suggests\nfine-tuning DLMs with prompts for task-specific\ndownstream tasks by adding a small number of\ntask-specific parameters as a prompt to guide the\nmodel’s output. However, these works are limited\nto a single method ELECTRA and do not explore\nbiomedical tasks. We follow the recipe of prompt\ntuning in Xia et al. (2022) and use it on biomedical\ndiscriminative tasks.\n248\nBiomedical Domain Adaptation. Biomedical\ndomain adaptation of PLMs is a fast-developed\ntopic summarized adequately in the survey from\nKalyan et al. (2022). Therefore, we only provide a\nhighly selected review. Alrowili and Vijay-Shanker\n(2021) propose a novel method for pre-training\nlarge biomedical language models that combine\nBERT, ALBERT, and ELECTRA architectures.\nRaj Kanakarajan et al. (2021) propose a biomedi-\ncal domain-specific language encoder model that\nextends ELECTRA to obtain state-of-the-art perfor-\nmance on numerous biomedical natural language\nunderstanding benchmarks. Tinn et al. (2023) pro-\npose PubmedELECTRA, a domain-specific version\nof ELECTRA by continually pertaining ELEC-\nTRA on PubMed articles. Luo et al. (2022) pro-\npose a generative pre-trained Transformer language\nmodel on a large corpus of biomedical articles\nfor biomedical text generation and mining. Our\nmethod, BIODLM proposes another perspective\nthat employs prompt-based continual pretraining\nto adapt DLMs to the biomedical domain, which is\nunderstudied in this topic.\n3 Methods\nWe describe preliminaries (§3.1), prompt-based\ncontinual pretraining with RTD (§3.2), and prompt\ntuning for discriminative PLMs (§3.3).\n3.1 Preliminaries\nReplaced Token Detection. BIODLM is a prompt-\nbased method based on the RTD task. RTD is one\nof the core pretraining objectives of DLMs (Clark\net al., 2020). During the pretaining of DLMs, the\ninput is a sequence of tokens x = {xi}n\ni=1, where\nn is the length of input sequences. A random set of\ntokens in this sequence is selected and corrupted\nwith a generator by masked language modeling.\nPredictions from the generator will be used to re-\nplace the original tokens to obtain a corrupted input\n˜x = {˜x}n\ni=1. At the same time, token-level binary\nlabels are constructed by y = {I(xi = ˜xi)}n\ni=1,\nwhere I(·) is the indicator function1. The discrim-\ninator of DLMs is trained with token-level classi-\nfication on the corrupted input and corresponding\nlabels to detect the replaced tokens.\nMethod Overview. Similar to the “pretraining-\nand-finetuning” workflow, BIODLM involves a\nprompt-based continual pretraining (§3.2) and a\n1The definition of labels may vary in different DLMs. Our\nintroduction follows the recipe in Bajaj et al. (2022).\nprompt-tuning method on downstream tasks (§3.3).\nAs shown in Fig. 2, BIODLM first builds a domain-\nspecific vocabulary for the prompt-based continual\npertaining. Then, we corrupt the original biomedi-\ncal corpus with a fixed in-domain language model\nas the generator. The corrupted corpus is used to\ntrain the general-domain discriminator with RTD\nfor domain adaptation. After the continual pertain-\ning, we explore prompt tuning with RTD to apply\nBIODLM to biomedical downstream tasks. We\nreformulate biomedical discriminative tasks into\nsingle-token or multi-token RTD, as the example\nin Fig. 1. BIODLM can also be further tuned on\na reformulated training set with RTD objective to\nenhance downstream performance.\n3.2 Prompt-based Continual Pretraining\nContinual pretraining on in-domain corpus signifi-\ncantly improve downstream performance on down-\nstream tasks (Gu et al., 2021). However, unlike\nother training objectives, pretraining with RTD re-\nquires self-supervised training corpus construction\nwith corruption. Therefore, we develop a prompt-\nbased continual pretraining method to adapt DMLs\nto the biomedical domain. The continual pretrain-\ning involves a token corruption generator and an\nRTD discriminator. The recipe of token corruption\nis essential for both efficiency and effectiveness\nof the pretraining of DLMs (Bajaj et al., 2022).\nTherefore, we design a corrupted token selection\nrecipe focusing on in-domain vocabulary and em-\nploy fixed in-domain PLMs as generators to corrupt\nthese tokens.\nCorrupted Token Selection. Corrupted token se-\nlection aims to select the tokens in the in-domain\ncorpus that the generator will corrupt. We first\nbuild a domain-specific vocabulary by extracting\ndifferent tokens from in-domain to general-domain\nvocabulary. The first challenge is that in-domain\nand general language models may have very differ-\nent tokenizers. However, most of them share sim-\nilar pre-tokenizers to segment context into words.\nTherefore, we conduct word-level corruption in-\nstead of token-level corruption in the traditional de-\nsign of RTD so that in-domain and general-domain\nvocabulary can be aligned with each other in the\ncorruption. The detailed selection recipe is de-\nscribed below:\n1. We filter tokens that are in in-domain vocabulary\nbut not in the general-domain vocabulary.\n2. To conduct word-level corruption, we filter out\n249\nHeterotopicpancreas,alsoknownasectopicpancreas,ispancreatictissuelocatedoutsidethepancreaticductwithoutvascularorlymphaticcommunicationwiththegland.\nFixedIn-domainLMasGenerator(MLMInference)General-domainDiscriminator(RTDTraining)\nHeterotopic pancreas, also known as [MASK]pancreas, is pancreatic tissue located outside the pancreaticparenchymawithout vascular or ductalcommunication with the gland.\nDetectionCorruption\nectopicectopicpancreaticpancreaticparenchymaductductallymphatic\nIn-domainPLMVocabularyGeneral-domainPLMVocabulary\nDomain-specificVocabulary•ectopic•pancreatic•parenchyma•ductal•…\nAnnotate\nOriginalBiomedicalCorpus\nCorruptedBiomedicalCorpus\nFigure 2: Overview of prompt-based continual petraining in BIODLM . A vocabulary is collected by differing\nin-domain and general-domain PLMs vocabulary. And we annotate the in-domain corpus with this vocabulary and\nuse this annotation as a set of words for sampling corrupted tokens. Selected tokens are corrupted with a fixed\nin-domain language model as the generator via masked language modeling inference. The corrupted corpus is then\nused to continually pretrain a general-domain discriminator with replaced token detection.\nall tokens that are not a whole word in the set of\ntokens we collect in the previous step.\n3. We tokenize the remained words in the previous\nstep with the tokenizer of general-domain DLM\nand filter out any words that contain “unknown”\ntokens2. The rest are our domain-specific vocab-\nulary D.\nWe use the vocabulary of PubmedBERT (Gu et al.,\n2021) as our in-domain vocabulary and the vocab-\nulary of MetroLM (Bajaj et al., 2022) as general-\ndomain vocabulary. We eventually have 12,919\nwords remaining in domain-specific vocabulary D.\nMost words in Dare biomedical terms, and a sam-\nple is listed in §4.3.\nToken Corruption. With domain-specific vocabu-\nlary D, we employ fixed in-domain LM as a gen-\nerator to corrupt the in-domain corpus with the\ninference of masked language modeling. Given an\ninput of the in-domain corpus, such as a PubMed\nabstract3, we sample a fixed proportion of words in\nthe input to corrupt. We follow Clark et al. (2020)\nto set the percentage to 30%. We first pre-tokenize\nit into words x = {xi}n\ni=1, where the length of\nword sequence is n. Then, we identify any domain-\nspecific words in D, denoting them as a bag of\nwords C. The words for corruption are sampled\n2These “unknown” tokens refer to out-of-vocabulary to-\nkens in the general-domain tokenizer, such as the “[UNK]”\ntoken in the MetroLM (Bajaj et al., 2022).\n3PubMed Official Site: https://pubmed.gov\nwith a strategy that favors domain-specific words:\n• |C|>⌊0.3n⌋: We randomly select 0.3n words\nfrom Cas candidates for corruption.\n• |C|⩽ ⌊0.3n⌋: We randomly select ⌊0.3n⌋−|C|\nwords from the rest of the input to meet the re-\nquirement of the proportion of corrupted words.\nThis strategy ensures domain-specific words will be\ncorrupted first, which leads the pretraining to focus\non domain knowledge and enhances pretraining\nefficiency. After identifying the candidates, each\nword in the candidates will be replaced with a mask\ntoken, such as “[MASK]” in the PubmedBERT, and\nconduct inference of whole-word masked language\nmodeling with the in-domain PLM. The predictions\nfrom the in-domain PLM then replace the words\nin the original inputs to obtain the corrupted in-\ndomain training corpus.\nTraining. We use the corrupted biomedical cor-\npus for continual pretraining general-domain dis-\ncriminators with RTD. We conduct word-level\ncorruption—all tokens in corrupted words are la-\nbeled with “replaced” and the rest are “original”.\nOtherwise, continual pretraining is the same as\n§3.1.\n3.3 Prompt Tuning with RTD\nWe explore prompt tuning withRTD on biomedical\ndownstream tasks in BIODLM. Prompt tuning en-\nables DLMs to conduct low-resource inference and\n250\nhelps DLMs better leverage pretraining knowledge\nin the general domain. Here, we introduce how\nto reformulate inputs of biomedical discriminative\ntasks to conduct low-resource inference with RTD.\nInput Reformulation. We follow the recipe from\nXia et al. (2022) to prompt DLMs on biomed-\nical downstream tasks. We denote the context\nas C and labels as y = {yi}c\ni=1 of discrimina-\ntive tasks, where c is the number of labels. We\nfirst verbalize labels with predefined words or\ntemplates and denote the verbalized templates as\nT(y) = {t(yi)}c\ni=1, where t(·) is a manually de-\nsigned verbalizer for each label. For example, la-\nbels from a binary classification task are verbalized\nas “yes” and “no”. As for multi-choice question\nanswering, the labels are already phrases so no ver-\nbalization will be applied. Each verbalized label is\nconcatenated with context and a predefined prompt\nas inputs, denoting as x = {C ⊕t(yi)}c\ni=1, where\n⊕is the text concatenation operation. The inputs\nare fed into the DLMs, and we collect scores from\nthe RTD head within the spans of labels as outputs.\nThe RTD head classifies tokens in labels into “re-\nplaced” or “original”, where “original” suggests\nthe correct answer to the discriminative problem.\nThe classification scores from the RTD head re-\nveal the semantic correlation between the context\nand verbalized labels. When verbalized labels are\ntokenized into more than one token, we use the\naverage RTD scores as the score of these labels.\nHowever, the RTD head aims to identify token-\nlevel corruption, so averaging multiple tokens do\nnot align well with the pretraining objective and\npotentially hinders the performance of prompt in-\nference. Therefore, we separately analyze single-\ntoken and multi-token labels in this work. This re-\nformulation allows us to conduct zero-shot prompt\ninference with DLMs on biomedical discriminative\ntasks.\nFig. 1 shows a case that we apply prompt in-\nference for DLMs on a multi-choice biomedical\nquestion answering dataset. The context is made\nof a description of the patient background marked\nin blue and a question marked in green. Then, it is\nconcatenated with four options individually, with a\npredefined prompt, “The answer is”. We consider\nthe average RTD score in each option span as the\nclassification score. And we select the option with\nthe highest average RTD score as the prediction.\nTraining. In addition to the zero-shot inference, we\nalso conduct prompt tuning on downstream tasks.\nWith the input reformulation described before, dis-\ncriminative tasks can be reformulated as multi-label\nbinary classification tasks. We further tune the pa-\nrameters of DLMs in this way to conduct few-shot\nand fully supervised inference.\n4 Experiments\nThis section introduces an experimental evaluation\nof prompting discriminative PLMs for biomedical\ndomain adaptation. We describe the experimental\nsetup (§4.1), main results (§4.2), and ablation study\n(§4.3) on incorporated techniques.\n4.1 Experimental Setup\nTraining corpus. The biomedical corpus for the\ncontinual pretraining in this work is the PubMed\nabstracts in the PubMed Central (PMC) Open Ac-\ncess (OA) Subset4 (Gamble, 2017; Bethesda, 2003).\nWe process this dump with the open-source tool\npubmed_parser5 (Achakulvisut et al., 2020) to ex-\ntract abstracts of articles. We then follow the pre-\nprocessing recipe of Bajaj et al. (2022) and segment\nthe corpus into paragraphs. The original PMC\nOA Subset contains 21 million paragraphs from\nbiomedical journal articles. We only randomly se-\nlect three million paragraphs for continual pretrain-\ning due to the limitation of computation resources.\nBenchmarks. We evaluate BIODLM on five\npublic biomedical datasets: (1) PubmedQA (Jin\net al., 2019b) contains 1k expert-labeled question-\nanswer pairs based on PubMed abstracts with\nyes/no/maybe multiple-choice answers. (2)\nBioASQ (Tsatsaronis et al., 2012) is a large\nquestion-answering dataset containing biological\nquestions and answers, and related biomedical pa-\npers and abstracts. (3) MedQA(USMLE) (Jin\net al., 2021) is a question-answering dataset\ncontaining multiple-choice questions and related\nanswer options in US Medical License Exam\n(USMLE) format, which were obtained with a\nchoice of 4 or 5 possible answers from the National\nMedical Board Examination in the United States.\n(4) MMLU (Professional Medicine) (Hendrycks\net al., 2020) involves difficult exam questions con-\nsisting of four multiple-choice questions with cor-\nresponding answers in the biomedical domain. (5)\nMedMCQA (Pal et al., 2022) is a new large-scale\n4PMC OA Subset: https://www.ncbi.nlm.nih.gov/\npmc/tools/openftlist/\n5Github repository of pubmed_parser: https://github.\ncom/titipata/pubmed_parser\n251\nDataset Size Random Zero-shot (RTD Prompt) Fully Supervised (CLS Finetuning)\nMetroLM Electra BioElectra MetroLM Electra BioElectra PubmedBERT\nPubmedQA 500 33.3 64.0 58.0 48.0 63.8 57.0 62.2 55.8\nSingleBioASQ 140 50.0 74.3 73.6 67.1 94.3 73.6 75.7 87.6\nMedQA(USMLE) 1273 25.0 25.3 22.1 19.5 28.1 27.4 40.3 39.3\nMMLU 272 25.0 25.7 19.9 20.6 27.6 25.8 44.1 29.1\nMultiMedMCQA* 4183 25.0 26.6 26.8 20.7 35.5 34.8 40.8 41.2\nMacro Avg. 31.7 43.2 40.1 35.2 49.8 43.7 52.6 50.6\nTable 1: Probing experiment results display the zero-shot performance with the RTD prompt of various DLMs on\nthe test sets of our benchmark. We also report the CLS-based finetuning performance in the full training setting and\ninvolve an in-domain PLM, PubmedBERT, for comparison. We report accuracy on each data split and the macro\naverage accuracy on our benchmark. The best zero-shot performance on each dataset is marked in bold. * We report\nperformance on the development set of MedMCQA since we have not received official scores on the test set.\nDataset Random 0% (Zero-shot) 10% (Few-shot) 100% (Full)\nCLS Prompt B IODLM CLS Prompt B IODLM CLS Prompt B IODLM\nPubmedQA 33.3 31.1 64.0 57.0 56.0 62.8 58.0 63.8 69.9 66.8\nSingle BioASQ 50.0 35.2 74.3 77.1 77.9 77.9 80.0 94.3 85.3 89.8\nMedQA(USMLE) 25.0 9.8 25.3 27.7 26.5 25.7 29.1 28.1 27.0 29.6\nMMLU 25.0 11.0 25.7 26.8 21.7 30.8 32.7 27.6 31.2 31.6\nMultiMedMCQA* 25.0 6.4 26.6 27.4 30.7 22.9 30.1 35.5 27.2 33.2\nMacro Avg. 33.7 18.7 43.1 43.4 42.6 44.1 49.9 50.0 48.1 50.2\nTable 2: Results of BIODLM in the zero-shot, few-shot, and full settings compared with finetuning CLS representa-\ntions on the test sets of our benchmark. We use MetroLM as the backbone in BIODLM for results in this table.\nThe prompt baseline is MetroLM with RTD prompt tuning without continual pretraining in BIODLM . We report\naccuracy on each data split and the macro average accuracy on our benchmark. Finetuning CLS requires the training\nof a classification head, so we conduct zero-shot inference of CLS representations by semantic matching between\ncontext and options. The best accuracy on each dataset in each setting is marked in bold. * We report performance\non the development set of MedMCQA since we have not received official scores on the test set.\nMultiple-Choice Question Answering dataset con-\ntaining about 194k 4-option multiple-choice ques-\ntions from Indian medical entrance exams (AI-\nIMS/NEET). In our benchmark, PubmedQA and\nBioASQ are single-token datasets as their labels\nare short as “yes/no/maybe”. However, other multi-\ntoken datasets, such as MedQA(USMLE), are more\nchallenging since they have longer options and at\nleast four options. We report accuracy scores on the\ntest sets. And we only report the performance on\nthe development set on MedMCQA since we have\nnot received official feedback for the test scores.\nBaselines. In the probing experiments, we consider\nElectra and BioElectra as baselines for MetroLM.\nWe also include PubmedBERT for reference. Elec-\ntra (Clark et al., 2020) is a PLM that uses replaced\ntoken detection as a self-supervised task for lan-\nguage representation learning. The central concept\nof Electra is to train a text encoder to identify input\ntokens from high-quality negative samples gener-\nated by a small generator network, resulting in\nsuperior performance on downstream tasks com-\npared to conventional masked language modeling.\nBioELECTRA (Raj Kanakarajan et al., 2021) is\na biomedical PLM adapted from the ELECTRA\nmodel for the biomedical domain. It is pretrained\nfrom scratch on the biomedical domain-specific\ntext and achieves state-of-the-art performance on\nvarious biomedical NLP tasks, demonstrating that\npretraining from scratch with biomedical domain\ntext enhances the model’s capacity. PubMed-\nBERT (Gu et al., 2021) is a biomedical PLM\nthat has been pretrained on PubMed abstracts. It\nachieves state-of-the-art results in several bench-\nmark datasets, making it a strong baseline model\nfor biomedical language understanding tasks. We\nalso include a random baseline, which is the accu-\nracy based on a random guess.\nConfigurations. We develop BIODLM based on\na strong discriminative pretrained language model\nMetroLM-base (Bajaj et al., 2022). This DLM\ndemonstrates the best zero-shot performance in\nour probing experiments described in §4.2. We run\ncontinual pretraining on 8 NVIDIA V100 GPUs\n252\nfor 10 hours and evaluation on each dataset in our\nbenchmark on 1 NVIDIA V100 GPU for less than\n1 hour. The hyper-parameters are determined with\nthe grid search based on the accuracy of the devel-\nopment set. Detailed hyper-parameters are shown\nin Appx. §A.\n4.2 Results\nWe first show the results of a probing experiment\nto demonstrate DLMs are zero-shot learners on\nbiomedical tasks. Then we present our main results\nto show the effectiveness of BIODLM on both full-\nand low-resource scenarios on our benchmark.\nProbing Experiments. Tab. 1 shows the probing\nexperiment results about the zero-shot performance\nof three DLMs with RTD prompt. We also report\nfinetuning results based on the CLS representations\nof these DLMs, along with the zero-shot prompt\ntuning performance. First, we notice that models\nwith zero-shot RTD prompt tuning even outperform\ntheir finetuning counterparts on several datasets,\nmarked with the underline in Tab. 1. For exam-\nple, the accuracy of MetroLM with zero-shot RTD\nprompt tuning in PubmedQA is 64.0, 0.2 absolute\npercentage higher than its fully supervised finetun-\ning counterpart. Similar cases are also witnessed\nin other DLMs, such as Electra on the test split\nof PubmedQA and BioElectra on the development\nset of BioASQ. These cases show that prompt tun-\ning of general-domain DLMs has great potential\nas zero-shot learners on biomedical tasks. And\nthese results also provide evidence that reformu-\nlating biomedical discriminative tasks as replaced\ntoken detection contributes to leveraging general-\ndomain knowledge in pertaining, which is proposed\nin §3.3. Furthermore, MetroLM significantly out-\nperforms other DLMs on most datasets, achiev-\ning 43.2 macro average accuracy. Therefore, we\nchoose MetroLM as the backbone to conduct the\nfollowing experiments and analyses of BIODLM .\nTab. 8 in Appx. §B is an extended version of Tab. 1\ncontaining results on both development and test\nsets.\nMain Results. Tab. 2 shows the main results of\nBIODLM in the zero-shot, few-shot, and fully su-\npervised settings on the test sets of our bench-\nmark. In the zero-shot setting, BIODLM out-\nperforms MetroLM with only prompt tuning on\nmost datasets, improving macro average accuracy\nby 0.3 percent. We conduct zero-shot inference\nwith CLS representations by semantic matching\nbetween context and options based on CLS repre-\nsentations. However, it can not perform well in\nthe zero-shot setting since the context and options\nare significantly different. In the few-shot setting,\nthe macro average accuracy of BIODLM is higher\nthan CLS and prompt methods by 7.3% and 5.8%,\nrespectively. These results prove that BIODLM\nenables general-domain DLMs to conduct infer-\nence on biomedical downstream tasks under low-\nresource scenarios. Furthermore, even though the\ntraditional finetuning method outperforms prompt\ntuning in the fully supervised setting by 1.9% in\naccuracy, we notice BIODLM still slightly outper-\nforms the finetuning method by 0.2% on macro\naverage accuracy. This observation suggests that\nBIODLM benefits from the prompt-based contin-\nual pertaining. And we summarize that BIODLM\nis a better choice under low-resource scenarios, but\nboth traditional CLS finetuning and BIODLM per-\nform well with adequate supervision.\n4.3 Study\nWe provide the following analyses to evaluate fur-\nther the core components of BIODLM , including\ncorruption methods, prompt templates, and domain-\nspecific vocabulary.\nCorruption Methods. In this analysis, we con-\nduct ablation study experiments to demonstrate the\neffectiveness and data efficiency of the corruption\nmethod proposed in BIODLM . We design a ran-\ndom strategy that randomly selects 30% words in\nthe input as the baseline of the domain-specific to-\nken selection strategy for corruption. As for the\ngenerator, we use the general-domain pretrained\nlanguage model BERT as the baseline of the in-\ndomain pretrained language model PubmedBERT.\nWe conduct continual pretraining on different com-\nbinations of corrupted token selection and genera-\ntors with 1 million to 3 million samples.\nTab. 3 shows the results of the ablation study\non corruption models of BIODLM. Comparing the\nrandom and domain-specific token selection, we\nnotice the macro average accuracy on the bench-\nmark of the domain-specific strategy is consistently\nhigher than that of the random strategy. Within\neach corrupted token selection strategy, Pubmed-\nBERT, as the generator, outperforms BERT in most\ncases, showing that fixed in-domain PLMs with\nmore precise corruption benefit the continual pre-\ntraining in BIODLM . Furthermore, we notice the\ndomain-specific token selection strategy with Pub-\n253\nToken Selection Generator Pretraining Samples\n1M 2M 3M\nRandom BERT 49.3 49.6 50.0\nPubmedBERT 49.3 49.5 49.9\nDomain-specific BERT 49.7 50.4 51.8\nPubmedBERT 50.2 50.9 52.1\nTable 3: Ablation study on corruption methods. We\ncompare two token selection recipes based on Random\nand In-domain vocabulary and two generators BERT\n(general-domain) and PubmedBERT (in-domain), with\npertaining samples from 1 million to 3 million. We re-\nport macro average accuracy scores on our benchmark.\nmedBERT as the generator used in BIODLM with\nonly 1 million training samples can outperform\nthe random token strategy with 3 million training\nsamples. This result provides valuable insight that\ncorruption methods in BIODLM can significantly\nimprove data efficiency in continual pretraining.\nPrompt Templates. Prompt templates play a vital\nrole in prompt tuning. We adopt manually designed\nprompt templates in BIODLM to verbalize labels\nand reformulate inputs of discriminative tasks. To\nbetter evaluate the influence of manual template\ndesign, we construct three prompt templates for\ntwo biomedical question answering datasets:\n• Template A: “[Context]. [Question]? The an-\nswer is [prompt label].”\n• Template B: “[Context] [Question]? The answer\nis [prompt label].”\n• Template C: “Context: [Context]. Question:\n[Question]? The answer is [prompt label].”\nThere are only minor differences among these\ntemplates. Using each prompt template, we then\nrun zero-shot inference with MetroLM and RTD\nprompt on two datasets. Tab. 5 shows that the de-\nsign of prompt templates may influence zero-shot\nperformance, which could be related to the specific\ndataset. It is worth noticing that prompt template\nB only slightly differs from prompt template A but\nperformance on the test set of BioASQ dropped by\nhalf, suggesting an obvious spurious correlation on\nthe punctuation in prompt templates.\nWe also conduct additional prompt ablation stud-\nies on the multi-token prompt datasets. We have\nmanually designed two prompts for multi-choice\nquestion-answering datasets in our benchmark:\n• Template D: “[Context]. [Question]? The an-\nswer is [Option].”\nPrompt MedQA(USMLE) MMLU MedMCQA\ndev test dev test dev\nD 27.6 25.3 38.7 25.7 26.6\nE 25.9 25.1 31.2 23.9 24.0\nTable 4: Zero-shot accuracy of MetroLM with RTD\nprompting on multi-token prompt datasets with two\nmanually designed prompt templates.\nPrompt PubmedQA BioASQ\ndev test dev test\nA 50.2 64.0 72.0 74.3\nB 50.2 64.0 62.9 38.7\nC 48.6 68.0 71.7 72.3\nTable 5: Zero-shot accuracy of MetroLM with RTD\nprompting on BiomedQA and BioASQ with three man-\nually designed prompt templates.\n• Template E: “[Context] [Question]? The answer\n[Option] is [right/wrong].”\nThe underlined spans include tokens for the RTD.\nTemplate D is used in our main results, while\ntemplate E reformulates multi-token prompts into\nsingle-token prompts by simply judging whether\nthe option is right or wrong. Tab. 4 shows the\nresults of these two templates. Template D con-\nsistently outperforms template E, suggesting di-\nrect RTD on the option spans works better in our\nmulti-token prompt datasets. Therefore, prompt\ntemplates need to be carefully designed to achieve\nthe best performance on each dataset.\nDomain-specific Vocabulary.We present a brief\ncase study of vocabulary differences between in-\ndomain and general-domain PLMs to justify our\ndesign in the corrupted token selection. §4.3 shows\ncases in the domain-specific vocabulary and their\ncorresponding categories. Most words in this vo-\ncabulary fall into categories such as Gene, Protein,\nDisease, Chemical, and Drug. These categories\ncontain rich biomedical terms frequently used in\nthe downstream tasks. Therefore, continual pre-\ntraining on the domain-specific vocabulary helps\nDLMs focus on biomedical knowledge and im-\nproves data efficiency of domain adaptation.\n5 Conclusion\nWe study an efficient way to adapt general-domain\nDLMs to the biomedical domain and propose\nBIODLM. BIODLM consists of data-efficient con-\ntinual pretraining that focuses on domain-specific\nvocabulary and leverages domain knowledge in the\n254\nCategories Words\nGene & Protein TGFβ1, IGF1R,\nphosphatases,Synaptophysin\nDisease\nAdenomatous,malarial,\natherosclerotic,\ncholangiocarcinoma\nChemical & Drug\nPhosphatidylcholine,\ncycloheximide,azithromycin,\nminocycline,hygromycin,\nMethylprednisolone\nTable 6: A case study of domain-specific vocabulary\nused for continual pretraining. We present randomly-\nselected words and their categories in this vocabulary.\nin-domain PLMs by employing them as RTD gen-\nerators. We also conduct experiments on a biomedi-\ncal benchmark with six biomedical datasets, verify-\ning that prompt tuning is an effective way to adapt\nDLMs on biomedical discriminative tasks directly.\nFuture works include extending BIODLM to more\nDLMs, such as ELECTRA (Clark et al., 2020) and\nCOCO-LM (Meng et al., 2021), and experimenting\nwith BIODLM on other discriminative tasks in the\nbiomedical domain.\nLimitations\nBIODLM adopts DLMs as backbone models.\nCompared to PLMs with other training objectives,\nDLMs may miss language modeling benefits and\nsqueeze representation space. Besides, our bench-\nmarks can be extended to more biomedical discrim-\ninative tasks, such as relation extraction, document\nclassification, and entity disambiguation. We con-\nsider extending our exploration to more DLMs and\nbiomedical tasks as valuable future works.\nEthics Statement\nAll datasets in our benchmark and continual pre-\ntraining are obtained according to each dataset’s\nrespective data usage policy.\nReferences\nTitipat Achakulvisut, Daniel Acuna, and Konrad Ko-\nrding. 2020. Pubmed parser: A python parser for\npubmed open-access xml subset and medline xml\ndataset xml dataset. Journal of Open Source Soft-\nware, 5(46):1979.\nSultan Alrowili and K Vijay-Shanker. 2021. Biom-\ntransformers: building large biomedical language\nmodels with bert, albert and electra. In Proceedings\nof the 20th Workshop on Biomedical Language Pro-\ncessing, pages 221–227.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nPayal Bajaj, Chenyan Xiong, Guolin Ke, Xiaodong Liu,\nDi He, Saurabh Tiwary, Tie-Yan Liu, Paul Bennett,\nXia Song, and Jianfeng Gao. 2022. Metro: Efficient\ndenoising pretraining of large scale autoencoding lan-\nguage models with model generated signals. arXiv\npreprint arXiv:2204.06644.\nBethesda. 2003. PMC Open Access Subset. National\nLibrary of Medicine.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nJason Fries, Leon Weber, Natasha Seelam, Gabriel Al-\ntay, Debajyoti Datta, Samuele Garda, Sunny Kang,\nRosaline Su, Wojciech Kusa, Samuel Cahyawijaya,\net al. 2022. Bigbio: A framework for data-centric\nbiomedical natural language processing. Advances\nin Neural Information Processing Systems, 35:25792–\n25806.\nAlyson Gamble. 2017. Pubmed central (pmc). The\nCharleston Advisor, 19(2):48–54.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019a. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n255\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019b. Pubmedqa: A\ndataset for biomedical research question answering.\narXiv preprint arXiv:1909.06146.\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan,\nand Sivanesan Sangeetha. 2022. Ammu: a sur-\nvey of transformer-based biomedical pretrained lan-\nguage models. Journal of biomedical informatics ,\n126:103982.\nAnshita Khandelwal, Alok Kar, Veera Raghavendra\nChikka, and Kamalakar Karlapalem. 2022. Biomedi-\ncal NER using novel schema and distant supervision.\nIn Proceedings of the 21st Workshop on Biomedi-\ncal Language Processing, pages 155–160, Dublin,\nIreland. Association for Computational Linguistics.\nZhaohong Lai, Biao Fu, Shangfei Wei, and Xiaodong\nShi. 2022. Continuous prompt enhanced biomedical\nentity normalization. In Natural Language Process-\ning and Chinese Computing: 11th CCF International\nConference, NLPCC 2022, Guilin, China, Septem-\nber 24–25, 2022, Proceedings, Part II, pages 61–72.\nSpringer.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nLishuang Li, Ruiyuan Lian, Hongbin Lu, and Jingyao\nTang. 2022a. Document-level biomedical relation\nextraction based on multi-dimensional fusion infor-\nmation and multi-granularity logical reasoning. In\nProceedings of the 29th International Conference\non Computational Linguistics , pages 2098–2107,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nZicheng Li, Shoushan Li, and Guodong Zhou. 2022b.\nPre-trained token-replaced detection model as few-\nshot learner. arXiv preprint arXiv:2203.03235.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2020. Self-alignment\npretraining for biomedical entity representations.\narXiv preprint arXiv:2010.11784.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBiogpt: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6).\nYu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett,\nJiawei Han, Xia Song, et al. 2021. Coco-lm: Cor-\nrecting and contrasting text sequences for language\nmodel pretraining. Advances in Neural Information\nProcessing Systems, 34:23102–23114.\nShiwen Ni and Hung-Yu Kao. 2022. Electra is a zero-\nshot learner, too. arXiv preprint arXiv:2207.08141.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on Health,\nInference, and Learning, pages 248–260. PMLR.\nDimitris Pappas, Prodromos Malakasiotis, and Ion An-\ndroutsopoulos. 2022. Data augmentation for biomed-\nical factoid question answering. In Proceedings of\nthe 21st Workshop on Biomedical Language Process-\ning, pages 63–81, Dublin, Ireland. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nKamal Raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021. Bioelectra: pre-\ntrained biomedical text encoder using discriminators.\nIn Proceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 143–154.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and\nDegui Zhi. 2021. Med-bert: pretrained contextual-\nized embeddings on large-scale structured electronic\nhealth records for disease prediction. NPJ digital\nmedicine, 4(1):86.\nMourad Sarrouti, Carson Tao, and Yoann Mamy Ran-\ndriamihaja. 2022. Comparing encoder-only and\nencoder-decoder transformers for relation extraction\nfrom biomedical texts: An empirical study on ten\nbenchmark datasets. In Proceedings of the 21st Work-\nshop on Biomedical Language Processing, pages 376–\n382, Dublin, Ireland. Association for Computational\nLinguistics.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? arXiv\npreprint arXiv:2109.07154.\nRobert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xi-\naodong Liu, Tristan Naumann, Jianfeng Gao, and\nHoifung Poon. 2023. Fine-tuning large neural lan-\nguage models for biomedical natural language pro-\ncessing. Patterns, 4(4).\nGeorge Tsatsaronis, Michael Schroeder, Georgios\nPaliouras, Yannis Almirantis, Ion Androutsopoulos,\nEric Gaussier, Patrick Gallinari, Thierry Artieres,\nMichael R Alvers, Matthias Zschunke, et al. 2012.\nBioasq: A challenge on large-scale biomedical se-\nmantic indexing and question answering. In AAAI\nfall symposium: Information retrieval and knowledge\ndiscovery in biomedical text. Arlington, V A: Citeseer.\n256\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nTaiki Watanabe, Tomoya Ichikawa, Akihiro Tamura,\nTomoya Iwakura, Chunpeng Ma, and Tsuneo Kato.\n2022. Auxiliary learning for named entity recogni-\ntion with multiple auxiliary biomedical training data.\nIn Proceedings of the 21st Workshop on Biomedi-\ncal Language Processing, pages 130–139, Dublin,\nIreland. Association for Computational Linguistics.\nMengzhou Xia, Mikel Artetxe, Jingfei Du, Danqi Chen,\nand Ves Stoyanov. 2022. Prompting electra: Few-\nshot learning with discriminative pre-trained models.\narXiv preprint arXiv:2205.15223.\nYuan Yao, Bowen Dong, Ao Zhang, Zhengyan Zhang,\nRuobing Xie, Zhiyuan Liu, Leyu Lin, Maosong\nSun, and Jianyong Wang. 2022a. Prompt tuning for\ndiscriminative pre-trained language models. arXiv\npreprint arXiv:2205.11166.\nZonghai Yao, Yi Cao, Zhichao Yang, and Hong Yu.\n2022b. Context variance evaluation of pretrained lan-\nguage models for prompt-based biomedical knowl-\nedge probing. arXiv preprint arXiv:2211.10265.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. LinkBERT: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8003–8016,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nHui-Syuan Yeh, Thomas Lavergne, and Pierre Zweigen-\nbaum. 2022. Decorate the examples: A simple\nmethod of prompt design for biomedical relation ex-\ntraction. arXiv preprint arXiv:2204.10360.\nHongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang,\nYutao Xie, and Sheng Yu. 2022. BioBART: Pretrain-\ning and evaluation of a biomedical generative lan-\nguage model. In Proceedings of the 21st Workshop\non Biomedical Language Processing, pages 97–109,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,\nand Fei Huang. 2021. Improving biomedical pre-\ntrained language models with knowledge. In Pro-\nceedings of the 20th Workshop on Biomedical Lan-\nguage Processing, pages 180–190, Online. Associa-\ntion for Computational Linguistics.\nSheng Zhang, Hao Cheng, Shikhar Vashishth, Cliff\nWong, Jinfeng Xiao, Xiaodong Liu, Tristan Nau-\nmann, Jianfeng Gao, and Hoifung Poon. 2022.\nKnowledge-rich self-supervision for biomedical en-\ntity linking. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022 , pages 868–\n880.\nA Hyper-parameters\nTab. 7 shows details of hyper-parameters in the\nexperiments of continual pretraining and prompt\ntuning. Hyper-parameters are determined by grid\nsearch.\nB Comprehensive Results\nWe demonstrate extensive results, including perfor-\nmance on development sets in Tab. 8 and Tab. 9.\n257\nEvaluation Continual TrainingParameters PubmedQA BioASQ MedQA(USMLE) MedMCQA PubmedQA\nBatch Size 8 8 32 32 8\nLearning Rate 2e-5 2e-5 5e-5 5e-5 2e-5\nWarmup Steps 500 500 1000 1000 100\nEpochs 20 20 10 10 1\nMax Sequence Length 512 512 512 512 512\nTable 7: Hyper-parameters used for BIODLM evaluation and continual training on PubmedQA, BioASQ,\nMedQA(USMLE), and MedMCQA.\nDataset Split Size Random Zero-shot (RTD Prompt) Fully Supervised (CLS Finetuning)\nMetroLM Electra BioElectra MetroLM Electra BioElectra PubmedBERT\nPubmedQA dev 50 33.3 50.2 46.9 46.4 62.0 56.0 54.0 52.3\ntest 500 33.3 64.0 58.0 48.0 63.8 57.0 62.2 55.8\nBioASQ dev 75 50.0 72.0 78.6 82.7 93.3 85.3 81.3 89.3\nSingle test 140 50.0 74.3 73.6 67.1 94.3 73.6 75.7 87.6\nMedQA(USMLE) dev 1272 25.0 27.6 24.8 18.2 28.5 27.8 43.5 36.8\ntest 1273 25.0 25.3 22.1 19.5 28.1 27.4 40.3 39.3\nMMLU dev 31 25.0 38.7 29.0 16.1 25.8 29.7 45.2 32.2\ntest 272 25.0 25.7 19.9 20.6 27.6 25.8 44.1 29.1\nMulti\nMedMCQA dev 4183 25.0 26.6 26.8 20.7 35.5 34.8 40.8 41.2\nMacro Avg. 32.4 44.9 42.2 37.7 51.0 46.4 54.1 51.5\nTable 8: Probing experiment results display the zero-shot performance with the RTD prompt of various DLMs on\nour benchmark. We also report the CLS-based finetuning performance of these DLMs in the full training setting and\ninvolve an in-domain PLM, PubmedBERT, for comparison. We report accuracy on each data split and the macro\naverage accuracy on our benchmark. The best zero-shot performance on each dataset is marked in bold.\nDataset Split Random 0% (Zero-shot) 10% (Few-shot) 100% (Full)\nCLS Prompt B IODLM CLS Prompt B IODLM CLS Prompt B IODLM\nPubmedQA dev 33.3 28.7 50.2 52.4 48.4 62.0 66.0 62.0 58.7 66.0\ntest 33.3 31.1 64.0 57.0 56.0 62.8 58.0 63.8 69.9 66.8\nBioASQ dev 50.0 34.0 72.0 75.0 89.3 87.9 88.0 93.3 90.6 90.7\nSingle test 50.0 35.2 74.3 77.1 77.9 77.9 80.0 94.3 85.3 89.8\nMedQA(USMLE) dev 25.0 10.4 27.6 26.4 25.6 28.3 27.9 28.5 25.4 29.7\ntest 25.0 9.8 25.3 27.7 26.5 25.7 29.1 28.1 27.0 29.6\nMMLU dev 25.0 4.2 38.7 39.4 29.0 29.3 32.9 25.8 30.9 35.4\ntest 25.0 11.0 25.7 26.8 21.7 30.8 32.7 27.6 31.2 31.6\nMulti\nMedMCQA dev 25.0 6.4 26.6 27.4 30.7 22.9 30.1 35.5 27.2 33.2\nMacro Avg. 32.4 19.0 44.9 45.5 45.0 47.5 49.4 51.0 49.6 52.5\nTable 9: Results of BIODLM in the zero-shot, few-shot, and full settings compared with finetuning CLS representa-\ntions. We use MetroLM as the backbone in BIODLM for results in this table. The prompt baseline is MetroLM\nwith RTD prompt tuning but without continual pretraining in BIODLM. We report accuracy on each data split and\nthe macro average accuracy on our benchmark. Finetuning CLS requires the training of a classification head, so it is\ninfeasible in the zero-shot setting. The best accuracy on each dataset in each setting is marked in bold.\n258",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.9666194915771484
    },
    {
      "name": "Computer science",
      "score": 0.8085348606109619
    },
    {
      "name": "Language model",
      "score": 0.6605511903762817
    },
    {
      "name": "Artificial intelligence",
      "score": 0.592663586139679
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5795772671699524
    },
    {
      "name": "Domain adaptation",
      "score": 0.5726632475852966
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5400884747505188
    },
    {
      "name": "Generative grammar",
      "score": 0.508815586566925
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.48910385370254517
    },
    {
      "name": "Machine learning",
      "score": 0.43760591745376587
    },
    {
      "name": "Natural language processing",
      "score": 0.34698933362960815
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}