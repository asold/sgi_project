{
  "title": "Live Streaming Speech Recognition Using Deep Bidirectional LSTM Acoustic Models and Interpolated Language Models",
  "url": "https://openalex.org/W4206017443",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2494044996",
      "name": "Javier Jorge",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2156879238",
      "name": "Adrià Giménez",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2707597753",
      "name": "Joan Albert Silvestre-Cerdà",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2140870289",
      "name": "Jorge Civera",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2241938498",
      "name": "Albert Sanchís",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2164133112",
      "name": "Alfons Juan",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2494044996",
      "name": "Javier Jorge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156879238",
      "name": "Adrià Giménez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2707597753",
      "name": "Joan Albert Silvestre-Cerdà",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140870289",
      "name": "Jorge Civera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2241938498",
      "name": "Albert Sanchís",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164133112",
      "name": "Alfons Juan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4256161595",
    "https://openalex.org/W2079735306",
    "https://openalex.org/W2329068866",
    "https://openalex.org/W2471933213",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W6692563993",
    "https://openalex.org/W3015974384",
    "https://openalex.org/W3015457435",
    "https://openalex.org/W3096339849",
    "https://openalex.org/W6745635926",
    "https://openalex.org/W2139009147",
    "https://openalex.org/W2963451498",
    "https://openalex.org/W2890817076",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W2288502450",
    "https://openalex.org/W2512608784",
    "https://openalex.org/W2964084166",
    "https://openalex.org/W2631415506",
    "https://openalex.org/W1994536225",
    "https://openalex.org/W2091981305",
    "https://openalex.org/W2066378046",
    "https://openalex.org/W2394835536",
    "https://openalex.org/W2964191536",
    "https://openalex.org/W2037942319",
    "https://openalex.org/W2748092010",
    "https://openalex.org/W2972528057",
    "https://openalex.org/W3015926140",
    "https://openalex.org/W3095714920",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W6691770337",
    "https://openalex.org/W2008554732",
    "https://openalex.org/W2088802182",
    "https://openalex.org/W2101663079",
    "https://openalex.org/W2104911399",
    "https://openalex.org/W280003846",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2012256292",
    "https://openalex.org/W2158069733",
    "https://openalex.org/W6717262007",
    "https://openalex.org/W6672538089",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W6678040779",
    "https://openalex.org/W1494108583",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3163211337",
    "https://openalex.org/W3015726069",
    "https://openalex.org/W2973201641",
    "https://openalex.org/W2900707131",
    "https://openalex.org/W2763120645",
    "https://openalex.org/W2259472270"
  ],
  "abstract": "[EN] Although Long-Short Term Memory (LSTM) networks and deep Transformers are now extensively used in offline ASR, it is unclear how best offline systems can be adapted to work with them under the streaming setup. After gaining considerable experience on this regard in recent years, in this paper we show how an optimized, low-latency streaming decoder can be built in which bidirectional LSTM acoustic models, together with general interpolated language models, can be nicely integrated with minimal performance degradation. In brief, our streaming decoder consists of a one-pass, real-time search engine relying on a limited-duration window sliding over time and a number of ad hoc acoustic and language model pruning techniques. Extensive empirical assessment is provided on truly streaming tasks derived from the well-known LibriSpeech and TED talks datasets, as well as from TV shows on a main Spanish broadcasting station.",
  "full_text": "148 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nLive Streaming Speech Recognition Using Deep\nBidirectional LSTM Acoustic Models and\nInterpolated Language Models\nJavier Jorge , Adrià Giménez , Joan Albert Silvestre-Cerdà , Jorge Civera , Albert Sanchis ,\nand Alfons Juan\nAbstract—Although Long-Short Term Memory (LSTM) net-\nworks and deep Transformers are now extensively used in ofﬂine\nASR, it is unclear how best ofﬂine systems can be adapted to work\nwith them under the streaming setup. After gaining considerable\nexperience on this regard in recent years, in this paper we show how\nan optimized, low-latency streaming decoder can be built in which\nbidirectional LSTM acoustic models, together with general inter-\npolated language models, can be nicely integrated with minimal\nperfomance degradation. In brief, our streaming decoder consists\nof a one-pass, real-time search engine relying on a limited-duration\nwindow sliding over time and a number of ad hoc acoustic and lan-\nguage model pruning techniques. Extensive empirical assessment\nis provided on truly streaming tasks derived from the well-known\nLibriSpeech and TED talks datasets, as well as from TV shows on\na main Spanish broadcasting station.\nIndex Terms —Automatic speech recognition, streaming,\ndecoding, acoustic modeling, language modeling, neural networks.\nI. I NTRODUCTION\nL\nIVE video streaming services over the Internet have in-\ncreased dramatically in recent years because of higher user\ndemand and bandwidth speeds. This has resulted in a growing\nneed by live video streaming platforms to provide high-quality\nautomatic speech transcriptions. However, the application of\nstate-of-the-art neural-based Automatic Speech Recognition\n(ASR) models to video streaming is a highly complex and\nchallenging task due to real-time and low-latency decoding\nconstraints.\nManuscript received January 12, 2021; revised June 23, 2021; accepted\nNovember 23, 2021. Date of publication December 10, 2021; date of current\nversion January 8, 2022. This work was supported in part by European Union’s\nHorizon 2020 Research and Innovation Programme under Grant\n761758 (X5gon), and 952215 (TAILOR) and Erasmus + Education\nProgram under Grant Agreement 20-226-093604-SCH, in part by\nMCIN/AEI/10.13039/501100011033 ERDF A way of making Europe under\nGrant RTI2018-094879-B-I00, and in part by Generalitat Valenciana’s Research\nProject Classroom Activity Recognition under Grant PROMETEO/2019/111.\nFunding for open access charge: CRUE-Universitat Politècnica de València.\nThe associate editor coordinating the review of this manuscript and approving\nit for publication was Prof. Lei Xie. (Corresponding author: Javier Jorge.)\nThe authors are with the Valencian Research Institute for Artiﬁcial\nIntelligence, Machine Learning and Language Processing Group, E-46022\nValencia, Spain (e-mail: jjorge@dsic.upv.es; agimenez@dsic.upv.es;\njuasilce@vrain.upv.es; jcivera@dsic.upv.es; josanna@dsic.upv.es;\najuan@dsic.upv.es).\nDigital Object Identiﬁer 10.1109/TASLP.2021.3133216\nAt this time, state-of-the-art ASR systems are based on the\nhybrid Hidden Markov Model (HMM) and neural network ap-\nproach [1]. In particular, deep Bidirectional Long-Short Term\nMemory (BLSTM) networks have proven to be a powerful\narchitecture for acoustic modeling in a wide range of ASR\ntasks [2]–[4]. In the same way, Transformer-based architec-\ntures have recently reached very promising results for lan-\nguage modeling [5], though LSTM recurrent neural networks\n(LSTM-RNN) are still broadly used [6]. It goes without saying\nthat end-to-end systems are attracting great attention, and this\nincludes a number of proposals for operation under low-latency\nstreaming decoding [7]–[9]. However, despite their simplicity\nand promising prospects, it is still unclear whether or not they\nwill soon surpass state-of-the-art hybrid systems combining\nindependent models trained from vast amounts of data.\nTwo main challenges need to be addressed so as to properly\nadapt hybrid ASR systems to the streaming setup. The ﬁrst\none is due to the fact that BLSTM acoustic models can no\nlonger be applied in their full extent, over the whole input\nsignal. Instead, they need to be time-limited within a window\nsliding over time in which only a small fraction of non-decoded\nsignal (right context) can be captured for the system to respond\nquickly after the incoming audio stream. This adaptation of\nBLSTM acoustic models to deal properly with the incoming\naudio stream also implies to dynamically carry out acoustic\nmean normalization as opposed to full normalization over the\nwhole signal. An additional issue to be considered is to adapt\nwell-known pruning techniques as acoustic look-ahead [10],\n[11] to work with BLSTM acoustic models to speed up the\ndecoding process. The second one is that Transformer and\nLSTM-RNN language models (LMs) cannot likewise be applied\nas they use to be, by rescoring n-best hypotheses or lattices\nin a two-pass decoding approach [12]–[14]. In all these cases,\nefﬁcient techniques are required for on-the-ﬂy scoring under a\nreal-time one-pass decoding scheme.\nThe use of BLSTM acoustic models under streaming con-\nditions has been explored in several recent works. In [15], a\nﬁnite sliding window was applied to approximate the acoustic\nposterior probability of the center frame. This approach was\nimproved in [16] by using a more accurate weighting scheme\nof overlapping windows. Under this approach, BLSTM-based\nmodels outperformed deep neural networks (DNNs) under the\nstreaming setup, also showing that a right context of limited\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 149\nduration sufﬁces to reach a performance similar to that of the\nofﬂine setup. In contrast to using a sliding window over the\nincoming signal, a different approach consists in splitting it into\noverlapping chunks with appended (past and future) contextual\nobservations. This approach was followed in [3], where the\nso-called Context-Sensitive-Chunk (CSC) method was proposed\nto speed up BLSTM training for low-latency decoding by just\nadding some delay in between consecutive chunks. This method\ncan be accelerated by simply avoiding computations on the left\ncontext, as done with the Latency-Controlled BLSTMs proposed\nin [17], which in turn can be further improved as shown in [18].\nHowever, in all these previous works, empirical evaluations\nwere not performed under genuine streaming conditions, that is,\ndealing with the speech signal as an incoming audio stream and,\ntherefore, on-the-ﬂy mean normalization was not considered at\nall. Moreover, basic n-grams LMs were used in experiments\nwhich greatly helped to improve the responsiveness of the\nsystem in the evaluation of system latencies as were reported\nonly in the case of the CSC approach.\nRegarding the use of neural LMs, to our knowledge, the\ndirect use of this technique during decoding was ﬁrst explored\nin [19], where the authors proposed the use of a Variance\nRegularization term together with caching strategies for fast\ndecoding. Despite using feed-forward neural LMs in decoding,\nempirical results showed signiﬁcant relative improvements both\nin speed and accuracy. Other relevant contributions addressing\none-pass decoding with neural LMs have focused on heuristics\nto reduce the number of queries to the model and catching\nnetwork states [20], alternative one-pass decoding strategies\nsuch as on-the-ﬂy rescoring [21], improving CPU-GPU commu-\nnications [22] and, more recently, combining Gated Recurrent\nUnits with more efﬁcient objective functions, such as Noise\nContrastive Estimation [23]. Certainly different from these con-\ntributions, other authors have explored the idea of converting\nneural LMs, either recurrent or not, into n-gram models that can\nthus be smoothly integrated into a conventional decoder [24],\n[25]. It is worth noting, however, that these approaches were\nonly focused on language modeling and not on the low-latency\nstreaming decoding problem.\nThis work takes as a starting point a novel architecture for\nreal-time one-pass decoding with LSTM-RNN LMs proposed\nin [26]. In it, one-pass decoding was accelerated by estimating\nlook-ahead scores using precomputed static look-ahead tables.\nMoreover, LSTM-RNN LM probabilities were efﬁciently com-\nputed using Variance Regularization and lazy evaluation. Later\non, in [27], this architecture for real-time one-pass decoding was\nextended to include BLSTM acoustic models within a time slid-\ning window, also used as a window for time-constrained, on-the-\nﬂy acoustic feature normalization. Not surprisingly, empirical\nassessment of this extended architecture under strict streaming\nconditions proved it was really effective, indeed keeping the\npace with non-streaming (ofﬂine) systems. The most recent\nreﬁnement in connection to this research line has consisted in re-\nplacing streaming-adapted LSTM-RNN LMs with Transformer\nLMs [28]. In doing so, empirical results on the well-known\nLibriSpeech [29] and TED-LIUM [30] tasks have shown that\nthis reﬁnement leads to top, state-of-the-art recognition rates and\nlatencies under streaming conditions. In short, it has been shown\nthat hybrid one-pass ASR systems built in this way can work\nunder both, ofﬂine and streaming conditions with no signiﬁcant\ndifferences in quality.\nThis work is intended to provide a complete, detailed refer-\nence of the main contributions made along the research line\ndescribed above, also including a number of new additional\nenhancements and a new and extensive empirical evaluation un-\nder streaming conditions. In particular, the following important\nnovel algorithmic enhancements are provided:\nr The sliding window framework proposed in [16] is revis-\nited to include and adapt necessary concepts for proper\nstreaming decoding.\nr Also for proper streaming decoding, novel methods for\nacoustic feature normalization are explored.\nr Along with these two streaming-oriented enhancements,\nand to improve the general performance of the decoder, new\npruning techniques for fast decoding are also considered.\nMore precisely, a new approach for the acoustic look-ahead\nis provided, together with a more efﬁcient pruning to speed\nup the use of interpolated neural LMs.\nOnly after including these enhancements, a fully-ﬂedged\nstreaming ASR system can be effectively deployed into pro-\nduction. For empirical evaluation, apart from the conventional\nLibriSpeech and TED-LIUM tasks considered in previous work,\ntwo genuine streaming tasks also posed for streaming bench-\nmarking: a video-based version of TED-LIUM with unseg-\nmented talks, and a set of full-length videos from a Spanish\nTV broadcaster.\nThe paper is organized describing separately the two main\ncomponents which generally speaking deserve special attention\nin the deployment of a streaming decoder. In particular, the use of\ndeep BLSTM acoustic models for streaming is described in Sec-\ntion II. On the other hand, the efﬁcient pruning technique for fast\none-pass decoding using interpolated neural LMs is presented\nin Section III. All these components are empirically assessed\nin Section IV with emphasis on the key adaptation parameters\nrequired for ﬁnding an appropriate (task-dependent) trade-off\nbetween accuracy and latency. Finally, the main conclusions\ndrawn from on this research line are summarized in Section V.\nII. D EEP BIDIRECTIONAL LSTM ACOUSTIC MODELS FOR\nSTREAMING\nIn this section, all the issues concerning the use of deep\nBLSTM acoustic models for streaming are described. Firstly,\nthe sliding window framework proposed in [16] is revisited in\nSection II-A to include and adapt necessary concepts for proper\nstreaming decoding using BLSTM acoustic models. Secondly, a\nnovel approximation for computing acoustic look-ahead scores\nis proposed in Section II-B. Lastly, several acoustic mean nor-\nmalization methods for streaming are proposed in Section II-C.\nA. Streaming Decoding Using BLSTM Acoustic Models\nLet x∞\n1 be an unbounded sequence of frames computed from\nthe incoming audio stream, which is being processed by appli-\ncation of a sliding window of w frames shifting one frame to\n150 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nFig. 1. Frame sequence at the top and just below a sliding window of w =4\nframes at all steps embracing frame t, xt.\nFig. 2. Computing the acoustic scores for b =3 consecutive frames starting\nat t, xt+b−1\nt , within a sliding window of size w =4 during two consecutive\nb-step batches, Bj−1 and Bj .\nthe right at each step (though a step size of more than one frame\ncan be also used if convenient). Thus, frames tto t+ w−1 are\ncovered by the sliding window at step t, Xt = xt+w−1\nt .T h i si s\nillustrated in Fig. 1, where the sliding window is also depicted\nat all the w−1 preceding steps embracing frame t. Xt−1\nt−(w−1).\nFor each acoustic state a, we assume that a BLSTM acoustic\nmodel is available to compute the posterior probability of the\nn-th frame within the sliding window at step t, pn(a | Xt).\nAs frame t falls into position w−i+1 of the sliding window\nat step t−(w −i), 1 ≤ i ≤ w, it gets w different posteriors\nfrom which its acoustic score is computed by just (weighted)\naveraging:\nq(xt,a)= 1\nw\nw∑\ni=1\npw−i+1(a | Xt−(w−i)) (1)\nFor this score to be efﬁciently computed, we assume that the\nrecurrent state of each BLSTM does not depend on its previous\nstates, and hence the posterior it provides for frame t only\ndepends on (its position in) the window context considered. This\nassumption enables fast computation of posteriors, not only by\nrunning independent BLSTM queries in parallel, but also by\navoiding repeated posterior computations during consecutive\nstep batches. Fig. 2 shows how this looks like in a simple example\nwhere the acoustic scores for b =3 consecutive frames starting\nat t, xt+b−1\nt , are computed within a sliding window of size w =4\nduring two consecutive b-step batches, Bj−1 and Bj.\nAs shown in Fig. 2, scoring the frame subsequence xt+b−1\nt can\nbe efﬁciently done by keeping all precomputed posteriors for it\nand just running bindependent BLSTM parallel queries in Bj to\nget the posteriors still required. In the example given, for each\nposition i(i =1 ,2,3), we have w−iposteriors available and i\nposteriors to be computed. In general, for any b ≥ 1, a carousel\nmay be used at each position i, 1 ≤ i ≤ min(b,w −1), to keep\ntrack of w−iprecomputed posteriors and ﬁll in the remaining i\nfrom the bparallel BLSTM calls in batch Bj.C l e a r l y ,i fb ≥ w,\nno precomputed posteriors are involved from position w to b.\nIt is obvious that the window size w is a key adaptation\nparameter for streaming as it controls the duration of the acoustic\ncontext, both in the past (xt−1\nt−(w−1)) and the future (xt+w−1\nt+1 ),\nthat is used when scoring the current frame xt. Needless to\nsay, we are assuming that the most relevant acoustic context at\neach frame occurs within a time window of a handful tenths\nof second. Also, as we need the complete future context to be\navailable for (exact) scoring, time windows longer than that\nmay prevent the system to respond after a reasonable latency\nof, say, one second. This is of course a topic to be explored\nempirically. In this regard, note that we are limiting ourselves\nto symmetrical time windows of ﬁxed duration ( w) and exact\nscoring, as deﬁned in (1). However, if convenient, more general\nschemes for acoustic context management and scoring can be\nalso devised, such as asymmetrical time windows of variable\nduration and approximate scoring.\nAs w, the batch size b is also a key adaptation parameter for\nstreaming though, in contrast to w, its effect is only compu-\ntational. In principle, we may want b as large as possible, for\nmaximum parallelism, but also small enough for the additional\nfuture context (of b−1 frames) it requires not to become the\ndominant factor in the observed system latency. This is easily\nunderstood from Figs. 1 and 2. In Fig. 1, we have b =1 and thus,\napart from the future w −1 frames required to complete the\nsliding window at t, Xt, no additional frame is needed to score\nxt. Instead, in Fig. 2, we have b =3 , and hence 2 additional\nframes are needed before running a 3-step batch of parallel\nBLSTM calls. There are also other hardware-dependent factors\nsuch as (GPU) memory bandwidth that may add up signiﬁcantly\nto the observed latency as the batch size increases. Therefore,\nas with w, this is best studied empirically.\nB. Acoustic Model Look-Ahead\nThe acoustic look-ahead refers to the best acoustic score\n(emission and transition probabilities) that can be reached from a\ngiven frame xt and an acoustic state a. More precisely, following\na similar notation to [10], the exact acoustic look-ahead l(t,a)\nis deﬁned as\nl(t,a)= m a x\naL\n0 :a0=a\nL∑\nτ=1\nq(xt+τ,aτ)+ q(aτ,aτ−1) , (2)\nbeing L the number of remaining frames until the end of the\nspeech signal, and q(xt,at)=l o gp(at|xt)\np(at) and q(at,at−1)=\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 151\nlog p(at | at−1) the emission and transition probabilities, re-\nspectively.\nDuring decoding, for a given partial hypothesis at\n1, an upper\nbound of the acoustic score ˆq(at\n1) is computed by adding the\nacoustic look-ahead score as\nˆq(at\n1)= q(at\n1)+ l(t,at) (3)\nwhere\nq(at\n1)=\nt∑\nτ=1\nq(xτ,aτ)+ q(aτ,aτ−1) (4)\nIt is worth noting that ˆq(at\n1) is an optimistic estimation of the\nacoustic score, since it is not only considering the score until\ninstant t, but also the score of the best future path that could be\nreached from that instant according to the acoustic model. This\nestimated score leads to a more guided beam search, which in\nturn, could lead to speed up the decoding process.\nTherefore, the challenge resides in efﬁciently computing the\nacoustic look-ahead score l(t,a) and how to speed up the search\nof the best future path (see (2)). In fact, let us to refer previous\nworks in which exact look-ahead scores were approximated by\nlimiting the future context to a few frames and/or simplifying\nthe emission models used for look-ahead calculation [10], [11].\nA major issue when acoustic look-ahead was applied over\nGaussian HMMs lied in the cost of estimating the emission\nscores. However, in current hybrid systems based on neural\nnetworks this is not a problem anymore since, for each frame,\nthe neural network estimates the scores for all HMM states,\nand usually this is performed in batch mode using GPUs. In\ncurrent state-of-the-art hybrid system based on triphonemes the\nnumber of different states that must be considered during the\nsearch of an acoustic alignment (without considering the LM)\nmay vary between half a million and several millions, depending\non the total number of HMMs and the vocabulary size. This\nmakes unfeasible to directly apply exact acoustic look-ahead\nestimation.\nAs alternative to circumvent all these drawbacks, we propose\nto approximate the search space by a bigram model using HMM\nstates as tokens. In the bigram model we only consider those\ntransitions that are allowed in the original search space. Addi-\ntionally, all transitions scores are set to zero, i.e., we only focus\non emission scores. Using this approach the acoustic look-ahead\ncan be estimated at low cost using dynamic programming, and\nwithout the need of limiting the number of future frames or\nsimplifying emission models. More precisely, for a given frame\nxt and state a, the proposed acoustic look-ahead approximation\nˆl(t,a) is estimated as\nˆl(t,a)=\n{\n0 t = T\nmax\na′∈A\nˆq(a,a′)+q(xt+1,a′)+ˆl(t+1 ,a′) t<T (5)\nwhere T is the total number of frames, A is the set of HMM\nacoustic states, and ˆq(a,a′) is a function that returns 0 if (a,a′)\nis a non-zero probability bigram transition or −∞ otherwise.\nDuring decoding, the look-ahead based acoustic score for a\nhypothesis can be incrementally updated as\nˆq(at\n1)=ˆq(at−1\n1 ) −l(t−1,at−1)+\n+ q(at,at−1)+ q(xt,at)+ l(t,at). (6)\nIn an ofﬂine setup the proposed acoustic look-ahead scores\ncould be precomputed before decoding without limitation on the\nnumber of future frames. However, the streaming setup requires\nan on-the-ﬂy estimation of acoustic look-ahead scores. More\nprecisely, as described before and was illustrated in Fig. 2, every\nb frames the BLSTM is queried with b+ w−1 frames, and\noutputs the emission score for the ﬁrst b frames. In this setup,\nevery time the BLSTM is queried the acoustic look-ahead scores\nare estimated for the ﬁrst bframes of the query. In order to take\nfull advantage of the available data, when querying the BLSTM\nwe also retrieve partially averaged emission scores for the frames\nof the future context, therefore, the acoustic look-ahead score\nfor the ﬁrst frame of the batch will be estimated considering\nb+ w−2 future frames, while in the last frame of the batch only\nw−1 frames will be considered. It is worth noting, that in every\nBLSTM query a computational overhead of w is introduced\nwhen compared with an ofﬂine scenario. Consequently, the\nlarger the batch size, the smaller the computational overhead and\nthe future context limitation. However, a large batch size also\nmeans higher latencies. Therefore, this is a trade-off that must\nbe taken into account when applying the proposed technique for\nstreaming as will be appropriately evaluated in Section IV.\nC. Acoustic Feature Normalization for Streaming\nUnder the ofﬂine scenario, Full Sequence Normalization\n(FSN) is usually performed beforehand applying mean normal-\nization to the whole speech utterance. However, since FSN is\nnot feasible under streaming conditions, we propose different\nalternatives to carry out on-the-ﬂy sequence normalization.\nThe ﬁrst alternative, called Dynamic Threshold Normaliza-\ntion (DTN), consists on the initialization of the mean by con-\nsidering an initial delay of nnorm frames. Afterwards, the mean\nis dynamically updated for every new frame. In previous works,\nwe proved that two seconds of initial delay should be enough to\nachieve similar performance to FSN [27], [28]. Although, two\nseconds of delay could be reasonable in a continuous streaming\nsetup, it could be not so suitable for short utterances such as\nvoice commands.\nTo overcome this limitation and taking advantage of the slid-\ning window technique introduced in Section II-A, we propose in\nthis work a novel normalization scheme called Weighted Moving\nAverage (WMA), in which mean normalization is performed\nusing the frames of the sliding window. In this way, WMA is\napplied over a batch Bj of frames as\nˆBj = Bj − ˆμj (7)\nwhere\nˆμj = fj−1 + ∑b+w\nt=1 Bj,t\nnj−1 + b+ w (8)\nbeing fj−1 the accumulated values of previous frames until batch\nBj−1, Bj,t the t-th frame in batch Bj, nj−1 the number of\n152 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nframes until batch Bj−1, and b and w the batch and window\nsizes, respectively.\nThe accumulated values fj and nj are updated by weighting\nthe contribution of previous batches using a parameter α as\nfj = α·fj−1 +\nb∑\nt=1\nBj,t (9)\nnj = α·nj−1 + b (10)\nUnlike FSN, WMA dynamically adapts the normalization of\nthe speech signal to local changes, and differently from DTN,\nwithout introducing an initial delay. Therefore, it can be used\nwithout affecting the global latency even from the beginning\nof the utterance. Although an initial mean could be precom-\nputed from the training set, WMA has been evaluated in this\nwork starting from scratch on each sample and using only the\ninformation that comes from the audio stream as expected in\nstreaming conditions. Similarly to the acoustic look-ahead, the\nbatch size has also an impact regarding the amount of frames\nused to compute the mean. This impact will be evaluated during\nthe experimental section.\nIII. E FFICIENT ONE-PASS DECODING USING INTERPOLATED\nNEURAL LMS\nThe direct use of neural LMs in one-pass decoding takes\nfull advantage of its ability to deal with histories of unlimited\nlength in contrast to n-gram LMs [14]. This makes History\nConditioned Search (HCS) decoders perfectly suited for its\nuse with neural LMs, as HCS technique group hypotheses by\nits history allowing potentially an unlimited representation of\ncontinuous contexts [31]. However, in practice, the integration\nof neural LMs in one-pass HCS decoders for streaming presents\nsome relevant difﬁculties which need to be solved. For instance,\nthe very efﬁcient computation of LM look-ahead scores and\nword neural LM probabilities or the use of speciﬁc LM pruning\nparameters to reduce the search space. In the following, all these\ndecoding issues are discussed and how they have been efﬁciently\naddressed in this proposal.\nA. Language Model Look-Ahead\nThere are many techniques to deal with look-ahead LM scores\nas the use of cache strategies, perfect hashing and precompu-\ntation of scores [32], computing look-ahead scores bottom-up\nfrom back-off LM [33] or leveraging the LM sparseness to par-\ntially compute look-ahead tables [34]. Some of these approaches\nuse a lower order n-gram to obtain look-ahead scores, as higher\nn-gram orders are not feasible due to memory and computation\nrequirements. In the case of [34], 3-grams and 4-grams are also\nevaluated, but in this case look-ahead scores are dynamically\ncomputed.\nIn HCS-based decoders, LM look-ahead scores are dynami-\ncally computed every time a word-end node is reached during\ndecoding. In order to do this efﬁciently for streaming, we pro-\npose to compute beforehand all these look-ahead scores in static\nlook-ahead tables. With this purpose, a heavily pruned version\nof the n-gram model can be used to represent this model in a\ncompact structure that can be used during decoding efﬁciently.\nThis is a critical part of the search when it comes to speed, as\nthis score is queried many times during search to ﬁll the search\nnetwork structure. Therefore, reducing the n-gram model to this\nstatic structure and following a cascade structure of look-ahead\ntables similar to that proposed in [34], we apply an efﬁcient\ntechnique to compute the look-ahead scores during decoding.\nIt is worth noting that this pruned n-gram does not constrain\nthe search space in any way, as this is only used to compute\nthe look-ahead scores. This allows the decoder to consider very\nlong word contexts in search hypotheses leveraging the beneﬁts\nof using neural LMs.\nB. Neural LM Integration\nDuring decoding, when word-end nodes are reached, look-\nahead table scores are replaced with those computed from the\nreal LM (i.e. n-grams or neural LMs). In the case of neural LM\nprobabilities, this is an important drawback since it involves\ncomputational issues, reducing the speed of the decoder as they\nare usually more complex models than count-based ones. For\nthis reason, neural LMs are typically applied in a second step\nof recognition using n-best or lattice rescoring. To alleviate\nthis drawback we propose to apply the Variance Regularization\ntechnique [19] reducing the complexity of the computation of\nthe output layer where the softmax function is computed. This\ntechnique involves a regularization term during training that\naims to reduce the variance of the denominator of the softmax\nadjusting it to a constant. This constant is kept and then used\nduring decoding when computing neural LM probabilities, in-\nstead of computing the denominator. As opposed to LSTM-RNN\nLMs which store the previous context in an internal vector,\nTransformer LMs need to compute all the previous history when\na new word comes in for the attention to work properly. To deal\nwith long audio streams (possibly hours of continuous speech)\nwe should limit the history to the n previous words, where n\nis a parameter provided in decoding. It is important to remark\nthat the Transformer LM training enforced no history restriction,\nindeed there could be a training-decoding mismatch regarding\nhistory length that may harm the performance, as sentences of\ndifferent lengths are devoted to training and the history length\nis not adjusted beforehand.\nC. LM Pruning Parameters\nIn order to further speed up the decoding process, speciﬁc\nLM pruning parameters had to be incorporated to the one-pass\ndecoder, to reduce the search space or the number of queries\nin the computation of neural LM probabilities [26]. One of\nthese parameters is the Language Model History Recombination\n(LMHR) which deﬁnes the number of words to be considered\nbefore performing hypothesis recombination during decoding.\nLMHR parameter is needed to control the length of histories\nsince, in HCS decoders, hypotheses are grouped according\nto their history, meaning that without enforcing any back-off\nrecombination previous histories of active hypotheses tend to\ngrow without any limitation. However, this effect that could be\nconsidered a feature turned to be a problem when long histories\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 153\nTABLE I\nBASIC STATISTICS OF DEV AND TESTS SETS IN THE EVA L UAT I O NTASKS:\nDURATION IN HOURS,N UMBER OF SAMPLES (SEGMENTS OR VIDEOS),\nAVERAGE DURATION OF SAMPLES IN SECONDS PLUS-MINUS STANDARD\nDEVIATION (dµ ±σ), AND RUNNING WORDS (RW) IN THOUSANDS (K)\nare considered, as active hypotheses cluster over similar contexts\nthat differ only in words far from the current frame. This param-\neter aims to reduce the uncertainty of having these long and\nvery similar hypotheses making pruning less effective. This is\nachieved by combining them when they share a given number of\nwords, and that is indeed what this parameter deﬁnes, the number\nof previous words evaluated to combine active hypotheses. The\nsecond pruning parameter, named Language Model Histogram\nPruning (LMHP), limits the number of hypotheses that will\nquery the neural LM after reaching new word-end nodes during\ndecoding. This is particularly effective in reducing the costly\nneural LMs computation, as active hypotheses are pruned before\nperforming any computation. Unlike global histogram pruning\napplied to thousands of hypotheses after each decoding step,\nLMHP affects tens or hundreds of hypotheses.\nD. LM Interpolation\nIt is worth stressing that the proposed one-pass HCS-based\ndecoder enables the use of linearly interpolated count-based\nand/or neural LMs which to our knowledge is unprecedented\nin streaming ASR.\nIV . E XPERIMENTS\nA. Evaluation Datasets\nThe proposed ASR system for streaming was evaluated on\nLibriSpeech (LS) and TED-LIUM release 2 speech corpus. In\nthe case of the TED-LIUM corpus, we deﬁned a new evaluation\ntask referred to as TDv, in which complete video talks were tran-\nscribed without any previous segmentation in order to simulate\na streaming scenario. This is, to the best of our knowledge, the\nﬁrst time that the TED-LIUM corpus is considered at the talk\nlevel and it could be useful to assess streaming ASR systems in\nfuture works. In order to do that, we used the complete audio\ntrack for each talk along with the STM ﬁles provided in the\ndataset to evaluate the WER. The conventional segment-based\nTED-LIUM task is referred to as TDs in this work. Additionally,\nwe used the RTVE2018 dataset which comprises a collection of\ncomplete TV shows drawn from diverse genres and broadcasted\nby the public Spanish national television from 2015 to 2018 [35].\nTable I summarizes the basic statistics for the dev and test sets of\nthe tasks mentioned above. In the case of RTVE2018, an internal\nTABLE II\nSTATISTICS OF SPANISH TEXT RESOURCES USED FOR LANGUAGE MODELING.\nS=SENTENCES,R W=RUNNING WORDS,V =VOCABULARY.U NITS ARE IN\nTHOUSANDS (K)\npartition of the provided dev1 set (dev1-dev) was created for\ndevelopment purposes, reserving the test set for evaluation.\nB. Training Setup\nIn order to build the English and Spanish hybrid ASR systems,\na context-dependent feed-forward DNN-HMM with three left-\nto-right states using MFCC 16 plus ﬁrst and second derivatives\n(48-dim) was initially trained with our own transLectures-UPV\nASR toolkit (TLK) [36]. Then, a BLSTM-HMM acoustic model\nwas trained following the procedure described in [4] using ﬁlter\nbank 85-dimensional features and the previous DNN-HMM\nalignments. The architecture of the BLSTM model has eight\nbidirectional hidden layers with 512 LSTM cells per layer and\ndirection trained using both, TLK and TensorFlow [37]. Follow-\ning [4], we performed chunking during training by considering\na context to perform back propagation through time to a window\nsize of 50 frames. Additionally, SpecAugmentation was applied\nby means of time and frequency distortions [38]. Finally, a ﬁnal\nstep of sequence discriminative training was performed using\nour in-house implementation of lattice-based MMI to adjust the\ntransition scores and the weights of the softmax layer [39].\nEnglish acoustic models were trained on 961 and 207 hours of\ntraining speech corpus for LibriSpeech and TED-LIUM release\n2, respectively. After applying a phonetic decision tree [40],\n8.3 K and 10.8 K tied-states (or senones) were obtained for\nLibriSpeech and TED-LIUM, respectively. On the other hand,\nSpanish acoustic models were trained using the 208 hours pro-\nvided in the RTVE2018 dataset plus about 3.7 k hours of internal\nresources. The Spanish ASR system comprises 10 K tied-states.\nRegarding the LM training, we used the approximately 800 M\nwords of text provided for LibriSpeech to train neural LMs, as the\nngram model is provided with the corpus ( fglarge), whereas for\nTED-LIUM we trained the LMs with the six provided subsets\nplus the TED-LIUM training audio transcriptions with up to\n230 M running words. V ocabularies were restricted to 200 K and\n153 K words for LibriSpeech and TED-LIUM, respectively. In\nthe case of the Spanish system, text resources were obtained from\ninternal sources and other public repositories shown in Table II.\nThe vocabulary size was over 254 K words and a 1-gigaword\nrandom subset of the LM data was selected to train the Spanish\n154 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nTABLE III\nPERPLEXITY (PPL) AND WEIGHT (W) FIGURES ON DEVELOPMENT SETS,\nCONSIDERING SINGLE MODELS AND TWO-WAY AND THREE-WAY\nINTERPOLATION OF N-GRAM (N), LSTM-RNN LM (L) AND TRANSFORMER\nLM (T). I NTERPOLATION WEIGHTS WERE OPTIMIZED BY MINIMIZING PPLSO F\nTHE INTERPOLATED MODELS\nneural LMs. To train neural LMs when the vocabulary is deﬁned\nin advance, we decided to obtain the vocabulary as the intersec-\ntion between the provided vocabulary and that of the training\ndata. In this way, the model avoids having null-word probabili-\nties for words that are in the vocabulary but not in the training\nset. We take this into account when computing perplexities by\nrenormalizing the unknown-word score accordingly.\nAs LMs, we used n-grams, LSTM-RNN LMs and Trans-\nformer LM (TLM), combining them through a linear interpola-\ntion. Count-based models were trained using SRILM [49]. Apart\nfrom the 4-gram model provided for LibriSpeech, we trained a\n4-gram Kneser-Ney smoothed LM for TED-LIUM using the\nsame data as [30]. To compute the static look-ahead tables, a\npruned version of these n-gram models was computed for each\ntask. We obtained OOV ratios of less than 0.6% in all tasks.\nThe CUED-RNNLM toolkit [50] was used to train LSTM-\nRNN LMs with Noise Contrastive Estimation (NCE) crite-\nrion [51], and the normalization constant learned from training\nwas used during decoding [52]. Based on the lowest perplexity\non the dev sets, we selected as ﬁnal models those with 256-unit\nembedding layer and two hidden LSTM-RNN layer of 2048\nunits.\nThe training of TLMs was carried out using our own cus-\ntomized version of the FairSeq toolkit [53] using a 24-layer\nnetwork with 768 units per layer, 4096-unit FFN, 12 attention\nheads, and an embedding of 768 dimensions. These models were\ntrained until convergence with batches limited to 512 tokens, 512\nsentences, and 512 words per sentence. Parameters of these mod-\nels were updated every 32 batches. During inference, Variance\nRegularization was also applied to speed up the computation of\nthe TLM score.\nTable III shows the perplexity of LMs on the development\nsets for all tasks. When comparing single LM performance,\nneural models outperformed count-based models on every task,\nwith enough margin, almost halving the perplexity for LS and\nRTVE in the case of the LSTM-RNN. These results were further\nimproved with TLM, reducing the perplexity in approximately\n25% for LS, 15% for TDs, 32% for TDv, and 35% for RTVE,\nwith respect to the LSTM-RNN LM. Model interpolation had\ndiverse impact depending on the combination, but in general\nusing all three models provided the best perplexity for each task.\nConsistently with the single performance, the TLM obtains the\nhighest weights in the different LM combinations for LS and\nFig. 3. WER vs. window size in seconds for all tasks.\nRTVE (∼85-95%), while LSTM-RNN and ngram models still\nhave an important weight for TED-LIUM tasks, ranging from\n22% to 38% when are combined with TLM. When considering\nthe three-way interpolation, again LS and RTVE perplexities\nshow a similar behavior to that of the two-way interpolation\nwith high weights for the TLM, while TLM reduces its weight\nin favor of LSTM-RNN and ngram in TED-LIUM tasks. TLM\nhistory limitation was optimized for best perplexity in each case\nusing the same history size when TLM was interpolated with\nother LMs.\nC. Experiments on Acoustic Modeling for Streaming\nThe use of BLSTM acoustic models under streaming con-\nditions was evaluated in the following way. First, we studied\nthe effect of the window size presented in Section II-A in the\nperformance of the decoder considering that Full Sequence\nNormalization (FSN) is performed beforehand. In this way, the\noptimal window size was ﬁxed for each task in order to be used in\nthe following experiments. Then, the impact of the acoustic look-\nahead was gauged to prove its pruning effectiveness, and the\ndifferent methods for acoustic feature normalization proposed\nin Section II-C were also assessed. In all these experiments, only\ncount-based LMs were used in order to isolate the effect of the\nproposed acoustic-related techniques on the decoding.\nFig. 3 shows Word Error Rate (WER) as a function of the\nwindow size ( w) in seconds from 0.1 (or 10 frames) to 1 s\n(or 100 frames) for each task. It is worth noting that in this\nexperiment the acoustic models were the same and only the\nwindow size was varied during decoding. In LibriSpeech and\nthe TED-LIUM tasks, more context means better performance\nup to the point at which the windows size is equal to the chunk\nsize used during acoustic training. Beyond this point there is no\nimprovement by increasing the future context leading to more\nﬁxed latency without any decrease in WER. Differently from\nthese tasks, RTVE obtains slight improvements after increasing\nthe window size up to about 1 s of future context. This different\nbehavior can be explained because of RTVE is composed of real\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 155\nFig. 4. WER vs. latency in seconds with or without AMLA enabled for each\ntask.\nTV shows with heterogeneous conditions and, therefore, further\nimprovements can be expected considering larger contexts to\nbetter deal with changing audio conditions. Based on these\nempirical results, the optimal window sizes were ﬁxed to 0.5\nseconds for LibriSpeech and TED-LIUM tasks and, considering\nthat 0.6 for RTVE pays off in WER, we selected this value to\ninclude similar ﬁxed delays between all tasks.\nIn the following experiments, the trade-off between WER and\nlatency is evaluated as it is a critical factor in streaming systems.\nIn all these experiments, latency is measured as the time elapsed\nbetween the instant at which an acoustic frame is generated and\nit is fully processed by the decoder. The ﬁnal latency for a sample\n(segment or video) is estimated as the average of the latencies at\nframe level. These measurements were run on an Intel i7-3820\nCPU @ 3.60 GHz, with 64 GB of RAM and a RTX 2080 Ti\nGPU card. For simplicity, the time required to transform raw\naudio into ﬁlter bank was not included in our measures since\nthis time is negligible and complicates the procedure used to\nestimate latencies.\nFig. 4 shows WER as a function of latency with or without\nAcoustic Model Look-Ahead (AMLA) enabled, using b =2 0.\nAs observed, AMLA is effective to decrease the search effort\nby exploring more promising paths and, consequently, better\nperformance can be achieved at the same level of latency. This\nis mainly observed for LibriSpeech and TED-LIUM tasks, but\nnot in RTVE. In RTVE, the number of active hypotheses is less\nthan in LibriSpeech or TED-LIUM, meaning that the reduction\nin the number of active hypotheses does not compensate for the\ncomputational overhead of AMLA.\nAs stated in Section II, the batch size directly impacts on both,\nlatency and WER when AMLA is enabled, as the set of windows\nin the batch will be used to compute not only the acoustic scores,\nbut also the acoustic look-ahead with partial acoustic scores.\nRegarding this impact, we explored batch sizes of 20 and 40 with\nAMLA enabled, observing that more context to compute AMLA\nscores lead to similar accuracy for segment-based tasks, such\nas LibriSpeech and the conventional TED-LIUM, but slightly\nbetter WERs in video-based tasks, such as TED-LIUM videos\nFig. 5. FSN, DTN and WMA normalization (with different αvalues) schemes\nevaluated on WER for segment-based tasks.\nand RTVE. This is explained by the limited context of short\nsegments of the former tasks compared to the latter tasks. In the\ncase of RTVE, as very similar performance is achieved with or\nwithout AMLA enabled, the effect of batch size seems to be neg-\nligible. Finally, as expected, higher latencies are obtained with\nlarger batch sizes in all tasks, as longer delays are introduced to\ngather enough frames to complete the batch. According to these\nresults, in the remaining experiments, AMLA was enabled for\nLibriSpeech and both TED-LIUM tasks, but not for RTVE.\nFigs. 5 and 6 depict for segment-based and video-based\ntasks, respectively, the effect in WER for the acoustic feature\nnormalization schemes described in Section II-C. In the case\nof the WMA scheme, WER is also shown as a function of\nthe batch size ( b) and the parameter α used to weight the\nimportance of frames in previous batches. As observed, FSN\nand DTN provided similar WER in all tasks with the exception\nof LibriSpeech where higher improvements were achieved when\nFSN is applied.\nNevertheless, WMA clearly outperforms FSN and DTN when\ndecoding long sequences, as shown in Fig. 6 for TED-LIUM\n156 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nFig. 6. FSN, DTN and WMA normalization (with different αvalues) schemes\nevaluated on WER for video-based tasks.\nvideos and RTVE tasks. The capacity of WMA to partially\nforget the previous context and adapt to new acoustic conditions\nseems to improve the performance of the recognition as more\nacoustic variations are likely to appear in long sequences.\nThis is not the case of segment-based tasks shown in Fig. 5,\nin which WMA did not outperform FSN, since sequences\nare shorter and acoustic conditions more stable (i.e. usually\none sentence with a single speaker). When looking into the\nparameter α of WMA, it is observed that values close to 1.0\n(equally weighting the previous and current batches) beneﬁt\nsegment-based tasks, as this is close to consider the complete\nsequence for normalization. However, values of α close to 0.9\nprovide better results in video-based tasks.\nAdditionally, we assessed the impact of the batch size in the\nnormalization context. In this regard for normalization, unlike\nAMLA, using a batch size of 40 instead of 20 provided consis-\ntently better results along all tasks. Despite of this, taking the\nWER-latency trade-off into consideration, a batch size of 20 was\nselected in the following experiments to keep the latency as low\nas possible.\nFig. 7. WER (left y-axis) and PPL (right y-axis) as a function of TLM history\nlimitation and different LMHR values for segment-based tasks. Solid curves\nrepresent WER, while the dashed curve is PPL.\nD. Experiments on Language Modeling for Streaming\nAs shown in Table III, the TLM provided the best performance\nmeasured in terms of PPL for all tasks. This is in line with\nprevious results reported in [5]. For this reason, the following\nexperiments are focused on performing a comprehensive evalu-\nation of the TLM behavior in streaming conditions.\nAs introduced in Section III, the previous history of word\nsequences should be limited in order to keep the performance of\nthe streaming decoder. This enforces a limitation in the number\nof words to consider when computing the LM probabilities\nthat matches the history limitation of the TLM. In addition,\nthe LMHR parameter controls the LM previous context to\ndecide whether hypothesis recombination is performed. Both\nparameters are interrelated in streaming decoding as shown in\nthe following experiments.\nFigs. 7 and 8 plot WER as solid curves (left y-axis) and\nPPL as a dashed curve (right y-axis), as a function of the TLM\nhistory limitation in number of words using different LMHR\nvalues for all tasks. To better understand the values of the\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 157\nFig. 8. WER (left y-axis) and PPL (right y-axis) as a function of TLM\nhistory limitation and different LMHR values for video-based tasks. Solid curves\nrepresent WER, while the dashed curve is PPL.\nLMHR parameter, for instance, a LMHR of 3 would indicate\nthat hypothesis recombination is performed at 4-gram level.\nSimilarly to previous experiments related to acoustic mod-\neling, segment-based tasks in Fig. 7 show a different trend\ncompared to video-based tasks in Fig. 8. Fig. 7 shows that\nLMHR curves behaved similarly when increasing the TLM\nhistory limitation. This limit reached the best operating point\nin 60 words for LibriSpeech and TED-LIUM matching up with\nthe lowest perplexity in both cases. The best performance was\nachieved with LMHR values of 12 and 9 for LibriSpeech and\nTED-LIUM, respectively, while higher values provided slightly\nhigher WERs. This would indicate that for these segment-based\ntasks, longer histories have very similar contexts that make\npruning less effective.\nIn the case of the video-based tasks, Fig. 8 shows that PPL and\nWER ﬁgures increase beyond a TLM history of about 40 words.\nIt is worth noting that in both TED-LIUM tasks the trained TLM\nwas the same, that is, it was trained from full sentences not\ncomplete videos. This would explain why the PPL increased on\nvideo-based tasks when more than 60 words were considered\nfor the TLM history. This fact was also reﬂected in WER, since\nthe best results were consistently achieved using a LMHR value\nof 9 for both TED-LIUM tasks. The aforementioned optimum\noperating point of 60 words became on 40 words in video-based\ntasks as the speech input now is not as structured as in the\nsegment-based tasks, and the beginning and end of sentences can\nbe mixed during decoding, a situation that was not considered\nwhen training the TLM.\nRegarding the RTVE task, a more stable behavior was ob-\nserved in performance using a broader range of the TLM history.\nIn this case, the performance only degraded when very high\nvalues of TLM history of about 140 words were considered.\nThis might be explained by the fact that this LM was trained\nwith a huge variety of text resources with very different sentence\nlengths and contexts. However, the performance degradation\nas a result of longer histories highlights the need of training\nspeciﬁc models that take into account intra and inter sentence\ncontexts considering complete videos or documents. Finally, the\nLM seemed not to play a crucial role in the RTVE task as can be\ninterpreted by the very similar performance achieved by different\nLMHR values with a slight improvement when using a LMHR\nvalue of 9 and a TLM history of 40 words.\nAs introduced in Section III-C, the LMHP parameter limits\nthe number of active hypotheses that can query the neural LM\nto obtain its probability score reducing in this way the computa-\ntional cost. However, the LMHP has a direct impact on WER as\nit limits the number of hypotheses to be considered during the\nrescoring step in decoding.\nFigs. 9 and 10 depict WER as a function of latency for\nsegment-based and video-based tasks, respectively. The val-\nues of the parameter LMHP represent the number of active\nhypotheses, being LMHP =Inf an unlimited number of active\nhypotheses. As shown, the use of the LMHP pruning technique\nachieved an overall reduction in the system latency as can be\nobserved by the left-shifting of the LMHP curves in almost all\nthe LMHP values with respect to an unlimited number of active\nhypotheses. On the other hand, higher LMHP values translate\ninto better WERs.\nNonetheless, the trade-off between WER and latency is very\ntask dependent. In LibriSpeech, a LMHP value of 20 allows low\nlatencies (about 0.7 seconds) but this limits the best WER to\nabout 6.0%. However, allowing more LM queries (e.g. 40) leads\nto latencies about 0.9 seconds and WERs of about 5.8%. This\nbehavior is different for TED-LIUM in both versions, where a\nLMHP value of 20 means an important increase in WER and\nnot so much improvement in latency. This would mean that\nmore queries in this case seems to help the decoder during the\nsearch to ﬁnd best paths. Beyond this LMHP value, competitive\nWERs were obtained for low latencies getting better results\nwhen coming closer to latencies of about one second. In the\nRTVE task, a LMHP value of 20 provided a very good operating\npoint when latency is closed to 0.8 seconds. Again, in this task\nthe LM did not provide much information so limiting the number\nof queries only helped the performance of the system in terms of\nspeed. This is specially helpful in this kind of long-recognition\ntasks where using conservative parameters allows us to discard\na high number of active hypotheses during the search speeding\n158 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nFig. 9. WER vs. latency in seconds varying LMHP values for segment-based\ntasks.\nup the decoding. These results show how the streaming decoder\ncan be adapted very easily to our needs just adjusting the LMHP\nparameter in order to obtain the desired trade-off between WER\nand latency.\nAs described in Section III-D, a relevant feature of the pro-\nposed streaming decoder is its capability to interpolate on-the-ﬂy\ncount-based and neural LMs. Extensive experiments were car-\nried out with the aim of evaluating the performance of different\nLM interpolations combining n-grams, LSTM-RNN LMs and\nTLMs.\nFigs. 11 and 12 show WER as a function of latency applying\ndifferent combination of LMs for all tasks. Similarly to previous\nexperiments, different behaviors depending on the task can be\nobserved in these ﬁgures. In LibriSpeech, the single use of the\nTLM provided the best result when considering the trade-off\nbetween WER and latency. This could be the expected behavior\nbased on the interpolation weights reported in Table III for the\nTLM (86-96%). The slight improvements in PPL achieved by the\ninterpolation seemed not to have an inﬂuence on WER within\nthe considered range of latencies. In the case of TED-LIUM,\nFig. 10. WER vs. latency in seconds varying LMHP values for video-based\ntasks.\nthe weights were distributed in a different way, since n-gram\nand LSTM-RNN LMs weights were from about 22% to 38%\nwhen combined one-on-one with TLM and 30-40% when all\nthe LMs were combined. In this case, while the combination\nof TLM with n-gram or with LSTM-RNN provided similar\nperformance, the combination of the three LMs consistently\nprovided the best WERs for all the considered range of latencies\nand for both, segment-based and video-based tasks. In RTVE,\nno signiﬁcant differences were found in performance across\ndifferent LM combinations. As in LibriSpeech, the TLM weight\nin the interpolation was between 85-94% for RTVE and this\nseemed to be the reason why the LM interpolation did not have\na signiﬁcant effect on WER.\nAs a ﬁnal experiment, the performance of the streaming\ndecoder was evaluated on the test set of all tasks using the hyper-\nparameters optimized on the development sets in the previous\nexperiments. Hyperparameters were optimized aiming at mini-\nmizing WER while the average latency was close to 1 s. Table IV\nreports WER and latency ﬁgures on the test sets comparing them\nwith the best WER reported in previous works. In the case of\nCSC [3] and LC [17], the setup recommended by the authors was\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 159\nFig. 11. WER vs. latency in seconds considering different interpolation\nschemes with TLM for each segmented task.\nTABLE IV\nWER AND LATENCY IN SECONDS ON THE TEST SETS USING THE OPTIMIZED\nSTREAMING SYSTEMS FOR ALL THE EVA L UAT I O NTASKS COMPARED TO\nPREVIOUS WORKS\nFig. 12. WER vs. latency in seconds considering different interpolation\nschemes with TLM for each video task.\nproperly adapted to our framework. As observed, our streaming\ndecoder offers competitive WERs even compared with ofﬂine\ndecoders, demonstrating its applicability to real-world streaming\napplications.\nRegarding latency ﬁgures, segment-based tasks, such as LS\nand TDs, showed a greater variability. This is explained by the\nfact that pruning was not so aggressive in these tasks in order\nto minimize WER, leading to latency peaks in some samples in\nwhich the decoder could not catch up before the sample ends.\nHowever, pruning was easier to adjust to stabilize latency in\nvideo-based tasks, as observed in TDv and RTVE.\nV. C ONCLUSION AND FUTURE WORK\nIn this work an improved decoder based on the conventional\nhybrid ASR approach was proposed by adapting state-of-the-art\nmodels to the streaming setup. In particular, deep BLSTM\nacoustic models were adapted to the streaming conditions by\nusing a sliding window of future context. Other techniques such\n160 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 30, 2022\nas on-the-ﬂy normalization of acoustic features and the improve-\nment of pruning techniques related to acoustic and language\nmodels were also addressed.\nThe proposed decoder was evaluated by carrying out a\ncomprehensive experimentation on well-known academic\ndatasets and real-world challenging tasks. As reported, this\ndecoder presented a very competitive performance being easily\nadapted to the task by tuning the desired trade-off between\nWER and latency.\nEven so, the streaming setup opens some interesting chal-\nlenges to be further investigated. For instance, in our experiments\nthe same acoustic models were used independently from the\nwindow size employed at decoding time in order to alleviate the\ncomputational cost of training acoustic models. In order to ad-\ndress this mismatch between training and decoding conditions,\nthe same window size in both training and decoding is desirable\nto better capture the nature of the task (i.e., segment-based or\nvideo-based). Moreover, according to the requirements of the\nlatency, the window size could be dynamically adjusted in the\ndecoding phase. On the other hand, real streaming tasks involve\nthe recognition of long recordings across sentences, in this sense\nit would be very interesting to evaluate the performance of TLMs\ntaking into account larger contexts.\nREFERENCES\n[1] D. Yu and L. Deng, Automatic Speech Recognition: A Deep Learning\nApproach. New York NY , USA: Springer, 2014.\n[2] A. Graves and J. Schmidhuber, “Framewise phoneme classiﬁcation with\nbidirectional LSTM and other neural network architectures,” Neural Netw.,\nvol. 18, no. 5/6, pp. 602–610, 2005.\n[3] K. Chen and Q. Huo, “Training deep bidirectional LSTM acoustic model\nfor LVCSR by a context-sensitive-chunk BPTT approach,” IEEE/ACM\nTrans. Audio, Speech, Lang. Process., vol. 24, no. 7, pp. 1185–1193,\nJul. 2016.\n[4] A. Zeyer, P. Doetsch, P. V oigtlaender, R. Schlüter, and H. Ney, “A com-\nprehensive study of deep bidirectional LSTM RNNs for acoustic modeling\nin speech recognition,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2017, pp. 2462–2466.\n[5] K. Irie, A. Zeyer, R. Schlüter, and H. Ney, “Language modeling with deep\ntransformers,” in Proc. InterSpeech, 2019, pp. 3905–3909.\n[6] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y . Wu, “Exploring\nthe limits of language modeling,” 2016, Accessed: Dec. 14 2021. [Online].\nAvailable: https://research.google/pubs/pub45446/.\n[7] N. Moritz, T. Hori, and J. Le Roux, “Streaming automatic speech recogni-\ntion with the transformer model,” in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process., 2020, pp. 6074–6078.\n[8] H. Miao, G. Cheng, C. Gao, P. Zhang, and Y . Yan, “Transformer-based\nonline CTC/attention end-to-end speech recognition architecture,” in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process., 2020, pp. 6084–6088.\n[9] T.-S. Nguyen, N.-Q. Pham, S. Stueker, and A. Waibel, “High performance\nsequence-to-sequence model for streaming speech recognition,” in Proc.\nInterSpeech, 2020, arXiv:2003.10022.\n[10] D. Nolden, “Progress in decoding for large vocabulary continuous speech\nrecognition,” Ph.D. dissertation, RWTH Aachen Univ., Aachen, Germany,\nApr. 2017.\n[11] B. Chen, J.-W. Kuo, and W.-H. Tsai, “Lightly supervised and data-driven\napproaches to mandarin broadcast news transcription,” Proc. IEEE Int.\nConf. Acoust., Speech Signal Process., vol. 1, pp. I- 777, 2004.\n[12] X. Chen, X. Liu, A. Ragni, Y . Wang, and M. Gales, “Future word contexts\nin neural network language models,” in Proc. Autom. Speech Recognit.\nUnderstanding, 2017, pp. 97–103.\n[13] A. Ogawa, M. Delcroix, S. Karita, and T. Nakatani, “Rescoring N-best\nspeech recognition list based on one-on-one hypothesis comparison using\nencoder-classiﬁer model,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2018, pp. 6099–6103.\n[14] S. Kombrink, T. Mikolov, M. Karaﬁát, and L. Burget, “Recurrent neu-\nral network based language modeling in meeting recognition,” in Proc.\nInterSpeech, 2011, pp. 2877–2880.\n[15] A. Mohamed et al., “Deep bi-directional recurrent networks over spec-\ntral windows,” in Proc. Autom. Speech Recognit. Understanding, 2015,\npp. 78–83.\n[16] A. Zeyer, R. Schlüter, and H. Ney, “Towards online-recognition with\ndeep bidirectional LSTM acoustic models,” in Proc. InterSpeech, 2016,\npp. 3424–3428.\n[17] Y . Zhang, G. Chen, D. Yu, K. Yao, S. Khudanpur, and J. Glass, “Highway\nlong short-term memory RNNs for distant speech recognition,” in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process., 2016, pp. 5755–5759.\n[18] S. Xue and Z. Yan, “Improving latency-controlled BLSTM acoustic mod-\nels for online speech recognition,” in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process., 2017, pp. 5340–5344.\n[19] Y . Shi, W. Zhang, M. Cai, and J. Liu, “Efﬁcient one-pass decoding with\nNNLM for speech recognition,” IEEE Signal Process. Lett., vol. 21, no. 4,\npp. 377–381, Apr. 2014.\n[20] Z. Huang, G. Zweig, and B. Dumoulin, “Cache based recurrent neu-\nral network language model inference for ﬁrst pass speech recogni-\ntion,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2014,\npp. 6354–6358.\n[21] T. Hori, Y . Kubo, and A. Nakamura, “Real-time one-pass decoding with\nrecurrent neural network language model for speech recognition,” in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process., 2014, pp. 6364–6368.\n[22] K. Lee, C. Park, I. Kim, N. Kim, and J. Lee, “Applying GPGPU to recurrent\nneural network language model based fast network search in the real-time\nLVCSR,” inProc. InterSpeech, 2015, pp. 2102–2106.\n[23] K. Lee, C. Park, N. Kim, and J. Lee, “Accelerating recurrent neural network\nlanguage model based online speech recognition system,” in Proc. IEEE\nInt. Conf. Acoust. Speech Signal Process., 2018, pp. 5904–5908.\n[24] E. Ar1soy, S. F. Chen, B. Ramabhadran, and A. Sethy, “Converting neural\nnetwork language models into back-off language models for efﬁcient\ndecoding in automatic speech recognition,” IEEE/ACM Trans. Audio,\nSpeech, Lang. Process., vol. 22, no. 1, pp. 184–192, May 2014.\n[25] M. Singh, Y . Oualil, and D. Klakow, “Approximated and domain-adapted\nLSTM language models for ﬁrst-pass decoding in speech recognition,” in\nProc. InterSpeech, 2017, pp. 2720–2724.\n[26] J. Jorge, A. Giménez, J. Iranzo-Sánchez, J. Civera, A. Sanchis, and A.\nJuan, “Real-time one-pass decoder for speech recognition using LSTM\nlanguage models,” in Proc. InterSpeech, 2019, pp. 3820–3824.\n[27] J. Jorge et al., “LSTM-based one-pass decoder for low-latency stream-\ning,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2020,\npp. 7814–7818.\n[28] P. Baquero-Arnal et al., “Improved hybrid streaming ASR with transformer\nlanguage models,” in Proc. InterSpeech, 2020, pp. 2127–2131.\n[29] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An\nASR corpus based on public domain audio books,” in Proc. IEEE Int.\nConf. Acoust., Speech Signal Process., 2015, pp. 5206–5210.\n[30] A. Rousseau, P. Deléglise, and Y . Esteve, “Enhancing the TED-LIUM\ncorpus with selected data for language modeling and more TED talks,” in\nProc. LREC, 2014, pp. 3935–3939.\n[31] H. Ney and S. Ortmanns, “Progress in dynamic programming search for\nLVCSR,”Proc. IEEE, vol. 88, no. 8, pp. 1224–1240, Aug. 2000.\n[32] A. Cardenal-López, F. J. Diéguez-Tirado, and C. García-Mateo, “Fast\nLM look-ahead for large vocabulary continuous speech recognition using\nperfect hashing,” Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\nvol. 1, pp. I-705–I-708, 2002.\n[33] L. Chen and K. K. Chin, “Efﬁcient language model look-ahead probabil-\nities generation using lower order LM look-ahead information,” in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process., 2008, pp. 4925–4928.\n[34] D. Nolden, H. Ney, and R. Schlüter, “Exploiting sparseness of backing-off\nlanguage models for efﬁcient look-ahead in LVCSR,” in Proc. IEEE Int.\nConf. Acoust., Speech Signal Process., 2011, pp. 4684–4687.\n[35] E. Lleida et al., “RTVE2018 database description,” Cátedra RTVE - Uni-\nversidad Zaragoza, 2018. [Online]. Available: http://catedrartve.unizar.es/\nreto2018/RTVE2018DB.pd.\n[36] M. del Agua et al., “The translectures-UPV toolkit,” in Proc. Adv. Speech\nLang. Technol. Iberian Lang., 2014, pp. 269–278.\n[37] M. Abadi et al., “TensorFlow: a system for large-scale machine learning,”\nin Proc. 12th USENIX Conf. Operating Syst. Des. Implementation, 2016,\npp. 265–283.\n[38] D. S. Park et al., “SpecAugment: A simple data augmentation method for\nautomatic speech recognition,” inProc. InterSpeech, 2019, pp. 2613–2617.\nJORGE et al.: LIVE STREAMING SPEECH RECOGNITION USING DEEP BIDIRECTIONAL LSTM ACOUSTIC MODELS 161\n[39] A. Giménez, J. Andrés-Ferrer, and A. Juan, “Discriminative bernoulli\nHMMs for isolated handwritten word recognition,” Pattern Recognit. Lett.,\nvol. 35, pp. 157–168, 2014.\n[40] S. J. Young, J. J. Odell, and P. C. Woodland, “Tree-based state tying for high\naccuracy acoustic modelling,” in Proc. Workshop Hum. Lang. Technol.\nConf., 1994, pp. 307–312.\n[41] S. Lison, and J. Tiedemann,“OpenSubtitles2016: Extracting Large Parallel\nCorpora from Movie and TV Subtitles,” in Proc. 10th Int. Conf. Lang.\nResour. Eval., 2016.\n[42] UFAL MEDICORP, “UFAL Medical Corpus v.1.0,” 2017, Accessed:\nDec. 14, 2021. [Online]. Available: https://ufal.mff.cuni.cz/ufal_medical_\ncorpus.\n[43] Wikimedia Foundation, “Wikipedia,” Accessed: Dec. 14, 2021. [Online].\nAvailable: https://www.wikipedia.org/\n[44] C. Callison-Burch et al., “Findings of the 2012 workshop on statistical\nmachine translation,” in Proc. WMT, 2012, pp. 10–51.\n[45] Common Crawl, “News crawl corpus (WMT workshop) 2015,” Ac-\ncessed: Dec. 14, 2021. [Online]. Available: http://www.statmt.org/wmt15/\ntranslation-task.html\n[46] “Eldiario.es,” 2020, Accessed: Dec. 14, 2021. [Online]. Available: https:\n//www.eldiario.es/\n[47] “ElPeriodico.com,” 2020, Accessed: Dec. 14, 2021. [Online]. Available:\nhttps://www.elperiodico.com/\n[48] “CommonCrawl 2014,” Accessed: Dec. 14, 2021. [Online]. Available:\nhttp://commoncrawl.org/\n[49] A. Stolcke, “SRILM - an extensible language modeling toolkit,” in Proc.\nInterSpeech, 2002, pp. 901–904.\n[50] X. Chen, X. Liu, Y . Qian, M. J. F. Gales, and P. C. Woodland, “CUED-\nRNNLM - an open-source toolkit for efﬁcient training and evaluation\nof recurrent neural network language models,” in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process., 2016, pp. 6000–6004.\n[51] A. Mnih and Y . W. Teh, “A fast and simple algorithm for training neural\nprobabilistic language models,”Proc. 29th Int. Conf. Mach. Learn.,,v o l .2 ,\n2012, arXiv:1206.6426.\n[52] X. Chen, X. Liu, Y . Wang, M. J. F. Gales, and P. C. Woodland, “Improving\nthe training and evaluation efﬁciency of recurrent neural network language\nmodels,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2015,\npp. 5401–5405.\n[53] M. Ott et al., “Fairseq: A fast, extensible toolkit for sequence modeling,”\nin Proc. NAACL-HLT, 2019, pp. 48–53.\n[54] K. J. Han, J. Pan, V . K. N. Tadala, T. Ma, and D. Povey, “Multistream CNN\nfor robust acoustic modeling,” in Proc. IEEE Int. Conf. Acoust., Speech\nSignal Process., 2021, pp. 6873–6877.\n[55] W. Zhou, W. Michel, K. Irie, M. Kitza, R. Schlüter, and H. Ney, “The\nRWTH ASR system for TED-LIUM release 2: Improving hybrid HMM\nwith specAugment,” in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess., 2020, pp. 7839–7843.\n[56] K. J. Han, J. Huang, Y . Tang, X. He, and B. Zhou, “Multi-stride\nself-attention for speech recognition,” in Proc. InterSpeech, 2019,\npp. 2788–2792.\n[57] J. Jorge et al., “MLLP-UPV and RWTH aachen spanish ASR systems\nfor the IberSpeech-RTVE 2018 speech-to-text transcription challenge,” in\nProc IberSpeech, 2018, pp. 257–261.\nJavier Jorge received the B.Sc. degree in computer\nscience, in 2014, the M.Sc. degree in artiﬁcial in-\ntelligence, pattern recognition, and digital imaging,\nin 2015, and is currently working toward the Ph.D.\ndegree with the Universitat Politècnica de València,\nSpain, ﬁnanced by Grant FPU14/03981 from the\nSpanish Ministry of Education, Culture and Sport.\nHe is a Co-Author of several articles presented at\ninternational conferences, and is actively involved\nin R&D projects (X5gon, Multisubs). His current\nresearch interests include streaming speech recogni-\ntion, speciﬁcally decoding with acoustic and language modeling adapted to this\nenvironment.\nAdrià Giménez received the Ph.D. degree in com-\nputer science from Universitat Politècnica de Valèn-\ncia (UPV), Valencia, Spain, in 2014. He is currently\na Postdoctoral Researcher with UPV . He is currently\na Member of the UPV’s Machine Learning and Lan-\nguage Processing Research Group (MLLP). He has\nauthored or coauthored more than 30 articles in inter-\nnational journals and conferences, and he has been ac-\ntively involved in three EU research projects and sev-\neral Spanish research projects. His current research\nfocuses on deep learning for speech recognition.\nJoan Albert Silvestre-Cerdà is currently a Ph.D. As-\nsistant Professor of computer science with the Univer-\nsitat Politècnica de València, Valencia, Spain, - cam-\npus d’Alcoi, and a Member of the Machine Learning\nand Language Processing (MLLP) Research Group,\nintegrated into the Valencian Research Institute on\nArtiﬁcial Intelligence (VRAIN). He has more than\nten years of research experience on machine learning\nand natural language processing applications, mainly\nin the area of automatic speech recognition, with an\nspecial focus on the development of technologies and\nsolutions that can be deployed into real-life production environments. During\nthis time period, he has coauthored more than 20 articles published in inter-\nnational journals and conferences, and has participated in ten publicly-funded\nEU/Spanish research projects.\nJorge Civera received the Ph.D. degree from Univer-\nsitat Politècnica de València (UPV), Valencia, Spain,\nin 2008. He is currently an Associate Professor of\ncomputer science with the Universitat Politècnica\nde València (UPV), and has been a Member of the\nMachine Learning and Language Processing (MLLP)\nResearch Group since 2014, and also part of the Va-\nlencian Research Institute for Artiﬁcial INtelligence\n(VRAIN). He has participated in 30 research projects\nand has authored or coauthored more than 75 articles\nin international journals and conferences. He is also\nan Advisor for three Ph.D. thesis on different MLLP topics. His most recent work\ninclude his participation in the EU research projects transLectures, EMMA, and\nX5gon.\nAlbert Sanchis received the Ph.D. degree in 2004. He\nis currently a Ph.D. Associate Professor of computer\nscience with the Universitat Politècnica de Valèn-\ncia (UPV), Valencia, Spain. He is a Member of the\nUPV’s Machine Learning and Language Processing\nResearch Group (MLLP). He is a Co-Author of more\nthan 60 articles in international journals and confer-\nences. He has participated in six EU research projects\nand more than ten Spanish research projects. He is\nalso an Advisor for three Ph.D. theses on different\nMLLP topics. His most recent work includes the\nparticipation in the EU projects transLectures, EMMA and X5gon. He is also\nleading a Spanish government-funded project on fostering open education and\nparliamentary openness by providing multilingual access to video resources.\nAlfons Juan received the Ph.D. degree from Univer-\nsitat Politècnica de València, Valencia, Spain, in 2000.\nHe is currently a Full Professor of computer science\nwith the Universitat Politècnica de València, where\nhe has been leading a Research Group on Machine\nLearning and Language Processing (MLLP), since\n2014. He has participated in more than 30 research\nprojects and has authored or coauthored more than\n150 articles in international journals and conferences.\nHe is also an Advisor for 13 Ph.D. theses on different\nMLLP topics. His most recent work includes the\nparticipation in the EU projects transLectures, EMMA, and X5gon.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8447279930114746
    },
    {
      "name": "Language model",
      "score": 0.7289530038833618
    },
    {
      "name": "Speech recognition",
      "score": 0.6036984324455261
    },
    {
      "name": "Latency (audio)",
      "score": 0.57408207654953
    },
    {
      "name": "Transformer",
      "score": 0.566144585609436
    },
    {
      "name": "Streaming data",
      "score": 0.504801869392395
    },
    {
      "name": "Pruning",
      "score": 0.45067527890205383
    },
    {
      "name": "Sliding window protocol",
      "score": 0.41893208026885986
    },
    {
      "name": "Hidden Markov model",
      "score": 0.41659799218177795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3424336910247803
    },
    {
      "name": "Real-time computing",
      "score": 0.3362378478050232
    },
    {
      "name": "Window (computing)",
      "score": 0.19575753808021545
    },
    {
      "name": "Data mining",
      "score": 0.1442495882511139
    },
    {
      "name": "Telecommunications",
      "score": 0.10115200281143188
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210131846",
      "name": "Artificial Intelligence Research Institute",
      "country": "ES"
    }
  ],
  "cited_by": 23
}