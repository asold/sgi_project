{
  "title": "A Study of Errors in the Output of Large Language Models for Domain-Specific Few-Shot Named Entity Recognition",
  "url": "https://openalex.org/W4412115807",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5092846923",
      "name": "Elena Volkanovska",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A5092846923",
      "name": "Elena Volkanovska",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4303182848",
    "https://openalex.org/W6852670792",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4287020173",
    "https://openalex.org/W6851275496",
    "https://openalex.org/W4384697538",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6982934678"
  ],
  "abstract": "This paper proposes an error classification framework for a comprehensive analysis of the output that large language models (LLMs) generate in a few-shot named entity recognition (NER) task in a specialised domain. The framework should be seen as an exploratory analysis complementary to established performance metrics for NER classifiers, such as F1 score, as it accounts for outcomes possible in a few-shot, LLMbased NER task. By categorising and assessing incorrect named entity predictions quantitatively, the paper shows how the proposed error classification could support a deeper cross-model and cross-prompt performance comparison, alongside a roadmap for a guided qualitative error analysis.",
  "full_text": "Elena Volkanovska\nA Study of Errors in the Output of Large Language Models for\nDomain-Specific Few-Shot Named Entity Recognition\nAbstract\nThis paper proposes an error classification framework for a comprehensive analysis of\nthe output that large language models (LLMs) generate in a few-shot named entity\nrecognition (NER) task in a specialised domain. The framework should be seen as\nan exploratory analysis complementary to established performance metrics for NER\nclassifiers, such as F1 score, as it accounts for outcomes possible in a few-shot, LLM-\nbased NER task. By categorising and assessing incorrect named entity predictions\nquantitatively, the paper shows how the proposed error classification could support a\ndeeper cross-model and cross-prompt performance comparison, alongside a roadmap for\na guided qualitative error analysis.\n1 Introduction\nThe advent of generative large language models (LLMs) created an increased interest\nin experimenting with few-shot methods for named entity recognition (NER). With\nLLMs, NER can be defined as a question-answering task, where a model is prompted to\nidentify1 named entities based on a named entity definition and named entity examples\nprovided in the prompt. In real-world scenarios, the need for few-shot NER is driven\nby scarcity of resources, legal constraints for sharing annotated data, and the cost of\nannotation (Moscato, Postiglione, & Sperlí, 2023). However, the success of few-shot\nNER techniques is not consistent. Some studies using known NER datasets and LLMs\nhave reported promising results (Ashok & Lipton, 2023; Epure & Hennequin, 2022; Wang\net al., 2023).2 At the same time, experiments using more specialised NER datasets, such\nas the one described in Section 3, do not achieve the same degree of success. Moscato\net al. (2023) also mention that the success of few-shot NER in real-world deployment\nscenarios is yet to be proven.\nThis study investigates the possible causes of such inconsistencies by analysing LLMs’\noutput in experiments that yielded F1 scores that were substantially below the task\nbaseline. Rather than discarding the output as noise, the paper aims to identify what\n1An effort was made to refrain from using anthropomorphising terms when describing LLMs (see\nInie, Druga, Zukerman, and Bender (2024) for more information on this topic); nevertheless, this\ntype of language is common in the context of generative language models and, in some cases,\ndifficult to evade.\n2Some authors acknowledge that data contamination i.e. the likelihood of the used LLMs having\nbeen previously exposed to the NER datasets might affect the outcome.\nJLCL 2025 – Band 38(2) – 31–42\nVolkanovska\nlessons can be learned by proposing a draft framework for a descriptive error analysis.\nTo do so, the study first reviews existing approaches to error analysis in few-shot NER in\nSection 2, followed by a brief description of the experiments underpinning the analysed\ndata in Section 3. The proposed error classification and the insights it provides into\nLLM performance are discussed in Sections 4 and 5 respectively.\n2 Related Work\nGenerative pre-trained language models employed in some studies exploring few-shot\nmethods for in- and cross-domain NER include the Pretrained Conditional Generation\nModel of Flan-T5-XXL (11B) (Chung et al., 2024), GPT-3.5 (Brown et al., 2020), and\nGPT4 (Achiam et al., 2023), all of which have been used in the study by Ashok and\nLipton (2023); GPT-3 (davinci-003) used by Wang et al. (2023), and a medium-sized\nGPT-2 model used in few-shot NER experiments by Epure and Hennequin (2022).\nThese studies showed that the named entities (NEs) identified by LLMs can lead to\nvaluable insights. Ashok and Lipton (2023) conduct a human survey of errors, where\nthey (1) create a list containing 20 randomly selected examples of predicted named\nentity instances, (2) create a ground truth list containing NEs from the same sentences\nused to create list (1), and (3) ask three different human annotators to evaluate each\nentity of lists (1) and (2). The human annotators are given a definition of the NER\nproblem relevant to the dataset from which the lists are created. The results from this\nevaluation show that many of the predictions could be acceptable NE candidates and\nwere not considered errors by the human annotators.\nThe evaluation approach adopted by Epure and Hennequin (2022) for NER in a\nfew-shot setting is case-insensitive and accommodates for output where the model\ngenerates an NE with a different spelling or when it fails to follow the instruction for\nsentences containing no entities. The study dubs asconfusion patternscases when the\nLM fails to generate the correct entity type, conflating, for example,corporation or\ngroup with location. The study’s authors provide a brief overview of NE categories that\nperform well and categories that do not. Wang et al. (2023) also find that the LLM\nconflates location and geographical entitiesin a nested NER scenario.\nWhile it is evident that language models’ output is manually inspected, with re-\nsearchers working in few-shot NER performing an error analysis in order to compare\nthe effects of various prompt designs and task requirements, the insights that come from\nthe manual inspection are mostly captured in the recommendations for prompt design\nin future studies. In other words, such analyses have not amounted to a systematic\nclassification of errors identified in models’ output.\nContribution This paper proposes a descriptive error analysis method for LLM output\nin a few-shot NER task on two domain-specific NER datasets. It combines categories\nfrom existing NER evaluation metrics, such as F1 scores, and error analyses encountered\nin previous studies on few-shot NER into a single error classification framework for\nmodel output analysis. This framework could be used to (1) gauge weak points in\n32 JLCL\nA Study of Errors for LLM Output in Domain-Specific Few-Shot NER\nthe task design and in the LLMs’ performance and (2) make informed decisions for\nqualitative error analysis and iterative changes to the prompt design.\n3 Data\nLLMs and datasetsThe data analysed in this study is the LLM output from a series\nof few-shot NER experiments, where 7762 prompts are run on four LLMs: OpenAI’s\ngpt-4o-2024-05-13 and gpt-4o-mini (hereinafter: gpt-4o and gpt-4o-mini), and Meta’s\nMeta-Llama-3.1-70B-Instruct and Meta-Llama-3.1-405B-Instruct (hereinafter: Llama-\n70B and Llama-405B). The experiments are conducted on the test data splits of two\nNER datasets comprising scientific texts: Climate-Change-NER (Bhattacharjee et al.,\n2024) with 13 climate-change-relevant NE categories (climate-assets, climate-datasets,\nclimate-greenhouse-gases, climate-hazards, climate-impacts, climate-mitigations, climate-\nmodels, climate-nature, climate-observations, climate-organisms, climate-organizations,\nclimate-problem-origins, and climate-properties), and BiodivNER (Abdelmageed et\nal., 2022) with 6 biodiversity-relevant NE categories (organism, phenomena, matter,\nenvironment, quality, andlocation). The LLMs’ output and dataset information are\navailable in a dedicated GitHub repository.3\nPrompts The rationale behind the prompting methodology, the prompt design, and\nthe results for each prompt and language model are described in detail in Volkanovska\n(2025). The prompt design was inspired by the study of Ashok and Lipton (2023), with\nthe final promts differing in three major ways: (1) the input/output requirement (either\na Python string or a tokenized sentence i.e. a Python list of word-based tokens and their\nindices), (2) the number of NE categories tested, and (3) the method of selecting task\nexamples (TEs) in the prompt. Under (1), the prompts can have eitherstring-based\nor token-based input (TEs) and output (a requirement for the model to generate an\nanswer in a format that corresponds to the TEs). Under (2), there arefull prompts,\nwhere models are tested on the complete set of NE categories, andcluster prompts,\nwhere the models are tested on subgroups of NE categories.\nThe categoryfull promptscontains 6 prompt versions, which differ in the number\nof TEs provided to the model (3, 4 or 5). Regardingcluster prompts, named entities\nare divided into clusters of categories. For Climate-Change-NER, the clusters are:\n(1) climate-hazards, climate-problem-origins, climate-greenhouse-gases; (2) climate-\nimpacts, climate-assets, climate-nature, climate-organisms; (3)climate-datasets, climate-\nmodels, climate-observations, climate-properties, and (4)climate-mitigations, climate-\norganisations. For BiodivNER, the three clusters are: (1)environment, location; (2)\norganism, matter, and (3)phenomena, quality. Finally, under (3), TEs contained either\nrandomly selected sentences from the train data split, or sentences with a high semantic\nsimilarity score to the sentence the model was to annotate. Semantic similarity scores\nwere calculated with the library sentence-transformers (Reimers & Gurevych, 2019)\nand the modelsentence-transformers/stsbdistilroberta-base-v2.\n3https://github.com/volkanovska/NER-annotation-with-LLMs\nJLCL 2025 – Band 38(2) 33\nVolkanovska\nThe different prompting scenarios showed that token-based prompts performed, on\naverage, slightly better than string-based prompts. For the former, LLMs’ averaged F1\nscores4 ranged between 0.27 (lowest) and 0.41 (highest). For string-based prompts, the\naveraged F1 scores ranged from 0.28 to 0.39. LLMs generally performed better when\nthere were more TEs, while the TEs’ similarity to the task sentence had a greater impact\non the result when the original dataset contained some noise, most likely introduced by\ntext extraction from PDF sources. As token-based prompts performed slightly better\nthan string-based prompts, the error analysis proposed in this paper is conducted on\nthe output from token-based prompts. See Appendix 7 for a prompt example.\n4 Methodology\nIn the context of this study,error encompasses all instances where the model’s output\ndoes not fully match the correct answer. For a candidate entity to be consideredcorrect,\nthere must be a full span-and-category match between the candidate and the gold\nstandard named entity. Partial matches, as well as minor hallucinations, such as an\nincorrectly spelled entity type, are considered errors.\nThe LLM output of named entity candidates is thus analysed as follows: first, a\ncount of all predicted entities is provided. Perfect and missed matches of (entity, entity\ncategory) are counted by comparing the model’s predictions to the gold standard. Then,\npredicted entities that are notperfect matches are divided into four error classes: (1)\nLLM output where a valid NE instance5 is assigned the wrong category from the set\nof valid NE categories6 (dubbed sources of confusion), (2) a valid NE category is\nassigned to spans that have not been identified as named entities in the original dataset\n(possible candidates), (3) a valid named entity is assigned a named entity category\nthat is not part of the original dataset (new categories) and (4) neither the named\nentity span nor the assigned entity category is valid (pure noise).\nThis error classification is a descriptive overview of the errors found in the models’\noutput and aims to complement established evaluation metrics. Missed and perfect\nmatches, as well assources of confusionand possible candidates, are output categories\nthat have been accounted for in existing evaluation metrics.7 The classesnew cate-\ngories and pure noise are added to capture LLM-specific issues arising from LLMs’\n“hallucinations”.\nCounting error instancesFor cluster prompts, the counts of each error class\nrepresent the number of unique error instances found in each error class per cluster. For\nexample, in cluster 1 of Climate-Change-NER (climate-hazards, climate-problem-origins,\nclimate-greenhouse-gases), errors of the classsources of confusionare counted for\nthis cluster only for each LLM. Forfull prompts, the reported counts per error class\n4An average of the F1 scores calculated for each prompt.\n5Valid NE instanceis an instance that exists as a named entity span in the dataset.\n6Valid NE categoryis a named entity category that is part of the dataset’s entity types.\n7These include missed entity spans, hypothesised entity spans where there are none, entity spans\nthat are assigned the wrong category, entity spans with incorrect boundaries and correct NE\ncategory, and entity spans with incorrect boundaries and incorrect NE category.\n34 JLCL\nA Study of Errors for LLM Output in Domain-Specific Few-Shot NER\nrepresent the average from the six full-prompt versions. For example, the reported\ncount of the error classsources of confusionwill be the sum of the error counts for\neach of the six prompt versions8 divided by six. The Python script for classification of\nerror instances, and the tables with error counts for each error class and each model\nare available in the GitHub repository.\nPoints of comparison In a supervised NE recognition task, a model’s output is only\ncompared to the test split of the gold dataset, given that the train and development\nsplits are used in the model’s training. In the few-shot scenario described in Section\n3, however, the model had not been exposed to the development set at all and had\nbeen exposed to a maximum of five sentences from the train set. For this reason,\nthe LLMs’ output is also compared to the combinationstest and trainand test and\ndevelopment data splits of the gold standard dataset. Differences in the number of\nmissed matches between a model’s predictions and the gold standard across the three\npoints of comparison will show whether some of the candidates generated by the model\nare valid entities in the development and the train data splits.\nIn terms of F1-score, comparative performance has been seen between the larger\nmodels, gpt-4o and LLama-405B, and the smaller models, gpt-4o-mini and Llama-70B.\nFor this reason, error classes are further analysed per two groups of models:large\nand small. The error class ranking for individual models is available in the GitHub\nrepository.\n5 Results\nTables 1 and 2 summarize the error class counts per each prompt type and model,\nshown as percentages: themissed column shows the percentage of missed unique gold\nentities, while the other four columns show the percentage the respective error class\nhas in the total number of unique predicted entity candidates. The columnspredicted\nand gold capture the unique pairs of (named entity, named entity type) in a model’s\noutput and in the gold dataset, respectively. The recurrence of instances is not taken\ninto account for the calculation of percentages in the two tables, as the focus is on\nthe portion of unique instances in each error class; however, repeated occurrences are\naccounted for in the rankings of most-frequently represented categories and named\nentities in each error class; see the discussion underZeroing in on error classesfor more\ndetails.\nAll models generate a substantially higher number of entity candidates in a cluster-\nprompt scenario in Climate-Change-NER and across all prompt scenarios in BiodivNER.\nIn terms of model families, Llama models generate, on average, more entity candidates,\nwhile OpenAI models tend to be more conservative.\nA higher number of entity candidates does not necessarily translate into better\nperformance, as can be seen from the error count results for smaller models, which\n8Prompts with random task examples with 3, 4 and 5 shots, and prompts with similar task examples\nwith 3, 4 and 5 shots.\nJLCL 2025 – Band 38(2) 35\nVolkanovska\nTable 1:Climate-Change-NER: Missed entities as % of gold entities and error class counts as % of\npredicted entities.\nTable 2:BiodivNER: Missed entities as % of gold entities and error class counts as % of predicted\nentities.\ngenerate more noise. Across all models and almost all prompt types, the number of\npossible candidates drops once the spans from the train split of the gold dataset are\nadded to the comparison set. This means that the models generated spans that are\npart of the train split - albeit not under the right category. This tendency is present, to\na lesser extent, in the comparison with the development set. The miscategorisation of\nentity instances also explains why error counts of the categorysources of confusion\nslightly increase once the train data split is added to the comparison. Percentage-wise,\nthe error classes new categories and pure noise have generally very low values\nacross the two datasets and all models. This indicates that the models can “follow” the\nguidance for identifying entities belonging to certain categories only.\n36 JLCL\nA Study of Errors for LLM Output in Domain-Specific Few-Shot NER\nZeroing in on error classesThe top three categories ofpossible entity candidates\nin Climate-Change-NER, identified by larger and smaller LLMs alike, albeit in\ndifferent order, are:climate-models, climate-nature, andclimate-properties. Among the\nmost frequent candidates forclimate-models are instances such asGCM or General\nCirculation Models, which in the gold datasets are only sometimes annotated asclimate-\nmodels, usually when the term is more narrowly defined.9 This echoes some of the\nfindings by Epure and Hennequin (2022), who notice that in few-shot settings, pre-\ntrained models tend to prioritize named entity cues more than context cues. The fact\nthat the acronymGCM appears both as an entity and a non-entity adds a layer of\ncomplexity in the recognition stage that the LLMs cannot resolve based on context cues\ni.e. the term being narrowly-defined or not. All models identify spans such asrandom\nforests as valid instances, which indicates that there seems to be no differentiation\nbetween a climate-specific model and a general model that can be used in a climate\nscenario. LLMs sometimes delete extra whitespaces found in the gold dataset. Models\nwould thus extractWRF-UCM instead ofWRF - UCMas a climate model.\nIn the top-three categories of themissed error class, the categoryclimate-models\ncame in third for large and small models alike, followingclimate-nature and climate-\nproperties. It included instances of LLMs failing to extract acronyms separately from\nthe full name of a climate model, in situations where the acronym followed the name of\na climate model.10\nSmall models tend to generate more invalid categories than their larger counterparts,\nespecially in the error classpure noise. The invalid NE categories range from mis-\nspellings (climate-greenhouse-gasses, climate-impats), labels that are seemingly correct\nbut contain a combination of Latin and Cyrillic letters, to categories that are not part of\nthe original label set at all (climate-projects, climate-regulations, climate-study-field...).\nFor BiodivNER, the top three categories ofpossible candidates identified by large\nLLMs are: quality, organism, andenvironment; a slightly different frequency ranking\nwas noticed in smaller LLMs, namely:organism, quality, andphenomena. While some\nof the candidates could be considered valid instances, such asguinea pigand termites\nfor organism, other candidates include names of organisations and people, which is not\nin line with the NE class description.11\nThe top three entity types in themissed error class for large and small models are:\nquality, matter, andorganism. Some of the most frequently missed instances include\nspecies, tree, and plant, which are found in the error classpossible candidates as\nparts of longer spans.\nWhen it comes to “hallucinations”, models that are on the smaller side tend to\ngenerate them more frequently and in greater variety. Large models did not have any\nerrors in thenew categorieserror class, and generated only 4 invalid categories in the\n9For example, in the spanNASA / GIS GCM, GCM is annotated as a climate model.\n10In the gold dataset, acronyms are annotated as separate entities. For example, in the spanCoupled\nModel Intercomparison Project Phase 5 (CMIP5), Coupled Model Intercomparison Project\nPhase 5 and CMIP5 are two separate entities of the typeclimate-models.\n11The class is defined as “All individual life forms such as microorganisms, plants, animals, mammals,\ninsects, fungi, bacteria etc.”\nJLCL 2025 – Band 38(2) 37\nVolkanovska\npure noiseerror class. Smaller models, on the other hand, generated 6 new categories\nfor existing spans and identified 56 invalid spans across more than 15 invalid categories,\nincluding combined labels such asorganism (quality).\n6 Conclusion and discussion\nThis paper proposes a methodology for classifying errors detected in the output of LLMs\nfollowing a few-shot NER task, where NER is defined as a question-answering task with\na specific output requirement. The proposed error classification provides a snapshot of\nhow LLMs fail and a systematic comparison of the output from multiple LLMs. The\ndescriptive error counts could serve as a basis for (a) additional quantitative and (b)\nguided qualitative analyses. Under (a), one may explore what percentage of the errors\nclassified aspossible candidatesare partial matches with spans from the gold dataset.\nAnother useful information would be the average span lengths across entity instances\nin different error categories, and possible variations in the lengths of sentences where\nentities belonging to different error categories are found. This could help steer efforts\nunder (b), which might include a hands-on comparison of sentences where repeated\nerror instances are found.\nIn this study, the counts of errors in different prompt versions (random and similar\ntask examples with 3, 4, and 5 shots) were averaged due to the limited variations in the\nF1 score achieved by different prompts and the primary focus being on the comparison\nof the four models’ performance rather than prompt-specific variations. It would be\nbeneficial to conduct error comparison per prompt output, which might show if and\nhow each model’s generation had been affected by the prompt design.\nFinally, the few-shot NER task might benefit from a (self)-verification step (Li et al.,\n2024; Madaan et al., 2023), where either the same model or a different model “checks”\nthe errors classified aspossible candidatesby theannotator model and flags up valid\nentity candidates. In addition, the prompt may include an instruction for the LLM\nto not change the input text, which might help with cases where the model removes\nwhitespaces in the generated texts.\n7 Acknowledgements\nThe research presented in this paper was conducted within the research project In-\nsightsNet (https://insightsnet.org/), which is funded by the Federal Ministry of\nEducation and Research (BMBF) under grant no. 01UG2130A. The funder had no\nrole in study design, data collection and analysis, decision to publish, or preparation\nof the manuscript. The author would like to thank the anonymous reviewers for their\ninsightful feedback and comments.\n38 JLCL\nA Study of Errors for LLM Output in Domain-Specific Few-Shot NER\nReferences\nAbdelmageed, N., Löffler, F., Feddoul, L., Algergawy, A., Samuel, S., Gaikwad, J.,\n... König-Ries, B. (2022). Biodivnere: Gold standard corpora for named entity\nrecognition and relation extraction in the biodiversity domain.Biodiversity Data\nJournal, 10.\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F., ... others\n(2023). Gpt-4 technical report. arxiv.arXiv preprint arXiv:2303.08774.\nAshok, D., & Lipton, Z. C. (2023). Promptner: Prompting for named entity recogni-\ntion. ArXiv, abs/2305.15444. Retrieved fromhttps://api.semanticscholar.org/\nCorpusID:258887456\nBhattacharjee, B., Trivedi, A., Muraoka, M., Ramasubramanian, M., Udagawa, T.,\nGurung, I., ... others (2024). Indus: Effective and efficient language models for\nscientific applications.arXiv preprint arXiv:2405.10725.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... others\n(2020). Language models are few-shot learners.Advances in neural information\nprocessing systems, 33, 1877–1901.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... others (2024).\nScaling instruction-finetuned language models. Journal of Machine Learning\nResearch, 25(70), 1–53.\nEpure, E. V., & Hennequin, R. (2022, June). Probing pre-trained auto-regressive\nlanguage models for named entity typing and recognition. In N. Calzolari et al.\n(Eds.), Proceedings of the thirteenth language resources and evaluation conference\n(pp. 1408–1417). Marseille, France: European Language Resources Association.\nRetrieved fromhttps://aclanthology.org/2022.lrec-1.151\nInie, N., Druga, S., Zukerman, P., & Bender, E. M. (2024). From\" ai\" to probabilistic\nautomation: How does anthropomorphization of technical systems descriptions\ninfluence trust? In The 2024 acm conference on fairness, accountability, and\ntransparency(pp. 2322–2347).\nLi, Z., Xu, X., Shen, T., Xu, C., Gu, J.-C., Lai, Y., ... Ma, S. (2024). Leveraging\nlarge language models for nlg evaluation: Advances and challenges.arXiv preprint\narXiv:2401.07103.\nMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., ... others\n(2023). Self-refine: Iterative refinement with self-feedback.Advances in Neural\nInformation Processing Systems, 36, 46534–46594.\nMoscato, V., Postiglione, M., & Sperlí, G. (2023). Few-shot named entity recognition:\nDefinition, taxonomy and research directions.ACM Transactions on Intelligent\nSystems and Technology, 14(5), 1–46.\nReimers, N., & Gurevych, I. (2019, 11). Sentence-bert: Sentence embeddings using\nsiamese bert-networks. InProceedings of the 2019 conference on empirical meth-\nods in natural language processing.Association for Computational Linguistics.\nRetrieved fromhttps://arxiv.org/abs/1908.10084\nVolkanovska, E. (2025, March). Large language models as annotators of named\nJLCL 2025 – Band 38(2) 39\nVolkanovska\nentities in climate change and biodiversity: A preliminary study. In V. Basile,\nC. Bosco, F. Grasso, M. O. Ibrohim, M. Skeppstedt, & M. Stede (Eds.),Proceedings\nof the 1st workshop on ecology, environment, and natural language processing\n(nlp4ecology2025) (pp. 24–33). Tallinn, Estonia: University of Tartu Library.\nRetrieved fromhttps://aclanthology.org/2025.nlp4ecology-1.7/\nWang, S., Sun, X., Li, X., Ouyang, R., Wu, F., Zhang, T., ... Wang, G. (2023).\nGpt-ner: Named entity recognition via large language models.arXiv preprint\narXiv:2304.10428.\n40 JLCL\nA Study of Errors for LLM Output in Domain-Specific Few-Shot NER\nAppendix A: Prompt example\nThe prompt included here is motivated by the prompt used by Ashok and Lipton\n(2023). One major difference is that in this prompt, the LLM processes a task require-\nment comprised of natural language and Python code, and is instructed to generate\noutput as a Python list. The prompt in this Appendix contains three random task\nexamples from the dataset BiodivNER.\nFigure 1:Prompt example: Three randomly selected task examples (question-answer pairs) from\nBiodivNER’s training data.\nJLCL 2025 – Band 38(2) 41\nVolkanovska\nCorrespondence\nElena Volkanovska\nTechnische Universität Darmstadt\nInstitute of Linguistics and Literary Studies\nDarmstadt, Germany\nelena.volkanovska@tu-darmstadt.de\n42 JLCL",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.6380782127380371
    },
    {
      "name": "Computer science",
      "score": 0.6310520172119141
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6293388605117798
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5439940690994263
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5083994269371033
    },
    {
      "name": "Speech recognition",
      "score": 0.3761056065559387
    },
    {
      "name": "Mathematics",
      "score": 0.14666777849197388
    },
    {
      "name": "Chemistry",
      "score": 0.06804245710372925
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}