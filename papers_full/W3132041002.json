{
    "title": "Linear Transformers Are Secretly Fast Weight Memory Systems",
    "url": "https://openalex.org/W3132041002",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5019167028",
            "name": "Imanol Schlag",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5002810304",
            "name": "Kazuki Irie",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5071172037",
            "name": "Jürgen Schmidhuber",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W1525859397",
        "https://openalex.org/W2129217160",
        "https://openalex.org/W2952509486",
        "https://openalex.org/W3120633509",
        "https://openalex.org/W2128084896",
        "https://openalex.org/W2016760105",
        "https://openalex.org/W2883582441",
        "https://openalex.org/W185620388",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3102889924",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2950159625",
        "https://openalex.org/W2963168530",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3116353560",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2057529293",
        "https://openalex.org/W2089217417",
        "https://openalex.org/W2806311723",
        "https://openalex.org/W2964138017",
        "https://openalex.org/W2962940432",
        "https://openalex.org/W2024585065",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2535697732",
        "https://openalex.org/W3037798801",
        "https://openalex.org/W2013494846",
        "https://openalex.org/W2963921132",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2963285578",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W3129603602",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3113055895",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2970555085",
        "https://openalex.org/W3007776460",
        "https://openalex.org/W3097318237",
        "https://openalex.org/W2095425517"
    ],
    "abstract": "We show the formal equivalence of linearised self-attention mechanisms and fast weight memories from the early '90s. From this observation we infer a memory capacity limitation of recent linearised softmax attention variants. With finite memory, a desirable behaviour of fast weight memory models is to manipulate the contents of memory and dynamically interact with it. Inspired by previous work on fast weights, we propose to replace the update rule with an alternative rule yielding such behaviour. We also propose a new kernel function to linearise attention, balancing simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",
    "full_text": "Linear Transformers Are Secretly Fast Weight Programmers\nImanol Schlag∗1 Kazuki Irie∗1 J¨urgen Schmidhuber1\nAbstract\nWe show the formal equivalence of linearised self-\nattention mechanisms and fast weight controllers\nfrom the early ’90s, where a “slow” neural net\nlearns by gradient descent to program the “fast\nweights” of another net through sequences of ele-\nmentary programming instructions which are ad-\nditive outer products of self-invented activation\npatterns (today called keys and values). Such Fast\nWeight Programmers (FWPs) learn to manipulate\nthe contents of a ﬁnite memory and dynamically\ninteract with it. We infer a memory capacity limi-\ntation of recent linearised softmax attention vari-\nants, and replace the purely additive outer prod-\nucts by a delta rule-like programming instruction,\nsuch that the FWP can more easily learn to cor-\nrect the current mapping from keys to values. The\nFWP also learns to compute dynamically chang-\ning learning rates. We also propose a new kernel\nfunction to linearise attention which balances sim-\nplicity and effectiveness. We conduct experiments\non synthetic retrieval problems as well as standard\nmachine translation and language modelling tasks\nwhich demonstrate the beneﬁts of our methods.\n1. Introduction\nTransformers (Vaswani et al., 2017) have achieved impres-\nsive results in a myriad of sequence processing tasks, in-\ncluding machine translation, language modelling (Al-Rfou\net al., 2019; Dai et al., 2019; Baevski & Auli, 2019; Radford\net al., 2019), and question answering (Devlin et al., 2019),\ndomains previously dominated by recurrent neural networks\n(Graves, 2013; Bahdanau et al., 2015).\nThe core component of a Transformer is the self-attention\nmechanism (Cheng et al., 2016; Parikh et al., 2016; Lin et al.,\n2017) which was recently connected to the modern Hop-\n*Equal contribution 1The Swiss AI Lab IDSIA, USI &\nSUPSI. Correspondence to: Imanol Schlag <imanol@idsia.ch>,\nKazuki Irie <kazuki@idsia.ch>, J ¨urgen Schmidhuber <juer-\ngen@idsia.ch>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\nﬁeld network (Ramsauer et al., 2021; Krotov & Hopﬁeld,\n2016; Demircigil et al., 2017). It extends a form of attention\n(Bahdanau et al., 2015) originally introduced to complement\nrecurrent neural networks, e.g., (Hochreiter & Schmidhuber,\n1997). While relinquishing the recurrence property, all com-\nputations across the time axis can be parallelised. However,\nthis comes with drawbacks: self-attention computations\nscale quadratically with sequence length while the mem-\nory of the model grows linearly. Therefore, practitioners\nare forced to limit the context window to a reasonable size,\nwhich in turn makes it impossible to capture longer-term\ndependencies.\nRecent work proposed “linear Transformers” with constant\nsize memory and time complexity linear in sequence length\n(Katharopoulos et al., 2020; Choromanski et al., 2021; Peng\net al., 2021; Shen et al., 2018). This complexity reduction\nis mainly due to a linearisation of the softmax (reviewed in\nSec. 3.2).\nHere we emphasize the formal equivalence of this family of\nlinear Transformers and the Fast Weight Controllers or Fast\nWeight Programmers (FWPs) from the ’90s (Schmidhuber,\n1991; 1992; 1993; AI Blog, 2021) (apart from normalisa-\ntion). The memories of such FWPs contain key-value asso-\nciations, and an FWP can learn to reprogram them through\nsequences of differentiable elementary instructions (also\ncalled update rules), which are additive outer products be-\ntween keys and values invented by the FWP.\nThis view allows us to derive a limitation of the memory\ncapacity of linear Transformers and similar models. When\nthe sequence length exceeds storage capacity, the model\nmay end up in an overcapacity regime (discussed in depth\nin Sec. 4.1). To properly operate under such a regime, the\nmodel should learn to dynamically interact with the memory\ncontents and selectively decide which key-value associations\nto keep and which ones to delete. The purely additive in-\nstruction may be inappropriate for this purpose. Therefore,\ninspired by recent work on FWPs (Schlag et al., 2021), we\nintroduce an improved programming instruction akin to the\nfamous error-correcting delta-rule (Widrow & Hoff, 1960).\nFurthermore, softmax linearisation techniques for Trans-\nformers are still underexplored. The existing tech-\nniques are either very simplistic (Katharopoulos et al.,\n2020) or mathematically well explained but complex\narXiv:2102.11174v3  [cs.LG]  9 Jun 2021\nLinear Transformers Are Secretly Fast Weight Programmers\n(Choromanski et al., 2021; Peng et al., 2021). We provide\na comprehensive comparison and propose a new method\nwhich is both simple and effective.\nWe demonstrate the beneﬁts of the proposed methods on\nour own synthetic retrieval dataset (Sec. 6.1), the stan-\ndard WMT14 English to German machine translation task\n(Sec. 6.2), and the Wikitext-103 (Merity et al., 2017) lan-\nguage modelling task (Sec. 6.3)2.\n2. Background on Fast Weight Programmers\nHere we review the concepts of Fast Weight Programmers\n(FWPs) before relating them to linear Transformer variants\nin Sec. 3.\nIn standard neural networks, the weights remain ﬁxed after\ntraining, unlike the activations, which change depending on\nthe inputs at test time. The general idea of fast weights is to\nmake the weights also variable and input-dependent. This\nconcept was called synaptic modulation (von der Malsburg,\n1981), a method for variable binding in neural networks (see\ne.g. the recent survey by Greff et al. (2020)), or dynamic\nconnections (Feldman, 1982). V on der Malsburg deﬁnes\nthe effective weights as a (multiplicative) superposition of\nconventional, context-independent slow weights, and fast\nchanging, context-dependent fast weights. Hinton & Plaut\n(1987) studied a net with (additive) superposition of two sets\nof weights with two different learning rates in a scenario\nof model retraining. Before 1991, however, no network\nlearned by gradient descent to quickly compute the changes\nof the fast weight storage of another network or of itself.\nContext-dependent FWPs were introduced in two-network\nsystems of the early ’90s (Schmidhuber, 1991; 1992; 1993;\nAI Blog, 2021). A traditional slow net with slow weights\ncontinually changes or reprograms the fast weights of a fast\nnet, making the fast weights effectively dependent on the\nspatio-temporal context of a given input stream. Simply\nput, the slow net learns to program its fast net. Among\nthe proposed elementary differentiable instructions that the\nslow net can use to program the fast weights, a particularly\nattractive one makes use of outer products (Schmidhuber,\n1991; 1992): for a sequential input {x(i)}L\ni=1,x(i) ∈Rdin ,\nthe model outputs the sequence {y(i)}L\ni=1,y(i) ∈Rdout as\na(i),b(i) = Wax(i),Wbx(i) (1)\nW(i) = σ\n(\nW(i−1) + a(i) ⊗b(i))\n(2)\ny(i) = W(i)x(i) (3)\nwhere ⊗denotes the outer product, σis an activation func-\ntion, Wa and Wb are trainable slow weights, while the fast\nweights W(i) are generated at each time step iand serve\n2Source code used in this paper is available at\ngithub.com/ischlag/fast-weight-transformers.\nas a short-term memory. This is a key-value associative\nmemory model in which the write operation is based on\na summation (Eq. 2) and the retrieval is a matrix-vector\nmultiplication (Eq. 3). Schmidhuber (1993) describes a\nrecurrent version and discusses “internal spotlights of atten-\ntion” (such attention terminology is now widely used in the\ncontext of transformers). The use of outer products results\nin a model of associations similar to tensor product pre-\nsentations (Smolensky, 1990). In fact, outer-product based\nassociative memory can be found in numerous works since\nHebb’s informal rule (Hebb, 1949) and its more concrete\nformal variants (Steinbuch, 1961; Steinbuch & Piske, 1963;\nKohonen, 1972; Palm, 1980) including Hopﬁeld networks\n(Hopﬁeld, 1982; Little, 1974) and bi-directional associa-\ntive nets (Kosko, 1988). However, these authors described\npre-wired rules to associate given patterns with each other.\nTheir systems did not learn to use such rules for associating\nself-invented patterns like the FWPs since 1991.\nThe concept of FWPs has been revisited recently (Ba et al.,\n2016; Schlag & Schmidhuber, 2017), also under different\nnames, e.g., hypernetworks (Ha et al., 2017; Perez et al.,\n2018; Galanti & Wolf, 2020), dynamic plasticity (Miconi\net al., 2018; 2019), dynamic convolution (Klein et al., 2015;\nNoh et al., 2016; Jia et al., 2016), or lambda networks\n(Bello, 2021) used for applications including meta-learning\n(Munkhdalai & Yu, 2017; Munkhdalai & Trischler, 2018;\nMunkhdalai et al., 2019; Kirsch & Schmidhuber, 2020).\nFWPs recently also improved memory models through ex-\nplicit mechanisms for facilitating the replacement of dep-\nrecated information and updating associations (Schlag &\nSchmidhuber, 2018; Schlag et al., 2021).\n3. Relation to Transformers\nBa et al. (2016) have already pointed out a relation between\na variant of outer product-based FWPs (Schmidhuber, 1993)\nand attention (Bahdanau et al., 2015). Katharopoulos et al.\n(2020) have analysed linearised transformers. We review\nthese derivations, emphasising the relation between Trans-\nformers and the FWPs of the previous section.\n3.1. Self-Attention Without Softmax Is a Fast Weight\nProgrammer\nA self-attention layer in auto-regressive Transform-\ners (Vaswani et al., 2017) maps an input sequence\n{x(i)}L\ni=1,x(i) ∈ Rd×1 to an output sequence\n{y(i)}L\ni=1,y(i) ∈Rdvalue×1 as\nk(i),v(i),q(i) = Wkx(i),Wvx(i),Wqx(i) (4)\nK(i) =\n[\nK(i−1),k(i)] ∈Rdkey×i (5)\nV (i) =\n[\nV (i−1),v(i)] ∈Rdvalue×i (6)\ny(i) = V (i)softmax((K(i))⊤q(i)) (7)\nLinear Transformers Are Secretly Fast Weight Programmers\nwhere [A,a] denotes the concatenation of vectora to matrix\nA along the time dimension, softmax is applied along the\ntime dimension, and Wk, Wv, Wq are trainable weight ma-\ntrices. We omit the scaling by 1/\n√\ndkey inside the softmax\nwithout loss of generality.\nNow if we remove the softmax in Eq. 7 we obtain:\ny(i) = V (i)(\n(K(i))⊤q(i))\n=\n(\nV (i)(K(i))⊤)\nq(i)\n=\n( i∑\nj=1\nv(j) ⊗k(j))\nq(i) (8)\nDenoting by W(i) the corresponding weight matrix gener-\nated from key and value vectors:\nW(i) =\n( i∑\nj=1\nv(j) ⊗k(j)) (9)\nwe can rewrite Eqs. 4-7 such that they directly relate to\nEqs. 1-3 where the activation function σis the identity func-\ntion and without query projection Wq:\nk(i),v(i),q(i) = Wkx(i),Wvx(i),Wqx(i) (4)\nW(i) = W(i−1) + v(i) ⊗k(i) (10)\ny(i) = W(i)q(i) (11)\n3.2. Linearising Self-Attention\nInstead of removing the softmax as in Sec. 3.1, prior works\nhave introduced techniques for linearising the softmax (Tsai\net al., 2019), which has been shown to improve com-\nputational efﬁciency of self-attention for long sequences\n(Katharopoulos et al., 2020; Choromanski et al., 2021; Peng\net al., 2021).\nBy writing the softmax explicitly, Eq. 7 can be written as:\ny(i) =\ni∑\nj=1\nv(j)κ(k(j),q(i))∑i\nj′=1 κ(k(j′),q(i))\n(12)\nwhere κ(k,q) = exp(k ·q) ∈R>0 is the softmax kernel\nand k ·q = k⊤q is the vector dot product.\nThe general idea is to replace the softmax kernel κby an-\nother kernel: κ′(k,q) = φ(k)⊤φ(q) where φis a function\nRdkey →Rddot . We discuss the necessary properties of φin\nSec. 5.1. By replacing κin Eq. 12 by κ′, we obtain\ny(i) =\ni∑\nj=1\nv(j)φ(k(j))⊤φ(q(i))∑i\nj′=1 φ(k(j′)) ·φ(q(i))\n(13)\n=\n∑i\nj=1\n(\nv(j)φ(k(j))⊤)\nφ(q(i))\n(∑i\nj′=1 φ(k(j′))\n)\n·φ(q(i))\n(14)\nUsing the outer-product notation, the numerator is analo-\ngous to the case without softmax (Sec. 3.1):\ni∑\nj=1\n(\nv(j)φ(k(j))⊤)\nφ(q(i)) =\n( i∑\nj=1\nv(j) ⊗φ(k(j))\n)\nφ(q(i))\nBy introducing the fast weight matrix W(i) and an addi-\ntional vector z(i) for the denominator,\nW(i) =\ni∑\nj=1\nv(j) ⊗φ(k(j)) (15)\nz(i) =\ni∑\nj=1\nφ(k(j)) (16)\nforward computations of linear Transformers can be written\nas (Katharopoulos et al., 2020):\nk(i),v(i),q(i) = Wkx(i),Wvx(i),Wqx(i) (4)\nW(i) = W(i−1) + v(i) ⊗φ(k(i)) (17)\nz(i) = z(i−1) + φ(k(i)) (18)\ny(i) = 1\nz(i) ·φ(q(i))W(i)φ(q(i)) (19)\nwhich is a Fast Weight Programmer (Sec. 2) with normali-\nsation. Hence, the core of linear Transformer variants are\nouter product-based Fast Weight Programmers.\n4. Analysing and Improving Linear\nTransformers as Fast Weight Programmers\nViewing linear Transformer variants as Fast Weight Pro-\ngrammers provides us with two insights which we investi-\ngate in this work: their capacity limits as associative memo-\nries (Sec. 4.1), and their ineptness to edit previously stored\nassociations (Sec. 4.2).\n4.1. Capacity Limitation\nIntuition. Endlessly adding new associations to a memory\nof ﬁnite size, as in Eq. 17, inevitably will reach a limit. In\nlinear attention, information is stored in a matrix and is\nretrieved using matrix multiplication (see Eq. 19). As a\nconsequence, to prevent associations from interfering with\neach other upon retrieval, the respective keys need to be\northogonal. Otherwise, the dot product will attend to more\nthan one key and return a linear combination of values.\nWith keys embedded in a ddot space, there cannot be more\nthan ddot orthogonal vectors. That is, storing more than\nddot associations will result in a retrieval error. In linear\nTransformers, when the length of the sequence is longer than\nddot, the model might be in such an overcapacity regime.\nWhile we experimentally demonstrate this effect on toy tasks\n(Sec. 6.1), prior work on tensor product representations\nallows for a more formal discussion.\nLinear Transformers Are Secretly Fast Weight Programmers\nTensor Product Representation Theory.Early work in\nconnectionist research investigated the usage of distributed\nrepresentations as a means for storing symbolic structures.\nOne highly-inﬂuential work is the tensor-product-based vari-\nable binding mechanism (Smolensky, 1990). A tensor prod-\nuct representation (TPR) of a structured symbolic system\nconsisting of a set of variables and values constructed from\nouter products of the so called role and ﬁller vectors. These\nterms directly translate into keys and values in our context.\nThe fast weight memories of Eq. 17 are the most basic form\nof such representations (second order tensors). Therefore,\nmany results discussed in Smolensky’s work transfer to our\nmodel. In particular, Theorem 3.3 and 3.1 of Smolensky\n(1990) discuss more formally the crosstalk and retrieval\nerror intuitively described in the previous paragraph.\nHowever, we also note an important difference: the classic\nTPRs of Smolensky (1990) are constructed with a priori\nknowledge of the symbolic structure. In contrast, our FWPs\nsince 1991, including recent FWPs (Schlag & Schmidhuber,\n2018), learn all the vectors involved in constructing such a\nrepresentation.\n4.2. Improving the FWP’s Programming Instruction\nSec. 4.1 argues that the linear Transformers can end up in\nan overcapacity regime, if the sequence length Lexceeds\nthe dimension ddot of the keys. Once in overcapacity, an\nideal memory model should dynamically interact with the\nmemory contents and selectively determine which associa-\ntions to remember or to forget. This is in stark contrast to\nthe standard Transformer which stores immutable pairs of\nkey and value vectors by concatenation, thus increasing the\nstorage size. While such models work well in practice, we\nconsider a model’s capability to update previously acquired\nknowledge to be critical for many problems. Hence, from\nthe perspective of dynamic interaction with the memory, the\npurely additive update rule of Eqs. 17 may be sub-optimal.\nThis motivates us to improve the elementary differentiable\nprogramming instruction (i.e. the update rule) of FWPs.\nInspired by the recent work by Schlag et al. (2021), we\npropose a basic instruction that essentially implements the\nfamous error-correcting delta rule (Widrow & Hoff, 1960)\nin an end-to-end differentiable way, such that the FWP\ncan learn to use it wisely, through self-invented, dynami-\ncally changing learning rates. Given a new input key-value\npair (k(i),v(i)), the FWP ﬁrst accesses the current state of\nthe memory W(i−1) and retrieves the value ¯v(i) currently\npaired with the key k(i). Then the model stores a convex\ncombination v(i)\nnew of the retrieved value ¯v(i) and the input\nv(i) using an interpolation weight 0 ≤β(i) ≤1 also gener-\nated by the model. The model thus sequentially transforms\nan input sequence {x(i)}L\ni=1,x(i) ∈Rd×1 into an output\nsequence {y(i)}L\ni=1,y(i) ∈Rdvalue×1 as:\nk(i),v(i),q(i) = Wkx(i),Wvx(i),Wqx(i) (4)\n¯v(i) = W(i−1)φ(k(i)) (20)\nβ(i) = σ(Wβx(i)) (21)\nv(i)\nnew = β(i)v(i) + (1 −β(i))¯v(i) (22)\nwhere Wβ ∈R1×d, and σ is the sigmoid function. The\ninterpolation weight β(i) is the “write-strength” as it deﬁnes\nto which extent the new value will replace the previous value.\nWe note that while β(i) only depends on x(i), in a multi-\nlayer model, x(i) has the full context information except in\nthe ﬁrst layer. We set W(0) = 0 and z(0) = 0. Then the\nfast weight update rule and the ﬁnal output y(i) are deﬁned\nas follows (see Appendix A.1 for detailed derivations):\nW(i) = W(i−1) +v(i)\nnew ⊗φ(k(i))  \nwrite\n−¯v(i) ⊗φ(k(i))  \nremove\n(23)\n= W(i−1) + β(i)(v(i) −¯v(i)) ⊗φ(k(i)) (24)\ny(i) = W(i)φ(q(i)) (25)\nAs shown in Eq. 24, our programming instruction or up-\ndate rule is effectively a delta rule with a dynamic learning\nrate β(i). The model thus learns to correct the current key\nto value association. In Appendix B, we formally show\nthe advantage of this approach over the gated update rule\nconcurrently proposed by Peng et al. (2021).\nNormalisation. In the equations above, no normalisation\nis applied to the value we retrieve. A straightforward nor-\nmalisation can be obtained by following the derivation in\nSec. 3.2, i.e. by introducing an accumulator:\nz(i) = z(i−1) + φ(k(i)) (26)\nand replacing Eqs. 20 and 25 respectively by:\n¯v(i) = W(i−1)φ(k(i))\nz(i−1) ·φ(k(i)) (27)\ny(i) = W(i)φ(q(i))\nz(i) ·φ(q(i)) (28)\nwhere we deﬁne ¯v(1) = 0. In this approach, the output y(i)\nis a weighted average of β(j)(v(j) −¯v(j)) for 1 ≤j ≤i.\nWe refer to this approach as attention normalisation.\nThis approach, however, has drawbacks. First, the accu-\nmulation of positive values in Eq. 26 always grows with\nthe number of steps, and may result in instability. Second,\nspeciﬁcally for our update rule, this normalisation is not\nsufﬁcient to balance the weights between write and remove\noperations in Eq. 23 (see derivations in Appendix A.2). Here\nLinear Transformers Are Secretly Fast Weight Programmers\nwe propose a better approach based on simple normalisation.\nWe divide the effective key and query vectorsφ(k(i)) and\nφ(q(i)) by the sum of its components, e.g., for the query:\nφ′(q(i)) = φ(q(i))\nddot∑\nj=1\nφ(q(i))j\n(29)\nbefore applying Eqs. 20-25. A general consequence of this\nnormalisation is intuitively understood by noticing that the\noutput of any matrix-vector operations (like Eq. 25) is a\nweighted sum of columns of the matrix where weights are\nthe components of the vector; thus, if the vector components\nsum up to one, the operation can be viewed as an attention\nover the columns of the matrix. We provide further expla-\nnations and precise implications for our FWP in Appendix\nA.2. We refer to this approach as sum normalisation.\nSince this is a simple substitution of φ(k(i)) and φ(q(i)) in\nEqs. 20-25, one might still ask whether additional attention\nnormalisation is needed. In language modelling experiments\n(Sec. 6.3), we show that this is not the case.\n5. Linear Attention Functions\nThe central component of softmax linearisation (Sec. 3.2)\nis the φfunction which maps key and query vectors to the\nspace where the dot product is executed: Rdkey →Rddot . We\nﬁrst list desirable properties of such a function, and review\nthe existing φfunctions from the perspective of fast weight\nmemories. Finally, we also propose our own φfunction.\n5.1. Properties\nFor Eq. 13 to deﬁne proper attention weights between 0 and\n1, the codomain of φshould be positive. Another property\nof φ derives from the discussion of memory capacity in\nSec. 4.1. The dimensionality of its codomain ddot deﬁnes\nthe model’s capacity. Therefore, by including a transfor-\nmation which projects the input dimension dkey to a larger\ndimension ddot, the φfunction can potentially increase the\nupper bound of the capacity.\n5.2. Katharopoulos’ Linear Attention\nKatharopoulos et al. (2020) propose to use the simple\nelement-wise ELU + 1function (Clevert et al., 2016):\nφ(x) = ELU(x) + 1 =\n{\nx+ 1, if x> 0\nexp(x), if x≤0 (30)\nThe choice of ELU over ReLU is motivated by non-zero\ngradients on the negative part. Importantly, as a simple\nelement-wise function, this φfunction preserves the dimen-\nsion of the input key vector (dkey = ddot), without modifying\nthe memory capacity as discussed in Sec. 4.1.\n5.3. FA VOR+\nIn contrast to Katharopoulos et al. (2020)’sφfunction which\nmerely satisﬁes positivity (and a good gradient) property,\nChoromanski et al. (2021) propose a mathematically rigor-\nous method to approximate the softmax with random fea-\ntures. They propose the following φfunction:\nh(x) = 1√\n2 exp(−||x||2\n2 ) (31)\nφ(x) = h(x)√m\n[exp(Rx)\nexp(−Rx)\n]\n(32)\nwhere the concatenation\n[a\nb\n]\nof two vectorsa and b is along\nthe feature dimension, and R ∈Rm×dkey is a matrix with\nmrandom features where each row vector r ∈R1×dkey is\ndrawn from N(0,Idkey ). A similar approach is also proposed\nby Peng et al. (2021).\nWith FA VOR+, the dimension of the codomainddot is 2m\nwhich increases the theoretical capacity of the memory if\n2m>d key. At the same time, the model’s capacity is still\nlimited, and equals the inﬁnite capacity of the softmax mem-\nory only when mgoes to inﬁnity, which is never achieved\nin practice. During training, we redraw these mrandom\nvectors for each mini-batch. During evaluation, we draw\na set of mrandom vectors once, and keep them ﬁxed. m\nis the only hyperparameter of FA VOR+ and inﬂuences the\nquality of the softmax approximation. Choromanski et al.\n(2021) suggest to choose min the order of dkey log(dkey).\nThis sampling process is the main drawback of FA VOR+ as\nit introduces variance into the model’s output.\n5.4. Deterministic Parameter-Free Projection (DPFP)\nThe two previous sub-sections highlight the sub-optimality\nof the existing φfunctions. Sampling introduces extra com-\nplexity to FA VOR+ (Sec. 5.3), while the Linear Transformer\n(Sec. 5.2) lacks the ability to project up the dot product\ndimension. Here we propose an alternative approach called\ndeterministic parameter-free projection (DPFP). It is de-\nterministic and easy to compute like Linear Transformers\nwhile increasing the dot product dimension without requir-\ning FA VOR+’s random features.\nWe begin with a low-dimensional example to foster an in-\ntuitive understanding before moving on to the general for-\nmulation. Consider 4 keys k(i),i ∈{1,2,3,4}in R2 and\nφ : R2 →R4\n≥0 where the l-th element of φ(x) is gener-\nated by the partial function φl : R2 →R≥0. We design φ\nsuch that it facilitates orthogonality in the projected space,\ni.e. φ(k(i)) ·φ(k(j)) = 0 for i ̸= j. Towards this end,\nwe construct φsuch that if φl(x) >0 then φn(x) = 0 for\nall n ̸= l. Such a constraint can be enforced by limiting\nthe domains of the partial functions to be non-overlapping.\nLinear Transformers Are Secretly Fast Weight Programmers\nWith the element-wise rectiﬁer function r(a) = max(0,a)\nthe partial functions are deﬁned as:\nφ1(k) = r(k1)r(k2) (33)\nφ2(k) = r(−k1)r(k2) (34)\nφ3(k) = r(k1)r(−k2) (35)\nφ4(k) = r(−k1)r(−k2) (36)\nFigure 1 illustrates this function. The elements of the 4-\ndimensional space are displayed as the zcomponent of the\nfour coloured surfaces. The ﬁgure shows how each vector\nin the 2d plane will have a single non-zero component in the\n4d space and equally splits the input space into four areas\nwhich will be orthogonal in the projected space.\nFigure 1.A visualisation of a DPFP from a 2d space (the xy-plane)\nto a 4d space (the four colored surfaces). Each surface is a partial\nfunction which represents one element of the 4d vector.\nWe generalise this method to higher dimensional inputs by\nconstructing additional two-factor features. Given an input\nvector k ∈Rdkey and i∈[1,2dkey], the partial function\nφiν(k) = r(\n[k\n−k\n]\n)ir(\n[k\n−k\n]\n)i+ν (37)\nwhere ν ∈ {1,2,..,d key2 −1}is a capacity controlling\nhyperparameter. The codomain dimensionality of φ(k) is\nthus ddot = 2dkeyν. Eq. 37 is highly parallelisable because\neach partial function can be computed independently. This\ncan be implemented in few lines of code as we show in\nAppendix C.\nFinally we note that Choromanski et al. (2021) empirically\nshow that replacing exp in Eq. 32 by ReLU typically im-\nproves model performance. While this result has not been\ntheoretically justiﬁed, it supports the design of our DPFP\nwhich aims for sparsity and orthogonality.\n6. Experimental Results\nNow we present our experimental results on synthetic re-\ntrieval problems (Sec. 6.1.1 and 6.1.2), machine translation\n(Sec. 6.2), and language modelling (Sec. 6.3).\n6.1. Synthetic Settings\nWe illustrate the capacity issue (Sec. 4.1) of linear attention\nand the effectiveness of our new update rule (Sec. 4.2) on\ntwo synthetic problems.\nIn both settings, our toy problem consists of retrieving the\ncorrect value from a sequence of randomly sampled key-\nvalue associations when queried with one of the used keys.\nCrucially, the query is given at the end of the sequence, such\nthat the model is not aware of it while processing the inputs.\nTo succeed, the model has to learn to store the observed\nassociations in its memory without interference.\nLet Kand Vbe the ﬁnite and ﬁxed sets of keys and values\nand S = |K|= |V|. Then, the input to the model is the\nsequence [(k,v)1,..., (k,v)L] followed by q where every\npair (k,v) ∈K×V is sampled randomly, and q is randomly\nchosen to be one of the Lkeys.\nEach value v(i),i ∈[1,..,S ] is assigned a ﬁxed one-hot\nvector v(i) ∈RS. Hence, the set of value vectors is an\northonormal basis. In contrast, the vector embedding of\nthe key symbols is the learned function e: K→ Rdemb and\nk = WK[e(k); v] where WK ∈Rdkey×(demb+S).\nFollowing the L write operations, the read function and\nthe query vector q = WQe(q),WQ ∈Rdkey×demb are used\nto retrieve ˆv ∈RS from memory. Finally, the loss is de-\nﬁned as l(ˆv,v∗) = ∑S\nj\n1\n2 (v∗\nj −ˆvj)2 where v∗is the value\nvector assigned to q in the input sequence. Each model\nis trained in mini-batches using this loss and Adam with\ndefault hyperparameters unless stated otherwise. For evalu-\nation, we sample 20 sequences and test all possible queries,\ne.g., with S = 100 unique keys, the evaluation batch is of\nsize 100 ∗20 = 2000.\n6.1.1. S ETTING 1: T ESTING CAPACITY\nIn this setting, we experimentally demonstrate the capacity\nlimit of linear attention (Sec. 4.1). We conduct experiments\nfor the various φfunctions described in Sec. 5. We ﬁx dkey\nto be 64, while different φfunctions produce different ddot.\nWe set the sequence length to be equal to the number of\nunique keys (L= S), and sample the keys and values with-\nout replacement to generate the sequences. By varying the\nsequence length S, our goal is to show that all linear atten-\ntion models (using the simple sum update rule of Sec. 3.2)\nfail at retrieving when Sexceeds ddot.\nAll models are trained with a mini-batch size of 32 until the\nevaluation loss falls below0.001 or until lack of progress for\n1000 steps. In Figure 2, the best validation set performance\nfor each model and each S is displayed (for the learning\ncurves see Appendix D.1). The number of unique keys\nis initially S = 20 and is incremented by 20 until S =\n600. The following models are compared: Softmax, Linear-\nLinear Transformers Are Secretly Fast Weight Programmers\nAttention, FA VOR+ with 64, 128, and 512 random features,\nDPFP-νwith ν ∈{1,2,3}.\nFigure 2.Final evaluation loss of the softmax memory and various\nlinear attention mechanisms on associative retrieval problems with\nthe total number of unique associations ranging from 20 to 600.\nEach individual symbol is a model trained until convergence.\nThe results support our theoretical analysis. Linear-\nAttention has a capacity of 64 due to the choice of dkey =\nddot = 64. Experimentally, Linear-Attention begins to ac-\ncumulate errors with 60 or more associations. Similarly,\nDPFP projections 1, 2 and 3 start to accumulate errors as\nthey approach their respective limits at 128, 256, and 384.\nFA VOR+, on the other hand, fails to achieve a loss of 0 in\nany experiment. Finally, as expected, softmax attention is\noutperforming all φfunctions, although it struggles to fully\nconverge with more than 500 keys.\n6.1.2. S ETTING 2: C OMPARING UPDATE RULES\nIn the second setting, we compare variations of the update\nrule. Unlike in setting 1, keys and values will be sampled\nwith replacement and sequence length L= 2S. As a result,\nin the same sequence, multiple keys can be re-assigned to a\nnew value more than once. The expected value to retrieve\nis the most recent one associated with the query. With\nevery new key, the previous value associated with this key\ndeprecates and the model is required to update its ﬁnite size\nmemory. The ability to update values associated with keys\nis essential to bind context-speciﬁc values to a key.\nWe use DPFP-1 as the φ function. The sequence length\nis ﬁxed at 40 with 20 unique keys and values. While this\nsetting does not exceed the capacity of DPFP-1, our result is\nindependent of the capacity regime (see results for different\nSand φin Appendix D.2).\nWe compare the proposed fast weight memory programming\ninstruction with normalisation of Sec. 4.2 (denoted here by\nours) to three baselines: the sum update rule of Sec. 3 (sum\nrule), and two variants of previous update rules (Schlag\net al., 2021): Schlag (2021) and Schlag (2021) with DPFP.\nSchlag (2021) is simply the model from Schlag et al. (2021)\nported to this setting (i.e. without the LSTM layer). Schlag\n(2021) has neither a φfunction, nor the sum normalisation\nterm of Sec. 4.2. Instead it uses a tanh nonlinearity for its\nkey representations. As an ablation we replace it with our\nDPFP-1 but we don’t use the normalisation term of Sec. 4.2,\nwhich we refer to as Schlag (2021) with DPFP.\nFigure 3 presents the learning curves. They demonstrate\nthat our new update rule outperforms all other variants. As\nexpected, the baseline sum update rule fails.\nFigure 3.Learning curves for different update rules. Sequence\nlength of 40 and 20 unique keys/values sampled with replacement.\n6.2. Machine Translation Experiments\nHere we compare φfunctions on the standard machine trans-\nlation task. We compare Linear Transformer (Katharopou-\nlos et al., 2020), Performer (Choromanski et al., 2021) and\nour φfunction DPFP (Sec. 5.4) to the regular Transformer,\ncomplementing prior comparisons, e.g., Tay et al. (2021).\nWe use the standard WMT14 English to German Translation\ndataset and standard data setups (Ott et al., 2018; Vaswani\net al., 2017). We adapt the recipe of Ott et al. (2019) (see\nAppendix E) and train Vaswani et al. (2017)’s “big” models\nfor about 4 days on three V100 GPUs. We use the exact\nsame training conﬁgurations for all models without model-\nspeciﬁc hyper-parameter tuning. We only vary the model\nhyper-parameters min Performers and νin DPFP models.\nTable 1 shows the BLEU score (Papineni et al., 2002; Post,\n2018) results. The Performer is as good as the basic Trans-\nformer when the number of samples mis large enough (for\nddot = 512, we have m = 256). In fact, with dkey = 64,\nthe recommended value for mis ddot log(ddot) = 266. Our\nDPFP model outperforms the Linear Transformer as well\nas the Performer when ddot is relatively small; providing a\ngood trade-off between simplicity and performance.\nLinear Transformers Are Secretly Fast Weight Programmers\nTable 1.WMT14 En-De Translation BLEU scores for various\nTransformer models. Neither model averaging, nor model spe-\nciﬁc tuning is done. Standard denotes the basic Transformer.\nValid Test\nddot 64 256 512 64 256 512\nStandard 26.6 - - 27.7 - -\nLinear 25.5 - - 26.8 - -\nPerformer 24.2 24.9 26.7 24.4 25.3 27.7\nDPFP (ours) - 26.2 26.2 - 26.9 27.1\n6.3. Language Modelling Experiments\nToy experimental Setting 2 (Sec. 6.1.2) illustrated the effect\nof our update rule. Now our goal is to conﬁrm its effective-\nness on a large-vocabulary word-level language modelling\ntask, and investigate its further potential.\nExperimental setups. Our update rule should be evalu-\nated on a dataset with sufﬁciently long contextual depen-\ndencies. We use the standard WikiText-103 (Merity et al.,\n2017) dataset. WikiText-103 consists of long articles from\nWikipedia; the training set contains about 28 K articles with\na total of 103 M running words. This results in contextual\ntext blocks of about 3600 words. The validation and test\nsets also contain similarly long dependencies, respectively\nwith 218 K and 246 K running words for 60 articles each.\nThe vocabulary size is about 268 K words.\nWe split the training data intoL-word long segments (which\nis the backpropagation span). Unless stated otherwise, we\ntreat these segments independently during training. For eval-\nuation, we use a batch size of one, and go through the text\nwith a sliding window of size L, taking into account only\nthe last position for computing perplexity (except in the ﬁrst\nsegment where all positions are evaluated). This is usually\ndone for Transformers with a limited context (Al-Rfou et al.,\n2019). Appendix F provides further experimental details.\nEffectiveness of our new update rule.We ﬁrst evaluate\nour update rule in two conﬁgurations. In the small conﬁg-\nuration, we set the model dimension (same for key, value,\nand query) Dto 128, and the training and evaluation context\nlength Lto 256. We note that D = H ∗ddot where H is\nthe number of heads. H is set to 8. The feed-forward layer\ndimension is 2048. The number of layers is 16 in all conﬁg-\nurations. In the medium conﬁguration, we set D= 256 and\nL = 384. Both conﬁgurations represent an overcapacity\nregime. We evaluate both Linear Transformers (Katharopou-\nlos et al., 2020) and Performers (Choromanski et al., 2021).\nHowever, to keep the comparison simple, we set the ca-\npacity of Performers (Sec. 5.3) equal to the one of linear\nTransformers, by the right choice of projection dimension\n(m = 8 and m = 16, respectively, in small and medium\nTable 2.WikiText-103 language model perplexity results showing\neffects of our update rule. The number of trainable parameters\nare almost the same for all models, up to the small difference\nintroduced by gating in our update rule (16 K and 33 K parameters\nrespectively for the small and medium conﬁgurations). We have\nD= 128, L= 256(40 M parameters) in thesmall, and D= 256,\nL = 384 (90 M parameters) in the medium conﬁguration. For\nPerformers, mis 8 and 16, respectively.\nUpdate small medium\nRule Valid Test Valid Test\nTransformer - 33.0 34.1 27.9 29.6\nLinear Transformer sum 37.1 38.3 31.1 33.0\nDelta Network delta 34.1 35.5 29.7 31.5\nPerformer sum 39.0 39.6 32.2 33.8\ndelta 36.1 37.2 30.0 31.8\nconﬁgurations), even though this limits performance. We\ndo not include DPFP here, since in both conﬁgurations even\nthe smallest value for νprovides enough capacity. Here we\ninvestigate the effect of the update rule in an overcapacity\nscenario (see Appendix D.3 for experimental results in a\nnon-overcapacity regime including DPFP). All models can\nbe trained using two V100 GPUs in less than four days. We\nrefer to the Linear Transformer with our delta update rule\nas a Delta Network. Table 2 shows the perplexity results.\nIn both conﬁgurations, our update rule provides convincing\nimprovements over the models with the sum update rule.\nWe also conduct an ablation study to test the effect of the\nabsolute positional encoding and an extra attention normal-\nisation (Sec. 4.2). Table 3 shows the results. The sum\nnormalisation (Sec. 4.2) is used in all cases: the models\ndiverged otherwise. In contrast, better perplexities are ob-\ntained when no additional attention normalisation is applied.\nWe also observe that the absolute positional encoding is not\nneeded, conﬁrming results of prior work (Irie et al., 2019a).\nTable 3. WikiText-103 language model perplexities for Linear\nTransformers (medium conﬁguration) with our update rule.\nPosition Encoding Attn. Normalisation Valid Test\nYes Yes 30.4 32.1\nNo Yes 29.2 31.2\nYes No 29.7 31.5\nNo No 28.1 31.1\nComplexity, wall clock time, memory.All methods we\npropose are within the framework of “linear Transform-\ners”. Thus, there is no change to be discussed in terms of\ncomplexity which is constant in space and linear in time\nw.r.t. sequence length. However, our modiﬁed update rule in-\nLinear Transformers Are Secretly Fast Weight Programmers\nTable 4. WikiText-103 language model perplexities when the\nmodel is trained and evaluated without truncating context, as\nopposed to Table 2 where the context window is limited. The\nmedium conﬁg is used. Neither positional encoding nor attention\nnormalisation is used for the Delta Net. The numbers of trainable\nparameters (Prms.) are given in millions. We compare with the\nTransformer-XL at different memory segment lengths. This re-\nsults in different state sizes which are proportional to the memory\nrequirements during evaluation, and highlights the memory efﬁ-\nciency of the Delta Network. The state sizes are given in millions.\nModel Prms. State size Perplexity\nin M. in M. Valid Test\nLinear Transformer 89.8 0.13 >260 >260\nDelta Network 89.9 0.13 27.8 29.4\nTransformer-XL 90.9 0.13 65.7 65.5\n1.05 29.3 30.1\n2.10 26.4 27.4\n6.29 24.6 25.5\ntroduces a few extra computations. The wall clock time and\nmemory requirement (for the small LM setting) for the Lin-\near Transformer with and without our delta update rule are:\n63 K and 66 K words/sec, and 14 and 13 GB respectively in\nour implementation. The extra resource requirement is thus\nmarginal. As we use custom CUDA kernels for these linear\nTransformers, they are faster than the regular Transformers\nimplemented in PyTorch which process 33K words/sec and\nrequire 17 GB memory. The speed of the DPFP and Per-\nformer models (for Table 5 in Appendix with a larger ddot)\nare 63 K and 57 K words/sec. Performers are slower because\nof the sampling logic, which also motivates our DPFP.\nWithout truncating context. Given the constant space\nrequirements, we can feed inputs to linear Transformers\nfor an arbitrary number of steps. To properly assess the\nmodel’s ability to process arbitrary long sequences, it is\ncrucial to make the training consistent with the evaluation\nmode (Irie et al., 2019b). During training, we carry over\nthe fast weight memory from one training segment to the\nfollowing one, while still limiting the backpropagation span\nto be within the segment. We train a Delta Net, using neither\npositional encoding nor attention normalisation (the best\nsetting from Table 3). It was crucial to remove the attention\nnormalisation for the Delta Net since the accumulator blows\nup as indicated in Sec. 4.2, while for the Linear Transformer,\nremoving it resulted in an even worse perplexity of over\n1600. Table 4 shows the corresponding results. The Delta\nNet yields a slight improvement over the best model with a\nlimited context window (Table 3), unlike the baseline Linear\nTransformer model with the naive sum update rule which\nbreaks. We also train a Transformer-XL in our medium\nconﬁguration as a baseline model speciﬁcally designed for\nthis use case (Dai et al., 2019; Rae et al., 2020). We evaluate\nit using different state sizes by changing the Transformer\nXL’s memory and target segment lengths (see Appendix F\nfor further details). Performance of the Delta Net does not\nyet match the performance of the Transformer XL when\nthe latter is evaluated with a large state size (large attention\nwindow). However, when we take the state size into account\n(Table 4), we observe that the Delta Net performs very\nwell with a small state size, which is a crucial property in\nsome practical applications (Irie et al., 2020). These results\nare promising for future work on alternative Transformer\nmodels which can run for an unlimited number of steps.\n7. Conclusion\nWe emphasise the connection between linearised self-\nattention and Fast Weight Programmers (FWPs, 1991) that\nprogram their fast weight memories through sequences of\nouter products between self-invented key and value pat-\nterns. The FWP perspective allows for discussing associa-\ntive memory capacity limitations of linear attention, and\nfor introducing an alternative differentiable elementary pro-\ngramming instruction that the FWP can use to dynamically\nedit the memory, akin to the famous delta rule, but such that\nthe FWP can learn to use the rule wisely through gradient\ndescent. We also propose and discuss a new method for\nlinearising attention. Experiments on synthetic and real lan-\nguage tasks demonstrate the effectiveness of our proposals.\nThe FWP perspective opens up new avenues for investigat-\ning even better programming instructions and designs for\nTransformers with ﬁnite memory.\nAcknowledgements\nWe thank Sjoerd van Steenkiste, Hubert Ramsauer and Sepp\nHochreiter for valuable comments and suggestions on the\nﬁrst version of the manuscript. This research was partially\nfunded by ERC Advanced grant no: 742870, project Algo-\nRNN, and by Swiss National Science Foundation grant no:\n200021 192356, project NEUSYM. We thank NVIDIA Cor-\nporation for donating several DGX machines, and IBM for\ndonating a Minsky machine. We also thank Katharopoulos\net al. (2020) for releasing their CUDA implementation of\nLinear Transformers, which was helpful to implement our\nmodels.\nReferences\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,\nL. Character-level language modeling with deeper self-\nattention. In Proc. Conference on Artiﬁcial Intelligence\n(AAAI), pp. 3159–3166, Honolulu, HI, USA, January\n2019.\nLinear Transformers Are Secretly Fast Weight Programmers\nBa, J., Hinton, G. E., Mnih, V ., Leibo, J. Z., and Ionescu, C.\nUsing fast weights to attend to the recent past. InProc. Ad-\nvances in Neural Information Processing Systems (NIPS),\npp. 4331–4339, Barcelona, Spain, December 2016.\nBaevski, A. and Auli, M. Adaptive input representations\nfor neural language modeling. In Int. Conf. on Learning\nRepresentations (ICLR), New Orleans, LA, USA, May\n2019.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. InInt.\nConf. on Learning Representations (ICLR), San Diego,\nCA, USA, May 2015.\nBello, I. Lambdanetworks: Modeling long-range interac-\ntions without attention. In Int. Conf. on Learning Repre-\nsentations (ICLR), Virtual only, May 2021.\nCheng, J., Dong, L., and Lapata, M. Long short-term\nmemory-networks for machine reading. In Proc. Conf.\non Empirical Methods in Natural Language Processing\n(EMNLP), pp. 551–561, Austin, TX, USA, November\n2016.\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,\nA., Kaiser, L., et al. Rethinking attention with performers.\nIn Int. Conf. on Learning Representations (ICLR), Virtual\nonly, 2021.\nClevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast\nand accurate deep network learning by exponential linear\nunits (ELUs). In Int. Conf. on Learning Representations\n(ICLR), San Juan, Puerto Rico, May 2016.\nDai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J.,\nLe, Q. V ., and Salakhutdinov, R. Transformer-XL: Atten-\ntive language models beyond a ﬁxed-length context. In\nProc. Association for Computational Linguistics (ACL),\npp. 2978–2988, Florence, Italy, July 2019.\nDemircigil, M., Heusel, J., L ¨owe, M., Upgang, S., and\nVermet, F. On a model of associative memory with huge\nstorage capacity. Journal of Statistical Physics, 168(2):\n288–299, 2017.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. In Proc. North American Chapter\nof the Association for Computational Linguistics on Hu-\nman Language Technologies (NAACL-HLT), pp. 4171–\n4186, Minneapolis, MN, USA, June 2019.\nFeldman, J. A. Dynamic connections in neural networks.\nBiological cybernetics, 46(1):27–39, 1982.\nGalanti, T. and Wolf, L. On the modularity of hypernet-\nworks. In Proc. Advances in Neural Information Process-\ning Systems (NeurIPS), Virtual only, 2020.\nGraves, A. Generating sequences with recurrent neural\nnetworks. Preprint arXiv:1308.0850, 2013.\nGreff, K., van Steenkiste, S., and Schmidhuber, J. On the\nbinding problem in artiﬁcial neural networks. Preprint\narXiv:2012.05208, 2020.\nHa, D., Dai, A., and Le, Q. V . Hypernetworks. InInt. Conf.\non Learning Representations (ICLR) , Toulon, France,\nApril 2017.\nHanson, S. J. A stochastic version of the delta rule. Physica\nD: Nonlinear Phenomena, 42(1-3):265–272, 1990.\nHebb, D. O. The organization of behavior: a neuropsy-\ncholocigal theory. A Wiley Book in Clinical Psychology,\n62:78, 1949.\nHinton, G. E. and Plaut, D. C. Using fast weights to de-\nblur old memories. In Proc. Conf. of Cognitive Science\nSociety, pp. 177–186, Seatle, W A, USA, July 1987.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nHopﬁeld, J. J. Neural networks and physical systems with\nemergent collective computational abilities. Proc. of the\nnational academy of sciences, 79(8):2554–2558, 1982.\nIrie, K., Zeyer, A., Schl ¨uter, R., and Ney, H. Language\nmodeling with deep Transformers. In Proc. Interspeech,\npp. 3905–3909, Graz, Austria, September 2019a.\nIrie, K., Zeyer, A., Schl¨uter, R., and Ney, H. Training lan-\nguage models for long-span cross-sentence evaluation. In\nProc. IEEE Automatic Speech Recog. and Understand-\ning Workshop (ASRU), pp. 419–426, Sentosa, Singapore,\nDecember 2019b.\nIrie, K., Gerstenberger, A., Schl¨uter, R., and Ney, H. How\nmuch self-attention do we need? Trading attention for\nfeed-forward layers. In Proc. IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 6154–\n6158, Virtual only, May 2020.\nJia, X., De Brabandere, B., Tuytelaars, T., and Gool, L. V .\nDynamic ﬁlter networks. In Proc. Advances in Neural\nInformation Processing Systems (NIPS) , pp. 667–675,\nBarcelona, Spain, 2016.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In Proc. Int. Conf. on Machine\nLearning (ICML), Virtual only, July 2020.\nLinear Transformers Are Secretly Fast Weight Programmers\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. Preprint arXiv:1412.6980, 2014.\nKirsch, L. and Schmidhuber, J. Meta learning backprop-\nagation and improving it. NeurIPS Workshop on Meta-\nLearning, 2020.\nKlein, B., Wolf, L., and Afek, Y . A dynamic convolu-\ntional layer for short rangeweather prediction. In Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), pp. 4840–4848, Boston, MA, USA, June 2015.\nKohonen, T. Correlation matrix memories. IEEE Transac-\ntions on Computers, 21(4):353–359, 1972.\nKosko, B. Bidirectional associative memories. IEEE Trans-\nactions on Systems, Man, and Cybernetics, 18(1):49–60,\n1988.\nKrotov, D. and Hopﬁeld, J. J. Dense associative memory\nfor pattern recognition. In Proc. Advances in Neural\nInformation Processing Systems (NIPS), pp. 1172–1180,\nBarcelona, Spain, December 2016.\nLin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou,\nB., and Bengio, Y . A structured self-attentive sentence\nembedding. In Int. Conf. on Learning Representations\n(ICLR), Toulon, France, April 2017.\nLittle, W. A. The existence of persistent states in the brain.\nMathematical biosciences, 19(1-2):101–120, 1974.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models. In Int. Conf. on Learning Rep-\nresentations (ICLR), Toulon, France, April 2017.\nMiconi, T., Stanley, K., and Clune, J. Differentiable plastic-\nity: training plastic neural networks with backpropaga-\ntion. In Proc. Int. Conf. on Machine Learning (ICML),\npp. 3559–3568, Stockholm, Sweden, July 2018.\nMiconi, T., Rawal, A., Clune, J., and Stanley, K. O. Back-\npropamine: training self-modifying neural networks with\ndifferentiable neuromodulated plasticity. In Int. Conf.\non Learning Representations (ICLR), New Orleans, LA,\nUSA, May 2019.\nMunkhdalai, T. and Trischler, A. Metalearning with hebbian\nfast weights. Preprint arXiv:1807.05076, 2018.\nMunkhdalai, T. and Yu, H. Meta networks. In Proc. Int.\nConf. on Machine Learning (ICML), pp. 2554–2563, Syd-\nney, Australia, August 2017.\nMunkhdalai, T., Sordoni, A., Wang, T., and Trischler, A.\nMetalearned neural memory. InProc. Advances in Neural\nInformation Processing Systems (NeurIPS), pp. 13310–\n13321, Vancouver, Canada, December 2019.\nNoh, H., Seo, P. H., and Han, B. Image question answering\nusing convolutional neural network with dynamic param-\neter prediction. In Proc. IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR), pp. 30–38, Las Vegas,\nNV, USA, 2016.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling\nneural machine translation. In Proc. Conf. on Machine\nTranslation: Research Papers , pp. 1–9, Brussels, Bel-\ngium, October 2018.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\nN., Grangier, D., and Auli, M. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proc. North American\nChapter of the Association for Computational Linguis-\ntics on Human Language Technologies (NAACL-HLT),\nDemonstrations, pp. 48–53, Minneapolis, MN, USA,\nJune 2019.\nPalm, G. On associative memory. Biological cybernetics,\n36(1):19–31, 1980.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proc. Association for Computational Linguistics\n(ACL), pp. 311–318, Philadelphia, PA, USA, July 2002.\nParikh, A. P., T¨ackstr¨om, O., Das, D., and Uszkoreit, J. A\ndecomposable attention model for natural language infer-\nence. In Proc. Conf. on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 2249–2255, Austin,\nTX, USA, November 2016.\nPaszke, A. et al. Pytorch: An imperative style, high-\nperformance deep learning library. In Proc. Advances in\nNeural Information Processing Systems (NeurIPS) , pp.\n8026–8037, Vancouver, Canada, December 2019.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,\nN. A., and Kong, L. Random feature attention. In Int.\nConf. on Learning Representations (ICLR), Virtual only,\n2021.\nPerez, E., Strub, F., De Vries, H., Dumoulin, V ., and\nCourville, A. FiLM: Visual reasoning with a general\nconditioning layer. In Proc. AAAI Conf. on Artiﬁcial\nIntelligence, pp. 3942–3951, New Orleans, LA, USA,\nFebruary 2018.\nPost, M. A call for clarity in reporting BLEU scores. InProc.\nConf. on Machine Translation , pp. 186–191, Brussels,\nBelgium, October 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multi-\ntask learners. [Online]. : https://blog.openai.com/better-\nlanguage-models/, 2019.\nLinear Transformers Are Secretly Fast Weight Programmers\nRae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,\nand Lillicrap, T. P. Compressive transformers for long-\nrange sequence modelling. In Int. Conf. on Learning\nRepresentations (ICLR), Virtual only, April 2020.\nRamsauer, H., Sch¨aﬂ, B., Lehner, J., Seidl, P., Widrich, M.,\nGruber, L., Holzleitner, M., Adler, T., Kreil, D., Kopp,\nM. K., Klambauer, G., Brandstetter, J., and Hochreiter, S.\nHopﬁeld networks is all you need. In Int. Conf. on Learn-\ning Representations (ICLR), Virtual only, May 2021.\nSchlag, I. and Schmidhuber, J. Gated fast weights for on-\nthe-ﬂy neural program generation. In NIPS Metalearning\nWorkshop, Long Beach, CA, USA, December 2017.\nSchlag, I. and Schmidhuber, J. Learning to reason with\nthird order tensor products. In Proc. Advances in Neural\nInformation Processing Systems (NIPS), pp. 9981–9993,\nMontr´eal, Canada, December 2018.\nSchlag, I., Munkhdalai, T., and Schmidhuber, J. Learning\nassociative inference using fast weight memory. In Int.\nConf. on Learning Representations (ICLR), Virtual only,\nMay 2021.\nSchmidhuber, J. Learning to control fast-weight memories:\nAn alternative to recurrent nets. Technical Report FKI-\n147-91, Institut f ¨ur Informatik, Technische Universit ¨at\nM¨unchen, March 1991.\nSchmidhuber, J. Learning to control fast-weight memories:\nAn alternative to dynamic recurrent networks. Neural\nComputation, 4(1):131–139, 1992.\nSchmidhuber, J. Reducing the ratio between learning com-\nplexity and number of time varying variables in fully\nrecurrent nets. In International Conference on Artiﬁcial\nNeural Networks (ICANN) , pp. 460–463, Amsterdam,\nNetherlands, September 1993.\nSchmidhuber, J. 26 March 1991: Neural nets learn to\nprogram neural nets with fast weights—like today’s\nTransformer variants. 2021: New stuff!, AI Blog, 2021.\nURL https://people.idsia.ch/˜juergen/\nfast-weight-programmer-1991-transformer.\nhtml.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In Proc. As-\nsociation for Computational Linguistics (ACL), pp. 1715–\n1725, Berlin, Germany, August 2016.\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efﬁcient\nattention: Attention with linear complexities. Preprint\narXiv:1812.01243, 2018.\nSmolensky, P. Tensor product variable binding and the\nrepresentation of symbolic structures in connectionist\nsystems. Artiﬁcial intelligence, 46(1-2):159–216, 1990.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\nSteinbuch, K. Die lernmatrix. Kybernetik, 1(1):36–45, 1961.\nSteinbuch, K. and Piske, U. A. W. Learning matrices and\ntheir applications. IEEE Transactions on Electronic Com-\nputers, 12(6):846–862, 1963.\nTay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,\nP., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long\nrange arena: A benchmark for efﬁcient transformers. In\nInt. Conf. on Learning Representations (ICLR), Virtual\nonly, May 2021.\nTsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and\nSalakhutdinov, R. Transformer dissection: An uniﬁed\nunderstanding for transformer’s attention via the lens of\nkernel. In Proc. Conf. on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 4344–4353, Hong\nKong, China, November 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis all you need. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), pp. 5998–6008, Long Beach,\nCA, USA, December 2017.\nvon der Malsburg, C. The correlation theory of brain func-\ntion. Internal Report 81-2, G¨ottingen: Department of Neu-\nrobiology, Max Planck Intitute for Biophysical Chemistry,\n1981.\nWidrow, B. and Hoff, M. E. Adaptive switching circuits.\nIn Proc. IRE WESCON Convention Record, pp. 96–104,\nLos Angeles, CA, USA, August 1960.\nLinear Transformers Are Secretly Fast Weight Programmers\nA. Update Rule Derivation\nA.1. The Update Rule\nHere we provide the intermediate steps from Eq. 23 to\nEq. 24.\nW(i) = W(i−1) +v(i)\nnew ⊗φ(k(i))  \nwrite\n−¯v(i) ⊗φ(k(i))  \nremove\n(23)\n= W(i−1) + β(i)(v(i) −¯v(i)) ⊗φ(k(i)) (24)\nBy grouping the last two terms, Eq. 23 becomes:\nW(i) = W(i−1) + (v(i)\nnew −¯v(i)) ⊗φ(k(i)) (38)\nBy using the deﬁnition of v(i)\nnew from Eq. 22:\nv(i)\nnew = β(i)v(i) + (1 −β(i))¯v(i) (22)\nwe obtain:\nv(i)\nnew −¯v(i) = β(i)v(i) + (1 −β(i))¯v(i) −¯v(i) (39)\n= β(i)(v(i) −¯v(i)) (40)\nBy substituting this expression to Eq. 38, we obtain Eq. 24\n.\nA.2. Key Sum Normalisation\nBy considering one-hot vectors {e(1),..., e(i),..., e(dkey)}\nwhich form the Cartesian basis of Rdkey , any matrix W ∈\nRdvalue×dkey can be written as\nW =\ndkey∑\ni=1\nw(i) ⊗e(i) (41)\nwhere {w(1),..., w(i),..., w(dkey)}are the column vectors of\nW. In the context of associative memory, we can interpret\nthis expression as a set of associations with ﬁxed keys e(i)\nand the associated values w(i).\nIn this view, any update of W can be written as updates\nof each w(i). This perspective allows us to derive the sum\nnormalisation of Sec. 4.2. For that, we start by deriving the\nupdate of w(i).\nGiven an arbitrary weight W, we consider updating it to\nW′by adding a new association (k,v) using our update\nrule of Sec. 4.2 (where we omit β):\n¯v = Wk (42)\nW′= W + (v −¯v) ⊗k (43)\nBy substituting k in Eq. 43 by its expression in the Cartesian\nbasis k =\ndkey∑\ni=1\nkie(i) with ki ∈R, we obtain:\nW′= W + (v −¯v) ⊗\ndkey∑\ni=1\nkie(i) (44)\n= W +\ndkey∑\ni=1\nki(v −¯v) ⊗e(i) (45)\nNow by substituting W by its expression of Eq. 41:\nW′=\ndkey∑\ni=1\nw(i) ⊗e(i) +\ndkey∑\ni=1\nki(v −¯v) ⊗e(i) (46)\n=\ndkey∑\ni=1\n(\nw(i) + ki(v −¯v)\n)\n⊗e(i) (47)\nThe column-wise update is thus:\nw′(i) = w(i) + ki(v −¯v) (48)\nWe can explicitly write down ¯v as:\n¯v = Wk = W\ndkey∑\nj=1\nkje(j) =\ndkey∑\nj=1\nkjw(j) (49)\nwhich we can substitute in Eq. 48 to obtain:\nw′(i) = w(i) + ki(v −\ndkey∑\nj=1\nkjw(j)) (50)\n= w(i) + kiv −\ndkey∑\nj=1\nkikjw(j) (51)\nIn Eq. 51, the weight ki on the positive term v is in gen-\neral not equal to the total weights on the negative terms∑dkey\nj=1 kikj. We can force these weights to be balanced by\nintroducing the normalisation:\ndkey∑\nj=1\nkikj = ki.\nIf ki is non zero, we obtain:\ndkey∑\nj=1\nkj = 1\nThis corresponds to the sum normalisation we introduced\nin Sec. 4.2 .\nB. Formal comparison to Peng et al. (2021)\nConcurrently to our work, Peng et al. (2021) proposed the\nfollowing gated update rule:\nW(i) = (1 −β(i))W(i−1) + β(i)v(i) ⊗φ(k(i)) (52)\nLinear Transformers Are Secretly Fast Weight Programmers\nwhich is motivated by the gating mechanism in recurrent\nneural networks (Hochreiter & Schmidhuber, 1997). In\ncontrast, our update rule of Eq. 24\nW(i) = W(i−1) + β(i)(v(i) −¯v(i)) ⊗φ(k(i)) (24)\nis driven by an associative memory perspective, relates to\nthe famous error-correcting delta rule, and offers a crucial\nproperty.\nTo illustrate a similarity and a crucial difference between the\ntwo update rules, we consider a fast weight matrixW which\nis constructed by two associations (k1,v1) and (k2,v2), i.e.\nW = v1 ⊗k1 + v2 ⊗k2 (53)\nwhere we assume k1 and k2 to be orthonormal, and we\nomit φ. Now we consider updating W to W′by adding a\nnew association (k3,v3) where k3 = k2. Using Peng et al.\n(2021)’s update rule, we have:\nW′= (1 −β)W + βv3 ⊗k3\nThis rule thus updates the value associated with the key\nk2 = k3 to be a convex combination of the old and the new\nvalues (1 −β)v2 + βv3:\nW′k3 = (1 −β)Wk3 + βv3\n= (1 −β)v2 + βv3\nHowever, it also modiﬁes or in the worst case erases the\nvalue associated with the key k1:\nW′k1 = (1 −β)Wk1 = (1 −β)v1\nIn contrast, using our update rule, we have:\nW′= W + β(v3 −v2) ⊗k3\nsince ¯v = Wk3 = Wk2 = v2.\nOur rule thus also updates the value associated with the key\nk2 = k3 to be a convex combination of the old and the new\nvalues (1 −β)v2 + βv3:\nW′k3 = Wk3 + β(v3 −v2)\n= v2 + β(v3 −v2)\n= (1 −β)v2 + βv3\nwhile crucially, it keeps the value associated with k1 un-\nmodiﬁed:\nW′k1 = Wk1 = v1\nOur update rule thus differs from Peng et al. (2021)’s one on\nthis property of updating associations while keeping other\n“unrelated” ones intact in an associative memory.\nC. DPFP-ν Implementation\nListing 1 is a simple PyTorch implementation of DPFP-ν\n(Eq. 37) which consist of two concatenations followed by\none element-wise multiplication.\n1 import torch\n2 from torch import cat\n3 from torch.nn.functional import relu as r\n4\n5 def dpfp(x, nu=1):\n6 x = cat([r(x), r(-x)], dim=-1)\n7 x_rolled = cat([x.roll(shifts=j, dims=-1)\n8 for j in range(1,nu+1)], dim=-1)\n9 x_repeat = cat([x] * nu, dim=-1)\n10 return x_repeat * x_rolled\nListing 1.Simple PyTorch implementation of DPFP-ν(Eq. 37).\nD. Additional Experimental Results\nIn this section, we provide additional experimental results\nwhich we could not include in the main paper because of\nspace limitations.\nD.1. Synthetic Task Setting 1\nFigure 4 shows learning curves for the synthetic setting 1\n(without replacement) with 600 unique keys and values. The\nscripts used to generate such ﬁgures can be found in our\nGitHub repository.\nFigure 4.Training curves for setting 1 with 600 unique keys/values\n(sampled without replacement) as described in Sec. 6.1.1.\nD.2. Synthetic Task Setting 2\nFigure 5 is a capacity plot for setting 2 with an increasing\nnumber of unique keys and queries (analogous to Figure 2\nof setting 1 apart from the log-scale of the y-axis). We did\nnot include FA VOR+ in this plot, because its combination\nwith our update rule resulted in not-a-number in this setting.\nLinear Transformers Are Secretly Fast Weight Programmers\nFigure 5.Final evaluation loss on synthetic setting 2 (with replace-\nment) problems with the total number of unique associations rang-\ning from 20 to 200. Each individual symbol is a model trained\nuntil convergence as described in Sec. 6.1.2. In all problems, with\ndifferent sequence lengths and a different number of unique keys,\nour update rule outperforms all other approaches.\nD.3. Language Modelling\nIn Sec. 6.3, we evaluated our update rule when the model\nis under overcapacity regime. Here we present an extra\nlanguage modelling experiment which evaluate the bene-\nﬁts of our update rule in non-overcapacity scenarios. This\nalso allows us to include DPFP in the evaluation. We train\nboth, Performer and DPFP, in the small setting (D= 128,\nL= 256) with m= 16 and ν = 1, resulting in ddot = 256\nfor both cases. Table 5 shows the perplexity results. First\nwe observe that the Performer and DPFP baseline mod-\nels with the sum update rule do not outperform the Linear\nTransformer baseline from Table 2. In fact, language mod-\nelling might be less affected by the capacity issue than the\nsynthetic retrieval task, as it might not require the exact\nretrieval. Second we observe that our update rule improves\nboth variants of linear attention over the sum update-rule\nbaselines even in this condition. This indicates the general\nbeneﬁts of our update rule in Fast Weight Programmers. We\nnote that the improvement is larger for the DPFP model\nthan for the Performer. This is similar to Table 2 where our\nupdate rule improves the deterministic Linear Transformers\nmore than the Performers. Finally, we note that we also\ntried the DPFP and Performer models with an increased\nddot by setting ν = 2 and m= 32 respectively. While this\nincreases ddot by a factor of two, it was not beneﬁcial for\nthis language modelling setting.\nE. Details on Machine Translation\nExperiments\nWe implemented differentφfunctions in the FAIRSEQ tookit\n(Ott et al., 2019). The Transformer architecture used in the\nexperiment is the one referred to as big in the original Trans-\nTable 5.WikiText-103 language model perplexity results showing\neffects of our update rule in non-overcapacity regime. The number\nof trainable parameters are almost the same for all models, up to\nthe small difference introduced by gating in our update rule (16 K\nparameters). The small conﬁg is used, i.e. D = 128, L = 256\n(40 M parameters). We set m= 16for the Performers and ν = 1\nfor the DPFP models, which result in ddot = 256 for both cases.\nThe model is thus not necessary in an overcapacity regime.\nUpdate small\nRule Valid Test\nTransformer - 33.0 34.1\nPerformer sum 38.0 38.8\ndelta 36.0 37.0\nDPFP sum 37.7 38.8\ndelta 33.9 35.0\nformer paper (Vaswani et al., 2017): the model has 6 layers\neach in the encoder and the decoder, with a hidden layer size\nof 1024 with 16 attention heads, 4096-dimensional feed-\nforward layers, using 32 K byte-pair encoding sub-word\nunits (Sennrich et al., 2016). FAIRSEQ provides a training\nconﬁguration for the corresponding model (Ott et al., 2018),\nwhich we adapted for our infrastructure. We trained our\nmodels on three GPUs using a batch size of up to 3584\ntokens per GPU and accumulating gradients over 16 batches\nfor 45 epochs, and selected the best model based on the\nvalidation BLEU score. In Table 1, we directly report BLEU\nfor different values of ddot; Table 6 provides the conversion\nfrom hyper-parameters mof Performers or νin the DPFP\nto ddot.\nTable 6.Relation between dot product space dimension and the\nhyper-parameters in the Performer and our DPFP models. dkey =\n64 in all our translation models.\nddot 256 384 512\nPerformer m 128 192 256\nDPFP ν 2 3 4\nF. Details on Language Modelling\nExperiments\nImplementation notes. All our implementations are\nbased on PyTorch (Paszke et al., 2019). Our base language\nmodelling code has been developed by using the public code\nby Dai et al. (2019) for Transformer-XL as a starting point.\nFor φ functions, we ported the same implementation we\nused for our translation experiments. For the implementa-\ntion of our update rule, we modiﬁed the CUDA kernel for the\nLinear Transformer made publicly available by Katharopou-\nlos et al. (2020). We note that a custom implementation of\nLinear Transformers Are Secretly Fast Weight Programmers\nthe backward pass for fast weights is crucial for language\nmodelling. A naive backward computation generated by au-\ntomatic differentiation would store the fast weights for each\ntime step, which can quickly hit the GPU memory limit.\nThe custom implementation ensures that we need to store\nonly one set of weights by recomputing the fast weights\nneeded for computing the gradients for each time step in\nthe backward pass (which still remains time-efﬁcient as the\noperations involved in the computation of our fast weights\nare rather inexpensive).\nExperimental details. Here we provide extra experimen-\ntal details to complement the descriptions of Sec. 6.3. For\nthe small and medium conﬁgurations, we use batch sizes\nof 96 and 56 sequences, respectively, and train for about\n120 and 70 epochs. In both settings, we apply 10% dropout\n(Hanson, 1990; Srivastava et al., 2014), and train using the\nAdam optimiser (Kingma & Ba, 2014) with an initial learn-\ning rate of 0.00025 and 2000 learning rate warm-up steps.\nFor further details, we refer the readers to our code. For\nexperiments with Transformer-XL (Table 4), we train it\nwith the same backpropagation span as our models (i.e. 384\nwords in the medium conﬁguration). The model is trained\nwith memory and target segment lengths of 384. The mod-\nels with different state sizes in Table 4 are obtained by using\ndifferent Transformer-XL memory segment lengths at eval-\nuation time. The models with state sizes of 1.05 M, 2.10 M,\nand 6.29 M are obtained by using memory and target lengths\nof 64, 128, and 384, respectively. The model with a state\nsize of 0.13 M uses a memory length of 15 and a target\nlength of 1. Like for other models, a batch size of 1 is used\nfor evaluating the Transformer XL. The state sizes in Table\n4 are computed as follows. The per-layer state size of the\nLinear Transformer and the Delta Net are: number of heads\n(here 8) ×fast weight matrix size which is per-head key\ndimension (here 32) ×per-head value dimension (here 32).\nThis yields a total size of 8,192. The per-layer state size of\nthe Transformer XL is: memory segment length ×target\nsegment length ×(total key dimension, here 256 + total\nvalue dimension, here 256). We obtain the total state size\nwe report in Table 4 by multiplying the per-layer state size\nby the number of layers which is 16 for all our models."
}