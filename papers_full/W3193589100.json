{
    "title": "Modeling Protein Using Large-scale Pretrain Language Model",
    "url": "https://openalex.org/W3193589100",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4303863748",
            "name": "Xiao, Yijia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227085011",
            "name": "Qiu, Jiezhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1659051576",
            "name": "LI Ziang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202068179",
            "name": "Hsieh, Chang-Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2076873566",
            "name": "Tang Jie",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2963938814",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2937766897",
        "https://openalex.org/W2104972430",
        "https://openalex.org/W1501531009",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2943495267",
        "https://openalex.org/W2161072217",
        "https://openalex.org/W2735621019",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2913820882",
        "https://openalex.org/W2058206198",
        "https://openalex.org/W2898402099",
        "https://openalex.org/W2379594833",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2963457143",
        "https://openalex.org/W2058191353",
        "https://openalex.org/W2951599627",
        "https://openalex.org/W2232067880",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W2109801072"
    ],
    "abstract": "Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",
    "full_text": "Modeling Protein Using Large-scale Pretrain\nLanguage Model\nYijia Xiao1 2, Jiezhong Qiu1 2, Ziang Li1, Chang-Yu Hsieh3, Jie Tang1 2\n1 Department of Computer Science and Technology, Tsinghua University\n2 Beijing Academy of Artificial Intelligence\n3 Tencent Quantum Lab\n{xiaoyiji18,qiujz16,li-za19}@mails.tsinghua.edu.cn\nkimhsieh@tencent.com\njietang@tsinghua.edu.cn\nABSTRACT\nProtein is linked to almost every life process. Therefore,\nanalyzing the biological structure and property of pro-\ntein sequences is critical to the exploration of life, as\nwell as disease detection and drug discovery. Traditional\nprotein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning mod-\nels makes modeling data patterns in large quantities of\ndata possible. Interdisciplinary researchers have begun\nto leverage deep learning methods to model large bi-\nological datasets, e.g. using long short-term memory\nand convolutional neural network for protein sequence\nclassification. After millions of years of evolution, evo-\nlutionary information is encoded in protein sequences.\nInspired by the similarity between natural language\nand protein sequences, we use large-scale language\nmodels to model evolutionary-scale protein sequences,\nencoding protein biology information in representation.\nSignificant improvements are observed in both token-\nlevel and sequence-level tasks, demonstrating that our\nlarge-scale model can accurately capture evolution in-\nformation from pretraining on evolutionary-scale indi-\nvidual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.\nCCS CONCEPTS\nâ€¢ Applied computing â†’Life and medical sciences;\nComputational biology; Computational proteomics;\nKEYWORDS\nprotein modeling, language model, high performance\ncomputing\nâˆ—Corresponding author: Jie Tang.\n1 INTRODUCTION\nAs an indispensable part of life activities, protein is\nresponsible for catalysis (such as enzymes), transporta-\ntion (such as hemoglobin), etc. Therefore, understand-\ning the structure and functionality of protein is criti-\ncal to the study of life science, as well as disease de-\ntection and drug discovery. Traditional protein anal-\nysis paradigms can be divided into experimental and\nanalytical. Experimental methods usually require pu-\nrification, crystallization, and X-ray crystallography.\nAnalytical methods, like sequence alignment[16], and\nmolecular dynamics simulation [10], tend to be inca-\npable of handling large-scale protein data. Sequence\nalignment and similarity analysis leverage the idea\nthat \"structure determines properties\", that sequential\nmolecules with similar sequence order tend to have\ncommon ancestors and are relatively similar in struc-\nture and functionality. So similarity analysis often re-\nquires a large-scale annotated database, the properties\nof the sequence to be analyzed can be inferred from the\nlabels of aligned sequences in the database. However,\nlabeling such large databases requires lots of manpower\nand material resources. Molecular dynamics simulation\n(MD) and Monte Carlo (MC) simulations can be applied\nto protein analysis[11][14], and can be quite accurate\n(simulation at atom-scale). However, requires a lot of\ncomputing resources and is time-consuming.\nGenerally speaking, most of the proteins that exist\nstably in nature have undergone millions of years of\nnatural selection and evolution, and are in a low-energy\nstable state. The polarity of some amino acids makes\ncertain amino acid arrangements in a lower energy\nstate, and motifs in proteins are also made up of spe-\ncific amino acid stretches folded. Such patterns can be\narXiv:2108.07435v2  [cs.LG]  7 Dec 2021\ncaptured by deep learning models. Researchers have\nexplored various strategies. Inspired by Word2Vec[17],\nBioVec[3] proposed ProtVec for proteins GeneVec for\ngene sequences modeling. However, the vocabulary\nsize grows exponentially with dependence range (n-\ngram), making the cost of modeling long-range depen-\ndencies unbearable (n-grams representation). With the\nrise of representation learning, sequence representation\nlearning[1] and transfer learning [ 12] are also intro-\nduced to protein analysis. Recent years, the emergence\nof the attention mechanism[ 26], which can compute\nhidden representations in parallel, allows researchers\nto better model long sequential data. ProtTrans[8] also\nshow that large-scale auto-regressive language mod-\nels can model protein sequences quite well. Besides,\nthe information encoded in an individual sequence is\nlimited, MSA Transformer[20], ESM[21] leverage se-\nquence alignment information to model protein even\nbetter. Other research like Neural Potts Model[24] ob-\ntained inspiration from the Potts model.\nThanks to the advancement of high-throughput se-\nquencing technology, we have larger amino acid se-\nquence databases than ever before. However, most of\nthese data are unlabeled primary structures of proteins,\nthe labeled sequences (like structure, stability) are rel-\natively scarce. The amazing achievements of BERT[6]\nreveal the fact that data patterns can be extracted us-\ning unsupervised learning from massive unlabeled data,\nwhich inspired us to train language models on mas-\nsive protein sequences. We have trained multiple large-\nscale models on the PFAM[7] dataset, the largest with\n3 billion parameters, outperforming TAPEâ€™s [19] per-\nformance.\n2 RELATED WORKS\n2.1 Standardized Datasets and\nTasks\nThere are plenty of data in the computational pro-\nteomics field, however, current literature is fragmented\nin terms of unified datasets and evaluation metrics.\nThe methods and models introduced by researchers\nare often evaluated on different datasets with different\nevaluation metrics. To solve this dilemma, TAPE[19]\nput forward a set of five biologically related tasks,\nincluding secondary structure prediction ([ 15], [ 4],\n[18]), contact prediction ([9], [4], [18]), remote homol-\nogy detection ([ 9]), fluorescence ([ 23]), and stability\n([22]). Besides, commonly used models, like LSTM[13],\nTransformer[26], and ResNet[28] are implemented for\nthese tasks, serving as benchmarks for semi-supervised\nrepresentation learning. One of their conclusions is\nthat self-supervised training is beneficial for almost\nall models on all tasks, doubling performance in some\ndownstream tasks. Our work is based on standardized\ndatasets and evaluation metrics provided in TAPE[19].\n2.2 Large-scale Pretraining\nThe success of pretraining makes researchers wonder\nwhether the in language model scale can always bring\nabout improved performance. ProtTrans[8] is one of\nthe representatives, the researchers trained a series of\nlanguage models with tens of billions of parameters,\nthe largest one ProtT5-XXL with 11B parameters, and\nachieved excellent performance on downstream tasks\nsuch as secondary structure prediction and solubility\nprediction.\n2.3 Efficient Pretraining of\nLanguage Models\nDifferent from the usual pretraining, large-scale pre-\ntraining requires distributed training techniques, in-\ncluding model parallelism, data parallelism, memory\noptimization, data synchronization, etc. Fortunately,\nMegatron-LM[25] provides us with an efficient training\nframework for language models. We have implemented\nand trained our protein language model within this\nframework, as well as downstream classification and\nregression tasks.\n3 METHODOLOGY\n3.1 Pretrain Tasks\nDescription\nThe goal for protein pretraining is modeling data pat-\nterns in massive unlabeled sequences. One closed-\nrelated model is BERT[6] from natural language pro-\ncessing. We made some modifications to its loss func-\ntion and model structure.\nDataset\nOur work takes the dataset put forward by TAPE[19].\nSo some date descriptions are inherited. PFAM[7] is a\nwidely-used database consisting of more than 32 million\n2\nprotein sequences. Sequences in PFAM are clustered\ninto evolutionarily related groups (protein families).\nLeveraging this family information, TAPE constructed\na test set (about 1% of the data) of fully held out families.\nThe remaining data are used for constructing training\nand test sets using a random 95%/5% split. We use pre-\nprocessed PFAM from TAPE as the pretrain corpus.\nTraining Objective\nBERT[6] original loss consists of masked language\nmodel loss and next sentence prediction loss.\nLBERT = ğ¿ğ‘œğ‘ ğ‘ ğ‘€ğ¿ğ‘€ +ğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘†ğ‘ƒ\nğ¿ğ‘œğ‘ ğ‘ ğ‘€ğ¿ğ‘€ =\nâˆ‘ï¸\nLğ‘šğ‘ğ‘ ğ‘˜ğ‘’ğ‘‘ ğ‘¥ğ‘– MLM (ğ‘¥ğ‘–)\nğ¿ğ‘œğ‘ ğ‘ ğ‘ğ‘†ğ‘ƒ =\nâˆ‘ï¸\nLğ‘ ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘›ğ‘ğ‘’ğ‘ğ‘ğ‘–ğ‘Ÿ ğ‘ğ‘– NSP (ğ‘ğ‘–)\nFor protein pretraining, we inherited the masking strat-\negy of the masked language model (MLM) in BERT[6],\nrandomly masking 15% of all the amino acid tokens,\nand then train the protein model to be able to predict\nthe masked token from the rest of tokens. As for next\nsentence prediction (NSP), considering that the input\nsequences are randomly shuffled, we assume there is\nno evident semantic/biological correlation between se-\nquences. So we discard the next sentence prediction\nloss, only keep the masked language model loss.\nAs for the training objective function, we modified\nthe loss function of BERT: BERTâ€™s loss function includes\nmasked language model and next sentence prediction.\nConsidering that there is no obvious contextual seman-\ntic relationship between protein and protein, we only\nretain masked language model loss.\nLMLM = âˆ’\nâˆ‘ï¸\nË†ğ‘¥âˆˆğ‘š(x)\nlog ğ‘\u0000Ë†ğ‘¥ |X\\ğ‘š(x)\n\u0001\nIn terms of model structure, Megatron-LM [ 25] pro-\nposes that when the scale of the model grows huge, the\nposition of the layernorm becomes critical. Therefore,\nthe sublayers in the transformer layer have been re-\nordered. The original layernorm is in the output layer,\nbut now it is placed ahead of the input layer to prevent\nthe input data from drifting.\n3.2 Downstream Classification\nTasks\nThere are three classification tasks, corresponding to\ntoken, sequence, and token-pair classification.\n \nFigure 1: Secondary Structure Task\n3.2.1 Secondary Structure.\nDescription\nSecondary structure classification is a token-level\ntask. The input is protein sequence, the output is a\nsequence of labels with the same length, indicating\nthe secondary structure position of the corresponding\namino acid. As for Q3, the labels are Helix, Strand, and\nOther. As for Q8, the labels are helix (G), ğ›¼-helix (H),\nğœ‹-helix (I), ğ›½-stand (E), bridge (B), turn (T), bend (S),\nand others (C).\nInput\nğ¼ğ‘›ğ‘ ğ‘’ğ‘ = (ğ‘¥1,...,ğ‘¥ ğ¿)\nOutput\nğ‘‚ğ‘¢ğ‘¡ğ‘ ğ‘’ğ‘ = (ğ‘¦1,...,ğ‘¦ ğ¿).ğ‘¦ğ‘– âˆˆğ‘„ğ‘–\nDataset\nThe dataset for secondary structure task is the\nCB513[5] dataset.\nTraining Objective\nA one-dimensional convolution layer can be applied\nto secondary structure prediction[ 27]. However, due\nto the powerful modeling capabilities of our model,\nthe encoding output from the protein language model\nalready contains sufficient information for this task, so\nwe take ProteinLM followed by a multilayer perceptron\nas the secondary structure classifier.\nğ¿ğ‘œğ‘ ğ‘ (ğ‘†ğ‘’ğ‘)=\nâˆ‘ï¸\nğ‘¥ğ‘– âˆˆğ‘†ğ‘’ğ‘\nğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ğ¿ğ‘œğ‘ ğ‘  (ğ‘šğ‘œğ‘‘ğ‘’ğ‘™(ğ‘¥ğ‘–),ğ‘¥ğ‘–)\n3.2.2 Remote Homology.\nDescription\nRemote homology detection is a sequence-level clas-\nsification task. This task is introduced to measures a\nmodelâ€™s ability to detect structural similarity across dis-\ntantly related inputs. The input is a protein sequence,\nand the target is to predict which fold family this se-\nquence belongs to. There are 1195 classes in all. Similar\n3\n \nFigure 2: Remote Homology Task\nto the token-level prediction tasks, we adopt the multi-\nlayer perceptron for classification.\nğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘‘_ğ‘†ğ‘’ğ‘ = ğ‘ƒğ‘Ÿğ‘œğ‘¡ğ‘’ğ‘–ğ‘›ğ¿ğ‘€ (ğ‘†ğ‘’ğ‘(ğ´ğ¶[1],...ğ´ğ¶ [ğ‘›))\nğ¸ğ‘›([ğ¶ğ¿ğ‘†],ğ´ğ¶[1],...ğ´ğ¶ [ğ‘›])= ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘‘_ğ‘†ğ‘’ğ‘\nğ¿ğ‘ğ‘ğ‘’ğ‘™(ğ‘†ğ‘’ğ‘(ğ´ğ¶[1],...ğ´ğ¶ [ğ‘›))= ğ‘€ğ¿ğ‘ƒ(ğ¸ğ‘›([ğ¶ğ¿ğ‘†]))\nHere, ğ¸ğ‘›(ğ‘¥)means encoding results for token ğ‘¥,\nAC[i] means the ğ‘–ğ‘¡â„ amino acid in protein sequence.\nTraining Objective\nThis is a classical classification task, we take the naive\ncross-entropy loss.\nğ¿ğ‘œğ‘ ğ‘ (ğ‘†ğ‘’ğ‘)= ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ğ¿ğ‘œğ‘ ğ‘  (ğ‘šğ‘œğ‘‘ğ‘’ğ‘™(ğ‘ ğ‘’ğ‘),ğ‘™ğ‘ğ‘ğ‘’ğ‘™)\n3.2.3 Contact Prediction.\n \nFigure 3: Contact Prediction Task\nDescription\nContact prediction means predicting whether or not\namino acid pairs are in \"contact\" in folded structure (in\n\"contact\" means the distance in folded structure within\n8 angstroms); facilitating 3-dimensional free modeling\nof protein. It is a classification task, assigning a binary\nlabel to amino acid pairs, indicating whether they are\nin â€™contactâ€™. The contact prediction task can evaluate\nthe modelâ€™s ability to capture protein sequenceâ€™s global\ninformation. Unlike the commonly used residual con-\nnected 2D-convolution network, we adopted a simple\npredictor, concatenating embedding pairs and using\nmultilayer perceptron to do this binary classification.\nNumerous hidden units, presentation layers, as well\nas huge corpus guarantee that our model can capture\neven more long-range dependence information than\ncommon models.\nDataset\nThe dataset from ProteinNet[2]. And evaluation met-\nric isğ¿/5, ğ¿/2, ğ¿most likely contact prediction accuracy\ncontacts (ğ¿is the length of protein sequence).\n3.3 Downstream Regression Tasks\n3.3.1 Fluorescence.\n \nFigure 4: Fluorescence Task\nDescription\nDistinguishing protein sequences with different mu-\ntations can be difficult, since the computational cost\ngrows exponentially with the number of mutations\nğ‘š. The computational complexity for a sequence with\nğ‘š mutation away is ğ‘‚(ğ¿ğ‘š). The fluorescence predic-\ntion task can evaluate the modelâ€™s capacity to distin-\nguish between very similar protein sequences, as well\nas its ability to generalize to unseen combinations of\nmutations[19]. Accurate predictions can facilitate the\nexploration of the protein landscape.\nDataset\nThe train set[23] is made up of ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ»ğ‘ğ‘šğ‘šğ‘–ğ‘›ğ‘” =\n3 neighborhoods from the parent green fluorescent\nprotein[23], while the test set sequences with four or\nmore mutations.\n3.3.2 Stability.\nDescription\nStability is very important in the design of protein\ndrugs, because drugs with low stability are often de-\ngraded before they take effect. The stability of one\n4\n \nFigure 5: Stability Task\nprotein sequence is measured experimentally and in-\ndirectly: the upper limit of concentration at which the\nprotein can maintain its original folding structure[22].\nTherefore, for this task, The input is the amino acid\nsequence, while the output is a continuous value pre-\ndicting to which extent the protein can maintain its fold\nstructure.\nDataset\nThe train set consists of proteins from four rounds\nof experimental design, while the test set contains\nHamming distance-1 neighbors of top candidate\nproteins[19].\n4 RESULTS\nOur model has obtained amazing results in downstream\ntasks. There are great improvements in four tasks: sec-\nondary structure prediction, distant homology detec-\ntion, stability, and contact prediction. It is worth men-\ntioning that the performance of the 3B model on contact\nprediction has almost doubled compared with the base-\nline model.\nBesides, we used 10 sets of model hyper-parameters\nin total, and conducted very sufficient experiments on\na series of tasks. The results can be found inTable 7.\n4.1 Training\nWe pretrained two large models on a 480 GPUs (Tesla-\nV100-32GB) cluster for about three weeks. The MLM\nloss and PPL of the pretrained models can be found in\nTable 1.\nThe 3B parameters model reached language model\nloss ğ‘€ğ¿ğ‘€ğ‘™ğ‘œğ‘ ğ‘ 3ğµ = 1.318, perplexity ğ‘ƒğ‘ƒğ¿3ğµ = 3.736.\nThe 1.2B parameters model reached language model\nloss ğ‘€ğ¿ğ‘€ğ‘™ğ‘œğ‘ ğ‘ 1.2ğµ = 1.335, perplexity ğ‘ƒğ‘ƒğ¿1.2ğµ = 3.802.\nIn pretraining, although the overall training iteration\nfor the 3 billion model is only half of that for the 1.2 bil-\nlion model, it reached even smaller MLM loss and PPL.\nThis phenomenon demonstrated that, when handled\nproperly, the expansion in model scale can contribute\nto the accurate capture of patterns in the data.\nModel Protein LM (1.2B) Protein LM (3B)\nMLM Loss 1.335 1.318\nPPL 3.802 3.736\nTable 1: MLM loss and PPL\n4.2 Evaluation\nOut of the five tasks, the results of four tasks have been\nimproved.\n(1) Contact Prediction: Table 2\n(2) Remote Homology: Table 3\n(3) Secondary Structure: Table 4\n(4) Fluorescence: Table 5\n(5) Stability: Table 6\nTask contact prediction\nMetric P@L/5\nTAPE 0.36\nProteinLM (200M) 0.52\nProteinLM (3B) 0.75\nTable 2: Contact Prediction\nTask remote homology\nMetric Top 1 Accuracy\nTAPE 0.21\nProteinLM (200M) 0.26\nProteinLM (3B) 0.30\nTable 3: Remote Homology\n5 CONTACT MAP\nVISUALIZATION\nGenerally, the accuracy of predictions on the anti-\ndiagonal can reflect the modelâ€™s ability to capture long-\nrange dependency. Therefore, we also visualized the\nground truth contact maps, as well as contact maps\n5\nTask secondary structure\nMetric Accuracy (Q-3)\nTAPE 0.73\nProteinLM (200M) 0.75\nProteinLM (3B) 0.79\nTable 4: Secondary Structure\nTask fluorescence\nMetric Spearmanâ€™s rho\nTAPE 0.68\nProteinLM (200M) 0.68\nProteinLM (3B) 0.68\nTable 5: Fluorescence\nTask stability\nMetric Spearmanâ€™s rho\nTAPE 0.73\nProteinLM (200M) 0.77\nProteinLM (3B) 0.79\nTable 6: Stability\npredicted by our model and TAPE. The contact map\nbelow demonstrates that our model is good at capturing\nlong-range dependency.\n5.1 Factual Contact Map\nWe visualize the contact map of protein #TBM-T0889\nin Figure 6, and we can intuitively see that there are\nmany long-range contacts (contacts that are separated\nby at least 24 residues). This protein sequence can be\nused to distinguish the ability to capture long-distance\ndependence of different models.\n5.2 TAPE Contact Map\nThrough the visualized predictions from TAPE (7), we\ncan see that the TAPE model (small-scale transformer)\ncan capture medium-range contacts (contacts that are\nseparated by 12-23 residues). As for the long-range\ncontact prediction, there are lots of missings on the\nanti-diagonal belt.\nFigure 6: Factual Contact Map\nFigure 7: TAPE Prediction\n5.3 ProteinLM Contact Map\nProteinLM-3B model shows very good performance\nin contact prediction, and the visualized prediction\nmap confirmed this. ProtrinLM-3B can capture medium-\nrange and long-range dependencies, with lots of hits\non the anti-diagonal belt.\n5.4 Analysis and Discussion\nWith the limited amount of computing resources and\ncomputing time, the selection of hyper-parameters is\ncritical to the modelâ€™s performance. A trade-off is nec-\nessary. The depth (transformer layers) of the model has\n6\nFigure 8: ProteinLM-3B Prediction\na greater impact on the performance than the width\n(hidden size).\nOn the one hand, the model that is too flat\n(#ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ _ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  < 8) performs poorly, even\nthough it has a large hidden size. We trained a model\nwith â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’ = 8192, #ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ _ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  = 3.\nAlthough its training speed (average time for each it-\neration) is the fastest among all models, it failed to\nconverge after 5 days of training.\nOn the other hand, the model that is too deep is not\na feasible choice in this scenario (limited training time).\nThe training time of Figure 9 is 3.5 times that of Fig-\nure 10. And it takes about 25 days to train the model\nwith 32 transformer layers Figure 9.\nOur empirical conclusion is that the model pa-\nrameters of 512 < â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘ ğ‘–ğ‘§ğ‘’ < 3072, 8 <\n#ğ‘¡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ _ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  < 24 are feasible and can well\nbalance training efficiency and resource consumption.\n6 SUMMARY\nWe proposed ProteinLM, a large-scale pretrain model\nfor protein sequences. The optimizations we introduced\ninto protein pretraining make billion-level model train-\ning possible and efficient. And the significantly im-\nproved performance in downstream tasks shows that\nas the scale of the model increases, the biological infor-\nmation in the sequence and the long-term dependence\ncan be captured more accurately. In addition, through a\nlarge number of controlled experiments, we found and\nsummarized some empirical rules for hyperparameter\nselection.\nModel\nPerformance Task\nP@L/5 P@L/2 P@L Fluorescence RH SS Q@3 SS Q@8 Stability\nhidden-512-layer-32-head-8 0.503 0.477 0.409 0.679 0.205 0.716 0.578 0.758\nhidden-768-layer-12-head-6 0.487 0.428 0.369 0.677 0.198 0.721 0.570 0.770\nhidden-768-layer-16-head-16 0.534 0.469 0.396 0.676 0.205 0.722 0.575 0.762\nhidden-768-layer-16-head-24 0.519 0.427 0.376 0.678 0.192 0.719 0.572 0.687\nhidden-1024-layer-12-head-16 0.572 0.490 0.419 0.676 0.209 0.729 0.584 0.744\nhidden-1024-layer-12-head-32 0.500 0.446 0.377 0.680 0.201 0.721 0.575 0.762\nhidden-2048-layer-12-head-16 0.676 0.576 0.495 0.677 0.266 0.752 0.614 0.732\nhidden-2048-layer-24-head-16 0.710 0.658 0.563 0.678 0.271 0.791 0.652 0.679\nhidden-2048-layer-24-head-8 0.673 0.600 0.531 0.674 0.262 0.762 0.624 0.785\nhidden-3072-layer-24-head-16 0.753 0.662 0.566 0.681 0.298 0.791 0.654 0.794\nTable 7: Results for comparative experiments.\nğ‘ƒ@ğ‘‹ means contact prediction precision@X.\nğ‘†ğ‘†ğ‘„ @ğ‘‹ means ğ‘‹ ğ‘ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦classification for secondary structure.\nğ‘…ğ» means remote homology.\n7\nREFERENCES\n[1] Ethan C. Alley, Grigory Khimulya, Surojit Biswas, Mohammed\nAlQuraishi, and George M. Church. Unified rational protein\nengineering with sequence-only deep representation learning.\nbioRxiv, 2019.\n[2] Mohammed AlQuraishi. Proteinnet: a standardized data set\nfor machine learning of protein structure. arXiv preprint\narXiv:1902.00249, 2019.\n[3] Ehsaneddin Asgari and Mohammad RK Mofrad. Continuous\ndistributed representation of biological sequences for deep\nproteomics and genomics. PloS one , 10(11):e0141287, 2015.\n[4] Helen M Berman, John Westbrook, Zukang Feng, Gary\nGilliland, Talapady N Bhat, Helge Weissig, Ilya N Shindyalov,\nand Philip E Bourne. The protein data bank. Nucleic acids\nresearch, 28(1):235â€“242, 2000.\n[5] James A. Cuff and Geoffrey J. Barton. Evaluation and improve-\nment of multiple sequence methods for protein secondary\nstructure prediction. Proteins: Structure, Function, and Bioin-\nformatics, 34(4):508â€“519, 1999.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. CoRR, abs/1810.04805,\n2018.\n[7] Sara El-Gebali, Jaina Mistry, Alex Bateman, Sean R Eddy, Au-\nrÃ©lien Luciani, Simon C Potter, Matloob Qureshi, Lorna J\nRichardson, Gustavo A Salazar, Alfredo Smart, Erik L L\nSonnhammer, Layla Hirsh, Lisanna Paladin, Damiano Pi-\novesan, Silvio C E Tosatto, and Robert D Finn. The Pfam\nprotein families database in 2019. Nucleic Acids Research ,\n47(D1):D427â€“D432, 10 2018.\n[8] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago,\nGhalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher,\nChristoph Angerer, Martin Steinegger, Debsindhu Bhowmik,\nand Burkhard Rost. Prottrans: Towards cracking the language\nof lifeâ€™s code through self-supervised deep learning and high\nperformance computing. CoRR, abs/2007.06225, 2020.\n[9] Naomi K Fox, Steven E Brenner, and John-Marc Chandonia.\nScope: Structural classification of proteinsâ€”extended, integrat-\ning scop and astral data and classification of new structures.\nNucleic acids research , 42(D1):D304â€“D309, 2013.\n[10] Hao Geng, Fangfang Chen, Jing Ye, and Fan Jiang. Applications\nof molecular dynamics simulation in structure prediction of\npeptides and proteins. Computational and Structural Biotech-\nnology Journal , 17:1162â€“1170, 2019.\n[11] JÃ¶rg Gsponer and Amedeo Caflisch. Molecular dynamics simu-\nlations of protein folding from the transition state.Proceedings\nof the National Academy of Sciences , 99(10):6719â€“6724, 2002.\n[12] Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian\nDallago, Dmitrii Nechaev, Florian Matthes, and Burkhard Rost.\nModeling the language of life â€“ deep learning protein se-\nquences. bioRxiv, 2019.\n[13] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term\nmemory. Neural Computation , 9(8):1735â€“1780, 1997.\n[14] M. Karplus and J. Kuriyan. Molecular dynamics and protein\nfunction. Proceedings of the National Academy of Sciences ,\n102(19):6679â€“6685, 2005.\n[15] Michael Schantz Klausen, Martin Closter Jespersen, Henrik\nNielsen, Kamilla Kjaergaard Jensen, Vanessa Isabell Jurtz,\nCasper Kaae Soenderby, Morten Otto Alexander Sommer, Ole\nWinther, Morten Nielsen, Bent Petersen, et al. Netsurfp-2.0: Im-\nproved prediction of protein structural features by integrated\ndeep learning. Proteins: Structure, Function, and Bioinformatics ,\n2019.\n[16] Jianzhu Ma. Protein structure prediction by protein align-\nments. CoRR, abs/1510.05682, 2015.\n[17] TomÃ¡s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.\nEfficient estimation of word representations in vector space.\nIn Yoshua Bengio and Yann LeCun, editors,1st International\nConference on Learning Representations, ICLR 2013, Scottsdale,\nArizona, USA, May 2-4, 2013, Workshop Track Proceedings , 2013.\n[18] John Moult, Krzysztof Fidelis, Andriy Kryshtafovych, Torsten\nSchwede, and Anna Tramontano. Critical assessment of meth-\nods of protein structure prediction (CASP)-Round XII.Proteins:\nStructure, Function, and Bioinformatics , 86:7â€“15, 2018.\n[19] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan,\nXi Chen, John Canny, Pieter Abbeel, and Yun S. Song. Evalu-\nating protein transfer learning with tape, 2019.\n[20] Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F.\nCanny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa\ntransformer. bioRxiv, 2021.\n[21] Alexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo,\nMyle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus.\nBiological structure and function emerge from scaling unsu-\npervised learning to 250 million protein sequences. bioRxiv,\n2019.\n[22] Gabriel J Rocklin, Tamuka M Chidyausiku, Inna Goreshnik,\nAlex Ford, Scott Houliston, Alexander Lemak, Lauren Carter,\nRashmi Ravichandran, Vikram K Mulligan, Aaron Chevalier,\net al. Global analysis of protein folding using massively parallel\ndesign, synthesis, and testing. Science, 357(6347):168â€“175,\n2017.\n[23] Karen S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Di-\nnara R Usmanova, Alexander S Mishin, George V Sharonov,\nDmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov,\nOnuralp Soylemez, et al. Local fitness landscape of the green\nfluorescent protein. Nature, 533(7603):397, 2016.\n[24] Tom Sercu, Robert Verkuil, Joshua Meier, Brandon Amos, Zem-\ning Lin, Caroline Chen, Jason Liu, Yann LeCun, and Alexander\nRives. Neural potts model. bioRxiv, 2021.\n[25] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick\nLeGresley, Jared Casper, and Bryan Catanzaro. Megatron-\nlm: Training multi-billion parameter language models using\nmodel parallelism, 2020.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need, 2017.\n[27] Sheng Wang, Jian Peng, Jianzhu Ma, and Jinbo Xu. Protein sec-\nondary structure prediction using deep convolutional neural\nfields. Scientific reports , 6, 01 2016.\n[28] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\nresidual networks. In 2017 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 636â€“644, 2017.\n8\n0 50000 100000 150000 200000 250000 300000 350000 400000\n# iterations\n6\n8\n10\n12\n14\n16ppl-validation\nppl-validation\n0 50000 100000 150000 200000 250000 300000 350000 400000\n# iterations\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8lm-loss-train\nlm-loss-train\n0 50000 100000 150000 200000 250000 300000 350000 400000\n# iterations\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8lm-loss-validation\nlm-loss-validation\nFigure 9: 32 layers, hidden size = 512, 8 attention heads\n0 100000 200000 300000 400000 500000 600000 700000 800000\n# iterations\n6\n8\n10\n12\n14ppl-validation\nppl-validation\n0 100000 200000 300000 400000 500000 600000 700000 800000\n# iterations\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6lm-loss-train\nlm-loss-train\n0 100000 200000 300000 400000 500000 600000 700000 800000\n# iterations\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6lm-loss-validation\nlm-loss-validation\nFigure 10: 12 layers, hidden size = 2048, 16 attention heads\n9"
}