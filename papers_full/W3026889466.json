{
  "title": "Table Search Using a Deep Contextualized Language Model",
  "url": "https://openalex.org/W3026889466",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1459615597",
      "name": "Chen, Zhiyu",
      "affiliations": [
        "Lehigh University"
      ]
    },
    {
      "id": "https://openalex.org/A2556203398",
      "name": "Trabelsi, Mohamed",
      "affiliations": [
        "Lehigh University"
      ]
    },
    {
      "id": "https://openalex.org/A4222944261",
      "name": "Heflin, Jeff",
      "affiliations": [
        "Lehigh University"
      ]
    },
    {
      "id": "https://openalex.org/A2752971064",
      "name": "Xu Yinan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222944260",
      "name": "Davison, Brian D.",
      "affiliations": [
        "Lehigh University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2398606196",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2140116426",
    "https://openalex.org/W2108223890",
    "https://openalex.org/W2971060546",
    "https://openalex.org/W2138556038",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2949847757",
    "https://openalex.org/W1512789266",
    "https://openalex.org/W18398563",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2799081691",
    "https://openalex.org/W2940927814",
    "https://openalex.org/W2083745421",
    "https://openalex.org/W1567631110",
    "https://openalex.org/W2085030399",
    "https://openalex.org/W2945465473",
    "https://openalex.org/W2342096063",
    "https://openalex.org/W2963759819",
    "https://openalex.org/W2165613971",
    "https://openalex.org/W3007429250",
    "https://openalex.org/W2092364718",
    "https://openalex.org/W2964181805",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W4233620316",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2768459074",
    "https://openalex.org/W2899286282",
    "https://openalex.org/W2788550262"
  ],
  "abstract": "Pretrained contextualized language models such as BERT have achieved\\nimpressive results on various natural language processing benchmarks.\\nBenefiting from multiple pretraining tasks and large scale training corpora,\\npretrained models can capture complex syntactic word relations. In this paper,\\nwe use the deep contextualized language model BERT for the task of ad hoc table\\nretrieval. We investigate how to encode table content considering the table\\nstructure and input length limit of BERT. We also propose an approach that\\nincorporates features from prior literature on table retrieval and jointly\\ntrains them with BERT. In experiments on public datasets, we show that our best\\napproach can outperform the previous state-of-the-art method and BERT baselines\\nwith a large margin under different evaluation metrics.\\n",
  "full_text": "Table Search Using a Deep Contextualized Language Model\nZhiyu Chen\nzhc415@lehigh.edu\nLehigh University\nBethlehem, PA, USA\nMohamed Trabelsi\nmot218@lehigh.edu\nLehigh University\nBethlehem, PA, USA\nJeff Heflin\nheflin@cse.lehigh.edu\nLehigh University\nBethlehem, PA, USA\nYinan Xu\nyinanxu@wezhuiyi.com\nZhuiyi Technology\nShenzhen, China\nBrian D. Davison\ndavison@cse.lehigh.edu\nLehigh University\nBethlehem, PA, USA\nABSTRACT\nPretrained contextualized language models such as BERT have\nachieved impressive results on various natural language process-\ning benchmarks. Benefiting from multiple pretraining tasks and\nlarge scale training corpora, pretrained models can capture complex\nsyntactic word relations. In this paper, we use the deep contextu-\nalized language model BERT for the task of ad hoc table retrieval.\nWe investigate how to encode table content considering the table\nstructure and input length limit of BERT. We also propose an ap-\nproach that incorporates features from prior literature on table\nretrieval and jointly trains them with BERT. In experiments on\npublic datasets, we show that our best approach can outperform\nthe previous state-of-the-art method and BERT baselines with a\nlarge margin under different evaluation metrics.\nCCS CONCEPTS\n• Information systems →Content analysis and feature se-\nlection; Retrieval models and ranking ; • Computing method-\nologies →Search methodologies.\nKEYWORDS\ntable search; neural networks; pretrained language model; informa-\ntion retrieval\nACM Reference Format:\nZhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davi-\nson. 2020. Table Search Using a Deep Contextualized Language Model. In\nProceedings of the 43rd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual\nEvent, China. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/\n3397271.3401044\n1 INTRODUCTION\nAs an efficient way to organize and display data, tables are broadly\nused in different applications: researchers use tables to present their\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401044\nexperimental results; companies store information about customers\nand products in spreadsheets; flight information display systems in\nthe airports show flight schedules to passengers in tables. According\nto Cafarella et al. [5], there are more than 14.1 billion tables on the\nWeb. Among those tables, many are very informative which means\nthey include relations and attributes of real-world entities, and have\nbeen used for a variety of downstream tasks. For example, tables like\nWikipedia infoboxes have been used to construct knowledge bases\nsince they are of high quality and consistent structure [1]. Data-to-\ntext models take tables from specific domains as input and transform\nthem into fluent natural language sentences such as sports news\n[44] and product descriptions [6]. With structure information and\nmetadata, tables store factual knowledge and therefore are also\nused to build question answering (QA) systems [34] .\nThe table retrieval task is related but different from the table QA\ntask. Both of them aim to satisfy users’ information need. QA models\nusually take a natural language question as input and aim to find\none or more specific answers. However, queries for table retrieval\nsystems may have ambiguous intent and usually consist of several\nkeywords. The returned tables from a table retrieval system in\nFigure 1a and 1c can be both positive samples for a table QA system.\nA user may want to know the list of all dog breeds in Figure 1a\nand the 2nd column of the table provides the relevant and accurate\ninformation. A user may ask the profiles of professional wrestlers\nin Figure 1c and the returned table contains that information. For\nthis example, all the cells provide informative content for the user.\nThe 2nd column tells who are professional wrestlers and the other\ncolumns provide context information. However, in Figure 1b, the\nquery has more ambiguous intent. The user may ask the results of\n2008 Beijing Olympic Games which means the returned table is a\nnegative sample for a QA system. If a user does not have a clear\nquestion and just wants to explore what he/she could find, then the\nreturned table is a positive sample for a table retrieval system. For\nthis example, the row that includes “Beijing” is relevant and the\nremaining rows are less useful. We note that the unit of relevant\ninformation in the table can be rows, columns or cells. Based on\nthis observation, we propose different methods to select row items,\ncolumn items and cell items from a table.\nIn this paper, we consider the task of ad hoc table retrieval where\ngiven a keyword query, a list of ranked tables are returned. In\nprevious studies of table retrieval, various features are used. Word\nlevel, phrase level and sentence level features are calculated by Sun\net al. [35]. Zhang et al. [ 49] use 23 hand-crafted features and 16\nembedding based features to train a random forest for pointwise\narXiv:2005.09207v2  [cs.IR]  26 May 2020\nPosition Breed Registrations\n1 Labrador Retriever 45,700\n2 English Cocker Spaniel 20,459\n… … …\nSearch Query: dog breeds\n(a) An example of a returned table in which one col-\numn is relevant to the query.\nCity Country Year …\nAthens Greece 1896 …\n… … … …\nBeijing China 2008 …\n… … … …\nSearch Query: 2008 Beijing Olympics\n(b) An example of a returned table in which one row is\nrelevant to the query.\nRank Name Sex …\n1 Harry Elliott M …\n2 Abe Coleman M …\n3 Angelo Savoldi M …\n… … … …\nSearch Query: professional wrestlers\n(c) An example of a returned table in which all cells are\nrelevant to the query.\nFigure 1: Three examples of returned tables reflecting differ-\nent relevant unit types.\ntable ranking. Recently, the pre-trained language model BERT [13]\nand its variants like RoBERTa [21] have achieved impressive results\non different natural language understanding tasks [40]. The self-\nattention structure and pre-training tasks enable BERT to learn\ncomplex linguistic features from a large corpus. Researchers from\nIR communities have applied BERT to ranking tasks and achieved\nnew state-of-the-art results on multiple benchmarks [23, 26, 45, 46].\nHere we apply BERT to the ad hoc table retrieval task. In previous\nwork, the input of BERT is either a single sequence or sequence pairs.\nThe question of how to effectively encode a structured document\ninto a BERT representation has not been previously explored. We\nconstruct input for BERT considering the structure of a table and\nthen combine BERT features with other table features together to\ntreat table retrieval as a regression task.\nWe summarize our contributions as the following:\n•We propose three content selectors to encode different table\nitems into the fixed-width BERT representation.\n•We experiment on two public datasets and demonstrate that\nour method achieves the best results and generalizes to other\ndomains.\n•We analyze the experiment results and discuss why the max\nsalience selector for row items performs the best among all\nother methods.\n•We analyze the fine-tuned BERT attention maps and embed-\ndings, and explain what information is captured by BERT.\n2 RELATED WORK\n2.1 Table Search\nZhang et al. [49] propose a semantic table retrieval (STR) method\nfor ad hoc table retrieval. They first map queries and tables into\na set of word embeddings or graph embeddings. Four ways to\ncalculate query-table similarity based on embeddings are then pro-\nposed. In the end, the resulting four semantic similarity features are\ncombined with other features into a learning-to-rank framework.\nTable2Vec [48] obtains semantic features in a similar way but uses\nembeddings trained from different fields. This method is built upon\nand does not outperform STR, so we only compare our methods\nwith STR instead of Table2Vec.\nUnsupervised methods for table search are also studied. Trabelsi\net al. [37] propose custom embeddings for column headers based on\nmultiple contexts for table retrieval, and find representing numeri-\ncal cell values to be useful. Chen et al. [8] utilize matrix factorization\nto generate additional table headers and then show that those gen-\nerated headers can improve the performance of unsupervised table\nsearch.\n2.2 Retrieval Models for Multifield Documents\nA table is often associated with important context information\nsuch as its caption and can be considered as a multifield document.\nTherefore, table search can be treated as a multifield document\nretrieval task and we introduce some related work in the area of\nmultifield document ranking.\nConsidering the structure of a document when designing re-\ntrieval models can usually improve retrieval results. It has been\nshown that combining similarities and rankings of different sec-\ntions can lead to better performance [43]. Ogilvie et al. [27] present\na mixture-based language model combining different document\nrepresentations for known-item search in structured document\ncollections. They find that document representations that perform\npoorly can be combined with other representations to improve the\noverall performance. Robertson et al. [31] introduce BM25F which\nis an extension of BM25 that combines original term frequencies in\nthe different fields in a weighted manner. A field relevance model is\nproposed by Kim and Croft [16] to incorporate relevance feedback\nfor field weights estimation. There are also supervised methods for\nmultifield document ranking. A Bayesian networks-based model for\nstructured documents is proposed by Piwowarski and Gallinari [29].\nKim et al. [15] propose a probabilistic model for semi-structured\ndocument retrieval. They calculate the mapping probability of each\nquery term and use it as a weight to combine the language models\nestimated from each field. Svore et al. [36] develop LambdaBM25, a\nmachine learning approach to BM25-style retrieval that learns from\nthe input attributes of BM25 and performs better than BM25F for\nmultifield document ranking. Zamani et al. [47] propose a neural\nranking model that learns an aggregated document representation\nfrom field-level representations and then uses a matching network\nto produce the final relevance score.\n2.3 BERT for Information Retrieval\nGiven the advances of deep contextualized language models for\nnatural language understanding tasks, researchers from IR commu-\nnity also begin to study BERT for IR problems. Nogueira et al. [25]\ndescribe an initial application of BERT for passage re-ranking task\nwhere the sentence-pair classification score is used. Nogueira et\nal. [26] then propose a multi-stage document ranking framework\nwhere BERT is used for pointwise and pairwise re-ranking. Yang et\nal. [46] show that treating social media text retrieval as a sentence\npair classification task can achieve new state-of-the-art results.\nThen they apply BERT to a dataset with longer documents and\nrank a document with linear interpolation of the original document\nscore and weighted top-n sentence scores. Similarly, Dai et al. [11]\nuse passage-level evidence to fine-tune BERT and consider all pas-\nsages from a relevant document as relevant. They first predict the\nrelevance score of each passage independently. The document rele-\nvance is the score of the first passage, the best passage, or the sum\nof all passage scores. BERT has also been applied to FAQ retrieval\ntask by Sakata et al [ 32] where given a user query, a question is\nscored by the combination of question-question BM25 score and\nquestion-answer BERT score. MacAvaney et al. [23] combine the\nBERT classification token with existing neural IR models. The ex-\nperiments show that this joint approach can outperform a vanilla\nBERT ranker.\nIR researchers also investigate the possible reasons why BERT\ncan have such substantial improvements for IR problems. Through\ncarefully designed experiments, Padigela et al. [28] show that BM25\nis more biased towards high-frequency terms which hurt its per-\nformance while BERT has a better ability to discover the semantic\nmeaning of novel terms in documents with respect to query terms.\nThey also find that BERT has less performance improvement com-\npared with BM25 for long queries. Dai et al. [11] demonstrate that\nBERT can take advantage of stopwords and punctuation in the\nqueries which is in contrast to traditional IR models. Qiao et al. [30]\nshow that BERT can be categorized into interaction-based IR mod-\nels because simply obtaining query and document representations\nfrom BERT independently and then computing their cosine simi-\nlarity results in performance close to random. They also find that\nBERT assigns extreme matching scores to query-document pairs\nand most pairs get either one or zero ranking scores.\nMany researchers (e.g., [11, 24, 26, 46]) find that the length limit\nof BERT causes difficulties in training. Mass et al. [24] specifically\nstudy the effect of passage length and segmentation configurations\non passage re-rank performance. They find that mid-sized (256\ntokens) inputs achieve the best results for the selected datasets. Dai\nand Callan’s method [11] to deal with long documents as mentioned\nbefore may result in noisy positive samples because for a relevant\ndocument, not all sentences are relevant to a query. The splitting\nand then aggregating methods in these approaches can increase\nthe training and inference cost several times. In this paper, we pre-\nselect the segments from the input with low-cost methods and then\nuse BERT for the downstream table retrieval task.\n3 PREREQUISITES\n3.1 BERT\nBERT [13], consisting of L layers of Transformer blocks, is a deep\ncontextual language model which has achieved impressive results\non various natural language processing tasks. Given a sequence\nof input token embeddings X = {x1, x2, ..., xn }, the Transformer\nblock at layer l outputs the contextualized embeddings (hidden\nstates) of input tokens Hl = {hl\n1, hl\n2, ..., hln }. The Transformer\nblock is originally proposed by Vaswani et al. [ 38] and each has\nthe same structure: multi-head self-attention followed by a feed-\nforward network.\nTrans f ormerl (Hl−1)\n= FF N(MH _Attn(Hl−1))\n= FF N(W [Attn1(Hl−1), ..., Attnm (Hl−1)])\n(1)\nMulti-head self-attention aggregates the output from m attention\nheads.\nWhen using BERT for downstream tasks, special tokens ([SEP]\nand [CLS]) are added into the input. For single sequence classifi-\ncation/regression tasks, [CLS] and [SEP] are added to the begin-\nning and end of the input sequence. For sequence-pair classifica-\ntion/regression, the two input sequences are concatenated by [SEP]\nand then processed the same as single sequence tasks. The embed-\nding of [CLS] from the last Transformer block is fed into a final\nclassification/regression layer.\n3.2 BERT Characteristics\nLimit on input length. BERT cannot take input sequences longer\nthan 512 tokens. In previous studies of BERT for long document\ntasks like text classification [ 7], the input tokens are truncated.\nBetter ways to preprocess the inputs beyond length limitation are\nworth studying since trivially throwing away part of the inputs\ncould lose important information. Transformer-XL [12] solves the\nfixed-length issue with recursion and relative position encoding.\nHowever, this method requires further pre-training and is only eval-\nuated on text generation tasks. Though we focus on table retrieval,\nour methods to alleviate the long sequence issue are off-the-shelf\nwithout any further training and can also be applied to other do-\nmains.\nThe secrets behind special tokens. Before BERT was pro-\nposed, neural models for NLP and IR tasks usually take the em-\nbeddings of all input tokens for training. While for BERT and its\nvariants, fine-tuning on the target tasks only requires an additional\nsoftmax layer on the top of the [CLS] embedding from the last\nlayer and the remaining embeddings are not used. The function\nof [SEP] is often disregarded, as when constructing the input of\nBERT, the role of [SEP] is just a symbol to mark the end or de-\nlimiter of a sequence. Recently, researchers begin to analyze why\nBERT is so effective for different tasks. Clark et al. [9] suggest that\n[SEP] might be used as a \"no-op\" sometimes and does not aggre-\ngate segment-level information. However, Ma et al. [22] show that\nusing the embedding of [SEP] instead of [CLS] can also achieve\ncomparable results, which indicates that [SEP] also learns contex-\ntualized information of the sequence. In our experiments, we study\nthe relationship between special tokens and other input tokens in\norder to explore what BERT embeddings learn after fine-tuning on\nthe target task.\n4 METHOD\nHere we define the task and then describe our method in detail.\n4.1 Task Definition\nIn ad hoc table retrieval, given a query q ∈Q usually consisting\nof several keywords q = {k1, k2, ..., kl }, our goal is to rank a set\nof tables T = {t1, t2, ..., tn }in descending order of their relevance\nscores with respect to q. A table is a set of cells arranged in rows\nand columns like a matrix. Each cell could a be single word, a real\nnumber, a phrase or even sentences. The first row of a table is the\nheader row and consists of header cells. In practice, tables from\nthe Web could have more complex structures [10]. In this paper,\nwe only consider tables that have the simplest structure since they\nare the most commonly used. Each table could have context fields\n{p1, ..., pk }depending on the source of the table. For example, a\ntable from Wikipedia can have a caption, its section title and page\ntitle.\nContent SelectorRegression Layer\nMLP\nrelevancescoreBERT\n[CLS]\ncuratedfeatures\nquerycontextfields\nselected items\ndata table\nFigure 2: Overview of the proposed model. Blue blocks are\nmodel components. Orange blocks are raw text of the input.\nGreen blocks are either manually curated features or out-\nputs from models (BERT and MLP).\n4.2 BERT for Table Retrieval\nWe show the overview of our framework which includes four com-\nponents in Figure 2. The content selector extracts informative items\n(rows, columns or cells) from a table. BERT is used to extract fea-\ntures fber t from the query, corresponding table context fields, and\nselected items. A neural network is used to transform additional\nfeatures va (if provided) to fa . Then fber t and fa are concatenated\ninto a single feature vector. This vector is fed into a regression layer\nto predict the relevance score. In the rest of this section, we describe\nthe model components in detail.\n4.2.1 Content Selector. As previously mentioned, BERT can only\ntake input sequences that are no longer than 512 tokens. But for\nmany tasks including table retrieval, the lengths of inputs can easily\nexceed that limit. To deal with the limit for various downstream\ntasks, inputs are typically truncated into valid lengths or multi-\nple instances for a single document are created [18, 45]. In open-\ndomain question answering and machine reading comprehension,\na single instance usually involves long documents or multiple docu-\nments, and the proposed methods usually have a select-then-extract\ntwo-stage schema [14, 19, 20, 41, 42]. Inspired by those works, we\npropose a select-then-rank framework for ad hoc table retrieval.\nFirst, we select a set of potentially informative items from a table.\nThen we pack the context fields of a table and its selected items\nas the final table representation. Finally we extract BERT features\nfber t for all the tables based on the new constructed representation.\nIn the ad hoc table retrieval task, we notice that there are three\ntypes of relevant tables in terms of the unit of the relevant infor-\nmation:\n•One or more columns are relevant to the query. For example,\nonly the second column is relevant to the query in Figure 1a;\n•One or more rows are relevant to the query. For example,\nonly the row that includes “Beijing” is relevant to the query\nin Figure 1b.\n•The relevant information is spread over the whole table. For\nexample, in Figure 1c, the table includes a list of records\nabout the entities asked by the query.\nTherefore, we slice a table t into a list of items {c1, ..., cm }, i.e., a\nlist of rows, columns or cells and select the top-ranked items for the\nfinal BERT input representation. Here we propose three methods\nto measure the salience score of a table item c:\n•Mean Salience: it assumes that the relevance signal can be\ncaptured by the similarity of query representation and item\nrepresentation. We use the average word embeddings to\nrepresent queries and items respectively.\nSALmean (c)= cosine (\nÍ\nw ∈c vw\nlc\n,\nÍ\nk ∈q vk\nlq\n)\n•Sum Salience: it assumes relevance signals between every\npair of query and item terms are useful for content selection.\nSALsum (c)=\nÕ\nk ∈q\nÕ\nw ∈c\ncosine (vk , vw )\n•Max Salience: it assumes that only the most salient signal be-\ntween any pair of query and item terms is useful for content\nselection.\nSALmax (c)= max\nk ∈q, w ∈c\ncosine (vk , vw )\nInstead of trivially truncating table information, we rank the items\nof a table and keep items with higher salience scores in the front.\n4.2.2 Encoding Table for BERT.Given a query q ∈Q, a table t ∈T ,\nthe context fields {p1, ..., pk }and selected items of t {c1, ..., cm },\nwe construct the final input sequence for BERT as\nS = [[CLS], q, [SEP ], p1, [SEP ], ..., pk , [SEP ], c1, [SEP ], ..., cm, [SEP ]]\nLike Hu et al. [ 13], we use WordPiece tokenization for input se-\nquences and the input representation of each token is constructed\nby summing its token embedding, segment embedding and position\nembedding. All the queries share the same segment embedding and\ncontext fields, selected items share another segment embedding. As\nillustrated in Section 3.1, we use the embedding of [CLS]from the\nlast layer as BERT features fber t.\n4.2.3 Feature Fusion and Prediction. Feature-based methods have\nshown impressive performance and achieved previous state-of-the-\nart results on ad hoc table retrieval [49]. When additional feature\nva ∈Rd for a query-table pair is available, we combine them with\nBERT features fber t by:\nfa = vaW1 + b1 (2)\nwhere W1 ∈Rd×d . Then fa and fber t are concatenated into single\nvector and fed to the final regression layer.\nf = [fa ; fber t] (3)\nscore = Reдression (f ) (4)\nWhen only BERT features are available, f equals fber t. A sim-\nple linear transformation is used as regression layer which means\nReдression (f )= f W2 + b2 where W2 ∈R(d+h)×1 and h is the size\nof BERT hidden states.\n4.2.4 Training. We use the pre-trained BERT-large-cased model\nwhich consists of 24 layers of Transformer blocks, 16 self-attention\nheads per layer and has a hidden size of 1024. Considering pro-\ncessing speed, the size of GPU memory, and the fact that BERT is\ngood for short text tasks, the maximum input length is set to 128.\nSince the selected items are at the end of the input (as described in\nSection 4.2.2) and ranked by their salience scores with respect to\nthe query, we assume the truncated part will have the least nega-\ntive impact with a given length constraint. Considering the dataset\nstatistics in Table 1, we limit the caption to 20 tokens, section title\nand page title to 10 tokens each, and table headers to 20 tokens.\nSince queries are short, we keep all the query tokens. As a result,\nwe leave about half of the space for table content. We fine-tune the\nframework by minimizing the Mean Square Error (MSE) between\nmodel predictions and gold standard relevance scores.1 We train\nthe model with 5 epochs and batch size of 16. The Adam optimizer\nwith learning rate of 1e-5 is used. We also use a linear learning rate\ndecay schedule with warm-up of 0.1. Our implementation is based\non code from an open source repository.2\n5 EXPERIMENTS\nIn this section, we aim to answer the following research questions:\nRQ1: What is the performance gain of BERT with content selection\nmethods, with respect to state-of-the-art performance?\nRQ2: Could BERT with content selection methods outperform\nstate-of-the-art performance without additional features?\nRQ3: Which content selection method/item type is the most effec-\ntive?\n5.1 Dataset Description\nWe use the WikiTables dataset created by Zhang and Balog [ 49]\nwhere the previous state-of-the-art method is proposed. The table\ncorpus is originally extracted from Wikipedia [2]. The context fields\ninclude page title and section title. From Figure 1b and Figure 1a\n1We also tried binary classification to predict relevance probabilities as in Sakata et\nal. [33] and found that regression is much better in our scenario.\n2https://github.com/huggingface/transformers\nTable 1: The length statistics of data provided by Zhang and\nBalog [49]. The length is calculated after WordPiece tok-\nenization.\nField Mean Max > 512 > 128\nquery 3.5 8 - -\ncaption 4.3 76 - -\npage title 5.6 26 - -\nsection title 3.3 22 - -\nheader 19.7 729 0.032% 2%\ntable 549.1 20545 24.2% 65.3%\nall 585.5 20605 27.3% 72.4%\nTable 2: The settings of all proposed methods, which use dif-\nferent item types and content selectors.\nMethod Name Item type Content Selector\nHybrid-BERT-Row-Sum Row Sum Salience\nHybrid-BERT-Row-Mean Row Mean Salience\nHybrid-BERT-Row-Max Row Max Salience\nHybrid-BERT-Col-Sum Column Sum Salience\nHybrid-BERT-Col-Mean Column Mean Salience\nHybrid-BERT-Col-Max Column Max Salience\nHybrid-BERT-Cell-Sum Cell Sum Salience\nHybrid-BERT-Cell-Mean Cell Mean Salience\nHybrid-BERT-Cell-Max Cell Max Salience\nwe can see that the first row of a table usually contains some high-\nlevel concepts and provides informative context. Therefore we also\nconsider the table header as a context field. When slicing the tables,\nwe still have table headers included. The queries are sampled from\nthe collections in [4, 39]. In total, they annotated 3120 query-table\npairs. The statistics of the corpus are shown in Table 1. We also use\nthe curated features proposed by Zhang and Balog [49] for feature\nfusion.\n5.2 Experimental Setup\nThe performance of table retrieval methods is evaluated with Mean\nAverage Precision (MAP), Mean Reciprocal Rank (MRR) and Nor-\nmalized Discounted Cumulative Gain (NDCG) at cut-off points 5, 10,\n15, and 20. To test significance, we use a two-tailed paired t-test and\nuse †/‡ to denote significance levels at p =0.05, 0.005 respectively.\nBased on Section 4.2.1, we have three strategies to calculate\nsalience scores of items and three ways to construct items (as a\nlist of columns, rows, or cells) from a table. We list all the methods\nsettings in Table 2. To obtain the salience scores, we use fastText\nword embeddings [3].3 Note that a different tokenization approach\nis used because fastText is not pre-trained on WordPiece tokenized\ncorpus. We replace all non-numerical and non-alphabet characters\nwith space and simply split sequences by space. Following the same\nexperimental setup in [49], five-fold cross-validation is used when\nevaluating different methods. We release our code on GitHub.4\n3https://github.com/facebookresearch/fastText/\n4https://github.com/Zhiyu-Chen/SIGIR2020-BERT-Table-Search\nTable 3: The superscript † shows statistically significant improvements for the method compared with all other methods.\nMethod Name MAP MRR NDCG@5 NDCG@10 NDCG@15 NDCG@20\nSTR 0.5711 0.6062 0.5762 0.6048 0.6102 0.6111\nHybrid-BERT-text 0.6003 0.6321 0.6023 0.6284 0.6322 0.6336\nHybrid-BERT-Rand-Row 0.6056 0.6356 0.6110 0.6294 0.6340 0.6350\nHybrid-BERT-Rand-Col 0.6105 0.6441 0.6094 0.6321 0.6388 0.6392\nHybrid-BERT-Rand-Cell 0.6124 0.6411 0.6117 0.6317 0.6381 0.6386\nHybrid-BERT-Cell-Mean 0.6104 0.6364 0.6148 0.6337 0.6385 0.6388\nHybrid-BERT-Cell-Max 0.6129 0.6410 0.6166 0.6349 0.6391 0.6395\nHybrid-BERT-Cell-Sum 0.6207 0.6473 0.6227 0.6397 0.6450 0.6454\nHybrid-BERT-Row-Mean 0.6196 0.6490 0.6216 0.6406 0.6456 0.6463\nHybrid-BERT-Row-Max 0.6311 0.6673 † 0.6361 0.6519 0.6558 0.6564\nHybrid-BERT-Row-Sum 0.6199 0.6487 0.6168 0.6385 0.6436 0.6445\nHybrid-BERT-Col-Mean 0.6108 0.6395 0.6168 0.6340 0.6406 0.6412\nHybrid-BERT-Col-Max 0.6086 0.6324 0.6133 0.6297 0.6357 0.6362\nHybrid-BERT-Col-Sum 0.6131 0.6399 0.6131 0.6308 0.6384 0.6390\n5.3 Baselines\nWe implement the following baseline methods:\n•Semantic Table Retrieval (STR) This is the method pro-\nposed by Zhang and Balag [49] which is the previous state-\nof-the-art method. It first represents queries and tables in\nmultiple semantic spaces. Then multiple semantic matching\nscores are calculated based on the representations of queries\nand tables. Pointwise regression using Random Forest is used\nto fit those semantic features combined with other features.\nLike the original STR implementation, we set the number of\ntrees to 1000 and the maximum number of features in each\ntree to 3.\n•Hybrid-BERT-text Only context fields are used and the\ntable is not encoded except the table headers which are also\nconsidered as a context field.\n•Hybrid-BERT-Rand-ColRandomly selecting column items\nwhen constructing the BERT input.\n•Hybrid-BERT-Rand-Row Randomly selecting row items\nwhen constructing the BERT input.\n•Hybrid-BERT-Rand-Cell Randomly selecting cells from\nthe table when constructing the BERT input.\nFor the BERT-based methods, we use the features proposed in [49]\nas va .\n5.4 Experimental Results\nWe summarize our experimental results in Table 3. We can see that\nall BERT-based models can achieve better results than semantic\ntable retrieval (STR). Even without encoding the tables, Hybrid-\nBERT-text can still outperform STR, which demonstrates that BERT\ncan extract informative features from tables and context fields for\nad hoc table retrieval. Randomly selecting columns, rows and cells\nhave a marginal improvement on Hybrid-BERT-text, indicating\nthat encoding the table content has the potential to further boost\nperformance. In addition, the differences in performance among\nrandomly selecting columns, rows and cells are not statistically\nsignificant. The answer to RQ1 is very straightforward: all BERT\nbased models with different content selection methods can perform\nbetter than the previous state-of-the-art method. Though the gain\nof performance is statistically significant at p = 0.005 level, BERT\nmakes the main contribution, since only encoding context fields\ncan achieve impressive results.\nNext, we discuss the impact of item type and content selector.\nComparing the results in Table 3, we observe that in general row\nitem based methods are better than cell item based methods, and\ncell item based methods are better than column item based methods.\nAmong all the methods, Hybrid-BERT-Row-Max achieves the best\nresults across all metrics compared with all other methods. The\nimprovement over all other methods is statistically significant at\n0.05 level for MRR, and statistically significant at 0.05 level for\nNDCG@5, NDCG@15 and NDCG@20 except for Hybrid-BERT-\nCell-Sum. It means that selecting rows that have the most significant\nsignals is an effective strategy to construct BERT input within\nthe length limit. In contrast to row items, column selection and\ncell selection based methods seem to be less effective. For several\ncases, content selection strategies for column/cell items even have\nworse performance than randomly selecting columns/cells. For\nexample, Hybrid-BERT-Col-Max has MRR of 0.6324 while Hybrid-\nBERT-Rand-Col has MRR of 0.6441. Different from row items, max\nsalience selector does not show superiority over other selectors for\ncolumn items and cell items. It is expected that Hybrid-BERT-Rand-\nCol has better performance than Hybrid-BERT-Rand-Row, because\na table is less likely to have more columns than rows, which means\nthe probability of a potential optimal column to be selected is higher\nthan that of a potential optimal row to be selected. For cell items,\nthe sum salience selector shows marginally better performance\nthan the other two selectors. And for column items, there is no\nclear best content selector but max salience selector seems to be\nthe least effective.\nThree types of items are coherent units of the table with different\ngranularities. A cell is the smallest unit compared with a row item\nor a column item. Usually, a column item is longer than a row item\ndepending on the layout of the table. After manually examining\nsome returned items, we find that cell item based methods are more\nTable 4: The setting of our methods where only BERT features are used.\nMethod Name MAP MRR NDCG@5 NDCG@10 NDCG@15 NDCG@20\nBERT-text 0.5958 0.6240 0.5972 0.6206 0.6283 0.6287\nBERT-Rand-Row 0.6005 0.6271 0.6063 0.6266 0.6310 0.6314\nBERT-Rand-Col 0.6067 0.6400 0.6093 0.6327 0.6374 0.6380\nBERT-Rand-Cell 0.6075 0.6358 0.6116 0.6287 0.6362 0.6369\nBERT-Cell-Mean 0.6056 0.6331 0.6017 0.6274 0.6340 0.6343\nBERT-Cell-Max 0.5967 0.6275 0.6013 0.6209 0.6299 0.6307\nBERT-Cell-Sum 0.6149 0.6436 0.6151 0.6345 0.6420 0.6424\nBERT-Row-Mean 0.6055 0.6365 0.6064 0.6314 0.6358 0.6363\nBERT-Row-Max 0.6277 0.6600 0.6274 0.6465 0.6517 0.6532\nBERT-Row-Sum 0.6113 0.6302 0.6077 0.6307 0.6356 0.6370\nBERT-Col-Mean 0.6026 0.6318 0.6079 0.6269 0.6334 0.6339\nBERT-Col-Max 0.6095 0.6398 0.6109 0.6319 0.6379 0.6385\nBERT-Col-Sum 0.6059 0.6257 0.6050 0.6260 0.6339 0.6343\nTable 5: Results using feature-based approaches. The superscript ‡ denotes statistically significant improvements over all\nbaseline methods.\nMethod Name MAP MRR NDCG@5 NDCG@10 NDCG@15 NDCG@20\nHybrid-BERT-text 0.6287 0.6546 0.6171 0.6489 0.6531 0.6536\nHybrid-BERT-Rand-Col 0.6590 0.6722 0.6481 0.6629 0.6692 0.6694\nHybrid-BERT-Rand-Row 0.6139 0.6418 0.6107 0.6345 0.6409 0.6411\nHybrid-BERT-Rand-Cell 0.6195 0.6554 0.6195 0.6382 0.6465 0.6466\nHybrid-BERT-Row-Max 0.6737‡ 0.7139‡ 0.6633‡ 0.6875‡ 0.6924‡ 0.6926‡\nHybrid-BERT-Col-Mean 0.6379 0.6582 0.6229 0.6449 0.6540 0.6542\nHybrid-BERT-Cell-Sum 0.6643 0.6806 0.6529 0.6686 0.6739 0.6740\nbiased towards returning items including query terms, while the\nmethods based on the other two item types are forced to include\nsome context information. Taking Figure 1c as an example, all the\nreturned items include the term “wrestler” which appears in the\nrightmost column that includes a list of short biographies of profes-\nsional wrestlers. However, for row items, other context information\nsuch as the names of the wrestlers are forced to be included. Since\ncolumn items are usually longer than row items, if the content\nselector fails to return the most relevant column item as the first\none, the model is less likely to achieve good performance. Based on\nour experiment results, we observe that max salience selector with\nrow items has the best balance between accuracy and robustness,\nwhich answers RQ3.\n6 DISCUSSION\nIn this section, we continue the discussion of our proposed methods.\n6.1 Ranking Only with BERT\nTo answer RQ2, we run the experiments that only use BERT fea-\ntures which means f equals fber t in Equation 3. The results are\nshown in Table 4 where the method names correspond to the ones\nin Table 3 except the STR baseline and the prefix “Hybrid-” is re-\nmoved. In all cases, performance decreases slightly when additional\nfeatures are not used. In answer toRQ2, without additional features,\nall the proposed methods including baselines can outperform STR.\nEven without encoding table content, BERT-text can still achieve\ngood performance which means the context fields are very impor-\ntant for ad hoc table retrieval. The conclusions are consistent with\nSection 5.4: sum salience selector is the best for cell items and max\nsalience selector with row items still performs the best when only\nBERT features are used.\n6.2 Generalization to Another Domain\nThough we conclude that the max salience selector with row items is\nthe best method, the conclusion may depend on the corpus. There-\nfore, we also conduct experiments on the dataset from another\ndomain. To do this, we use an open-domain dataset WebQuery-\nTable5 introduced by Sun et al. [35]. Unlike WikiTables where all\nthe tables are from Wikipedia, the tables in WebQueryTable are\ncollected from queried web pages returned by a commercial search\nengine. In total, 21,113 query-table pairs are manually annotated\nand the dataset is pre-split into training (70%), development (10%)\nand test (20%). In this scenario, no additional features are avail-\nable for this corpus so only BERT features are used. Additionally,\ntable caption, sub-caption and headers are used as context fields.\nThe preprocessing is the same with WikiTables. We do not use the\ndevelopment set since we do not search for hyperparameters. We\n5https://github.com/tangduyu/Table-Intelligence/tree/master/table-search\nTable 6: Results on WebQueryTable dataset.\nMethod Name MAP\nFeature + NeuralNet [35] 0.6718\nBERT-Rand-Cell 0.6414\nBERT-Row-Max 0.7104\ncalculate the MAP scores of our models which are also reported by\nSun et al. [35]. The results of the best BERT baseline method and\nthe proposed method are shown in Table 6. The final results are\nalso consistent with conclusions in Section 5.4—that max salience\nselector with row items is the best strategy.6 Therefore, we can see\nthat training BERT on row items with max salience selector is also\nan effective strategy for datasets in other domains, which makes\nthe answers to RQ2 and RQ3 more convincing.\n6.3 Feature-Based Approach of BERT\nIn Section 5.4, we use the fine-tuning approach that jointly fine-\ntunes the whole framework. In the experiment, we tried different\nmethods to incorporate additional features. For example, we can\ndirectly concatenate additional features without any transforma-\ntion with BERT features and feed the concatenated vector to the\nregression layer. We also tried to predict two relevance scores with\nBERT features and additional features separately, and then linearly\ntransform them into a weighted relevance score. However, all of\nthose variants perform worse than BERT-text. It is possible that\nBERT performance highly depends on the optimization strategy\nand adding other components for joint training can have negative\nimpact on the fine-tuning process. To avoid such a case, we adapt\nBERT to a feature-based approach. First we use the fine-tuning ap-\nproach to train BERT without additional features like in Section 6.1.\nThen we optimize the whole framework as in Section 5.4 except that\nBERT weights are not updated. The results are shown in Table 5.\nFor the three item types, we only include the results of models using\nthe best content selectors. All methods have significant improve-\nments compared with fine-tuned approaches. Among the baselines,\nHybrid-BERT-Rand-Col has the most improvement, which is even\nbetter than the best performance of BERT using content selectors\nfor column items. Hybrid-BERT-Row-Max still achieves the best\nperformance and the improvements over baselines are statistically\nsignificant at the level of p = 0.005.\nSo far, we observe that max salience selector with row items is\nthe best strategy to construct inputs for BERT. In the feature-based\napproaches, it is more obvious that sum salience selector is the best\none for cell items and mean salience selector is the best one for\ncolumn items.\n7 ANALYSIS OF BERT FEATURES\nThough BERT achieved new state-of-the-art results on various tasks,\nit is still unclear what are the exact mechanisms behind its success.\nIn this section, we dive into the analysis of BERT for the table\nretrieval task. For illustration purposes, the results presented in\nthis section are based on the weights of BERT-Row-Max. However,\n6We did not reproduce their method. We assume the results are comparable since the\ndataset is pre-split.\nwe observe similar patterns among different BERT-based methods\nand therefore the conclusions can also be applied to other methods.\n7.1 Self-Attention Patterns\nCompared to general scenarios where BERT is used for single-\nsequence or sequence-pair tasks, there are more than two sequences\ninvolved in the input of BERT for the table retrieval task and the\nsequence could have a lot of [SEP] tokens. BERT practitioners know\n[SEP] is a special token that is used as a delimiter of sequences. For\nour case, there could be a lot of [SEP] tokens in a single input and\nthe number of [SEP] tokens are different across different samples. In\nthis section we explore whether the self-attention patterns of BERT\nused in this paper which involve multiple sequences are different\nfrom a BERT model fine-tuned on single sequence/sequence pair\ntasks.\nWe draw all the attention maps of a random example from the\ntest set in Figure 3. We find all the types of self-attention maps\ncategorized in [17]: vertical, diagonal, vertical with diagonal, block\nand heterogeneous. We find that [SEP] embeddings in lower layers\nare attended or attending more differently than those in higher\nlayers. Taking the 1st self-attention head in the 4th layer as an\nexample, the 1st [SEP] embedding mainly attends to itself, while\nthe other [SEP] embeddings mainly attend to [CLS] embedding. In\ncontrast, the attentions for [SEP] tokens are very similar in higher\nlayers resulting in a lot of grid-like attention maps (head 1 in the\nlast layer as shown in Figure 3). We also quantitatively measure\nthe embeddings of different [SEP] tokens and calculate the smallest\ncosine similarity among all pairs of [SEP] in the same layer. The\nsmallest cosine similarity is 0.78 in the 1st layer but increases close\nto 1 in higher layers, which means [SEP] tokens have different\nembeddings in lower layers, and after layers of self-attention, they\nhave almost the same representations.\nBesides the types of attention maps described by Kovaleva et\nal. [17], we observe some attention maps that look like scatter\nplots, which include sparse small blocks (e.g., head 1 in the 4th\nlayer). This is because multiple sequences are included in a single\ninput separated by [SEP] and some attention heads have a strong\npreference to put attention on multiple sequences (inter-sequence\nattention). We also observe there are self-attention heads that show\nintra-sequence attention patterns. For example, caption and section\ntitle both attend to themselves a lot in head 9 of the 1st layer.\nQuery tokens attend a lot to themselves in head 5 of the 1st layer\n(right in Figure 3). The existence of intra-sequence and inter-\nsequence attention patterns may indicate that BERT can learn\nvarious sequence-level features through self-attention.\n7.2 BERT Embedding Comparison\nIn the experiments, only the [CLS] embedding in the last layer\nis used as BERT features and the rest are not utilized. Here we\nfurther analyze the relationships among different types of BERT\nembeddings.\nFor each sample in the testing set, we extract embeddings corre-\nsponding to query tokens and average them as the query represen-\ntation. We do the same for [SEP] and caption. Then we calculate\nthe cosine similarity between every two of [CLS], [SEP], query\nFigure 3: Middle figure includes all attention maps of a random test set example. Left figure shows the attention map of head\n1 in the last layer. Similar attentions for [SEP] tokens result in the grid-look. Right figure shows the attention map of head 5\nin the 1st layer with intra-sequence attention pattern. Attention weights with larger absolute values have darker colors.\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nCosine Similarity\nLayer\nCLS-Query CLS-SEP CLS-Caption SEP-Query SEP-Caption Query-Caption\nFigure 4: Average cosine similarity among different types of\ntokens in different layers.\nand caption. We show the average cosine similarity of testing sam-\nples at different layers in Figure 4. We observe that the patterns\nbetween special/query tokens and table/context field tokens are\nsimilar, which means in Figure 4, if we replace caption with other\ncontext fields or selected items, the general patterns do not change.\nFor example, “Query-Caption” is similar to “Query-Page title” and\n“CLS-Caption” is similar to “CLS-Page title”.\nIn the 1st layer, [SEP] is close to query and caption while [CLS]\nis far from [SEP], query and caption. From layer 2 to layer 8, we\nnote that [CLS] is very close to [SEP], which may indicate that\n[CLS] aggregates segment-level information through these layers.\nIn contrast, the similarities among [SEP], query and caption do not\nchange significantly from layer 2 to layer 14. It is interesting that\nfrom layer 23 to the last layer, query and [CLS] become closer but\nfar away from caption. In the last layer, [SEP] is closer to query\nthan [CLS], which may indicate [SEP] captures more query features\nthan [CLS].\n8 CONCLUSION\nWe have addressed the problem of ad hoc table retrieval with the\ndeep contextualized language model BERT. Considering the struc-\nture of a table, we propose three content selectors to rank table\nitems in order to construct input for BERT which effectively utilize\nuseful information from tables and overcome the input length limit\nof BERT to some extent. We combine BERT features and other tables\nfeatures to solve the table retrieval task as a pointwise regression\nproblem. Our proposed Hybrid-BERT-Row-Max method outper-\nforms the previous state-of-the-art and BERT baselines with a large\nmargin on WikiTables dataset. Through empirical experiments, we\nfind that using the max salience selector with row items is the\nbest strategy to construct BERT input. Overall, we also find that\nsum salience selector is the best for cell items. While for column\nitems, mean salience selector only seems to be the best when a\nfeature-based approach is used. We further show that the feature-\nbased approach of BERT is better than jointly training BERT with\na feature fusion component. We also conduct experiments on Web-\nQueryTable dataset and demonstrate that our method generalizes\nto other domains.\nOur analysis on fine-tuned BERT shows that various sequence-\nlevel features are captured by the self-attention of BERT and [CLS]\nembedding tends to aggregate sequence-level information, which\ncould explain why using it as features is effective for the ad hoc\ntable retrieval task. We also find that [SEP] embeddings from the\nlast layer of BERT are very close to query embeddings, which\nindicates that making use of [SEP] has the potential to further\nimprove the performance. Though the motivation behind this paper\nis that different content selection strategies should be used for\ndifferent queries, we do not explore how to design a model to\nchoose the best selector. In fact, it is possible that for different types\nof queries, we should choose different content selector. Future work\ncould design a framework that automatically chooses the strategy\nconsidering the query types. Besides, designing pretraining tasks\nfor tables and pretraining BERT on a large table collection could be\npromising to further improve the performance of BERT on table-\nrelated tasks such as table retrieval.\nACKNOWLEDGMENTS\nThis material is based upon work supported by the National Science\nFoundation under Grant No. IIS-1816325. The authors would like\nto thank the anonymous reviewers for valuable comments, and Ao\nLuo and Shengfeng Pan from Shenzhen Zhuiyi Technology Co., Ltd.\nfor useful discussion about BERT.\nREFERENCES\n[1] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,\nand Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In The\nsemantic web (ISWC) . Springer, 722–735.\n[2] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL:\nentity linking in web tables. In Proc. Int’l Semantic Web Conf. (ISWC) . 425–441.\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching Word Vectors with Subword Information. Transactions of the Association\nfor Computational Linguistics 5 (2017), 135–146.\n[4] Michael J Cafarella, Alon Halevy, and Nodira Khoussainova. 2009. Data integra-\ntion for the relational web. Proc. of the VLDB Endowment 2, 1 (2009), 1090–1101.\n[5] Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang.\n2008. Webtables: exploring the power of tables on the web. Proceedings of the\nVLDB Endowment 1, 1 (2008), 538–549.\n[6] Zhangming Chan, Xiuying Chen, Yongliang Wang, Juntao Li, Zhiqiang Zhang,\nKun Gai, Dongyan Zhao, and Rui Yan. 2019. Stick to the Facts: Learning towards\na Fidelity-oriented E-Commerce Product Description Generation. In Proceedings\nof the 2019 Conference on EMNLP and the 9th IJCNLP . 4958–4967.\n[7] Wei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and Inderjit Dhillon.\n2019. X-BERT: eXtreme Multi-label Text Classification with using Bidirectional\nEncoder Representations from Transformers. In Proceedings of NeurIPS Science\nMeets Engineering of Deep Learning Workshop .\n[8] Zhiyu Chen, Haiyan Jia, Jeff Heflin, and Brian D. Davison. 2020. Leveraging\nSchema Labels to Enhance Dataset Search. InEuropean Conference on Information\nRetrieval. Springer, 267–280.\n[9] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.\n2019. What Does BERT Look At? An Analysis of BERT’s Attention. In Black-\nBoxNLP@ACL.\n[10] Eric Crestan and Patrick Pantel. 2011. Web-scale table census and classification.\nIn Proceedings 4th ACM International Conference on Web Search and Data Mining\n(WSDM). ACM, 545–554.\n[11] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n985âĂŞ988. https://doi.org/10.1145/3331184.3331303\n[12] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell,\nQuoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language\nmodels beyond a fixed-length context. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics . 2978âĂŞ2988.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171âĂŞ4186.\n[14] Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. Retrieve, Read,\nRerank: Towards End-to-End Multi-Document Reading Comprehension. In Proc.\n57th An. Meeting of the Assoc. for Computational Linguistics (ACL) . 2285âĂŞ2295.\n[15] Jinyoung Kim, Xiaobing Xue, and W Bruce Croft. 2009. A probabilistic retrieval\nmodel for semistructured data. In Proc. European Conference on Information\nRetrieval (ECIR) . Springer, 228–239.\n[16] Jin Young Kim and W Bruce Croft. 2012. A field relevance model for structured\ndocument retrieval. In Proc. European Conf. on Info. Retrieval . Springer, 97–108.\n[17] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019.\nRevealing the Dark Secrets of BERT. In Proceedings of the 2019 Conference on\nEMNLP and the 9th IJCNLP (EMNLP-IJCNLP) . 4364–4373.\n[18] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob\nDevlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew\nDai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a\nBenchmark for Question Answering Research. TACL 7 (2019), 453–466.\n[19] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for\nWeakly Supervised Open Domain Question Answering. InProceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics . Association for\nComputational Linguistics, Florence, Italy, 6086–6096. https://doi.org/10.18653/\nv1/P19-1612\n[20] Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly\nsupervised open-domain question answering. In Proc. 56th Annual Meeting of the\nAssoc. for Computational Linguistics (Vol. 1: Long Papers) . 1736–1745.\n[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach. (2019). arXiv preprint\narXiv:1907.11692.\n[22] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2019.\nUniversal Text Representation from BERT: An Empirical Study. arXiv preprint\narXiv:1910.07973 (2019).\n[23] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:\nContextualized Embeddings for Document Ranking. InProc. 42nd Int’l ACM SIGIR\nConference on Research and Development in Information Retrieval . 1101–1104.\n[24] Yosi Mass, Haggai Roitman, Shai Erera, Or Rivlin, Bar Weiner, and David Konop-\nnicki. 2019. A Study of BERT for Non-Factoid Question-Answering under Passage\nLength Constraints. arXiv preprint arXiv:1908.06780 (2019).\n[25] Rodrigo Nogueira and Kyunghyun Cho. 2020. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2020).\n[26] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage\ndocument ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).\n[27] Paul Ogilvie and Jamie Callan. 2003. Combining document representations for\nknown-item search. In Proceedings of the 26th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval . ACM, 143–150.\n[28] Harshith Padigela, Hamed Zamani, and W Bruce Croft. 2019. Investigating\nthe Successes and Failures of BERT for Passage Re-Ranking. arXiv preprint\narXiv:1905.01758 (2019).\n[29] Benjamin Piwowarski and Patrick Gallinari. 2003. A machine learning model for\ninformation retrieval with structured documents. In International Workshop on\nMachine Learning and Data Mining in Pattern Recognition . Springer, 425–438.\n[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding\nthe Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).\n[31] Stephen Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple BM25\nextension to multiple weighted fields. In Proc. 13th ACM International Conference\non Information and Knowledge Management (CIKM) . 42–49.\n[32] Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and Sadao Kurohashi. 2019.\nFAQ Retrieval Using Query-Question Similarity and BERT-Based Query-Answer\nRelevance. In Proceedings of the 42nd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . 1113âĂŞ1116.\n[33] Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and Sadao Kurohashi. 2019.\nFAQ Retrieval Using Query-Question Similarity and BERT-Based Query-Answer\nRelevance. In Proc. 42nd Int’l ACM SIGIR Conf. on Research and Development\nin Information Retrieval (Paris, France). 1113âĂŞ1116. https://doi.org/10.1145/\n3331184.3331326\n[34] Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016.\nTable cell search for question answering. In Proceedings of the 25th International\nConference on World Wide Web . International World Wide Web Conferences\nSteering Committee, 771–782.\n[35] Yibo Sun, Zhao Yan, Duyu Tang, Nan Duan, and Bing Qin. 2019. Content-based\ntable retrieval for web queries. Neurocomputing 349 (2019), 183–189.\n[36] Krysta M Svore and Christopher JC Burges. 2009. A machine learning approach\nfor improved BM25 retrieval. In Proc. 18th ACM Conf. on Information and Knowl-\nedge Management (CIKM) . 1811–1814.\n[37] Mohamed Trabelsi, Brian D. Davison, and Jeff Heflin. 2019. Improved Table\nRetrieval Using Multiple Context Embeddings for Attributes. In Proc. IEEE Big\nData. 1238–1244.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Processing Systems . 5998–6008.\n[39] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei\nWu, Gengxin Miao, and Chung Wu. 2011. Recovering semantics of tables on the\nweb. Proceedings of the VLDB Endowment 4, 9 (2011), 528–538.\n[40] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. In Proceedings of ICLR .\n[41] Zhen Wang, Jiachen Liu, Xinyan Xiao, Yajuan Lyu, and Tian Wu. 2018. Joint Train-\ning of Candidate Extraction and Answer Selection for Reading Comprehension.\nIn Proceedings of the 56th Annual Meeting of the ACL . 1715–1724.\n[42] Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019.\nMulti-passage BERT: A Globally Normalized BERT Model for Open-domain\nQuestion Answering. In EMNLP-IJCNLP 2019 . ACL, Hong Kong, China, 5877–\n5881. https://doi.org/10.18653/v1/D19-1599\n[43] Ross Wilkinson. 1994. Effective retrieval of structured documents. In Proc. ACM\nSIGIR Int’l Conf. on Research and Dev. in Information Retrieval . Springer, 311–317.\n[44] Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in\ndata-to-document generation. arXiv preprint arXiv:1707.08052 (2017).\n[45] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019. End-to-end open-domain question answering with\nBERTserini. In NAACL-HLT (Demonstrations). 72–77.\n[46] Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of BERT for\nad hoc document retrieval. arXiv preprint arXiv:1903.10972 (2019).\n[47] Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, and Saurabh Tiwary.\n2018. Neural ranking models with multiple document fields. In Proc. 11th ACM\nInt’l Conf. on Web Search and Data Mining (WSDM) . 700–708.\n[48] Li Zhang, Shuo Zhang, and Krisztian Balog. 2019. Table2Vec: Neural Word and\nEntity Embeddings for Table Population and Retrieval. In Proc. 42nd Int’l ACM\nSIGIR Conf. on Research and Development in Information Retrieval . 1029–1032.\n[49] Shuo Zhang and Krisztian Balog. 2018. Ad hoc table retrieval using semantic\nsimilarity. In Proc. World Wide Web Conference (TheWebConf) . 1553–1562.\n[CLS]\ncompany\nincome\nstatements\n[SEP]\nLeading\ncompanies\n[SEP]\nP\n##ulp\nand\npaper\nindustry\nin\nCanada\n[SEP]\nLeading\ncompanies\n[SEP]Rank\nCompany\n2003 Total\nRevenue\n( C $ M )\n2003 Net Inc\n##ome\n(\nLoss\n) ( C $\n[SEP]\n[CLS]\ncompany\nincome\nstatements\n[SEP]\nLeading\ncompanies\n[SEP]\nP\n##ulp\nand\npaper\nindustry\nin\nCanada\n[SEP]\nLeading\ncompanies\n[SEP]\nRank\nCompany\n2003\nTotal\nRevenue\n(\nC\n$\nM\n)\n2003\nNet\nInc\n##ome\n(\nLoss\n)\n(\nC\n$\n[SEP]\nFigure 5: The attention map of 1st attention head at last layer.\nA ATTENTION MAPS\nHere we list all the attention maps mentioned in Figure 3.\n[CLS]\ncompany\nincome\nstatements\n[SEP]\nLeading\ncompanies\n[SEP]\nP\n##ulp\nand\npaper\nindustry\nin\nCanada\n[SEP]\nLeading\ncompanies\n[SEP]Rank\nCompany\n2003 Total\nRevenue\n( C $ M )\n2003 Net Inc\n##ome\n(\nLoss\n) ( C $\n[SEP]\n[CLS]\ncompany\nincome\nstatements\n[SEP]\nLeading\ncompanies\n[SEP]\nP\n##ulp\nand\npaper\nindustry\nin\nCanada\n[SEP]\nLeading\ncompanies\n[SEP]\nRank\nCompany\n2003\nTotal\nRevenue\n(\nC\n$\nM\n)\n2003\nNet\nInc\n##ome\n(\nLoss\n)\n(\nC\n$\n[SEP]\nFigure 6: The attention map of 1st attention head at 4-th layer.\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 7: The attention maps of all attention heads.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I186143895",
      "name": "Lehigh University",
      "country": "US"
    }
  ]
}