{
  "title": "Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models",
  "url": "https://openalex.org/W4385570601",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108131945",
      "name": "Lijing Wang",
      "affiliations": [
        "New Jersey Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099536826",
      "name": "Yingya Li",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2096515443",
      "name": "Timothy Miller",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2133231644",
      "name": "Steven Bethard",
      "affiliations": [
        "University of Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A705491505",
      "name": "Guergana Savova",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2076118331",
    "https://openalex.org/W4378767342",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W4235130247",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2951696358",
    "https://openalex.org/W3169445878",
    "https://openalex.org/W2293452551",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4287854737",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2108069432",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4287599161",
    "https://openalex.org/W2963286056",
    "https://openalex.org/W4226093202",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2137407193",
    "https://openalex.org/W3103341697",
    "https://openalex.org/W2943552823"
  ],
  "abstract": "The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease. In this work, we first provide a variance decomposition-based justification criteria to examine whether large pretrained neural models in a fine-tuning setting are generalizable enough to have low bias and variance. We then perform theoretical and empirical analysis using ensemble methods explicitly designed to decrease variance due to optimization. This results in essentially a two-stage fine-tuning algorithm that first ratchets down bias and variance iteratively, and then uses a selected fixed-bias model to further reduce variance due to optimization by ensembling. We also analyze the nature of variance change with the ensemble size in low- and high-resource classes. Empirical results show that this two-stage method obtains strong results on SuperGLUE tasks and clinical information extraction tasks. Code and settings are available: https://github.com/christa60/bias-var-fine-tuning-plms.git",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15746–15761\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTwo-Stage Fine-Tuning for Improved Bias and Variance for Large\nPretrained Language Models\nLijing Wang1∗ Yingya Li2∗ Timothy Miller2 Steven Bethard3 Guergana Savova2\n1New Jersey Institute of Technology, lijing.wang@njit.edu\n2Boston Children’s Hospital and Harvard Medical School, {firstname.lastname}@childrens.harvard.edu\n3 University of Arizona, bethard@email.arizona.edu\nAbstract\nThe bias-variance tradeoff is the idea that\nlearning methods need to balance model\ncomplexity with data size to minimize\nboth under-fitting and over-fitting. Recent\nempirical work and theoretical analyses with\nover-parameterized neural networks challenge\nthe classic bias-variance trade-off notion\nsuggesting that no such trade-off holds: as the\nwidth of the network grows, bias monotonically\ndecreases while variance initially increases\nfollowed by a decrease. In this work, we\nfirst provide a variance decomposition-based\njustification criteria to examine whether large\npretrained neural models in a fine-tuning\nsetting are generalizable enough to have\nlow bias and variance. We then perform\ntheoretical and empirical analysis using\nensemble methods explicitly designed to\ndecrease variance due to optimization. This\nresults in essentially a two-stage fine-tuning\nalgorithm that first ratchets down bias and\nvariance iteratively, and then uses a selected\nfixed-bias model to further reduce variance\ndue to optimization by ensembling. We also\nanalyze the nature of variance change with\nthe ensemble size in low- and high-resource\nclasses. Empirical results show that this\ntwo-stage method obtains strong results on\nSuperGLUE tasks and clinical information ex-\ntraction tasks. Code and settings are available:\nhttps://github.com/christa60/\nbias-var-fine-tuning-plms.git\n1 Introduction\nTransformer-based neural language models, such\nas Bidirectional Encoder Representations from\nTransformers (BERT) (Devlin et al., 2019), have\nachieved state-of-the-art (SOTA) performance for\na variety of natural language processing (NLP)\ntasks through the process of fine-tuning. Given\nan NLP task, the process often involves searching\nfor optimal pretrained models and hyperparameters\n∗ co-first authors\nwhile continuing to train the pretrained model on a\ndomain-specific dataset, with the aim of building\ngeneralizable and robust fine-tuned models.\nBased on the classic notion of the bias-variance\ntradeoff (Geman et al., 1992), where increasing\nmodel capacity decreases bias but increases vari-\nance (leading to a U-shaped test error curve), large\npretrained models (LPMs) should have large vari-\nance and overfit domain-specific data which is of-\nten sparsely labeled and extremely imbalanced for\nclassification. However, recent empirical work\nand theoretical analysis of neural networks chal-\nlenge this classic bias-variance trade-off notion\n(Neal et al., 2018; Yang et al., 2020). It has been\nsuggested that no such trade-off holds: as the\nwidth/depth of the network grows, bias monoton-\nically decreases while variance initially increases\nfollowed by a decrease. This is why transformer-\nbased LPMs often achieve better performance com-\npared to less complex models like long short-term\nmemory (LSTM)-based models or feature-rich\nmethods (e.g., support vector machines). In the\ncontext of the new bias-variance paradigm, these\nLPMs are seemingly complex enough to have low\nbias and variance, however, so far there has been\nno method to justify whether those SOTA models\nare generalizable and robust in solving a variety of\ndownstream tasks. In this paper, we (1) show that\nmany SOTA models are very sensitive to data and\ntraining randomness, and (2) provide a variance\ndecomposition-based justification method.\nWe also aim to improve model performance, re-\nducing the generalization error of LPMs by reduc-\ning their bias and variance. Recent findings in\nYang et al. (2020) show that the generalization er-\nror mainly comes from bias. Bias can be reduced\nby modifying the model architecture, e.g., mak-\ning the neural networks wider and deeper as in\ntransformer-based LPMs. However, pretraining\nnew or larger language models can be challenging\ndue to the technical and computational resource\n15746\nrequirements afforded by only a few institutions\n– a topic outside the scope of this paper. We fo-\ncus on the problem of reducing variance of neural\nmodels to further boost model performance, given\na fixed bias (i.e., a fixed pretrained model). En-\nsemble methods have been successful in boosting\npredictive performance of single learners (Ren et al.\n(2016) presents a comprehensive review) and thus\nare a promising venue to explore.\nWe propose a two-stage fine-tuning framework\nthat first justifies the generalization status of a se-\nlected pretrained model through a concrete metric,\nand then uses the fixed-bias model to further reduce\nvariance due to optimization through ensembling\ntechniques. To the best of our knowledge, we are\nthe first to provide such a metric and perform theo-\nretical and empirical analysis using ensembles for\nimproved bias and variance for LPMs. We conduct\nexperiments on the SuperGLUE tasks as well as\non information extraction from clinical text as it\nis a domain of high significance and presents data\nchallenges due to the limitations of sharing patient-\nrelated data. We believe our proposal is of interest\nto any unstructured domain where neural models\nare used. Specifically we make the following con-\ntributions:\n• We propose a two-stage fine-tuning algorithm\nfor improving bias and variance in the new\nbias-variance paradigm;\n• We provide a variance decomposition-based\nstrategy to examine whether LPMs in fine-\ntuning settings are generalizable enough to\nhave low bias and variance;\n• We perform theoretical and empirical anal-\nyses using ensembles explicitly designed to\ndecrease variance due to optimization while\nkeeping bias unchanged;\n• We analyze the nature of variance change\ndue to ensembling in low- and high-resource\nclasses in classification tasks;\n• We conduct comprehensive experiments and\nshow that the proposed two-stage method ob-\ntains strong results on SuperGLUE tasks and\ntwo clinical NLP tasks.\n2 Preliminaries\nIn this section we present the bias-variance decom-\nposition for squared loss in the new paradigm stud-\nied in Neal et al. (2018) and Yang et al. (2020).\nWe also present a further decomposition of vari-\nance. We denote f as a supervised learning\ntask such that f : X → Y, based on a train-\ning dataset S of m i.i.d. samples drawn from a\njoint distribution Dof (X,Y). The learning tar-\nget is to minimize the mean squared error E(f) =\nE(x,y)\n[\n∥y−f(x) ∥2]\n, where (x,y) ∼D.\nWe consider the predictor fθ as a random vari-\nable depending on the random variable Sfor train-\ning dataset and the random variable O for opti-\nmization randomness, where θ = A(S,O) ∈Rp\nrepresents the weights of neural networks produced\nby the learning algorithm A. pdenotes the dimen-\nsion of θ. The notations and their descriptions are\nshown in Table 3 in the Appendix.\n2.1 Bias-variance decomposition\nIn the context of classification of Cclasses, where\ny ∈ RC is represented as a one-hot vector and\nfθ(x) ∈RC denotes an output probability vector\nby the predictor, the risk Rof the learning algo-\nrithm can be decomposed into three sources of\nerrors (Geman et al., 1992):\nR= Enoise + Ebias + Evar (1)\nThe first term is the irreducible noise and comes\nfrom the intrinsic error of data independent of the\npredictor. The second is a bias term:\nEbias = E(x,y)\n[\n∥Eθ[fθ(x)] −E[y|x] ∥2]\n(2)\nThe third is a variance term\nEvar = ExVar(fθ(x))\n= Ex\n[\nEθ\n[\n∥fθ(x) −Eθ[fθ(x)] ∥2]](3)\nand can be further decomposed into the variance\ndue to optimization Varopt and the variance due to\nsampling Varsamp (Neal et al., 2018):\nVar(fθ(x)) = Varopt + Varsamp\n= ES[VarO(fθ(x)|S)]\n+ VarS(EO[fθ(x)|S])\n(4)\nwhere we denote the expectation of the decom-\nposed variance as EvarO = Ex[Varopt] and EvarS =\nEx[Varsamp].\n2.2 Theoretical findings from variance\ndecomposition\nAssuming the learning task is to learn a linear map-\nping y= θTx+ϵwhere ϵdenotes the noise random\n15747\nFigure 1: Empirical findings from Neal et al. (2018).\nvariable with E[ϵ] = 0 and Var(ϵ) = σ2\nϵ, and the\ninput vector is x ∈ Rp where p is the input or\nparameter dimensionality.\nIn this context, the over-parameterized setting is\nwhen p>m and the under-parameterized setting\nis when p≤m.\nThe theoretical findings in Neal et al.\n(2018) prove that Evar grows with p in the\nunder-parameterized case, while in the over-\nparameterized case, the variance does not grow\nwith pbut scales with the dimension of the data:\nExVar(fθ(x)) =\n{ p\nmσ2\nϵ for p≤m\nr\nmσ2\nϵ for p>m (5)\nwhere r = rank(X) and X ∈Rm×p denotes the\ndata matrix whose ith row is the training point xT\ni .\nFurthermore, EvarO vanishes as pincreases under\nthe linear squared regression assumption and EvarS\ndepends on critical parameter dimensionality d(p).\n2.3 Empirical findings from variance\ndecomposition\nFinding-I: as shown in the left panel of Figure 11,\nEbias decreases quickly and levels off once suf-\nficiently over-parameterized, while Evar is uni-\nmodal contrary to the classic theory. Finding-II:\nin the right panel of Figure 1, EvarO is significant\nand higher than EvarS in the under-parameterized\nregime. The two variances cross at a certain ponce\nsufficiently over-parameterized. However, empir-\nical pand mof the over-parameterized setting is\nnot strictly following the theoretical findings in sec-\ntion 2.2. Finding-III: in multi-layer models wherep\nis the width andqis the depth, given a fixedp, Ebias\ndecreases while Evar increases as qincreases (Yang\net al., 2020). These empirical findings hold for mul-\ntiple datasets in the original papers.\n1The two underlying subplots are from Netzer et al. (2011)\non the SVHN dataset\n3 Two-Stage Fine-Tuning for Improved\nBias and Variance\n3.1 Overview\nThe prevailing fine-tuning methods first build or\nselect an LPM and then fine-tune its parameters on\nthe downstream datasets. The SOTA setting for the\nLPM and its best hyperparameters for fine-tuning\nare chosen based on evaluation results, such as pre-\ncision (P), recall (R), F1 and accuracy scores, using\ngrid-search or random-search. Given a fine-tuning\ntask with a fixed training dataset, there is an upper\nlimit to the learning ability of an LPM which is\nhard to measure by traditional evaluation methods.\nFor a selected LPM, it is usually hard to decide\nwhen to stop searching for hyperparameters. Dif-\nferent from the prevailing fine-tuning setting, we\npropose a two-stage fine-tuning method. We first\nprovide a variance decomposition-based justifica-\ntion method to roughly measure the generalization\nability of a pretrained model w.r.t. the upper limit\nof its learning ability. In Stage-I, the SOTA set-\nting is built by ratcheting down bias and variance\nin an iterative way. The searching loop stops un-\ntil an acceptable performance appears or no more\nimprovement is observed. In Stage-II, given the\nSOTA setting built in Stage-I, the variance due to\noptimization is reduced by ensembling techniques.\nAlgorithm 1 outlines the procedure of the proposed\ntwo-stage fine-tuning method. The details of each\nstage are presented below.\n3.2 Stage-I: Justification of generalization\nability\nBased on the preliminaries in Section 2.3, assum-\ning an algorithm A(S,O) is fixed, the Ebias, Evar,\nEvarO, and EvarS changes as p, q, and mchange.\nTaking the crossing point (EvarO = EvarS) as a di-\nviding point, we define the generalization ability\nGp as:\nGp =\n{ Phase-I for EvarO >EvarS\nPhase-II for EvarO ≤EvarS\n(6)\nwhere Phase-I implies large bias and variance lead-\ning to large generalization error. Phase-II implies\nsmall bias and variance leading to small general-\nization error which may be good enough w.r.t. the\nupper limit of the learning ability of A.\nJustification criteria: After each evaluation, ifGpis\nin Phase-I, it is necessary to explore more hyperpa-\nrameter settings or new pretrained models until Gp\n15748\nis in Phase-II or an acceptable performance (e.g., P,\nR, F1) is achieved given the limited computing re-\nsources available in practice. Then fine-tuning can\nmove to Stage-II. If Gp is in Phase-II, the current\nsetting may be generalizable enough for the given\nlearning task so that the searching can be stopped.\nStage-II can be applied but is not necessary. We\nnote that similar to Finding-II in Section 2.3, Gp\ncannot be determined directly based on p, q, and\nm. This breakdown provides a high-level guideline\nfor evaluating the generalization of LPMs in an\nempirical way.\n3.3 Stage-II: Ensembling to reduce variance\nEnsembles have been proven to be able to improve\nprediction accuracy and consistency of any learn-\ning models in Bonab and Can (2019); Wang et al.\n(2020). Bagging-based ensembles which are com-\nmonly used in various learning tasks have been\nproven to be able to reduce Evar while keeping\nEbias unchanged. However, no theoretical analysis\nhas been discussed in the context of the variance\ndecomposition paradigm. In Stage-II, we focus on\nbagging-based ensembles to further improve the\nmodel performance by reducing EvarO while keep-\ning EvarS unchanged. Applying Stage-II can either\nmove a model from Phase-I to Phase-II though en-\nsembling, i.e., reducing EvarO until EvarO ≤EvarS;\nor further improve a model’s generalization ability\nfrom Phase-II by reducing EvarO.\nWe perform empirical analysis in Section 4 and\ntheoretical analysis in Section 5 to investigate why\nand how bagging-based ensembles can guarantee\nsuch improvements in this context. We also analyze\nthe nature of variance change with the ensemble\nsize in low- and high-resource classes in classi-\nfication tasks. Boosting ensembles have a more\ncomplex behaviour thus are out of scope for this\npaper.\n4 Experiments\n4.1 Data and models\nWe conduct experiments on the SuperGLUE tasks\nand two major clinical information extraction\ndatasets. The data processing and statistics and\nhyperparameter settings are shown in Appendix\nTable 4 and Table 5. Their brief descriptions are:\n• SuperGLUE (Wang et al., 2019) is a bench-\nmark dataset designed for a more rigorous test\nof general-purpose language understanding\nAlgorithm 1: Pseudocode of the two-stage\nfine-tuning method.\nInput: S: training dataset; A: optimization\nalgorithm; N: number of ensemble single\nlearners; F: majority voting for ensemble;\nOutput: ζ: ensemble learner\n/* Stage-I */\n1 Gp ←Phase-I\n2 E∗ ←0\n3 while (Gp = Phase-I) or (E∗ is not acceptable) do\n4 Choose a pretrained model f and an initialization\nseed O.\n5 θ←A(S,O,f )\n6 Compute EvarO and EvarS by Equation 4.\n7 Gp ←Phase-I if EvarO >EvarS otherwise\nPhase-II\n8 E∗ ←score(fθ) // P,R,or F1.\n9 ζ := f\n/* Stage-II */\n10 if (Gp ̸= Phase-II) or (E∗ is not satisfied) then\n11 ξ←∅ // The set of ensemble\ncomponents.\n12 N is set to be ≥EvarO /EvarS\n13 while len(ξ) < N do\n14 Resample training and validation sets Si\nfrom S.\n15 Train f∗ on Si using snapshot learning and\nsave N trained learners f∗\nθ1 ,...,f ∗\nθN\nwhere θi = A(Si,O∗,f∗).\n16 Select ltrained learners from the saved ones\nby applying pruning algorithm (Wang\net al., 2020).\n17 ξ∪f∗\nθ1 ,...,f ∗\nθl\n18 ζ := F(ξ)\nsystems after GLUE (Wang et al., 2018). We\nreplicate the scores on dev set2, and select six\ntasks (BoolQ, CB, RTE, MultiRC, WiC, and\nCOPA). The selected tasks cover question an-\nswering (QA), natural language inference, and\nword sense disambiguation. The SOTA setting\nis based on the setting in Liu et al. (2019) us-\ning roberta-large as the pretrained model.\n• THYME (Temporal Histories of Your Med-\nical Events) corpus (Styler IV et al., 2014)\nfor temporal relation extraction, consisting\nof 594 de-identified clinical and pathol-\nogy notes on colon cancer patients. We\nuse the THYME+ version of the corpus\n(Wright-Bettner et al., 2020). There are\n10 classes of extremely imbalanced class\ndistribution. The SOTA setting is based\non the setting in Lin et al. (2021) using\nPubmedBERTbase-MimicBig-EntityBERT as\nthe pretrained model.\n2https://github.com/pytorch/fairseq/\ntree/master/examples/roberta\n15749\n• 2012 i2b2 Temporal Relations (Sun et al.,\n2013) consists of 394 training de-identified\nreports, 477 test de-identified reports, and\n877 unannotated reports. There are 3\nclasses of slightly imbalanced class distri-\nbution. The SOTA setting is based on\nthe setting in Haq et al. (2021) using\nBioBERT-base-cased-v1.1 as the pretrained\nmodel.\n4.2 Metrics and settings\nWe use an NVIDIA Titan RTX GPU cluster of 7\nnodes for fine-tuning experiments through Hug-\ngingFace’s Transformer API (Wolf et al., 2020)\nversion 4.13.0. We leverage the run_glue.py\npytorch version as our fine-tuning script. Unless\nspecified, default settings are used in our experi-\nments. Due to differences in the fine-tuning script\nand some missing settings not provided by the orig-\ninal authors, we were unable to reproduce the exact\nSOTA scores but we achieved scores close to the\nSOTA ones. Our implementation are denoted as\nreplicated-SOTA (RSOTA). We compare our imple-\nmentation and reference SOTA scores in Appendix\nTable 7. We use the RSOTA settings as the starting\npoint to conduct the experiments. We use Ebias,\nEvar, EvarO, and EvarS for Stage-I, and adopt clas-\nsic evaluation metrics (P, R, and F1) for Stage-I\nand Stage-II. For the purposes of consistency, we\nreport P, R, and F1 of SuperGLUE tasks for con-\nsistency with the reported results for the THYME\nand i2b2 tasks in the experiment results. Accuracy\nscores are reported in Appendix Table 7.\n4.3 Experimental design\nThere are two stages in the proposed method. For\nStage-I, we use 5 random seeds for the randomness\nover data samples Sand 5 for the randomness over\ninitialization O, resulting in a total of 25 fine-tuned\nmodels. The averages over data samples are per-\nformed by taking the training set Sand creating 5\nbootstrap replicate training/validation splits with\nthe same class distribution. The bias expectation in\nEquation 2 is estimated as the averages over both\nSand O. The variance decomposition is estimated\nbased on Equation 4. More specifically, EvarO is\nestimated as the averages over S of the variance\nover O, and EvarS is estimated as the variance over\nS of the averages over O. Furthermore, we also\napply RoBERTa-base-uncased (RBU) as the pre-\ntrained model for each fine-tuning task using the\nRSOTA setting except for pretrained models. Their\nEbias Evar EvarO EvarS Gp F1\nBoolQ-RBU 162 5.4 3.9 1.6 Phase-I 77.8\nBoolQ-RSOTA 142 9.9 6.2 3.7 Phase-I 84.3\nCB-RBU 175 0.2 0.1 0.1 - 49.2\nCB-RSOTA 149 1.7 1.5 0.2 Phase-I 62.0\nRTE-RBU 176 11.4 8.0 3.4 Phase-I 74.0\nRTE-RSOTA 153 13.2 11.2 2.1 Phase-I 83.5\nMultiRC-RBU 164 5.9 4.6 1.3 Phase-I 78.5\nMultiRC-RSOTA 178 13.3 10.5 2.8 Phase-I 74.7\nWiC-RBU 212 5.5 4.2 1.3 Phase-I 63.6\nWiC-RSOTA 199 12.7 10.1 2.5 Phase-I 70.3\nCOPA-RBU 250 0.0 0.0 0.0 - 38.0\nCOPA-RSOTA 185 4.3 3.9 0.5 Phase-I 81.2\nTHYME-RBU 81 0.17 0.14 0.02 Phase-I 57.0\nTHYME-RSOTA 80 0.09 0.07 0.02 Phase-I 61.8\ni2b2-RBU 150 0.76 0.62 0.14 Phase-I 76.8\ni2b2-RSOTA 152 0.73 0.58 0.14 Phase-I 78.1\nTable 1: Bias and variance of different pretrained mod-\nels on the SuperGLUE, THYME and i2b2 datasets.\nRoBERTa-base-uncased (RBU). Values ofEbias, Evar,\nEvarO, and EvarS are relative values to 0.001. F1 scores\nare the means over 5 random seeds for initialization.\ndescriptions are shown in Appendix Table 6. To\nreplicate SOTA scores and obtain RSOTA settings\nfor each task, we conduct hyperparameter search-\ning in an iterative way. This process is considered\nas the experiment of Stage-I.\nFor Stage-II, any bagging-based ensemble al-\ngorithms are feasible. In our preliminary exper-\niments (Wang et al., 2022), we have shown that\nthe dynamic snapshot ensemble algorithm (Wang\net al., 2020), which we call ENS in this paper,\nworks better than vanilla bagging ensembles. ENS\nis a bagging-based ensemble explicitly designed to\nreduce variance over optimization-related hyperpa-\nrameters in one framework, with the aim of build-\ning computationally efficient strategies to boost\nmodel performance on top of any given setting with\na guarantee (i.e., simple bagging ensemble cannot\nguarantee an improvement). In our implementa-\ntion, we employ ENS. The ensemble size is 5 and\nmajority voting is used to generate ensemble predic-\ntions. To explore the ensemble impact on low- and\nhigh-resource classes, we compute and compare\nperformance improvements of each class from the\nextremely imbalanced THYME dataset. To investi-\ngate the impact of ensemble size on improving the\nmodel performance of imbalanced classes, we also\nevaluate performance of individual classes using\nENS of size 1 to 10. We compute 95% confidence\nintervals for these estimates using bootstrapping\nover 5 samples. More details are in Appendix B.\n15750\nBoolQ CB RTE MultiRC\nMethod P R F1 P R F1 P R F1 P R F1\nRSOTA 84.58 84.10 84.34 60.76 63.80 62.06 83.94 83.40 83.54 73.02 77.54 74.68\n(±0.34) ( ±0.32) ( ±0.30) ( ±19.0) ( ±15.0) ( ±16.9) ( ±0.89) ( ±0.94) ( ±0.94) ( ±21.8) ( ±13.5) ( ±18.76)\nENS 84.96 84.38 84.74 92.68 93.14 92.66 86.00 85.38 85.48 82.14 82.30 82.16\n(±0.34) ( ±0.58) ( ±0.47) ( ±1.3) ( ±2.8) ( ±1.2) ( ±0.79) ( ±1.16) ( ±1.10) ( ±1.41) ( ±1.41) ( ±1.3)\nIPV 0.45% 0.33% 0.47% 52.53% 45.99% 49.31% 2.45% 2.37% 2.32% 12.49% 6.14% 10.02%\nWiC COPA THYME i2b2\nMethod P R F1 P R F1 P R F1 P R F1\nRSOTA 72.06 70.74 70.30 82.54 81.72 81.24 66.6 58.2 61.8 78.3 76.9 78.1\n(±0.12) ( ±1.7) ( ±1.9) ( ±11.40) ( ±12.10) ( ±12.4) ( ±1.02) ( ±1.47) ( ±1.25) ( ±1.56) ( ±0.98) ( ±1.24)\nENS 72.18 71.30 70.98 93.84 93.80 93.60 72.9 60.1 65.9 80.5 78.3 79.3\n(±0.17) ( ±0.36) ( ±0.42) ( ±0.43) ( ±0.48) ( ±0.48) ( ±0.86) ( ±1.16) ( ±0.95) ( ±1.23) ( ±0.79) ( ±0.97)\nIPV 0.17% 0.79% 0.97% 13.69% 14.78% 15.21% 9.46% 3.26% 6.63% 2.81% 1.82% 1.54%\nTable 2: Ensemble model performance. Test set results with average and 95% confidence interval of 5 random\nsamples, where the t value for 95% confidence is 2.776. ENS denotes ensemble of 5 components. IPV denotes\nimprovement percentage by ENS compared with RSOTA.\n4.4 Justification results\nTable 1 shows Ebias, Evar, EvarO, and EvarS com-\nputed on the datasets with different pretrained mod-\nels. It is noted that in our experiment, we are\nnot applying the algorithm for Stage-I to ratchet\ndown bias and variance in an iterative manner. The\ngoal of this table is to analyze both RBU and\nRSOTA models for the bias and variance trends\ndiscussed in Section 2. Interestingly, we observe\nthat EvarO >EvarS for all datasets and models ex-\ncept for CB-RBU and COPA-RBU where models\nare not well trained given that F1 score is around\n0.5 indicating random guess. This implies that\nthe vast majority of the SOTA models we experi-\nmented with are in Phase-I (i.e., not generalizable\nenough for their tasks), which is contrary to our\nintuition that these transformer-based models are\ncomplex enough given the moderate sized labeled\ndatasets. It is also observable that Ebias is much\nlarger than Evar indicating that the model perfor-\nmance is dominated more by bias than by variance.\nFor SuperGLUE tasks, with the same hyperparame-\nter setting (i.e., A(S,O)), the RSOTA models (i.e.,\nlarger pand qthan RBU models) achieve smaller\nEbias but larger Evar than RBU models except for\nMultiRC. The change of Evar mainly comes from\nthe change of EvarO. As Finding-I in Section 2.3\nthat Ebias decreases while Evar is unimodal, our\nobservation implies that RBU models are before\npeak and long way toward Phase-II while RSOTA\nmodels get closer to Phase-II than RBU models.\nThe exception is the result on the MultiRC dataset\nwhich is QA corpus listing a passage which con-\nsists of text, a question, and multiple answers to\nthat question each with a true/false label. Although\nMultiRC represents a variety of question types (e.g.\nyes/no, factual, etc.), the passages are not annotated\nfor question type. As explained in Appendix sec-\ntion B.1., we represented the QA pairs within each\npassage as text, question, answer, label in-\nstances and sampled from these instances. Using\nthis instance-based sampling likely leads to sam-\nples not stratified by question types, therefore not\nnecessarily representative. This probably explains\nthe better mean F1 for MultiRC-RBU as compared\nto the mean F1 for MultiRC-RSOTA in Table 1\n(different samples are created for each run). How-\never, when we drill down to the best model F1 for\nMultiRC-RBU and MultiRC-RSOTA, the results\nare 78.9 and 85.4 F1-scores respectively, which\nsupports the trend in Table 1.\nFor the SuperGLUE tasks, the bias and variance\nof RBU models and RSOTA models are shown to-\ngether to illustrate a trend (like Fig. 1) as pand q\nare the only variables. However for THYME and\ni2b2 tasks, similar trends could not be interpreted\nsince the RSOTA models are pretrained with do-\nmain specific corpora while the RBU models are\npretrained with general corpora. This implies that\nfor fine-tuning tasks such as temporal relation ex-\ntraction, other factors (e.g., domain corpora used\nto pretrain models) may have larger impact than\nthe model complexity. Our observations are con-\nsistent with Finding-II that empirical pand mset-\nting is not strictly following theoretical findings\nwhich are under linear squared regression assump-\ntion. This also indicates that p, q, and mcannot be\nused to measure Gp empirically. On the other hand,\n15751\nour variance decomposition-based method does not\nrely on p, q, and m, therefore it provides the basis\nfor a more generalized justification method.\n4.5 Ensemble results\nTable 2 presents P, R, and F1 scores of RSOTA and\nENS methods on all datasets. Similar to prior stud-\nies (e.g. Zhang et al., 2020; Du et al., 2022), results\nfor the SuperGLUE tasks are reported on the dev\nset. The accuracy scores for each task are presented\nin the Appendix Table 7. Compared to the RSOTA\nsetting, the ENS method boosts performance on all\ndatasets, with the largest gains of 49.3% and 15.2%\nrelative F1 improvements on the low-resource CB\nand COPA datasets respectively. In Section 5, we\nanalyze why ensembles work from the variance de-\ncomposition perspective, which provides insights\ninto how ensembles help reduce EvarO and lead to\nbetter prediction accuracy.\n4.6 Ensemble impact on low- and\nhigh-resource classes\nWe further investigate the improvements on low-\nresource datasets (e.g. CB and COPA). To elim-\ninate all interference from p, q, A(S,O), pre-\ntrained models and only keep m as the vari-\nable, we tease apart the results of the extremely\nunbalanced THYME dataset and analyze the\nperformance on each class. Its most frequent\nclasses (i.e., high-resource classes) are CON-\nTAINS (2895), OVERLAP (2359), and BEFORE\n(2241); and the least frequent classes (i.e., low-\nresource classes) are NOTED-ON (140), BEGINS-\nON (160), and ENDS-ON (244). The initial\nF1 scores are: CONTAINS-0.776, OVERLAP-\n0.539, and BEFORE-0.469; NOTED-ON-0.618,\nBEGINS-ON-0.608, and ENDS-ON-0.695. In Fig-\nure 2, we show absolute improvement and improve-\nment percentage of F1 with various ensemble size\nN (compared with single learners i.e., N = 1 ).\nThese values are computed based on the mean with\n95% confidence interval over 5 random samples\nfor each class and each ensemble size. It is ob-\nservable that given a fixed N, the performance\nimprovements by F1 scores on the low-resource\nclasses – NOTED-ON (brown), BEGINS-ON (red),\nand ENDS-ON (orange) – are larger than the ones\nof high-resource classes. The difference becomes\nlarger as N increases. The scales of improvement\nare not affected by the initial results; i.e., the larger\nimprovements on low-resource classes are not due\nto lower initial F1 scores. This is an interesting ob-\nservation and may introduce a new solution for im-\nproving performance of imbalanced datasets. More\nsimilar results on P and R are shown in in Ap-\npendix Fig. 3. We explore theoretical insights into\nthese observations in Section 5.\n5 Discussion and Theoretical Analysis\n5.1 Basic statistics\nLet X1,X2,··· ,XN be a random sample from\na population with mean µ and variance σ2 and\n¯X = 1\nN\n∑N\ni=1 Xi. Then the following two items\nhold.\na: E[ ¯X] = E[Xi] = µ\nb: Var( ¯X) = 1\nNVar(Xi) = 1\nNσ2\n5.2 Ensemble in bias-variance decomposition\nWe work in the context of bagging-based ensem-\nbles, assuming the ensemble predictor ¯f(x) is\n¯f(x) = 1\nN\n∑N\ni=1 fi(x) is the averaging of N sin-\ngle learners trained with different samples ofSand\nO. Based on the basic statistics, the Ebias of ¯f(x)\nin Equation 2 is unchanged while the Evar of ¯f(x)\nin Equation 3 decreases by 1\nN. Furthermore, we\nhave:\nEvarO = Ex\n[\nES[VarO( ¯fθ(x)|S)]\n]\n= 1\nNEx[ES[VarO(fθ(x)|S)]]\n(7)\nand:\nEvarS = Ex\n[\nVarS(EO[ ¯fθ(x)|S])\n]\n= Ex[VarS(EO[fθ(x)|S])] (8)\nwhich indicates that EvarO reduces while EvarS\nkeeps unchanged as the ensemble size N increases.\nEvarO vanishes when N is sufficiently large. The\nimprovement of the variance by ensembling comes\nfrom the reduction of the variance due to optimiza-\ntion.\nAs mentioned in Section 2.2 that under linear\nsquared regression assumption, EvarO vanishes as\npincreases and EvarS depends on critical param-\neter dimensionality d(p). In this paper, we also\nproved that EvarO vanishes as N increases. Given\nthat pretraining LPMs with larger pand/or qis ex-\ntremely difficult, increasing N is a much better\nway for improving performance of LPMs. This\nalso proves the effectiveness of Stage-II in our pro-\nposed two-stage fine-tuning method. To ensure that\na fine-tuned LPM can move from Phase-I to Phase-\nII, the ensemble size N in Stage-II should be set to\na value that is larger or equal to\nEvarO\nEvarS\n.\n15752\n(a) F1 absolute improvement\n (b) F1 percentage improvement\nFigure 2: Performance improvement of low-resource (NOTED-ON, BEGINS-ON, ENDS-ON) and high-resource\nclasses (CONTAINS, OVERLAP, BEFORE) on THYME data. We show absolute improvement (a) and improvement\npercentage (b) (compared with single learners i.e., N = 1) with F1.\n5.3 Ensemble in low- and high-resource\nclasses\nOne interesting experimental observation is that\nthe improvement on low-resource classes is larger\nthan that on high-resource classes. To further in-\nvestigate the impact of the ensemble learners on\nthe imbalanced datasets, we make the following\nanalysis to Equation 5.\nExVar( ¯fθ(x)) =\n{ 1\nN ·p\nmσ2\nϵ for p≤m\n1\nN ·r\nmσ2\nϵ for p>m\n(9)\nwhere the impact of ensembles is represented as a\nfunction of ensemble size N, denoted as π(N) =\n1\nN ∈[0,1] that a smaller π(N) means a larger per-\nformance improvement. Given a fixed p, in over-\nparameterized cases ( p > m) where m is small,\nsince the samples in S are i.i.d, thus X ∈Rm×p\nis full rank so that r = rank(X) = m. The vari-\nance becomes 1\nN ·σ2\nϵ which does not change with\nm. The impact of ensemble solely depends on N\nthus is significant. On the other hand, in under-\nparameterized cases (p≤m) where mis large, the\nvariance is negligible as mbecomes much larger\nthan p, i.e., lim\nm→∞\n1\nN · p\nmσ2\nϵ = 0, so that the vari-\nance becomes 0 regardless of π(N). This implies\nthat the impact of ensemble can be ignored as m\nincreases. In general, given a fixedp, for both cases\nthe impact of ensemble is significant when mis\nsmall and insignificant as mbecomes very large.\nThese theoretical findings explain why we ob-\nserve larger performance improvement on low-\nresource than on high-resource classes using en-\nsembles. Similar to the discussion in Section 4.4,\nempirically, it is hard to define low-resource and\nhigh-resource classes using mand pbecause our\nanalysis is based on least squared linear regression\nassumption which is simplified compared to condi-\ntions in real scenarios. Besides pand m, there are\nother factors that may have implicit but significant\nimpact on model performance. This also explains\nwhy the improvement does not strictly follow the\nsorting of classes by their sample size. However,\nour findings show another advantage of using en-\nsembles. The empirical impact of ensemble size on\nimbalanced classes has been examined and shown\nin Section 4.6 and Appendix C, which is consis-\ntent with the theoretical findings discussed in this\nsection.\n6 Related Works\nIn a fine-tuning setting, searching for an optimal\nsetting of pretrained models and hyperparameters\nis challenging due to the high dimensionality of the\nsearch space, as well as the infinite values for each\ndimension. In previous works of fine-tuning tasks\n(Lee et al., 2020; Alsentzer et al., 2019; Beltagy\net al., 2019; Lin et al., 2021), the SOTA models\nare single learners carefully selected and fine-tuned\nbased on evaluation results, such as P, R, and F1\nscores, using grid-search or random-search. To\nimprove the stability of the pre-trained transformer-\nbased language models, Mosbach et al. (2021) sug-\ngests using small learning rates with bias correction\nand increasing the number of iterations to avoid\nvanishing gradients. Prior efforts also highlight the\ncomparable effect of weight initialization and train-\ning data order to the variance of model performance\n(Dodge et al., 2020).\nEnsemble methods have been successful in\nboosting the predictive performance of single learn-\ners (Ren et al., 2016 present a comprehensive re-\nview; also see Wang et al., 2003; Cire¸ sAn et al.,\n2012; Xie et al., 2013; Huang et al., 2017) as well\n15753\nas in estimating predictive uncertainty (Gal and\nGhahramani, 2016; Lakshminarayanan et al., 2017;\nSnoek et al., 2019). Among these studies, Bonab\nand Can (2019) and Wang et al. (2020) theoreti-\ncally prove that ensembles can perform better than\nthe average performance of component learners for\nprediction accuracy and consistency of learning\nmodels. Wang et al. (2022) empirically evaluates\nthe application of ensemble methods to fine-tuned\ntransformer-based models for clinical NLP tasks.\nThe findings demonstrate that ensemble methods\nimprove model performance, particularly when em-\nploying dynamic snapshot ensembling. Although it\nis common knowledge that ensembles can reduce\nvariance thus reducing the generalization error, no\nprevious work has discussed or measured this in the\ncontext of variance decomposition. Furthermore,\nno previous work has investigated the impact of\nensembles on imbalanced datasets.\n7 Conclusion\nDifferent from the prevailing fine-tuning settings,\nwe propose a two-stage fine-tuning method to im-\nprove the generalization ability of a fine-tuned\nLPM. We provide a variance decomposition-based\njustification method to empirically measure the gen-\neralization ability of the LPM w.r.t. the upper limit\nof its learning ability. In Stage-I, the RSOTA set-\nting is built by ratcheting down bias and variance\nin an iterative way. In Stage-II, given the RSOTA\nsetting, the fine-tuned LPM is guaranteed to be\nfurther generalized through ensembling techniques\nby reducing the variance due to optimization. The\nproposed justification method provides a concrete\nmetric to track this process.\nWe provide empirical evidence by conducting\nexperiments on the SuperGLUE tasks and two clin-\nical datasets. Furthermore, we perform theoreti-\ncal analysis on how ensembles improve variance\ndue to optimization. We investigate the nature\nof variance change for the ensemble size in low-\nand high-resource classes in classification tasks.\nDifferent from previous theoretical analyses us-\ning only model complexity and data size which\ndepends on least squared regression, our variance\ndecomposition-based justification method in Stage-\nI does not rely on specific factors thus leading to a\nmore generalizable measurement. The ENS further\nboosts performance without risk of computational\ncost and overfitting. Our analysis on imbalanced\ndata reveals another advantage of ensemble algo-\nrithms in improving model performance on low-\nresource classes.\nAs future work, we are interested in (1) rigor-\nously proving variance decomposition-based jus-\ntification criteria, (2) quantifying low- and high-\nresource classes with specific features that interplay\nwith ensemble size. If properly used, we believe\nthe theoretical and empirical findings discussed in\nthis paper can guide practitioners to fine-tune more\ngeneralizable models.\nLimitations\nAs we stated under future work, one of the limi-\ntations is the variance decomposition-based proof.\nOur work is based on simplified settings, i.e., linear\nsquared regression assumption. Post-ensemble vari-\nance is not evaluated due to the nature of the ENS\nensemble algorithm. Extended experiments using\nvanilla bagging ensemble would enable analysis of\npost-ensemble variance. Further investigation into\nrefining the two stages would help understand the\nperformance of LPMs, e.g. those that are in Phase-I\nbut before the peak in Figure 1. Our results for Mul-\ntiRC are based on the instance sampling, however a\nbetter sampling technique should be based on strat-\nified sampling based on the ratio of the question\ntypes in the MultiRC set. However, to achieve this,\nthe MultiRC set needs to be annotated for ques-\ntion types, which is currently missing. Sampling\ntechniques by themselves can become a research\ntopic so that a further decrease of variance due to\nsampling can be achieved. Although we list these\nitems as limitations, they are also topics for future\nresearch within the greater theme of understanding\nthe new bias-prevalence paradigm for LPMs.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for feedback that improved the paper, the\nUS National Institutes of Health (NIH) and the New\nJersey Institute of Technology (NJIT) for providing\nfunding. This research is supported by NJIT FY24\nFaculty Seed Grant, NIH Grants U24CA248010,\nR01LM013486 and R01GM114355. Any opinions,\nfindings, and conclusions or recommendations ex-\npressed in this material are those of the authors and\ndo not necessarily reflect the views of NJIT or NIH.\n15754\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHamed Bonab and Fazli Can. 2019. Less is more: a\ncomprehensive framework for the number of compo-\nnents of ensemble classifiers. IEEE Transactions on\nneural networks and learning systems.\nDan Cire¸ sAn, Ueli Meier, Jonathan Masci, and Jürgen\nSchmidhuber. 2012. Multi-column deep neural net-\nwork for traffic sign classification. Neural networks,\n32:333–338.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In international conference\non machine learning, pages 1050–1059.\nStuart Geman, Elie Bienenstock, and René Dour-\nsat. 1992. Neural networks and the bias/variance\ndilemma. Neural computation, 4(1):1–58.\nHasham Ul Haq, Veysel Kocaman, and David Talby.\n2021. Deeper clinical document understand-\ning using relation extraction. arXiv preprint\narXiv:2112.13259.\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu,\nJohn E Hopcroft, and Kilian Q Weinberger. 2017.\nSnapshot ensembles: Train 1, get m for free. arXiv\npreprint arXiv:1704.00109.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in neural information processing systems,\npages 6402–6413.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nChen Lin, Timothy Miller, Dmitriy Dligach, Steven\nBethard, and Guergana Savova. 2021. EntityBERT:\nEntity-centric masking strategy for model pretrain-\ning for the clinical domain. In Proceedings of the\n20th Workshop on Biomedical Language Processing,\npages 191–201, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. In 9th International Conference on Learning\nRepresentations, CONF.\nBrady Neal, Sarthak Mittal, Aristide Baratin, Vinayak\nTantia, Matthew Scicluna, Simon Lacoste-Julien, and\nIoannis Mitliagkas. 2018. A modern take on the bias-\nvariance tradeoff in neural networks. arXiv preprint\narXiv:1810.08591.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro\nBissacco, Bo Wu, and Andrew Y Ng. 2011. Reading\ndigits in natural images with unsupervised feature\nlearning.\nYe Ren, Le Zhang, and Ponnuthurai N Suganthan. 2016.\nEnsemble classification and regression-recent devel-\nopments, applications and future directions. IEEE\nComputational Intelligence Magazine, 11(1):41–53.\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Laksh-\nminarayanan, Sebastian Nowozin, D Sculley, Joshua\nDillon, Jie Ren, and Zachary Nado. 2019. Can you\ntrust your model’s uncertainty? evaluating predictive\nuncertainty under dataset shift. In Advances in Neu-\nral Information Processing Systems, pages 13969–\n13980.\nWilliam F. Styler IV , Steven Bethard, Sean Finan,\nMartha Palmer, Sameer Pradhan, Piet C de Groen,\nBrad Erickson, Timothy Miller, Chen Lin, Guergana\nSavova, and James Pustejovsky. 2014. Temporal an-\nnotation in the clinical domain. Transactions of the\n15755\nAssociation for Computational Linguistics , 2:143–\n154.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.\nEvaluating temporal relations in clinical text: 2012\ni2b2 challenge. Journal of the American Medical\nInformatics Association, 20(5):806–813.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nHaixun Wang, Wei Fan, Philip S Yu, and Jiawei Han.\n2003. Mining concept-drifting data streams using en-\nsemble classifiers. In Proceedings of the ninth ACM\nSIGKDD international conference on Knowledge dis-\ncovery and data mining, pages 226–235. AcM.\nLijing Wang, Dipanjan Ghosh, Maria Gonzalez Diaz,\nAhmed Farahat, Mahbubul Alam, Chetan Gupta,\nJiangzhuo Chen, and Madhav Marathe. 2020. Wis-\ndom of the ensemble: Improving consistency of deep\nlearning models. Advances in Neural Information\nProcessing Systems, 33:19750–19761.\nLijing Wang, Timothy Miller, Steven Bethard, and Guer-\ngana Savova. 2022. Ensemble-based fine-tuning strat-\negy for temporal relation extraction from the clinical\nnarrative. In Proceedings of the 4th Clinical Natu-\nral Language Processing Workshop, pages 103–108,\nSeattle, W A. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nKristin Wright-Bettner, Chen Lin, Timothy Miller,\nSteven Bethard, Dmitriy Dligach, Martha Palmer,\nJames H. Martin, and Guergana Savova. 2020. Defin-\ning and learning refined temporal relations in the\nclinical narrative. In Proceedings of the 11th Interna-\ntional Workshop on Health Text Mining and Informa-\ntion Analysis, pages 104–114, Online. Association\nfor Computational Linguistics.\nNotation Description\nX,Y The input and output sets of a learning task.\nD The unknown joint distribution of (X,Y).\n(x,y) The pair drawn from D; x∈X,y ∈Y.\nS A finite training dataset of mi.i.d. samples\nfrom D.\nm The sample size of S.\nC The number of classes in a training dataset.\np The number of hidden units in a neural\nnetwork layer.\nq The number of hidden layers in a neural network.\nfθ The predictors that are parameterized by the\nweights θ∈Rp of neural networks.\nfθ(x) The output prediction given x.\nO The random variable for optimization randomness.\nA The learning algorithm that produces\nθ= A(S,O).\nE[y|x] The expectation of ygiven x.\nRm The performance of a learning algorithm using\ntraining sets of size m.\nEnoise The expected noise of the output predictions.\nEbias The expected bias of the output predictions.\nEvar The expected variance of the output predictions.\nVaropt The variance due to optimization.\nVarsamp The variance due to sampling.\nEvarO The expected variance due to optimization.\nEvarS The expected variance due to sampling.\nN The ensemble size.\nGp Generalization ability of a learning algorithm.\nTable 3: Notations and their descriptions.\nJingjing Xie, Bing Xu, and Zhang Chuang. 2013. Hori-\nzontal and vertical ensemble with deep representation\nfor classification. arXiv preprint arXiv:1306.2759.\nZitong Yang, Yaodong Yu, Chong You, Jacob Stein-\nhardt, and Yi Ma. 2020. Rethinking bias-variance\ntrade-off for generalization of neural networks. In In-\nternational Conference on Machine Learning, pages\n10767–10777. PMLR.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R Bowman. 2020. When do you need bil-\nlions of words of pretraining data? arXiv preprint\narXiv:2011.04946.\nA Notations\nTable 3 shows major notations and their descrip-\ntions.\nB Experiments\nB.1 Data description\nTable 4 shows the statistics of all datasets used in\nour experiments. Each downloaded SuperGLUE\ndataset includes train, val, and test sets in json for-\nmat.2 The downloaded test set does not have gold-\nstandard labels thus is not used in our experiment.\n2https://super.gluebenchmark.com/\ntasks\n15756\nBoolQ CB RTE MultiRC WiC COPA THYME i2b2\nClasses 2 3 2 2 2 2 10 3\nTrain samples 7541 200 2000 4080 4800 320 423612 9909\nDev samples 1886 50 500 1020 1200 80 235117 2478\nVal samples 3270 57 278 953 638 100 208892 9630\nTable 4: Data statistics.\nBoolQ CB RTE MultiRC WiC COPA THYME i2b2\nRandom seed 62 52 72 72 42 72 42 42\nBatch size 10 10 10 10 10 10 32 32\nEpoch 8 7 10 6 8 8 3 3\nLearning rate 1e-5 2e-5 2e-5 2e-5 1e-5 1e-5 4e-5 4e-5\nLearning rate schedule type linear linear linear linear linear linear linear linear\nMax sequence length 512 512 512 512 512 512 100 128\nGradient accumulation steps 2 2 2 2 2 2 2 2\nTable 5: The RSOTA settings for SuperGLUE tasks and clinical information extraction tasks.\nWe split the train set into train (80%) and dev (20%)\nsets, and evaluate the model performance on val set.\nThe i2b2 does not have a development (dev) set in\nthe released data and we split the train set into train\n(80%) and dev (20%) sets. Random seed 42 is used\nto replicate the sampling process. For MultiRC,\nbecause each question can have more than one cor-\nrect answer, we sampled the instances based on\nindividual question-answer options in the train set\nfor training and validation in our experiment.\nB.2 Hyperparameter settings\nTable 5 shows the details of hyperparameter set-\ntings. Unless otherwise specified, we use default\nvalues of the hyperparameters in Huggingface. We\nalso summarize pretrained models used in our ex-\nperiments in Table 6.\nB.3 Replicated SOTA scores\nTo ensure that our experiments on the Super-\nGLUE tasks are reproducible, we followed\nthe settings and replicated the SOTA accuracy\nscores reported in: https://github.com/\nfacebookresearch/fairseq/tree/\nmain/examples/roberta. We could not\nreplicate the representation (special token ex-\ntractions) and the model settings (unpublished\npretrained model) for WSC task, thus it is omitted\nin our paper. In our experiments, we report\nthe classic metrics of precision/recall/F1 for\nconsistency with the reported results for the\nTHYME and i2b2 tasks. Our accuracy scores\nfor the SuperGlue tasks (shown in Table 7) are\ndirectly comparable and are consistent with those\nin Table 2 in the main paper.\nB.4 Implementation details of ENS\nENS allows a pretrained model to be fine-tuned\nmultiple times (i.e., multiple training runs) sequen-\ntially with different random seeds and data shuf-\nfling of train/validation splits. It uses a cyclic an-\nnealing schedule and cyclic snapshot strategy to\nperiodically save the best model during each train-\ning run. Different from the simple bagging en-\nsemble, after each training run, a dynamic pruning\nalgorithm is applied to select a few single learners\nfrom the saved ones which can lead to better per-\nformance of the ensemble learner with theoretical\nguarantees. The sequential training runs stop when\nthe accumulated number of selected single learners\nreaches a preset ensemble size. The total amount\nof training runs is a dynamic value rather than a\npreset value, which is determined by the snapshot\nstrategy and pruning factor during the sequential\ntraining.\nIn our experiments, we implemented ENS on the\ntop of RSOTA setting. The ensemble size is set as\n5 and majority voting is used to generate ensem-\nble predictions. We reuse RSOTA settings except\nthat we set cosine with restarts as the learning rate\nscheduler and set the learning rate to restart every\nkepochs which, based on the RSOTA setting, al-\nlows the model to converge to a reasonable state\nbefore each restart. The total number of epochs for\neach training run is 5 ×k and we save the top 4\nmodels for pruning based on validation accuracy.\nThe random seeds for initialization and data shuf-\nfling are [42, 52, 62, 72, 82]. The logic behind\nthe above settings is to retain the benefits from\nRSOTA fine-tuning settings as much as possible.\nCode and settings to reproduce the results are avail-\n15757\nModel name Model Details\nRoBERTa-base 12-layer, 768-hidden, 12-heads, 125M parameters.\nRoBERTa-large 24-layer, 1024-hidden, 16-heads, 355M parameters\nPubmedBERTbase-MimicBig-EntityBERT 12-layer, 768-hidden, 12-heads, 110M parameters.\nBioBERT-base-cased-v1.1 12-layer, 768-hidden, 12-heads, 110M parameters.\nTable 6: Details of pretrained models.\nBoolQ CB RTE MultiRC WiC COPA\nReference 86.9 98.2 89.5 85.7 75.6 94.0\nReplicated 86.3 98.2 87.4 84.7 72.1 93.4\nRSOTA 85.4±0.31 81.1±9.9 83.7±0.91 79.1±10.72 70.7±1.7 81.6±11.9\nENS 85.7±0.41 92.2±1.4 85.6±1.05 82.5±1.3 71.3±0.36 93.6±0.48\nTable 7: Accuracy scores on the SuperGLUE tasks. For \"Reference\" and \"Replicated\": training on the original train\nset, validating and testing on the original dev set. For \"RSOTA\" and \"ENS\": training on 80% of the original train\nset, validating on 20% of the original train set, and testing on the original dev set.\nable at https://github.com/christa60/\nbias-var-fine-tuning.git.\nB.5 Experimental design of bagging ensemble\nfor investigating various ensemble sizes\nTo analyze the nature of the variance change\nwith the ensemble size in low-resource classes\n(NOTED-ON, BEGINS-ON, END-ON relations\nin the THYME corpus) and high-resource (CON-\nTAINS, OVERLAP, BEFORE relations in the\nTHYME corpus) classes, we vary the ensemble\nsize from 1 to 10 and then compute the P, R, and\nF1 scores for each class on THYME data.\nWe create 10 bootstrap replicate training sets\nby resampling training and dev datasets with the\nsame size and class distribution. The random\nseeds for resampling are randomly chosen and\nthen fixed. The various splits are denoted as\n[’split_r42’, ’split_r52’, ’split_r62’, ’split_r72’,\n’split_r82’, ’split_r92’, ’split_r102’, ’split_r112’,\n’split_r122’, ’split_r132’]. Given a random seed of\ninitialization, we train N fine-tuned single learn-\ners. To compute 95% confidence intervals for these\nestimates, we use 5 random seeds of initialization,\nresulting in 5 ensemble models for each ensemble\nsize. We vary the ensemble size N from 1 to 10\nand have 100 ensemble models in total.\nC Section 4.6: Additional Results\nWe show the absolute and percentage improve-\nment (compared with single learners i.e., N = 1)\nchange over the ensemble size N using P and R\nin Figure 3. Together with Figure 2, the major\nobservations are: (a) The absolute and percentage\nimprovements of P, R, and F1 increase as N in-\ncreases. (b) The precision improvements are more\npronounced than those of recall thus contributing\nthe major part of the F1 improvements. This phe-\nnomenon is more pronounced for high-resource\nclasses. (c) Given a fixed N, the improvements\non low-resource classes are larger than those on\nhigh-resource classes across the three metrics. The\ndifference becomes larger as N increases.\nDiscussion: Our experimental results are con-\nsistent with our theoretical findings in Section 5\nthat model performance keeps improving because\nvariance due to optimization decreases as ensemble\nsize increases. Furthermore, the impact of ensem-\nble is more pronounced on low-resource classes\nthan on high-resource classes.\n15758\n(a) Precision absolute improvement\n (b) Precision percentage improvement\n(c) Recall absolute improvement\n (d) Recall percentage improvement\nFigure 3: Performance improvement on low-resource classes (NOTED-ON, BEGINS-ON, ENDS-ON) and high-\nresource classes (CONTAINS, OVERLAP, BEFORE) from the THYME dataset. We show absolute and percentage\nimprovement (compared with single learners i.e., N = 1) for precision ((a) and (b)), and recall ((c) and (d)). Values\nare computed based on the mean with 95% confidence interval over 5 random samples for each class and each\nensemble size.\n15759\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1 Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Appendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15760\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 and Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.4 and 4.5\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n15761",
  "topic": "Variance (accounting)",
  "concepts": [
    {
      "name": "Variance (accounting)",
      "score": 0.8823155760765076
    },
    {
      "name": "Computer science",
      "score": 0.7059222459793091
    },
    {
      "name": "Parameterized complexity",
      "score": 0.6517714262008667
    },
    {
      "name": "Variance decomposition of forecast errors",
      "score": 0.5228667259216309
    },
    {
      "name": "Artificial neural network",
      "score": 0.45112931728363037
    },
    {
      "name": "Fine-tuning",
      "score": 0.4440215826034546
    },
    {
      "name": "Code (set theory)",
      "score": 0.4407757818698883
    },
    {
      "name": "Monotonic function",
      "score": 0.4296434223651886
    },
    {
      "name": "Algorithm",
      "score": 0.39034464955329895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37494516372680664
    },
    {
      "name": "Machine learning",
      "score": 0.340711772441864
    },
    {
      "name": "Statistics",
      "score": 0.304850697517395
    },
    {
      "name": "Mathematics",
      "score": 0.19747602939605713
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118118575",
      "name": "New Jersey Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I138006243",
      "name": "University of Arizona",
      "country": "US"
    }
  ],
  "cited_by": 9
}