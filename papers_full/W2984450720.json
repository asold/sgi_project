{
    "title": "Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
    "url": "https://openalex.org/W2984450720",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5019823426",
            "name": "Xiepeng Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058644734",
            "name": "Zhexi Zhang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A5100339289",
            "name": "Wei Zhu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100414942",
            "name": "Zheng Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5112109363",
            "name": "Yuan Ni",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028272399",
            "name": "Peng Gao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087158377",
            "name": "Junchi Yan",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A5023775139",
            "name": "Guotong Xie",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2898662126",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962727366",
        "https://openalex.org/W2964222271",
        "https://openalex.org/W2107670443",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2806055002",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W2788285996",
        "https://openalex.org/W1533230146",
        "https://openalex.org/W2889583850",
        "https://openalex.org/W2911529999",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2785442519",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2789078277",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W4289360545",
        "https://openalex.org/W2920665390",
        "https://openalex.org/W2963159690"
    ],
    "abstract": "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task's official test data, outperforming all the other submissions.",
    "full_text": "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 93–98\nHongkong, China, November 3, 2019.c⃝2019 Association for Computational Linguistics\n93\nPingan Smart Health and SJTU at COIN - Shared Task: utilizing\nPre-trained Language Models and Common-sense Knowledge in Machine\nReading Tasks\nXiepeng Li1∗ Zhexi Zhang2∗ Wei Zhu1∗† Zheng Li1 Yuan Ni1\nPeng Gao1 Junchi Yan2 Guotong Xie1\n1 Pingan Health Tech, Shanghai, China\n2 Shanghai Jiaotong University, Shanghai, China\nAbstract\nTo solve the shared tasks of COIN: COmmon-\nsense INference in Natural Language Process-\ning) Workshop in EMNLP-IJCNLP 2019, we\nneed explore the impact of knowledge repre-\nsentation in modeling commonsense knowl-\nedge to boost performance of machine reading\ncomprehension beyond simple text matching.\nThere are two approaches to represent knowl-\nedge in the low-dimensional space. The ﬁrst is\nto leverage large-scale unsupervised text cor-\npus to train ﬁxed or contextual language repre-\nsentations. The second approach is to explic-\nitly express knowledge into a knowledge graph\n(KG), and then ﬁt a model to represent the\nfacts in the KG. We have experimented both\n(a) improving the ﬁne-tuning of pre-trained\nlanguage models on a task with a small dataset\nsize, by leveraging datasets of similar tasks;\nand (b) incorporating the distributional repre-\nsentations of a KG onto the representations of\npre-trained language models, via simply con-\ncatenation or multi-head attention. We ﬁnd\nout that: (a) for task 1, ﬁrst ﬁne-tuning on\nlarger datasets like RACE (Lai et al., 2017)\nand SW AG (Zellers et al., 2018), and then ﬁne-\ntuning on the target task improve the perfor-\nmance signiﬁcantly; (b) for task 2, we ﬁnd\nout the incorporating a KG of commonsense\nknowledge, WordNet (Miller, 1995) into the\nBert model (Devlin et al., 2018) is helpful,\nhowever, it will hurts the performace of XL-\nNET (Yang et al., 2019), a more powerful pre-\ntrained model. Our approaches achieve the\nstate-of-the-art results on both shared task’s\nofﬁcial test data, outperforming all the other\nsubmissions.\n1 Introduction\nMachine reading comprehension (MRC) tasks\nhave always been the most studied tasks in the\n∗ Equal contribution.\n† Corresponding email: michaelwzhu91@gmail.com;\nzhuwei972@pingan.com.cn.\nﬁeld of natural language understanding. Common\nforms of reading comprehension tasks involve\nquestion answer (QA) , cloze-style and multiple-\nchoice questions. Many models have achieved\nexcellent results on MRC datasets such as (Ra-\njpurkar et al., 2016; Nguyen et al., 2016; Lai et al.,\n2017; Zhang et al., 2018a). However, Kaushik and\nLipton (2018) demonstrate that most questions in\nprevious MRC tasks can be answered by simply\nmatching the patterns in the textual level even with\npassage or question only, but existing models per-\nform badly on questions that require incorporating\nknowledge in more sophisticated ways. In con-\ntrast, human beings can easily reason with knowl-\nedge from contexts or commonsense knowledge\nwhen doing MRC task. Thus, it is of signiﬁcance\nfor models to be able to reason with knowledge,\nespecially commonsense knowledge.\nVarious deep learning models have been pro-\nposed and shown pretty good performance on\nMRC tasks (Parikh et al., 2019; Zhu et al., 2018;\nSun et al., 2018; Xu et al., 2017). Majority of\nthese approaches utilize sequence relevant neu-\nral networks such as GRU (Cho et al., 2014),\nLSTM (Hochreiter and Schmidhuber, 1997) and\nAttention mechanism (Vaswani et al., 2017) to\nmodel the implicit relation among passages, ques-\ntions and answers.\nAs pre-trained language models have shown\nmiraculous performance on several NLP tasks, a\nlarge number of methods utilize this pre-trained\nlanguage model to extract textual level features\nin MRC tasks. (Zhang et al., 2019; Ran et al.,\n2019) compute the contextual representation of\npassages, questions and options separately with\nBERT and match the representation in down-\nstream networks. They achieved the best results\non RACE dataset at their submission time.\nShared task 1 in COIN workshop is a two-\nchoice question task with short narrations about\n94\neveryday scenarios, which is an extended ver-\nsion of SemEval 2018 Task 11 (Ostermann\net al., 2018). Shared task 2 uses the ReCoRD\ndataset (Zhang et al., 2018a), a machine reading\ncomprehension dataset in news articles. It anno-\ntates named entities in the news articles and ad-\nditionally provides some brief bullet points that\nsummarize the news. It then asks for cloze-style\nanswers, ﬁlling in a blank in a sentence related to\nthe news article. Accomplishing these tasks re-\nquires both the capability of reading comprehen-\nsion and commonsense knowledge inference.\nOur system is based on XLNet (Yang et al.,\n2019), a generalized auto-regressive pretraining\nmethod which achieves state-of-the-art results on\nmany NLP tasks. For task 1, We ﬁrst pre-train\nthe model on multiple-choice question dataset\nRACE (Lai et al., 2017) to gain certain reading\ncomprehension abilities. Afterwards, we mine\ncommonsense knowledge by ﬁne-tuning grounded\ncommonsense inference dataset SW AG (Zellers\net al., 2018) on XLNet instead of introduc-\ning knowledge graph of general knowledge such\nas ConceptNet (Speer et al., 2017) or Word-\nNet (Miller, 1995). For task 2, other than utiliz-\ning XLNet’s representation power, we also exper-\niment on enhancing the representation and regu-\nlarizing predicted prior of named entities, by con-\ncatenating the pre-trained embedding of WordNet\nof contextual word embedding. We ﬁnally im-\nplement a series of post-processing strategies to\nimprove the model prediction results. Our sys-\ntem achieves state-of-the-art performance on the\nboth shared tasks’ ofﬁcial test data, even though\nwe only train on the train sets and only submit sin-\ngle models.\n2 Model settings\nIn this section, we present the system designs we\nexperimented for the two shared tasks.\n2.1 Pretrained language model Fine-tuning\nAs shown in Devlin et al. (2018) and Yang et al.\n(2019), the usual way to employ pre-trained lan-\nguage models in representing multiple text input is\nconcatenation of text inputs in certain orders. For\nthis section, the denotations mainly follow Yang\net al. (2019) since we mainly use XLNet as the\ntext encoder. Since the notations for Bert will be\nquite similar, which will not be included in this\nwork.\nFor task 1, the inputs are a context passage (de-\nnoted as P), two queries (denoted as Qi, i = 1, 2),\nand two answer options for each query, Ai,j, for\nj = 1, 2. Following Yang et al. (2019)’s solution\non the RACE dataset, we concatenate the inputs as\nfollows:\nConcat 2\ni=1[P, [SEP ], QAi, [SEP ], [CLS]],\n(1)\nwhere QAi is the concatenation of the query-\nanswer pairs:\nQAi = Concat 2\nj=1[Qi, Ai,j]. (2)\nAs for Task 2, the inputs are a context passage (de-\nnoted as P), which in this case is a piece of a news-\npaper article, and a assertive sentence (denoted as\nS) part of which is masked out, thus they are con-\ncatenated as follows:\nConcat [P, [SEP ], S,[SEP ], [CLS]]. (3)\nAfter the text inputs are concatenated accord-\ningly, they will go through XLNet to get a contex-\ntual representation. The output layer for the two\ntasks are different. For task 1, a fully-connected\nlayer is put on the [CLS] token’s representation\ntwo give out the likelihood of which answer op-\ntion is the answer. For task 2, the answer is se-\nlected from the context passage, thus we have to\npredict the start position and end position. Thus\ntwo fully-connected layers are needed, where the\nﬁrst is to estimate the likelihood of being the start\nposition for each token, and the second combines\nthe encoded representation and the output of the\nﬁrst fully-connected layer and predicts the end po-\nsition.\n2.2 Multi-funetuning\nFinetuning a pre-trained language model on a\nsmall target task dataset has shown signiﬁcant per-\nformance gains, as is shown in Devlin et al. (2018)\nand Yang et al. (2019). However, directly ﬁne-\ntuning is proven not to be the most effective way,\nsince although pre-trained LMs are known to gen-\neralize well, overﬁtting problem is still inevitable.\nThus, related corpus or similar datasets are often\nused, such as Wang et al. (2019) and Phang et al.\n(2018), to form a multi-stage ﬁne-tuning proce-\ndure. For example, Wang et al. (2019) ﬁrst ﬁne-\ntune on the MultiNLI datset before training the\nCB, RTE, and BoolQ tasks. The intuition behind\nwhy this multi-stage ﬁne-tuning strategy works is\n95\nDataset Options Sentence A Sentence B\nRACE 4 passage query+option\nSW AG 4 query option\nTask 1 2 passage query+option\nTable 1: Structure of inputs for the two supplementary\ntasks and the target task dataset\nDataset Train Dev\nRACE 87866 4887\nSW AG 73546 20006\nTask 1 14191 2020\nTable 2: Basic statistics for the three datasets involved\nin solving task 1\nthat (a) to let the pre-trained LMs to adopt to the\nsimilar contextual environment, (b) and make the\nmodel more suitable for this speciﬁc task forma-\ntion.\nDue to the fact that task 1 dataset is small and\nduring ﬁne-tuning the original XLNet model over-\nﬁts very quickly, we experimented on a multi-\nstage ﬁne-tuning strategy. The ﬁrst additional\ndataset we choose is RACE, which is relatively\nlarger. Then we choose to fune-tune on SW AG,\nwhose queries are similar to our target task and\nrequires commonsense reasoning. Then we ﬁne-\ntune till convergence on the task 1 train set.\nTable 1 presents the structure of the inputs for\nthe three datasets, where k is the number of op-\ntions for each query. After each stage before\nthe ﬁnal ﬁne-tuning, we disregard the ﬁnal fully-\nconnected output layer and use the updated XLNet\nlayers to ﬁne-tune on the next dataset.\n2.3 Knowledge fusing\nBesides the original basic ﬁne-tuning architecture\nadopted by the XLNet, we also experiment on in-\nvolving commonsense knowledge for context en-\ncoding, as is depicted in Figure 1.\nThe commonsense knowledge graph we use is\nthe WordNet (Miller, 1995). The KG embedding\nis trained using DistMult (Yang et al., 2014a).\nFirst, we will match the phrases in the passage to\nentities in the WordNet, using Aho-Corasick algo-\nrithm (Arudchutha et al., 2014). 1 Then each to-\nken in the entity will be given the same embed-\n1If a phrase is matched to multiple entities in the KG, we\nwill take the average of all entity embeddings as the entity\nembedding for the phrase.\nFigure 1: The architecture of our KG infusing model\nwith XLNet as text encoder\nding vector, which is the embedding of the en-\ntity in WordNet. Tokens not in any entity will be\ngiven a zero vector as embedding. The KG en-\ncoded text input will be incorporated with the en-\ncoded output of XLNet using a multi-head atten-\ntion layer (Vaswani et al., 2017), where the XLNet\nencoded output acts as the query and the KG en-\ncoded output acts the key and value. Then the out-\nput layer is the same with answer span prediction\nlayers described in the previous subsection.\n2.4 Answer Veriﬁcation\nTo improve the prediction results of a model, we\nimplement a series of answer veriﬁcation strate-\ngies, which are the following:\n• as there are additional entity information pro-\nvided with the dataset, at the span predict\nstage, we ﬁlter invalid predicted spans ac-\ncording to whether it match a named entity\n• if we can not ﬁnd any entities in all predic-\ntions, we randomly select one from the enti-\nties provided to us\n• some entity is a part of the ‘-‘ concatenation\nspan, then we match the answer by its left or\nright concatenated contexts\n3 Experiment\n3.1 Dataset\nStatistics for the datasets involved in training for\ntask 1, which are RACE, SW AG and the ofﬁcial\ntask 1 dataset are shown in Table 2. The statistics\nrepresent the total number of queries in the corre-\nsponding dataset. The ﬁnal submission result on\n96\nthe leader-board is calculated on ofﬁcial test data,\nwhich will not be published. Only training and de-\nvelopment data for the task are available to us.\nTask 2, which is the ReCoRD dataset (Zhang\net al., 2018b) has 65, 000 queries on the train set\nand 10, 000 queries on the dev set. The answer for\nthe ReCoRD dataset is not unique, since an entity\nis likely to be mentioned multiple times in a news\narticle. Thus, we take each passage-query-answer-\nspan as one sample during training, which can also\nbe seen as a kind of data augmentation.\nFor both tasks, we only submit the models\ntrained on the train sets of the target tasks.\n3.2 Experimental setting\nWe use XLNet (large, cased) as the pre-trained\nlanguage model. For task 1 dataset, we truncate\nthe query-answer pair to a maximum length of\n128, and set the maximum length of the passage-\nquery-answer pair to 384. So the max length of\nthe whole text inputs of one sample is 768. With a\nTesla V100-PCIE-16GB GPU card, the batch size\ncan only be set to be 2 on each card, thus we em-\nploy 8 GPUs for training. Firstly we ﬁne-tune the\noriginal XLNet on RACE for 100,000 steps with\nthe sequence length of 192, query-answer length\nof 96 and Adam optimizer leaning rate of 1e-6.\nAfterwards, we ﬁne-tune the model on SW AG for\n12,500 steps with the same parameters as RACE’s.\nEventually, the model is ﬁne-tuned on the task 1\ndataset till convergence, where the learning rate is\nset as 8e-6.\nFor task 2, the maximum length of the passage-\nquerypair is set to be 384, in which the maximum\nlength for the query is 64. During training the\nlearning rate is 5e-6 and batch size is 4 on each\nGPU card.\nWhen we try to infuse the KG into the XL-\nNet, we use the OpenKE library (Han et al., 2018)\nto train the KG representions of WordNet. We\nchoose DistMult (Yang et al., 2014b) as the em-\nbedding model, set the embedding size as 100,\nepoches as 10, batch size as 32 and the learning\nrate as 1e-4. The multi-head attention betwen the\nXLNet encoded output and the entity encoded out-\nput has the same number of attention head as the\nXLNet large model. During training, we will keep\nthe KG embedding trainable. Besides multi-head\nattention, we also experiment using a whole trans-\nformer block, i.e., a multi-head attention layer fol-\nlowed by a position-wise feed-forward network,\nModel Dev Test\nHuman - 97.4%\nFinal submission 91.44% 90.6%\nXLNet 91.09% -\nXLNet+RACE 92.46% -\nXLNet+SW AG 89.36% -\nXLNet+RACE+SW AG 92.76% -\nTable 3: Main results on the task 1.\nand combining the entity encoding by simply con-\ncatenating it onto the XLNet encoded output. For\ncomparison, we switch XLNet with Bert (large\nmodel), and repeat the above experiments.\n3.3 Results\nThe main experimental accuracy results for task\n1 are shown in Table 3, in which human perfor-\nmance is provided by task organizers. Our sys-\ntem consists of ﬁne-tuning XLNet on RACE and\nSW AG. We also conduct an ablation experiment to\ninvestigate the effects of the two external dataset.\nAs a result, Table 3 illustrates that pre-training on\nRACE plays a signiﬁcant role in the system. Ori-\ngin XLNet achieves an accuracy of 91.09%, indi-\ncating its powerful text representation ability, es-\npecially in the reading comprehension task. Pre-\ntraining on SW AG without RACE does not im-\nprove the accuracy perhaps because SW AG mis-\nleads the model to better adjust its task forma-\ntion, thus making it worse on the machine reading\ncomprehension. Meanwhile, combining SW AG\ntogether with RACE makes sense, indicating the\nmodel can improve its commonsense inference\nability.\nWe achieved the best performance on the ofﬁ-\ncial dev dataset with the training steps described in\nsection 3.2 while our submission result on the ofﬁ-\ncial leader-board was obtained with fewer training\nsteps due to queue submission time impact on Co-\ndalab. Despite being not fully trained, our system\nstill achieve the best result with the accuracy of\n90.6%, outperforming other participating teams.\nFor task 2, the results are presented on Table 4,\nwhere the bolded models are our submissions on\nthe test leaderboard. Due to limited resources and,\nthe results are not run multiple times, thus the re-\nsults may be affected by random effects. We ﬁnd\nout the original XLNet performs the best, signif-\nicantly outperforming the Bert models. While it\nseems adding a commonsense KG is beneﬁcial for\n97\nModel Dev EM Dev F1 Test EM Test F1\nHuman - - 91.31 % 91.69 %\nBert 69.83 % 71.05 % - -\nBert + KG (multi-head attn) 71.08 % 72.69 % - -\nBert + KG (transformer) 70.36 % 71.84 % - -\nBert + KG (concat) 70.74 % 72.09 % - -\nXLNet 80.64 % 82.10 % 81.46 % 82.66 %\nXLNet + KG (multi-head attn) 80.31 % 81.62 % - -\nXLNet + KG (transformer) 80.16 % 81.55 % - -\nXLNet + KG (concat) 80.25 % 81.67 % - -\nXLNet + answer veriﬁcation 82.72 % 83.38 % 83.09 % 83.74 %\nTable 4: The main results on task 2.\nBert, it does not help improving XLNet models.\nFor the models with KG, regardless of what the\nunderlying pre-trained langugae model is, multi-\nhead attention works best on infusing the knowl-\nedge, and simple concatenation works better than\nadding a whole transformer block.\nFor task 2, implementing the answer veriﬁca-\ntion process after we obtain the predictions of\nXLNet model boost the performance signiﬁcantly,\nboth on the dev set and the test set. Since we did\nnot see signiﬁcant improvements by adding KG\ninto the model, we did not submit results from KG\ninfused models.\nConclusions\nTo conclude, we have shown that XLNet, a re-\ncently proposed pre-trained language model, is\npowerful in text representation for machine read-\ning tasks. Simply ﬁne-tuning XLNet on the shared\ntasks already outperforms the other models which\nuse Bert as text encoder. However, we demon-\nstrate on task 1 that multi-stage ﬁne-tuning on sim-\nilar tasks can help providing more stable conver-\ngence and improve the ﬁnal results signiﬁcantly.\nFor task 2, we also show that the model predic-\ntions can be improved by adding human designed\npost-processing strategies. We also experiments\non incorporating commonsense KG into the archi-\ntecture of XLNet, however, due to our limited ex-\nperiments, we haven’t obtain signiﬁcant improve-\nments by adding KG into the model, especially\nmodels based on XLNet. However, it is a direc-\ntion worth further research.\nReferences\nS. Arudchutha, T. Nishanthy, and R. G. Ragel. 2014.\nString matching with multicore cpus: Performing\nbetter with the aho-corasick algorithm. In IEEE In-\nternational Conference on Industrial Information\nSystems.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. Openke: An\nopen toolkit for knowledge embedding. In Proceed-\nings of EMNLP, pages 139–144.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nDivyansh Kaushik and Zachary C Lipton. 2018. How\nmuch reading does reading comprehension require?\na critical investigation of popular benchmarks.\narXiv preprint arXiv:1808.04926.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM , 38(11):39–\n41.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human-generated machine\nreading comprehension dataset.\n98\nSimon Ostermann, Michael Roth, Ashutosh Modi, Ste-\nfan Thater, and Manfred Pinkal. 2018. Semeval-\n2018 task 11: Machine comprehension using com-\nmonsense knowledge. In Proceedings of The 12th\nInternational Workshop on Semantic Evaluation ,\npages 747–757.\nSoham Parikh, Ananya B Sai, Preksha Nema, and\nMitesh M Khapra. 2019. Eliminet: A model\nfor eliminating options for reading comprehension\nwith multiple choice questions. arXiv preprint\narXiv:1904.02651.\nJason Phang, Thibault Fvry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nQiu Ran, Peng Li, Weiwei Hu, and Jie Zhou.\n2019. Option comparison network for multiple-\nchoice reading comprehension. arXiv preprint\narXiv:1903.03033.\nRobert Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie.\n2018. Improving machine reading comprehension\nwith general reading strategies. arXiv preprint\narXiv:1810.13441.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. Superglue:\nA stickier benchmark for general-purpose language\nunderstanding systems.\nYichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen,\nand Xiaodong Liu. 2017. Dynamic fusion networks\nfor machine reading comprehension. arXiv preprint\narXiv:1711.04964.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2014a. Embedding Entities and\nRelations for Learning and Inference in Knowledge\nBases. arXiv e-prints, page arXiv:1412.6575.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2014b. Embedding Entities and\nRelations for Learning and Inference in Knowledge\nBases. arXiv e-prints, page arXiv:1412.6575.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv:1808.05326.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018a.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. CoRR,\nabs/1810.12885.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, and Benjamin Van Durme. 2018b. Record:\nBridging the gap between human and machine com-\nmonsense reading comprehension.\nShuailiang Zhang, Hai Zhao, Yuwei Wu, Zhuosheng\nZhang, Xi Zhou, and Xiang Zhou. 2019. Dual co-\nmatching network for multi-choice reading compre-\nhension. arXiv preprint arXiv:1901.09381.\nHaichao Zhu, Furu Wei, Bing Qin, and Ting Liu. 2018.\nHierarchical attention ﬂow for multiple-choice read-\ning comprehension. In Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence."
}