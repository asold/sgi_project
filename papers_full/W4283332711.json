{
    "title": "Trusting deep learning natural-language models via local and global explanations",
    "url": "https://openalex.org/W4283332711",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2098398284",
            "name": "Francesco Ventura",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A1965690711",
            "name": "Salvatore Greco",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A290216965",
            "name": "Daniele Apiletti",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A102100071",
            "name": "Tania Cerquitelli",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A2098398284",
            "name": "Francesco Ventura",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A1965690711",
            "name": "Salvatore Greco",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A290216965",
            "name": "Daniele Apiletti",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        },
        {
            "id": "https://openalex.org/A102100071",
            "name": "Tania Cerquitelli",
            "affiliations": [
                "Polytechnic University of Turin"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891503716",
        "https://openalex.org/W2962807820",
        "https://openalex.org/W3081987387",
        "https://openalex.org/W2920807444",
        "https://openalex.org/W3023640868",
        "https://openalex.org/W2996905632",
        "https://openalex.org/W2510508396",
        "https://openalex.org/W3094359574",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W2606462007",
        "https://openalex.org/W2962772482",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2954283498",
        "https://openalex.org/W2970801625",
        "https://openalex.org/W3097041292",
        "https://openalex.org/W3148040514",
        "https://openalex.org/W2963233086",
        "https://openalex.org/W2970014349",
        "https://openalex.org/W2150593711",
        "https://openalex.org/W2742340224",
        "https://openalex.org/W2958992432",
        "https://openalex.org/W3039554467",
        "https://openalex.org/W2943031807",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3000716014",
        "https://openalex.org/W2616247523",
        "https://openalex.org/W2890809607",
        "https://openalex.org/W2889313789",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2892868562",
        "https://openalex.org/W3044950360",
        "https://openalex.org/W2558100504",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3102564565",
        "https://openalex.org/W3101609372"
    ],
    "abstract": "Abstract Despite the high accuracy offered by state-of-the-art deep natural-language models (e.g., LSTM, BERT), their application in real-life settings is still widely limited, as they behave like a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental requirement of future-generation data-driven systems based on deep-learning approaches. Several attempts to fulfill the existing gap between accuracy and interpretability have been made. However, robust and specialized eXplainable Artificial Intelligence solutions, tailored to deep natural-language models, are still missing. We propose a new framework, named T-EBAnO , which provides innovative prediction-local and class-based model-global explanation strategies tailored to deep learning natural-language models. Given a deep NLP model and the textual input data, T-EBAnO provides an objective, human-readable, domain-specific assessment of the reasons behind the automatic decision-making process. Specifically, the framework extracts sets of interpretable features mining the inner knowledge of the model. Then, it quantifies the influence of each feature during the prediction process by exploiting the normalized Perturbation Influence Relation index at the local level and the novel Global Absolute Influence and Global Relative Influence indexes at the global level. The effectiveness and the quality of the local and global explanations obtained with T-EBAnO are proved on an extensive set of experiments addressing different tasks, such as a sentiment-analysis task performed by a fine-tuned BERT model and a toxic-comment classification task performed by an LSTM model. The quality of the explanations proposed by T-EBAnO , and, specifically, the correlation between the influence index and human judgment, has been evaluated by humans in a survey with more than 4000 judgments. To prove the generality of T-EBAnO and its model/task-independent methodology, experiments with other models (ALBERT, ULMFit) on popular public datasets (Ag News and Cola) are also discussed in detail.",
    "full_text": "Knowledge and Information Systems (2022) 64:1863–1907\nhttps://doi.org/10.1007/s10115-022-01690-9\nREGULAR PAPER\nTrusting deep learning natural-language models via local\nand global explanations\nFrancesco Ventura 1 · Salvatore Greco 1 · Daniele Apiletti 1 ·\nTania Cerquitelli 1\nReceived: 22 December 2020 / Revised: 10 May 2022 / Accepted: 14 May 2022 /\nPublished online: 22 June 2022\n© The Author(s) 2022\nAbstract\nDespite the high accuracy offered by state-of-the-art deep natural-language models (e.g.,\nLSTM, BERT), their application in real-life settings is still widely limited, as they behave\nlike a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental\nrequirement of future-generation data-driven systems based on deep-learning approaches.\nSeveral attempts to fulﬁll the existing gap between accuracy and interpretability have been\nmade. However, robust and specialized eXplainable Artiﬁcial Intelligence solutions, tai-\nlored to deep natural-language models, are still missing. We propose a new framework,\nnamed T- EBAnO , which provides innovative prediction-local and class-based model-global\nexplanation strategies tailored to deep learning natural-language models. Given a deep\nNLP model and the textual input data, T- EBAnO provides an objective, human-readable,\ndomain-speciﬁc assessment of the reasons behind the automatic decision-making process.\nSpeciﬁcally, the framework extracts sets of interpretable features mining the inner knowl-\nedge of the model. Then, it quantiﬁes the inﬂuence of each feature during the prediction\nprocess by exploiting the normalized Perturbation Inﬂuence Relation index at the local level\nand the novel Global Absolute Inﬂuence and Global Relative Inﬂuence indexes at the global\nlevel. The effectiveness and the quality of the local and global explanations obtained with\nT- EBAnO are proved on an extensive set of experiments addressing different tasks, such\nas a sentiment-analysis task performed by a ﬁne-tuned BERT model and a toxic-comment\nclassiﬁcation task performed by an LSTM model. The quality of the explanations proposed\nby T- EBAnO , and, speciﬁcally, the correlation between the inﬂuence index and human\nFrancesco V entura and Salvatore Greco have contributed equally this work.\nB Salvatore Greco\nsalvatore_greco@polito.it\nFrancesco V entura\nfrancesco.ventura@polito.it\nDaniele Apiletti\ndaniele.apiletti@polito.it\nTania Cerquitelli\ntania.cerquitelli@polito.it\n1 Department of Control and Computer Engineering, Politecnico di Torino, Turin, Italy\n123\n1864 F. Ventura et al.\njudgment, has been evaluated by humans in a survey with more than 4000 judgments.\nTo prove the generality of T- EBAnO and its model/task-independent methodology, exper-\niments with other models (ALBERT, ULMFit) on popular public datasets (Ag News and\nCola) are also discussed in detail.\nKeywords eXplainable artiﬁcial intelligence · Natural language processing ·\nText classiﬁcation · Black-box classiﬁer · Neural network\n1 Introduction\nNowadays, more and more deep learning models such as BERT [ 11] and LSTM [ 18]a r e\nexploited as the ground basis to build new powerful automatic decision-making systems to\nautomatically address complex natural language processing (NLP) tasks, e.g., text classiﬁ-\ncation, question answering (QA), and sentiment analysis. Although deep learning models\nare often very accurate, even exceeding human performance (e.g., in [ 4, 36, 39, 49]), they\nare very opaque and deﬁned as “black-boxes”: given an input, deep learning models provide\nan output, without any human-understandable insight about their inner behavior. The huge\namount of data required to train these black-box models is usually collected from people’s\ndaily lives (e.g., web searches, social networks, e-commerce), increasing the risk of inher-\niting human prejudices, racism, gender discrimination, and other forms of bias [ 5, 26]. For\nthese reasons, new eXplainable Artiﬁcial Intelligence (XAI) solutions are needed to produce\nmore credible and reliable information and services. XAI components will become, shortly,\na design requirement in most data-driven decision-making processes [ 10], and they will be\nrewarded by increased trust, interaction, and access to new forms of data.\nTable1 shows a clear example of a misleading prediction provided by an LSTM model\n1.I n\nthe example, both sentences express Clean language. However, the predictions are extremely\ncontradictory, and the black-box nature of the LSTM model does not allow us to understand\nwhy. Thus, the complexity and the opacity of the learning process signiﬁcantly reduce the\nadoption of those neural networks in real-life scenarios where a higher level of transparency\nis needed. The new eXplainable Artiﬁcial Intelligence (XAI) ﬁeld of research is currently\ntrying to close the gap between model accuracy and model interpretability to effectively\nincrease the adoption of those models in real-life settings.\nThis work proposes T- EBAnO (Text-Explaining BlAck-box mOdels), a novel explanation\nframework that allows understanding the decisions made by deep neural networks in the\ncontext of Natural Language Processing .\nHuman-readable prediction-local and model-global explanations are offered to users to\nunderstand why and how a prediction is made, hence allowing them to consciously trust\nthe model’s outcomes. With the term prediction-local explanation,w em e a nt op r o v i d et h e\nrelation of a speciﬁc input text with the predicted label: the explanation is local to the label\nand the input, and it aims at identifying which regions of the inputs, i.e., the tokens for NLP\nmodels or pixels for computer vision models, are mostly impacting/inﬂuencing the output\nprediction of the model. Instead, with the term model-global explanation, we mean to obtain\ngeneral insights about the model behavior by globally analyzing many local explanations\nover different input texts.\n1 Details on the experiments leading to the reported result are provided in Sect. 6.1 trained to distinguish\nbetween Clean and Toxic comments.\n123\nTrusting deep learning natural-language models via local… 1865\nTable 1 Misleading prediction\nexample of a clean/toxic\ncomment classiﬁcation. The\nsurname of a well-known\npolitician is anonymized\nSentence P(Toxic)\nPolitician-1 is an awesome man 0.17\nPolitician-1 is an intellectual 0.89\nT- EBAnO produces prediction-local explanations through a perturbation process applied\non different sets of interpretable features, i.e., parts of speech, sentences, and multi-layer word\nembedding clusters, which are accurately selected to be meaningful for the model and under-\nstandable by humans. Then, T- EBAnO evaluates the model’s performance in the presence of\nthe perturbed inputs, quantifying the contribution that each feature had in the prediction pro-\ncess through qualitative and objective indexes. The proposed explanations enable end-users\nto decide whether a speciﬁc local prediction made by a deep learning model is reliable and\nto evaluate the general behavior of the global model across predictions. Prediction-local and\nmodel-global explanations are summarized in reports consisting of textual and quantitative\ncontributions, allowing both expert and non-expert users to understand the reasons why a\ncertain decision has been taken by the model under analysis.\nExperimentally, T- EBAnO has been applied to explain: (i) the well-known state-of-the-\nart transformer-based language model BERT [ 11] in a sentiment analysis task, (ii) a custom\nsequence LSTM [ 18] model trained to solve a toxic comment binary classiﬁcation task, i.e.,\ndetecting whether a document contains threats, obscenity, insults, or hate speech, and (iii)\nadditional models like ALBERT [ 24] and ULMFit [ 19] on other two classiﬁcation tasks\nof popular public datasets ( Ag News topic classiﬁcation and Cola sentence acceptability).\nExperimental results show the effectiveness of T- EBAnO in providing human-readable,\nlocal vs global interpretations of different model outcomes.\nThe novel contributions of the current work are provided in the following.\n• The design and development of a new XAI methodology, named T- EBAnO , tailored to\nNLP tasks, to produce both prediction-local and model-global explanations, consisting\nof textual and numerical human-readable reports.\n• The design of effective strategies to describe input textual documents through a set of\nmodel-wise interpretable features exploiting speciﬁc inner-model and domain-speciﬁc\nknowledge (Sect. 4.1).\n• The deﬁnition of a cutting-edge model-global explanation strategy, analyzing the inﬂu-\nence of inter- and intra-class concepts, based on two new metrics, the Global Absolute\nInﬂuence and the Global Relative Inﬂuence scores (Sect. 5.2).\n• A thorough experimental evaluation on many state-of-the-art black-box deep learning\nmodels, such as BERT, LSTM, ALBERT, and ULMFit, on different textual data collec-\ntions and text classiﬁcation tasks. Results show that the proposed approach is general\nand widely applicable, independently from the model or task.\n• A human evaluation of the correlation between the inﬂuence index exploited by\nT- EBAnO (normalized Perturbation Inﬂuence Relation ) and human judgment. We col-\nlected 4320 user evaluations from 108 participants, each evaluating 2 explanations from\n20 input texts, showing that the proposed index is highly correlated with human judgment.\nThe paper is organized as follows. Section 2 discusses XAI literature, Sect. 3 provides an\noverview of the proposed solution, Sect. 4 provides the details about the interpretable features\nextracted by our framework, and Sect. 5 describes how the local and global explanations are\ncomputed. Section 6 presents the experimental results and discusses the prediction-local and\n123\n1866 F. Ventura et al.\nmodel-global explanation reports produced by T- EBAnO . Finally, Sect. 7 concludes this\nwork and presents future research directions.\n2 Literature review\nResearch activities in XAI can be classiﬁed based on [ 1, 17, 41] data-type (e.g., structured\ndata, images, texts), machine learning task (e.g., classiﬁcation, forecasting, clusterization),\nand characteristics of the explanations (e.g., local vs global). More generally, explanation\nframeworks can be grouped into (i) model-agnostic, (ii) domain-speciﬁc, and (iii) task-\nspeciﬁc approaches.\nUp to now, many efforts have been devoted to explaining the prediction process in the\ncontext of structured data (e.g., measuring quantitative input inﬂuence [ 9], by means of local\nrules in [ 7, 37]) and of deep learning models for image classiﬁcation (e.g., [ 15, 43, 48]).\nIn contrast, less attention has been devoted to domain-speciﬁc explanation frameworks for\ntextual data analytics.\n2.1 Model-agnostic approaches\nTools like [ 20, 32, 40] can be applied to explain the decisions made by a black-box model on\nunstructured inputs (e.g., images or texts), and they provide interesting and human-readable\nresults. LIME [ 40] is a model-agnostic strategy that allows a local explanation to be generated\nfor any predictive model. It approximates the prediction performed by the model with an\ninterpretable model built locally to the data object to be predicted. However, the interpretable\nmodel approximates the prediction locally, and it could not represent faithfully what the real\nmodel has effectively learned. SHAP [ 32], instead, is a uniﬁed framework able to interpret\npredictions produced by any machine learning model, exploiting a game-theoretic approach\nbased on the concept of Shapley V alues [44], by iteratively removing possible combinations\nof input features and measuring the impact that the removal of the features has over the\noutcome of the prediction task. PALEX [ 20] is a model-agnostic explanation method that\nprovides multiple local explanations for individual predictions. It uses frequent input patterns\nto generate a precise neighborhood of the prediction and exploits intrinsic interpretability\nof contrast patterns to capture locally important information. Since the above-mentioned\ntechniques are model-agnostic, they might not fully exploit the speciﬁc characteristics of\nthe data domain and the latent semantic information speciﬁcally learned by the predictive\nmodels when computing an explanation. Although they can be applied in the context of\nNLP , they do not provide inner-model awareness , i.e., they are not able to deeply explain\nwhat the model has speciﬁcally learned since they do not exploit such information in their\nexplanation process, leading to less speciﬁc explanations. Moreover, in the speciﬁc case of\nNLP , model-agnostic techniques analyze the impact of singular words over the prediction\nwithout taking into account the complex semantic relations that exist in textual documents\n(i.e., semantically correlated portions of text) and that is actually learned by modern neural\nnetworks. Also, perturbing singular words can have a very limited impact on the prediction\nprocess, in particular when dealing with long texts, other than being very computationally\nintensive, compromising the quality of the explanations.\nT- EBAnO addresses such limitations and is able to increase the precision of the produced\nexplanations and limit the feature search space by i) using domain-speciﬁc feature extraction\n123\nTrusting deep learning natural-language models via local… 1867\ntechniques and ii) exploiting the inner knowledge of the neural network to identify meaningful\ninter-word relations learned by the NLP model.\n2.2 Domain-specific approaches\nAn exhaustive overview of the existing XAI techniques for NLP models, applied in dif-\nferent contexts, such as social networks, medical, and cybersecurity, is presented in [ 34].\nMany works exploit feature-perturbation strategies in the explanation process, analyzing the\nmodel reactions to produce prediction-local explanations, like in [ 2, 28, 32, 35, 40, 48]. This\nstraightforward idea is very powerful but requires a careful selection of the input features to\nbe perturbed.\nDifferently from model-agnostic and domain-agnostic frameworks [32, 40], some strate-\ngies have been explored by domain-speciﬁc works to determine the information contained in\nthe target model, with the aim to select the most relevant features to be perturbed. The feature\nextraction process is of utmost importance in the explanation process since the quality of the\nproduced explanations strictly depends on this step. In [ 35], the authors propose the use of an\napproximate brute-force strategy to analyze the impact that phrases in the input text have over\nthe predictions made by LSTM models. Also, they deﬁne an importance score that exploits\nthe parameters learned by an LSTM model to select the phrases which consistently provided\na large contribution in the prediction of a speciﬁc class. However, this approach has been\ntailored to LSTM models, making it difﬁcult to generalize the solution. In [ 2], the authors\nproposed an explanation strategy tailored to structured and sequential data models with a\nperturbation strategy that exploits the training of a variational autoencoder to perturb the\ninput data with semantically related variations, introducing controlled perturbations. How-\never, this explanation strategy has been mainly focused on explaining sequence to sequence\nscenarios (e.g., machine translation), and the perturbation requires the training, in advance, of\na variational autoencoder model, introducing a further level of opacity and complexity in the\nexplanation process. The authors in [ 25] propose to learn how to explain a predictive model\njointly with the training of the predictor. To this aim, they introduce an encoder-generator\nframework that extracts a subset of inputs from the original text as an interpretable summary\nof the prediction process. Again, the training of a separate model is required to extract the\nwhole explanation, also making this solution equivocal for the end-user. The authors in [ 28]\nproposed an explanation process based on a novel strategy to select the minimal set of words\nto perturb what causes a change in the model’s decision. To this aim, a reinforcement learn-\ning approach has been exploited. However, as in previous cases, this method requires the\ntraining of an external model to extract features to be perturbed, increasing the complexity\nand affecting the reliability of the explanation process itself. The authors in [ 12] propose a\nframework called CREX that allows regularizing the training of DNNs using prior human\nknowledge. The prior human knowledge, consisting of a subset of features highlighted by\ndomain experts, is exploited to let the model focus more on what actually matters for the task.\nHowever, the highlighting operation is time-consuming, it is not always feasible, and it is not\napplicable to already trained models. The authors in [ 8] propose LS-Tree, a model-agnostic\nbut domain-speciﬁc game-theoretic technique based on the Banzhaf value [3] and parse trees\nto analyze several aspects of NLP models such as the nonlinearity, adversarial relationship\ncaptured, and overﬁtting. However, it is more suitable to acquire global insights about the\nmodel behavior instead of explaining single predictions of the model. Moreover, it has high\ncomplexity, especially for long sentences. Finally, its explanations are more suited for an\nexpert audience.\n123\n1868 F. Ventura et al.\nInstead, other techniques are gradient-based and, thus, exploit gradients to produce expla-\nnations [ 43, 45]. In [ 43], the authors propose Grad-CAM, a gradient-based approach that\nhighlights the important regions in the image for the prediction. However, it is suitable for\nconvolutional-based neural networks, and thus, for the computer vision domain. The authors\nin [ 27] propose a Grad-CAM implementation for text classiﬁcation named Grad-CAM-Text.\nHowever, it is only applicable to 1D convolutional neural networks for text classiﬁcation.\nTherefore, it is inapplicable for sequence models such as RNNs or transformer-based mod-\nels as BERT, which are currently the most widespread architectures for NLP tasks. Finally,\nthe authors in [ 45] propose DeepLIFT, a gradient-based technique that computes importance\nscores by explaining the difference between the outputs of the input to explain from the outputs\nobtained by a “reference” input. However, it requires prior knowledge to make assumptions\non reference data. Moreover, it has been only tested on convolutional neural networks, and,\nagain, the version presented in the paper is unsuitable for sequence or transformer-based\nmodels.\nDifferent from the above-mentioned works, T- EBAnO implements a feature-extraction\nprocess that exploits the speciﬁc information learned by the predictive deep natural-language\nmodel, without the need to train external resources. T- EBAnO exploits the embedding\nrepresentation of the textual input data, available in the inner layer of the neural network,\nto identify correlated portions of input text accordingly to the model, which are used in the\nexplanation process. To support this choice, we recall that textual embeddings have interesting\ninterpretable properties, as described in [ 46]. Following the insights discussed by the authors\nin [13], modern natural-language models incorporate most of the context-speciﬁc information\nin the latest and inmost layers. T- EBAnO exploits the textual embedding representations as\ninterpretable features to explain model outcomes.\n2.3 Task-specific approaches\nFinally, not every task can be explained with model-agnostic or domain-speciﬁc approaches.\nThis is why interpretable task-speciﬁc solutions are also relevant. In [ 53], the authors focused\ntheir attention on explaining the duplicate question detection task developing a speciﬁc model\nbased on the attention mechanism, proposing to interpret the model results by visually ana-\nlyzing their attention matrix to understand the inter-words relations learned by the model.\nHowever, exploiting attention can be performed only for black-box models that are based\non this mechanism, and it can be hard to interpret for non-expert users. The authors of [ 52]\ndeveloped an explainable tag-based recommendation model that increases the interpretability\nof its results by proposing an overview of user’s preference correlated with learned topics\nand predicted tags, but without actually focusing on the reliability of the model or on the\npossible presence of bias. In [ 31], the authors introduced a speciﬁc linguistic explanation\napproach for fuzzy classiﬁer decisions, which are shown in textual form to users. They focus\non a high abstraction level of explanations providing reasons, conﬁdence, coverage, and\nfeature importance. However, their approach does not take into account the complexity of\ndeep learning models. In [ 22], the authors propose a framework for recognizing symptoms\nof cognitive decline that provides natural language explanations of the detected anomalies\ngenerated from a trained tree regression algorithm. However, this solution is customized for\nthis speciﬁc task and not easily extendable to other contexts. In [ 21], the authors propose\ntwo solutions, for the k-nearest neighbor and the random shapelet forest algorithms, solving\nthe problem of locally and globally explainable time-series tweaking. These solutions are\nsuitable for time-series classiﬁcation, and they are not easily applicable for different tasks.\n123\nTrusting deep learning natural-language models via local… 1869\nT- EBAnO proposes a new local and global explanation process for state-of-the-art deep\nNLP models. By exploiting a perturbation-based strategy similar to that described in [ 48],\nwhich was successfully tailored to image data, T- EBAnO ﬁlls in the gap of missing cus-\ntomized solutions for explaining deep NLP models by introducing a totally redesigned\narchitecture and experimental section. Speciﬁcally, we introduce (i) a novel feature extraction\nprocess speciﬁcally tailored to textual data and deep natural language models, (ii) new per-\nturbation strategies, (iii) an improved version of the index proposed in [ 48] able to quantify\nthe inﬂuence of the input feature over local predictions tested in a new domain (NLP), and\n(iv) novel class-based global explanations, besides extending the experiments to new mod-\nels and use cases, and presenting a human evaluation of the exploited index and proposed\nexplanations.\n3 T-EBAnO overview\nT- EBAnO explains the inner functionalities of deep neural networks (DNN) models in the\ncontext of NLP analytics tasks. Deep learning models act as a black-box to the end-user\nbecause the model’s internal decision process is obscure [ 17]. However, T- EBAnO requires\nthat the model’s architecture is known. For instance, for explaining the decision-making\nprocess of a transformer-based model, its architecture is known, but why it produces certain\npredictions is unknown and requires an explanation. Thus, T- EBAnO exploits the knowledge\nabout the architecture of the speciﬁc model to make more reliable and faithful explanations,\nin contrast to completely model-agnostic methodologies that could be applied to arbitrary\nmodels but that cannot exploit the knowledge hidden into the model.\nT- EBAnO ’s architecture is shown in Fig. 1 and includes different building blocks. Both\nmodel-agnostic (i.e., part of speech, sentences) and model-aware (i.e., multi-layer word\nembeddings) features are extracted by T- EBAnO .T h e model-aware technique is the one\nthat requires and exploits the knowledge about the model’s architecture, while the model-\nagnostic techniques are completely independent of the model. However, all the techniques\nare domain-speciﬁc and exploit the semantic feature of textual data.\nFor a given classiﬁcation task, an input document is provided to the pre-trained deep\nlearning model\n1⃝ that outputs its predicted class label 2⃝ . Thus, T- EBAnO extracts a\nset of interpretable features 3⃝ by exploiting either NLP techniques or the analysis of the\nknowledge hidden in the model itself (Sect. 4.1). Then, it performs the perturbation of the set\nMulti-LayerWord\nEmbedding PartofSpeech\nSentences\nNouns,Verbs,\nAdjectives\nClustersof\ninterpretablesetof\nwords\nInterpretableFeatureExtraction3\nRemoval\nSubstitution\nPerturbation4\nInputDocument\nExplanation\nComputation\nLocalExplanations\nClassLabel1\nDNN Model\n1\n5\n2\nPerturbedTexts\nPerturbedProbabilities\nFig. 1 T- EBAnO local explanation process\n123\n1870 F. Ventura et al.\nof interpretable features and tests the model’s outcomes on the perturbed inputs 4⃝ (Sect. 4.2).\nSpeciﬁcally, the perturbed inputs are new texts produced by applying the perturbation to each\ninterpretable feature extracted. Then, the model’s predictions (perturbed probabilities) for\nthe perturbed texts are evaluated to measure each feature’s impact. The perturbation of the\ninterpretable features can inﬂuence the model outcome in different ways, as described in the\nfollowing:\n• Case (a): the probability of the class under analysis increases. It means that the analyzed\nfeatures were negatively impacting the process;\n• Case (b): the predicted probability decreases. It means that the perturbed features were\npositively impacting the class under analysis;\n• Case (c): the predicted probability remains roughly unchanged. It means that the portion\nof the input is irrelevant to the predictive model under analysis.\nThe signiﬁcance of the difference in the prediction process before and after the perturbation\nis evaluated through the nPIR index, a quantitative metric to estimate the effect of the\nperturbation strategy (Sect. 4.3). Thus, T- EBAnO generates the local explanation report\n5⃝ ,\nshowing the results of the analysis of the perturbations through an informative dashboard\n(Sect. 5.1).\nFinally, aggregating the local explanations produced for a corpus of input documents,\nT- EBAnO provides model-global explanations highlighting relevant inter- and intra- class\nsemantic concepts that are inﬂuencing the deep neural network decision-making process at\na model-global level (Sect. 5.2).\n4 Interpretable features\nThis section describes the interpretable feature extraction (Sect. 4.1) and perturbation (Sect.\n4.2). Then, it introduces the quantitative index that measures the feature importance (Sect.\n4.3) exploited by T- EBAnO . Finally, it details the Multi-layer Word Embedding feature\nextraction technique (Sect. 4.4).\n4.1 Interpretable feature extraction\nThe interpretable feature extraction block identiﬁes meaningful and correlated sets of words\n(tokens) having an inﬂuence on the outcomes of the NLP model under the exam. It represents\nthe most critical and complex phase in the explanation process workﬂow. A set of words is\nmeaningful for the model if its perturbation in the input document produces a meaningful\nchange in the prediction outcome. On the other hand, a set of words is meaningful for a user\nif s/he can easily understand and use it to support the decision-making process.\nT- EBAnO considers both word (tokens) and sentence granularity levels to extract the set\nof interpretable features. Moreover, T- EBAnO records the position of the extracted features\nin the input text since the context in which words appear is often very important for NLP\nmodels.\nT- EBAnO includes three different kinds of interpretable feature extraction techniques:\n1. Multi-layer Word Embedding (MLWE) feature extraction. This strategy is the most pow-\nerful technique since it exploits the inner knowledge learned by the model to perform the\nprediction. Speciﬁcally, it performs an unsupervised clustering analysis to group related\ninput tokens based on the inner representation (i.e., embedding) assigned by the model.\n123\nTrusting deep learning natural-language models via local… 1871\nEach group of tokens could have inﬂuenced the prediction of the model in a similar way.\nThe unsupervised analysis performed by the MLWE ﬁgures out by itself which and the\nright number of tokens to assign to each cluster and which cluster of tokens is the most\ninﬂuential. To access the inner knowledge of the network, this technique needs to know\nthe inner details of the model under analysis. However, the process can be easily adapted\nto be compliant with different deep architectures (e.g., as reported in [ 48]) and their hidden\nlayers. A detailed description of the MLWE feature extraction technique is provided in\nSect. 4.4.\n2. Part-of-Speech (PoS) feature extraction. This strategy explores the semantic meaning of\nwords by looking at which part-of-speech they belong to (e.g., nouns, adjectives). The\nintuition behind this type of feature extraction is that the semantic difference corresponding\nto distinct parts-of-speech can differently inﬂuence the model outcome. Firstly, the input\ntext is tokenized, leading to three features: the token itself, its position in the text, and\nits pos-tag (i.e., part-of-speech tagging). Then, tokens are divided into correlated groups:\nadjectives, nouns, verbs, adverbs, and others. Each group is considered as a separate\ninterpretable feature by T- EBAnO in the perturbation phase (e.g., the POS-Adjectives\ninterpretable feature extracts all the adjectives present in the input text and not a partial\nsubset of it). This is because the main objective of the POS is to measure the inﬂuence of\neach entire part-of-speech, while the MLWE feature extraction discovers the exact more\ninﬂuential tokens. Understanding which POS most inﬂuenced the original prediction of\nthe model can be useful to understand if the model is looking to the correct semantic\naspect. Indeed, different tasks are usually inﬂuenced by different parts-of-speech. For\ninstance, a well-trained model for sentiment analysis usually exploits adjectives to predict\nthe sentiment. Therefore, adjectives should be the most inﬂuential and important part-of-\nspeech in the model’s decision-making process. Thus, T- EBAnO creates an interpretable\nfeature for each analyzed part-of-speech.\n3. Sentence-based (SEN) feature extraction. This strategy considers each sentence separately\nto assess its inﬂuence on the model decisions. The straightforward intuition behind this\nstrategy is to verify if the model captures the complete meaning of a sentence and uses it\nto derive the outcome. The sentence feature extraction characterizes the input text with the\nposition of the sentence and the sentence itself. In this case, T- EBAnO creates a feature\nfor each sentence in the input text.\nThen, separately for each feature extraction method, T- EBAnO tests pairwise combina-\ntions of features to create larger groups of tokens corresponding to more complex concepts.\nFor instance, for Part-of-Speech, it creates a feature with the combination of Adjectives and\nVe r b s, Adjectives and Nouns, etc. For the Sentence-based feature extraction, it creates a fea-\nture with the combination of the ﬁrst sentence and the second, another with the ﬁrst sentence\nand the third, and so on. Finally, for the Multi-layer Word Embedding feature extraction, it\ncreates a feature with the combination of the ﬁrst cluster of words and the second, the ﬁrst\nand the third, and so on (more details on MLWE features are provided in Sect. 4.4). T-\nEBAnO creates pairwise combinations of features only within the same feature extraction\nmethod and not among different feature extraction methods because each of them considers\ndifferent aspects of the input text, i.e., PoS features are combined with PoS features and not\nwith MLWE features. This allows T- EBAnO to efﬁciently explore a wider search space of\ninterpretable features, hence ﬁnding even more relevant prediction-local explanations.\n123\n1872 F. Ventura et al.\n4.2 Interpretable feature perturbation\nAfter the extraction of the interpretable feature sets, a perturbation phase is performed by\nintroducing noise and consequently assessing the impact of the perturbed features on the\nmodel outcomes. Adding noise to the model input is a well-known technique adopted by\ndifferent state-of-the-art approaches [ 2, 32, 40, 48] to study the model behavior through the\neffects on the outcomes. Different input data types require different perturbation strategies.\nIn case of textual data, the perturbation can be performed by feature removal or feature\nsubstitution.\nIn the feature removal perturbation approach provided by T- EBAnO , all the interpretable\nfeatures are iteratively removed from the input text, producing new perturbed variations of\nthe input itself. The perturbed variations of the input are then fed back into the model under\nanalysis, and its predictions are collected and analyzed by T- EBAnO to produce the local\nexplanation report (see Sect. 5.1). For instance, for multi-layer word embedding features, each\ncluster of tokens is removed (one cluster at a time) from the input text, each one producing\na new perturbed text. For part-of-speech features, each part-of-speech removal produces a\nnew perturbed text. Finally, for sentence-based features, the removal of each sentence, one\nat a time, produces a new perturbed text.\nExamples of explanations produced by feature removal perturbation are shown in Fig. 3b–\nd. From the input text in Fig. 3a, the words highlighted in Fig. 3b, the sentence highlighted\nin Fig. 3c and the words identiﬁed by MLWE in Fig. 3d are removed. A discussion on these\nexamples is provided in Sect. 5.1.\nThe feature substitution perturbation was also explored by T- EBAnO . While the removal\nperturbation causes an absence of the concept associated with the removed words, the sub-\nstitution perturbation introduces a new, possibly related, concept that can cause a change in\nthe prediction. The feature substitution perturbation requires an additional step to select new\nwords that will replace the current ones. In T- EBAnO , the substitution of words with their\nantonyms is exploited. This strategy turned out to be very powerful in some speciﬁc cases\n(e.g., Adjective-POS perturbation), but in general, it has several limitations: (i) some words\ncan have many antonyms and the optimal choice might depend on the context, (ii) antonyms\ndo not exist for some words (e.g., nouns), and (iii) the choice of the new words to be inserted\nin the substitution of the feature is task-speciﬁc (e.g., antonyms work with opposite class\nlabels like Positive and Negative in sentiment analysis, but are not suited with independent\nclass labels as in topic detection). Thus, the effectiveness of this perturbation strategy is\naffected by these limitations. Figure 3e, f shows two examples of explanations performed\nusing this technique. For the Adjective-POS features, it is straightforward to ﬁnd meaningful\nantonyms. On the contrary, for V erb-POS features, the result is very difﬁcult to evaluate\nsince verbs like { was, have } are substituted with { differ, lack }. This feature\nperturbation strategy remains an open task left for further inspection in future works. For\ninstance, we plan to analyze task-speciﬁc and expert-driven substitution perturbations. For\nexample, for a comment toxicity classiﬁcation (i.e., predicting if an input text contains toxic\nor clean language), the effects of substitution w.r.t. gender, minority, named entity, or other\npossible biases is of absolute interest. For a sentence grammar acceptability classiﬁcation\ntask (i.e., predicting if a sentence is grammatically acceptable or unacceptable), introducing\nexpert-driven substitutions to understand if the classiﬁer is robust to critical linguistic aspects\nis another example. In this paper, such implementations are out of scope because we currently\ndevise T- EBAnO to be as general as possible across different classiﬁcation tasks without\nrequiring human expertise, and we reach this goal by means of the removal perturbation.\n123\nTrusting deep learning natural-language models via local… 1873\n4.3 Interpretable feature influence measurement\nT- EBAnO exploits an improved version of the quantitative index proposed in [ 48], namely\nnormalized Perturbation Inﬂuence Relation (nPIR) to measure the inﬂuence of each inter-\npretable feature extracted. This improved index solves the issues of asymmetry and unbounded\nvalues, which affect the index previously proposed in [ 48]. It assesses the importance of an\ninput feature for a given prediction, analyzing its performance before and after the perturba-\ntion of a feature (or set of features) extracted from the input data.\nFormally, given a model able to distinguish between a set of classes c ∈ C.L e t ci ∈ C\nbe the class-of-interest for which the local explanation has to be computed. Given the input\nsample I , the explanation process extracts the set of interpretable features F. For each feature\nf ∈ F, the perturbation is applied, and the reactions of the predictive model are evaluated.\nThese reactions represent the contribution of f to the prediction process. We quantify the\ninﬂuence of f over ci through the nPIR index.\nLet p\no,ci be the output probability of the original input I (the unperturbed input) to belong\nto the class-of-interest ci,a n d pf,ci the probability of the same input, with the feature f\nperturbed, to belong to the same class. Let us consider the predicted class distributions as∑ C\nc Po,c = 1 and similarly ∑ C\nc P f ,ci = 1. For instance, the output of the model is given by\na SoftMax layer.\nWe introduce a generic deﬁnition of inﬂuence relation for a feature f by combining the\noutcomes of the model po,ci and pf,ci before and after the perturbation process. We want\nsuch inﬂuence relation (i.e., the nPIR) to range in the [−1;1] interval. An nPIR value for f\nclose or equal to 1 represents a positive relevance for the concept in f over the prediction\nof class ci. On the opposite, an nPIR value for f close or equal to −1 represents a negative\nimpact of that feature over the prediction of class ci.A n nPIR value close to 0 means that f\nis neutral w.r.t. the prediction of class ci.\nThe nPIR derives from the combination of two sub-indicators: the Amplitude of Inﬂuence\nΔI and the Symmetric Relative Inﬂuence S R I .T h e ΔI for a feature f is deﬁned as in Eq.\n1 and ranges from −1 to 1 since the domain for probability values is included in [0,1].\nΔI f = po,ci − pf,ci (1)\nA ΔI f > 0 represents a positive inﬂuence of the feature f for class ci since the perturbation\nof the corresponding portion of input causes a decrease in its probability to belong to the class-\nof-interest. Thus, f is relevant for class ci. Similar reasoning could be made for ΔI\nf < 0\nrepresenting a negative inﬂuence of the feature f for ci.\nThe amplitude alone does not reﬂect the overall contribution of f completely. In par-\nticular, the absolute distance between two values can be low if the values are small w.r.t.\nthe probability values domain, but, their relative distance can still be signiﬁcant. This effect\nshould not be ignored as well. Because of this, we need to consider also the relative inﬂuence\nof f . To capture the relative inﬂuence of f , a straightforward approach would be to compute\nthe ratio between the probabilities. However, as shown in [ 48], such score is asymmetric: the\nratio\npo,ci\npf,ci will range from 0 to 1 in case of negative inﬂuence and from 1 to ∞ in the other\ncase. So, it will be difﬁcult to quantitatively compare positive and negative inﬂuences. To\novercome this problem, we deﬁne the Symmetric Relative Inﬂuence for a feature f as in Eq.\n2. This index evaluates the relative inﬂuence that f has over po,ci and pf,ci. The symmetry\nof this score allows measuring the relative inﬂuence of the feature f before and after the\n123\n1874 F. Ventura et al.\nperturbation regardless of its positiveness or negativeness.\nSRI f = po,ci\npf,ci\n+ pf,ci\npo,ci\n(2)\nBy combining Eqs. 1 and 2,w ed e ﬁ n et h e Perturbation Inﬂuence Relation for f in the\nrange (−∞,+∞).W eﬁ n a l l ya d dt h eSoftsign [16] function to obtain a linear approximation\nof the inﬂuence close to 0 and to bound in a nonlinear way the very high positive or negative\nvalues in the [−1;1] range. Hence, the normalized Perturbation Inﬂuence Relation (nPIR )\nof a feature f for a class-of-interest ci is deﬁned in Eq. 3.\nnPIR\nf (ci) = softsign(ΔI f ∗ SRI f )\n= softsign(pf,ci ∗ b − po,ci ∗ a) (3)\na = 1 − po,ci\npf,ci\n;b = 1 − pf,ci\npo,ci\n(4)\nThe coefﬁcient a is the contribution of input o w.r.t. the perturbed input. Similarly, b\nrepresents the contribution of the perturbation of f w.r.t. the original feature. The higher the\nnPIR f (close to 1), the more the feature f is positively inﬂuencing the class-of-interest.\nOn the opposite, the lower the nPIR f (close to -1), the more the feature f is negatively\ninﬂuencing the class-of-interest.\n4.4 Multi-layer word embedding (MLWE) feature extraction\nIn this section, the terms words and tokens are often used interchangeably. However, the\ntokenization process of the explained model also drives T- EBAnO . For example, if the\ntokenizer of the explained model removes the punctuation and stopwords, T- EBAnO -MLWE\ndoes not consider it. Otherwise, if the tokenization step keeps the punctuation and stopwords,\nthen also T- EBAnO -MLWE considers them as possible inﬂuential tokens/words.\nDeep neural networks are trained to extract knowledge from training data learning a\ncomplex numerical model spreading this knowledge on multiple hidden layers. During the\nClustering\n(K-Means)\nPerturbation&\nLocalExplanation2-kfeatures\n3-kfeatures\nkmax features\nExpl.1\nExpl.2\nExpl.Kmax-1 MostInformative\nLocalExplanation\nKscore\nEvaluation\nDNN Model\nMulti-Layer\nKnowledge\nExtraction\nVectorial\nAggregation\nMulti-LayerWord\nEmbeddings\nMostInformative\nLocalExplanation\nInputDocument\nK=2\nK=3\nK=Kmax\n11 2\n34\n5\n6\n7\n8\nFig. 2 T- EBAnO MLWE feature extraction process\n123\nTrusting deep learning natural-language models via local… 1875\nprediction process of previously unseen data, all these layers contribute to the outcome. Thus,\nto get a reliable explanation, it is necessary to mine all the knowledge hidden along with the\nlayers of the model. Thanks to the Multi-layer Word Embedding (MLWE) feature extraction,\nT- EBAnO can achieve this goal. Speciﬁcally, T- EBAnO analyzes the outcomes of multiple\nhidden layers to extract the numerical representation of the input at different levels of the\nnetwork. The Multi-layer Word Embedding feature extraction process is shown in Fig. 2.\n4.4.1 Embedding knowledge extraction and aggregation\nFirstly, given an input document 1⃝ , a tensor containing the numerical embedding representa-\ntions of different tokens in different layers is extracted 2⃝ . Then, the intermediate embeddings\nof each layer are aggregated (e.g., through average, sum or concatenation) on the layers’ axis\nto a single-layer vector representation for each token. Then, only in the case of sub-words\nrepresentation, another aggregation is performed to reconstruct full-words from the sub-\nwords tokens. The aggregation of the multiple channels’ and the sub-tokens representations\ncompose the vectorial aggregation step and depends on the speciﬁc model’s architecture.\nFinally, their dimensionalities are further reduced through PCA to obtain an embedding vec-\ntor representation for each input token\n3⃝ . The outcomes of the vectorial aggregation and\nthe dimensionality reduction steps are the Multi-layer Word Embedding representation of\nthe input document 4⃝ , where each full-token is represented with a small and dense vector\nthat approximates the meaning and knowledge learned by the model. The intuition is that\nwords with a similar MLWE representation are considered highly correlated by the model,\nand, if grouped together, they represent key input concepts that most probably are inﬂuenc-\ning the current prediction. The MLWE feature extraction, and in particular, the extraction\nof the aggregated word embeddings from multiple layers has to be achieved in different\nways depending on the neural network architecture under the exam. Further details about\nMLWE feature extraction tailored to LSTM and to BERT and how MLWE ﬁts other NLP\narchitectures are provided in Sect. 6.2.\n4.4.2 Unsupervised embedding analysis\nOnce the MLWEs are extracted, they are analyzed through an unsupervised clustering anal-\nysis 5⃝ to identify sets of correlated words that share common behaviors inside the model\nunder exam. Speciﬁcally, the unsupervised analysis aims to identify the smallest groups of\ninput words (tokens) that have the highest impact on the model outcome. For this purpose,\nT- EBAnO exploits the K-Means [ 30] clustering algorithm since it provided good perfor-\nmance in a similar context [ 48] and represents a good trade-off with computational time. A\ncritical parameter when dealing with K-Means is setting the desired number of groups K to\ncorrectly model interesting subsets of data. T- EBAnO applies K-Means to identify a number\nof groups ranging in [2, K\nmax], where the max number of clusters Kmax is a function of the\ninput size and has been empirically set to:\nKmax =\n√\nntk + 1( 5 )\nOn the one hand, using small ﬁxed values of K with large input texts leads to large clusters of\nwords containing both inﬂuential and less impacting words, and consequently, the explanation\nprovided will be of low interest. On the other hand, the number of tokens n\ntk in a text can be\nvery high, and it would be neither feasible nor useful to evaluate partitioning that takes into\naccount values of K as large as the number of tokens n\ntk. For this reason, the evaluation of a\n123\n1876 F. Ventura et al.\nnumber of clusters K that is at most equal to the root of the number of tokens ntk in a text allows\nmaintaining a good trade-off between partitioning size and performance. This allows for\nreducing the search space, without affecting the quality of the features. T- EBAnO produces\na quantitative explanation (as detailed in Sect. 5.1) exploiting the normalized Perturbation\nInﬂuence Relation (nPIR) index (introduced in Sect. 4.3) for each K\n6⃝ . Speciﬁcally, for each\nvalue of K ∈[ 2, Kmax], K perturbations will be analyzed, each one producing a new version\nof the input text applying the perturbation over the tokens of the current cluster. Then, the\noutcomes of the model by presenting the new perturbed texts are evaluated, producing the\nnPIR index for each cluster perturbation of each possible K (dot lines in\n6⃝ ). In this way, a\nlarge number of potentially useful local explanations are produced by T- EBAnO .\n4.4.3 Most informative local explanation evaluation\nThe objective, however, is to provide only the best explanation to the end-user. T- EBAnO\nselects the most informative local explanations as those extracting the most valuable knowl-\nedge from the behavior of the model over a single prediction. To this aim, ﬁrstly, T- EBAnO\nassigns a feature informative score (FIS) to each feature (i.e., each cluster of words), exploit-\ning the nPIR index, as follows:\nFIS(κ) = max\n( (\nα(nPIR κ) + β(1 − κtk/ntk)\n)\n,0\n)\n(6)\nwhere κ is the current cluster, κtk is the number of tokens inside the cluster κ, ntk is the total\nnumber of tokens and nPIR κ is the inﬂuence score of the current cluster κ, which measures\nthe positive or negative inﬂuence of perturbing the tokens in κ (as discussed in Sect. 4.3). The\nratio κtk /ntk represents the percentage of tokens inside the cluster over the total number of\ntokens. The FIS (κ) score tends to maximize the inﬂuence of the feature ( nPIR )a n d minimize\nthe size of the feature κtk/ntk (maximizing (1 − (κtk /ntk))).\nThe hyperparameters α and β are the weights assigned, respectively, to the nPIR and the\ntokens ratio score (1−(κtk /ntk)). They determine the relative contribution of the inﬂuence of\nthe feature and its size. In our settings, we assigned a weight of 0.60 to the inﬂuence and 0.40\nto the size of the features ( α = 0.60 and β = 0.40) because selecting inﬂuential features is of\nprevalent importance, and only secondly, we would like to minimize the number of tokens.\nOn the contrary, selecting small-size features which are not inﬂuential would be useless. An\nexperimental evaluation of α and β hyperparameters is provided in Sect. 6.7.\nThe range of nPIR is [ −1,1] (as discussed in Sect. 4.3), where 1 indicates a very high\npositive inﬂuence for the class of interest. The range of (1 − (κ\ntk /ntk )) is [0,1]. Therefore,\nthe feature informative score FIS , with α = 0.60 and β = 0.40 (or any values of α and\nβ whose sum is 1), is in the range [0,1]. The negative values are undesired because we are\nlooking for positively inﬂuential features for the class of interest. A FIS = 0 is obtained by\na feature whose size is toward the 100% of the tokens and whose inﬂuence is toward 0. A\nFIS = 1 is obtained by a feature with few tokens (e.g., less then 1%) and with an inﬂuential\nscore toward 1. The higher the FIS score, the more informative and shorter the feature is.\nThen, for each value of K (i.e., each possible partition analyzed), a score is computed 7⃝\nby taking the max of the FIS score over its clusters of words.\nKscore = max\nκ∈K\n(\nFIS(κ)\n)\n= max\nκ∈K\n(\nmax\n(\n(α(nPIRκ) + β(1 − κtk/ntk)),0\n)) (7)\n123\nTrusting deep learning natural-language models via local… 1877\nTable 2 Example of the most informative local explanation (cluster 3.1)a n d best K division (K = 3) selection\nusing MLWE for an input text with 9 tokens predicted as Positive by an NLP model ﬁne-tuned for sentiment\nanalysis\nK k Highlighted Clusters ktk ktk/ntk nPIR FIS\n2 2.1 Yesterday I saw a movie that 3 3/9 0.990 0.861\npositively surprised me\n2 2.2 Yesterday I saw a movie that 6 6/9 0.001 0.134\npositively surprised me\n3 3.1 Yesterday I saw a movie that 2 2/9 0.999 0.911\npositively surprised me\n3 3.2 Yesterday I saw a movie that 4 4/9 0.001 0.228\npositively surprised me\n3 3.3 Yesterday I saw a movie that 3 3/9 0.000 0.267\npositively surprised me\nThe K with the highest Kscore is selected as the best. Hence, K clusters of words are created,\nwith each κ ∈ K being a feature including some neutral features (or negative inﬂuential, i.e.,\nnPIR ≤ 0) and, generally, one very highly inﬂuential feature. Finally, the cluster κ with the\nhighest FIS (κ) will be the most informative local explanation 8⃝ .\nTable 2 shows an example of the analysis made by T- EBAnO using MLWE with a\nshort input text consisting of 9 tokens predicted with the label Positive with high conﬁdence\n(≈ 0.99) in a sentiment analysis task. The column K represents the different numbers of\nclusters analyzed by T- EBAnO ,c l u s t e r κ ∈ K is denoted as K .k (e.g., 2.1 is the ﬁrst\ncluster of the division K = 2), k tk represents the number of tokens inside the cluster, k tk /ntk\nrepresents the ratio between the tokens in the cluster and the total number of tokens, nPIR\nand FIS are the inﬂuence score and the feature informative score obtained by cluster k,\nrespectively. The tokens of each cluster are highlighted in cyan in the input text (column\nHighlighted Clusters ).\nThe partitions analyzed by T- EBAnO are K ∈[ 2,3] (K\nmax = √9 + 1 ≈ 3). The\nﬁrst partition ( K = 2) ﬁnds two clusters of tokens: cluster 2.1 containing 3 tokens and\ncluster 2.2 containing 6 tokens (highlighted in cyan). The current most informative local\nexplanation is cluster 2.1, because it has the highest FIS score among the clusters of K = 2.\nThen, T- EBAnO analyzes the clustering results with K = 3. The current most informative\nlocal explanation is cluster 3.1 , because it has the highest FIS score among the clusters of\nK = 3. Overall, the local explanation cluster 3.1 has a higher FIS score than cluster 2.1\n(0.911 > 0.861), then K = 3 is selected as the best K value, and cluster 3.1 is the ﬁnal most\ninformative local explanation.\n5 Explanations\nThis section presents the prediction-local (Sect. 5.1) and the model-global (Sect. 5.2) expla-\nnation processes implemented in T- EBAnO .\n5.1 Prediction-local explanations\n123\n1878 F. Ventura et al.\nThis film was very awful. I have never seen such a bad movie.\n(a) Original text\nThis film was very awful. I have never seen such a bad movie.\n(b) EXP1: Adjective - POS feature extraction with removal perturbation.\nThis film was very awful. I have never seen such a bad movie.\n(c) EXP2: Sentence feature extraction with removal perturbation.\nThis film was very awful. I have never seen such a bad movie\n(d) EXP3: Multi-layer word embedding feature extraction with removal perturbation.\nThis film was very [awful] nice. I have never seen such a [bad] good movie\n(e) EXP4: Adjective-POS feature extraction with substitution perturbation.\nThis film [was] differ very awful. I [have] lack never seen such a bad movie.\n(f) EXP5: Verb-POS feature extraction with substitution perturbation.\nFig. 3 Examples of a textual explanation report. The original text was labeled by BERT as Negative with a\nprobability of 0.99. The most relevant features are reported and highlighted in cyan. Removed tokens for the\nsubstitution perturbation are in squared brackets and followed by the new inserted tokens\nTable 3 Quantitative explanation for example in Fig. 3. P is the positive label, N is the negative label. The\n(sub.) sufﬁx indicates that the substitution perturbation has been applied. Otherwise, the removal perturbation\nhas been applied\nExplanation Feature f Lo Lf nPIRf (N)\nEXP1 POS-Adjective N P 0.998\nEXP2 Sentence N N 0.000\nEXP3 MLWE N P 0.984\nEXP4 POS-Adjective (sub.) N P 0.999\nEXP5 POS-Verb (sub.) N N 0.000\nTo produce the local explanations, T- EBAnO exploits the outcomes of the model when\nfed with the original input and its perturbed versions. A local explanation consists of two\nmain parts: a textual explanation (Fig. 3)a n da quantitative explanation (Table 3), as detailed\nin the following.\n5.1.1 Textual explanation\nThe textual explanation highlights the most relevant sets of features for the model under\nanalysis, also allowing the understanding of the context in which they appear. Many sets of\nfeatures can be extracted for each interpretable feature extraction technique. Figure 3 shows a\nsimple example of textual explanations. For this example, the BERT model has been trained\nto detect the sentiment of a textual document, either positive (P) or negative (N). Given the\ninput document in Fig. 3a, the model outputs a negative sentiment. So, the user can inspect\nthe highlighted features (in cyan) in the textual explanations in Fig. 3b–f to ﬁnd out which\n123\nTrusting deep learning natural-language models via local… 1879\nare the most important sections of the input that have been exploited by the model to make\nits decision.\n5.1.2 Quantitative explanation\nThe quantitative explanation shows the inﬂuence of each set of extracted features for the\nprediction (separately) by evaluating the nPIR index ( normalized Perturbation Inﬂuence\nRelation) introduced in Sect. 4.3.T h e nPIR index is computed by T- EBAnO for each fea-\nture extracted by all the feature extraction techniques, for the class-of-interest (usually the\npredicted label for the input text).\nExploiting the nPIR index, we can deﬁne thresholds to identify highly inﬂuential\nand informative explanations. For instance, considering a threshold nPIR\nt > 0, if\n−nPIR t ≤ nPIR f ≤ nPIR t , then the difference between the probabilities before and\nafter the perturbation of f could be considered not sufﬁciently informative. Instead, values\nof nPIR f < −nPIR t (or nPIR f > nPIR t ) mean that the perturbation of feature f is\ncontributing negatively (or positively) to the prediction by decreasing (or increasing) the\nprobability of belonging to the class-of-interest.\nTable3 shows the quantitative explanations for the textual explanations in Fig. 3. For each\ninterpretable feature f , the labels assigned by the model before and after their perturbation\nare reported in columns L\no and L f , respectively, along with the nPIR value calculated for\nthe class-of-interest negative (N). Perturbing the POS adjectives in Fig. 3b( E X P 1 )o rt h e\nMLWE cluster in Fig. 3d( E X P 3 )t h enPIR is very close to 1. This means that these sets of\nfeatures are very relevant for the model outcome: removing one of these features will cause\ncompletely different outcomes from the model, changing the prediction from negative (N)\nto positive (P). Instead, the perturbation of the sentence in Fig. 3c (EXP2) is not relevant at\nall for the model, showing a value of nPIR equal to 0. We can conclude that the feature sets\n{ awful, bad }a n d{ was, awful, bad, movie } are the real reason why the\nmodel is predicting the negative class. The information contained in the sentence { This\nfilm was very awful } instead does not justify the model outcome alone, like the rest\nof the text that is also contributing to the prediction.\nThe quantitative explanations obtained through the substitution perturbation ( EXP4 and\nEXP5) have been also reported in Table 3. Even from these results, it is evident that the sub-\nstitution perturbation has great potential in expressiveness when it is possible to ﬁnd suitable\nantonyms. In the case of Adjective-POS substitution (EXP4), the quantitative explanation\nshows a nPIR value close to 1. On the contrary, in the case of EXP5, verbs are replaced\nwith semantically incorrect words (not antonyms) in the context of the phrases, showing no\nimpact in the prediction process with a nPIR equal to 0. Therefore, as discussed in Sect. 4.2,\nthis perturbation strategy remains an open task left for further inspection in future works.\n5.2 Per class model-global explanation\nT- EBAnO is able to provide per-class model-global explanations of the prediction process.\nThe local explanations computed for a corpus of input documents are aggregated and analyzed\ntogether, highlighting possible misleading behaviors of the predictive model.\nTwo indices have been introduced to measure the global inﬂuence of the corpus of input\ndocuments: (i) the Global Absolute Inﬂuence (GAI) described by Algorithm 1, and (ii) the\nGlobal Relative Inﬂuence (GRI) deﬁned in Eq. 8.T h e GAI score measures the global impor-\ntance of all the words impacting the class-of-interest, without distinction concerning other\n123\n1880 F. Ventura et al.\nc₀\ncn\nc₁\n(a) Absolute inﬂuential words (GAI).\nc₀\ncn\nc₁\n(b) Relative inﬂuential words (GRI).\nFig. 4 Inﬂuential set of words at model-global level for class-of-interest c0\nclasses (Fig. 4a). On the other hand, the GRI score evaluates the relevance of the words\ninﬂuential only (or mostly) for the class-of-interest, differently from other classes (Fig. 4b).\nThe global explanations are computed for each available class c ∈ C, analyzing the set of\nlocal explanations produced by T- EBAnO from a dataset of input texts D. For each document\nd ∈ D, the set of local explanations Ed are produced, where each explanation ed, f ∈ Ed is\nthe explanation computed over the feature f (e.g., cluster of tokens) containing the list of\ntokens of the current feature and their inﬂuence value (nPIR). Only MLWE explanations are\nexploited to produce the global explanations since it is the only feature extraction strategy\nthat exploits inner model knowledge (as discussed in Sect. 4.4).\nAlgorithm 1: Global Absolute Inﬂuence.\nInput: Dataset D, Classes C .\nOutput: GAI score ∀ class label c ∈ C and lemma l ∈ L.\n1 GAI ← initHashMap(0);\n2 PredictionsCounter (C) ← init(0);\n3 L ← empty list;\n4 for d in D do\n5 ˆc ← Model.Predict(d);\n6 PredictionsCounter (ˆc) ← PredictionsCounter (ˆc) + 1;\n7 Ed ← T- EBAnO .LocalExplanation(Model , d, ˆc);\n8 ˆed, f ← T- EBAnO .GetMostInﬂuentialExplanation( Ed , ˆc, \"MLWE\");\n9 for tk in ˆed, f . fe a t u re To k e n sdo\n10 l ← Lemmatize(tk );\n11 L.insert(l);\n12 GAI (ˆc,l) ← GAI (ˆc,l) + Max [0, ˆed, f .nPIR ];\n13 end\n14 end\n15 for c in C do\n16 for l in L do\n17 GAI (c,l) ← GAI (c,l)/PredictionsCounter (c);\n18 end\n19 end\n20 return GAI;\n123\nTrusting deep learning natural-language models via local… 1881\n5.2.1 Global absolute inﬂuence\nThe Global Absolute Inﬂuence value is computed following the process described in Algo-\nrithm 1. Firstly, are initialized the HashMap containing the GAI score for each class C and\neach lemma L (line 1), the counter of predictions for each class (line 2) and the list of unique\nlemmas (line 3). Then, given a corpus of documents D, for each input textual document\nd ∈ D the following steps are repeated (line 4). Firstly, the estimated class label ˆc ∈ C for\nthe input text d is predicted by the DNN model to explain (line 5) and the counter value for\nthe class ˆc is incremented (line 6). Then, T- EBAnO produces the local explanation set Ed\nfor the input d and the class-of-interest ˆc (line 7). Thus, the most inﬂuential explanation ˆed, f ,\ni.e., the one with the highest nPIR , is selected (line 8). We recall that T- EBAnO exploits\nonly the MLWE features to produce the global explanations. Therefore, the most inﬂuential\nexplanation ˆe\nd, f is the cluster of tokens with the highest inﬂuence, measured with the nPIR,\nfor the original predicted class label ˆc. Finally, for each token tk belonging the most inﬂuential\nfeature ˆed, f . fe a t u re To k e n s(line 9), T- EBAnO extracts the lemma l (line 10) of each token\ntk , adds it to the list of unique lemmas L (line 11) and updates the GAI score GAI (ˆc,l) for\nthe class ˆc of the lemma l (line 12) by summing the nPIR score of the the explanation ˆed, f ,\nonly if it is positively impacting the prediction (i.e., if nPIR > 0). The algorithm analyzes\nlemmas instead of tokens (words) in order to group together their inﬂected forms, obtaining\nmore signiﬁcant results. Finally, T- EBAnO normalizes the GAI score of each lemma l ∈ L\nand each class c ∈ C dividing by the number of inputs predicted with the class label c (lines\n15,16,17). This normalization step is required to handle also unbalance classes cases. The\noutput of the algorithm is the set of Global Absolute Inﬂuence scores. Speciﬁcally, for each\nlemma found in corpus D,a GAI score is computed for each possible class c ∈ C.T h ev a l u e\nGAI (c,l) is in range [0,+∞] and measures the absolute global inﬂuence of the lemma l for\nthe class c. Notice that, in the current T- EBAnO implementation, the GAI score can exceed\n1 because if a lemma is present n times in an inﬂuential feature, its global score is updated by\nsumming the nPIR index n times. We could obtain a score in range [0,1] by taking the list of\nunique lemmas of the feature (in lines 9 and 10). However, we preferred to reward lemmas\nthat are highlighted multiple times as important for the prediction in a single explanation.\nIn conclusion, the GAI score will be 0 for all the lemmas that have always brought a\nnegative inﬂuence on class c, and it will grow proportionally to the frequency and to the\npositive inﬂuence of each lemma positively inﬂuencing class c. The higher the GAI score,\nthe most positively inﬂuential a lemma is for the model under analysis with respect to class\nc.\n5.2.2 Global relative inﬂuence\nThe Global Relative Inﬂuence score highlights the most inﬂuential and differentiating lemmas\nfor each class-of-interest, discarding lemmas with multiple impact on other classes. The GRI\nfor a class-of-interest c,f o ras p e c i ﬁ cl e m m a l, and for a classiﬁcation task with nC classes\nis deﬁned as:\nGRI(c,l) = Max\n⎡\n⎣0,\n(\nGAI(c,l) −\nC∑\nci ̸=c\nGAI(ci ,l)/(nC − 1)\n) ⎤\n⎦ (8)\nThe GRI score is 0 when a lemma is more relevant for other classes than for the one under\nexam, while GRI> 0 if its inﬂuence is higher for class c than all the other classes. The higher\nthe GRI value, the more speciﬁc the lemma inﬂuence is with respect to the class-of-interest.\n123\n1882 F. Ventura et al.\nThe normalization over the number of predicted samples for each class performed on the\nGAI allows the GRI to be fair in case of unbalanced classes, while the division by nC − 1\nallows handling multi-class tasks.\nAnalyzing GAI and GRI scores, the user can extrapolate which are the most relevant\ninter- and intra-class semantic concepts that are affecting the decision-making process at a\nmodel-global level. For example, if a word is inﬂuential for all the possible classes, it will\nhave a high GAI score and a GRI score close to 0 for all the classes. On the contrary, if a\nword is most inﬂuential for a speciﬁc class, the GAI score will be higher for that speciﬁc\nclass. Therefore, the GRI score for that class will be greater than 0 for that class and usually\n0 (or close to 0) for all the other classes. Section 6.4 provides an experimental analysis of the\ninsights provided by T- EBAnO at a model-global level.\n6 Experimental results\nIn this section, we present the experiments performed to assess the ability of T- EBAnO to\nprovide useful and human-readable insights on the decisions made by deep learning NLP\nmodels. Firstly, we describe the experimental use cases in terms of NLP models and datasets\n(Sect. 6.1). Before discussing the core results, i.e., the explanations, we show how MLWE\nadapts to different NLP-model architectures (Sect. 6.2). The effectiveness of T- EBAnO\nin extracting useful local explanations is presented in Sect. 6.3, whereas results for global\ninsights are discussed in Sect. 6.4. Then, we evaluate the application of T- EBAnO to different\nuse cases (Sect. 6.5), the effectiveness of MLWE with respect to a random choice of the features\n(Sect. 6.6), and we perform a hyperparameters analysis (Sect. 6.7). Finally, we evalaute the\ncapacity of the proposed inﬂuence index ( nPIR) to model the human judgment (Sect. 6.8), and\nwe perform an experimental comparison with two model-agnostic XAI techniques (Sect. 6.9).\n6.1 Use cases\nTo discuss how T- EBAnO is able to provide useful prediction-local explanations and model-\nglobal explanations, we selected two main use cases consisting of different NLP models and\nclassiﬁcation tasks ( Use cases 1-2 ). We chose a sequence model and a transformer-based\nmodel from the state-of-the-art, speciﬁcally LSTM and BERT, applied on two different\nbinary text classiﬁcation tasks: sentiment analysis and toxic comment classiﬁcation. Then, to\nevaluate the ﬂexibility of T- EBAnO , independently of the speciﬁc deep learning model and\nthe classiﬁcation task, we selected additional classiﬁcation tasks ( Ag News topic classiﬁcation\nand Cola sentence acceptability) on different models like BERT, ALBERT, and ULMFit\n(Use cases 3-8 ). The removal perturbation has been exploited for all the experiments. Table\n4 summarizes all the experimental use cases.\n6.1.1 Use case 1\nThe ﬁrst task is a binary toxic comment classiﬁcation , and it consists of predicting whether the\ninput comment is clean or toxic, i.e., it contains inappropriate content. The toxic class label\ncontains several subtypes of toxic comments such as identity attacks, insults, explicit sexual-\nity, obscenity, insult, and threats. An LSTM model applied to a civil comments dataset [ 6]h a s\nbeen used. The LSTM model is composed of an embedding 300-dimensional layer, two bidi-\nrectional LSTM layers (with 256 units for each direction), and ﬁnally, a dense layer with 128\n123\nTrusting deep learning natural-language models via local… 1883\nTable 4 Experimental use cases\nUse case Model Dataset Task (classiﬁcation) Test accuracy (%)\n1 LSTM Civil comments Comment toxicity 90\n2 BERT Imdb Sentiment analysis 86\n3 BERT Ag news Topic classiﬁcation 94\n4 BERT Cola Sentence acceptability 81\n5 ALBERT Ag News Topic classiﬁcation 93\n6 ALBERT Cola Sentence acceptability 77\n7 ULMFit Ag News Topic classiﬁcation 92\n8 ULMFit Cola Sentence acceptability 71\nhidden units. Transfer learning has been exploited using GloV e [ 38] (with 300-dimensional\nvectors) for the embedding layer. After training, the custom LSTM model reached an accuracy\nof 90%.\n6.1.2 Use case 2\nThe second selected task is sentiment analysis, and it consists of predicting if the underlying\nsentiment of an input text is either positive or negative. The BERT base (uncased) pre-trained\nmodel [11] has been chosen as deep learning predictive model with obscure decision-making\nprocess, and it has been applied to the IMDB dataset [33], which is a reference set of data\nfor sentiment analysis. We performed a ﬁne-tuning step of the BERT model [ 11] by adding\na classiﬁcation layer on top of the last encoder transformer’s stack. The BERT model, ﬁne-\ntuned on the IMDB textual reviews, reached an accuracy of 86%.\n6.1.3 Other use cases\nFor the additional use cases, we selected different models and classiﬁcation tasks. We kept\nBERT from the transformer-encoder family of models as a reference milestone of the state-\nof-the-art, then we added ALBERT [ 24] as a representative of the variations proposed for the\nBERT model (like RoBERTa [ 29], DistilBERT [ 42]), and ULMFit [ 19] as a representative\nof the general language model family, with a completely different architecture. The two\nadditional tasks are (i) a binary classiﬁcation, predicting the grammatical acceptability or\nunacceptability of the sentence with the Cola (Corpus of Linguistic Acceptability) dataset\n[50], and (ii) a multi-class news topic classiﬁcation task consisting of four classes ( World,\nSport, Business and Science/Technology)o ft h e Ag News dataset [ 51] (a subset version with\nthe four largest classes of the original corpus).\n6.2 Multi-layer word embedding model-specific implementations\nIn this section, we discuss the model-speciﬁc MLWE implementations for the deep learning\nmodels used in the experimental use cases.\n123\n1884 F. Ventura et al.\n6.2.1 LSTM\nRNNs with LSTM units are robust architectures that can learn both the time sequence dimen-\nsion and the feature vector dimension. Multiple LSTM layers usually characterize them, and\nthey can take as input an embedded representation of the text. As highlighted in Sect. 6.1,\nthe developed LSTM model exploits one embedding layer that works with full tokens and\ntwo bidirectional LSTM layers. For these reasons, the MLWE exploits the single embedding\nlayer to extract a tensor of shape (tk × 300 × 1). In this case, the vectorial aggregation step\n(Sect. 4.4) is unnecessary because the embedding is extracted from a single layer, and the\nmodel does not present sub-tokens. Then, a principal component analysis is used to reduce the\nembedding matrix shape to (tk ×c), obtaining the multi-layer word embedding representation\nfor the custom LSTM model.\n6.2.2 BERT\nFigure 5 shows all the steps of the multi-layer word embedding (MLWE) feature extraction\nprocess in BERT. The base version of the BERT model [ 11] is composed of 12 transformer\nlayers [ 47], each producing an output of shape (wp ×768),w h e r ewp is the number of word\npieces extracted by BERT in its preprocessing phase. The MLWE, in this case, analyzes\nthe word embeddings extracted from the last four transformer layers of the model. It has\nbeen motivated in the literature [ 13] that modern natural language models incorporate most\nof the context-speciﬁc information in the last and deepest layers. Thus, the joint analysis\nof these layers allows the MLWE to extract features more related to the task under exam,\navoiding too speciﬁc (if analyzing only the last layer) or too general (if analyzing only the\nﬁrst layers) word embeddings. In the ﬁrst step of the MLWE feature extraction, the last four\ntransformer layer outputs (i.e., L\n9, L10, L11, L12) are extracted (Fig. 5-left), resulting in a\ntensor of shape (wp × 768 × 4). Each row is the embedding representation for each word\npiece in each layer. Then, the outputs of the four layers are aggregated, summing the values\nof the embeddings over the layer axes in a matrix of shape wp × 768 (Fig. 5-center-left),\nas suggested by [ 14]. Since BERT works with word pieces but T- EBAnO objective is to\nextract full tokens (words), the embedding of word pieces belonging to the same word are\naggregated, averaging their values over the word-piece axes, and obtaining a new matrix of\ntokens embedding of shape tk × 768, where tk is the number of input tokens (Fig. 5-center-\nright). The 4-layers to single-layer and word-pieces to full-tokens aggregations compose the\nvectorial aggregation step (Sect. 4.4) for the BERT model. In the end, due to the sparse\nN°WordPieces\nEmbeddingDim.=768\nN°Layers\nE\nwp\n768[9]E\nwp\n0 [9]\nE\n0\n0 [9] E\n0\n768[9]\nN°WordPieces\nN°Tokens\nN°Tokens\nEmbeddingDim.=c\n(Reduced)\nE\ntk\ncE\ntk\n0\nE\n0\n0 E\n0\nc\nEmbeddingDim.=768\nEmbeddingDim.=768\nSUMExtract AVG PCA\nE\nwp\n768ΣLE\nwp\n0ΣL\nE\n0\n0ΣL E\n0\n768ΣL\nE\ntk\n768E\ntk\n0\nE\n0\n0 E\n0\n768\nBERT Model\nFig. 5 BERT multi-layer word embedding feature extraction process. With: E<wp or tk >\n<768 or c> [Lid ] such that: E is\nthe word embedding matrix, wp and tk indicate the position of the word-piece and token respectively in the\ninput text, 768 is the original embedding dimension, c is the number of reduced principal components of the\nword embedding vector and Lid is the layer from which is extracted\n123\nTrusting deep learning natural-language models via local… 1885\nnature of data, the dimensionality reduction technique, i.e., Principal Component Analysis,\nis exploited, reducing the ﬁnal shape of the tokens embeddings matrix to (tk × c),w h e r e c\nis the reduced number of principal components extracted (Fig. 5-right). This last result is the\nMulti-layer word embedding representation for the BERT model.\n6.2.3 Other models\nIn general, to adapt T- EBAnO to different NLP deep-learning architectures, the MLWE\napproach requires providing one or more layers of word embedding (a vector or a tensor\nfor each token), an aggregation function if there are more layers of word embedding (i.e., to\nrepresent each token from the n-dimensional tensor to a one-dimensional vector) and, ﬁnally,\nan aggregation function if wordpieces tokenization is performed (i.e., some tokens are divided\ninto sub-tokens) to create full tokens representations instead of wordpieces representations\n(vectorial aggregation step).\nT- EBAnO provides an interface to be implemented with such speciﬁcations, hence allow-\ning T- EBAnO -MLWE to potentially work with any NLP deep-learning model. This interface\nhas been used to exploit MLWE with all the models included in the experiments (LSTM,\nBERT, ALBERT, ULMFit). For instance, the MLWE implementation for the ALBERT archi-\ntecture is exactly the same as used for BERT. It extracts the last four transformer-encoder\nlayers, aggregates the multi-layer to a single vectorial representation for each wordpiece\n(sum), and, ﬁnally, aggregates the wordpieces vectorial representation to full token represen-\ntation (avg) before the dimensionality reduction. For the ULMFit model, instead, the MLWE\nimplementation is very similar to the LSTM implementation. T- EBAnO extracts a represen-\ntation for each input token by the LSTM-encoder part of ULMFit (a vector of length 400 for\neach token) and then applies the dimensionality reduction. The two aggregation functions, in\nthis case, are not necessary because a single-layer representation is extracted for each token,\nand ULMFit already works with full tokens.\n6.3 Local explanations\nFor each input document, the local explanations were computed exploiting all the feature\nextraction methods described in Sect. 4.1 and the removal perturbation for use cases 1 and\n2.\n6.3.1 Overview of use cases 1 and 2\nIn the explanation process of the sentiment analysis task with the BERT model (use case 2),\nT- EBAnO has been experimentally evaluated on 400 textual documents, 202 belonging to the\nclass Positive and 198 to the class Negative, for a total of almost 100,000 local explanations,\nwith an average of 250 local explanations for each input document. However, only the highly\ninﬂuential local explanations are automatically shown by the engine to the user. A local\nexplanation has been deﬁned to be highly inﬂuential when having an nPIR value equal to\nor higher than the threshold nPIR\nt = 0.5. All the rest of the local explanations produced\nby T- EBAnO are still available to the users, should they like to investigate further insights\ninto the prediction process. To show the effectiveness of the proposed feature extraction\ntechniques, we analyzed the percentage of documents for which T- EBAnO computed local\nexplanations with at least one highly inﬂuential feature for the class-of-interest. Experiments\non the same input texts have been repeated twice, ﬁrstly without combining the different\n123\n1886 F. Ventura et al.\nTable 5 Explanations of the BERT model: percentage of documents for which each feature extraction strat-\negy produces at least one highly inﬂuential local explanation (i.e., with nPIR ≥ 0.5), with and without\ncombination of features\nFeature extraction type No combination (%) Pairwise combination (%)\nPart-of-speech 33 70\nSentence 22 30\nMLWE 75 86\nOverall 80 90\nThe pairwise combinations are inner feature extraction methods (like Adjs with V erbs for POS, Sentence 1\nwith Sentence 2 for SEN and Cluster 1 with Cluster 2 for MLWE). Overall is the percentage of documents for\nwhich at least one method provided a local explanation with nPIR ≥ 0.5\nTable 6 Explanation of the custom LSTM model: percentage of documents for which each feature extraction\nstrategy produces at least one highly inﬂuential local explanation (i.e., with nPIR ≥ 0.5), with combination\nof features, for the class labels Clean and Toxic separately, and together ( Clean/Toxic).\nFeature extraction type Clean (%) Toxic (%) Clean/toxic (%)\nPart-of-speech 8 98 53\nSentence 2 76 39\nMLWE 12 98 55\nOverall 15 99 58\nThe pairwise combinations are inner feature extraction methods (like Adjs with V erbs for POS, Sentence 1\nwith Sentence 2 for SEN,a n d Cluster 1 with Cluster 2 for MLWE)\nfeatures, then including the pairwise combinations for each feature extraction method. Table 5\nshows the percentage of documents required to ﬁnd at least one highly inﬂuential feature\n(nPIR ≥ 0.5) with and without combinations of pairwise features. The MLWE method leads\nto abundantly better results than the other methods. The part-of-speech strategy beneﬁts the\nmost from the pairwise combinations, allowing the creation of features representing more\ncomplex concepts. For example, the combination of adjectives and nouns allows the creation\nof features composed of words like { bad, film } that, together, can better express a\nsentiment.\nIn the explanation process of the toxic comment task with the custom LSTM model (use\ncase 1), T- EBAnO has been experimentally evaluated on 2250 documents, 1121 belonging\nto the class Toxic and 1129 to class Clean, leading to almost 160,000 local explanations\nin total. Table 6 shows the percentage of input documents for which T- EBAnO has been\nable to extract at least one highly inﬂuential local explanation ( nPIR ≥ 0.5). For the Toxic\nclass, T- EBAnO has been able to identify at least one highly inﬂuential explanation for\nalmost all the documents, with most of the feature extraction strategies. Only the sentence-\nbased feature extraction has a lower percentage of highly inﬂuential explanations w.r.t. the\nother techniques. This suggests that toxic words tend to be sparse in the input text and not\nconcentrated in a single sentence. Finding highly inﬂuential explanations for the Clean class\nhas proven to be harder. None of the feature extraction techniques can explain more than 15%\nof the predictions for the Clean input texts. The nature of the use case under exam can explain\npossible causes: usually, a document is considered clean; it can become toxic because of the\npresence of speciﬁc words or linguistic expressions. Thus, the hypothesis is that there is no\nspeciﬁc pattern of words that represents the Clean class (see Sect. 6.4 for further details).\n123\nTrusting deep learning natural-language models via local… 1887\nCriticize a black man and the left calls you a racist. Criticize a woman and you\nare a sexist. Now I will criticize you as a fool and you can call me intolerant.\n(a) Original text\nCriticize a black man and the left calls you a racist. Criticize a woman and you\nare a sexist. Now I will criticize you as a fool and you can call me intolerant.\n(b) EXP1: Adjective & Noun - POS feature extraction\nCriticize a black man and the left calls you a racist. Criticize a woman and you\nare a sexist. Now I will criticize you as a fool and you can call me intolerant.\n(c) EXP2: Multi-layer word embedding feature extraction\nFig. 6 Examples of textual explanation report for the input in Fig. 6a originally labeled by custom LSTM\nmodel as Toxic with a probability of 0.98. The most relevant features are highlighted in cyan\nTable 7 Quantitative explanation for the example reported in Fig. 6\nExplanation Feature f Lo Lf nPIRf (N)\nEXP1 POS-Adj&Noun T C 0.839\nEXP2 MLWE T C 0.883\nTi st h e Toxic label, C is the Clean label. Positively highly inﬂuential features ( nPIR ≥ 0.5) for the Lo class\nare highlighted in green in the nPIR f (N ) column\n6.3.2 Local explanation: example 1\nIn the following, we present and discuss some speciﬁc local explanations provided by T-\nEBAnO in different conditions to show their relevance and usefulness in explaining the deep\nNLP model behavior for both the custom LSTM and the BERT models of the use cases 1\nand 2.\nIn the ﬁrst example, reported in Fig. 6, the custom LSTM model classiﬁes the input com-\nment in Fig. 6aa s Toxic. The most inﬂuential features identiﬁed by T- EBAnO are shown\nin Fig. 6b, c. The different feature extraction strategies ﬁnd that the most positively inﬂuen-\ntial features for the Toxic class labels are { black man, left, racist, woman,\nsexist, fool, intolerant }. In particular, the most inﬂuential explanations are\nextracted with the combination of adjectives and nouns (Table 7-EXP1) and with MLWE\n(Table 7-EXP2). It is interesting to notice that in this case, the combination of adjectives\nand nouns is very relevant for this model, e.g., it is not just the word black that makes a\ncomment toxic, but the combination black man . Furthermore, the POS feature extraction\nand the MLWE highlighted very similar sets of words. In this case, the prediction is trustful,\nand in particular, it is relevant that the model learned features like black man and woman\nto be inﬂuential for the Toxic class.\n6.3.3 Local explanation: example 2\nIn the second example, the BERT model makes a wrong prediction by classifying the sen-\ntiment of the input text in Fig. 7aa s Negative, while the expected label (ground-truth) is\nPositive. A user requiring to decide whether to trust such prediction can take advantage of\nT- EBAnO to understand which are the words inﬂuencing the outcome. Figure 7 shows the\ntextual explanations provided by the most inﬂuential features. Table 8 contains the corre-\nsponding quantitative explanations with the nPIR values. T- EBAnO identiﬁed three local\n123\n1888 F. Ventura et al.\nHow many movies are there that you can think of when you see a movie like this? I\ncan’t count them but it sure seemed like the movie makers were trying to give me a\nhint. I was reminded so often of other movies, it became a big distraction. One of\nthe borrowed memorable lines came from a movie from 2003 - Day After Tomorrow. One\nline by itself, is not so bad but this movie borrows so much from so many movies\nit becomes a bad risk. BUT... See The Movie! Despite its downfalls there is enough\nto make it interesting and maybe make it appear clever. While borrowing so much\nfrom other movies it never goes overboard. In fact, you’ll probably find yourself\nbattening down the hatches and riding the storm out. Why? ...Costner and Kutcher\nplayed their characters very well. I have never been a fan of Kutcher’s and I\nnearly gave up on him in The Guardian, but he surfaced in good fashion. Costner\ncarries the movie swimmingly with the best of Costner’s ability. I don’t think Mrs.\nRobinson had anything to do with his success. The supporting cast all around played\ntheir parts well. I had no problem with any of them in the end. But some of these\ncharacters were used too much. From here on out I can only nit-pick so I will save\nyou the wear and tear. Enjoy the movie, the parts that work, work well enough to\nkeep your head above water. Just don’t expect a smooth ride. 7 of 10 but almost a\n6.\n(a) Original text\nHow many movies are there that you can think of when you see a movie [...] I was\nreminded so often of other movies, it became a big distraction. One of the borrowed\nmemorable lines came from a movie from 2003 - Day After Tomorrow. One line by\nitself, is not so bad but this movie borrows so much from so many movies it becomes\na bad risk. BUT ... See The Movie! Despite its downfalls there is enough to make\nit interesting and maybe make it appear clever. While borrowing so much from other\nmovies it never goes overboard. [...] I have never been a fan of Kutcher ’s and\nI nearly gave up on him in The Guardian, but he surfaced in good fashion. Costner\ncarries the movie swimmingly with the best of Costner ’s ability. [...] But some of\nthese characters were used too much. [...] Just do n’t expect a smooth ride. 7 of\n10 but almost a 6.\n(b) EXP1: Adjective - POS feature extraction\nHow many movies are there that you can think of when you see a movie like this? I\ncan’t count them but it sure seemed like the movie makers were trying to give me a\nhint. I was reminded so often of other movies, it became a big distraction. One of\n[...]\n(c) EXP2: Sentence feature extraction\nHow many movies are there that you can think of when you see a movie like this?\n[...] See the movie despite its downfalls there is enough to make it interesting\nand maybe make it appear clever. [...]\n(d) EXP3: Multi-layer word embedding feature extraction\nFig. 7 Examples of textual explanation report for the input in Figure 7a, wrongly labeled by BERT as Negative\nwith a probability of 0.99. The most relevant features are highlighted in cyan\nTable 8 Quantitative explanation for the example in Fig. 7\nExplanation Feature f Lo Lf nPIRf (N)\nEXP1 POS-Adjective N P 0.884\nEXP2 Sentence N P 0.663\nEXP3 MLWE N P 0.651\nP is the positive label, N is the negative label. Positively highly inﬂuential features ( nPIR ≥ 0.5) for the Lo\nclass are highlighted in green in the nPIR f (N ) column\n123\nTrusting deep learning natural-language models via local… 1889\nexplanations for the Negative class with nPIR values higher than 0.5, whose perturbation\nwould cause a change in the predicted label from Negative to Positive). The top relevant\nfeatures were extracted exploiting Adjectives-POS (Fig. 7b), Sentence (Fig. 7c), and MLWE\n(Fig. 7d). Regarding the Adjectives-POS feature extraction, Fig. 7b shows that general words\nlike { many, other, big, ..., smooth }h a v ea n nPIR value for the class Neg-\native close to 0.88 (Table 8-EXP1). General words with a very strong impact on the ﬁnal\nprediction for this speciﬁc input text are not a trustful indicator: their absence might lead to\nentirely different outcomes.\nRegarding the sentence-based feature extraction, the negative prediction is triggered by\nonly one speciﬁc phrase (Fig. 7c), whose absence leads to a Positive prediction with a nPIR\nvalue of 0 .66 (Table 8-EXP2).\nFinally, the MLWE feature extraction strategy identiﬁes a cluster composed of only two\ninstances of a very general single word { there}a st h e most informative feature (Fig. 7d). By\nremoving the two occurrences of the word { there}, the prediction changes from Negative\nto Positive with an nPIR value of 0.651 (Table 8-EXP3).\nSince the output of the prediction model can be drifted (from Negative to Positive)b y\nsimply removing occurrences of general words such as { there, many, other, big,\nsmooth, ... } from the input text (actually removing only { there} is enough!), doubts\non the predicted class reliability are reasonable. More details related to the global behavior\nand the robustness of the model are addressed in Sect. 6.4.\n6.3.4 Local explanation: example 3\nThe example is reported in Fig. 8, where the BERT model correctly classiﬁes the input text\nin Fig. 8aa s Negative.T h e textual explanations produced by T- EBAnO exploiting different\nfeature extraction strategies are reported as follows: adjective-POS in Fig. 8b, verb-POS in\nFig. 8c, adjective-verb-POS in Fig. 8d, sentence in Fig. 8e, and multi-layer word embedding\nin Fig. 8f. Their nPIR values are reported in the quantitative explanations of Table 9.W e\nnote that only the adjective-verb-POS, sentence, and MLWE techniques provide informative\nexplanations, whereas the adjective-POS and verb-POS yield uninformative explanations,\nyet we include them in the example for discussion.\nThe POS feature analysis (Fig. 8b, c) shows that the different parts-of-speech, taken\nseparately one at a time, are not inﬂuential for the prediction of the class Negative.F r o mt h e\nquantitative explanation o fE X P 1a n dE X P 2i nT a b l e9, indeed, it can be observed that they\nachieve an nPIR close to 0.003 and 0.137, respectively. A similar result was obtained for all the\nother POS features considered individually. Consequently, T- EBAnO explores the pairwise\ncombinations (as explained in Sect. 4.1) of the parts-of-speech to create more sophisticated\nfeatures and to analyze more complex semantic concepts. In this case, the feature composed\nof Adjectives and Ve r b s(Fig. 8d) is reported to be impacting for the predicted class label\nreaching a nPIR value close to 0.915 (EXP3 in Table 9).\nThe sentence feature extraction strategy, instead, identiﬁes the feature composed of the phrase\nin Fig. 8e as positively inﬂuential for the predicted class with an nPIR score of about 0.638\n(EXP4 in Table 9).\nFinally, the MLWE feature extraction identiﬁes K = 15 as the best K partitioning of words,\nand the most informative feature (i.e., cluster of words that maximize nPIR and minimize\ntokens ratio) with a signiﬁcant impact on the output prediction is shown in Fig. 8f, reaching\nan nPIR of 0.899 (EXP5 in Table 9).\nAnalyzing the content of the most inﬂuential textual explanations (EXP3, EXP4 and\nEXP5), it can be observed that interestingly, all the local explanations with high values of\n123\n1890 F. Ventura et al.\nThere were so many classic movies that were made where the leading people were\nout-and- out liars and yet they are made to look good. I never bought into that\nstuff. The \"screwball comedies\" were full of that stuff and so were a lot of the\nFred Astaire films. Here, Barbara Stanwyck plays a famous \"country\" magazine writer\nwho has been lying to the public for years, and feels she has to keep lying to keep\nher persona (and her job). She even lies to a guy about getting married, another\ntopic that was always trivialized in classic films. She’s a New York City woman\nwho pretends she’s a great cook and someone who knows how to handle babies, etc.\nObviously she knows nothing and the lies pile up so fast you lose track. I guess\nall of that is supposed to be funny because lessons are learned in the end and true\nlove prevails, etc. etc. Please pass the barf bag. Most of this film is NOT funny.\nStanwyck was far better in the film noir genre. As for Dennis Morgan, well, pass\nthe bag again.\n(a) Original text\nThere were so many classic movies that were made where the leading people were\nout-and- out liars and yet they are made to look good. I never bought into that\nstuff. The ‘‘ screwball comedies’’ were full of [...] plays a famous ‘‘country’’\n[...] getting married, another topic that was always trivialized in classic films.\n[...] she’s a great cook and someone [...] supposed to be funny because lessons\nare learned in the end and true love [...] bag. Most of this film is NOT funny.\nStanwyck was far better in the film noir genre. [...]\n(b) EXP1: Adjective - POS feature extraction\nThere were so [...] that were made where the leading people were out-and- out liars\nand yet they are made to look good. I never bought into that stuff. The ‘‘screwball\ncomedies’’ were full of that stuff and so were a lot [...] Barbara Stanwyck plays\na famous ‘‘country’’ magazine writer who has been lying to [...] she has to keep\nlying to keep her persona (and her job). She even lies to a guy about getting\nmarried, another topic that was always trivialized in classic films. She ’s a New\nYork City woman who pretends she ’s a great cook and someone who knows how to han\ndle babies, etc. Obviously she knows nothing and the lies pile up so fast you lose\ntrack. I guess all of that is supposed to be funny because lessons are learned in\nthe [...] Please pass the barf bag. Most of this film is NOT funny. Stanwyck was\nfar [...] well, pass the bag again.\n(c) EXP2: Verb - POS feature extraction\nThere were so many classic movies that were made where the leading people were out-\nand- out liars and yet they are made to look good. I never bought into that stuff.\nThe ‘‘ screwball comedies’’ were full of that stuff and so were a lot of the Fred\nAstaire films. Here, Barbara Stanwyck plays a famous ‘‘country’’ magazine writer\nwho has been lying to the public for years, and feels she has to keep lying to keep\nher persona (and her job). She even lies to a guy about getting married, another\ntopic that was always trivialized in classic films. She ’s a New York City woman\nwho pretends she ’s a great cook and someone who knows how to handle babies, etc.\nObviously she knows nothing and the lies pile up so fast you lose track. I guess\nall of that is supposed to be funny because lessons are learned in the end and true\nlove prevails, etc. etc. Please pass the barf bag. Most of this film is NOT funny.\nStanwyck was far better in the film noir genre. As for Dennis Morgan, well, pass\nthe bag again.\n(d) EXP3: Adjective & Verb - POS feature extraction\n[...] She even lies to a guy about getting married, another topic that was always\ntrivialized in classic films. [...]\n(e) EXP4: Sentence feature extraction\n[...] I never bought into that stuff. The \"screwball comedies\" were full of that\nstuff and so were a lot of the Fred Astaire films. Here, Barbara Stanwyck plays a\nfamous \"country\" magazine writer who has been lying to the public for years, and\nfeels she has to keep lying to keep her persona (and her job). she even lies to\na guy about getting married,\nanother topic that was always trivialized in classic\nfilms. she’s a new york city woman who pretends she’s a great cook and someone who\nknows how to handle babies, etc. Obviously she knows nothing and the lies pile up\nso fast you lose track. I guess all of that is supposed to be funny because lessons\nare learned in the end and true love prevails, etc. [...] Most of this film is not\nfunny. Stanwyck was far better in the film noir genre. as for Dennis Morgan, well,\npass the bag again\n(f) EXP5: Multi-layer word embedding feature extraction\nFig. 8 Examples of textual explanations for the input in Figure 8a, originally labeled by BERT as Negative\nwith a probability of 0.99. Features extracted by T- EBAnO are highlighted in cyan\n123\nTrusting deep learning natural-language models via local… 1891\nTable 9 Quantitative explanations for the example reported in Fig. 8\nExplanation Feature f Lo Lf nPIRf (N)\nEXP1 POS-Adjective N N 0.003\nEXP2 POS-Verb N N 0.137\nEXP3 POS-Adj&Verb N P 0.915\nEXP4 Sentence N P 0.638\nEXP5 MLWE N P 0.899\nP is the positive label, N is the negative label. Positively highly inﬂuential features ( nPIR ≥ 0.5) for the Lo\nclass are highlighted in green in the nPIR f (N ) column\nnPIR contain the word { trivialized}. It might seem that a single word can be the only\none responsible for the original prediction. However, also the explanation EXP2 contains\nthe same word but is not inﬂuential for the class label. Therefore, it emerges that the output\npredictions are not inﬂuenced by single words, but is the combination of different words that\nallows creating more complex concepts which determine the predicted class label. Moreover,\nit is possible to say that in this speciﬁc prediction, the model is not sensible to the perturbation\nof adjectives (EXP1 in Fig. 8b) or verbs (EXP2 in Fig. 8c) separately, highlighting that the\nproposed prediction has been produced taking into account the whole context of the input\ntext. Only in EXP3 (Fig. 8d) it is possible to notice that when adjectives and verbs are\nperturbed together, changing the meaning of the input text radically, the predicted class\nchanges. The joint perturbation can be considered a good measure of robustness for the\nprediction performed by the ﬁne-tuned BERT model under analysis.\nHowever, as for the previous example, it is shown in EXP4 (Fig. 8e) that exist a singular\nphrase more relevant than the others in the decision-making process. The perturbation of\nthe sentence in EXP4 will bring the model to change the prediction from class Negative\nto Positive. Furthermore, EXP5 (Fig. 8f), obtained through the MLWE feature extraction\ntechnique, shows an apparently random pool of words very relevant in the prediction process.\nThe MLWE feature extraction is able to ﬁnd the inﬂuential feature with higher precision\nconcerning EXP3 (obtained by the combination of all verbs and adjectives), with a very\nsmall penalty on the nPIR score. Indeed, the MLWE strategy is able to ﬁnd a small number\nof words belonging to different part-of-speeches and different sentences that are affecting the\nmodel’s output. So, also the resulting explanations are more understandable and meaningful\nfor the end-user.\nAs in the previous example, this last experiment shows that the predictive model is par-\nticularly sensitive to a few speciﬁc variations of, apparently not correlated, input words.\nFrom these examples, it emerges that the different feature extraction strategies should\nbe used in a complementary manner, as they look at different aspects of the input text and\nprovide different kinds of explanations. Furthermore, the proposed examples showed that:\n• T- EBAnO can be successfully applied to different deep learning models;\n• the proposed prediction explanation process can be applied with success to different use\ncases and NLP tasks;\n• T- EBAnO can extract meaningful explanations from both long and short text documents\nwithout limiting their interpretability;\n• the end-user is provided with informative details to analyze critically and judge the quality\nof the model outcomes, being supported in deciding whether its decision-making process\nis trustful.\n123\n1892 F. Ventura et al.\nFig. 9 Global explanation of toxic comment classiﬁcation with LSTM\n6.4 Model-global explanations\nExploiting the prediction-local explanations computed by T- EBAnO for all the input docu-\nments, model-global insights can be provided.\n6.4.1 Use case 1\nFor the toxic comment classiﬁcation, Fig. 9 shows the GAI and GRI scores for each inﬂuential\nword under the form of word clouds for the classes Toxic (Fig. 9a, c) and Clean (Fig. 9b, d),\nrespectively. They are generated by analyzing all the local explanations produced over the\n2250 texts of use case 1 (as discussed in Sect. 5.2). The font size of words is proportional to\nthe GAI or GRI scores obtained for each class separately. The proportion of the font size is\nrelative only to the single word cloud (i.e., two words with the same size in different word\nclouds do not necessarily have the same score, while two words with the same size in the\nsame word cloud have almost the same score).\nFirstly, as discussed in Sect. 5.2, T- EBAnO analyzes the most inﬂuential local explana-\ntions produced and computes the GAI score for each lemma and the labels Toxic and Clean.\nThen, it generates the word clouds (Fig. 9a, b) to provide a visual impact of the most impor-\ntant lemmas for each class. The GAI word clouds (Fig. 9a, b) show that the two classes are\ninﬂuenced by a non-overlapping set of words. Indeed, the most important lemmas for the\nToxic class (i.e., with higher GAI for the Toxic class) are stupid (0.31), Politician1\n(0.28), people (0.26), idiot (0.17), and white (0.15). Instead, the lemmas with higher\nGAI for the Clean class are the (0.04), people (0.02), and (0.01), if (0.01), and like\n(0.01). This conﬁrms that the model learned that if a word is attributable to toxic language\nin some context, it is unlikely to be associated with clean language in others. Toxic com-\nments are identiﬁed by terms that are strongly related to toxic language, discrimination, or\nracism. Instead, there is no speciﬁc pattern of words that identiﬁes clean comments. Just a\nfew concepts like people have an inter-class inﬂuence.\n123\nTrusting deep learning natural-language models via local… 1893\nThen, T- EBAnO computes the GRI score for each lemma and the Toxic and Clean\nclasses and generates the corresponding word clouds (Fig. 9c, d) to determine which are the\nmore differentiating concepts between the two classes, among those selected by the model.\nThe GRI word cloud highlights, even more, the impact of words like stupid, idiot,\nand ignorant which obtained a GRI score for the Toxic class of 0.31, 0.17, and 0.12,\nrespectively. But also terms related to minorities and genders like woman, black, white,\ngay, (which obtained a GRI for the Toxic class of 0.10, 0.10, 0.15, and 0.06, respectively)\nmeaning that the model has learned to recognize racists or sexists comments when these\nterms are present. Also, the presence of speciﬁc politician family names, anonymized as\nPolitician1, Politician2, etc., highlights that those people’s names are related to\ntoxic comments. In particular, Politician1 achieved the second higher GRI score for the\nToxic class with 0.28. These results demonstrate that a deep learning model, if not carefully\ntrained, can learn from sensible content, including prejudices and various forms of bias that\nshould be avoided in critical contexts. Finally, associating a speciﬁc person’s family name to\na class also raises ethical issues.\n6.4.2 Use case 2\nAnalyzing the prediction-local explanations produced for the 400 input texts in the sentiment\nanalysis use case is possible to extract global insights regarding the ﬁne-tuned BERT model.\nFigure 10 shows the GAI and GRI word clouds for the Positive (Fig. 10a, c) and Negative\n(Fig. 10b, d) class labels.\nAgain, T- EBAnO ﬁrstly produces the GAI score for each lemma for the Positive and\nNegative classes analyzing all the most inﬂuential local explanations (as discussed in Sect.\n5.2). The most important lemmas for the Positive class (i.e., with higher GAI for the Positive\nclass) are film (0.60), movie (0.48), one (0.34), like (0.22), story (0.21), good\n(0.21), great (0.20), and love (0.19). Instead, the lemmas with higher GAI for the Negative\nclass are movie (0.37), film (0.25), like (0.17), one (0.16), even (0.14), and story\nFig. 10 Global explanation of sentiment analysis with BERT\n123\n1894 F. Ventura et al.\n(0.12). From these values, T- EBAnO generates the GAI word clouds for the classes Positive\n(Fig. 10a) and Negative (Fig. 10b).\nDifferently from the previous example, the GAI word clouds for the Positive and the Neg-\native class labels show that several words like story, movie, film, like are impacting\non both classes. This means that the model exploits overlapping concepts that do not directly\nexpress a sentiment but that, if considered together in their context, can be associated with\nwords that express the mood of the writer (e.g., This film is not as good as\nexpected). Thus, to understand which are the lemmas that mostly impact one class with\nrespect to the other, it computes the GRI score for each lemma for the two classes and\ngenerates the word clouds.\nThe GRI word cloud for the Positive class (Fig. 10c) shows that words like movie and\nfilm are still very relevant for it, while they do not appear anymore for the Negative class\n(Fig. 10d) that is now highly characterized by the concept of book. Indeed, movie and film\nobtain a GRI for the Positive class of 0.35 and 0.11, respectively (while for the Negative class\nis 0). Instead, book achieved a GRI score for the Negative class of 0.07 (while for the\nPositive is 0). Exploring the dataset, we noticed that movies inspired by books are used to be\nassociated with negative comments, as typically, the original book is more detailed or slightly\ndifferent. Therefore, this can be considered a form of bias that the model has learned, in the\nsense that a movie evaluation might not be based on its comparison with a book. However, the\nGRI shows also that most of the inﬂuential words for positive input texts are concepts strictly\nrelated to positive sentiments like good, great, best, love achieving a GRI score for\nthe Positive class of 0.12, 0.17, 0.12, and 0.17, respectively. Similarly, the negative sentiment\nis associated with words like worst, bad, awful achieving a GRI score for the\nNegative\nclass of 0.07, 0.06, and 0.05, respectively. For these concepts, the model behaves as expected.\nThanks to the model-global explanation process, the user can better understand how the\npredictive model is taking its decisions, identifying the presence of prejudice and/or bias,\nand allowing to decide if and which corrective actions have to be taken to make the decision-\nmaking process more reliable.\n6.5 Framework extendibility\nIn this section, we evaluate the ability of T- EBAnO to adapt to different architectures and\ndifferent tasks (use cases 3-8). For this purpose, we deﬁned the following additional tasks.\n• Ag News : a multi-class news topic classiﬁcation task consisting of four classes: World,\nSport, Business and Science/Technology [51].\n• Cola: Corpus of Linguistic Acceptability, a binary classiﬁcation task that consists of\npredicting the grammatical acceptability or unacceptability of the sentence [ 50].\nBoth tasks differ from the previous ones (sentiment analysis and toxic comment) because they\ndo not strictly depend on a speciﬁc part of the speech. Furthermore, Ag News is a multi-class\nclassiﬁcation problem.\nFor each task, we trained three different models:\n• BERT: Bidirectional Encoder Representations from Transformers\n• ALBERT: A Lite BERT [ 24]\n• ULMﬁt: Universal Language Model Fine-tuning [ 19]\nFor each model and task, corresponding to the use cases 3-8 of Table 4, we produced with T-\nEBAnO the local explanations of 512 input texts exploiting the removal perturbation. Then,\n123\nTrusting deep learning natural-language models via local… 1895\nFig. 11 nPIR distribution of the most inﬂuential features ( Max nPIR ) and the least inﬂuential features ( Min\nnPIR) over 512 input texts for each model and task\nfor each local explanation, we selected the most inﬂuential feature (i.e., with the highest\nnPIR) and the least inﬂuential feature (i.e., with the lowest nPIR).\nFigure 11 shows the nPIR distribution of the most inﬂuential features ( Max nPIR )a n d\nthe least inﬂuential features ( Min nPIR ) for all input texts, separately by each model-task.\nThe nPIR values of the least inﬂuential features are close to zero for all models, whereas\nthe most inﬂuential features have nPIR values close to 1 for all models and generally higher\nthan 0.5. BERT performs better on these tasks and, consequently, T- EBAnO is able to ﬁnd\nfeatures having extreme nPIR values. A model like ULMﬁt, instead, is more uncertain in the\nprediction, and T- EBAnO ﬁnds features with variable Max nPIR values from 0.5 to 1. Such\nresults show that T- EBAnO is able to extract different features from the input texts, both\nhighly inﬂuential and neutral ones, for the prediction of the class label. Moreover, T- EBAnO\nis able to ﬁnd explanations for models with different architectures and different classiﬁcation\ntasks.\nTable10 shows examples of local explanations for the different models and tasks. For each\ninput text i, one highly inﬂuential feature ( i.a) and one neutral feature (or less inﬂuential)\n(i.b) are reported. The original predicted label L\no, the label predicted after the perturbation\nLo, and the relative nPIR score obtained by the feature (with respect to the original predicted\nlabel L o) applying the removal perturbation are also reported.\nFor Ag News , inputs from 1 to 6 show that all models correctly learned the concepts of\nWorld, Business, Sport,a n d Science/Technology. All the inﬂuential features 1-6.a contain\nconcepts related to the predicted class L0, whereas less inﬂuential features 1-6.b contain neu-\ntral concepts or tokens. The only exception is the 6.a example, which shows that the ULMFit\nmodel overﬁts some tokens, as the speciﬁc name of the London’s agency { Reuters}h a s\nbeen learned as important for the class label Business. The behavior of the explanations is\nalso coherent with the performance in terms of the accuracy of the models. Analyzing a wider\nset of explanations, also BERT and ALBERT models overﬁt some tokens, such as HTML\nstrings of web pages, that are often related to misclassiﬁed inputs in Science/Technology.\nRegarding Cola, the explanations from id 7 to 13 show that the models generally learned\nto classify grammatically correct sentences. The explanations of the Acceptable class label\nusually contain most of the input text, while the explanations of the Unacceptable class labels\n123\n1896 F. Ventura et al.\nTable 10 Features extracted by MLWE on different models and different tasks (highlighted in cyan). For Ag\nNews, the labels are Sport (S), World (W), Business (B), Science/Technology (S/T). For Cola, the labels are\nUnacceptable (U) and Acceptable (A)\nid MLWE feature Lo Lf nPIR\nBERT-Ag News\n1.a uk gives blessing to open source . with most organizations that\nplanned to move already moved to microsoft server 2003 ,\nos migration has dropped to the bottom ranks after making its\nS/T W 0.983\n1.b uk gives blessing to open source . with most organizations that\nplanned to move already moved to microsoft server 2003 ,\nos migration has dropped to the bottom ranks after making its\nS/T S/T 0.000\n2.a radcliﬀe to run in new york marathon . london ( reuters ) - world\nmarathon record holder paula radcliﬀe believes she has put her\nfailure at the athens olympics behind her after announcing on tuesday\nthat she will run in the new york marathon on november 7 .\nS W 0.893\n2.b radcliﬀe to run in new york marathon . london ( reuters ) - world\nmarathon record holder paula radcliﬀe believes she has put her\nfailure at the athens olympics behind her after announcing on tuesday\nthat she will run in the new york marathon on november 7 .\nS S 0.006\nALBERT-Ag News\n3.a eu seeks joint asylum policy. eu ministers meeting in luxembourg\nplan moves to integrate their asylum and immigration procedures.\nW S/T 0.709\n3.b eu seeks joint asylum policy. eu ministers meeting in luxembourg\nplan moves to integrate their asylum and immigration procedures.\nW W -0.023\n4.a job numbers give candidates room to debate. washington - employers\nstepped up hiring in august, expanding payrolls by 144,000 and\nlowering the unemployment rate to 5.4 percent.\nB W 0.912\n4.b job numbers give candidates room to debate. washington - employers\nstepped up hiring in august, expanding payrolls by 144,000 and\nlowering the unemployment rate to 5.4 percent.\nB B 0.008\nULMﬁt-Ag News\n5.a nato to send staﬀ to iraq . nato will send military trainers to iraq\nbefore the end of the year in response to appeals by iraqi leaders for\nspeedy action , us ambassador to nato nicholas burns said today .\nW S/T 0.706\n5.b nato to send staﬀ to iraq . nato will send military trainers to iraq\nbefore the end of the year in response to appeals by iraqi leaders for\nspeedy action , us ambassador to nato nicholas burns said today .\nW W 0.001\n6.a court seen lifting yukos block – lawyers . london ( reuters )-au . s.\nbankruptcy court is likely to revoke its temporary ban on the sale of\nrussian oil group yukos ’s main production unit, lawyers said on friday\nB W 0.993\n6.b court seen lifting yukos block -- lawyers . london ( reuters ) - a u.s .\nbankruptcy court is likely to revoke its temporary ban on the sale of\nrussian oil group yukos ’s main production unit, lawyers said on friday\nB B 0.043\nBERT-Cola\n7.a many people said they were sick who weren’ t . U A 0.985\n7.b many people said they were sick who weren’ t . U U 0.200\n8.a charlie will leave town if his mother - in - law doesn’ t. A U 0.995\n8.b charlie will leave town if his mother - in - law doesn’ t . A A 0.452\n9.a snow white poisoned . U A 0.754\n9.b snow white poisoned . U U 0.014\nALBERT-Cola\n10.a mary runs not the marathon. U A 0.819\n10.b mary runs not the marathon. U U 0.267\n11.a both workers will wear carnations. A U 0.744\n11.b both workers will wear carnations. A A 0.033\nULMﬁt-Cola\n12.a you could give a headache to a tylenol . U A 0.930\n12.b you could give a headache to a tylenol . U U 0.119\n13.a paul breathed on mary . U A 0.999\n13.b paul breathed on mary . U U 0.320\nLo is the original predicted label, L f is the label predicted after the perturbation on the feature, nPIR is the\nscore obtained by the feature with respect to the original predicted label (nPIR(L o)). For each input i,t h e r e\nare two features, one highly inﬂuential ( i.a) and one neutral or less inﬂuential ( i.b)\n123\nTrusting deep learning natural-language models via local… 1897\nTable 11 Most informative local\nexplanation example Feature ID nPIR Tokens ratio FIS\nFeature 1 0 .50 10 /50 = 0.20 0 .620\nFeature 2 0 .99 15 /50 = 0.30 0 .874\nFeature 3 1 .00 25 /50 = 0.50 0 .800\ntend to highlight small portions of the input text containing errors. This behavior is reasonable\nbecause a sentence is correct if all its tokens are correct, while it is incorrect if it contains\nsome wrong tokens.\n6.6 MLWE effectiveness\nFor a given input text, while the number of tokens of each feature extracted by the part-of-\nspeech (PoS) and sentence-based (SEN) approaches is ﬁxed, the MLWE feature extraction\nﬁgures out by itself the right number and which tokens to assign to each feature. For an\neffective explanation , we want that the most inﬂuential feature extracted by the MLWE\nmaximizes the nPIR while minimizing the number of tokens (i.e., it selects only the tokens\ncontributing to a high nPIR).\nFor instance, Table 11 reports a sample (partial) result where the MLWE is applied to an\ninput text with 50 total tokens: three possible clustering results are discussed (note that the\ndiscussion is limited to 3 for simplicity, but K\nmax should be used for full results, as described\nin Sect. 4.4).\n• The most inﬂuential feature in the ﬁrst clustering is Feature 1 with nPIR = 0.50 and it\nconsists of 10 tokens.\n• The most inﬂuential feature in the second clustering is Feature 2 with nPIR = 0.99 and\nit consists of 15 tokens.\n• The most inﬂuential feature in the third clustering is Feature 3 with nPIR = 1.00 and\nit consists of 25 tokens.\nThe feature informative score FIS , as explained in Sect. 4.4, is computed accordingly to the\nfollowing formula:\nFIS (κ) = max\n( (\nα(nPIR κ) + β(1 − κtk /ntk )\n)\n,0\n)\n= max\n( (\n0.60(nPIR κ) + 0.40(1 − κtk /ntk )\n)\n,0\n) (9)\nThen, the ﬁnal most informative local-explanation selected by T- EBAnO is Feature 2\nbecause it provides a high nPIR with a limited number of tokens. Feature 1 has a smaller\nnumber of tokens, but its lower nPIR leads to a lower FIS. Feature 3, on the contrary, has a\nhigher nPIR but includes much more tokens, hence having a lower FIS too.\nThus, MLWE can be viewed as a heuristic that exploiting the inner information of the\nmodel, ﬁgures out exactly the group of tokens that inﬂuenced mostly the original input\nprediction in a reasonable amount of time. Indeed, in theory, the best possible solution (i.e.,\nthe smallest amount of tokens that mostly inﬂuenced the prediction) could be found by\nexploring all the n-combinations of tokens for each n in the range [2,n\ntk ] (where ntk is the\nnumber of tokens in the input text) and taking the one that maximizes a performance metric\nsuch as the FIS score. For instance, if an input text contains 100 tokens, it would be necessary\n123\n1898 F. Ventura et al.\nto explore and evaluate all the two-combinations, three-combinations up to 100-combinations\nof 100 input tokens, making the problem unfeasible, especially for long texts.\nTherefore, to evaluate the effectiveness of the MLWE, we compare its performance with a\nRandom feature extraction method. The Random feature extraction creates several features,\neach composed of a group of nr random tokens, with different sizes (i.e., number of tokens)\nin the range nr ∈[ 1,nr _max ] where nr _max is set to 80% of the total tokens of the input\ntext ntk . Speciﬁcally, for each nr value, it creates ﬁve random features, each composed of\na different group of nr random tokens selected from the input text. For instance, if an input\ntext has 100 tokens, the Random feature extraction creates ﬁve features composed of one\nrandom token, ﬁve features each composed of a group of two random tokens, up until ﬁve\nfeatures each composed of a group of 80 random tokens. We chose to create ﬁve features\nfor each random feature size n\nr value in the speciﬁed range because, with these settings, the\nRandom feature extraction creates at least 5 times more features than MLWE. Consequently,\nit has a clear advantage in the comparison at the cost of more computational power. Thus,\nwe want to see if, selecting a random subset of all the possible solutions (i.e., the random\nfeatures of different sizes are a subset of all the possible combinations of tokens), where the\ncost of extracting and evaluating the inﬂuence of these random features is much higher with\nrespect to the MLWE (i.e., higher computation time), the most inﬂuential features founded by\nthe MLWE are more effective in terms of inﬂuence and compactness (i.e., nPIR and tokens\nratio).\nWe experimented on BERT-IMDB and BERT-Ag News since BERT-Cola contains very\nshort sentences, which was not meaningful for our goals. We produced the local explanations\nwith both the MLWE and the Random feature extraction from 512 input documents for each\ntask. For BERT-IMDB, about 230 thousand features have been produced with nPIR _mean =\n0.1 (about 460 for each input). Instead, for BERT-Ag News, about 91 thousand features have\nbeen produced with nPIR _mean = 0.07 (about 185 for each input). This shows that simply\nremoving some random groups of tokens to obtain a high nPIR value would be insufﬁcient,\nhence the need to carefully and smartly select the tokens. However, as expected, due to the\nlarge number of features extracted from each input text, some Random features obtain a high\nnPIR score by chance. For each input text, we selected the most informative local-explanation\nextracted with the Random feature extraction method exploiting the same formula used by\nthe MLWE (Eq. 9).\nTo understand the effectiveness of the MLWE, we compared the percentage of selected\ntokens ratio (i.e., the number of tokens of the feature with respect to the total number of\ntokens in the input) belonging to the very high inﬂuential features ( nPIR ≥ 0.9) extracted\nby MLWE and Random on the two tasks.\nFigure 12 shows the CDF (Cumulative Distribution Function) of the very high inﬂuential\nfeatures with respect to the percentage of tokens. The chart shows that T- EBAnO with\nMLWE ﬁnds very high inﬂuential features selecting fewer tokens with respect to the Random\nfeature extraction method. Indeed, looking at the CDF, the 75% of very high inﬂuential\nfeatures (i.e., nPIR ≥ 0.90) found by MLWE (blu lines) on IMDB and Ag News contains,\nrespectively, less than 35% and 50% of tokens. On the other hand, the 75% of very high\ninﬂuential features found by Random (orange lines) on IMDB and Ag News contains less\nthan or equal to 55% and 70% of tokens. MLWE is then more effective in selecting a lower\nnumber of more inﬂuential tokens.\nWe also compared the execution time of the MLWE and the Random feature extraction.\nFor the IMDB task, the MLWE feature extraction method is about 6 times faster than the\nRandom approach, with 35 seconds per input versus 215 seconds per input, on average. For\nthe\nAg News task, the MLWE feature extraction method is about 4.5 times faster than the\n123\nTrusting deep learning natural-language models via local… 1899\nFig. 12 CDF of tokens percentage ratio (i.e., percentage of feature tokens over total input tokens) for very\nhigh inﬂuential features (i.e., features with nPIR ≥ 0.9) extracted with T- EBAnO -MLWE and Random,\nseparately for BERT-IMDB and BERT-Ag News\nRandom approach, with 10 seconds per input versus 46 seconds per input, on average. By\nexploiting the inner knowledge learned by the model, the MLWE feature extraction method\nprovides higher effectiveness and efﬁciency with respect to searching random features. The\nMLWE approach ﬁnds high inﬂuential features containing a lower percentage of tokens and,\nat the same time, reduces the execution time.\n6.7 Hyperparameters evaluation\nWe evaluated the impact on the most informative local explanation produced by the Multi-\nlayer Word Embedding feature extraction (MLWE) by changing the hyperparameters α and\nβ of the Feature Informative Score (FIS) computation. We recall that α and β sum up to\none and weights the inﬂuence (nPIR) and the compactness (1 - tokens ratio), respectively,\nin the Feature Importance Score (FIS) computation (Eq. 9). The tokens ratio is computed\nas the number of tokens in the feature over the total number of tokens. The objective of Eq.\n9 in the unsupervised clustering analysis is to maximize the inﬂuence (nPIR) and minimize\nthe number of tokens inside the most inﬂuential cluster of each k division. Thus, the most\ninﬂuential clusters founded will change based on these hyperparameters.\nWe used the BERT model ﬁne-tuned for topic classiﬁcation on Ag News dataset (Use cases\n3i nT a b l e4) for this purpose. We randomly selected 512 input texts from the dataset, and we\nproduced the local explanations with T- EBAnO .T a b l e12 shows the mean nPIR and tokens\nratio for different α and β values for the most informative local explanations extracted by\nT- EBAnO (i.e., with max FIS score). Speciﬁcally, for each local explanation of each input\ntext, we selected the most informative explanation, and we averaged the tokens ratio and the\ninﬂuence (nPIR) of the most inﬂuential features over the entire dataset.\nOn the one hand, with smaller α values (0.2 and 0.3), and respectively high β values (0.8\nand 0.7), the most informative features founded by the MLWE have a low mean inﬂuence\n(mean nPIR 0.31 and 0.45 respectively). But the most informative clusters are very compact,\nbeing composed of only 11% and 15%. However, even if tiny clusters increase the compre-\nhensibility of the explanation, they lack completeness because they select only a partial set\nof the relevant tokens mostly used by the model for the original prediction.\nOn the other hand, greater α values (0.7 and 0.8), and respectively high β values (0.3 and\n0.2), obtain larger nPIR mean values (close to 0.64) with the pain of larger clusters found.\nHowever, the increase in the nPIR mean is too small compared with the cost of the increasing\nsize with respect to values of α ∈[ 0.4,0.5,0.6] and relative β ∈[ 0.6,0.5,0.4] values.\nIndeed, they achieve a mean nPIR of 0.54, 0.59, and 0.61, with a mean tokens ratio of 21%,\n24%, and 25%, respectively. Finally, the couple α = 0.3a n d β = 0.\n7 values obtained an\n123\n1900 F. Ventura et al.\nTable 12 Mean inﬂuence ( nPIR ) and mean selected tokens ratio ( To ke nsRatio ) in the most inﬂuential\nexplanations with different α and β values\nHyperparameter values\nα = 0.2 α = 0.3 α = 0.4 α = 0.5 α = 0.6 α = 0.7 α = 0.8\nβ = 0.8 β = 0.7 β = 0.6 β = 0.5 β = 0.4 β = 0.3 β = 0.2\nnPIR 0.31 0.45 0.54 0.59 0.61 0.64 0.64\nTo ke nsRatio 11% 15% 21% 24% 25% 29% 31%\nalready good mean nPIR value of 0.45 with a very small percentage of tokens highlighted,\nequal to 15%.\nIn conclusion, in this paper, we used α = 0.6a n d β = 0.4 as default values because\nthey allow us to reach a good trade-off between the number of highlighted tokens and their\ninﬂuence. Indeed, even if a clear best value does not emerge from this experiment, setting\nα = 0.4a n d β = 0.6 seems good to obtain small clusters with a strong inﬂuence. However,\nother possible values could be useful in different scenarios. Thus, the ﬁnal user can change\nthis parameter accordingly to speciﬁc needs.\n6.8 nPIR correlation with human judgement\nTo assess the quality of the explanations, which are selected by T- EBAnO based on their nPIR\nvalue, we evaluate the correlation between the nPIR value and human judgment. The human\nvalidation is performed by interviewing both expert and non-expert users with a survey\n2.T h e\nsurvey contains local explanations extracted by T- EBAnO , and their nPIR value is compared\nwith the relevance assigned by the users. More precisely, we selected 12 input texts from Ag\nNews with BERT and eight input texts from the Toxic Comment use case with the LSTM\nmodel. In such use cases, input texts are shorter and then more suitable for a survey. For\nthe purpose of this survey, we selected only correctly classiﬁed examples. For each input\ntext, we randomly picked one highly inﬂuential feature and one neutral feature extracted\nby T- EBAnO . Those features are then presented to the user, who is requested to select\none option among \"V ery Relevant\", \"Relevant\",a n d \"Not Relevant\" for each feature. The\nmain scope of the survey is to measure and evaluate the correlation between the inﬂuence\nindex (nPIR) and human judgment. However, we also indirectly validate the quality and\nreadability of the explanations produced by T- EBAnO : for correctly classiﬁed examples, if\nthe proposed explanations are effective and human-readable, then the user should understand\nwhich features are important ( \"Relevant\" or \"V ery Relevant\") and which are neutral ( \"Not\nRelevant\").\nFigure 13 shows the introductory example of the survey. In the ﬁrst box (input text),\nthe user can read the original input text, the predicted label, and the probabilities of such\nprediction computed by the NLP model. Then, two explanations are presented for each input\ntext, with the feature words highlighted in light blue. In total, at the time of writing, we\ncollected 4320 user evaluations from 108 participants (each evaluating 2 explanations from\n20 input texts), with 76% being expert machine learning users, and 18% being also expert\nusers of Natural Language Processing with deep learning (as anonymously self-declared by\n2 The link to the online survey is available in the T- EBAnO GitHub repository.\n123\nTrusting deep learning natural-language models via local… 1901\nFig. 13 Survey’s introduction example\nthemselves in the survey). Participants have been invited among researchers and students of\nPhD and Master courses in Computer Science.\nTo evaluate the correlation between nPIR and the human judgment, we assigned to each\nquestion (that corresponds to an explanation/feature extracted by T- EBAnO )a manual score\nof 0 if the user selected \"Not Relevant\" ,0 . 5f o r \"Relevant\", and 1 for \"V ery Relevant\".\nFigure 14 shows, for each of the 40 explanations (2 for each of the 20 input texts), the\nnPIR assigned by T- EBAnO (the blue bars), and the mean relevance assigned by the 108\nusers (the red bars), according to the manual scores (the data are presented in descending\norder of Human Score). The chart shows an explicit correlation between the nPIR assigned by\nT- EBAnO for both inﬂuential and neutral features, for both tasks, topic detection and toxic\ncomment classiﬁcation. This also implies that T- EBAnO produces effective and human-\nreadable explanations for the ﬁnal users.\nWe also measured the inter-annotator agreement between the 108 annotators (survey\nparticipants) by using each explanation as input (for a total of 40 annotations). Then, we\nobtained only two possible labels by aggregating the \"Relevant\" and the \"V ery Relevant\"\ninto the same label. We exploited the Krippendorff’s alpha coefﬁcient\n3 [23] to measure the\ninter-annotator reliability agreement. We obtained a Krippendorff’s alpha coefﬁcient of 0.65,\ndenoting a good agreement between the 108 participants and the 40 explanations.\nAdditionally, we asked the participants (i) if the task of the survey was clear, as a self-\nevaluation check: 44% answered 5 (max value), and 41% chose 4 out of 5; (ii) if the\nexplanations proposed by T- EBAnO were easy to understand, the answers from top (5)\nto bottom (1) were distributed as follows: 34%, 45%, 18% 3%, and 0%.\n6.9 Effectiveness evaluation with respect to model-agnostic techniques\nIn this section, we compare the effectiveness of the T- EBAnO -MLWE explanations with two\nmodel-agnostic explainability techniques. Comparing explainability methodologies is still an\nopen issue in the research community, as a deﬁnitive deﬁnition of good explanation is missing.\n3 It has been exploited the implementation in: https://github.com/LightTag/simpledorff.\n123\n1902 F. Ventura et al.\nFig. 14 Comparison between nPIR assigned by T- EBAnO (blue bars) and mean human scores (red bars).\nQuestions are ordered by descending mean human score\nHowever, explanations should have important properties such as: Fidelity, Comprehensibility,\nComplexity, Effectiveness, Trustworthiness,e t c .\nWe performed an experiment to evaluate the Fidelity and the Effectiveness of the explana-\ntions proposed by T- EBAnO with respect to two state-of-the-art model-agnostic techniques,\nLIME4 [40]a n d SHAP5 [32].\nTo measure the Fidelity and Effectiveness of the proposed explanations, we removed the\nwords/tokens highlighted as important by the different methodologies, and we measured\nthe change in probability caused by this deletion. Basically, we are asking the following\nquestions:\n1. Are the important words/tokens highlighted by the explainability techniques effectively\nthe ones used by the model to perform the original prediction?\n2. How does the model prediction change if the highlighted words/tokens are not present in\nthe original text?\nIf removing the words/tokens highlighted by the explanation does not correlate with a\nreduction in the probability of the original class label, then the selected words are not among\nthe important features used by the model to produce the original label. On the contrary, the\nlarger the probability changes, the more the model relied on those words/tokens to predict\nthe original label.\nTo make a fair comparison, we created features composed of the same percentage of the\nmost important tokens identiﬁed by the different methodologies. LIME assigns an importance\nscore to each token. However, it requires deﬁning the percentage of the most important tokens\nfor the importance score. Therefore, we set this parameter so that the number of the most\nimportant tokens for the class of interest is almost equal to the mean number of tokens\nhighlighted by the T- EBAnO explanations. Instead, SHAP assigns an importance score to\neach token of the input text ( Shapley V alues [44]). Thus, we selected the most important\nones with the same percentage of T- EBAnO . In this way, we selected subsets with similar\ncardinality and importance.\nWe chose as experimental use cases (i) a BERT model ﬁne-tuned for sentiment analysis\nwith IMDB and (ii) a BERT model ﬁne-tuned for topic classiﬁcation on Ag News Subset.\nWe did not use the same models trained in Table 4 due to compatibility issues. We trained\ntwo new models exploiting the HuggingFace\n6 library. The ﬁne-tuned models reached 93%\nand 95% accuracy on the validation set for IMDB and Ag News Subset, respectively. The\n4 The LIME parameter number of permutations has been set to 5000.\n5 The Partition Explainer version of SHAP has been used.\n6 https://huggingface.co/.\n123\nTrusting deep learning natural-language models via local… 1903\nexperiments were performed on a single node of the SmartData BigData cluster at Polito 7.\nThe node contains two Intel Xeon Gold 6140 CPUs with 2.30 GHz frequency and 384 GB\nof RAM. However, for the experiment, we limited the process to using only one CPU with\na maximum of 120 GB of RAM (without exploiting GPUs).\nFor the IMDB case, T- EBAnO -MLWE highlights on average about 20% of tokens, so\nwe also removed the top-20% of tokens selected by LIME and SHAP . We evaluated the\nprobability difference before and after removing the highlighted tokens for each methodology.\nRemoving the most inﬂuential tokens highlighted by T- EBAnO causes a mean decrease of\nprobability around 71%, the same removal for LIME causes a 48% probability drop on\naverage, and for SHAP , the mean probability decrease is 59%. We also compared the mean\nexecution time to produce an explanation. The IMDB dataset contains relatively long texts\nand, on average, T- EBAnO took 38 seconds, while LIME 304 and SHAP 484 seconds.\nFor the second use case, on the Ag News dataset, T- EBAnO -MLWE highlights, on aver-\nage, about 30% of tokens. We removed the top-30% of tokens selected by LIME and SHAP .\nThis time, removing the most important tokens yields a mean decrease of probability around\n75% for T- EBAnO , 60% for LIME, and 61% for SHAP . The mean execution time to pro-\nduce each explanation is lower because this dataset contains shorter sentences. Speciﬁcally,\nT- EBAnO takes on average 4 seconds, LIME 239 seconds, and SHAP 16 seconds.\nWe notice that not only T- EBAnO is much faster than the other two methodologies\n(approximately from 1 to 2 orders of magnitude), but also the explanations provided are\nmore faithful and effective, by highlighting as important tokens the ones that were the most\nimpacting for the prediction of the model under analysis.\n7 Conclusion and future research directions\nThis paper proposed T- EBAnO , a new engine able to provide both prediction-local and\nmodel-global interpretable explanations in the context of NLP analytics tasks that exploit\ndeep learning models. T- EBAnO ’s experimental assessment includes different NLP classiﬁ-\ncation tasks, i.e., sentiment analysis task, comment toxicity, topic classiﬁcation, and sentence\nacceptability, performed through state-of-the-art techniques: ﬁne-tuned models like BERT,\nALBERT, and ULMFit and a custom LSTM model.\nResults showed that T- EBAnO can (i) identify speciﬁc features of the textual input data\nthat are predominantly inﬂuencing the model’s predictions, (ii) highlight such features to the\nend-user, and (iii) quantify their impact through effective indexes. The proposed explanations\nenable end-users to decide whether a speciﬁc local prediction made by a deep learning\nmodel is reliable and to evaluate the general behavior of the global model across predictions.\nBesides being useful to general-purpose end users, explanations provided by T- EBAnO are\nespecially useful for data scientists, artiﬁcial intelligence and machine learning experts in\nneed of understanding the behavior of their models since the extracted features, both textual\nand numeric, are an efﬁcient way to harness the complex knowledge learned by the models\nthemselves.\nFuture research directions include: (a) investigating new strategies for the perturbation of\nthe input features, such as new kinds of substitution perturbations, exploiting task-speciﬁc or\nexpert-driven directives; (b) integrating T- EBAnO in a real-life setting to measure the effec-\ntiveness of the proposed textual explanations by real-world human evaluation; (c) extending\nT- EBAnO to address new data analytics activities, such as guiding data scientists in applying\n7 https://smartdata.polito.it/computing-facilities/ .\n123\n1904 F. Ventura et al.\nﬁne-tuned deep-learning models, explaining concept drifts, and providing insights on Adver-\nsarial Attack countermeasures; (d) extending the proposed methodology and inﬂuence index\n(i.e., nPIR) to new NLP tasks such as Question Answering and Named Entity Recognition.\n(e) designing an XAI comparison methodology tailored to the NLP domain containing both\nobjective and subjective comparison criteria and applying it to compare T- EBAnO with\nseveral XAI methodologies.\nFunding Open access funding provided by Politecnico di Torino within the CRUI-CARE Agreement. Not\napplicable.\nData availability Not applicable.\nCode Availability The code for T- EBAnO is available at https://github.com/EBAnO-Ecosystem/Text-\nEBAnO-Express.\nDeclarations\nConﬂict of interest The authors declare that they have no known competing ﬁnancial interests or personal\nrelationships that could have appeared to inﬂuence the work reported in this paper.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. Adadi A, Berrada M (2018) Peeking inside the black-box: a survey on explainable artiﬁcial intelligence\n(xai). IEEE Access 6:52138–52160. https://doi.org/10.1109/ACCESS.2018.2870052\n2. Alvarez-Melis D, Jaakkola TS (2017) A causal framework for explaining the predictions of black-box\nsequence-to-sequence models. arXiv preprint arXiv:1707.01943\n3. Banzhaf J (1965) Weighted voting doesn’t work: a mathematical analysis. Rutgers Law Rev 19(2):317–\n343\n4. Basiri ME, Nemati S, Abdar M, Cambria E, Acharya UR (2021) Abcdm: an attention-based bidirectional\ncnn-rnn deep model for sentiment analysis. Futur Gener Comput Syst 115:279–294. https://doi.org/10.\n1016/j.future.2020.08.005\n5. Bolukbasi T, Chang KW, Zou J, Saligrama V , Kalai A (2016) Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings\n6. Borkan D, Dixon L, Sorensen J, Thain N, V asserman L (2019) Nuanced metrics for measuring unintended\nbias with real data for text classiﬁcation. CoRR arXiv:1903.04561\n7. Chakraborty M, Biswas SK, Purkayastha B (2020) Rule extraction from neural network trained using\ndeep belief network and back propagation. Knowl Inf Syst 62(9):3753–3781. https://doi.org/10.1007/\ns10115-020-01473-0\n8. Chen J, Jordan M (2020) Ls-tree: Model interpretation when the data are linguistic. Proc AAAI Conf\nArtif Intell 34(04):3454–3461. https://doi.org/10.1609/aaai.v34i04.5749\n9. Datta A, Sen S, Zick Y (2016) Algorithmic transparency via quantitative input inﬂuence: Theory and\nexperiments with learning systems. In: 2016 IEEE symposium on security and privacy (SP), pp. 598–\n617. https://doi.org/10.1109/SP .2016.42\n10. Deeks A (2019) The judicial demand for explainable artiﬁcial intelligence. Columbia Law Rev\n119(7):1829–1850\n11. Devlin J, Chang M, Lee K, Toutanova K (2018) BERT: pre-training of deep bidirectional transformers\nfor language understanding. CoRR arXiv:abs/1810.04805\n123\nTrusting deep learning natural-language models via local… 1905\n12. Du M, Liu N, Y ang F, Hu X (2020) Learning credible dnns via incorporating prior knowledge and model\nlocal explanation. Knowledge Inf Syst. https://doi.org/10.1007/s10115-020-01517-5\n13. Ethayarajh K (2019) How contextual are contextualized word representations? comparing the geometry\nof bert, elmo, and gpt-2 embeddings. ArXiv arXiv:abs/1909.00512\n14. Ethayarajh K (2019) How contextual are contextualized word representations? comparing the geometry\nof bert, elmo, and gpt-2 embeddings\n15. Fong RC, V edaldi A (2017) Interpretable explanations of black boxes by meaningful perturbation. In:\n2017 IEEE international conference on computer vision (ICCV). https://doi.org/10.1109/iccv.2017.371\n16. Glorot X, Bengio Y (2010) Understanding the difﬁculty of training deep feedforward neural networks.\nIn: Y .W. Teh, M. Titterington (eds.) In: Proceedings of the thirteenth international conference on artiﬁcial\nintelligence and statistics, Proceedings of machine learning research, vol. 9, pp. 249–256. JMLR Workshop\nand Conference Proceedings, Chia Laguna Resort, Sardinia, Italy\n17. Guidotti R, Monreale A, Ruggieri S, Turini F, Giannotti F, Pedreschi D (2018) A survey of methods for\nexplaining black box models. ACM Comput Surv 51(5):93:1-93:42. https://doi.org/10.1145/3236009\n18. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9:1735–80. https://doi.\norg/10.1162/neco.1997.9.8.1735\n19. Howard J, Ruder S (2018) Universal language model ﬁne-tuning for text classiﬁcation\n20. Jia Y , Bailey J, Ramamohanarao K, Leckie C, Ma X (2020) Exploiting patterns to explain individual\npredictions. Knowl Inf Syst 62(3):927–950. https://doi.org/10.1007/s10115-019-01368-9\n21. Karlsson I, Rebane J, Papapetrou P , Gionis A (2020) Locally and globally explainable time series tweaking.\nKnowl Inf Syst 62(5):1671–1700. https://doi.org/10.1007/s10115-019-01389-4\n22. Khodabandehloo E, Riboni D, Alimohammadi A (2020) Healthxai: collaborative and explainable ai for\nsupporting early diagnosis of cognitive decline. Fut Gener Comput Syst. https://doi.org/10.1016/j.future.\n2020.10.030\n23. Krippendorff K (2011) Computing krippendorff’s alpha-reliability\n24. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P , Soricut R (2020) Albert: a lite bert for self-supervised\nlearning of language representations\n25. Lei T, Barzilay R, Jaakkola T (2016) Rationalizing neural predictions\n26. Lepri B, Staiano J, Sangokoya D, Letouzé E, Oliver N (2017) The Tyranny of data? The bright and dark\nsides of data-driven decision-making for social good. Springer, Cham, pp 3–24\n27. Lertvittayakumjorn P , Toni F (2019) Human-grounded evaluations of explanation methods for text clas-\nsiﬁcation. ArXiv arXiv:abs/1908.11355\n28. Li J, Monroe W, Jurafsky D (2016) Understanding neural networks through representation erasure\n29. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis, M, Zettlemoyer L, Stoyanov V (2019)\nRoberta: a robustly optimized BERT pretraining approach. CoRR arXiv:abs/1907.11692\n30. Lloyd S (1982) Least squares quantization in pcm. IEEE Trans Inf Theory 28(2):129–137. https://doi.\norg/10.1109/TIT.1982.1056489\n31. Lughofer E, Richter R, Neissl U, Heidl W, Eitzinger C, Radauer T (2017) Explaining classiﬁer decisions\nlinguistically for stimulating and improving operators labeling behavior. Inf Sci 420:16–36\n32. Lundberg SM, Lee SI (2017) A uniﬁed approach to interpreting model predictions. In: Guyon I, Luxburg\nUV , Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds) Advances in neural information\nprocessing systems. Curran Associates Inc, Red Hook, pp 4765–4774\n33. Maas AL, Daly RE, Pham PT, Huang D, Ng A Y , Potts C (2011) Learning word vectors for sentiment\nanalysis. In: Proceedings of the 49th Annual Meeting of the association for computational linguistics:\nhuman language technologies, pp. 142–150. Association for Computational Linguistics, Portland, Oregon,\nUSA\n34. Mathews SM (2019) Explainable artiﬁcial intelligence applications in nlp, biomedical, and malware\nclassiﬁcation: A literature review. In: Arai K, Bhatia R, Kapoor S (eds) Intelligent Computing. Springer\nInternational Publishing, Cham, pp 1269–1292\n35. Murdoch WJ, Szlam A (2017) Automatic rule extraction from long short term memory networks\n36. Naseem U, Razzak I, Musial K, Imran M (2020) Transformer based deep intelligent contextual embedding\nfor twitter sentiment analysis. Futur Gener Comput Syst 113:58–69. https://doi.org/10.1016/j.future.2020.\n06.050\n37. Pastor E, Baralis E (2019) Explaining black box models by means of local rules. In: Proceedings of the\n34th ACM/SIGAPP symposium on applied computing, SAC ’19, pp. 510–517. ACM, New Y ork, NY ,\nUSA. https://doi.org/10.1145/3297280.3297328\n38. Pennington J, Socher R, Manning CD (2014) Glove: Global vectors for word representation. In: Empirical\nmethods in natural language processing (EMNLP), pp. 1532–1543\n39. Rajpurkar P , Zhang J, Lopyrev K, Liang P (2016) Squad: 100,000+ questions for machine comprehension\nof text. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,\n123\n1906 F. Ventura et al.\nAustin, Texas. Association for Computational Linguistics, pp 2383–2392, https://doi.org/10.18653/v1/\nD16-1264\n40. Ribeiro MT, Singh S, Guestrin C (2016) Why should i trust you? explaining the predictions of any\nclassiﬁer. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery\nand data mining, pp. 1135–1144\n41. Samek W, Montavon G, V edaldi A, Hansen L, Muller KR (2019) Explainable AI interpreting, explaining\nand visualizing deep. Learning. https://doi.org/10.1007/978-3-030-28954-6\n42. Sanh V , Debut L, Chaumond J, Wolf T (2019) Distilbert, a distilled version of BERT: smaller, faster,\ncheaper and lighter. CoRR arXiv:abs/1910.01108\n43. Selvaraju RR, Cogswell M, Das A, V edantam R, Parikh D, Batra D (2019) Grad-cam: visual explanations\nfrom deep networks via gradient-based localization. Int J Comput Vis. https://doi.org/10.1007/s11263-\n019-01228-7\n44. Shapley LS (1953) A value for n-person games. Contrib Theory Games 2(28):307–317\n45. Shrikumar A, Greenside P , Kundaje A (2017) Learning important features through propagating activation\ndifferences. In: D. Precup, Y .W. Teh (eds.) In: Proceedings of the 34th international conference on machine\nlearning, Proceedings of machine learning research, vol. 70, pp. 3145–3153. PMLR. https://proceedings.\nmlr.press/v70/shrikumar17a.html\n46. Trifonov V , Ganea OE, Potapenko A, Hofmann T (2018) Learning and evaluating sparse interpretable\nsentence embeddings. In: Proceedings of the 2018 EMNLP Workshop BlackboxNLP: analyzing and\ninterpreting neural networks for NLP , pp. 200–210. Association for Computational Linguistics, Brussels,\nBelgium. https://doi.org/10.18653/v1/W18-5422\n47. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017)\nAttention is all you need. CoRR arXiv:abs/1706.03762\n48. V entura F, Cerquitelli T, Giacalone F (2018) Black-box model explained through an assessment of its\ninterpretable features. In: New trends in databases and information systems: ADBIS 2018 Short Papers and\nWorkshops, AI*QA, BIGPMED, CSACDB, M2U, BigDataMAPS, ISTREND, DC, Budapest, Hungary,\nSeptember, 2-5, 2018, Proceedings, pp. 138–149. https://doi.org/10.1007/978-3-030-00063-9_15\n49. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman SR (2018) Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv preprint arXiv:1804.07461\n50. Warstadt A, Singh A, Bowman SR (2018) Neural network acceptability judgments. arXiv preprint\narXiv:1805.12471\n51. Zhang X, Zhao J, LeCun Y (2015) Character-level convolutional networks for text classiﬁcation\n52. Zheng X, Wang M, Chen C, Wang Y , Cheng Z (2019) Explore: explainable item-tag co-recommendation.\nInf Sci 474:170–186\n53. Zhou Q, Liu X, Wang Q (2020) Interpretable duplicate question detection models based on attention\nmechanism. Information Sciences\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\nFrancesco Ventura received the master’s degree in Computer Engineer-\ning in 2017, and the PhD in Computer and Control Engineering in 2021\nboth from the Politecnico di Torino, Italy. His research interests include\nthe design and the development of complex systems for machine learn-\ning and data analytics with a speciﬁc attention to machine learning\nmodels’ explainability and transparent data processing, with applica-\ntions in both structured and unstructured data.\n123\nTrusting deep learning natural-language models via local… 1907\nSalvatore Greco received the BS degree in computer engineering from\nthe University of Modena and Reggio Emilia, Italy, in 2016, and the\nMS degree in computer engineering from the Polytechnic of Turin,\nItaly, in 2019. He is currently a PhD student since November 2020\nat the Polytechnic of Turin. His current research interests include\neXplainable Artiﬁcial Intelligence and Natural Language Processing.\nDaniele Apiletti has been an Assistant Professor at the Department\nof Control and Computer Engineering of Politecnico di Torino, Italy,\nsince 2019. He received his master’s and PhD degrees in Computer\nEngineering from Politecnico di Torino, Italy, in 2005 and 2008,\nrespectively. His research interests are in the ﬁeld of NoSQL databases,\nlarge-scale data mining techniques, and big data machine learning, with\na speciﬁc focus on network trafﬁc, sensor data, and industrial applica-\ntions.\nTania Cerquitelli has been an Associate Professor at the Department\nof Control and Computer Engineering of Politecnico di Torino, Italy,\nsince March 2018. Her research activities are mainly devoted to fos-\ntering and sharing research and innovation in the areas of automated\ndata science, explainable artiﬁcial intelligence solutions, and machine\nlearning for engineering applications. Tania has been involved in many\nEuropean and Italian research projects addressing research issues on\nmachine learning for Industry 4.0 and societal challenges. She received\nthe Master’s degree with honours in Computer Engineering (2003) and\nthe PhD (2007) from Politecnico di Torino, Italy, and the Master’s\ndegree with honours in Computer Science (2003) from Universidad De\nLas Américas Puebla, Mexico.\n123"
}