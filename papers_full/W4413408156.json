{
  "title": "From large language models to multimodal AI: a scoping review on the potential of generative AI in medicine",
  "url": "https://openalex.org/W4413408156",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5116271420",
      "name": "Lukas Buess",
      "affiliations": [
        null,
        "Friedrich-Alexander-Universität Erlangen-Nürnberg"
      ]
    },
    {
      "id": "https://openalex.org/A5088605747",
      "name": "Matthias Keicher",
      "affiliations": [
        null,
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5046896448",
      "name": "Nassir Navab",
      "affiliations": [
        null,
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5101619735",
      "name": "Andreas Maier",
      "affiliations": [
        null,
        "Friedrich-Alexander-Universität Erlangen-Nürnberg"
      ]
    },
    {
      "id": "https://openalex.org/A5076251937",
      "name": "Soroosh Tayebi Arasteh",
      "affiliations": [
        null,
        "Friedrich-Alexander-Universität Erlangen-Nürnberg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4295951577",
    "https://openalex.org/W4392016947",
    "https://openalex.org/W4205403018",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W6810819334",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W4388890632",
    "https://openalex.org/W4404263922",
    "https://openalex.org/W4387296225",
    "https://openalex.org/W3111061871",
    "https://openalex.org/W4401397240",
    "https://openalex.org/W4393248026",
    "https://openalex.org/W4403062749",
    "https://openalex.org/W4404356490",
    "https://openalex.org/W4406302098",
    "https://openalex.org/W4308885870",
    "https://openalex.org/W4404294187",
    "https://openalex.org/W2891378911",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3199422761",
    "https://openalex.org/W4391940656",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4389520259",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4391098193",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3012137864",
    "https://openalex.org/W4395050972",
    "https://openalex.org/W3154504677",
    "https://openalex.org/W4392618992",
    "https://openalex.org/W3215317537",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W4402923149",
    "https://openalex.org/W3180779372",
    "https://openalex.org/W4400577313",
    "https://openalex.org/W4391174596",
    "https://openalex.org/W4220757565",
    "https://openalex.org/W4390616142",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4366823175",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4400864148",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4395049441",
    "https://openalex.org/W4323920618",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W2891400669",
    "https://openalex.org/W3101223450",
    "https://openalex.org/W2154139219",
    "https://openalex.org/W4242729757",
    "https://openalex.org/W2329674354",
    "https://openalex.org/W6869240675",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4400136387",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4405669265",
    "https://openalex.org/W4312533035",
    "https://openalex.org/W4386075916",
    "https://openalex.org/W3091546937",
    "https://openalex.org/W4402726790",
    "https://openalex.org/W4396758996",
    "https://openalex.org/W4392931760",
    "https://openalex.org/W4402753927",
    "https://openalex.org/W4402754067",
    "https://openalex.org/W3201906559",
    "https://openalex.org/W4385347692",
    "https://openalex.org/W4402134103",
    "https://openalex.org/W4399952271",
    "https://openalex.org/W4403345210",
    "https://openalex.org/W4389267014",
    "https://openalex.org/W3164654615",
    "https://openalex.org/W4389888290",
    "https://openalex.org/W4400127918",
    "https://openalex.org/W4385948838",
    "https://openalex.org/W4398201291",
    "https://openalex.org/W4405434097",
    "https://openalex.org/W4387225987",
    "https://openalex.org/W4387655217",
    "https://openalex.org/W4360866750",
    "https://openalex.org/W4400362569",
    "https://openalex.org/W4386065580",
    "https://openalex.org/W4382998948",
    "https://openalex.org/W4403990404",
    "https://openalex.org/W4401888965",
    "https://openalex.org/W4372336638",
    "https://openalex.org/W4404781870",
    "https://openalex.org/W4296177022",
    "https://openalex.org/W4402582321",
    "https://openalex.org/W4391159111",
    "https://openalex.org/W4394597485",
    "https://openalex.org/W4396605652",
    "https://openalex.org/W4322827061",
    "https://openalex.org/W4404570870",
    "https://openalex.org/W3164670515",
    "https://openalex.org/W4399567248",
    "https://openalex.org/W4394593144",
    "https://openalex.org/W4402727946",
    "https://openalex.org/W4390889814",
    "https://openalex.org/W6903583810",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W4403622706",
    "https://openalex.org/W4360600216",
    "https://openalex.org/W4402774103",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W3093937711",
    "https://openalex.org/W4404782407",
    "https://openalex.org/W4387211738",
    "https://openalex.org/W3165058054",
    "https://openalex.org/W2901125500",
    "https://openalex.org/W4283072464",
    "https://openalex.org/W4403069431",
    "https://openalex.org/W2884716214",
    "https://openalex.org/W3027572331",
    "https://openalex.org/W6849954598",
    "https://openalex.org/W4404584849",
    "https://openalex.org/W4404781831",
    "https://openalex.org/W4399986073",
    "https://openalex.org/W4293918660",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3103694015",
    "https://openalex.org/W4405995927",
    "https://openalex.org/W4396894318",
    "https://openalex.org/W4404622590",
    "https://openalex.org/W4385242971",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4407135816",
    "https://openalex.org/W4409922985",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4313312731"
  ],
  "abstract": "Abstract Generative artificial intelligence (AI) models, such as diffusion models and OpenAI’s ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 145 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.",
  "full_text": "REVIEW ARTICLE\nBiomedical Engineering Letters (2025) 15:845–863\nhttps://doi.org/10.1007/s13534-025-00497-1\nMRI  Magnetic resonance imaging\nNER  Named entity recognition\nNIfTI  Neuroimaging informatics technology \ninitiative\nPLM  Pretrained language model\nPRISMA  Preferred reporting items for systematic \nreviews and meta-analyses\nPRISMA-ScR  Preferred reporting items for systematic \nreviews and meta-analyses extension for \nscoping reviews\nQA  Question answering\nRAG  Retrieval augmented generation\nRLHF  Reinforcement learning from human \nfeedback\nRLAIF  Reinforcement learning from AI feedback\nSFT\t \tSupervised\tfinetuning\nAbbreviations\nAI\t \tArtificial\tintelligence\nAPI  Application programming interface\nCLIP  Contrastive language-image pretraining\nCoT  Chain-of-thought\nCT  Computed tomography\nDICOM  Digital imaging and communications in \nmedicine\nECG  Electrocardiogram\nEHR  Electronic health record\nLLM  Large language model\nMLLM  Multimodal large language models\n \r Lukas Buess\nlukas.buess@fau.de\n1 Pattern Recognition Lab, Friedrich-Alexander-Universität \nErlangen-Nürnberg, Erlangen, Germany\n2 Computer Aided Medical Procedures, Technical University \nof Munich, Munich, Germany\nAbstract\nGenerative\tartificial\tintelligence\t(AI)\tmodels,\tsuch\tas\tdiffusion\tmodels\tand\tOpenAI’s\tChatGPT,\tare\ttransforming\tmedi-\ncine\tby\tenhancing\tdiagnostic\taccuracy\tand\tautomating\tclinical\tworkflows.\tThe\tfield\thas\tadvanced\trapidly,\tevolving\tfrom\t\ntext-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems \ncapable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The \ndiverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of \ntheir applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, \napplications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried \nPubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous \nscreening,\t145\tpapers\twere\tincluded,\trevealing\tkey\ttrends\tand\tchallenges\tin\tthis\tdynamic\tfield.\tOur\tfindings\tunderscore\t\na shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, \ndrug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous \ndata types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical \nsettings.\tThis\treview\tsummarizes\tthe\tcurrent\tstate\tof\tthe\tart,\tidentifies\tcritical\tgaps,\tand\tprovides\tinsights\tto\tguide\tthe\t\ndevelopment of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.\nKeywords Large language models · Generative AI · Multimodal AI · Scoping review\nReceived: 31 January 2025 / Revised: 2 June 2025 / Accepted: 17 July 2025 / Published online: 22 August 2025\n© The Author(s) 2025\nFrom large language models to multimodal AI: a scoping review on \nthe potential of generative AI in medicine\nLukas Buess1  · Matthias Keicher2 · Nassir Navab2 · Andreas Maier1 · Soroosh Tayebi Arasteh1\n1 3\nBiomedical Engineering Letters (2025) 15:845–863\n1 Introduction\nGenerative\t artificial\t intelligence\t (AI),\t exemplified\t by\t\nmodels like ChatGPT, has drawn widespread attention for \nits ability to process and generate human-like text, sub -\nstantially advancing various domains. In healthcare, these \nmodels have rapidly transformed traditional approaches by \noffering\tcapabilities\tbeyond\tconventional\tdata\tanalysis\t[1, \n2].\tFor\tinstance,\tlarge\tlanguage\tmodels\t(LLMs)\thave\tbeen\t\napplied\tin\ttasks\tsuch\tas\tsummarizing\tmedical\trecords\t[3], \nassisting\tin\tdiagnostic\treasoning\t[4], and conducting bio -\ninformatics\tresearch\t[5]. These advancements highlight the \nability of LLMs to process and interpret complex clinical \nlanguage,\timproving\tefficiency\tand\taccuracy\tacross\ttasks\t\nsuch as radiology reporting. Recent studies further demon -\nstrate their impact, showing that AI-generated draft radiol -\nogy reports can reduce reporting time by about 25% while \nmaintaining\tdiagnostic\taccuracy\t[6], thus addressing work-\nload\tchallenges\tin\tclinical\tpractice\t[7].\nHowever, healthcare data extends far beyond clinical \ntexts, encompassing diverse modalities such as medical \nimages\t[8, 9],\tlaboratory\tresults\t[10, 11], and genomic data \n[12]. To address this diversity, multimodal AI systems have \nemerged, integrating these data types within a single model \nto support more comprehensive and clinically relevant \ndecision-making.\tRecent\tadvancements\tin\tthis\tfield\tmark\t\na shift beyond language-focused tasks toward complex, \nmultimodal\tdata\tintegration\t[13–15]. These systems hold \npotential for improved diagnostic accuracy and broader \napplications, from predictive analytics to complex inter -\nventional\tsupport\t[16]. Figure 1 illustrates how such mod -\nels transform heterogeneous medical inputs into clinically \nmeaningful insights through an iterative pipeline.\nSeveral recent review articles have provided valuable \noverviews of multimodal AI and LLMs. Comprehensive \nsurveys\tof\tmultimodal\tlarge\tlanguage\tmodels\t(MLLMs)\tin\t\nthe broader computer vision domain were presented by Yin \net\tal.\t[17]\tand\tWang\tet\tal.\t[18], highlighting recent advance-\nments, providing a summary of architectural developments, \nand identifying key trends in model evolution. A broader \nperspective on multimodal approaches in healthcare was \nprovided\tby\tKline\tet\tal.\t[19]\tand\tAcosta\tet\tal.\t[1]. He et al. \n[20] present a comprehensive collection of foundation mod-\nels, spanning from image-only architectures to advanced \nmultimodal models.\nWhile previous reviews provide essential insights, the \ndynamic\tand\trapidly\tevolving\tnature\tof\tthis\tfield\tnecessi-\ntates an up-to-date and focused exploration of recent devel-\nopments in LLM-based multimodal AI for medicine. This \nreview\taims\tto\tfill\tthis\tgap\tby\tproviding\ta\tcomprehensive\t\noverview of the evolution from text-only LLMs to multi -\nmodal AI systems in medicine, with a particular emphasis \non recent advancements. Unlike prior reviews, we also dis -\ncuss\tevaluation\tmethods\tspecifically\ttailored\tto\tthe\tchal-\nlenges and requirements of medical generative AI, ensuring \nreal-world clinical utility and reliability.\nTo guide this review, we formulated the following \nresearch questions:\n ● What methods are commonly used in the development \nof generative AI for healthcare applications?\n ● What datasets support the development of generative AI \nin medical contexts?\n ● Which evaluation metrics are employed to assess the \nutility of generative AI models in medical contexts?\nIn\tthe\tfollowing\tsections,\twe\tfirst\toutline\tthe\tmethodology\t\nemployed for literature collection and selection, detailing \nthe search strategy, inclusion criteria, and data extraction \nprocesses used to ensure a comprehensive review. We then \nFig. 1 Multimodal AI pipeline in \nhealthcare: A Diverse medical data \nmodalities\t(e.g.,\timages,\tgenomics,\t\nand\tclinical\tnotes)\tare\tcollected\t\nand processed, B transformed \ninto\tunified\trepresentations\tby\tAI\t\nmodels, C used to generate insights \nsuch as reports, conversational \nassistance, and treatment plans, \nand D\trefined\tthrough\titerative\t\nfeedback to continuously optimize \ndata collection and AI performance\n \n1 3\n846\nBiomedical Engineering Letters (2025) 15:845–863\npresent\tour\tfindings,\temphasizing\tthe\tshift\tfrom\ttext-only\t\nLLMs to multimodal AI systems in medicine, with a par -\nticular focus on their applications, datasets, model architec-\ntures,\tand\tevaluation\tmetrics.\tOur\tresults\treveal\ta\tsignificant\t\nshift towards multimodal models, which are driving innova-\ntion across various areas of healthcare. However, persistent \nchallenges remain, particularly in the evaluation of these \nmodels, including the assessment of their reliability, clini -\ncal relevance, and generalizability. Finally, we provide an \noutlook\ton\tthe\tfuture\tof\tgenerative\tAI\tin\tmedicine,\toffering\t\ninsights to guide further research and development in this \nrapidly\tevolving\tfield.\n2 Methods\nOur scoping review followed the Preferred Reporting Items \nfor Systematic Reviews and Meta-Analyses extension for \nScoping\tReviews\t(PRISMA-ScR)\t[21, 22], which provides \na standardized framework for methodological transparency \nin scoping reviews. This section details the data collection \nmethods used in our review. The complete PRISMA-ScR \nchecklist is available in Supplementary Table S.1.\n2.1 Eligibility criteria\nWe included studies published between January 2020 and \nDecember 2024 to capture recent advancements in the rap -\nidly\tadvancing\tfield\tof\tgenerative\tAI\tin\tmedicine.\tOnly\t\noriginal research in English was eligible, as our focus is \non\tprimary\tcontributions\trather\tthan\tsynthesized\tfindings.\t\nReview and meta-analysis papers were therefore excluded. \nWe included peer-reviewed conference and journal publi -\ncations, alongside manually selected preprints with high \nrelevance and potential impact. To ensure a comprehensive \noverview, foundational dataset papers published before \n2020 were also included when they were widely used in \nthe selected studies or remained relevant for benchmarking. \nThis approach ensured a focus on current, state-of-the-art \ndevelopments in multimodal AI applications in medicine.\n2.2 Information sources\nWe performed a systematic search in PubMed, IEEE \nXplore, and Web of Science, employing a standardized \nset of keywords derived from our research objectives. Full \nsearch queries are detailed in Supplementary Table S.2. The \nsearches, conducted on October 1, 2024, were imported into \nRayyan\t[23], a web-based tool designed to facilitate litera -\nture screening and semi-automated duplicate removal.\n2.3 Search strategy\nThe literature search consisted of a systematic database \nsearch structured into two subsearches to capture the devel-\nopment and application of text-only LLMs and multimodal \nmodels\tin\tmedicine.\tThe\tfirst\tsubsearch\ttargeted\ttext-only\t\nLLMs using the keyword groups \"medical\" and \"language \nmodel\". The second subsearch focused on multimodal \nmodels, using three groups of keywords: \"multimodal\", \n\"medical\" and \"language model\". The full search queries, \nincluding\tthe\tspecific\tcombinations\tused,\tare\tprovided\tin\t\nSupplementary Table S.2. Additionally, a manual search \nwas performed to identify recent preprints, datasets, and \nother resources not captured by the database search, which \ncontinued through the end of 2024 to ensure the inclusion of \nthe most current and impactful studies.\n2.4 Inclusion and exclusion criteria\nThe selection process began with structured database \nqueries, followed by duplicate removal, title and abstract \nscreening, and subsequent full-text reviews for potentially \nrelevant papers. We excluded articles that were non-medical \nor lacked methodological novelty. To ensure balanced rep -\nresentation across application areas, we aimed for propor -\ntional\tinclusion\tfrom\tprevalent\tfields,\tsuch\tas\tX-ray\treport\t\ngeneration.\n2.5 Synthesis of results\nThe selected papers were categorized through a two-step \nprocess. First, they were grouped by topics, including text-\nonly LLMs, multimodal models, datasets, and evaluation \nmetrics. Within each topic, papers were further categorized \nbased on their application areas. This dual-layer categoriza-\ntion provides a structured overview of developments in gen-\nerative AI for medicine, illustrating the progression from \ntext-only LLMs to multimodal models. Key publications \nare summarized through narrative descriptions and tables, \noffering\tinsights\tinto\tmethodological\tapproaches,\tapplica-\ntion domains, datasets, and evaluation frameworks to pro -\nvide a comprehensive understanding of current trends and \nchallenges. Table 1\t(text-only\tLLMs),\tTable\t2\t(text-only\t\ndatasets),\tTable\t3\t(contrastive\tlearning\tmethods),\tTable\t4 \n(MLLMs),\tTable\t5\t(multimodal\tdatasets),\tand\tTable\t6\t(eval-\nuation\tmetrics)\tsummarize\tthe\tresults.\n1 3\n847\nBiomedical Engineering Letters (2025) 15:845–863\nstep led to the exclusion of an additional 249 papers. Ulti -\nmately, 60 papers from the database search were included in \nthe\treview.\tAdditionally,\t84\tpapers\twere\tidentified\tthrough\t\nmanual searches to capture the most current and relevant \nstudies not covered in the database queries. Figure 2 pro-\nvides an overview of the full screening process. In total, 145 \npapers were included in this review.\n4 Language models in medicine\nMono-modal LLMs, which process textual data exclusively, \nhave laid the foundation for the development of multimodal \nsystems, demonstrating remarkable capabilities in under -\nstanding and generating human-like text. In the medical \ndomain,\tLLMs\tdemonstrated\thigh\teffectiveness\tin\tprocess-\ning and analyzing complex clinical data, enabling advance-\nments in applications such as clinical documentation, \nmedical literature summarization, and diagnostic support \n[3, 24]. Their success is rooted in the transformer architec -\nture\t[25], which uses self-attention mechanisms to capture \ncontextual relationships and long-range dependencies in \ntext. Language models use self-supervised learning objec -\ntives such as masked language modeling or causal language \nmodeling, which allow them to learn from vast unlabeled \ndatasets. This enables scalable pretraining and contributes \nto\ttheir\teffectiveness\twhen\tadapted\tto\tmedical\tapplications.\n3 Included studies\nA total of 4,384 papers were retrieved from three databases. \nAfter removing duplicates, 2,656 articles were excluded \nduring the initial screening based on their titles and \nabstracts,\tfollowing\tthe\tpredefined\tinclusion\tand\texclusion\t\ncriteria. The remaining articles underwent a full-text review, \nduring which both relevance and topic diversity were con -\nsidered to avoid overrepresentation of similar studies. This \nTable 1 Summary of language model methods categorized by applica-\ntion to clinical text and bioinformatics\nStudy Downstream task\nClinical text\nAlmanac\t[33] QA\nBioALBERT\t[27] NER\nBioBERT\t[26] NER, QA\nBioGPT\t[34] Classification,\tQA\nBioMistral\t[28] QA\nChatDoctor\t[4] Dialogue\nChestXRayBERT\t[3] Summarization\nDRG-LLaMA\t[35] Classification\nGatorTron\t[36] QA\nHuatuoGPT\t[32] Dialogue\nHuatuoGPT-o1\t[32] Dialogue\nJohnson\tet\tal.\t[37] Deidentification\nKresevic\tet\tal.\t[38] Summarization\nMahendran\tand\tMcInnes\t[39] NER\nMAPLEZ\t[40] Classification\nMed-BERT\t[41] NER\nMedAlpaca\t[42] QA\nMEDITRON-70B\t[43] QA\nMED-PaLM\t[29] QA\nMMed-Llama\t3\t[44] QA\nMu\tet\tal.\t[45] Classification\nNYUTron\t[24] Clinical outcome prediction\nPMC-LLaMA\t[46] QA\nPodGPT\t[47] QA\nRadBERT\t[48] Classification,\tSummarization\nSchmidt\tet\tal.\t[49] Error detection\nBioinformatics\nAlphaFold\t[5] Structure prediction\nBioPhi\t[50] Antibody design\nCADD\tv1.7\t[51] Scoring\nDNABERT\t[52] Structure analysis\nGeneformer\t[53] Classification\nHie\tet\tal.\t[54] Antibody design\nMSA\tTransformer\t[55] Structure analysis\nProGen\t[56] Structure prediction\nProtGPT2\t[57] Protein design\nProtTrans\t[58] Structure analysis\nscBERT\t[59] Classification\nToxinPred\t3.0\t[60] Classification\nThe table includes method names and target applications\nNER, named entity recognition; QA, question answering\nTable 2 Summary of datasets used for training medical LLMs, catego-\nrized into clinical text and bioinformatics data\nDataset Size Application\nClinical text\neICU-CRD\t[67] 200K instances EHR\nGAP-Replay\t[43] 48.1B tokens Literature\nMedDialog-EN\t[68] 250K dialogues Dialogue\nMedC-K\t[46] 4.8M papers, 30K \ntextbooks\nLiterature\nMedC-I\t[46] 202M tokens Dialogue, QA\nMedical\tMeadow\t[42] 160K instances QA\nMIMIC-IV\t[66] 299K patients EHR\nMMedC\t[44] 25.5B tokens Multilingual \nliterature\nMultiMedQA\t[29] 213K instances QA\nBioinformatics\nAlphaFold\tDB\t[5] 200M entries Protein Design\nCPTAC\tData\tPortal\t[71] NA Genomics, \nProtein Design\nGenBank\t[70] NA sequences Genomics\nGENCODE\t[12] NA Genomics\nUniProtKB\t[69] 227M sequences Protein Design\nThe table includes dataset names, sizes, and primary application \nareas\nNA, not available; NER, named entity recognition; QA, question \nanswering; EHR, electronic health record\n1 3\n848\nBiomedical Engineering Letters (2025) 15:845–863\nmodeling.\tThey\tare\ttypically\tfinetuned\tfor\tspecific\ttasks\t\nlike\tnamed\tentity\trecognition\t(NER)\tor\tquestion\tanswering\t\n(QA).\tThese\tmodels\tare\toptimized\tfor\tunderstanding\tand\t\nclassification\trather\tthan\topen-ended\tgeneration,\tand\tthus\t\ndo\tnot\tfall\tunder\tthe\tcurrent\tdefinition\tof\tmodern\tLLMs.\nIn contrast, modern LLMs are generally generative and \nadapted for medical use through instruction-tuned super -\nvised\tfinetuning\t(SFT),\tenabling\tcapabilities\tlike\tzero-shot\t\nor few-shot generalization across diverse tasks. Prominent \nexamples\t include\t BioMistral\t [28],\t ChatDoctor\t [4], and \nMed-PaLM\t[29].\nIn contrast to SFT, prompt engineering techniques have \nemerged as a lightweight alternative for guiding pretrained \nmodels without additional training, relying on carefully \ndesigned input prompts to achieve strong task performance \nin\tmedical\ttext\tunderstanding\tand\tgeneration\t[30].\nAdvanced alignment techniques such as reinforce -\nment\tlearning\tfrom\thuman\tfeedback\t(RLHF)\thave\tbeen\t\ndeveloped\tto\tfurther\trefine\tthe\toutputs\tof\tLLMs\tfor\tmedi-\ncal applications. RLHF leverages reward models trained \non expert feedback to align model responses with clinical \nexpectations. However, due to the cost of obtaining expert \nfeedback in the healthcare domain, reinforcement learning \nfrom\tAI\tfeedback\t(RLAIF)\thas\temerged\tas\tan\talternative\t\n[31]. This technique replaces human feedback with evalua -\ntions from auxiliary AI models, reducing reliance on scarce \nhuman resources while maintaining alignment capabilities. \nA\tnotable\texample\tis\tHuatuoGPT\t[32], which uses RLAIF \nfor clinical alignment.\nAnother recent development in model adaptation is \nchain-of-thought\t(CoT)\tprompting,\ta\ttechnique\twhere\tmod-\nels generate intermediate reasoning steps before producing \na\tfinal\tanswer.\tBy\tbreaking\tdown\tcomplex\ttasks\tinto\tsub-\nsteps, CoT enhances model explainability and task perfor -\nmance, which is especially valuable in the medical domain \nas it not only improves accuracy but also increases trust in \nthe\tmodel’s\treasoning\tprocess.\tFor\texample,\tHuatuoGPT-\no1\t[61] applies CoT prompting to improve medical response \nclarity and ensure step-by-step diagnostic reasoning.\nAn additional adaptation technique is retrieval augmented \ngeneration\t(RAG)\t[62], which equips LLMs with mecha -\nnisms to query external knowledge bases during inference. \nThis approach enables models to access up-to-date informa-\ntion,\tsuch\tas\tmedical\tguidelines\tor\trecent\tresearch\tfindings,\t\nwithout\trequiring\tretraining.\tFor\tinstance,\tAlmanac\t[33], \nChatDoctor\t[4],\tand\tRadioRAG\t[63] combine generative \ncapabilities with retrieval systems. However, maintaining \nthe retrieval database and ensuring its comprehensiveness \npose\tongoing\tchallenges\t[4, 64].\n4.1 Language model methods\nLanguage\tmodels\tfor\tmedical\tapplications\t(Table\t1)\tdiffer\t\nin their architectures and adaptation strategies. Early pre -\ntrained\tlanguage\tmodels\t(PLMs)\tsuch\tas\tBioBERT\t[26] and \nBioALBERT\t[27] are based on the BERT architecture and \npretrained on biomedical corpora using masked language \nTable 3 Summary of multimodal CLIP-based methods\nStudy Modalities Application\nBiomedCLIP \n[82]\nMedical images, \nDescriptions\nClassification,\tRetrieval,\t\nVisual QA\nBioViL\t[83] X-ray, Reports Classification,\tGrounding\nBioViL-T\t[84] X-ray, Reports Classification,\tGround-\ning, Reporting\nCheXzero\t[74] X-ray, Reports Classification,\tRetrieval\nConVIRT\t[85] X-ray, Reports Classification,\tRetrieval\nCPLIP\t[86] Histopathology \nimages, Descriptions\nClassification\nCT-CLIP\t[14] CT, Reports, Labels Classification,\tRetrieval\nCT Foundation \n[87]\nCT, Reports Classification,\tRetrieval\nCXR-RePaiR \n[75]\nX-ray, Reports Reporting\nETP\t[88] ECG, Reports Classification\nFairCLIP\t[89] SLO fundus images, \nClinical notes\nClassification\nFiVE\t[90] Histopathology \nimages, Descriptions\nClassification\nFlexR\t[91] X-ray, Reports Classification\nGLoRIA\t[92] X-ray, Reports Classification,\tRetrieval,\t\nSegmentation\nKAD\t[93] X-ray, Reports Classification\nMaCo\t[94] X-ray, Reports Classification\nMCPL\t[95] X-ray, Reports Classification\nMedImageIn-\nsight\t[96]\nMedical images, \nDescriptions\nClassification,\tRetrieval,\t\nReporting\nMed-MLLM \n[97]\nCT, X-ray, \nDescriptions\nClassification,\tReporting\nMerlin\t[77] CT, EHR, Reports Classification,\tRetrieval,\t\nReporting, Segmentation\nMedViLL\t[98] X-ray, Reports Classification,\tRetrieval,\t\nReporting, Visual QA\nMoleculeSTM \n[99]\nMolecule structure, \nDescriptions\nRetrieval\nMolLM\t[100] Molecule structures, \nDescriptions\nRetrieval, Molecule \ndescription\nPLIP\t[101] Histopathology \nimages, Descriptions\nClassification,\tRetrieval\nProv-GigaPath \n[102]\nHistopathology \nimages, Reports\nClassification\nUniMed-CLIP \n[103]\nMedical images, \nCaptions\nClassification\nXplainer\t[104] X-ray, Reports Classification\nThe\ttable\tincludes\tmethod\tnames,\tthe\tmodalities\tutilized\t(e.g.,\ttext\t\nand\t medical\t images),\t and\t the\t primary\t tasks\t addressed,\t such\t as\t\nimage-text\tretrieval,\treport\tgeneration,\tand\tdisease\tclassification\nQA, question answering\n1 3\n849\nBiomedical Engineering Letters (2025) 15:845–863\nconsultations, and assist in providing medical information \nand guidance. These systems aim to reduce barriers to medi-\ncal access by providing instant responses.\nIn summarization tasks, medical LLMs condense lengthy \nelectronic\thealth\trecords\t(EHRs)\tinto\tconcise\tsummaries.\t\nThis\tapplication\tsignificantly\treduces\tthe\tdocumentation\t\nburden on healthcare providers and aids decision-making by \npresenting critical patient information in a structured format \n[3, 65].\nDeidentification\tand\tprivacy-preserving\tapplications\tare\t\ncritical areas where LLMs contribute to medical data man -\nagement\tby\tsafeguarding\tpatient\tconfidentiality\tin\tsensitive\t\n4.2 Language model applications\nLLMs have revolutionized various applications in bio -\nmedical language processing, demonstrating utility across \na range of tasks. In named entity recognition, they enable \nthe extraction of critical medical entities, such as diseases, \ndrugs, and symptoms from unstructured text. This capabil -\nity supports clinical data annotation, which is crucial for \nautomated\tclinical\tdecision\tsupport\tsystems\t[27].\nDialogue systems represent another application of LLMs \nin\tmedicine.\tModels\tlike\tChatDoctor\t[4] and HuatuoGPT \n[32] facilitate patient interactions, simulate doctor-patient \nStudy Modalities Downstream task\nAlsharid\tet\tal.\t[115] US video, Transcriptions, Gaze data Captioning\nAutoRG-Brain\t[108] MRI, Reports, Masks Reporting, Grounding\nBiomedGPT\t[13] Medical images, Literature, EHR Reporting, Summarization, Visual \nQA\nBioMed-VITAL\t[116] Medical images, Instructions Visual QA\nChatCAD\t[117] X-ray, Reports Reporting\nCheXagent\t[118] X-ray, Reports Classification,\tReporting,\t\nGrounding\nCOMG\t[119] X-ray, Reports, Masks Reporting\nCT-CHAT\t[14] CT, Reports Reporting, Visual QA\nFFA-GPT\t[120] Fundus\tfluorescein\tangiography,\t\nReports\nReporting, Visual QA\nGenerateCT\t[111] CT, Reports Image generation\nHuh\tet\tal.\t[121] X-ray, Reports Reporting\nLLaVA-Med\t[15] Medical images, Captions Visual QA\nLViT\t[110] CT, X-ray, Masks, Text annotations Segmentation\nM3D-LaMed\t[122] CT, Reports, Masks Reporting, Visual QA, Segmentation\nMAIRA-2\t[72] X-ray, Reports, Masks Reporting, Grounding\nMAIRA-Seg\t[123] X-ray, Reports, Masks Reporting\nMed-Flamingo\t[124] Medical images, Captions Visual QA\nMed-MoE\t[114] Medical images, Captions Visual QA\nMed-PaLM\tM\t[79] Medical images, Reports, Genomics Classification,\tReporting,\tVisual\t\nQA, Summarization\nMedVersa\t[80] CT, X-ray, Dermatology images, \nReports\nClassification,\tReporting,\tVisual\t\nQA, Segmentation\nMMBERT\t[125] Radiology images, Captions Visual QA\nMVG\t[126] Medical images, Text Disease simulation\nORacle\t[16] Multi-view images, SSG, Descriptions OR scene graph prediction\nPathChat\t[127] Histopathology images, QA-pairs Visual QA\nPathLDM\t[128] Histopathology images, Reports Image generation\nQUILT-LLaVA\t[129] Histopathology images, QA-pairs Visual QA\nR2GenGPT\t[130] X-ray, Reports Reporting\nRaDialog\t[73] X-ray, Reports Reporting, Dialogue\nRadFM\t[81] Medical images, Reports, Descriptions Reporting, Visual QA\nReXplain\t[131] Video, Reports, Masks Video report generation\nRGRG\t[109] X-ray, Reports, Bounding-boxes Reporting\nRoentGen\t[112] X-ray, Reports Image generation\nSkinGPT-4\t[107] Dermatology images, Clinical notes Visual QA, Dialogue\nSurgical-VQLA++\t[132] Surgical images, QA-pairs Visual QA\nUniversal\tModel\t[133] CT, Masks, Descriptions Segmentation\nVote-MI\t[134] MRI, Reports Visual QA\nTable 4 Summary of multimodal \nMLLM-based methods\nThe table includes method \nnames, the modalities utilized \n(e.g.,\ttext\tand\tmedical\timages),\t\nand the primary tasks addressed, \nsuch as report generation, visual \nQA,\tand\tdisease\tclassification\nQA, question answering\n \n1 3\n850\nBiomedical Engineering Letters (2025) 15:845–863\nbased on clinical text, highlighting their ability to extract \nstructured\tinsights\tfrom\tunstructured\tdata\t[24].\nIn bioinformatics, LLMs have expanded beyond lan -\nguage processing to analyze biological sequences like \nDNA, RNA, and proteins. Models such as DNABERT \n[52]\thave\tadvanced\tgene\tannotation,\twhile\tAlphaFold\t[5] \nhas achieved groundbreaking success in protein structure \nprediction.\nclinical texts. LLMs can automate the removal of protected \nhealth information from medical documents by anonymiz -\ning\tidentifiers\tsuch\tas\tnames\tand\tdates\twhile\tpreserving\t\ndata\tutility\t[37, 40].\nText\tclassification\tis\tanother\timportant\tapplication\tarea\t\nfor LLMs in medicine. These models have been used to cat-\negorize medical literature and to predict patient outcomes \nDataset Modalities Size Application\n2D-image-text\nCheXpert\t[135] X-ray, Reports, Labels 224K triplets Chest X-ray\nCheXinstruct\t[118] X-ray, Instructions 8.5M instruction triplets Chest X-ray\nHarvard-FairVLMed \n[89]\nSLO fundus images, Demo-\ngraphics, Notes\n10K samples Ophthalmology\nMedTrinity-25\tM\t[136] Medical images, Captions, \nBounding-boxes\n25M pairs Medical imaging\nMedVidQA\t[137] Medical videos, Labels, \nQA-pairs\n6K videos, 6K labels, 3K QA Medical videos\nMIMIC-CXR\t[8] X-ray, Reports 377K images, 227K reports Chest X-ray\nMS-CXR\t[83] X-ray, Descriptions, \nBounding-boxes\n1K image-sentence pairs, \nBounding-boxes\nChest X-ray\nOmniMedVQA\t[138] Medical images, QA 118K images, 127K \nQA-pairs\nMedical imaging\nOpenPath\t[101] Histopathology images, \nCaptions\n208K pairs Digital \npathology\nPadChest\t[139] X-ray, Reports 160K images, 109K texts Chest X-ray\nPathVQA\t[140] Medical images, QA 5K images, 33K QA Medical imaging\nPMC-15\tM\t[82] Medical images, Captions 15M image-text pairs Medical imaging\nPubMedVision\t[141] Medical images, QA 1.3M QA pairs Medical imaging\nQuilt-1\tM\t[142] Histopathology images, \nCaptions\n1M pairs Digital \npathology\nRad-ReStruct\t[143] X-ray, Structured reports 3720 images, 3597 Reports Chest X-ray\nSLAKE\t[144] Medical images, QA 642 images, 14K QA pairs Medical imaging\nUniMed\t[103] Medical images, Captions 5.3M image-text pairs Medical imaging\nVQA-RAD\t[145] Radiology images, Captions 315 images, 3.5K QA pairs Radiology\n3D-volume-text\nAMOS-MM\t[146, 147] CT, Reports, QA 2K image-report pairs, 7K \nQA\nChest, abdomen, \npelvis CT\nBrainMD\t[134] MRI, Reports, EHR 2.5K cases Brain MRI\nBIMCV-R\t[148] CT, Reports 8K image-report pairs CT\nCT-RATE\t[14] CT, Reports, Labels 25K triplets Chest CT\nINSPECT\t[9] CT, Reports, EHR, labels 23K image-report pairs, \nEHRs\nChest CT\nM3D-Data\t[122] CT, Captions, QA, Masks 120K images, 42K captions, \n509K QA, 149K masks\nCT\nRadGenome-Brain MRI \n[108]\nMRI, Reports, Masks 3.4K image-region-report \ntriplets\nBrain MRI\nRadGenome-Chest CT \n[149]\nCT, Reports, Masks, QA 25K image-report pairs, \n665K masks, 1.3M QA\nChest CT\nOthers\nDuke Breast Cancer \nMRI\t[150]\nGenomic, MRI images, \nClinical data\n922 subjects Breast cancer\nPTB-XL\t[151] ECG signals, Reports, Labels 21K triplets ECG\nPubChemSTM\t[99] Chemical structures, \nDescriptions\n280K chemical structure–\ntext pairs\nDrug design\nSwissProtCLAP\t[152] Protein Sequence, Text 441K sequence-text pairs Protein design\nTable 5 Summary of multimodal \ndatasets used for medical AI, \ngrouped by modality categories\nThe table lists dataset names, \nthe\ttypes\tof\tmodalities\t(e.g.,\ttext\t\nand\tmedical\timages),\tdataset\t\nsizes, and key applications such \nas image-text retrieval, report \ngeneration, and disease clas -\nsification\nQA, question answering\n \n1 3\n851\nBiomedical Engineering Letters (2025) 15:845–863\nautomating documentation and decision-making processes \nin\thealthcare.\tThe\teICU-CRD\tdataset\t[67], another EHR \nresource, focuses on intensive care unit patient data, further \nbroadening the scope of potential applications.\nTo\tintroduce\t domain-specific\t knowledge\t into\t LLMs,\t\ndatasets\tlike\tGAP-Replay\t[43]\tand\tMedC-K\t[46], com -\nposed of biomedical literature and textbooks, are essential. \nThese datasets are designed to equip models with the spe -\ncialized terminology and reasoning patterns found in bio -\nmedical research and education.\nFor conversational AI in medicine, dialogue datasets \nare\tcrucial.\tMedDialog\t[68] provides examples of doctor-\npatient interactions, enabling LLMs to learn medical dia -\nlogues, including patient concerns, physician responses, and \ndiagnostic reasoning. These datasets are essential for devel-\noping medical conversational assistance systems capable of \nsimulating clinical dialogues and supporting in patient edu-\ncation, diagnostic reasoning, and post-treatment follow-ups.\nBioinformatics datasets extend the scope of LLM appli -\ncations beyond clinical text, supporting tasks in genomics \nand\tmolecular\tbiology.\tResources\tlike\tAlphaFold\tDB\t[5] \nand\tUniProtKB\t[69] provide structured data for protein \nstructure and sequence analysis, making them valuable for \ndrug discovery and molecular research. Similarly, genomic \ndatasets\tsuch\tas\tGENCODE\t[12]\tand\tGenBank\t[70]\toffer\t\ndata for tasks like gene prediction, helping models to better \nunderstand complex biological patterns.\n4.3 LLM datasets\nThe development of medical LLMs relies on diverse and \nspecialized datasets that capture the complexity of medical \nlanguage, context, and tasks. These datasets fall into cat -\negories\tsuch\tas\tclinical\ttext,\tdomain-specific\tliterature,\tcon-\nversational data, and bioinformatics resources, each serving \ndistinct purposes in the development of medical LLMs. \nThese datasets enable general-purpose LLMs to align with \nthe medical domain, which is critical for achieving reliable \nand accurate outputs in clinical settings.\nClinical text datasets play a central role in training medi-\ncal\tLLMs\t(see\tTable\t2).\tFor\tinstance,\tEHR\tdatasets\tlike\t\nMIMIC-IV\t[66] provide a rich source of structured and \nunstructured clinical data, commonly used for tasks such \nas summarization and NER, which are both essential for \nTable 6 Evaluation metrics for medical report generation\nMetric Type Application\nCheXbert\t[162] Classification Chest X-ray report \nlabeling\nCRAFT-MD\t[163] Generative Conversation \nevaluation\nFineRadScore\t[161] Generative Report evaluation\nGREEN\t[155] Generative Report evaluation\nOng\tLy\tet\tal.\t[164] Calibration Model generalization\nRadCliQ\t[157] Composite metric Report evaluation\nRadFact\t[72] Grounding Grounded report \nevaluation\nRadGraph-F1\t[157] NER similarity Report evaluation\nRaTEScore\t[156] NER similarity Report evaluation\nThis table summarizes key metrics used to evaluate generative AI \nsystems in medical report generation, categorized by type and pri -\nmary application\nNER, named entity recognition\nFig. 2\t PRISMA\tflow\tdiagram\t\nillustrating the study selection \nprocess for the scoping review. \nThe diagram shows the number of \nrecords\tidentified\tthrough\tdatabase\t\nsearches and manual searches, \nthe removal of duplicates, the \nscreening of titles and abstracts, \nthe review of full-text articles, and \nthe\tfinal\tinclusion\tof\tstudies\tin\tthe\t\nreview\n \n1 3\n852\nBiomedical Engineering Letters (2025) 15:845–863\nfor various multimodal learning tasks. By jointly training \non paired modalities data, it excels in tasks like zero-shot \nimage\tclassification\t[74, 77], where new classes can be rec-\nognized without additional training. This makes CLIP par -\nticularly useful for situations where annotated medical data \nis limited.\nOn\tthe\tother\thand,\tMLLMs,\tsuch\tas\tLLaVA\t[78], extend \nthe capabilities of LLMs by integrating non-textual data \ndirectly into their embeddings. This integration allows for \na more holistic understanding of complex datasets, combin-\ning linguistic context with multimodal features like images \nor clinical measurements. These models excel in tasks such \nas\tradiology\treport\tgeneration\t[72, 73], question answering \nabout\tmedical\timages\t[79, 80], and decision support in diag-\nnosis\t[13, 77, 81].\nBy leveraging complementary strengths, these architec -\ntures address the diverse challenges posed by multimodal \nmedical\tdata.\tCLIP\tis\teffective\tfor\taligning\tdifferent\tdata\t\nmodalities, while MLLMs excel in diagnostic reasoning, \ntogether forming a powerful combination for improving \nmultimodal AI in medicine.\n5.2 Contrastive multimodal methods\nContrastive models like CLIP and its medical variants align \npaired inputs in a shared embedding space and are widely \nused for representation learning. Table 3 summarizes recent \nCLIP-based approaches across modalities.\nFor\tinstance,\tBiomedCLIP\t[82] uses contrastive learn -\ning to align medical images with paired reports, achieving \nstate-of-the-art results in retrieval tasks. Building on this \nframework,\tCheXzero\t[74] adapts CLIP for zero-shot clas -\nsification\tof\tX-ray\timages,\twhile\tCT-CLIP\t[14] extends this \napproach\tto\tcomputed\ttomography\t(CT)\tscans.\tSimilarly,\t\n5 Multimodal language models in medicine\nBy showcasing the potential of LLMs in processing clini -\ncal text, these models have established a strong foundation \nfor integrating additional modalities, leading to the develop-\nment\tof\tmultimodal\tlanguage\tmodels\tspecifically\tdesigned\t\nfor healthcare. Multimodal models combine diverse data \ntypes, such as text and medical images, to tackle complex \nmedical\ttasks,\tincluding\treport\tgeneration\t[72, 73], image-\ntext\tretrieval\t[74, 75],\tand\tmedical\tconsultation\t[14]. By \nbuilding on advancements in LLMs, multimodal language \nmodels improve the integration and contextual understand -\ning of multimodal medical data. This section provides an \noverview of recent architectures and methods addressing \nthe unique challenges posed by multimodal medical data.\n5.1 Architectures\nBefore presenting the literature, we outline two distinct \narchitectural paradigms in multimodal AI: contrastive mod-\nels\t(e.g.,\tCLIP\t[76])\tand\tgenerative\tmultimodal\tlarge\tlan-\nguage\tmodels\t(MLLMs)\t(see\tFig.\t3).\tContrastive\tmodels\t\nlearn\tjoint\tembeddings\tof\tdifferent\tmodality\tpairs\tand\tare\t\nprimarily\tused\tfor\ttasks\tsuch\tas\tretrieval\tor\tclassification.\t\nWhile they do not support language generation or multi -\nmodal reasoning, they form the foundation for many down-\nstream applications and are often used to pretrain modality \nencoders for MLLMs. In contrast, MLLMs directly inte -\ngrate multimodal inputs into a language model to generate \nnatural language outputs and perform complex reasoning.\nCLIP\t[76]\tis\tdesigned\tto\talign\tdifferent\tmodalities,\tsuch\t\nas image and text, in a shared embedding space. Although \noriginally developed for image-text pairs, its framework can \nbe extended to other modalities, making it a versatile tool \nFig. 3 Multimodal architectures: A CLIP-based models, which align \nembeddings\tof\tdifferent\tmodalities\tin\ta\tshared\tlatent\tspace\tfor\tretrieval\t\nor\tclassification;\tand\tB MLLM-based models, which integrate mul -\ntimodal inputs directly into the language model for generative tasks \nsuch as reporting or reasoning\n \n1 3\n853\nBiomedical Engineering Letters (2025) 15:845–863\nspecialized\tmodules\tto\tprocess\tdifferent\tmodalities\twhile\ta\t\ncentral language model coordinates their outputs, enabling \ntasks\t such\t as\t classification,\t segmentation,\t retrieval,\t and\t\nvisual QA. This approach highlights the scalability and ver-\nsatility of generalist models in addressing complex multi -\nmodal challenges in medicine.\n5.4 Multimodal LLM applications\nMLLMs have been increasingly applied across diverse \nmedical tasks, showcasing their potential to transform clini-\ncal\tworkflows\tand\tdecision\tsupport\tsystems.\tThis\tsection\t\nhighlights key applications where MLLMs contribute to \nimproving healthcare.\nA key advancement in multimodal AI is generalist mod-\nels capable of handling diverse medical data types and tasks. \nModels\tsuch\tas\tBiomedGPT\t[13]\tand\tRadFM\t[81] support \na wide range of imaging modalities and anatomical regions, \nenabling comprehensive diagnostic assistance across mul -\ntiple specialties.\nRadiology report generation remains one of the most \nimportant applications of MLLMs in healthcare, providing \ndetailed textual descriptions directly from medical images. \nSystems\tsuch\tas\tMAIRA-2\t[72]\tand\tRaDialog\t[73] have \ndemonstrated their ability to generate comprehensive reports \nfrom\tX-rays,\twhile\tCT-CHAT\t[14]\tand\tAutoRG-Brain\t[108] \nextend this capability to CT and magnetic resonance imag -\ning\t(MRI)\tscans,\trespectively.\tThese\ttools\tassist\tradiolo-\ngists by automating preliminary reporting and standardizing \ndocumentation, potentially reducing reporting delays.\nVisual QA systems support clinicians in querying medi -\ncal images using natural language prompts, supporting \nreal-time decision-making and diagnostic interpretation. \nFor\tinstance,\tmodels\tlike\tLLaVA-Med\t[15] and Med-Fla -\nmingo\t[124] provide concise, contextually relevant answers \nto clinical queries, assisting radiologists and physicians in \ncomplex cases.\nSynthetic medical image generation has become increas-\ningly important for data augmentation and simulating rare \npathological\tconditions.\tModels\tlike\tGenerateCT\t[111] and \nRoentGen\t[112] generate realistic CT and X-ray images \nfrom textual prompts, enhancing dataset diversity.\nSemantic scene modeling is another emerging applica -\ntion where models create structured representations of \ncomplex environments, such as the operating room. For \nexample,\tORacle\t[16] generates semantic scene graphs to \nassist with surgical planning and intraoperative navigation \nby representing tools, anatomy, and procedural stages in a \ncomprehensive framework.\nFinally,\t systems\t like\t ReXplain\t [131] aim to bridge \ncommunication gaps between clinicians and patients. By \ntransforming radiology reports into patient-friendly video \nUniMed-CLIP\t[103] enhances this paradigm by using clas -\nsification\tdatasets\taugmented\tby\tLLM-generated\tcaptions\t\nto train a foundation model capable of handling various \nmedical image modalities.\nMore\t recent\t efforts\t have\t focused\t on\t large-scale\tpre-\ntrained models developed by industry leaders, aiming to \ngeneralize across diverse medical imaging tasks. Models \nlike\tCT\tFoundation\t[87]\tand\tMedImageInsight\t[96], acces-\nsible\tvia\tapplication\tprogramming\tinterfaces\t(APIs),\texem-\nplify\tthis\ttrend\tby\toffering\trobust\tpretrained\tembeddings\t\nthat address data scarcity in medical imaging and support \ndownstream applications.\nWhile many CLIP-based methods focus on aligning text \nwith medical images, recent approaches have extended this \nto\tother\tmodalities.\tFor\texample,\tETP\t[88] aligns electro -\ncardiogram\t(ECG)\tsignals\t[105, 106] with clinical reports, \nwhile\tMolLM\t[100] pairs chemical structures with textual \ndescriptions to support drug discovery.\n5.3 Multimodal LLM methods\nLLM-based methods, in contrast to CLIP approaches, \ndirectly integrate multimodal inputs into the language \nmodel’s\tembeddings,\t enabling\t more\t complex\t reasoning\t\nand generative tasks. These approaches rely on modality-\nspecific\tencoders\tto\tprocess\tnon-textual\tdata,\tconverting\t\nthem\tinto\tfeature\tembeddings\tcompatible\twith\tthe\tLLM’s\t\ntext-based\t representation\t space\t (Table\t4).\t For\t instance,\t\nSkinGPT-4\t[107]\tand\tRaDialog\t[73] integrate features from \ntwo-dimensional\t(2D)\timages,\twhile\tmodels\tlike\tMerlin\t\n[77]\tand\tCT-CHAT\t[14] extend this capability to volumet -\nric\tthree-dimensional\t(3D)\tCT\tdata.\tSome\tmodels,\tsuch\tas\t\nMAIRA-2\t[72]\tand\tAutoRG-Brain\t[108], further ground \ntext predictions by incorporating bounding boxes and seg -\nmentation masks, enabling interactive, region-based report \ngeneration\tfor\tenhanced\texplainability\t[109].\nCurrent advancements also focus on text-guided segmen-\ntation and synthetic medical image generation. Text-guided \nsegmentation models like LViT create segmentation masks \nfrom textual prompts, enabling tasks such as tumor detec -\ntion\tand\torgan\tidentification\t[110]. Beyond segmentation, \nsynthetic image generation has emerged as another multi -\nmodal approach for data augmentation and model training. \nMethods\tsuch\tas\tGenerateCT\t[111] for CT volumes and \nRoentGen\t[112]\tfor\tX-rays\tuse\ttext-conditioned\tdiffusion\t\nmodels\tto\tproduce\trealistic\tmedical\timages\t[113].\nGeneralist\tmodels,\tsuch\tas\tBiomedGPT\t[13] and Med -\nVersa\t [80], unify multiple modalities and tasks either \nthrough shared representations, by combining special -\nized expert models under a common orchestrator, or by \nemploying\t mixture-of-experts\t (MoE)\t architectures\t with\t\nlearnable\trouting\tmechanisms\t[114]. These models employ \n1 3\n854\nBiomedical Engineering Letters (2025) 15:845–863\ncomplete clinical information and enabling compatibility \nwith healthcare systems.\nBeyond traditional imaging and text combinations, data-\nsets have also begun exploring additional modalities for \nspecialized biomedical tasks. For example, SwissProtCLAP \n[152] integrates protein sequence data to support protein \ndesign frameworks, highlighting the potential of multi -\nmodal datasets to extend AI applications beyond diagnostic \nimaging into molecular and genomic research.\n6 Evaluation metrics for generative AI in \nmedicine\nEvaluating generative AI in medicine is essential to ensure \nmodels produce accurate, clinically relevant, and reliable \noutputs\t[155]. This section explores evaluation metrics for \nboth text generation, such as radiology report generation, \nand image generation, emphasizing the importance of clini-\ncal validity and utility. As general-purpose metrics often \nfall\tshort\tin\tcapturing\tmedical\taccuracy,\tdomain-specific\t\napproaches are required.\nAs report generation is a key application of generative \nAI in medicine, research has focused on developing robust \nevaluation strategies. While standard lexical metrics such \nas\tBLEU\t[158],\tROUGE\t[159],\tand\tMETEOR\t[160] are \ncommonly\tused,\tthey\toften\tfail\tto\treflect\tclinical\taccuracy,\t\nas high scores can be achieved despite factually incor -\nrect outputs. These metrics focus on surface-level simi -\nlarity between generated and reference texts, employing \nn-gram\tprecision\t(BLEU),\trecall\tof\toverlapping\tsegments\t\n(ROUGE),\tand\tflexible\tword-level\tmatching\tthat\taccounts\t\nfor\tstemming\tand\tsynonyms\t(METEOR).\tHowever,\tthey\t\nlack sensitivity to clinical nuances such as negation, ana -\ntomical\tdetail,\tand\tfactual\tcorrectness\t(see\tFig.\t4).\nTo address these shortcomings, several specialized met -\nrics have been proposed for evaluating medical reports \n(Table\t6).\tRaTEScore\t[156] is an entity-aware metric that \nevaluates the overlap of extracted clinical concepts, includ-\ning\tanatomical\tstructures,\tfindings,\tand\ttheir\trelationships.\t\nIt\tcaptures\tclinically\tsignificant\tvariations\tin\texpression,\t\nsuch\tas\tsynonymous\tterms\tand\tnegated\tfindings,\tproviding\t\na more informative measure of clinical relevance.\nFineRadScore\t [161] evaluates radiology reports by \nprompting powerful language models like GPT-4 to assess \nclinical aspects such as factuality, temporal consistency, and \nseverity.\tWhile\teffective,\tit\tdepends\ton\taccess\tto\tstrong\tlan-\nguage models, which may limit reproducibility and practical \nuse.\nGREEN\t [155] addresses this limitation by distilling \nthe evaluation capability of large models into a smaller, \ninstruction-tuned language model. It detects and categorizes \nsummaries, these models provide an accessible way to con-\nvey complex clinical information, further highlighting mul-\ntimodal\tAI’s\tpotential\tto\timprove\tpatient\tcare.\n5.5 Multimodal LLM datasets\nMultimodal datasets integrating images, text, and other \nclinical\tinformation\t(Table\t5)\tare\tessential\tfor\ttasks\tsuch\t\nas radiology report generation, visual QA, and cross-modal \nretrieval.\tThese\tdatasets\tnot\tonly\tenable\teffective\tmodel\t\ntraining but are also crucial for ensuring fairness and gen -\neralization in medical AI systems. A range of multimodal \ndatasets has been curated to support various medical imag -\ning and diagnostic tasks.\nA substantial proportion of multimodal datasets focus on \npairing vision and text data, as this combination is central to \ntasks where both visual context and descriptive language are \ncritical for diagnostic interpretation. Notable public datas -\nets\tlike\tMIMIC-CXR\t[8]\tand\tCheXpert\t[135] provide rich \nresources for training 2D vision-language models in radiol-\nogy. These datasets include not only radiology reports but \nalso\tdisease-specific\tlabels,\tenabling\tmore\tcomprehensive\t\nevaluations. For benchmarking report generation, ReXGra-\ndient\t[153], a private benchmark dataset of 10,000 studies \ncollected\tacross\t67\tmedical\tsites\tin\tthe\tUnited\tStates,\toffers\t\ndiverse coverage and serves as a reliable standard for radi -\nology-specific\tperformance\tevaluation.\nExpanding beyond radiology, datasets like Quilt-1 M \n[142] have introduced multimodal resources covering addi-\ntional\tdomains\tsuch\tas\tdigital\tpathology\t[127, 154].\nRecent advancements have also led to datasets tailored \nfor\t3D\timaging\tmodalities\tsuch\tas\tCT\t[9, 14, 146, 148] \nand\tMRI\t[108].\tNotably,\tRadMD\t[81] integrates both 2D \nand 3D imaging modalities, supporting a broader range of \napplications.\nIn addition to image-text pairs, a few datasets now \ninclude\t task-specific\t annotations\t to\t support\t specialized\t\napplications.\tFor\tinstance,\tRadGenome-Brain\tMRI\t[108] \nand\t RadGenome-Chest\t CT\t [149] provide segmentation \nmasks,\twhile\tdatasets\tlike\tMedTrinity-25\tM\t[136]\toffer\t\nbounding box annotations. These annotations are critical \nfor\tgrounding\ttext\tpredictions\tto\tspecific\tregions\tof\tinter-\nest, enhancing both explainability and diagnostic accuracy \nin multimodal models.\nThe data formats of multimodal datasets also vary sig -\nnificantly\tbased\ton\ttheir\tintended\tuse\tcases.\tWhile\tdatasets\t\nlike\tOpenPath\t[101] present images from publicly available \nsources in formats such as JPEG, datasets like MIMIC-\nCXR\t [8]\t and\t CT-RATE\t[14] preserve clinical formats \nsuch as Digital Imaging and Communications in Medicine \n(DICOM)\tand\tNeuroimaging\tInformatics\tTechnology\tIni-\ntiative\t(NIfTI).\tThese\tformats\tare\tessential\tfor\tmaintaining\t\n1 3\n855\nBiomedical Engineering Letters (2025) 15:845–863\nits\tgeneralization\tperformance\ton\treal\tclinical\tcases\t[111]. \nThis ensures that the generated images are not only visu -\nally realistic but also contribute to model performance \non\tdownstream\ttasks,\tsuch\tas\tdisease\tclassification\tand\t\nsegmentation.\nDespite advancements in specialized evaluation met -\nrics for both text and image generation, challenges remain \nregarding their generalizability across clinical sites and \ndatasets.\tFrameworks\tlike\tReXamine-Global\t[166] address \nthis by evaluating the robustness of metrics across diverse \ninstitutions and data distributions. For text generation, a \ncombination of lexical metrics and clinically grounded \nassessments is essential to ensure factual correctness and \nclinical relevance. Similarly, for image generation, both \nvisual quality and downstream clinical utility, such as diag-\nnostic performance on real clinical cases, should be jointly \nevaluated. Ultimately, a multi-dimensional evaluation \napproach\tthat\tconsiders\tboth\tdata\tdiversity\tand\ttask-specific\t\nrequirements\tis\tcrucial\tfor\tthe\tsafe\tand\teffective\tdeployment\t\nof generative AI in healthcare.\ninconsistencies between generated and reference reports and \nproduces human-interpretable feedback alongside a numeri-\ncal score. This makes GREEN a practical and clinically \ngrounded tool for both benchmarking and error analysis.\nRadFact\t[72] uses a LLM to evaluate sentence-level fac-\ntual consistency by comparing generated text to reference \nreports. In grounding tasks, it also incorporates image anno-\ntations to assess whether model predictions are supported \nby visual evidence.\nIn\taddition\tto\ttext-based\tevaluation,\tclinical\tefficacy\tcan\t\nalso\tbe\tassessed\tusing\tstandard\tclassification\tmetrics\tsuch\t\nas\tprecision,\trecall,\tspecificity,\tand\tF1-score.\tThis\tis\tpar-\nticularly\trelevant\twhen\tusing\tlabel-based\tdatasets\t[8, 14], \nwhere\ta\tclassifier\tis\tused\tto\tassign\tdiagnostic\tlabels\tto\tgen-\nerated\treports\t(e.g.\tCheXbert\t[162]),\tallowing\tcomparison\t\nto ground truth annotations.\nEvaluating image generation in medical AI requires \nconsiderations beyond standard image quality metrics like \nFréchet\tInception\tDistance\t[165] and mean squared error. \nSince synthetic medical images are often used for data aug-\nmentation or diagnostic training, their clinical utility must \nbe\tassessed\talongside\tvisual\tquality.\tOne\teffective\tstrat-\negy\tinvolves\tgenerating\tcondition-specific\tmedical\timages\t\nand\ttraining\ta\tclassifier\ton\tthe\tsynthetic\tdata\tto\tevaluate\t\nFig. 4 Evaluation of generative \nAI in medicine: Lexical metrics \nfrom the general domain cannot \ncompletely capture the clinical \ncorrectness as they mainly cover \ntext similarity. Clinically-relevant \nmetrics\tlike\tGREEN\t[155], \nRaTEScore\t[156], or RadGraph \n[157] also evaluate the clinical \ncorrectness\n \n1 3\n856\nBiomedical Engineering Letters (2025) 15:845–863\nmodels on non-public datasets, limiting comparative per -\nformance\t assessments\t across\t different\t models\t and\t data\t\nsources. Expanding such benchmarks and ensuring their \napplicability to a broader range of clinical tasks is essential \nfor developing trustworthy generative models in medicine.\nWhile this scoping review provides a comprehensive \noverview of generative AI advancements in medicine, it has \ncertain limitations. Despite the systematic search strategy \nusing the PRISMA-ScR framework, the literature search \nmay not have captured all relevant studies due to the rap -\nidly\tevolving\tnature\tof\tthe\tfield.\tTo\tmitigate\tthis,\ta\tmanual\t\nsearch was conducted alongside the database queries to \nensure the inclusion of recent and high-impact publica -\ntions.\tMoreover,\twhile\tefforts\twere\tmade\tto\tcover\tmultiple\t\nclinical specialties, there remains an overrepresentation of \nradiology-focused\tdatasets\tand\tmodels,\treflecting\ta\tbroader\t\ntrend in the literature. We aimed to balance the inclusion of \ntopics and application areas by diversifying the datasets and \nmodels included in our analysis, but certain domains such as \npathology and genomics remain less represented due to the \ncurrent\tavailability\tof\tmultimodal\tdatasets\tin\tthese\tfields.\nTo further advance the development and responsible \ndeployment of generative AI in medicine, several areas need \nattention\t[169–171]. First, evaluation frameworks need to \nevolve beyond lexical metrics by incorporating clinically \ngrounded\tassessments\tand\tdomain-specific\terror\tanalysis.\t\nSecond, expanding the diversity of training datasets is criti-\ncal. The current overrepresentation of western institutions \nand radiology-focused datasets risks introducing biases that \nlimit\tglobal\tapplicability\t[8, 135]. Future datasets should \nencompass a wider range of medical specialties, imaging \nmodalities, and patient demographics, with careful attention \nto privacy protection and data fairness. Third, improving \nmodel\texplainability\tremains\ta\tpriority\t[172, 173]. Tech-\nniques\tsuch\tas\tregion-specific\tgrounding\tcan\thelp\tbuild\t\nclinician trust. Finally, the emergence of generalist mod -\nels\t[13, 80] capable of handling multiple modalities and \ntasks\twithin\ta\tunified\tarchitecture\trepresents\tan\timportant\t\nstep forward, but broader coverage across medical special -\nties and improved datasets remain essential for widespread \nadoption.\nThis scoping review provides a structured analysis of \nthe evolution from unimodal LLMs to multimodal genera -\ntive AI models in medicine, highlighting their potential for \nimproving diagnostic support, clinical documentation, and \ndecision-making. However, challenges related to data diver-\nsity, clinical relevance, model interpretability, and the stan-\ndardization of evaluation metrics remain critical barriers to \nwidespread adoption. Addressing these challenges through \ninterdisciplinary collaboration, improved datasets, and \nclinically grounded evaluation strategies will be essential \n7 Discussion\nIn this scoping review, we systematically explored the evo-\nlution of generative AI in medicine, focusing on LLMs, \nmultimodal LLMs, and their evaluation metrics. Using the \nPRISMA-ScR\tframework\t[21], we collected 145 papers \npublished between January 2020 and December 2024 from \nPubMed, IEEE Xplore, and Web of Science, complemented \nby a manual search to ensure comprehensive coverage. Our \nfindings\thighlight\tthe\tshift\tfrom\tunimodal\tLLMs\tfocused\ton\t\ntextual tasks to more complex multimodal systems capable \nof integrating medical images, clinical notes, and structured \ndata. These models have shown promise in enhancing diag-\nnostic\tsupport,\tautomating\tclinical\tworkflows,\tand\treducing\t\nthe workload of healthcare professionals.\nLLMs have advanced biomedical language processing, \nimproving tasks like medical report summarization, named \nentity recognition, and conversational AI. Adaptation tech -\nniques\tsuch\tas\tsupervised\tfinetuning,\treinforcement\tlearn-\ning, and RAG have further specialized language models \nfor clinical tasks. However, reliance on static datasets like \nMIMIC-IV\t[66] limits the ability to capture evolving medi-\ncal knowledge. Moreover, privacy issues persist due to the \nneed\tfor\textensive\tdata\tdeidentification,\tand\tdataset\tbiases\t\ncan\taffect\tfairness\tby\toverrepresenting\tspecific\tpopulations\t\n[167, 168].\nMultimodal LLMs extend LLM capabilities by integrat -\ning multiple data types, such as medical images and text, to \naddress tasks like report generation, cross-modal retrieval, \nand clinical question answering. Despite these advance -\nments, data heterogeneity remains a challenge, as clinical \ndatasets\toften\tvary\tsignificantly\tin\tformat,\tquality,\tand\tcom-\npleteness across institutions. Additionally, most widely used \ndatasets,\tsuch\tas\tMIMIC-CXR\tand\tCT-RATE\t[8, 14], focus \nheavily on radiology, limiting the generalizability of models \nto other medical domains.\nEvaluating generative AI models in medicine requires \nspecialized metrics that go beyond standard language evalu-\nation\tmetrics.\tWhile\tlexical\tmetrics\tlike\tBLEU\t[158] and \nROUGE\t[159] are commonly used, they often fail to cap -\nture clinical relevance and factual accuracy. To address this, \ndomain-specific\tmetrics\tsuch\tas\tRadGraph\t[157], RaTE -\nScore\t[156],\tand\tGREEN\t[155] have been developed to \nassess the clinical validity of generated medical reports. \nHowever, challenges remain in standardizing evaluation \npractices across diverse medical tasks and datasets. Most \nevaluations are limited to radiology, with less attention \ngiven to other specialties. The limited availability of well-\nannotated\t multimodal\t datasets\t with\t fine-grained\t clinical\t\nlabels further complicates performance benchmarking. \nAdditionally, only a few benchmarking frameworks, such \nas\tReXrank\t[153],\toffer\tthe\tability\tto\tneutrally\tevaluate\t\n1 3\n857\nBiomedical Engineering Letters (2025) 15:845–863\n3. Cai X, Liu S, Han J, Yang L, Liu Z, Liu T. Chestxraybert: a pre -\ntrained language model for chest radiology report summarization. \nIEEE Trans Multimed. 2021;25:845–55.\n4. Li Y , Li Z, Zhang K, Dan R, Jiang S, Zhang Y . Chatdoctor: a \nmedical\tchat\tmodel\tfine-tuned\ton\ta\tlarge\tlanguage\tmodel\tmeta-ai\t\n(llama)\tusing\tmedical\tdomain\tknowledge.\tCureus.\t2023;15(6):65.\n5. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronne -\nberger\tO,\tTunyasuvunakool\tK,\tBates\tR,\tŽídek\tA,\tPotapenko\tA,\t\net al. Highly accurate protein structure prediction with alphafold. \nNature.\t2021;596(7873):583–9.\n6. Acosta JN, Dogra S, Adithan S, Wu K, Moritz M, Kwak S, \nRajpurkar P. The impact of AI assistance on radiology report -\ning: a pilot study using simulated AI draft reports 2024; arxiv: \n2412.12042.\n7. Van Veen D, Van Uden C, Blankemeier L, Delbrouck J-B, Aali A, \nBluethgen C, Pareek A, Polacin M, Reis EP, Seehofnerová A, et al. \nAdapted large language models can outperform medical experts \nin\tclinical\ttext\tsummarization.\tNat\tMed.\t2024;30(4):1134–42.\n8. Johnson AE, Pollard TJ, Berkowitz SJ, Greenbaum NR, Lungren \nMP,\tDeng\tC-Y,\tMark\tRG,\tHorng\tS.\tMimic-cxr,\ta\tde-identified\t\npublicly available database of chest radiographs with free-text \nreports.\tSci\tData.\t2019;6(1):317.\n9. Huang S-C, Huo Z, Steinberg E, Chiang C-C, Lungren MP, Lan-\nglotz CP, Yeung S, Shah NH, Fries JA. Inspect: a multimodal \ndataset for pulmonary embolism diagnosis and prognosis. 2023; \narXiv preprint arXiv:2311.10798.\n10.\t Tayebi\tArasteh\tS,\tSiepmann\tR,\tHuppertz\tM,\tLotfinia\tM,\tPuladi\t\nB, Kuhl C, Truhn D, Nebelung S. The treasure trove hidden in \nplain sight: the utility of gpt-4 in chest radiograph evaluation. \nRadiology.\t2024;313(2):\t233441.\n11. Khader F, Müller-Franzes G, Wang T, Han T, Tayebi Arasteh S, \nHaarburger C, Stegmaier J, Bressem K, Kuhl C, Nebelung S, et \nal. Multimodal deep learning for integrating chest radiographs \nand clinical parameters: a case for transformers. Radiology. \n2023;309(1):\t230806.\n12. Frankish A, Diekhans M, Jungreis I, Lagarde J, Loveland JE, \nMudge JM, Sisu C, Wright JC, Armstrong J, Barnes I, et al. Gen-\ncode\t2021.\tNucleic\tAcids\tRes.\t2021;49(D1):916–23.\n13. Zhang K, Zhou R, Adhikarla E, Yan Z, Liu Y , Yu J, Liu Z, Chen X, \nDavison BD, Ren H, et al. A generalist vision-language foundation \nmodel\tfor\tdiverse\tbiomedical\ttasks.\tNat\tMed.\t2024;30(11):1–13.\n14. Hamamci IE, Er S, Almas F, Simsek AG, Esirgun SN, Dogan I, \nDasdelen MF, Wittmann B, Simsar E, Simsar M, et al. A foun -\ndation model utilizing chest ct volumes and radiology reports \nfor supervised-level zero-shot detection of abnormalities. 2024; \narXiv preprint arXiv:2403.17834 .\n15. Li C, Wong C, Zhang S, Usuyama N, Liu H, Yang J, Naumann T, \nPoon H, Gao J. Llava-med: training a large language-and-vision \nassistant for biomedicine in one day. Adv Neural Inf Process Syst. \n2024;8:36.\n16. Özsoy E, Pellegrini C, Keicher M, Navab N. Oracle: large vision-\nlanguage models for knowledge-guided holistic or domain mod -\neling. In: International conference on medical image computing \nand computer-assisted intervention. Springer; 2024. pp. 455–465.\n17. Yin S, Fu C, Zhao S, Li K, Sun X, Xu T, Chen E. A survey \non multimodal large language models. 2023; arXiv preprint \narXiv:2306.13549.\n18. Wang J, Jiang H, Liu Y , Ma C, Zhang X, Pan Y , Liu M, Gu P, Xia \nS, Li W, et al. A comprehensive review of multimodal large lan -\nguage\tmodels:\tperformance\tand\tchallenges\tacross\tdifferent\ttasks.\t\n2024; arXiv preprint arXiv:2408.01319.\n19. Kline A, Wang H, Li Y , Dennis S, Hutch M, Xu Z, Wang F, Cheng \nF, Luo Y . Multimodal machine learning in precision health: a \nscoping\treview.\tNpj\tDigit\tMed.\t2022;5(1):171.\nto ensure the responsible deployment of generative AI in \nhealthcare.\nSupplementary Information  The online version contains \nsupplementary material available at  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  3 5 3 4 - 0 \n2 5 - 0 0 4 9 7 - 1.\nAcknowledgements This work was partially funded via the EVUK \nprogramme\t(“Next-generation\tAI\tfor\tIntegrated\tDiagnostics”)\tof\tthe\t\nFree\tState\tof\tBavaria,\tthe\tDeutsche\tForschungsgemeinschaft\t(DFG),\t\nand Friedrich-Alexander-Universität Erlangen-Nürnberg within the \nfunding program Open Access Publication Funding.\nAuthor contributions The idea for this review article was developed \nby all authors. L.B. performed the literature search, paper screening, \nand\tselection.\tThe\tfirst\tdraft\tof\tthe\tmanuscript\twas\twritten\tby\tL.B.\t\nand\tsubsequently\trefined\tby\tL.B.\tand\tS.T.A..\tS.T.A.\tprovided\tclini-\ncal expertise. L.B., M.K., N.N., A.M., and S.T.A. provided technical \nexpertise.\tAll\tauthors\trevised\tthe\tmanuscript\tand\tapproved\tthe\tfinal\t\nversion for submission.\nFunding Open Access funding enabled and organized by Projekt \nDEAL.\nDeclarations\nConflict of interest\tThe\tauthors\tdeclare\tno\tconflict\tof\tinterest.\nEthical approval  No human or animal subjects are involved in this \nstudy.\nConsent to participate  No human or animal subjects are involved in \nthis study.\nConsent to publish No human or animal subjects are involved in this \nstudy.\nOpen Access   This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas\tlong\tas\tyou\tgive\tappropriate\tcredit\tto\tthe\toriginal\tauthor(s)\tand\tthe\t\nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle\tare\tincluded\tin\tthe\tarticle’s\tCreative\tCommons\tlicence,\tunless\t\nindicated otherwise in a credit line to the material. If material is not \nincluded\tin\tthe\tarticle’s\tCreative\tCommons\tlicence\tand\tyour\tintended\t\nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  v e c  o m m o  n s .  o \nr g  / l i c e n s e s / b y / 4 . 0 /.\nReferences\n1. Acosta JN, Falcone GJ, Rajpurkar P, Topol EJ. Multimodal bio -\nmedical\tAI.\tNat\tMed.\t2022;28(9):1773–84.\n2.\t Tayebi\tArasteh\t S,\t Han\t T,\tLotfinia\t M,\t Kuhl\t C,\t Kather\t JN,\t\nTruhn D, Nebelung S. Large language models streamline auto -\nmated machine learning for clinical studies. Nat Commun. \n2024;15(1):1603.\n1 3\n858\nBiomedical Engineering Letters (2025) 15:845–863\n39. Mahendran D, McInnes BT. Extracting adverse drug events from \nclinical notes. AMIA Summits Transl Sci Proc. 2021;2021:420.\n40. Lanfredi RB, Mukherjee P, Summers RM. Enhancing chest x-ray \ndatasets with privacy-preserving large language models and \nmulti-type annotations: a data-driven approach for improved clas-\nsification.\tMed\tImage\tAnal.\t2025;99:\t103383.\n41. Liu N, Hu Q, Xu H, Xu X, Chen M. Med-bert: a pretraining \nframework for medical records named entity recognition. IEEE \nTrans\tInd\tInf.\t2021;18(8):5600–8.\n42. Han T, Adams LC, Papaioannou J-M, Grundmann P, Oberhauser \nT, Löser A, Truhn D, Bressem KK. Medalpaca—an open-source \ncollection of medical conversational ai models and training data. \n2023; arXiv preprint arXiv:2304.08247.\n43. Chen Z, Cano AH, Romanou A, Bonnet A, Matoba K, Salvi F, \nPagliardini M, Fan S, Köpf A, Mohtashami A, et al. Meditron-\n70b: scaling medical pretraining for large language models. 2023; \narXiv preprint arXiv:2311.16079.\n44. Qiu P, Wu C, Zhang X, Lin W, Wang H, Zhang Y , Wang Y , Xie W. \nTowards building multilingual language model for medicine. Nat \nCommun.\t2024;15(1):8384.\n45. Mu Y , Tizhoosh HR, Tayebi RM, Ross C, Sur M, Leber B, Camp-\nbell CJ. A bert model generates diagnostically relevant semantic \nembeddings from pathology synopses with active learning. Com-\nmun\tMed.\t2021;1(1):11.\n46. Wu C, Lin W, Zhang X, Zhang Y , Xie W, Wang Y . Pmc-llama: \ntoward building open-source language models for medicine. J \nAm Med Inform Assoc. 2024;6:045.\n47. Jia S, Bit S, Searls E, Claus LA, Fan P, Jasodanand VH, Lau -\nber MV , Veerapaneni D, Wang WM, Au R, et al. Medpodgpt: a \nmultilingual audio-augmented large language model for medical \nresearch and education. medRxiv 2024.\n48. Yan A, McAuley J, Lu X, Du J, Chang EY , Gentili A, Hsu C-N. \nRadbert: adapting transformer-based language models to radiol -\nogy.\tRadiol\tArtif\tIntell.\t2022;4(4):\t210258.\n49. Schmidt RA, Seah JC, Cao K, Lim L, Lim W, Yeung J. Genera -\ntive large language models for detection of speech recognition \nerrors\tin\tradiology\treports.\tRadiol\tArtif\tIntell.\t2024;6(2):\t230205.\n50. Prihoda D, Maamary J, Waight A, Juan V , Fayadat-Dilman L, \nSvozil D, Bitton DA. Biophi: A platform for antibody design, \nhumanization, and humanness evaluation based on natural anti -\nbody repertoires and deep learning. In: MAbs, vol 14. Taylor & \nFrancis; 2022. p. 2020203.\n51. Schubach M, Maass T, Nazaretyan L, Röner S, Kircher M. Cadd \nv1. 7: using protein language models, regulatory cnns and other \nnucleotide-level scores to improve genome-wide variant predic -\ntions.\tNucleic\tAcids\tRes.\t2024;52(D1):1143–54.\n52. Ji Y , Zhou Z, Liu H, Davuluri RV . Dnabert: pre-trained bidirec-\ntional encoder representations from transformers model for dna-\nlanguage\tin\tgenome.\tBioinformatics.\t2021;37(15):2112–20.\n53.\t Theodoris\tCV,\tXiao\tL,\tChopra\tA,\tChaffin\tMD,\tAl\tSayed\tZR,\t\nHill MC, Mantineo H, Brydon EM, Zeng Z, Liu XS, et al. Trans-\nfer learning enables predictions in network biology. Nature. \n2023;618(7965):616–24.\n54. Hie BL, Shanker VR, Xu D, Bruun TU, Weidenbacher PA, Tang \nS,\tWu\tW,\tPak\tJE,\tKim\tPS.\tEfficient\tevolution\tof\thuman\tanti-\nbodies from general protein language models. Nat Biotechnol. \n2024;42(2):275–83.\n55. Rao RM, Liu J, Verkuil R, Meier J, Canny J, Abbeel P, Sercu \nT, Rives A. Msa transformer. In: International conference on \nmachine learning. PMLR; 2021. pp. 8844–8856.\n56. Madani A, Krause B, Greene ER, Subramanian S, Mohr BP, \nHolton JM, Olmos JL, Xiong C, Sun ZZ, Socher R, et al. Large \nlanguage models generate functional protein sequences across \ndiverse\tfamilies.\tNat\tBiotechnol.\t2023;41(8):1099–106.\n20. He Y , Huang F, Jiang X, Nie Y , Wang M, Wang J, Chen H. Foun-\ndation model for advancing healthcare: challenges, opportunities, \nand future directions. 2024; arXiv preprint arXiv:2404.03264.\n21.\t Tricco\tAC,\tLillie\tE,\tZarin\tW,\tO’Brien\tKK,\tColquhoun\tH,\tLevac\t\nD, Moher D, Peters MD, Horsley T, Weeks L, et al. Prisma exten-\nsion\tfor\tscoping\treviews\t(prisma-scr):\tchecklist\tand\texplanation.\t\nAnn\tIntern\tMed.\t2018;169(7):467–73.\n22.\t Page\tMJ,\tMcKenzie\tJE,\tBossuyt\tPM,\tBoutron\tI,\tHoffmann\tTC,\t\nMulrow\tCD,\tShamseer\tL,\tTetzlaff\tJM,\tAkl\tEA,\tBrennan\tSE,\tet\t\nal. The prisma 2020 statement: an updated guideline for reporting \nsystematic reviews. BMJ. 2021;6:372.\n23. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. \nRayyan—a web and mobile app for systematic reviews. Syst Rev. \n2016;5:1–10.\n24. Jiang LY , Liu XC, Nejatian NP, Nasir-Moin M, Wang D, Abidin \nA, Eaton K, Riina HA, Laufer I, Punjabi P, et al. Health system-\nscale language models are all-purpose prediction engines. Nature. \n2023;619(7969):357–62.\n25. Vaswani A. Attention is all you need. Adv Neural Inf Process \nSyst. 2017.\n26. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, Kang J. Biobert: \na pre-trained biomedical language representation model for bio -\nmedical\ttext\tmining.\tBioinformatics.\t2020;36(4):1234–40.\n27. Naseem U, Khushi M, Reddy V , Rajendran S, Razzak I, Kim J. \nBioalbert:\ta\tsimple\tand\teffective\tpre-trained\tlanguage\tmodel\tfor\t\nbiomedical named entity recognition. In: 2021 International joint \nconference\ton\tneural\tnetworks\t(IJCNN).\tIEEE;\t2021.\tpp.\t1–7.\n28. Labrak Y , Bazoge A, Morin E, Gourraud P-A, Rouvier M, \nDufour R. Biomistral: a collection of open-source pretrained \nlarge language models for medical domains. 2024; arXiv preprint \narXiv:2402.10373.\n29. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, Scales \nN, Tanwani A, Cole-Lewis H, Pfohl S, et al. Large language mod-\nels\tencode\tclinical\tknowledge.\tNature.\t2023;620(7972):172–80.\n30. Wang L, Chen X, Deng X, Wen H, You M, Liu W, Li Q, Li J. \nPrompt engineering in consistency and reliability with the evi -\ndence-based\tguideline\tfor\tllms.\tNpj\tDigit\tMed.\t2024;7(1):41.\n31. Lee H, Phatale S, Mansoor H, Lu KR, Mesnard T, Ferret J, \nBishop C, Hall E, Carbune V , Rastogi A. Rlaif: scaling reinforce-\nment learning from human feedback with ai feedback; 2023.\n32. Zhang H, Chen J, Jiang F, Yu F, Chen Z, Li J, Chen G, Wu X, \nZhang Z, Xiao Q, et al. Huatuogpt, towards taming language \nmodel to be a doctor. 2023; arXiv preprint arXiv:2305.15075.\n33. Zakka C, Shad R, Chaurasia A, Dalal AR, Kim JL, Moor M, Fong \nR, Phillips C, Alexander K, Ashley E, et al. Almanac-retrieval-\naugmented language models for clinical medicine. NEJM AI. \n2024;1(2):2300068.\n34. Luo R, Sun L, Xia Y , Qin T, Zhang S, Poon H, Liu T-Y . Biogpt: \ngenerative pre-trained transformer for biomedical text generation \nand\tmining.\tBrief\tBioinform.\t2022;23(6):409.\n35. Wang H, Gao C, Dantona C, Hull B, Sun J. Drg-llama: tuning \nllama model to predict diagnosis-related group for hospitalized \npatients.\tNpj\tDigit\tMed.\t2024;7(1):16.\n36. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien \nC, Compas C, Martin C, Costa AB, Flores MG, et al. A large \nlanguage model for electronic health records. NPJ Digit Med. \n2022;5(1):194.\n37.\t Johnson\tAE,\tBulgarelli\tL,\tPollard\tTJ.\tDeidentification\tof\tfree-\ntext medical records using pre-trained bidirectional transformers. \nIn: Proceedings of the ACM conference on health, inference, and \nlearning; 2020. pp. 214–221.\n38.\t Kresevic\tS,\tGiuffrè\tM,\tAjcevic\tM,\tAccardo\tA,\tCrocè\tLS,\tShung\t\nDL. Optimization of hepatological clinical guidelines interpreta -\ntion by large language models: a retrieval augmented generation-\nbased\tframework.\tNPJ\tDigit\tMed.\t2024;7(1):102.\n1 3\n859\nBiomedical Engineering Letters (2025) 15:845–863\nx-ray images via self-supervised learning. Nat Biomed Eng. \n2022;6(12):1399–406.\n75. Endo M, Krishnan R, Krishna V , Ng AY , Rajpurkar P. Retrieval-\nbased chest x-ray report generation using a pre-trained contras -\ntive language-image model. In: Machine learning for health. \nPMLR 2021. pp. 209–219.\n76. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, \nSastry G, Askell A, Mishkin P, Clark J, et al. Learning transfer -\nable visual models from natural language supervision. In: Inter -\nnational conference on machine learning. PMLR; 2021. pp. \n8748–8763.\n77. Blankemeier L, Cohen JP, Kumar A, Van Veen D, Gardezi SJS, \nPaschali M, Chen Z, Delbrouck J-B, Reis E, Truyts C, et al. Mer-\nlin: a vision language foundation model for 3d computed tomog-\nraphy. 2024; arXiv preprint arXiv:2406.06512.\n78. Liu H, Li C, Wu Q, Lee YJ. Visual instruction tuning. Adv Neural \nInf Process Syst. 2024;6:36.\n79. Tu T, Azizi S, Driess D, Schaekermann M, Amin M, Chang P-C, \nCarroll A, Lau C, Tanno R, Ktena I, et al. Towards generalist bio-\nmedical\tai.\tNEJM\tAI.\t2024;1(3):2300138.\n80. Zhou H-Y , Adithan S, Acosta JN, Topol EJ, Rajpurkar P. A gener-\nalist learner for multifaceted medical image interpretation. 2024; \narXiv preprint arXiv:2405.07988.\n81. Wu C, Zhang X, Zhang Y , Wang Y , Xie W. Towards gener -\nalist foundation model for radiology. 2023; arXiv preprint \narXiv:2308.02463.\n82. Zhang S, Xu Y , Usuyama N, Xu H, Bagga J, Tinn R, Preston S, \nRao R, Wei M, Valluri N, et al. A multimodal biomedical founda-\ntion\tmodel\ttrained\tfrom\tfifteen\tmillion\timage-text\tpairs.\tNEJM\t\nAI. 2024. p. 2400640.\n83. Boecking B, Usuyama N, Bannur S, Castro DC, Schwaighofer \nA, Hyland S, Wetscherek M, Naumann T, Nori A, Alvarez-Valle \nJ, et al. Making the most of text semantics to improve biomedi -\ncal vision-language processing. In: European conference on com-\nputer vision. Springer; 2022. pp. 1–21.\n84. Bannur S, Hyland S, Liu Q, Perez-Garcia F, Ilse M, Castro DC, \nBoecking B, Sharma H, Bouzid K, Thieme A, et al. Learning to \nexploit temporal structure for biomedical vision-language pro -\ncessing. In: Proceedings of the IEEE/CVF conference on com -\nputer vision and pattern recognition; 2023. pp. 15016–15027.\n85. Zhang Y , Jiang H, Miura Y , Manning CD, Langlotz CP. Con -\ntrastive learning of medical visual representations from paired \nimages and text. In: Machine learning for healthcare conference. \nPMLR; 2022. pp. 2–25.\n86. Javed S, Mahmood A, Ganapathi II, Dharejo FA, Werghi N, Ben-\nnamoun M. Cplip: zero-shot learning for histopathology with \ncomprehensive vision-language alignment. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recogni -\ntion; 2024. pp. 11450–11459.\n87. Yang L, Xu S, Sellergren A, Kohlberger T, Zhou Y , Ktena I, \nKiraly A, Ahmed F, Hormozdiari F, Jaroensri T, et al. Advancing \nmultimodal medical capabilities of gemini. 2024; arXiv preprint \narXiv:2405.03162.\n88. Liu C, Wan Z, Cheng S, Zhang M, Arcucci R. Etp: learning trans-\nferable ecg representations via ecg-text pre-training. In: ICASSP \n2024-2024 IEEE international conference on acoustics, speech \nand\tsignal\tprocessing\t(ICASSP).\tIEEE;\t2024.\tpp.\t8230–8234.\n89. Luo Y , Shi M, Khan MO, Afzal MM, Huang H, Yuan S, Tian Y , \nSong L, Kouhana A, Elze T, et al. Fairclip: harnessing fairness \nin vision-language learning. In: Proceedings of the IEEE/CVF \nconference on computer vision and pattern recognition; 2024. pp. \n12289–12301.\n90. Li H, Chen Y , Chen Y , Yu R, Yang W, Wang L, Ding B, Han Y . \nGeneralizable\twhole\tslide\timage\tclassification\twith\tfine-grained\t\nvisual-semantic interaction. In: Proceedings of the IEEE/CVF \n57. Ferruz N, Schmidt S, Höcker B. Protgpt2 is a deep unsu -\npervised language model for protein design. Nat Commun. \n2022;13(1):4348.\n58. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y , \nJones L, Gibbs T, Feher T, Angerer C, Steinegger M, et al. \nProttrans: toward understanding the language of life through \nself-supervised learning. IEEE Trans Pattern Anal Mach Intell. \n2021;44(10):7112–27.\n59. Yang F, Wang W, Wang F, Fang Y , Tang D, Huang J, Lu H, Yao \nJ. scbert as a large-scale pretrained deep language model for \ncell type annotation of single-cell rna-seq data. Nat Mach Intell. \n2022;4(10):852–66.\n60. Rathore AS, Choudhury S, Arora A, Tijare P, Raghava GP. Toxin-\npred 3.0: an improved method for predicting the toxicity of pep -\ntides. Comput Biol Med. 2024;179: 108926.\n61. Chen J, Cai Z, Ji K, Wang X, Liu W, Wang R, Hou J, Wang B. \nHuatuogpt-o1, towards medical complex reasoning with llms. \n2024; arXiv preprint arXiv:2412.18925.\n62. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, \nKüttler H, Lewis M, Yih W-T, Rocktäschel T, et al. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. Adv \nNeural Inf Process Syst. 2020;33:9459–74.\n63.\t Arasteh\tST,\tLotfinia\tM,\tBressem\tK,\tSiepmann\tR,\tAdams\tL,\tFer-\nber D, Kuhl C, Kather JN, Nebelung S, Truhn D. RadioRAG: \nfactual large language models for enhanced diagnostics in radi -\nology using online retrieval augmented generation 2024; arxiv: \n2407.15621.\n64. Gilbert S, Kather JN, Hogan A. Augmented non-hallucinating \nlarge language models as medical information curators. NPJ Digit \nMed.\t2024;7(1):100.\n65. Nowak S, Biesner D, Layer Y , Theis M, Schneider H, Block W, \nWulff\tB,\tAttenberger\tU,\tSifa\tR,\tSprinkart\tA.\tTransformer-based\t\nstructuring of free-text radiology report databases. Eur Radiol. \n2023;33(6):4228–36.\n66. Johnson AE, Bulgarelli L, Shen L, Gayles A, Shammout A, \nHorng S, Pollard TJ, Hao S, Moody B, Gow B, et al. Mimic-\niv, a freely accessible electronic health record dataset. Sci Data. \n2023;10(1):1.\n67.\t Pollard\tTJ,\tJohnson\tAE,\tRaffa\tJD,\tCeli\tLA,\tMark\tRG,\tBadawi\t\nO. The eicu collaborative research database, a freely avail -\nable multi-center database for critical care research. Sci Data. \n2018;5(1):1–13.\n68. Zeng G, Yang W, Ju Z, Yang Y , Wang S, Zhang R, Zhou M, Zeng \nJ, Dong X, Zhang R, et al. Meddialog: large-scale medical dia -\nlogue datasets. In: Proceedings of the 2020 conference on empiri-\ncal\tmethods\tin\tnatural\tlanguage\tprocessing\t(EMNLP);\t2020.\tpp.\t\n9241–9250.\n69. Uniprot: the universal protein knowledgebase in 2023. Nucleic \nAcids\tRes.\t2023;51(D1):523–531.\n70. Benson DA, Cavanaugh M, Clark K, Karsch-Mizrachi I, Lip -\nman DJ, Ostell J, Sayers EW. Genbank. Nucleic Acids Res. \n2012;41(D1):36–42.\n71. Edwards NJ, Oberti M, Thangudu RR, Cai S, McGarvey PB, \nJacob S, Madhavan S, Ketchum KA. The cptac data portal: \na resource for cancer proteomics research. J Proteome Res. \n2015;14(6):2707–13.\n72. Bannur S, Bouzid K, Castro DC, Schwaighofer A, Bond-Taylor \nS,\tIlse\tM,\tPérez-García\tF,\tSalvatelli\tV,\tSharma\tH,\tMeissen\tF,\tet\t\nal. Maira-2: grounded radiology report generation. 2024; arXiv \npreprint arXiv:2406.04449.\n73. Pellegrini C, Özsoy E, Busam B, Navab N, Keicher M. Radia -\nlog: a large vision-language model for radiology report gen -\neration and conversational assistance. 2023; arXiv preprint \narXiv:2311.18681.\n74. Tiu E, Talius E, Patel P, Langlotz CP, Ng AY , Rajpurkar P. \nExpert-level detection of pathologies from unannotated chest \n1 3\n860\nBiomedical Engineering Letters (2025) 15:845–863\n108. Lei J, Zhang X, Wu C, Dai L, Zhang Y , Zhang Y , Wang Y , Xie \nW, Li Y . Autorg-brain: grounded report generation for brain mri. \n2024; arXiv preprint arXiv:2407.16684.\n109. Tanida T, Müller P, Kaissis G, Rueckert D. Interactive and \nexplainable region-guided radiology report generation. In: Pro -\nceedings of the IEEE/CVF conference on computer vision and \npattern recognition; 2023. pp. 7433–7442.\n110. Li Z, Li Y , Li Q, Wang P, Guo D, Lu L, Jin D, Zhang Y , Hong Q. \nLvit: language meets vision transformer in medical image seg -\nmentation. IEEE Trans Med Imaging; 2023.\n111. Hamamci IE, Er S, Sekuboyina A, Simsar E, Tezcan A, Simsek \nAG, Esirgun SN, Almas F, Dogan I, Dasdelen MF, et al. Gen -\neratect: text-conditional generation of 3d chest ct volumes. 2023; \narXiv preprint arXiv:2305.16037.\n112.\tBluethgen\tC,\tChambon\tP,\tDelbrouck\tJ-B,\tSluijs\tR,\tPołacin\tM,\t\nZambrano Chaves JM, Abraham TM, Purohit S, Langlotz CP, \nChaudhari AS. A vision-language foundation model for the \ngeneration of realistic chest x-ray images. Nat Biomed Eng. \n2024;6:1–13.\n113. Khader F, Müller-Franzes G, Tayebi Arasteh S, Han T, Haar -\nburger C, Schulze-Hagen M, Schad P, Engelhardt S, Baeßler B, \nFoersch\tS,\tet\tal.\tDenoising\tdiffusion\tprobabilistic\tmodels\tfor\t3d\t\nmedical\timage\tgeneration.\tSci\tRep.\t2023;13(1):7303.\n114. Jiang S, Zheng T, Zhang Y , Jin Y , Yuan L, Liu Z. Med-moe: mix-\nture\tof\tdomain-specific\texperts\tfor\tlightweight\tmedical\tvision-\nlanguage models. 2024; arXiv preprint arXiv:2404.10237.\n115. Alsharid M, Cai Y , Sharma H, Drukker L, Papageorghiou AT, \nNoble JA. Gaze-assisted automatic captioning of fetal ultrasound \nvideos using three-way multi-modal deep neural networks. Med \nImage Anal. 2022;82: 102630.\n116. Cui H, Mao L, Liang X, Zhang J, Ren H, Li Q, Li X, Yang C. \nBiomedical visual instruction tuning with clinician preference \nalignment. 2024; arXiv preprint arXiv:2406.13173.\n117. Wang S, Zhao Z, Ouyang X, Wang Q, Shen D. Chatcad: interac-\ntive computer-aided diagnosis on medical image using large lan -\nguage models. 2023; arXiv preprint arXiv:2302.07257.\n118. Chen Z, Varma M, Delbrouck J-B, Paschali M, Blankemeier L, \nVan Veen D, Valanarasu JMJ, Youssef A, Cohen JP, Reis EP, et al. \nChexagent: towards a foundation model for chest x-ray interpre -\ntation. 2024; arXiv preprint arXiv:2401.12208.\n119. Gu T, Liu D, Li Z, Cai W. Complex organ mask guided radiology \nreport generation. In: Proceedings of the IEEE/CVF winter con -\nference on applications of computer vision; 2024. pp. 7995–8004.\n120. Chen X, Zhang W, Xu P, Zhao Z, Zheng Y , Shi D, He M. Ffa-gpt: \nan\tautomated\tpipeline\tfor\tfundus\tfluorescein\tangiography\tinter-\npretation\tand\tquestion-answer.\tNpj\tDigit\tMed.\t2024;7(1):111.\n121. Huh J, Park S, Lee JE, Ye JC. Improving medical speech-to-text \naccuracy with vision-language pre-training model. 2023; arXiv \npreprint arXiv:2303.00091.\n122. Bai F, Du Y , Huang T, Meng MQ-H, Zhao B. M3d: advancing 3d \nmedical image analysis with multi-modal large language models. \n2024; arXiv preprint arXiv:2404.00578.\n123. Sharma H, Salvatelli V , Srivastav S, Bouzid K, Bannur S, \nCastro DC, Ilse M, Bond-Taylor S, Ranjit MP, Falck F, et al. \nMaira-seg: enhancing radiology report generation with segmen -\ntation-aware multimodal large language models. 2024; arXiv pre-\nprint arXiv:2411.11362.\n124. Moor M, Huang Q, Wu S, Yasunaga M, Dalmia Y , Leskovec J, \nZakka\tC,\tReis\tEP,\tRajpurkar\tP.\tMed-flamingo:\ta\tmultimodal\tmed-\nical\tfew-shot\tlearner.\tIn:\tMachine\tlearning\tfor\thealth\t(ML4H).\t\nPMLR; 2023. pp. 353–367.\n125. Khare Y , Bagal V , Mathew M, Devi A, Priyakumar UD, Jawahar \nC. Mmbert: multimodal bert pretraining for improved medical \nvqa. In: 2021 IEEE 18th international symposium on biomedical \nimaging\t(ISBI).\tIEEE;\t2021.\tpp.\t1033–1036.\nconference on computer vision and pattern recognition; 2024. pp. \n11398–11407.\n91. Keicher M, Zaripova K, Czempiel T, Mach K, Khakzar A, Navab \nN.\tFlexr:\tfew-shot\tclassification\twith\tlanguage\tembeddings\tfor\t\nstructured reporting of chest x-rays. In: Medical imaging with \ndeep learning. PMLR; 2024. pp. 1493–1508.\n92. Huang S-C, Shen L, Lungren MP, Yeung S. Gloria: a multimodal \nglobal-local\t representation\t learning\t framework\t for\t label-effi-\ncient medical image recognition. In: Proceedings of the IEEE/\nCVF international conference on computer vision; 2021. pp. \n3942–3951.\n93. Zhang X, Wu C, Zhang Y , Xie W, Wang Y . Knowledge-enhanced \nvisual-language pre-training on chest radiology images. Nat \nCommun.\t2023;14(1):4542.\n94. Huang W, Li C, Zhou H-Y , Yang H, Liu J, Liang Y , Zheng H, \nZhang S, Wang S. Enhancing representation in radiography-\nreports foundation model: a granular alignment algorithm using \nmasked\tcontrastive\tlearning.\tNat\tCommun.\t2024;15(1):7620.\n95. Wang P, Zhang H, Yuan Y . Mcpl: multi-modal collaborative \nprompt learning for medical vision-language model. IEEE Trans \nMed Imaging 2024.\n96. Codella NC, Jin Y , Jain S, Gu Y , Lee HH, Abacha AB, Santa -\nmaria-Pang A, Guyman W, Sangani N, Zhang S, et al. Medim -\nageinsight: an open-source embedding model for general domain \nmedical imaging. 2024; arXiv preprint arXiv:2410.06542.\n97. Liu F, Zhu T, Wu X, Yang B, You C, Wang C, Lu L, Liu Z, Zheng \nY , Sun X, et al. A medical multimodal large language model for \nfuture\tpandemics.\tNPJ\tDigit\tMed.\t2023;6(1):226.\n98. Moon JH, Lee H, Shin W, Kim Y-H, Choi E. Multi-modal \nunderstanding and generation for medical images and text via \nvision-language pre-training. IEEE J Biomed Health Inform. \n2022;26(12):6070–80.\n99. Liu S, Nie W, Wang C, Lu J, Qiao Z, Liu L, Tang J, Xiao C, Anan-\ndkumar A. Multi-modal molecule structure-text model for text-\nbased\tretrieval\tand\tediting.\tNat\tMach\tIntell.\t2023;5(12):1447–57.\n100.\tTang\tX,\tTran\tA,\tTan\tJ,\tGerstein\tMB.\tMollm:\ta\tunified\tlanguage\t\nmodel for integrating biomedical text with 2d and 3d molecular \nrepresentations.\tBioinformatics.\t2024;40(1):357–68.\n101. Huang Z, Bianchi F, Yuksekgonul M, Montine TJ, Zou J. A \nvisual-language foundation model for pathology image analysis \nusing\tmedical\ttwitter.\tNat\tMed.\t2023;29(9):2307–16.\n102. Xu H, Usuyama N, Bagga J, Zhang S, Rao R, Naumann T, \nWong C, Gero Z, González J, Gu Y , et al. A whole-slide foun -\ndation model for digital pathology from real-world data. Nature. \n2024;6:1–8.\n103. Khattak MU, Kunhimon S, Naseer M, Khan S, Khan FS. \nUnimed-clip:\ttowards\ta\tunified\timage-text\tpretraining\tparadigm\t\nfor diverse medical imaging modalities. 2024; arXiv preprint \narXiv:2412.10372.\n104. Pellegrini C, Keicher M, Özsoy E, Jiraskova P, Braren R, Navab \nN. Xplainer: from x-ray observations to explainable zero-shot \ndiagnosis. In: International conference on medical image com -\nputing and computer-assisted intervention. Springer 2023. pp. \n420–429.\n105. Ran A, Liu H. Joint spatio-temporal features constrained self-\nsupervised electrocardiogram representation learning. Biomed \nEng\tLett.\t2024;14(2):209–20.\n106. Kang Y , Yang G, Eom H, Han S, Baek S, Noh S, Shin Y , Park C. \nGan-based patient information hiding for an ecg authentication \nsystem.\tBiomed\tEng\tLett.\t2023;13(2):197–207.\n107. Zhou J, He X, Sun L, Xu J, Chen X, Chu Y , Zhou L, Liao X, \nZhang B, Afvari S, et al. Pre-trained multimodal large language \nmodel enhances dermatological diagnosis using skingpt-4. Nat \nCommun.\t2024;15(1):5649.\n1 3\n861\nBiomedical Engineering Letters (2025) 15:845–863\n143. Pellegrini C, Keicher M, Özsoy E, Navab N. Rad-restruct: a novel \nvqa benchmark and method for structured radiology reporting. \nIn: International conference on medical image computing and \ncomputer-assisted intervention. Springer; 2023. pp. 409–419.\n144. Liu B, Zhan L-M, Xu L, Ma L, Yang Y , Wu X-M. Slake: a seman-\ntically-labeled knowledge-enhanced dataset for medical visual \nquestion answering. In: 2021 IEEE 18th international symposium \non\tbiomedical\timaging\t(ISBI).\tIEEE;\t2021.\tpp.\t1650–1654.\n145. Lau JJ, Gayen S, Ben Abacha A, Demner-Fushman D. A dataset \nof clinically generated visual questions and answers about radiol-\nogy\timages.\tSci\tData.\t2018;5(1):1–10.\n146. codabench: MICCAI24 AMOS-MM: abdominal multimodal \nanalysis challenge; 2024. Accessed 11 Apr 2024.  h t t p  s : /  / w w w  . c  \no d a  b e n c  h . o  r g /  c o m p e t i t i o n s / 3 1 3 7 /.\n147. Ji Y , Bai H, Ge C, Yang J, Zhu Y , Zhang R, Li Z, Zhanng L, Ma W, \nWan X, et al. Amos: a large-scale abdominal multi-organ bench-\nmark for versatile medical image segmentation. Adv Neural Inf \nProcess Syst. 2022;35:36722–32.\n148. Chen Y , Liu C, Liu X, Arcucci R, Xiong Z. Bimcv-r: a landmark \ndataset for 3d ct text-image retrieval. In: International conference \non medical image computing and computer-assisted intervention. \nSpringer; 2024. pp. 124–134\n149. Zhang X, Wu C, Zhao Z, Lei J, Zhang Y , Wang Y , Xie W. Rad-\ngenome-chest ct: a grounded vision-language dataset for chest ct \nanalysis. 2024; arXiv preprint arXiv:2404.16754 .\n150. Saha A, Harowicz MR, Grimm LJ, Kim CE, Ghate SV , Walsh R, \nMazurowski MA. A machine learning approach to radiogenomics \nof breast cancer: a study of 922 subjects and 529 dce-mri features. \nBr\tJ\tCancer.\t2018;119(4):508–16.\n151.\tWagner\tP,\tStrodthoff\tN,\tBousseljot\tR-D,\tKreiseler\tD,\tLunze\tFI,\t\nSamek\tW,\tSchaeffter\tT.\tPtb-xl,\ta\tlarge\tpublicly\tavailable\telectro-\ncardiography\tdataset.\tSci\tData.\t2020;7(1):1–15.\n152. Liu S, Li Y , Li Z, Gitter A, Zhu Y , Lu J, Xu Z, Nie W, Ramanathan \nA, Xiao C, et al. A text-guided protein design framework. 2023; \narXiv preprint arXiv:2302.04611.\n153. Zhang X, Zhou H-Y , Yang X, Banerjee O, Acosta JN, Miller \nJ, Huang O, Rajpurkar P. Rexrank: a public leaderboard for \nai-powered radiology report generation. 2024; arXiv preprint \narXiv:2411.15122.\n154.\tFerber\tD,\tWölflein\tG,\tWiest\tIC,\tLigero\tM,\tSainath\tS,\tGhaffari\t\nLaleh N, El Nahhas OS, Müller-Franzes G, Jäger D, Truhn D, \net al. In-context learning enables multimodal large language \nmodels to classify cancer pathology images. Nat Commun. \n2024;15(1):10104.\n155. Ostmeier S, Xu J, Chen Z, Varma M, Blankemeier L, Bluethgen \nC, Michalson AE, Moseley M, Langlotz C, Chaudhari AS, et al. \nGreen: generative radiology report evaluation and error notation. \n2024; arXiv preprint arXiv:2405.03595.\n156. Zhao W, Wu C, Zhang X, Zhang Y , Wang Y , Xie W. Ratescore: a \nmetric for radiology report generation. medRxiv; 2024;2024-06.\n157. Yu F, Endo M, Krishnan R, Pan I, Tsai A, Reis EP, Fonseca \nEKUN, Lee HMH, Abad ZSH, Ng AY , et al. Evaluating progress \nin automatic chest x-ray radiology report generation. Patterns. \n2023;4(9):63.\n158. Papineni K, Roukos S, Ward T, Zhu W-J. Bleu: a method for auto-\nmatic evaluation of machine translation. In: Proceedings of the \n40th annual meeting of the association for computational linguis-\ntics; 2002. pp. 311–318.\n159. Lin C-Y . Rouge: a package for automatic evaluation of summa -\nries. In: Text summarization branches out; 2004. pp. 74–81.\n160. Banerjee S, Lavie A. Meteor: an automatic metric for mt evalu -\nation with improved correlation with human judgments. In: \nProceedings of the Acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or summarization; \n2005. pp. 65–72.\n126. Cao X, Liang K, Liao K-D, Gao T, Ye W, Chen J, Ding Z, Cao J, \nRehg JM, Sun J. Medical video generation for disease progres -\nsion simulation. 2024; arXiv preprint arXiv:2411.11943.\n127. Lu MY , Chen B, Williamson DF, Chen RJ, Zhao M, Chow AK, \nIkemura K, Kim A, Pouli D, Patel A, et al. A multimodal generative \nAI\tcopilot\tfor\thuman\tpathology.\tNature.\t2024;634(8033):466–73.\n128. Yellapragada S, Graikos A, Prasanna P, Kurc T, Saltz J, Samaras \nD.\tPathldm:\ttext\tconditioned\tlatent\tdiffusion\tmodel\tfor\thistopa-\nthology. In: Proceedings of the IEEE/CVF winter conference on \napplications of computer vision; 2024. pp. 5182–5191.\n129.\tSeyfioglu\tMS,\tIkezogwo\tWO,\tGhezloo\tF,\tKrishna\tR,\tShapiro\tL.\t\nQuilt-llava: visual instruction tuning by extracting localized nar -\nratives from open-source histopathology videos. In: Proceedings \nof the IEEE/CVF conference on computer vision and pattern rec-\nognition; 2024. pp. 13183–13192.\n130. Wang Z, Liu L, Wang L, Zhou L. R2gengpt: radiology report gen-\neration\twith\tfrozen\tllms.\tMeta\tRadiol.\t2023;1(3):\t100033.\n131. Luo L, Vairavamurthy J, Zhang X, Kumar A, Ter-Oganesyan RR, \nSchroff\tST,\tShilo\tD,\tHossain\tR,\tMoritz\tM,\tRajpurkar\tP.\tRexplain:\t\nTranslating radiology into patient-friendly video reports. 2024; \narXiv preprint arXiv:2410.00441.\n132. Bai L, Wang G, Islam M, Seenivasan L, Wang A, Ren H. Sur -\ngical-vqla++: adversarial contrastive learning for calibrated \nrobust visual question-localized answering in robotic surgery. Inf \nFusion. 2025;113: 102602.\n133. Liu J, Zhang Y , Chen J-N, Xiao J, Lu Y , Landman B, Yuan Y , \nYuille A, Tang Y , Zhou Z. Clip-driven universal model for organ \nsegmentation and tumor detection. In: Proceedings of the IEEE/\nCVF international conference on computer vision; 2023. pp. \n21152–21164.\n134. Wang Y , Dai Y , Jones C, Sair HI, Shen J, Loizou N, Hsu W-C, \nImami MR, Jiao Z, Zhang PJ, et al. Enhancing vision-language \nmodels for medical imaging: bridging the 3d gap with innovative \nslice selection. In: The thirty-eight conference on neural informa-\ntion processing systems datasets and benchmarks track.\n135. Irvin J, Rajpurkar P, Ko M, Yu Y , Ciurea-Ilcus S, Chute C, \nMarklund H, Haghgoo B, Ball R, Shpanskaya K, et al. Chexpert: \na large chest radiograph dataset with uncertainty labels and expert \ncomparison.\tIn:\tProceedings\tof\tthe\tAAAI\tconference\ton\tartificial\t\nintelligence; 2019, vol 33, pp. 590–597.\n136. Xie Y , Zhou C, Gao L, Wu J, Li X, Zhou H-Y , Liu S, Xing L, Zou \nJ, Xie C, et al. Medtrinity-25m: a large-scale multimodal dataset \nwith multigranular annotations for medicine. 2024; arXiv pre -\nprint arXiv:2408.02900.\n137. Gupta D, Attal K, Demner-Fushman D. A dataset for medical \ninstructional\t video\t classification\t and\t question\t answering.\t Sci\t\nData.\t2023;10(1):158.\n138. Hu Y , Li T, Lu Q, Shao W, He J, Qiao Y , Luo P. Omnimedvqa: a \nnew large-scale comprehensive evaluation benchmark for medi -\ncal lvlm. In: Proceedings of the IEEE/CVF conference on com -\nputer vision and pattern recognition; 2024. pp. 22170–22183.\n139. Bustos A, Pertusa A, Salinas J-M, De La Iglesia-Vaya M. \nPadchest: a large chest x-ray image dataset with multi-label anno-\ntated reports. Med Image Anal. 2020;66: 101797.\n140. He X, Zhang Y , Mou L, Xing E, Xie P. Pathvqa: 30000+ ques -\ntions for medical visual question answering. 2020; arXiv preprint \narXiv:2003.10286.\n141. Chen J, Ouyang R, Gao A, Chen S, Chen GH, Wang X, Zhang \nR, Cai Z, Ji K, Yu G, et al. Huatuogpt-vision, towards injecting \nmedical visual knowledge into multimodal llms at scale. 2024; \narXiv preprint arXiv:2406.19280.\n142.\tIkezogwo\tW,\tSeyfioglu\tS,\tGhezloo\tF,\tGeva\tD,\tSheikh\tMoham-\nmed F, Anand PK, Krishna R, Shapiro L. Quilt-1m: one million \nimage-text pairs for histopathology. Adv Neural Inf Process Syst. \n2024;36:65.\n1 3\n862\nBiomedical Engineering Letters (2025) 15:845–863\n168. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya \nJW. Ethics of large language models in medicine and medical \nresearch.\tLancet\tDigit\tHealth.\t2023;5(6):333–5.\n169. Paschali M, Chen Z, Blankemeier L, Varma M, Youssef A, Blu -\nethgen C, Langlotz C, Gatidis S, Chaudhari A. Foundation mod -\nels in radiology: what, how, when, why and why not. 2024; arXiv \npreprint arXiv:2411.18730.\n170. Bluethgen C, Van Veen D, Zakka C, Link K, Fanous A, Danesh-\njou R, Frauenfelder T, Langlotz C, Gatidis S, Chaudhari A. Best \npractices for large language models in radiology. 2024; arXiv pre-\nprint arXiv:2412.01233.\n171. Hager P, Jungmann F, Holland R, Bhagat K, Hubrecht I, Knauer \nM, Vielhauer J, Makowski M, Braren R, Kaissis G, et al. Evalua-\ntion and mitigation of the limitations of large language models in \nclinical\tdecision-making.\tNat\tMed.\t2024;30(9):2613–22.\n172. Lightman H, Kosaraju V , Burda Y , Edwards H, Baker B, Lee T, \nLeike\tJ,\tSchulman\tJ,\tSutskever\tI,\tCobbe\tK.\tLet’s\tverify\tstep\tby\t\nstep. 2023; arXiv preprint arXiv:2305.20050.\n173. Chua M, Kim D, Choi J, Lee NG, Deshpande V , Schwab J, Lev \nMH, Gonzalez RG, Gee MS, Do S. Tackling prediction uncer -\ntainty in machine learning for healthcare. Nat Biomed Eng. \n2023;7(6):711–8.\nPublisher's Note Springer Nature remains neutral with regard to juris-\ndictional\tclaims\tin\tpublished\tmaps\tand\tinstitutional\taffiliations.\n161. Huang A, Banerjee O, Wu K, Reis EP, Rajpurkar P. Finerad -\nscore: a radiology report line-by-line evaluation technique gen -\nerating corrections with severity scores. 2024; arXiv preprint \narXiv:2405.20613.\n162. Smit A, Jain S, Rajpurkar P, Pareek A, Ng AY , Lungren MP. \nChexbert: combining automatic labelers and expert annotations \nfor accurate radiology report labeling using bert. 2020; arXiv pre-\nprint arXiv:2004.09167.\n163. Johri S, Jeong J, Tran BA, Schlessinger DI, Wongvibulsin S, \nBarnes LA, Zhou H-Y , Cai ZR, Van Allen EM, Kim D, Daneshjou \nR, Rajpurkar P. An evaluation framework for clinical use of large \nlanguage models in patient interaction tasks. Nat Med. 2025.  h t t p \ns :   /  / d o  i . o  r  g  /  1 0  . 1 0   3 8 /  s 4 1  5 9 1 -  0 2 4 - 0  3 3 2 8 - 5.\n164. Ong Ly C, Unnikrishnan B, Tadic T, Patel T, Duhamel J, Kandel \nS, Moayedi Y , Brudno M, Hope A, Ross H, et al. Shortcut learn-\ning in medical AI hinders generalization: method for estimating \nAI model generalization without external data. NPJ Digital Med. \n2024;7(1):124.\n165. Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. \nGans trained by a two time-scale update rule converge to a local \nnash equilibrium. Adv Neural Inf Process Syst. 2017;30:65.\n166. Banerjee O, Saenz A, Wu K, Clements W, Zia A, Buensalido \nD, Kavnoudias H, Abi-Ghanem AS, Ghawi NE, Luna C, et al. \nRexamine-global: A framework for uncovering inconsistencies in \nradiology report generation metrics. In: Biocomputing 2025: pro-\nceedings\tof\tthe\tpacific\tsymposium.\tWorld\tScientific;\t2024.\tpp.\t\n185–198\n167. Wang C, Liu S, Yang H, Guo J, Wu Y , Liu J. Ethical consid -\nerations of using chatgpt in health care. J Med Internet Res. \n2023;25:48009.\n1 3\n863",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.6995667815208435
    },
    {
      "name": "Computer science",
      "score": 0.47044673562049866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45098456740379333
    },
    {
      "name": "Engineering",
      "score": 0.3334534168243408
    }
  ]
}