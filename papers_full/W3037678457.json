{
  "title": "An Accurate Ensemble Forecasting Approach for Highly Dynamic Cloud Workload With VMD and R-Transformer",
  "url": "https://openalex.org/W3037678457",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2310282863",
      "name": "Shaojing Zhou",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2126165547",
      "name": "Jinguo Li",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A1988944048",
      "name": "Kai Zhang",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2104376657",
      "name": "Mi Wen",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2467713300",
      "name": "Qijie Guan",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2310282863",
      "name": "Shaojing Zhou",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2126165547",
      "name": "Jinguo Li",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A1988944048",
      "name": "Kai Zhang",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2104376657",
      "name": "Mi Wen",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    },
    {
      "id": "https://openalex.org/A2467713300",
      "name": "Qijie Guan",
      "affiliations": [
        "Shanghai University of Electric Power"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2888030630",
    "https://openalex.org/W2345862676",
    "https://openalex.org/W2898777543",
    "https://openalex.org/W2000982976",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W6765571568",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W6638545294",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W6722346062",
    "https://openalex.org/W2891698321",
    "https://openalex.org/W1984255960",
    "https://openalex.org/W2029177847",
    "https://openalex.org/W2782132135",
    "https://openalex.org/W2406349003",
    "https://openalex.org/W2791512297",
    "https://openalex.org/W2578279218",
    "https://openalex.org/W2880593079",
    "https://openalex.org/W2992296379",
    "https://openalex.org/W2953169926",
    "https://openalex.org/W2473271044",
    "https://openalex.org/W2082410059",
    "https://openalex.org/W2986615528",
    "https://openalex.org/W2963155467",
    "https://openalex.org/W2800479622",
    "https://openalex.org/W2901607622",
    "https://openalex.org/W2587964702",
    "https://openalex.org/W2901895730",
    "https://openalex.org/W2511380230",
    "https://openalex.org/W2803075950",
    "https://openalex.org/W2969851632",
    "https://openalex.org/W2763398542",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W2969482165",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2996361803",
    "https://openalex.org/W2956480774",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2483335313",
    "https://openalex.org/W2954731415",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W1924770834"
  ],
  "abstract": "To efficiently manage the cloud resources, improve the quality of service, and avoid the violations of Service-Level Agreement (SLA) agreements, it is very important to make accurate forecast for cloud workload. Prior works concerning cloud workload forecasting are mainly designed based on Recurrent Neural Networks (RNN). However, when it comes to a highly-dynamic cloud workload scenario where resource utilization changes faster and more frequently, these RNN-based methods are not effective in obtaining the linear and non-linear relationships and cannot give accurate forecast, because classic RNN has the problem of vanishing gradient. To address this issue, we propose an Ensemble Forecasting Approach (EFA) for highly-dynamic cloud workload by applying Variational Mode Decomposition (VMD) and R-Transformer. Specifically, to decrease the non-stationarity and high randomness of highly-dynamic cloud workload sequences, we decompose the workload into multiple Intrinsic Mode Functions (IMFs) by VMD. The IMFs are then imported into our ensemble forecasting module based on R-Transformer and Autoregressive model, in order to capture long-term dependencies and local non-linear relationship of workload sequences. The effectiveness and adaptability of proposed EFA is verified on real-world workload from Google and Alibaba cluster traces. Moreover, the performance evaluation results show that the EFA performs higher forecasting accuracy than prior related works over various forecasting time lengths for highly-dynamic cloud workload.",
  "full_text": "Received June 11, 2020, accepted June 18, 2020, date of publication June 23, 2020, date of current version July 2, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3004370\nAn Accurate Ensemble Forecasting Approach\nfor Highly Dynamic Cloud Workload With\nVMD and R-Transformer\nSHAOJING ZHOU\n, JINGUO LI\n , (Member, IEEE), KAI ZHANG\n,\nMI WEN, (Member, IEEE), AND QIJIE GUAN\nCollege of Computer Science and Technology, Shanghai University of Electric Power, Shanghai 200090, China\nCorresponding author: Jinguo Li (lijg@shiep.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 61702321, Grant 61802248, and\nGrant U1936213, and in part by the Shanghai Education Development Foundation and Shanghai Municipal Education Commission\nthrough the Chenguang Program under Grant 18CG62.\nABSTRACT To efﬁciently manage the cloud resources, improve the quality of service, and avoid the\nviolations of Service-Level Agreement (SLA) agreements, it is very important to make accurate forecast\nfor cloud workload. Prior works concerning cloud workload forecasting are mainly designed based on\nRecurrent Neural Networks (RNN). However, when it comes to a highly-dynamic cloud workload scenario\nwhere resource utilization changes faster and more frequently, these RNN-based methods are not effective\nin obtaining the linear and non-linear relationships and cannot give accurate forecast, because classic\nRNN has the problem of vanishing gradient. To address this issue, we propose an Ensemble Forecasting\nApproach (EFA) for highly-dynamic cloud workload by applying Variational Mode Decomposition (VMD)\nand R-Transformer. Speciﬁcally, to decrease the non-stationarity and high randomness of highly-dynamic\ncloud workload sequences, we decompose the workload into multiple Intrinsic Mode Functions (IMFs)\nby VMD. The IMFs are then imported into our ensemble forecasting module based on R-Transformer\nand Autoregressive model, in order to capture long-term dependencies and local non-linear relationship of\nworkload sequences. The effectiveness and adaptability of proposed EFA is veriﬁed on real-world workload\nfrom Google and Alibaba cluster traces. Moreover, the performance evaluation results show that the EFA\nperforms higher forecasting accuracy than prior related works over various forecasting time lengths for\nhighly-dynamic cloud workload.\nINDEX TERMS Workload forecasting, cloud computing, deep learning, variational mode decomposition.\nI. INTRODUCTION\nOne of the main outstanding properties of cloud computing\nsystem is elasticity [1]. The elasticity implies that the sys-\ntem can automatically provision or de-provision resources\nto adapt to workload changes [2]. An efﬁcient resource\nmanagement scheme can proactively identify the possible\nresource usage patterns to predict the future workload of\nthe cloud center and provide the required resources [3].\nHowever, conducting proactive resource management is not\neasy, for instance, some hot events may attract lots of\ntrafﬁc in a short period in social networks. If the cloud\nservice providers cannot provide enough resources in time,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Quan Zou\n.\ni.e., under-provisioning, it would reduce the Quality of\nService (QoS) and violate the Service-Level Agreement\n(SLA), which would lead to customer churn. On the other\nhand, if the cloud service provider offers excess available\nresources all the time, i.e., over-provisioning, it would waste a\nlot of energy, and incur extra costs on network trafﬁc, device\ncooling and maintenance [4]. Therefore, accurate workload\nforecasting is a key factor in implementing efﬁcient proac-\ntive resource management schemes and quickly allocating\nresources to what users need [3].\nMoreover, for large-scale cloud centers, the workload pat-\nterns are very diverse and random [5], which makes it chal-\nlenging to compute accurate forecast results on workload.\nAnalysis of the Alibaba cluster dataset shows that the CPU\nusage of the entire cluster changes from 5% to 80% in a day\n115992 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nwith high ﬂuctuations [6]. Similarly, in the Google Cloud\nData Center, changes in CPU, memory, and other resources\nare highly dynamic and random [7]. Therefore, an accurate\nworkload forecasting approach is required. The approach can\neffectively capture the linear and non-linear correlations of\nworkload, and can be adapted to highly variable workload.\nIn addition, corresponding to the randomness of the work-\nload, the characteristics of the original workload data should\nbe further analyzed and extracted. The goal of our research\nis to extract characteristics from the historical sequence of\nworkload and accurately forecast future workload changes.\nIn recent years, there have been many studies on cloud\nworkload forecasting [12]–[25]. However, most of these\nresearches are based on traditional regression methods or\nclassic machine learning methods. In general, they have\naccurate results only in speciﬁc scenarios, such as when\nworkload sequences have signiﬁcant periodicity or regularity.\nSpeciﬁcally, the traditional back-propagation neural network\nhas considerable ability to capture the non-linear features of\nthe sequence, but it does not make full use of the correlation\nbetween neurons [8], and it is mostly used for low variance\ncloud workload. In addition, in the cloud workload fore-\ncasting scenario, deep learning methods based on Recurrent\nNeural Networks (RNN) [9] have become popular due to\nits excellent ability to process sequence data. RNN is very\nsuitable to forecast random workload, but traditional RNNs\ncannot accurately capture the dependency information of long\nsequences because of the vanishing gradient problem. The\nvanishing gradient means that when RNN is used to train\nlong-sequence data, the weights of the previous neural units\nin the network cannot be updated, which eventually leads to\nthe failure of network training [10], [11]. Variants of RNN,\nsuch as Long Short-Term Memory network (LSTM) [12] and\nGated Recurrent Unit (GRU) [13], have been proposed to\nsolve this problem. However, these RNN-based deep learning\nmethods cannot give accurate forecast results when it comes\nto the highly-dynamic cloud center workload.\nTo address the above mentioned problems, we proposed an\nEnsemble Forecasting Approach (EFA) for highly-dynamic\ncloud workload based on Variational Mode Decomposition\n(VMD) and R-Transformer. The main contributions of this\npaper are summarized as follows:\n• In order to decrease the non-stationarity of highly-\ndynamic cloud workload sequences, the Variational\nMode Decomposition is used to preprocess workload\ndata sequence and decompose it into multiple Intrin-\nsic Mode Functions (IMFs) with good characteristics.\nIn particular, nonlinear features in the original sequence\nare also effectively extracted.\n• In order to overcome the shortcomings of RNN, IMFs\nwill be input to an ensemble cloud workload forecast-\ning module based on R-Transformer and Autoregres-\nsive model. This module uses LocalRNN to obtain the\nlocal non-linear relationship of the sequences, and cap-\ntures long-term dependencies through the multi-head\nattention mechanism. Moreover, The Autoregressive\nmodel can obtain the linear relationship of workload\nand improve the robustness and forecasting accuracy of\nproposed EFA.\n• To demonstrate the effectiveness of the proposed\nEFA, we apply this methodology to real-world cloud\ncenter workload datasets from Google’s cluster and\nAlibaba’s cluster. The experimental results show that\nthe proposed approach can capture local and global\ninformation and achieve higher forecasting accuracy\nthan prior related works over various forecasting time\nlengths.\nThe rest of the paper is organized as follows. Section II\nreviews the most relevant related works to cloud workload\nforecasting. Section III describes the system architecture of\nEFA. In Section IV, the proposed EFA is discussed in detail.\nPerformance evaluation is presented in Section V. Finally,\nSection VI concludes this paper.\nII. RELATED WORK\nAt present, there are many researches on workload forecast-\ning in cloud computing, which can be divided into traditional\nregression models, classical machine learning methods and\ndeep learning models. They all extract patterns from histori-\ncal workload data to forecast future changes.\nA. TRADITIONAL REGRESSION METHODS\nTraditional regression methods mainly include Autoregres-\nsive (AR), Moving Average (MA), Autoregressive Moving\nAverage (ARMA), Autoregressive Integrated Moving Aver-\nage (ARIMA). Hu et al. [14] proposed an Autoregressive-\nbased approach to forecast the workload, but the model is\nstrictly linear in nature and lacks adaptability to workload\nin complex cloud environments. Amekraz and Hadi [15]\nused an adaptive ARMA model and validated it on a web\ndataset. However, this model is limited to cloud environments\nthat have signiﬁcant periodicity. Calheiros et al.[16] used a\npredictor based on the ARIMA to proactively conﬁgure vir-\ntual machine (VM) instances. They demonstrated ARIMA’s\nability to perform short-term forecasting and used the results\nto dynamically provision resources for SaaS applications.\nSaripalli et al.[17] proposed a two-step approach, including\nload trace (LT) and load forecasting (LF). The proposed\ncubic-spline LT can model the high ﬂuctuations of the loads\nbetter than the other linear LTs based on Moving Average.\nIn another work, Guo et al. [18] proposed a type-aware\nworkload forecasting strategy, which dynamically determines\nthe type of workload and relies on its type to adaptively switch\nforecasting algorithms.\nTraditional regression methods make time series forecast-\ning model less complex compared to other methods such\nas artiﬁcial neural networks. Although the model based on\nregression methods are simple, they are based on oversim-\npliﬁed assumptions (linear relationships) of workload [4].\nTherefore, they are unable to capture nonlinear changes in\nworkload.\nVOLUME 8, 2020 115993\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nB. CLASSICAL MACHINE LEARNING METHODS\nClassic machine learning technology has been widely used\nin many large and complex data-intensive ﬁelds, especially\nin the ﬁeld of data processing and workload forecasting [19].\nThe classical machine learning methods mainly include\nMarkov-based models, Bayesian models, support vector\nregression (SVR), decision tree, and traditional artiﬁcial\nneural networks. Ghobaei-Arani et al.[20] proposed a hybrid\napproach using the Markov Decision Process (MDP) model.\nThe Markov decision process is present with ﬁnite states\nand transitions among states. Shyam and Manvi [21] pro-\nposed a Bayesian model to determine short and long-term\nvirtual resource requirement of the CPU/memory intensive\napplications on the basis of workload patterns at several\ndata centers in the cloud during several time intervals.\nSingh et al. [22] proposed an adaptive forecasting model\ncalled TASM by using linear regression, ARIMA, and SVR\nfor web applications, and they proposed a workload classiﬁer\nthat can select models based on workload characteristics.\nRahmanian et al. [23] proposed an ensemble cloud\nresources usage forecasting algorithm based on Learning\nAutomata (LA) theory. The algorithm employed two\nmethods, namely Single Window (SW) and Multiple Window\n(MW) for cloud resource forecasting.\nMost traditional machine learning or classic artiﬁcial neu-\nral network workload forecasting methods do not require\nrestrictive assumptions about the form of workload, and can\nextract non-linear characteristics of workload. However, they\nrequire workload with obvious regularity or trends to achieve\naccurate forecast because they rely mostly on heuristic\nalgorithms.\nC. DEEP LEARNING METHODS\nOver the past few years, researchers have applied several\ndeep learning algorithms to forecast workload in the cloud,\nmainly including Recurrent Neural Network (RNN), Long\nShort-Term Memory network (LSTM), Convolutional Neural\nNetwork (CNN) and Deep Belief Network (DBN) [9].\nDuggan et al.[24] used the classic RNN architecture to fore-\ncast the future workload of cloud data centers. It turned out\nthat RNN can work well when coping with short-term depen-\ndencies. Zhu et al. [25] proposed a forecasting approach\nusing attention-based LSTM encoder-decoder network. The\nmethod extracts the characteristics of historical workload data\nthrough an encoder. They integrate an attention mechanism\nin the decoder to obtain the weight of predictions at different\nhistorical time steps. [26] designed and proposed an improved\nLSTM-based model N-LSTM to solve the problem of virtual\nmachine workload forecasting, and it could make forecast at\nirregular time intervals. Guo and Yao [27] proposed a method\nfor workload prediction based on GRU [13]. The model can\nlearn temporal patterns and long-term dependencies of large\nsequences of arbitrary length, and the model training time\nis shorter than that of LSTM. Zhang et al.[28] proposed an\nefﬁcient deep learning model based on the canonical polyadic\ndecomposition to forecast the cloud workload for industry\ninformatics. They compressed the parameters signiﬁcantly\nby converting the weight matrices to the canonical polyadic\nformat, and designed a learning algorithm to train the\nparameters.\nHowever, most deep learning algorithms, including LSTM\nand GRU, are designed based on RNN architecture. Although\nthese algorithms are excellent at capturing non-linear infor-\nmation for cloud workload, they cannot adapt to workload\nwith high dynamics and high random, which might lead to\nlow accuracy and high computational complexity for work-\nload forecasting.\nIII. SYSTEM ARCHITECTURE\nIn order to accurately and efﬁciently adjust resources in a\ncloud environment, cloud service providers need to forecast\nworkload changes in the future based on historical workload\ndata. In response to the highly-dynamic workload, we pro-\npose the EFA in cloud systems. The details of EFA are shown\nin Fig. 1.\nFIGURE 1. The architecture of proposed EFA in cloud systems. The data\npreprocessing part uses VMD to decompose the normalized historical\nworkload, the forecasting module consists of R-Transformer and\nAutoregressive model.\n115994 VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nFirst, the monitoring system in the cloud center records the\nworkloads of the system in real time. In proposed EFA, these\nhistorical workloads are from the monitoring system. Then,\nhistorical data is processed using Z-score normalization and\nVMD, which will be explained in detail in Section IV-A.\nThe results are considered as inputs to the ensemble fore-\ncasting module. Next, use the ensemble forecasting module\ncomposed of R-Transformer and Autoregressive model to\nforecast different IMF, and we will explain this process in\nSection IV-B. Finally, the forecasting results of each IMF are\nadded to obtain the ﬁnal forecasting result, and this result is\ninput to the resource management system of the cloud center\nfor resources allocation or recovery.\nGenerally, cloud workload data includes various indica-\ntors about the operating status of the system, like CPU and\nmemory usage, disk space, disk I/O time, bandwidth, etc.\nCPU load can be regarded as the most important and limited\nresources in a computer system, and also the main bottleneck\nin cloud platforms [29]. Therefore, the industry regards CPU\nusage as a key factor in improving the resource allocation of\ncloud centers.\nIn this paper, we focus on CPU usage. In order to capture\ninformation from historical data, the historical CPU usage of\nthe cloud center can be presented in the form of a time series\n⃗x =(x1,x2,..., xt ) which is a sequence of recorded values\narranged in chronological order with constant time intervals,\nand the CPU usage record value at time t is xt . The EFA\nuse sliding window forecasting method to predict future CPU\nusage xt+h based on ⃗x, where h is the forecasting length ahead\nof the current time stamp t. Moreover, the EFA predict the\nfuture CPU usage of xt+h+n based on (x 1+n,x2+n,..., xt+n),\nn ∈R+.\nIV. THE ENSEMBLE FORECASTING APPROACH FOR\nCLOUD WORKLOAD\nThis section presents the proposed EFA, an ensemble fore-\ncasting approach for cloud workload. In Section IV-A we\ndescribe the process of data preprocessing. In Section IV-B,\nthe ensemble forecasting module is described in detail.\nA. DATA PREPROCESSING\nIn this section, details of data preprocessing is given as\nfollows. We will detail the process of data preprocessing,\nincluding Z-score normalization and VMD.\n1) Z-SCORE NORMALIZATION\nAccording to the actual highly-dynamic cloud workload,\nthe value of CPU usage varies greatly in different time inter-\nvals, so the original time series datas need to be normalized.\nIn addition, this can also accelerate the convergence of deep\nlearning algorithms. Considering the large range of ﬂuctu-\nations in cloud workloads, such as in the Google cluster\ndataset, where most of the data values are in a small range,\nusing Min-Max normalization would cause these large num-\nbers of smaller values to be concentrated in some small\nrange, which is not conducive to training and convergence of\nthe forecasting module. Therefore, in this part, Z-score nor-\nmalization is used to normalize the original sequences. The\nprocessed data conforms to the standard normal distribution,\nthat is, the mean is 0 and the standard deviation is 1. The\nformula for Z-score normalization is below:\nx′=x −mean(X)\nσ , (1)\nwhere X is the set of x, mean(X) is the mean value of X and\nσ is the standard deviation\n√\nE((X −E(X))2) of X.\n2) VARIATIONAL MODE DECOMPOSITION\nThe VMD is a non-recursive signal processing algorithm,\nthe purpose of VMD is to decompose an input signal into\nk discrete number of sub-signals (modes), where each mode\nhas limited bandwidth in spectral domain [30]. Each mode\ncan be compacted around a center pulsation ωk determined\nduring the decomposition process. To obtain the bandwidth\nof a one dimension signal, three steps should be fulﬁlled [30]:\nStep 1, for each mode, adopt Hibert transform to obtain a\nunilateral frequency spectrum. Step 2, for each mode, transfer\nthe mode’s frequency spectrum to the baseband by apply-\ning an exponential tuned to the respective estimated center\nfrequency. Step 3, for each mode, estimate the bandwidth\nby utilizing the Gaussian smoothness of the demodulated\nsignal. The constrained variational problem can be given as\nfollows [30]:\nminuk ,ωk\n{∑\nk\n∂t\n[(\nδ(t) + j\nπt\n)\n∗uk (t)\n]\ne−jωk t\n\n\n2\n2\n}\n,\ns.t.\n∑\nk\nuk =f , (2)\nwhere ∂is the Dirac distribution, f is the original signal and\nu is its mode, uk denotes the k-th mode, ωk is the center\nfrequency, t is time script, and ∗is the convolution operator.\nThe mode u with high-order k represents low frequency\ncomponents.\nThe components obtained after using VMD are often called\nIntrinsic Mode Function (IMF) signals. In the context of this\npaper, they can also be seen as a set of different time series.\nCompared with the original time series, the instability of\nIMFs is reduced, and the information contained in each IMF\ncovers different parts of the original time series [31].\nIn the proposed EFA, VMD is used to decompose\nhighly-dynamic cloud workload data into stable and pre-\ndictable IMFs. Similar to [32] and [33], each IMF can be\nregarded as a new time series. Usually the ﬁrst IMF contains\nthe low-frequency part of the original workload sequence,\nwhich can be regarded as a smooth trend change of the\noriginal sequence. The high-frequency part is included in the\nremaining IMF, which can accurately identify small details\nof the original sequence. Each IMF is independently input\ninto the ensemble forecasting module, which will output the\nforecasting result of this IMF. The forecasting results of these\nIMFs are added to obtain the ﬁnal workload forecast result.\nVOLUME 8, 2020 115995\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nB. ENSEMBLE WORKLOAD FORECASTING MODULE\nIn this part, we continue to describe the proposed ensemble\nforecasting module in detail, which can make accurate fore-\ncast for the IMFs from the previous part.\n1) R-TRANSFORMER\nModels based on RNN and its variants such as LSTM\nare widely used in the ﬁeld of time series forecasting.\nHowever, these models cannot capture long-term depen-\ndencies well and cannot perform parallel calculations on\nsequences. Transformer [34] has been proposed and has\nproven to be extremely efﬁcient at capturing long-term\ndependencies in various sequence modeling tasks especially\nin NLP [35]. Nevertheless, standard Transformer is not very\nsuitable for time series forecasting, this is because it highly\nrelies on position embedding for solving the loss of position\nsequence information, and it also lacks necessary components\nto model local structures in sequences, which can make the\nmodel prone to anomalies in time series [36], [37]. Inspired\nby [36], we use R-Transformer to obtain local and global\ninformation of time series. R-Transformer takes the advan-\ntages of RNN and multi-head attention mechanism while\navoiding their respective shortcomings.\nThe architecture of R-Transformer is shown in Fig. 2.\nThe R-Transformer is introduced in detail from three parts:\nLocalRNN layer, Multi-Head Attention layer and Feed-\nforward layer.\nFIGURE 2. The architecture of one layer of R-Transformer. In particular,\nLocalRNN layer captures the local short-term dependencies; Multi-head\nattention layer captures the global long-term dependencies;\nFeed-forward layer performs non-linear feature transformation.\nStep 1:LocalRNN reorganizes the original long workload\nsequence into many short sequences, so these short sequences\ncontain only local information. These local sequences are\nprocessed independently and identically using RNN that\nshare weights. Speciﬁcally, LocalRNN constructs a local\nwindows of size M for each target position, so that the local\nwindow includes M consecutive positions and ends at the tar-\nget position, and each window contains only local short-term\ndependencies. It is worth noting that M is always smaller than\nthe length of the original long sequence. Fig. 3 shows how\nLocalRNN works.\nFIGURE 3. LocalRNN obtains local correlation information, it only\noperates on positions within a local window.\nConcretely, given the positions (x t−M+1,xt−M+2,..., xt )\nof local sequences of length M. LocalRNN processes these\nshort sequences and outputs M hidden states, then uses the\nlast hidden states as a feature of the local sequence:\nht =LocalRNN(xt−M+1,xt−M+2,..., xt ), (3)\nwhere t is the target time stamp, and RNN represents any\nRNN unit, such as LSTM and GRU. For an entire long\nworkload sequence, to ensure that the hidden state represen-\ntation of each time step is obtained and does not contain\nany future information, LocalRNNLayer ﬁrst performs zero\npadding of length M −1 before the start of the sequence [36].\nThen LocalRNN slides each window and outputs a hidden\nrepresentation sequence containing local information:\nh1,h2,..., hT =LocalRNNLayer(x1,x2,..., xT ), (4)\nwhere T denotes the length of input sequence. Then, the rep-\nresentation of local hidden states is input to multi-head atten-\ntion layer to capture long-term global dependencies.\nStep 2:According to recent works, the multi-headed atten-\ntion mechanism is very effective at learning the long-term\ndependencies of sequences, as it can establish a direct connec-\ntion between each pair of positions. The multi-head attention\nmechanism relies on scaled dot-product attention [34]:\nAttention(Q,K,V ) =softmax(QKT\n√dk\n)V , (5)\namong them, Q, K and V are the matrix of query, key and\nvalue, and dk is the key dimensionality. The queries, keys and\nvalues are from the output of LocalRNN layer.\nThe multi-head attention mechanism obtains h different\nrepresentations of (Q, K,V ) respectively, h can also be\nregarded as the number of heads, then calculates a scaled\ndot-product attention of each representation, and ﬁnally con-\ncatenates the results. Speciﬁcally, the current representations\n115996 VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nh1,h2,..., hT is input into the multi-head attention layer, and\nthe new representation ut is calculated as follows:\nut =MultiHeadAttention(h1,h2,..., hT )\n=Concatenation(head1(hT ),head2(hT ),\n..., headi(hT ),..., headh(hT ))W o, (6)\nwhere headi(hT ) is the result of ith attention head:\nheadi =Attention(QW Q\ni ,KW K\ni ,VW V\ni ), (7)\nwhere the W o and Wi are parameter matrices and each atten-\ntion head headi has its own mapping matrices Wi. As can\nbe seen from Eq. (6), let hT participate in all past positions\nto get each headi, so any long-term dependencies can be\ncaptured. And different heads can focus on obtaining different\ncorrelations.\nStep 3:The layer behind the multi-head attention layer is\nthe feed-forward layer, which is used to linearly transform\nfeatures. Feed-forward layer is a fully connected layer, and it\nis applied to each position separately and identically. It con-\nsists of two linear transformations and uses a ReLU activation\nfunction between them. Feed-forward layer is deﬁned as\nfollows:\nmt =FeedForward(ut )=max(0,ut W1 +b1)W2 +b2, (8)\nwhere mt is the output of the feed-forward layer, W1 and W2\nare two different parameter matrices, b1 and b2 are the biases.\nFinally, according to [34], a residual [38] and layer-\nnorm [39] connection is added between all sub-layers of\nR-Transformer.\n2) AUTOREGRESSIVE MODEL\nDue to the non-linear nature of both Recurrent and\nMulti-Head Attention components, the scale of neural net-\nwork model output is not sensitive to the scale of input. And\nin cloud systems, the scale of workload constantly changes in\na non-periodic manner, which greatly reduces the forecasting\naccuracy of forecasting models. To address the shortcoming,\nwe use a mixture of linear and non-linear components as the\nﬁnal forecasting result. In our forecasting module, the classic\nAutoregressive model [40] is used as the linear component,\nand the Autoregressive model is formulated as follows:\nlt =\nT −1∑\nk=0\nW ar\nk xt−k +bar , (9)\nwhere lt denote the forecasting result of Autoregressive com-\nponent, W ar is the parameters matrix, T is the length of input\nwindow.\nThe ﬁnal forecast result of the forecasting module is a com-\nbination of the outputs of R-Transformer part and Autoregres-\nsive part:\nˆYt =mt +lt , (10)\nwhere ˆYt denotes the ﬁnal forecast result of the forecasting\nmodule at time stamp t.\nOverall, the EFA consists of two parts: data preprocessing\nand forecasting module. (1) The data preprocessing part can\nstandardize the highly dynamic workload of cloud center\nand use VMD to divide the original sequence into different\ncomponents (IMFs), which contain the low-frequency and\nhigh-frequency information of the original sequence. After\nusing VMD, the interference between different types of infor-\nmation is reduced. (2) The forecasting module composed\nof R-Transformer and AR model can accurately extract the\nhidden representations of these components. Among them,\nR-Transformer uses LocalRNN and the multi-head attention\nmechanism to obtain the local and global nonlinear relation-\nship of the IMFs, and the AR model is used to obtain the\nlinear relationship of the IMFs. The combination of these\ncomponents effectively improves the accuracy of forecasting\nhighly dynamic workload in cloud environment.\nV. PERFORMANCE EVALUATION\nIn this section, we evaluate the performance of the proposed\nEFA using two real-world cloud center datasets and compare\nit with RNN-based workload forecasting models and other\nclassic methods [16], [25], [27], [41].\nA. DATASETS\nTwo real-world datasets are used in the experiments. The ﬁrst\none is Google cluster trace dataset [42], which contains the\nrunning information approximately an 12.5k-machine cluster\nwith a span of 29 days during May 2011. The second one\nis Alibaba cluster trace dataset [6], which includes about\n4000 machines with the runtime resource usage in a period\nof 8 days. In the experiments, we used CPU usage (also\nknown as CPU rate) as the primary performance index of\nworkload.\nIn addition, to ensure the efﬁciency and generality of the\nforecasting methods, two different data types are used for\nvalidation. Speciﬁcally, for the Google dataset, the target\nof forecast is the sum of CPU usage of all machines used\nby a single task. For example, a long-running job with an\nID of 6176858948, the job called multiple machines at dif-\nferent times within 29 days. Fig. 4(a) shows the sum of\nthe CPU usage of all these machines at each time point.\nFor the alibaba dataset, we forecast the CPU usage of each\nindividual machine, such as a machine with ID 649 (as shown\nin Fig. 4(b)). As we can see from Fig. 4, Google workload\nappears more random and has no obvious periodicity, but\nits peak is very signiﬁcant and more difﬁcult to forecast.\nAlibaba workload displays periodicity, and it varies from 20%\nto 80% with more frequent changes. Fig. 5 shows the origi-\nnal sequence and its IMFs decomposed by VMD (Alibaba\ndataset, 3 IMFs).\nB. EXPERIMENTAL SETTING\n1) BASELINE METHODS\nIn order to verify the effectiveness of the EFA, several base-\nline methods are chosen for comparison as follows:\nVOLUME 8, 2020 115997\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nFIGURE 4. Examples of CPU usage from Google and Alibaba cluster trace datasets. For (a) Google dataset, the CPU usage is the sum of all machines\nused by one task. For (b) Alibaba dataset, the CPU usage is from one single machine.\nFIGURE 5. Use VMD to decompose a sample of the Alibaba dataset. IMF1\ncan be regarded as a smooth trend change of the original sequence,\nwhich belongs to the low frequency part. IMF2 and IMF3 are high-\nfrequency parts that contain small details of the original sequence.\nARIMA [16]: Autoregressive integrated moving aver-\nage (ARIMA) model is a traditional statistical model for\ntime series forecasting. ARIMA converts non-stationary time\nseries into stationary time series data by using d-order differ-\nence method. The autocorrelation and partial autocorrelation\nplots of the historical data are analyzed to determine the order\np for the lags of the autoregressive model and the order q\nfor the lags of the moving average model. Finally, ARIMA\nuses the determined parameters p, d, and q to predict future\nworkload values.\nGRU [27]: The basic gate recurrent unit (GRU) [13] net-\nwork is a recurrent neural network using GRU cell as the\ncalculation unit. The historical data will be input to the\nmulti-layer GRU network, and the last output value of the last\nlayer will be taken as the forecast value.\nLSTM-ED [41]: The long-short term memory (LSTM)\nencoder-decoder network is similar to the sequence-\nto-sequence model [43]. LSTM-ED uses LSTM as the basic\ncalculation unit. The historical data is fed into the LSTM\nencoder network sequentially, and the hidden state and cell\nstate at the end of the encoder are sent to the context vector.\nThe decoder network composed of LSTM outputs the pre-\ndicted sequence by iteratively decoding the context vector.\nLSTM-ED with Attention [25]: Attention based LSTM\nencoder-decoder network introduces attention mechanism on\nLSTM-ED. The attention module is part of the decoder net-\nwork. This module calculates the attention weight of the\noutput of the encoder and the current time step in the decoder.\nEach weight represents the impact of historical workload on\nthe current workload.\n2) PARAMETERS SETTING\nAll methods are implemented in Python 3.6, where the neural\nnetwork and deep learning methods are implemented using\nPyTorch 1.3.1 [44]. The experiments are performed on a\nmachine with Intel Core i7-7800X CPU, NVIDIA GeForce\nRTX 2080 Ti GPU, and 24 GB RAM.\nAll datasets are divided into training set (80%), validating\nset (10%) and testing set (10%). The training set is used for\nforecasting module training, the validating set is used for\nparameters selection, and the testing set is used to evaluate\nthe performance. Mean square error (MSE) is selected as the\nloss function during training.\nFor the proposed EFA, the Adam optimizer is used as the\ntraining optimizer and GRU is selected as the calculation\nunit of LocalRNN. We use grid search to select the length\nof the history window, the hidden state dimension, the win-\ndow length of LocalRNN and the number of heads h in\nthe multi-head attention. In detail, search the history win-\ndow length in {12, 18, 24, 30, 36}, search the hidden state\ndimension in {64, 128, 256, 512}, search the window length\nof LocalRNN in {3, 4, 5, 6} and search for the value of h\nin {4, 8, 12}. In addition, the forecasting length of Google\ndataset is ﬁxed at 10 minutes, which is 2 time steps while the\nforecasting length of Alibaba dataset is ﬁxed to 3 minutes,\nwhich is 3 time steps. The number of IMFs in both datasets\nis set to 3. The impact of forecasting length and number of\n115998 VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nTABLE 1. Hyper parameters of the proposed EFA.\nIMFs on performance will be discussed in Section V-D. Some\nother parameters, such as batch size and learning rate, are\ndetermined based on some tuning. The hyper parameters of\nproposed EFA for Google and Alibaba datasets are shown\nin Table 1 in detail.\nThe parameters of the comparison methods are determined\nbased on previous works [16], [25], [27], [41] and some tun-\ning to ensure that each method exhibits its best performance.\nWe perform ten experiments on each method and take the\naverage value as the ﬁnal result.\n3) EVALUATION METRICS\nTo assess the approach considered in this paper, we used three\nconventional evaluation metrics deﬁned as follows.\n• Mean Squared Error (MSE):\nMSE =1\nn\nn∑\ni=1\n(Yi −ˆYi)2 (11)\n• Root Relative Squared Error (RRSE):\nRRSE =\n√∑n\ni=1(Yi −ˆYi)2\n√∑n\ni=1 (Yi −mean(Y ))2\n(12)\n• R-Squared (R2):\nR2 =1 −\n∑n\ni=1(Yi −ˆYi)2\n∑n\ni=1 (Yi −mean(Y ))2 (13)\nwhere Y , ˆY are original values and forecast values, respec-\ntively. The RRSE is a scaled version of the Root Mean\nSquare Error (RMSE) that makes the evaluation results more\nreadable at any data scale [40]. For MSE and RRSE, lower\nvalues are better, and for R2, higher value is better.\nC. EVALUATION RESULTS\nTable 2 shows the performance of the proposed EFA and\nbaseline methods on Google and Alibaba datasets. Fig. 6\nshows the forecasting curves of these methods on the Google\nand Alibaba datasets. As mentioned in the parameter settings\nabove, for the Google dataset, the default sampling interval\nis 5 minutes and the forecasting length is 10 minutes. For\nthe Alibaba dataset, the default sampling interval is 1 minute,\nand the forecasting length is 3 minutes. It can be seen from\nTable2 that on the Google and Alibaba datasets, the four deep\nlearning based methods (GRU, LSTM-ED, LSTM-ED with\nAttention and our proposed EFA) are better than traditional\nstatistical method (ARIMA) in terms of forecasting accuracy.\nFig. 6 shows that ARIMA only forecasts future trends and\ncannot capture sudden changes in cloud workloads. We found\nthat although the ARIMA model is widely used and has\na perfect theoretical basis, it is difﬁcult to adapt to the\nhigh dynamic load in the cloud environment, resulting in\nan increase in forecasting error. For deep learning based\nmethods, the performance of the network using encoder-\ndecoder structure and EFA is better than basic GRU. Because\nthe encoder-decoder structure can extract hidden features of\nthe entire sequence. In addition, the attention-based encoder-\ndecoder network (LSTM-ED with attention) can pay attention\nto the impact of each time step on the forecasting results,\nenhancing the basic encoder-decoder network. Regarding the\nproposed EFA, VMD decomposes the sequence into multiple\nIMFs for independent forecast, reducing the interference\nbetween different IMFs. And R-Transformer can not only\nextract hidden information in a small local window, but\nalso use the multi-head attention mechanism to obtain the\nTABLE 2. The evaluation results on Google and Alibaba dataset.\nVOLUME 8, 2020 115999\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nFIGURE 6. The forecasting performance of the baseline methods and proposed EFA on Google and Alibaba datasets. The EFA can make more\naccurate forecasting in the face of each time step that changes significantly. This shows that the EFA can adapt to random changes in highly dynamic\ncloud workloads.\nglobal information of the entire sequence, which signiﬁ-\ncantly improves the accuracy of the forecasting results. More\nimportantly, VMD separates the low-frequency and high-\nfrequency parts of the original sequence. The low-frequency\npart contains the trend and seasonal characteristics of the\nsequence, and the high-frequency part contains the details\nof the sequence. In EFA, the forecasting module trains these\nIMFs containing different information separately, which can\neffectively improve the forecasting accuracy. For example,\nin Fig. 6(a), there was a small drop before the CPU usage\nsurged, and only the EFA predicted this change.\nFig. 7 shows the average training time (each epoch training\ntime) and total training time of each method on the Google\nand Alibaba datasets. In particular, ARIMA is not like deep\nlearning methods that need to update the weights in an iter-\nations manner (training), so the ARIMA is omitted from the\ncomparison of training time. It can be seen that the fastest\ntraining speed of a single epoch is the GRU model, which has\na simple structure. The single epoch training of LSTM-ED\nwith Attention model is the slowest. Although the EFA also\nFIGURE 7. Comparison of training time for baseline methods and the\nproposed EFA on Google and Alibaba dataset. Sub-figure (a) shows the\ntraining time of each epoch of these methods, and sub-figure (b) shows\nthe total training time of these methods. For the Google dataset,\nthe number of training epochs of baseline methods is 500. For the Alibaba\ndataset, the number of training epochs of baseline methods is 400.\nhas an attention mechanism, its LocalRNN part uses a GRU\nunit with shared weights, which is faster than LSTM, so a\nsingle epoch training speed is faster than LSTM-ED with\nAttention. Since EFA requires separate training for different\nIMFs, the total training time is slower than the baseline meth-\nods. In addition, we found that IMF converges faster than the\noriginal sequence during training, so the number of epochs\nfor a single IMF can be less, which has a positive effect on\nreducing the total training time of EFA.\nD. DISCUSSION ON THE NUMBER OF IMFs AND\nFORECASTING LENGTH\nIn this part, we use VMD to decompose the original workload\nsequence into {0, 2, 3, 4, 5, 6} IMFs respectively, and explore\nthe impact of the number of IMFs on the forecasting results\nand forecasting efﬁciency. All experiments used the Google\ncluster dataset. The model parameters are consistent with\nTable 1 except for the number of IMFs, and the predicted\nlength is 10 minutes.\nTable 3 shows the variance of each IMF and the total train-\ning time of the corresponding model under different number\nof IMFs. The variance of the original workload sequence\nis 5.075. The variance of each IMF after using VMD is\nlower than that of the original sequence, which shows that\nVMD reduces the randomness and volatility of the original\nsequence. Moreover, the greater the number of IMFs decom-\nposed by VMD, the smaller the mean variance of IMFs, which\nfacilitates the training of deep learning models and achieves\nmore accurate forecasting results. Fig. 8 shows the effect of\ndifferent number of IMFs on the model forecasting results\n(measured using MSE). When VMD is not used to process\nthe original data, the forecasting results is the worst. After\nusing VMD, the forecasting accuracy has been signiﬁcantly\nimproved. And as the number of IMFs increases, the MSE\nwill become smaller. But the cost of improving forecasting\naccuracy is longer training time, because the greater number\nof IMFs means training more models.\n116000 VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nTABLE 3. The variance of each IMF and the influence of the number of\nIMFs on the total training time.\nFIGURE 8. The mean square error of the forecasting results for different\nnumber of IMFs. 0 means no VMD is used.\nIn order to study the forecasting accuracy at different\nforecasting lengths, more cases are discussed in Table 4.\nFor the Google dataset, the default sampling time length is\n5 minutes, and we resample it to 10 minutes and 30 minutes.\nSimilarly, for the Alibaba dataset, the default sampling time\nis 1 minute, and we resample it to 5 minutes and 10 minutes.\nThe experimental results were evaluated using mean square\nerror (MSE), and other parameters are the same as in Table 1.\nFrom Table 4, we can see that with the same sampling\nrate, the model that predicts the longer the future has a higher\nMSE. This means that the model becomes worse when deal-\ning with longer forecasting intervals. At different sampling\nrates, overall, the proposed EFA can maintain a low MSE\nlevel, which means that if the original sequence is resampled,\nor using a larger time interval during data collection, the fore-\ncasting accuracy can still be guaranteed.\nTABLE 4. Mean square error of different forecasting lengths at different\ntime sampling rate on Google and Alibaba datasets.\nE. ABLATION STUDY\nTo demonstrate the effectiveness of our approach design,\nwe perform an ablation study. Speciﬁcally, in order to verify\nthat the improvement in forecast accuracy comes from each\ncomponent rather than a speciﬁc hyper-parameter, we remove\neach component one at a time in the proposed EFA and\ncompare it with the complete EFA. In addition, we replaced\nR-Transformer component with GRU to prove the ability of\nR-Transformer to extract local and global nonlinear informa-\ntion. We name these models as follows:\n• Model w/o VMD: The model without the VMD compo-\nnent.\n• Model w/o AR: The model without the Autoregressive\ncomponent.\n• Model with GRU: The model without the R-Transformer\ncomponent and replace it with a basic GRU network.\nThe ablation study results are shown in Fig. 9. It can be\nseen that no matter which proposed component is removed\nFIGURE 9. The ablation study results on Google and Alibaba datasets.\nVOLUME 8, 2020 116001\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\nfrom the model, performance will decrease. For example,\nfor the 15-minute forecast length of the google dataset,\nusing GRU instead of R-Transformer raises the MSE from\n0.00838 to 0.00885. We have discussed the effect of VMD\nin Section V-D. Similarly, for the Google dataset, in the case\nof a forecasting length of 15 minutes, we get worse results\nwith the MSE of 0.00923 without VMD, and the same is\ntrue for all results, which shows that VMD can decrease the\nnon-stationarity of highly-dynamic cloud workload and this\nis beneﬁcial for forecasting module to extract correlations.\nOverall, the proposed EFA can achieve stable and competitive\nresults on different datasets.\nVI. CONCLUSION\nIn this paper, we propose a novel Ensemble Forecasting\nApproach (EFA) for highly-dynamic cloud workload.\nTo reduce the impact of high variance and unstable workload\non forecast accuracy, we use VMD to decompose workload\ninto multiple IMFs. To extracted the nonlinear correlation of\nthe input IMF, we further develop a forecasting module com-\nposed of R-Transformer, which uses LocalRNN and multi-\nhead attention mechanism to obtain the local and global inter-\nnal representation of sequences. In addition, we also added an\nautoregressive model in parallel with R-Transformer to obtain\nthe linear correlation of IMF to improve the robustness of the\nEFA. Compared with existing forecasting methods, simula-\ntion results have demonstrated the effectiveness of proposed\napproach on real-world datasets from multiple large-scale\ncloud centers. Finally, we discussed in detail the impact of\ndifferent numbers of IMFs and the length of the forecasting\ntime on the forecasting results.\nOne open problem is how to use a non-decomposition and\nefﬁcient method to reduce the instability and randomness of\nhighly-dynamic workload data, which is a good solution for\nreducing the training cost of deep neural networks.\nREFERENCES\n[1] S. R. Ali, ‘‘Cloud computing reliability analysis,’’ in Next Generation\nand Advanced Network Reliability Analysis. Cham, Switzerland: Springer,\n2019, pp. 157–187.\n[2] A. Ullah, J. Li, Y . Shen, and A. Hussain, ‘‘A control theoretical view\nof cloud elasticity: Taxonomy, survey and challenges,’’ Cluster Comput.,\nvol. 21, no. 4, pp. 1735–1764, Dec. 2018.\n[3] M. Masdari and A. Khoshnevis, ‘‘A survey and classiﬁcation of the work-\nload forecasting methods in cloud computing,’’ Cluster Comput., pp. 1–26,\nDec. 2019.\n[4] M. Amiri and L. Mohammad-Khanli, ‘‘Survey on prediction models of\napplications for resources provisioning in cloud,’’ J. Netw. Comput. Appl.,\nvol. 82, pp. 93–113, Mar. 2017.\n[5] I. S. Moreno, P. Garraghan, P. Townend, and J. Xu, ‘‘Analysis, modeling\nand simulation of workload patterns in a large-scale utility cloud,’’ IEEE\nTrans. Cloud Comput., vol. 2, no. 2, pp. 208–221, Apr. 2014.\n[6] J. Guo, Z. Chang, S. Wang, H. Ding, Y . Feng, L. Mao, and Y . Bao,\n‘‘Who limits the resource efﬁciency of my datacenter: An analysis of\nalibaba datacenter traces,’’ in Proc. Int. Symp. Qual. Service, Jun. 2019,\np. 39.\n[7] M. Alam, K. A. Shakil, and S. Sethi, ‘‘Analysis and clustering of workload\nin Google cluster trace based on resource usage,’’ in Proc. IEEE Int. Conf.\nComput. Sci. Eng. (CSE) IEEE Intl Conf. Embedded Ubiquitous Comput.\n(EUC) 15th Int. Symp. Distrib. Comput. Appl. for Bus. Eng. (DCABES),\nAug. 2016, pp. 740–747.\n[8] Z. Chen, J. Hu, G. Min, A. Y . Zomaya, and T. El-Ghazawi, ‘‘Towards accu-\nrate prediction for high-dimensional and highly-variable cloud workloads\nwith deep learning,’’ IEEE Trans. Parallel Distrib. Syst., vol. 31, no. 4,\npp. 923–934, Apr. 2020.\n[9] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning. Cambridge,\nMA, USA: MIT Press, 2016.\n[10] S. Hochreiter, ‘‘The vanishing gradient problem during learning recurrent\nneural nets and problem solutions,’’ Int. J. Uncertainty, Fuzziness Knowl.-\nBased Syst., vol. 06, no. 2, pp. 107–116, Apr. 1998.\n[11] R. Pascanu, T. Mikolov, and Y . Bengio, ‘‘On the difﬁculty of train-\ning recurrent neural networks,’’ in Proc. Int. Conf. Mach. Learn., 2013,\npp. 1310–1318.\n[12] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[13] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, ‘‘Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling,’’ 2014,\narXiv:1412.3555. [Online]. Available: http://arxiv.org/abs/1412.3555\n[14] Y . Hu, B. Deng, F. Peng, and D. Wang, ‘‘Workload prediction for cloud\ncomputing elasticity mechanism,’’ in Proc. IEEE Int. Conf. Cloud Comput.\nBig Data Anal. (ICCCBDA), Jul. 2016, pp. 244–249.\n[15] Z. Amekraz and M. Y . Hadi, ‘‘An adaptive workload prediction strategy for\nnon-Gaussian cloud service using ARMA model with higher order statis-\ntics,’’ in Proc. IEEE 11th Int. Conf. Cloud Comput. (CLOUD), Jul. 2018,\npp. 646–651.\n[16] R. N. Calheiros, E. Masoumi, R. Ranjan, and R. Buyya, ‘‘Workload pre-\ndiction using ARIMA model and its impact on cloud applications’ QoS,’’\nIEEE Trans. Cloud Comput., vol. 3, no. 4, pp. 449–458, Oct. 2015.\n[17] P. Saripalli, G. V . R. Kiran, R. R. Shankar, H. Narware, and N. Bindal,\n‘‘Load prediction and hot spot detection models for autonomic cloud com-\nputing,’’ in Proc. 4th IEEE Int. Conf. Utility Cloud Comput., Dec. 2011,\npp. 397–402.\n[18] J. Guo, J. Wu, J. Na, and B. Zhang, ‘‘A type-aware workload predic-\ntion strategy for non-stationary cloud service,’’ in Proc. IEEE 10th Conf.\nService-Oriented Comput. Appl. (SOCA), Nov. 2017, pp. 98–103.\n[19] J. Qiu, Q. Wu, G. Ding, Y . Xu, and S. Feng, ‘‘A survey of machine learning\nfor big data processing,’’ EURASIP J. Adv. Signal Process., vol. 2016,\nno. 1, p. 67, Dec. 2016.\n[20] M. Ghobaei-Arani, S. Jabbehdari, and M. A. Pourmina, ‘‘An autonomic\nresource provisioning approach for service-based cloud applications:\nA hybrid approach,’’ Future Gener. Comput. Syst., vol. 78, pp. 191–210,\nJan. 2018.\n[21] G. K. Shyam and S. S. Manvi, ‘‘Virtual resource prediction in cloud\nenvironment: A Bayesian approach,’’ J. Netw. Comput. Appl., vol. 65,\npp. 144–154, Apr. 2016.\n[22] P. Singh, P. Gupta, and K. Jyoti, ‘‘TASM: Technocrat ARIMA and SVR\nmodel for workload prediction of Web applications in cloud,’’ Cluster\nComput., vol. 22, no. 2, pp. 619–633, Jun. 2019.\n[23] A. A. Rahmanian, M. Ghobaei-Arani, and S. Toﬁghy, ‘‘A learning\nautomata-based ensemble resource usage prediction algorithm for cloud\ncomputing environment,’’Future Gener. Comput. Syst., vol. 79, pp. 54–71,\nFeb. 2018.\n[24] M. Duggan, K. Mason, J. Duggan, E. Howley, and E. Barrett, ‘‘Predicting\nhost CPU utilization in cloud computing using recurrent neural networks,’’\nin Proc. 12th Int. Conf. for Internet Technol. Secured Trans. (ICITST),\nDec. 2017, pp. 67–72.\n[25] Y . Zhu, W. Zhang, Y . Chen, and H. Gao, ‘‘A novel approach to workload\nprediction using attention-based LSTM encoder-decoder network in cloud\nenvironment,’’ EURASIP J. Wireless Commun. Netw., vol. 2019, no. 1,\np. 274, Dec. 2019.\n[26] W. Guo, W. Ge, X. Lu, and H. Li, ‘‘Short-term load forecasting of vir-\ntual machines based on improved neural network,’’ IEEE Access, vol. 7,\npp. 121037–121045, 2019.\n[27] Y . Guo and W. Yao, ‘‘Applying gated recurrent units pproaches for work-\nload prediction,’’ in Proc. IEEE/IFIP Netw. Oper. Manage. Symp. (NOMS),\nApr. 2018, pp. 1–6.\n[28] Q. Zhang, L. T. Yang, Z. Yan, Z. Chen, and P. Li, ‘‘An efﬁcient deep\nlearning model to predict cloud workload for industry informatics,’’ IEEE\nTrans. Ind. Informat., vol. 14, no. 7, pp. 3170–3178, Jul. 2018.\n[29] F. J. Baldan, S. Ramirez-Gallego, C. Bergmeir, F. Herrera, and\nJ. M. Benitez, ‘‘A forecasting methodology for workload forecasting in\ncloud systems,’’ IEEE Trans. Cloud Comput., vol. 6, no. 4, pp. 929–941,\nOct. 2018.\n[30] K. Dragomiretskiy and D. Zosso, ‘‘Variational mode decomposition,’’\nIEEE Trans. Signal Process., vol. 62, no. 3, pp. 531–544, Feb. 2014.\n116002 VOLUME 8, 2020\nS. Zhouet al.: Accurate EFA for Highly Dynamic Cloud Workload With VMD and R-Transformer\n[31] R. Salles, K. Belloze, F. Porto, P. H. Gonzalez, and E. Ogasawara, ‘‘Non-\nstationary time series transformation methods: An experimental review,’’\nKnowl.-Based Syst., vol. 164, pp. 274–291, Jan. 2019.\n[32] A. A. Abdoos, ‘‘A new intelligent method based on combination of\nVMD and ELM for short term wind power forecasting,’’ Neurocomputing,\nvol. 203, pp. 111–120, Aug. 2016.\n[33] M. Gendeel, Z. Yuxian, and H. Aoqi, ‘‘Performance comparison of ANNs\nmodel with VMD for short-term wind speed forecasting,’’ IET Renew.\nPower Gener., vol. 12, no. 12, pp. 1424–1430, Sep. 2018.\n[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nneural Inf. Process. Syst., 2017, pp. 5998–6008.\n[35] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[36] Z. Wang, Y . Ma, Z. Liu, and J. Tang, ‘‘R-transformer: Recurrent neural\nnetwork enhanced transformer,’’ 2019, arXiv:1907.05572. [Online]. Avail-\nable: http://arxiv.org/abs/1907.05572\n[37] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X. Yan,\n‘‘Enhancing the locality and breaking the memory bottleneck of trans-\nformer on time series forecasting,’’ in Proc. Adv. Neural Inf. Process. Syst.,\n2019, pp. 5244–5254.\n[38] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\nimage recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2016, pp. 770–778.\n[39] J. Lei Ba, J. Ryan Kiros, and G. E. Hinton, ‘‘Layer normalization,’’ 2016,\narXiv:1607.06450. [Online]. Available: http://arxiv.org/abs/1607.06450\n[40] G. Lai, W.-C. Chang, Y . Yang, and H. Liu, ‘‘Modeling long-and short-\nterm temporal patterns with deep neural networks,’’ in Proc. 41st Int. ACM\nSIGIR Conf. Res. Develop. Inf. Retr., Jun. 2018, pp. 95–104.\n[41] H. M. Nguyen, G. Kalra, and D. Kim, ‘‘Host load prediction in cloud com-\nputing using long short-term memory encoder–decoder,’’ J. Supercomput.,\nvol. 75, no. 11, pp. 7592–7605, Nov. 2019.\n[42] C. Reiss, J. Wilkes, and J. L. Hellerstein, ‘‘Google cluster-usage traces:\nFormat+ schema,’’ Google, Menlo Park, CA, USA, White Paper, 2011,\npp. 1–14.\n[43] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder-decoder for statistical machine translation,’’ 2014,\narXiv:1406.1078. [Online]. Available: http://arxiv.org/abs/1406.1078\n[44] A. Paszke et al., ‘‘Pytorch: An imperative style, high-performance\ndeep learning library,’’ in Proc. Adv. Neural Inf. Process. Syst., 2019,\npp. 8024–8035.\nSHAOJING ZHOUreceived the bachelor’s degree\nfrom the College of Civil Engineering and\nArchitecture, Shandong University of Science\nand Technology, China, in 2018. He is currently\npursuing the M.S. degree with the College of\nComputer Science and Technology, Shanghai\nUniversity of Electric Power, China. His research\ninterests include cloud computing and deep\nlearning.\nJINGUO LI (Member, IEEE) received the B.S.\ndegree in information security and the Ph.D.\ndegree in computer science and technology from\nHunan University, China, in 2007 and 2014,\nrespectively. He is currently an Associate Pro-\nfessor with the College of Computer Science\nand Technology, Shanghai University of Electric\nPower. His research interests include information\nsecurity and privacy, applied cryptography, and\ncloud computing.\nKAI ZHANG received the bachelor’s degree in\ncomputer science and technology from Shandong\nNormal University, China, in 2012, and the Ph.D.\ndegree in computer science and technology from\nEast China Normal University, China, in 2017.\nHe visited Nanyang Technological University,\nin 2017. He is currently an Assistant Professor\nwith the Shanghai University of Electric Power,\nChina. His research interests include data-driven\nprivacy enhanced techniques and cloud security.\nMI WEN (Member, IEEE) received the Ph.D.\ndegree in computer science from Shanghai Jiao\nTong University, Shanghai, China, in 2008. She is\ncurrently a Professor with the College of Computer\nScience and Technology, Shanghai University of\nElectric Power, Shanghai. Her current research\ninterests include security in wireless sensor net-\nworks, cloud security, and privacy preserving in\nsmart grid.\nQIJIE GUAN received the bachelor’s degree in\nmeasurement and control technology and instru-\nments from the Jiangsu University of Technology,\nin 2017. She is currently pursuing the master’s\ndegree with the College of Computer Science\nand Technology, Shanghai University of Electric\nPower, China. Her research interests include geo-\nlogical modeling and deep learning.\nVOLUME 8, 2020 116003",
  "topic": "Workload",
  "concepts": [
    {
      "name": "Workload",
      "score": 0.8868052959442139
    },
    {
      "name": "Cloud computing",
      "score": 0.7911421060562134
    },
    {
      "name": "Computer science",
      "score": 0.7529889345169067
    },
    {
      "name": "Autoregressive model",
      "score": 0.5694257616996765
    },
    {
      "name": "Data mining",
      "score": 0.4317009449005127
    },
    {
      "name": "Real-time computing",
      "score": 0.3974449932575226
    },
    {
      "name": "Statistics",
      "score": 0.09533989429473877
    },
    {
      "name": "Operating system",
      "score": 0.08742928504943848
    },
    {
      "name": "Mathematics",
      "score": 0.08620703220367432
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I23632641",
      "name": "Shanghai University of Electric Power",
      "country": "CN"
    }
  ]
}