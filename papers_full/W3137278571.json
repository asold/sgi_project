{
  "title": "ConViT: improving vision transformers with soft convolutional inductive biases*",
  "url": "https://openalex.org/W3137278571",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "d'Ascoli, St\\'ephane",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222055943",
      "name": "Touvron, Hugo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4324317301",
      "name": "Leavitt, Matthew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288532882",
      "name": "Morcos, Ari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222401456",
      "name": "Biroli, Giulio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221725695",
      "name": "Sagun, Levent",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222401454",
      "name": "d'Ascoli, St√©phane",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6778751070",
    "https://openalex.org/W6729518879",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6762205418",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6776721752",
    "https://openalex.org/W6769955919",
    "https://openalex.org/W2952475655",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6791776128",
    "https://openalex.org/W6773603168",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6746975906",
    "https://openalex.org/W6678174250",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6779809370",
    "https://openalex.org/W6780643669",
    "https://openalex.org/W6776188000",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W2042755403",
    "https://openalex.org/W6631319189",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W2151035455",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6795252178",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6763015118",
    "https://openalex.org/W6786354595",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W4287773438",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W3035413931",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1523493493",
    "https://openalex.org/W2981952041",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W4376983087",
    "https://openalex.org/W3107790194",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2970452384",
    "https://openalex.org/W4298395628",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W4294326629",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W36434594",
    "https://openalex.org/W2898732869",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W3037784242",
    "https://openalex.org/W4287704709",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W4376546474"
  ],
  "abstract": "Abstract Convolutional architectures have proven to be extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision transformers rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ‚Äòsoft‚Äô convolutional inductive bias. We initialize the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT , outperforms the DeiT (Touvron et al 2020 arXiv: 2012.12877 ) on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analyzing how it has escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit .",
  "full_text": "ConViT: Improving Vision Transformers\nwith Soft Convolutional Inductive Biases\nSt¬¥ephane d‚ÄôAscoli1 2 Hugo Touvron2 Matthew L. Leavitt 2 Ari S. Morcos 2 Giulio Biroli 1 2 Levent Sagun 2\nAbstract\nConvolutional architectures have proven ex-\ntremely successful for vision tasks. Their hard\ninductive biases enable sample-efÔ¨Åcient learning,\nbut come at the cost of a potentially lower perfor-\nmance ceiling. Vision Transformers (ViTs) rely\non more Ô¨Çexible self-attention layers, and have\nrecently outperformed CNNs for image classiÔ¨Å-\ncation. However, they require costly pre-training\non large external datasets or distillation from pre-\ntrained convolutional networks. In this paper, we\nask the following question: is it possible to com-\nbine the strengths of these two architectures while\navoiding their respective limitations? To this\nend, we introduce gated positional self-attention\n(GPSA), a form of positional self-attention which\ncan be equipped with a ‚Äúsoft‚Äù convolutional in-\nductive bias. We initialize the GPSA layers to\nmimic the locality of convolutional layers, then\ngive each attention head the freedom to escape\nlocality by adjusting a gating parameter regu-\nlating the attention paid to position versus con-\ntent information. The resulting convolutional-\nlike ViT architecture, ConViT, outperforms the\nDeiT (Touvron et al., 2020) on ImageNet, while\noffering a much improved sample efÔ¨Åciency. We\nfurther investigate the role of locality in learn-\ning by Ô¨Årst quantifying how it is encouraged in\nvanilla self-attention layers, then analyzing how it\nis escaped in GPSA layers. We conclude by pre-\nsenting various ablations to better understand the\nsuccess of the ConViT. Our code and models are\nreleased publicly at https://github.com/\nfacebookresearch/convit.\n1Department of Physics, Ecole Normale Sup ¬¥erieure, Paris,\nFrance 2Facebook AI Research, Paris, France. Correspondence to:\nSt¬¥ephane d‚ÄôAscoli<stephane.dascoli@ens.fr>.\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n1. Introduction\nThe success of deep learning over the last decade has largely\nbeen fueled by models with strong inductive biases, al-\nlowing efÔ¨Åcient training across domains (Mitchell, 1980;\nGoodfellow et al., 2016). The use of Convolutional Neural\nNetworks (CNNs) (LeCun et al., 1998; 1989), which have\nbecome ubiquitous in computer vision since the success of\nAlexNet in 2012 (Krizhevsky et al., 2017), epitomizes this\ntrend. Inductive biases are hard-coded into the architectural\nstructure of CNNs in the form of two strong constraints\non the weights: locality and weight sharing. By encourag-\ning translation equivariance (without pooling layers) and\ntranslation invariance (with pooling layers) (Scherer et al.,\n2010; Schmidhuber, 2015; Goodfellow et al., 2016), the\nconvolutional inductive bias makes models more sample-\nefÔ¨Åcient and parameter-efÔ¨Åcient (Simoncelli & Olshausen,\n2001; Ruderman & Bialek, 1994). Similarly, for sequence-\nbased tasks, recurrent networks with hard-coded memory\ncells have been shown to simplify the learning of long-range\ndependencies (LSTMs) and outperform vanilla recurrent\nneural networks in a variety of settings (Gers et al., 1999;\nSundermeyer et al., 2012; Greff et al., 2017).\nHowever, the rise of models based purely on attention in\nrecent years calls into question the necessity of hard-coded\ninductive biases. First introduced as an add-on to recurrent\nneural networks for Sequence-to-Sequence models (Bah-\ndanau et al., 2014), attention has led to a breakthrough in\nNatural Language Processing through the emergence of\nTransformer models, which rely solely on a particular kind\nof attention: Self-Attention (SA) (Vaswani et al., 2017).\nThe strong performance of these models when pre-trained\non large datasets has quickly led to Transformer-based ap-\nproaches becoming the default choice over recurrent models\nlike LSTMs (Devlin et al., 2018).\nIn vision tasks, the locality of CNNs impairs the ability to\ncapture long-range dependencies, whereas attention does\nnot suffer from this limitation. Chen et al. (2018) and Bello\net al. (2019) leveraged this complementarity by augmenting\nconvolutional layers with attention. More recently, Ra-\nmachandran et al. (2019) ran a series of experiments replac-\ning some or all convolutional layers in ResNets with atten-\ntion, and found the best performing models used convolu-\narXiv:2103.10697v2  [cs.CV]  10 Jun 2021\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n# samples\nNeutral\n# parameters\n# samples\n# parameters\nHard inductive bias\n(CNN)\nHelpful\nHarmful\nSoft inductive bias\n(ConViT)\nFigure 1.Soft inductive biases can help models learn without\nbeing restrictive. Hard inductive biases, such as the architectural\nconstraints of CNNs, can greatly improve the sample-efÔ¨Åciency of\nlearning, but can become constraining when the size of the dataset\nis not an issue. The soft inductive biases introduced by the ConViT\navoid this limitation by vanishing away when not required.\ntions in early layers and attention in later layers. The Vision\nTransformer (ViT), introduced by Dosovitskiy et al. (2020),\nentirely dispenses with the convolutional inductive bias by\nperforming SA across embeddings of patches of pixels. The\nViT is able to match or exceed the performance of CNNs\nbut requires pre-training on vast amounts of data. More\nrecently, the Data-efÔ¨Åcient Vision Transformer (DeiT) (Tou-\nvron et al., 2020) was able to reach similar performances\nwithout any pre-training on supplementary data, instead re-\nlying on Knowledge Distillation (Hinton et al., 2015) from\na convolutional teacher.\nSoft inductive biases The recent success of the ViT\ndemonstrates that while convolutional constraints can enable\nstrongly sample-efÔ¨Åcient training in the small-data regime,\nthey can also become limiting as the dataset size is not\nan issue. In data-plentiful regimes, hard inductive biases\ncan be overly restrictive and learning the most appropriate\ninductive bias can prove more effective. The practitioner\nis therefore confronted with a dilemma between using a\nconvolutional model, which has a high performance Ô¨Çoor\nbut a potentially lower performance ceiling due to the hard\ninductive biases, or a self-attention based model, which has\na lower Ô¨Çoor but a higher ceiling. This dilemma leads to\nthe following question: can one get the best of both worlds,\nand obtain the beneÔ¨Åts of the convolutional inductive biases\nwithout suffering from its limitations (see Fig. 1)?\nIn this direction, one successful approach is the combina-\ntion of the two architectures in ‚Äúhybrid‚Äù models. These\nmodels, which interleave or combine convolutional and self-\nattention layers, have fueled successful results in a variety\nof tasks (Carion et al., 2020; Hu et al., 2018a; Ramachan-\ndran et al., 2019; Chen et al., 2020; Locatello et al., 2020;\nSun et al., 2019; Srinivas et al., 2021; Wu et al., 2020). An-\nother approach is that of Knowledge Distillation (Hinton\net al., 2015), which has recently been applied to transfer\nthe inductive bias of a convolutional teacher to a student\ntransformer (Touvron et al., 2020). While these two meth-\nods offer an interesting compromise, they forcefully induce\nconvolutional inductive biases into the Transformers, poten-\ntially affecting the Transformer with their limitations.\nContribution In this paper, we take a new step towards\nbridging the gap between CNNs and Transformers, by pre-\nsenting a new method to ‚Äúsoftly‚Äù introduce a convolutional\ninductive bias into the ViT. The idea is to let each SA layer\ndecide whether to behave as a convolutional layer or not,\ndepending on the context. We make the following contribu-\ntions:\n1. We present a new form of SA layer, named gated posi-\ntional self-attention (GPSA), which one can initialize\nas a convolutional layer. Each attention head then has\nthe freedom to recover expressivity by adjusting a gat-\ning parameter.\n2. We then perform experiments based on the DeiT (Tou-\nvron et al., 2020), with a certain number of SA layers\nreplaced by GPSA layers. The resulting Convolutional\nVision Transformer (ConViT) outperforms the DeiT\nwhile boasting a much improved sample-efÔ¨Åciency\n(Fig. 2).\n3. We analyze quantitatively how local attention is natu-\nrally encouraged in vanilla ViTs, then investigate the\ninner workings of the ConViT and perform ablations\nto investigate how it beneÔ¨Åts from the convolution ini-\ntialization.\nOverall, our work demonstrates the effectiveness of ‚Äùsoft‚Äù\ninductive biases, especially in the low-data regime where\nthe learning model is highly underspeciÔ¨Åed (see Fig. 1), and\nmotivates the exploration of further methods to induce them.\nRelated work Our work is motivated by combining the\nrecent success of pure Transformer models (Dosovitskiy\net al., 2020) with the formalized relationship between SA\nand convolution. Indeed, Cordonnier et al. (2019) showed\nthat a SA layer with Nh heads can express a convolution of\nkernel size\n‚àö\nNh, if each head focuses on one of the pixels\nin the kernel patch. By investigating the qualitative aspect of\nattention maps of models trained on CIFAR-10, it is shown\nthat SA layers with relative positional encodings naturally\nconverge towards convolutional-like conÔ¨Ågurations, sug-\ngesting that some degree of convolutional inductive bias is\ndesirable.\nConversely, the restrictiveness of hard locality constraints\nhas been proven by Elsayed et al. (2020). A breadth of\napproaches have been taken to imbue CNN architectures\nwith nonlocality (Hu et al., 2018b;c; Wang et al., 2018; Wu\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n10 100\nImages in training set (%)\n40\n50\n60\n70\n80Top-1 accuracy\nConViT-S\nDeiT-S\n5\n10\n15\n20\n25\n30\n35\nRelative gain (%)\n(a) Sample efÔ¨Åciency\n101 102\nNumber of params [M]\n70\n72\n74\n76\n78\n80\n82\n84Top-1 accuracy\nConViT\nDeiT\nEffNet\nResNet\nViT\nT2T-ViT\nVT (b) Parameter efÔ¨Åciency\nFigure 2.The ConViT outperforms the DeiT both in sample\nand parameter efÔ¨Åciency . Left: we compare the sample efÔ¨Å-\nciency of our ConViT-S (see Tab. 1) with that of the DeiT-S by\ntraining them on restricted portions of ImageNet-1k, where we\nonly keep a certain fraction of the images of each class. Both mod-\nels are trained with the hyperparameters reported in (Touvron et al.,\n2020). We display the the relative improvement of the ConViT\nover the DeiT in green. Right: we compare the top-1 accuracies\nof our ConViT models with those of other ViTs (diamonds) and\nCNNs (squares) on ImageNet-1k. The performance of other mod-\nels on ImageNet are taken from (Touvron et al., 2020; He et al.,\n2016; Tan & Le, 2019; Wu et al., 2020; Yuan et al., 2021).\net al., 2020). Another line of research is to induce a convolu-\ntional inductive bias is different architectures. For example,\nNeyshabur (2020) uses a regularization method to encour-\nage fully-connected networks (FCNs) to learn convolutions\nfrom scratch throughout training.\nMost related to our approach, d‚ÄôAscoli et al. (2019) explored\na method to initialize FCNs networks as CNNs. This en-\nables the resulting FCN to reach much higher performance\nthan achievable with standard initialization. Moreover, if\nthe FCN is initialized from a partially trained CNN, the\nrecovered degrees of freedom allow the FCN to outperform\nthe CNN it stems from. This method relates more generally\nto ‚Äúwarm start‚Äù approaches such as those used in spiked\ntensor models (Anandkumar et al., 2016), where a smart\ninitialization, containing prior information on the problem,\nis used to ease the learning task.\nReproducibility We provide an open-source implemen-\ntation of our method as well as pretrained models\nat the following address: https://github.com/\nfacebookresearch/convit.\n2. Background\nWe begin by introducing the basics of SA layers, and show\nhow positional attention can allow SA layers to express\nconvolutional layers.\nMulti-head self-attention The attention mechanism is\nbased on a trainable associative memory with (key, query)\nvector pairs. A sequence of L1 ‚Äúquery‚Äù embeddings Q‚àà\nRL1√óDh is matched against another sequence of L2 ‚Äúkey‚Äù\nembeddings K ‚ààRL2√óDh using inner products. The re-\nsult is an attention matrix whose entry (ij) quantiÔ¨Åes how\nsemantically ‚Äúrelevant‚ÄùQi is to Kj:\nA= softmax\n(QK‚ä§\n‚àöDh\n)\n‚ààRL1√óL2 , (1)\nwhere (softmax [X])ij = eXij /‚àë\nkeXik .\nSelf-attention is a special case of attention where a sequence\nis matched to itself, to extract the semantic dependencies\nbetween its parts. In the ViT, the queries and keys are linear\nprojections of the embeddings of 16 √ó16 pixel patches\nX ‚ààRL√óDemb. Hence, we have Q= WqryXand K =\nWkeyX, where Wkey,Wqry ‚ààRDemb√óDh .\nMulti-head SA layers use several self-attention heads in\nparallel to allow the learning of different kinds of interde-\npendencies. They take as input a sequence of Lembeddings\nof dimension Demb = NhDh, and output a sequence of L\nembeddings of the same dimension through the following\nmechanism:\nMSA(X) := concat\nh‚àà[Nh]\n[SAh(X)] Wout + bout, (2)\nwhere Wout ‚àà RDemb√óDemb,bout ‚àà RDemb. Each self-\nattention head hperforms the following operation:\nSAh(X) := AhXWh\nval, (3)\nwhere Wh\nval ‚ààRDemb√óDh is the value matrix.\nHowever, in the vanilla form of Eq. 1, SA layers are position-\nagnostic: they do not know how the patches are located ac-\ncording to each other. To incorporate positional information,\nthere are several options. One is to add some positional\ninformation to the input at embedding time, before propa-\ngating it through the SA layers: (Dosovitskiy et al., 2020)\nuse this approach in their ViT. Another possibility is to re-\nplace the vanilla SA with positional self-attention (PSA),\nusing encodings rij of the relative position of patches iand\nj(Ramachandran et al., 2019):\nAh\nij := softmax\n(\nQh\niKh‚ä§\nj + vh‚ä§\nposrij\n)\n(4)\nEach attention head uses a trainable embedding vh\npos ‚àà\nRDpos , and the relative positional encodings rij ‚ààRDpos\nonly depend on the distance between pixels iand j, denoted\ndenoted by a two-dimensional vector Œ¥ij.\nSelf-attention as a generalized convolution Cordonnier\net al. (2019) show that a multi-head PSA layer with Nh\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n(a) Input\nHead 1\n Head 2\n Head 3 (b) Standard initialization\nHead 1\n Head 2\n Head 3\n Head 4\n(c) Convolutional initialization, strength Œ±= 0.5\nHead 1\n Head 2\n Head 3\n Head 4\n(d) Convolutional initialization, strength Œ±= 2\nFigure 3.Positional self-attention layers can be initialized as\nconvolutional layers. (a): Input image from ImageNet, where the\nquery patch is highlighted by a red box. (b),(c),(d): attention maps\nof an untrained SA layer (b) and those of a PSA layer using the\nconvolutional-like initialization scheme of Eq. 5 with two different\nvalues of the locality strength parameter, Œ±(c, d). Note that the\nshapes of the image can easily be distinguished in (b), but not in\n(c) or (d), when the attention is purely positional.\nheads and learnable relative positional encodings (Eq. 4) of\ndimension Dpos ‚â•3 can express any convolutional layer of\nÔ¨Ålter size\n‚àö\nNh √ó\n‚àö\nNh, by setting the following:\nÔ£±\nÔ£¥Ô£≤\nÔ£¥Ô£≥\nvh\npos := ‚àíŒ±h(\n1,‚àí2‚àÜh\n1 ,‚àí2‚àÜh\n2 ,0,... 0\n)\nrŒ¥ :=\n(\n‚à•Œ¥‚à•2,Œ¥1,Œ¥2,0,... 0\n)\nWqry = Wkey := 0, Wval := I\n(5)\nIn the above,\n‚Ä¢ The center of attention ‚àÜh ‚ààR2 is the position to\nwhich head h pays most attention to, relative to the\nquery patch. For example, in Fig. 3(c), the four heads\ncorrespond, from left to right, to ‚àÜ1 = (‚àí1,1),‚àÜ2 =\n(‚àí1,‚àí1),‚àÜ3 = (1,1),‚àÜ4 = (1,‚àí1).\n‚Ä¢ The locality strength Œ±h >0 determines how focused\nthe attention is around its center ‚àÜh (it can also by un-\nderstood as the ‚Äútemperature‚Äù of the softmax in Eq. 1).\nWhen Œ±h is large, the attention is focused only on the\npatch(es) located at ‚àÜh, as in Fig. 3(d); when Œ±h is\nsmall, the attention is spread out into a larger area, as\nin Fig. 3(c).\nThus, the PSA layer can achieve a strictly convolutional\nattention map by setting the centers of attention ‚àÜh to\nPatches\nClass\ntoken\nImage \nembedding\nNonlocal \nGPSA \nFFN \nSA \nFFN \nSA \nFFN \nLocal \nùëäùëûùëüùë¶ ùëäùëòùëíùë¶ ùë£ùëùùëúùë† ùëüùëñùëó\n‚àó ‚àó\nsoftmax softmax \n+1‚àíùúé(ùúÜ) ùúé(ùúÜ)\nnormalize \nGated Positional\n Self-AttentionConViT\nùëãùëñ ùëãùëó\nùê¥ùëñùëó\nGPSA \nFFN \nFigure 4.Architecture of the ConViT.The ConViT (left) is a ver-\nsion of the ViT in which some of the self-attention (SA) layers\nare replaced with gated positional self-attention layers (GPSA;\nright). Because GPSA layers involve positional information, the\nclass token is concatenated with hidden representation after the\nlast GPSA layer. In this paper, we typically take 10 GPSA lay-\ners followed by 2 vanilla SA layers. FFN: feedforward network\n(2 linear layers separated by a GeLU activation); Wqry : query\nweights; Wkey: key weights; vpos: attention center and span em-\nbeddings (learned); rqk: relative position encodings (Ô¨Åxed); Œª:\ngating parameter (learned); œÉ: sigmoid function.\neach of the possible positional offsets of a ‚àöNh √ó‚àöNh\nconvolutional kernel, and sending the locality strengths Œ±h\nto some large value.\n3. Approach\nBuilding on the insight of (Cordonnier et al., 2019), we in-\ntroduce the ConVit, a variant of the ViT (Dosovitskiy et al.,\n2020) obtained by replacing some of the SA layers by a new\ntype of layer which we call gated positional self-attention\n(GPSA) layers. The core idea is to enforce the ‚Äúinformed‚Äù\nconvolutional conÔ¨Åguration of Eqs. 5 in the GPSA layers at\ninitialization, then let them decide whether to stay convo-\nlutional or not. However, the standard parameterization of\nPSA layers (Eq. 4) suffers from two limitations, which lead\nus two introduce two modiÔ¨Åcations.\nAdaptive attention span The Ô¨Årst caveat in PSA is the\nvast number of trainable parameters involved, since the num-\nber of relative positional encodings rŒ¥ is quadratic in the\nnumber of patches. This led some authors to restrict the\nattention to a subset of patches around the query patch (Ra-\nmachandran et al., 2019), at the cost of losing long-range\ninformation.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nTo avoid this, we leave the relative positional encodings\nrŒ¥ Ô¨Åxed, and train only the embeddings vh\npos which de-\ntermine the center and span of the attention heads; this\napproach relates to the adaptive attention span introduced\nin Sukhbaatar et al. (2019) for Language Transformers. The\ninitial values of rŒ¥ and vh\npos are given by Eq. 5, where we\ntake Dpos = 3 to get rid of the useless zero components.\nThanks to Dpos ‚â™Dh, the number of parameters involved\nin the positional attention is negligible compared to the num-\nber of parameters involved in the content attention. This\nmakes sense, as content interactions are inherently much\nsimpler to model than positional interactions.\nPositional gating The second issue with standard PSA is\nthe fact that the content and positional terms in Eq. 4 are po-\ntentially of different magnitudes, in which case the softmax\nwill ignore the smallest of the two. In particular, the con-\nvolutional initialization scheme discussed above involves\nhighly concentrated attention scores, i.e. high-magnitude\nvalues in the softmax. In practice, we observed that using a\nconvolutional initialization scheme on vanilla PSA layers\ngives a boost in early epochs, but degrades late-time perfor-\nmance as the attention mechanism lazily ignores the content\ninformation (see SM. A).\nTo avoid this, GPSA layers sum the content and positional\nterms after the softmax, with their relative importances gov-\nerned by a learnable gating parameter Œªh (one for each\nattention head). Finally, we normalize the resulting sum of\nmatrices (whose terms are positive) to ensure that the result-\ning attention scores deÔ¨Åne a probability distribution. The\nresulting GPSA layer is therefore parametrized as follows\n(see also Fig. 4):\nGPSAh(X) := normalize\n[\nAh]\nXWh\nval (6)\nAh\nij := (1‚àíœÉ(Œªh)) softmax\n(\nQh\niKh‚ä§\nj\n)\n+ œÉ(Œªh) softmax\n(\nvh‚ä§\nposrij\n)\n, (7)\nwhere (normalize [A])ij = Aij/‚àë\nkAik and œÉ : x ‚Ü¶‚Üí\n1/(1+e‚àíx) is the sigmoid function. By setting the gating\nparameter Œªh to a large positive value at initialization, one\nhas œÉ(Œªh) ‚âÉ1 : the GPSA bases its attention purely on\nposition, dispensing with the need of setting Wqry and\nWkey to zero as in Eq. 5. However, to avoid the ConViT\nstaying stuck at Œªh ‚â´1, we initialize Œªh = 1 for all layers\nand all heads.\nArchitectural details The ViT slices input images of size\n224 into 16 √ó16 non-overlapping patches of 14 √ó14 pixels\nand embeds them into vectors of dimension Demb = 64Nh\nusing a convolutional stem. It then propagates the patches\nthrough 12 blocks which keep their dimensionality constant.\nEach block consists in a SA layer followed by a 2-layer\nFeed-Forward Network (FFN) with GeLU activation, both\nequipped with residual connections. The ConViT is simply\na ViT where the Ô¨Årst 10 blocks replace the SA layers by\nGPSA layers with a convolutional initialization.\nSimilar to language Transformers like BERT (Devlin et al.,\n2018), the ViT uses an extra ‚Äúclass token‚Äù, appended to the\nsequence of patches to predict the class of the input. Since\nthis class token does not carry any positional information,\nthe SA layers of the ViT do not use positional attention:\nthe positional information is instead injected to each patch\nbefore the Ô¨Årst layer, by adding a learnable positional em-\nbedding of dimension Demb. As GPSA layers involve posi-\ntional attention, they are not well suited for the class token\napproach. We solve this problem by appending the class\ntoken to the patches after the last GPSA layer, similarly to\nwhat is done in (Touvron et al., 2021b) (see Fig. 4)1.\nFor fairness, and since they are computationally cheap, we\nkeep the absolute positional embeddings of the ViT active\nin the ConViT. However, as shown in SM. F, the ConViT\nrelies much less on them, since the GPSA layers already use\nrelative positional encodings. Hence, the absolute positional\nembeddings could easily be removed, dispensing with the\nneed to interpolate the embeddings when changing the input\nresolution (the relative positional encodings simply need to\nbe resampled according to Eq. 5, as performed automatically\nin our open-source implementation).\nTraining details We based our ConVit on the DeiT (Tou-\nvron et al., 2020), a hyperparameter-optimized version of\nthe ViT which has been open-sourced2. Thanks to its ability\nto achieve competitive results without using any external\ndata, the DeiT both an excellent baseline and relatively easy\nto train: the largest model (DeiT-B) only requires a few days\nof training on 8 GPUs.\nTo mimic 2 √ó2, 3 √ó3 and 4 √ó4 convolutional Ô¨Ålters, we\nconsider three different ConViT models with 4, 9 and 16\nattention heads (see Tab. 1). Their number of heads are\nslightly larger than the DeiT-Ti, ConViT-S and ConViT-B\nof Touvron et al. (2020), which respectively use 3, 6 and 12\nattention heads. To obtain models of similar sizes, we use\ntwo methods of comparison.\n‚Ä¢ To establish a direct comparison with Touvron et al.\n(2020), we lower the embedding dimension of the Con-\nViTs to Demb/Nh = 48 instead of 64 used for the\nDeiTs. Importantly, we leave all hyperparameters\n(scheduling, data-augmentation, regularization) pre-\nsented in (Touvron et al., 2020) unchanged in order to\n1We also experimented incorporating the class token as an\nextra patch of the image to which all heads pay attention to at\ninitialization, but results were worse than concatenating the class\ntoken after the GPSA layers (not shown).\n2https://github.com/facebookresearch/deit\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nName Model Nh Demb Size Flops Speed Top-1 Top-5\nTi DeiT 3 192 6M 1G 1442 72.2 -\nConViT 4 192 6M 1G 734 73.1 91.7\nTi+ DeiT 4 256 10M 2G 1036 75.9 93.2\nConViT 4 256 10M 2G 625 76.7 93.6\nS DeiT 6 384 22M 4.3G 587 79.8 -\nConViT 9 432 27M 5.4G 305 81.3 95.7\nS+ DeiT 9 576 48M 10G 480 79.0 94.4\nConViT 9 576 48M 10G 382 82.2 95.9\nB DeiT 12 768 86M 17G 187 81.8 -\nConViT 16 768 86M 17G 141 82.4 95.9\nB+ DeiT 16 1024 152M 30G 114 77.5 93.5\nConViT 16 1024 152M 30G 96 82.5 95.9\nTable 1.Performance of the models considered, trained from scratch on ImageNet. Speed is the number of images processed per\nsecond on a Nvidia Quadro GP100 GPU at batch size 128. Top-1 accuracy is measured on ImageNet-1k test set without distillation (see\nSM. B for distillation). The results for DeiT-Ti, DeiT-S and DeiT-B are reported from (Touvron et al., 2020).\nTrain Top-1 Top-5\nsize DeiT ConViT Gap DeiT ConViT Gap\n5% 34.8 47.8 37% 57.8 70.7 22%\n10% 48.0 59.6 24% 71.5 80.3 12%\n30% 66.1 73.7 12% 86.0 90.7 5%\n50% 74.6 78.2 5% 91.8 93.8 2%\n100% 79.9 81.4 2% 95.0 95.8 1%\nTable 2.The convolutional inductive bias strongly improves\nsample efÔ¨Åciency. We compare the top-1 and top-5 accuracy\nof our ConViT-S with that of the DeiT-S, both trained using the\noriginal hyperparameters of the DeiT (Touvron et al., 2020), as\nwell as the relative improvement of the ConViT over the DeiT.\nBoth models are trained on a subsampled version of ImageNet-1k,\nwhere we only keep a variable fraction (leftmost column) of the\nimages of each class for training.\nachieve a fair comparison. The resulting models are\nnamed ConViT-Ti, ConViT-S and ConViT-B.\n‚Ä¢ We also trained DeiTs and ConViTs using the same\nnumber of heads and Demb/Nh = 64, to ensure that\nthe improvement due to ConViT is not simply due to\nthe larger number of heads (Touvron et al., 2021b).\nThis leads to slightly larger models denoted with a ‚Äú+‚Äù\nin Tab. 1. To maintain stable training while Ô¨Åtting these\nmodels on 8 GPUs, we lowered the learning rate from\n0.0005 to 0.0004 and the batch size from 1024 to 512.\nThese minimal hyperparameter changes lead the DeiT-\nB+ to perform less well than the DeiT-S+, which is not\nthe case for the ConViT, suggesting a higher stability\nto hyperparameter changes.\nPerformance of the ConViT In Tab. 1, we display the\ntop-1 accuracy achieved by these models evaluated on the\nImageNet test set after 300 epochs of training, alongside\ntheir number of parameters, number of Ô¨Çops and throughput.\nEach ConViT outperforms its DeiT of same size and same\nnumber of Ô¨Çops by a margin. Importantly, although the\npositional self-attention does slow down the throughput\nof the ConViTs, they also outperform the DeiTs at equal\nthroughput. For example, The ConViT-S+ reaches a top-\n1 of 82.2%, outperforming the original DeiT-B with less\nparameters and higher throughput. Without any tuning, the\nConViT also reaches high performance on CIFAR100, see\nSM. C where we also report learning curves.\nNote that our ConViT is compatible with the distillation\nmethods introduced in Touvron et al. (2020) at no extra cost.\nAs shown in SM. B, hard distillation improves performance,\nenabling the hard-distilled ConViT-S+ to reach82.9% top-1\naccuracy, on the same footing as the hard-distilled DeiT-\nB with half the number of parameters. However, while\ndistillation requires an additional forward pass through a\npre-trained CNN at each step of training, ConViT has no\nsuch requirement, providing similar beneÔ¨Åts to distillation\nwithout additonal computational requirements.\nSample efÔ¨Åciency of the ConViT In Tab. 2, we investi-\ngate the sample-efÔ¨Åciency of the ConViT in a systematic\nway, by subsampling each class of the ImageNet-1k dataset\nby a fraction f = {0.05,0.1,0.3,0.5,1}while multiply-\ning the number of epochs by 1/f so that the total number\nimages presented to the model remains constant. As one\nmight expect, the top-1 accuracy of both the DeiT-S and its\nConViT-S counterpart drops asf decreases. However, the\nConViT suffers much less: while training on only 10% of\nthe data, the ConVit reaches 59.5% top-1 accuracy, com-\npared to 46.5% for its DeiT counterpart.\nThis result can be directly compared to (Zhai et al., 2019),\nwhich after testing several thousand convolutional models\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nreaches a top-1 accuracy of 56.4%; the ConViT is therefore\nhighly competitive in terms of sample efÔ¨Åciency. These Ô¨Ånd-\nings conÔ¨Årm our hypothesis that the convolutional inductive\nbias is most helpful on small datasets, as depicted in Fig. 1.\n4. Investigating the role of locality\nIn this section, we demonstrate that locality is naturally\nencouraged in standard SA layers, and examine how the\nConViT beneÔ¨Åts from locality being imposed at initializa-\ntion.\nSA layers are pulled towards locality We begin by in-\nvestigating whether the hypothesis that PSA layers are nat-\nurally encouraged to become ‚Äúlocal‚Äù over the course of\ntraining (Cordonnier et al., 2019) holds for the vanilla SA\nlayers used in ViTs, which do not beneÔ¨Åt from positional\nattention. To quantify this, we deÔ¨Åne a measure of ‚Äúnonlo-\ncality‚Äù by summing, for each query patch i, the distances\n‚à•Œ¥ij‚à•to all the key patches j weighted by their attention\nscore Aij. We average the number obtained over the query\npatch to obtain the nonlocality metric of head h, which\ncan then be averaged over the attention heads to obtain the\nnonlocality of the whole layer ‚Ñì:\nD‚Ñì,h\nloc := 1\nL\n‚àë\nij\nAh,‚Ñì\nij ‚à•Œ¥ij‚à•,\nD‚Ñì\nloc := 1\nNh\n‚àë\nh\nD‚Ñì,h\nloc (8)\nIntuitively,Dloc is the number of patches between the center\nof attention and the query patch: the further the attention\nheads look from the query patch, the higher the nonlocality.\nIn Fig. 5 (left panel), we show how the nonlocality metric\nevolves during training across the 12 layers of a DeiT-S\ntrained for 300 epochs on ImageNet. During the Ô¨Årst few\nepochs, the nonlocality falls from its initial value in all\nlayers, conÔ¨Årming that the DeiT becomes more ‚Äúconvolu-\ntional‚Äù. During the later stages of training, the nonlocality\nmetric stays low for lower layers, and gradually climbs back\nup for upper layers, revealing that the latter capture long\nrange dependencies, as observed for language Transform-\ners (Sukhbaatar et al., 2019).\nThese observations are particularly clear when examining\nthe attention maps (Fig. 15 of the SM), and point to the\nbeneÔ¨Åcial effect of locality in lower layers. In Fig. 10 of\nthe SM., we also show that the nonlocality metric is lower\nwhen training with distillation from a convolutional network\nas in Touvron et al. (2020), suggesting that the locality of\nthe teacher is partly transferred to the student (Abnar et al.,\n2020).\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7Non-locality\nDeiT\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nFigure 5.SA layers try to become local, GPSA layers escape\nlocality. We plot the nonlocality metric deÔ¨Åned in Eq. 8, averaged\nover a batch of 1024 images: the higher, the further the attention\nheads look from the query pixel. We trained the DeiT-S and\nConViT-S for 300 epochs on ImageNet. Similar results for DeiT-\nTi/ConViT-Ti and DeiT-B/ConViT-B are shown in SM. D.\n0 250\n0.0\n0.5\n1.0Positional attn\nLayer 1\n0 250\n0.0\n0.5\n1.0\nLayer 2\n0 250\n0.0\n0.5\nLayer 3\n0 250\n0.0\n0.5\n1.0\nLayer 4\n0 250\n0.0\n0.5\nLayer 5\n0 250\nEpochs\n0.0\n0.5\n1.0Positional attn\nLayer 6\n0 250\nEpochs\n0.0\n0.5\nLayer 7\n0 250\nEpochs\n0.0\n0.5\nLayer 8\n0 250\nEpochs\n0.0\n0.5\nLayer 9\n0 250\nEpochs\n0.0\n0.5\nLayer 10\nFigure 6.The gating parameters reveal the inner workings of\nthe ConViT.For each layer, the colored lines (one for each of the\n9 attention heads) quantify how much attention head hpays to\npositional information versus content, i.e. the value of œÉ(Œªh), see\nEq. 7. The black line represents the value averaged over all heads.\nWe trained the ConViT-S for 300 epochs on ImageNet. Similar\nresults for ConViT-Ti and ConViT-B are shown in SM D.\nGPSA layers escape locality In the ConViT, strong lo-\ncality is imposed at the beginning of training in the GPSA\nlayers thanks to the convolutional initialization. In Fig. 5\n(right panel), we see that this local conÔ¨Åguration is escaped\nthroughout training, as the nonlocality metric grows in all\nthe GPSA layers. However, the nonlocality at the end of\ntraining is lower than that reached by the DeiT, showing\nthat some information about the initialization is preserved\nthroughout training. Interestingly, the Ô¨Ånal nonlocality does\nnot increase monotonically throughout the layers as for the\nDeiT. The Ô¨Årst layer and the Ô¨Ånal layers strongly escape\nlocality, whereas the intermediate layers (particularly the\nsecond layer) stay more local.\nTo gain more understanding, we examine the dynamics of\nthe gating parameters in Fig. 6. We see that in all layers,\nthe average gating parameter EhœÉ(Œªh) (in black), which\nreÔ¨Çects the average amount of attention paid to positional\ninformation versus content, decreases throughout training.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nThis quantity reaches 0 in layers 6-10, meaning that posi-\ntional information is practically ignored. However, in layers\n1-5, some of the attention heads keep a high value of œÉ(Œªh),\nhence take advantage of positional information. Interest-\ningly, the ConViT-Ti only uses positional information up\nto layer 4, whereas the ConViT-B uses it up to layer 6 (see\nApp. D), suggesting that larger models - which are more\nunder-speciÔ¨Åed - beneÔ¨Åt more from the convolutional prior.\nThese observations highlight the usefulness of the gating\nparameter in terms of interpretability.\nThe inner workings of the ConViT are further revealed by\nthe attention maps of Fig. 7, which are obtained by prop-\nagating an embedded input image through the layers and\nselecting a query patch at the center of the image3. In layer\n10, (bottom row), the attention maps of DeiT and ConViT\nlook qualitatively similar: they both perform content-based\nattention. In layer 2 however (top row), the attention maps\nof the ConViT are more varied: some heads pay attention\nto content (heads 1 and 2) whereas other focus mainly on\nposition (heads 3 and 4). Among the heads which focus on\nposition, some stay highly localized (head 4) whereas others\nbroaden their attention span (head 3). The interested reader\ncan Ô¨Ånd more attention maps in SM. E.\nRef. Train\ngating\nConv\ninit\nTrain\nGPSA\nUse\nGPSA\nFull\ndata\n10%\ndata\na (ConViT) \u0013 \u0013 \u0013 \u0013 82.2 59.7\nb \u0017 \u0013 \u0013 \u0013 82.0 57.4\nc \u0013 \u0017 \u0013 \u0013 81.4 56.9\nd \u0017 \u0017 \u0013 \u0013 81.6 54.6\ne (DeiT) \u0017 \u0017 \u0017 \u0017 79.1 47.8\nf \u0017 \u0013 \u0017 \u0013 78.6 54.3\ng \u0017 \u0017 \u0017 \u0013 73.7 44.8\nTable 3.Gating and convolutional initialization play nicely to-\ngether. We ran an ablation study on the ConViT-S+ trained for 300\nepochs on the full ImageNet training set and on 10% of the train-\ning data. From the left column to right column, we experimented\nfreezing the gating parameters to 0, removing the convolutional\ninitialization, freezing the GPSA layers and removing them alto-\ngether.\nStrong locality is desirable We next investigate how the\nperformance of the ConViT is affected by two important hy-\nperparameters of the ConViT: thelocality strength, Œ±, which\ndetermines how focused the heads are around their center of\nattention, and the number of SA layers replaced by GPSA\nlayers. We examined the effects of these hyperparameters\non ConViT-S, trained on the Ô¨Årst 100 classes of ImageNet.\nAs shown in Fig. 8(a), Ô¨Ånal test accuracy increases both with\nthe locality strength and with the number of GPSA layers;\nin other words, the more convolutional, the better.\n3We do not show the attention paid to the class token in the SA\nlayers\n(a) Input\nLayer 2\nHead 1\n Head 2\n Head 3\nLayer 10\n (b) DeiT\nLayer 2\n( ) = 0.00\nHead 1\n( ) = 0.09\nHead 2\n( ) = 0.51\nHead 3\n( ) = 0.73\nHead 4\nLayer 10\n( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n(c) ConViT\nFigure 7.The ConViT learns more diverse attention maps .\nLeft: input image which is embedded then fed into the models. The\nquery patch is highlighted by a red box and the colormap is loga-\nrithmic to better reveal details. Center: attention maps obtained\nby a DeiT-Ti after 300 epochs of training on ImageNet. Right:\nSame for ConViT-Ti. In each map, we indicated the value of the\ngating parameter in a color varying from white (for heads paying\nattention to content) to red (for heads paying attention to position).\nAttention maps for more images and heads are shown in SM. E.\nIn Fig. 8(b), we show how performance at various stages of\ntraining is impacted by the presence of GPSA layers. We\nsee that the boost due to GPSA is particularly strong during\nthe early stages of training: after 20 epochs, using 9 GPSA\nlayers leads the test-accuracy to almost double, suggesting\nthat the convolution initialization gives the model a substan-\ntial ‚Äúhead start‚Äù. This speedup is of practical interest in\nitself, on top of the boost in Ô¨Ånal performance.\nAblation study In Tab. 3, we present an ablation on the\nConViT, denoted as[a]. We experiment removing the posi-\ntional gating [b]4, the convolutional initialization [c], both\ngating and the convolutional initialization[d], and the GPSA\naltogether ([e], which leaves us with a plain DeiT).\nSurprisingly, on full ImageNet, GPSA without gating [d]\nalready brings a substantial beneÔ¨Åt over the DeiT (+2.5),\nwhich is mildly increased by the convolutional initializa-\ntion ([b], +2.9). As for gating, it helps a little in presence\n4To remove gating, we freeze all gating parameters toŒª= 0so\nthat the same amount of attention is paid to content and position.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0.01 0.1 1\nLocality strength\n0\n3\n6\n9Number of GPSA layers\n77\n78\n79\n80\n81\n0 3 6 9\nNumber of GPSA layers\n20\n40\n60\n80Top-1 accuracy\n20 epochs\n75 epochs\n130 epochs\n185 epochs\n240 epochs\n295 epochs\nFigure 8.The beneÔ¨Åcial effect of locality . Left: As we increase\nthe locality strength (i.e. how focused each attention head is its\nassociated patch) and the number of GPSA layers of a ConViT-S+,\nthe Ô¨Ånal top-1 accuracy increases signiÔ¨Åcantly. Right: The beneÔ¨Å-\ncial effect of locality is particularly strong in the early epochs.\nof the convolutional initialization ( [a], +3.1), and is un-\nhelpful otherwise. These mild improvements due to gating\nand convolutional initialization (likely due to performance\nsaturation above 80% top-1) become much clearer in the\nlow data regime. Here, GPSA alone brings +6.8, with an\nextra +2.3 coming from gating, +2.8 from convolution ini-\ntialization and +5.1 with the two together, illustrating their\ncomplementarity.\nWe also investigated the performance of the ConViT with\nall GPSA layers frozen, leaving only the FFNs to be trained\nin the Ô¨Årst 10 layers. As one could expect, performance\nis strongly degraded in the full data regime if we initial-\nize the GPSA layers randomly ( [f], -5.4 compared to the\nDeiT). However, the convolutional initialization remarkably\nenables the frozen ConViT to reach a very decent perfor-\nmance, almost equalling that of the DeiT ( [e], -0.5). In\nother words, replacing SA layers by random ‚Äúconvolutions‚Äù\nhardly impacts performance. In the low data regime, the\nfrozen ConViT even outperforms the DeiT by a margin\n(+6.5). This naturally begs the question: is attention really\nkey to the success of ViTs (Dong et al., 2021; Tolstikhin\net al., 2021; Touvron et al., 2021a)?\n5. Conclusion and perspectives\nThe present work investigates the importance of initializa-\ntion and inductive biases in learning with vision transform-\ners. By showing that one can take advantage of convolu-\ntional constraints in a soft way, we merge the beneÔ¨Åts of\narchitectural priors and expressive power. The result is a sim-\nple recipe that improves trainability and sample efÔ¨Åciency,\nwithout increasing model size or requiring any tuning.\nOur approach can be summarized as follows: instead of\ninterleaving convolutional layers with SA layers as done\nin hybrid models, let the layers decide whether to be con-\nvolutional or not by adjusting a set of gating parameters.\nMore generally, combining the biases of varied architec-\ntures and letting the model choose which ones are best for a\ngiven task could become a promising direction, reducing the\nneed for greedy architectural search while offering higher\ninterpretability.\nAnother direction which will be explored in future work\nis the following: if SA layers beneÔ¨Åt from being initial-\nized as random convolutions, could one reduce even more\ndrastically their sample complexity by initializing them as\npre-trained convolutions?\nAcknowledgements We thank Herv ¬¥e J ¬¥egou and Fran-\ncisco Massa for helpful discussions. SD and GB acknowl-\nedge funding from the French government under manage-\nment of Agence Nationale de la Recherche as part of the ‚ÄúIn-\nvestissements d‚Äôavenir‚Äù program, reference ANR-19-P3IA-\n0001 (PRAIRIE 3IA Institute).\nReferences\nAbnar, S., Dehghani, M., and Zuidema, W. Transferring\ninductive biases through knowledge distillation. arXiv\npreprint arXiv:2006.00555, 2020.\nAnandkumar, A., Deng, Y ., Ge, R., and Mobahi, H.\nHomotopy analysis for tensor pca. arXiv preprint\narXiv:1610.09322, 2016.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.arXiv\npreprint arXiv:1409.0473, 2014.\nBello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V .\nAttention augmented convolutional networks. InProceed-\nings of the IEEE International Conference on Computer\nVision, pp. 3286‚Äì3295, 2019.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA., and Zagoruyko, S. End-to-end object detection with\ntransformers. arXiv preprint arXiv:2005.12872, 2020.\nChen, Y ., Kalantidis, Y ., Li, J., Yan, S., and Feng, J.\nA2-nets: Double attention networks. arXiv preprint\narXiv:1810.11579, 2018.\nChen, Y .-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z.,\nCheng, Y ., and Liu, J. Uniter: Universal image-text repre-\nsentation learning. In European Conference on Computer\nVision, pp. 104‚Äì120. Springer, 2020.\nCordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-\ntionship between self-attention and convolutional layers.\narXiv preprint arXiv:1911.03584, 2019.\nd‚ÄôAscoli, S., Sagun, L., Biroli, G., and Bruna, J. Finding the\nneedle in the haystack with convolutions: on the beneÔ¨Åts\nof architectural bias. In Advances in Neural Information\nProcessing Systems, pp. 9334‚Äì9345, 2019.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDong, Y ., Cordonnier, J.-B., and Loukas, A. Attention is\nnot all you need: Pure attention loses rank doubly expo-\nnentially with depth. arXiv preprint arXiv:2103.03404,\n2021.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nElsayed, G., Ramachandran, P., Shlens, J., and Kornblith, S.\nRevisiting spatial invariance with low-rank local connec-\ntivity. In International Conference on Machine Learning,\npp. 2868‚Äì2879. PMLR, 2020.\nGers, F. A., Schmidhuber, J., and Cummins, F. Learning to\nforget: Continual prediction with lstm. 1999.\nGoodfellow, I., Bengio, Y ., and Courville, A.Deep Learning.\nMIT Press, 2016.\nGreff, K., Srivastava, R. K., Koutn¬¥ƒ±k, J., Steunebrink, B. R.,\nand Schmidhuber, J. LSTM: A Search Space Odyssey.\nIEEE Transactions on Neural Networks and Learning Sys-\ntems, 28(10):2222‚Äì2232, October 2017. ISSN 2162-2388.\ndoi: 10.1109/TNNLS.2016.2582924. Conference Name:\nIEEE Transactions on Neural Networks and Learning\nSystems.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770‚Äì778, 2016.\nHinton, G., Vinyals, O., and Dean, J. Distilling\nthe knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nHu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y . Relation\nnetworks for object detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 3588‚Äì3597, 2018a.\nHu, J., Shen, L., Albanie, S., Sun, G., and Vedaldi, A.\nGather-Excite: Exploiting Feature Context in Convo-\nlutional Neural Networks. In Bengio, S., Wallach, H.,\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., and Gar-\nnett, R. (eds.), Advances in Neural Information Process-\ning Systems 31, pp. 9401‚Äì9411. Curran Associates, Inc.,\n2018b.\nHu, J., Shen, L., and Sun, G. Squeeze-and-Excitation Net-\nworks. In 2018 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pp. 7132‚Äì7141, June 2018c.\ndoi: 10.1109/CVPR.2018.00745. ISSN: 2575-7075.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet\nclassiÔ¨Åcation with deep convolutional neural networks.\nCommunications of the ACM, 60(6):84‚Äì90, 2017.\nLeCun, Y ., Boser, B., Denker, J. S., Henderson, D., Howard,\nR. E., Hubbard, W., and Jackel, L. D. Backpropaga-\ntion applied to handwritten zip code recognition. Neural\ncomputation, 1(4):541‚Äì551, 1989.\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278‚Äì2324, 1998.\nLocatello, F., Weissenborn, D., Unterthiner, T., Mahendran,\nA., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf,\nT. Object-centric learning with slot attention. arXiv\npreprint arXiv:2006.15055, 2020.\nMitchell, T. M. The need for biases in learning generaliza-\ntions. Department of Computer Science, Laboratory for\nComputer Science Research . . . , 1980.\nNeyshabur, B. Towards learning convolutions from scratch.\nAdvances in Neural Information Processing Systems, 33,\n2020.\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and\nDoll¬¥ar, P. Designing network design spaces. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 10428‚Äì10436, 2020.\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. arXiv preprint arXiv:1906.05909, 2019.\nRuderman, D. L. and Bialek, W. Statistics of natural images:\nScaling in the woods. Physical review letters, 73(6):814,\n1994.\nScherer, D., M ¬®uller, A., and Behnke, S. Evaluation of\nPooling Operations in Convolutional Architectures for\nObject Recognition. In Diamantaras, K., Duch, W., and\nIliadis, L. S. (eds.), ArtiÔ¨Åcial Neural Networks ‚Äì ICANN\n2010, Lecture Notes in Computer Science, pp. 92‚Äì101,\nBerlin, Heidelberg, 2010. Springer. ISBN 978-3-642-\n15825-4. doi: 10.1007/978-3-642-15825-4 10.\nSchmidhuber, J. Deep learning in neural networks:\nAn overview. Neural Networks , 61:85‚Äì117, January\n2015. ISSN 0893-6080. doi: 10.1016/j.neunet.2014.09.\n003. URL http://www.sciencedirect.com/\nscience/article/pii/S0893608014002135.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nSimoncelli, E. P. and Olshausen, B. A. Natural image statis-\ntics and neural representation. Annual review of neuro-\nscience, 24(1):1193‚Äì1216, 2001.\nSrinivas, A., Lin, T.-Y ., Parmar, N., Shlens, J., Abbeel, P.,\nand Vaswani, A. Bottleneck Transformers for Visual\nRecognition. arXiv e-prints, art. arXiv:2101.11605, Jan-\nuary 2021.\nSukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.\nAdaptive attention span in transformers. arXiv preprint\narXiv:1905.07799, 2019.\nSun, C., Myers, A., V ondrick, C., Murphy, K., and Schmid,\nC. Videobert: A joint model for video and language\nrepresentation learning. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pp. 7464‚Äì7473,\n2019.\nSundermeyer, M., Schl¬®uter, R., and Ney, H. LSTM neural\nnetworks for language modeling. In Thirteenth annual\nconference of the international speech communication\nassociation, 2012.\nTan, M. and Le, Q. EfÔ¨Åcientnet: Rethinking model scaling\nfor convolutional neural networks. In International Con-\nference on Machine Learning , pp. 6105‚Äì6114. PMLR,\n2019.\nTolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,\nX., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J.,\nLucic, M., et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,\nA., and J ¬¥egou, H. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\nTouvron, H., Bojanowski, P., Caron, M., Cord, M., El-\nNouby, A., Grave, E., Joulin, A., Synnaeve, G., Verbeek,\nJ., and J ¬¥egou, H. Resmlp: Feedforward networks for\nimage classiÔ¨Åcation with data-efÔ¨Åcient training. arXiv\npreprint arXiv:2105.03404, 2021a.\nTouvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., and\nJ¬¥egou, H. Going deeper with image transformers, 2021b.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, ≈Å., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998‚Äì6008, 2017.\nWang, X., Girshick, R., Gupta, A., and He, K. Non-\nlocal Neural Networks. In 2018 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pp.\n7794‚Äì7803, Salt Lake City, UT, USA, June 2018. IEEE.\nISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.\n00813. URL https://ieeexplore.ieee.org/\ndocument/8578911/.\nWu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Tomizuka, M.,\nKeutzer, K., and Vajda, P. Visual Transformers: Token-\nbased Image Representation and Processing for Computer\nVision. arXiv:2006.03677 [cs, eess], July 2020. URL\nhttp://arxiv.org/abs/2006.03677. arXiv:\n2006.03677.\nYuan, L., Chen, Y ., Wang, T., Yu, W., Shi, Y ., Tay, F. E.,\nFeng, J., and Yan, S. Tokens-to-token vit: Training vision\ntransformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\nZhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self-\nsupervised semi-supervised learning. In Proceedings of\nthe IEEE/CVF International Conference on Computer\nVision, pp. 1476‚Äì1485, 2019.\nZhao, S., Zhou, L., Wang, W., Cai, D., Lam, T. L., and\nXu, Y . Splitnet: Divide and co-training. arXiv preprint\narXiv:2011.14660, 2020.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nA. The importance of positional gating\nIn the main text, we discussed the importance of using GPSA layers instead of the standard PSA layers, where content\nand positional information are summed before the softmax and lead the attention heads to focus only on the positional\ninformation. We give evidence for this claim in Fig. 9, where we train a ConViT-B for 300 epochs on ImageNet, but\nreplace the GPSA by standard PSA. The convolutional initialization of the PSA still gives the ConViT a large advantage\nover the DeiT baseline early in training. However, the ConViT stays in the convolutional conÔ¨Åguration and ignores the\ncontent information, as can be seen by looking at the attention maps (not shown). Later in training, the DeiT catches up and\nsurpasses the performance of the ConViT by utilizing content information.\n0 50 100 150 200 250 300\nEpochs\n0\n20\n40\n60\n80Top-1 accuracy\nConViT - vanilla PSA\nDeiT\nFigure 9.Convolutional initialization without GPSA is helfpul during early training but deteriorates Ô¨Ånal performance. We\ntrained the ConViT-B along with its DeiT-B counterpart for 300 epochs on ImageNet, replacing the GPSA layers of the ConViT-B by\nvanilla PSA layers.\nB. The effect of distillation\nNonlocality In Fig. 10, we compare the nonlocality curves of Fig. 5 of the main text with those obtained when the DeiT is\ntrained via hard distillation from a RegNetY-16GF (84M parameters) (Radosavovic et al., 2020), as in Touvron et al. (2020).\nIn the distillation setup, the nonlocality still drops in the early epochs of training, but increases less at late times compared to\nwithout distillation. Hence, the Ô¨Ånal internal states of the DeiT are more ‚Äúlocal‚Äù due to the distillation. This suggests that\nknowledge distillation transfers the locality of the convolutional teacher to the student, in line with the results of (Abnar\net al., 2020).\nPerformance The hard distillation introduced in Touvron et al. (2020) greatly improves the performance of the DeiT. We\nhave veriÔ¨Åed the complementarity of their distillation methods with our ConViT. In the same way as in the DeiT paper, we\nused a RegNet-16GF teacher and experimented hard distillation during 300 epochs on ImageNet. The results we obtain are\nsummarized in Tab. 4.\nMethod DeiT-S (22M) DeiT-B (86M) ConViT-S+ (48M)\nNo distillation 79.8 81.8 82.2\nHard distillation 80.9 83.0 82.9\nTable 4.Top-1 accuracies of the ConViT-S+ compared to the DeiT-S and DeiT-B, both trained for 300 epochs on ImageNet.\nJust like the DeiT, the ConViT beneÔ¨Åts from distillation, albeit somewhat less than the DeiT, as can be seen from the DeiT-B\nperforming less well than the ConViT-S+ without distillation but better with distillation. This hints to the fact that the\nconvolutional inductive bias transferred from the teacher is redundant with its own convolutional prior.\nNevertheless, the performance improvement obtained by the ConViT with hard distillation demonstrates that instantiating\nsoft inductive biases directly in a model can yield beneÔ¨Åts on top of those obtained by instantiating such biases indirectly, in\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0 100 200 300\nEpochs\n5.75\n6.00\n6.25\n6.50\n6.75\n7.00\n7.25\n7.50Non-locality\n0 100 200 300\nEpochs\n5.75\n6.00\n6.25\n6.50\n6.75\n7.00\n7.25\n7.50\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nFigure 10.Distillation pulls the DeiT towards a more local conÔ¨Åguration.We plotted the nonlocality metric deÔ¨Åned in Eq. 8 throughout\ntraining, for the DeiT-S trained on ImageNet. Left: regular training. Right: training with hard distillation from a RegNet teacher, by means\nof the distillation introduced in (Touvron et al., 2020).\nthis case via distillation.\nC. Further performance results\nIn Fig. 11, we display the time evolution of the top-1 accuracy of our ConViT+ models on CIFAR100, ImageNet and\nsubsampled ImageNet, along with a comparison with the corresponding DeiT+ models.\nFor CIFAR100, we kept all hyperparameters unchanged, but rescaled the images to224 √ó224 and increased the number\nof epochs (adapting the learning rate schedule correspondingly) to mimic the ImageNet scenario. After 1000 epochs, the\nConViTs shows clear signs of overÔ¨Åtting, but reach impressive performances (82.1% top-1 accuracy with 10M parameters,\nwhich is better than the EfÔ¨ÅcientNets reported in (Zhao et al., 2020)).\n0 1000 2000 3000\nEpochs\n60\n65\n70\n75\n80\n85Top-1 accuracy\nbase+\nsmall+\ntiny+\nBaseline\nConViT\nbase+small+tiny+\nModel\n0\n2\n4\n6\n8\n10\n12Relative improvement (%)\n(a) CIFAR100\n0 100 200 300\nEpochs\n60\n65\n70\n75\n80\n85Top-1 accuracy\nbase+\nsmall+\ntiny+\nBaseline\nConViT\nbase+small+tiny+\nModel\n0\n1\n2\n3\n4\n5\n6Relative improvement (%) (b) ImageNet-1k\n0 100 200 300\nEpochs * f\n20\n30\n40\n50\n60\n70\n80Top-1 accuracy\nf = 5%\nf = 10%\nf = 30%\nf = 50%\nf = 100%\nDeiT-S\nConViT-S\n0.050.1 0.3 0.5 1\nSubsampling f\n0\n5\n10\n15\n20\n25\n30\n35Relative improvement (%) (c) Subsampled ImageNet\nFigure 11.The convolutional inductive bias is particularly useful for large models applied to small datasets. Each of the three\npanels displays the top-1 accuracy of the ConViT+ model and their corresponding DeiT+ throughout training, as well as the relative\nimprovement between the best top-1 accuracy reached by the DeiT+ and that reached by the ConViT+. Left: tiny, small and base models\ntrained for 3000 epochs on CIFAR100. Middle: tiny, small and base models trained for 300 epochs on ImageNet-1k. The relative\nimprovement of the ConViT over the DeiT increases with model size.Right: small model trained on a subsampled version of ImageNet-1k,\nwhere we only keep a fraction f ‚àà {0.05,0.1,0.3,0.5,1} of the images of each class. The relative improvement of the ConViT over the\nDeiT increases as the dataset becomes smaller.\nIn Fig. 12, we study the impact of the various ingredients of the ConViT (presence and number of GPSA layers, gating\nparameters, convolutional initialization) on the dynamics of learning.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0 100 200 300\nEpochs\n40\n50\n60\n70\n80Top-1 accuracy\n0 GPSA layers\n3 GPSA layers\n6 GPSA layers\n9 GPSA layers\n100 200 300\nEpochs\n60\n65\n70\n75\n80Top-1 accuracy\nFrom best to worst\nGPSA+conv 82.2\nPSA+conv 82.0\nPSA 81.6\nGPSA 81.4\nBaseline 79.1\nFrozen conv 78.6\nFrozen  73.7\nFigure 12.Impact of various ingredients of the ConViT on the dynamics of learning. In both cases, we train the ConViT-S+ for 300\nepochs on Ô¨Årst 100 classes of ImageNet. Left: ablation on number of GPSA layers, as in Fig. 8. Right: ablation on various ingredients\nof the ConViT, as in Tab. 3. The baseline is the DeiT-S+ (pink). We experimented (i) replacing the 10 Ô¨Årst SA layers by GPSA layers\n(‚ÄúGPSA‚Äù) (ii) freezing the gating parameter of the GPSA layers (‚Äúfrozen gate‚Äù); (iii) removing the convolutional initialization (‚Äúconv‚Äù);\n(iv) freezing all attention modules in the GPSA layers (‚Äúfrozen‚Äù). The Ô¨Ånal top-1 accuracy of the various models trained is reported in the\nlegend.\nD. Effect of model size\nIn Fig. 13, we show the analog of Fig. 5 of the main text for the tiny and base models. Results are qualitatively similar\nto those observed for the small model. Interestingly, the Ô¨Årst layers of DeiT-B and ConViT-B reach signiÔ¨Åcantly higher\nnonlocality than those of the DeiT-Ti and ConViT-Ti.\nIn Fig. 14, we show the analog of Fig. 6 of the main text for the tiny and base models. Again, results are qualitatively\nsimilar: the average weight of the positional attention, EhœÉ(Œªh), decreases over time, so that more attention goes to the\ncontent of the image. Note that in the ConViT-Ti, only the Ô¨Årst 4 layers still pay attention to position at the end of training\n(average gating parameter smaller than one), whereas for ConViT-S, the 5 Ô¨Årst layers still do, and for the ConViT-B, the 6\nÔ¨Årst layers still do. This suggests that the larger (i.e. the more underspeciÔ¨Åed) the model is, the more layers make use of the\nconvolutional prior.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7Non-locality\nDeiT\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\n(a) DeiT-Ti and ConViT-Ti\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7Non-locality\nDeiT\n0 100 200 300\nEpochs\n3\n4\n5\n6\n7\nConViT\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\n(b) DeiT-B and ConViT-B\nFigure 13.The bigger the model, the more non-local the attention. We plotted the nonlocality metric deÔ¨Åned in Eq. 8 of the main text\n(the higher, the further the attention heads look from the query pixel) throughout 300 epochs of training on ImageNet-1k.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n0 250\n0.0\n0.5\n1.0Positional attn\nLayer 1\n0 250\n0.0\n0.5\nLayer 2\n0 250\n0.0\n0.5\nLayer 3\n0 250\n0.0\n0.5\nLayer 4\n0 250\n0.0\n0.5\nLayer 5\n0 250\nEpochs\n0.0\n0.5\n1.0Positional attn\nLayer 6\n0 250\nEpochs\n0.0\n0.5\nLayer 7\n0 250\nEpochs\n0.0\n0.5\nLayer 8\n0 250\nEpochs\n0.0\n0.5\nLayer 9\n0 250\nEpochs\n0.0\n0.5\nLayer 10\n(a) ConViT-Ti\n0 250\n0.0\n0.5\n1.0Positional attn\nLayer 1\n0 250\n0.0\n0.5\n1.0\nLayer 2\n0 250\n0.0\n0.5\n1.0\nLayer 3\n0 250\n0.0\n0.5\n1.0\nLayer 4\n0 250\n0.0\n0.5\n1.0\nLayer 5\n0 250\nEpochs\n0.0\n0.5\n1.0Positional attn\nLayer 6\n0 250\nEpochs\n0.0\n0.5\n1.0\nLayer 7\n0 250\nEpochs\n0.0\n0.5\n1.0\nLayer 8\n0 250\nEpochs\n0.0\n0.5\n1.0\nLayer 9\n0 250\nEpochs\n0.0\n0.5\n1.0\nLayer 10\n(b) ConViT-B\nFigure 14.The bigger the model, the more layers pay attention to position. We plotted the gating parameters of various heads and\nvarious layers, as in Fig. 6 of the main text (the lower, the less attention is paid to positional information) throughout 300 epochs of\ntraining on ImageNet-1k. Note that the ConViT-Ti only has 4 attention heads whereas the ConViT-B has 16, hence the different number of\ncurves.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nE. Attention maps\nAttention maps of the DeiT reveal locality In Fig. 15, we give some visual evidence for the fact that vanilla SA layers\nextract local information by averaging the attention map of the Ô¨Årst and tenth layer of the DeiT over 100 images. Before\ntraining, the maps look essentially random. After training, however, most of the attention heads of the Ô¨Årst layer focus on the\nquery pixel and its immediate surroundings, whereas the attention heads of the tenth layer capture long-range dependencies.\nLayer 1\nHead 1\n Head 2\n Head 3\nLayer 10\n(a) Before training\nLayer 1\nHead 1\n Head 2\n Head 3\nLayer 10\n (b) After training\nFigure 15.The averaged attention maps of the DeiT reveal locality at the end of training. To better visualise the center of attention,\nwe averaged the attention maps over 100 images. Top: before training, the attention patterns exhibit a random structure. Bottom: after\ntraining, most of the attention is devoted to the query pixel, and the rest is focused on its immediate surroundings.\nAttention maps of the ConViT reveal the diversity of the attention heads In Fig. 16, we show a comparison of the\nattention maps of Deit-Ti and ConViT-Ti for different images of the ImageNet validation set. In Fig. 17, we compare the\nattention maps of DeiT-S and ConViT-S.\nIn all cases, results are qualitatively similar: the DeiT attention maps look similar across different heads and different layers,\nwhereas those of the ConViT perform very different operations. Notice that in the second layer, the third and forth head\nfocus stay local whereas the Ô¨Årst two heads focus on content. In the last layer, all the heads ignore positional information,\nfocusing only on content.\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nLayer 2\nHead 1\n Head 2\n Head 3\nLayer 10\nLayer 2\n( ) = 0.00\nHead 1\n( ) = 0.09\nHead 2\n( ) = 0.51\nHead 3\n( ) = 0.73\nHead 4\nLayer 10\n( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\nLayer 2\nHead 1\n Head 2\n Head 3\nLayer 10\nLayer 2\n( ) = 0.00\nHead 1\n( ) = 0.09\nHead 2\n( ) = 0.51\nHead 3\n( ) = 0.73\nHead 4\nLayer 10\n( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n(a) Input images\nLayer 2\nHead 1\n Head 2\n Head 3\nLayer 10\n (b) DeiT\nLayer 2\n( ) = 0.00\nHead 1\n( ) = 0.09\nHead 2\n( ) = 0.51\nHead 3\n( ) = 0.73\nHead 4\nLayer 10\n( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n (c) ConViT\nFigure 16.Left: input image which is embedded then fed into the models. The query patch is highlighted by a red box and the colormap is\nlogarithmic to better reveal details. Center: attention maps obtained by a DeiT-Ti after 300 epochs of training on ImageNet. Right: Same\nfor ConViT-Ti. In each map, we indicated the value of the gating parameter in a color varying from white (for heads paying attention to\ncontent) to red (for heads paying attention to position).\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\n(a) Input image\nLayer 1\nHead 1\n Head 2\n Head 3\n Head 4\n Head 5\n Head 6\nLayer 4\nLayer 7\nLayer 10\n (b) DeiT\nLayer 1\n( ) = 0.01\nHead 1\n( ) = 0.52\nHead 2\n( ) = 0.01\nHead 3\n( ) = 0.00\nHead 4\n( ) = 0.04\nHead 5\n( ) = 0.92\nHead 6\n( ) = 0.07\nHead 7\n( ) = 0.01\nHead 8\n( ) = 0.07\nHead 9\nLayer 4\n( ) = 0.00\n ( ) = 0.82\n ( ) = 0.03\n ( ) = 0.82\n ( ) = 0.14\n ( ) = 0.84\n ( ) = 0.75\n ( ) = 0.00\n ( ) = 0.34\nLayer 7\n( ) = 0.15\n ( ) = 0.00\n ( ) = 0.59\n ( ) = 0.00\n ( ) = 0.04\n ( ) = 0.02\n ( ) = 0.04\n ( ) = 0.00\n ( ) = 0.00\nLayer 10\n( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n ( ) = 0.00\n(c) ConViT\nFigure 17.Attention maps obtained by a DeiT-S and ConViT-S after 300 epochs of training on ImageNet. In each map, we indicated the\nvalue of the gating parameter in a color varying from white (for heads paying attention to content) to red (for heads paying attention to\nposition).\nConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases\nF. Further ablations\nIn this section, we explore masking off various parts of the network to understand which are most crucial.\nIn Tab. 5, we explore the importance of the absolute positional embeddings injected to the input in both the DeiT and\nConViT. We see that masking them off at test time a mild impact on accuracy for the ConViT, but a signiÔ¨Åcant impact for\nthe DeiT, which is expected as the ConViT already has relative positional information in each of the GPSA layers. This also\nshows that the absolute positional information contained in the embeddings is not very useful.\nIn Tab. 6, we explore the relative importance of the positional and content information by masking them off at test time. To\ndo so, we manually set the gating parameter œÉ(Œª) to 1 (no content attention) or 0 (no positional attention). In the Ô¨Årst GPSA\nlayers, both procedures affect performance similarly, signalling that positional and content information are both useful.\nHowever in the last GPSA layers, masking the content information kills performance, whereas masking the positional\ninformation does not, conÔ¨Årming that content information is more crucial.\nModel Mask pos embed No mask\nDeiT-Ti 38.3 72.2\nConViT-Ti 67.1 73.1\nTable 5.Performance on ImageNet with the positional embeddings masked off at test time.\n# layers masked Mask content Mask position No mask\n3 62.3 63.5 73.1\n5 35.0 53.1 73.1\n10 1.3 46.8 73.1\nTable 6.Performance of ConViT-Ti on ImageNet with positional or content attention masked off at test time.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8225035667419434
    },
    {
      "name": "Inductive bias",
      "score": 0.7431643605232239
    },
    {
      "name": "Locality",
      "score": 0.7170910239219666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5904019474983215
    },
    {
      "name": "Convolutional neural network",
      "score": 0.49711063504219055
    },
    {
      "name": "Transformer",
      "score": 0.45993542671203613
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4079510271549225
    },
    {
      "name": "Machine learning",
      "score": 0.39428067207336426
    },
    {
      "name": "Multi-task learning",
      "score": 0.25257229804992676
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": []
}