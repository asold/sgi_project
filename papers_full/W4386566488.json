{
  "title": "Large Language Models are few(1)-shot Table Reasoners",
  "url": "https://openalex.org/W4386566488",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108963801",
      "name": "Wenhu Chen",
      "affiliations": [
        "University of Waterloo",
        "Vector Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3099873751",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3212606841",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2917052767",
    "https://openalex.org/W3098295417",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W3124424060",
    "https://openalex.org/W4225141790",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W2891991579",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4287727281",
    "https://openalex.org/W2970212756",
    "https://openalex.org/W4210451781",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3035620114",
    "https://openalex.org/W4287214436",
    "https://openalex.org/W3037082750",
    "https://openalex.org/W3103667349",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W4298187912",
    "https://openalex.org/W3174986053",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W3034556525",
    "https://openalex.org/W2963319870",
    "https://openalex.org/W4221161695"
  ],
  "abstract": "Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with 'chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1120–1130\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models are few(1)-shot Table Reasoners\nWenhu Chen\nUniversity of Waterloo, Vector Institute\nwenhuchen@uwaterloo.ca\nAbstract\nRecent literature has shown that large language\nmodels (LLMs) are generally excellent few-\nshot reasoners to solve text reasoning tasks.\nHowever, the capability of LLMs on table rea-\nsoning tasks is yet to be explored. In this pa-\nper, we aim at understanding how well LLMs\ncan perform table-related tasks with few-shot\nin-context learning. Specifically, we evalu-\nated LLMs on popular table QA and fact ver-\nification datasets like WikiTableQuestion, Fe-\ntaQA, TabFact, and FEVEROUS and found\nthat LLMs are competent at complex reason-\ning over table structures, though these models\nare not pre-trained on any table corpus. When\ncombined with ‘chain of thoughts’ prompting,\nLLMs can achieve very strong performance\nwith only a 1-shot demonstration, even on par\nwith some SoTA models. We show that LLMs\nare even more competent at generating com-\nprehensive long-form answers on FetaQA than\ntuned T5-large. We further manually studied\nthe reasoning chains elicited from LLMs and\nfound that these reasoning chains are highly\nconsistent with the underlying semantic form.\nWe believe that LLMs can serve as a simple\nyet generic baseline for future research. The\ncode and data are released inhttps://github.\ncom/wenhuchen/TableCoT.\n1 Introduction\nThe problem of structured knowledge grounding\nhas been extensively studied for many years. Ta-\nbles, as one of the most popular (semi)-structured\nforms to store world knowledge receive signifi-\ncant attention from the natural language processing\n(NLP) community. Traditional approaches mostly\nrely on synthesizing executable languages like SQL\nor SPARQL to access the information inside the ta-\nble. However, these symbolic languages normally\nmake a rigid assumption about the table and can-\nnot capture the semantics of text chunks inside the\ntable. Such issues are even more pronounced with\nweb tables due to their irregular forms. To fully\nunderstand web tables, both structured reasoning\nand textual reasoning are required. Such challenges\nhave attracted many researchers to work in the field.\nRecently, a wide range of table-based tasks have\nbeen proposed like table question answering (Pasu-\npat and Liang, 2015; Chen et al., 2020c; Zhu et al.,\n2021; Chen et al., 2021b; Talmor et al., 2020; Chen\net al., 2020a; Nan et al., 2022), table fact verifi-\ncation (Chen et al., 2019; Aly et al., 2021), table-\nbased generation (Chen et al., 2020b; Parikh et al.,\n2020; Nan et al., 2021), and table-grounded con-\nversation (Budzianowski et al., 2018; Nakamura\net al., 2022). This wide range of table-based tasks\nall come with different input-output formats and\ndomains. Due to the heterogeneity of these tasks,\nmodels achieving the best results on these tasks\nnormally need to be fully fine-tuned on the specific\ndownstream dataset with 10K-100K examples to\nachieve reasonable performance.\nRecently, there have been efforts like Unified-\nSKG (Xie et al., 2022) aiming to unify these het-\nerogeneous table-based tasks as a generic text-to-\ntext format. UnifiedSKG has shown that using\nT5-3B (Raffel et al., 2020) with the text-to-text\nformat can already achieve state-of-the-art perfor-\nmance on almost all the table-based tasks without\ntask-specific designs. However, the proposed text-\nto-text models still need to be fully fine-tuned on\nthe downstream tasks. UnifiedSKG also identified\nthat T0-style (Sanh et al., 2022) cross-task transfer\ncan only achieve almost random performance.\nWei et al. (2022); Wang et al. (2022); Zhou et al.\n(2022); Drozdov et al. (2022) have recently dis-\ncovered that large language models (Brown et al.,\n2020; Chowdhery et al., 2022; Ouyang et al., 2022)\ncan be used to solve complex mathematical and\ncommonsense reasoning tasks with few-shot in-\ncontext learning. Inspired by this discovery, we\naim at understanding whether these LLMs can also\nsolve complex table-based reasoning tasks. Though\nthe LLMs are not specifically designed to encode ta-\n1120\nFigure 1: In-context learning for table-related tasks with\nchain-of-thoughts reasoning.\nbles, given the enormous number of tables present\nin the pre-training corpus, we believe they are also\ncompetent at reasoning over table information.\nIn this paper, we experimented with few-shot\nin-context learning for LLMs as depicted in Fig-\nure 1. Instead of fine-tuning the model, we only\nprovide a few examples to showcase the desired\ninput-output format as the condition for the model\nto follow to solve unseen test examples. We ex-\nperiment with several prompting variants including\n(1) direct prediction, (2) Chain of Thoughts (Wei\net al., 2022) (CoT), (3) Chains of thoughts with\nself-consistency (Wang et al., 2022) (CoT+SC).\nWe evaluate these methods on WikiTableQA (Pa-\nsupat and Liang, 2015), FetaQA (Nan et al., 2022),\nTabFact (Chen et al., 2019) and FEVEROUS (Aly\net al., 2021). Our results reveal that LLMs (Ouyang\net al., 2022; Chen et al., 2021a; Chowdhery et al.,\n2022) can achieve striking performance with only\n1 or 2 demonstrations, e.g. 48.8% on WikiTable-\nQuestions and 78.8% on TabFact, which are on par\nsome near-SoTA models (Yu et al., 2021; Eisen-\nschlos et al., 2020). On other datasets like FetaQA\nwith long-form answers, our human evaluation re-\nveals that GPT-3 can significantly outperform the\nfine-tuned T5-large by more than 30% in terms of\ncorrectness and adequacy.\nFurthermore, we manually studied the chain of\nthoughts elicited from LLMs and found that the ra-\ntionale is highly consistent with the ‘ground truth’\nsemantic forms when the model predictions are\ncorrect. We found that these models are surpris-\ningly competent at performing symbolic operations\nover the table, like maximum, minimum, counting,\ncomparison, addition, and difference. However, we\nalso identify several issues of the LLMs on these ta-\nble reasoning tasks: (1) due to the token limitation,\nthe model is unable to generalize to ‘huge’ tables\nwith 30+ rows, which is the major error source, (2)\nLLMs can sometimes make simple mistakes when\nperforming symbolic operations.\nDue to the simplicity and generality, we believe\nLLMs with CoT should be used as an important\nbaseline for any future table-related research.\n2 Related Work\n2.1 Reasoning over Tables\nTable-based reasoning is traditionally accom-\nplished by semantic parsing to execute commands\non tables like WikiTableQuestions (Pasupat and\nLiang, 2015), WikiSQL (Zhong et al., 2017), and\nSpider (Yu et al., 2018). These models aim to\nsynthesize SQL/SPARQL to interact with tables.\nHowever, these machine languages have a rigorous\nrequirement regarding the tables, e.g. the value\nin the same column should follow the same data\ntype. Such rigorous assumptions are frequently vi-\nolated by web tables containing unnormalized free-\nform text in cells. Therefore, language understand-\ning inside the table is essential to achieve a better\nscore. Recently, Yin et al. (2020); Herzig et al.\n(2020); Liu et al. (2021); Deng et al. (2022) have\nproposed to pre-train table and text to learn joint\nrepresentation. These pre-trained models can use\njoint representation to perform reasoning implicitly\nwithout relying on symbolic execution. By pre-\ntraining the model on large-scale crawled or syn-\nthesized data, these models can normally achieve\nthe best-known performance on table tasks. How-\never, these models still require a significant amount\nof fine-tuning on the downstream datasets. Un-\nlike these methods, we are interested in in-context\nlearning, where the model can only learn with a\n1121\nfew examples (demonstration) without any fine-\ntuning. One contemporary work similar to ours\nis BINDER (Cheng et al., 2022), which utilizes\nCodex to synthesize SQL to execute logical forms\nagainst tables for question answering. One big\ndifference is that BINDER (Cheng et al., 2022) in-\nvolves logical form execution, if the execution fails,\nBINDER will fall back to using language models\nto answer the question, which is more similar to\nours.\n2.2 In-context Learning with LLMs\nGPT-3 (Brown et al., 2020) and other large lan-\nguage models demonstrated strong abilities to\nperform few-shot predictions without fine-tuning,\nwhere the model is given a description of the task\nin natural language with few examples. Scaling\nmodel size, data, and computing are crucial to en-\nable this learning ability. Recently, (Rae et al.,\n2021; Smith et al., 2022; Chowdhery et al., 2022;\nDu et al., 2022) have proposed to train different\ntypes of large language models with different train-\ning recipes. The LLMs have demonstrated a strik-\ning capability utilizing the few-shot prompts to\naccomplish unseen tasks without any fine-tuning,\nwhich is found to be an emergent capability not\npresented in smaller language models.\n2.3 Chain of Thoughts Reasoning\nAlthough LLMs (Brown et al., 2020; Chowdhery\net al., 2022) have demonstrated remarkable success\nacross a range of NLP tasks, their ability to demon-\nstrate reasoning is often seen as a limitation. Such\ncapability cannot be acquired simply by scaling up\nthe model size. Recently, the ‘chain of thoughts’\nprompting (Wei et al., 2022) has been discovered to\nempower LLMs to perform complex reasoning over\ntext. By providing the model with several exem-\nplars of reasoning chains, LLMs can learn to follow\nthe template to solve difficult unseen tasks. Later,\nWang et al. (2022) propose to use self-consistency\nwith CoT to further improve performance. Later\non, Kojima et al. (2022) discovered that LLMs can\neven perform reasoning without any demonstra-\ntion by using appropriate prompts. These recent\nfindings reveal the strong capability of LLMs to\nperform complex reasoning. However, the current\nstudies are still heavily focused on text-based tasks\nlike question answering, common sense reasoning,\netc. The models’ capability to reason over tables\nis yet unknown. In this paper, we are specifically\ninterested in understanding LLMs’ capability to\nFigure 2: Prompts used for question answering and fact\nverification tasks.\nreason over web tables with CoT prompting.\n3 Method\nWe experiment with different in-context learning\nmethods to solve the table-based reasoning tasks.\nTo formulate the prompt, we linearize the table\nand concatenate it with a few examples as demon-\nstrations of the language model to predict the out-\nput from an unseen test example. The format\nis described in Figure 2. We mainly investigate\nthree different variants for language model prompt-\ning, including (1) Direct Prediction, (2) Chain\nof Thoughts (CoT), and (3) Chain of Thoughts\n+ Celf-Consistentcy decoding (CoT+SC). For self-\nconsistency methods, we use LLMs to generate\nfive diverse reasoning paths and then use majority\nvoting to select the most voted answer.\nTo limit the budget and constrain the input token\nlength, we truncate the input tables to contain only\n1122\nthe first 22 rows and the first 8 columns. For each\ncell, we truncate the word length to contain only\nthe first 10 words. Through such truncation, we\ncan restrict the input token length to within 2000\ntokens. We will talk about the impact of input token\nlength on the final performance.\n4 Experimental Results\nFor the GPT-3 experiments, we used the four\nprovided models, Ada, Babbage, Curie, and\nDavinci with 350M, 1.3B, 6.7B, and 175B param-\neters respectively. We mainly use Davinci-text-\n002 (Ouyang et al., 2022) in our experiments. We\nalso report results for Codex (Chen et al., 2021a)\n(Davinci-code-002) on some datasets. We use a\ntemperature of 0.7 without any frequency penalty\nand without top-k truncation. We found that the\nmodel performance is robust to the sampling strate-\ngies and the hyper-parameters. These models are\nmainly trained on web-crawled data and code data,\nwithout any specialized training on table corpus.\n4.1 Datasets\nHere we list all of our datasets as follows:\nWikiTableQuestions Pasupat and Liang (2015)\nconsists of complex questions annotated based on\nWikipedia tables. Crowd Workers are asked to\ncompose a series of complex questions that include\ncomparisons, superlatives, aggregation, or arith-\nmetic operations. The annotated dataset is cross-\nvalidated by other crowd workers. In our exper-\niments, we use the unseen test set for evaluation.\nWe evaluate the standard test set with roughly 4000\nquestions. In this dataset, we adopt the answer\nexact match as our evaluation metric.\nFetaQA Nan et al. (2022) consists of free-form\ntable questions. These questions are mostly com-\nplex questions that require integrating information\nfrom discontinuous chunks in the table. Instead of\nhaving short answers, the dataset annotates long\nfree-form answers. Unlike other datasets using\ncopies of short text spans from the source, the ques-\ntions in FetaQA require a high-level understanding.\nWe adopt sacre-BLEU and human evaluation as\nour evaluation metrics. The evaluation set contains\na total of 2003 examples.\nTabFact Chen et al. (2019) consists of both\nsimple and complex claims annotated by crowd\nworkers based on Wikipedia tables. In the simple\nsubset, the claims normally do not involve higher-\norder operations like max/min/count, etc. While\nthe complex subset mainly contains claims involv-\ning higher-order operations. We evaluate the origi-\nnal test set containing 12,779 examples. We report\nbinary classification accuracy on the set.\nFEVEROUS Aly et al. (2021) consists of com-\npositional claims annotated by crowd workers\nregarding Wikipedia tables. Since the dataset\ncontains both table-supported and text-supported\nclaims. We filter out text-supported claims and only\nkeep the 2,295 table-supported claims as our test\nset. Different from TabFact, FEVEROUS consists\nof more complex tables with irregular structures\nlike multi-row, multi-column, multi-table, etc. We\nreport dev-set accuracy.\n4.2 Baselines\nIn these experiments, we mainly consider the fol-\nlowing baseline models.\nPre-trained Encoder-Decoder Model Pre-\ntrained encoder-decoder model is one of our\ncompetitors, which aims to encode the table as a\nplain sequence into the encoder, and then apply\nthe decoder to generate either an answer or a\nverdict. In this paper, we mainly compare against\nT5 (Raffel et al., 2020) and BART (Lewis et al.,\n2020) as our baselines.\nPre-trained Table Understanding ModelThis\nfamily of models is specifically pre-trained on the\ntable-related corpus, which utilizes specific archi-\ntecture to encode table structure and handle sym-\nbolic computation. In this paper, we mainly con-\nsider TAPAS (Herzig et al., 2020), TABERT (Yin\net al., 2020), and TAPEX (Liu et al., 2021).\nNeural Symbolic Model This family of models\nincludes a non-pre-trained neural symbolic model,\nwhich can synthesize machine language to interact\nwith the table. This line of work includes Logic-\nFactChecker (Zhong et al., 2020), Neural-Symbolic\nMachine (Liang et al., 2018), etc.\n4.3 Main Results\nHere we show our main results for different\ndatasets as follows.\nWikiTableQuestions As can be seen from Ta-\nble 1, directly asking GPT-3 to generate answers\ncan only lead to 26% EM score. However, if we\nprompt the model with the CoT demonstrations,\n1123\nType Model Test EM\nTrain Pasupat and Liang (2015) 37.1\nTrain Zhang et al. (2017) 43.7\nTrain Liang et al. (2018) 43.7\nTrain Agarwal et al. (2019) 44.1\nTrain Wang et al. (2019) 44.5\nPT + FT Herzig et al. (2020) 48.8\nPT + FT Yu et al. (2021) 52.7\n1-shot GPT-3 Direct 24.0\n2-shot GPT-3 Direct 27.3\n1-shot GPT-3 CoT 44.2\n2-shot GPT-3 CoT 45.7\n2-shot Codex CoT 48.8\nTable 1: Experimental Results on WikiTableQuestions.\nPT means pre-training and FT means fine-tuning.\nGPT-3 is more likely to follow the logical operation\nto derive the answers. With two demonstrations,\nGPT-3 can achieve roughly 46% EM score. By\nswitching from GPT-3 to Codex, we are able to fur-\nther improve the EM score to over 48.8%. These\nresults are particularly surprising given that TAPAS\nhas a built-in module to complete symbolic oper-\nations, while GPT-3 was not trained on any table-\nspecific dataset. These results demonstrate GPT-3’s\nbuilt-in capabilities to perform diverse types of rea-\nsoning over tables.\nFetaQA As demonstrated in Table 2, we compare\nGPT-3 with different fine-tuned models from Nan\net al. (2022). Unlike the other datasets with short\nphrase answers, the goal of this dataset is to gen-\nerate a complete long-form answer. Unlike Wik-\niTableQuestion, the questions normally do not in-\nvolve complex operations like max, min, compare,\naverage, etc. The long-form answer is similar to\nthe role of CoT. Therefore, we only applied ‘di-\nrect generation’ in this experiment. In terms of\nBLEU score (Papineni et al., 2002), GPT-3 is still\na bit behind the fine-tuned T5-large. However, the\nBLEU score cannot reflect the faithfulness and cor-\nrectness of the model generation. Thus, we fol-\nlow Nan et al. (2021) to do human evaluation over\nthe four aspects: (1) fluency (whether the generated\nsentence contains the linguistic error), (2) correct-\nness (whether the generated sentence answers the\nquestion correctly), (3) faithfulness (whether the\ngenerated sentence is grounded on the input table),\nand (4) adequacy (whether the generated sentence\nis comprehensive enough to cover all the answers).\nWe list our results in Table 3. Similarly, we also\nsample 100 model predictions and manually evalu-\nate their quality and adopt binary scores for each\nType Model sacreBLEU\nzero-shot Pipeline (Nan et al., 2022) 9.16\nFT Pipeline (Nan et al., 2022) 11.00\nFT T5-small (Nan et al., 2022) 21.60\nFT T5-base (Nan et al., 2022) 28.14\nFT T5-large (Nan et al., 2022) 30.54\n1-shot GPT-3 Direct 26.88\n2-shot GPT-3 Direct 27.02\nTable 2: Experimental Results on FetaQA. PT means\npre-training and FT means fine-tuning.\nSource Fluency Correct Adequate Faithful\nPipeline 85.2 25.4 23.6 23.6\nT5-large 94.6 54.8 50.4 50.4\nHuman 95.0 92.4 95.6 95.6\nGPT-3 98.0 84.0 78.0 90.0\nTable 3: Human Evaluation Results on FetaQA.\nexample. As can be seen, GPT-3 can significantly\noutperform T5-large over all the aspects, i.e. more\nthan 30% improvement over correctness, adequacy,\nand faithfulness. The evaluation indicates that the\nmodel output is almost on par with the average\nhuman performance on this dataset.\nTabFact As demonstrated in Table 4, we com-\npare GPT-3 against the other pre-trained and fine-\ntuned models including TAPAS (Eisenschlos et al.,\n2020), TAPEX (Liu et al., 2021), etc. We show\nthat GPT-3 direct prediction is already getting a de-\ncent accuracy of 72%, which is slightly higher than\nLogic FactChecker (Zhong et al., 2020). When\ncombined with CoT reasoning, the model accu-\nracy increases to over 77%. Similar to before, we\nfound that Codex can generate more accurate rea-\nsoning chains, thus achieving better accuracy of\n78.8%, which is only 2% lower than pre-trained\ntable understanding model TAPAS (Eisenschlos\net al., 2020). The more intriguing property about\nLLM + CoT is that the intermediate rationale can\nbe produced without any training. All the existing\ntrained models do not have the capability to pro-\nduce the intermediate reasoning steps due to the\nlack of annotation in the dataset.\nFEVEROUS We demonstrate our results on\nFEVEROUS dev-set in Table 5 and compare\ndifferent-sized UnifiedSKG models (built with T5).\nWe found that GPT-3’s performance with direct\nprediction is similar to UnifiedSKG-base. Similar\nto TabFact, we found that the model performance\ncan be boosted with ‘chain of thoughts’ prompt-\n1124\nType Model Overall\nFT Chen et al. (2019) 65.1\nFT Zhong et al. (2020) 71.1\nFT Zhang et al. (2020) 73.2\nFT Yang et al. (2020) 74.4\nFT Lewis et al. (2020) 82.5\nPT + FT Eisenschlos et al. (2020) 81.0\nPT + FT Liu et al. (2021) 84.2\n1-shot GPT-3 Direct 72.0\n2-shot GPT-3 Direct 73.9\n1-shot GPT-3 CoT 75.5\n2-shot GPT-3 CoT 76.0\n1-shot GPT-3 CoT+SC 77.3\n2-shot Codex CoT 78.8\nTable 4: Experimental Results on TabFact. PT means\npre-training and FT means fine-tuning.\nType Model Dev Set\nFT Aly et al. (2021) 82.23\nFT UnifiedSKG-base (Xie et al., 2022) 75.05\nFT UnifiedSKG-large (Xie et al., 2022) 79.81\nFT UnifiedSKG-3B (Xie et al., 2022) 82.40\n1-shot GPT-3 Direct 74.20\n2-shot GPT-3 Direct 75.22\n1-shot GPT-3 CoT 75.70\n2-shot GPT-3 CoT 76.44\n1-shot GPT-3 CoT+SC 77.22\nTable 5: Experimental Results on FEVEROUS. PT\nmeans pre-training and FT means fine-tuning.\ning. The best-performing model is roughly between\nUnifiedSKG-base and UnifiedSKG-large. Com-\npared to TabFact, the model’s overall performance\nis weaker mainly because the table structure in\nFEVEROUS is more irregular, containing lots of\nsegments and subtables. Such structural difficulties\npose great challenges to GPT-3.\nModel Scaling We investigate the model scal-\ning’s impact on the final performance and plot our\nfindings in Figure 3. On the WebTableQuestions\ndataset, we found that model size is essential for\nachieving the best performance. As can be seen,\nthe 6.7B GPT-3 model is only achieving half of the\nperformance of the 175B GPT-3 model. Similarly,\non TabFact, we found that the smaller models with\n6.7B or fewer parameters are almost getting ran-\ndom accuracy, which is even worse than QA tasks.\nThis again suggests that LLMs’ reasoning ability\nover web tables is emergent as the model scales up.\n4.4 Case Study\nWe demonstrate a few examples in Figure 4 where\nGPT-3 makes correct predictions. In the first exam-\nple, GPT-3 is able to first identify all the Belgian\n0.35B 1.3B 6.7B 175B\n14.5\n20.2 22\n46.450.7 50.3 52.6\n77.2WikiTableQuestions TabFact\nFigure 3: The model performance with respect to model\nsize on WikiTableQuestions and TabFact.\nriders from the table and then perform the addi-\ntion of 3+3+1=7 precisely. In the second example,\nGPT-3 can identify the players with the position\nof ‘d’ and count the number correctly to refute a\nfalse claim. In the third example, we can see that\nGPT-3 is able to associate multiple blocks of in-\nformation to generate a comprehensive long-form\nanswer. The elicited ‘chain of thoughts’ in these\nexamples are highly aligned with the underlying\nsemantic forms. These findings suggest that LLMs\nlike GPT-3 can provide high-quality explanations\nto justify their decision-making.\nWe also provide a few mistakes made by GPT-3\nin Figure 5. In the first example, GPT-3 miscounts\nthe ‘number of countries above 1 billion box office’\nbecause it misidentifies ‘world’ also as a country.\nIn the second example, GPT-3 misunderstood ‘2nd\nhighest’ as ‘highest’, which leads to prediction er-\nror. In the last example, GPT-3 misunderstands the\nsemantics of the question and answers ‘left office\ntime’ instead of ‘took office time’. These examples\nshow the typical errors of grounding the inputs to\nthe wrong rows or columns of the table.\n4.5 Analysis\nImpact of Number of ShotsFirst of all, we con-\nduct an ablation study to understand the impact\nof a number of shots in the final performance. In\norder to control the budget, we only sample 200\nsamples from WikiTableQuestions, TabFact and\nFEVEROUS for this ablation study. As can be seen\nfrom Figure 7, GPT-3 is not quite sensitive to the\nnumber of provided demonstrations. Increasing\nfrom 1-shot to 2-shot can often benefit the model,\nhowever, increasing the shot number further does\nnot yield more performance gain. We conjecture\nthat instruct fine-tuning used in GPT-3 (Ouyang\net al., 2022) can easily extrapolate the task mean-\ning, thus, having a single demonstration is already\nenough for the model to understand the task.\n1125\nFigure 4: ‘Correct’ predictions from WikiTableQues-\ntions, TabFact, and FetaQA datasets, where the ‘blue’\ntext are the outputs from the GPT-3, ‘red’ means the\ncorrect rows to reference.\nQuality Evaluation of Reasoning ChainsWe\nconduct a human evaluation to assess whether GPT-\n3 is making the correct prediction with the correct\nreasons. Specifically, we sample 100 reasoning\npaths from the correctly predicted examples and\nmanually study whether these reasoning chains are\ngrounded on the table or simply ‘hallucination’. As\ncan be seen from Figure 7, we found that around\n90% of reasoning chains are faithful to the infor-\nFigure 5: ‘Wrong’ predictions from WikiTableQues-\ntions, TabFact, and FetaQA datasets, where ‘blue’ text\nare the outputs from the GPT-3, ‘red’ means the region\nof the correct cell to reference, and ‘green’ means the\nreference trusted by GPT-3.\nmation in the table, and only less than 10% of the\nreasoning chains are hallucinated. Based on this\n1126\n0 1 2 3 4 5\nnum of shots\nWikiTQ TabFct FEV\nFigure 6: k-shot ablation study over WikiTableQues-\ntions and TabFact and FEVEROUS.\ncorrect wrong no reason\n94\n6\n0\n86\n10\n4\n88\n4 8\nWikiTQ TabFact FEV\nFigure 7: human evaluation of ‘reasoning chains’ in\nWikiTableQuestions, TabFact, and FEVEROUS.\nevaluation, we believe that LLMs are not guessing\nthe answers correctly by chance.\nWe believe these ‘reasoning chains’ are useful\nin many aspects: (1) the chains can provide a ra-\ntionale to humans to justify the decision-making\nprocess. (2) one of the notorious annotation tasks\nis to annotate the ‘underlying’ semantic form for\nmany NLP tasks, which require expertise for hu-\nman annotators, on the other hand, the annotation\ncost is huge. Using GPT-3 to demonstrate useful\nnatural language ‘semantic forms’ could potentially\ngreatly lower the annotation burden of these tasks.\nImpact of Table Size An important factor for\nmodel performance is the size of the table. Here\nwe want to understand how relevant the model per-\nformance is w.r.t the input table length. We group\nthe table token length into different groups like\n‘0-100’, ‘100-200’, etc, and plot the group-wise ac-\ncuracy for WikiTables and TabFact in Figure 8. As\ncan be seen from the table, we found that GPT-3’s\nperformance is highly sensitive to the table size. As\nthe table size grows, the accuracy almost decreases\nmonotonically. After the table size exceeds 1000\ntokens (e.g. 1500 word pieces), GPT-3’s perfor-\nmance almost degrades to random guesses. This\nablation study reveals one of the drawbacks of us-\ning LLMs for table reasoning. To further enhance\nLLMs’ performance, we need to develop better\nmethods to maintain more consistent performance\nacross different-sized tables.\n0 200 400 600 800 1,000 1,200\ntable size in token length\nTabFct WikiTQ\nFigure 8: Model performance on WikiTableQuestions\nand TabFact w.r.t the input table size.\nDiscussions In this study, we investigate the pos-\nsibilities of prompting LLMs to perform complex\nreasoning tasks over tables. However, we do not\nbelieve LLM prompting can replace the existing\nsymbolic methods. LLMs have several favorable\nproperties: (1) no annotation is needed, and (2)\nthe functional coverage is broader than symbolic\nmethods. However, LLM prompting exhibits un-\npredictable randomness and cannot generalize to\nlarge tables. In contrast, symbolic models are (1)\nagnostic to the table size, and (2) can reliably per-\nform designed functions without much randomness.\nBut they in general require a significant amount of\nannotated data to learn.\nIn conclusion, these two types of models are\ncomplementary to each other. To push the limit\nforward, we need to investigate how to combine\nthe merits of these two types of methods. For ex-\nample, the symbolic methods can perform certain\noperations to narrow down to a targeted region in\nthe table, and then LLMs can be used to reason\nover the limited information.\n5 Conclusion\nIn this paper, we investigate whether the current\nLLMs (GPT-3) can be directly utilized to perform\ntable reasoning tasks. Surprisingly, though LLMs\nare not optimized for table-based tasks, we found\nthese models highly competent in performing com-\nplex table reasoning tasks, especially when com-\nbined with ‘chain of thoughts’ prompting. We be-\nlieve this study can open new possibilities for LLM\napplication in table-related tasks to either directly\npredict the output or to serve as an auxiliary tool\nfor annotating complex intermediate forms.\n1127\nLimitations\nOur approach has several limitations: (1) the pro-\nposed approach is still far from state-of-the-art\nperformance, and there is still room for improve\nbefore it can be used as an alternative. (2) the\nmethod is still costly, we show that the model can\nonly achieve superior performance when scaling\nup. Smaller-sized models are still weak at table\nreasoning. Therefore, we need to consider how\nto empower smaller models with such reasoning\ncapabilities.\nReferences\nRishabh Agarwal, Chen Liang, Dale Schuurmans, and\nMohammad Norouzi. 2019. Learning to generalize\nfrom sparse and underspecified rewards. In Interna-\ntional conference on machine learning, pages 130–\n140. PMLR.\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. The fact extraction and verification\nover unstructured and structured information\n(feverous) shared task. In Proceedings of the Fourth\nWorkshop on Fact Extraction and VERification\n(FEVER), pages 1–13.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016–5026.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021a. Evaluating large lan-\nguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWenhu Chen, Ming-Wei Chang, Eva Schlinger,\nWilliam Yang Wang, and William W Cohen. 2020a.\nOpen question answering over tables and text. In In-\nternational Conference on Learning Representations.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020b. Logical natural lan-\nguage generation from open-domain tables. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7929–\n7942.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-scale\ndataset for table-based fact verification. In Interna-\ntional Conference on Learning Representations.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong,\nHong Wang, and William Yang Wang. 2020c. Hy-\nbridqa: A dataset of multi-hop question answering\nover tabular and textual data. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 1026–1036.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan R Routledge,\net al. 2021b. Finqa: A dataset of numerical reasoning\nover financial data. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3697–3711.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\net al. 2022. Binding language models in symbolic\nlanguages. arXiv preprint arXiv:2210.02875.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong\nYu. 2022. Turl: Table understanding through repre-\nsentation learning. ACM SIGMOD Record, 51(1):33–\n40.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.\nGlam: Efficient scaling of language models with\nmixture-of-experts. In International Conference on\nMachine Learning, pages 5547–5569. PMLR.\nJulian Eisenschlos, Syrine Krichene, and Thomas\nMueller. 2020. Understanding tables with interme-\ndiate pre-training. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n281–296.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMueller, Francesco Piccinno, and Julian Eisensch-\nlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4320–4333.\n1128\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nChen Liang, Mohammad Norouzi, Jonathan Berant,\nQuoc V Le, and Ni Lao. 2018. Memory augmented\npolicy optimization for program synthesis and se-\nmantic parsing. Advances in Neural Information\nProcessing Systems, 31.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. In International Conference on Learning\nRepresentations.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen,\nand William Yang Wang. 2022. Hybridialogue: An\ninformation-seeking dialogue dataset grounded on\ntabular and textual data. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2022, pages\n481–492.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria\nLin, Neha Verma, Rui Zhang, Wojciech Kry´sci´nski,\nNick Schoelkopf, Riley Kong, Xiangru Tang, et al.\n2022. Fetaqa: Free-form table question answering.\nTransactions of the Association for Computational\nLinguistics, 10:35–49.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, et al.\n2021. Dart: Open-domain structured data record to\ntext generation. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 432–447.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. 2020. Totto: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables zero-\nshot task generalization. In The Tenth International\nConference on Learning Representations.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,\nYizhong Wang, Akari Asai, Gabriel Ilharco, Han-\nnaneh Hajishirzi, and Jonathan Berant. 2020. Mul-\ntimodalqa: complex question answering over text,\ntables and images. In International Conference on\nLearning Representations.\nBailin Wang, Ivan Titov, and Mirella Lapata. 2019.\nLearning semantic parsers from denotations with la-\ntent structured alignments and abstract programs. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3774–3785.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\n1129\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. Unifiedskg:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. arXiv\npreprint arXiv:2201.05966.\nXiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang\nChen, and Xiaodan Zhu. 2020. Program enhanced\nfact verification with verbalization and graph atten-\ntion network. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 7810–7825.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. Tabert: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir R Radev,\nRichard Socher, and Caiming Xiong. 2021. Grappa:\nGrammar-augmented pre-training for table semantic\nparsing. In ICLR.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3911–3921.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629.\nYuchen Zhang, Panupong Pasupat, and Percy Liang.\n2017. Macro grammars and holistic triggering for\nefficient semantic parsing. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1214–1223.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\nWanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan,\nMing Zhou, Ming Gong, Linjun Shou, Daxin Jiang,\nJiahai Wang, and Jian Yin. 2020. Logicalfactchecker:\nLeveraging logical operations for fact checking with\ngraph module network. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 6053–6065.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\nTat-Seng Chua. 2021. Tat-qa: A question answering\nbenchmark on a hybrid of tabular and textual content\nin finance. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3277–3287.\n1130",
  "topic": "Table (database)",
  "concepts": [
    {
      "name": "Table (database)",
      "score": 0.6712635159492493
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6267147064208984
    },
    {
      "name": "Computer science",
      "score": 0.5715676546096802
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5195748805999756
    },
    {
      "name": "Code (set theory)",
      "score": 0.4175226390361786
    },
    {
      "name": "Natural language processing",
      "score": 0.4173389673233032
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3534090518951416
    },
    {
      "name": "Programming language",
      "score": 0.17099803686141968
    },
    {
      "name": "Database",
      "score": 0.15413016080856323
    },
    {
      "name": "Geography",
      "score": 0.12526994943618774
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}