{
  "title": "Is Your Language Model Ready for Dense Representation Fine-tuning?",
  "url": "https://openalex.org/W3153507317",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5060885692",
      "name": "Luyu Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009879041",
      "name": "Jamie Callan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2593864460",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2794557536"
  ],
  "abstract": "Pre-trained language models (LM) have become go-to text representation encoders. Prior research used deep LMs to encode text sequences such as sentences and passages into single dense vector representations. These dense representations have been used in efficient text comparison and embedding-based retrieval. However, dense encoders suffer in low resource situations. Many techniques have been developed to solve this problem. Despite their success, not much is known about why this happens. This paper shows that one cause lies in the readiness of the LM to expose its knowledge through dense representation in fine-tuning, which we term Optimization Readiness. To validate the theory, we present Condenser, a general pre-training architecture based on Transformer LMs, to improve dense optimization readiness. We show that fine-tuning from Condenser significantly improves performance for small and/or noisy training sets.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7449793219566345
    },
    {
      "name": "Encoder",
      "score": 0.723571240901947
    },
    {
      "name": "Embedding",
      "score": 0.711733877658844
    },
    {
      "name": "Language model",
      "score": 0.6958640813827515
    },
    {
      "name": "Transformer",
      "score": 0.6786304712295532
    },
    {
      "name": "Representation (politics)",
      "score": 0.6333790421485901
    },
    {
      "name": "Fine-tuning",
      "score": 0.5132288932800293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4950670301914215
    },
    {
      "name": "ENCODE",
      "score": 0.4553811848163605
    },
    {
      "name": "Natural language processing",
      "score": 0.3776955306529999
    },
    {
      "name": "Machine learning",
      "score": 0.35506922006607056
    },
    {
      "name": "Engineering",
      "score": 0.07690107822418213
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}