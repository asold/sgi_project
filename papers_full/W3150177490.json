{
  "title": "TubeR: Tube-Transformer for Action Detection.",
  "url": "https://openalex.org/W3150177490",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5072790694",
      "name": "Jiaojiao Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100406106",
      "name": "Xinyu Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100761540",
      "name": "Chunhui Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5048015379",
      "name": "Bing Shuai",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100353664",
      "name": "Hao Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024508073",
      "name": "Cees G. M. Snoek",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002114219",
      "name": "Joseph Tighe",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949343965",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3035503132",
    "https://openalex.org/W3035029089",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963529931",
    "https://openalex.org/W2034014085",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2962790054",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W3094798944",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2954431383",
    "https://openalex.org/W2884146233",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2611596598",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2981808500",
    "https://openalex.org/W3035413240",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2962722947",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1923332106",
    "https://openalex.org/W1797109199",
    "https://openalex.org/W2964318666",
    "https://openalex.org/W2982515679",
    "https://openalex.org/W3096824106",
    "https://openalex.org/W2618799552",
    "https://openalex.org/W3037777866",
    "https://openalex.org/W2883275382",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2962688385"
  ],
  "abstract": "In this paper, we propose TubeR: the first transformer based network for end-to-end action detection, with an encoder and decoder optimized for modeling action tubes with variable lengths and aspect ratios. TubeR does not rely on hand-designed tube structures, automatically links predicted action boxes over time and learns a set of tube queries related to actions. By learning action tube embeddings, TubeR predicts more precise action tubes with flexible spatial and temporal extents. Our experiments demonstrate TubeR achieves state-of-the-art among single-stream methods on UCF101-24 and J-HMDB. TubeR outperforms existing one-model methods on AVA and is even competitive with the two-model methods. Moreover, we observe TubeR has the potential on tracking actors with different actions, which will foster future research in long-range video understanding.",
  "full_text": "TubeR: Tubelet Transformer for Video Action Detection\nJiaojiao Zhao1*, Yanyi Zhang2*, Xinyu Li3*, Hao Chen3, Bing Shuai3, Mingze Xu3, Chunhui Liu3,\nKaustav Kundu3, Yuanjun Xiong3, Davide Modolo3, Ivan Marsic2, Cees G.M. Snoek1, Joseph Tighe3\n1University of Amsterdam 2Rutgers University 3AWS AI Labs\nAbstract\nWe propose TubeR: a simple solution for spatio-temporal\nvideo action detection. Different from existing methods that\ndepend on either an off-line actor detector or hand-designed\nactor-positional hypotheses like proposals or anchors, we\npropose to directly detect an action tubelet in a video by si-\nmultaneously performing action localization and recognition\nfrom a single representation. TubeR learns a set of tubelet-\nqueries and utilizes a tubelet-attention module to model the\ndynamic spatio-temporal nature of a video clip, which ef-\nfectively reinforces the model capacity compared to using\nactor-positional hypotheses in the spatio-temporal space.\nFor videos containing transitional states or scene changes,\nwe propose a context aware classification head to utilize\nshort-term and long-term context to strengthen action classi-\nfication, and an action switch regression head for detecting\nthe precise temporal action extent. TubeR directly produces\naction tubelets with variable lengths and even maintains\ngood results for long video clips. TubeR outperforms the\nprevious state-of-the-art on commonly used action detection\ndatasets AVA, UCF101-24 and JHMDB51-21. Code will be\navailable on GluonCV(https://cv.gluon.ai/).\n1. Introduction\nThis paper tackles the problem of spatio-temporal human\naction detection in videos [3, 17, 39], which plays a central\nrole in advanced video search engines, robotics, and self-\ndriving cars. Action detection is a compound task, requir-\ning the localization of per-frame person instances, the link-\ning of these detected person instances into action tubes and\nthe prediction of their action class labels. Two approaches\nfor spatio-temporal action detection are prevalent in the lit-\nerature: frame-level detection and tubelet-level detection.\nFrame-level detection approaches detect and classify the ac-\ntion independently on each frame [14, 29, 32], and then link\nper-frame detections together into coherent action tubes. To\ncompensate for the lack of temporal information, several\n*Equally contributed and work done while at AWS AI Labs\nt\nFencingFencing\nt\nTubeR\nt\nperson detection\nhypotheses on fixed action location\nseq-to-seqno detectorno proposalno hypotheses\nhypotheses on cuboid anchor\ncuboid refine\nt\nanchor\nFigure 1. TubeR takes as input a video clip and directly outputs\ntubelets: sequences of bounding boxes and their action labels. Tu-\nbeR runs end-to-end without person detectors, anchors or proposals.\nmethods simply repeat 2D proposals [12, 15, 35] or offline\nperson detections [9, 28, 37, 43] over time to obtain spatio-\ntemporal features (Figure 1 top left).\nAlternatively, tubelet-level detection approaches [16, 19,\n26, 33, 45, 49], directly generate spatio-temporal volumes\nfrom a video clip to capture the coherence and dynamic na-\nture of actions. They typically predict action localization and\nclassification jointly over spatio-temporal hypotheses, like\n3D cuboid proposals [16, 19] (Figure 1 top right). Unfortu-\nnately, these 3D cuboids can only capture a short period of\ntime, also when the spatial location of a person changes as\nsoon as they move, or due to camera motion. Ideally, this\nfamily of models would use flexible spatio-temporal tubelets\nthat can track the person over a longer time, but the large\nconfiguration space of such a parameterization has restricted\nprevious methods to short cuboids. In this work we present\na tubelet-level detection approach that is able to simulta-\nneously localize and recognize action tubelets in a flexible\nmanner, which allows tubelets to change in size and location\nover time (Figure 1 bottom). This allows our system to lever-\nage longer tubelets, which aggregate visual information of a\nperson and their actions over longer periods of time.\nWe draw inspiration from sequence-to-sequence mod-\nelling in natural language processing (NLP), particularly\nmachine translation [21, 24, 36, 40], and its application to\nobject detection, DETR [4]. Being a detection framework,\nDETR can be applied as a frame-level action detection ap-\nproach trivially, but the power of the transformer framework,\non which DETR is built, is its ability to generate complex\nstructured outputs over sequences. In NLP, this typically\ntakes the form of sentences but in this work we use the no-\ntion of decoder queries to represent people and their actions\nover video sequences, without having to restrict tubelets to\nfixed cuboids.\nWe propose a tubelet-transformer, we call TubeR, for\nlocalizing and recognizing actions from a single representa-\ntion. Building on the DETR framework [4], TubeR learns\na set of tubelet queries to pull action-specific tubelet-level\nfeatures from a spatio-temporal video representation. Our\nTubeR design includes a specialized spatial and temporal\ntubelet attention to allow our tubelets to be unrestricted in\ntheir spatial location and scale over time, thus overcoming\nprevious limitations of methods restricted to cuboids. Tu-\nbeR regresses bounding boxes within a tubelet jointly across\ntime, considering temporal correlations between tubelets,\nand aggregates visual features over the tubelet to classify\nactions. This core design already performs well, outperform-\ning many previous model designs, but still does not improve\nupon frame-level approaches using offline person detectors.\nWe hypothesize that this is partially due to the lack of more\nglobal context in our query based feature as it is hard to clas-\nsify actions referring to relationships such as ‘listening-to’\nand ‘talking-to’ by only looking at a single person. There-\nfore, we introduce a context aware classification head that,\nalong with the tubelet feature, takes the full clip feature from\nwhich our classification head can draw contextual informa-\ntion. This design allows the network to effectively relate a\nperson tubelet to the full scene context where the tubelet\nappears and is shown to be effective on its own in our results\nsection. One limitation of this design is the context feature\nis only drawn from the same clip our tubelet occupies. It has\nbeen shown [43] to be important to also include long term\ncontextual features for the final action classification. Thus,\nwe introduce a memory system inspired by [44] to compress\nand store contextual features from video content around the\ntubelet. We feed this long term contextual memory to our\nclassification head using the same feature injection strategy\nand again show this gives an important improvement over\nthe short term context alone. We test our full system on\nthree popular action detection datasets (A V A [15], UCF101-\n24 [34] and JHMDB51-21 [18]) and show our method can\noutperform other state-of-the-art results.\nIn summary, our contributions are as follows:\n1. We propose TubeR: a tubelet-level transformer frame-\nwork for human action detection.\n2. Our tubelet query and attention based formulation is\nable to generate tubelets of arbitrary location and scale.\n3. Our context aware classification head is able to aggre-\ngate short-term and long-term contextual information.\n4. We present state-of-the-art results on three challenging\naction detection datasets.\n2. Related Work\nFrame-level action detection. Spatio-temporal action de-\ntection in video has a long tradition, e.g. [3, 15, 17, 28, 29, 37,\n39, 42]. Inspired by object detection using deep convolution\nneural networks, action detection in video has been consider-\nably improved by frame-level methods [29,31,32,42]. These\nmethods perform localization and recognition per-frame and\nthen link frame-wise predictions to action tubes. Specifically,\nthey apply 2D positional hypotheses (anchors) or an offline\nperson detector on a keyframe for localizing actors, and then\nfocus more on improving action recognition. They incorpo-\nrate temporal patterns by an extra stream utilizing optical\nflow. Others [12, 15, 35] apply 3D convolution networks to\ncapture temporal information for recognizing actions. Feicht-\nenhofer et al. [9] present a slowfast network to even better\ncapture spatio-temporal information. Both Tang et al. [37]\nand Pan et al. [28] propose to explicitly model relations be-\ntween actors and objects. Recently, Chenet al. [5] propose to\ntrain actor localization and action classification end-to-end\nfrom a single backbone. Different from these frame-level\napproaches, we target on tubelet-level video action detec-\ntion, with a unified configuration to simultaneously perform\nlocalization and recognition.\nTubelet-level action detection. Detecting actions by tak-\ning a tubelet as a representation unit [23, 26, 33, 45, 49] has\nbeen popular since it was proposed by Jain et al. [17]. Kalo-\ngeiton et al. [19] repeat 2D anchors per-frame for pooling\nROI features and then stack the frame-wise features to pre-\ndict action labels. Hou et al. [16] and Yanget al. [45] depend\non carefully-designed 3D cuboid proposals. The former di-\nrectly detects tubelets and the later progressively refines 3D\ncuboid proposals across time. Besides box/cuboid anchors,\nLi et al. [26] detect tubelet instances by relying on center\nposition hypotheses. Hypotheses-based methods have diffi-\nculties to process long video clips, as we discussed in the\nintroduction. We add to the tubelet tradition by learning a\nsmall set of tubelet queries to represent the dynamic nature\nof tubelets. We reformulate the action detection task as a\nsequence-to-sequence learning problem and explicitly model\nthe temporal correlations within a tubelet. Our method is ca-\npable to handle long video clips.\nTransformer-based action detection. Vaswani et al. [40]\nproposed the transformer for machine translation, which\nsoon after became the most popular backbone for sequence-\nto-sequence tasks, e.g., [21, 24, 36]. Recently, it has also\ndemonstrated impressive advances in object detection [4,50],\nimage classification [6, 46] and video recognition [7, 10, 47].\nGirdhar et al. [13] propose a video action transformer net-\nwork for detecting actions. They apply a region-proposal-\nnetwork for localization. The transformer is utilized for fur-\nther improving action recognition by aggregating features\nfrom the spatio-temporal context around actors. We propose\na unified solution to simultaneously localize and recognize\nactions.\n3. Action Detection by TubeR\nIn this section, we present our TubeR that takes as input a\nvideo clip and directly outputs a tubelet: a sequence of bound-\ning boxes and the action label. The TubeR design takes inspi-\nration from the image-based DETR [4] but reformulates the\ntransformer architecture for sequence-to-sequence(s) model-\ning in video (Figure 2).\nGiven a video clip I ∈ RTin×H×W×C where\nTin, H, W, Cdenote the number of frames, height, width, and\ncolour channels, TubeR first applies a 3D backbone to extract\nvideo feature Fb ∈ RT′×H′×W′×C′\n, where T′ is the tempo-\nral dimension and C′ is the feature dimension. A transformer\nencoder-decoder is then utilized to transform the video fea-\nture into a set of tubelet-specific feature Ftub ∈ RN×Tout×C′\n,\nwith Tout the output temporal dimension and N the num-\nber of tubelets. In order to process long video clips, we use\ntemporal down-sampling to make Tout < T′ < Tin, which\nreduces our memory requirement. In this case, TubeR gen-\nerates sparse tubelets. For short video clips we remove the\ntemporal down-sampling to make sure Tout=T′=Tin, which\nresults in dense tubelets. Tubelet regression and associated\naction classification can be achieved simultaneously with\nseparated task heads as:\nycoor = f(Ftub); yclass = g(Ftub), (1)\nwhere f denotes the tubelet regression head and ycoor ∈\nRN×Tout×4 stands for the coordinates of N tubelets, each of\nwhich is across Tout frames (or Tout sampled frames for long\nvideo clips). Here g denotes the action classification head,\nand yclass ∈ RN×L stands for the action classification for N\ntubelets with L possible labels.\n3.1. TubeR Encoder\nDifferent from the vanilla transformer encoder, the TubeR\nencoder is designed for processing information in the 3D\nspatio-temporal space. Each encoder layer is made up of\na self-attention layer (SA), two normalization layers and a\nfeed forward network (FFN), following [40]. We only put\nthe core attention layers in all equations below.\nFen = Encoder(Fb), (2)\nSA(Fb) =softmax( σq(Fb) × σk(Fb)T\n√\nC′ ) × σv(Fb), (3)\nσ(∗) =Linear(∗) +Embpos, (4)\ncross-attention layer(CA)\ntubeletqueries\nself-attention layer(SA)DecoderEncoder\n!\ntubelet-attention layer(TA)\nI3D cross-attention layerself-attention layertemporal poolclassification head\"#\ntubeletlocationsactionswitchFFN$×&'()×4$×&'()$×+\n3D positional encoding\n×,×,\n+ regression headFFNFFN\"-.#\n\"/0 \"1\naction scores\n\"2\n\"-.#\nFigure 2. The overall structure of TubeR. Both encoder and decoder\ncontain n stacked modules. We only show the key components in\nthe encoder and decoder modules. The encoder models the spatio-\ntemporal features from the backboneFb by self-attention layers (see\nSection 3.1). The decoder transforms a set of tubelet queries Q and\ngenerates tubelet-level features Ftub. We utilize tubelet-attention\nlayers to model the relations between box query embeddings within\na tubelet (see Section 3.2). Finally, we apply the context aware\nclassification head and action switch regression head to predict\ntubelet labels and coordinates (see Section 3.3).\nwhere Fb is the backbone feature and Fen ∈ RT′H′W′×C′\ndenotes the C′ dimensional encoded feature embedding. The\nσ(∗) is the linear transformation plus positional embedding.\nEmbpos is the 3D positional embedding [47]. The optional\ntemporal down-sampling can be applied to the backbone\nfeature to shrink the input sequence length to the transformer\nfor better memory efficiency.\n3.2. TubeR Decoder\nTubelet query.Directly detecting tubelets is quite challeng-\ning based on anchor hypotheses. The tubelet space along the\nspatio-temporal dimension is huge compared to the single-\nframe bounding box space. Consider for example Faster-\nRCNN [30] for object detection, which requires for each\nposition in a feature map with spatial size H′×W′, K(=9)\nanchors. There are in total KH ′W′ anchors. For a tubelet\nacross Tout frames, it would require (KH ′W′)Tout anchors\nto maintain the same sampling in space-time. To reduce the\ntubelet space, several methods [16, 45] adopt 3D cuboids to\napproximate tubelets by ignoring the spatial action displace-\nments in a short video clip. However, the longer the video\nclip is, the less accurately a 3D cuboid hypotheses represents\na tubelet. We propose to learn a small set of tubelet queries\nQ={Q1, ..., QN } driven by the video data. N is the num-\nber of queries. The i-th tubelet query Qi={qi,1, ..., qi,Tout }\ncontains Tout box query embeddings qi,t ∈ RC′\nacross Tout\nframes. We learn a tubelet query to represent the dynam-\nics of a tubelet, instead of hand-designing 3D anchors. We\ninitialize the box embeddings identically for a tubelet query.\nTubelet attention. In order to model relations in the tubelet\nqueries, we propose a tubelet-attention (TA) module which\ncontains two self-attention layers (shown in Figure 2). First\nwe have a spatial self-attention layer that processes the\nspatial relations between box query embeddings within a\nframe i.e. {q1,t, ..., qN,t}, t={1, ..., Tout}. The intuition of\nthis layer is that recognizing actions benefits from the inter-\nactions between actors, or between actors and objects in the\nsame frame. Next we have our temporal self-attention layer\nthat models the correlations between box query embeddings\nacross time within the same tubelet, i.e. {qi,1, ..., qi,Tout },\ni={1, ..., N}. This layer facilitates a TubeR query to track\nactors and generate action tubelets that focus on single ac-\ntors instead of a fixed area in the frame. TubeR decoder\napplies the tubelet attention module to tubelet queries Q for\ngenerating the tubelet query feature Fq ∈ RN×Tout×C′\n:\nFq = TA(Q). (5)\nDecoder. The decoder contains a tubelet-attention module\nand a cross-attention (CA) layer which is used to decode the\ntubelet-specific feature Ftub from Fen and Fq:\nCA(Fq, Fen) =softmax( Fq × σk(Fen)T\n√\nC′ ) × σv(Fen), (6)\nFtub = Decoder(Fq, Fen). (7)\nFtub ∈ RN×Tout×C′\nis the tubelet specific feature. Note that\nwith temporal pooling, Tout < Tin, TubeR produces sparse\ntubelets; For Tout=Tin, TubeR produces dense tubelets.\n3.3. Task-Specific Heads\nThe bounding boxes and action classification for each\ntubelet can be done simultaneously with independent task-\nspecific heads. Such design maximally reduces the computa-\ntional overheads and makes our system expandable.\nContext aware classification head. The classification can\nbe simply achieved with a linear projection.\nyclass = Linearc(Ftub), (8)\nwhere yclass ∈ RN×L denotes the classification score on L\npossible labels, one for each tubelet.\nShort-term context head. It is known that context is impor-\ntant for understanding sequences [40]. We further propose\nto leverage spatio-temporal video context to help video se-\nquence understanding. We query the action specific feature\nFtub from some context feature Fcontext to strengthen Ftub,\nand get the feature Fc ∈ RN×C′\nfor the final classification:\nFc = CA(Poolt(Ftub), SA(Fcontext)) +Poolt(Ftub). (9)\nWhen we set Fcontext=Fb for utilizing the short-term con-\ntext in the backbone feature, we call it short-term context\nhead. A self-attention layer is first applied to Fcontext, then a\ncross-attention layer utilizes Ftub to query from Fcontext. The\nLinearc is applied to Fc for final classification.\nLong-term context head. Inspired by [41, 43, 47] which\nexplore long-range temporal information for video under-\nstanding, we propose a long-term context head. To utilize\nlong-range temporal information but under certain memory\nbudget, we adopt a two-stage decoder for long-term context\ncompression as described in [44]:\nEmblong = Decoder(Emnn1, Decoder(Embn0, Flong).\n(10)\nThe long-term context Flong ∈ RTlong×H′W′×C′\n(Tlong=(2w + 1)T′) is a buffer that contains the backbone\nfeature extracted from a long 2w adjacent clips concatenated\nalong time. In order to compress the long-term video feature\nbuffer to an embedding Emblong with a lower temporal\ndimension, we apply two stacked decoders with two token\nembedding Emnn0 and Emnn1. Specifically, we first apply a\ncompressed token Embn0 (n0 < Tlong) to query important\ninformation from Flong and get an intermediary compressed\nembedding with temporal dimension n0. Then we further\nutilize another compressed token Embn1 (n1 < n0) to query\nfrom the intermediary compressed embedding and get the\nfinal compressed embedding Emblong. Emblong contains the\nlong-term video information but with a lower temporal\ndimension n1. Then we adopt a cross-attention layer to\nFb and Emblong to generate a long-term context feature\nFlt ∈ RT′×H′×W′×C′\n:\nFlt = CA(Fb, Emblong), (11)\nwe set Fcontext = Flt in Eq. 9 to utilize the long-term context\nfor classification.\nAction switch regression head. The Tout bounding boxes\nin a tubelet are simultaneously regressed with an FC layer\nas:\nycoor = Linearb(Ftub), (12)\nwhere ycoor ∈ RN×Tout×4, N is the number of action tubelets,\nand Tout is the temporal length of an action tubelet. To re-\nmove non-action boxes in a tubelet, we further include an\nFC layer for deciding whether a box prediction depicts the\nactor performing the action(s) of the tubelet, we call action\nswitch. The action switch allows our method to generate\naction tubelets with a more precise temporal extent. The\nprobabilities of the Tout predicted boxes in a tubelet being\nvisible are:\nyswitch = Linears(Ftub), (13)\nwhere yswitch ∈ RN×Tout . For each predicted tubelet, each of\nits Tout bounding boxes obtain an action switch score.\n3.4. Losses\nThe total loss is a linear combination of four losses:\nL = λ1Lswitch(yswitch, Yswitch) +λ2Lclass(yclass, Yclass)\n+λ3Lbox(ycoor, Ycoor) +λ4Liou(ycoor, Ycoor),\n(14)\nwhere y is the model output and Y denotes the ground truth.\nThe action switch loss Lswitch is a binary cross entropy loss.\nThe classification loss Lclass is a cross entropy loss. The Lbox\nand Liou denote the per-frame bounding box matching er-\nror. It is noted when Tout < Tin, the tubelet is sparse and\nthe coordinate ground truth Ycoor are from the correspond-\ning temporally down-sampled frame sequence. We used the\nHungarian matching similar to [4] and more details can be\nfound in the supplementary. We empirically set the scale\nparameter as λ1=1, λ2=5, λ3=2, λ4=2.\n4. Experiments\n4.1. Experimental Setup\nDatasets. We report experiments on three commonly used\nvideo datasets for action detection. UCF101-24 [34] is a\nsubset of UCF101. It contains 24 sport classes in 3207\nuntrimmed videos. We use the revised annotations for\nUCF101-24 from [32] and report the performance on split-\n1. JHMDB51-21 [18] contains 21 action categories in 928\ntrimmed videos. We report the average results over all three\nsplits. A V A[15] is larger-scale and includes 299 15-minute\nmovies, 235 for training, and the remaining 64 for validating.\nBox and label annotations are provided on per-second sam-\npled keyframes. We evaluate on A V A with both annotation\nversions v2.1 and v2.2.\nEvaluation criteria. We report the video-mAP at different\nIoUs on UCF101-24 and JHMDB51-21. As A V A only has\nkeyframe annotations, we report frame-mAP@IoU=0.5 fol-\nlowing [15] using a single, center-crop inference protocol.\nImplementation details. We pre-train the backbone on\nKinetics-400 [20]. The encoder and decoder contain 6 blocks\non A V A. For the smaller UCF101-24 and JHMDB51-21, we\nreduce the numbers of blocks to 3 to avoid overfitting. We\nempirically set the number of tubelet query N to 15. Dur-\ning training, we use the bipartite matching [11] based on\nthe Hungarian algorithm [22] between predictions and the\nground truth. We use the AdamW [27] optimizer with an\ninitial learning rate 1e−5 for the backbone and 1e−4 for the\ntransformers. We decrease the learning rate 10× when the\nvalidation loss saturates. We set 1e−4 as the weight decay.\nScale jittering in the range of (288, 320) and color jittering\nare used for data augmentation. Duringinference, we always\nresize the short edge to 256 and use a single center-crop (1-\nview). We also tested the horizontal flip trick to create 2-view\ninference. For fair comparisons with previous methods on\nUCF101-24 and JHMDB51-21, we also test a two-stream\nsetting with optical flow following [49].\n4.2. Ablations\nWe perform our ablations on both UCF101-24 and A V A\n2.1 to demonstrate the effectiveness of our designs on differ-\nent evaluation protocols. Only RGB inputs are considered.\nBasketball\nBasketball(a) with action switch\n(b) without action switch  \nBasketballBasketballBasketballBasketball\nBasketballBasketballBasketballBasketballBasketball\nBasketballBasketballBasketball\ntransitional states action\nFigure 3. Visualizations of action switch on UCF101-24. Best view\nin color. The red box and label represent the ground truth. Yellow\nindicates our detected tubelets. With the action switch (top row),\nTubeR avoids misclassification for the transitional states.\nFor UCF101-24 with per-frame annotations, we reportvideo-\nmAP at IoU=0.5. A standard backbone I3D-VGG [15] is\nutilized and the input length is set to 7 frames if not speci-\nfied. For A V A 2.1 with 1-fps annotation, we only take the\nmodel prediction on keyframes and report frame-mAP at\nIoU=0.5. We use a CSN-50 backbone [38] with a single view\nevaluation protocol if not specified.\nBenefit of tubelet queries. We first show the benefit of the\nproposed tubelet query sets. Each query set is composed of\nTout per-frame query embeddings (see section 3.2), which\npredict the spatial location of the action on their respective\nframes. We compare this to using a single query embedding\nthat represents a whole tubelet and must regress Tout box\nlocations for all frames in the clip. Our results are shown in\nTable 1a. Compared to using a single query embedding, our\ntubelets query set improves performance by +4.1% video\nmAP on UCF101-24, showing that modeling action detec-\ntion as a sequence-to-sequence task effectively leverages the\ncapabilities of transformer architectures.\nEffect of tubelet attention. In Table 1b, we show using\nour tubelet attention module helps improve video-mAP on\nUCF101-24 by 0.9% and 0.3% on A V A. The tubelet attention\nsaves about 10% memory (4, 414MB) compared to the typi-\ncal self-attention implementation (5, 026MB) during training\n(16 frames input with batch size of 1).\nBenefit of action switch. We report the effectiveness of our\naction switch head in Table 1c. On UCF101-24 the action\nswitch increases the video-mAP from 53.8% to 57.7% by\nprecisely determining the temporal start and end point of\nactions. Without action switch, TubeR misclassifies transi-\ntional states as actions, like the example shown in Figure 3\n(bottom row). As only the frame-level evaluation can be done\non A V A, the advantage of the action switch is not shown by\nthe frame-mAP. Instead, we demonstrate its effect in Fig-\nure 4 and Figure 5. The action switch produces tubelets with\nprecise temporal extent for videos with shot changes.\nEffect of short and long term context head. We report the\nimpact of our context aware classification head with both\nshort and long-term features in Table 1d. The context head\nUCF101-24 AVA\nsingle query 48.8 26.2\ntubelet query set 52.9 27.4\n(a) Analysis on tubelet query. Our tubelet query\nset design allows for each query to focus on the\nspatial location of the action on a specific frame.\nUCF101-24 AVA\nself-attention 52.9 27.4\ntubelet attention 53.8 27.7\n(b) Effect of tubelet attention. With tubelet\nattention modeling relations within a tubelet\nand across tubelets improves.\nUCF101-24 AVA\nw/o switch 53.8 27.7\nw/ switch 57.7 27.7\n(c) Benefit of action switch. Action switch\nproduces a more precise temporal extent,\nwhich can only be shown by video-mAP.\nUCF101-24 AVA\nFC head 57.8 23.4\n+ short-term context 58.4 27.7\n+ long-term context - 28.8\n(d) Effectiveness of short- and long-term con-\ntext. The short-term context and long-term context\nhelp with performance, more noticeable on A V A.\nUCF101-24 AVA\n8 53.9 24.4\n16 58.2 26.9\n32 58.4 27.7\n(e) Length of input clip. Longer input\nvideo leads to a better performance on both\nUCF101-24 and A V A.\nw # of clips duration (s) mAP\n- 1 2.1 27.7\n2 5 10.6 28.4\n3 7 14.9 28.8\n5 11 23.5 28.6\n(f) Long-term context length analysis on\nA V A. The right amount of long-term context\nhelps improve frame-mAP on A V A.\nTable 1. Ablation studies on UCF101-24 and A V A 2.1. The proposed tubelet query, tubelet attention, the action switch and context-awareness\ngenerally improve model performance. The proposed TubeR works well on long clips with shot changes. We report video-mAP@IoU=0.5\nfor UCF101-24 and frame-mAP@IoU=0.5 for A V A.\nbrings a decent performance gain (+4.3%) on A V A. This\nis probably because the movie clips in A V A contain shot\nchanges and so the network benefits from seeing the full\ncontext of the clip. On UCF101-24, the videos are usually\nshort and without shot changes. The context does not bring\na significant improvement on UCF101-24.\nLength of input clip. We report results with variable input\nlengths in Table 1e. We compare with input length of 8,\n16 and 32 on both UCF101-24 and A V A with CSN-152\nas backbone. TubeR is able to handle long video clips as\nexpected. We notice that our performance on UCF101-24\nsaturates faster than on A V A, probably because UCF101-24\ndoes not contain shot changes that requires longer temporal\ncontext for classification.\nLength of long-term context. This ablation is only con-\nducted on A V A as videos on UCF101-24 are too short to\nuse long-term context. Table 1f shows that the right amount\nof long-term context helps performance, but overwhelming\nthe amount of long-term context harms performance. This is\nprobably because the long-term feature contains both useful\ninformation and noise. The experiments show that about 15s\ncontext serves best. Note that the context length varies per\ndataset, but can be easily determined empirically.\n4.3. Frame-Level State-of-the-Art\nA V A 2.1 Comparison.We first compare our results with pre-\nviously proposed methods on A V A 2.1 in Table 2. Compared\nto previous end-to-end models, with comparable backbone\n(I3D-Res50) and the same inference protocol, the proposed\nTubeR outperforms all. TubeR outperforms the most recent\nend-to-end works WOO [5] by 0.9% and VTr [13] by 1.2%.\nThis demonstrates the effectiveness of our designs.\nCompared to previous work using an offline person detec-\ntor, the proposed TubeR is also more effective under the same\ninference protocols. This is because TubeR generates tubelet-\nspecific features without assumptions on location, while the\ntwo-stage methods have to assume the actions occur at a\nfixed location. It is also worth mentioning that the TubeR\nwith CSN backbones outperforms the two-stage model with\nthe same backbone by +4.4%, demonstrating that the gain is\nnot from the backbone but our TubeR design. TubeR even\noutperforms the methods with multi-view augmentations\n(horizontal flip, multiple spatial crops and multi-scale). Tu-\nbeR is also considerably faster than previous models, we\nhave attempted to collect the reported FLOPs from previous\nworks (Table 2). Our TubeR has 8% fewer FLOPs than the\nmost recently published end-to-end model [5] with higher\naccuracy. Tuber is also 4× more efficient than the two-stage\nmodel [9] with noticeable performance gain. Thanks to our\nsequence-to-sequence design, the heavy backbone is shared\nand we do not need temporal iteration for tubelet regression.\nWe finally present the highest number reported in the\nliterature, regardless of the inference protocol, pre-training\ndataset and additional information used. TubeR still achieves\nthe best performance, even better than the model using addi-\ntional object bounding-boxes as input [37].The results show\nthat the proposed sequence-to-sequence model with tubelet\nspecific feature is a promising direction for action detection.\nA V A 2.2 Comparison.The results are shown in Table 3.\nUnder the same single-view protocol, TubeR is considerably\nbetter than previous methods, including the most recent work\nwith an end-to-end design (WOO [5] +5.1%) and the two-\nstage work with strong backbones (MViT [7] +4.7%). A fair\ncomparison between TubeR and a two-stage model [48] with\nthe same backbone CSN-152, shows TubeR gains +5.5%\nframe-mAP. It demonstrates TubeR’s superior performance\ncomes from our design rather than the backbone.\nUCF101-24 Comparison. We also compare TubeR with the\nstate-of-the-art using frame-mAP@IoU=0.5 on UCF101-24\n(see the first column with numbers in Table 4). Compared to\nexisting methods, TubeR acquires better results with com-\nparable backbones, for both RGB-stream and two-stream\nModel Detector Input Backbone Pre-train Inference GFLOPs mAP\nComparison to end-to-end models\nI3D [15] ✗ 32 × 2 I3D-VGG K400 1 view NA 14.5\nACRN [35] ✗ 32 × 2 S3D-G K400 1 view NA 17.4\nSTEP [45] ✗ 32 × 2 I3D-VGG K400 1 view NA 18.6\nVTr [13] ✗ 64 × 1 I3D-VGG K400 1 view NA 24.9\nWOO [5] ✗ 8 × 8 SF-50 K400 1 view 142 25.2\nTubeR ✗ 16 × 4 I3D-Res50 K400 1 view 132 26.1\nTubeR ✗ 16 × 4 I3D-Res101 K400 1 view 246 28.6\nComparison to two-stage models\nSlowfast-50 [9] F-RCNN 16 × 4 SF-50 K400 1 view 308 24.2\nX3D-XL [8] F-RCNN 16 × 5 X3D-XL K400 1 view 290 26.1\nCSN-152* F-RCNN 32 × 2 CSN-152 IG + K400 1 views 342 27.3\nLFB [43] F-RCNN 32 × 2 I3D-101-NL K400 18 views NA 27.7\nACAR-NET [28] F-RCNN 32 × 2 SF-50 K400 6 views NA 28.3\nTubeR ✗ 32 × 2 CSN-50 K400 1 view 78 28.8\nTubeR ✗ 32 × 2 CSN-152 IG + K400 1 view 120 31.7\nComparison to best reported results\nWOO [5] ✗ 8 × 8 SF-101 K400+K600 1 view 246 28.0\nSF-101-NL [9] F-RCNN 32 × 2 SF-101+NL K400+K600 6 views 962 28.2\nACAR-NET [28] F-RCNN 32 × 2 SF-101 K400+K600 6 views NA 30.0\nAIA [37] F-RCNN 32 × 2 SF-101 K400+K700 18 views NA 31.2\nTubeR ✗ 32 × 2 SF-101 K400+K700 1 view 240 31.6\nTubeR ✗ 32 × 2 CSN-152 IG + K400 2 view 240 32.0\nTable 2. Comparison on A V A v2.1validation set. Detector shows if additional detector is required; * denotes the results we tested. IG\ndenotes the IG-65M dataset, SF denotes the slowfast network. The FLOPs for two-stage models are the sum of Faster RCNN-R101-FPN\nFLOPs (246 GFLOPs [4]) plus classifier FLOPs multiplied by view number. TubeR performs more effectively and efficiently.\nModel backbone pre-train inference mAP\nSingle-view\nX3D-XL [8] X3D-XL K600+ K400 1 view 27.4\nCSN-152 [48] CSN-152 IG + K400 1 view 27.9\nWOO [5] SF-101 K600+ K400 1 view 28.3\nM-ViT-B-24 [7] MViT-B-24 K600+ K400 1 view 28.7\nTubeR CSN-50 IG + K400 1 view 29.2\nTubeR CSN-152 IG + K400 1 view 33.4\nMulti-view\nSlowFast-101 [9] SF-101 K600+ K400 6 views 29.8\nACAR-Net [28] SF-101 K700+ K400 6 views 33.3\nAIA (obj) [37] SF-101 K700+ K400 18 views 32.2\nTubeR CSN-152 IG + K400 2 views 33.6\nTable 3. Comparison on A V A v2.2validation set. IG denotes the\nIG-65M, SF denotes the slowfast. TubeR achieves the best result.\nsettings. Further with a CSN-152 backbone, TubeR gets 83.2\nframe-mAP, even better than two-stream methods. Though\nTubeR targets on tubelet-level detection, it performs well on\nframe-level evaluation on both A V A and UCF101-24.\n4.4. Video-Level State-of-the-Art\nWe also compare TubeR with various settings to state-of-\nthe-art reporting video-mAP on UCF101-24 and JHMDB51-\n21 in Table 4. For fair comparisons, TubeR with a 2D\nbackbone gains +4.4% video-mAP@IoU=0.5 compared to\nthe recent state-of-the-art [26] on UCF101-24 without us-\ning optical flow, which demonstrates that TubeR learning\nUCF101-24 JHMDB51-21Backbonef-mAP 0.20 0.50 0.50:0.95 0.20 0.50RGB-streamMOC [26] DLA34 72.1 78.2 50.7 26.2 - -TubeRRes50 79.5 81.2 55.1 28.1 - -T-CNN [16] C3D 41.4 47.1 - - 78.4 76.9TubeR I3D 80.1 82.8 57.7 28.6 79.7 78.3TubeRCSN-15283.2 83.3 58.4 28.9 87.4 82.3Two-streamTacNet [33] VGG 72.1 77.5 52.9 24.1 - -2in1 [49] VGG 78.5 50.3 24.5 - 74.7ACT [19] VGG 67.1 77.2 51.4 25.0 74.2 73.7MOC [26] DLA34 78.0 82.8 53.8 28.3 77.3 77.2STEP [45] I3D 75.0 76.6 - - - -I3D [15] I3D 76.3 - 59.9 - - 78.6*CFAD [25] I3D 72.5 81.664.626.786.8 85.3TubeR I3D 81.3 85.360.229.781.8 80.7\nTable 4. Comparison on UCF101-24 and JHMDB51-21 with\nvideo-mAP. TubeR achieves better results compared to most state-\nof-arts. f-mAP denotes the frame mAP@IoU=0.5. *CFAD is pre-\ntrained on K600 but others on K400.\ntubelet queries is more effective compared to using posi-\ntional hypotheses. Compared to TacNet [33] which proposes\na transition-aware context network to distinguish transitional\nstates, TubeR with action switch performs better even with a\none-stream setting. When incorporating optical flow inputs,\nthe TubeR with I3D further boosts the video-level results. It\nis noted that TubeR pretrained on K400 even outperforms\nTubelet 5: walk\nTubelet 1: stand; listen to (a person); watch (a person)\nTubelet 2: stand; listen to (a person); watch (a person)\nInput frames\nTubelet 3: sit; listen to (a person); watch (a person)\nTubelet 4: stand; talk to (e.g., a group); watch (a person)\nResults\nFigure 4. Visualization of tubelet specific feature with attention\nrollout. Each tubelet covers a separated action instance. Best viewed\nin color.\nCFAD pretrained on K600 on some metrics. We test Tu-\nbeR inference speed on UCF101-24 by following CFAD. To\ndirectly generate a tubelet without an offline linker, TubeR\nruns at 156 fps. Faster than CFAD (130fps) and most existing\nSOTA methods (40-53 fps). The result illustrates our design\nis effective and efficient for video-level action detection.\n4.5. Visualization\nWe first provide visualizations (Figure 4) of the tubelet-\nspecific features by overlaying the tubelet-specific feature\nactivation over the input frames using attention rollout [1].\nThe example in Figure 4 is challenging as it contains multiple\npeople and concurrent actions. The visualization show that: 1.\nOur proposed TubeR is able to generate highly discriminative\ntubelet-specific features. Different actions in this case are\nclearly separated in different tubelets. 2. Our action switch\nworks as expected and initiates/cuts the tubelets when the\naction starts/stops. 3. Our TubeR generalizes well to scale\nchanges (the brown tubelet). 4. The generated tubelets are\ntightly associated with tubelet specific feature as expected.\nWe further show our TubeR performs well in various\nscenarios. TubeR works well on videos with shot changes\n(Figure 5 top); TubeR is able to detect an actor moving with\ndistance (Figure 5 middle); and TubeR is robust to action\ndetection even for small people (Figure 5 bottom).\nsit,talk towatch \nsitsit,carry/holdlisten to\nstand,talk to,watchstand,listen to watch \nwalk,watch\nwalkwalkstand,talk\nwalk\n walk\nFigure 5. Results visualization, with different colors to label dif-\nferent tubelets. Each action tubelet contains its action labels and\nboxes per frame. We only show the action labels on the first frame\nof an action tubelet. Some challenging cases are shown. Top: shot\nchanges; Middle: actors moving with distance; Bottom: multiple\nactors with small and large scales. Best viewed in color.\n5. Discussion and Conclusion\nLimitations. Although proposed for long videos, we noticed\ntwo potential limitations that stop us from feeding in very\nlong videos in one shot.\n1. We observe that 90% of computation (FLOPs) and 67%\nof memory usage was used by our 3D backbone. This heavy\nbackbone restricts us from applying TubeR on long videos.\nRecent works show that transformer encoders can be used\nfor video embedding [2, 7, 47] and are less memory and\ncomputationally hungry. We will explore these transformer\nbased embeddings in future work.\n2. If we were to process a long video in one pass we’d need\nenough queries to cover the maximum number of different\nactions per-person in that video. This would likely require a\nlarge number of queries which would cause memeory issues\nin our self attention layers. A possible solution is to generate\nperson tubelets, instead of action tubelets, so that we do not\nneed to split tubelets when a new action happens. Then we\nwould only need a query for each person instance.\nPotential negative impact. There are real-world applica-\ntions of action detection technology such as patient or el-\nderly health monitoring, public safety, Augmented/Virtual\nReality, and collaborative robots. However, there could be\nunintended usages and we advocate responsible usage and\ncomplying with applicable laws and regulations.\nConclusion. This paper introduces TubeR, a unified solution\nfor spatio-temporal video action detection in a sequence-to-\nsequence manner. Our design of tubelet-specific features\nallows TubeR to generate tubelets (a set of linked bound-\ning boxes) with action predictions for each of the tubelets.\nTubeR does not rely on positional hypotheses and therefore\nscales well to longer video clips. TubeR achieves state-of-the-\nart performance and better efficiency compared to previous\nworks.\nReferences\n[1] Samira Abnar and Willem H Zuidema. Quantifying attention\nflow in transformers. In ACL, 2020. 8\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Lu ˇci´c, and Cordelia Schmid. Vivit: A video vision\ntransformer. arXiv:2103.15691, 2021. 8\n[3] Liangliang Cao, Zicheng Liu, and Thomas S Huang. Cross-\ndataset action detection. In CVPR, 2010. 1, 2\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1, 2,\n3, 5, 7\n[5] Shoufa Chen, Peize Sun, Enze Xie, Chongjian Ge, Jiannan\nWu, Lan Ma, Jiajun Shen, and Ping Luo. Watch only once:\nAn end-to-end video action detection framework. In ICCV,\n2021. 2, 6, 7\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. In ICLR, 2021. 2\n[7] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. arXiv:2104.11227, 2021. 2,\n6, 7, 8\n[8] Christoph Feichtenhofer. X3D: Expanding architectures for\nefficient video recognition. In CVPR, 2020. 7\n[9] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nCVPR, 2019. 1, 2, 6, 7\n[10] Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees GM\nSnoek. Actor-transformers for group activity recognition. In\nCVPR, 2020. 2\n[11] Dobrik Georgiev and Pietro Li´o. Neural bipartite matching.\narXiv:2005.11304, 2020. 5\n[12] Rohit Girdhar, Jo˜ao Carreira, Carl Doersch, and Andrew Zis-\nserman. A better baseline for ava. arXiv:1807.10066, 2018.\n1, 2\n[13] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n2, 6, 7\n[14] Georgia Gkioxari and Jitendra Malik. Finding action tubes.\nIn CVPR, 2015. 1\n[15] Chunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Car-\noline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,\nGeorge Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia\nSchmid, and Jitendra Malik. Ava: A video dataset of spatio-\ntemporally localized atomic visual actions. In CVPR, 2018.\n1, 2, 5, 7\n[16] Rui Hou, Chen Chen, and Mubarak Shah. Tube convolutional\nneural network (t-cnn) for action detection in videos. InICCV,\n2017. 1, 2, 3, 7\n[17] Mihir Jain, Jan van Gemert, Herv´e J´egou, Patrick Bouthemy,\nand Cees GM Snoek. Action localization with tubelets from\nmotion. In CVPR, 2014. 1, 2\n[18] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid,\nand Michael J Black. Towards understanding action recogni-\ntion. In ICCV, 2013. 2, 5\n[19] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and\nCordelia Schmid. Action tubelet detector for spatio-temporal\naction localization. In ICCV, 2017. 1, 2, 7\n[20] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The kinetics human action video\ndataset. arXiv:1705.06950, 2017. 5\n[21] Aisha Urooj Khan, Amir Mazaheri, Niels da Vitoria Lobo,\nand Mubarak Shah. MMFT-BERT: Multimodal fusion trans-\nformer with bert encodings for visual question answering.\narXiv:2010.14095, 2020. 1, 2\n[22] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 1955. 5\n[23] Dong Li, Zhaofan Qiu, Qi Dai, Ting Yao, and Tao Mei. Re-\ncurrent tubelet proposal and recognition networks for action\ndetection. In ECCV, 2018. 2\n[24] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. Entangled\ntransformer for image captioning. In ICCV, 2019. 1, 2\n[25] Yuxi Li, Weiyao Lin, John See, Ning Xu, Shugong Xu, Ke\nYan, and Cong Yang. Cfad: Coarse-to-fine action detector for\nspatiotemporal action localization. In ECCV, 2020. 7\n[26] Yixuan Li, Zixu Wang, Limin Wang, and Gangshan Wu. Ac-\ntions as moving points. In ECCV, 2020. 1, 2, 7\n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. ICLR, 2017. 5\n[28] Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao,\nand Hongsheng Li. Actor-context-actor relation network for\nspatio-temporal action localization. In CVPR, 2021. 1, 2, 7\n[29] Xiaojiang Peng and Cordelia Schmid. Multi-region two-\nstream r-cnn for action detection. In ECCV, 2016. 1, 2\n[30] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015. 3\n[31] Suman Saha, Gurkirt Singh, Michael Sapienza, Philip HS\nTorr, and Fabio Cuzzolin. Deep learning for detecting multiple\nspace-time action tubes in videos. arXiv:1608.01529, 2016.\n2\n[32] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS\nTorr, and Fabio Cuzzolin. Online real-time multiple spa-\ntiotemporal action localisation and prediction. In ICCV, 2017.\n1, 2, 5\n[33] Lin Song, Shiwei Zhang, Gang Yu, and Hongbin Sun. Tacnet:\nTransition-aware context network for spatio-temporal action\ndetection. In CVPR, 2019. 1, 2, 7\n[34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv:1212.0402, 2012. 2, 5\n[35] Chen Sun, Abhinav Shrivastava, Carl V ondrick, Kevin Mur-\nphy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric\nrelation network. In ECCV, 2018. 1, 2, 7\n[36] Chiranjib Sur. Self-segregating and coordinated-segregating\ntransformer for focused deep multi-modular network for vi-\nsual question answering. arXiv:2006.14264, 2020. 1, 2\n[37] Jiajun Tang, Jin Xia, Xinzhi Mu, Bo Pang, and Cewu Lu.\nAsynchronous interaction aggregation for action detection. In\nECCV, 2020. 1, 2, 6, 7\n[38] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli.\nVideo classification with channel-separated convolutional net-\nworks. In ICCV, 2019. 5\n[39] Du Tran and Junsong Yuan. Max-margin structured output\nregression for spatio-temporal action localization. In NIPS,\n2012. 1, 2\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 1, 2, 3,\n4\n[41] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In CVPR, 2018. 4\n[42] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid.\nLearning to track for spatio-temporal action localization. In\nICCV, 2015. 2\n[43] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-\ning He, Philipp Krahenbuhl, and Ross Girshick. Long-term\nfeature banks for detailed video understanding. In CVPR,\n2019. 1, 2, 4, 7\n[44] Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia,\nZhuowen Tu, and Stefano Soatto. Long short-term trans-\nformer for online action detection. In NeurIPS, 2021. 2,\n4\n[45] Xitong Yang, Xiaodong Yang, Ming-Yu Liu, Fanyi Xiao,\nLarry S Davis, and Jan Kautz. Step: Spatio-temporal progres-\nsive learning for video action detection. In CVPR, 2019. 1, 2,\n3, 7\n[46] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv:2101.11986, 2021. 2\n[47] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu, Bi-\nagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe. Vidtr-\nswitch: Video transformer without convolutions. In ICCV,\n2021. 2, 3, 4, 8\n[48] Yanyi Zhang, Xinyu Li, and Ivan Marsic. Multi-label activ-\nity recognition using activity-specific features and activity\ncorrelations. In CVPR, 2021. 6, 7\n[49] Jiaojiao Zhao and Cees GM Snoek. Dance with flow: Two-in-\none stream action detection. In CVPR, 2019. 1, 2, 5, 7\n[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 2",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7112356424331665
    },
    {
      "name": "Encoder",
      "score": 0.6910052299499512
    },
    {
      "name": "Computer science",
      "score": 0.5811666250228882
    },
    {
      "name": "Action (physics)",
      "score": 0.44755107164382935
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4384625554084778
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4154640734195709
    },
    {
      "name": "Tube (container)",
      "score": 0.41425377130508423
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3364208936691284
    },
    {
      "name": "Physics",
      "score": 0.1298685371875763
    },
    {
      "name": "Engineering",
      "score": 0.12753155827522278
    },
    {
      "name": "Mechanical engineering",
      "score": 0.08774280548095703
    },
    {
      "name": "Electrical engineering",
      "score": 0.07343417406082153
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": []
}