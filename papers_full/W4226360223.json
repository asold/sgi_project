{
    "title": "CrowdFormer: An Overlap Patching Vision Transformer for Top-Down Crowd Counting",
    "url": "https://openalex.org/W4226360223",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2172068624",
            "name": "Shaopeng Yang",
            "affiliations": [
                "World Water Watch"
            ]
        },
        {
            "id": "https://openalex.org/A2117571538",
            "name": "Weiyu Guo",
            "affiliations": [
                "Central University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2223031732",
            "name": "Yuheng Ren",
            "affiliations": [
                "World Water Watch"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3087861058",
        "https://openalex.org/W2123175289",
        "https://openalex.org/W6863994431",
        "https://openalex.org/W3194417266",
        "https://openalex.org/W2886443245",
        "https://openalex.org/W3101165561",
        "https://openalex.org/W3176458063",
        "https://openalex.org/W3182335293",
        "https://openalex.org/W3192782181",
        "https://openalex.org/W2120815373",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3027606690",
        "https://openalex.org/W3176047859",
        "https://openalex.org/W3035193053",
        "https://openalex.org/W3186104459",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2982130202",
        "https://openalex.org/W3084883754",
        "https://openalex.org/W3097407159",
        "https://openalex.org/W2463631526",
        "https://openalex.org/W2788040570",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6868564194",
        "https://openalex.org/W4287775237",
        "https://openalex.org/W2145983039",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964209782",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3203845557",
        "https://openalex.org/W4289709982",
        "https://openalex.org/W3190723141"
    ],
    "abstract": "Crowd counting methods typically predict a density map as an intermediate representation of counting, and achieve good performance. However, due to the perspective phenomenon, there is a scale variation in real scenes, which causes the density map-based methods suffer from a severe scene generalization problem because only a limited number of scales are fitted in density map prediction and generation. To address this issue, we propose a novel vision transformer network, i.e., CrowdFormer, and a density kernels fusion framework for more accurate density map estimation and generation, respectively. Thereafter, we incorporate these two innovations into an adaptive learning system, which can take both the annotation dot map and original image as input, and jointly learns the density map estimator and generator within an end-to-end framework. The experimental results demonstrate that the proposed model achieves the state-of-the-art in the terms of MAE and MSE (e.g., it achieved a MAE of 67.1 and MSE of 301.6 on NWPU-Crowd dataset.), and confirm the effectiveness of the proposed two designs. The code is https://github.com/special-yang/Top_Down-CrowdCounting.",
    "full_text": "CrowdFormer: An Overlap Patching Vision Transformer\nfor Top-Down Crowd Counting\nShangpeng Yang2 , Weiyu Guo1;\u0003 , Yuheng Ren2\n1Information School, Central University of Finance and Economics, Beijing, China\n2Watrix Technology Co. LTD., Beijing, China\nshaopeng.yang@watrix.ai, weiyu.guo@cufe.edu.cn, yuheng.ren@watrix.ai\nAbstract\nCrowd counting methods typically predict a density\nmap as an intermediate representation of counting,\nand achieve good performance. However, due to\nthe perspective phenomenon, there is a scale varia-\ntion in real scenes, which causes the density map-\nbased methods suffer from a severe scene general-\nization problem because only a limited number of\nscales are ﬁtted in density map prediction and gen-\neration. To address this issue, we propose a novel\nvision transformer network, i.e., CrowdFormer, and\na density kernels fusion framework for more accu-\nrate density map estimation and generation, respec-\ntively. Thereafter, we incorporate these two innova-\ntions into an adaptive learning system, which can\ntake both the annotation dot map and original im-\nage as input, and jointly learns the density map es-\ntimator and generator within an end-to-end frame-\nwork. The experimental results demonstrate that\nthe proposed model achieves the state-of-the-art in\nthe terms of MAE and MSE (e.g., it achieved a\nMAE of 67.1 and MSE of 301.6 on NWPU-Crowd\ndataset.), and conﬁrm the effectiveness of the pro-\nposed two designs. The code is https://github.com/\nspecial-yang/Top\nDown-CrowdCounting.\n1 Introduction\nCrowd counting is an essential topic in a variety of appli-\ncations such as public safety, activity recognition, and trans-\nportation management, aiming to estimate the number of peo-\nple in a crowd image. Current methods [Wan et al., 2020;\nJia et al., 2021; Boyuet al., 2020] obtained excellent progress\nby utilizing the CNNs to regress the corresponding density\nmaps of the input images, where the summed value in a den-\nsity map indicates the total counting of people in the crowd\nimage. The density map is an intermediate representation of\nthe crowd, which groundtruth is typically generated from the\noriginal image by placing a density kernel on each person’s\ndot annotation.\nDue to the perspective phenomenon in real crowd scenes,\nthere is a large-scale variation on different targets, which ne-\n∗Corresponding author\ncessitates a crowd counting method that can deal with various\nscales. However, the current density map based crowd count-\ning methods remain the following drawbacks: 1) Due to the\nlimited and ﬁxed receptive ﬁelds of pure CNNs, which can-\nnot ﬁt a wider range of continuous scale variation of target-\ns, their density map estimators typically only deal with dis-\ncrete scales. 2) For an efﬁcient counting context information\nextraction, the dense or far-focus crowd scenarios are better\nsuitable to a small density kernel, whereas sparse or close-\nfocus crowd scenarios adapt to a large-scaled kernel. Howev-\ner, the groundtruth of density maps are typically generated by\nplacing a hand-crafted density kernel on the corresponding\ndot annotations, which cannot ﬁt to the scale change of the\ntargets and scenarios. Based on the preceding observations, it\nis preferable to exploit a comprehensive solution for coping\nthe continuous scale variation of crowds in both density map\nestimation and generation.\nA human can estimate the approximate number of peo-\nple in a crowd image quickly using a global to local vi-\nsual perception mechanism, without the scale and receptive\nﬁeld problems. People using the Top-Down visual percep-\ntion mechanism typically scan a crowd image with a glob-\nal receptive ﬁled, and then estimate the degree of conges-\ntion of crowd regions based on the prior knowledge from the\nglobal scanning. Furthermore, Transformer architectures can\nprovide a global to local receptive ﬁeld [Zheng et al., 2021 ].\nInspired by the human’s Top-Down visual perception mech-\nanism and recent research progress about the Transformer,\nwe propose a Crowd Counting Transformer network, namely,\nCrowdFormer, which models the human’s Top-Down visual\nperception mechanism based on a series of Overlap Patching\nTransformer (OPT) blocks. An OPT block ﬁrst learns global\nprior knowledge about a crowd image from an overlap visu-\nal patch sequence before focusing on analysing local crowd\nregions. In the OPT block, we can obtain a crowd counting\nfrom global to local, and encode the relative spatial position\nof visual tokens rather than absolute position, which is adopt-\ned in the standard vision transformer[Alexey et al., 2021] and\nmay result in the predictive model losing the translation and\nrotation invariance.\nMoreover, to generate more accurate groundtruth of den-\nsity maps considering various target scales, we propose a\nmultiple density kernels fusion-based density map generator\n(KFMG), which can ﬁt targets of various scales by fusing\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1545\nmultiple density kernels. Finally, we incorporate the densi-\nty map generator and the estimator into an adaptive learning\nsystem, which takes both the dot annotation map and original\nimage as input, and jointly learns the density map estimator\nand generator within an end-to-end framework.\nIn a nutshell, the main contributions of this paper are as\nfollows:\n•We propose to model the human’s Top-Down visual\nperception mechanism in crowd counting by using the\nTransformer architecture. To the best of our knowledge,\nthis is the ﬁrst work to utilize Transformer architecture\nnetwork for regressing density maps in crowd counting.\n•A learnable density map generator, which takes various\ntarget scales into account, is proposed to generate more\naccurate density map groundtruths by fusing multiple\ndensity kernels.\n•Extensive experiments are carried out to validate the pro-\nposed method, and demonstrate that our method is the-\nstate-of-the-art from multiple perspectives.\n2 Related Work\nCrowd counting is a fundamental task in computing vision.\nIn the early phase, people usually treat the crowd counting\nas target localization tasks. For example, Min Li et al. [Min\net al., 2008 ] used heads and shoulders localization to con-\nstruct crowd counting. However, in severely occluded sce-\nnarios, such paradigms will be invalid. To cope with the\ndensely crowded scene, two types of regression-based meth-\nods were proposed: global regression and density map regres-\nsion. The global regression-based methods[Chan et al., 2008;\nChattopadhyay et al., 2017; Yifan et al., 2020 ] typically es-\ntimate the ﬁnal count of people directly from images, while\nthe density map regression-based approaches [Yuhong et al.,\n2018; Shuai et al., 2020; Jia et al., 2020] ﬁrst predicts a den-\nsity map, which is then summed to obtain the ﬁnal count.\nBecause density maps contain more spatial context informa-\ntion, the performance is usually superior to that of global\nregression-based methods. Our work falls under the catego-\nry of density map regression-based methods. As a result, we\nprimarily examine two types of density map regression-based\nmethods in recent literature. Moreover, since we introduce\nthe Transformer architecture into the crowd counting task, ex-\nisting Transformer approaches are also discussed.\n2.1 Density Map Ground-truth Generation\nThe use of density maps as the supervised information is a\ncommon choice among most of recent crowd counting meth-\nods. Traditional work [Lempitsky et al., 2010] typically con-\nvolve the crowd image by placing a ﬁxed bandwidth Gaus-\nsian kernel on the dot annotation map to obtain a density map\ngroundtruth. However, due to target scale variation, the Gaus-\nsian kernel with ﬁxed bandwidth and parameters cannot con-\ntribute to all variation in existing images.\nTo cope with the scale variation on targets, some work\nwas proposed to ﬁt the scale variation by adjusting the band-\nwidth or parameters of the Gaussian kernel manually ac-\ncording to the scene perspective or crowdedness. For exam-\nple, Idrees et al. [Haroon et al., 2018 ] fuse multiple densi-\nty maps which were generated by multiple Gaussian kernels\nwith different bandwidths. Wan et al. [Wan and Chan, 2019;\nWan et al., 2020] propose using a CNN-based network as the\ndensity map generator. The parameters of the Gaussian ker-\nnel are changed in this way to make them learnable. Although\nmany compelling methods for improving the quality of den-\nsity map generation have been proposed, the problem of scale\nvariation on targets remains unsolved due to the hand-crafted\nsettings lacking generalization. In contrast to these previous\nwork that requires manually setting the parameters or band-\nwidth of Gaussian kernel, we propose to adaptively fuse mul-\ntiple density kernels, and jointly learn the density map gener-\nator with the estimator in an end-to-end manner.\n2.2 Multiscale Density Map Estimation\nTo deal with a large-scale variation on targets, some literature\nproposed using multiscale features or multicontext informa-\ntion during the feature extraction phase. As a result, using the\nfeature pyramid network (FPN)[Tsung-Yi et al., 2017] to fuse\nmulti-scale information is a common choice among recen-\nt crowd counting methods. For example, Chen et al. [Chen et\nal., 2021] utilized FPN to extract multiscale features with d-\nifferent receptive ﬁelds, whereas a pyramid region awareness\nloss[Qingyu et al., 2021 ] is utilized to recursively searches\nthe most over-estimated sub-regions.\nAlthough multiscale features and multicontext information\nabout targets have been focused on in recent literature, these\nmultiscale learning-based methods still fail to cope with the\ncontinuous scale variation on targets. In contrast to previ-\nous work that only consider a few scales, we present Crowd-\nFormer, a novel Transformer network, that can ﬁt targets with\ncontinuous scales by modeling the human Top-Down visual\nperception mechanism.\n2.3 Transformers in Crowd Counting\nCNNs are regarded as a hierarchical ensemble of local fea-\ntures with different reception ﬁelds. Unfortunately, most C-\nNNs excel at extracting local features but struggle to capture\nglobal cues. Transformer [Ashish et al., 2017 ], which was\nproposed in the ﬁeld of natural language processing to cap-\nture the long-term dependencies between input and output,\nwas recently introduced to vision tasks. Due to fusing local\nand global features, Transformers promote the performance\nof many vision tasks signiﬁcantly. For example, Dosovitskiy\net al.[Alexey et al., 2021], constructed a Transformer network\nfor image classiﬁcation, and achieved excellent results com-\npared to CNN-based models. While, Nicolas et al.[Nicolas et\nal., 2020], utilized Transformer structures to augment a stan-\ndard CNN network for improving the performance on object\ndetection. Moreover, Liang et al.[Dingkang et al., 2021] pro-\nposed TransCrowd, which involves a Transformer network\ninto the weakly-supervised crowd counting task, and achieves\ngood performance. However, because the ﬁnal counts are di-\nrectly regressed the TransCrowd fails to learn the spatial and\ncontextual information of scenes. In contrast to TransCrowd,\nwe propose a novel cutting-edge Transformer framework for\ndensity map estimation that takes efﬁciency, accuracy, and\nrobustness into consideration.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1546\n1 1Conv/g117\nOverlap Patch \nTransformer \nBlock\nStage 1\n CNN \nP-sigmoid\nEncoder De coder\nOverlap Patch \nEmbedings\nMSA\nProject\nStage i: Overlap Patching Transformer Block\nUpSample\n Local Feature Embedding\nF0 : 0m mH W C/g117 /g117 F1: 12 2\nm mH W C/g117 /g117 F2 : 24 4\nm mH W C/g117 /g117 F3: 38 8\nm mH W C/g117 /g117 F4 : 3 p pH W C/g117 /g117 F5 : 1 p pH W/g117 /g117Input : 3H W/g117 /g117\np pH W C/g117 /g117C2 2\nm m\nii i\nH W/g117 /g117\nC2 2\nm m\nii i\nH W/g117 /g117 C2 2\nm m\ni i\nH W/g117 /g117\n/g11 /g121 12 2\nm m\nii i\nH W k k C/g14 /g14\n/g167 /g183 /g117 /g117 /g117 /g117/g168 /g184/g169 /g185\n+11 1 C2 2\nm m\nii i\nH W\n/g14 /g14 /g117 /g117\nEncoder\n1 1Conv/g117\n+\nFeature Collection Unit\nFeature \nCollection Unit\nOverlap Patch \nTransformer \nBlock\nStage 2\nOverlap Patch \nTransformer \nBlock\nStage 2\nOverlap Patch \nTransformer \nBlock\nStage 3\nFigure 1: Overview of CrowdFormer. The CrowdFormer framework mainly consists of three modules: a local feature embedding module\nfor efﬁciently learning abstract and low-resolution feature maps from large input images; a vision Transformer-based encoder for extracting\ncoarse and ﬁne features; and a lightweight decoder for directly fusing these multilevel features and generating the predictive density map.\n3 CrowdFormer\nIn the real scene, there is a large-scale variation on different\ntargets due to the perspective phenomenon. Recent work typ-\nically only can ﬁt a few scales. However, Humans, on the\nother hand, are usually not affected by such scale variation,\nsince we can take the global context information of the crowd\nscene into account with a global receptive ﬁeld. In light of\nthis, we propose CrowdFormer, which is capable of model-\ning the human’s Top-Down visual perception mechanism in\ncrowd counting task.\nThe CrowdFormer cascades CNN and visual Transformer\nstructures for local crowd region enhancement and global\ncontext information capturing. An overview of CrowdFormer\nis depicted in Figure 1, which is made up of three major mod-\nules, i.e., a local feature embedding module, a Transformer\nstructure-based encoder, and a lightweight decoder. To be-\ngin, the local feature embedding module extracts local fea-\ntures about crowd counting, and embeds them into the given\nlow-resolution representations using a CNN structure inher-\nited from the YOLOv5 backbone. Then, the encoder, which\nis stacked by our proposed Transformer blocks to generate\nfeature maps of different resolution, is used to fuse the local\ncrowd features and the global information of crowd scene. Fi-\nnally, the lightweight decoder combines the multi-scale fea-\nture maps to predict a density map.\n3.1 Local Feature Embedding\nConsidering that self-attention-based Transformer when per-\nformed across n entities requires O(n2) memory and compu-\ntation, which is memory and computation consuming. While\nCNN can extract local but compact feature representation\nfrom the inputs efﬁciently. We ﬁrst implement a CNN fea-\nture extractor to obtain a compact feature representation from\nthe inputs for computing resource saving and local informa-\ntion reﬁnement.\nSpeciﬁcally, the structure of local feature extractor we\nadopted in the CrowdFormer is inherited from the backbone\nof YOLOv5, and used for 1/4 downsampling feature extrac-\ntion. In this way, we can obtain a good ﬁtting ability with\na small computing cost, as well as alleviate the effective in-\nformation loss in the downsampling process. In practice, we\ndirectly use the part of pretrained backbone of YOLOv5 as\nthe local feature extractor, because the tinny object detection\nand crowd counting have similarities in terms of task prop-\nerties and their domain knowledge may can be transferred to\neach other.\n3.2 Overlap Patching Transformer Block\nThe human could focus on the region of interest in the image,\nbased on the goal of current task and global prior knowledge.\nSimilarly, we boost the model attention on crowd regions by\nmining global context information. As a result, we propose\na Transformer-based encoder to model the human Top-Down\nvisual perception mechanism.\nThe encoder is stacked by Overlap Patching Transformer\n(OPT) blocks, which can mining the global prior knowledge\nof the crowd scene to reinforce the local crowd region fea-\nture from coarse to ﬁne-grained. As shown in Figure.1, giv-\nen the feature maps with a resolution of Hm ×Wm, our\nencoder performs i-th OPT block to obtain feature maps\nFi ∈R\nHm\n2i \u0002Wm\n2i \u0002Ci, where i ∈ {1; 2; 3}. An OPT com-\nposed of two sequential parts: an overlap patching layer and\na self-attention based feature encoder.\nThe overlap patching layer is to resolve the inputted 3D\nfeature maps into a sequence of 2D patches xi ∈RN\u0002D like\nthe stander vision transformer [Alexey et al., 2021 ], where\nN = Hi\nri\n×Wi\nri\n, D = ki ×ki ×Ci, ki ×ki is the resolution of\nthe patches and ri is the stride of the sliding window. Instead\nof splitting inputs into non-overlapping patches in manner of\nViT[Alexey et al., 2021], we use a ki ×ki slide window with\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1547\nstride ri (ri < ki and ri = 2 ) on inputted feature maps to\nobtain overlapped 2D feature patch sequence, which patches\nmaintain the relative position relation, while traditional trans-\nformers patch an input without overlapping, and may lost the\nrelative position information between patches.\nTransformer encoder utilizes a multi-head self-attention\nmodule (MSA) to learn the global prior knowledge about\ncrowd sense from the patchesxi in multiple feature subspace.\nThen, a learnable projector is used to project patches into their\nrelative spatial positions, which is shown as in the overlap\npatch transformer block of Figure 1. Formally, the feature\nencoder can be denoted as:\nx′\ni = MSA (xi) + xi\nFi+1 = fold(x′\ni ; Wi)\n(1)\nwhere the MSA ﬁrst splits the query, key, and value matri-\nces of the standard self-attention into h ploids and performs\nattention functions of these ploids in parallel:\n[Qi;p; Ki;p; Vi;p] = xi ·[WQi;p; WKi;p; WVi;p]\nx′\ni;p = softmax(Qi;p ·KT\ni;p\n√\nd\n) ·Vi;p\nx′\ni = cat(x′\ni;1; ::; x′\ni;p; :::; x′\ni;h)\n(2)\nwhere Qi;p; Ki;p; Vi;p ∈ RN\u0002D\nh are the query, key, val-\nue, and input matrices of the p-th ploid self-attention, and\nWQi;p; WKi;p; WVi;p ∈R\nD\nh \u0002D\nh are the corresponding map-\nping matrices. The cat(∗)concatenates the outputs of h self-\nattentions together.\nSpeciﬁcally, fold(∗; ∗)is a compound operator to keep\nthe relative position of patches unchanged, and fold the patch\nsequence x0\ni into feature maps Fi ∈R\nHi\nri\n\u0002Wi\nri\n\u0002Ci\n. It ﬁrst re-\nshapes the learnable project tensor Wi ∈RCi\u0002Ci−1\u0002ki\u0002ki to\nbe the ^Wi ∈RCi\u0002D, and then performs a dot product be-\ntween x0\ni and ^WT\ni . Finally, the result of dot product is fold\ninto Fi. With the weights sharing and overlap patching, sim-\nilar to convolution network, the relative spatial position in-\nformation of patches can be encoded into their feature repre-\nsentation. Comparing with adding the absolute position em-\nbeddings with feature vectors of patch sequence used in the\nstandard vision Transformer [Alexey et al., 2021 ], proposed\nmethod may result in improved generalization performance in\nthe terms of translation and rotation. It should be noted that,\nrecent work[Ze et al., 2021] shows that encoding the absolute\nposition of vision tokens may led to losing the translation and\nrotation invariance, which are vital to the generalization per-\nformance of visual models.\n3.3 Density Map Estimation\nTo predict the ﬁnal density map, we propose a lightweight de-\ncoder. As shown in the decoder phase of Figure.1, each fea-\nture Fi outputted by the corresponding OPT block is sent into\na feature collection unit (CFU), which utilizes a 1 ×1 convo-\nlution layer to fuse the Fi to be the ^Fi ∈R\nHm\nri\n\u0002Wm\nri\n\u0002C, and\nthen upsamples each ^Fi with bilinear interpolation into the\ngiven resolution Hp ×Wp. Then, all features ^Fi are concate-\nnated to be the feature F ∈RHp\u0002Wp\u00023C, where C = 256 is\n/g11 /g12\n2 2 2\n1 2 + + mh w k k k/g117 /g117 /g14\n /g12\n2 2 2\nm\nk\n2 2 2\n2 2 2\n + +\n2 2 2\n2 2 2\nm\nGenerator\n&split norm\n2\n1h w k/g117 /g117\n2\n2h w k/g117 /g117\n2\nih w k/g117 /g117\n2\nmh w k/g117 /g117\nfuse\n2h w k/g117 /g117\nplace\nImage Dot map\nDensitymap\nFigure 2: Multiple kernels fusion-based density map generation.\na hyper-parameter, and a 1 ×1 convolution layer following a\nnovel activation function, i.e., P-Sigmoid, is used to fuse the\ndifferent level features and generate the density map predic-\ntion.\nSpeciﬁcally, P-Sigmoid can be deﬁned as:\nxout = a\n1 + e−x (3)\nwhere a is a learnable parameter. It should note that a point\non density map may express more than one person. Usu-\nally used activation functions in deep learning, e.g., tanh,\nSigmoid and PReLU, are unsuitable for this task due to their\nranges. Therefore, most of existing methods adopt the abso-\nlute function to be the activation function for generating the\nﬁnal densisy map. In comparison to the absolute function,\nthe scale of P-Sigmoid values are close to the absolute func-\ntion due to the learnable parameter a, as well as the gradients\nare smoother due to having the attributes of Sigmoid. The\nproposed P-Sigmoid is better suited to crowd counting.\n4 Density Kernels Learning and Fusion\nPrevious related work mainly focuses on the density map esti-\nmation but ignores the generation of density map label. They\nusually generate a density map label by using a hand-crafted\nGaussian kernel to convolve the dot annotation map, where\nthe spatial location of each annotated person takes the val-\nue 1, (and 0 otherwise). In this way, these work failed to ﬁt\nthe scale change of the targets and scenarios. As a result, we\npropose a multiple density kernels fusion based density map\ngenerator, i.e.,KFMG, to generate density map label taking\nmultiple scales content information into consideration. The\npipeline is shown in Figure 2. We ﬁrst generate multiple\nadaptive density kernels with different sizes from the origi-\nnal image, and then fuse those density kernels to be one fused\nkernel. Finally, the fused kernel is used to convolve the dot\nannotation map and generate the density map label.\nGiven an input image I, we construct a CNN to generate\nmultiple adaptive density kernels K1; K2; ::; Km, and fuse\nthem to be one adaptive density kernel ^K. Formally, the gen-\nerating process can be denoted as:\nK = \n(I)\nK1; K2; ::; Km = split(K; [k2\n1; k2\n2; ::; k2\nm])\n^K =\nmX\ni=1\nNormalize (Ki)=m\n(4)\nwhere \n(∗)denotes a CNN, which ﬁrst downsamples I to be\nthe feature maps K ∈RHp\u0002Wp\u0002CK with the given resolution\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1548\nof Hp ×Wp. CK = Pm\ni=1 k2\ni is the number of channels\nin K, and ki is the size of the i-th adaptive density kernel.\nThen, split(K; [k2\n1; k2\n2; ::; k2\nm]) is to divide K into m groups\non the channel dimension, and each group Ki can be treated\nas an adaptive density kernel with the given size of ki ×ki.\nFinally, these adaptive density kernels are aligned along with\ntheir center points, and fused to be one adaptive density kernel\n^K with the size of ^k ×^k, where ^k = max(k1; k2; :::; km).\nLet D = {pj}N\nj=1 be the set of N annotated 2D coordi-\nnates of the persons in dot annotation map, pj = (xj; yj) in\nthe image I. For each annotation position pj, we retrieve the\ncorresponding kernel map ^Kpj ∈R^k\u0002^k from ^K, and then\nnormalize it to sum to 1, resulting in the location-speciﬁc\nkernel ^kpj = ^Kpj =sum( ^Kpj ). Finally, the density map is\ngenerated by placing the location-speciﬁc kernel maps on the\ncorresponding positions in the density map,\nM(p) =\nNX\nj=1\n^kpj (p −pj) (5)\nwhere the indexing of ^kpj is on p ∈(−r; :::; r) ×(−r; :::; r),\nand r = (^k −1)=2. It is important to note that the equation\n(5) is analogous to placing a Gaussians kernel on each dot an-\nnotation to generate a traditional density map, except that the\nmultiple kernels are learned, and could be different for each\nposition and each image. The learned density map M is then\nused as the groundtruth to train our density map estimator.\n5 Experiments\nIn this section, we conduct experiments evaluating our pro-\nposed CrowdFormer and density map generation framework.\nFirst, we go over the datasets, training, and evaluation met-\nrics in detail. Second, we compare our method with other\napproaches. Finally, we ablate the key design elements of our\nframework.\n5.1 Experimental Setup\nIn this experiment, three real-world datasets, i.e., NWPU-\nCrowd[Qi et al., 2021 ], UCF-QNRF [Haroon et al., 2018 ],\nand ShanghaiTech[Yingying et al., 2016], are used for evalu-\nation in our experiments.\nIn model training, the CNN based local feature embedding\nare initialized from the ﬁrst three blocks of pre-trained Y-\nOLOv5, and the rest parts of network are randomly initial-\nized by a Gaussian distribution with the mean of 0 and the\nstandard deviation of 0.01. The scales of adaptive density k-\nernels we adopted for generating the density map groundtruth\nare 3 ×3 and 5 ×5, respectively. For parameters training, an\nAdam optimizer are employed 900 epochs with a cosine de-\ncay learning rate scheduler and 10 epochs of linear warm-up.\nThe initial learning rate and weight decay are set to 1e-5 and\n1e-4, respectively. The training batch size is set to12, and da-\nta augmentations such as random-cropping of raw input with\nsize 512 ×512, random horizontal ﬂipping and color jittering\nare adopted.\nDuring the training, we incorporated the proposed density\nmap estimator and generator into an adaptive system, which\ntakes the both annotation dot map and original image as input,\nand jointly learns the estimator and generator by pixel-level\nL2 loss between the estimation and the generation of density\nmaps within an end-to-end framework.\n5.2 Evaluation and Analysis\nOn four real-world datasets, we compare our proposed den-\nsity map prediction framework to previous state-of-the-art\nmethods to assess overall counting performance. Following\nthe previous work, we adopt Mean Absolute Error (MAE)\nand Mean Squared Error (MSE) as the criterion to evaluate\nthe counting performance. According to the results shown in\nTable1, comprehensively compared with previous methods,\nthe performance of our method is the state-of-the-art, despite\nbeing challenged by other approaches in the highly congested\nscene, i.e., ShTech A.\nSpeciﬁcally, the TransCrowd [Dingkang et al., 2021 ],\nwhich also use the Transformer architecture for crowd count-\ning task, appears to be inadequate to other methods. How-\never, due to CrowdFormer, which encodes relative spatial\nposition of visual tokens rather than absolute position, our\napproach outperforms TransCrowd on a variety of bench-\nmarks. Furthermore, on the largest dataset, i.e., NWPU,\nour method achieves the best performance, which improves\nthe state-of-the-arts by 13.3% on MAE and 12.8% on MSE.\nThis result validates the effectiveness of the proposed method\nin real-world application. Finally, our method outperform-\ns the dot counting regression methods, such as NoiseCC [Jia\net al., 2020], DM-count[Boyu et al., 2020], BM-count[Liu et\nal., 2021 ], GLoss [Jia et al., 2021 ] and P2PNet[Song et al.,\n2021], on most of the datasets, because our density map gen-\neration method can utilize more context information about\ncrowd counting during both the training and testing phases,\nand allowing us to deal with a wide range of complex crowd\nscenes. Moreover, comparing with the recent density map\nand multiscale methods, such as KDMG [Wan et al., 2020 ],\nDKPNet[Chen et al., 2021 ], SASNet [Qingyu et al., 2021 ]\nand S3[Lin et al., 2021 ], our method also can obtain signif-\nicant advantage on most of the datasets, because the Crowd-\nFormer can obtain a crowd counting in manner of global to\nlocal.\n5.3 Ablation Studies\nAblation on the structure of CrowdFormer:We exam-\nine the effectiveness of the CrowdFormer structure. As the\nresults shown in Table 2, when we use 3 ×3 convolution\nlayers to replace all OPT blocks in CrowdFormer, the per-\nformance of crowd counting, i.e., Non\nOPT, degrades signif-\nicantly when compared to the performance of CrowdFormer.\nMoreover, when we use an OPT block to replace the local fea-\nture embedding module of CrowdFormer, the performance,\ni.e., Pure\nOPT, can approach the performance of Crowd-\nFormer, but a huge computation cost. These results demon-\nstrate that the effectiveness of the CrowdFormer structure on\ncontext information extraction about crowd regions.\nAblation on activation function:We substitute a common\nused absolute function (Abs) for the P-Sigmoid to determine\nthe effect of the proposed P-Sigmoid activation function. In\ndetail, we use model training and testing to demonstrate the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1549\nMethods Venue Features NWPU UCF-QNRF ShTech A ShTech B\ndr dmg ms vt MAE MSE MAE MSE MAE MSE MAE MSE\nNoiseCC NeurIPS 20 X × × × 96.9 534.2 85.8 150.6 61.9 99.6 7.4 11.3\nDM-count NeurIPS 20 X × × × 88.4 357.6 85.6 148.3 59.7 95.7 7.4 11.8\nBM-count IJCAI 21 X × × × 83.4 358.4 81.2 138.6 57.3 90.7 7.4 11.8\nGLoss CVPR 21 X × × × 79.3 346.1 84.3 147.5 61.3 95.4 7.3 11.4\nP2PNet ICCV 21 X × × × 77.4 362.0 85.3 154.5 52.7 85.1 6.2 9.9\nKDMG PAMI 20 × X X × 100.5 415.5 99.5 173.0 63.8 99.2 7.8 12.7\nDKPNet ICCV 21 × X X × 74.5 327.4 81.4 147.2 55.6 91.0 6.6 10.9\nSASNet AAAI 21 × X X × -- -- 85.2 147.3 53.6 88.4 6.4 9.9\nS3 IJCAI 21 X X × × 83.5 346.9 80.6 139.8 57.0 96.0 6.3 10.6\nTransCrowd -- X × × X 117.7 451.0 97.2 168.5 66.1 105.1 9.3 16.1\nCrowdFormer -- × X X X 67.1 301.6 78.8 136.1 56.9 97.4 5.7 9.6\nTable 1: Comparison with the state-of-the-art methods on benchmark datasets, where “dr”, “dmg”, “ms”, and “vt” indicate whether the\nmethods belong or not to the categories of dot counting regression, density map generation, multiscale fusion, and using vision transformer\narchitecture, respectively. In this experiment, we fuse two density kernels with sizes of3 ×3 and 5 ×5 for generating the density map labels.\n142.6 138.7 14 4.1\n136.1 13 6.5\n83.8 80.1 83.8 78.8 82.5\n70\n95\n120\n145\n170\n{3} {5} {7} {3,5} {5,7}\nKerenl size\n(a)  Performance on UCF-QNRF\nMSE\nMAE\n10.3 10.1\n9.5 9.6 9.4\n7.1\n6.5\n5.9 5.7 5.8\n4\n6\n8\n10\n12\n{3} {5} {7} {3,5} {5,7}\nKernel size\n(c\n)  Performance on ShTechB\nMSE\nMAE105.9\n100.6 100.9 97.4 100.3\n63.2\n57.7 59.3 56.9 59.3\n50\n7 0\n90\n110\n130\n{3} {5} {7} {3,5} {5,7}\nKernel size\n(b\n)  Performance on ShTechA\nMSE\nMAE\nFigure 3: Testing performance on different datasets w.r.t. using different sizes of density kernels for density map generation.\nModels MSE MAE\nNon OPT 152.3 90.2\nPure OPT 138.3 81.1\nCrowdFormer 136.1 78.8\nTable 2: Comparison of different model structures on the UCF-\nQNRF dataset.\n140.1\n81.6\n1\n36.1\n78.8\n70\n95\n120\n145\nMSE MAE\n(b) Testing performance\nAbs\nP-Sigmoid\n0\n2\n4\n6\n8\n0\n200 400 600 800\nLoss\nEpochs\n(a) Training performance\nAbs\nP-Sigmoid\nFigure 4: Training and testing performance w.r.t. using different\nactivation functions for density map generation.\nperformance gain. As shown in Figure 4(a), in the training\nstage, the convergence performance of the model using P-\nSigmoid is better than using Abs, because the gradients of\nP-Sigmoid are smoother. Furthermore, in the testing stage,\nthe P-Sigmoid outperforms the Abs in the terms of MSE and\nMAE as shown in Figure 4(b). The preceding demonstrates\nthat the performance gain of our model in terms of activation\nfunction is due to the proposed P-Sigmoid.\nAblation on density kernel fusion:We perform a series of\nexperiments to determine the effect of density kernel size on\nthe proposed density map generator. As the results shown in\nFigure 3, the small single kernel, e.g., k = 5, tends to yield\nbetter performance on the ShTech A and UCF-QNRF because\ntheir crowd scenarios are congested, while the large size, e.g.,\nk = 7, can obtain better performance in relative sparse crowd\nscenarios, i.e., ShTech B. We thus choose to fuse 3 ×3 and\n5 ×5 kernels in our density map generation framework, and\nachieve the best performance on all the datasets. This demon-\nstrates that the suitable kernel fusion can improve the density\nmap generation, because the fused density kernel has a better\nscene generalization.\n6 Conclusion\nIn this paper, we primarily propose a novel vision transformer\nfor modeling the humans’ Top-Down visual perception mech-\nanism in the crowd counting task, as well as a density kernels\nfusion framework for obtaining more accurate ground-truth\nof density map from the dot annotation maps. Moreover, we\nalso incorporate these two innovations into an adaptive crowd\ncounting model, which can jointly learn density map estima-\ntor and generator within an end-to-end framework. Finally,\nwe conduct extensive experiments, and prove our proposed\napproach can achieve superior performance in the terms of\nMAE and MSE on widely used datasets.\nAcknowledgments\nThis work is jointly supported by the National Natural Sci-\nence Foundation of China(62106290) and the Program for\nInnovation Research in Central University of Finance and E-\nconomics.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1550\nReferences\n[Alexey et al., 2021] Dosovitskiy Alexey, Beyer Lucas, et al.\nAn image is worth 16x16 words: Transformers for im-\nage recognition at scale. In International Conference on\nLearning Representations, 2021.\n[Ashish et al., 2017] Vaswani Ashish, Shazeer Noam, et al.\nAttention is all you need. In Annual Conference on Neural\nInformation Processing Systems, pages 5998–6008, 2017.\n[Boyu et al., 2020] Wang Boyu, Liu Huidong, et al. Distri-\nbution matching for crowd counting. In Annual Confer-\nence on Neural Information Processing Systems, 2020.\n[Chan et al., 2008] Antoni B. Chan, Zhang-Sheng John\nLiang, and Nuno Vasconcelos. Privacy preserving crowd\nmonitoring: Counting people without people models or\ntracking. In Computer Society Conference on Computer\nVision and Pattern Recognition, pages 1–7, 2008.\n[Chattopadhyay et al., 2017] Prithvijit Chattopadhyay, Ra-\nmakrishna Vedantam, et al. Counting everyday objects in\neveryday scenes. In Conference on Computer Vision and\nPattern Recognition, pages 4428–4437, 2017.\n[Chen et al., 2021] Binghui Chen, Zhaoyi Yan, et al. Varia-\ntional attention: Propagating domain-speciﬁc knowledge\nfor multi-domain learning in crowd counting. In Inter-\nnational Conference on Computer Vision, pages 16065–\n16075, 2021.\n[Dingkang et al., 2021] Liang Dingkang, Chen Xiwu, et al.\nTranscrowd: Weakly-supervised crowd counting with\ntransformer. CoRR, abs/2104.09116, 2021.\n[Haroon et al., 2018] Idrees Haroon, Tayyab Muhmmad,\nAthrey Kishan, et al. Composition loss for counting, den-\nsity map estimation and localization in dense crowds. In\nECCV, volume 11206, pages 544–559, 2018.\n[Jia et al., 2020] Wan Jia, Antoni B. Chan, et al. Modeling\nnoisy annotations for crowd counting. In Annual Confer-\nence on Neural Information Processing Systems, 2020.\n[Jia et al., 2021] Wan Jia, Liu Ziquan, et al. A general-\nized loss function for crowd counting and localization. In\nConference on Computer Vision and Pattern Recognition,\npages 1974–1983, 2021.\n[Lempitsky et al., 2010] Victor S. Lempitsky, Andrew Zis-\nserman, et al. Learning to count objects in images. In\nAnnual Conference on Neural Information Processing Sys-\ntems, pages 1324–1332, 2010.\n[Lin et al., 2021] Hui Lin, Xiaopeng Hong, et al. Direc-\nt measure matching for crowd counting. In Internation-\nal Joint Conference on Artiﬁcial Intelligence, pages 837–\n844, 2021.\n[Liu et al., 2021] Hao Liu, Qiang Zhao, et al. Bipartite\nmatching for crowd counting with point supervision. In\nInternational Joint Conference on Artiﬁcial Intelligence,\npages 860–866, 2021.\n[Min et al., 2008] Li Min, Zhang Zhaoxiang, Huang Kaiqi,\nand Tan Tieniu. Estimating the number of people in\ncrowded scenes by mid based foreground segmentation\nand head-shoulder detection. In International Conference\non Pattern Recognition, pages 1–4, 2008.\n[Nicolas et al., 2020] Carion Nicolas, Massa Francisco, et al.\nEnd-to-end object detection with transformers. In EC-\nCV, volume 12346 of Lecture Notes in Computer Science,\npages 213–229, 2020.\n[Qi et al., 2021] Wang Qi, Gao Junyu, et al. Nwpu-crowd:\nA large-scale benchmark for crowd counting and localiza-\ntion. Trans. Pattern Anal. Mach. Intell., 43(6):2141–2149,\n2021.\n[Qingyu et al., 2021] Song Qingyu, Wang Changan, et al. To\nchoose or to fuse? scale selection for crowd counting.\nIn AAAI Conference on Artiﬁcial Intelligence, volume 35,\npages 2576–2583, 2021.\n[Shuai et al., 2020] Bai Shuai, He Zhiqun, et al. Adaptive\ndilated network with self-correction supervision for count-\ning. In Conference on Computer Vision and Pattern Recog-\nnition, pages 4593–4602, 2020.\n[Song et al., 2021] Qingyu Song, Changan Wang, et al. Re-\nthinking counting and localization in crowds: A purely\npoint-based framework. In International Conference on\nComputer Vision, pages 3365–3374, 2021.\n[Tsung-Yi et al., 2017] Lin Tsung-Yi, Doll ´ar Piotr, et al.\nFeature pyramid networks for object detection. In Confer-\nence on Computer Vision and Pattern Recognition, pages\n936–944, 2017.\n[Wan and Chan, 2019] Jia Wan and Antoni B. Chan. Adap-\ntive density map generation for crowd counting. In In-\nternational Conference on Computer Vision, pages 1130–\n1139, 2019.\n[Wan et al., 2020] Jia Wan, Qingzhong Wang, and Antoni B\nChan. Kernel-based density map generation for dense ob-\nject counting. Transactions on Pattern Analysis and Ma-\nchine Intelligence, 2020.\n[Yifan et al., 2020] Yang Yifan, Li Guorong, et al. Weakly-\nsupervised crowd counting learns from sorting rather than\nlocations. In ECCV, pages 1–17, 2020.\n[Yingying et al., 2016] Zhang Yingying, Zhou Desen, et al.\nSingle-image crowd counting via multi-column convolu-\ntional neural network. In Conference on Computer Vision\nand Pattern Recognition, pages 589–597, 2016.\n[Yuhong et al., 2018] Li Yuhong, Zhang Xiaofan, and Chen\nDeming. Csrnet: Dilated convolutional neural networks\nfor understanding the highly congested scenes. In Confer-\nence on Computer Vision and Pattern Recognition, pages\n1091–1100, 2018.\n[Ze et al., 2021] Liu Ze, Lin Yutong, et al. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. In International Conference on Computer Vision,\npages 10012–1110022, 2021.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, et al. Re-\nthinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In Conference\non Computer Vision and Pattern Recognition, pages 6881–\n6890, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1551"
}