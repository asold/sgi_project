{
  "title": "Do Transformers Need Deep Long-Range Memory?",
  "url": "https://openalex.org/W3034561418",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2337829049",
      "name": "Jack Rae",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2001605895",
      "name": "Ali Razavi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W4288284003",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W4297779254",
    "https://openalex.org/W3007773043",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2952136670",
    "https://openalex.org/W4288088025",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4297788867",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2970567787",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W4295838474"
  ],
  "abstract": "Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524–7529\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7524\nDo Transformers Need Deep Long-Range Memory?\nJack W. Rae\nDeepMind & UCL\nLondon, UK\njwrae@google.com\nAli Razavi\nDeepMind\nLondon, UK\nalirazavi@google.com\nAbstract\nDeep attention models have advanced the mod-\nelling of sequential data across many do-\nmains. For language modelling in particu-\nlar, the Transformer-XL — a Transformer aug-\nmented with a long-range memory of past ac-\ntivations — has been shown to be state-of-\nthe-art across a variety of well-studied bench-\nmarks. The Transformer-XL incorporates a\nlong-range memory at every layer of the net-\nwork, which renders its state to be thousands\nof times larger than RNN predecessors. How-\never it is unclear whether this is necessary. We\nperform a set of interventions to show that\ncomparable performance can be obtained with\n6X fewer long range memories and better per-\nformance can be obtained by limiting the range\nof attention in lower layers of the network.\n1 Introduction\nWhen we read a book, we maintain representations\nof the characters and events in the text that help us\nunderstand the story. We do this with a selective\nmemorisation process; most of the ﬁner details of\nthe text are quickly forgotten and we retain a rela-\ntively compact representation of the book’s details.\nEarly models of natural language used recurrent\nneural networks (RNNs) such as the Long Short-\nTerm Memory (Hochreiter and Schmidhuber, 1997)\nwhich emulated this selective memory approach by\nmodelling the past in a compact state vector. The\nmodel learns to store relevant information within\nits state implicitly in order to optimise the task loss.\nThe LSTM has reigned as a state-of-the-art lan-\nguage model for over two decades since its incep-\ntion in the ’90s (Melis et al., 2017) and is arguably\nthe most ubiquitous neural sequence model. Un-\nlike human memory systems, however, the LSTM\nstruggles to reason over long-range contexts when\nreading text. This has been observed in multi-\nple contexts. In the carefully curated LAMBADA\nbenchmark (Paperno et al., 2016) which tests lan-\nguage model predictions on sections of book text\nthat have long term structure as decided by human\nraters, LSTMs completely fail. Namely LSTMs\nguess the correct word 0% of the time, where hu-\nmans are considered to be above70% accuracy. For\nregular language modelling, Daniluk et al. (2017)\nobserved that an LSTM augmented with attention\nwould rarely attend beyond seven preceding words\nof context. Samples from LSTMs language models\nquickly devolve into generic text devoid of an over-\nall theme. This has lead many to wonder whether\nthere is any non-negligible long-range signal in the\ntask of language modelling.\nRecently we have seen that deep attention mod-\nels can draw long-range signal from text, even\nwhen the objective is as simple as next-word predic-\ntion. With the advent of the Transformer (Vaswani\net al., 2017), signiﬁcant gains in language mod-\nelling performance can be obtained by extending\nthe models’ attention to thousands of words. The\nTransformer-XL (Dai et al., 2019), a Transformer\nvariant specialised for long-range sequence mod-\nelling via the introduction of a cache of past acti-\nvations, obtained state-of-the-art results in the four\nmajor LM benchmarks — PTB (Mikolov et al.,\n2010), LM1B (Chelba et al., 2013), Enwik8 (Hut-\nter, 2012), and WikiText (Merity et al., 2016). In\nthe case of the latter two, Dai et al. (2019) showed\nthe model effectively used over one thousand words\nof context, and the resulting samples reﬂect a the-\nmatic consistency spanning paragraphs. When\nTransformers are paired with long contexts and\na large amount of data, e.g. GPT-2 (Radford et al.,\n2019) and Megatron (Shoeybi et al., 2019), the re-\nsulting samples are remarkable in their long-range\nconsistency and stylistic realism.\nHowever Transformers abandon the compact and\nselective representation of the past. They store a\nhidden activation at every time-step (up to a given\n7525\nattention range) and every layer within the net-\nwork. This can consume orders of magnitude more\nspace than prior RNN hidden states, or the orig-\ninal text. E.g. a typical state-of-the-art LSTM\nlanguage model state size may range from 4KB\n(Rae et al., 2018) to model Wikipedia articles to\n64KB (Jozefowicz et al., 2016) to model news —\nand is never greater than 1MB. Whereas a current\nstate-of-the-art 18-layer Transformer-XL state size\nfor Wikipedia articles is 112MB. The state is so\nlarge because a separate memory (e.g. 1600 vec-\ntors of size d=1024) is maintained per layer. If this\nwere found to be unnecessary then we can reduce\nthe state’s memory considerably.\nIn this paper we investigate a simple question:\ncan we use short-range attention for the majority\nof layers in the Transformer and recover the same\nperformance? The hypothesis is that this should be\npossible, because many steps of reasoning will only\ninvolve short-range correlations, i.e. to piece char-\nacters together to form words or phrases. We ﬁnd\nindeed it is possible. We recover comparable perfor-\nmance for long-range language modelling by using\na small fraction (1/6th) of long-range memories to\nthe baseline TransformerXL. Crucially, we ﬁnd it\nmatters where long-range memories are placed in\nthe network. Placing them in the lower layers of\nthe network is ineffective; placing them in the lat-\nter layers or interleaved across the network works\nmuch better. We show that such a model trains with\n2X less time and memory, due to the reduction in\nexpensive attention operations.\n2 Background\nThe Transformer is a deep neural network for\nprocessing sequences (Vaswani et al., 2017), it\nprocesses a window of n consecutive inputs\nxt−n,...,x t in parallel. At each layer it rea-\nsons over time using multi-head attentionwhich\nwe will brieﬂy describe. For a given layer l, let\nht ∈R1×d be the hidden activation at time t, and\nh≤t ∈Rt×d be the preceding activations in the\nsame window. Let k be the number of attention\nheads, then Qi,Ki,Vi ∈Rd×d\nk are a set of learn-\nable weight matrices which generate queries, keys,\nand values per attention head. These are deﬁned to\nbe qi = htQi as the query, ki = h≤tKi to be the\nkeys, and vi = h≤tVi to be the values for attention\nhead i. The attention head output is deﬁned to be,\nattni(ht,h≤t) =σ(qikT\ni )vi\nFigure 1: Comparison of arrangement patterns for long-\nrange and short-range memories across the layers of\na Transformer. Baseline contains equally long-range\nmemories at every layer.\nwhere σ(·) is deﬁned to be the softmax operator.\nAttention is the linear combination of each atten-\ntion head, attn= ∑k\ni=1 Wi attni with a learnable\nweight.\nThe attention operation consumes O(n) com-\npute per step and thus O(n2) for the window of in-\nputs at each layer. The Transformer-XL (TXL) pro-\nposes concatenating the past activations from the\nsame window h≤t with a memory of size m≥n\nof past activations from the preceding windows of\ninputs (Dai et al., 2019). This results in an attention\ncost of O(n(n+ m)) which can be signiﬁcantly\ncheaper than processing all n+ minputs in paral-\nlel, which would require O((n+ m)2). The TXL’s\nmemory can be considered to be a state, alike to\nan RNN. However it requires a considerable space:\nl×m×d. For character-level language modelling\nDai et al. (2019) use a 24-layer model on Enwik8,\nwith memory size m = 3800, and hidden size\nd = 1024; this consumes 356MB at single preci-\nsion. In contrast, the average article size is 8KB.\n3 Experiments\nWe investigate whether the Transformer-XL can\nperform comparably with fewer long-range mem-\nory (LRM) layers on the two prominent long-\nrange language modelling benchmarks, Enwik8\nand WikiText-103.\n3.1 Interventions\nWe perform intervention experiments where we\nreplace the long-range memory, for a given layer,\nwith a short-range memory (SRM) of size ms =\n128 for a subset of layers. We choose ms = 128\nbecause the TPUv3 contains a 128x128 matrix mul-\ntiply unit, and any smaller size (other than zero) is\npadded up to 128. Thus it is a reasonable small size.\nWe chose ms >0 such that the oldest activations\nhave some context. Because we only modify the\n7526\n0.0 0.5 1.0 1.5\nTraining tokens (B)\n1.0\n1.05\n1.1\n1.15\n1.2\n1.25\n1.3\n1.35Bits-per-character (BPC)\nNo. long-range mems -- best valid bpc\n1 (first layer) -- 1.107\n1 (last layer) -- 1.069\n12 (1st half) -- 1.072\n12 (2nd half) -- 1.059\n12 (interleaved) -- 1.062\n24 -- 1.058\n61\n75\n92\n114\n140\n171\n211\n259\nWord-level perplexity\nFigure 2: Enwik8 learning curves for varying long-\nrange memory arrangements and no. layers. BPC over\nthe ﬁrst 500K characters from validation.\nmemory sizes of the model, which are independent\nof parameter count, the number of model param-\neters is always held constant(277M for Enwik8\nand 257M for WikiText-103).\nWe consider a model with a varying number of\nLRMs from l(the number of layers in the network,\ni.e. the usual case) to a range of fewer values, l\n2 , l\n6 ,\n1, and 0. We also consider where the LRMs should\nbe arranged within the network; considering (i)\ninterleaved with equal spacing, (ii) the ﬁrst layer(s)\nof the network, and (iii) the latter layer(s) of the\nnetwork. This is displayed visually in Figure 1.\n3.2 Model Setup\nAside from memory conﬁgurations, we use an iden-\ntical model setup to Dai et al. (2019). During train-\ning we periodically evaluate on the validation set\nto choose an early stopping criterion. In the case of\nEnwik8 we periodically evaluate on the ﬁrst 500K\ncharacters of the validation set to speed up model\nevaluation. We train all models with an overall\nbatch size of 32, using 16 TPUv3 chips running\nsynchronously. We use a window size of n= 384,\na long-range memory (LRM) size of m = 2304.\nAt test-time we extend the LRM size to m= 6000,\nchosen from a sweep over the validation set.\n4 Results\nWe plot the Enwik8 learning curves for a subset of\nlayer variants in Figure 2. The worst-performing,\nis the variant with a single long-term memory at\nthe lowest layer (black curve). However perhaps\nmore surprisingly, we see a model with 12 LRMs\nat the lower layers of the network is actually worse\nthan a model with a single LRM on the ﬁnal layer\n0 1 4 12 24\nNum. long-range memories\n0.98\n1.00\n1.02\n1.04\n1.06\n1.08Bits-per-character (BPC)\nLRM arrangement\nn/a\nfirst layer(s)\ninterleaved\nlast layer(s)\n52.9\n57.4\n62.2\n67.5\n73.2\n79.4\nWord-level perplexity (ppl)\nFigure 3: Enwik8 test performance over a varying num-\nber of long-range memories and arrangement patterns.\nLower is better. Model: 24-layer Transformer-XL,\nevaluation long-range memory size: 6000 (trained with\n2304) and short-range memories size: 128.\n(dark green). We then see that the full TXL with\n24 LRMs is seemingly identical to the 12 LRM\nmodels, with either LRMs interleaved across the\nwhole model or LRMs placed in the ﬁnal 12 layers.\nNote, we were not able to run these models with\nmultiple seeds per hyper-parameter conﬁguration\n- but we do generally ﬁnd language models opti-\nmise consistently (e.g. unlike deep reinforcement\nlearning models).\nWe show the ﬁnal test performance in bits-per-\ncharacter (BPC) alongside the corresponding word-\nlevel perplexity for models with a varying num-\nber of LRMs and LRM arrangements in Figure 3.\nPosition clearly matters, if we place long-range\nmemories in the ﬁrst layers then performance is\nsigniﬁcantly worse. We hypothesise that this is\nbecause it is better to build up representations with\nlocal context before exploiting long-range corre-\nlations. For example, we need to piece together\ncharacters into an identiﬁed named entity (say) be-\nfore we should query thousands of time-steps back\nfor its prior occurrence.\nWe followed-up by running an additional ar-\nrangement of only placing LRMs in the middle\nlayers and found this to be worse than interleaved\nor ﬁnal (1.01bpc for 4 long-range memories) which\nshows there is signiﬁcant beneﬁt to having some\nlong-range memories in the higher layers.\nCrucially, we are able to match (and slightly\nexceed) the full model’s test performance with 12\nLRMs, and even a model with 4 LRMs is very close\n(0.9846 w/ 24 vs 0.9916 w/ 4 interleaved). It is\nworth noting that our TXL baseline actually out-\nperforms the published version on Enwik8: 0.985\nBPC (ours) vs 0.993 (Dai et al., 2019), which pro-\nvides credence to the quality of the experimental\nsetup.\n7527\nNum. LRMs Memory (GB) Time / token (us)\n24 3.4 405\n12 2.8 273\n4 1.1 191\n1 0.50 155\n0 0.20 143\nTable 1: Proﬁling a 24-layer TXL training on Enwik8.\nWe also inspect word-level language modelling\non WikiText-103, using the same 18-layer Trans-\nformerXL parameters (Dai et al., 2019). We obtain\na baseline test perplexity of 18.3 (matching the\npublished value), and obtain 18.4 and 18.6 for in-\nterleaved and last-layer spacing respectively when\nusing l/6 (i.e. 3) LRMs. We also try placing 3\nLRMs on the ﬁrst three layers and obtain 20.1 per-\nplexity. We remark that (i) long-range memory is\nimportant for a signiﬁcant improvement in perfor-\nmance, (ii) it is better to not place LRMs in the\nshallow layers, and (iii) it is not necessary to have\nas many long-range memories as model-layers for\ncomparable modelling performance.\n4.1 Performance\nWe show the performance of training the\nTransformer-XL with a varying number of LRMs\nfor the Enwik8 architecture in Table 1. This shows\nthe latency (per input token) and peak activation\nmemory consumption during a training iteration on\nEnwik8 for a range of long-range memory layers.\nWe see the reduction of long-range memories from\n24 layers to 4 layers cuts the activation peak mem-\nory by 3X. Thus it can be a worthwhile and simple\nperformance improvement.\n4.2 Varying Short-Range Memory\nIn the preceding experiments we ﬁx the short-range\nmemory (SRM) length to 128 and vary the fre-\nquency and arrangement of long-range memory\nlayers. We now consider varying the length of\nSRM for an architecture with l\n6 long-range memo-\nries to determine whether this impacts modelling\nperformance.\nWe train (and evaluate) the model with twenty\nSRM lengths from 32-2048, and incorporate four\ninterleaved LRM layers (trained at 2304, evaluated\nat 6000). The results are plotted in Figure 4. Short-\nening the memory size to less than 128 provides no\nspeedup for our TPU training setup, as matrices are\nmultiplied in 128x128 blocks, however it incurs\na drop in modelling performance. Furthermore\n32.0 64.0 128.0 256.0 512.0 1024.0 2048.0\nShort-Range Memory Size\n0.96\n0.98\n1.00\n1.02\n1.04\n1.06BPC\nFigure 4: Enwik8 test performance for varying short-\nrange memory length (at both train and test). Trans-\nformerXL model uses 4 interleaved long-range mem-\nories (trained 2304, tested 6000) and 20 short-range\nmemory layers.\nincreasing the memory size beyond 512 further\nslows the model down and reduces modelling per-\nformance. We see an optimal SRM length is around\n512 steps which obtains 0.974BPC on Enwik8 —\na non-trivial performance boost over the 0.99BPC\nTransformerXL baseline. Thus we conclude that\nlimiting the range of attention can not only speed\nup the model but improve performance.\n5 Related Work\nThere have been several recent works exploring\ndeep sequence models with a small attention win-\ndow per layer. Wu et al. (2019) proposed the dy-\nnamic convolution, where the model directly pro-\nduces a set of weights over a sequence in memory\nand then combines them with a convolution. The at-\ntention window is thus restricted to the convolution\nkernel size — a couple of words. Wu et al. (2019)\nshow comparable performance to the Transformer\nat sentence-level machine translation. However\nthey do not investigate longer-context applications.\nRae et al. (2019) propose shortening the range\nof attention for Transformers by compressing the\ndistant past. They ﬁnd the ﬁrst layers of the model\nare the most compressible, and obtain state-of-the-\nart in several long-range language model bench-\nmarks (WikiText-103 and Enwik8). However they\ndo not consider restricting the range of attention\nfor a subset of layers to save compute and space.\nSukhbaatar et al. (2019) propose an adaptive at-\ntention scheme for the TransformerXL where the\nmodel can learn to modulate the size of its attention\nwindow per attention head. They observe the neu-\nral network converges to using smaller attention\nspans for lower layers in the network, which adds\nadditional evidence to the ﬁnding that long-range\nmemories are not useful in these lower layers. Be-\n7528\ncause Sukhbaatar et al. (2019) place the range of\nattention in the optimisation problem it is very ﬂex-\nible. In this study we promote interpretability by\nmaking a set of direct interventions to the memory\nsize across layers. This does result in less general-\nity, as we explicitly create two types of attention\nranges, where adaptive attention can select many.\nHowever ultimately the two approaches of general-\nity and interpretability complement one another.\n(Fan et al., 2020) show that one can train a trans-\nformer by having all layers attend to a single mem-\nory that is the linear combination of all layers’\nmemories. Thus at training all layers’ memories\nare maintained, but at evaluation or generation time\nthere can be a single memory. This gives evidence\nthat we do not need to store many separate repre-\nsentations for long-range memory to perform well\nat test time, but the approach does require storing\nthem during training — and incurs signiﬁcant slow-\ndown to the model.\n6 Discussion\nWe explore a set of interventions to the\nTransformer-XL’s architecture that are very sim-\nple to implement, i.e. a few lines of code, but shed\nlight on the fundamental workings of the model\nwhen modelling long sequences of text. In our\nset of interventions, we only modify the ﬂow of\ninformation within the network, versus the num-\nber of trainable parameters. Thus we do not have\nconfounding factors of varying network capacity.\nOur ﬁnding is that we do not need long-range\nmemories at every layer of the network. Com-\nparable performance can be obtained with a frac-\ntion (1/6th) of long-range memories if they are\nspaced equally across the network, or in the latter\nlayers. We hypothesise this is because modelling\nlong-range correlations is best done when represen-\ntations are ﬁrst formed from short-range correla-\ntions. We also ﬁnd a real performance drop using\na single long-range memory, proving long-range\ndependency is not superﬂuous to the task.\nThis study has implications for practitioners in-\nterested in speeding up deep Transformer-XL mod-\nels. There have been a number of long-range trans-\nformer variants published in the past year (Lample\net al., 2019; Rae et al., 2019; Roy et al., 2020; Ki-\ntaev et al., 2020) which aim to extend the range\nof attention via sparsity or compression. However\nthese models maintain the use of uniform memory\ncapacity for each layer. Here we show that long-\nrange attention does not need to be scaled for every\nlayer, and thus these architectures can be further\nsped-up with this observation.\nThis study also has implications for researchers\nusing a single long-range memory, which has typ-\nically been the approach in traditional RNN + at-\ntention systems. For example, the Differentiable\nNeural Computer (Graves et al., 2016) and recent\nmemory-augmented agents for reinforcement learn-\ning, which utilise a distinct working memory with\na single long-range episodic memory (Fortunato\net al., 2019). Perhaps performance could be im-\nproved by adding additional layers of episodic\nmemories.\nThe practice of storing deep long-range memo-\nries is not scalable if we wish for neural networks\nto have the kinds of large-horizon reasoning that\nhumans possess. We believe the solution of main-\ntaining a small number of long-range memories is\na step towards tractable lifelong memory.\nReferences\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nMichal Daniluk, Tim Rocktaschel, Johannes Welbl,\nand Sebastian Riedel. 2017. Frustratingly short at-\ntention spans in neural language modeling. Proceed-\nings of International Conference on Learning Repre-\nsentations (ICLR).\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2020. Access-\ning higher-level representations in sequential trans-\nformers with feedback memory. arXiv preprint\narXiv:2002.09402.\nMeire Fortunato, Melissa Tan, Ryan Faulkner, Steven\nHansen, Adri `a Puigdom `enech Badia, Gavin Butti-\nmore, Charles Deck, Joel Z Leibo, and Charles Blun-\ndell. 2019. Generalization of reinforcement learners\nwith working and episodic memory. In Advances\nin Neural Information Processing Systems, pages\n12448–12457.\nAlex Graves, Greg Wayne, Malcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio G ´omez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John Agapiou, et al.\n7529\n2016. Hybrid computing using a neural net-\nwork with dynamic external memory. Nature,\n538(7626):471.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nMarcus Hutter. 2012. The human knowledge compres-\nsion contest. URL http://prize. hutter1. net, 6.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´e J ´egou. 2019. Large memory layers with\nproduct keys. arXiv preprint arXiv:1907.05242.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nAnnual Conference of the International Speech Com-\nmunication Association.\nDenis Paperno, Germ ´an Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fern ´andez. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nJack W Rae, Chris Dyer, Peter Dayan, and Tim-\nothy P Lillicrap. 2018. Fast parametric learn-\ning with activation memorization. arXiv preprint\narXiv:1803.10049.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nand Timothy P Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling. arXiv\npreprint arXiv:1911.05507.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. arXiv\npreprint arXiv:2003.05997.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive\nattention span in transformers. arXiv preprint\narXiv:1905.07799.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7944551706314087
    },
    {
      "name": "Limiting",
      "score": 0.7179374694824219
    },
    {
      "name": "Computer science",
      "score": 0.6776632070541382
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34672385454177856
    },
    {
      "name": "Electrical engineering",
      "score": 0.22162023186683655
    },
    {
      "name": "Engineering",
      "score": 0.1683872938156128
    },
    {
      "name": "Voltage",
      "score": 0.08119463920593262
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ],
  "cited_by": 26
}