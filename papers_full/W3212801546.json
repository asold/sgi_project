{
  "title": "Improving Visual Quality of Image Synthesis by A Token-based Generator\\n with Transformers",
  "url": "https://openalex.org/W3212801546",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5101629056",
      "name": "Yanhong Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101953781",
      "name": "Huan Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061525421",
      "name": "Hongyang Chao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100338007",
      "name": "Jianbo Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5072029041",
      "name": "Jianlong Fu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2792263949",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3034352949",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3035355202",
    "https://openalex.org/W2963636093",
    "https://openalex.org/W2962760235",
    "https://openalex.org/W3102061158",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W648143168",
    "https://openalex.org/W3104727832",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3035570181",
    "https://openalex.org/W3035653890",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3104876213",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W3034973631",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2805984778",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W967544008",
    "https://openalex.org/W2572730214",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2971128425",
    "https://openalex.org/W2962975391",
    "https://openalex.org/W2963836885",
    "https://openalex.org/W2985068832",
    "https://openalex.org/W3034625979",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3034431451",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3126210001",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W3135404760",
    "https://openalex.org/W3176913662",
    "https://openalex.org/W2995801453",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2962785568"
  ],
  "abstract": "We present a new perspective of achieving image synthesis by viewing this\\ntask as a visual token generation problem. Different from existing paradigms\\nthat directly synthesize a full image from a single input (e.g., a latent\\ncode), the new formulation enables a flexible local manipulation for different\\nimage regions, which makes it possible to learn content-aware and fine-grained\\nstyle control for image synthesis. Specifically, it takes as input a sequence\\nof latent tokens to predict the visual tokens for synthesizing an image. Under\\nthis perspective, we propose a token-based generator (i.e.,TokenGAN).\\nParticularly, the TokenGAN inputs two semantically different visual tokens,\\ni.e., the learned constant content tokens and the style tokens from the latent\\nspace. Given a sequence of style tokens, the TokenGAN is able to control the\\nimage synthesis by assigning the styles to the content tokens by attention\\nmechanism with a Transformer. We conduct extensive experiments and show that\\nthe proposed TokenGAN has achieved state-of-the-art results on several\\nwidely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with\\ndifferent resolutions. In particular, the generator is able to synthesize\\nhigh-fidelity images with 1024x1024 size, dispensing with convolutions\\nentirely.\\n",
  "full_text": "Improving Visual Quality of Image Synthesis by A\nToken-based Generator with Transformers\nYanhong Zeng1,2∗ Huan Yang3 Hongyang Chao1,2 Jianbo Wang4 Jianlong Fu3\n1School of Computer Science and Engineering, Sun Yat-sen University\n2Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\n3Microsoft Research Asia 4University of Tokyo\nAbstract\nWe present a new perspective of achieving image synthesis by viewing this task as\na visual token generation problem. Different from existing paradigms that directly\nsynthesize a full image from a single input (e.g., a latent code), the new formulation\nenables a ﬂexible local manipulation for different image regions, which makes it\npossible to learn content-aware and ﬁne-grained style control for image synthesis.\nSpeciﬁcally, it takes as input a sequence of latent tokens to predict the visual tokens\nfor synthesizing an image. Under this perspective, we propose a token-based\ngenerator (i.e.,TokenGAN). Particularly, the TokenGAN inputs two semantically\ndifferent visual tokens, i.e., the learned constant content tokens and the style tokens\nfrom the latent space. Given a sequence of style tokens, the TokenGAN is able to\ncontrol the image synthesis by assigning the styles to the content tokens by attention\nmechanism with a Transformer. We conduct extensive experiments and show that\nthe proposed TokenGAN has achieved state-of-the-art results on several widely-\nused image synthesis benchmarks, including FFHQ and LSUN CHURCH with\ndifferent resolutions. In particular, the generator is able to synthesize high-ﬁdelity\nimages with 1024 ×1024 size, dispensing with convolutions entirely.\n1 Introduction\nUnconditional image synthesis generates images from latent codes by adversarial training [10, 16,\n19, 25, 27, 33, 50]. Recent advances have been achieved by a style-based generator architecture in\nterms of both the visual quality and resolution of generated images [26–28, 37, 52]. In particular, the\nstyle-based generator has been widely used in many other generative tasks, including facial editing\n[9, 40], style transfer [1, 38], image super-resolution [17, 31], and image inpainting [2, 52].\nThe key to the success of the style-based generator lies in the learning of the style control based on\nthe intermediate latent space W[27, 28]. Instead of feeding the input latent code z ∈Z through\nthe input layer only (Figure1-a), the style-based generator maps the input z to an intermediate latent\nspace w ∈W , which then controls the “style” of the image at each layer via adaptive instance\nnormalization (AdaIN [21]) (Figure 1-b). It has been demonstrated that such a design allows a less\nentangled representation learning in W, leading to better generative image modeling [12, 23, 27, 40].\nDespite the promising results, the style-based generator can suffer from the style control via AdaIN\noperation [28, 52]. Speciﬁcally, the style control is content-independent. It “washes away” the\noriginal information of features by normalization and assigns new styles decided by the latent codes\nregardless of the image/feature content. Besides, the style code w affects the entire image by scaling\nand biasing complete feature maps with a single value via AdaIN operation [ 35, 44, 54]. Such an\nimposed single style over multiple image regions can inevitably result in entangled representation of\n∗This work was done while Yanhong Zeng was a research intern at Microsoft Research Asia.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2111.03481v2  [cs.CV]  18 Dec 2021\n(a) Traditional Generator (b) Style-based Generator (c) Token-based Generator\n...\n...\n...\nFigure 1: Overview of different generators. (a) A traditional generator [16] feeds a single latent z as\ninput to control the image synthesis. (b) A style-based generator [27] maps z to an intermediate latent\nspace w ∈W to control the styles of the content c via AdaIN [21]. (c) Our token-based generator\nstarts from a sequence of content tokens {c1,···,cm}and controls each content token with a set of\nstyle tokens {s1,···,sn}∈S by attention mechanism with a visual Transformer. M denotes the\nmapping network and G denotes the generator network.\ndifferent image variations (e.g., hairstyle, facial expression) [28, 44, 52]. These limitations for image\nmodeling can even lead to visible artifacts in the synthesized results (e.g., droplet artifacts [27]).\nTo get rid of the issues caused by StyleGAN’s style modeling, we introduce a new perspective\nthat views image synthesis as a visual token generation problem. The visual token is a popular\nrepresentation of an image patch with a predeﬁned size and position [ 6, 7, 13]; and has shown an\nimpressive superiority in various tasks with the development of Transformer models [6, 13, 43, 46].\nInspired by the appealing property of the token-based representation, we propose to achieve image\nsynthesis by visual token generation. Speciﬁcally, it takes as input a sequence of latent tokens\nto predict the visual tokens of an image. Such a token-based representation enables a ﬂexible\nlocal manipulation for different image regions, which makes it possible to learn content-aware and\nﬁne-grained style control for image synthesis.\nUnder this new paradigm, we design a token-based generator, i.e., TokenGAN, for the visual token\ngeneration problem. Speciﬁcally, we consider two different types of input tokens in the generator,\ni.e., the content tokens and the style tokens. The content tokens are learned as the constant input in\nthe generator network and the style tokens are projected from a learned intermediate latent space\n(Figure1-c). Given a sequence of style tokens, the TokenGAN learns to control the visual token\ngeneration by rendering each content token with related style tokens according to their semantics. In\nparticular, since the Transformer has been veriﬁed to be effective in sequence modeling in a broad\nrange of tasks [6, 43], we adopt a generator network architecture from a visual Transformer to model\nthe relations between the content tokens and the style tokens. Through such a content-dependent style\nmodeling by the attention mechanism in Transformer, the TokenGAN is able to achieve content-aware\nand ﬁne-grained style learning for image synthesis.\nWe conduct both quantitative comparisons and qualitative analysis on several unconditional image\ngeneration benchmarks. The results show that the token-based generator has achieved comparable\nresults to the state-of-the-art in image synthesis. We summarize our contributions as below:\n• We propose a new perspective of achieving image synthesis by visual token generation.\nSuch a token-based representation enables ﬂexible local manipulation for different image\nregions, leading to a better image modeling.\n• We propose a token-based generator (i.e., TokenGAN) for the visual token generation.\nSpeciﬁcally, the TokenGAN introduces the style tokens and the content tokens. It adopts a\nTransformer-based network for content-aware style modeling.\n• We show extensive experiments (quantitative and qualitative comparisons, study on style\nediting, image inversion and image interpolation) to verify the effectiveness of the token-\nbased generator. Speciﬁcally, the token-based generator is able to synthesize high-ﬁdelity\n1024 ×1024 images without any convolutions in the generator.\n2 Related works\n2.1 Style-based generator\nThe distinguishing feature of the style-based generator is its unconventional generator architecture\n[26–28, 52]. Typically, the style-based generator consists of a mapping networkf and a synthesis\n2\nnetwork g. The mapping network is used to transform the input latent code z to an intermediate latent\ncode w ∈W for learning a less entangled latent space. The synthesis network follows a progressive\ngrowing design that is able to ﬁrst output low-resolution images that are not affected signiﬁcantly by\nhigh-resolution layers [25, 27]. Such an architecture design can lead to an automatic, unsupervised\nseparation of high-level attributes in different layers. For example, when trained on human faces, the\nlow-resolution layers control coarse styles (e.g., pose, identity) of images and high-resolution layers\ncontrol ﬁne styles (e.g., micro-structure, color scheme) [25].\nTo control the attributes of the image, the style-based generator produces styles from the intermediate\nlatent code w by afﬁne transforms, which then control each layer of the synthesis network via adaptive\ninstance normalization (AdaIN) [21, 35]. The AdaIN operation removes styles from previous layers\nby normalizing each feature map to zero mean and unit deviation, and it assigns the new styles by\nscaling and biasing the complete normalized feature maps. However, it has been demonstrated such an\nAdaIN operation can destroy the magnitude information of features and thus results in the well-known\ndroplet artifacts [28]. Besides, the scale-speciﬁc style disentanglement in StyleGAN can be limited\ndue to the complex semantic components in each feature map [28, 52]. In this paper, we explore a\nnew architecture by ﬂattening the image as a sequence of tokens, which enables ﬁne-grained control\nof the image by assigning token-wise semantic-aware styles based on the attention mechanism.\n2.2 Transformer in vision\nThe Transformer typically takes as input a sequence of vectors, called tokens [43]. The Transformer\nis built for sequence modeling solely on attention mechanisms over the tokens, dispensing with\nrecurrence and convolutions entirely [5, 11, 43, 30]. Due to its great success in the ﬁeld of natural\nlanguage processing, an increasing number of works attempt to extend Transformer for computer\nvision tasks [6–8, 13, 18, 36, 55]. For example, iGPT shows promising results in image classiﬁcation\nby pre-training a sequence Transformer with the task of auto-regressive next pixel prediction and\nmasked pixel prediction [ 8]. Dosoyvitskiy et al. propose to split an image into patches and feed\nthese patches into a standard Transformer (ViT) for image classiﬁcation [13]. They show that a ViT\nwith large-scale training can trump CNNs equipped with inductive bias. DETR is a seminal work\nthat views object detection as a direct set prediction problem, eliminating the need for hand-crafted\ncomponents and achieving impressive performance by Transformer [6]. To mitigate the issue of high\ncomputation complexity associated with long sequences caused by high-resolution images, a branch\nof works explores lightweight Transformer (e.g.., Deformable DETR) for vision tasks [55].\nTransformer is also attracting increasing attention in low-level vision tasks [14, 24, 34, 36, 42]. Parmar\net al. cast image generation as an autoregressive sequence generation problem and propose Image\nTransformer with a local self-attention mechanism [ 36]. However, Image Transformer can suffer\nfrom its quadratic computation cost and its long inference time due to auto-regressive prediction.\nMost existing works adopt a hybrid CNN-Transformer architecture, which consists of a CNN\nhead for feature extraction, a Transformer encoder-decoder backbone, and a CNN tail for feature\ndecoding [7, 46, 48]. For example, IPT fully utilizes the Transformer architecture by a large-scale\npre-training and achieves promising results in several image restoration tasks [7]. In parallel work,\nHudson et al. introduce bipartite structure to maintain computation of linear efﬁciency for long-range\ninteractions across the image based on a convolution backbone [ 22]. Our token-based generator\ninherits the network architecture from Transformer without any convolution layers and is able to\nyield surprising promising results for high-resolution image synthesis.\n3 Approach\nIn this section, we introduce the details of the proposed token-based generator (TokenGAN). As\ndepicted in Figure 2, the TokenGAN takes as input two kinds of tokens to generate the visual tokens of\nimages by a Transformer. We introduce the input tokens in Section 3.1 and the token-based generator\narchitecture in Section 3.2, following the overall optimization in Section 3.3.\n3.1 Visual tokens\nIt’s intuitive that the visual tokens of an image contain the information of both content (i.e., semantics)\nand related styles. In our token-based generator, we choose to separate these two kinds of information\n3\nContent Tokens\nStyle Tokens\n(a) Visual Tokens\n...\nMapping \nNetwork\nPosition EncodingPosition Encoding\n...\n...\nStyle Normalization\n......\nStyle ModulationStyle Modulation\nStyle Normalization\nStyle Normalization\nStyle ModulationStyle Modulation\n......\nStyle ModulationStyle ModulationStyle Modulation\n......\n...\n...\n...\n(b) Token-based Generator with Transformers\nStyle ModulationStyle Modulation\n... ......\nSoftmax\n...\nToken to ImageToken to Image\n...\nToken to ImageToken to Image\n...\nConcat\n&\nReshape\nLearnableNorm(                      )\nFigure 2: The overview of the TokenGAN for the visual token generation task for image synthesis.\nThe TokenGAN takes as input two kinds of visual tokens, i.e., the style tokens and the content tokens,\nto generate visual tokens of an image. Speciﬁcally, TokenGAN learns to render each content token\nby attended style tokens with a Transformer, leading to content-aware and ﬁne-grained style control.\nby a sequence of content tokens and a sequence of style tokens, so that the interactions between them\ncan be modeled and controlled for the image synthesis. We introduce the details of each as below.\nContent tokens. As depicted in Figure 2-a, the TokenGAN starts from the sequence of learned\nconstant content tokens c ∈Rm×(p2×d) through the input layer. Speciﬁcally, dis the dimension in\nterms of channels, p×pis the patch size of the content token, and mis the length of the content\ntoken sequence. To maintain the position information of each token, we add position encodings\np ∈Rm×(p2×d) to each content token following a commonly used paradigm [7, 13]. For simplicity,\nwe rewrite the notation for the content tokens as {c1,···,cm}, where ci ∈Rd.\nStyle tokens. Given a latent code z from the input latent space Z, the mapping network adopts\nseveral MLPs to map the input z to a set of different style tokens {s1,···,sn}∈S , where si ∈Rd.\nAs shown in Figure 2-b, the style tokens are paired with a set of learnable semantic embedding as a\nkey-value structure in each style modulation layer of the TokenGAN. With the Transformer modeling,\nall the content tokens will match with the semantic embedding and then fetch the new style from the\nstyle tokens based on the matching results. The fetched new styles are used to control the values of\nthe content tokens, which are ﬁnally decoded to images. Such a token-wise style control enables\ncontent-aware and ﬁne-grained style learning for image synthesis.\n3.2 Token-based generator\nAs shown in Figure 2, the TokenGAN consists of multiple layers of style blocks and each style\nblock consists of a style normalization layer and a style modulation layer. Speciﬁcally, the style\nnormalization layer removes the styles from previous layers by relieving the dependence on the\noriginal statistics of input token features. At the same time, the style modulation layer assigns new\nstyles from the current layer to the content tokens. In particular, the new styles are calculated based\non the pairwise interactions between the style tokens and the content tokens by a cross-attention\nmechanism. We introduce more details about the style normalization, content-aware style modeling,\nand style modulation in our generator as below.\nStyle normalization. We apply style normalization on the input content tokens to remove the styles\nfrom previous layers, which is denoted as:\nNorm(ci) = (ci −µ(ci)) /σ(ci), i= 1,···,m, (1)\n4\nnose eyes jaw hair backgroundmouth\nFigure 3: Visualization of the attention maps obtained by Eq. (2). Each column highlights the\nimage regions that are affected most by the corresponding content-aware style token. It shows that\nTokenGAN is able to attend to meaningful semantics for different persons in an unsupervised way.\nThe tags are associated by human based on the attention responses for clear presentation.\nwhere each ci ∈Rd denotes a content token, ddenotes the dimension of each content token, and mis\nthe length of the token sequence. Speciﬁcally, the style-based generator typically removes the styles\nfrom previous layers by Instance Normalization[41], which may destroy the information conveyed\nby the magnitude of the feature map relative to each other [ 27, 28, 41]. To avoid the above issue,\nwe adopt LayerNorm [43] to relive the style dependence while maintaining the information of the\nrelative magnitude following previous works [28, 29, 43].\nContent-aware style modeling. After style normalization, the TokenGAN calculates new styles to\ncontrol the input content tokens. As shown in the style modulation in Figure 2-b, the style tokens are\npaired with a set of learnable semantic embedding as a key-value structure. Given the content tokens\n{c1,···,cm}from the input and the style tokens from the mapping network {s1,···,sn}∈S , the\ntoken-wise new styles are calculated by attention mechanism:\nS′= Attention(C,K,S) =softmax(CKT\n√\nd\n)S, (2)\nwhere S′∈Rm×d denotes new styles for all the content tokens, the input content tokens are packed\ntogether into a matrix C ∈Rm×d, similarly with the style tokens as matrix S ∈Rn×d, and the\nmatrix K ∈Rn×d for the learnable semantic embedding in where each row vector indicates a learned\nsemantic. We omit the linear projection in the standard attention calculation for simplicity [43].\nThrough such a cross-attention mechanism, the new token-wise styles are fetched for style modulation\nbased on the matching results. Particularly, tokens with similar semantics will have similar styles\n(e.g., the same color for the two eyes). We visualize the attended regions for different key-value pairs\n(i.e., semantic-style embedding) in Figure 3, and we can ﬁnd that the learned embedding K is able to\nattend to meaningful image regions.\nStyle modulation. Different from the traditional Transformer that adds the attention results back\nto the input features as residual features, our attention results are used as new styles by amplifying\neach channel of the content tokens:\nC′= C ⊙S′, (3)\nwhere each row vector in C ∈Rm×d indicates a content token, and ⊙indicates element-wise\nmultiplication. Such a style modulation affects the operation of subsequent embedding layers (which\nwe implemented by fully-connected layers) and thus control the style of generated images.\nImplementation details. We translate the generated visual tokens to the image by concatenation\nand reshaping. In practice, we follow StyleGAN2 and adopt a skip-generator architecture in Token-\nGAN. Such a skip generator generates images in each layer at different resolutions (e.g., from 42 to\n82 ) and progressively upsamples and sums the images from the previous layer by skip connections\nto the next layer [28, 25]. After progressive summing, the generator takes the output in the last layer\nas the ﬁnal results.\n5\n3.3 Overall optimization\nStyle mixing. To further encourage the styles to localize, we employ the mixing regularization\ntechnique during training [27]. To be speciﬁc, a given percentage of images are generated using two\nrandom latent codes. We run two latent codes z1, z2 through the mapping network, and have the\ncorresponding {s1\n1,···,s1\nn}, {s2\n1,···,s2\nn}. We randomly choose an inject point to mix the styles, so\nthat a part of the ﬁnal style tokens from z1 and another part from z2. This regularization technique\nprevents the network from assuming that a set of style tokens are correlated. A similar strategy is\nadopted in terms of different layers following the style-based generator [27].\nOptimization objectives. We denote the generated image output by the token-based generator as:\nG(z) =g(f(z),C), (4)\nwhere z ∈Z∼N (0,1) indicates the input latent code, C = {c1,···,cm}denotes the sequence of\nlearned constant content tokens through the input layer, and f, g, Gindicate the mapping network,\nthe generator network and the token-based generator, respectively. Our token-based generator follows\nthe same optimization objectives used by the style-based generator [27, 28], i.e., an non-saturating\nlogistic adversarial loss with R1 regularization. To be speciﬁc, the generator loss is:\nLG = Ez∼N(0,1)[log(1 +exp(−D(G(z))))], (5)\nwhere Dis the discriminator and the discriminator loss is\nLD = Ex∼Pd [log(1 +exp(−D(x)))] +Ez∼N(0,1)[log(1 +exp(D(G(z))))] +R1(ψ), (6)\nwhere x∼Pd denotes images from the real data, and R1(ψ) is the regularization term calculated by:\nR1(ψ) =γ\n2 Ex∼Pd\n[\n∥∇Dψ(x)∥2]\n. (7)\nSpeciﬁcally, the R1 regularization term [ 32] is computed less frequently than the main loss function\nfollowing the commonly used lazy regularization strategy, thus greatly diminishing the computational\ncost and the overall memory usage [27, 28, 52].\n4 Experiments\n4.1 Experiment Setup\nDataset To evaluate the token-based generator and make fair comparisons with the SOTA approach\n(i.e., the style-based generator [28]), we conduct experiments on the most commonly-used public\nresearch datasets, i.e., Flickr-Faces-HQ (FFHQ) [27] and Large-scale Scene Understanding (LSUN)\n[47]. Speciﬁcally, FFHQ consists of 70,000 high-quality images of human face, which is able to\nevaluate the model’s ability to synthesize high-frequency details. We conduct experiments on FFHQ\nwith both 256 ×256 and 1024 ×1024 image size following previous works [15, 22]. Besides, we\nchoose the church set of LSUN [47] to evaluate the synthesis in terms of complex scenes.\nEvaluation. In this section, we conduct both quantitative and qualitative experiments on the token-\nbased generator. Speciﬁcally, we report quantitative results by three commonly-used metrics, i.e.,\nFréchet Inception Distance (FID) [ 19], Precision and Recall [ 29, 39]. We use FID as it has been\nwidely used in many generation tasks as an effective perceptual metric [ 4, 26, 27]. In addition to\nthe image quality assessment, we use the metrics of Precision and Recall to evaluate the distribution\nlearned by GAN. Speciﬁcally, the Precision metric intuitively measures the quality of samples from\nthe generated images, while recall measures the proportion of real images that is covered by the\nlearned distribution by GAN [39].\nTraining details We use 8 NVIDIA V100 GPUs for training each model. We build upon the public\nPytorch implementation of StyleGAN22 [28]. For fair comparisons, we inherit all of the training\ndetails and parameter settings from the conﬁguration E setup. All models are trained and report the\nbest results under the same training iterations. For each iteration, we set the size of mini-batch as 32.\nWe use the Adam solver [3] with the same momentum parameters β1 = 0, β2 = 0.99 to train both the\ngenerator and discriminator. We apply R1 regularization for every 16 iterations.\n2https://github.com/rosinality/stylegan2-pytorch\n6\nFigure 4: Uncurated results for FFHQ-1024 [27] and LSUN CHURCH [47]. The results show that\ntoken-based generator is able to synthesize both high-frequency details (e.g., hair and beard on the\nhuman face) and plausible structures for complex scenes (e.g., the church).\nTable 1: Quantitative comparisons with the state-of-the-art model (i.e., StyleGAN2 [ 28]) on FFHQ-\n256 [27], FFHQ-1024[ 27] and LSUN CHURCH [ 47]. We compute each metric 10 times with\ndifferent random seeds and report their average. The result shows that the token-based generator\nachieves comparable performance with the SOTA.↑the higher, the better. ↓the lower, the better.\nModel Metrics FFHQ-256 [27] FFHQ-1024 [27] LSUN CHURCH [47]\nFID↓ 6.09 ±0.051 5 .20 ±0.049 5 .88 ±0.043\nStyleGAN2 [27] Precesion ↑ 0.643 ±0.002 0 .653 ±0.002 0 .587 ±.002\nRecall ↑ 0.401 ±0.002 0 .411 ±0.002 0 .358 ±0.002\nFID↓ 5.41 ±0.050 5 .21 ±0.032 5 .56 ±0.037\nTokenGAN(Ours) Precesion ↑ 0.660 ±0.002 0 .651 ±0.001 0 .577 ±0.002\nRecall↑ 0.447 ±0.003 0 .442 ±0.004 0 .376 ±0.002\n4.2 Unconditional image synthesis\nQuantitative comparisons. We report quantitative comparisons with StyleGAN2 [28] due to its\nstate-of-the-art performance. For fair comparisons, we report the results with the same training\niterations on FFHQ-256 [27], FFHQ-1024 [27] and LSUN CHURCH [47] by three objective metrics,\ni.e., FID, Precision and Recall in Table 1. The results show that the token-based generator has\nachieved the state-of-the-art results in terms of both perceived quality and distribution modeling.\nQualitative results. To demonstrate the image quality of the synthesis results for the token-based\ngenerator, we randomly sample noise z ∈Z ∼N (0,1) to generate images for high-resolution\nimages (i.e., with resolution 1024 ×1024) of human face and the images of complex church scenes.\nThe uncurated results can be found in Figure 4. It shows that the token-based generator is able to\nsynthesize both high-frequency details of the human face and plausible structures for complex scenes.\nStyle editing. We study and report the results of style editing by the TokenGAN in Figure 5. All\nthe results are obtained by editing the style token of interest and then re-synthesizing images using\nedited style tokens. The result shows several interesting properties of the TokenGAN. First, the\nTokenGAN has a similar behavior as in StyleGAN2, i.e., controlling coarse/middle/ﬁne styles at\ndifferent layers (e.g., pose/hairstyle/color scheme in 82/162 −642/1282 −2562 layers). Besides,\nthe TokenGAN shows localized behaviors for the style tokens. For example, in the ﬁrst row, editing\nthe style token for the pose \"globally\" changes the pose while remaining the styles of other regions\n(e.g., hairstyle, background). In the second row, editing the style token for hair length, TokenGAN\n\"locally\" turns the hair longer while remaining other styles.\n7\nchanging pose\nediting hair length\nediting hairstyle\nadding beards\nadding smiling\nchanging color scheme\ncoarse stylefine style middle style\nFigure 5: Results of style editing. All the results are obtained by editing the style token of in-\nterest and then re-synthesizing images using edited style tokens. It shows that TokenGAN con-\ntrols coarse/middle/ﬁne styles at different resolution layers (e.g., pose/hairstyle/color scheme in\n82/162 −642/1282 −2562 layers). Besides, the TokenGAN is able to get localized behavior by\nediting a speciﬁc style while remaining other styles at the same time. For example, in the second row,\nthe TokenGAN turns the hair length longer without changing other styles.\nImage inversion. To better apply well-trained GANs to real-world applications, the technique of\nGAN inversion has attracted an increasing attention [ 40, 53, 1, 2]. Such an inversion technique\nenables real image editing by searching the most accurate latent code in the learned latent space to\nrecover the real image [53]. It has been demonstrated that the model with better inversion results tend\nto learn a better image modeling of the real data [28]. Speciﬁcally, we randomly sample several real\nimages from FFHQ-256 [28] and adopt the optimization-based inversion technique used by most\nworks [27, 40, 53]. For fair comparisons, we use the same setting following StyleGAN2 [27]. The\ninversion results by the style-based generator and the token-based generator are shown in Figure\n6. It shows that the token-based generator is able to reconstruct real images better. For example,\nStyleGAN2 tends to reconstruct the lips and the headscarf in the ﬁfth case with the same red color,\n8\nReal ImageStyleGAN2Ours\n Real ImageStyleGAN2Ours\nFigure 6: Visual results of image inversion by StyleGAN2 [28] and the token-based generator. We\nadopt the same inversion technique following the common paradigm [ 1, 2, 27]. It shows that the\ntoken-based generator is able to reconstruct ﬁne-grained details with the dense style control over\nimage regions (e.g., the facial expressions in the third case).\nTable 2: Quantitative comparison of the reconstructed images by image inversion in terms of mean\naverage error (MAE, range=[0,255]) and LPIPS distance [51].\nModel MAE ↓ LPIPS ↓ Model MAE ↓ LPIPS ↓\nStyleGAN2[28] 16.45 0.1539 TokenGAN 13.43 0.1238\nwhile the token-based generator is able to reconstruct accurate colors for the lips and the headscarf\nrespectively. This is because TokenGAN renders different regions by using different style tokens via\nattention mechanism, while StyleGAN2 renders them by a single style vector.\nFor a better quantitative comparison for image inversion, we randomly sample 1,000 real images from\nFFHQ-256 and report the mean absolute error (MAE, range=[0,255]) and the LPIPS distance [51] of\nthe inversion results by StyleGAN2 and TokenGAN. The results in Table 2 show that TokenGAN\nis able to reconstruct signiﬁcantly more accurate results with much lower MAE and shorter LPIPS\ndistance, which is in line with the visual results in Figure 6.\nImage interpolation. To further explore the property of the learned latent space by the token-based\ngenerator, we perform image interpolation by a linear interpolation in the learned intermediate latent\nspace S. In practice, we randomly sample two noises z1,z2 and run them through the mapping\nnetwork to have their corresponding sequence of style tokenss1 := {s1\n1,···,s1\nn}, s2 := {s2\n1,···,s2\nn}.\nAfter that we perform linear interpolation for each token by s3\ni = α×s1\ni + (1−α) ×s2\ni,α ∈(0,1)\nand use the new style tokens s3 := {s3\n1,···,s3\nn}to synthesize a new image. As shown in Figure\n7, we show the sampled styles s1,s2 in the ﬁrst column and the last column, and the interpolated\nstyles between them. The results show that the token-based generator is able to perform perceptually\nsmooth transition between two different styles (e.g., glasses, age, hair length, coloring, etc.)\n4.3 Ablation study\nTo study the effectiveness of each component of the token-based generator, this section presents abla-\ntion studies on FFHQ-256 [27] after 0.4 million training iterations in terms of FID [19]. Speciﬁcally,\nwe calculate the metric 10 times with different random seeds and report the average results.\n9\nFigure 7: Results of image interpolation by the token-based generator. In practice, we sample two\nlatent codes (the 1st and the 7th column) and perform linear interpolation of them in the learned\nintermediate latent space (interpolated results are from the 2th to the 6th column). The token-based\ngenerator shows smooth transition of high-level attributes in results (e.g., glasses in the second row).\nTable 3: Quantitative ablation study on the number of style tokens.\nstyle tokens 8 16 32 64\nFID 7.66 ±0.060 7.02 ±0.060 6.81 ±0.044 7.60 ±0.071\nThe number of style tokens.To study the effectiveness of the style tokens, we conduct ablation\nstudies by using different number of style tokens. As shown in Table 3, with the growing number of\nthe style tokens, there are more style tokens to model the distribution of styles for different semantics\nand the quality of images are improved. However, when the number of style tokens grows to 64, the\nlearning will be difﬁcult and performance would drop.\nTable 4: Comparisons on the content tokens.\n(m64,m128,m256) FID↓\n(162,162,322) 18.54±0.087\n(322,322,642) 15.69±0.067\n(642,642,1282) 6.81 ±0.044\nTable 5: Comparisons on the style normalization.\nModel FID\nInstanceNorm [41] 13.7 ±0.051\nPixelNorm [25] 6.96 ±0.050\nLayerNorm [43] 6.81 ±0.044\nThe number of content tokens.We empirically set the number of content tokens mat different\nlayers according to the image size for the best performance and training efﬁciency. We conduct\nablation study from layer 642 to 2562 and denote the number of content tokens in these layers\nas (m64,m128,m256). The results in Table 4 show that more content tokens could provide more\nﬁne-grained control over the whole images, leading to better results in image generation.\nStyle normalization layers.We compare different style normalization layers in Table 5. The results\nshow that, PixelNorm [25] and LayerNorm [43] signiﬁcantly outperforms the style normalization\nby instance normalization in the token-based generator [ 41]. We choose LayerNorm as the style\nnormalization layer in the token-based generator.\n5 Conclusion\nIn this paper, we propose a novel TokenGAN showing that the token-based representation of image\nfeatures and styles could enable content-aware and ﬁne-grained style learning for image synthesis.\nSpeciﬁcally, we use Transformer to model the interaction between content tokens and style tokens,\nwhich facilitates the perceived quality of generated images and shows promising properties in terms\nof style editing. In the future, we will study to extend the generative transformers to extensive\napplications, e.g., style transfer [20], animation generation [45], image inpainting [49], etc.\n10\nAcknowledgments and Disclosure of Funding\nFunding in direct support of this work: NSF of China under Grant 61672548 and U1611461, GPUs\nprovided by Microsoft Research Asia. Additional revenues related to this work: internship at\nMicrosoft Research Asia.\nReferences\n[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the\nstylegan latent space? In ICCV, pages 4432–4441, 2019.\n[2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded\nimages? In CVPR, pages 8296–8305, 2020.\n[3] Diederik P. Kingma andJimmy Ba. Adam: A method for stochastic optimization. In ICLR,\n2015.\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity\nnatural image synthesis. In ICLR, 2018.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213–229,\n2020.\n[7] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR,\npages 12299–12310, 2021.\n[8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In ICML, pages 1691–1703, 2020.\n[9] Edo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. Editing in style: Uncovering the local\nsemantics of gans. In CVPR, pages 5771–5780, 2020.\n[10] Emily Denton, Soumith Chintala, Arthur Szlam, and Robert Fergus. Deep generative image\nmodels using a laplacian pyramid of adversarial networks. In NeurIPS, pages 1486–1494, 2015.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171–4186,\n2019.\n[12] Chris Donahue, Zachary C Lipton, Akshay Balsubramani, and Julian McAuley. Semantically\ndecomposing the latent spaces of generative adversarial networks. In ICLR, 2018.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution\nimage synthesis. In CVPR, pages 12873–12883, 2021.\n[15] Rinon Gal, Dana Cohen, Amit Bermano, and Daniel Cohen-Or. Swagan: A style-based\nwavelet-driven generative model. arXiv preprint arXiv:2102.06108, 2021.\n[16] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron C Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\n[17] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In CVPR,\npages 3012–3021, 2020.\n11\n[18] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui\nTang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. arXiv preprint\narXiv:2012.12556, 2020.\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS,\npages 6629–6640, 2017.\n[20] Zhiyuan Hu, Jia Jia, Bei Liu, Yaohua Bu, and Jianlong Fu. Aesthetic-aware image style transfer.\nIn ACM MM, pages 3320–3329, 2020.\n[21] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance\nnormalization. In ICCV, pages 1501–1510, 2017.\n[22] Drew A Hudson and C. Lawrence Zitnick. Generative adversarial transformers. ICML, 2021.\n[23] Ali Jahanian, Lucy Chai, and Phillip Isola. On the \"steerability\" of generative adversarial\nnetworks. In ICLR, 2019.\n[24] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make\none strong gan, and that can scale up. In NeurIPS, 2021.\n[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. In ICLR, 2018.\n[26] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.\nTraining generative adversarial networks with limited data. In NeurIPS, 2020.\n[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In CVPR, pages 4401–4410, 2019.\n[28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of stylegan. In CVPR, pages 8110–8119, 2020.\n[29] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved\nprecision and recall metric for assessing generative models. NeurIPS, 32:3927–3936, 2019.\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\n[31] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-\nsupervised photo upsampling via latent space exploration of generative models. In CVPR, pages\n2437–2445, 2020.\n[32] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\nactually converge? In ICML, pages 3481–3490, 2018.\n[33] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization\nfor generative adversarial networks. In ICLR, 2018.\n[34] Aäron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and\nKoray Kavukcuoglu. Conditional image generation with pixelcnn decoders. In NeurIPS, pages\n4797–4805, 2016.\n[35] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis\nwith spatially-adaptive normalization. In CVPR, pages 2337–2346, 2019.\n[36] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In ICML, pages 4055–4064, 2018.\n[37] Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoen-\ncoders. In CVPR, pages 14104–14113, 2020.\n[38] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and\nDaniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In\nCVPR, pages 2287–2296, 2021.\n12\n[39] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assess-\ning generative models via precision and recall. In NeurIPS, pages 5234–5243, 2018.\n[40] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for\nsemantic face editing. In CVPR, pages 9243–9252, 2020.\n[41] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Improved texture networks: Maxi-\nmizing quality and diversity in feed-forward stylization and texture synthesis. In CVPR, pages\n6924–6932, 2017.\n[42] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nIn ICML, pages 1747–1756, 2016.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008,\n2017.\n[44] Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Attentive normalization\nfor conditional image generation. In CVPR, pages 5094–5103, 2020.\n[45] Hongwei Xue, Bei Liu, Huan Yang, Jianlong Fu, Houqiang Li, and Jiebo Luo. Learning\nﬁne-grained motion embedding for landscape animation. In ACM MM, pages 291–299, 2021.\n[46] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture trans-\nformer network for image super-resolution. In CVPR, pages 5791–5800, 2020.\n[47] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop. arXiv\npreprint arXiv:1506.03365, 2015.\n[48] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transforma-\ntions for video inpainting. In ECCV, pages 528–543, 2020.\n[49] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning pyramid-context\nencoder network for high-quality image inpainting. In CVPR, pages 1486–1494, 2019.\n[50] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative\nadversarial networks. In ICML, pages 7354–7363, 2019.\n[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The un-\nreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586–595,\n2018.\n[52] Heliang Zheng, Jianlong Fu, Yanhong Zeng, Jiebo Luo, and Zheng-Jun Zha. Learning semantic-\naware normalization for generative adversarial networks. NeurIPS, 33:21853–21864, 2020.\n[53] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image\nediting. In ECCV, pages 592–608, 2020.\n[54] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic\nregion-adaptive normalization. In CVPR, pages 5104–5113, 2020.\n[55] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. ICLR, 2021.\n13",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.8195071220397949
    },
    {
      "name": "Computer science",
      "score": 0.7955869436264038
    },
    {
      "name": "Fidelity",
      "score": 0.6753774881362915
    },
    {
      "name": "Transformer",
      "score": 0.6713461875915527
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.637853741645813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4767554700374603
    },
    {
      "name": "Image (mathematics)",
      "score": 0.44398415088653564
    },
    {
      "name": "Computer vision",
      "score": 0.3613132834434509
    },
    {
      "name": "Voltage",
      "score": 0.10485437512397766
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}