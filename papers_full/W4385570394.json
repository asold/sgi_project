{
  "title": "Can Language Models Be Specific? How?",
  "url": "https://openalex.org/W4385570394",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104755790",
      "name": "Jie Huang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4201822862",
      "name": "Kevin Chen-Chuan Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187349416",
      "name": "Jinjun Xiong",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A3171834798",
      "name": "Wen mei Hwu",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "Nvidia (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3104163040",
    "https://openalex.org/W2250571530",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4366736258",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2953345635",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2963161084",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W3193084543",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4205275361",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W3100475986",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3201254286",
    "https://openalex.org/W4297795751"
  ],
  "abstract": "“He is a person”, “Paris is located on the earth”. Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given “Toronto is located in [MASK].”, we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research community to further explore this important but understudied problem.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 716–727\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCan Language Models Be Specific? How?\nJie Huang1 Kevin Chen-Chuan Chang1 Jinjun Xiong2 Wen-mei Hwu1,3\n1University of Illinois at Urbana-Champaign, USA\n2University at Buffalo, USA\n3NVIDIA, USA\n{jeffhj, kcchang, w-hwu}@illinois.edu\njinjun@buffalo.edu\nAbstract\n“He is a person ”, “ Paris is located on the\nearth”. Both statements are correct but mean-\ningless – due to lack of specificity. In this paper,\nwe propose to measure how specific the lan-\nguage of pre-trained language models (PLMs)\nis. To achieve this, we introduce a novel ap-\nproach to build a benchmark for specificity test-\ning by forming masked token prediction tasks\nwith prompts. For instance, given “Toronto is\nlocated in [MASK].”, we want to test whether\na more specific answer will be better filled in\nby PLMs, e.g., Ontario instead of Canada.\nFrom our evaluations, we show that existing\nPLMs have only a slight preference for more\nspecific answers. We identify underlying fac-\ntors affecting the specificity and design two\nprompt-based methods to improve the speci-\nficity. Results show that the specificity of the\nmodels can be improved by the proposed meth-\nods without additional training. We hope this\nwork can bring to awareness the notion of speci-\nficity of language models and encourage the\nresearch community to further explore this im-\nportant but understudied problem.1\n1 Introduction\nPre-trained language models (PLMs) such as BERT\n(Devlin et al., 2019) and GPT-2/3 (Radford et al.,\n2019; Brown et al., 2020) have achieved quite im-\npressive results in various natural language pro-\ncessing tasks. Recent works show that the param-\neters of these models contain significant amounts\nof knowledge (Petroni et al., 2019; Roberts et al.,\n2020; Jiang et al., 2020a,b; Wang et al., 2020), and\nknowledge stored in PLMs can be extracted by\npredicting the mask token(s) using prompts. For\ninstance, given prompt “J. K. Rowling was born\nin [MASK].”, PLMs can predict the birthplace of\nRowling based on its knowledge.\n1Code and data are available at https://github.com/\njeffhj/S-TEST.\nToronto is located on the earth.Dante is a person.Cat is a subclass ofanimal.\nFigure 1: Examples of language modeling that lack\nspecificity. More specific descriptions could be: feline,\npoet, and in Ontario, respectively.\nHowever, there may exist multiple answers for\na query, while not all answers are equally specific.\nIn many situations, we desire a specific answer.\nFor the example above, the masked token can be re-\nplaced by Yate(a town), Gloucestershire (a county),\nor England (a country). To acquire the maximum\nknowledge (in this example, the town, the county,\nand the country where Rowling was born), we may\nprefer the model to fill in Yate since Gloucester-\nshire and England can be further predicted using\nprompts, e.g., “Yate is located in [MASK].” This\nmeans, if the prediction is more specific, we can re-\ntrieve more fine-grained information from language\nmodels, and further acquire more information. Be-\nsides, sometimes, the less specific answer is not\nuseful. For instance, it is well known that Chicago\nis located in the USA, users will not get additional\ninformation if the model only predicts Chicago is\nlocated in the USA instead of Illinois. More exam-\nples are shown in Figure 1. To make an analogy: A\ngood speaker not only needs to be correct, but also\nhas the ability to be specific when desired. The\nsame is true for language models.\nAlthough there are works on measuring how\nmuch knowledge is stored in PLMs or improving\nthe correctness of the predictions (Petroni et al.,\n2019; Roberts et al., 2020; Jiang et al., 2020b),\nfew attempted to measure or improve the speci-\nficity of predictions made by PLMs. Noteworthy\nexceptions include the work by Adiwardana et al.\n(2020); Thoppilan et al. (2022), who evaluated the\nspecificity of conversational language models. In\n716\ntheir research, specificity was defined and mea-\nsured within a conversational context – for instance,\nthe response “Me too. I love Eurovision songs” is\ndeemed more specific than simply “Me too” to the\nstatement “I love Eurovision”. Understanding how\nspecific the language of PLMs is can help us better\nunderstand the behavior of language models and\nfacilitate downstream applications such as ques-\ntion answering, text generation, and information\nextraction (Liu et al., 2021a; Khashabi et al., 2020;\nBrown et al., 2020; Wang et al., 2020), e.g., mak-\ning the generated answers/sentences or extracted\ninformation more specific or fine-grained.\nTherefore, we propose to build a benchmark to\nmeasure the specificity of the language of PLMs.\nFor reducing human effort and easier to further\nexpand the dataset (e.g., to specific domains), we\nintroduce a novel way to construct test data au-\ntomatically based on transitive relations in Wiki-\ndata (Vrande ˇci´c and Krötzsch, 2014). Specifi-\ncally, we extract reasoning paths from Wikidata,\ne.g., ( J. K. Rowling, birthplace, Yate, location,\nGloucestershire, location, England). Based on\nthe average distance of each object to the subject\nand the property of transitive relations, we form\nmasked-token-prediction based probing tasks to\nmeasure the specificity, e.g., whether the masked\ntoken in “J. K. Rowling was born in [MASK].” is\nbetter filled by Yate than England by PLMs. The\nresulting benchmark dataset contains more than\n20,000 probes covering queries from 5 different\ncategories. The quality of the benchmark is high,\nwhere the judgment on which answer is more spe-\ncific is ∼97% consistent with humans.\nWe provide in-depth analyses on model speci-\nficity and study two factors that affect the speci-\nficity with our benchmark. As shown by our evalu-\nations in Section 4, existing PLMs, e.g., BERT and\nGPT-2, similarly have only a slight preference for\nmore specific answers (in only about 60% of cases\nwhere a more specific answer is preferred). We also\nshow that, in general, PLMs prefer less specific an-\nswers without subjects given, and they only have\na weak ability to differentiate coarse-grained/fine-\ngrained objects by measuring their similarities to\nsubjects. The results indicate that specificity was\nneglected by existing research on language models.\nHow to improve and control it is undoubtedly an\ninteresting and valuable problem.\nBased on our observations and analyses, we pro-\npose two techniques to improve the specificity of\nthe predictions by modifying the prompts without\nadditional training: Few-shot Prompting, where\ndemonstrations with more specific answers are pro-\nvided to guide the models to produce more specific\nanswers; and Cascade Prompting, where which\nclauses are added as suffixes to bias the predictions\nto be more specific. Results show that Few-shot\nPrompting can improve the specificity for unidi-\nrectional language models like GPT-2 well, while\nCascade Prompting works well for bidirectional\nlanguage models such as BERT.\nThe main contributions of our work are summa-\nrized as follows:\n• We propose a novel automatic approach to\nbuild a benchmark for specificity testing based\non the property of transitive relations.\n• We analyze the specificity of several existing\nPLMs and study two factors that affect the\nspecificity.\n• We propose two methods to improve the speci-\nficity by modifying the prompts without addi-\ntional training.\n• We provide in-depth analyses and discussions,\nsuggesting further works to explore and fur-\nther improve the specificity.\n2 Background and Related Work\nPre-Trained Language Models: Pre-trained lan-\nguage models (PLMs) are language models pre-\ntrained on large corpora. In this paper, we will\ncover two types of pre-trained language models:\nunidirectional language models, such as GPT-2\n(Radford et al., 2019), where the prediction of the\ncurrent token is only based on previous tokens; and\nbidirectional language models, such as BERT (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019),\nwhere both left and right contexts are utilized to\npredict the current token.\nKnowledge Retrieval from LMs and Prompt-\ning: Previous works have worked on extracting\nfactual knowledge from PLMs without incorporat-\ning external knowledge, which is usually achieved\nby creating prompts and letting PLMs predict the\nmasked token(s) (Petroni et al., 2019; Bouraoui\net al., 2020; Jiang et al., 2020a,b; Wang et al., 2020).\nThey demonstrated that PLMs contain a significant\namount of knowledge. By creating appropriate\nprompts with some additional training, such meth-\nods can even achieve performance comparable to\nSOTA for some specific tasks (Shin et al., 2020;\nLiu et al., 2021b). Our work is inspired by these\n717\nworks; but different from these works, where the\nfocus is to measure or improve the correctness of\nthe predictions, our work focuses on measuring and\nimproving the specificity of the predictions.\n3 S-TEST: Specificity Testing\nIn this section, we introduce our specificity testing\n(S-TEST) task, describe the creation process of\nthe dataset, and design the metric to measure the\nspecificity of predictions made by PLMs.\n3.1 Task Formulation\nSpecificity is a semantic feature of language to de-\nscribe things specifically in a given context. In\nthis work, we focus on measuring the specificity\nof the predictions produced by pre-trained lan-\nguage models for entity relations. Formally, if\n(x, r, y)∧(y, r, z) implies (x, r, z), then y is consid-\nered as a more fine-grained object ofx than entity z\nunder relation r, and y is more specific than z. For\ninstance, to extract the answer (object) for relation\n(Toronto, location, X), we convert the query to a\nmasked token prediction task using prompts, e.g.,\n“Toronto is located in [MASK].” and let PLMs pre-\ndict the masked token. The answer here can be a\ncoarse-grained one, e.g., Canada, or a fine-grained\none, e.g., Ontario. The model is considered to be\nmore specific if it tends to fill in Ontario instead of\nCanada. More general scenarios are discussed in\nSection 7 as future work.\n3.2 Test Data Construction\nWe build a benchmark dataset for measuring the\nspecificity based on Wikidata (Vrande ˇci´c and\nKrötzsch, 2014), which is a knowledge base con-\ntaining a large number of entities and relations.\nSpecifically, we utilize transitive relations2 in Wiki-\ndata to create the test set automatically. Transitive\nrelations are binary relations with properties such\nthat (x, r, y) and (y, r, z) implies (x, r, z), where\nentity y can be considered as a more fine-grained\nobject of x than entity z under relation r.\nFor instance, relation P131 is a transitive rela-\ntion, whose label is “located in the administrative\nterritorial entity”. From Wikidata, we can extract\nfacts (Toronto, P131, Ontario) and (Ontario, P131,\nCanada), which furthermore forms a reasoning\npath (Toronto, P131, Ontario, P131, Canada). And\nOntario is considered more fine-grained (specific)\n2https://www.wikidata.org/wiki/Wikidata:\nList_of_properties/transitive_relation\nthan Canada in terms of relation P131 because\nits distance to Toronto is shorter than Canada in\nthe reasoning path. Based on this, for a transitive\nrelation, we collect reasoning paths with length\n≤5 for each subject and calculate the average dis-\ntance of each object to the subject. E.g., if there\nare two reasoning paths connecting the subject and\nobject, with lengths 2 and 3, the average distance\nis 2.5. In this way, we can construct pairs with\ncoarse-grained/fine-grained objects for each sub-\nject, e.g., (Toronto, Ontario) and (Toronto, Canada)\nfor Toronto in terms of relation P131 (or a triplet\ndenoted as (Toronto, Ontario, Canada)). The con-\nstructed pairs can be used to test the specificity\nwith prompt: “Toronto is located in [MASK].”\nWe also combine different relations to form tasks.\nFor instance, for relationP19, whose label is “place\nof birth”, we combine it with P131 and further\nform a mask token prediction task, such as “[X]\nwas born in [MASK].” An example reasoning path\ncontaining coarse-grained/fine-grained objects is\n(John G. Bennett, P19, London, P131, England),\ncorresponding to pairs (John G. Bennett, London)\nand (John G. Bennett, England).\nConsidering the representativeness and compre-\nhensiveness, we select 5 relations (Table 1) and\nrandomly sample up to 5,000 pairs for each rela-\ntion, with the difference of average distance of the\nobjects to the subject being greater than or equal\nto 1 (to filter out entity pairs whose specificity is\ndifficult to differentiate). Similar to Petroni et al.\n(2019), we only choose single-token objects as the\nprediction targets, since multi-token generation is\nstill an area that needs further exploration, and the\nmulti-token decoding process will introduce many\ntunable parameters that obscure the performance\n(Welleck et al., 2019; Jiang et al., 2020a). Statistics\nand examples of the resulting benchmark dataset\nare shown in Table 1.\n3.3 Metric\nIf a model tends to be more specific, it should have\nhigher confidence that the more specific answer is\ncorrect. For instance, given “Toronto is located in\n[MASK].”, the model should assign a higher proba-\nbility for Ontario than Toronto. Therefore, we can\nmeasure the specificity by calculating how much\ntimes the probability of the fine-grained answer is\nhigher than that of the coarse-grained answer:\npr = 1\n|Tr|\n∑\n(x,y1,y2)∈Tr\n1[c(y1|x, r) > c(y2|x, r)],\n718\nID Relation Number Prompt Answer 1 Answer 2\nP19 birthplace 5,000 John G. Bennett was born in [MASK]. London England\nP106 occupation 5,000 Jenny Burton is a [MASK] by profession. singer musician\nP131 location 5,000 Carey River is located in [MASK]. Victoria Australia\nP279 subclass-of 5,000 Tracking ship is a subclass of [MASK]. vessel vehicle\nP361 part-of 628 Hard palate is part of [MASK]. mouth head\nTable 1: Statistics and examples of the S-TEST benchmark, where we use the same templates in Petroni et al. (2019)\nto create prompts. Answer 1 is more specific than Answer 2.\nwhere Tr is the set of test examples for relation r.\ny1 is the fine-grained object and y2 is the coarse-\ngrained object. c(y|x, r) is the probability of the\nmodel with y as the prediction of the masked to-\nken, and x refers to the subject. pr ranges from\n0 to 1, and 0.5 means the model does not have\na preference in terms of specificity. The metric\nis similar to the one used in Marvin and Linzen\n(2018), which compares the probability of a pair\nof words for creating a grammatical sentence, e..g,\nThe author laughs (grammatical) vs The author\nlaugh (ungrammatical).\n4 Analysis\nIn this section, we first analyze the results of S-\nTEST and then identify and study two underlying\nfactors that affect the specificity of predictions pro-\nduced by pre-trained language models.\n4.1 Experimental Setup\nWe test on the following pre-trained case-sensitive\nlanguage models: GPT-2, BERT-Base, BERT-\nLarge, RoBERTa-Base, and RoBERTa-Large. For\na fair comparison, following (Petroni et al., 2019),\nwe use the intersection of the vocabularies of all\nthe models as the unified vocabulary for prediction\n(∼18k case-sensitive tokens). Since fine-grained\nanswers may be used less frequently in the corpus\n(e.g., Yate is much less frequent than England), we\nalso design a simple method by filling the masked\ntokens with less frequent answers (Freq).3\nTo verify the quality of the dataset, we randomly\nsampled 400 examples (80 for each relation) and\nasked human annotators to fill in the masked token\nwith both the coarse-grained and fine-grained an-\nswers provided (the order of answers in each pair\nis randomly shuffled). For example, we give anno-\ntators both query “Toronto is located in [MASK].”\nand answer pair (Ontario, Toronto) and ask them\nto select the more specific one. Humans can make\n3The frequency is calculated with Wikipedia dump https:\n//dumps.wikimedia.org/enwiki/.\njudgments based on their own knowledge or rele-\nvant information about the entities on the Web.\n4.2 Results of S-TEST\nTable 2 reports the results of specificity testing.\nWe observe that existing pre-trained language mod-\nels have only a slight preference for more specific\nanswers, where the probability that more specific\nanswers are preferred by them is around 60%. This\nis reasonable since the training of PLMs does not\nintroduce any constraint/bias in terms of specificity.\nIn Table 3, the Freq method performs quite\nwell on relation birthplace and location whose an-\nswers are both locations, which indicates low fre-\nquency may hinder outputting more specific con-\ncepts. However, for other relations, the results are\nclose to random guess. We also observe that the\nresults of “ human” is high, which demonstrates\nthat the quality of the dataset is high.\nTo investigate the correctness of the predic-\ntions as in Petroni et al. (2019), we also calculate\nAcc@10 (the value is 1 if the coarse/fine-grained\nanswer is ranked among the top 10 results, which\nare selected from ∼18k tokens, and 0 otherwise)\namong all relations in Table 4. We draw a conclu-\nsion similar to Petroni et al. (2019) that PLMs have\na good ability to recover factual knowledge.4\nAnother interesting finding is that for a single re-\nlation, the specificity of different models is highly\ncorrelated. For instance, for relation location, the\nspecificity measured by pr of all models is slightly\nlower than 50%, while for relation part-of, the\nspecificity of all models is around 60%. The aver-\nage pairwise Pearson correlation coefficient among\nall relations (calculated between different rows) is\n0.803. We think this is because these PLMs are\ntrained on large general corpora; therefore, their\nknowledge overlaps to a large extent, as is the pref-\nerence on the specificity of predictions.\n4The results can be further improved by using techniques\nsuch as in (Jiang et al., 2020b) or applying more advanced\nlanguage models such as GPT-3 (Brown et al., 2020) – not the\nfocus of this paper.\n719\nbirthplace occupation location subclass-of part-of Average\nGPT-2 59.72 57.28 48.25 57.98 60.86 56.82\nBERT-Base 60.68 70.46 49.09 67.64 67.41 63.06\nBERT-Large 56.52 71.76 42.36 77.25 66.77 62.93\nRoBERTa-Base 54.48 61.80 49.99 61.59 59.11 57.39\nRoBERTa-Large 42.16 71.44 43.28 80.63 59.27 59.36\nTable 2: Results of specificity testing with pr(%).\nbirthplace occupation location subclass-of part-of Average\nFreq 85.87 52.86 95.11 51.12 49.68 66.93\nHuman 98.75 92.50 100.00 96.25 97.75 97.05\nTable 3: Results of Freq and Human.\n4.3 Factors Affecting Specificity\nSome types of questions may be answered specif-\nically naturally. For instance, when discussing\nanyone’s occupation, people may be inclined to\nuse a more specific description; but for the lo-\ncation of a place, people may not be so. In\naddition, specific answers may be easier to re-\nlate to the entities in the query than the coarse-\ngrained ones since their connections may be\nmore close, e.g., similarity(Toronto, Ontario) >\nsimilarity(Toronto, Canada). In this case, the\nmodels should tend to select more specific answers.\nBased on the above analysis, the specificity of the\npredictions mainly depends on question types (e.g.,\nrelations) and entities in the query (e.g., subjects),\nwhich is also indicated by the metric for measur-\ning specificity, i.e., c(y|x, r). To investigate the\neffect of each component, we split the query, e.g.,\n“Toronto is located in [MASK].”, into two parts: the\nrelations, e.g., is located in, and the subjects, e.g,\nToronto, corresponding to naturalness and related-\nness respectively.\nNaturalness: For some questions, they may be\nanswered more specifically naturally than others by\nPLMs. For instance, for questions about the place\nof birth, if in the corpora, the birthplace is usually\ndescribed more specifically, e.g., ... was born in\nHonolulu, Hawaii , PLMs will also describe the\nbirthplace more specifically. This is intuitive since\nPLMs are trained on large corpora based on tasks\nlike masked language modeling; therefore, it will\nproduce more fine-grained predictions conditioned\nwith contexts that are more likely to associate with\nspecific answers.\nTo measure how natural a type of questions will\nbe answered more specifically by PLMs, we mask\nthe subject in each prompt, e.g., “[MASK] was\nborn in [MASK].”, and let PLMs predict the second\nmasked token. We get the probability of each token\nin the vocabulary, i.e., c(y|·, r), and use our metric\nand dataset to measure the naturalness, e.g., how\nnatural birthplace will be described more specifi-\ncally in general.\nRelatedness: Considering the following situation:\nthe model can predict that bothA and B are likely to\nbe the correct answers, and judgesA is more related\nto the the subject than B in general. Intuitively, it\nwill prefer answer A.\nTherefore, another factor that affects the speci-\nficity of predictions made by PLMs is relatedness,\ni.e., to what extent are the fine-grained objects\nmore related to the corresponding subjects than the\ncoarse-grained ones considered by PLMs. (More\ngenerally, this is the ability of PLMs to identify\nmore related entities).\nWe measure relatedness with phrase embeddings\nfrom PLMs. Following Yu and Ettinger (2020);\nWang et al. (2021), we use the mean-pooled repre-\nsentations over the final-layer outputs from PLMs\nas phrase embeddings, and calculate the cosine sim-\nilarities between the subject and the corresponding\nobjects. If the cosine similarity between the sub-\nject and the fine-grained object is higher than that\nbetween the subject and the coarse-grained object,\nwe think PLMs consider the fine-grained one is\nmore related to the subject. According to this, we\ncan use our metric and dataset to measure the re-\nlatedness, with confidence, i.e., c(y|x, ·), based on\ncosine similarity between x and y.\nFindings. In Table 5, we report the naturalness\nand relatedness with pr as the metric. We find that,\n1) the highest average naturalness and relatedness\nare achieved by BERT-Large and BERT-Base, re-\nspectively, corresponding to the highest average\n720\nGPT-2 BERT-Base BERT-Large RoBERTa-Base RoBERTa-Large\nAcc@10 23.87 42.65 46.80 30.79 31.81\nTable 4: The correctness of the predictions measured with Acc@10 (%).\nbirthplace occupation location subclass-of part-of Average\nGPT-2 Naturalness 46.42 50.86 10.94 60.06 51.12 43.88\nRelatedness 68.51 78.50 82.84 40.00 50.16 64.00\nBERT-Base Naturalness 64.81 75.04 4.99 47.96 50.80 48.72\nRelatedness 74.89 51.96 76.43 71.67 58.79 66.75\nBERT-Large Naturalness 66.35 79.22 10.03 48.92 47.60 50.42\nRelatedness 54.46 49.16 56.22 72.96 65.50 59.66\nRoBERTa-Base Naturalness 44.80 61.12 23.27 42.06 36.90 41.63\nRelatedness 68.73 58.50 65.73 39.51 56.87 57.87\nRoBERTa-Large Naturalness 31.37 66.24 3.67 43.64 41.69 37.32\nRelatedness 47.82 41.32 34.89 55.17 64.22 48.68\nTable 5: Relatedness and naturalness measured with pr(%).\nspecificity; 2) in many cases, naturalness is lower\nthan 0.5, which indicates that, without the subjects\nprovided, PLMs are more likely to provide coarse-\ngrained answers, we think this is because a single\ncoarse-grained entity encompasses the probability\nmass of many fine-grained entities; 3) relatedness\nis usually higher than0.5, which means PLMs have\na certain ability to distinguish fine-grained/coarse-\ngrained answers based on semantic similarities be-\ntween entities. But the ability is weak since the\naverage scores are just around 60%.\n5 Can Language Models Be MORE\nSpecific?\nFrom the previous sections, we observe that exist-\ning pre-trained language models do not have much\npreference for more specific answers in a vanilla\nsetting. We also observe that PLMs achieve natu-\nralness lower than 0.5, i.e., naturally, PLMs tend\nto fill in coarse-grained answers with respect to cer-\ntain types of questions, and relatedness around 0.6,\ni.e., PLMs only have a weak ability to distinguish\nmore related entities. Naturalness depends on both\nthe parameters of PLMs and prompts while relat-\nedness only depends on the parameters of PLMs.\nSince it is expensive to change the parameters of\nPLMs (both time and space), to improve the speci-\nficity, we focus on improving the naturalness by\nmodifying the prompts.\nIntuitively, to get more specific answers, a practi-\ncal approach is to ask more specific questions. For\ninstance, to know where Toronto is located more\nspecifically, we may change the prompt “Toronto is\nlocated in [MASK].” to “Toronto is located in the\nprovince of [MASK].” However, to achieve this,\nhumans are required to have additional knowledge,\ne.g., Toronto is a city, and in Canada, the adminis-\ntrative unit larger than city is province rather than\nstate. Besides, designing such manually crafted\nprompts can also be time-consuming and laborious\nif there are a large number of queries. Furthermore,\nsome questions may be difficult to ask more specif-\nically. For instance, for question “Hard palate is\npart of [MASK].”, it is not easy to come up with a\nmore specific query.\nBased on the above considerations, we propose\ntwo novel and simple techniques to improve the\nspecificity of the predictions. The proposed meth-\nods can apply to different models on various types\nof queries while no additional training is required.\n5.1 Few-Shot Prompting\nWe refer to using prompts in Table 1 to extract\nanswers as Vanilla Prompting(e.g., we let PLMs\npredict the masked token in “John G. Bennett was\nborn in [MASK].”). Vanilla Prompting cannot elicit\nspecific answers since the designed prompts can not\ntell the models the preference regarding specificity;\ntherefore, the models are not aware of whether a\nmore specific answer is preferred.\nBased on the above analysis, we need to give the\nmodel some “hints” in terms of specificity, which\ncan be achieved by providing some demonstrations.\nFor instance, to predict where Toronto is located,\nif we provide some examples with coarse-grained\nanswers using prompt “Melbourne is located in\nAustralia, Guangzhou is located in China, Toronto\nis located in [MASK].”, the model may know by\nanalogy that we prefer a coarse-grained answer,\nwhich is Canada (a country). In contrast, if we pro-\n721\nRelation Prompt\nbirthplace John G. Bennett was born in [MASK], which is located in [MASK].\noccupation Jenny Burton is a [MASK] by profession, which belongs to [MASK].\nlocation Carey River is located in [MASK], which is located in [MASK].\nsubclass-of Tracking ship is a subclass of [MASK], which is a subclass of [MASK].\npart-of Hard palate is part of [MASK], which is part of [MASK].\nTable 6: Example prompts for Cascade Prompting.\nbirthplace occupation location subclass-of part-of Average\nGPT-2 (VP) 59.72 57.28 48.25 57.98 60.86 56.82\nGPT-2 (FP) 81.01 71.66 50.33 64.15 57.67 64.96\nGPT-2 (CP)* 59.72 57.28 48.25 57.98 60.86 56.82\nBERT-Base (VP) 60.68 70.46 49.09 67.64 67.41 63.06\nBERT-Base (FP) 67.85 70.54 50.11 69.11 53.83 62.29\nBERT-Base (CP) 59.68 70.54 55.06 67.42 69.49 64.44\nBERT-Large (VP) 56.52 71.76 42.36 77.25 66.77 62.93\nBERT-Large (FP) 66.17 64.70 50.37 65.44 52.24 59.78\nBERT-Large (CP) 82.25 70.02 53.55 77.67 71.88 71.07\nRoBERTa-Base (VP) 54.48 61.80 49.99 61.59 59.11 57.39\nRoBERTa-Base (FP) 64.85 72.38 35.85 63.01 51.11 57.44\nRoBERTa-Base (CP) 63.09 64.54 54.56 61.81 62.78 61.36\nRoBERTa-Large (VP) 42.16 71.44 43.28 80.63 59.27 59.36\nRoBERTa-Large (FP) 70.51 71.94 42.26 73.70 62.94 64.27\nRoBERTa-Large (CP) 89.00 74.02 66.09 79.87 65.18 74.83\nTable 7: Results of specificity testing with different prompts. The best results in each group are bold. VP: Vanilla\nPrompting, FP: Few-shot Prompting, CP: Cascade Prompting. *We do not rescore all suffixes for GPT-2 (CP).\nvide some fine-grained answers with “Melbourne is\nlocated in Victoria, Guangzhou is located in Guang-\ndong, Toronto is located in [MASK].”, the model\nmay realize through analogy that we prefer a fine-\ngrained answer here, which is Ontario (a province).\nWe refer to the method described above as Few-\nshot Prompting, which supposes to bias the predic-\ntion to be more specific by providing some exam-\nples with fine-grained answers. The technique here\nis similar to the few-shot setting in GPT-3 (Brown\net al., 2020) and (Adolphs et al., 2021), where sev-\neral demonstrations are given to the model as con-\ndition to help the model make the prediction.\n5.2 Cascade Prompting\nTo make the answer more specific, we can also\nutilize the relationship between coarse-grained and\nfine-grained objects. For instance, in Table 1,track-\ning ship is a subclass of vessel, while vessel is also\na subclass of vehicle. To combine the three entities,\nwe can write: Tracking ship is a subclass of vessel,\nwhich is a subclass of vehicle. By masking the ob-\njects, we get prompt: “Tracking ship is a subclass\nof [MASK], which is a subclass of [MASK].” Intu-\nitively, the first masked token will be more likely to\nbe filled by vessel, while the second masked token\ntends to be vehicle. Another example in Table 1\nis to predict the birthplace, we can create prompt\n“John G. Bennett was born in [MASK], which is\nlocated in [MASK].” to bias the prediction of the\nfirst masked token to be more specific.\nWe refer to the above method as Cascade\nPrompting, which aims to improve the specificity\nby adding “which clauses” as constraints accord-\ning to the relationship between coarse-grained and\nfine-grained answers. The “which clauses” here\ncan be considered as suffixes and the prediction of\nthe first masked token is returned as the answer.\n6 Experiments\nIn this section, we conduct experiments with the\nprompt-based methods proposed in Section 5.\n6.1 Experimental Setup\nWe follow the setup in Section 4.1. For Few-shot\nPrompting, we set K, i.e., the number of demon-\nstrations, as 10. For Cascade Prompting, we apply\nthe prompts in Table 6, which are constructed au-\ntomatically based on the prompts for the transitive\nrelations, e.g., “... is located in [MASK].” ⇒“...,\nwhich is located in [MASK].”\n6.2 Results\nTable 7 summarizes the results of specificity testing\nwith different prompting methods. From the re-\nsults, we observe that Cascade Prompting achieves\nthe best performance in most cases. In addition,\n722\nGPT-2 BERT-Base BERT-Large RoBERTa-Base RoBERTa-Large\nAcc@10 w/ FP + 10.62 + 0.05 + 2.74 + 8.09 + 16.45\nAcc@10 w/ CP 0.00 - 0.07 - 4.28 + 2.06 + 0.77\nTable 8: Change in correctness of the predictions compared to Vanilla Prompting(%) on fine-grained answers. w/\nFP & CP means Few-shot & Cascade Prompting is used to create prompts.\nGPT-2 BERT-Base BERT-Large RoBERTa-Base RoBERTa-Large\nNaturalness w/ VP 43.88 48.72 50.42 41.63 37.32\nSpecificity w/ VP 56.82 63.06 62.93 57.39 59.36\nNaturalness w/ FP 52.02 51.05 47.36 49.11 49.96\nSpecificity w/ FP 64.96 62.29 59.78 57.44 64.27\nNaturalness w/ CP 43.88 51.44 56.54 45.81 57.69\nSpecificity w/ CP 56.82 64.44 71.07 61.36 74.83\nTable 9: Average naturalness measured with pr(%) with different prompts, with corresponding average specificity\nas reference. w/ VP means Vanilla Prompting is used to create prompts. For each model, the best naturalness is\nunderlined and the best specificity is bold.\nthe performance improvement for BERT-Large and\nRoBERTa-Large with Cascade Prompting is quite\nsignificant. We think this is because the large mod-\nels can understand which clauses better than the\nbase models.\nWe also observe that Few-shot Prompting does\nnot always improve the specificity for bidirectional\nlanguage models. However, for GPT-2, which is a\nunidirectional language model, Few-shot Prompt-\ning achieves a significant performance improve-\nment, while the results of Cascade Prompting are\nthe same as those of Vanilla Prompting.\nTo observe the impact of the two methods on\ncorrectness, we report the change in correctness in\nTable 8. We observe that the correctness of Cas-\ncade Prompting is close to that of Vanilla Prompt-\ning, while the correctness of Few-shot Prompting\nimproves significantly. This is because Cascade\nPrompting is in a zero-shot setting, while in Few-\nshot Prompting, demonstrations can provide some\nsupervision to help the model make predictions.\nWe also measure naturalness of different mod-\nels with different prompting methods. From Table\n9, we find that, for each model, the best prompt-\ning method is usually associated with the highest\nnaturalness: Cascade Prompting improves the nat-\nuralness for bidirectional language models signifi-\ncantly, which corresponds to better performance on\nspecificity; while for GPT-2, the naturalness using\nFew-shot Prompting is the highest, corresponding\nto the highest specificity.\n7 Discussion\nSpecificity Testing in More General Scenarios:\nIn this work, we test the specificity of PLMs on\nseveral relations with manually crafted prompts,\nwith test data created automatically based on the\nproperty of transitive relations. For future work, we\nmay test the specificity in more general scenarios.\nFor instance, for numerical knowledge (Lin et al.,\n2020), we can test how specifically PLMs describe\nthe numbers, e.g., Obama was born in 1961 vs\nObama was born in 1960s, A car has four wheels\nvs A car has several wheels. In addition, we may\ntest on multi-token answers (Jiang et al., 2020a),\nand measure the specificity of sentences generated\nby PLMs (Louis and Nenkova, 2011; Ko et al.,\n2019; Adiwardana et al., 2020; Thoppilan et al.,\n2022), e.g., This is a very good paper. I really\nlike it. vs This paper conducts a very novel and\ninteresting study, which provides a new insight for\nfuture work on language models.\nFurther Improvement of Specificity: In this pa-\nper, we propose Few-shot Promptingand Cascade\nPrompting to improve the specificity of PLMs with-\nout any additional training. Future work may im-\nprove the specificity by including prompt-based\nfine-tuning (Shin et al., 2020; Gao et al., 2021).\nThe observation also encourages future work to\ntake into account the specificity, e.g., adding con-\nstraints regarding specificity, in the pre-training\nprocess. It is also interesting to design methods to\ncontrol the degree of specificity for different usage\nscenarios (Huang et al., 2021).\n8 Conclusion\nIn this paper, we build a benchmark to measure the\nspecificity of predictions produced by pre-trained\nlanguage models. To achieve this, we propose a\nnovel approach to construct test data for specificity\n723\ntesting automatically. From our evaluations, we\nshow that existing PLMs have only a slight prefer-\nence for more specific answers. We also propose\ntwo prompt-based methods, i.e., Few-shot Prompt-\ning and Cascade Prompting, to improve the speci-\nficity of the predictions. Extensive experiments\nand in-depth analyses demonstrate the effective-\nness of the proposed methods. We hope this work\ncan encourage future research in this direction and\ngive some insights to improve downstream tasks\nsuch as question answering, information extraction,\nand text generation: 1) to make the answers, the\nextracted information, or the generated sentences\nmore specific; 2) to control the degree of specificity\nfor different usage scenarios.\nLimitations\nThis work presents some limitations. Firstly, our\nfocus is confined to evaluating the specificity of\npredictions made by pre-trained language models\nfor entity relations. As noted in Section 7, speci-\nficity can potentially be tested in a broader range\nof scenarios. Despite this restriction, we consider\nthis work as an initial attempt to highlight the con-\ncept of language model specificity. We believe it\nwill stimulate further research into this crucial, yet\nunder-explored, area.\nA second limitation is the scale of the models\nevaluated in this work. Given the swift evolution of\nlarge language models concurrent with the drafting\nof this paper, the models we examined are compar-\natively small. As pointed out in the work of Zheng\net al. (2023), large language models may fail to\nanswer a problem at the appropriate level of speci-\nficity. We thus encourage future investigations to\ndelve into the specificity of these rapidly evolving,\nlarger language models.\nAcknowledgements\nWe thank the reviewers for their constructive feed-\nback. This material is based upon work supported\nby the National Science Foundation IIS 16-19302\nand IIS 16-33755, Zhejiang University ZJU Re-\nsearch 083650, IBM-Illinois Center for Cognitive\nComputing Systems Research (C3SR) and IBM-\nIllinois Discovery Accelerator Institute (IIDAI),\ngift grants from eBay and Microsoft Azure, UIUC\nOVCR CCIL Planning Grant 434S34, UIUC CSBS\nSmall Grant 434C8U, and UIUC New Frontiers\nInitiative. Any opinions, findings, and conclusions\nor recommendations expressed in this publication\nare those of the author(s) and do not necessarily\nreflect the views of the funding agencies.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. ArXiv preprint, abs/2001.09977.\nLeonard Adolphs, Shehzaad Dhuliawala, and Thomas\nHofmann. 2021. How to query language models?\narXiv preprint arXiv:2108.01928.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 7456–7463.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Association for Computational Linguis-\ntics (ACL).\nHan Huang, Tomoyuki Kajiwara, and Yuki Arase. 2021.\nDefinition modelling for appropriate specificity. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n2499–2509.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-factr:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\n724\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sab-\nharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. 2020. Unifiedqa: Crossing format bound-\naries with a single qa system. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, pages 1896–1907.\nWei-Jen Ko, Greg Durrett, and Junyi Jessy Li. 2019.\nDomain agnostic real-valued specificity prediction.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 33, pages 6610–6617.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6862–6868.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAnnie Louis and Ani Nenkova. 2011. Automatic iden-\ntification of general and specific sentences by lever-\naging discourse annotations. In Proceedings of 5th\ninternational joint conference on natural language\nprocessing, pages 605–613.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1192–1202.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Eliciting\nknowledge from language models using automati-\ncally generated prompts. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4222–4235.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs. arXiv\npreprint arXiv:2010.11967.\nShufan Wang, Laure Thompson, and Mohit Iyyer. 2021.\nPhrase-bert: Improved phrase embeddings from bert\nwith an application to corpus exploration.\nSean Welleck, Kianté Brantley, Hal Daumé Iii, and\nKyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. In International Conference on Ma-\nchine Learning, pages 6716–6726. PMLR.\nLang Yu and Allyson Ettinger. 2020. Assessing phrasal\nrepresentation and composition in transformers. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4896–4907.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers? ArXiv preprint, abs/2304.10513.\n725\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLeft blank.\n□\u0017 A2. Did you discuss any potential risks of your work?\nno/low risk\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThose models and data are commonly used\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsame as above\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nC □\u0013 Did you run computational experiments?\nSection 4, 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4, 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n726\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4, 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4, 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4, 5\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4, 5\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 4, 5\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n727",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7548220157623291
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6174085140228271
    },
    {
      "name": "Language model",
      "score": 0.5894514322280884
    },
    {
      "name": "Machine learning",
      "score": 0.5638635158538818
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5419027209281921
    },
    {
      "name": "Preference",
      "score": 0.4931570887565613
    },
    {
      "name": "Natural language processing",
      "score": 0.4366675615310669
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.4317634105682373
    },
    {
      "name": "Test (biology)",
      "score": 0.4118034839630127
    },
    {
      "name": "Data mining",
      "score": 0.20345857739448547
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210127875",
      "name": "Nvidia (United States)",
      "country": "US"
    }
  ]
}