{
  "title": "Dynamic Entity Representations in Neural Language Models",
  "url": "https://openalex.org/W2963925965",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2153992776",
      "name": "Yangfeng Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131562809",
      "name": "Chen-hao Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2014118654",
      "name": "Sebastian Martschat",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2133417374",
      "name": "Yejin Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2053218206",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2593847145",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1573488949",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2131340601",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2252247041",
    "https://openalex.org/W2098345921",
    "https://openalex.org/W2104619957",
    "https://openalex.org/W2250668331",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2962769558",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2341790067",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1789782362",
    "https://openalex.org/W2963167649",
    "https://openalex.org/W2951976932",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2577255746",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W2964036636",
    "https://openalex.org/W2963077125",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2180160918",
    "https://openalex.org/W179314280",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W2963084471",
    "https://openalex.org/W1965693266",
    "https://openalex.org/W3018009202",
    "https://openalex.org/W2963695529"
  ],
  "abstract": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, EntityNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.",
  "full_text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1830–1839\nCopenhagen, Denmark, September 7–11, 2017.c⃝2017 Association for Computational Linguistics\nDynamic Entity Representations in Neural Language Models\nYangfeng Ji∗ Chenhao Tan∗ Sebastian Martschat† Yejin Choi∗ Noah A. Smith∗\n∗Paul G. Allen School of Computer Science & Engineering, University of Washington\n†Department of Computational Linguistics, Heidelberg University\n{yangfeng,chenhao,yejin,nasmith}@cs.washington.edu\nmartschat@cl.uni-heidelberg.de\nAbstract\nUnderstanding a long document requires\ntracking how entities are introduced and\nevolve over time. We present a new type of\nlanguage model, E NTITY NLM, that can\nexplicitly model entities, dynamically up-\ndate their representations, and contextu-\nally generate their mentions. Our model is\ngenerative and ﬂexible; it can model an ar-\nbitrary number of entities in context while\ngenerating each entity mention at an arbi-\ntrary length. In addition, it can be used\nfor several different tasks such as language\nmodeling, coreference resolution, and en-\ntity prediction. Experimental results with\nall these tasks demonstrate that our model\nconsistently outperforms strong baselines\nand prior work.\n1 Introduction\nUnderstanding a narrative requires keeping track\nof its participants over a long-term context. As a\nstory unfolds, the information a reader associates\nwith each character in a story increases, and ex-\npectations about what will happen next change ac-\ncordingly. At present, models of natural language\ndo not explicitly track entities; indeed, in today’s\nlanguage models, entities are no more than the\nwords used to mention them.\nIn this paper, we endow a generative language\nmodel with the ability to build up a dynamic rep-\nresentation of each entity mentioned in the text.\nOur language model deﬁnes a probability distribu-\ntion over the whole text, with a distinct generative\nstory for entity mentions. It explicitly groups those\nmentions that corefer and associates with each en-\ntity a continuous representation that is updated by\nevery contextualized mention of the entity, and\nthat in turn affects the text that follows.\n[John]1 wanted to go to [ the coffee shop]2 in\n[downtown Copenhagen]3. [He]1 was told that\n[it]2 sold [the best beans]4.\nFigure 1: E NTITY NLM explicitly tracks entities\nin a text, including coreferring relationships be-\ntween entities like [ John]1 and [He]1. As a lan-\nguage model, it is designed to predict that a coref-\nerent of [the coffee shop]2 is likely to follow “told\nthat,” that the referring expression will be “it”, and\nthat “sold the best beans” is likely to come next, by\nusing entity information encoded in the dynamic\ndistributed representation.\nOur method builds on recent advances in repre-\nsentation learning, creating local probability dis-\ntributions from neural networks. It can be un-\nderstood as a recurrent neural network language\nmodel, augmented with random variables for en-\ntity mentions that capture coreference, and with\ndynamic representations of entities. We estimate\nthe model’s parameters from data that is annotated\nwith entity mentions and coreference.\nBecause our model is generative, it can be\nqueried in different ways. Marginalizing every-\nthing except the words, it can play the role of a lan-\nguage model. In §5.1, we ﬁnd that it outperforms\nboth a strong n-gram language model and a strong\nrecurrent neural network language model on the\nEnglish test set of the CoNLL 2012 shared task\non coreference evaluation (Pradhan et al., 2012).\nThe model can also identify entity mentions and\ncoreference relationships among them. In §5.2,\nwe show that it can easily be used to add a per-\nformance boost to a strong coreference resolution\nsystem, by reranking a list ofk-best candidate out-\nputs. On the CoNLL 2012 shared task test set, the\nreranked outputs are signiﬁcantly better than the\noriginal top choices from the same system. Fi-\n1830\nnally, the model can perform entity cloze tasks.\nAs presented in §5.3, it achieves state-of-the-art\nperformance on the InScript corpus (Modi et al.,\n2017).\n2 Model\nA language model deﬁnes a distribution over se-\nquences of word tokens; let Xt denote the random\nvariable for the tth word in the sequence, xt de-\nnote the value of Xt and xt the distributed repre-\nsentation (embedding) of this word. Our starting\npoint for language modeling is a recurrent neural\nnetwork (Mikolov et al., 2010), which deﬁnes\np(Xt |history) = softmax (Whht−1 + b) (1)\nht−1 = LSTM (ht−2,xt−1) (2)\nwhere Wh and b are parameters of the model\n(along with word embeddings xt), LSTM is the\nwidely used recurrent function known as “long\nshort-term memory” (Hochreiter and Schmidhu-\nber, 1997), andhtis a LSTM hidden state encoding\nthe history of the sequence up to the tth word.\nGreat success has been reported for this model\n(Zaremba et al., 2015), which posits nothing ex-\nplicitly about the words appearing in the text se-\nquence. Its generative story is simple: the value\nof each Xt is randomly chosen conditioned on the\nvector ht−1 encoding its history.\n2.1 Additional random variables and\nrepresentations for entities\nTo introduce our model, we associate with each\nword an additional set of random variables. At po-\nsition t,\n•Rt is a binary random variable that indi-\ncates whether xt belongs to an entity men-\ntion (Rt = 1) or not ( Rt = 0). Though not\nexplored here, this is easily generalized to a\ncategorial variable for the type of the entity\n(e.g., person, organization, etc.).\n•Lt ∈{1,...,ℓ max }is a categorical random\nvariable if Rt = 1, which indicates the num-\nber of remaining words in this mention, in-\ncluding the current word (i.e., Lt = 1 for\nthe last word in any mention). ℓmax is a\npredeﬁned maximum length ﬁxed to be 25,\nwhich is an empirical value derived from the\ntraining corpora used in the experiments. If\nRt = 0, then Lt = 1. We denote the value of\nLt by ℓt.\n•Et ∈Et is the index of the entity referred to,\nif Rt = 1. The set Et consists of {1,..., 1 +\nmaxt′<tet′ }, i.e., the indices of all previously\nmentioned entities plus an additional value\nfor a new entity. Thus Et starts as {1}and\ngrows monotonically with t, allowing for an\narbitrary number of entities to be mentioned.\nWe denote the value of Et by et. If Rt = 0,\nthen Et is ﬁxed to a special value ø.\nThe values of these random variables for our run-\nning example are shown in Figure 2.\nIn addition to using symbolic variables to en-\ncode mentions and coreference relationships, we\nmaintain a vector representation of each entity that\nevolves over time. For the ith entity, let ei,t be\nits representation at time t. These vectors are\ndifferent from word vectors ( xt), in that they are\nnot parameters of the model. They are similar to\nhistory representations ( ht), in that they are de-\nrived through parameterized functions of the ran-\ndom variables’ values, which we will describe in\nthe next subsections.\n2.2 Generative story\nThe generative story for the word (and other\nvariables) at timestep t is as follows; forward-\nreferenced equations are in the detailed discussion\nthat follows.\n1. If ℓt−1 = 1 (i.e., xt is not continuing an\nalready-started entity mention):\n•Choose rt (Equation 3).\n•If rt = 0, set ℓt = 1 and et = ø; then go\nto step 3. Otherwise:\n– If there is no embedding for the\nnew candidate entity with index\n1 + maxt′<tet′ , create one follow-\ning §2.4.\n– Select the entity et from {1,..., 1 +\nmaxt′<tet′ }(Equation 4).\n– Set ecurrent = eet,t−1, which is\nthe entity embedding of et before\ntimestep t.\n– Select the length of the mention, ℓt\n(Equation 5).\n2. Otherwise,\n•Set ℓt = ℓt−1 −1, rt = rt−1, et = et−1.\n3. Sample xt from the word distribution given\nthe LSTM hidden state ht−1 and the current\n1831\nX1:12: John wanted to go to the coffee shop in downtown Copenhagen .\nR1:12: 1 0 0 0 0 1 1 1 0 1 1 0\nE1:12: 1 ø ø ø ø 2 2 2 ø 3 3 ø\nL1:12: 1 1 1 1 1 3 2 1 1 2 1 1\nX13:22: He was told that it sold the best beans .\nR13:22: 1 0 0 0 1 0 1 1 1 .\nE13:22: 1 ø ø ø 2 ø 4 4 4 ø\nL13:22: 1 1 1 1 1 1 3 2 1 0\nFigure 2: The random variable values in ENTITY NLM for the running example in Figure 1.\n(or most recent) entity embedding ecurrent\n(Equation 6). (If rt = 0 , then ecurrent still\nrepresents the most recently mentioned en-\ntity.)\n4. Advance the RNN, i.e., feed it the word vec-\ntor xt to compute ht (Equation 2).\n5. If rt = 1, update eet,t using eet,t−1 and ht,\nthen set ecurrent = eet,t. Details of the entity\nupdating are given in §2.4.\n6. For every entity eι ∈ Et \\{et}, set eι,t =\neι,t−1 (i.e., no changes to other entities’ rep-\nresentations).\nNote that at any given time step t, ecurrent will al-\nways contain the most recent vector representation\nof the most recently mentioned entity.\nA generative model with a similar hierarchi-\ncal structure was used by Haghighi and Klein\n(2010) for coreference resolution. Our approach\ndiffers in two important ways. First, our model\ndeﬁnes a joint distribution over all of the text, not\njust the entity mentions. Second, we use repre-\nsentation learning rather than Bayesian nonpara-\nmetrics, allowing natural integration with the lan-\nguage model.\n2.3 Probability distributions\nThe generative story above referenced several\nparametric distributions deﬁned based on vector\nrepresentations of histories and entities. These are\ndeﬁned as follows.\nFor r∈{0,1},\np(Rt = r|ht−1) ∝exp(h⊤\nt−1Wrr), (3)\nwhere r is the parameterized embedding associ-\nated with r, which paves the way for exploring en-\ntity type representations in future work; Wr is a\nparameter matrix for the bilinear score for ht−1\nand r.\nTo give the possibility of predicting a new en-\ntity, we need an entity embedding beforehand with\nindex (1 + maxt′<tet′ ), which is randomly sam-\npled from Equation 7. Then, for every e ∈\n{1,..., 1 + maxt′<tet′ }:\np(Et = e|Rt = 1,ht−1)\n∝exp(h⊤\nt−1Wentity ee,t−1 + w⊤\ndist f(e)),\n(4)\nwhere ee,t−1 is the embedding of entity eat time\nstep t−1 and Wentity is the weight matrix for pre-\ndicting entities using their continuous representa-\ntions. The score above is normalized over values\n{1,..., 1 + maxt′<tet′ }. f(e) represents a vec-\ntor of distance features associated with eand the\nmentions of the existing entities. Hence two in-\nformation sources are used to predict the next en-\ntity: (i) contextual information ht−1, and (ii) dis-\ntance features f(e) from the current mention to the\nclosest mention from each previously mentioned\nentity. f(e) = 0 if e is a new entity. This term\ncan also be extended to include other surface-form\nfeatures for coreference resolution (Martschat and\nStrube, 2015; Clark and Manning, 2016b).\nFor the chosen entity et from Equation 4, the\ndistribution over its mention length is drawn ac-\ncording to\np(Lt = ℓ|ht−1,eet,t−1)\n∝exp(W⊤\nlength,ℓ[ht−1; eet,t−1]),\n(5)\nwhere eet,t−1 is the most recent embedding of the\nentity et, not updated with ht. The intuition is that\neet,t−1 will help contextual information ht−1 to\nselect the residual length of entity et. Wlength\nis the weight matrix for length prediction, with\nℓmax = 25 rows.\nFinally, the probability of a word xas the next\ntoken is jointly modeled by ht−1 and the vector\nrepresentation of the most recently mentioned en-\ntity ecurrent:\np(Xt = x|ht−1,ecurrent)\n∝CFSM (ht−1 + Weecurrent), (6)\n1832\nwhere We is a transformation matrix to adjust the\ndimensionality of ecurrent. CFSM is a class factor-\nized softmax function (Goodman, 2001; Baltescu\nand Blunsom, 2015). It uses a two-step prediction\nwith predeﬁned word classes instead of direct pre-\ndiction on the whole vocabulary, and reduces the\ntime complexity to the log of vocabulary size.\n2.4 Dynamic entity representations\nBefore predicting the entity at step t, we need an\nembedding for the new candidate entity with index\ne′= 1 + maxt′<tet′ if it does not exist. The new\nembedding is generated randomly, according to a\nnormal distribution, then projected onto the unit\nball:\nu ∼N(r1,σ2I);\nee′,t−1 = u\n∥u∥2\n, (7)\nwhere σ = 0 .01. The time step t−1 in ee′,t−1\nmeans the current embedding contains no infor-\nmation from step t, although it will be updated\nonce we have ht and if Et = e′. r1 is the pa-\nrameterized embedding for Rt = 1, which will be\njointly optimized with other parameters and is ex-\npected to encode some generic information about\nentities. All the initial entity embeddings are cen-\ntered on the mean r1, which is used in Equation 3\nto determine whether the next token belongs to an\nentity mention. Another choice would be to ini-\ntialize with a zero vector, although our preliminary\nexperiments showed this did not work as well as\nrandom initialization in Equation 7.\nAssume Rt = 1 and Et = et, which means xt\nis part of a mention of entity et. Then, we need\nto update eet,t−1 based on the new information we\nhave from ht. The new embedding eet,t is a con-\nvex combination of the old embedding ( eet,t−1)\nand current LSTM hidden state ( ht) with the in-\nterpolation (δt) determined dynamically based on\na bilinear function:\nδt = σ(h⊤\nt Wδeet,t−1);\nu = δteet,t−1 + (1 −δt)ht;\neet,t = u\n∥u∥2\n, (8)\nThis updating scheme will be used to update et in\neach of all the following ℓt steps. The projection\nin the last step keeps the magnitude of the entity\nembedding ﬁxed, avoiding numeric overﬂow. A\nsimilar updating scheme has been used by Henaff\net al. (2016) for the “memory blocks” in their re-\ncurrent entity network models. The difference is\nthat their model updates all memory blocks in each\ntime step. Instead, our updating scheme in Equa-\ntion 8 only applies to the selected entity et at time\nstep t.\n2.5 Training objective\nThe model is trained to maximize the log of the\njoint probability of R,E,L, and X:\nℓ(θ) = log P(R,E,L,X; θ)\n=\n∑\nt\nlog P(Rt,Et,Lt,Xt; θ), (9)\nwhere θ is the collection of all the parameters\nin this model. Based on the formulation in §2.3,\nEquation 9 can be decomposed as the sum of con-\nditional log-probabilities of each random variable\nat each time step.\nThis objective requires the training data anno-\ntated as in Figure 2. We do not assume that these\nvariables are observed at test time.\n3 Implementation Details\nOur model is implemented with DyNet (Neu-\nbig et al., 2017) and available at https://\ngithub.com/jiyfeng/entitynlm. We\nuse AdaGrad (Duchi et al., 2011) with learning\nrate λ = 0.1 and ADAM (Kingma and Ba, 2014)\nwith default learning rate λ= 0.001 as the candi-\ndate optimizers of our model. For all the parame-\nters, we use the initialization tricks recommended\nby Glorot and Bengio (2010). To avoid overﬁtting,\nwe also employ dropout (Srivastava et al., 2014)\nwith the candidate rates as {0.2,0.5}.\nIn addition, there are two tunable hyperpa-\nrameters of E NTITY NLM: the size of word em-\nbeddings and the dimension of LSTM hidden\nstates. For both of them, we consider the values\n{32,48,64,128,256}. We also experiment with\nthe option to either use the pretrained GloVe word\nembeddings (Pennington et al., 2014) or randomly\ninitialized word embeddings (then updated during\ntraining). For all experiments, the best conﬁgura-\ntion of hyperparameters and optimizers is selected\nbased on the objective value on the development\ndata.\n4 Evaluation Tasks and Datasets\nWe evaluate our model in diverse use scenarios:\n(i) language modeling, (ii) coreference resolution,\n1833\nand (iii) entity prediction. The evaluation on lan-\nguage modeling shows how the internal entity rep-\nresentation, when marginalized out, can improve\nthe perplexity of language models. The evaluation\non coreference resolution experiment shows how\nour new language model can improve a compet-\nitive coreference resolution system. Finally, we\nemploy an entity cloze task to demonstrate the\ngenerative performance of our model in predicting\nthe next entity given the previous context.\nWe use two datasets for the three evaluation\ntasks. For language modeling and coreference\nresolution, we use the English benchmark data\nfrom the CoNLL 2012 shared task on corefer-\nence resolution (Pradhan et al., 2012). We employ\nthe standard training/development/test split, which\nincludes 2,802/343/348 documents with roughly\n1M/150K/150K tokens, respectively. We follow\nthe coreference annotation in the CoNLL dataset\nto extract entities and ignore the singleton men-\ntions in texts.\nFor entity prediction, we employ the InScript\ncorpus created by Modi et al. (2017). It consists of\n10 scenarios, including grocery shopping, taking\na ﬂight, etc. It includes 910 crowdsourced simple\nnarrative texts in total and 18 stories were ignored\ndue to labeling problems (Modi et al., 2017). On\naverage, each story has 12.4 sentences, 24.9 en-\ntities and 217.2 tokens. Each entity mention is\nlabeled with its entity index. We use the same\ntraining/development/test split as in (Modi et al.,\n2017), which includes 619, 91, 182 texts, respec-\ntively.\nData preprocessing\nFor the CoNLL dataset, we lowercase all tokens\nand remove any token that only contains a punctu-\nation symbol unless it is in an entity mention. We\nalso replace numbers in the documents with the\nspecial token NUM and low-frequency word types\nwith UNK . The vocabulary size of the CoNLL data\nafter preprocessing is 10K. For entity mention ex-\ntraction, in the CoNLL dataset, one entity men-\ntion could be embedded in another. For embed-\nded mentions, only the enclosing entity mention\nis kept. We use the same preprocessed data for\nboth language modeling and coreference resolu-\ntion evaluation.\nFor the InScript corpus, we apply similar data\npreprocessing to lowercase all tokens, and we re-\nplace low-frequency word types with UNK . The\nvocabulary size after preprocessing is 1K.\n5 Experiments\nIn this section, we present the experimental results\non the three evaluation tasks.\n5.1 Language modeling\nTask description. The goal of language model-\ning is to compute the marginal probability:\nP(X) =\n∑\nR,E,L\nP(X,R,E,L). (10)\nHowever, due to the long-range dependency in\nrecurrent neural networks, the search space of\nR,E,L during inference grows exponentially.\nWe thus use importance sampling to approxi-\nmate the marginal distribution of X. Speciﬁ-\ncally, with the samples from a proposal distri-\nbution Q(R,E,L|X), the approximated marginal\nprobability is deﬁned as\nP(X) =\n∑\nR,E,L\nP(X,R,E,L)\n=\n∑\nR,E,L\nQ(R,E,L |X) P(X,R,E,L)\nQ(R,E,L |X)\n≈ 1\nN\n∑\n{r(i),e(i),ℓ(i)}∼Q\nP(r(i),e(i),ℓ(i),x)\nQ(r(i),e(i),ℓ(i) |x)\n(11)\nA similar idea of using importance sampling for\nlanguage modeling evaluation has been used by\nDyer et al. (2016).\nFor language modeling evaluation, we train our\nmodel on the training set from the CoNLL 2012\ndataset with coreference annotation. On the test\ndata, we treat coreference structure as latent vari-\nables and use importance sampling to approximate\nthe marginal distribution of X. For each docu-\nment, the model randomly draws N = 100 sam-\nples from the proposal distribution, discussed next.\nProposal distribution. For implementation of\nQ, we use a discriminative variant of E NTI -\nTYNLM by taking the current word xt for predict-\ning the entity-related variables in the same time\nstep. Speciﬁcally, in the generative story described\nin §2.2, we delete step 3 (words are not gener-\nated, but rather conditioned upon), move step 4\nbefore step 1, and replace ht−1 with ht in the\nsteps for predicting entity type Rt, entity Et and\nmention length Lt. This model variant provides a\n1834\nModel Perplexity\n1. 5-gram LM 138.37\n2. RNNLM 134.79\n3. E NTITY NLM 131.64\nTable 1: Language modeling evaluation on the test\nsets of the English section in the CoNLL 2012\nshared task. As mentioned in §4, the vocabulary\nsize is 10K. E NTITY NLM does not require any\ncoreference annotation on the test data.\nconditional probability Q(Rt,Et,Lt |Xt) at each\ntimestep.\nBaselines. We compare the language modeling\nperformance with two competitive baselines: 5-\ngram language model implemented in KenLM\n(Heaﬁeld et al., 2013) and RNNLM with LSTM\nunits implemented in DyNet (Neubig et al., 2017).\nFor RNNLM, we use the same hyperparameters\ndescribed in §3 and grid search on the develop-\nment data to ﬁnd the best conﬁguration.\nResults. The results of E NTITY NLM and the\nbaselines on both development and test data are\nreported in Table 1. For ENTITY NLM, we use the\nvalue of 2−1\nT\n∑T\nt=1 log P(Xt,Rt,Et,Lt) on the devel-\nopment set with coreference annotation to select\nthe best model conﬁguration and report the best\nnumber. On the test data, we are able to calcu-\nlate perplexity by marginalizing all other random\nvariables using Equation 11. To compute the per-\nplexity numbers on the test data, our model only\ntakes account of log probabilities on word predic-\ntion. The difference is that coreference informa-\ntion is only used for training E NTITY NLM and\nnot for test. All three models reported in Table 1\nshare the same vocabulary, therefore the numbers\non the test data are directly comparable. As shown\nin Table 1, ENTITY NLM outperforms both the 5-\ngram language model and the RNNLM on the test\ndata. Better performance of ENTITY NLM on lan-\nguage modeling can be expected, if we also use the\nmarginalization method deﬁned in Equation 11 on\nthe development data to select the best conﬁgura-\ntion. However, we plan to use the same experi-\nmental setup for all experiments, instead of cus-\ntomizing our model for each individual task.\n5.2 Coreference reranking\nTask description. We show how E NTITY LM,\nwhich allows an efﬁcient computation of the\nprobability P(R,E,L,X), can be used as a\ncoreference reranker to improve a competitive\ncoreference resolution system due to Martschat\nand Strube (2015). This task is analogous to\nthe reranking approach used in machine transla-\ntion (Shen et al., 2004). The speciﬁc formulation\nis as follows:\narg max\n{r(i),e(i),l(i)}∈K\nP(r(i),e(i),l(i),x) (12)\nwhere Kis the k-best list for a given document.\nIn our experiments, k = 100. To the best of our\nknowledge, the problem of obtaining k-best out-\nputs of a coreference resolution system has not\nbeen studied before.\nApproximate k-best decoding. We rerank the\noutput of a system that predicts an antecedent for\neach mention by relying on pairwise scores for\nmention pairs. This is the dominant approach\nfor coreference resolution (Martschat and Strube,\n2015; Clark and Manning, 2016a). The predic-\ntions induce an antecedent tree, which represents\nantecedent decisions for all mentions in the doc-\nument. Coreference chains are obtained by tran-\nsitive closure over the antecedent decisions en-\ncoded in the tree. A mention also can have an\nempty mention as antecedent, which denotes that\nthe mention is non-anaphoric.\nFor extending Martschat and Strube’s greedy\ndecoding approach to k-best inference, we can-\nnot simply take the k highest scoring trees ac-\ncording to the sum of edge scores, because dif-\nferent trees may represent the same coreference\nchain. Instead, we use an heuristic that creates\nan approximate k-best list on candidate antecedent\ntrees. The idea is to generate trees from the orig-\ninal system output by considering suboptimal an-\ntecedent choices that lead to different coreference\nchains. For each mention pair (mj,mi), we com-\npute the difference of its score to the score of the\noptimal antecedent choice for mj. We then sort\npairs in ascending order according to this differ-\nence and iterate through the list of pairs. For each\npair (mj,mi), we create a tree tj,i by replacing\nthe antecedent of mj in the original system output\nwith mi. If this yields a tree that encodes differ-\nent coreference chains from all chains encoded by\ntrees in the k-best list, we addti,j to the k-best list.\nIn the case that we cannot generate a given num-\nber of trees (particularly for a short document with\na large k), we pad the list with the last item added\nto the list.\n1835\nEvaluation measures. For coreference resolu-\ntion evaluation, we employ the CoNLL scorer\n(Pradhan et al., 2014). It computes three com-\nmonly used evaluation measures MUC (Vilain\net al., 1995), B 3 (Bagga and Baldwin, 1998), and\nCEAFe (Luo, 2005). We report the F1 score of\neach evaluation measure and their average as the\nCoNLL score.\nCompeting systems. We employed CORT 1\n(Martschat and Strube, 2015) as our baseline\ncoreference resolution system. Here, we com-\npare with the original (one best) outputs of\nCORT ’s latent ranking model, which is the best-\nperforming model implemented in CORT . We\nconsider two rerankers based on E NTITY NLM.\nThe ﬁrst reranking method only uses the log\nprobability for ENTITY NLM to sort the candidate\nlist (Equation 12). The second method uses a\nlinear combination of both log probabilities from\nENTITY NLM and the scores from CORT , where\nthe coefﬁcients were found via grid search with\nthe CoNLL score on the development set.\nResults. The reranked results on the CoNLL\n2012 test set are reported in Table 2. The numbers\nof the baseline are higher than the results reported\nin Martschat and Strube (2015) since the feature\nset of CORT was subsequently extended. Lines 2\nand 3 in Table 2 present the reranked best results.\nAs shown in this table, both reranked results give\nmore than 1% of CoNLL score improvement on\nthe test set over CORT , which are signiﬁcant based\non an approximate randomization test2.\nAdditional experiments also found that increas-\ning kfrom 100 to 500 had a minor effect. That is\nbecause the diversity of each k-best list is limited\nby (i) the number of entity mentions in the docu-\nment, (ii) the performance of the baseline corefer-\nence resolution system, and possibly (iii) the ap-\nproximate nature of our k-best inference proce-\ndure. We suspect that a stronger baseline system\n(such as that of Clark and Manning, 2016a) could\ngive greater improvements, if it can be adapted to\nprovide k-best lists. Future work might incorpo-\nrate the techniques embedded in such systems into\nENTITY NLM.\n[I]1 was about to ride [ my]1 [bicycle]2 to the\n[park]3 one day when [I]1 noticed that the front\n[tire]4 was ﬂat . [ I]1 realized that [ I]1 would\nhave to repair [ it]4 . [ I]1 went into [ my]1\n[garage]5 to get some [tools]5 . The ﬁrst thing\n[I]1 did was remove the xxxx\nFigure 3: A short story on bicycles from the\nInScript corpus (Modi et al., 2017). The entity\nprediction task requires predicting xxxx given\nthe preceding text either by choosing a previously\nmentioned entity or deciding that this is a “new en-\ntity”. In this example, the ground-truth prediction\nis [tire]4. For training, E NTITY NLM attempts to\npredict every entity. While, for testing, it predicts\na maximum of 30 entities after the ﬁrst three sen-\ntences, which is consistent with the experimental\nsetup suggested by Modi et al. (2017).\n5.3 Entity prediction\nTask description. Based on Modi et al. (2017),\nwe introduce a novel entity prediction task that\ntries to predict the next entity given the preced-\ning text. For a given text as in Figure 3, this task\nmakes a forward prediction based on only the left\ncontext. This is different from coreference reso-\nlution, where both left and right contexts from a\ngiven entity mention are used in decoding. It is\nalso different from language modeling, since this\ntask only requires predicting entities. Since E N-\nTITY NLM is generative, it can be directly applied\nto this task. To predict entities in test data, Rt is\nalways given and ENTITY NLM only needs to pre-\ndict Et when Rt = 1.\nBaselines and human prediction. We intro-\nduce two baselines in this task: (i) thealways-new\nbaseline that always predicts “new entity”; (ii) a\nlinear classiﬁcation model using shallow features\nfrom Modi et al. (2017), including the recency of\nan entity’s last mention and the frequency. We also\ncompare with the model proposed by Modi et al.\n(2017). Their work assumes that the model has\nprior knowledge of all the participant types, which\nare speciﬁc to each scenario and ﬁne-grained, e.g.,\nrider in the bicycle narrative, and predicts partic-\nipant types for new entities. This assumption is\nunrealistic for pure generative models like ours.\n1https://github.com/smartschat/cort, we\nused version 0.2.4.5.\n2https://github.com/smartschat/art\n1836\nMUC B 3 CEAFe\nModel CoNLL P R F1 P R F1 P R F1\n1. Baseline: CORT ’s one best 62.93 77.15 68.67 72.66 66.00 54.92 59.95 60.07 52.76 56.18\n2. Rerank: E NTITY NLM 64.00 77.90 69.45 73.44 66.84 56.12 61.01 61.73 53.90 57.55\n3. Rerank: E NTITY NLM + CORT 64.04 77.93 69.49 73.47 67.08 55.99 61.04 61.76 53.98 57.61\nTable 2: Coreference resolution scores on the CoNLL 2012 test set. CORT is the best-performing model\nof Martschat and Strube (2015) with greedy decoding.\nAccuracy (%)\n1. Baseline: always-new 31.08\n2. Baseline: shallow features 45.34\n3. Modi et al. (2017) 62.65\n4. E NTITY NLM 74.23\n5. Human prediction 77.35\nTable 3: Entity prediction accuracy on the test set\nof the InScript corpus.\nTherefore, we remove this assumption and adapt\ntheir prediction results to our formulation by map-\nping all the predicted entities that have not been\nmentioned to “new entity”. We also compare to\nthe adapted human prediction used in the In-\nScript corpus. For each entity slot, Modi et al.\n(2017) acquired 20 human predictions, and the\nmajority vote was selected. More details about\nhuman predictions are discussed in (Modi et al.,\n2017).\nResults. Table 3 shows the prediction accura-\ncies. E NTITY NLM (line 4) signiﬁcantly outper-\nforms both baselines (line 1 and 2) and prior work\n(line 3) ( p ≪0.01, paired t-test). The compari-\nson between line 4 and 5 shows our model is even\nclose to the human prediction performance.\n6 Related Work\nRich-context language models. The originally\nproposed recurrent neural network language mod-\nels only capture information within sentences.\nTo extend the capacity of RNNLMs, various re-\nsearchers have incorporated information beyond\nsentence boundaries. Previous work focuses\non contextual information from previous sen-\ntences (Ji et al., 2016a) or discourse relations be-\ntween adjacent sentences (Ji et al., 2016b), show-\ning improvements to language modeling and re-\nlated tasks like coherence evaluation and discourse\nrelation prediction. In this work, E NTITY NLM\nadds explicit entity information to the language\nmodel, which is another way of adding a memory\nnetwork for language modeling. Unlike the work\nby Tran et al. (2016), where memory blocks are\nused to store general contextual information for\nlanguage modeling, E NTITY NLM assigns each\nmemory block speciﬁcally to only one entity.\nEntity-related models. Two recent approaches\nto modeling entities in text are closely related to\nour model. The ﬁrst is the “reference-aware” lan-\nguage models proposed by Yang et al. (2016),\nwhere the referred entities are from either a pre-\ndeﬁned item list, an external database, or the con-\ntext from the same document. Yang et al. (2016)\npresent three models, one for each case. For mod-\neling a document with entities, they use corefer-\nence links to recover entity clusters, though they\nonly model entity mentions as containing a single\nword (an inappropriate assumption, in our view).\nTheir entity updating method takes the latest hid-\nden state (similar tohtwhen Rt = 1 in our model)\nas the new representation of the current entity; no\nlong-term history of the entity is maintained, just\nthe current local context. In addition, their lan-\nguage model evaluation assumes that entity infor-\nmation is provided at test time (Yang, personal\ncommunication), which makes a direct compari-\nson with our model impossible. Our entity updat-\ning scheme is similar to the “dynamic memory”\nmethod used by Henaff et al. (2016). Our entity\nrepresentations are dynamically allocated and up-\ndated only when an entity appears up, while the\nEntNet from Henaff et al. (2016) does not model\nentities and their relationships explicitly. In their\nmodel, entity memory blocks are pre-allocated\nand updated simultaneously in each timestep. So\nthere is no dedicated memory block for every en-\ntity and no distinction between entity mentions\nand non-mention words. As a consequence, it is\nnot clear how to use their model for coreference\nreranking and entity prediction.\nCoreference resolution. The hierarchical struc-\nture of our entity generation model is inspired by\n1837\nHaghighi and Klein (2010). They implemented\nthis idea as a probabillistic graphical model with\nthe distance-dependent Chinese Restaurant Pro-\ncess (Pitman, 1995) for entity assignment, while\nour model is built on a recurrent neural network\narchitecture. The reranking method considered in\nour coreference resolution evaluation could also\nbe extended with samples from additional coref-\nerence resolution systems, to produce more va-\nriety (Ng, 2005). The beneﬁt of such a system\ncomes, we believe, from the explicit tracking of\neach entity throughout the text, providing entity-\nspeciﬁc representations. In previous work, such\ninformation has been added as features (Luo et al.,\n2004; Bj ¨orkelund and Kuhn, 2014) or by com-\nputing distributed entity representations (Wiseman\net al., 2016; Clark and Manning, 2016b). Our ap-\nproach complements these previous methods.\nEntity prediction. The entity prediction task\ndiscussed in §5.3 is based on work by Modi et al.\n(2017). The main difference is that we do not as-\nsume that all entities belong to a previously known\nset of entity types speciﬁed for each narrative sce-\nnario. This task is also closely related to the\n“narrative cloze” task of Chambers and Jurafsky\n(2008) and the “story cloze test” of Mostafazadeh\net al. (2016). Those studies aim to understand re-\nlationships between events, while our task focuses\non predicting upcoming entity mentions.\n7 Conclusion\nWe have presented a neural language model, E N-\nTITY NLM, that deﬁnes a distribution over texts\nand the mentioned entities. It provides vector rep-\nresentations for the entities and updates them dy-\nnamically in context. The dynamic representations\nare further used to help generate speciﬁc entity\nmentions and the following text. This model out-\nperforms strong baselines and prior work on three\ntasks: language modeling, coreference resolution,\nand entity prediction.\nAcknowledgments\nWe thank anonymous reviewers for the helpful\nfeedback on this work. We also thank the members\nof Noah’s ARK and XLab at University of Wash-\nington for their valuable comments, particularly\nEunsol Choi for pointing out the InScript corpus.\nThis research was supported in part by a Univer-\nsity of Washington Innovation Award, Samsung\nGRO, NSF grant IIS-1524371, the DARPA CwC\nprogram through ARO (W911NF-15-1-0543), and\ngifts by Google and Facebook.\nReferences\nAmit Bagga and Breck Baldwin. 1998. Algorithms for\nscoring coreference chains. In LREC Workshop on\nLinguistic Coreference.\nPaul Baltescu and Phil Blunsom. 2015. Pragmatic neu-\nral language modelling in machine translation. In\nNAACL.\nAnders Bj ¨orkelund and Jonas Kuhn. 2014. Learn-\ning structured perceptrons for coreference resolution\nwith latent antecedents and non-local features. In\nACL.\nNathanael Chambers and Daniel Jurafsky. 2008. Un-\nsupervised Learning of Narrative Event Chains. In\nACL.\nKevin Clark and Christopher D. Manning. 2016a.\nDeep reinforcement learning for mention-ranking\ncoreference models. In EMNLP.\nKevin Clark and Christopher D. Manning. 2016b. Im-\nproving coreference resolution by learning entity-\nlevel distributed representations. In ACL.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12:2121–2159.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In EMNLP.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS, pages 249–256.\nJoshua Goodman. 2001. Classes for fast maximum en-\ntropy training. In ICASSP.\nAria Haghighi and Dan Klein. 2010. Coreference res-\nolution in a modular, entity-centered model. In\nNAACL.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nKneser-Ney language model estimation. In ACL.\nMikael Henaff, Jason Weston, Arthur Szlam, An-\ntoine Bordes, and Yann LeCun. 2016. Track-\ning the world state with recurrent entity networks.\narXiv:1612.03969.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\n1838\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer,\nand Jacob Eisenstein. 2016a. Document context lan-\nguage models. In ICLR (workshop track).\nYangfeng Ji, Gholamreza Haffari, and Jacob Eisen-\nstein. 2016b. A latent variable recurrent neural\nnetwork for discourse-driven language models. In\nNAACL-HLT.\nDiederik Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization.\narXiv:1412.6980.\nXiaoqiang Luo. 2005. On coreference resolution per-\nformance metrics. In HLT-EMNLP.\nXiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda\nKambhatla, and Salim Roukos. 2004. A mention-\nsynchronous coreference resolution algorithm based\non the Bell tree. In ACL.\nSebastian Martschat and Michael Strube. 2015. La-\ntent structures for coreference resolution. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:405–418.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH.\nAshutosh Modi, Ivan Titov, Vera Demberg, Asad Say-\need, and Manfred Pinkal. 2017. Modeling seman-\ntic expectation: Using script knowledge for referent\nprediction. Transactions of the Association of Com-\nputational Linguistics, 5:31–44.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand evaluation framework for deeper understanding\nof commonsense stories. In NAACL.\nGraham Neubig, Chris Dyer, Yoav Goldberg, Austin\nMatthews, Waleed Ammar, Antonios Anastasopou-\nlos, Miguel Ballesteros, David Chiang, Daniel\nClothiaux, Trevor Cohn, et al. 2017. Dynet: The\ndynamic neural network toolkit. arXiv:1701.03980.\nVincent Ng. 2005. Machine learning for coreference\nresolution: From local classiﬁcation to global rank-\ning. In ACL.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In EMNLP.\nJim Pitman. 1995. Exchangeable and partially ex-\nchangeable random partitions. Probability Theory\nand Related Fields, 102(2):145–158.\nSameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-\nuard Hovy, Vincent Ng, and Michael Strube. 2014.\nScoring coreference partitions of predicted men-\ntions: A reference implementation. In ACL.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In EMNLP-\nCoNLL.\nLibin Shen, Anoop Sarkar, and Franz Josef Och. 2004.\nDiscriminative reranking for machine translation. In\nNAACL.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(1):1929–1958.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\nIn NAACL-HLT.\nMarc Vilain, John Burger, John Aberdeen, Dennis Con-\nnolly, and Lynette Hirschman. 1995. A model-\ntheoretic coreference scoring scheme. In MUC.\nSam Wiseman, Alexander M. Rush, and Stuart M.\nShieber. 2016. Learning global features for coref-\nerence resolution. In NAACL.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2016. Reference-aware language models.\narXiv:1611.01628.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2015. Recurrent neural network regularization.\nICLR.\n1839",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7875511646270752
    },
    {
      "name": "Natural language processing",
      "score": 0.5932818651199341
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4898669421672821
    },
    {
      "name": "Programming language",
      "score": 0.3444070518016815
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I85384741",
      "name": "Heidelberg University",
      "country": "US"
    }
  ],
  "cited_by": 96
}