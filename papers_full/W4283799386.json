{
  "title": "Hierarchical Image Generation via Transformer-Based Sequential Patch Selection",
  "url": "https://openalex.org/W4283799386",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5015910981",
      "name": "Xiaogang Xu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5068584522",
      "name": "Ning Xu",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2973045729",
    "https://openalex.org/W6730587030",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W6779237541",
    "https://openalex.org/W2953421261",
    "https://openalex.org/W2991997773",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W6782264332",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W6750771589",
    "https://openalex.org/W6776700526",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6767384525",
    "https://openalex.org/W2916671372",
    "https://openalex.org/W2943259397",
    "https://openalex.org/W6787712174",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W6734564793",
    "https://openalex.org/W2920879895",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2921737854",
    "https://openalex.org/W6718379498",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W6767023862",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3134351519",
    "https://openalex.org/W3043801304",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2771088323",
    "https://openalex.org/W3182270175",
    "https://openalex.org/W6752670383",
    "https://openalex.org/W2783879794",
    "https://openalex.org/W6756954426",
    "https://openalex.org/W3034600949",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W3169001068",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W2972328244",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W4287551933",
    "https://openalex.org/W4301206121",
    "https://openalex.org/W2982763192",
    "https://openalex.org/W2561196672",
    "https://openalex.org/W2965289598",
    "https://openalex.org/W2593414223",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2983248633",
    "https://openalex.org/W3035592938",
    "https://openalex.org/W3096456322",
    "https://openalex.org/W3108984808",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2963966654",
    "https://openalex.org/W2965833116",
    "https://openalex.org/W2987919422",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2962845008",
    "https://openalex.org/W2963184176",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "To synthesize images with preferred objects and interactions, a controllable way is to generate the image from a scene graph and a large pool of object crops, where the spatial arrangements of the objects in the image are defined by the scene graph while their appearances are determined by the retrieved crops from the pool. In this paper, we propose a novel framework with such a semi-parametric generation strategy. First, to encourage the retrieval of mutually compatible crops, we design a sequential selection strategy where the crop selection for each object is determined by the contents and locations of all object crops that have been chosen previously. Such process is implemented via a transformer trained with contrastive losses. Second, to generate the final image, our hierarchical generation strategy leverages hierarchical gated convolutions which are employed to synthesize areas not covered by any image crops, and a patch guided spatially adaptive normalization module which is proposed to guarantee the final generated images complying with the crop appearance and the scene graph. Evaluated on the challenging Visual Genome and COCO-Stuff dataset, our experimental results demonstrate the superiority of our proposed method over existing state-of-the-art methods.",
  "full_text": "Hierarchical Image Generation via Transformer-Based Sequential Patch Selection\nXiaogang Xu,1 Ning Xu 2\n1 Department of Computer Science and Engineering, The Chinese University of Hong Kong\n2 Adobe Research\nxgxu@cse.cuhk.edu.hk, nxu@adobe.com\nAbstract\nTo synthesize images with preferred objects and interactions,\na controllable way is to generate the image from a scene graph\nand a large pool of object crops, where the spatial arrange-\nments of the objects in the image are deÔ¨Åned by the scene\ngraph while their appearances are determined by the retrieved\ncrops from the pool. In this paper, we propose a novel frame-\nwork with such a semi-parametric generation strategy. First,\nto encourage the retrieval of mutually compatible crops, we\ndesign a sequential selection strategy where the crop selection\nfor each object is determined by the contents and locations of\nall object crops that have been chosen previously. Such pro-\ncess is implemented via a transformer trained with contrastive\nlosses. Second, to generate the Ô¨Ånal image, our hierarchical\ngeneration strategy leverages hierarchical gated convolutions\nwhich are employed to synthesize areas not covered by any im-\nage crops, and a patch-guided spatially adaptive normalization\nmodule which is proposed to guarantee the Ô¨Ånal generated im-\nages complying with the crop appearance and the scene graph.\nEvaluated on the challenging Visual Genome and COCO-Stuff\ndataset, our experimental results demonstrate the superiority\nof our proposed method over existing state-of-the-art methods.\nIntroduction\nIt is challenging to generate an image from a scene graph con-\nsists of several objects with sophisticated interactions. With\nsuch a framework, users just need to provide Ô¨Çexible scene\ndescriptions to deÔ¨Åne the objects as well as their interactions,\nand the framework would synthesize images, achieving a\nuser-controllable generation process. Current frameworks for\ngenerating images from scene descriptions take advantage of\ngenerative adversarial networks (GANs) (Goodfellow et al.\n2014). Compared with parametric models (Johnson, Gupta,\nand Fei-Fei 2018) that solely lean upon the networks to model\nthe appearance of objects, semi-parametric approaches (Li\net al. 2019c; Tseng et al. 2020) have recently been proposed\nand shown superior performance. Such methods Ô¨Årst lever-\nage a memory bank to retrieve image crops for objects in\nthe scene graph (called retrieve stage), and then synthesize\nrealistic images from scene graphs and retrieved crops (called\ngeneration stage). In this work, we focus on the amelioration\nof the semi-parametric model, improving the retrieve stage\nas well as the generation stage.\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nThe retrieved crops from the retrieve stage should be mu-\ntually compatible for synthesizing an image and consistent\nwith the corresponding scene graph. To complete the retrieve\nstage, existing retrieval-based image synthesis methods ei-\nther employ pre-deÔ¨Åned embeddings for retrieval (Li et al.\n2019c) or proposes a differentiable retrieval process (Tseng\net al. 2020) to retrieve the image crop that is compatible with\nthe previously selected ones. However, they all neglect the\nusage of the crops‚Äô spatial information during the optimizing\nof retrieve stage. In this paper, we reformulate the retrieve\nstage and complete it as a novel sequential process. In our\nprocess, the selection of the crop for each object in the scene\ngraph would be determined by spatial, style, and content fea-\ntures of crops that have already been chosen. To implement\nsuch sequential selection, we propose to adopt a transformer\n(Vaswani et al. 2017) structure that is trained with contrastive\nlearning (Oord, Li, and Vinyals 2018; Chen et al. 2020). In\nthe transformer, the candidate image crops for selection and\npreviously selected crops are embedded with two speciÔ¨Åc\nheads and incorporated with spatial information via position\nembedding. Iterative operations on the transformer can re-\ntrieve the image crops that are mutually compatible. This\nis the Ô¨Årst successful attempt to complete the crop retrieval\nwith self-supervised contrastive learning for image synthesis,\nand experiments on public datasets demonstrate the superior-\nity of our sequential selection strategy over existing retrieve\napproaches.\nMoreover, in this paper, we further propose a novel gen-\nerator that generates realistic images from a scene graph\nwith selected image crops as the guidance. To synthesize the\nrealistic and high-resolution images, we design the genera-\ntor with a hierarchical generation strategy, using hierarchi-\ncal gated convolutions and proposing patch-guided spatially\nadaptive normalization module. The patch-guided spatially\nadaptive normalization module is designed to guarantee the\nsynthesized images highly respecting the selected crops. The\ngenerator is trained with crops selected by our transformer,\nboosting the performance of the generator in the inference\nstage, and the generator boosts the mutual compatibility be-\ntween the selected crops in the output image. Evaluated on\nVisual Genome and COCO-Stuff dataset (including objective\nanalyses and a user study for subjective evaluation) , our pro-\nposed method signiÔ¨Åcantly outperforms the state-of-the-art\n(SOTA) generation approaches.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2938\nChosen Crops\nScene Graph G\nPSGIM\nRetrieve Stage\nGeneration\nStage\nsand\nbelow below below\nclouds sea\nCandidate Crops Bounding\nBox\nSCSM\nScene Graph\nEncoding\nFigure 1: Illustration of our framework for synthesizing images from scene graphs, including the scene graph encoding, retrieve\nstage and the generation stage.\nRelated Work\nConditional Image Generation\nCurrent conditional generative models can synthesize images\naccording to additional conditions such as image (Choi et al.\n2020; Lee et al. 2020; Liu et al. 2021), label (Chen et al.\n2019; Yang et al. 2021), segmentation mask (Park et al. 2019;\nHuang et al. 2020; Tan et al. 2021), text (Xu et al. 2018; Qiao\net al. 2019; Li et al. 2019a; Zhu et al. 2019; Li et al. 2019b)\nand layout (Sun and Wu 2019; Ashual and Wolf 2019; Zhao\net al. 2019). The text conditions can either be natural language\nsentences or scene graphs (Li et al. 2019c; Tseng et al. 2020).\nCompared with sentences, the scene graph description is\nmore well-structured, since the nodes in a scene graph can\nrepresent objects and the edges can denote their relationship.\nTherefore, the synthesis from scene graphs allows better\ncontrollability. In this work, we focus on employing the scene\ngraph description as the conditions.\nImage Generation from Scene Descriptions\nWith the development of deep generative models, especially\nthe adversarial generative networks, the synthesis from scene\ndescriptions becomes feasible. Existing methods for such\nsynthesis can be divided into two categories. The Ô¨Årst kind\nof approach employs parametric generative models to tackle\nthis task (Johnson, Gupta, and Fei-Fei 2018). The feature of\nobjects and the relationships among objects are captured via\na graph convolution network from a scene graph, then im-\nages are synthesized based on the extracted feature with the\nconditional generative models (Mirza and Osindero 2014).\nHowever, these methods often fail to generate realistic images\nfor complicated scene descriptions, due to various objects and\ncomplex interactions in the scene. To this, semi-parametric\napproaches (Li et al. 2019c; Tseng et al. 2020) are recently\nproposed and they perform generation based on reference ob-\nject crops. The reference crops are retrieved from an external\nbank and help to synthesize the Ô¨Ånal images. The retrieval\nmodule is a crucial component. PasteGAN (Li et al. 2019c)\nemployed predeÔ¨Åned retrieval modules that cannot be opti-\nmized during the training. RetrieveGAN (Tseng et al. 2020)\nlater designed a differentiable retrieval process, thus enable\noptimizing the retrieve stage through the end-to-end training.\nHowever, they ignore the employing of image crops‚Äô spatial\nfeatures in the optimization of retrieve stage.\nContrastive Learning\nContrastive learning has recently become a prominent ap-\nproach in unsupervised representation learning. These meth-\nods learn representations by a ‚Äúcontrastive loss‚Äù which pushes\napart dissimilar pairs (called negative pairs) while pulling\ntogether similar pairs (called positive pairs). The main differ-\nence between different approaches to complete contrastive\nlearning lies in their strategies for obtaining positive and neg-\native pairs. For the semantic computer vision task, e.g., image\nclassiÔ¨Åcation, different types of data augmentation are em-\nployed for obtaining positive pairs (Chen et al. 2020; Oord,\nLi, and Vinyals 2018; Khosla et al. 2020), including random\ncropping and Ô¨Çipping. For the language task, Logeswaran et\nal. (Logeswaran and Lee 2018) treat the context sentences as\npositive samples to learn representations. However, no exist-\ning works have analyzed the effect of applying contrastive\nlearning in the crop retrieval for image synthesis.\nMethod\nGiven a scene graph Gwhich contains a set of nobjects O=\n{o1;:::;o n}and their pairwise relations R = {r1;:::;r m},\nour goal is to synthesize an image x ‚ààRH√óW√ó3 from the\nscene graph. In addition, our method leverages an external\npool of object crops (can be either segmented out or not)\nto facilitate the generation process. The overall framework\nis shown in Fig. 1 which consists of three stages. In the\nÔ¨Årst stage, we leverage the scene graph to extract semantic\nfeatures which are useful for crop retrieval and location pre-\ndiction for each object. In the second stage, the sequential\ncrop selection module (SCSM) sequentially selects a most\ncompatible crop for each object given all previously chosen\ncrops, which will be used for the image synthesis. Finally,\nthe progressive scene graph to image module (PSGIM) syn-\nthesizes the target image based on the scene graph features\nand selected crops.\nScene Graph Encoding\nFollowing (Li et al. 2019c), our method Ô¨Årst processes the\ninput scene graph to extracts text embeddings for all the\n2939\nTransformer\nùëù\"\nùëù#$%\nùëù#$%\n& , ùëô ‚àà [1, ùëò] ùëì/#$%\n& , ùëô ‚àà [1, ùëò] ‚Ñí1234567489:\n‚Ä¶‚Ä¶\n‚Ä¶Linear\nTransformation\nPosition\nEmbedding ‚Ä¶\nùëì\" ùëì% ùëì#‚Ä¶\nùëì/\" ùëì/% ùëì/#‚Ä¶ùëì/#$%\n+ + + +\nùëù% ùëù#‚Ä¶\nFigure 2: The illustration for the SCSM during training.\nnobjects as {t1;:::;t n}= Eg(O;R) via a graph convolu-\ntion network Eg, where ti ‚ààRCt is the text embedding for\nobject oi. For each object oi, we match ti with the text em-\nbeddings of other object crops in the external object pool to\nretrieve a set of its candidate crops M(oi) = {p1\ni;:::pk\ni|pj\ni ‚àà\nRHp√óWp√ó3;j ‚àà[1;k]}with a Ô¨Åxed size k. In addition, ti is\nfurther used to predict a bounding box bi ‚ààR4 for object oi.\nPlease refer to (Li et al. 2019c) for details of these steps.\nSequential Crop Selection Module (SCSM)\nTo synthesize the Ô¨Ånal image, we Ô¨Årst need to select only\none crop from every object‚Äôskcandidate crops. Our method\nperforms the crop selection operation in an iterative fashion.\nSpeciÔ¨Åcally, suppose there are already m ‚àà[1;n) crops\np1;:::;p m selected for object o1;:::;o m. Let us deÔ¨Åne the set\n{p1;:::;p m}as the chosen crop set P. Given a new object\nom+1, our SCSM aims to select one crop from itskcandidate\ncrops which is most compatible with all the crops in P, and\nthus can improve the realism of the Ô¨Ånal synthesized image.\nTo achieve this goal, we propose a novel contrastive learning\nframework, i.e. given a chosen crop set P from the same\nimage, the compatible score between P and a new crop from\nthe same image should be higher than the compatible score\nbetween P and a new crop from a different image. Such\nlearning objective helps our model to select object crops\nlikely belonging to the same image, and thus improves the\ncompatibility among the selected crops.\nWe leverage a novel transformer to implement the idea, as\nshown in Fig. 2. SpeciÔ¨Åcally, for every croppi ‚ààP (the shape\nis Hp√óWp√ó3) with its predicted bounding box location bi,\nwe embed both its appearance and position information as an\ninput token to the transformer asfi = W1¬∑pi+Eb(bi), where\nW1 is a trainable linear transformation matrix to convert pi\ninto a 1-D embedding with shape RCp. Eb is a position en-\ncoder with three nonlinear layers, the output shape of which\nis also RCp. In addition, we add a learnable start token f0\nto represent the overall compatible feature of all the input\ntokens. Its appearance input p0 is randomly initialized with\nthe normal distribution while its position input is initialized\nwith bm+1, which is the predicted bounding box location of\nthe new object om+1. A notable difference between previous\nmethods (Li et al. 2019c; Tseng et al. 2020) and ours is that\nwe explicitly leverage the position information of each crop,\nwhich is proven to be effective in our ablation study.\nFollowing recent transformer structures (Vaswani et al.\n2017), our transformer has a total of six layers, each of which\nconsists of multi-head self-attention as well as MLP Let us\ndenote the output embedding of the start token as bf0 ‚ààRCp.\nGiven a new candidate crop pl\nm+1;l ‚àà[1;k], we Ô¨Årst apply\nanother trainable linear matrix W2 with the same shape as\nW1 to obtain its appearance features as bfl\nm+1 = W2 ¬∑pl\nm+1.\nThen its compatible score with the chosen crop set P is\ncomputed as the cosine similarity between their embeddings,\ni.e. bf0 ¬∑bfl\nm+1. Note that both the embeddings are normalized\nto the unit hypersphere before matching.\nDuring training, given an image with its paired scene graph,\nour method randomly selects a crop setP = {p1;:::;p m|m‚àà\n[1;n)}from the image. The size mis randomly determined\nto mimic the iterative selection process for inference. Then\nfor a new objectom+1, its crop pm+1 from the original image\nis treated as the positive crop while its retrieved candidate\ncrops {p1\nm+1;:::;p k\nm+1}from different images are treated\nas negative crops. Then the contrastive loss for this training\nimage can be deÔ¨Åned as\nLcontrastive= ‚àílog exp( bf0¬∑bfm+1=\u001c)\nexp( bf0¬∑bfm+1=\u001c)+Pk\nl=1 exp( bf0¬∑bfl\nm+1=\u001c); (1)\nwhere bfm+1 and bfl\nm+1 are the embeddings of the positive\ncrop pm+1 and a negative crop pl\nm+1. \u001c is a positive scalar\ntemperature parameter.\nDuring inference, given a scene graph and the predicted\nbounding boxes, since initially there is no crop selected for\nany object, our method simply randomly samples one object\nand randomly set one of its candidate crops as the chosen\ncrop. Then, for each remaining object oi(i ‚àà(2;n]), we\napply the trained SCSM once to Ô¨Ånd its candidate crop with\n2940\nStyleEncoder\nPSANMGatedConvolution\n{ùëê!,‚Ä¶,ùëê\"}{ùëù!,‚Ä¶,ùëù\"}\nContentEncoder {ÃÇùëê!,‚Ä¶,ÃÇùëê\"}{ùëè!,‚Ä¶,ùëè\"}\nStyleUnify{ùë°!,‚Ä¶,ùë°\"} ÃÇùëê!\n{ùë†!,‚Ä¶,ùë†\"}\nGenerationatScale1 GenerationatScalej GenerationatScaleN‚Ä¶ ‚Ä¶\nConvolutionùõΩ#\nùõæ#BN √ó +\nGenerationatScalej‚àà[1,ùëÅ] TheCreationofReferenceMap\nùë•$\nùëü! ùëü$ ùëü%\nùë•! ùë•$ ùë•%\nùëü$ÃÇùëê$\nUpSampling\nÃÇùëê$&!\nFigure 3: Our overall framework for PSGIM.\nthe highest compatible score and add it to the chosen set P.\nThe process is repeated until every object has selected its\nown crop.\nProgressive Scene Graph to Image Module\nFor every object oi in the given scene graph, we now have\nits scene-graph embedding ti, bounding box prediction bi\nas well as a selected object crop pi. Our PSGIM leverages\nthese inputs to generate the Ô¨Ånal image x‚ààRH√óW√ó3. The\nframework is shown in Fig. 3.\nOur generation module Ô¨Årst leverages a content encoderEc\nand a style encoder Es to extract different features from the\nobject crop pi. The output content feature ofEc is used as the\ngenerator input to provide the structural information of object\noi in the generated image. The output style feature of Es is\nused to modulate the content feature to have a uniÔ¨Åed style\nwith the other object crops, and thus improves the overall\nrealism of the Ô¨Ånal generated image. It is worth noting that\nalthough our SCSM is able to select mutually compatible\nobject crops, it is still possible that the initially retrieved\ncandidate crop sets do not have enough compatible crops to\nchoose. Therefore we propose a style uniÔ¨Åer in the generator\nto handle the limitation.\nSpeciÔ¨Åcally, we extract the content feature as ci =\nEc(pi);ci ‚ààRHc√óWc√óCc. We further combine the spatially-\nexpanded scene-graph feature ti with ci to include more\nsemantics. The new feature is further interpolated and pasted\nonto a speciÔ¨Åc region of a zero feature map with the shape\nR\nH\n2N‚àí1 √ó W\n2N‚àí1 √ó(Cc+Ct), where the location of the region is\ndetermined by the location bi scaled by 1\n2N‚àí1 and N is the\nnumber of output scales of the generator. Let ^ci denote the\nÔ¨Ånal pasted feature.\nThe style feature is extracted as si = Es(pi);si ‚ààRCs.\nOur style uniÔ¨Åer takes as input the averaged style features\nof all object crops and produces as output the modulation\nparameters which are applied to normalize the channels of\neach content feature ^ci independently.\n\rs;\fs = StyleUniÔ¨Åer(s1+:::+sn\nn ); \r s;\fs ‚ààRCc+Ct;\n^ci = \rs\n^ci‚àí\u0016i‚àö\u001bi+\u000f + \fs; ^ci ‚ààR\nH\n2N‚àí1 √ó W\n2N‚àí1 √ó(Cc+Ct);\n(2)\nwhere the style uniÔ¨Åer is implemented via a MLP with several\nnonlinear layers. \u0016i and \u001bi are the mean and variance of\nthe content feature ^ci and \u000fis a small positive constant for\nnumerical stability. Finally, the normalized content features\nof all crops are aggregated together to represent the generator\ninput at the coarsest level, i.e. ^c1 = Pn\ni=1 ^ci.\nOur generator has a hierarchical structure with N output\nscales. At every scale j ‚àà[1;N], the generator takes as in-\nput ^cj and produces an output image xj ‚ààR\nH\n2N‚àíj √ó W\n2N‚àíj √ó3.\nThere are two important network components at every scale\nof the generator. The Ô¨Årst component is the gated convolu-\ntions employed from (Yu et al. 2019) which aims to inpaint\nthe missing areas uncovered by any object crops. The second\ncomponent is the patch-guided spatially adaptive normaliza-\ntion module (PSANM) that is inspired from the SPADE (Park\net al. 2019) module, where we Ô¨Årst copy and paste the object\ncrops into a reference image and then use it to guide the\nstructure and content of the generated image. SpeciÔ¨Åcally,\nthe reference image rj ‚ààR\nH\n2N‚àíj √ó W\n2N‚àíj √ó1 is generated by\npasting the gray scales of all object crops pi onto an empty\ncanvas based on their location bi scaled by a factor 1\n2N‚àíj .\nThe crops are turned into gray scale to eliminate the negative\neffects of possible inconsistent color styles, which is already\nhandled by our style uniÔ¨Åer. Then similar to the semantic\nmask input to SPADE, our reference imagerj is employed to\npredict spatially adaptive normalization parameters. Please\nrefer to Fig. 3 for more details about PSANM.\nExperiments demonstrate the superiority of our PSGIM to\nexisting generators for the synthesis from scene graphs.\n2941\nTraining losses. Given the generated image output at the\nj-th scale, we propose several losses to train our generator.\nFirst, for a training image ywith its paired scene graph, we\ncan use ground truth crops and bounding boxes for each\nobject, and thus the generated output xj should reconstruct\n(y)‚Üìj, where (¬∑)‚Üìjdenotes the operation of downsampling of\nan image to the j-th scale.\nLj\nr = E[‚à•(y)‚Üìj ‚àíxj‚à•2]: (3)\nWe also leverage the perceptual loss (Johnson, Alahi, and\nFei-Fei 2016; Chen, Xu, and Jia 2020) to compare xj with\n(y)‚Üìj using ImageNet (Deng et al. 2009) pretrained VGG\n(Simonyan and Zisserman 2015) features \bk, as\nLj\np =\nX\nl\nE[‚à•\bl((y)‚Üìj) ‚àí\bl(xj)‚à•2]: (4)\nIn addition, we apply the adversarial loss (Mao et al. 2017)\nwith a discriminator Dj at the j-the scale, as\nLj\nd = E[(Dj((y)‚Üìj)‚àí1)2+(Dj(xj))2]; Lj\ng = E[(Dj(xj)‚àí1)2]:\n(5)\nFurthermore, we propose a consistency loss term to encour-\nage the similarity between the generated outputs at different\nscales, as\nLc =\nN‚àí1X\nj=1\nE[‚à•(xN )‚Üìj ‚àíxj‚à•]: (6)\nBesides using ground truth crops and bounding boxes, we\ncan also use retrieved crops and predicted bounding boxes\nto generate a new image \u0016xj which does not have correspond-\ning ground truth image. Therefore, we can only apply the\nadversarial loss and consistency loss for it.\nIn summary, the total loss to train our generator at all scales\ncan be written as\nL= \u00151\nX\nxj;j\nLj\nr + \u00152\nX\nxj;j\nLj\np + \u00153\nX\nxj;\u0016xj;j\nLj\ng + \u00154\nX\nxj;\u0016xj\nLc; (7)\nwhere \u00151 to \u00154 are parameters to balance various losses.\nExperiments\nDatasets\nThe COCO-Stuff (Caesar, Uijlings, and Ferrari 2018) and\nVisual Genome (Krishna et al. 2017) datasets are standard\nbenchmark datasets for evaluating scene-graph-to-image gen-\neration models. Our framework can synthesize images with\narbitrary resolution. However, due to the computation lim-\nitation, we use the image resolution of 256 √ó256 for all\nthe experiments. We follow the protocol in sg2im (Johnson,\nGupta, and Fei-Fei 2018) to pre-process the dataset and com-\nplete the train-test split.\nImplementation Details\nWe implement with PyTorch (Paszke et al. 2017) and train\nSCSM and PSGIM with 90 epochs on both the COCO-Stuff\nand Visual Genome datasets. In addition, we use the Adam\noptimizer (Kingma and Ba 2015) with a batch size of 16.\nThe learning rates for the generator and discriminator are\nboth 0.0001, and the exponential decay rates ( \f1, \f2) are\nset to be (0, 0.9). We set the hyper-parameters as follows:\n\u00151 = 1:0, \u00152 = 1:0, \u00153 = 0:02, \u00154 = 1:0. For the training of\nSCSM, the proportion between positive samples and negative\nsamples is 1:10. The number of candidate crops for each\nobject during inference is 5. To implement the perceptual loss\nterm, we use the ReLU1 2, ReLU2 2, ReLU3 3, ReLU4 3,\nReLU5 3 layers of an ImageNet-pretrained VGG-16 network\n(Simonyan and Zisserman 2015). The crop size is set to\n64 √ó64 and 32 √ó32 for COCO-Stuff and Visual Genome.\nBaselines\nThe baselines for the comparison include four categories: 1)\nthe parametric generative models for mapping scene graphs\nto images sg2im (Johnson, Gupta, and Fei-Fei 2018); 2) the\nsemi-parametric approaches PasteGAN and RetrieveGAN\n(Li et al. 2019c; Tseng et al. 2020); 3) the text-to-image\nmethods AttnGAN (Xu et al. 2018), MirrorGAN (Qiao et al.\n2019),ControlGAN (Li et al. 2019a), DM-GAN (Zhu et al.\n2019),Obj-GAN (Li et al. 2019b); 4) the layout-to-image\nmethods ReconÔ¨Ågurable (Sun and Wu 2019), Specifying\n(Ashual and Wolf 2019),Layout2im (Zhao et al. 2019). For\nthe text-to-image methods, we follow the strategy in Re-\ntrieveGAN (Tseng et al. 2020) for comparison: we convert\nthe scene graph to the corresponding text description. Specif-\nically, we convert each relationship in the graph into a sen-\ntence, and link every sentence via the conjunction word ‚Äúand‚Äù.\nThe layout-to-image methods take input as the ground-truth\nbounding boxes. For a fair comparison, all baselines are\ntrained to synthesize with resolution of 256 √ó256.\nMetrics\nWe employ three metrics. 1) Inception Score (IS) (Salimans\net al. 2016): IS uses the Inception V3 (Szegedy et al. 2016)\nmodel to measure the visual quality of the generated images.\n2) Fr/acute.ts1echet Inception Distance (FID) (Heusel et al. 2017):\nFID measures the visual quality and diversity of the synthe-\nsized images. 3) Diversity (DS): we follow the setting of (Li\net al. 2019c; Tseng et al. 2020) to evaluate the diversity by\nmeasuring distances among features of synthesized images\n(these images are synthesized with same input scene graph\nwhile different crops) using the Learned Perceptual Image\nPatch Similarity (LPIPS) (Zhang et al. 2018) metric.\nComparison with Existing Approaches\nQuantitative evaluation. To have a fair comparison with\ndifferent methods, we conduct the evaluation using two dif-\nferent settings. First, bounding boxes of objects are predicted\nby models. Second, ground-truth bounding boxes are given as\ninputs in addition to the scene graph. The results of these two\nsettings are shown in Table 1. Since our approach has an op-\ntimized crop retrieval process and better generator structure,\nour approach performs favorably against the other algorithms.\nAnd our results are also superior over SOTA text-to-image\nand layout-to-image methods on both COCO-Stuff and Vi-\nsual Genome datasets. Moreover, since our framework can\nhave different crops for the initialization of the chosen crops\nin SCSM, our framework synthesizes comparably diverse\nimages compared to others.\n2942\nGraph\nperson\nsnow\nabove\nsurrounding\nskis\nbelow\nsand\nhill\nbelow\nsea\nabove\nsky-other\nocean\nhill\nbelow\nnear\ncloud\nboat\nman\nhill\nright of\nshirtfield\nwearing\nbelow\nhas\nleft of\njean\nsky tree\nperson\ninside\nsurrounding\nsea\nsurfboard\nclouds\nabove\nsurrounding\nsandkite\nbelow\nsky\ninside\nmountain\nsnow\nwith below\nabove\nman\nocean\nsurfboard\nwater\nsky\nbelow\nleft of\nhold\nright of\nPasteGAN\nRetriev\neGAN\nOurs\nCOCO-Stuff V\nisual Genome COCO-Stuff (GT) Visual Genome (GT)\nFigure 4:\nThe synthesis on COCO-Stuff (left four columns), Visual Genome (right four columns).\nDatasets COCO-Stuf\nf Visual Genome\nFID ‚Üì IS ‚Üë DS ‚Üë FID ‚Üì IS ‚Üë DS ‚Üë\nsg2im 226.3\n3.8 0.02 210.0 4.7 0.10\nAttnGAN 80.1 9.2 0.15 126.1 8.4 0.30\nMirrorGAN 78.3 9.5 0.15 123.3 8.7 0.31\nControlGAN 86.1 8.6 0.14 135.6 7.7 0.28\nDM-GAN 68.0 10.9 0.18 107.0 10.5 0.34\nObj-GAN 68.4 10.8 0.19 107.8 10.4 0.35\nPasteGAN 78.8 8.5 0.60 131.6 6.5 0.38\nRetrieveGAN 56.9 10.2 0.47 113.1 7.5 0.30\nOurs 51.6 15.2 0.63 63.7 10.8 0.59\nsg2im (GT)\n100.9 9.9 0.02 141.3 6.8 0.14\nLayout2im 50.6 11.4 0.55 60.6 10.7 0.53\nReconÔ¨Ågurable 48.6 14.0 0.56 57.6 10.1 0.54\nSpecifying 65.2 12.4 0.62 63.3 11.0 0.61\nPasteGAN (GT) 70.2 11.0 0.45 114.3 9.5 0.27\nRetrieveGAN (GT) 54.6 12.3 0.25 77.7 10.8 0.22\nOurs (GT) 46.9 15.5 0.64 56.7 11.4 0.59\nTable 1: ‚Üëmeans the higher the better, ‚Üìmeans the lower the\nbetter. The top part shows results of employing the predicted\nbounding boxes during the inference, and the bottom part\ndisplays results of using the ground-truth bounding boxes.\nQualitative evaluation. Furthermore, we qualitatively\ncompare the visual results generated by different methods\nin Fig. 4. We show the results on the COCO-Stuff and the\nVisual Genome datasets under two settings of using predicted\nand ground-truth bounding boxes. Moreover, our model syn-\nthesizes comparably diverse images compared to the other\nschemes. Our framework can synthesize diverse images by\nsetting different crops to initialize the set of chosen crops in\nSCSM, and the diverse results can be viewed in Fig. 5.\nDatasets COCO-Stuf\nf Visual Genome\nOther Same\nOurs Other Same Ours\nsg2im 15.3\n8.7 76.0 17.3 2.7 80.0\nAttnGAN 24.0 4.7 71.3 16.7 8.0 75.3\nMirrorGAN 20.7 6.0 73.3 8.0 10.0 82.0\nControlGAN 18.0 13.3 68.7 16.6 16.7 66.7\nDM-GAN 20.7 12.7 66.6 15.3 28.7 56.0\nObj-GAN 17.3 10.7 72.0 10.0 14.0 76.0\nPasteGAN 8.0 17.3 74.7 3.4 19.3 77.3\nRetrieveGAN 12.7 5.3 82.0 6.0 6.0 88.0\nsg2im (GT)\n8.7 9.3 82.0 2.6 6.7 90.7\nLayout2im 19.3 10.0 70.7 14.0 12.7 73.3\nReconÔ¨Ågurable 25.3 7.3 67.4 26.6 10.7 62.7\nSpecifying 22.7 12.7 64.6 14.0 18.0 68.0\nPasteGAN (GT) 18.7 11.3 70.0 9.3 14.7 76.0\nRetrieveGAN (GT) 28.7 16.0 55.3 26.6 12.7 60.7\nTable 2: User preference in the user study for the synthe-\nsis. ‚ÄúOurs‚Äù is the percentage (%) that our result is pre-\nferred, ‚ÄúOther‚Äù is the percentage that other method is selected,\n‚ÄúSame‚Äù means the percentage that the users can not decide.\nUser study. We synthesize images with our framework and\nall other baselines, and conduct a user study. We invite 30\nparticipants to see one scene graph and two images synthe-\nsized by our framework and one of the baselines. And they\nwill choose which one is better (they can also select that\nthey have no preference). The criteria include the consistency\nwith the input scene graph and the quality of synthesized\nimages. Each participant is required to complete 70 pairs of\nAB-test. The results are shown in Table 2, demonstrating that\nour framework can better implement the synthesis.\n2943\nDatasets COCO-Stuf\nf Visual Genome\nRetriev\ne Generation FID ‚ÜìIS ‚ÜëDS ‚ÜëFID ‚ÜìIS ‚ÜëDS ‚Üë\nPasteGAN\nPSGIM 58.9 12.9 0.64 83.8 8.1 0.56\nRetrieveGAN PSGIM 55.3 13.2 0.65 80.2 8.5 0.57\nSCSM w/o P PSGIM 54.7 13.5 0.62 81.5 9.2 0.53\nSCSM PasteGAN 53.5 11.7 0.58 98.7 8.0 0.54\nSCSM PSGIM 51.6 15.2 0.63 63.7 10.8 0.59\nTable 3: Ablation study‚Äôs results. ‚Äúw/o P‚Äù denotes the setting\nwithout using the position encoding of bounding boxes.\ntree\nabove\nbelow\nriver\nsky-other\nocean\nabove\nbelow\nwater\nsky\ncloud\nInput Graph\nDiverse Synthesized Images\nFigure 5: Diverse synthesis on COCO-Stuff (Ô¨Årst row) and\nVisual Genome (bottom row).\nAblation Study\nThe effects of SCSM and PSGIM. First, we prove the\neffectiveness of our proposed SCSM. We replace SCSM with\nthe pre-trained embedding function in PasteGAN (Li et al.\n2019c), and the differentiable retrieval module in Retrieve-\nGAN (Tseng et al. 2020). Second, we validate the effect of\nour PSGIM. We replace PSGIM with the generator in Paste-\nGAN and RetrieveGAN while keeping our trained SCSM. We\nconduct the ablation studies on both COCO-Stuff dataset and\nVisual Genome dataset, and the results are shown in Table 3.\nObviously, our retrieve process is better than the retrieve strat-\negy proposed by PasteGAN and RetrieveGAN. Moreover,\nthe ablation study also proves the superiority of our generator\ncompared with the generator proposed by PasteGAN.\nThe effect of the position encoding. We also validate the\neffect of using the position encoding in our SCSM, by delet-\ning the position encoding of bounding boxes in SCSM (the\nresults are denoted as ‚ÄúSCSM w/o P‚Äù). As shown in Table 3,\nthe deletion of position encoding causes the decrease of the\nperformance.\nThe inÔ¨Çuence of candidate crops‚Äô number. Moreover, we\nalso analyze the impact from the number of candidate crops\nfor each object. The candidate crops are pre-retrieved by the\nmethod of PasteGAN (Li et al. 2019c) and the number of\ncandidate crops is 5 in the above experiments. In this section,\nwe conduct experiments with the number of candidate crops\nas 50 and 100. As shown in Table 4, the performance is\nimproved a little since our retrieve module will preferentially\nchoose the candidate crop that has a higher rank after the\npre-retrieve. We expect the improvement will be more clear\nif a large-scale object bank (e.g., Google search engine) is\navailable. However, if we set the number of candidate crops\nhigh, the time cost for retrieve is increased a lot. Thus, the\nchoice of 5 in this paper is rational and practical.\nDatasets COCO-Stuf\nf Visual Genome\nFID ‚Üì IS ‚Üë DS ‚Üë FID ‚Üì IS ‚Üë DS ‚Üë\nOurs (50)\n50.8 15.8 0.66 62.5 11.2 0.62\nOurs (100) 50.5 16.3 0.68 62.1 11.7 0.64\nOurs (5) 51.6 15.2 0.63 63.7 10.8 0.59\nTable 4: ‚ÄúOurs (xx)‚Äù means the results with the number of\ncandidate crops as xx.\nDatasets COCO-Stuf\nf Visual Genome\nRetriev\ne FID ‚Üì IS ‚Üë DS ‚Üë FID ‚Üì IS ‚Üë DS ‚Üë\nRetriev\neGAN 65.1 8.5 0.59 126.5 4.2 0.45\nSCSM w/o P 64.9 8.6 0.58 126.0 4.3 0.44\nSCSM 63.5 8.9 0.59 125.6 4.5 0.46\nTable 5: Comparison among retrieve strategies.\nThe average number of external images. For the gener-\nation from one scene graph, our SCSM would obtain a set\nof patches for synthesis. These crops are extracted from a\nset of external images, and we calculate the average num-\nber. For COCO-stuff, retrieved patches for one scene graph\nare cropped from 5.46 external images averagely; for Visual\nGenome, the average number is 4.18. These results effec-\ntively demonstrate that the good results of our method are not\nsimply because it retrieves a real image matching the given\nscene graph. Instead, it is because of its ability to search\nmutually compatible patches from different image sources\nand strong generation capability.\nEvaluation of crop selection. We can paste retrieved crops\nfrom different methods onto one image, building a canvas.\nAnd we sort the crops according to their area and put the\ncrops with a larger size to the back. To further demonstrate\nthe superiority of our retrieve strategy directly, we compute\nthe FID, IS, and DS for the canvas formed by different re-\ntrieve methods. The results are shown in Table 5. The results\ndemonstrate three important conclusions: 1) our retrieved\ncrops are more mutually compatible than other baselines; 2)\nour generator (PSGIM) can further enhance the mutual com-\npatibility, producing more realistic images than the canvas;\n3) position encoding is useful to improve the results while\nprevious methods fail to leverage such information.\nConclusion\nWe in this paper propose a novel Sequential Crop Selection\nModule (SCSM) and Progressive Scene Graph to Image Mod-\nule (PSGIM). In SCSM, the selection of the image crop for\neach object would be determined with the contents and loca-\ntions of image crops that have been chosen previously. Such\nsequential selection is implemented with a transformer that is\ntrained with contrastive learning. Hierarchical gated convolu-\ntions in the generator are employed to enhance the areas that\nare not covered by any image crops; a patch-guided spatially\nadaptive normalization module is also proposed to guarantee\nthe generated images highly respecting the crops. Evaluated\non Visual Genome and COCO-Stuff, the results demonstrate\nthe superiority of our proposed over SOTA methods.\n2944\nReferences\nAshual, O.; and Wolf, L. 2019. Specifying object attributes\nand relations in interactive scene generation. In ICCV.\nCaesar, H.; Uijlings, J.; and Ferrari, V . 2018. Coco-stuff:\nThing and stuff classes in context. In CVPR.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML.\nChen, Y .-C.; Xu, X.; and Jia, J. 2020. Domain adaptive\nimage-to-image translation. In CVPR.\nChen, Y .-C.; Xu, X.; Tian, Z.; and Jia, J. 2019. Homomor-\nphic latent space interpolation for unpaired image-to-image\ntranslation. In CVPR.\nChoi, Y .; Uh, Y .; Yoo, J.; and Ha, J.-W. 2020. Stargan v2:\nDiverse image synthesis for multiple domains. In CVPR.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei,\nL. 2009. Imagenet: A large-scale hierarchical image database.\nIn CVPR.\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial networks. In NIPS.\nHeusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and\nHochreiter, S. 2017. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In NIPS.\nHuang, H.-P.; Tseng, H.-Y .; Lee, H.-Y .; and Huang, J.-B.\n2020. Semantic view synthesis. In ECCV.\nJohnson, J.; Alahi, A.; and Fei-Fei, L. 2016. Perceptual losses\nfor real-time style transfer and super-resolution. In ECCV.\nJohnson, J.; Gupta, A.; and Fei-Fei, L. 2018. Image genera-\ntion from scene graphs. In CVPR.\nKhosla, P.; Teterwak, P.; Wang, C.; Sarna, A.; Tian, Y .; Isola,\nP.; Maschinot, A.; Liu, C.; and Krishnan, D. 2020. Supervised\ncontrastive learning. In NIPS.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for stochas-\ntic optimization. In ICLR.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.; Kravitz,\nJ.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma, D. A.; et al.\n2017. Visual genome: Connecting language and vision using\ncrowdsourced dense image annotations. IJCV.\nLee, H.-Y .; Tseng, H.-Y .; Mao, Q.; Huang, J.-B.; Lu, Y .-D.;\nSingh, M.; and Yang, M.-H. 2020. Drit++: Diverse image-to-\nimage translation via disentangled representations. IJCV.\nLi, B.; Qi, X.; Lukasiewicz, T.; and Torr, P. H. 2019a. Con-\ntrollable text-to-image generation. In NIPS.\nLi, W.; Zhang, P.; Zhang, L.; Huang, Q.; He, X.; Lyu, S.;\nand Gao, J. 2019b. Object-driven text-to-image synthesis via\nadversarial training. In CVPR.\nLi, Y .; Ma, T.; Bai, Y .; Duan, N.; Wei, S.; and Wang, X. 2019c.\nPastegan: A semi-parametric method to generate image from\nscene graph. In NIPS.\nLiu, B.; Zhu, Y .; Song, K.; and Elgammal, A. 2021. Self-\nSupervised Sketch-to-Image Synthesis. In AAAI.\nLogeswaran, L.; and Lee, H. 2018. An efÔ¨Åcient framework\nfor learning sentence representations. In ICLR.\nMao, X.; Li, Q.; Xie, H.; Lau, R. Y .; Wang, Z.; and Paul Smol-\nley, S. 2017. Least squares generative adversarial networks.\nIn ICCV.\nMirza, M.; and Osindero, S. 2014. Conditional generative\nadversarial nets. arXiv:1411.1784.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Rep-\nresentation learning with contrastive predictive coding.\narXiv:1807.03748.\nPark, T.; Liu, M.-Y .; Wang, T.-C.; and Zhu, J.-Y . 2019. Se-\nmantic image synthesis with spatially-adaptive normalization.\nIn CVPR.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A.\n2017. Automatic differentiation in pytorch.\nQiao, T.; Zhang, J.; Xu, D.; and Tao, D. 2019. Mirror-\ngan: Learning text-to-image generation by redescription. In\nCVPR.\nSalimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V .; Rad-\nford, A.; and Chen, X. 2016. Improved techniques for training\ngans. In NIPS.\nSimonyan, K.; and Zisserman, A. 2015. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR.\nSun, W.; and Wu, T. 2019. Image synthesis from reconÔ¨Åg-\nurable layout and style. In ICCV.\nSzegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,\nZ. 2016. Rethinking the inception architecture for computer\nvision. In CVPR.\nTan, Z.; Chai, M.; Chen, D.; Liao, J.; Chu, Q.; Liu, B.; Hua,\nG.; and Yu, N. 2021. Diverse Semantic Image Synthesis via\nProbability Distribution Modeling. In CVPR.\nTseng, H.-Y .; Lee, H.-Y .; Jiang, L.; Yang, M.-H.; and Yang,\nW. 2020. Retrievegan: Image synthesis via differentiable\npatch retrieval. In ECCV.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nis all you need. In NIPS.\nXu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang,\nX.; and He, X. 2018. Attngan: Fine-grained text to image\ngeneration with attentional generative adversarial networks.\nIn CVPR.\nYang, G.; Fei, N.; Ding, M.; Liu, G.; Lu, Z.; and Xiang, T.\n2021. L2M-GAN: Learning To Manipulate Latent Space\nSemantics for Facial Attribute Editing. In CVPR.\nYu, J.; Lin, Z.; Yang, J.; Shen, X.; Lu, X.; and Huang, T. S.\n2019. Free-form image inpainting with gated convolution. In\nICCV.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The unreasonable effectiveness of deep features as\na perceptual metric. In CVPR.\nZhao, B.; Meng, L.; Yin, W.; and Sigal, L. 2019. Image\ngeneration from layout. In CVPR.\nZhu, M.; Pan, P.; Chen, W.; and Yang, Y . 2019. Dm-gan:\nDynamic memory generative adversarial networks for text-\nto-image synthesis. In CVPR.\n2945",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7045815587043762
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.6033816933631897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5968508720397949
    },
    {
      "name": "Transformer",
      "score": 0.5248090028762817
    },
    {
      "name": "Graph",
      "score": 0.49919867515563965
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42323780059814453
    },
    {
      "name": "Computer vision",
      "score": 0.3647775948047638
    },
    {
      "name": "Theoretical computer science",
      "score": 0.17587953805923462
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I1306409833",
      "name": "Adobe Systems (United States)",
      "country": "US"
    }
  ]
}