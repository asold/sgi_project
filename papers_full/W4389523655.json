{
  "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
  "url": "https://openalex.org/W4389523655",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3094866536",
      "name": "Gabriel Sarch",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2096475003",
      "name": "Yue Wu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2116376189",
      "name": "Michael Tarr",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A1128290216",
      "name": "Katerina Fragkiadaki",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4294010056",
    "https://openalex.org/W2987529085",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W4287079536",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W3110701971",
    "https://openalex.org/W4308614384",
    "https://openalex.org/W4385430538",
    "https://openalex.org/W4322616103",
    "https://openalex.org/W4385574172",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W3203663566",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4385430679",
    "https://openalex.org/W4385573572",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4382362889",
    "https://openalex.org/W4287026640",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W2981344907",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4372272282",
    "https://openalex.org/W2899059606",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W125693051",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4383108296",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4309584580",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4285306713",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4214700710",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4385570707",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4293566037",
    "https://openalex.org/W3109085430",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4382603221",
    "https://openalex.org/W4320342509",
    "https://openalex.org/W2307268612",
    "https://openalex.org/W3168783492",
    "https://openalex.org/W4285102371",
    "https://openalex.org/W4312807436",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4225768122",
    "https://openalex.org/W4319323461",
    "https://openalex.org/W4362655923",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W3040041096",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4286905705",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3011144238",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4377164392",
    "https://openalex.org/W3100923070"
  ],
  "abstract": "Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x improvement over the previous SOTA for TfD. Our models, code and video results can be found in our project's website: https://helper-agent-llm.github.io.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3468–3500\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOpen-Ended Instructable Embodied Agents with\nMemory-Augmented Large Language Models\nGabriel Sarch Yue Wu Michael J. Tarr Katerina Fragkiadaki\nCarnegie Mellon University\n{gsarch,ywu5,mt01}@andrew.cmu.edu,katef@cs.cmu.edu\nhelper-agent-llm.github.io\nAbstract\nPre-trained and frozen LLMs can effectively\nmap simple scene re-arrangement instructions\nto programs over a robot’s visuomotor func-\ntions through appropriate few-shot example\nprompting. To parse open-domain natural lan-\nguage and adapt to a user’s idiosyncratic pro-\ncedures, not known during prompt engineer-\ning time, fixed prompts fall short. In this pa-\nper, we introduce HELPER, an embodied agent\nequipped with as external memory of language-\nprogram pairs that parses free-form human-\nrobot dialogue into action programs through\nretrieval-augmented LLM prompting: relevant\nmemories are retrieved based on the current\ndialogue, instruction, correction or VLM de-\nscription, and used as in-context prompt ex-\namples for LLM querying. The memory is\nexpanded during deployment to include pairs\nof user’s language and action plans, to assist\nfuture inferences and personalize them to the\nuser’s language and routines. HELPER sets a\nnew state-of-the-art in the TEACh benchmark\nin both Execution from Dialog History (EDH)\nand Trajectory from Dialogue (TfD), with 1.7x\nimprovement over the previous SOTA for TfD.\nOur models, code and video results can be\nfound in our project’s website: helper-agent-\nllm.github.io.\n1 Introduction\nParsing free-form human instructions and human-\nrobot dialogue into task plans that a robot can ex-\necute is challenging due to the open-endedness of\nenvironments and procedures to accomplish, and to\nthe diversity and complexity of language humans\nuse to communicate their desires. Human language\noften contains long-term references, questions, er-\nrors, omissions, or references and descriptions of\nroutines specific to a particular user (Tellex et al.,\n2011; Liang, 2016; Klein and Manning, 2003). In-\nstructions need to be interpreted in the environmen-\ntal context in which they are issued, and plans need\nto adapt in a closed-loop to execution failures.\nLarge Language Models (LLMs) trained on\nInternet-scale text can parse language instructions\nto task plans with appropriate plan-like or code-like\nprompts, without any finetuning of the language\nmodel, as shown in recent works (Ahn et al., 2022;\nLiang et al., 2022; Zeng et al., 2022; Huang et al.,\n2022b; Singh et al., 2022a). The state of the envi-\nronment is provided as a list of objects and their\nspatial coordinates, or as a free-form text descrip-\ntion from a vision-language model (Liang et al.,\n2022; Liu et al., 2023b; Wu et al., 2023a; Ahn et al.,\n2022). Using LLMs for task planning requires engi-\nneering a prompt that includes a description of the\ntask for the LLM to perform, a robot API with func-\ntion documentation and expressive function names,\nenvironment and task instruction inputs, and a set\nof in-context examples for inputs and outputs for\nthe task (Liang et al., 2022). These methods are\nnot trained in the domain of interest; rather they\nare prompt-engineered having in mind the domain\nat hand.\nHow can we extend LLM-prompting for seman-\ntic parsing and task planning to open-domain, free-\nform instructions, corrections, human-robot dia-\nlogue, and users’ idiosyncratic routines, not known\nat prompt engineering time? The prompts used\nfor the domain of tabletop rearrangement are al-\nready approaching the maximum context window\nof widely used LLMs (Singh et al., 2022a; Liang\net al., 2022). Even as context window size grows,\nmore prompt examples result in larger attention\noperations and cause an increase in both inference\ntime and resource usage.\nTo this end, we introduce HELPER (Human-\ninstructable Embodied Language Parsing via\nEvolving Routines), a model that uses retrieval-\naugmented situated prompting of LLMs to parse\nfree-form dialogue, instructions and corrections\nfrom humans and vision-language models to pro-\ngrams over a set of parameterized visuomotor rou-\n3468\nFigure 1: Open-ended instructable agents with retrieval-augmented LLMs. We equip LLMs with an external\nmemory of language and program pairs to retrieve in-context examples for prompts during LLM querying for task\nplans. Our model takes as input instructions, dialogue segments, corrections and VLM environment descriptions,\nretrieves relevant memories to use as in-context examples, and prompts LLMs to predict task plans and plan\nadjustments. Our agent executes the predicted plans from visual input using occupancy and semantic map building,\n3D object detection and state tracking, and active exploration using guidance from LLMs’ common sense to locate\nobjects not present in the maps. Successful programs are added to the memory paired with their language context,\nallowing for personalized subsequent interactions.\ntines. HELPER is equipped with an external non-\nparametric key-value memory of language - pro-\ngram pairs. HELPER uses its memory to retrieve\nrelevant in-context language and action program\nexamples, and generates prompts tailored to the cur-\nrent language input. HELPER expands its memory\nwith successful executions of user specific proce-\ndures; it then recalls them and adapts them in future\ninteractions with the user. HELPER uses pre-trained\nvision-language models (VLMs) to diagnose plan\nfailures in language format, and uses these to re-\ntrieve similar failure cases with solutions from its\nmemory to seed the prompt. To execute a pro-\ngram predicted by the LLM, HELPER combines\nsuccessful practices of previous home embodied\nagents, such as semantic and occupancy map build-\ning (Chaplot et al., 2020a; Blukis et al., 2022; Min\net al., 2021), LLM-based common sense object\nsearch (Inoue and Ohashi, 2022), object detection\nand tracking with off-the-shelf detectors (Chap-\nlot et al., 2020b), object attribute detection with\nVLMs (Zhang et al., 2022), and verification of ac-\ntion preconditions during execution.\nWe test HELPER on the TEACh benchmark (Pad-\nmakumar et al., 2021), which evaluates agents\nin their ability to complete a variety of long-\nhorizon household tasks from RGB input given\nnatural language dialogue between a commander\n(the instruction-giving user) and a follower (the\ninstruction-seeking user). We achieve a new state-\nof-the-art in the TEACh Execution from Dialog\nHistory and Trajectory-from-Dialogue settings, im-\nproving task success by 1.7x and goal-condition\nsuccess by 2.1x compared to prior work in TfD.\nBy further soliciting and incorporating user feed-\nback, HELPER attains an additional 1.3x boost in\ntask success. Our work is inspired by works in the\nlanguage domain (Perez et al., 2021; Schick and\nSchütze, 2020; Gao et al., 2020; Liu et al., 2021)\nthat retrieve in-context prompt examples based on\nthe input language query for NLP tasks. HELPER\nextends this capability to the domain of instructable\nembodied agents, and demonstrates the potential\nof memory-augmented LLMs for semantic parsing\nof open-ended free-form instructive language into\nan expandable library of programs.\n2 Related Work\nInstructable Embodied Agents Significant\nstrides have been made by training large neural\n3469\nnetworks to jointly map instructions and their sen-\nsory contexts to agent actions or macro-actions\nusing imitation learning (Anderson et al., 2018b;\nKu et al., 2020; Anderson et al., 2018a; Savva et al.,\n2019; Gervet et al., 2022; Shridhar et al., 2020; Cao\net al.; Suglia et al., 2021; Fan et al., 2018; Yu et al.,\n2020; Brohan et al., 2022; Stone et al., 2023; Yu\net al., 2023). Existing approaches differ—among\nothers—in the way the state of the environment\nis communicated to the model. Many methods\nmap RGB image tokens and language inputs di-\nrectly to actions or macro-actions (Pashevich et al.,\n2021; Wijmans et al., 2020; Suglia et al., 2021;\nKrantz et al., 2020). Other methods map language\ninstructions and linguistic descriptions of the envi-\nronment’s state in terms of object lists or objects\nspatial coordinates to macro-actions, foregoing vi-\nsual feature description of the scene, in a attempt to\ngeneralize better (Liang et al., 2022; Singh et al.,\n2022a; Chaplot et al., 2020a; Min et al., 2021; Liu\net al., 2022a; Murray and Cakmak, 2022; Liu et al.,\n2022b; Inoue and Ohashi, 2022; Song et al., 2022;\nZheng et al., 2022; Zhang et al., 2022; Huang et al.,\n2022b, 2023; Ahn et al., 2022; Zeng et al., 2022;\nHuang et al., 2022a). Some of these methods fine-\ntune language models to map language input to\nmacro-actions, while others prompt frozen LLMs\nto predict action programs, relying on the emergent\nin-context learning property of LLMs to emulate\nnovel tasks at test time. Some methods use natural\nlanguage as the output format of the LLM (Wu\net al., 2023a; Song et al., 2022; Blukis et al., 2022;\nHuang et al., 2022b), and others use code format\n(Singh et al., 2022a; Liang et al., 2022; Huang et al.,\n2023). HELPER prompts frozen LLMs to predict\nPython programs over visuo-motor functions for\nparsing dialogue, instructions and corrective hu-\nman feedback.\nThe work closest to HELPER is LLM Plan-\nner (Song et al., 2022) which uses memory-\naugmented prompting of pretrained LLMs for in-\nstruction following. However, it differs from\nHELPER in several areas such as plan memory\nexpansion, VLM-guided correction, and usage\nof LLMs for object search. Furthermore, while\nSingh et al. (2022b) frequently seeks human feed-\nback, HELPER requests feedback only post full task\nexecution and employs Visual-Language Models\n(VLMs) for error feedback, reducing user interrup-\ntions.\nNumerous simulation environments exist for\nFigure 2: HELPER’s architecture. The model uses\nmemory-augmented LLM prompting for task planning\nfrom instructions, corrections and human-robot dialogue\nand for re-planning during failures given feedback from\na VLM model. The generated program is executed the\nExecutor module. The Executor builds semantic,\noccupancy and 3D object maps, tracks object states, ver-\nifies action preconditions, and queries LLMs for search\nlocations for objects missing from the maps, using the\nLocatormodule.\nevaluating home assistant frameworks, including\nHabitat (Savva et al., 2019), GibsonWorld (Shen\net al., 2021), ThreeDWorld (Gan et al., 2022), and\nAI2THOR (Kolve et al., 2017). ALFRED (Shrid-\nhar et al., 2020) and TEACh (Padmakumar et al.,\n2021) are benchmarks in the AI2THOR environ-\nment (Kolve et al., 2017), measuring agents’ com-\npetence in household tasks through natural lan-\nguage. Our research focuses on the ‘Trajectory\nfrom Dialogue’ (TfD) evaluation in TEACh, mir-\nroring ALFRED but with greater task and input\ncomplexity.\nPrompting LLMs for action prediction and vi-\nsual reasoning Since the introduction of few-\nshot prompting by (Brown et al., 2020), several\napproaches have improved the prompting ability of\nLLMs by automatically learning prompts (Lester\net al., 2021), chain of thought prompting (Nye\net al.; Gao et al., 2022; Wei et al., 2022; Wang\net al., 2022; Chen et al., 2022; Yao et al., 2023)\nand retrieval-augmented LLM prompting (Nakano\net al., 2021; Shi et al., 2023; Jiang et al., 2023)\nfor language modeling, question answering, and\nlong-form, multi-hop text generation. HELPER uses\nmemory-augmented prompting by retrieving and\nintegrating similar task plans into the prompt to\nfacilitate language parsing to programs.\nLLMs have been used as policies in Minecraft to\npredict actions (Wang et al., 2023b,a), error cor-\nrection (Liu et al., 2023b), and for understand-\n3470\ning instruction manuals for game play in some\nAtari games (Wu et al., 2023b). They have also\nsignificantly improved text-based agents in text-\nbased simulated worlds (Yao et al., 2022; Shinn\net al., 2023; Wu et al., 2023c; Richards, 2023).\nViperGPT (Surís et al., 2023), and CodeVQA (Sub-\nramanian et al., 2023) use LLM prompting to de-\ncompose referential expressions and questions to\nprograms over simpler visual routines. Our work\nuses LLMs for planning from free-form dialogue\nand user corrective feedback for home task com-\npletion, a domain not addressed in previous works.\n3 Method\nHELPER is an embodied agent designed to map\nhuman-robot dialogue, corrections and VLM de-\nscriptions to actions programs over a fixed API of\nparameterized navigation and manipulation prim-\nitives. Its architecture is outlined in Figure 2.\nAt its heart, it generates plans and plan adjust-\nments by querying LLMs using retrieval of relevant\nlanguage-program pairs to include as in-context ex-\namples in the LLM prompt. The generated pro-\ngrams are then sent to the Executor module,\nwhich translates each program step into specific\nnavigation and manipulation action. Before exe-\ncuting each step in the program, the Executor\nverifies if the necessary preconditions for an action,\nsuch as the robot already holding an object, are\nmet. If not, the plan is adjusted according to the\ncurrent environmental and agent state. Should a\nstep involve an undetected object, the Executor\ncalls on the Locatormodule to efficiently search\nfor the required object by utilizing previous user\ninstructions and LLMs’ common sense knowledge.\nIf any action fails during execution, a VLM pre-\ndicts the reason for this failure from pixel input and\nfeeds this into the Planner for generating plan\nadjustments.\n3.1 Planner: Retrieval-Augmented LLM\nPlanning\nGiven an input I consisting of a dialogue segment,\ninstruction, or correction, HELPER uses memory-\naugmented prompting of frozen LLMs to map\nthe input into an executable Python program over\na parametrized set of manipulation and naviga-\ntion primitives G ∈{Gmanipulation ∪Gnavigation}\nthat the Executor can perform (e.g., goto(X),\npickup(X), slice(X), ...). Our action API can be\nfound in Section D of the Appendix.\nHELPER maintains a key-value memory of lan-\nguage - program pairs, as shown in Figure 3A. Each\nlanguage key is mapped to a 1D vector using an\nLLM’s frozen language encoder. Given current\ncontext I, the model retrieves the top-K keys, i.e.,\nthe keys with the smallest L2 distance with the\nembedding of the input context I, and adds the cor-\nresponding language - program pairs to the LLM\nprompt as in-context examples for parsing the cur-\nrent input I.\nFigure 3B illustrates the prompt format for the\nPlanner. It includes the API specifying the\nprimitives G parameterized as Python functions,\nthe retrieved examples, and the language input I.\nThe LLM is tasked to generate a Python program\nover parameterized primitives G. Examples of our\nprompts and LLM responses can be found in Sec-\ntion F of the Appendix.\n3.1.1 Memory Expansion\nThe key-value memory of HELPER can be contin-\nually expanded with successful executions of in-\nstructions to adapt to a user’s specific routines, as\nshown in Figure 1. An additional key-value pair\nis added with the language instruction paired with\nthe execution plan if the user indicates the task was\nsuccessful. Then, HELPER can recall this plan and\nadapt it in subsequent interactions with the user.\nFor example, if a user instructs HELPER one day to\n\"Perform the Mary cleaning. This involves clean-\ning two plates and two cups in the sink\", the user\nneed only say \"Do the Mary cleaning\" in future\ninteractions, and HELPER will retrieve the previ-\nous plan, include it in the examples section of the\nprompt, and query the LLM to adapt it accord-\ningly. The personalization capabilities of HELPER\nare evaluated in Section 4.4.\n3.1.2 Incorporating user feedback\nA user’s feedback can improve a robot’s perfor-\nmance, but requesting feedback frequently can de-\nteriorate the overall user experience. Thus, we\nenable HELPER to elicit user feedback only when it\nhas completed execution of the program. Specifi-\ncally, it asks “Is the task completed to your satis-\nfaction? Did I miss anything?\" once it believes it\nhas completed the task. The user responds either\nthat the task has been completed (at which point\nHELPER stops acting) or points out problems and\ncorrections in free-form natural language, such as,\n“You failed to cook a slice of potato. The potato slice\nneeds to be cooked.\". HELPER uses the language\n3471\nFigure 3: HELPER parses dialogue segments, instructions, and corrections into visuomotor programs using retrieval-\naugmented LLM prompting. A. Illustration of the encoding and memory retrieval process. B. Prompt format and\noutput of the Planner.\nfeedback to re-plan using the Planner. We eval-\nuate HELPER in its ability to seek and utilize user\nfeedback in Section 4.3.\n3.1.3 Visually-Grounded Plan Correction\nusing Vision-Language Models\nGenerated programs may fail for various reasons,\nsuch as when a step in the plan is missed or\nan object-of-interest is occluded. When the pro-\ngram fails, HELPER uses a vision-language model\n(VLM) pre-trained on web-scale data, specifically\nthe ALIGN model (Jia et al., 2021), to match the\ncurrent visual observation with a pre-defined list of\ntextual failure cases, such as an object is blocking\nyou from interacting with the selected object , as\nillustrated in Figure 4. The best match is taken to\nbe the failure feedback F. The Planner module\nthen retrieves the top- K most relevant error cor-\nrection examples, each containing input dialogue,\nfailure feedback, and the corresponding corrective\nprogram, from memory based on encodings of in-\nput I and failure feedback F from the VLM. The\nLLM is prompted with the the failed program step,\nthe predicted failure description F from the VLM,\nthe in-context examples, and the original dialogue\nsegment I. The LLM outputs a self-reflection as\nto why the failure occurred, and generates a pro-\ngram over manipulation and navigation primitives\nG, and an additional set of corrective primitives\nGcorrective (e.g., step-back(), move-to-an-alternate-\nviewpoint(), ...). This program is sent to the Ex-\necutor for execution.\nFigure 4: Inference of a failure feedback description\nby matching potential failure language descriptions\nwith the current image using a vision-language model\n(VLM).\n3.2 Executor: Scene Perception,\nPre-Condition Checks, Object Search and\nAction Execution\nThe Executor module executes the predicted\nPython programs in the environment, converting\nthe code into low-level manipulation and naviga-\ntion actions, as shown in Figure 2. At each time\nstep, the Executor receives an RGB image and\nobtains an estimated depth map via monocular\ndepth estimation (Bhat et al., 2023) and object\nmasks via an off-the-shelf object detector (Dong\net al., 2021).\n3.2.1 Scene and object state perception\nUsing the depth maps, object masks, and approxi-\nmate egomotion of the agent at each time step, the\nExecutor maintains a 3D occupancy map and ob-\nject memory of the home environment to navigate\naround obstacles and keep track of previously seen\nobjects, similar to previous works (Sarch et al.,\n2022). Objects are detected in every frame and\nare merged into object instances based on close-\nness of the predicted 3D centroids. Each object\ninstance is initialized with a set of object state at-\n3472\ntributes (cooked, sliced, dirty, ...) by matching\nthe object crop against each attribute with the pre-\ntrained ALIGN model (Jia et al., 2021). Object\nattribute states are updated when an object is acted\nupon via a manipulation action.\n3.2.2 Manipulation and navigation\npre-condition checks\nThe Executor module verifies the pre-conditions\nof an action before the action is taken to ensure\nthe action is likely to succeed. In our case, these\nconstraints are predefined for each action (for exam-\nple, the agent must first be holding a knife to slice\nan object). If any pre-conditions are not satisfied,\nthe Executor adjusts the plan accordingly. In\nmore open-ended action interfaces, an LLM’s com-\nmon sense knowledge can be used to infer the pre-\nconditions for an action, rather than pre-defining\nthem.\n3.2.3 Locator: LLM-based common sense\nobject search\nWhen HELPER needs to find an object that has not\nbeen detected before, it calls on the Locatormod-\nule. The Locator prompts an LLM to suggest\npotential object search location for the Executor\nto search nearby, e.g. “search near the sink\" or\n“search in the cupboard\". The Locator prompt\ntakes in the language I (which may reference the\nobject location, e.g., “take the mug from the cup-\nboard\" ) and queries the LLM to generate proposed\nlocations by essentially parsing the instruction as\nwell as using its common sense. Based on these\npredictions, HELPER will go to the suggested lo-\ncations if they exist in the semantic map (e.g., to\nthe sink) and search for the object-of-interest. The\nLocator’s prompt can be found in Section D of\nthe Appendix.\nImplementation details. We use OpenAI’s\ngpt-4-0613 (gpt, 2023) API, except when\nmentioned otherwise. We resort to the\ntext-embedding-ada-002 (ada, 2022) API to ob-\ntain text embeddings. Furthermore, we use the\nSOLQ object detector (Dong et al., 2021), which is\npretrained on MSCOCO (Lin et al., 2014) and fine-\ntuned on the training rooms of TEACh. For monoc-\nular depth estimation, we use the ZoeDepth net-\nwork (Bhat et al., 2023), pretrained on the NYU in-\ndoor dataset (Nathan Silberman and Fergus, 2012)\nand subsequently fine-tuned on the training rooms\nof TEACh. In the TEACh evaluations, we useK=3\nfor retrieval.\n4 Experiments\nWe test HELPER in the TEACh benchmark (Pad-\nmakumar et al., 2021). Our experiments aim to\nanswer the following questions:\n1. How does HELPER compare to the SOTA on\ntask planning and execution from free-form\ndialogue?\n2. How much do different components of\nHELPER contribute to performance?\n3. How much does eliciting human feedback\nhelp task completion?\n4. How effectively doesHELPER adapt to a user’s\nspecific procedures?\n4.1 Evaluation on the TEACh dataset\nDataset The dataset comprises over 3,000 hu-\nman–human, interactive dialogues, geared towards\ncompleting household tasks within the AI2-THOR\nsimulation environment (Kolve et al., 2017). We\nevaluate on the Trajectory from Dialogue (TfD)\nevaluation variant, where the agent is given a di-\nalogue segment at the start of the episode. The\nmodel is then tasked to infer the sequence of ac-\ntions to execute based on the user’s intents in the\ndialogue segment, ranging from MAKE COFFEE\nto PREPARE BREAKFAST . We show examples of\nsuch dialogues in Figures 1 & 3. We also test on\nthe Execution from Dialogue History (EDH) task\nin TEACh, where the TfD episodes are partitioned\ninto \"sessions\". The agent is spawned at the start of\none of the sessions and must predict the actions to\nreach the next session given the dialogue and action\nhistory of the previous sessions. The dataset is split\ninto training and validation sets. The validation set\nis divided into ’seen’ and ’unseen’ rooms based on\ntheir presence in the training set. Validation ’seen’\nhas the same room instances but different object\nlocations and initial states than any episodes in the\ntraining set. At each time step, the agent obtains an\negocentric RGB image and must choose an action\nfrom a specified set to transition to the next step,\nsuch as pickup(X), turn left(), etc. Please see Ap-\npendix Section G for more details on the simulation\nenvironment.\nEvaluation metrics Following evaluation prac-\ntises for the TEACh benchmark, we use the follow-\ning two metrics: 1. Task success rate (SR), which\nrefers to the fraction of task sessions in which the\n3473\nNo Memory- User Accepts VLM- LLM- Pre-\nIn-Domain Augmented Personal- User Guided Guided Condition\nLLM LLM ization Feedback correction Search Check\nE.T.(Pashevich et al., 2021) x x x x x x x\nJARVIS(Zheng et al., 2022) x x x x x x x\nFILM(Min et al., 2021, 2022) x x x x x x x\nDANLI(Zhang et al., 2022) x x x x x x /Check-mark\nLLM-Planner(Song et al., 2022) /Check-mark/Check-markx x x x x\nCode as Policies(Liang et al., 2022) /Check-markx x x x x x\nHELPER (ours) /Check-mark/Check-mark/Check-mark/Check-mark/Check-mark/Check-mark/Check-mark\nTable 1: Comparison of HELPER to previous work.\nagent successfully fulfills all goal conditions. 2.\nGoal condition success rate (GC), which quan-\ntifies the proportion of achieved goal conditions\nacross all sessions. Both of these metrics have cor-\nresponding path length weighted (PLW) variants.\nIn these versions, the agent incurs penalties for ex-\necuting a sequence of actions that surpasses the\nlength of the reference path annotated by human\nexperts.\nBaselines We consider the following baselines:\n1. Episodic Transformer (E.T.)(Pashevich et al.,\n2021) is an end-to-end multimodal transformer that\nencodes language inputs and a history of visual ob-\nservations to predict actions, trained with imitation\nlearning from human demonstrations.\n2. Jarvis (Zheng et al., 2022) trains an LLM on\nthe TEACh dialogue to generate high-level sub-\ngoals that mimic those performed by the human\ndemonstrator. Jarvis uses a semantic map and the\nEpisodic Transformer for object search.\n3. FILM (Min et al., 2021, 2022) fine-tunes an\nLLM to produce parametrized plan templates. Sim-\nilar to Jarvis, FILM uses a semantic map for carry-\ning out subgoals and a semantic policy for object\nsearch.\n4. DANLI (Zhang et al., 2022) fine-tunes an LLM\nto predict high-level subgoals, and uses symbolic\nplanning over an object state and spatial map to cre-\nate an execution plan. DANLI uses an object search\nmodule and manually-defined error correction.\nHELPER differs from the baselines in its use of\nmemory-augmented context-dependent prompting\nof pretrained LLMs and pretrained visual-language\nmodels for planning, failure diagnosis and recovery,\nand object search. We provide a more in-depth\ncomparison of HELPER to previous work in Table 1.\nEvaluation We show quantitative results for\nHELPER and the baselines on the TEACh Trajec-\ntory from Dialogue (TfD) and Execution from Di-\nalogue History (EDH) validation split in Table 2.\nOn the TfD validation unseen, HELPER achieves\na 13.73% task success rate and 14.17% goal-\ncondition success rate, a relative improvement\nof 1.7x and 2.1x, respectively, over DANLI, the\nprior SOTA in this setting. HELPER additionally\nsets a new SOTA in the EDH task, achieving\na 17.40% task success rate and 25.86% goal-\ncondition success rate on validation unseen.\n4.2 Ablations\nWe ablate components of HELPER in order to quan-\ntify what matters for performance in Table 2 Ab-\nlations. We perform all ablations on the TEACh\nTfD validation unseen split. We draw the following\nconclusions:\n1. Retrieval-augmented prompting helps for\nplanning, re-planning and failure recovery. Replac-\ning the memory-augmented prompts with a fixed\nprompt (w/o Mem Aug; Table 2) led to a relative\n18% reduction in success rate.\n2. VLM error correction helps the agent recover\nfrom failures. Removal of the visually-grounded\nplan correction (w/o Correction; Table 2) led to a\nrelative 6% reduction in success rate.\n3. The pre-condition check and the LLM search\nhelp. Removal of the action pre-condition checks\n(w/o Pre Check; Table 2) led to a relative 16% re-\nduction in success rate. Replacing the Locator\nLLM-based search with a random search (w/o Lo-\ncator; Table 2) led to a relative 12% reduction in\nsuccess rate.\n4. Larger LLMs perform better. Using GPT-3.5\n(w GPT-3.5; Table 2) exhibits a relative 31% re-\nduction in success rate compared to using GPT-4.\nOur findings on GPT-4’s superior planning abilities\nalign with similar findings from recent studies of\nWu et al. (2023d); Bubeck et al. (2023); Liu et al.\n(2023a); Wang et al. (2023a).\n5. Perception is a bottleneck. Using GT depth\n3474\nTable 2: Trajectory from Dialogue (TfD) and Execution from Dialog History (EDH) evaluation on the TEACh\nvalidation set. Trajectory length weighted metrics are included in ( parentheses ). SR = success rate. GC = goal\ncondition success rate.\nTfD EDH\nUnseen Seen Unseen Seen\nSR GC SR GC SR GC SR GC\nE.T. 0.48 (0.12) 0.35 (0.59) 1.02 (0.17) 1.42 (4.82) 7.8 (0.9) 9.1 (1.7) 10.2 (1.7) 15.7 (4.1)\nJARVIS 1.80 (0.30) 3.10 (1.60) 1.70 (0.20) 5.40 (4.50) 15.80 (2.60) 16.60 (8.20) 15.10 (3.30) 22.60 (8.70)\nFILM 2.9 (1.0) 6.1 (2.5) 5.5 (2.6) 5.8 (11.6) 10.2 (1.0) 18.3 (2.7) 14.3 (2.1) 26.4 (5.6)\nDANLI 7.98 (3.20) 6.79 (6.57) 4.97 (1.86) 10.50 (10.27) 16.98 (7.24) 23.44 (19.95) 17.76 (9.28) 24.93 (22.20)\nHELPER (ours) 13.73 (1.61) 14.17 (4.56) 12.15 (1.79) 18.62 (9.28) 17.40 (2.91) 25.86 (7.90) 18.59 (4.00) 32.09 (9.81)\nAblations\nw/o Mem Aug 11.27 (1.39) 11.09 (4.00)\nw/o Pre Check 11.6 (1.36) 11.32 (4.15)\nw/o Correction 12.9 (1.53) 12.45 (4.91)\nw/o Locator 12.09 (1.29) 10.89 (3.83)\nw/ GPT-3.5 9.48 (1.21) 10.05 (3.68)\nw/ GT depth 15.85 (2.85) 14.49 (6.89)\nw/ GT depth,seg22.55 (6.39) 30.00 (14.56)\nw/ GT percept 30.23 (9.12) 50.46 (20.24)\nUser Feedback\nw/ Feedback 1 16.34 (1.67) 14.70 (4.69)\nw/ Feedback 2 17.48 (1.97) 14.93 (4.74)\nw/ GT percept, 37.75 (10.96)56.77 (19.80)\nFeedback 2\n(w/ GT depth; Table 2) led to an improvement\nof 1.15x compared to using estimated depth from\nRGB. Notable is the 1.77x improvement in path-\nlength weighted success when using GT depth.\nThis change is due to lower accuracy for far depths\nin our depth estimation network lower, thereby\ncausing the agent to spend more time mapping the\nenvironment and navigating noisy obstacle maps.\nUsing lidar or better map estimation techniques\ncould mitigate this issue.\nUsing ground truth segmentation masks and\ndepth (w/ GT depth, seg; Table 2) improves task\nsuccess and goal-conditioned task success by 1.64x\nand 2.11x, respectively. This shows the limitations\nof frame-based object detection and late fusion of\ndetection responses over time. 3D scene represen-\ntations that fuse features earlier across views may\nsignificantly improve 3D object detection. Using\nGT perception (w/ GT percept; Table 2), which in-\ncludes depth, segmentation, action success, oracle\nfailure feedback, and increased API failure limit\n(50), led to 2.20x and 3.56x improvement.\n4.3 Eliciting Users’ Feedback\nWe enable HELPER to elicit sparse user feedback\nby asking “Is the task completed to your satisfac-\ntion? Did I miss anything?\" once it believes it has\ncompleted the task, as explained in Section 3.1.2.\nThe user will then respond with steps missed by\nHELPER, and HELPER will re-plan based on this\nfeedback. As shown in in Table 2 User Feedback,\nasking for a user’s feedback twice improves perfor-\nmance by 1.27x. Previous works do not explore this\nopportunity of eliciting human feedback partly due\nto the difficulty of interpreting it—being free-form\nlanguage—which our work addresses.\n4.4 Personalization\nWe evaluate HELPER’s ability to retrieve user-\nspecific routines, as well as on their ability to mod-\nify the retrieved routines, with one, two, or three\nmodifications, as discussed in 3.1.1. For example,\nfor three modifications we might instruct HELPER:\n\"Make me a Dax sandwich with 1 slice of tomato,\n2 lettuce leaves, and add a slice of bread\".\nDataset The evaluation tests 10 user-specific\nplans for each modification category in five distinct\ntasks: MAKE A SANDWICH ; PREPARE BREAK -\nFAST ; MAKE A SALAD ; PLACE X ON Y; and\nCLEAN X. The evaluation contains 40 user re-\nquests. The complete list of user-specific plans\nand modification requests can be found in the Ap-\npendix, Section C.\nEvaluation We report the success rate in Table 3.\nHELPER generates the correct personalized plan\nfor all but three instances, out of 40 evaluation\nrequests. This showcases the ability of HELPER to\nacquire, retrieve and adapt plans based on context\nand previous user interactions.\n5 Limitations\nOur model in its current form has the following\nlimitations:\n1. Simplified failure detection. The AI2-THOR\nsimulator much simplifies action failure detection\nwhich our work and previous works exploit (Min\net al., 2021; Inoue and Ohashi, 2022). In a more\ngeneral setting, continuous progress monitoring\n3475\nTable 3: Evaluation of HELPER for user personaliza-\ntion. Reported is success of generating the correct plan\nfor 10 personalized plans for a request of the original\nplan without modifications, and one, two, or three modi-\nfications to the original plan. These experiments use the\ntext-davinci-003 model as the prompted LLM.\nSuccess\nOriginal Plan 100%\nOne Change 100%\nTwo Changes 80%\nThree Changes 90%\nfrom pixels would be required for failure detection,\nwhich model VLMs can deliver and we will address\nin future work.\n2. 3D perception bottleneck. HELPER relies on\n2D object detectors and depth 3D lifting for 3D ob-\nject localization. We observe a 2X boost in TEACh\nsuccess rate from using ground truth segmentation\nin HELPER. In future work we plan to integrate\nearly 2D features into persistent 3D scene feature\nrepresentations for more accurate 3D object detec-\ntion.\n4. Cost from LLM querying. GPT-4 API is the\nmost accurate LLM used in HELPER and incurs a\nsignificant cost. NLP research in model compres-\nsion may help decreasing these costs, or finetuning\nsmaller models with enough input-output pairs.\n3. Multimodal (vision and language) memory\nretrieval. Currently, we use a text bottleneck in\nour environment state descriptions. Exciting fu-\nture directions include exploration of visual state\nincorporation to the language model and partial\nadaptation of its parameters. A multi-modal ap-\nproach to the memory and plan generation would\nhelp contextualize the planning more with the vi-\nsual state.\nLast, to follow human instructions outside of\nsimulation environments our model would need to\ninterface with robot closed-loop policies instead of\nabstract manipulation primitives, following previ-\nous work (Liang et al., 2022).\n6 Conclusion\nWe presented HELPER, an instructable embodied\nagent that uses memory-augmented prompting of\npre-trained LLMs to parse dialogue segments, in-\nstructions and corrections to programs over action\nprimitives, that it executes in household environ-\nments from visual input. HELPER updates its mem-\nory with user-instructed action programs after suc-\ncessful execution, allowing personalized interac-\ntions by recalling and adapting them. It sets a new\nstate-of-the-art in the TEACh benchmark. Future\nresearch directions include extending the model\nto include a visual modality by encoding visual\ncontext during memory retrieval or as direct in-\nput to the LLM. We believe our work contributes\ntowards exciting new capabilities for instructable\nand conversable systems, for assisting users and\npersonalizing human-robot communication.\n7 Acknowledgements\nThis material is based upon work supported\nby National Science Foundation grants GRF\nDGE1745016 & DGE2140739 (GS), a DARPA\nYoung Investigator Award, a NSF CAREER award,\nan AFOSR Young Investigator Award, and DARPA\nMachine Common Sense, and an ONR award\nAWD00002287 . Any opinions, findings and con-\nclusions or recommendations expressed in this ma-\nterial are those of the authors and do not necessarily\nreflect the views of the United States Army, the Na-\ntional Science Foundation, or the United States Air\nForce.\nThis research project has benefitted from the Mi-\ncrosoft Accelerate Foundation Models Research\n(AFMR) grant program through which leading\nfoundation models hosted by Microsoft Azure\nalong with access to Azure credits were provided\nto conduct the research.\nEthics Statement\nThe objective of this research is to construct au-\ntonomous agents. Despite the absence of human\nexperimentation, practitioners could potentially im-\nplement this technology in human-inclusive envi-\nronments. Therefore, applications of our research\nshould appropriately address privacy considera-\ntions.\nAll the models developed in this study were\ntrained using Ai2Thor (Kolve et al., 2017). Con-\nsequently, there might be an inherent bias towards\nNorth American homes. Additionally, we only con-\nsider English language inputs in this study.\n3476\nReferences\n2022. New and improved embedding model.\n2023. Openai. gpt-4 technical report. arXiv preprint\narxiv:2303.08774.\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can\nand not as i say: Grounding language in robotic af-\nfordances. In arXiv preprint arXiv:2204.01691.\nPeter Anderson, Angel Chang, Devendra Singh Chap-\nlot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen\nKoltun, Jana Kosecka, Jitendra Malik, Roozbeh Mot-\ntaghi, Manolis Savva, et al. 2018a. On evalua-\ntion of embodied navigation agents. arXiv preprint\narXiv:1807.06757.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko Sünderhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. 2018b. Vision-\nand-language navigation: Interpreting visually-\ngrounded navigation instructions in real environ-\nments. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3674–\n3683.\nShariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter\nWonka, and Matthias Müller. 2023. Zoedepth: Zero-\nshot transfer by combining relative and metric depth.\narXiv preprint arXiv:2302.12288.\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. 2022. A persistent spatial semantic\nrepresentation for high-level natural language instruc-\ntion execution. In Conference on Robot Learning ,\npages 706–717. PMLR.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jas-\nmine Hsu, et al. 2022. Rt-1: Robotics transformer\nfor real-world control at scale. arXiv preprint\narXiv:2212.06817.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nYuchen Cao, Nilay Pande, Ayush Jain, Shikhar Sharma,\nGabriel Sarch, Nikolaos Gkanatsios, Xian Zhou, and\nKaterina Fragkiadaki. Embodied symbiotic assis-\ntants that see, act, infer and chat.\nDevendra Singh Chaplot, Dhiraj Gandhi, Saurabh\nGupta, Abhinav Gupta, and Ruslan Salakhutdinov.\n2020a. Learning to explore using active neural slam.\narXiv preprint arXiv:2004.05155.\nDevendra Singh Chaplot, Dhiraj Prakashchand Gandhi,\nAbhinav Gupta, and Russ R Salakhutdinov. 2020b.\nObject goal navigation using goal-oriented semantic\nexploration. Advances in Neural Information Pro-\ncessing Systems, 33.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nBin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang,\nand Yichen Wei. 2021. Solq: Segmenting objects by\nlearning queries. Advances in Neural Information\nProcessing Systems, 34:21898–21909.\nLinxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng,\nAnchit Gupta, Joan Creus-Costa, Silvio Savarese,\nand Li Fei-Fei. 2018. Surreal: Open-source rein-\nforcement learning framework and robot manipula-\ntion benchmark. In Conference on Robot Learning,\npages 767–782. PMLR.\nChuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter,\nAbhishek Bhandwaldar, Dan Gutfreund, Daniel L.K.\nYamins, James J. DiCarlo, Josh McDermott, Antonio\nTorralba, and Joshua B. Tenenbaum. 2022. The three-\ndworld transport challenge: A visually guided task-\nand-motion planning benchmark towards physically\nrealistic embodied ai. In 2022 International Con-\nference on Robotics and Automation (ICRA), pages\n8847–8854.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nTheophile Gervet, Soumith Chintala, Dhruv Batra, Jiten-\ndra Malik, and Devendra Singh Chaplot. 2022. Nav-\nigating to objects in the real world. arXiv preprint\narXiv:2212.00922.\n3477\nChenguang Huang, Oier Mees, Andy Zeng, and Wol-\nfram Burgard. 2022a. Visual language maps for robot\nnavigation. arXiv preprint arXiv:2210.05714.\nSiyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao,\nPeng Gao, and Hongsheng Li. 2023. Instruct2act:\nMapping multi-modality instructions to robotic ac-\ntions with large language model. arXiv preprint\narXiv:2305.11176.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan,\nJacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al.\n2022b. Inner monologue: Embodied reasoning\nthrough planning with language models. arXiv\npreprint arXiv:2207.05608.\nYuki Inoue and Hiroki Ohashi. 2022. Prompter: Utiliz-\ning large language model prompting for a data effi-\ncient embodied instruction following. arXiv preprint\narXiv:2211.03267.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision. In International Conference on\nMachine Learning, pages 4904–4916. PMLR.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation.\nDan Klein and Christopher D Manning. 2003. Accurate\nunlexicalized parsing. In Proceedings of the 41st\nannual meeting of the association for computational\nlinguistics, pages 423–430.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon,\nYuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017.\nAI2-THOR: An Interactive 3D Environment for Vi-\nsual AI. arXiv.\nJacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv\nBatra, and Stefan Lee. 2020. Beyond the nav-graph:\nVision-and-language navigation in continuous envi-\nronments. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XXVIII 16, pages 104–120.\nSpringer.\nAlexander Ku, Peter Anderson, Roma Patel, Eugene\nIe, and Jason Baldridge. 2020. Room-across-room:\nMultilingual vision-and-language navigation with\ndense spatiotemporal grounding. arXiv preprint\narXiv:2010.07954.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy\nZeng. 2022. Code as policies: Language model\nprograms for embodied control. arXiv preprint\narXiv:2209.07753.\nPercy Liang. 2016. Learning executable semantic\nparsers for natural language understanding. Com-\nmunications of the ACM, 59(9):68–76.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision–\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740–755. Springer.\nHaoyu Liu, Yang Liu, Hongkai He, and Hangfang Yang.\n2022a. Lebp–language expectation & binding pol-\nicy: A two-stream framework for embodied vision-\nand-language interaction task learning agents. arXiv\npreprint arXiv:2203.04637.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nXiaotian Liu, Hector Palacios, and Christian Muise.\n2022b. A planning based neural-symbolic approach\nfor embodied instruction following. Interactions,\n9(8):17.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023a. Sum-\nmary of chatgpt/gpt-4 research and perspective to-\nwards the future of large language models. arXiv\npreprint arXiv:2304.01852.\nZeyi Liu, Arpit Bahety, and Shuran Song. 2023b.\nReflect: Summarizing robot experiences for fail-\nure explanation and correction. arXiv preprint\narXiv:2306.15724.\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. 2021.\nFilm: Following instructions in language with modu-\nlar methods.\nSo Yeon Min, Hao Zhu, Ruslan Salakhutdinov, and\nYonatan Bisk. 2022. Don’t copy the teacher: Data\nand model challenges in embodied dialogue. arXiv\npreprint arXiv:2210.04443.\nMichael Murray and Maya Cakmak. 2022. Follow-\ning natural language instructions for household tasks\nwith landmark guided search and reinforced pose ad-\njustment. IEEE Robotics and Automation Letters ,\n7(3):6870–6877.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\n3478\nPushmeet Kohli Nathan Silberman, Derek Hoiem and\nRob Fergus. 2012. Indoor segmentation and support\ninference from rgbd images. In ECCV.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-\nAri, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten\nBosma, David Luan, et al. Show your work:\nScratchpads for intermediate computation with lan-\nguage models, november 2021. URL http://arxiv.\norg/abs/2112.00114.\nAishwarya Padmakumar, Jesse Thomason, Ayush Shri-\nvastava, Patrick Lange, Anjali Narayan-Chen, Span-\ndana Gella, Robinson Piramuthu, Gokhan Tur, and\nDilek Hakkani-Tur. 2021. Teach: Task-driven em-\nbodied agents that chat.\nAlexander Pashevich, Cordelia Schmid, and Chen Sun.\n2021. Episodic transformer for vision-and-language\nnavigation. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 15942–\n15952.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. Ad-\nvances in neural information processing systems ,\n34:11054–11070.\nToran Bruce Richards. 2023. Auto-gpt: An autonomous\ngpt-4 experiment.\nGabriel Sarch, Zhaoyuan Fang, Adam W Harley, Paul\nSchydlo, Michael J Tarr, Saurabh Gupta, and Ka-\nterina Fragkiadaki. 2022. Tidee: Tidying up novel\nrooms using visuo-semantic commonsense priors. In\nComputer Vision–ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23–27, 2022, Pro-\nceedings, Part XXXIX, pages 480–496. Springer.\nManolis Savva, Abhishek Kadian, Oleksandr\nMaksymets, Yili Zhao, Erik Wijmans, Bhavana\nJain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra\nMalik, et al. 2019. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n9339–9347.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nBokui Shen, Fei Xia, Chengshu Li, Roberto Martín-\nMartín, Linxi Fan, Guanzhi Wang, Claudia Pérez-\nD’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P.\nTchapmi, Micael E. Tchapmi, Kent Vainio, Josiah\nWong, Li Fei-Fei, and Silvio Savarese. 2021. igibson\n1.0: a simulation environment for interactive tasks\nin large realistic scenes. In 2021 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems,\npage accepted. IEEE.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023. Reflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. Alfred: A bench-\nmark for interpreting grounded instructions for ev-\neryday tasks. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 10740–10749.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. 2022a.\nProgprompt: Generating situated robot task plans\nusing large language models. arXiv preprint\narXiv:2209.11302.\nKunal Pratap Singh, Luca Weihs, Alvaro Herrasti,\nJonghyun Choi, Aniruddha Kembhavi, and Roozbeh\nMottaghi. 2022b. Ask4help: Learning to leverage\nan expert for embodied tasks. Advances in Neural\nInformation Processing Systems, 35:16221–16232.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M. Sadler, Wei-Lun Chao, and Yu Su. 2022.\nLlm-planner: Few-shot grounded planning for em-\nbodied agents with large language models. arXiv\npreprint arXiv:2212.04088.\nAustin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrish-\nnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart,\nSean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea\nFinn, and Karol Hausman. 2023. Open-world ob-\nject manipulation using pre-trained vision-language\nmodel. In arXiv preprint.\nSanjay Subramanian, Medhini Narasimhan, Kushal\nKhangaonkar, Kevin Yang, Arsha Nagrani, Cordelia\nSchmid, Andy Zeng, Trevor Darrell, and Dan Klein.\n2023. Modular visual question answering via code\ngeneration. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics,\nToronto, Canada. Association for Computational Lin-\nguistics.\nAlessandro Suglia, Qiaozi Gao, Jesse Thomason,\nGovind Thattai, and Gaurav S. Sukhatme. 2021. Em-\nbodied bert: A transformer model for embodied,\nlanguage-guided visual task completion. In EMNLP\n2021 Workshop on Novel Ideas in Learning-to-Learn\nthrough Interaction.\nDídac Surís, Sachit Menon, and Carl V ondrick. 2023.\nVipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128.\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew Walter, Ashis Banerjee, Seth Teller, and\nNicholas Roy. 2011. Understanding natural language\ncommands for robotic navigation and mobile manip-\nulation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 25, pages 1507–1514.\n3479\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv: Arxiv-2305.16291.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and\nYitao Liang. 2023b. Describe, explain, plan and se-\nlect: Interactive planning with large language models\nenables open-world multi-task agents. arXiv preprint\narXiv:2302.01560.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nErik Wijmans, Abhishek Kadian, Ari Morcos, Stefan\nLee, Irfan Essa, Devi Parikh, Manolis Savva, and\nDhruv Batra. 2020. Decentralized distributed ppo:\nSolving pointgoal navigation. In International Con-\nference on Lefvarning Representations (ICLR).\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lep-\nert, Andy Zeng, Shuran Song, Jeannette Bohg, Szy-\nmon Rusinkiewicz, and Thomas Funkhouser. 2023a.\nTidybot: Personalized robot assistance with large\nlanguage models. arXiv preprint arXiv:2305.05658.\nYue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria,\nYuanzhi Li, and Tom M Mitchell. 2023b. Read and\nreap the rewards: Learning to play atari with the help\nof instruction manuals. In NeurIPS.\nYue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhut-\ndinov, Amos Azaria, Yuanzhi Li, Tom Mitchell, and\nShrimai Prabhumoye. 2023c. Plan, eliminate, and\ntrack–language models are good teachers for embod-\nied agents. arXiv preprint arXiv:2305.02412.\nYue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan\nBisk, Ruslan Salakhutdinov, Amos Azaria, Tom\nMitchell, and Yuanzhi Li. 2023d. Spring: Gpt-4\nout-performs rl algorithms by studying papers and\nreasoning. In NeurIPS.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine.\n2020. Meta-world: A benchmark and evaluation\nfor multi-task and meta reinforcement learning. In\nConference on Robot Learning , pages 1094–1100.\nPMLR.\nTianhe Yu, Ted Xiao, Austin Stone, Jonathan Tomp-\nson, Anthony Brohan, Su Wang, Jaspiar Singh, Clay-\nton Tan, Dee M, Jodilyn Peralta, Brian Ichter, Karol\nHausman, and Fei Xia. 2023. Scaling robot learn-\ning with semantically imagined experience. In arXiv\npreprint arXiv:2302.11550.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof\nChoromanski, Adrian Wong, Stefan Welker, Fed-\nerico Tombari, Aveek Purohit, Michael Ryoo, Vikas\nSindhwani, et al. 2022. Socratic models: Compos-\ning zero-shot multimodal reasoning with language.\narXiv preprint arXiv:2204.00598.\nYichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks,\nNikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, Yuwei\nBao, and Joyce Chai. 2022. Danli: Deliberative agent\nfor following natural language instructions. arXiv\npreprint arXiv:2210.12485.\nKaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu\nWang, Zonglin Li, Xuehai He, and Xin Eric Wang.\n2022. Jarvis: A neuro-symbolic commonsense\nreasoning framework for conversational embodied\nagents.\n3480\nA Analysis of User Personalization\nFailures\nThe three instances where the Planner made er-\nrors in the user personalization experiment (Sec-\ntion 4.4) involved logical mistakes or inappropriate\nalterations to parts of the original plan that were\nnot requested for modification. For instance, in one\ncase — involving a step to modify the plan from\nmaking one coffee to two coffees — the Planner\nincludes placing two mugs in the coffee maker si-\nmultaneously, which is not a valid plan. In the\nother two instances, the Planner omits an object\nfrom the original plan that was not mentioned in\nthe modification.\nB User Feedback Details\nIn the user feedback evaluation, once the agent has\nindicated completion of the task from the original\ninput dialogue, the agent will query feedback from\nthe user. If the simulator indicates success of the\ntask, the agent will end the episode. If the simu-\nlator indicates the task is not successful, feedback\nwill be given to the agent for additional planning.\nThis feedback is programatically generated from\nthe TEACh simulator metadata, which gives us in-\nformation about if the task is successful, and what\nobject state changes are missing in order to com-\nplete the task (e.g., bread slice is not toasted, etc.).\nFor each object state that is incorrect, we form a\nsentence of the following form: \"You failed to com-\nplete the subtask: subtask. For the object object:\ndescription of desired object state.\" We combine all\nsubtask sentences to create the feedback. HELPER\nfollows the same pipeline (including examples, re-\ntrieval, planning, etc.) to process the feedback as\nwith the input dialogue in the normal TfD eval-\nuation. We show experiments with one and two\nuser feedback requests in Section 4.3 of the main\npaper (a second request is queried if the first user\nfeedback fails to produce task success).\nC User Personalization Inputs\nWe provide a full list of the user personalization\nrequests in Listing 1 for the user personalization\nexperiments in Section 4.4.\nD Prompts\nWe provide our full API (Listing 2), corrective\nAPI (Listing 3), Planner prompt (Listing 4), re-\nplanning prompt (Listing 5), and Locatorprompt\n(Listing 6).\nE Pre-conditions\nAn example of a pre-condition check for a macro-\naction is provided in Listing 7.\nF Example LLM inputs & Outputs\nWe provide examples of dialogue input, retrieved\nexamples, and LLM output for a TEACh sample in\nListing 8, Listing 9, and Listing 10.\nG Simulation environment\nThe TEACh dataset builds on the Ai2thor sim-\nulation environment (Kolve et al., 2017). At\neach time step the agent may choose from the\nfollowing actions: Forward(), Backward(), Turn\nLeft(), Turn Right(), Look Up(), Look Down(),\nStrafe Left(), Strafe Right(), Pickup(X), Place(X),\nOpen(X), Close(X), ToggleOn(X), ToggleOff(X),\nSlice(X), and Pour(X), where X refers an object\nspecified via a relative coordinate(x, y) on the ego-\ncentric RGB frame. Navigation actions move the\nagent in discrete steps. We rotate in the yaw direc-\ntion by 90 degrees, and rotate in the pitch direction\nby 30 degrees. The RGB and depth sensors are at a\nresolution of 480x480, a field of view of 90 degrees,\nand lie at a height of 0.9015 meters. The agent’s\ncoordinates are parameterized by a single (x, y, z)\ncoordinate triplet with x and z corresponding to\nmovement in the horizontal plane and y reserved\nfor the vertical direction. The TEACh benchmark\nallows a maximum of 1000 steps and 30 API fail-\nures per episode.\nH Executor details\nH.1 Semantic mapping and planning\nObstacle map HELPER maintains a 2D overhead\noccupancy map of its environment ∈RH×W that\nit updates at each time step from the input RGB-\nD stream. The map is used for exploration and\nnavigation in the environment.\nAt every time stept, we unproject the input depth\nmaps using intrinsic and extrinsic information of\nthe camera to obtain a 3D occupancy map regis-\ntered to the coordinate frame of the agent, sim-\nilar to earlier navigation agents (Chaplot et al.,\n2020a). The 2D overhead maps of obstacles and\nfree space are computed by projecting the 3D occu-\npancy along the height direction at multiple height\nlevels and summing. For each input RGB image,\n3481\nwe run a SOLQ object segmentor (Dong et al.,\n2021) (pretrained on COCO (Lin et al., 2014) then\nfinetuned on TEACh rooms) to localize each of 116\nsemantic object categories. For failure detection,\nwe use a simple matching approach from Min et al.\n(2021) to compare RGB pixel values before and\nafter taking an action.\nObject location and state tracking We main-\ntain an object memory as a list of object detection\n3D centroids and their predicted semantic labels\n{[(X, Y, Z)i, ℓi ∈{1...N}], i= 1..K}, where K\nis the number of objects detected thus far. The\nobject centroids are expressed with respect to the\ncoordinate system of the agent, and, similar to the\nsemantic maps, updated over time using egomotion.\nWe track previously detected objects by their 3D\ncentroid C ∈R3. We estimate the centroid by tak-\ning the 3D point corresponding to the median depth\nwithin the segmentation mask and bring it to a com-\nmon coordinate frame. We do a simple form of\nnon-maximum suppression on the object memory,\nby comparing the euclidean distance of centroids in\nthe memory to new detected centroids of the same\ncategory, and keep the one with the highest score if\nthey fall within a distance threshold.\nFor each object in the object memory, we main-\ntain an object state dictionary with a pre-defined\nlist of attributes. These attributes include: category\nlabel, centroid location, holding, detection score,\ncan use, sliced, toasted, clean, cooked. For the\nbinary attributes, these are initialized by sending\nthe object crop, defined by the detector mask, to\nthe VLM model, and checking its match to each\nof [f\"The {object_category} is {attribute}\", f\"The\n{object_category} is not {attribute}\"]. We found\nthat initializing these attributes with the VLM gave\nonly a marginal difference to initializing them to\ndefault values in the TEACh benchmark, so we do\nnot use it for the TEACh evaluations. However, we\nanticipate a general method beyond dataset biases\nof TEACh would much benefit from such vision-\nbased attribute classification.\nExploration and path planning HELPER ex-\nplores the scene using a classical mapping method.\nWe take the initial position of the agent to be the\ncenter coordinate in the map. We rotate the agent\nin-place and use the observations to instantiate an\ninitial map. Second, the agent incrementally com-\npletes the maps by randomly sampling an unex-\nplored, traversible location based on the 2D occu-\npancy map built so far, and then navigates to the\nsampled location, accumulating the new informa-\ntion into the maps at each time step. The number of\nobservations collected at each point in the 2D occu-\npancy map is thresholded to determine whether a\ngiven map location is explored or not. Unexplored\npositions are sampled until the environment has\nbeen fully explored, meaning that the number of\nunexplored points is fewer than a predefined thresh-\nold.\nTo navigate to a goal location, we compute the\ngeodesic distance to the goal from all map locations\nusing graph search (Inoue and Ohashi, 2022) given\nthe top-down occupancy map and the goal location\nin the map. We then simulate action sequences and\ngreedily take the action sequence which results in\nthe largest reduction in geodesic distance.\nH.2 2D-to-3D unprojection\nFor the i-th view, a 2D pixel coordinate (u, v)\nwith depth z is unprojected and transformed to its\ncoordinate (X, Y, Z)T in the reference frame:\n(X, Y, Z,1) =G−1\ni\n(\nz u −cx\nfx\n, zv −cy\nfy\n, z,1\n)T\n(1)\nwhere (fx, fy) and (cx, cy) are the focal lengths\nand center of the pinhole camera model and Gi ∈\nSE(3) is the camera pose for view i relative to\nthe reference view. This module unprojects each\ndepth image Ii ∈RH×W×3 into a pointcloud in\nthe reference frame Pi ∈RMi×3 with Mi being the\nnumber of pixels with an associated depth value.\nI Additional Experiments\nI.1 Alternate EDH Evaluation Split\nCurrently, the leaderboard for the TEACh EDH\nbenchmark is not active. Thus, we are not able\nto evaluate on the true test set for TEACh. We\nused the original validation seen and unseen splits,\nwhich have been used in most previous works (Pa-\nshevich et al., 2021; Zheng et al., 2022; Min et al.,\n2022; Zhang et al., 2022). In Table 4 we report the\nalternative validation and test split as mentioned in\nthe TEACh github README, and also reported by\nDANLI (Zhang et al., 2022).\n3482\nTable 4: Alternative TEACh Execution from Dialog History (EDH) evaluation split. Trajectory length weighted\nmetrics are included in ( parentheses ). SR = success rate. GC = goal condition success rate. Note that Test Seen\nand Unseen are not the true TEACh test sets, but an alternative split of the validation set used until the true test\nevaluation is released, as mentioned in the TEACh github README, and also reported by DANLI (Zhang et al.,\n2022).\nValidation Test\nUnseen Seen Unseen Seen\nSR GC SR GC SR GC SR GC\nE.T. 8.35 (0.86) 6.34 (3.69) 8.28 (1.13) 8.72 (3.82) 7.38 (0.97) 6.06 (3.17) 8.82 (0.29) 9.46 (3.03)\nDANLI 17.25 (7.16) 23.88 (19.38) 16.89 (9.12) 25.10 (22.56) 16.71 (7.33) 23.00 (20.55) 18.63 (9.41) 24.77 (21.90)\nHELPER 17.25 (3.22) 25.24 (8.12) 19.21 (4.72) 33.54 (10.95) 17.55 (2.59) 26.49 (7.67) 17.97 (3.44) 30.81 (8.93)\n3483\nListing 1: Full list of user personalization requests for the user personalization evaluation.\noriginal input to LLM:\n[['Driver', 'What is my task?'], ['Commander', \"Make me a sandwich. The name of this sandwich is\ncalled the Larry sandwich. The sandwich has two slices of toast, 3 slices of tomato, and 3\nslice of lettuce on a clean plate.\"]]\n[['Driver', 'What is my task?'], ['Commander', 'Make me a salad. The name of this salad is called\nthe David salad. The salad has two slices of tomato and three slices of lettuce on a clean\nplate.']]\n[['Driver', 'What is my task?'], ['Commander', \"Make me a salad. The name of this salad is called\nthe Dax salad. The salad has two slices of cooked potato. You'll need to cook the potato on the\nstove. The salad also has a slice of lettuce and a slice of tomato. Put all components on a\nclean plate.\"]]\n[['Driver', 'What is my task?'], ['Commander', 'Make me breakfast. The name of this breakfast is\ncalled the Mary breakfast. The breakfast has a mug of coffee, and two slices of toast on a\nclean plate.']]\n[['Driver', 'What is my task?'], ['Commander', 'Make me breakfast. The name of this breakfast is\ncalled the Lion breakfast. The breakfast has a mug of coffee, and four slices of tomato on a\nclean plate.']]\n[['Driver', 'What is my task?'], ['Commander', 'Rearrange some objects. The name of this\nrearrangement is called the Lax rearrangement. Place three pillows on the sofa.']]\n[['Driver', 'What is my task?'], ['Commander', 'Rearrange some objects. The name of this\nrearrangement is called the Pax rearrangement. Place two pencils and two pens on the desk.']]\n[['Driver', 'What is my task?'], ['Commander', 'Clean some objects. The name of this cleaning is\ncalled the Gax cleaning. Clean two plates and two cups.']]\n[['Driver', 'What is my task?'], ['Commander', \"Make me a sandwich. The name of this sandwich is\ncalled the Gabe sandwich. The sandwich has two slices of toast, 2 slices of tomato, and 1 slice\nof lettuce on a clean plate.\"]]\n[['Driver', 'What is my task?'], ['Commander', 'Clean some objects. The name of this cleaning is\ncalled the Kax cleaning. Clean a mug and a pan.']]\nNo change:\n\"Make me the Larry sandwich\"\n\"Make me the David salad\"\n\"Make me the Dax salad\"\n\"Make me the Mary breakfast\"\n\"Make me the Lion breakfast\"\n\"Complete the Lax rearrangement\"\n\"Complete the Pax rearrangement\"\n\"Perform the Gax cleaning\"\n\"Make me the Gabe sandwich\"\n\"Perform the Kax cleaning\"\nOne change:\n\"Make me the Larry sandwich with four slices of lettuce\"\n\"Make me the David salad with a slice of potato\"\n\"Make me the Dax salad without lettuce\"\n\"Make me the Mary breakfast with no coffee\"\n\"Make me the Lion breakfast with three slice of tomato\"\n\"Complete the Lax rearrangement with two pillows\"\n\"Complete the Pax rearrangement but use one pencil instead of the the two pencils\"\n\"Perform the Gax cleaning with three plates instead of two\"\n\"Make me the Gabe sandwich with only 1 slice of tomato\"\n\"Perform the Kax cleaning with only a mug\"\n3484\nTwo changes:\n\"Make me the Larry sandwich with four slices of lettuce and two slices of tomato\"\n\"Make me the David salad but add a slice of potato and add one slice of egg\"\n\"Make me the Dax salad without lettuce and without potato\"\n\"Make me the Mary breakfast with no coffee and add an egg\"\n\"Make me the Lion breakfast with three slice of tomato and two mugs of coffee\"\n\"Complete the Lax rearrangement with two pillows and add a remote\"\n\"Complete the Pax rearrangement but use one pencil instead of the two pencils and add a book\"\n\"Perform the Gax cleaning with three plates instead of the two plates and include a fork\"\n\"Make me the Gabe sandwich with only 1 slice of tomato and two slices of lettuce\"\n\"Perform the Kax cleaning without the pan and include a spoon\"\nThree changes:\n\"Make me the Larry sandwich with four slices of lettuce, two slices of tomato, and place all\ncomponents directly on the countertop\"\n\"Make me the David salad and add a slice of potato, add one slice of egg, and bring a fork with it\"\n\"Make me the Dax salad without lettuce, without potato, and add an extra slice of tomato\"\n\"Make me the Mary breakfast with no coffee, add an egg, and add a cup filled with water\"\n\"Make me the Lion breakfast with three slice of tomato, two mugs of coffee, and add a fork\"\n\"Complete the Lax rearrangement with two pillows, a remote, and place it on the arm chair instead\"\n\"Complete the Pax rearrangement but use one pencil instead of the two pencils and include a book and\na baseball bat\"\n\"Perform the Gax cleaning with three plates instead of the two plates, include a fork, and do not\nclean any cups\"\n\"Make me the Gabe sandwich with only 1 slice of tomato, two slices of lettuce, and add a slice of\negg\"\n\"Perform the Kax cleaning without the pan, include a spoon, and include a pot\"\n3485\nListing 2: Full API for the parametrized macro-actions G used in the prompts.\nclass InteractionObject:\n\"\"\"\nThis class represents an expression that uniquely identifies an object in the house.\n\"\"\"\ndef __init__(self, object_class: str, landmark: str = None, attributes: list = []):\n'''\nobject_class: object category of the interaction object (e.g., \"Mug\", \"Apple\")\nlandmark: (optional if mentioned) landmark object category that the interaction object is in\nrelation to (e.g., \"CounterTop\" for \"apple is on the countertop\")\nattributes: (optional) list of strings of desired attributes for the object. These are not\nnecessarily attributes that currently exist, but ones that the object should eventually\nhave. Attributes can only be from the following: \"toasted\", \"clean\", \"cooked\"\n'''\nself.object_class = object_class\nself.landmark = landmark\nself.attributes = attributes\ndef pickup(self):\n\"\"\"pickup the object.\nThis function assumes the object is in view.\nExample:\ndialogue: <Commander> Go get the lettuce on the kitchen counter.\nPython script:\ntarget_lettuce = InteractionObject(\"Lettuce\", landmark = \"CounterTop\")\ntarget_lettuce.go_to()\ntarget_lettuce.pickup()\n\"\"\"\npass\ndef place(self, landmark_name):\n\"\"\"put the interaction object on the landmark_name object.\nlandmark_name must be a class InteractionObject instance\nThis function assumes the robot has picked up an object and the landmark object is in view.\nExample:\ndialogue: <Commander> Put the lettuce on the kitchen counter.\nPython script:\ntarget_lettuce = InteractionObject(\"Lettuce\", landmark = \"CounterTop\")\ntarget_lettuce.go_to()\ntarget_lettuce.pickup()\ntarget_countertop = InteractionObject(\"CounterTop\")\ntarget_countertop.go_to()\ntarget_lettuce.place(target_countertop)\n\"\"\"\npass\ndef slice(self):\n\"\"\"slice the object into pieces.\nThis function assumes the agent is holding a knife and the agent has navigated to the object\nusing go_to().\nExample:\ndialogue: <Commander> Cut the apple on the kitchen counter.\nPython script:\ntarget_knife = InteractionObject(\"Knife\") # first we need a knife to slice the apple with\ntarget_knife.go_to()\ntarget_knife.pickup()\ntarget_apple = InteractionObject(\"Apple\", landmark = \"CounterTop\")\ntarget_apple.go_to()\ntarget_apple.slice()\n\"\"\"\npass\n3486\ndef toggle_on(self):\n\"\"\"toggles on the interaction object.\nThis function assumes the interaction object is already off and the agent has navigated to\nthe object.\nOnly some landmark objects can be toggled on. Lamps, stoves, and microwaves are some\nexamples of objects that can be toggled on.\nExample:\ndialogue: <Commander> Turn on the lamp.\nPython script:\ntarget_floorlamp = InteractionObject(\"FloorLamp\")\ntarget_floorlamp.go_to()\ntarget_floorlamp.toggle_on()\n\"\"\"\npass\ndef toggle_off(self):\n\"\"\"toggles off the interaction object.\nThis function assumes the interaction object is already on and the agent has navigated to\nthe object.\nOnly some objects can be toggled off. Lamps, stoves, and microwaves are some examples of\nobjects that can be toggled off.\nExample:\ndialogue: <Commander> Turn off the lamp.\nPython script:\ntarget_floorlamp = InteractionObject(\"FloorLamp\")\ntarget_floorlamp.go_to()\ntarget_floorlamp.toggle_off()\n\"\"\"\npass\ndef go_to(self):\n\"\"\"Navigate to the object\n\"\"\"\npass\ndef open(self):\n\"\"\"open the interaction object.\nThis function assumes the landmark object is already closed and the agent has already\nnavigated to the object.\nOnly some objects can be opened. Fridges, cabinets, and drawers are some example of objects\nthat can be closed.\nExample:\ndialogue: <Commander> Get the lettuce in the fridge.\nPython script:\ntarget_fridge = InteractionObject(\"Fridge\")\ntarget_lettuce = InteractionObject(\"Lettuce\", landmark = \"Fridge\")\ntarget_fridge.go_to()\ntarget_fridge.open()\ntarget_lettuce.pickup()\n\"\"\"\npass\ndef close(self):\n\"\"\"close the interaction object.\nThis function assumes the object is already open and the agent has already navigated to the\nobject.\nOnly some objects can be closed. Fridges, cabinets, and drawers are some example of objects\nthat can be closed.\n\"\"\"\npass\ndef clean(self):\n3487\n\"\"\"wash the interaction object to clean it in the sink.\nThis function assumes the object is already picked up.\nExample:\ndialogue: <Commander> Clean the bowl\nPython script:\ntarget_bowl = InteractionObject(\"Bowl\", attributes = [\"clean\"])\ntarget_bowl.clean()\n\"\"\"\npass\ndef put_down(self):\n\"\"\"puts the interaction object currently in the agent's hand on the nearest available\nreceptacle\nThis function assumes the object is already picked up.\nThis function is most often used when the holding object is no longer needed, and the agent\nneeds to pick up another object\n\"\"\"\npass\ndef pour(self, landmark_name):\n\"\"\"pours the contents of the interaction object into the landmark object specified by the\nlandmark_name argument\nlandmark_name must be a class InteractionObject instance\nThis function assumes the object is already picked up and the object is filled with liquid.\n\"\"\"\npass\ndef fill_up(self):\n\"\"\"fill up the interaction object with water\nThis function assumes the object is already picked up. Note that only container objects can\nbe filled with liquid.\n\"\"\"\npass\ndef pickup_and_place(self, landmark_name):\n\"\"\"go_to() and pickup() this interaction object, then go_to() and place() the interaction\nobject on the landmark_name object.\nlandmark_name must be a class InteractionObject instance\n\"\"\"\npass\ndef empty(self):\n\"\"\"Empty the object of any other objects on/in it to clear it out.\nUseful when the object is too full to place an object inside it.\nExample:\ndialogue: <Commander> Clear out the sink.\nPython script:\ntarget_sink = InteractionObject(\"Sink\")\ntarget_sink.empty()\n\"\"\"\npass\ndef cook(self):\n\"\"\"Cook the object\nExample:\ndialogue: <Commander> Cook the potato.\nPython script:\ntarget_potato = InteractionObject(\"Potato\", attributes = [\"cooked\"])\ntarget_potato.cook()\n\"\"\"\n3488\npass\ndef toast(self):\n\"\"\"Toast a bread slice in a toaster\nToasting is only supported with slices of bread\nExample:\ndialogue: <Commander> Get me a toasted bread slice.\nPython script:\ntarget_breadslice = InteractionObject(\"BreadSliced\", attributes = [\"toasted\"])\ntarget_breadslice.toast()\n\"\"\"\npass\n3489\nListing 3: Full Corrective API for the parametrized corrective macro-actions Gcorrective used in the prompts.\nclass AgentCorrective:\n'''\nThis class represents agent corrective actions that can be taken to fix a subgoal error\nExample usage:\nagent = AgentCorrective()\nagent.move_back()\n'''\ndef move_back(self):\n\"\"\"Step backwards away from the object\nUseful when the object is too close for the agent to interact with it\n\"\"\"\npass\ndef move_closer(self):\n\"\"\"Step forward to towards the object to get closer to it\nUseful when the object is too far for the agent to interact with it\n\"\"\"\npass\ndef move_alternate_viewpoint(self):\n\"\"\"Move to an alternate viewpoint to look at the object\nUseful when the object is occluded or an interaction is failing due to collision or\nocclusion.\n\"\"\"\npass\n3490\nListing 4: Full Prompt for the Planner. {} indicates areas that are replaced in the prompt.\nYou are an adept at translating human dialogues into sequences of actions for household robots.\nGiven a dialogue between a <Driver> and a <Commander>, you convert the conversation into a\nPython program to be executed by a robot.\n{API}\nWrite a script using Python and the InteractionObject class and functions defined above that could\nbe executed by a household robot.\n{RETRIEVED_EXAMPLES}\nAdhere to these stringent guidelines:\n1. Use only the classes and functions defined previously. Do not create functions that are not\nprovided above.\n2. Make sure that you output a consistent plan. For example, opening of the same object should not\noccur in successive steps.\n3. Make sure the output is consistent with the proper affordances of objects. For example, a couch\ncannot be opened, so your output should never include the open() function for this object, but\na fridge can be opened.\n4. The input is dialogue between <Driver> and <Commander>. Interpret the dialogue into robot\nactions. Do not output any dialogue.\n5. Object categories should only be chosen from the following classes: ShowerDoor, Cabinet,\nCounterTop, Sink, Towel, HandTowel, TowelHolder, SoapBar, ToiletPaper, ToiletPaperHanger,\nHandTowelHolder, SoapBottle, GarbageCan, Candle, ScrubBrush, Plunger, SinkBasin, Cloth,\nSprayBottle, Toilet, Faucet, ShowerHead, Box, Bed, Book, DeskLamp, BasketBall, Pen, Pillow,\nPencil, CellPhone, KeyChain, Painting, CreditCard, AlarmClock, CD, Laptop, Drawer, SideTable,\nChair, Blinds, Desk, Curtains, Dresser, Watch, Television, WateringCan, Newspaper, FloorLamp,\nRemoteControl, HousePlant, Statue, Ottoman, ArmChair, Sofa, DogBed, BaseballBat, TennisRacket,\nVacuumCleaner, Mug, ShelvingUnit, Shelf, StoveBurner, Apple, Lettuce, Bottle, Egg, Microwave,\nCoffeeMachine, Fork, Fridge, WineBottle, Spatula, Bread, Tomato, Pan, Cup, Pot, SaltShaker,\nPotato, PepperShaker, ButterKnife, StoveKnob, Toaster, DishSponge, Spoon, Plate, Knife,\nDiningTable, Bowl, LaundryHamper, Vase, Stool, CoffeeTable, Poster, Bathtub, TissueBox,\nFootstool, BathtubBasin, ShowerCurtain, TVStand, Boots, RoomDecor, PaperTowelRoll, Ladle,\nKettle, Safe, GarbageBag, TeddyBear, TableTopDecor, Dumbbell, Desktop, AluminumFoil, Window,\nLightSwitch, AppleSliced, BreadSliced, LettuceSliced, PotatoSliced, TomatoSliced\n6. You can only pick up one object at a time. If the agent is holding an object, the agent should\nplace or put down the object before attempting to pick up a second object.\n7. Each object instance should instantiate a different InteractionObject class even if two object\ninstances are the same object category.\nFollow the output format provided earlier. Think step by step to carry out the instruction.\nWrite a Python script that could be executed by a household robot for the following:\ndialogue: {command}\nPython script:\n3491\nListing 5: Full Prompt for the Rectifier. {} indicates areas that are replaced in the prompt.\nYou are an excellent interpreter of human instructions for household tasks. Given a failed action\nsubgoal by a household robot, dialogue instructions between robot <Driver> and user\n<Commander>, and information about the environment and failure, you provide a sequence of\nrobotic subgoal actions to overcome the failure.\n{API}\n{API_CORRECTIVE}\nInformation about the failure and environment are given as follows:\nFailed subgoal: The robotic subgoal for which the failure occured.\nExecution error: feedback as to why the failed subgoal occurred.\nInput dialogue: full dialogue instructions between robot <Driver> and user <Commander> for the\ncomplete task. This may or may not be useful.\nI will give you examples of the input and output you will generate.\n{retrieved_plans}\nFix the subgoal exectuion error using only the InteractionObject class and functions defined above\nthat could be executed by a household robot. Follow these rules very strictly:\n1. Important! Use only the classes and functions defined previously. Do not create functions or\nadditional code that are not provided in the above API. Do not include if-else statements.\n2. Important! Make sure that you output a consistent plan. For example, opening of the same object\nshould not occur in successive steps.\n3. Important! Make sure the output is consistent with the proper affordances of objects. For\nexample, a couch cannot be opened, so your output should never include the open() function for\nthis object, but a fridge can be opened.\n4. Important! The dialogue is between <Driver> and <Commander>. The dialogue may or may not be\nhelpful. Do not output any dialogue.\n5. Important! Object classes should only be chosen from the following classes: ShowerDoor, Cabinet,\nCounterTop, Sink, Towel, HandTowel, TowelHolder, SoapBar, ToiletPaper, ToiletPaperHanger,\nHandTowelHolder, SoapBottle, GarbageCan, Candle, ScrubBrush, Plunger, SinkBasin, Cloth,\nSprayBottle, Toilet, Faucet, ShowerHead, Box, Bed, Book, DeskLamp, BasketBall, Pen, Pillow,\nPencil, CellPhone, KeyChain, Painting, CreditCard, AlarmClock, CD, Laptop, Drawer, SideTable,\nChair, Blinds, Desk, Curtains, Dresser, Watch, Television, WateringCan, Newspaper, FloorLamp,\nRemoteControl, HousePlant, Statue, Ottoman, ArmChair, Sofa, DogBed, BaseballBat, TennisRacket,\nVacuumCleaner, Mug, ShelvingUnit, Shelf, StoveBurner, Apple, Lettuce, Bottle, Egg, Microwave,\nCoffeeMachine, Fork, Fridge, WineBottle, Spatula, Bread, Tomato, Pan, Cup, Pot, SaltShaker,\nPotato, PepperShaker, ButterKnife, StoveKnob, Toaster, DishSponge, Spoon, Plate, Knife,\nDiningTable, Bowl, LaundryHamper, Vase, Stool, CoffeeTable, Poster, Bathtub, TissueBox,\nFootstool, BathtubBasin, ShowerCurtain, TVStand, Boots, RoomDecor, PaperTowelRoll, Ladle,\nKettle, Safe, GarbageBag, TeddyBear, TableTopDecor, Dumbbell, Desktop, AluminumFoil, Window,\nLightSwitch, AppleSliced, BreadSliced, LettuceSliced, PotatoSliced, TomatoSliced\n6. Important! You can only pick up one object at a time. If the agent is holding an object, the\nagent should place or put down the object before attempting to pick up a second object.\n7. Important! Each object instance should instantiate a different InteractionObject class even if\ntwo object instances are the same object category.\n8. Important! Your plan should ONLY fix the failed subgoal. Do not include plans for other parts of\nthe dialogue or future plan that are irrelevant to the execution error and failed subgoal.\n9. Important! output \"do_nothing()\" if the agent should not take any corrective actions.\nAdhere to the output format I defined above. Think step by step to carry out the instruction.\nMake use of the following information to help you fix the failed subgoal:\nFailed subgoal: ...\nExecution error: ...\nInput dialogue: ...\nYou should respond in the following format:\nExplain: Are there any steps missing to complete the subgoal? Why did the failed subgoal occur? What\ndoes the execution error imply for how to fix your future plan?\nPlan (Python script): A Python script to only fix the execution error.\nExplain:\n3492\nListing 6: Full Prompt for the Locator. {} indicates areas that are replaced in the prompt.\nYou are a household robot trying to locate objects within a house.\nYou will be given a target object category, your task is to output the top 3 most likely object\ncategories that the target object category is likely to be found near: {OBJECT_CLASSES}\nFor your answer, take into account commonsense co-occurances of objects within a house and (if\nrelevant) any hints given by the instruction dialogue between the robot <Driver> and user\n<Commander>.\nFor example, if given the target object category is \"Knife\" and the following dialogue: \"<Commander>\nhi, make a slice of tomato. <Driver> where is the tomato? <Driver> where is the knife?\n<Commander> in the sink.\", you might output the following top 3 most likely object categories:\n\"answer: Sink, CounterTop, Cabinet\". Important: Your output should ONLY a list (3 words\nseperated by commas) of three object categories from the list above.\nWhat are the top 3 most likely object categories for where to find the target category\n{INPUT_TARGET_OBJECT} near? Here is the dialogue that may or may not contain relevant\ninformation for your answer: {INPUT_DIALOGUE}\nanswer:\n3493\nListing 7: Example of a precondition check for the slice macro-action\nif subgoal==\"Slice\":\n# contraint check for \"slice\" macro-action\nif object_tracker.get_label_of_holding()!=\"Knife\":\n# first need to be holding a knife to slice\nprogram_to_add = [\n'target_knife = InteractionObject(\"Knife\")',\n'target_knife.go_to()',\n'target_knife.pickup()',\n]\n3494\nListing 8: Example of dialogue input, retrieved examples, and LLM output for a TEACh sample\nDialogue input:\n<Driver> how can I help you today?<Commander> can you please make me a salad on a clean plate with\ntomato and cooked potato?<Driver> does the salad require chopped lettuce?<Commander> nope!<Driver>\nis that all?<Commander> can you place them on a plate?<Driver> are they not already on a plate?\nRetrieved Examples:\nExample 1:\ndialogue: <Driver> What should I do today? <Commander> hi, make a slice of tomato. <Driver> where is\nthe tomato? <Driver> where is the knife? <Commander> in the sink. <Driver> Tomato sliced. Whatnext? <Commander> slice the potato. <Driver> Where is the potato? <Commander> in the microwave.\n<Commander> place all salad components on a plate. <Driver> How many slices of potato?\n<Commander> all salad components need to be place on a plate. <Driver> Where is the plate?\n<Commander> plate. <Commander> try the tomato piece. <Driver> done. <Commander> we are finished.\nPython script:\ntarget_tomato = InteractionObject(\"Tomato\")\ntarget_knife = InteractionObject(\"Knife\") # need knife to slice tomato\ntarget_knife.go_to() # first get knife\ntarget_knife.pickup()\ntarget_tomato.go_to()\ntarget_tomato.slice()\ntarget_tomato_slice1 = InteractionObject(\"TomatoSliced\") # instantiate a tomato slice object after\nslicing tomato\ntarget_potato = InteractionObject(\"Potato\", landmark = \"Microwave\")\ntarget_potato.go_to()\ntarget_potato.slice()\ntarget_potato_slice1 = InteractionObject(\"PotatoSliced\") # instantiate a potato slice object after\nslicing potato\ntarget_knife.put_down() # put down the knife to free up the agent's hand\ntarget_plate = InteractionObject(\"Plate\")\ntarget_tomato_slice1.pickup_and_place(target_plate) # pick up the tomato slice and place it on the\nplate\ntarget_potato_slice1.pickup_and_place(target_plate) # pick up the potato slice and place it on the\nplate\nExample 2:\ndialogue: <Driver> how can I help? <Commander> please serve 4 slices of tomato on a plate. <Driver>\nsure. <Driver> where can i find the tomato? <Driver> an knife. <Commander> Tomato on countertop\nand knife in fridge. <Commander> there should be a plate on the right cupboard by the sink.\n<Driver> completed. <Commander> great work thanks.\nPython script:\ntarget_tomato = InteractionObject(\"Tomato\")\ntarget_knife = InteractionObject(\"Knife\") # need knife to slice tomato\ntarget_knife.go_to() # first get knife\ntarget_knife.pickup()\ntarget_tomato.go_to()\ntarget_tomato.slice()\ntarget_plate = InteractionObject(\"Plate\", landmark = \"Sink\")\n# place 4 slices of the cut tomato on the plate\ntarget_tomato_slice1 = InteractionObject(\"TomatoSliced\") # instantiate first slice of tomato\ntarget_tomato_slice2 = InteractionObject(\"TomatoSliced\") # instantiate second slice of tomato\ntarget_tomato_slice3 = InteractionObject(\"TomatoSliced\") # instantiate third slice of tomato\ntarget_tomato_slice4 = InteractionObject(\"TomatoSliced\") # instantiate fourth slice of tomato\ntarget_tomato_slice1.pickup_and_place(target_plate) # pick up the first tomato slice and place it on\nthe plate\ntarget_tomato_slice2.pickup_and_place(target_plate) # pick up the second tomato slice and place it\non the plate\ntarget_tomato_slice3.pickup_and_place(target_plate) # pick up the third tomato slice and place it on\nthe plate\ntarget_tomato_slice4.pickup_and_place(target_plate) # pick up the fourth tomato slice and place it\non the plate\nExample 3:\ndialogue: <Driver> hi, what is task. <Commander> make a plate of toast. <Commander> bread in the sink.\n<Driver> knife. <Commander> on the chair beside the table. <Commander> clean the plate. <Driver>\nthere is a clean one, do i need to clean the other one. <Commander> you can use the clean one.\n<Driver> done. <Commander> good job. <Driver> :).\nPython script:\ntarget_bread = InteractionObject(\"Bread\", landmark = \"Sink\")\ntarget_knife = InteractionObject(\"Knife\", landmark = \"Chair\")\ntarget_toaster = InteractionObject(\"Toaster\") # need knife to slice bread\ntarget_knife.go_to() # first get knife\n3495\ntarget_knife.pickup()\ntarget_bread.go_to()\ntarget_bread.slice() # slice bread using knife\ntarget_knife.put_down() # put down the knife to free up the agent's hand\ntarget_bread_slice1 = InteractionObject(\"BreadSliced\") # instantiate a bread slice object after\nslicing bread\ntarget_bread_slice1.pickup_and_place(target_toaster) # pick up the bread slice and place it in the\ntoaster\ntarget_toaster.toggle_on() # turn on toaster to toast bread\ntarget_toaster.toggle_off() # turn off toaster to complete toasting process\n# place toast on plate\ntarget_plate = InteractionObject(\"Plate\")\ntarget_plate.clean()\ntarget_plate.put_down()\ntarget_bread_slice1.pickup_and_place(target_plate) # pick up the toasted bread slice and place it on\nthe plate\nLLM output:\ntarget_tomato = InteractionObject(\"Tomato\")\ntarget_potato = InteractionObject(\"Potato\")\ntarget_plate = InteractionObject(\"Plate\")\ntarget_stove = InteractionObject(\"StoveBurner\")\n# Clean the plate\ntarget_plate.go_to()\ntarget_plate.clean()\n# Cook the potato\ntarget_potato.go_to()\ntarget_potato.pickup()\ntarget_stove.go_to()\ntarget_potato.place(target_stove)\ntarget_stove.toggle_on()\ntarget_stove.toggle_off()\n# Place the cooked potato on the plate\ntarget_potato = InteractionObject(\"Potato\")\ntarget_potato.pickup()\ntarget_plate.go_to()\ntarget_potato.place(target_plate)\n# Place the tomato on the plate\ntarget_tomato.go_to()\ntarget_tomato.pickup()\ntarget_plate.go_to()\ntarget_tomato.place(target_plate)\n3496\nListing 9: Example of dialogue input, retrieved examples, and LLM output for a TEACh sample\nDialogue input:\n<Driver> what do i do today<Commander> Hi. Please place two tissue boxes on a table. One is on the\ngray chair in the corner.<Commander> The other is under the tv<Commander> tv*<Driver> where is\nthe other one\nRetrieved Examples:\nExample 1:\ndialogue: <Driver> hi, what is task. <Commander> make a plate of toast. <Commander> bread in the sink.\n<Driver> knife. <Commander> on the chair beside the table. <Commander> clean the plate. <Driver>\nthere is a clean one, do i need to clean the other one. <Commander> you can use the clean one.\n<Driver> done. <Commander> good job. <Driver> :).\nPython script:\ntarget_bread = InteractionObject(\"Bread\", landmark = \"Sink\")\ntarget_knife = InteractionObject(\"Knife\", landmark = \"Chair\")\ntarget_toaster = InteractionObject(\"Toaster\") # need knife to slice bread\ntarget_knife.go_to() # first get knife\ntarget_knife.pickup()\ntarget_bread.go_to()\ntarget_bread.slice() # slice bread using knife\ntarget_knife.put_down() # put down the knife to free up the agent's hand\ntarget_bread_slice1 = InteractionObject(\"BreadSliced\") # instantiate a bread slice object after\nslicing bread\ntarget_bread_slice1.pickup_and_place(target_toaster) # pick up the bread slice and place it in the\ntoaster\ntarget_toaster.toggle_on() # turn on toaster to toast bread\ntarget_toaster.toggle_off() # turn off toaster to complete toasting process\n# place toast on plate\ntarget_plate = InteractionObject(\"Plate\")\ntarget_plate.clean()\ntarget_plate.put_down()\ntarget_bread_slice1.pickup_and_place(target_plate) # pick up the toasted bread slice and place it on\nthe plate\nExample 2:\ndialogue: <Driver> how can I help? <Commander> please serve 4 slices of tomato on a plate. <Driver>\nsure. <Driver> where can i find the tomato? <Driver> an knife. <Commander> Tomato on countertop\nand knife in fridge. <Commander> there should be a plate on the right cupboard by the sink.\n<Driver> completed. <Commander> great work thanks.\nPython script:\ntarget_tomato = InteractionObject(\"Tomato\", landmark = \"CounterTop\")\ntarget_knife = InteractionObject(\"Knife\", landmark = \"Fridge\") # need knife to slice tomato\ntarget_knife.go_to() # first get knife\ntarget_knife.pickup()\ntarget_tomato.go_to()\ntarget_tomato.slice()\ntarget_plate = InteractionObject(\"Plate\", landmark = \"Sink\")\n# place 4 slices of the cut tomato on the plate\ntarget_tomato_slice1 = InteractionObject(\"TomatoSliced\") # instantiate first slice of tomato\ntarget_tomato_slice2 = InteractionObject(\"TomatoSliced\") # instantiate second slice of tomato\ntarget_tomato_slice3 = InteractionObject(\"TomatoSliced\") # instantiate third slice of tomato\ntarget_tomato_slice4 = InteractionObject(\"TomatoSliced\") # instantiate fourth slice of tomato\ntarget_tomato_slice1.pickup_and_place(target_plate) # pick up the first tomato slice and place it on\nthe plate\ntarget_tomato_slice2.pickup_and_place(target_plate) # pick up the second tomato slice and place it\non the plate\ntarget_tomato_slice3.pickup_and_place(target_plate) # pick up the third tomato slice and place it on\nthe plate\ntarget_tomato_slice4.pickup_and_place(target_plate) # pick up the fourth tomato slice and place it\non the plate\nExample 3:\ndialogue: <Driver> How can I help? <Commander> put all the scrub brushes on the counter top. <Driver>\nwhere can I find them? <Commander> one is on top of the toilet. <Driver> there is only a cloth.\n<Driver> I put it on the counter top. <Driver> are there more? <Commander> can you try with the\nbrush to the side of the toilet please? <Driver> okay.\nPython script:\ntarget_scrub_brush1 = InteractionObject(\"ScrubBrush\", landmark = \"Toilet\")\ntarget_counter_top = InteractionObject(\"CounterTop\")\ntarget_scrub_brush1.pickup_and_place(target_counter_top) # pick up the first ScrubBrush and place it\non the CounterTop\n3497\ntarget_scrub_brush2 = InteractionObject(\"ScrubBrush\", landmark = \"Toilet\")\ntarget_scrub_brush2.pickup_and_place(target_counter_top) # pick up the second ScrubBrush and place\nit on the CounterTop\nLLM output:\ntarget_tissue_box1 = InteractionObject(\"TissueBox\", landmark = \"Chair\")\ntarget_table = InteractionObject(\"Table\")\ntarget_tissue_box1.pickup_and_place(target_table) # pick up the first TissueBox and place it on the\nTable\ntarget_tissue_box2 = InteractionObject(\"TissueBox\", landmark = \"TVStand\")\ntarget_tissue_box2.pickup_and_place(target_table) # pick up the second TissueBox and place it on the\nTable\n3498\nListing 10: Example of dialogue input, retrieved examples, and LLM output for a TEACh sample\nDialogue input:\n<Driver> hi how can i help<Commander> Can you prepare me a coffee in a clean mug<Commander> There\nshould be a mug on the white table<Commander> it might need cleaning first<Commander> that’s\ngreat<Driver> are they not already on a plate?\nRetrieved Examples:\nExample 1:\ndialogue: <Commander> Prepare coffee in a clean mug.\nPython script:\ntarget_mug = InteractionObject(\"Mug\")\ntarget_mug.go_to()\ntarget_mug.pickup()\n# clean mug in the sink\ntarget_mug.clean()\ntarget_coffee_machine = InteractionObject(\"CoffeeMachine\")\ntarget_coffee_machine.go_to()\ntarget_mug.place(target_coffee_machine)\ntarget_coffee_machine.toggle_on() # turn on coffee machine to make coffee\ntarget_coffee_machine.toggle_off() # turn off coffee machine after done\nExample 2:\ndialogue: <Driver> what can i for you today? <Commander> could you boil a potato? <Driver> sure thing!\n<Driver> are there any pots? <Commander> pots for boiling potato? <Driver> yes. <Commander> just\nthe one that you were holding earlier and a couple others of the same size. <Commander> does it\nnot fit in the sink? <Driver> no. <Commander> see that mug of water next to the pot? <Driver> yes.\n<Commander> pour the mugs water into the pot. <Driver> thanks. <Commander> you’re welcome!\n<Commander> excellent thank you!\nPython script:\ntarget_pot = InteractionObject(\"Pot\")\ntarget_mug = InteractionObject(\"Mug\", landmark = \"Pot\")\ntarget_mug.go_to()\ntarget_mug.pickup()\ntarget_mug.pour(target_pot)\ntarget_mug.put_down() # put down mug on any location to free up agent's hand\ntarget_potato = InteractionObject(\"Potato\")\ntarget_potato.pickup_and_place(target_pot) # pick up the potato and place it in the pot\ntarget_stove_knob = InteractionObject(\"StoveKnob\") # turn on the stove to boil potato by turning on\nthe stove\ntarget_stove_knob.go_to()\ntarget_stove_knob.toggle_on() # boil water\ntarget_stove_knob.toggle_off() # complete boiling process by turning off\nExample 3:\ndialogue: <Driver> hi, what is task. <Commander> make a plate of toast. <Commander> bread in the sink.\n<Driver> knife. <Commander> on the chair beside the table. <Commander> clean the plate. <Driver>\nthere is a clean one, do i need to clean the other one. <Commander> you can use the clean one.\n<Driver> done. <Commander> good job. <Driver> :).\nPython script:\ntarget_bread = InteractionObject(\"Bread\", landmark = \"Sink\")\ntarget_knife = InteractionObject(\"Knife\", landmark = \"Chair\")\ntarget_toaster = InteractionObject(\"Toaster\", \"Toaster\") # need knife to slice bread\ntarget_knife.go_to() # first get knife\ntarget_knife.pickup()\ntarget_bread.go_to()\ntarget_bread.slice() # slice bread using knife\ntarget_knife.put_down() # put down the knife to free up the agent's hand\ntarget_bread_slice1 = InteractionObject(\"BreadSliced\") # instantiate a bread slice object after\nslicing bread\ntarget_bread_slice1.pickup_and_place(target_toaster) # pick up the bread slice and place it in the\ntoaster\ntarget_toaster.toggle_on() # turn on toaster to toast bread\ntarget_toaster.toggle_off() # turn off toaster to complete toasting process\n# place toast on plate\ntarget_plate = InteractionObject(\"Plate\")\ntarget_plate.clean()\ntarget_plate.put_down()\ntarget_bread_slice1.pickup_and_place(target_plate) # pick up the toasted bread slice and place it on\nthe plate\n3499\nLLM output:\ntarget_table = InteractionObject(\"Table\")\ntarget_mug = InteractionObject(\"Mug\", landmark = \"Table\")\ntarget_mug.go_to()\ntarget_mug.pickup()\n# clean mug in the sink\ntarget_sink = InteractionObject(\"Sink\")\ntarget_mug.place(target_sink)\ntarget_mug.clean()\ntarget_mug.pickup()\ntarget_coffee_machine = InteractionObject(\"CoffeeMachine\")\ntarget_coffee_machine.go_to()\ntarget_mug.place(target_coffee_machine)\ntarget_coffee_machine.toggle_on() # turn on coffee machine to make coffee\ntarget_coffee_machine.toggle_off() # turn off coffee machine after done\n3500",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8567230105400085
    },
    {
      "name": "Dialog box",
      "score": 0.6388940215110779
    },
    {
      "name": "Embodied cognition",
      "score": 0.6228465437889099
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5468078851699829
    },
    {
      "name": "Parsing",
      "score": 0.5405115485191345
    },
    {
      "name": "Context (archaeology)",
      "score": 0.537552535533905
    },
    {
      "name": "Natural language",
      "score": 0.514695942401886
    },
    {
      "name": "Action (physics)",
      "score": 0.477017879486084
    },
    {
      "name": "Robot",
      "score": 0.4646463096141815
    },
    {
      "name": "Code (set theory)",
      "score": 0.44955381751060486
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43593621253967285
    },
    {
      "name": "Dialog system",
      "score": 0.4348927438259125
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4342518448829651
    },
    {
      "name": "Programming language",
      "score": 0.43419137597084045
    },
    {
      "name": "Language model",
      "score": 0.42907026410102844
    },
    {
      "name": "Natural language processing",
      "score": 0.37336429953575134
    },
    {
      "name": "World Wide Web",
      "score": 0.20799395442008972
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 14
}