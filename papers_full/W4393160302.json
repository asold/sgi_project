{
  "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
  "url": "https://openalex.org/W4393160302",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5056549312",
      "name": "Maciej Besta",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5067303323",
      "name": "Nils Blach",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5092029666",
      "name": "Ales Kubicek",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5012247196",
      "name": "Robert Gerstenberger",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5005530395",
      "name": "Michał Podstawski",
      "affiliations": [
        "Warsaw University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5052014295",
      "name": "Lukas Gianinazzi",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5101376203",
      "name": "Joanna Gajda",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086009273",
      "name": "Tomasz Lehmann",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5106322942",
      "name": "H. Niewiadomski",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5092810033",
      "name": "Piotr Nyczyk",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026990786",
      "name": "Torsten Hoefler",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3112009340",
    "https://openalex.org/W6843504950",
    "https://openalex.org/W2558748708",
    "https://openalex.org/W6664520180",
    "https://openalex.org/W2938791877",
    "https://openalex.org/W2218076943",
    "https://openalex.org/W6644789060",
    "https://openalex.org/W1972595474",
    "https://openalex.org/W2755092149",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W6793601707",
    "https://openalex.org/W6661818267",
    "https://openalex.org/W6677316912",
    "https://openalex.org/W4221159404",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W6807384801",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W6757078134",
    "https://openalex.org/W4307786843",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W4376653732",
    "https://openalex.org/W4323651349",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4378713467",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4388098329",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4321636623",
    "https://openalex.org/W4293651439",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4287123996",
    "https://openalex.org/W4385570246",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3011667710",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W3166619165",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W3217295214",
    "https://openalex.org/W4287282215",
    "https://openalex.org/W4319323461",
    "https://openalex.org/W4287780403",
    "https://openalex.org/W2046526098",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2981117684",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385436532",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3042505432",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W2963212338",
    "https://openalex.org/W2056524360",
    "https://openalex.org/W4389518664",
    "https://openalex.org/W4323927478",
    "https://openalex.org/W4367859573",
    "https://openalex.org/W4281249911",
    "https://openalex.org/W3193434541",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W3206117777",
    "https://openalex.org/W4296594155",
    "https://openalex.org/W1977628053",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2556970231",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks",
  "full_text": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models\nMaciej Besta*1, Nils Blach*1, Ales Kubicek1, Robert Gerstenberger1,\nMichał Podstawski2, Lukas Gianinazzi1, Joanna Gajda3, Tomasz Lehmann3,\nHubert Niewiadomski3, Piotr Nyczyk3, Torsten Hoeﬂer1\n1ETH Zurich\n2Warsaw University of Technology\n3Cledar\nbestam@inf.ethz.ch, nils.blach@inf.ethz.ch, htor@inf.ethz.ch\nAbstract\nWe introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as Chain-of-\nThought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information gen-\nerated by an LLM as anarbitrary graph, where units of infor-\nmation (“LLM thoughts”) are vertices, and edges correspond\nto dependencies between these vertices. This approach en-\nables combining arbitrary LLM thoughts into synergistic out-\ncomes, distilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought transfor-\nmations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to hu-\nman thinking or brain mechanisms such as recurrence, both\nof which form complex networks.\n1 Introduction\nLarge language models (LLMs) are taking over the world of\nAI. Recent years saw a rapid development of models primar-\nily based on the decoder-only Transformer variant (Vaswani\net al. 2017), such as GPT (Radford et al. 2018, 2019; Bubeck\net al. 2023; Brown et al. 2020), PaLM (Chowdhery et al.\n2022), or LLaMA (Touvron et al. 2023b).\nPrompt engineering is a resource-efﬁcient approach for\nsolving different LLM tasks. In brief, one includes the task\ndescription within the input sent to an LLM. If this descrip-\ntion is appropriately formulated, the LLM solves the task\nusing its autoregressive token-based mechanism for gener-\nating text. Such prompts may contain example tasks with\nsolutions (few-shot prompting, also referred to as in-context\nlearning (ICL)), or even no example tasks at all (zero-shot\nprompting). In recent years it was shown that this mecha-\nnism can be used to solve a broad set of tasks that involve\nmathematical, commonsense, or symbolic reasoning.\nChain-of-Thought (CoT) (Wei et al. 2022) is an approach\nfor prompting, in which one includes the intermediate steps\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nof reasoning within the prompt (intermediate “thoughts”),\nbesides the task input/output. CoT was shown to signif-\nicantly improve the capability of LLMs to solve prob-\nlems without resorting to any model updates. One major\nimprovement over CoT, Self-Consistency with CoT (CoT-\nSC) (Wang et al. 2023b), is a scheme where multiple CoTs\nare generated, and then the best one is selected as the out-\ncome. More recently, CoT and CoT-SC were extended with\nTree of Thoughts (ToT) (Long 2023; Yao et al. 2023a; Xie\net al. 2023), which models the LLM reasoning process with\na tree. This facilitates using different paths of thoughts, and\noffers novel capabilities such as backtracking from non-\npromising outcomes. Unfortunately, the ToT approaches still\nfundamentally limit the reasoning abilities within a prompt\nby imposing the rigid tree structure on the thought process.\nIn this work, we argue that fundamentally more power-\nful prompting can be achieved by enabling LLM thoughts to\nform an arbitrary graph structure. This is motivated by nu-\nmerous phenomena such as human reasoning, brain struc-\nture, or algorithmic execution. When working on a novel\nidea, a human would not only follow a chain of thoughts\n(as in CoT) or try different separate ones (as in ToT), but\nwould actually form a more complex network of thoughts.\nFor example, one could explore a certain chain of reason-\ning, backtrack and start a new one, then realize that a certain\nidea from the previous chain could be combined with the\ncurrently explored one, and merge them both into a new so-\nlution, taking advantage of their strengths and eliminating\ntheir weaknesses. Similarly, brains form complex networks,\nwith graph-like patterns such as recurrence (Friston 2008).\nExecuting algorithms also expose networked patterns, often\nrepresented by Directed Acyclic Graphs. The correspond-\ning graph-enabled transformations bring a promise of more\npowerful prompting when applied to LLM thoughts, but they\nare not naturally expressible with CoT or ToT.\nWe observe that these (and many other) thought trans-\nformations can be naturally enabled when modeling the\nreasoning process of an LLM as a graph. For this, we\npropose Graph of Thoughts (GoT) 1, an approach that en-\nhances LLMs’ capabilities through networked reasoning\n(contribution #1). In GoT, an LLM thought is modeled\nas a vertex, while an edge is a dependency between such\n1Extended Technical Report: https://arxiv.org/abs/2308.09687\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17682\nthoughts. Using GoT, one can aggregate arbitrary thoughts\nby constructing vertices that have more than one incom-\ning edge. Overall, the graph abstraction harnessed by GoT\nseamlessly generalizes CoT and ToT to more complex\nthought patterns, without resorting to any model updates.\nYet, putting GoT to practice requires solving several de-\nsign challenges. For example, what is the best graph struc-\nture for different tasks? How to best aggregate thoughts to\nmaximize accuracy and minimize cost? To answer these and\nmany other questions, we carefully design a modular archi-\ntecture2 for implementing GoT (contribution #2), coming\nwith two design highlights. First, we enable a ﬁne-grained\ncontrol over individual thoughts. This enables us to fully\ncontrol the ongoing conversation with the LLM, and apply\nadvanced thought transformations, such as combining most\npromising thoughts from the ongoing reasoning into a new\none. Second, we ensure that our architecture can be seam-\nlessly extended with novel thought transformations, patterns\nof reasoning (i.e., graphs of thoughts), and LLM models.\nThis enables rapid prototyping of novel prompting ideas us-\ning GoT, while experimenting with different models such as\nGPT-3.5, GPT-4, or Llama 2 (Touvron et al. 2023a).\nWe illustrate several use cases for GoT (sorting, keyword\ncounting for summaries, set operations, document merging)\nand we detail how to implement them using the graph-based\nparadigm (contribution #3). We evaluate GoT and show its\nadvantages over the state of the art (contribution #4). Over-\nall, we observe that GoT is particularly well-suited for tasks\nthat can be naturally decomposed into smaller subtasks that\nare solved individually and then merged for a ﬁnal solution.\nHere, GoT outperforms other schemes, for example improv-\ning upon CoT and ToT by, respectively, \u001970% and \u001962%,\nin terms of the quality of sorting, while simultaneously re-\nducing costs by >31% over ToT.\nWe qualitatively compare GoT to other prompting\nschemes3 in Table 1. GoT is the only one to enable arbitrary\ngraph-based thought transformations within a prompt, such\nas aggregation, embracing all previously proposed schemes.\nFinally, we propose a new metric for evaluating a prompt-\ning strategy, the volume of a thought (contribution #5).\nWith this metric, we aim to understand better the differences\nbetween prompting schemes. For a given thought v, the vol-\nume of v is the number of LLM thoughts, from which one\ncan reach v using directed edges. Intuitively, these are all\nthe LLM thoughts that have had the potential to contribute\nto v. We show that GoT, by incorporating thought transfor-\nmations such as aggregation, enables thoughts to have fun-\ndamentally larger volumes than other schemes.\n2Website & Code: https://github.com/spcl/graph-of-thoughts\n3Note that we do not include a recent scheme called Graph-of-\nThought (Yao, Li, and Zhao 2023) because it is not a prompting\nscheme. While its name suggests close connections to ToT and\nCoT, as a ﬁne-tuning scheme, it resorts to model updates, and is\nthus outside the focus of this work. Similarly, the graph-of-thoughts\nrepository (qrdlgit 2023) does not enable general graph-based rea-\nsoning and harnesses instead ToT with BFS.\nScheme Sc? Mc? Tr? Ag?\nCoT /_540/remove /remove /remove\nCoT-SC /_540 /_540/remove /remove\nToT (Xie et al. 2023) /_540 /_540 /_542/remove\nToT (Long 2023) /_540 /_540 /_540/remove\nToT (Yao et al. 2023a) /_540 /_540 /_540/remove\nGoT /_540 /_540 /_540 /_540\nTable 1: Comparison of prompting schemes, with re-\nspect to the supported transformations of thoughts. “Sc?”:\nsingle chain of thoughts? “Mc?”: multiple chains of\nthoughts? “Tr?”: tree of thoughts? “Ag?”: arbitrary graph\nof thoughts? “/_540”: full support, “/_542”: partial support, “/remove”:\nno support.\n2 The GoT Framework\nWe now detail the GoT framework. We present it in Figure 1,\nand compare it to other prompting strategies.\nThe conversation with the LLM consists of user mes-\nsages (prompts) and the LLM replies (thoughts). We follow\nthe established notation (Yao et al. 2023a) and we denote a\npre-trained language model (LM) with parameters \u0012 as p\u0012.\nLowercase letters such as x;y;z;::: indicate LLM thoughts.\nFormally, GoT can be modeled as a tuple (G;T;E;R),\nwhere Gis the “LLM reasoning process” (i.e., all the LLM\nthoughts within the context, with their relationships), T are\nthe potential thought transformations,Eis an evaluator func-\ntion used to obtain scores of thoughts, and Ris a ranking\nfunction used to select most relevant thoughts.\n2.1 Reasoning Process\nWe model the reasoning process as a directed graph G =\n(V;E); V is a set of vertices and E \u0012V \u0002V is a set of\nedges. A vertex contains a solution to a problem at hand\n(be it an initial, intermediate, or a ﬁnal one). The concrete\nform of such a thought depends on the use case; it could\nbe a paragraph (in writing tasks) or a sequence of numbers\n(in sorting). A directed edge (t1;t2) indicates that thought\nt2 has been constructed using t1 as “direct input”, i.e., by\nexplicitly instructing the LLM to use t1 for generating t2.\nWe associate Gwith the LLM reasoning process. To ad-\nvance this process, one applies thought transformations to\nG. An example of such a transformation is to merge best-\nscoring (so far) thoughts into a new one. Another example\nis to loop over a thought, in order to enhance it. Note that\nthese transformations strictly extend the set of transforma-\ntions available in the CoT, CoT-SC, or ToT.\n2.2 Transformations of Thoughts\nGoT enables novel transformations of thoughts thanks to\nthe graph-based model for reasoning. We refer to them as\ngraph-enabled transformations. For example, in writing,\none could combine several input articles into one coherent\nsummary. In sorting, one could merge several sorted subar-\nrays of numbers into a ﬁnal sorted array.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17683\nInput\nOutput\nInput\nOutputOutput\nThoughts:\nUnscored\nNegative\nscore Output\nInput\nOutput\n[This work]\nInput\nPositive\nscore\nDependencies\nbetween thoughts\nAbandon thought\nBacktrack\nBasic Input-\nOutput (IO)\nLegend\nMultiple CoTs (CoT-SC)Chain-of-\n-Thought\n(CoT)\nTree of Thoughts (ToT) Graph of Thoughts (GoT)\nKey novelty:\nIntermediate\nLLM thoughts\nwithin a chain\nBranching out\nfrom a chain\nSelecting\na chain with\nthe best score\nAbandon a chain\nKey novelty\n(beyond CoT):\nHarnessing multiple\nindependent chains\nof thoughts\nKey novelty\n(beyond CoT-SC):\nGenerating several\nnew thoughts based\non a given arbitrary\nthought, exploring\nit further, and possibly\nbacktracking from it\nKey novelty (beyond ToT):\nArbitrary graph-based thought\ntransformations (aggregating \nthoughts into a new one, \nlooping over a thought to \nrefine it)\nBacktracking\nRefining\nAggregating\nthoughts\nBacktracking\nfrom a chain\nIntermediate\nthoughts are\nalso scored\nAggregating\nchains\nInput\nFigure 1: Comparison of Graph of Thoughts (GoT) to other prompting strategies.\nFormally, each such transformation can be modeled as\nT(G;p\u0012) where G = ( V;E) is the graph reﬂecting the\ncurrent state of the reasoning, and p\u0012 is the used LLM. T\nmodiﬁes Gusually by adding new vertices and their incom-\ning edges. We have G0 = T(G;p\u0012) = ( V0;E0), where\nV0 = ( V [V+) nV\u0000 and E0 = ( E [E+) nE\u0000. V+\nand E+ are new vertices and edges inserted intoGto model\nthe new thoughts and their dependencies, respectively. To\nmaximize the expressiveness of GoT – we also enable the\nuser to explicitly remove thoughts, by specifying the corre-\nsponding vertices and edges to be removed (V\u0000and E\u0000, re-\nspectively). This enables seamless incorporation of schemes\nwhere, in order to save space within the context, one can re-\nmove parts of reasoning that do not promise improvements.\nFirst, with GoT, one can aggregate arbitrary thoughts\ninto new ones, to combine and reinforce the advantages of\nthese thoughts, while eliminating their disadvantages. In the\nbasic form, in which only one new vertex is created, V+ =\nfv+gand E+ = f(v1;v+);:::; (vk;v+)g, where v1;:::;v k\nare the merged kthoughts. More generally, this enables ag-\ngregating reasoning paths, i.e., longer chains of thoughts,\nbeyond just individual thoughts. With the graph model, it is\nsimply achieved by adding outgoing edges from the vertices\nv1;:::;v k, modeling ﬁnal thoughts in several chains, into a\nsingle thought v+ combining these chains.\nAnother thought transformation is the reﬁning of a cur-\nrent thought v by modifying its content: V+ = fgand\nE+ = f(v;v)g. This loop in the graph indicates an iterated\nthought with the same connections as the original thought.\nFinally, one can generate one or more new thoughts\nbased on an existing single thoughtv. This class embraces\nanalogous reasoning steps from earlier schemes, such as ToT\nor CoT-SC. Formally, we have V+ = fv+\n1 ;:::;v +\nk gand\nE+ = f(v;v+\n1 );:::; (v;v+\nk )g.\n2.3 Scoring & Ranking Thoughts\nThoughts are scored to understand whether the current solu-\ntion is good enough. A score is modeled as a general func-\ntion E(v;G;p \u0012), where v is a thought to be evaluated. We\nuse the state of the whole reasoning process (G) in Efor\nmaximum generality, because – for example – in some eval-\nuation scenarios, scores may be relative to other thoughts.\nGoT can also rank thoughts. We model this with a func-\ntion R(G;p\u0012;h) where h speciﬁes the number of highest-\nranking thoughts in Gto be returned by R. While the spe-\nciﬁc form of Rdepends on the use case, we most often use a\nsimple yet effective strategy wherehthoughts with the high-\nest scores are returned, i.e., v1;:::;v h = R(G;p\u0012;h).\nSpeciﬁc forms of Eand Rdepend on the use case. We\ndiscuss the details in Section 4. For example, the score (or\nrank) for sorting corresponds to the count of elements cor-\nrectly sorted (or incorrectly, when using the error as a score).\n3 System Architecture & Extensibility\nThe GoT architecture consists of a set of interacting mod-\nules, see Figure 2 (the blue part). These modules are the\nPrompter (prepares the messages for the LLM), the Parser\n(extracts information from LLM thoughts), the Scoring\nmodule (veriﬁes and scores the LLM thoughts), and the\nController (coordinates the entire reasoning process, and de-\ncides on how to progress it). The Controller contains two fur-\nther important elements: the Graph of Operations (GoO) and\nthe Graph Reasoning State (GRS). GoO is a static structure\nthat speciﬁes the graph decomposition of a given task, i.e.,\nit prescribes transformations to be applied to LLM thoughts,\ntogether with their order & dependencies. GRS is a dynamic\nstructure that maintains the state of the ongoing LLM rea-\nsoning process (the history of its thoughts and their states).\nPrompter The Prompter prepares the prompts to be sent\nto the LLM. This module is responsible for the speciﬁcs of\nencoding the graph structure within the prompt. The GoT\narchitecture enables the user to implement use case speciﬁc\ngraph encodings by providing full access to the graph struc-\nture.\nParser The Parser extracts information from LLM\nthoughts. For each such thought, the Parser constructs the\nthought state, which contains this extracted information. The\nthought state is then used to update the GRS accordingly.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17684\nGoal: Build a prompt\nt\no be sent to the LLM\nLegend Architecture overview\nParser\nGoal:\n Extract\ninformation from\nLLM thought Goal: Assess the\nquality of the\nLLM's solution\nControllerGoal:  Initiate, coordinate, manage,\nand progress the GoT execution\nExternal entity Prompt Thought\nThought state\nScore\nOperation\nThought state + its\nassociated operations\nThought state\n+ thought's score\nDependencyModule of the\nGoT framework Graph of\nOpe\nrations\nGoal: Specify\nL\nLM thought\ntransformations\nGraph Reasoning State\nGoal:\n Maintain\nthe ongoing LLM\nreasoning process\nUser\nGoal: Indicate the\nt\nop-scoring thoughts\nGraph of Operations enables seamless specification of not only\nGoT, but also existing schemes such as CoT, CoT-SC, ToT\nAPI for Prompter (extensible)\n➡ Generate(t,k) //\ngenerate a prompt for k new thoughts, using thought t\n➡ //LLM params: model used, temperature, max tokens, api key, org, ...\n➡ //LLM cost features: prompt token cost, response token cost, ...\n➡ //Instances of Prompter + Parser + Graph of Operations,\n➡ //Any additional input parameters (e.g., numbers to be sorted).\n//Each of the above routines is responsible for parsing an LLM thought\n//to a corresponding Prompter routine (e.g., ParseScore parses Score).\n➡ Score(t) //score thought t\n➡ Validate(t) //generate a prompt to validate the correctness of thought t\n➡ ValidateAndImprove(t) //generate a prompt to enhance thought t,\n➡ Aggregate(t1,...,tk) //generate a prompt to combine thoughts t1, ..., tk \nAPI for Controller\nAPI for Parser (extensible)\nParseGenerate, ParseImprove, ParseScore,\nParseAggregate, ParseValidate, ...\n➡ Generate, Aggregate, Score, ... // see Prompter API\n➡ KeepBest(N) //preserves N best scoring thoughts\n➡ Repeat(k) //Repeat a given operation k times, generating k thoughts.\n    //For example, this enables \"Aggregate\" to generate multiple outcomes\n    //of the combination operation. Each such thought is maintained \n   //within the Graph Reasoning State and scored individually.\nAvailable operations when building the GoO (extensible)\nSpecifying the structure of the Graph of Operations (GoO)\nRankingScoring &\nvali\ndation\nPrompter\nLLM\nHuman\nor LL\nM\nGray block\nBl\nue block\nFigure 2: The system architecture of GoT, and the APIs of respective modules. The user can straightforwardly extend the design\ntowards new prompting schemes, experiment with novel thought transformations, and plug in different LLMs. The blue part of\nthe ﬁgure contains the architecture overview, and the green part lists the API.\nScoring & Validation Here, we verify whether a given\nLLM thought satisﬁes potential correctness conditions, and\nthen we assign it a score. Depending on how the score is\nderived, the module may consult the LLM. Moreover, de-\npending on the use case, the score may also be assigned by\na human. Finally, use cases such as sorting use simple local\nscoring functions.\nController The Controller implements a speciﬁc strategy\nfor selecting thoughts from its GRS structure. It also selects\nwhat transformations should be applied to which thoughts,\nand then passes this information to the Prompter. It also\ndecides whether the whole process should be ﬁnalized, or\nwhether the next round of interaction with the LLM should\nbe initiated. In our current design, this is dictated by the ex-\necution plan speciﬁed in the GoO.\nGoO & GRS The user constructs a GoO instance, which\nprescribes the execution plan of thought operations. The\nGoO is a static structure that is constructed once, before the\nexecution starts. Each operation object knows its predeces-\nsor and successor operations. Then, during the execution, an\ninstance of the GRS maintains the continually updated in-\nformation about the LLM reasoning process. This includes\nwhich operation has been executed so far, the states of all\nthe generated LLM thoughts, their validity and scores, and\nany other relevant information.\nThe above elements offer extensible APIs, enabling\nstraightforward implementations of different prompting\nschemes. The APIs are outlines in the green part of Figure 2,\nand detailed in the documentation.\n4 Example Use Cases\nDue to space constraints, we detail one use case (sorting).\nWe focus on its decomposition and Graph of Operations,\nwhich are central for implementing and executing any work-\nload within GoT. We consider sorting numbers 0–9 with du-\nplicates. The considered LLMs are unable to sort a sequence\nof such numbers correctly beyond a certain length consis-\ntently because duplicate counts do not match.\nIn GoT, we employ merge-based sorting: First, one de-\ncomposes the input sequence of numbers into subarrays.\nThen, one sorts these subarrays individually, and then re-\nspectively merges them into a ﬁnal solution. Figure 3 illus-\ntrates this use case together with its graph decomposition.\nHere, an LLM thought is a sequence of sorted numbers.\nMoreover, we also consider set operations, focusing on\nset intersection. They have numerous applications (partic-\nularly set intersection) in problems ranging from genome\nor document comparisons to pattern matching (Besta et al.\n2020, 2021a). Set intersection of two sets is implemented\nsimilarly as the sorting. The second input set is split into\nsubsets and the intersection of those subsets with the ﬁrst in-\nput set is determined with the help of the LLM. Afterwards\nthe resulting intersection sets are aggregated for the ﬁnal re-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17685\n.....\n.....\n..... .....\n1 4  ...  4 3\n16 numbe\nrs\n8 2  ...  1 3\n16 numbe\nrs\n1 1  ...  4 2 1 9  ...  5 4\n16 numbe\nrs\n16 numbers\nSort\nPartial\nGr\naph of Operations (GoO) for sorting 64 numbers\nPartial Partial Partial\nk = 3\nGenerate(k)\nScore\nSort\nGenerate(k)\nSort\nGenerate(k)\n1 2  ...  7 8\n16 numbe\nrs\n1 1  ...  5 7\n16 numbe\nrs\nPartial Partial\n1 2  ...  4 8\n16 numbe\nrs\nPartial\n1 2  ...  7 8\n16 numbe\nrs\n1 1  ...  5 7\n16 numbe\nrs\nPartial Partial\n1 2 ... 4 8\n16 numbe\nrs\nPartial\n78% 86%\nKeepBest(N)\nKee\np the best\nscored thoughts\nN = 1\nMerge 32 elements\nAggregate(k)\nk = 10\nk =\n 3 k = 3\nAssess how well each sequence is sorted\n64 numbers\n1 4 6 2 4  ...  9 8 7 5 4\nSplitting into four\n16-el\nement chunks\nGenerate(k) k = 1\nInput\nSort\nGenerate(k)\nk = 3\n100%\n1 2  ...  4 8\n16 numbe\nrs\nPartial\n100%\n1 3  ...  4 6\n16 numbe\nrs\nPartial\n97%\n..... .....\n.....\n1 1  ...  8 9\n32 numbe\nrs\nPartial\n100%\n1 1  ...  6 8\n32 numbe\nrs\nPartial\n97%\nMerge 64 elements\nAggregate(k)\nk = 10\nS\nScore\nS\nScore\nS\nScore\nS\nScore\nK\nG\nScore\nG\nScore\nScore Score\nK K K\nA\nG\nK\nA\nA\nScore\nScore\nK K\nK K\nK\nG\nS\nA\nK\nG\nLe\ngend\nGenerate\nDetails of the highlighted\npart of GoO are below \nDetails of the highlighted part of the GoO from above\nThe first Generate\nsplits the 64-element\ninput array into four\n16-element chunks.\nSorting is implemented\nwithin the Generate\noperation. Here, k=3\nmeans that, for each 16\nelement chunk, we\ngenerate three different\nsortings. \nHere, N=1 means that we\nmaintain a single best\nsorting outcome out of\nthe three input ones.\nHere, k=10 means that we try 10 different\naggregations of the two input 16-element subarrays.\nNote that this is an example graph decomposition. The\nstructure of connections can be arbitrarily modified.\nSort\nKeepBest\nAggregate\nFigure 3: An example graph decomposition of the sorting\nuse case in GoT. All used operations (Generate, Aggregate,\nScore, KeepBest) are described in Figure 2.\nsults. For the evaluation we use different set sizes of 32, 64\nand 128 elements and we vary the number of elements found\nin both sets to be between 25% and 75%.\nKeyword counting ﬁnds the frequency of keywords in a\ngiven category (countries in our example implementation)\nwithin the input text. GoT splits the input text into multiple\npassages, counts the keywords in each passage and aggre-\ngates the subresults. The number of passages is conﬁgurable\nand can also be left to the LLM, making it possible to treat\neach sentence as a separate passage. Here, to score a thought,\nwe ﬁrst – for each keyword – derive the absolute difference\nbetween the computed count and the correct one. We then\nsum all these differences to get the ﬁnal score.\nFinally, we also provide document merging. Here, the\ngoal is to generate a new Non-Disclosure Agreement (NDA)\ndocument based on several input ones that partially over-\nlap in terms of their contents. The goal is to ensure minimal\namount of duplication, while maximizing information reten-\ntion. Document merging is broadly applicable in, e.g., legal\nprocedures, where multiple sources of information have to\nbe combined into a single document or article. To score a\nsolution, we query the LLM for two values (3 times for each\nvalue, and take the average). The ﬁrst value corresponds to\nthe solution redundancy (10 indicates no redundancy, 0 im-\nplies at least half the information is redundant), the second\nvalue stands for information retention (10 indicates all infor-\nmation is retained, 0 says that none is retained). We compute\nthe harmonic mean of these values.\n5 The Latency-Volume Tradeoff\nWe now show that GoT improves upon previous prompting\nschemes in terms of the tradeoff between latency (number of\nhops in the graph of thoughts to reach a given ﬁnal thought)\nand volume. We deﬁne volume – for a given thought t– as\nthe number of preceding LLM thoughts that could have im-\npacted t. Formally, the volume oftis the number of thoughts\nfrom which there exists a path to tin the graph of thoughts.\nWe assume that outputting a single thought costs O(1) time\nand ﬁx the total cost to \u0002(n) for each prompting scheme.\nThe structure of the schemes is as follows. CoT-SC con-\nsists of kindependent chains originating from a single start-\ning thought. ToT is a complete k-ary tree. Finally, in GoT, a\ncomplete k-ary tree is joined at its leaves with a “mirrored”\nk-ary tree of the same size but with its edges reversed.\nThe analysis is detailed in Table 2. CoT offers a large vol-\nume of up to N, but at the cost of a high latency of N. CoT-\nScheme Latency Volume\nCoT N N\nCoT-SC N=k N=k\nToT logkN O (logkN)\nGoT logkN N\nTable 2: Comparison of prompting schemes, with respect\nto their fundamental tradeoff between latency and volume.\nGoT offers the best tradeoff.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17686\nIO CoT ToTToT2GoT0\n2\n4\n6\n8\n10\n12\n14\n16# in c o rre c tly  so rte d  e le m e n ts; t h e  lo w e r  t h e  b e t t e r\n32 elem en ts\nGoT:\nFigure\n3\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nIO CoT ToTToT2GoT0\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n64 elem en ts\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\n3.3\n3.6\n3.9\n4.2\n4.5\n4.8\nIO CoT ToTToT2GoT0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n96\n104\n112\n120\n128\n 128 elem en ts\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nTotal Cost ($); th e low er th e b etter\nL=2\nk=20 L=3\nk=10\nclipped\nL=4\nk=20\nL=7\nk=10\nGoT:\nFigure 3 clipped\nL=4\nk=20\nL/k\n=10\nGoT:\nFig. 3\nFigure 4: Accuracy and cost in sorting tasks with ChatGPT-\n3.5. Land kindicate the structure of ToT (see Section 2.2).\nSC reduces the latency by a factor of k(which corresponds\nto its branching factor), but it simultaneously decreases the\nvolume by k as well. ToT offers a latency of logkN but\nalso has low volume. GoT is the only scheme to come with\nboth a low latency of logkN and a high volume N. This\nis enabled by the fact that GoT harnesses aggregations of\nthoughts, making it possible to reach the ﬁnal thought from\nany other intermediate thought in the graph decomposition.\n6 Evaluation\nWe show the advantages of GoT over the state of the art. We\nfocus on comparing GoT to ToT, as it was shown to consis-\ntently outperform other schemes. Still, for a broad compari-\nson, we also experiment with IO, CoT, and CoT-SC. As our\nanalysis results in a large evaluation space, we present rep-\nresentative results and omit data that does not bring relevant\ninsights (e.g., CoT-SC).\n6.1 Evaluation Methodology\nWe use 100 input samples for each task and comparison\nbaseline. We set the temperature to 1.0 and use a 4k con-\ntext size unless stated otherwise. For each experiment, we\nﬁx the numbers of thoughts in respective schemes to achieve\nsimilar costs in each experiment.\nParameters We experiment extensively with the branch-\ning factor k and the number of levels L to ensure that we\ncompare GoT to cost-effective and advantageous conﬁgu-\nrations. We plot two variants of ToT: one with higher k\nand lower depth (ToT), the other with lower kbut higher L\n(ToT2). We usually aim to achieve a sweet spot in the trade-\noff between sparser generation rounds (lower k) vs. more\nrounds (larger L). Usually more responses per round is more\nexpensive (e.g., 80 vs. 60 total responses for Figure 6 but $6\nvs. $3 costs). We also try different problem sizes P (e.g., in\nsorting, P states how many numbers are to be sorted).\nUsed LLMs Due to budget restrictions, we focus on GPT-\n3.5. We also experimented with Llama 2, but it was usually\nworse than GPT-3.5 and also much slower to run, making it\ninfeasible to obtain enough samples.\n6.2 Analysis of GoT’s Advantages\nThe results of analysis are in Figure 4 (sorting), 5 (set inter-\nsection), and 6 (keyword counting and document merging);\nToTToT2\nSolved correctly\nIO GoT0\n2\n4\n6\n8\n10\n12\n14\n16\n18#incorrect elements; th e low er th e b etter\n7 6 31 29 43\n32 elem en ts\nIO CoTToTToT2GoT0\n4\n8\n12\n16\n20\n24\n28\n32\n 0 0 0 0 4\n64 elem en ts\nL=2\nk=20\nL=7\nk=10\nL=3\nk=10\nL=4\nk=200.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n3.6\n4.2\n4.8\nIO CoTToTToT2GoT0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n0 0 0 0 0\n128 elem en ts\nL=4\nk=25 L=9\nk=10\n88\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nTotal Cost ($); th e low er th e b etter\nCoT\nFigure 5: Accuracy and cost in set intersection tasks with\nChatGPT-3.5.Land kindicate the structure of ToT (see Sec-\ntions 2.2 and 5).\nsee Section 4 for the description of speciﬁc use cases. Over-\nall, GoT improves the quality of outcomes over all the con-\nsidered baselines and it reduces inference costs compared to\nToT.\nGoT vs. ToT GoT improves upon ToT and ToT2 by a\nlarge margin over all the considered problem instances. ToT\nusually comes with somewhat higher quality than ToT2, but\nsimultaneously much higher costs. GoT’s costs are always\nlower than ToT, and comparable (in some cases lower, in\nothers higher) to ToT2. For example, it reduces median er-\nror by \u001962%, thereby achieving a higher quality of sorting,\nfor P = 128 in comparison to ToT while ensuring >31%\ncost reductions. These advantages are due to GoT’s ability to\ndecompose complex tasks into simpler subtasks, solve these\nsubtasks independently, and then incrementally merge these\noutcomes into the ﬁnal result.\nGoT vs. IO and CoT GoT consistently delivers much\nhigher quality of outcomes than IO/CoT. For example, for\nsorting (P = 64), GoT’s median error is \u001965% and \u001983%\nlower than, respectively, CoT and IO. Yet, the costs of GoT\n– and ToT – are much higher than in IO and CoT. This is\nmostly due to our conﬁguration of CoT, where we do not ar-\ntiﬁcially inﬂate the lengths of the chains of reasoning if this\ndoes not improve the outcomes. The higher costs of GoT and\nToT are driven byknew thoughts built for eachGenerate\noperation; these multiple thoughts are one of the reasons for\nGoT’s superiority in quality.\nIncreasing Complexity of Tackled Problems Most im-\nportantly, the advantages of GoT in the quality increase for\nall the baselines with the growing size of the problemP. For\nexample, in sorting, while for P = 32 GoT only negligibly\nimproves upon ToT2, its median error count becomes lower\nby \u001961% for P = 64 and \u001969% for P = 128. The quar-\ntiles also become respectively better. The results for other\nschemes also follow the intuition; for example, IO becomes\nconsistently worse with the increasing P, which is expected\nas a single thought is unlikely to solve a large problem in-\nstance. Overall, this analysis illustrates that GoT is indeed\nwell-suited for elaborate problem cases, as the execution\nschedules usually become more complex with the growing\nproblem sizes.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17687\nIO CoT ToT ToT2 GoT4GoT8GoTx\n0\n5\n10\n15\n20\n25\n30\n35Number of errors; the lower the better\n0 0 1 0 8 7 25\nKeyword Counting\n0\n1\n2\n3\n4\n5\n6\n7\n8\nSolved\ncorrectly L=4\nk=20\nL=6\nk=10\nEach\nsentence\nseparately\n8 passages\nseparately\n4 passages\nseparately\nIO CoT ToT GoT GoT20\n2\n4\n6\n8Score (out of 10); the higher the better\nDocument Merging\n0\n3\n6\n9\n12\n15\nTotal Cost ($); the lower the better\nL=3\nk=10\nAggregation of fully merged NDAs\nAggregation of\npartially merged\n    NDAs\nFigure 6: Number of errors and cost in keyword counting\nand score in document merging with ChatGPT-3.5. Land k\nindicate the structure of ToT (see Sections 2.2 and 5). Num-\nber of samples for document merging: 50; context size for\ndocument merging: 16k tokens.\n6.3 Discussion on Task Decomposition\nWhen splitting a task into subtasks and then solving these\nsubtasks, the size of responses and the input (in tokens) are\nreduced proportionally to the degree of the task decomposi-\ntion. However, the “static” part of the prompt (i.e., few-shot\nexamples) may become a signiﬁcant overhead (see GoT4 to\nGoT8 in Figure 6). Here, we observe that these few-shot ex-\namples can usually also be reduced in size (e.g., the passages\nused to demonstrate keyword counting can also be made\nsmaller and still be indicative of the actual input size), thus\nactively working towards decreasing the cost (e.g., see the\ndifference between GoT8 and GoTx in Figure 6).\nThe overall goal when conducting graph decomposition is\nto break down a task to the point, where the LLM can solve\nit correctly for the majority of time using a single prompt\n(or with a few additional improvement steps). This signiﬁ-\ncantly lowers the number of improvement/reﬁnement steps\nneeded during the later stages of the graph exploration. Fur-\nthermore, as indicated by our results, combining or concate-\nnating subresults is usually an easier task than solving large\ntask instances from scratch. Hence, the LLM is often suc-\ncessful when aggregating the ﬁnal solution.\n7 Related Work\nWe summarize relations between GoT and related work.\nPrompting Paradigms & Approaches We detail differ-\nent prompting paradigms in Section 1 and Table 1. There are\nnumerous other works related to prompting, including Plan-\nand-Solve (Wang et al. 2023a), a scheme by Fu et al. (Fu\net al. 2022), the self-taught reasoner (Zelikman et al. 2022),\na scheme by Shum et al. (Shum, Diao, and Zhang 2023), au-\ntomatic prompt generation (Shin et al. 2020; Li and Liang\n2021; Lester, Al-Rfou, and Constant 2021), concurrent ex-\npansion of brief answers in the form of bullet points (Ning\net al. 2023), or selecting the best prompt out of a candidate\nset (Zhou et al. 2022). Most of these schemes could be ex-\npressed by the GoT abstraction.\nPrompt Chaining In prompt chaining, one cascades dif-\nferent LLMs (Creswell, Shanahan, and Higgins 2022; Nye\net al. 2021; Wu, Terry, and Cai 2022; Dohan et al. 2022;\nQiao et al. 2023; Wu et al. 2022). One could easily extend\nGoT so that it can serve as the execution engine for these\nschemes.\nSelf-Reﬂection & Self-Evaluation Self-reﬂection and\nself-evaluation were introduced recently (Shinn et al. 2023;\nPaul et al. 2023; Madaan et al. 2023; Xie et al. 2023; Zhu\net al. 2023). In GoT, we partially rely on self-evaluation\nwhen expanding the graph of thoughts within a prompt.\nLLMs & Planning There are many works on how to plan\ncomplex tasks with LLMs (Huang et al. 2022a,b; Zhang\net al. 2023; Yao et al. 2023b; Yang et al. 2023; Wang et al.\n2023c). GoT could be seen as a generic framework that\ncould potentially be used to enhance such schemes, by offer-\ning a paradigm for generating complex graph-based plans.\nGraphs & Graph Computing Graphs have become an\nimmensely popular and important part of the general com-\nputing landscape (Lumsdaine et al. 2007; Malewicz et al.\n2010; Gregor and Lumsdaine 2005a,b; Sakr et al. 2021). Re-\ncently, there has been a growing interest in domains such as\ngraph databases (Robinson et al. 2015; Besta et al. 2022b,\n2023b,d,c), graph pattern matching (Fan et al. 2010; Cheng\net al. 2008; Teixeira et al. 2015; Besta et al. 2021a,b, 2022d),\ngraph streaming (Feng, Meng, and Ammar 2015; Dhulipala,\nBlelloch, and Shun 2019; Besta et al. 2023a), and graph\nmachine learning as well as graph neural networks (Hamil-\nton, Ying, and Leskovec 2017; Wu et al. 2021; Zhou et al.\n2020; Zhang, Cui, and Zhu 2022; Chami et al. 2020; Bron-\nstein et al. 2017; Besta et al. 2022a,c; Gianinazzi et al. 2021;\nScarselli et al. 2008). In this work, we harness the graph ab-\nstraction as a key mechanism that enhances prompting capa-\nbilities in LLMs.\n8 Conclusion\nPrompt engineering is one of the central new domains of\nthe large language model (LLM) research. It enables using\nLLMs efﬁciently, without any model updates. However, de-\nsigning effective prompts is a challenging task.\nIn this work, we propose Graph of Thoughts (GoT), a new\nparadigm that enables the LLM to solve different tasks effec-\ntively without any model updates. The key idea is to model\nthe LLM reasoning as an arbitrary graph, where thoughts\nare vertices and dependencies between thoughts are edges.\nThis enables novel transformations of thoughts, such as ag-\ngregation. Human’s task solving is often non-linear, and it\ninvolves combining intermediate solutions into ﬁnal ones,\nor changing the ﬂow of reasoning upon discovering new in-\nsights. GoT reﬂects this with its graph structure.\nGoT outperforms other prompting schemes, for example\nensuring 62% increase in the quality of sorting over ToT,\nwhile simultaneously reducing costs by>31%. We also pro-\npose a novel metric for a prompting scheme, the volume of\na thought, to indicate the scope of information that a given\nLLM output could carry with it, where GoT also excels. This\nprovides a step towards more principled prompt engineering.\nThe graph abstraction has been the foundation of several\nsuccessful designs in computing and AI over last decades,\nfor example AlphaFold for protein predictions. Our work\nharnesses it within the realm of prompt engineering.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17688\nAcknowledgements\nWe thank Hussein Harake, Colin McMurtrie, Mark Klein,\nAngelo Mangili, and the whole CSCS team granting access\nto the Ault and Daint machines, and for their excellent tech-\nnical support. We thank Timo Schneider for help with infras-\ntructure at SPCL. This project received funding from the Eu-\nropean Research Council (Project PSAP, No. 101002047),\nand the European High-Performance Computing Joint Un-\ndertaking (JU) under grant agreement No. 955513 (MAEL-\nSTROM). This project was supported by the ETH Future\nComputing Laboratory (EFCL), ﬁnanced by a donation from\nHuawei Technologies. This project received funding from\nthe European Union’s HE research and innovation pro-\ngramme under the grant agreement No. 101070141 (Project\nGLACIATION).\nReferences\nBesta, M.; Fischer, M.; Kalavri, V .; Kapralov, M.; and Hoeﬂer,\nT. 2023a. Practice of Streaming Processing of Dynamic Graphs:\nConcepts, Models, and Systems. IEEE Transactions on Parallel\nand Distributed Systems, 34(6): 1860–1876.\nBesta, M.; Gerstenberger, R.; Blach, N.; Fischer, M.; and Hoeﬂer,\nT. 2023b. GDI: A Graph Database Interface Standard. https://\ngithub.com/spcl/GDI-RMA. Accessed: 2023-09-05.\nBesta, M.; Gerstenberger, R.; Peter, E.; et al. 2023c. Demystifying\nGraph Databases: Analysis and Taxonomy of Data Organization,\nSystem Designs, and Graph Queries. ACM Comput. Surv., 56(2).\nBesta, M.; Grob, R.; Miglioli, C.; et al. 2022a. Motif Prediction\nwith Graph Neural Networks. In 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, KDD ’22, 35–45.\nBesta, M.; Iff, P.; Scheidl, F.; Osawa, K.; Dryden, N.; Podstawski,\nM.; Chen, T.; and Hoeﬂer, T. 2022b. Neural Graph Databases. In\nFirst Learning on Graphs Conference, volume 198 ofProceedings\nof Machine Learning Research, 31:1–31:38. PMLR.\nBesta, M.; Kanakagiri, R.; Kwa ´sniewski, G.; et al. 2021a. SISA:\nSet-Centric Instruction Set Architecture for Graph Mining on\nProcessing-in-Memory Systems. In 54th Annual IEEE/ACM Inter-\nnational Symposium on Microarchitecture, MICRO ’21, 282–297.\nBesta, M.; et al. 2020. Communication-Efﬁcient Jaccard Similar-\nity for High-Performance Distributed Genome Comparisons. In\nIEEE International Parallel and Distributed Processing Sympo-\nsium, IPDPS ’20, 1122–1132.\nBesta, M.; et al. 2021b. GraphMineSuite: Enabling High-\nPerformance and Programmable Graph Mining Algorithms with\nSet Algebra. Proc. VLDB Endow., 14(11): 1922–1935.\nBesta, M.; et al. 2022c. Parallel and Distributed Graph Neural Net-\nworks: An In-Depth Concurrency Analysis. arXiv:2205.09702.\nBesta, M.; et al. 2022d. ProbGraph: High-Performance and High-\nAccuracy Graph Mining with Probabilistic Set Representations. In\nInternational Conference on High Performance Computing, Net-\nworking, Storage and Analysis, SC ’22. IEEE.\nBesta, M.; et al. 2023d. The Graph Database Interface: Scaling\nOnline Transactional and Analytical Graph Workloads to Hun-\ndreds of Thousands of Cores. In International Conference for\nHigh Performance Computing, Networking, Storage and Analysis,\nSC ’23. ACM.\nBronstein, M. M.; Bruna, J.; LeCun, Y .; Szlam, A.; and Van-\ndergheynst, P. 2017. Geometric Deep Learning: Going beyond\nEuclidean data. IEEE Signal Processing Magazine, 34(4): 18–42.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; et al. 2020. Language Models are Few-Shot Learn-\ners. In Advances in Neural Information Processing Systems\n(NeurIPS ’20), volume 33, 1877–1901. Curran Associates.\nBubeck, S.; et al. 2023. Sparks of Artiﬁcial General Intelligence:\nEarly experiments with GPT-4. arXiv:2303.12712.\nChami, I.; et al. 2020. Machine Learning on Graphs: A Model and\nComprehensive Taxonomy. arXiv:2005.03675.\nCheng, J.; Yu, J. X.; Ding, B.; Philip, S. Y .; and Wang, H. 2008.\nFast Graph Pattern Matching. In24th International Conference on\nData Engineering, ICDE ’08, 913–922. IEEE.\nChowdhery, A.; et al. 2022. PaLM: Scaling Language Modeling\nwith Pathways. arXiv:2204.02311.\nCreswell, A.; Shanahan, M.; and Higgins, I. 2022. Selection-\nInference: Exploiting Large Language Models for Interpretable\nLogical Reasoning. arXiv:2205.09712.\nDhulipala, L.; Blelloch, G. E.; and Shun, J. 2019. Low-Latency\nGraph Streaming Using Compressed Purely-Functional Trees. In\n40th ACM SIGPLAN Conference on Programming Language De-\nsign and Implementation, PLDI ’19, 918–934.\nDohan, D.; Xu, W.; Lewkowycz, A.; Austin, J.; Bieber, D.; et al.\n2022. Language Model Cascades. In Beyond Bayes: Paths To-\nwards Universal Reasoning Systems, Workshop at ICML ’22.\nFan, W.; Li, J.; Ma, S.; Tang, N.; Wu, Y .; and Wu, Y . 2010. Graph\nPattern Matching: From Intractable to Polynomial Time. Proc.\nVLDB Endow., 3(1–2): 264–275.\nFeng, G.; Meng, X.; and Ammar, K. 2015. DISTINGER: A dis-\ntributed graph data structure for massive dynamic graph process-\ning. In Proccedings of the IEEE International Conference on Big\nData, Big Data ’15, 1814–1822.\nFriston, K. 2008. Hierarchical Models in the Brain. PLOS Com-\nputational Biology, 4(11): 1–24.\nFu, Y .; et al. 2022. Complexity-Based Prompting for Multi-Step\nReasoning. arXiv:2210.00720.\nGianinazzi, L.; Fries, M.; Dryden, N.; et al. 2021. Learning Com-\nbinatorial Node Labeling Algorithms. arXiv:2106.03594.\nGregor, D.; and Lumsdaine, A. 2005a. Lifting Sequential Graph\nAlgorithms for Distributed-Memory Parallel Computation. SIG-\nPLAN Not., 40(10): 423–437.\nGregor, D.; and Lumsdaine, A. 2005b. The Parallel BGL:\nA generic library for distributed graph computations. Parallel\nObject-Oriented Scientiﬁc Computing (POOSC).\nHamilton, W. L.; Ying, R.; and Leskovec, J. 2017. Representation\nLearning on Graphs: Methods and Applications. Bulletin of the\nTechnical Committee on Data Engineering, 40(3): 52–74.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.\nLanguage Models as Zero-Shot Planners: Extracting Actionable\nKnowledge for Embodied Agents. In 39th International Confer-\nence on Machine Learning, volume 162 of Proceedings of Ma-\nchine Learning Research, 9118–9147. PMLR.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; et al. 2022b. In-\nner Monologue: Embodied Reasoning through Planning with Lan-\nguage Models. arXiv:2207.05608.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power of\nScale for Parameter-Efﬁcient Prompt Tuning. In Conference on\nEmpirical Methods in Natural Language Processing, EMNLP ’21,\n3045–3059. Association for Computational Linguistics.\nLi, X. L.; and Liang, P. 2021. Preﬁx-Tuning: Optimizing Contin-\nuous Prompts for Generation. arXiv:2101.00190.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17689\nLong, J. 2023. Large Language Model Guided Tree-of-Thought.\narXiv:2305.08291.\nLumsdaine, A.; Gregor, D.; Hendrickson, B.; and Berry, J. 2007.\nChallenges in Parallel Graph Processing. Parallel Processing Let-\nters, 17(1): 5–20.\nMadaan, A.; et al. 2023. Self-Reﬁne: Iterative Reﬁnement with\nSelf-Feedback. arXiv:2303.17651.\nMalewicz, G.; Austern, M. H.; Bik, A. J.; et al. 2010. Pregel: A\nSystem for Large-Scale Graph Processing. In International Con-\nference on Management of Data, SIGMOD ’10, 135–146. ACM.\nNing, X.; et al. 2023. Skeleton-of-Thought: Large Language Mod-\nels Can Do Parallel Decoding. arXiv:2307.15337.\nNye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski, H.; Austin,\nJ.; Bieber, D.; et al. 2021. Show Your Work: Scratchpads for Inter-\nmediate Computation with Language Models. arXiv:2112.00114.\nPaul, D.; et al. 2023. REFINER: Reasoning Feedback on Interme-\ndiate Representations. arXiv:2304.01904.\nQiao, S.; Ou, Y .; Zhang, N.; Chen, X.; Yao, Y .; et al. 2023. Reason-\ning with Language Model Prompting: A Survey. In 61st Annual\nMeeting of the Association for Computational Linguistics, ACL\n’23, 5368–5393. Association for Computational Linguistics.\nqrdlgit. 2023. graph-of-thoughts Repository. https://github.com/\nqrdlgit/graph-of-thoughts. Accessed: 2023-10-11.\nRadford, A.; et al. 2018. Improving Language Understanding by\nGenerative Pre-Training. https://openai.com/research/language-\nunsupervised. Accessed: 2023-09-06.\nRadford, A.; et al. 2019. Language Models are Unsupervised\nMultitask Learners. https://openai.com/research/better-language-\nmodels. Accessed: 2023-09-06.\nRobinson, I.; et al. 2015. Graph Databases: New Opportunities\nfor Connected Data. O’Reilly Media, 2nd edition.\nSakr, S.; Bonifati, A.; V oigt, H.; Iosup, A.; Ammar, K.; Angles,\nR.; et al. 2021. The Future is Big Graphs: A Community View on\nGraph Processing Systems. Commun. ACM, 64(9): 62–71.\nScarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and Mon-\nfardini, G. 2008. The Graph Neural Network Model. IEEE Trans-\nactions on Neural Networks, 20(1): 61–80.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and Singh, S.\n2020. AutoPrompt: Eliciting Knowledge from Language Models\nwith Automatically Generated Prompts. arXiv:2010.15980.\nShinn, N.; et al. 2023. Reﬂexion: Language Agents with Verbal\nReinforcement Learning. arXiv:2303.11366.\nShum, K.; Diao, S.; and Zhang, T. 2023. Automatic Prompt\nAugmentation and Selection with Chain-of-Thought from Labeled\nData. arXiv:2302.12822.\nTeixeira, C. H. C.; et al. 2015. Arabesque: A System for Dis-\ntributed Graph Mining. In 25th Symposium on Operating Systems\nPrinciples, SOSP ’15, 425–440. ACM.\nTouvron, H.; et al. 2023a. Llama 2: Open Foundation and Fine-\nTuned Chat Models. arXiv:2307.09288.\nTouvron, H.; et al. 2023b. LLaMA: Open and Efﬁcient Foundation\nLanguage Models. arXiv:2302.13971.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; et al. 2017.\nAttention is All you Need. In Advances in Neural Information\nProcessing Systems (NIPS ’17), volume 30. Curran Associates.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.; and\nLim, E.-P. 2023a. Plan-and-Solve Prompting: Improving Zero-\nShot Chain-of-Thought Reasoning by Large Language Models. In\n61st Annual Meeting of the Association for Computational Lin-\nguistics, ACL ’23, 2609–2634. Association for Computational\nLinguistics.\nWang, X.; et al. 2023b. Self-Consistency Improves Chain of\nThought Reasoning in Language Models. In Eleventh Interna-\ntional Conference on Learning Representations, ICLR ’23.\nWang, Z.; et al. 2023c. Describe, Explain, Plan and Select: Interac-\ntive Planning with Large Language Models Enables Open-World\nMulti-Task Agents. InAdvances in Neural Information Processing\nSystems (NeurIPS ’23), volume 36. Curran Associates.\nWei, J.; et al. 2022. Chain-of-Thought Prompting Elicits Reason-\ning in Large Language Models. arXiv:2201.11903.\nWu, T.; Jiang, E.; Donsbach, A.; Gray, J.; Molina, A.; Terry, M.;\nand Cai, C. J. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In Extended Ab-\nstracts of the Conference on Human Factors in Computing Sys-\ntems, CHI EA ’22. ACM.\nWu, T.; Terry, M.; and Cai, C. J. 2022. AI Chains: Transparent and\nControllable Human-AI Interaction by Chaining Large Language\nModel Prompts. In Conference on Human Factors in Computing\nSystems, CHI ’22. ACM.\nWu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Yu, P. S.\n2021. A Comprehensive Survey on Graph Neural Networks.IEEE\nTransactions on Neural Networks and Learning Systems, 32(1): 4–\n24.\nXie, Y .; et al. 2023. Self-Evaluation Guided Beam Search for Rea-\nsoning. In Advances in Neural Information Processing Systems\n(NeurIPS ’23), volume 36. Curran Associates.\nYang, S.; Nachum, O.; Du, Y .; Wei, J.; Abbeel, P.; and Schuur-\nmans, D. 2023. Foundation Models for Decision Making: Prob-\nlems, Methods, and Opportunities. arXiv:2303.04129.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Grifﬁths, T. L.; Cao, Y .; and\nNarasimhan, K. R. 2023a. Tree of Thoughts: Deliberate Problem\nSolving with Large Language Models. In Advances in Neural In-\nformation Processing Systems (NeurIPS ’23), volume 36. Curran\nAssociates.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; et al. 2023b. ReAct: Synergizing\nReasoning and Acting in Language Models. In Eleventh Interna-\ntional Conference on Learning Representations, ICLR ’23.\nYao, Y .; Li, Z.; and Zhao, H. 2023. Beyond Chain-of-Thought, Ef-\nfective Graph-of-Thought Reasoning in Large Language Models.\narXiv:2305.16582.\nZelikman, E.; et al. 2022. STaR: Bootstrapping Reasoning With\nReasoning. In Advances in Neural Information Processing Sys-\ntems (NeurIPS ’22), volume 35, 15476–15488. Curran Associates.\nZhang, S.; Chen, Z.; Shen, Y .; Ding, M.; et al. 2023. Planning\nwith Large Language Models for Code Generation. In Eleventh\nInternational Conference on Learning Representations, ICLR ’23.\nZhang, Z.; Cui, P.; and Zhu, W. 2022. Deep Learning on Graphs: A\nSurvey. IEEE Transactions on Knowledge and Data Engineering,\n34(1): 249–270.\nZhou, J.; Cui, G.; Hu, S.; et al. 2020. Graph neural networks: A\nreview of methods and applications. AI Open, 1: 57–81.\nZhou, Y .; et al. 2022. Large Language Models Are Human-Level\nPrompt Engineers. arXiv:2211.01910.\nZhu, X.; Wang, J.; Zhang, L.; Zhang, Y .; Huang, Y .; Gan, R.;\nZhang, J.; and Yang, Y . 2023. Solving Math Word Problems via\nCooperative Reasoning induced Language Models. In 61st An-\nnual Meeting of the Association for Computational Linguistics,\nACL ’23, 4471–4485. Association for Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n17690",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.48604851961135864
    },
    {
      "name": "Graph",
      "score": 0.4515990614891052
    },
    {
      "name": "Cognitive science",
      "score": 0.43000757694244385
    },
    {
      "name": "Epistemology",
      "score": 0.38128435611724854
    },
    {
      "name": "Psychology",
      "score": 0.3612373471260071
    },
    {
      "name": "Linguistics",
      "score": 0.34786587953567505
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2771519422531128
    },
    {
      "name": "Philosophy",
      "score": 0.2024046778678894
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I108403487",
      "name": "Warsaw University of Technology",
      "country": "PL"
    }
  ]
}