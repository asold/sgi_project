{
  "title": "Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization",
  "url": "https://openalex.org/W3102692503",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2402573852",
      "name": "Kunal Chawla",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2990530823",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970480900",
    "https://openalex.org/W4289306372",
    "https://openalex.org/W2946358633",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2962731009",
    "https://openalex.org/W2539350388",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2962912551",
    "https://openalex.org/W2901906577",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2902624892",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2467834614",
    "https://openalex.org/W2250616809",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2970441781",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2946393904",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W2971232986",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2890969459",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2933966104",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963241138",
    "https://openalex.org/W2962750014",
    "https://openalex.org/W2807895655",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2914442349",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2965033324",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3033129824",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2951883832",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W2986893290"
  ],
  "abstract": "Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2340–2354\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n2340\nSemi-supervised Formality Style Transfer using Language Model\nDiscriminator and Mutual Information Maximization\nKunal Chawla\nGeorgia Institute of Technology\nAtlanta, GA\nkunalchawla@gatech.edu\nDiyi Yang\nGeorgia Institute of Technology\nAtlanta, GA\ndyang888@gatech.edu\nAbstract\nFormality style transfer is the task of con-\nverting informal sentences to grammatically-\ncorrect formal sentences, which can be used\nto improve performance of many downstream\nNLP tasks. In this work, we propose a semi-\nsupervised formality style transfer model that\nutilizes a language model-based discriminator\nto maximize the likelihood of the output sen-\ntence being formal, which allows us to use\nmaximization of token-level conditional prob-\nabilities for training. We further propose to\nmaximize mutual information between source\nand target styles as our training objective in-\nstead of maximizing the regular likelihood\nthat often leads to repetitive and trivial gen-\nerated responses. Experiments showed that\nour model outperformed previous state-of-the-\nart baselines signiﬁcantly in terms of both au-\ntomated metrics and human judgement. We\nfurther generalized our model to unsupervised\ntext style transfer task, and achieved signiﬁ-\ncant improvements on two benchmark senti-\nment style transfer datasets.\n1 Introduction\nText style transfer is the task of changing the style\nof a sentence while preserving the content. It has\nmany useful applications, such as changing emo-\ntion of a sentence, removing biases in natural lan-\nguage, and increasing politeness in text (Sennrich\net al., 2016; Pryzant et al.; Rabinovich et al., 2017;\nYang et al., 2019; Chen et al., 2018).\nThere is a wide availability of “informal” data\nfrom online sources, yet current Natural Language\nProcessing (NLP) tasks and models could not lever-\nage or achieve good performance for such data due\nto informal expressions, and grammatical, spelling\nand semantic errors. Hence, formality style transfer,\na speciﬁc style transfer task that aims to preserve\nthe content of an informal sentence while mak-\ning it semantically and grammatically correct, has\nInformalI ﬂippin’ LOVE that movie, sweeeet!\nFormal I truly enjoy that movie.\nInformalwe was hanging out a little.\nFormal We were spending a small amount of time together.\nTable 1: Examples of (formal, informal) sentence pairs.\nrecently received a growing amount of attention.\nSome examples are given in Table 1.\nThe most widely-used models for formality style\ntransfer are based on a variational auto-encoder ar-\nchitecture, trained on parallel text data of (informal,\nformal) style sentence pairs with same content (Jing\net al., 2019). However, there is still a lot of incon-\nsistencies between human-generated sentences and\noutputs of current models, largely due to the lim-\nited availability of parallel data. In contrast, large\namount of data consisting of sentences with just ei-\nther informal or formal labels is relatively easier to\ncollect. To tackle the training data bottleneck, we\npropose a semi-supervised approach for formality\nstyle transfer, using both human-annotated parallel\ndata and large amount of unlabeled data.\nFollowing the success of Generative Adversar-\nial Nets (GAN) (Goodfellow et al., 2014), binary\nclassiﬁers are often used on the generator outputs\nin unsupervised text style transfer to ensure that\ntransferred sentences are similar to sentences in the\ntarget domain (Shen et al., 2017; Hu et al., 2017).\nHowever, Yang et al. (2018) showed that using a\nLanguage Model instead of a binary classiﬁer can\nprovide stronger, more stable training loss to the\nmodel, as it leverages probability of belonging to\nthe target domain for each token in the sentence.\nWe extend this line of work to semi-supervised\nformality style transfer, and propose to use two lan-\nguage models (one for source style and another for\ntarget) to help the model utilize information from\nboth styles for training.\nMoreover, style transfer models are usually\n2341\nFigure 1: Model architecture. Here (x,y) ∈Dis a (source, target) style sentence pair with same content, and S\nand T are source and target styles respectively. The parameters for encoder and decoder are shared across forward\nand backward style transfer directions. The red arrow corresponds to the cyclic reconstruction loss. Cyclic and\ndiscriminator losses are trained on x∈U, unsupervised class-labeled data.\ntrained by maximizing P(y|x), where (x,y) is a\n(informal, formal) sentence pair. Such models tend\nto generate trivial outputs, often involving high-\nfrequency phrases in the target domain (Li et al.,\n2016b). Building on prior work, to introduce more\ndiversity and connections between the input and\noutput, we propose to maximize mutual informa-\ntion (MMI) between source and target styles, which\ntake into account not only the dependency of out-\nput on input, but also the likelihood that the input\ncorresponds to the output. While this has only been\ndone at test-time so far, we extend this approach to\ntrain our model with MMI objective.\nWe evaluate our proposed models that in-\ncorporate both the language model discrimina-\ntors and mutual information maximization on\nGrammarly Yahoo Answers Corpus (GYAFC)\nDataset (Rao and Tetreault, 2018). Experiments\nshowed that our simple semi-supervised formal-\nity style transfer model outperformed state-of-\nthe-art methods signiﬁcantly, in terms of both\nautomatic metrics (BLEU) and human evalua-\ntion. We further show that our approach can be\nused for unsupervised style transfer, as demon-\nstrated by signiﬁcant improvements over base-\nlines on two sentiment style benchmarks: Yelp\nand Amazon Sentiment Transfer Corpus, where\nparallel data is not available. We have pub-\nlicly released our code at https://github.com/\nGT-SALT/FormalityStyleTransfer.\n2 Related Works\nSequence-to-Sequence Models Text style trans-\nfer is often modeled as a sequence-to-sequence\n(seq2seq) task (Yang et al., 2018; Xu et al., 2019;\nLi et al., 2018). A classical architecture for seq2seq\nmodels is variational autoencoders(V AE) which\nuses an “encoder” to encode the input sentence into\na hidden representation, and then uses a “decoder”\nto generate the new sentences (Shen et al., 2017;\nHu et al., 2017; Jing et al., 2019). Long Short Term\nMemory(LSTMs) (Hochreiter and Schmidhuber,\n1997), and more recently, self-attention based CNN\narchitectures (Vaswani et al., 2017) are often used\nas base architectures for such models.\nPre-training of the encoders on multiple tasks\nand datasets has been shown to be effective (Devlin\net al., 2018; Liu et al., 2019) in improving perfor-\nmances of individual tasks. These models are often\ntrained with the cross-entropy loss (Vaswani et al.,\n2017) on the output tokens, or in other words, max-\nimising P(y|x) where (x,y) is a pair of source\nand target style sentence respectively. Li et al.\n(2016b) showed that maximising mutual informa-\ntion (MMI) M(x,y) during test-time between the\nsource and target instead can lead to more diverse\nand appropriate outputs in seq2seq models. Some\nother works (Zhang et al., 2018) maximize a varia-\ntional lower bound on pairwise mutual information.\nWe use a denoising auto-encoder BART (Lewis\net al., 2019) trained with MMI objective.\nSemi-Supervised and Unsupervised Style\nTransfer Some approaches like Li et al. (2018)\nand Lai et al. (2019) focus on deleting style-related\nkeywords to make content style-independent.\nHowever, other works hypothesize that content\nand style cannot be separated, and use techniques\nsuch as back-translation (Lample et al., 2019),\n2342\ncross-projection between styles in latent space\n(Shang et al., 2019a), reinforcement learning-based\none step model (Luo et al., 2019), and iterative\nmatching and translation (Jin et al., 2019). Follow-\ning Goodfellow et al. (2014), using a generator\nalong with a style classiﬁer is often used for\nunsupervised tasks (Shen et al., 2017; Hu et al.,\n2017; Fu et al., 2018). However, recent work\nsuggests (Yang et al., 2018) that using Language\nModels instead of CNN discriminators can result\nin more ﬂuent, meaningful outputs. Maximizing\nlikelihood of reconstruction of the input from the\ngenerated output has been used in both image\ngeneration (Zhu et al., 2017) and text style transfer\n(Shang et al., 2019b; Luo et al., 2019; Logeswaran\net al., 2018) to improve performance. Motivated\nby these work, we use language models for our\ndiscriminator, and maximize cyclic reconstruction\nlikelihood as part of our training objective.\nFormality Style Transfer Grammarly (Rao and\nTetreault, 2018) released a large-scale dataset for\nFormality Style Transfer, and tested several rule-\nbased and deep neural networks-based baselines.\nCNN-based discriminators and cyclic reconstruc-\ntion objective have been used (Xu et al., 2019) in a\nsemi-supervised setting. Wang et al. (2019) used a\ncombination of original and rule-based processed\nsentences to train the model. There is also evidence\nthat using multi-task learning (Niu et al., 2018) and\nmodels pretrained on a large scale corpus (Wang\net al., 2019) improve performance. This work uses\na BART model (Lewis et al., 2019) pretrained on\nCNN-DM dataset (Nallapati et al., 2016) for our\nbase architecture.\n3 Method\nThis section presents our semi-supervised formal-\nity style transfer model. We detail the task and our\nbase architecture in Section 3.1. We add a language\nmodel-based discriminator to the model, described\nin Section 3.2, and explain the maximization of\nmutual information in Section 3.3. The ﬁnal archi-\ntecture for our model is summarized in Section 3.4\nand shown in Figure 1\n3.1 Formality Style Transfer\nDeﬁne T (=“formal” in our case) as the target\nstyle and S (=“informal”) as the source style\nfor the formality style transfer task. Let Dbe the\nparallel dataset containing (source, target) style\nsentence pairs and U be the additional unlabeled\ndata, denoted by US for sentences with source style\nand UT for sentences with target style.\nOur base model is a variational auto-encoder\nmechanism G that generates sentences of target\nstyle. The goal is to maximize P(y|x; θG) where\nθG are the parameters of the model. This is done\nby cross-entropy loss over the target sentence to-\nkens and generated output probabilities. To lever-\nage Maximum Mutual Information objective, as\ndescribed in Section 3.3, we make the model bi-\ndirectional. It can be used to transfer source style\nto target style as well as target style to source style.\nHence, an additional input c∈{S,T}is passed to\nGspecifying the style to which the sentence is to\nbe converted. Hence, our objective for base model\nis to maximize P(y|x,T; θG).\n3.2 Language Model Discriminator\nWe add a Language model(LM) based discrimina-\ntor to the model. It functions as a binary classiﬁer\nwhich scores the formality of the output generated\nby the decoder. It includes two language models\ntrained independently on informal and formal data.\nThe “score” of a sentence by a language model\nis calculated by the product of locally normalized\nprobabilities of each token given the previous to-\nkens. Let xbe a sentence from P with label c, then\nLM(x) =\nlen(x)∏\ni=0\nP(xi|x0:i−1; θLM) (1)\nwhere xi are the tokens in xand θLM are the pa-\nrameters of the language model. The softmax-\nnormalized score of the sentence by the language\nmodels is interpreted as the classiﬁer score:\nP(T|x) = eLMT (x)\neLMT (x) + eLMS(x) (2)\nThe language model discriminator is pre-trained\non source and target data from P with the cross\nentropy loss:\nθ∗\nC = minθC\n∑\nx∈U\n(−log P(c|x; θC)) (3)\nwhere c is the label of x, θC are the parameters\nof the LM discriminator and θ∗\nC are the trained pa-\nrameters. The weights are then frozen for the train-\ning. A common training objective ((Wang et al.,\n2019; Fu et al., 2018)) is to minimize the sum of\ntranslation loss Ltrans and discriminator loss Ldisc,\n2343\ndeﬁned as:\nLtrans((x,y) ∈D) =−log (P(y|x,T; θG)))\nLdisc(x∈US) =−log (P(T|x,T; θG,θC))\nLdisc(x∈UT) =−log (P(S|x,S; θG,θC))\nθ∗\nG =minθG(\n∑\n(x,y)∈D\nLtrans(x,y) +\n∑\nx∈U\nLdisc(x))\n(4)\n3.3 Maximum Mutual Information Objective\nAs discussed, instead of using usual translation loss\nwhich maximizes P(y|x; θG) and often produces\ntrivial and repetitive content, we chose to maximize\npairwise mutual information between the source\nand the target:\nˆy=argmaxylog P(x,y)\nP(y)P(x)\n=argmaxy(log P(y|x) −log P(y))\n(5)\nFollowing (Li et al., 2016b), we introduce a param-\neter λ“forward-translation weight” to generalize\nthe MMI objective and adjust the relative weights\nof forwards and backwards translation:\nˆy=argmaxy(log P(y|x) −(1 −λ) log P(y))\n=argmaxy(λlog P(y|x) + (1−λ) log P(x|y))\nThe translation loss thus becomes:\nLtrans((x,y) ∈D) =λ log P(y|x,T; θG)\n+(1 −λ) log P(x|y,S; θG) (6)\n3.4 Overall Model Architecture\nMaking the model bi-directional also allows us to\nleverage unsupervised data using cyclical recon-\nstruction loss Lcycle, which encourages a sentence\ntranslated to the opposite style and back to be sim-\nilar to itself (Shang et al., 2019b). Let G(x,c) be\nthe output of the model for a sentence xwith target\nstyle c. Then\nLcycle(x∈US) =−log P(x|G(x,T),S; θG))\nLcycle(x∈UT) =−log P(x|G(x,S),T; θG))\nLet wdisc and wcycle denote the weights for dis-\ncriminator and cyclic loss respectively. The overall\nloss function Lfor the training step is:\nL=\n∑\n(x,y)∈D\nLtrans(x,y)\n+\n∑\nx∈U\n(wdiscLdisc(x) +wcycleLcycle(x))\n(7)\nDataset Train Valid Test\nE&M 52595 2877 1416\nF&R 51967 2788 1432\nBookCorpus 214K - -\nTwitter 211K - -\nYelp (Positive) 270K 2000 500\nYelp (Negative) 180K 2000 500\nAmazon (Positive) 277K 985 500\nAmazon (Negative) 278K 1015 500\nTable 2: The statistics of train, validation and test sets\nof all used datasets.\n4 Experiments\n4.1 Dataset\nWe used Grammarly’s Yahoo Corpus Dataset\n(GY AFC) (Rao and Tetreault, 2018) as our parallel\ndata for supervised training. The dataset is divided\ninto two sub-domains- “Entertainment and Music”\n(E&M) and “Family and Relationships” (F&R). For\nthe unsupervised data, we crawled Twitter data for\ninformal data, and we used BookCorpus data (Zhu\net al., 2015) for the formal data. In the pre-training\nstep, we train the language model discriminator on\nthe unannotated informal and formal data. The de-\ntailed process of the data collection is given in the\nAppendix. The statistics of datasets are in Table 2.\n4.2 Pre-processing and Experiment Setup\nThe text was pre-processed with Byte Pair Encod-\ning(BPE) (Shibata et al., 1999) with a vocabulary\nsize of 50,000. For pre-training, we trained the\nLM Discriminator with the unsupervised data with\ncross entropy loss. For training, we merged both\ndatasets of GY AFC and used the training objective\nas described in Section 3.4 to train the model.\nWe used Fairseq (Ott et al., 2019) library built\non top of PyTorch (Paszke et al., 2019) to run\nour experiments. We used BART-large (Lewis\net al., 2019) model pretrained on CNN-DM sum-\nmarization data (Nallapati et al., 2016) for our base\nencoder and decoder. BART was chosen because\nof its bidirectional encoder which uses words from\nboth left and right for training, as well as superior\nperformance on text generation tasks. Its training\nobjective of reconstruction from noisy text data ﬁts\nour task well. We chose the model pre-trained on\nCNN-DM dataset because of the relevance of the\ndecoder pre-trained on formal words to our task.\nBoth decoder and the encoder have 12 layers\neach with 16 attention heads and a hidden embed-\n2344\nding size of 1024. We shared the weights for en-\ncoder and decoder across the forward and back-\nward translation, using a special input token to the\nencoder. For the language models, we used a Trans-\nformer (Vaswani et al., 2017) decoder with 4 layers\nand 8 attention heads per layer.\nOne NVIDIA RTX 2080 Ti with 11GB mem-\nory was used to run the experiments with the max\ntoken size of 64. We also used update frequency\n4, increasing the effective batch size. Adam Opti-\nmizer (Kingma and Ba, 2015) was used to train the\nmodel, and the parameters learning rate, λ,wdisc\nand wcycle were ﬁne-tuned. The model was se-\nlected based on perplexity of informal to formal\ntranslation on validation data. Beam search (size\n= 10) was used to generate sentences. A length\npenalty (= 2.0) was used to reduce redundancy in\nthe output sentence. Further details on model pa-\nrameters are mentioned in Appendix.\n4.3 Evaluation Metrics\nThe result was evaluated with BLEU (Papineni\net al., 2002). We used word tokenzier and corpus\nBLEU calculator from Natural Language Toolkit\n(NLTK) (Loper and Bird, 2002) to calculate the\nBLEU score. Due to the subjective nature of the\ntask, BLEU does not capture the output of the\nmodel well. Hence, we also used human anno-\ntations for some of the models.\nAmazon Mechanical Turk was used to evalu-\nate 100 randomly sampled sentences from each\ndataset of GY AFC. To increase annotation quality,\nwe required workers located in US to have a 98%\napproval rate and at least 5000 approved HITs for\ntheir previous work on MTurk. Each sentence was\nannotated by 3 workers, who rated each generated\nsentence using the following metrics, following\n(Rao and Tetreault, 2018):\n•Content: Annotators judge if the source and\ntranslated sentence convey the same informa-\ntion on a scale of 1-6: 6: Completely equiva-\nlent, 5: Mostly equivalent, 4: Roughly equiva-\nlent, 3: Not equivalent but share some details,\n2: Not equivalent but on same topic, 1: Com-\npletely dissimilar.\n•Fluency: Workers score the clarity and ease\nof understanding of the translated sentence\non a scale from 1-5: 5: Perfect, 4: Compre-\nhensible, 3: Somewhat Comprehensible, 2:\nIncomprehensible, 1: Incomplete.\n•Formality: Workers rate the formality of the\ntranslated sentence on a scale of -3 to 3. -3:\nVery Informal, -2: Informal, -1: Somewhat\nInformal, 0: Neutral, 1: Somewhat Formal, 2:\nFormal and 3: Very Formal.\nWe also provided detailed deﬁnitions and exam-\nples to workers, which are described together with\nannotation interface in Appendix. The intra-class\ncorrelation was estimated using ICC-2k (Random\nsample of k raters rate each target) and calculated\nusing Pingouin (Vallat, 2018) Python package. It\nvaried from 0.521-0.563 for various models, indi-\ncating moderate agreement (Koo and Li, 2016). We\nthen averaged the three human-provided labels to\nobtain the rating for each sentence.\n4.4 Baselines and Model Variants\nWe compared our approach with several baseline\nmethods as follows:\n•SimpleCopy: Simply copying the source sen-\ntence as the generated output.\n•Target: Human-generated outputs.\n•Rule-based (Rao and Tetreault, 2018): Using\nhand-made rules.\n•NMT (Jhamtani et al., 2017): A LSTM\nencoder-decoder model with attention.\n•Transformer (Vaswani et al., 2017): A\nTransformer architecture with the same con-\nﬁguration as our encoder and decoder.\nWe also compared our model with previous state-\nof-the-art works:\n•Hybrid Annotations(Xu et al., 2019): Uses\nCNN-based discriminator and cyclic recon-\nstruction loss in a semi-supervised setting.\n•NMT Multi-Task(Niu et al., 2018): Solves\ntwo tasks: monolingual formality transfer and\nformality-sensitive machine translation jointly\nusing multi-task learning.\n•Pretrained w/ Rules (Wang et al., 2019):\nUses a pre-trained OpenAI GPT-2 model and\na combination of original and rule-based pro-\ncessed sentences to train the model.\nThe performances for these works were taken\nfrom the respective papers. We also introduced\nseveral variants of our model for comparison:\n2345\nE&M F&R\nModel BLEU Content Fluency FormalityBLEU Content Fluency Formality\nSimpleCopy 50.28 - - - 51.66 - - -\nTarget 99.99 5.54 4.79 2.31 100.00 5.54 4.79 2.30\nRule-based 60.37 - - - 66.40 - - -\nNMT (Jhamtani et al., 2017) 68.41 - - - 74.22 - - -\nTransformer (Vaswani et al., 2017)67.97 - - - 74.20 - - -\nHybrid Annotations (Xu et al., 2019)*69.63 5.22 4.62 1.97 74.43 5.29 4.53 2.04\nNMT Multi-task (Niu et al., 2018)72.13 - - - 75.37 - - -\nPretrained w/ Rules (Wang et al., 2019)72.70 5.38 4.51 1.67 76.87 5.64 4.63 1.78\nDual Reinforcement** (Luo et al., 2019)- - - - 41.9 - - -\nOurs Base 74.66 4.93 4.33 1.82 78.89 5.06 4.36 1.84\nw/ CNN discriminator* 75.04 - - - 79.05 - - -\nw/ LM discriminator* 75.65 5.33 4.69 2.30 79.50 5.35 4.66 2.31\nw/ LM* + MMI 76.19 - - - 79.92 - - -\nOurs* 76.52 5.35 4.81 2.38 80.29 5.42 4.74 2.31\nTable 3: Results on GY AFC Dataset. An average of 3 runs was used for each model to calculate BLEU. Models\nwith * leverage extra data via semi-supervised methods. ** represents unsupervised models. The description for\nthe models is given in Section 4.3. The best scores (besides the target) for each metric are in bold.\n•Ours Base. Pretrained uni-directional auto-\nencoder architecture from BART (Lewis et al.,\n2019) ﬁne-tuned on our data.\n•Ours w/ CNN Discriminator: A CNN archi-\ntecture with 3 layers used on the output of the\ndecoder. The discriminant was trained with\nunsupervised class-labeled data.\n•Ours w/ LM Discriminator : Two\ntransformer-based language models with 4\nlayers, used on the output of the decoder.\n•Ours w/ LM + MMI: Model trained with\nMMI objective and LM discriminator.\n•Ours: Ours Base model trained with LM dis-\ncriminator, MMI objective, and cyclic recon-\nstruction loss.\n4.5 Results\nThe results are summarized in Table 2. Compared\nto various baselines such as Pretrained w/ Rules\n(Wang et al., 2019), our proposed models achieved\nsigniﬁcant improvements with 3.82 absolute in-\ncrease of BLEU on E&M and an increase of 3.42\non F&R. By utilizing the language model discrimi-\nnator and mutual information maximization, Ours\nachieved state-of-the-art results on both subsets of\nthe GY AFC dataset in terms of BLEU, boosting the\nBLEU to 76.52 and 80.29 on E&M and F&R re-\nspectively. Our contributions increase the score by\n2-3 points compared to the ﬁne-tuned BART base-\nline as well. This validates the effectiveness of our\nsemi-supervised formality style transfer models.\nDetails on runtime and memory requirements can\nbe found in Appendix. Our contributions increase\nthe performance without increasing the test-time or\nmemory requirements signiﬁcantly.\nConsistent with this quantitative result, human\nannotation results showed that Ours produced\nmore ﬂuent and more formal outputs compared\nto our selected baselines. Pretrained w/\nRules was rated to have better content preser-\nvation, but lower ﬂuency and formality. This is\npossibly due to different approaches taken to deal\nwith slang and idiomatic expressions in language,\nas described in Section 5.3 (Type 7). Wang et al.\n(2019) tends to keep the content at the cost of for-\nmality of the output, while Xu et al. (2019) and\nour model often ignore the content. For example,\nour model’s output of “the two boys rednecked as\nhell play guitar ” is “The two boys play guitar. ”,\nomitting details like “red-neck” which are rarely\nmentioned in formal language.\nMoreover, we observed that there are compa-\nrable human annotation results between Target\nand Ours. Our model achieved slightly higher\nscores on the formality of the sentences compared\nto human-generated outputs. This may suggest\nthat our model has a tendency to increase the\nformality of a sentence, even if it loses a bit of\nmeaning preservation. We also found that addi-\ntional unsupervised data helps: compared to Ours\nBase, language model discriminator improves per-\nformance signiﬁcantly (with BLEU scores from\n74.66 to 75.65, and from 78.89 to 79.50). Note that\nour method is generic, and can be further combined\nwith baseline methods, such as Wang et al. (2019);\n2346\nModel Sentence\nInformal ﬁdy cent he is ﬁne and musclar\nHybrid Annotations (Xu et al., 2019) Fidy Cent is ﬁne and Muslim.\nPretrained w/ rules (Wang et al., 2019) Fidy Cent is a ﬁne and musclar artist.\nOurs 50 Cent is ﬁne and muscular.\nHuman-Annotation 50 Cent is ﬁne and muscular.\nInformal Plus she is a cray ****.\nHybrid Annotations She is a clay.\nPretrained w/ rules She is a cray ****.\nOurs She is not very nice.\nHuman-Annotation Also, she is a mentally unstable woman.\nInformal So far i haven’t heard that shes come back here (Arkansas)?\nHybrid Annotations I have not heard that she is in Arkansas.\nPretrained w/ rules So far, I have not heard that she is coming back here(Arkansas).\nOurs So far I have not heard that she has returned to Arkansas.\nHuman-Annotation So far I have not heard that she returned to Arkansas.\nTable 4: Some sample outputs from various models.\nNiu et al. (2018).\nWe notice that BLEU does not necessarily corre-\nlate well with improved ﬂuency, which is consistent\nwith previous studies (Rao and Tetreault, 2018; Lin\nand Och, 2004). Many ﬂuent sentences did not\ncapture the meaning of the sentence well, which\nreduces BLEU. Conversely, it is possible to have\nhigh intersection with the gold label sentence but\nstill not be ﬂuent.\nSome qualitative results from our best-\nperforming model (by BLEU score in Table 3),\nXu et al. (2019), Wang et al. (2019) and target\nsentences, are provided in Table 4. We observed\nthat our model consistently generates better\ntranslations compared to the previous methods,\nespecially in terms of dealing with proper nouns,\ninformal phrases and grammatical mistakes.\n4.6 Testing on Unsupervised data\nWe further extended our method to unsupervised\ntasks, using only cyclic reconstruction and Lan-\nguage Discriminator losses as our training objec-\ntive. Sentiment Transfer corpus (Li et al., 2018)\nfrom Yelp and Amazon was used for evaluation.\nThe statistics are given in Table 2. The corpora in-\nclude separate negative and positive sentiment data\nwithout parallel data. We followed the evaluation\nprotocol and baselines from Li et al. (2018). In\naddition to BLEU, we used two additional metrics\nfor evaluation: (1) Accuracy: The percentage of\nsentences successfully translated into positive, as\nmeasured by a separate pre-trained classiﬁer. (2)\nG-Score: The geometric Mean of accuracy and\nBLEU scores. We rank our models by G-Score, fol-\nlowing Xu et al. (2012), since there is a trade-off\nbetween accuracy and BLEU, as changing more\nwords can get better accuracy but lower content\npreservation.\nWe used the script and sentiment classiﬁer from\nLi et al. (2018) to evaluate our outputs. Results\nwere averaged for the two directions: positive-to-\nnegative sentiment transfer and negative-to-positive\nsentiment transfer, with 500 sentences in the test\nset for each direction.\nWe compared our results with previous state-of-\nthe-art approaches. Style Embedding and Multi\nDecoding (Fu et al., 2018) learn an embedding of\nthe source sentence such that a decoder can use\nit to reconstruct the sentence, but a discriminator,\nwhich tries to identify the source attribute using this\nencoding, fails. Cross-Aligned (Shen et al., 2017)\nalso encodes the source sentence into a vector, but\nthe discriminator looks at the hidden states of the\nRNN decoder.\nLi et al. (2018) extract content words by delet-\ning style-related phrases, retrieves relevant target-\nrelated phrases and combines them using a neural\nmodel. They provide three variants of their model.\nWord-level Conditional GAN (Lai et al., 2019)\nalso tries to separate content and style with a word-\nlevel conditional architecture. Dual Reinforcement\n(Luo et al., 2019) uses reinforcement learning for\nbidirectional translation without separating style\nand content. Iterative Matching (Jin et al., 2019)\n2347\nYelp Amazon\nModel ACC BLEU G-Score ACC BLEU G-Score\nSimpleCopy 2.4 18.0 6.57 18.9 39.2 27.2\nTarget 69.6 100.0 83.4 41.3 99.9 64.2\nCross Aligned (Shen et al., 2017) 73.7 3.1 15.1 74.1 0.4 5.4\nStyle Embedding (Fu et al., 2018) 8.7 11.8 10.1 43.3 10.0 20.8\nMulti Decoding (Fu et al., 2018) 47.6 7.1 18.4 68.3 5.0 18.5\nTemplate Based (Li et al., 2018) 81.7 11.8 31.0 68.7 27.1 43.1\nRetrieve Only (Li et al., 2018) 95.4 0.4 6.2 70.3 0.9 8.0\nDelete Only (Li et al., 2018) 85.7 7.5 25.4 45.6 24.6 33.5\nDelete & Retrieve (Li et al., 2018) 88.7 8.4 27.3 48.0 22.8 33.1\nDual Reinforcement (Luo et al., 2019) 85.6 13.9 34.5 - - -\nWord-level Conditional GAN (Lai et al., 2019) 87.8 9.6 29.1 77.4 6.7 22.8\nIterative Matching (Jin et al., 2019) 87.9 4.3 19.4 - - -\nOurs 86.2 14.1 34.9 68.9 28.6 44.4\nTable 5: Results on Sentiment Transfer datasets. Results were averaged across two directions: negative-to-positive\nand positive-to-negative sentiment transfer. An average of three runs was used for each directions. Here ACC and\nGM mean Accuracy and G-Score respectively. The best scores (besides the target) for each metric are in bold.\niteratively reﬁnes imperfections in the alignment\nof semantically similar sentences from the source\nand target dataset. We used the performance num-\nbers for these approaches from either the original\npapers when the evaluation protocol is similar to\nours or by evaluating publicly released outputs of\nthe models.\nWe achieved state-of-the-art results on both Yelp\nand Amazon Sentiment Transfer corpus, as shown\nin Table 5. Our model attains slightly lower accu-\nracy on sentiment classiﬁcation of output sentences,\nbut preserves more content compared to previous\nmodels, resulting in the highest G-Score on both\ndatasets. This suggests that our approach can gen-\neralize well to unsupervised style transfer tasks.\n5 Model Analysis and Discussion\nAlthough our model performed well on formality\nstyle transfer, there is still a gap compared to hu-\nman performance. To understand why the task is\nchallenging and how future research could advance\nthis direction, we take a closer look at formality\ndataset, model generation errors, and certain chal-\nlenges that existing approaches struggle with.\n5.1 Effect of Forward Translation Weight\nAs mentioned in Section 3.3, MMI objective is\nequivalent to a weighted sum of source-to-target\nand target-to-source translation. We show the effect\nof forward translation weight, λin Figure 2, and\nﬁnd that using MMI objetive helps performance\nFigure 2: Performance with forward translation weight\nas compared to baseline translation loss (which\ncorresponds to λ = 1.0). However, equivalent\nweighing of the two directions (corresponding to\nλ= 0.5) does not result in the best performance: a\nbias towards the informal to formal direction(λ=\n0.8) gives better BLEU scores. We posit that this\ncould be because unlike formal sentences, informal\nsentences do not follow a particular style: they vary\nfrom structurally correct with some mistakes to just\na collection of telegram-style keywords, and hence\nthe objective of generating this should be assigned\nless importance than the forward task.\n5.2 Cyclic and Discriminator Loss\nIn our model, we used unsupervised class labeled\ndata to train our model using cyclic and discrim-\n2348\ninator loss. We also conducted experiments to\nuse these losses for parallel data as well. How-\never, training on parallel data using these objec-\ntives in addition to MMI objective did not result\nin additional improvements, while increasing the\ntraining time and memory requirements. Partially,\nthis could be because maximizing target sentence\nprobability already captures the target style, hence\ndiscriminator loss does not help. Similarly, maxi-\nmizing Mutual Information ensures that target-to-\nsource translation is also a maximisation objective\nduring training, hence reducing the effectiveness\nof cyclic reconstruction loss. Therefore, we con-\ncluded that maximizing mutual information during\ntraining is sufﬁcient for parallel data.\n5.3 Challenges in Formality Text Transfer\nWe conduct a thorough examination of the GY AFC\ndataset and categorize the challenges into the fol-\nlowing categories:\n1. Informal Phrases and Abbreviations: Pres-\nence of “informal” phrases ( what the hell ),\nemojis ( :)) and abbreviations (omg, brb).\n2. Missing Context: A lack of context of the\nconversation (for example, “It had to be the\nchickin”) or a lack of punctuation or proper\ncapitalization cues (“can play truth or dare or\nsnake and ladders”).\n3. Named Entities: Proper nouns and popular\nreferences like “Fifty Cent” or “eBay” should\nnot be changed despite the wrong pluraliza-\ntion and capitalization, respectively. This is\nworsened by the lack of any capitalization or\npunctuation cues to ﬁnd named entities.\n4. Sarcasm and Rhetorical Questions: Rhetor-\nical questions, sarcastic language and nega-\ntions have been long-standing problems in\nNLP (Li et al., 2016a). For example, “ sure,\nbecause this is so easy” is sarcastic and should\nnot be translated literally.\n5. Repetition: Informal text often has a lot of\nredundant information. For example, “I used\nto work at the store and met him while i was\nworking there.” can be formally structured as\n“I met him while i was working at the store.”.\n6. Spellings and Grammar Errors: This is\nprevalent in most (>60%) informal sentences.\nType Input (%) Output (%) Resolved (%)\n1 16 7 56\n2 5 4 20\n3 12 3 75\n4 2 2 0\n5 3 0 100\n6 61 7 89\n7 5 0 100\nTable 6: The breakdown of challenge types for formal-\nity style transfer, and their percentages in the sourceIn-\nput, generated Output, and the percentage of challenges\nsuccessfully resolved by our model.\n7. Slang and Idiomatic Expressions: Some\nsentences have words especially nouns, ad-\njectives and adverbs that can be considered as\nslang, idiom, and even discriminatory.\nWe randomly sampled 100 sentences from the\ndataset to estimate the prevalence of such chal-\nlenges. We also examined the output from our\nmodel to analyze if a challenge has been solved or\nstill presents an issue to the model. The result is\nsummarized in Table 6. We found that our models\nresolved most spelling and grammatical mistakes\n(Type 6), and performs well with avoiding repeti-\ntion (Type 5). However, missing context, informal\nexpressions and named entities continue to be chal-\nlenging. One major challenge is the inability to\ncorrect sarcastic/rhetorical sentences (Type 4).\n6 Conclusion\nThis work introduces a semi-supervised formality\nstyle transfer model that utilizes both a language\nmodel based discriminator to maximize the likeli-\nhood of the output sentences being formal, and\na mutual information maximization loss during\ntraining. Experiments conducted on a large-scale\nformality corpus showed that our simple method\nsigniﬁcantly outperformed previous approaches in\nterms of both automatic metrics and human judge-\nment. We also demonstrated that our model can\nbe generalized well to unsupervised style transfer\ntasks. We also discussed speciﬁc challenges that\ncurrent approaches faced with this task.\nAcknowledgements\nWe thank the reviewers and members of Georgia\nTech SALT group for their feedback on this work.\nWe acknowledge the support of NVIDIA Corpora-\ntion with the donation of GPU used for this work.\n2349\nReferences\nWei-Fan Chen, Henning Wachsmuth, Khalid Al Khatib,\nand Benno Stein. 2018. Learning to ﬂip the bias of\nnews headlines. In Proceedings of the 11th Interna-\ntional Conference on Natural Language Generation,\npages 79–88.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Explo-\nration and evaluation. In AAAI.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Proceedings of the 27th Interna-\ntional Conference on Neural Information Process-\ning Systems - Volume 2, NIPS’14, page 2672–2680,\nCambridge, MA, USA. MIT Press.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text. In Proceedings of the\n34th International Conference on Machine Learning\n- Volume 70, ICML’17, page 1587–1596. JMLR.org.\nHarsh Jhamtani, Varun Gangal, Eduard Hovy, and Eric\nNyberg. 2017. Shakespearizing modern language\nusing copy-enriched sequence to sequence models.\nIn Proceedings of the Workshop on Stylistic Varia-\ntion, pages 10–19, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nZhijing Jin, Di Jin, Jonas Mueller, Nicholas Matthews,\nand Enrico Santus. 2019. IMaT: Unsupervised text\nattribute transfer via iterative matching and transla-\ntion. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3097–3109, Hong Kong, China. Association for\nComputational Linguistics.\nYongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen\nYe, Yizhou Yu, and Mingli Song. 2019. Neural style\ntransfer: A review. IEEE transactions on visualiza-\ntion and computer graphics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nTerry K Koo and M. Y . Li. 2016. A guideline of\nselecting and reporting intraclass correlation coefﬁ-\ncients for reliability research. Journal of chiroprac-\ntic medicine, 15 2:155–63.\nChih-Te Lai, Yi-Te Hong, Hong-You Chen, Chi-Jen Lu,\nand Shou-De Lin. 2019. Multiple text style transfer\nby using word-level conditional generative adversar-\nial network with two-phase training. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3579–3584, Hong\nKong, China. Association for Computational Lin-\nguistics.\nGuillaume Lample, Sandeep Subramanian,\nEric Michael Smith, Ludovic Denoyer,\nMarc’Aurelio Ranzato, and Y-Lan Boureau.\n2019. Multiple-attribute text rewriting. In ICLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, trans-\nlation, and comprehension. arXiv preprint\narXiv:1910.13461.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016a. Visualizing and understanding neural mod-\nels in NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016b. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874, New Orleans, Louisiana. Associ-\nation for Computational Linguistics.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In Proceedings of the 42nd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL-04), pages 605–612, Barcelona, Spain.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nLajanugen Logeswaran, Honglak Lee, and Samy Ben-\ngio. 2018. Content preserving text generation with\n2350\nattribute controls. In Advances in Neural Informa-\ntion Processing Systems, pages 5103–5113.\nEdward Loper and Steven Bird. 2002. Nltk: The natu-\nral language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Com-\nputational Linguistics - Volume 1 , ETMTNLP ’02,\npage 63–70, USA. Association for Computational\nLinguistics.\nFuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao\nChang, Zhifang Sui, and Xu Sun. 2019. A dual rein-\nforcement learning framework for unsupervised text\nstyle transfer.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nC ¸ a˘glar Gu`I‡lc ¸ehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nXing Niu, Sudha Rao, and Marine Carpuat. 2018.\nMulti-task neural models for translating between\nstyles within and across languages. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, pages 1008–1021, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics , ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. dAlch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang. Au-\ntomatically neutralizing subjective bias in text.\nElla Rabinovich, Shachar Mirkin, Raj Patel, Mount\nCarmel, Lucia Specia, and Shuly Wintner. 2017.\nPersonalized machine translation: Preserving origi-\nnal author traits.\nSudha Rao and Joel R. Tetreault. 2018. Dear sir or\nmadam, may i introduce the yafc corpus: Corpus,\nbenchmarks and metrics for formality style transfer.\nIn NAACL-HLT.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Controlling politeness in neural machine\ntranslation via side constraints. pages 35–40.\nMingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing,\nDongyan Zhao, Shuming Shi, and Rui Yan. 2019a.\nSemi-supervised text style transfer: Cross projection\nin latent space. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4939–4948.\nMingyue Shang, Piji Li, Zhenxin Fu, Lidong Bing,\nDongyan Zhao, Shuming Shi, and Rui Yan. 2019b.\nSemi-supervised text style transfer: Cross projection\nin latent space.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in neural informa-\ntion processing systems, pages 6830–6841.\nYusuke Shibata, Takuya Kida, Shuichi Fukamachi,\nMasayuki Takeda, Ayumi Shinohara, and Takeshi\nShinohara. 1999. Byte pair encoding: A text com-\npression scheme that accelerates pattern matching.\nRaphael Vallat. 2018. Pingouin: statistics in python.\nThe Journal of Open Source Software, 3(31):1026.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, undeﬁne-\ndukasz Kaiser, and Illia Polosukhin. 2017. Attention\nis all you need. In Proceedings of the 31st Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nYunli Wang, Yu Wu, Lili Mou, Zhoujun Li, and Wen-\nHan Chao. 2019. Harnessing pre-trained neural net-\nworks with rules for formality style transfer. In\nEMNLP/IJCNLP.\nRuochen Xu, Tao Ge, and Furu Wei. 2019. Formality\nstyle transfer with hybrid textual annotations. ArXiv,\nabs/1903.06353.\nWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and\nColin Cherry. 2012. Paraphrasing for style. In Pro-\nceedings of COLING 2012, pages 2899–2914, Mum-\nbai, India. The COLING 2012 Organizing Commit-\ntee.\nDiyi Yang, Jiaao Chen, Zichao Yang, Dan Jurafsky,\nand Eduard Hovy. 2019. Let’s make your request\nmore persuasive: Modeling persuasive strategies via\n2351\nsemi-supervised neural nets on crowdfunding plat-\nforms. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n3620–3630, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nZichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing,\nand Taylor Berg-Kirkpatrick. 2018. Unsupervised\ntext style transfer using language models as discrim-\ninators. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31 , pages 7287–7298. Curran Associates,\nInc.\nY . Zhang, M. Galley, Jianfeng Gao, Zhe Gan, Xiujun\nLi, Chris Brockett, and W. Dolan. 2018. Generating\ninformative and diverse conversational responses via\nadversarial information maximization. In NeurIPS.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei\nEfros. 2017. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. pages\n2242–2251.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the 2015\nIEEE International Conference on Computer Vision\n(ICCV), ICCV ’15, page 19–27, USA. IEEE Com-\nputer Society.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7979917526245117
    },
    {
      "name": "Formality",
      "score": 0.7595033645629883
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6420963406562805
    },
    {
      "name": "Maximization",
      "score": 0.6402759552001953
    },
    {
      "name": "Mutual information",
      "score": 0.6243013739585876
    },
    {
      "name": "Task (project management)",
      "score": 0.6087669134140015
    },
    {
      "name": "Discriminator",
      "score": 0.5754179358482361
    },
    {
      "name": "Natural language processing",
      "score": 0.5309270024299622
    },
    {
      "name": "Language model",
      "score": 0.5017855167388916
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4991006851196289
    },
    {
      "name": "Machine learning",
      "score": 0.48388800024986267
    },
    {
      "name": "Sentence",
      "score": 0.4498080015182495
    },
    {
      "name": "Transfer of learning",
      "score": 0.4188603460788727
    },
    {
      "name": "Bayesian inference",
      "score": 0.41283226013183594
    },
    {
      "name": "Bayesian probability",
      "score": 0.28296181559562683
    },
    {
      "name": "Mathematics",
      "score": 0.09079420566558838
    },
    {
      "name": "Mathematical optimization",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 28
}