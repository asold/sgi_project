{
  "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
  "url": "https://openalex.org/W4389524520",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4224412768",
      "name": "An-Zi Yen",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2143955633",
      "name": "Wei-Ling Hsu",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4223626855",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4226325987",
    "https://openalex.org/W3056980705",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3211457251",
    "https://openalex.org/W4384807774",
    "https://openalex.org/W4385570371",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4379089709",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3212993480",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W2085864841",
    "https://openalex.org/W4285778194",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W3195507288",
    "https://openalex.org/W4294768175",
    "https://openalex.org/W4306291420",
    "https://openalex.org/W4381110793",
    "https://openalex.org/W4305028650",
    "https://openalex.org/W2945720633",
    "https://openalex.org/W4240187149",
    "https://openalex.org/W4303648884",
    "https://openalex.org/W4281611690",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4321177655"
  ],
  "abstract": "Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3055–3069\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThree Questions Concerning the Use of Large Language Models to\nFacilitate Mathematics Learning\nAn-Zi Yen∗ and Wei-Ling Hsu\nDepartment of Computer Science, National Yang Ming Chiao Tung University, Taiwan\nazyen@nycu.edu.tw, weiling.hsu.cs11@nycu.edu.tw\nAbstract\nDue to the remarkable language understand-\ning and generation abilities of large language\nmodels (LLMs), their use in educational ap-\nplications has been explored. However, little\nwork has been done on investigating the peda-\ngogical ability of LLMs in helping students to\nlearn mathematics. In this position paper, we\ndiscuss the challenges associated with employ-\ning LLMs to enhance students’ mathematical\nproblem-solving skills by providing adaptive\nfeedback. Apart from generating the wrong\nreasoning processes, LLMs can misinterpret\nthe meaning of the question, and also exhibit\ndifficulty in understanding the given questions’\nrationales when attempting to correct students’\nanswers. Three research questions are formu-\nlated.\n1 Introduction\nAfter the pandemic, e-learning has become part\nof mainstream education (Alqahtani and Rajkhan,\n2020; Jafar et al., 2022). However, for students,\nonline learning is not without its problems (Ab-\ndur Rehman et al., 2021). Apart from the difficulty\nin maintaining focus in online classes, the lack of\nreal-time communication and timely feedback are\nalso serious problems. In particular, in online as-\nsessments, students may fail to understand their\nmistakes, even after reviewing the provided an-\nswers; such failure to immediately clarify points\nof confusion yields poor learning outcomes. Syn-\nchronous communication between teachers and\nstudents in an e-learning setting is necessary, but\nteachers find that promptly responding to students’\nquestions is a significant challenge.\nAs large language models (LLMs) offer a wide\nrange of applications, several studies (Tack and\nPiech, 2022; Kasneci et al., 2023; Zhang et al.,\n2023) address the use of LLMs in education. In\nthis work, we seek to investigate the integration\n∗Corresponding author.\nof LLMs into education. Since each subject has\nits own issues that must be addressed, we explore\nthe scenario of LLM use in mathematics education\nand the challenges thereof. Many studies address\nmath word problem solving (Shen et al., 2021; Yu\net al., 2021; Jie et al., 2022), but the human utility\nof the mathematics reasoning processes and the\nnatural language explanations generated by mod-\nels within educational settings is rarely discussed.\nGiven the extraordinary capability of LLMs to gen-\nerate free-text rationalization, we investigate their\nmathematical problem-solving competence and as-\nsess whether the generated step-by-step explana-\ntions constitute a useful educational resource. In\nparticular, we analyze the ability of LLMs to pro-\nvide adaptive feedback by identifying and explain-\ning errors within a student’s math problem-solving\nprocess.\nWe address these issues using the MathQA\ndataset (Amini et al., 2019), which consists of\nGRE-level questions that require advanced math-\nematical knowledge. The questions cover arith-\nmetic, algebra, geometry, and data analysis. In\nAppendix A we explain in detail our reasoning be-\nhind the dataset selection. For all experiments, we\nuse GPT-3.5 (Ouyang et al., 2022). We raise and\ndiscuss a total of three research questions. In this\npilot study, we conduct both quantitative and quali-\ntative evaluations to answer the research questions.\nThe contributions of this work are threefold.\n1. We explore the application of LLMs for in-\nstructing students in solving math word prob-\nlems.\n2. Drawing on the results of LLMs to solve math\nword problems, we comprehensively identify\nthe existing problems of current models.\n3. We discuss the pedagogical suitability of the\ngenerated equations and the corresponding\nfree-text explanations in the context of mathe-\nmatics education.\n3055\nTeacher\nStudent\n48 * 100 / 3600 * 9 = 12 m\nQuestion bank\nThis question tests the students'\nunderstanding of unit and time\nconversions. \nThe student often makes mistakes\nwhen doing these conversions.\nThe unit conversion is incorrect.\n1000 meters equals 1 kilometer, \nso converting kilometers to meters\nrequires multiplication by 1000, \nnot division.\n48 * 1000 / 3600 * 9 = 120 m\nQuestion: A train running at the speed of 48 km / hr crosses\na pole in 9 seconds. What is the length of the train ?\nStudent's examination record\nFigure 1: An LLM helping a student to learn mathemat-\nics\n2 Utility of LLMs in Mathematics\nLearning\nTo explore the utility of LLMs in mathematics\nlearning, we raise the first research question (RQ1):\nHow can LLMs be utilized to assist students\nin learning mathematical problem-solving skills?\nSince exams are a common way for teachers to\nevaluate student learning progress, in this study, we\nfocus on leveraging LLMs to equip students with\nthe knowledge skills needed to solve math word\nproblems.\nIn terms of question types, in addition to\ntrue/false and multiple-choice questions, short-\nanswer questions are also included in math ex-\nams. Students answer the questions using problem-\nsolving processes. Automated assessment (Moore\net al., 2022) of the student’s problem-solving pro-\ncess remains to be investigated. In education, feed-\nback is also crucial (Shute, 2008). Simply scoring\nthe student answer is often insufficient, as scores\ndo not reflect the reasons for incorrect answers. To\nlearn from their mistakes, students need the cor-\nresponding explanation. Figure 1 illustrates the\nscenario of an LLM applied in mathematics edu-\ncation. After a student responds to a question, the\nmodel determines whether the student’s answer is\ncorrect or not. If the answer is correct, the system\ninforms the student of the test objective for that\nquestion, enhancing their understanding of the ex-\namined skills. If the answer is incorrect, the system\nprovides adaptive feedback (Bernius et al., 2022;\nSailer et al., 2023) by indicating the location of\nthe error and offering an explanation, assisting the\nstudent to clarify any misunderstanding.\nGiven that there are multiple methods for solv-\ning a math word problem, a model must be able to\nunderstand and correct a student’s thought process.\nIf the overall strategy is sound apart from minor\nconcept errors or obstacles, the model should guide\nstudents to the next step following their chosen ap-\nCategory Questions Zero-shot Few-shot CoT\nAll 1,605 66.54% 65.67% 66.11%\nGeneral 663 64.71% 63.20% 64.86%\nGain 345 71.88% 72.17% 70.14%\nPhysics 410 68.54% 65.37% 67.32%\nGeometry 100 63.00% 65.00% 60.00%\nProbability 12 58.33% 83.33% 75.00%\nOther 75 53.33% 57.33% 58.67%\nTable 1: MathQA results\nproach. Thus the ability of LLMs to solve math\nword problems and to understand and explain equa-\ntions is critical. The following sections address\nthese two key points.\n3 LLM Ability to Solve Math Problems\nMany textbook math exercises provide the calcu-\nlation process but lack a natural language explana-\ntion. Accompanying each step with an explanation\nwould greatly enhance the student’s understanding\nof each equation. This leads to the second research\nquestion (RQ2): What is the problem-solving ca-\npability of LLMs in mathematical problems and\ntheir ability to explain the computational process?\nAutomatic Evaluation:The dataset used in this\nwork is a modified version of MathQA in which\nunsolvable questions were removed by Jie et al.\n(2022). We utilized OpenAI’s API, in particu-\nlar the “gpt-3.5-turbo-0301” model. The temper-\nature was set to 0. Table 1 shows the accuracy of\nthree commonly used prompting methods: zero-\nshot prompting (Zero-shot), few-shot prompting\n(Few-shot), and chain-of-thought (CoT) prompt-\ning. Three prompting templates are shown in Ap-\npendix B. “Questions” denotes the number of ques-\ntions of each type in the test set. We also show the\nresults for six MathQA question types. In this ex-\nperiment, we compared only the model-generated\nanswers with the provided answers, without ver-\nifying the correctness of the reasoning process.\nAs shown in Table 1, the CoT performance was\npoorer than expected, possibly due to the higher\nmathematical skill demands of MathQA compared\nto previously-used datasets. Zero-shot prompting\ndoes not significantly outperform CoT ( p <0.8).\nExploring suitable prompting methods is left as fu-\nture work. Based on the results, we observe that the\ncalculation abilities of the GPT-3.5 model remain\nfar from satisfactory. It frequently fails at simple\narithmetic operations or counting.\nHuman Evaluation of LLM Results:It is known\nthat LLMs might produce the correct answer even\n3056\nError type Percentage\nMisconception in problem-solving 36.54%\nIncorrect provided answer* 17.31%\nUnclear question definition* 11.54%\nCalculation error in equation 9.61%\nMisinterpretation of question 7.69%\nArithmetic error 5.77%\nAbsence of necessary diagrams* 3.85%\nCounting error 3.85%\nUndefined symbols in question* 1.92%\nIncomplete problem-solving 1.92%\nTable 2: Error types annotated by an expert. * indicates\nthat the error is not from the model’s response but from\nan error in the question.\nwith incorrect reasoning, or give an incorrect an-\nswer despite correct reasoning (Laskar et al., 2023;\nLightman et al., 2023). However, detailed analy-\nses and statistical evaluations of model errors have\nbeen less extensively studied. To further analyze\nwhether LLMs are able to reason through complex\nmathematical problems, we invited an expert who\nmajored in Mathematics to evaluate the answers\ngenerated by the GPT-3.5 model. A total of 120\nquestions—20 from each of the six question types—\nwere selected by the expert. Each selected question\ninvolved a process of reasoning and calculation of\nmore than four steps. The error types are shown\nin Table 2. The most common error made by the\nmodel was “misconception in problem-solving”:\nthe model understood the question but used incor-\nrect formulas or methods to solve it. “Misinterpre-\ntation of the question”, in turn, is a different error:\nthe model does not understand the question and\ngenerates an unrelated result. We also find that the\nGPT-3.5 model is good at providing comprehensive\nexplanations for each equation without omitting\nparts of the problem-solving process. However,\nit exhibits inconsistent calculations, often making\nsimple arithmetic errors. Furthermore, its grasp\nof set theory and three-dimensional spatial reason-\ning is limited. Examples of some error types are\nprovided in Appendix C.\nResearch Issues of Augmented Language Mod-\nels: Using external tools for calculation may be\none way to address LLM drawbacks. Mialon et al.\n(2023) refer to language models that utilize exter-\nnal tools (Gao et al., 2022; Liu et al., 2022), retrieve\nrelevant information (Izacard et al., 2022), or use\nspecific reasoning strategies (Wei et al., 2022) as\naugmented language models (ALMs). We argue\nthat for a model to solve more complex tasks, it\nshould comprehend the tasks and know when, how,\nand why to request augmentation. Otherwise, the\nimprovements yielded by the augmented informa-\ntion would remain limited in various real-world\napplications. For instance, for mathematical cal-\nculations, Schick et al. (2023) propose having the\nLLM use a calculator API to solve arithmetic tasks.\nHowever, the propensity of LLMs to misinterpret\nquestion meanings and substitute incorrect num-\nbers remains a prominent challenge in advanced\nmathematical reasoning. From our observation, the\nGPT-3.5 model behaves much like a student fo-\ncused on formula memorization in that it struggles\nto adapt to variations in the question, particularly in\nprobability questions. Hence, enhancing the mathe-\nmatical reasoning capabilities of LLMs is a critical\nresearch direction.\n4 Pedagogical Ability of LLMs to Rectify\nStudents’ Answers\nThe most important thing when helping students\nto learn mathematical problem-solving is provid-\ning immediate adaptive feedback. In this section,\nwe measure the pedagogical ability of LLMs in\nmathematics. Pedagogical ability refers to the abil-\nity to understand and help the student (Tack and\nPiech, 2022). This leads to the third research ques-\ntion (RQ3): Are LLMs able to identify errors in\nstudents’ answers and provide corresponding ex-\nplanations?\nTeacher–Student Framework for Quantitative\nEvaluation: Due to the difficulty in obtaining\nreal-world student answers, we simulate a scenario\nin which students answer questions and teachers\ncorrect the students’ responses. Based on the ex-\nperimental results from Table 1, we use the re-\nsponses from zero-shot prompting as the student\nanswers. These answers are then input into the\nGPT-3.5 model, which acts as a teacher, correct-\ning the student’s answers according to the ques-\ntion. The GPT-3.5 model is tasked with identifying\nwhether the student’s answer is correct and explain-\ning why. Specifically, given an input question q,\nprompt Ps, and model M, we obtain the initial\nproblem-solving result ys = M(q; Ps). Next, we\ninput ys to M, and ask Mto act as a teacher with\nprompt Pt to correct ys based on q. Finally, we\nobtain the feedback yt = M(q, ys, r; Pt), where\nr is the rationale of q provided in MathQA. If M\nstruggles to understand its responses (with detailed\nprocesses and natural language explanations), then\nits potential to assist teachers in corrections be-\n3057\nType W/o rationale W/ rationale\nAll 53.96% 73.71%\nGeneral 54.75% 73.30%\nGain 54.78% 73.33%\nPhysics 50.00% 72.20%\nGeometry 60.00% 83.00%\nProbability 25.00% 83.33%\nOther 61.33% 73.33%\nTable 3: Results of teacher–student framework\nResult of ys ys is correct ys is incorrect\nPt w/o r w/ r w/o r w/ r\nIdentify ys is correct 63.24% 10.29% 53.85% 0.00%\nSay in other words 17.65% 67.65% 17.31% 17.31%\nCorrect the process 11.76% 16.18% 11.53% 50.00%\nCorrect the calculation 7.35% 5.88% 17.31% 32.69%\nTable 4: Error types in teacher–student framework\ncomes questionable. The Pt template is shown\nin Appendix B. We refer to this framework as the\n“teacher–student framework”.\nTable 3 presents the results of the teacher–\nstudent framework. Accuracy is adopted as the\nevaluation metric: accuracy measures whether M\ncorrectly identifies the correctness of ys. As yt\ncould be a lengthy paragraph, we simply use key-\nwords such as “is correct”, “is incorrect”, “not cor-\nrect”, “almost correct” or “not correct” to identify\nM. We compare the results with and without the\nquestion rationales. Interestingly, if the correspond-\ning rationales are not given as input, the accuracy\nof the GPT-3.5 model acting as a teacher to correct\nstudents’ answers (53.96%) is lower than that when\ndirectly answering questions (66.54%). However,\nif the corresponding rationales are given as input,\naccuracy is only 73.71%. Thus the GPT-3.5 model\nhas difficulty understanding the equations given in\nthe rationales.\nHuman Evaluation of Correcting Model-\nGenerated Answers:To understand why the GPT-\n3.5 model exhibits these characteristics when cor-\nrecting answers, our expert also analyzed the re-\nsults of the teacher–student framework based on\nthe selected 120 questions. Examples are given in\nAppendix D. Table 4 presents the correction results\nyt for ys by human evaluation. “w/o r” and “w/ r”\ndenote Pt without and with rationales, respectively.\nComparing the results of “w/o r” and “w/ r”, we\nfind that providing rationales seriously confuses\nM, such that it determines ys is wrong in most\ncases. Furthermore, Msimply rephrases the con-\ntent of ys (67.65%). Note that the results in Table 3\nshow that Pt with r is better than that without r.\nHowever, the results in Table 4 are different, be-\ncause the questions selected by the expert are more\nchallenging. If ys is incorrect, Mhas a 53.85%\nchance of erroneously claiming that ys is correct\nwhen r is not provided. When r is given, Mcor-\nrectly identifies that ys is incorrect. Nonetheless,\nMhas only a 10.29% chance to accurately identify\nys as correct with r. In addition, according to our\nstatistic, Mhas only a 3.85% and 1.92% chance\nto accurately correct the calculation results and the\nproblem-solving processes, respectively. This is\nprimarily because it has difficulty exploiting r and\nbecause it usually misunderstands the ys equations,\neven to the point of inaccurately correcting those\nthat are already correct. Furthermore, it often for-\ngets the equations and calculations in ys. To verify\nwhether LLMs can understand and correct human\nproblem-solving processes, we also invited five col-\nlege students to answer three questions. The results\nare presented in Appendix E.\nUtility of LLMs in Complex Reasoning Tasks\nfor Education:The failures of the GPT-3.5 model\nare comprehensively analyzed by Borji (2023). In\nthis work, we find that LLMs tend to be confused\nby human answers, especially when tasks demand\nadvanced knowledge or reasoning skills, even if rel-\nevant and correct information is provided. Hence,\nan alternative framework is needed under which to\nleverage LLMs’ language understanding ability in\ncomplex reasoning tasks. Other crucial research\nissues are how to make the models aware of what\nthey do not know and how to produce truthful and\ninterpretable results (Phillips et al., 2020). Besides,\nthis work primarily focuses on the capabilities and\nchallenges of directly using LLMs for correcting\nstudents’ answers. Developing a cognitive model\nfor reasoning processes and their potential roles in\nrectifying student mistakes is crucial.\n5 Conclusion\nIn this work we propose a research agenda for\nleveraging LLMs in mathematics learning. First,\nwe explore the use of LLMs to assist students in\nlearning math word problem-solving skills. Then,\nwe analyze the mathematical reasoning ability of\nLLMs. Finally, we investigate the pedagogical abil-\nity of LLMs in terms of rectifying model-generated\nor human answers and offering adaptive feedback.\nWe conduct experiments with the GPT-3.5 model,\nand conclude that there remains room for improve-\n3058\nment in the LLM’s performance in solving com-\nplex mathematical problems. In addition, although\nit generates comprehensive explanations, the LLM\nis limited in accurately identifying model’s and\nhuman’s errors due to its poor ability to interpret\nmathematical equations. In the future, we plan to\ndevelop an advanced method by which to improve\nthe utility of LLMs in mathematics education.\nLimitations\nConsidering that LLMs are widely accessible to the\ngeneral public and many educational institutions\nare examining the challenges and benefits of their\nuse by teachers and students, this paper primarily\nfocuses on the application of LLMs in educational\nsettings. However, our experiments employed only\nthe GPT-3.5 model and did not explore other LLMs\nsuch as GPT-4 (OpenAI, 2023) and LLaMA (Tou-\nvron et al., 2023). Furthermore, while our current\nwork investigates the utility of LLMs in enhanc-\ning students’ mathematical problem-solving skills,\nthere are many other applications in education. For\ninstance, LLMs could be utilized to help teachers\nto generate teaching content or questions. Addi-\ntionally, the potential issues of violating teaching\nethics involved in introducing LLMs into educa-\ntional applications are significant topics that have\nnot been included in this study currently. We also\nconducted a human evaluation on 120 questions,\nwhich may be insufficient. Although we invited the\nexpert to select questions that cover as many types\nof questions as possible, there may remain worth-\nwhile examples that were not selected for analysis.\nMoreover, the mistakes made by humans and those\nmade by LLMs may differ. However, at the cur-\nrent stage, we invited only five college students to\nanswer three questions: a larger-scale experiment\nwould be more helpful.\nEthics Statement\nIn the context of educational applications, we uti-\nlize students’ personal data and answer records\nfor our experiments, which raises privacy concerns.\nFor this study, we invited college students to answer\nmathematical questions to investigate issues that\nmight be found in real-world applications. These\nparticipants fully understood how their data would\nbe used, and we ensured that their personal infor-\nmation would not be leaked.\nAcknowledgements\nThis research was partially supported by National\nScience and Technology Council, Taiwan, under\ngrant NSTC 111-2222-E-A49-010-MY2.\nReferences\nMohsin Abdur Rehman, Saira Hanif Soroya, Zuhair\nAbbas, Farhan Mirza, and Khalid Mahmood. 2021.\nUnderstanding the challenges of e-learning during the\nglobal pandemic emergency: The students’ perspec-\ntive. Quality Assurance in Education, 29(2/3):259–\n276.\nAmmar Y Alqahtani and Albraa A Rajkhan. 2020. E-\nlearning critical success factors during the COVID-\n19 pandemic: A comprehensive analysis of e-\nlearning managerial perspectives. Education Sci-\nences, 10(9):216.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367.\nJan Philip Bernius, Stephan Krusche, and Bernd\nBruegge. 2022. Machine learning based feedback\non textual student answers in large courses. Comput-\ners and Education: Artificial Intelligence, 3:100081.\nAli Borji. 2023. A categorical archive of ChatGPT\nfailures. arXiv preprint arXiv:2302.03494.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. PAL: Program-aided language\nmodels. arXiv preprint arXiv:2211.10435.\n3059\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\nArabella Sinclair, Dennis Ulmer, Florian Schottmann,\nKhuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha,\nLeila Khalatbari, Maria Ryskina, Rita Frieske, Ryan\nCotterell, and Zhijing Jin. 2022. State-of-the-art gen-\neralisation research in NLP: a taxonomy and review.\nCoRR.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nAdi Jafar, Ramli Dollah, Nordin Sakke, Moham-\nmad Tahir Mapa, Ang Kean Hua, Oliver Valen-\ntine Eboy, Eko Prayitno Joko, Diana Hassan, and\nChong Vun Hung. 2022. Assessing the challenges\nof e-learning in Malaysia during the pandemic of\nCOVID-19 using the geo-spatial approach. Scientific\nReports, 12(1):17316.\nZhanming Jie, Jierui Li, and Wei Lu. 2022. Learning\nto reason deductively: Math word problem solving\nas complex relation extraction. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5944–5955.\nEnkelejda Kasneci, Kathrin Seßler, Stefan Küchemann,\nMaria Bannert, Daryna Dementieva, Frank Fischer,\nUrs Gasser, Georg Groh, Stephan Günnemann, Eyke\nHüllermeier, Stepha Krusche, Gitta Kutyniok, Tilman\nMichaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra\nPoquet, Michael Sailer, Albrecht Schmidt, Tina Sei-\ndel, Matthias Stadler, Jochen Weller, Jochen Kuhn,\nand Gjergji Kasneci. 2023. ChatGPT for good? On\nopportunities and challenges of large language mod-\nels for education. Learning and Individual Differ-\nences, 103:102274.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Xiangji Huang. 2023. A systematic study\nand comprehensive evaluation of chatgpt on bench-\nmark datasets. arXiv preprint arXiv:2305.18486.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023. Let’s verify step by step. arXiv preprint\narXiv:2305.20050.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush V osoughi, Claire Cui, Denny Zhou, and An-\ndrew M Dai. 2022. Mind’s Eye: Grounded language\nmodel reasoning through simulation. arXiv preprint\narXiv:2210.05359.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: A survey. arXiv preprint arXiv:2302.07842.\nSteven Moore, Huy A Nguyen, Norman Bier, Tanvi\nDomadia, and John Stamper. 2022. Assessing the\nquality of student-generated short answer questions\nusing GPT-3. In Educating for a New Future: Mak-\ning Sense of Technology-Enhanced Learning Adop-\ntion: 17th European Conference on Technology En-\nhanced Learning, EC-TEL 2022, Toulouse, France,\nSeptember 12–16, 2022, Proceedings , pages 243–\n257. Springer.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. Advances in Neural Information\nProcessing Systems, 35:27730–27744.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094.\nP Jonathon Phillips, Carina A Hahn, Peter C Fontana,\nDavid A Broniatowski, and Mark A Przybocki. 2020.\nFour principles of explainable artificial intelligence.\nGaithersburg, Maryland, page 18.\nMichael Sailer, Elisabeth Bauer, Riikka Hofmann, Jan\nKiesewetter, Julia Glas, Iryna Gurevych, and Frank\nFischer. 2023. Adaptive feedback from artificial neu-\nral networks facilitates pre-service teachers’ diagnos-\ntic reasoning in simulation-based learning. Learning\nand Instruction, 83:101620.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin\nJiang, Ming Zhang, and Qun Liu. 2021. Generate &\nrank: A multi-task framework for math word prob-\nlems. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2269–2279.\nValerie J Shute. 2008. Focus on formative feedback.\nReview of Educational Research, 78(1):153–189.\n3060\nAnaïs Tack and Chris Piech. 2022. The AI Teacher\nTest: Measuring the pedagogical ability of Blender\nand GPT-3 in educational dialogues. In Proceedings\nof the 15th International Conference on Educational\nData Mining, page 522.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 845–854.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nCheng-Kuang Wu, Wei-Lin Chen, and Hsin-Hsi Chen.\n2023. Large language models perform diagnostic\nreasoning.\nWeijiang Yu, Yingpeng Wen, Fudan Zheng, and Nong\nXiao. 2021. Improving math word problems with\npre-trained knowledge and hierarchical reasoning.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3384–3394.\nSarah J Zhang, Samuel Florin, Ariel N Lee, Ea-\nmon Niknafs, Andrei Marginean, Annie Wang,\nKeith Tyser, Zad Chin, Yann Hicke, Nikhil Singh,\nMadeleine Udell, Yoon Kim, Tonio Buonassisi, Ar-\nmando Solar-Lezama, and Iddo Drori. 2023. Ex-\nploring the MIT mathematics and eecs curricu-\nlum using large language models. arXiv preprint\narXiv:2306.08997.\nA Dataset Selection\nDatasets commonly used to evaluate a model’s\nability to solve math word problems include\nMathQA (Amini et al., 2019), SV AMP (Patel\net al., 2021), Math23K (Wang et al., 2017), and\nGSM8K (Cobbe et al., 2021). GSM8K consists of\ngrade-school math word problems, and SV AMP pri-\nmarily covers up to grade 4. Math23K is designed\nfor Chinese elementary school students, and fo-\ncuses on evaluating the ability of students to solve\none-unknown-variable linear math word problems.\nBy contrast, MathQA is composed of GRE-level\nmath problems, which serve as a benchmark for uni-\nversity research institute admissions in the United\nStates. Therefore, we use MathQA in our exper-\niments to examine the utility of LLMs in higher\neducation applications.\nInput: You are a student who is good at mathematics. Apply\nthe appropriate mathematical operations to solve the math\nproblem. Please choose an answer from the options.\nQ: a student gets 55 % in one subject , 65 % in the other . to\nget an overall of 55 % how much should get in third subject .\nOptions: a ) 75 % , b ) 25 % , c ) 45 % , d ) 55 % , e ) 65 %\nOutput: Let’s assume that the third subject carries the same\nweight as the other two subjects.\nTo get an overall of 55%, the average of the three subjects\nshould be 55%.\nLet x be the percentage the student needs to get in the third\nsubject.\nThen, we can set up the equation:\n(55% + 65% + x)/3 = 55%\nSimplifying this equation, we get:\n120% + x = 165%\nx = 45%\nTherefore, the student needs to get 45% in the third subject\nto get an overall of 55%.\nThe answer is c) 45%.\nTable 5: Zero-shot prompting\nB Input Formats\nLLMs are known for their success in zero-shot\nor few-shot learning (Brown et al., 2020). We\ncan solve various tasks (Liu et al., 2023) through\nprompt engineering by providing a few examples\nor instructions about the task descriptions. For\nsolving math word problem, Wei et al. (2022) pro-\npose chain-of-thought prompting (CoT) by pro-\nviding step-by-step reasoning examples to cause\nthe model to decompose the given question and\ngenerate reasoning steps. They demonstrate that\nCoT achieves promising results in solving math\nword problems. Moreover, Wu et al. (2023) demon-\nstrate the CoT’s effectiveness by incorporating it\ninto the medical diagnostic framework. Hence,\nwe use zero-shot, few-shot, and CoT prompting in\nour experiments. Tables 5, 6, and 7 contain tem-\nplates of zero-shot, few-shot, and CoT prompting,\nrespectively. The template of the proposed teacher–\nstudent framework is shown in Table 8.\nC Examples Error Types when Solving\nMath Word Problems\nAmong the 120 selected questions, the model cor-\nrectly answers 68 questions and incorrectly an-\nswers 52 questions. Eighteen of the incorrect re-\nsponses are due to unclear question definitions,\n24 stem from errors in the model’s problem-solving\nprocess, and 10 are caused by calculation errors.\nIn this section, we will present errors made by the\nGPT-3.5 model to show its limitations in mathemat-\nics reasoning.\n3061\nInput: You are a student who is good at mathematics. Apply\nthe appropriate mathematical operations to solve the math\nproblem. Please choose an answer from the options.\nQ: the banker ’ s gain of a certain sum due 3 years hence at 10\n% per annum is rs . 36 . what is the present worth ? Options:\na ) 38 , b ) 27.675 , c ) 30 , d ) data inadequate , e ) none of\nthese\nA: The answer is a.\nQ: sophia finished 2 / 3 of a book . she calculated that she\nfinished 90 more pages than she has yet to read . how long is\nher book ? Options: a ) 229 , b ) 270 , c ) 877 , d ) 266 , e )\n281\nA: The answer is b.\nQ: a student gets 55 % in one subject , 65 % in the other . to\nget an overall of 55 % how much should get in third subject .\nOptions: a ) 75 % , b ) 25 % , c ) 45 % , d ) 55 % , e ) 65 %\nOutput: Let’s assume that all three subjects carry equal\nmarks.\nTo get an overall percentage of 55%, the sum of the percent-\nages obtained in all three subjects should be 3 x 55% = 165%.\nThe student has already obtained 55% and 65% in the first\ntwo subjects.\nTherefore, the percentage required in the third subject to get\nan overall percentage of 55% is:\nPercentage required in the third subject = 165% - (55% +\n65%) = 45%\nHence, the answer is option c) 45%.\nTable 6: Few-shot prompting\nAs shown in Table 2, the most common mistake\nis misconception in problem-solving. Moreover, as\nmentioned in Section 3, one weakness of the GPT-\n3.5 model is in reasoning about three-dimensional\nspatial problems, as illustrated in Figure 3. The\nmodel directly divides the length, width, and height\nby three to calculate how many cubes can be placed\nin a box, without considering the fact that cubes\ncannot be accommodated if the height is not evenly\ndivisible. The question in the red box should be\n3 ×3 ×2, not 4 ×3 ×2.\nMisinterpretation of the question is another crit-\nical error. Figure 4 is an example of the GPT-3.5\nmodel misinterpreting the meaning of the question.\nBased on the result responded by the model, once\nwe obtain the value of “x”, the greater number is\n“2x”. However, the model incorrectly interprets the\nquestion as asking which number is greater after\nadding five.\nD Example Results from\nTeacher–Student Framework\nAccording to our observations, when acting as a\nteacher, the GPT-3.5 model achieves high accuracy\nin correcting simple equations when the rationale\ncontains a single equation. However, for more com-\nplex problems involving two or more equations, the\nquality of the rationale significantly impacts the\nInput: You are a student who is good at mathematics. Apply\nthe appropriate mathematical operations to solve the math\nproblem. Please choose an answer from the options.\nQ: a student gets 55 % in one subject , 65 % in the other . to\nget an overall of 55 % how much should get in third subject .\nOptions: a ) 75 % , b ) 25 % , c ) 45 % , d ) 55 % , e ) 65 %\nA: Let’s think step by step.\nOutput: To get an overall of 55%, the average of the three\nsubjects should be 55%.\nLet’s assume the third subject has a weightage of 100%.\nSo, the total weightage of the three subjects is 100% + 100%\n+ 100% = 300%.\nNow, we know that the student got 55% in the first subject\nand 65% in the second subject.\nSo, the total marks obtained in the first two subjects are: 55%\n+ 65% = 120%\nTo get an overall of 55%, the total marks obtained in all three\nsubjects should be:\n55% x 3 = 165%\nNow, we can find out how much the student should get in the\nthird subject:\nTotal marks in all three subjects = Marks obtained in the first\ntwo subjects + Marks obtained in the third subject\n165% = 120% + Marks obtained in the third subject\nMarks obtained in the third subject = 165% - 120% = 45%\nTherefore, the answer is option c) 45%.\nTable 7: CoT prompting\nmodel’s identification results. Figure 2 shows the\nconfusion matrix of answer correction. We define\nthe difficulty of the questions based on the number\nof equations in the given rationales. We categorize\nthe question difficulty into three groups: those with\nfewer than five equations, those with 5 to 10 equa-\ntions, and those with more than 11 equations. “Cor-\nrect” and “wrong” in the true labels indicate that\nys is correct and incorrect, respectively. “Correct”\nand “wrong” in the predicted labels label indicate\nthat the identification of yt is correct and incor-\nrect, respectively. Thus “correct” in the predicted\nlabels indicates that the model identifies ys to be\ncorrect when ys is indeed correct. As shown in Fig-\nure 2, the model’s accuracy in correcting answers\ndecreases as the problem becomes more difficult,\nespecially when no rationale is provided to the\nmodel. Clear and explicit rationales aligned with\nthe problem-solving process in ys cause the teacher\nmodel to understand correctly and thus accurately\nidentify whether the ys is correct.\nAdditionally, when ys is correct, the GPT-3.5\nmodel frequently identifies ys as incorrect but its\nexplanation is merely a paraphrase of ys. The rea-\nson may be that the GPT-3.5 model does not merely\ndetermine correctness based on the answer’s value.\nAlthough it has stringent requirements for this pro-\ncess, its comprehension of equations is less than\nideal, resulting in misinterpretations.\n3062\nInput: You are a math teacher. According to [Question] and\n[Rationale], please indicate whether the [Student’s Answer]\nis correct or not, and apply appropriate mathematical opera-\ntions to correct the [Student’s Answer].\n[Question]: a student gets 55 % in one subject , 65 % in the\nother . to get an overall of 55 % how much should get in third\nsubject . Options: a ) 75 % , b ) 25 % , c ) 45 % , d ) 55 % , e\n) 65 %\n[Rationale]: explanation : t = 3 years r = 10 % td = ( bg ×\n100 ) / tr = ( 36 × 100 ) / ( 3 × 10 ) = 12 × 10 = rs . 120 td = (\npw × tr ) / 100 => 120 = ( pw × 3 × 10 ) / 100 => 1200 = pw\n× 3 pw = 1200 / 3 = rs . 400 answer : option a\n[Student’s Answer]: Let’s assume that the third subject\ncarries the same weight as the other two subjects.\nTo get an overall of 55%, the average of the three subjects\nshould be 55%.\nLet x be the percentage the student needs to get in the third\nsubject.\nThen, we can set up the equation:\n(55% + 65% + x)/3 = 55%\nSimplifying this equation, we get:\n120% + x = 165%\nx = 45%\nTherefore, the student needs to get 45% in the third subject\nto get an overall of 55%.\nThe answer is c) 45%.\nOutput: The answer is correct.\nExplanation:\nTo find out how much the student should get in the third\nsubject to have an overall of 55%, we can use the formula:\n(55% + 65% + x)/3 = 55%\nWe can simplify this equation by multiplying both sides by 3:\n55% + 65% + x = 165%\nNow we can solve for x by subtracting 55% and 65% from\nboth sides:\nx = 165% - 55% - 65%\nx = 45%\nTherefore, the student needs to get 45% in the third subject\nto have an overall of 55%.\nTable 8: Prompting under the teacher–student frame-\nwork\nTo measure the ability of LLMs to correct stu-\ndents’ answers, we utilize the problem-solving re-\nsults returned by the GPT-3.5 model as the students’\nanswers and then ask the model to correct the input\nanswers. Figure 5 presents an example in which\nthe reasoning in the answer is not finished: it stops\nat calculating the total volume of the drinks. The\nmodel, which acts as a teacher, accurately points\nout the error and provides the complete solution.\nHowever, the GPT-3.5 model may also mistakenly\nrectify correct answers as incorrect. Taking Fig-\nure 6 as an example, the result in the given answer\ncorrectly solves for the number of sheep, yet the\nmodel identifies it as incorrect, further producing\nan erroneous reasoning process.\n Without Rationale \n   With Rationale \n                \nFigure 2: Confusion matrix of answer correction based\non question difficulty\nE Human Evaluation on Correcting\nHuman’s Answers\nSince the problem-solving approaches of LLMs\nmay differ from those of humans, to verify whether\nLLMs can understand and correct human problem-\nsolving processes, we also invited five college stu-\ndents from different departments (Mathematics,\nStatistics, and Japanese) to answer three questions\nselected by the expert. Their answers were in-\nput into the GPT-3.5 model for correction. Subse-\nquently, the expert analyzed the correction results.\nBased on the collected answers, we find that LLM\nis more likely to be misled by human answers and\neven ignore reasoning processes written by humans.\nWhether the answer is correct or not, LLM tends\nto identify the answer as incorrect. As shown in\nFigure 7, the model misidentifies the answer of the\nhuman as “$8000”. This may be because humans\nsometimes omit mathematical conversions in their\nanswers, or because the problem-solving strategy\nmay differ from that of the LLM. This can con-\nfuse the LLM. For instance, in Figure 8, the model\nfails to comprehend the equations without natu-\nral language explanations, erroneously identifies\nthe human’s problem-solving approach as incor-\nrect, and proceeds to solve it using its own method.\nEven when provided with a rationale, the model\nusually fails to make an accurate identification, of-\nten leading to erroneous outcomes.\nF Type of Generalization\nTable 9 shows the type of generalization we are\ninvestigating, based on the taxonomy defined by\nHupkes et al. (2022).\n3063\nFigure 3: Misconception in a three-dimensional question\nFigure 4: The GPT-3.5 model misinterpreting a mathematical question\n3064\nFigure 5: Completing the problem-solving process in the teacher–student framework\n3065\nFigure 6: Providing the wrong reasoning process in the teacher–student framework\n3066\nFigure 7: Misidentification of the human’s answer\n3067\nFigure 8: Misinterpretation of the human’s answer\n3068\nMotivation\nPractical Cognitive Intrinsic Fairness\nO\nGeneralisation type\nCompositional Structural Cross Task Cross Language Cross Domain Robustness\nO O\nShift type\nCovariate Label Full Assumed\nO\nShift locus\nTrain–test Finetune train–test Pretrain–train Pretrain–test\nO\nTable 9: GenBench evaluation card\n3069",
  "topic": "Meaning (existential)",
  "concepts": [
    {
      "name": "Meaning (existential)",
      "score": 0.5962506532669067
    },
    {
      "name": "Mathematics education",
      "score": 0.5657111406326294
    },
    {
      "name": "Work (physics)",
      "score": 0.5095329880714417
    },
    {
      "name": "Computer science",
      "score": 0.47667622566223145
    },
    {
      "name": "Psychology",
      "score": 0.28012996912002563
    },
    {
      "name": "Engineering",
      "score": 0.09670993685722351
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}