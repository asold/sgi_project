{
  "title": "Towards Efficient Post-training Quantization of Pre-trained Language Models",
  "url": "https://openalex.org/W3203149535",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226987373",
      "name": "Bai, Haoli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100296181",
      "name": "Hou, Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2379035984",
      "name": "Shang, Lifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753127173",
      "name": "King, Irwin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221719684",
      "name": "Lyu, Michael R.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3174730533",
    "https://openalex.org/W2405920868",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2993091699",
    "https://openalex.org/W3034870355",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2963547822",
    "https://openalex.org/W2787831171",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W3190449293",
    "https://openalex.org/W3034657713",
    "https://openalex.org/W3137609883",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W3016430712",
    "https://openalex.org/W3132107458",
    "https://openalex.org/W2883111419",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3122499249",
    "https://openalex.org/W3103395072",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2173213060",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2963114950",
    "https://openalex.org/W2905102070",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2994840239",
    "https://openalex.org/W3035078287",
    "https://openalex.org/W3007902335",
    "https://openalex.org/W3167266074",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2885820039",
    "https://openalex.org/W3035078980",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W2964264300",
    "https://openalex.org/W2985339713",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3165976180",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3015982254",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3103438624",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3031276512",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W3113709932",
    "https://openalex.org/W3035855442",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2997394738",
    "https://openalex.org/W3173195958",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W2997336365",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2127941149",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3164441165",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3136450870",
    "https://openalex.org/W2981751377",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W3108835732",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963206030",
    "https://openalex.org/W2132737349"
  ],
  "abstract": "Network quantization has gained increasing attention with the rapid growth of large pre-trained language models~(PLMs). However, most existing quantization methods for PLMs follow quantization-aware training~(QAT) that requires end-to-end training with full access to the entire dataset. Therefore, they suffer from slow training, large memory overhead, and data security issues. In this paper, we study post-training quantization~(PTQ) of PLMs, and propose module-wise quantization error minimization~(MREM), an efficient solution to mitigate these issues. By partitioning the PLM into multiple modules, we minimize the reconstruction error incurred by quantization for each module. In addition, we design a new model parallel training strategy such that each module can be trained locally on separate computing devices without waiting for preceding modules, which brings nearly the theoretical training speed-up (e.g., $4\\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our proposed PTQ solution not only performs close to QAT, but also enjoys significant reductions in training time, memory overhead, and data consumption.",
  "full_text": "1\nTowards Efﬁcient Post-training Quantization of\nPre-trained Language Models\nHaoli Bai, Student Member, IEEE,Lu Hou, Lifeng Shang, Xin Jiang,\nIrwin King, Fellow, IEEE,Michael R. Lyu, Fellow, IEEE.\nAbstract—Network quantization has gained increasing attention with the rapid growth of large pre-trained language models (PLMs).\nHowever, most existing quantization methods for PLMs follow quantization-aware training (QAT) that requires end-to-end training with\nfull access to the entire dataset. Therefore, they suffer from slow training, large memory overhead, and data security issues. In this\npaper, we study post-training quantization (PTQ) of PLMs, and propose module-wise quantization error minimization (MREM), an\nefﬁcient solution to mitigate these issues. By partitioning the PLM into multiple modules, we minimize the reconstruction error incurred\nby quantization for each module. In addition, we design a new model parallel training strategy such that each module can be trained\nlocally on separate computing devices without waiting for preceding modules, which brings nearly the theoretical training speed-up\n(e.g., 4×on 4 GPUs). Experiments on GLUE and SQuAD benchmarks show that our proposed PTQ solution not only performs close\nto QAT, but also enjoys signiﬁcant reductions in training time, memory overhead, and data consumption.\n!\n1 I NTRODUCTION\nL\nARGE pre-trained language models (PLMs) have\nachieved remarkable success in various natural lan-\nguage processing tasks [1], [2], [3]. However, the increasing\nsize and computation overhead also make it prohibitive\nto deploy these PLMs on resource-constrained devices. To\nobtain compact PLMs, various model compression methods\nhave been proposed, such as pruning [4], [5], knowledge\ndistillation [6], [7], [8], weight-sharing [9], [10], [11], [12],\ndynamic computation with adaptive depth or width [13],\n[14], [15], and quantization [16], [17], [18], [19], [20].\nAmong these methods, network quantization enjoys the\nreduction of both model size and computation overhead\nwithout modifying the network architecture. However, de-\nspite its remarkable performance, prior methods mostly\nfollow quantization-aware training (QAT) and thus suffer\nfrom multiple challenges: 1) QAT generally requires the\nsame order of training iterations with the original full-\nprecision training [17], [19], [20], which can be slow to obtain\nthe quantized model; 2) QAT usually adopts end-to-end\ntraining by back-propagation, while it can be challenging\nto load the entire PLM into memory on resource-limited\ndevices. Moreover, recent QAT efforts combine knowledge\ndistillation to enhance the performance [19], [20], and thus\nfurther increase the memory overhead due to the teacher\nmodel; 3) QAT needs full access to the entire training set,\nwhich may lead to data security issues when exposing them\nto third-party organizations for the quantization service.\nGiven the above challenges, post-training quantiza-\ntion (PTQ) serves as an appealing alternative. In contrast\nto QAT, PTQ is efﬁcient in both training time, memory\noverhead and data consumption. Instead of the full training\nH. Bai, I. King and M. Lyu are with the Department of Computer Science\nand Engineering, The Chinese University of Hong Kong, Hong Kong, e-mail:\n{hlbai, king, lyu}@cse.cuhk.edu.hk.\nL. Hou, L. Shang and X. Jiang are with Huawei Noah’s Ark Lab, e-mail:\n{houlu3, shang.lifeng, jiang.xin}@huawei.com\nFigure 1: An illustrative comparison between our paral-\nlel post-training quantization method (MREM) and QAT\non four dimensions of the quantization pipeline: accuracy,\ntraining time, memory overhead, and data consumption.\nThe results are based on a quantized BERT-large model with\n4-bit weights and 8-bit activations over the MNLI dataset.\nBest viewed in color.\nset, it is common in PTQ to adopt only a small portion\nof training data to minimize the reconstruction error in-\ncurred by quantization [21], [22], [23], [35]. This can be\ndone by calibrating the batch normalization statistics [21]\nor step sizes [22] in quantization functions in a layer-\nwise manner. The layer-wise objective is also more sample-\nefﬁcient [70] and memory-saving compared with end-to-end\ntraining in QAT. Despite the success of prior PTQ solutions\non convolutional neural networks (CNNs), we show that\nit is non-trivial to directly apply them to PLMs such as\nBERT [2]. Different from CNNs, there are multiple linear\nlayers coupled together within the multi-head self-attention\nand feed-forward network of the transformer architecture.\nTherefore, layer-wise training ignores the underlying corre-\nlation among layers and thus leads to poor performance.\nIn this paper, we aim at improving the performance of\narXiv:2109.15082v1  [cs.CL]  30 Sep 2021\n2\npost-training quantization for PLM, while simultaneously\nmaintaining its efﬁciency w.r.t training time, memory over-\nhead and data accessibility. Firstly, we propose module-\nwise reconstruction error minimization(MREM) to incor-\nporate more layer-wise correlation. By partitioning the PLM\ninto multiple modules, each module consists of multiple\nTransformer layers for joint optimization. Meanwhile, the\nmodule size can be ﬂexibly adjusted depending on the mem-\nory constraints, achieving an appropriate trade-off between\nlayer-wise correlation and memory overhead. While simi-\nlar block-wise objectives are previously considered in [24],\nthey require to compute second-order Hessian matrices for\noptimization, which can be computationally prohibitive for\nlarge PLMs. Secondly, we design a new model parallel\nstrategy to further accelerate the process of MREM. By\nallocating each module to an individual computing device,\nall modules can perform local training in parallel, achiev-\ning nearly the theoretical speed-up (e.g., 4×on 4 GPUs).\nThirdly, we developannealed teacher forcingfor the parallel\ntraining. We ﬁnd that the naive parallel training suffers\nfrom the propagation of reconstruction error, since each\nquantized module passes the error to its successors before it\nis fully converged. Inspired by [25], we use the full-precision\nmodule to provide clean signals to the next quantized\nmodule. This breaks the reconstruction error propagation\nand improves the performance of quantized PLMs.\nEmpirical results on the GLUE and SQuAD benchmarks\nshow that our proposed MREM not only signiﬁcantly im-\nproves the performance for post-training quantization, but\nalso enjoys advantages of faster training, less memory over-\nhead, and improved data security over QAT. For instance,\nas is shown in Figure 1, the BERT-large model trained by\nparallel MREM can achieve 85.5% accuracy based on only\n4K training samples. Moreover, it consumes merely one-\nthird of memory per GPU and is more than 150×faster\nthan previous QAT training.\nWe summarize the contributions of this paper as follows:\n• We study the post-training quantization of PLMs,\nand propose module-wise reconstruction error mini-\nmization (MREM), a fast, memory-saving, and data-\nefﬁcient approach to improve the quantized PLMs.\n• We design a new model parallel strategy based on\nMREM to accelerate post-training quantization with\ntheoretical speed-up for distributed training.\n• The parallel MREM can be combined with annealed\nteacher forcing to alleviate the propagation of recon-\nstruction error and boost the performance.\n• We conduct extensive experiments on both GLUE\nand SQuAD benchmarks to verify the advantages of\nour approach w.r.t. multiple aspects of the quantiza-\ntion pipeline. We also provide detailed discussions\non other important factors of our approach.\nThe rest of this paper is organized as follows. We sum-\nmarize the related work in Section 2. In Section 3, we review\nthe necessary backgrounds and explain the motivation for\nthis research. In Section 4, we propose our PTQ solution to-\ngether with the parallel training technique. The experiments\nare present in Section 5. Finally, we conclude this work in\nSection 6.\n2 R ELATED WORK\nIn this section, we review the literature related to our\nresearch, including both network compression and parallel\ntraining for pre-trained language models.\n2.1 Network Compression for Pre-trained Language\nModels\nAs the research focus in this paper, network quantization\nreplaces the original full-precision parameters and acti-\nvations with low-bit representations [26], [27], [28], [29],\n[30], [31], [32], [33]. To apply network quantization on\nPLMs, Q8BERT [16] convert both parameters and activa-\ntions with 8-bit representations, and ﬁnds that there is\nnegligible accuracy drop on natural language understand-\ning tasks. Q-BERT [17] exploit the hessian matrix of loss\ncurvature to determine the best layer-wise quantization bit-\nwidth, which achieves a higher compression rate. Addition-\nally, TernaryBERT [19] propose to ternarize the parameters\nwith 2-bit representations together with two-stage knowl-\nedge distillation [8]. Recently, BinaryBERT [20] binarize\nthe model parameters based by splitting from ternarized\nmodels, which further reduces the quantization bit-width.\nDespite the promising performance of these quantization\napproaches, they mostly follow quantization-aware training\nthat requires heavy ﬁne-tuning, which can be prohibitive\ngiven constraints on the training time, memory size, and\ndata accessibility. In this work, we shall follow the post-\ntraining quantization [21], [22], [34], [23], [24], [35], the other\nway to improve the quantized PLMs given limited training\nresources. More details on quantization will be discussed in\nSection 3.\nAside from quantization, there are also several other\npopular techniques to compress PLMs. Pruning removes\nunimportant parameters or connections in a well-trained\nmodel [36], [37], [38], [39], [40], and is widely explored in\nPLMs [41], [42]. A direct way is to remove connections with\nsmall magnitudes during the pre-training and adds them\nback when necessary in downstream tasks [41]. Structured\npruning can also be applied to directly reduce the width\nof BERT [43], which is more friendly for practical inference\nacceleration. In [4], it is shown that attention heads can\nalso be pruned without hurting the representation of PLMs.\nFurthermore, a comprehensive study is provided in [42] to\ninvestigate the pruning sensitivity of different parts of the\nTransformer model. There are also efforts on dropping the\nentire layers of transformer models [5], [14].\nKnowledge distillation [44], [45], [46], [47], [48] is an-\nother successful tool to design efﬁcient PLMs, by distilling\nknowledge from a large teacher model to a smaller student\nmodel. Knowledge distillation is ﬁrst applied to PLMs by\nminimizing the soft cross-entropy between output logits [6].\nAside from this, recent efforts show that it is also helpful\nto minimize the mean square error of hidden representa-\ntions [8], [7]. While knowledge distillation is promising in\nperformance, memory consumption is the main concern as\nthe teacher model itself or the size of its pre-computed rep-\nresentation for distillation is generally large. Post-training\nquantization is also closely related to knowledge distilla-\ntion [22], [21], [23], [24], when the full-precision model acts\n3\nas the teacher to provide layer-wise supervision signals to\nthe quantized student model.\nAn orthogonal direction is to apply neural architecture\nsearch (NAS) for efﬁcient PLMs structures. AdaBERT [49]\nadopt the differentiable search [50] to automatically com-\npress BERT into task-speciﬁc architectures. To obtain task-\nagnostic architectures, NAS can also be applied during\nthe pre-training stage [51], [52]. Recently, one-shot NAS\nis also developed to search tiny PLMs [53]. Despite the\npromising performance of these approaches, the algorithmic\nefﬁciency [54], [11] is a major concern for NAS-based PLMs.\n2.2 Parallel Training for Pre-trained Language Models\nParallel training is also a popular topic in training large\npre-trained language models, where pipeline model paral-\nlelism [55], [56], [57], [58], [59] is closely related to our pro-\nposed parallel training strategy. Speciﬁcally, by partitioning\nthe model into multiple modules, pipeline parallelism sim-\nilarly puts each module on individual computing devices.\nHowever, pipeline parallelism has a high computational\ncost among different workers to transmit the intermediate\ntensors in the forward and backward pass. GPipe [55]\nadopts mini-batches to reduce the bubble time, but still\nsuffers from limited speed-up as will be discussed in Sec-\ntion 4.2.2. PipeDream [56] optimizes the module partition to\nminimize the communication cost, but the resulting strategy\nstill highly depends on the model architecture. Our parallel\ntraining strategy, on the other hand, does not require real-\ntime communication among workers and thus brings the\ntheoretical speed-up. While the solution can be suboptimal,\nwe propose a novel teacher-forcing mechanism to alleviate\nthe problem and the resultant performance is already rea-\nsonably good for post-training quantization.\nAside from pipeline model parallelism, there are several\nother dimensions for parallel training. Data-parallelism [60],\n[61] is the most widely used solution in modern deep learn-\ning frameworks, when the training data are partitioned over\nmultiple workers. Op-level model parallelism [62], [63], [64]\nslices the parameters over multiple works to compute local\nresults and concatenate them together afterwards, which\nhas a high communication cost. Optimizer model paral-\nlelism [65] is capable of partitioning parameters, gradients,\nand optimizer states into each computing device, achieving\nlinear memory reduction with a number of workers. It\nwould be promising to combine all these approaches with\nour parallel strategy, enabling the post-training quantization\non gigantic pre-trained language models such as GPT-3 [66]\nand PanGu-α[67].\n3 M OTIVATION\nIn this section, we show that it is important yet challenging\nto conduct post-training quantization of PLMs. Before div-\ning into details, we ﬁrst review the necessary backgrounds\nfor network quantization.\n3.1 Quantization Background\nNetwork quantization replaces the original full-precision\nweight or activation x ∈Rm×n with its lower-bit counter-\npart ˆx. Denoting s∈R+ as the step size, the b-bit symmetric\nuniform quantization function Qb(·) can be written as\nˆx = Qb(x) = s·ΠΩ(b)(x/s), (1)\nwhere Ω(b) = {−2b−1 + 1,..., 0,..., 2b−1 −1}is the set of\nb-bit integers, and Π(·) is the projection function that maps\nx/sto its closest integer.\nIn the context of quantization of Transformer-based\nPLMs, we follow the default setting in previous works [16],\n[19], [20]: we quantize both the network weights and acti-\nvations in each matrix multiplication. We use symmetric\nuniform quantization for weights, embeddings, and activa-\ntions, except activations after the self-attention and GeLU\nfunction. For these two activations, we adopt asymmetric\nquantization since their elements are mostly positive. We\nskip the quantization for all layer-normalization layers, skip\nconnections, biases and the last classiﬁcation head due to\nlimited computation overhead or large performance drop.\nIn the following, we introduce two common branches in\nthe quantization literature: quantization-aware training and\npost-training quantization.\n3.1.1 Quantization-aware Training (QAT)\nQuantization-aware training resembles normal training, but\nperforms the forward propagation with the quantized net-\nwork. Thus it is also time-consuming to iterate over the\nentire training set D. Typical training objective can be either\nthe cross-entropy loss between the prediction and ground-\ntruth labels for classiﬁcation tasks [16], or the distillation\nobjective between the quantized model and a full-precision\nteacher model [19]. As the quantization function Qb(·) is\nnon-differentiable, straight-through estimator [26] is usually\nadopted to allow the gradient back propagation through\nthese discrete operations.\n3.1.2 Post-training Quantization (PTQ)\nUnlike QAT, post-training quantization seeks to recover the\nperformance degradation without intensive training over\nthe entire training set D. One line of PTQ research quantizes\nthe network purely without using any training data, but\nremoves outliers in the full-precision parameters. This can\nbe achieved by splitting an outlier neuron with a large\nmagnitude into two parts [34], where the magnitude can be\nhalved. Alternatively, one can scale down outlier magnitude\nand multiply it back in subsequent layers, a.k.a. weight\nequalization in [21]. Another solution is to treat the outliers\nand normal values in the distribution separately, by keeping\ntwo sets of quantization parameters [68], [18].\nAnother line of PTQ research [22], [69], [23], [35] aims at\nreconstruction error minimization (REM) using a very slight\nportion of unlabeled data (a.k.a. calibration set) ˜D ⊆ D\nfrom the original training set. Compared with training-free\nPTQ approaches, such an approach is able to signiﬁcantly\nimprove the performance of the quantized network. REM\ncan be achieved by minimizing the distance between the\noutput of multiplication between the quantized and the full-\nprecision counterpart as follows:\nmin\nw,s\n∥ˆw⊤ˆa −w⊤a∥2, (2)\ns.t. ˆw = Qbw (w), ˆa = Qba (a),\n4\n(a) Training Time.\n (b) Memory Overhead.\n (c) Data Accessibility.\n (d) Performance.\nFigure 2: Comparison between QAT and REM-based PTQ over four dimensions. We use a BERT-large model over MNLI\ndataset for illustration. The full-precision (FP) ﬁne-tuning is also included as a baseline. We follow the procedure in [19]\nfor QAT, and REM in Equation (2) for PTQ. The training time and memory in (a) and (b) are measured by 4-bit weights\nand 8-bit activations (i.e., W4A8) on an NVIDIA V100 GPU.\nwhere w and a are full-precision weights and activations,\nˆw and ˆa are their quantized representations with bw and\nba bit-widths, and s denotes all step-sizes involved for\nquantization. REM is usually conducted in a greedy man-\nner. It proceeds to the matrix multiplication only after the\ntraining of previous ones. Meanwhile, Equation (2) can be\nsolved quickly with the calibration set ˜D. Recent work [70]\nalso theoretically shows that such greedy objective is more\nsample-efﬁcient compared with conventional end-to-end\ntraining. In this paper, we shall extend REM-based post-\ntraining quantization given its past success.\n3.2 Why Post-training Quantization?\nIn this section, we discuss the difference between REM-\nbased PTQ and QAT along four dimensions of a quantiza-\ntion pipeline: 1) training time; 2) memory overhead; 3) data\naccessibility and 4) performance. According to Figure 2, we\nsummarize the ﬁndings in the following paragraphs.\n3.2.1 Training Time\nAs QAT iterates over the full training set Dfor multiple\nepochs, it is much more time-consuming than PTQ. Note\nthat recent QAT methods [19], [20] further combine two-\nstage knowledge distillation [8], which even prolongs the\ntraining compared with the full-precision (FP) ﬁne-tuning.\nAs shown in Figure 2(a), QAT can take nearly four times\nlonger than FP .\n3.2.2 Memory Overhead\nThe increasing size of recent large PLMs makes it prohibited\nto conduct QAT on memory-limited computing resources.\nFrom Figure 2(b), QAT [19] even consumes 8.3 GB more\nmemory than FP when combined with knowledge distil-\nlation to store the full-precision teacher model. On the\nother hand, PTQ only caches intermediate results during\nthe layer-wise REM in Equation (2), which can be fed into\na single GTX 1080 Ti. Therefore, PTQ is also applicable on\nmemory-limited computing devices.\n3.2.3 Data Accessibility\nThe quantization service can be usually offered by some\nthird-party organizations, where data security is always of\nhigh priority. As QAT requires access to the entire training\nset, it inevitably increases the risk of data exposure. PTQ,\non the other hand, needs only a small amount of calibration\ndata ˜D ⊆ D, and can be easily constructed by randomly\nsampling 1K ∼ 4K instances from D, as shown in Fig-\nure 2(c). Therefore, most original training instances are kept\nuntouched and data security can be largely guaranteed.\n3.2.4 Performance\nWhen ﬁne-tuned over the entire training set, QAT usually\nmaintains better quantized performance than PTQ. From\nFigure 2(d), the performances of QAT are close to FP results,\nand remain steady across different bit-widths, i.e., W4A8,\nW2A8 and W2A4. However, the performances of PTQ drop\nsigniﬁcantly, which has been the main concern to address.\nIn summary, REM-based PTQ is superior to QAT with\nregard to training efﬁciency, memory overhead, and data\naccessibility. Nevertheless, it is still often less preferred than\nQAT due to its severe performance drop especially for low\nquantization bit-width [16], [17], [19]. In this paper, we aim\nat improving the performance of post-training quantization\nfor PLMs, while preserving its merits of fast training, light\nmemory overhead, and data consumption.\n4 M ETHODOLOGY\nIn this section, we propose our solution to improve the post-\ntraining quantization of Transformer-based PLMs. We ﬁrst\nextend the existing reconstruction error minimization from\nthe layer-wise to the module-wise granularity to ﬁt Trans-\nformer models. Secondly, based on the module partition, we\nfurther design a new parallel training strategy that further\nspeeds up the PTQ pipeline. An overview of our solution\ncan be found in Figure 3.\n4.1 Module-wise Reconstruction Error Minimization\nWe propose a new PTQ solution called module-wise re-\nconstruction error minimization (MREM) for PLMs. Existing\nREM [23] solves Equation (2) for each matrix multiplication.\nHowever, a standard transformer layer in PLMs consists\nof a Multi-Head Attention (MHA) and a Feed-Forward\nNetwork (FFN), both of which contain a number of matrix\nmultiplications that are coupled together. Greedily tackling\n5\nFigure 3: The overview of the proposed module-wise reconstruction error minimization (MREM). We partition both the\nfull-precision model and quantized model into multiple modules and put these modules on different computing devices.\nBy sampling tensors from the input queue, each module can be trained locally without waiting for its predecessors. Teacher\nforcing is applied to mitigate the issue of reconstruction error propagation on the quantized module.\neach matrix multiplication in REM can thus lead to sub-\noptimal quantized networks. Moreover, the insufﬁciently\nminimized reconstruction error shall propagate and enlarge\nalong with transformer layers, and ﬁnally deteriorate the\nnetwork output [71], [72].\nTowards that end, the proposed module-wise reconstruc-\ntion error minimization admits larger granularity by jointly\noptimizing all the coupled linear layers inside each module.\nSpeciﬁcally, given a PLM with Ltransformer layers, embed-\nding layers and the classiﬁcation head, we partition them\ninto N modules, where the n-th module include [ln,ln+1)\ntransformer layers with ln being the ﬁrst layer of this\nmodule1. MREM aims at minimizing the joint reconstruction\nerrors between all quantized FFN output ˆfl in the module\nfrom their full-precision counterpart fl as follows:\nmin\nwn,sn\nℓ(n) =\n∑\nl∈[ln,ln+1)\n∥ˆfl −fl∥2, (3)\nwhere wn and sn are all learnable parameters and quantiza-\ntion step sizes in the n-th module. Similar to REM, MREM\ncan be optimized sequentially: given previously trained\nmodules, only parameters and quantization step sizes in\nthe current module are updated. Besides the grouped Trans-\nformer layers, we also minimize the MSE loss in the Trans-\nformer embedding and output logits respectively.\nNote that the number of modules N can be adjusted de-\npending on the memory constraint of computing resources.\nWhen N = 1, this reduces to intermediate-layer knowledge\ndistillation [8], which can be memory-demanding when\nquantizing large PLMs on a single GPU.\n4.2 Accelerated Parallel Training\nBased on the proposed MREM, we propose a new model\nparallel strategy to further accelerate the training. As shown\nin Figure 3, we put different modules on individual com-\nputing devices. A set of input queuesI = {I1,..., IN−1}\nis deployed between each pair of adjacent modules. For the\n1. Note that the embedding layers and the classiﬁcation head are\nincorporated in the ﬁrst and last module respectively.\nn-th module, the queue collects its output of the most recent\nt0 steps, i.e., It\nn = {ft\nln ,ft−1\nln ,..., ft−t0+1\nln }. Meanwhile, the\n(n + 1)-th module can always sample with replacement\nfln ∼ It\nn from the queue without waiting for the n-th\nmodule. Similar rules hold for the quantized module and\ntheir input queues ˆI as well. The design of the input queue\nresembles stale synchronous parallel [73] which stores the stale\noutput in a local cache so as to reduce the waiting time\namong workers, where t0 is the stale threshold.\nThe training workﬂow is as follows. Initially, every mod-\nule is computed one after another in the ﬁrst t0 step to\nﬁll in the input queue, after which parallel training takes\nplace. Then the module samples input from the queue and\ncalculates the loss ℓ(n) correspondingly for n = 1 ,...,N .\nMeanwhile, the input queue is also updated with the rule\nof ﬁrst-in-ﬁrst-out throughout the training. In the backward\npass, we constrain the gradients to propagate locally within\neach module, without affecting its predecessors. Such a\ndesign can avoid the load imbalance issue from straggler\nmodules, bringing nearly the theoretical N×speed-up.\n4.2.1 Annealed Teaching Forcing\nSince all modules proceed with training simultaneously\ninstead of the sequential manner, the next module takes\nthe output from the queue before its predecessor is fully\noptimized. Therefore, the reconstruction error from the pre-\ndecessor is propagated to the following modules before it is\nsufﬁciently minimized.\nInspired by teacher forcing [25] in training recurrent net-\nworks, the output fln from the n-th full-precision module\nnaturally serves as the clean input to the(n+1)-th quantized\nmodule to substitute ˆfln . Thus fln stops the propagation\nof the reconstruction error accumulated on the quantized\nmodule. Nevertheless, such an approach breaks the connec-\ntion to previous quantized modules and may suffer from\nforward inconsistency between training and inference [72]\non the quantized model. To achieve a proper trade-off, we\ntake the convex combination between the full-precision fln\nand quantized ˆfln as follows:\n˜fln = λfln + (1 −λ)ˆfln , λ ∈[0,1], (4)\n6\nAlgorithm 1Efﬁcient Post-training Quantization.\n1: procedure MAIN :\n2: Partition the PLM into N modules\n3: Fill in the input queues I, ˆI\n4: for nin 1, ...,N do\n5: ⊿ run in parallel\n6: while t<T do\n7: fln−1 ∼It\nn−1, ˆfln−1 ∼ˆI\nt\nn−1\n8: ft\nln ,ˆf\nt\nln ←MREM ( fln−1 ,ˆfln−1 ,t)\n9: Update It\nn, ˆI\nt\nn with ft\nln ,ˆf\nt\nln\n10: return the Quantized PLM\nwhere the hyperparameter λcontrols the strength of teacher\nforcing. λ = 1 gives the full correction of reconstruction\nerror but with forward inconsistency, while λ = 0 reduces\nto the conventional setting that suffers from the propagated\nreconstruction error. We adopt a linear decay strategy for λ:\nλt = max(1− t\nT0\n, 0), where T0 is the preset maximum steps\nof the decay. Intuitively, a largeλis desired at the beginning\nwhen each module is rarely optimized. Later, a small λ is\npreferred to transit to normal training such that the forward\ninconsistency can be bridged. The remaining T −T0 steps\nstick to normal training without teacher forcing, so as to\nmake each quantized module adapt to its own predecessors.\n4.2.2 Comparison with Pipeline Parallelism\nNotably, our MREM with stale synchronous parallel is dif-\nferent from the recent pipeline parallel [55], [56]. Pipeline\nparallel adopts end-to-end training with synchronous up-\ndates between adjacent modules, which gives rise to bubble\ntime on computing devices. While GPipe [55] divides the\noriginal data batch into M pipelined micro-batches, it still\nhas the bubble time of O( N−1\nN+M−1 ) under N partitions. On\nthe one hand, a larger N or smaller M would increase\nthe bubble time. On the other hand, a larger M leads to\nsmaller batches that still cannot fully exploit the computing\npower, which again affects the acceleration rate. Differently,\nour parallel strategy conducts local training with stale syn-\nchronous updates of the module. Hence there is negligible\nbubble time as long as the straggler is faster than the\nstaleness threshold t0, which can be easily satisﬁed with\nbalanced module partitions or larger t0.\nFinally, an overview of the proposed parallel module-\nwise reconstruction error minimization is shown in Algo-\nrithm 1 and Algorithm 2. The Update (·) in Algorithm 2 can\nbe any gradient update function such as AdamW [74] with\nlearning rate ηt.\n5 E XPERIMENTS\nIn this section, we empirically verify the proposed MREM\nfor post-training quantization of PLMs. We ﬁrst introduce\nthe experimental setup in Section 5.1. Then we present main\nresults in Section 5.2, including comparisons with QAT and\nREM, as well as other existing quantization baselines. In\nSection 5.4, we provide more discussions on a variety of\nfactors in our approach, such as the effect of teacher forcing,\nthe number of model partitions, and calibration data size.\nCode will be released upon acceptance.\nAlgorithm 2Module-wise Reconstruction Error Min.\n1: procedure MREM ( fln−1 ,ˆfln−1 ,t):\n2: if t<T 0 then\n3: λt ←max(1 − t\nT0\n, 0)\n4: Compute ˜fln−1 by Equation (4)\n5: Compute the full-precision module output ft\nln\n6: Compute the quantized module output ˆf\nt\nln\n7: Compute the loss ℓ(n) by Equation (3)\n8: wt+1\nn ←Update(wt\nn,∂ℓ(n)\n∂wtn\n,ηt)\n9: st+1\nn ←Update(st\nn,∂ℓ(n)\n∂stn\n,ηt)\n10: return ft\nln ,ˆf\nt\nln\n5.1 Experimental Setup\n5.1.1 Datasets and Metrics\nWe evaluate post-training quantization w.r.t. both text clas-\nsiﬁcation on the GLUE dataset [75], and reading compre-\nhension on SQuAD benchmarks [76]. The size of calibration\ndata is by default |˜D| = 4 ,096, with instances randomly\nsampled from the full training set. As both RTE and MRPC\ntasks in the GLUE benchmark contain fewer than 4,096\nsamples, we use their full training set on these two tasks. We\nleave the study of data size in Section 5.4. Each experiment\nis repeated ten times with different calibration sets, and both\nthe mean and standard deviations are reported.\nWe use the same evaluation metrics in [2], [19] for the\ndevelopment set of GLUE and SQuAD benchmarks. For\nresults in Section 5.2, we report accuracies on both the\nmatched section and mis-matched sections of MNLI, and\nEM (exact match) and F1 score for SQuAD. Additionally,\nwe also report the training time (min), memory overhead\n(GB) as well as the size of the training set (K). We also\nprovide comparisons with other existing methods in Sec-\ntion 5.3, where we adopt Matthews correlation for CoLA,\nSpearman correlation for STS-B, and accuracy for the rest\nones (i.e., RTE, MRPC, SST-2, QQP , MNLI). We also report\nthe averaged performance on GLUE as an overview.\n5.1.2 Implementation\nWe use the standardly ﬁne-tuned BERT-base and BERT-large\nmodels2 on downstream tasks for post-training quantiza-\ntion. We implement MREM in both the sequential training\n(abbv. MREM-S) in Section 4.1 and parallel training with\nteaching forcing (abbv. MREM-P) in Section 4.2.1. For each\nmodule, we train for 2,000 steps with an initial learning\nrate of 1e−4 on GLUE tasks, and 4,000 steps with an initial\nlearning rate of 5e−5 on SQuAD datasets. The learning rate\ndecays linearly as done in [8], [19]. By default, we partition\nthe model into 4 modules on 4 NVIDIA-V100 GPUs. The\nanalysis of the training steps and partition numbers will be\nprovided in Section 5.4.\nFor baselines, we mainly compare with QAT and REM,\nwhere the former measures how much PTQ can get close\nto QAT, and the latter studies the effect of objective gran-\nularity in PTQ training. We conduct QAT following the\nstate-of-the-art training pipeline [19], i.e., intermediate-layer\n2. We follow the default ﬁne-tuning hyperparameter settings in Hug-\ngingface: https://github.com/huggingface/transformers.\n7\n#Bits\n(W-E-A)\nQuant\nMethod\nBERT-base BERT-large\nTime\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓\nAcc\nm(%)↑\nAcc\nmm(%) ↑\nTime\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓\nAcc\nm(%)↑\nAcc\nmm(%) ↑\nMNLI\nfull-prec. N/A 220 8 .6 393 84 .5 84 .9 609 21 .5 393 86 .7 85 .9\n4-4-8\nQAT 1,320 11 .9 393 84 .6 84 .9 3,180 29 .8 393 86 .9 86 .7\nREM 28 2 .5 4 73 .3±0.3 74.9±0.2 84 5 .5 4 70 .0±0.4 71.8±0.3\nMREM-S 36 4 .6 4 83 .5±0.1 83.9±0.1 84 10 .8 4 86 .1±0.1 85.9±0.1\nMREM-P 9 3 .7×4 4 83 .4±0.1 83.7±0.1 21 8 .6×4 4 85 .5±0.1 85.4±0.2\n2-2-8\nQAT 882 11 .9 393 84 .4 84 .6 2,340 29 .8 393 86 .5 86 .1\nREM 24 2 .5 4 71 .6±0.4 73.4±0.4 64 5 .5 4 66 .9±0.4 68.6±0.7\nMREM-S 24 4 .6 4 82 .7±0.2 82.7±0.2 64 10 .8 4 85 .4±0.2 85.3±0.2\nMREM-P 6 3 .7×4 4 82 .3±0.2 82.6±0.2 16 8 .6×4 4 84 .6±0.2 84.6±0.1\n2-2-4\nQAT 875 11 .9 393 83 .5 84 .2 2,280 29 .8 393 85 .8 85 .9\nREM 24 2 .5 4 58 .3±0.5 60.6±0.6 64 5 .5 4 48 .8±0.6 51.4±0.8\nMREM-S 24 4 .6 4 81 .1±0.2 81.5±0.2 64 10 .8 4 83 .6±0.2 83.7±0.2\nMREM-P 6 3 .7×4 4 80 .8±0.2 81.2±0.2 16 8 .6×4 4 83 .0±0.3 83.2±0.2\nTable 1: Results of our proposed MREM-S and MREM-P against QAT and REM on the development set of MNLI. “#Bits (W-\nE-A)” represents the bit-width for weights of Transformer layers, word embedding, and activations. Acc-m and Acc-mm\ndenote accuracies on the matched and mismatched sections of MNLI respectively.\n#Bits\n(W-E-A)\nQuant\nMethod\nBERT-base BERT-large\nTime\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓ EM (%) ↑ F1 (%)↑ Time\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓ EM (%) ↑ F1 (%)↑\nSQuAD v1.1\nfull-prec. - 177 11 .7 88 81 .5 88 .7 488 30 .4 88 86 .9 93 .1\n4-4-8\nQAT 428 18 .4 88 80 .2 87 .9 1,920 27.0 88 86 .7 93 .0\nREM 65 3 .1 4 46 .1±0.5 60.0±0.5 175 7 .3 4 68 .3±0.1 79.3±0.1\nMREM-S 76 6 .4 4 79 .4±0.1 87.2±0.1 200 14 .5 4 86 .2±0.1 92.5±0.1\nMREM-P 19 5 .5×4 4 79 .6±0.1 87.3±0.1 50 12 .3×4 4 86 .0±0.1 92.4±0.1\n2-2-8\nQAT 335 18 .4 88 79 .3 87 .2 1,200 27.0 88 86 .1 92 .5\nREM 60 3 .1 4 40 .1±0.4 55.0±0.4 160 7 .3 4 66 .4±0.5 77.7±0.3\nMREM-S 60 6 .4 4 77 .8±0.2 86.0±0.1 156 14 .5 4 85 .4±0.1 91.9±0.1\nMREM-P 15 5 .5×4 4 77 .7±0.2 85.9±0.2 39 12 .3×4 4 85 .3±0.2 91.8±0.1\n2-2-4\nQAT 331 18 .4 88 77 .1 85 .9 1,186 27.0 88 84 .7 93 .1\nREM 60 3 .1 4 10 .4±0.2 24.6±0.2 160 7 .3 4 28 .3±0.6 45.0±0.5\nMREM-S 60 6 .4 4 72 .7±0.2 82.5±0.2 156 14 .5 4 81 .4±0.3 89.4±0.2\nMREM-P 15 5 .5×4 4 73 .0±0.3 82.7±0.2 39 12 .3×4 4 81 .8±0.3 89.6±0.2\nTable 2: Results of our proposed MREM-S and MREM-P against QAT and REM on the development set of SQuAD v1.1.\n“ ” denotes results with two gradient accumulation steps under the same total batch size due to memory constraint.\ndistillation followed by prediction-layer distillation, which\ntakes 6 training epochs in total. Detailed hyperparameter\nsettings can be found in [19]. In terms of REM, we follow\nthe practice in [23], [35] to minimize the reconstruction\nerror after each matrix multiplication, as introduced in\nSection 3.1. For a fair comparison of each method, we use\nthe same quantization scheme, i.e., TWN [27] or LAQ [29]\nfor 2-bit and 4-bit weight quantization, and LSQ [30] for\nall activation quantization. Unlike QAT that picks the best\nmodel based on the development set results, MREM is only\ntested once after training, which ensures data security of\nthe development set. We leave the comparison with more\nexisting quantization approaches in Section 5.3.\n5.2 Main Results: Comparison with QAT and REM\nWe ﬁrst compare MREM-S and MREM-P with QAT and\nREM over MNLI and SQuAD benchmarks. We take BERT-\nbase and BERT-large as backbone PLMs for quantization.\nThe results on MNLI, SQuADv1.1 and SQuADv2.0 are\nsummarized in Table 1, Table 2 and Table 3 respectively. We\nsummarize the results from the four dimensions mentioned\nin Section 3.2.\n5.2.1 Performance\nIt can be found that our proposed MREM-S improves the\nperformance of REM signiﬁcantly given the same training\ntime, and is much closer to QAT. For instance, according\nto in Table 1, MREM-S with 4-bit weight quantization on\nBERT-base and BERT-large achieves accuracies of83.5%±0.1\nand 86.1%±0.1 on the matched section of MNLI, which is\non average 10.2% ↑and 16.1% ↑better than REM, and\nonly 1.1% ↓and 0.8% ↓inferior to QAT, respectively. With\nREM, BERT-base sometimes even outperforms BERT-large\non MNLI. We speculate that this is due to the suboptimal\nsolutions in REM that lead to propagated reconstruction\nerror when more neurons or transformer layers are stacked\nin BERT.\nMoreover, with all modules trained in parallel, MREM-P\nis still close to or only slightly inferior to MREM-S. From\nresults of SQuAD v1.1 in Table 2, MREM-P can even outper-\nform MREM-S with the “W2-E2-A4” quantized BERT-large\nmodel (i.e., the EM score and F1 score are on average0.4% ↑\nand 0.2% ↑respectively).\n5.2.2 Training Time\nOur proposed MREM also enjoys signiﬁcantly less training\ntime than QAT. For instance, MREM only takes 84 minutes\n8\n#Bits\n(W-E-A)\nQuant\nMethod\nBERT-base BERT-large\nTime\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓ EM (%) ↑ F1 (%)↑ Time\n(min)↓\nMem\n(GB)↓\n# Data\n(K)↓ EM (%) ↑ F1 (%)↑\nSQuAD v2.0\nfull-prec. - 255 11 .7 130 74 .5 77 .7 730 30 .4 130 77 .7 81 .0\n4-4-8\nQAT 662 18 .4 130 74 .4 77 .5 2,820 28.3 130 77 .4 80 .5\nREM 60 3 .1 4 53 .1±0.4 53.6±0.4 175 7 .3 4 58 .2±0.2 61.4±0.3\nMREM-S 76 6 .4 4 73 .0±0.1 76.3±0.1 200 14 .5 4 76 .4±0.1 79.7±0.1\nMREM-P 19 5 .5×4 4 72 .6±0.2 75.9±0.2 50 12 .3×4 4 76 .3±0.1 79.6±0.1\n2-2-8\nQAT 508 17 .5 130 73 .0 76 .2 1,680 28.3 130 76 .7 80 .0\nREM 60 3 .1 4 51 .5±0.2 51.8±0.2 160 7 .3 4 56 .3±0.2 59.5±0.2\nMREM-S 60 6 .4 4 71 .4±0.2 74.8±0.2 156 14 .5 4 75 .4±0.2 78.7±0.1\nMREM-P 15 5 .5×4 4 70 .8±0.4 74.3±0.4 39 12 .3×4 4 75 .3±0.3 78.6±0.3\n2-2-4\nQAT 505 17 .5 130 71 .4 74 .6 1,655 28.3 130 75 .4 78 .9\nREM 60 3 .1 4 39 .3±1.5 41.4±1.3 160 7 .3 4 42 .9±0.8 44.2±0.7\nMREM-S 60 6 .4 4 67 .2±0.3 70.6±0.2 156 14 .5 4 71 .3±0.3 74.8±0.2\nMREM-P 15 5 .5×4 4 66 .1±0.5 69.8±0.5 39 12 .3×4 4 71 .5±0.3 75.0±0.3\nTable 3: Results of our proposed MREM-S and MREM-P against QAT and REM on the development set of SQuAD v2.0.\n“ ” denotes results with two gradient accumulation steps under the same total batch size due to memory constraint.\nQuant\nMethod\n#Bits\n(W-E-A)\nSize\n(MB) PTQ MNLI-m QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\n- full-prec. 418 - 84.9 91 .4 92 .1 93 .2 59 .7 90 .1 86 .3 72 .2 83.9\nQ-BERT 2-8-8 43 \u0017 76.6 - - 84.6 - - - - -\nQ-BERT 2/4-8-8 53 \u0017 83.5 - - 92.6 - - - - -\nQuant-Noise PQ 38 \u0017 83.6 - - - - - - - -\nTernaryBERT 2-2-8 28 \u0017 83.3 90 .1 91 .1 92 .8 55 .7 87 .9 87 .5 72 .9 82.7\nGOBO 3-4-32 43 \u0013 83.7 - - - - 88.3 - - -\nGOBO 2-2-32 28 \u0013 71.0 - - - - 82.7 - - -\nMREM-S 4-4-8 50 \u0013 83.5±0.1 90.2±0.1 91.2±0.1 91.4±0.4 55.1±0.8 89.1±0.1 84.8±0.0 71.8±0.0 82.4±0.1\n2-2-8 28 \u0013 82.7±0.2 89.6±0.1 90.3±0.2 91.2±0.4 52.3±1.0 88.7±0.1 86.0±0.0 71.1±0.0 81.5±0.2\nMREM-P 4-4-8 50 \u0013 83.4±0.1 90.2±0.1 91.0±0.2 91.5±0.4 54.7±0.9 89.1±0.1 86.3±0.0 71.1±0.0 82.2±0.1\n2-2-8 28 \u0013 82.3±0.2 89.4±0.1 90.3±0.2 91.3±0.4 52.9±1.2 88.3±0.2 85.8±0.0 72.9±0.0 81.6±0.2\nTable 4: Results on the GLUE development set. “Size” refers to model storage in “MB”. “PTQ” indicates whether the\nmethod belongs to post-training quantization. “Avg.” denotes the average results of all tasks.\nfor 4-bit weight quantized training on the BERT-large over\nMNLI, which is about 38×faster than QAT and 7×faster\nthan full-precision ﬁne-tuning. When compared with REM,\nMREM does not need to cache the output after every matrix\nmultiplication, which admits more iterations given the same\namount of time. We shall discuss this further in Section 5.4.2.\nMoreover, when armed with the proposed parallel training,\nMREM-P is further 4×faster than MREM-S, which achieves\nthe theoretical linear speedup on 4 GPUs. These together\nbring more than 150× reduction of training time when\ncompared with QAT.\n5.2.3 Memory Overhead\nWhile the module-wise training inevitably consumes more\nmemory than REM, it still takes only around a third of the\nmemory by QAT, and a half of that by the full-precision ﬁne-\ntuning. For instance, while QAT takes 29.8 GB memory on\nBERT-large, MREM only consumes 10.8 GB memory, which\ncan be even fed into a single NVIDIA GTX 1080 Ti. More-\nover, for input with a longer sequence length (i.e., 384 tokens\non the SQuAD dataset), QAT over BERT-large may suffer\nfrom memory overﬂow even on an NVIDIA V100 GPU with\n32GB memory. QAT with gradient accumulation inevitably\ndoubles the training time under the same total batch size\n(i.e., underlined ﬁgures (“ ”) in Table 2 and Table 3).\nOn the other hand, such issues can be easily mitigated in\nboth REM and our proposed MREM. Meanwhile, increasing\nthe number of modules can further decrease the memory\noverhead of each module, but may harm the performance,\nas will be discussed in Section 5.4.3.\n5.2.4 Data Accessibility\nBoth REM and our proposed MREM follow the common\npractice of PTQ, relying on only 4,096 randomly sampled\ntraining instances on both MNLI and SQuAD, which is a\ntiny fraction of the original dataset used in QAT. We shall\nprovide more discussion on the effect of calibration size in\nSection 5.4.\nIn summary, our MREM-S improves post-training quan-\ntization on PLMs signiﬁcantly, while still enjoys fast train-\ning, light memory overhead, and data security. More-\nover, with parallel training, the proposed MREM-P further\nstrengthens the advantages of PTQ without an apparent\nperformance drop.\n5.3 Main Results: Comparison with Existing Methods\nIn the next, we compare our MREM with a number of\nexisting state-of-the-art BERT quantization methods. They\ninclude various QAT approaches such as Q-BERT [17],\nQuant-Noise [77] and TernaryBERT [19], as well as the PTQ\nbaseline GOBO [18]. Their results are taken from the original\npapers, respectively.\nFrom Table 4, both our proposed MREM-S and MREM-\nP outperform existing PTQ approaches in most cases, and\neven achieve results close to QAT approaches. For example,\nthe “W4-E4-A8” quantized MREM-S and MREM-P have\nthe averaged accuracies of 83.5% and 83.4% on MNLI\n9\n#Bits\n(W-E-A) # Steps BERT-base BERT-large\nw/o TF w/ TF w/o TF w/ TF\n2-2-8\n250 79.6±0.3 80.7±0.2 82.1±0.4 83.1±0.2\n500 81.0±0.3 81.6±0.2 83.4±0.3 84.1±0.3\n2,000 82.2±0.2 82.7±0.2 84.3±0.3 84.6±0.2\n4,000 82.3±0.3 82.5±0.2 84.5±0.2 84.7±0.2\n2-2-4\n250 73.9±0.5 77.3±0.4 76.5±0.9 79.3±0.4\n500 77.9±0.2 79.0±0.2 80.0±0.5 81.4±0.2\n2,000 80.4±0.2 80.8±0.2 82.5±0.4 83.0±0.3\n4,000 80.7±0.2 81.0±0.2 83.1±0.1 83.3±0.3\nTable 5: Ablation studies of teacher forcing at different\ntraining steps over MNLI-m.\n#Bits\n(W-E-A)\nQuant\nMethod # Steps Time\n(min)↓\nMem\n(G)↓\nAcc\nm(%)↑\nAcc\nmm(%)↑\n4-4-8\nREM 200 36 2 .5 73 .3±0.3 74.9±0.2\nREM 2,000 319 2 .5 81 .8±0.2 82.5±0.1\nMREM-S 2,000 36 4 .6 83 .5±0.1 83.9±0.1\n2-2-8\nREM 200 24 2 .5 71 .6±0.4 73.4±0.4\nREM 2,000 213 2 .5 78 .7±0.2 79.2±0.2\nMREM-S 2,000 24 4 .6 82 .7±0.2 82.7±0.2\n2-2-4\nREM 200 24 2 .5 58 .3±0.5 60.6±0.6\nREM 2,000 213 2 .5 73 .0±0.3 74.4±0.4\nMREM-S 2,000 24 4 .6 81 .1±0.2 81.5±0.2\nTable 6: Comparison of REM with our MREM on BERT-\nbase over MNLI.\n(a) Module-1 (250 Steps).\n (b) Module-2 (250 Steps).\n (c) Module-3 (250 Steps).\n (d) Module-4 (250 Steps).\n(e) Module-1 (2,000 Steps).\n (f) Module-2 (2,000 Steps).\n (g) Module-3 (2,000 Steps).\n (h) Module-4 (2,000 Steps).\nFigure 4: The training loss curves with and without teacher forcing (TF) in MREM-P . The red area denotes teacher forcing\nin the ﬁrst 40% training steps. (a), (b), (c) and (d) in the ﬁrst row are the four modules trained for 250 steps, and (e), (f), (g)\nand (h) in the second row are trained for 2,000 steps.\nrespectively, both of which are on par with “W2/4-E8-A8”\nquantized Q-BERT. In terms of the “W2-E2-A8” quantized\nmodels, our MREM-S and MREM-P surpass GOBO by\n11.7% ↑and 11.3% ↑on MNLI-m respectively.\n5.4 Discussions\nIn this section, we provide further discussions to better un-\nderstand the proposed approach. By default, all experiments\nin this section are based on the BERT-base model over the\nMNLI dataset.\n5.4.1 Teacher Forcing\nWe now study how teacher forcing beneﬁts MREM-P with\ndifferent numbers of training steps, and results are listed\nin Table 5. It can be found that teacher forcing can bring\nconsistent improvement for both BERT-base and BERT-large\nmodels. Moreover, the gain of teacher forcing is more sig-\nniﬁcant with fewer training steps or lower quantization bit-\nwidth, i.e., 3.4% ↑and 2.8% ↑on the “W2-E2-A4” quantized\nBERT-base and BERT-large respectively under 250 steps.\nThis matches our intuition that fewer training steps or\nhigher compression ratio give larger reconstruction error,\nwhen the clean input from the full-precision module can\nbeneﬁt more the quantized module. As further increasing\nthe training steps brings only marginal improvement and\ndiminishes the effect of teacher forcing, we by default set\nthe training steps to 2,000.\nAdditionally, we also plot training loss curves of the four\nmodules under 250 and 2,000 training steps in Figure 4.\nWe ﬁnd that: 1) the loss curves with teacher forcing are\napparently lower, especially when trained with fewer steps,\nwhich matches the observations in Table 5; 2) the loss curves\nof late modules are usually lower than the earlier ones,\nindicating that modules closer to the output beneﬁt more\nfrom teacher forcing. This matches the intuition that the\nlate modules have more errors accumulated to correct. The\nred areas in Figure 4 show that 40% of the total iterations\nare used for teacher forcing. We also try tuning it within\n[20%,80%], and does not observe large difference in the ﬁnal\nperformance. We thus choose 40% by default.\n5.4.2 Further Comparison with REM\nHere we provide further discussions with REM on the\ntraining efﬁciency. Note that both REM and MREM-S follow\nthe sequential training procedure, where the output from\nthe previous objective is cached for the next objective.\nHowever, as there are many matrix multiplications in each\nTransformer layer, it can be time-consuming for REM to\n10\n(a) Number of Modules and Mem-\nory Overhead.\n(b) Size of Calibration Data.\n (c) Error Propagation (A8).\n (d) Error Propagation (A4).\nFigure 5: Discussions on the proposed MREM approach. In (a) and (b), the solid line and shaded area denote the averaged\nresults and standard deviation of a “W2-E2-A4” quantized BERT-base model from 10 different seeds. (c) and (d) visualize\nthe propagation of reconstruction error on “W2-E2-A8” and “W2-E2-A4” quantized BERT-base model, respectively.\nrepeat this procedure recursively. According to results in\nSection 5.2, while REM and MREM take roughly the same\namount of time, REM is only iterated for 250 steps on MNLI\nand 500 steps on SQuAD, while MREM takes 2,000 steps\nand 4,000 steps respectively.\nWe also provide results when REM takes the same\namount of training steps with MREM-S in Table 6. It can be\nfound that even with 2,000 iterations, REM is still inferior\nto MREM-S across all quantization bit-widths. Meanwhile,\nREM nearly takes around 9× more training time than\nMREM. Therefore, the module-wise granularity in MREM\nnot only improves the quantization performance with more\nlayer-wise dependencies considered, but also makes the\ntraining pipeline efﬁcient with fewer stages to cache inter-\nmediate results.\n5.4.3 Number of Modules and Memory Overhead\nWe verify the effect of model partition on the ﬁnal quantized\nperformance, as well as their corresponding memory con-\nsumption. According to Figure 5(a), by varying the number\nof modules within {1,2,3,4,6}, it can be found that fewer\nmodel partitions give slightly better performance, as layer-\nwise dependencies can be better incorporated for recon-\nstruction error minimization. However, this also comes with\nmore running memory, i.e., {11.9,6.7,4.7,3.7,2.7}GB for\nthese partitions correspondingly. The decrease of memory\nalso diminishes with fewer partitions. Therefore. as a trade-\noff, we partition the model into 4 modules by default.\n5.4.4 Size of Calibration Data\nThe size of calibration data directly relates to the security\nand privacy issues in post-training quantization. To learn\nits effects, we vary the calibration data size |˜D| within\n{32,64,128,512,1024,2048,4096,8192}, and list the results\nof REM, MREM-S and MREM-P . From Figure 5(b), it can be\nfound that while REM is ahead of MREM-S/P with fewer\nthan 128 training samples, the accuracy of REM rises slowly\nand saturates at around 60% afterwards. We hypothesize\nthat the simple training objective in REM can hardly hold\nmore training instances for optimization. MREM-S/P , on the\nother hand, can better exploit larger calibration data size,\nsince the module-wise granularity admits higher ﬂexibility\nfor the optimization. As we ﬁnd the diminishing gain to\n#Bits\n(W-E-A) Methods\nw/o PCQ w/ PCQ\nAcc\nm(%)\nAcc\nmm(%)\nAcc\nm(%)\nAcc\nmm(%)\n4-4-8 REM 73.3±0.3 74.9±0.2 75.9±0.3 77.4±0.2\nMREM 83.5±0.1 83.9±0.2 83.6±0.1 84.0±0.1\n2-2-8 REM 71.6±0.4 73.4±0.4 74.1±0.5 75.6±0.5\nMREM 82.7±0.2 82.7±0.2 82.8±0.1 82.9±0.1\n2-2-4 REM 58.3±0.5 60.6±0.6 59.3±0.4 62.0±0.4\nMREM 81.1±0.2 81.5±0.2 81.1±0.2 81.5±0.3\nTable 7: Comparison of BERT-base results with and without\nper-channel quantization (PCQ) on MNLI.\nincrease the training size after 4,096 samples, we by default\ntake 4,096 samples.\n5.4.5 Reconstruction Error Propagation\nWe visualize the propagation of reconstruction error for\nboth “W2-E2-A8” and “W2-E2-A4” quantized BERT-base\nmodels in Figure 5(c) and Figure 5(d) respectively. It can be\nobserved that our MREM achieves both lower values and\nslower rising rates of the reconstruction error than REM\nacross all layers, which veriﬁes the advantage of module-\nwise granularity to minimize the reconstruction error. In-\nterestingly, while the reconstruction error generally gets\nenlarged layer-wisely in the ﬁrst ten layers, it begins to\ndecrease afterwards. We speculate this is due to the effect of\nthe classiﬁcation head that encourages concentrated hidden\nrepresentations for the task.\n5.4.6 Per-channel Quantization\nPer-channel Quantization (PCQ) is prevalent in the post-\ntraining quantization of convolution neural networks [22],\n[23], [35]. To learn its effect in PLMs, PCQ assigns different\nquantization step-sizes at each output dimension of the\nlinear layer, which is also known as row-wise quantization\nin [19]. The PCQ results of REM and MREM are shown in\nTable 7. It can be found that while PCQ improves REM\nby around 1.0% to 2.5%, the gain is very incremental on\nMREM. We hypothesize that more training steps of MREM\ncan better adjust the quantization distribution for PLMs.\nOur results are also similar to the ﬁndings in [19], where the\nrow-wise quantization brings little improvement. As PCQ\nalso requires to store more full-precision step sizes, we do\nnot employ PCQ by default.\n11\n6 C ONCLUSION\nIn this paper, we study post-training quantization for\npre-trained language models. We show that existing\nquantization-aware training solutions suffer from slow\ntraining, huge memory overhead, and data privacy issues\nwhen accessing the full training set. To mitigate these issues,\nwe propose module-wise reconstruction error minimization,\nan efﬁcient solution to quantize PLMs. MREM can be con-\nducted either sequentially or in parallel, where the parallel\ntraining can achieve the speedup close to the theoretical\nlimit without apparent performance degradation. Exper-\nimental results show that the proposed solution greatly\nimproves the performance. Meanwhile, it signiﬁcantly re-\nduces the training time and memory overhead with only\nthousands of training instances.\nThere are several promising directions to explore in the\nfuture: 1) We can scale the proposed approach to larger\nPLMs, and thus more models can beneﬁt from post-training\nquantization; 2) The proposed parallel strategy can be ap-\nplied to warm up the pre-training of PLMs, such that\nthe overall pre-training cost can be reduced; 3) While the\ncurrent parallel strategy conducts local training separately,\nit would be interesting to cache module-wise gradients in\nsome queues for the backward pass so that there is less\ndiscrepancy with conventional end-to-end training.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems, 2017, pp. 5998–\n6008.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in North American Chapter of the Association for Computa-\ntional Linguistics, 2019.\n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dhari-\nwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , “Lan-\nguage models are few-shot learners,” in Advances in Neural Infor-\nmation Processing Systems, 2020.\n[4] P . Michel, O. Levy, and G. Neubig, “Are sixteen heads really better\nthan one?” in Advances in Neural Information Processing Systems ,\n2019.\n[5] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on\ndemand with structured dropout,” in International Conference on\nLearning Representations, 2019.\n[6] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” Preprint\narXiv:1910.01108, 2019.\n[7] S. Sun, Y. Cheng, Z. Gan, and J. Liu, “Patient knowledge dis-\ntillation for bert model compression,” in Conference on Empirical\nMethods in Natural Language Processing, 2019.\n[8] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and\nQ. Liu, “Tinybert: Distilling bert for natural language under-\nstanding,” in Findings of Empirical Methods in Natural Language\nProcessing, 2020.\n[9] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser,\n“Universal transformers,” in International Conference on Learning\nRepresentations, 2019.\n[10] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Sori-\ncut, “Albert: A lite bert for self-supervised learning of language\nrepresentations,” in International Conference on Learning Representa-\ntions, 2020.\n[11] J. Wang, H. Bai, J. Wu, X. Shi, J. Huang, I. King, M. Lyu, and\nJ. Cheng, “Revisiting parameter sharing for automatic neural\nchannel number search,” in Advances in Neural Information Pro-\ncessing Systems, vol. 33, 2020.\n[12] Z. Huang, L. Hou, L. Shang, X. Jiang, X. Chen, and Q. Liu, “Ghost-\nbert: Generate more features with cheap operations for bert,” in\nAnnual Meeting of the Association for Computational Linguistics, 2021.\n[13] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu,\n“Dynabert: Dynamic bert with adaptive width and depth,” in\nAdvances in Neural Information Processing Systems , 2020.\n[14] J. Xin, R. Tang, J. Lee, Y. Yu, and J. Lin, “Deebert: Dynamic early\nexiting for accelerating bert inference,” in Annual Meeting of the\nAssociation for Computational Linguistics, 2020.\n[15] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, “Bert loses\npatience: Fast and robust inference with early exit,” in Advances in\nNeural Information Processing Systems, 2020.\n[16] O. Zafrir, G. Boudoukh, P . Izsak, and M. Wasserblat, “Q8bert:\nQuantized 8bit bert,” Preprint arXiv:1910.06188, 2019.\n[17] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney,\nand K. Keutzer, “Q-bert: Hessian based ultra low precision quan-\ntization of bert,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 2020.\n[18] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, “Gobo:\nQuantizing attention-based nlp models for low latency and energy\nefﬁcient inference,” Preprint arXiv:2005.03842, 2020.\n[19] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu,\n“Ternarybert: Distillation-aware ultra-low bit bert,” in Conference\non Empirical Methods in Natural Language Processing , 2020.\n[20] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu,\nand I. King, “Binarybert: Pushing the limit of bert quantization,”\nin Annual Meeting of the Association for Computational Linguistics ,\n2021.\n[21] M. Nagel, M. v. Baalen, T. Blankevoort, and M. Welling, “Data-free\nquantization through weight equalization and bias correction,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 1325–1334.\n[22] Y. Nahshan, B. Chmiel, C. Baskin, E. Zheltonozhskii, R. Banner,\nA. M. Bronstein, and A. Mendelson, “Loss aware post-training\nquantization,” Preprint arXiv:1911.07190, 2019.\n[23] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and\nT. Blankevoort, “Up or down? adaptive rounding for post-training\nquantization,” in Proceedings of the International Conference on Ma-\nchine Learning, 2020, pp. 7197–7206.\n[24] Y. Li, R. Gong, X. Tan, Y. Yang, P . Hu, Q. Zhang, F. Yu, W. Wang,\nand S. Gu, “Brecq: Pushing the limit of post-training quantization\nby block reconstruction,” in International Conference on Learning\nRepresentations, 2021.\n[25] R. J. Williams and D. Zipser, “A learning algorithm for continu-\nally running fully recurrent neural networks,” Neural computation,\nvol. 1, no. 2, pp. 270–280, 1989.\n[26] M. Courbariaux, Y. Bengio, and J.-P . David, “Binaryconnect: Train-\ning deep neural networks with binary weights during propaga-\ntions,” in Advances in neural information processing systems , 2015.\n[27] F. Li, B. Zhang, and B. Liu, “Ternary weight networks,” Preprint\narXiv:1605.04711, 2016.\n[28] L. Hou, Q. Yao, and J. T. Kwok, “Loss-aware binarization of deep\nnetworks,” in International Conference on Learning Representations ,\n2017.\n[29] L. Hou and J. T. Kwok, “Loss-aware weight quantization of deep\nnetworks,” in International Conference on Learning Representations ,\n2018.\n[30] S. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and\nD. S. Modha, “Learned step size quantization,” in International\nConference on Learning Representations, 2019.\n[31] Y. Li, X. Dong, S. Q. Zhang, H. Bai, Y. Chen, and W. Wang, “Rtn:\nReparameterized ternary network,” in Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , vol. 34, no. 04, 2020, pp. 4780–\n4787.\n[32] B. Zhuang, M. Tan, J. Liu, L. Liu, I. Reid, and C. Shen, “Effec-\ntive training of convolutional neural networks with low-bitwidth\nweights and activations,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2021.\n[33] S. Young, Z. Wang, D. Taubman, and B. Girod, “Transform quanti-\nzation for cnn compression,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2021.\n[34] R. Zhao, Y. Hu, J. Dotzel, C. De Sa, and Z. Zhang, “Improving neu-\nral network quantization without retraining using outlier channel\nsplitting,” in Proceedings of the International Conference on Machine\nLearning, 2019.\n[35] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry,\n“Improving post training neural quantization: Layer-wise calibra-\ntion and integer programming,” in Proceedings of the International\nConference on Machine Learning, 2021.\n12\n[36] Y. He, X. Zhang, and J. Sun, “Channel pruning for accelerating\nvery deep neural networks,” in Proceedings of the IEEE International\nConference on Computer Vision, 2017, pp. 1398–1406.\n[37] J.-H. Luo, H. Zhang, H.-Y. Zhou, C.-W. Xie, J. Wu, and W. Lin,\n“Thinet: pruning cnn ﬁlters for a thinner net,” IEEE transactions on\npattern analysis and machine intelligence , vol. 41, no. 10, pp. 2525–\n2538, 2018.\n[38] L. Wen, X. Zhang, H. Bai, and Z. Xu, “Structured pruning of recur-\nrent neural networks through neuron selection,” Neural Networks,\npp. 134–141, 2020.\n[39] J. Wang, H. Bai, J. Wu, and J. Cheng, “Bayesian automatic model\ncompression,” IEEE Journal of Selected Topics in Signal Processing ,\nvol. 14, no. 4, pp. 727–736, 2020.\n[40] J. Liu, B. Zhuang, Z. Zhuang, Y. Guo, J. Huang, J. Zhu, and\nM. Tan, “Discrimination-aware network pruning for deep model\ncompression,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2021.\n[41] M. A. Gordon, K. Duh, and N. Andrews, “Compressing bert:\nStudying the effects of weight pruning on transfer learning,”\nPreprint arXiv:2002.08307, 2020.\n[42] W. Wang and Z. Tu, “Rethinking the value of transformer com-\nponents,” in Proceedings of the International Conference on Computa-\ntional Linguistics, 2020.\n[43] J. McCarley, R. Chakravarti, and A. Sil, “Structured pruning of a\nbert-based question answering model,” Preprint arXiv:1910.06360,\n2019.\n[44] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” Preprint arXiv:1503.02531, 2015.\n[45] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY. Bengio, “Fitnets: Hints for thin deep nets,” in International\nConference on Learning Representations, 2015.\n[46] G.-H. Wang, Y. Ge, and J. Wu, “Distilling knowledge by mimick-\ning features,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2021.\n[47] L. Zhang, C. Bao, and K. Ma, “Self-distillation: Towards efﬁcient\nand compact neural networks,” IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 2021.\n[48] Y. Liu, C. Shu, J. Wang, and C. Shen, “Structured knowledge dis-\ntillation for dense prediction,” IEEE transactions on pattern analysis\nand machine intelligence, 2020.\n[49] D. Chen, Y. Li, M. Qiu, Z. Wang, B. Li, B. Ding, H. Deng, J. Huang,\nW. Lin, and J. Zhou, “Adabert: Task-adaptive bert compression\nwith differentiable neural architecture search,” inInternational joint\nconference on artiﬁcial intelligence, 2020.\n[50] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable archi-\ntecture search,” in Proceedings of the International Conference of\nRepresentation Learning, 2019.\n[51] D. So, Q. Le, and C. Liang, “The evolved transformer,” in Proceed-\nings of the International Conference on Machine Learning . PMLR,\n2019, pp. 5877–5886.\n[52] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T.-Y. Liu,\n“Nas-bert: Task-agnostic and adaptive-size bert compression with\nneural architecture search,” in Proceedings of the ACM SIGKDD\ninternational conference on knowledge discovery and data mining, 2021.\n[53] Y. Yin, C. Chen, L. Shang, X. Jiang, X. Chen, and Q. Liu, “Au-\ntotinybert: Automatic hyper-parameter optimization for efﬁcient\npre-trained language models,” in Annual Meeting of the Association\nfor Computational Linguistics, 2021.\n[54] G. Bender, “Understanding and simplifying one-shot architecture\nsearch,” in Proceedings of the Proceedings of the International Confer-\nence on Machine Learning, 2019, pp. 549–558.\n[55] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen,\nH. Lee, J. Ngiam, Q. V . Le, Y. Wu et al., “Gpipe: Efﬁcient training\nof giant neural networks using pipeline parallelism,” in Advances\nin neural information processing systems, 2018.\n[56] D. Narayanan, A. Harlap, A. Phanishayee, V . Seshadri, N. R. De-\nvanur, G. R. Ganger, P . B. Gibbons, and M. Zaharia, “Pipedream:\ngeneralized pipeline parallelism for dnn training,” in Proceedings\nof the 27th ACM Symposium on Operating Systems Principles , 2019,\npp. 1–15.\n[57] J. Tarnawski, A. Phanishayee, N. R. Devanur, D. Mahajan, and\nF. N. Paravecino, “Efﬁcient algorithms for device placement of\ndnn graph operators,” in Advances in Neural Information Processing\nSystems, 2020.\n[58] J. H. Park, G. Yun, M. Y. Chang, N. T. Nguyen, S. Lee, J. Choi,\nS. H. Noh, and Y.-r. Choi, “Hetpipe: Enabling large {DNN}\ntraining on (whimpy) heterogeneous {GPU} clusters through\nintegration of pipelined model parallelism and data parallelism,”\nin 2020 {USENIX}Annual Technical Conference ( {USENIX}{ATC}\n20), 2020, pp. 307–321.\n[59] S. Fan, Y. Rong, C. Meng, Z. Cao, S. Wang, Z. Zheng, C. Wu,\nG. Long, J. Yang, L. Xia et al., “Dapple: A pipelined data parallel\napproach for training large models,” in Proceedings of the 26th\nACM SIGPLAN Symposium on Principles and Practice of Parallel\nProgramming, 2021, pp. 431–445.\n[60] J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing\non large clusters,” in Communications of the ACM , vol. 51, no. 1,\n2008, pp. 107–113.\n[61] M. Li, D. G. Andersen, A. J. Smola, and K. Yu, “Communication\nefﬁcient distributed machine learning with the parameter server,”\nin Advances in Neural Information Processing Systems , vol. 27, 2014,\npp. 19–27.\n[62] Z. Jia, S. Lin, C. R. Qi, and A. Aiken, “Exploring hidden dimen-\nsions in accelerating convolutional neural networks,” in Interna-\ntional Conference on Machine Learning, 2018, pp. 2274–2283.\n[63] M. Shoeybi, M. Patwary, R. Puri, P . LeGresley, J. Casper,\nand B. Catanzaro, “Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism,” Preprint\narXiv:1909.08053, 2019.\n[64] L. Song, F. Chen, Y. Zhuo, X. Qian, H. Li, and Y. Chen, “Accpar:\nTensor partitioning for heterogeneous deep learning accelerators,”\nin IEEE International Symposium on High Performance Computer\nArchitecture (HPCA), 2020, pp. 342–355.\n[65] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory\noptimizations toward training trillion parameter models,” inSC20:\nInternational Conference for High Performance Computing, Networking,\nStorage and Analysis. IEEE, 2020, pp. 1–16.\n[66] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,\nM. Chen, and I. Sutskever, “Zero-shot text-to-image generation,”\nPreprint arXiv:2102.12092, 2021.\n[67] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang,\nZ. Yang, K. Wang, X. Zhang et al., “Pangu-α: Large-scale autore-\ngressive pretrained chinese language models with auto-parallel\ncomputation,” Preprint arXiv:2104.12369, 2021.\n[68] J. Fang, A. Shaﬁee, H. Abdel-Aziz, D. Thorsley, G. Georgiadis,\nand J. H. Hassoun, “Post-training piecewise linear quantization for\ndeep neural networks,” in European Conference on Computer Vision,\n2020, pp. 69–86.\n[69] P . Wang, Q. Chen, X. He, and J. Cheng, “Towards accurate post-\ntraining network quantization via bit-split and stitching,” in Pro-\nceedings of the International Conference on Machine Learning , 2020,\npp. 9847–9856.\n[70] D. Zhou, M. Ye, C. Chen, T. Meng, M. Tan, X. Song, Q. Le, Q. Liu,\nand D. Schuurmans, “Go wide, then narrow: Efﬁcient training of\ndeep thin networks,” in Proceedings of the International Conference\non Machine Learning, 2020, pp. 11 546–11 555.\n[71] S. Chen, W. Wang, and S. J. Pan, “Deep neural network quanti-\nzation via layer-wise optimization using limited training data,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence, 2019, pp.\n3329–3336.\n[72] H. Bai, J. Wu, I. King, and M. Lyu, “Few shot network compression\nvia cross distillation,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 3203–3210.\n[73] Q. Ho, J. Cipar, H. Cui, J. K. Kim, S. Lee, P . B. Gibbons, G. A.\nGibson, G. R. Ganger, and E. P . Xing, “More effective distributed\nml via a stale synchronous parallel parameter server,” in Advances\nin Neural Information Processing Systems, 2013, p. 1223.\n[74] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-\ntion,” in International Conference on Learning Representations, 2018.\n[75] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” Preprint arXiv:1804.07461, 2018.\n[76] P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang, “Squad:\n100,000+ questions for machine comprehension of text,” Preprint\narXiv:1606.05250, 2016.\n[77] A. Fan, P . Stock, B. Graham, E. Grave, R. Gribonval, H. Jegou,\nand A. Joulin, “Training with quantization noise for extreme ﬁxed-\npoint compression,” Preprint arXiv:2004.07320, 2020.",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.8750070333480835
    },
    {
      "name": "Computer science",
      "score": 0.7533174753189087
    },
    {
      "name": "Minification",
      "score": 0.5383326411247253
    },
    {
      "name": "Computer engineering",
      "score": 0.45253828167915344
    },
    {
      "name": "Training set",
      "score": 0.44278663396835327
    },
    {
      "name": "Language model",
      "score": 0.429929256439209
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.4272858500480652
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3333355784416199
    },
    {
      "name": "Algorithm",
      "score": 0.3301585912704468
    },
    {
      "name": "Programming language",
      "score": 0.10516247153282166
    }
  ],
  "institutions": [],
  "cited_by": 21
}