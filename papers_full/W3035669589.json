{
  "title": "Multiplex Behavioral Relation Learning for Recommendation via Memory Augmented Transformer Network",
  "url": "https://openalex.org/W3035669589",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3034975406",
      "name": "Lianghao Xia",
      "affiliations": [
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2101072704",
      "name": "Chao Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096026659",
      "name": "Yong Xu",
      "affiliations": [
        "Peng Cheng Laboratory",
        "South China University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2014895805",
      "name": "Peng Dai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097431157",
      "name": "Bo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2278593545",
      "name": "Liefeng Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911575572",
    "https://openalex.org/W2945266622",
    "https://openalex.org/W2907827821",
    "https://openalex.org/W2783944588",
    "https://openalex.org/W2914721378",
    "https://openalex.org/W2951369132",
    "https://openalex.org/W2951045934",
    "https://openalex.org/W2951570486",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W2026773017",
    "https://openalex.org/W2912057614",
    "https://openalex.org/W2137245235",
    "https://openalex.org/W2907754626",
    "https://openalex.org/W1720514416",
    "https://openalex.org/W2908404712",
    "https://openalex.org/W2471920251",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W2783272285",
    "https://openalex.org/W6629340653",
    "https://openalex.org/W2963911286",
    "https://openalex.org/W2997878072",
    "https://openalex.org/W2984582982",
    "https://openalex.org/W2945623882",
    "https://openalex.org/W2945827670",
    "https://openalex.org/W2253995343",
    "https://openalex.org/W2962992837",
    "https://openalex.org/W2740920897",
    "https://openalex.org/W2947766753",
    "https://openalex.org/W2953968544",
    "https://openalex.org/W2575006718",
    "https://openalex.org/W2472158773",
    "https://openalex.org/W2723293840"
  ],
  "abstract": "Capturing users' precise preferences is of great importance in various\\nrecommender systems (eg., e-commerce platforms), which is the basis of how to\\npresent personalized interesting product lists to individual users. In spite of\\nsignificant progress has been made to consider relations between users and\\nitems, most of the existing recommendation techniques solely focus on singular\\ntype of user-item interactions. However, user-item interactive behavior is\\noften exhibited with multi-type (e.g., page view, add-to-favorite and purchase)\\nand inter-dependent in nature. The overlook of multiplex behavior relations can\\nhardly recognize the multi-modal contextual signals across different types of\\ninteractions, which limit the feasibility of current recommendation methods. To\\ntackle the above challenge, this work proposes a Memory-Augmented Transformer\\nNetworks (MATN), to enable the recommendation with multiplex behavioral\\nrelational information, and joint modeling of type-specific behavioral context\\nand type-wise behavior inter-dependencies, in a fully automatic manner. In our\\nMATN framework, we first develop a transformer-based multi-behavior relation\\nencoder, to make the learned interaction representations be reflective of the\\ncross-type behavior relations. Furthermore, a memory attention network is\\nproposed to supercharge MATN capturing the contextual signals of different\\ntypes of behavior into the category-specific latent embedding space. Finally, a\\ncross-behavior aggregation component is introduced to promote the comprehensive\\ncollaboration across type-aware interaction behavior representations, and\\ndiscriminate their inherent contributions in assisting recommendations.\\nExtensive experiments on two benchmark datasets and a real-world e-commence\\nuser behavior data demonstrate significant improvements obtained by MATN over\\nbaselines. Codes are available at: https://github.com/akaxlh/MATN.\\n",
  "full_text": "Multiplex Behavioral Relation Learning for Recommendation\nvia Memory Augmented Transformer Network\nLianghao Xiaâˆ—\nSouth China University of Technology\ncslianghao.xia@mail.scut.edu.cn\nChao Huangâˆ—\nJD Finance America Corporation\nchaohuang75@gmail.com\nYong Xuâ€ \nSouth China University of Technology\nPeng Cheng Laboratory\nyxu@scut.edu.cn\nPeng Dai\nJD Finance America Corporation\npeng.dai@jd.com\nBo Zhang\nBoshi Qiangzhi Science and\nTechnology Co., Ltd\n13922820911@139.com\nLiefeng Bo\nJD Finance America Corporation\nliefeng.bo@jd.com\nABSTRACT\nCapturing usersâ€™ precise preferences is of great importance in vari-\nous recommender systems (e.g., e-commerce platforms and online\nadvertising sites), which is the basis of how to present personalized\ninteresting product lists to individual users. In spite of significant\nprogress has been made to consider relations between users and\nitems, most of existing recommendation techniques solely focus\non singular type of user-item interactions. However, user-item in-\nteractive behavior is often exhibited with multi-type ( e.g., page\nview, add-to-favorite and purchase) and inter-dependent in nature.\nThe overlook of multiplex behavior relations can hardly recog-\nnize the multi-modal contextual signals across different types of\ninteractions, which limit the feasibility of current recommenda-\ntion methods. To tackle the above challenge, this work proposes\na Memory-Augmented Transformer Networks (MATN), to enable\nthe recommendation with multiplex behavioral relational informa-\ntion, and joint modeling of type-specific behavioral context and\ntype-wise behavior inter-dependencies, in a fully automatic man-\nner. In our MATN framework, we first develop a transformer-based\nmulti-behavior relation encoder, to make the learned interaction\nrepresentations be reflective of the cross-type behavior relations.\nFurthermore, a memory attention network is proposed to super-\ncharge MATN capturing the contextual signals of different types of\nbehavior into the category-specific latent embedding space. Finally,\na cross-behavior aggregation component is introduced to promote\nthe comprehensive collaboration across type-aware interaction\nbehavior representations, and discriminate their inherent contribu-\ntions in assisting recommendations. Extensive experiments on two\nbenchmark datasets and a real-world e-commence user behavior\nâˆ—These authors contributed equally to this work.\nâ€ Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR â€™20, July 25â€“30, 2020, Virtual Event, China\nÂ© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\nhttps://doi.org/10.1145/3397271.3401445\ndata demonstrate significant improvements obtained by MATN over\nbaselines. Codes are available at: https://github.com/akaxlh/MATN.\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\nCollaborative Filtering; Recommendation; Multi-Behavior Learning;\nTransformer Network; Deep Neural Networks\nACM Reference Format:\nLianghao Xia, Chao Huang, Yong Xu, Peng Dai, Bo Zhang, and Liefeng\nBo. 2020. Multiplex Behavioral Relation Learning for Recommendation via\nMemory Augmented Transformer Network. In Proceedings of the 43rd Inter-\nnational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™20), July 25â€“30, 2020, Virtual Event, China. ACM, Xiâ€™an,\nChina, 10 pages. https://doi.org/10.1145/3397271.3401445\n1 INTRODUCTION\nRecommender system, which facilitates the information-seeking\nprocess of users and meet their personalized interests, have played\na critical role in various online services, such as e-commerce sys-\ntems [13, 38], online review platforms [1, 44] and advertising [39].\nAt its core is to learn low-dimensional representations of user-item\ninteraction while capturing the user preference and the underly-\ning intrinsic characteristics [19]. Early methods towards this goal,\nhave made significant efforts on transforming user-item interac-\ntions through vectorized representations based on the conventional\nCollaborative Filtering (CF) techniques (e.g., matrix factorization\nscheme [15, 22] and its variations [12, 26]).\nInspired by the advancement of deep learning techniques, vari-\nous neural network-based collaborative filtering frameworks have\nbeen developed to model the relationships between users and\nitems. These methods aim to map sparse input interaction fea-\ntures into low-dimensional user/item embedding vectors and then\nproject them into fixed-length representations in a group-wise man-\nner [16, 54]. For example, neural collaborative filtering models re-\nplace the inner product function in the matrix factorization consider\nnon-linearities with multilayer perceptron [11] and metric learning\nscheme [33]. In addition, auto-encoder architecture has served as an\neffective solution to learn a mapping function between the explicit\narXiv:2110.04002v1  [cs.IR]  8 Oct 2021\ninteraction and latent representation through the reconstruction-\nbased encoder-decoder framework. To capture the rich graph-based\nneighborhood contextual signals, various graph neural encoders\nhave been proposed to aggregate information over the user-item\ninteraction graph, with the graph convolutional network [49] or\nmessage-passing mechanism [41].\nDespite the prevalence of the above recommendation solutions,\nthey has thus far focused on user preference representation learn-\ning with the consideration of singular type of user-item interactive\nbehavior. However, in many practical recommendation scenarios,\nuser-item interactions are multiplex and exhibited with relation-\nship diversity in nature. Letâ€™s consider the e-commerce system\nas an example, there exist multiple types of behavior ( e.g., page\nview, add-to-favourite, add-to-cart and purchase) between users\nand items [8], which are mutually inter-dependent. For instance,\nadd-to-cart behavior is more likely to co-occur with purchase than\nthe add-to-favorite behavior. The page view and add-to-favourite\nbehavior can also provide useful signals for making purchase deci-\nsions. In such cases, the ignorance of such multi-modal relations\nacross different types of user-item interaction behavior, makes\nexisting recommendation methods insufficient to distill effective\ncollaborative signals from the collective users behavior.\nThe recommendation framework with multiplex interactive be-\nhavior pose two key challenges: First, the dependencies across\ndifferent types of user-item interactions can be arbitrary since any\npair of type-specific behavior could potentially be correlated due\nto various factors [42]. For example, users often have correlated\nonline behaviors and exhibit different dependencies in choosing\nitems of different categories due to his/her specialty. Such inter-\ndependencies between different types of interaction behavior may\nvary by users and items. While a handful of studies attempt to learn\nuser preference from multi-behavior [7, 8], they merely consider the\nsingular dimensional cascading correlations between multi-type\ninteractions, and cannot comprehensively capture the arbitrary\ndependencies between different types of interaction over different\nitems. Hence, to build effective recommendation model with the\ncomplex behavior dependencies remains a significant challenge.\nSecond, when modeling the relationships across different types\nof behavior, it is also important to capture the context and seman-\ntics of individual type of user-item interactions, e.g., usersâ€™ page\nview are more frequent than their purchases, and add-to-favorite\nbehavior is more likely to happen over usersâ€™ interested items but\nmay postpone their buying decision. In addition, type-specific be-\nhavioral patterns interweave with each other in complex way (e.g.,\nsupport or mutually exclusive relations) and are difficult to be cap-\ntured. During the behavioral pattern integration, as the importance\nof various types of behavior can be different, their relevance in\nassisting the forecasting task on the target behavior need to be\ncarefully decided.\nMotivated by the aforementioned challenges, this work proposes\na general and flexible multi-behavior relation learning frameworkâ€“\nMemory-Augmented Transformer Networks (MATN). Specifically,\nthis work first proposes a multi-behavior transformer network to\nlearn type-specific behavioral representations with the incorpora-\ntion of inter-dependencies across different types of user-item inter-\nactions. By integrating the transformer network with a memory-\naugmented attention mechanism, we endow the MATN framework\nwith the capability of incorporating type-specific behavior con-\ntextual signals. to collectively model the implicit relevance across\nmulti-type behavioral patterns and perform comprehensive learn-\ning for making recommendations, we design a behavior type-wise\ngating mechanism which promotes the collaboration of different\ntypes of interactions. In the pattern aggregation layer, MATN could\nlearn cross-type representations in the latent feature spaces by au-\ntomatically adjusting the contribution of each behavior view point\nin the behavior predictive model.\nThe contributions of this paper are highlighted as follows:\nâ€¢We propose MATN, a new recommendation framework with\nmultiplex behavioral relation learning. MATN explicitly encodes\nmulti-behavior relational structures by preserving both the cross-\ntype behavior collaborative signals and type-specific behavior\ncontextual information.\nâ€¢We first develop a multi-behavior dependency encoder with a\ntransformer architecture, to inject collaborative signals across\ndifferent types of user-item interactions into the embedding pro-\ncess. Furthermore, we augment the multi-behavior transformer\nnetwork with a memory attention mechanism, which is capable\nof uncovering type-specific behavior semantics during the cus-\ntomized representation recalibration phase.\nâ€¢Finally, a type-wise pattern aggregation layer with gating mech-\nanism is developed to promote the collaboration of different\nbehavior views for robust representations on user preferences.\nâ€¢Our extensive experiments on two benchmark datasets and a user\nbehavior data from a major e-commence platform, demonstrate\nthat MATN outperforms 12 baselines from various research lines\nin yielding better recommendation performance. We further per-\nform case studies with qualitative examples to better understand\nthe interpretation ability of MATN framework, and study the\nmodel efficiency under different recommendation scenarios.\n2 PRELIMINARY\nIn the recommendation scenario, we first define the behavior (e.g.,\npurchase) which we aim to predict as target behavior , other rel-\nevant user-item interactive behavior ( e.g., click, add-to-cart and\nadd-to-favorite) is termed as source behavior . In this work, we aim\nto explore the latent relational structures between different types\nof user behavior (e.g., purchases and click) for making predictions\non the target behavior of users in recommender systems.\nDefinition 1. Multi-Behavior TensorX. We define a three-\ndimensional multi-behavior tensor Xâˆˆ Rğ¼Ã—ğ½Ã—ğ¿ to represent the ğ¿\n(indexed by ğ‘™) types of behavior from ğ¼ (indexed by ğ‘–) users over ğ½ (in-\ndexed by ğ‘—) items. Without loss of generality, we focus on the implicit\nuser feedback which is more common in practical recommendation\nscenarios [ 25, 40]. Particularly, in tensor X, each entry ğ‘¥ğ‘–,ğ‘—,ğ‘™ = 1 if\nuser ğ‘¢ğ‘– is interacted with item ğ‘¡ğ‘— given the ğ‘™-th behavior type. For\nexample, if user ğ‘¢ğ‘– purchases item ğ‘¡ğ‘—, the corresponding element ğ‘¥ğ‘–,ğ‘—,ğ‘™\nwill be set as 1 in the purchase behavior matrix Xğ‘™.\nProblem Statement. Based on the aforementioned definitions, the\nrecommendation task with multiplex behavior learning is formu-\nlated as follows: Input: the user-item interaction data represented\nwith multi-behavior tensor X(including both the target and source\nbehavior). Output: A predictive model to effectively infer the un-\nknown user-item interactions in Xwith the target behavior ğ‘™.\nPr(ğ‘¥ğ‘–,ğ‘—,ğ‘™ = 1)= ğ‘“(Xâˆˆ Rğ¼Ã—ğ½Ã—ğ¿)ğ‘– âˆˆ[1,...,ğ¼ ]; ğ‘— âˆˆ[1,...,ğ½ ] (1)\n3 METHODOLOGY\nIn this section, we present the technical details of MATN frame-\nwork, the architecture of which is illustrated in Figure 1. MATN is a\nhierarchical neural architecture with three key modules in MATN:\n(i) cross-behavior embedding layers that learn the representations\nby exploring the inter-dependencies across different types of in-\nteractions; (ii) a customized representation recalibration network\nthat refines the latent embeddings, with the preservation of indi-\nvidual behavioral contextual information; (iii) a forecasting layer\nthat aggregates the refined behavior type-specific embeddings and\noutputs a predicted likelihood of a user-item interaction pair.\n3.1 Multi-Behavior Dependency Modeling\nAs discussed before, different types of user behaviors are correlated\nwith each other, which brings in new challenges to the recom-\nmendation framework. To model the inter-dependencies across\ndifferent types of behavior, we design a multi-behavior transformer\nnetwork to promote the collaboration of different behavioral views.\nTo achieve this goal, we learns a robust representation for user-\nitem interactive patterns of each individual categorical behavior\nğ‘™, which integrates the relevant information from other behavior\nviews ğ‘™â€²âˆˆ[1,...,ğ¿ ]&ğ‘™â€²â‰  ğ‘™.\n3.1.1 Initialized Embedding Layer. Firstly, a projection layer\nis introduced to map the original multi-behavior user-item inter-\naction data into initial latent representations. We denote the inter-\naction vector of ğ‘™-th behavior type and ğ‘–-th user (ğ‘¢ğ‘–) over all items\n(ğ‘¡ğ‘—,1 â‰¤ğ‘— â‰¤ğ½) as Xğ‘–,ğ‘™ âˆˆRğ½. The projection operation for Xğ‘–,ğ‘™ is\nformally defined as ËœXğ‘–,ğ‘™ = V Â·Xğ‘–,ğ‘™, where V âˆˆRğ‘‘Ã—ğ½ and ğ‘‘ denotes\nlearned projection matrix and hidden state dimensionality, respec-\ntively. Note that V is shared across behavior categories to model\nthe common semantics of different interactions. The projected ËœXğ‘–,ğ‘™\nserves as an initial parameterized state for user-item interactions\nXğ‘–,ğ‘™, to be optimized with the following modules.\n3.1.2 Multi-Head Self-Attentive Mechanism. Inspired by the\npromising potential of self-attention mechanism in data correlation\nlearning [50], we build our multi-behavior dependency learning\nmodule upon the architecture of multi-head self-attention network,\nwhich allows the learned behavior type-specific representations\nto interact with each other and identify the most informative cor-\nrelated signals across different types of interaction behavior. Fur-\nthermore, considering the fact that different types of interaction\nbehavior (e.g., add-to-cart and purchase) can be mutually correlated\nin a complex way (due to personalized factors) [2], the multi-head\nlearning strategy enable our behavior dependency encoder with\nthe capability of jointly attending to information from different rep-\nresentation subspaces [48]. In our transformer network, we adopts\nthe scaled dot-product attention for each â„-th head with the defini-\ntions of query, key and value transformation matrices Qâ„ âˆˆR\nğ‘‘\nğ»Ã—ğ‘‘,\nKâ„ âˆˆR\nğ‘‘\nğ»Ã—ğ‘‘ and Vâ„ âˆˆR\nğ‘‘\nğ»Ã—ğ‘‘. Then, the weight Ë†ğ›¼â„\nğ‘™,ğ‘™â€² assigned to\neach input value is determined by the dot-product of the query\nwith all the keys as follows:\nğ›¼â„\nğ‘™,ğ‘™â€² = (Qâ„ Â· ËœXğ‘–,ğ‘™)âŠ¤(Kâ„ Â· ËœXğ‘–,ğ‘™â€²)\nâˆšï¸ƒ\nğ‘‘\nğ»\n; Ë†ğ›¼â„\nğ‘™,ğ‘™â€² =\nexp ğ›¼â„\nğ‘™,ğ‘™â€²\nÃğ¿\nğ‘™â€²=1 exp Ë†ğ›¼â„\nğ‘™,ğ‘™â€²\n(2)\nwhere ğ›¼â„\nğ‘™,ğ‘™â€² is the intermediate variable fed into the softmax opera-\ntion to generate the final relevance score Ë†ğ›¼â„\nğ‘™,ğ‘™â€² between the ğ‘™-th and\nğ‘™â€²-th type of behavior. Based on the learned head-specific attention\nweights, our dependency encoding module aims to learns a cross-\nhead relevance score for each behavior type-specific representation\nËœXğ‘–,ğ‘™ with the following multi-head learning operations:\nYğ‘–,ğ‘™ = MH-Att(ËœXğ‘–,ğ‘™)=\nğ»\f\f\f\n\f\f\f\nâ„=1\nğ¿âˆ‘ï¸\nğ‘™â€²=1\nğ›¼â„\nğ‘™,ğ‘™â€²Vâ„ Â· ËœXğ‘–,ğ‘™â€² (3)\nTo alleviate the gradient vanishing issue, the residual connection [9]\nis employed in the deep neural network structures. Additionally,\nwe element-wisely add the learned dependency-aware behavior\ntype-specific interaction representation Yğ‘–,ğ‘™ with the projected fea-\nture embedding ËœXğ‘–,ğ‘™ of ğ‘™-th behavior, so as to jointly preserve the\nbehavior type-specific interaction features and the underlying inter-\ndependent signals across various types of user behavior. Formally,\nsuch operation is given as: ËœYğ‘–,ğ‘™ = ËœXğ‘–,ğ‘™ +Yğ‘–,ğ‘™.\n3.2 Customized Behavioral Context Learning\nIn addition to the implicit multi-behavior dependency encoded by\nthe above introduced transformer network, each type of behavior\nmay have its own characteristics. For instance, usersâ€™ page view\nbehavior are more frequent than their purchases and add-to-cart\nbehavior is more likely to be followed by a purchase than the add-\nto-favorite behavior. While the cross-behavior inter-correlation\nstructure can be modeled by our transformer module, the behavior\ntype-specific semantic diversity has been overlooked. Hence, we\npropose to augment our MATN framework with the capability of\ncapturing the semantic signals of each individual type of interac-\ntion behavior. Motivated by the recent advancements of augmented\nneural architecture and attention mechanism [21, 33], we perform a\ncustomized representation recalibration process on behavior type-\nspecific context with a memory-augmented attention network. In\nour memory-based behavior context learning module, we provide\na customized transformations for each type of user behavior rep-\nresentation ËœYğ‘–,ğ‘™ by stacking a set of memory blocks. By doing so,\nwe endow MATN with the power of distilling the underlying se-\nmantics from the specific contextual user-item interaction scenario\n(e.g., page view, interested in, want to buy, or purchase).\nIn specific, our customized embedding recalibration module aims\nto learn ğ‘€ (indexed by ğ‘š) transformation matrices (individual is\nreferred as Uğ‘š âˆˆRğ‘‘Ã—ğ‘‘) as the corresponding augmented mem-\nory, in order to project the general behavior embedding ËœYğ‘–,ğ‘™ into\na type-aware latent learning space. By applying different memory\ntransformations over different types of behavior, each type of be-\nhavioral features are refined with respect to its own contexts with\nthe designed memory, and a customized behavioral representations\nare generated through this type-specific transformation procedure.\nFurthermore, in order to alleviate the overfitting phenomenon of\ntype-specific memory augmented neural network architecture [53],\nPV\nCart\nFav\nBuy\nDot-Product\nAttention\nLinearLinearLinear\nLinear\nLinearLinear\nâ€¦â€¦â€¦â€¦\nConcat\nLinear\nInitialized Embedding\nLinear Projection\nTransformer-based Behavior\nDependency Modeling\nVKQVKQ\nCustomized Behavior Context Learning\nU1\nU2\nU3\nBehavior Rep\nK2\nK3\nK1\nU1\nU2\nU3\nK1\nK2\nK3\nXğœ”1\nğœ”2\nğœ”3\nU\nCustomized\nTransformation\nBehavioral\nReps\nğ‘¤1\nğ‘¤2\nğ‘¤3\nğ‘¤4\nsoftmax\nX\nMultiplex Relation Aggregation\nFeed-forward\nNetwork\nDot-Product\nAttention\nQ1\nK1\nV1\nQH\nKH\nVH\nH\nÃ—\nğ‘‘ğ‘˜\nsoftmax\nFigure 1: The model architecture of the proposed MATN framework. The initialized embedding layer shares parameters across\ndifferent behavior types. The transformer-based behavior dependency encoder takes all kinds of behavioral interaction data\nfor dependency modeling. Different types of behaviors are individually transformed by the customized context learning with\nshared key and memory slots. Ã‹ is the dot-product between the embeddings and transformation weight matrix.\nwe employ an attention network to learn the relations between ğ¿\nbehavior types and ğ‘€ memory matrices in an explicit way, and\ngenerate a behavior type-specific transformations with weighted\nsummation. Formally, the refined representation with customized\nbehavioral context for the ğ‘™-th interaction type is\nZğ‘–,ğ‘™ =\nğ‘€âˆ‘ï¸\nğ‘š=1\nğœ”ğ‘š,ğ‘™Uğ‘š Â·ËœYğ‘–,ğ‘™; ğœ”ğ‘š,ğ‘™ = ReLU(K Â·ËœYğ‘–,ğ‘™ +b)(ğ‘š) (4)\nwhere K âˆˆRğ‘€Ã—ğ‘‘,b âˆˆRğ‘€ are the transformation and bias for\ncalculating attention weights. Instead of using Softmax, we use\nReLU to relieve the gradient vanishing issue and mke it easier to\ntrain the attentive weight calculating. The memory transformation\nmatrices Uğ‘š and calculating attention weights ğœ”ğ‘š,ğ‘™ are jointly\ntrained with other components of MATN.\n3.3 Multiplex Relation Aggregation Layer\nNext, we build upon a behavior type-wise gating mechanism to\naggregate the learned latent representations from the memory-\naugmented transformer network, with the exploration of their\ncontributions in capturing userâ€™s preference and assisting mak-\ning predictions on the target behavior. Considering the distinct\neffects of different types of behavior in characterizing userâ€™s in-\nterest, e.g. userâ€™s historical purchases may be more relevant to\nhis future purchases as compared to his page view activities, the\ntype-specific importance is learned in our gated mechanism in an\nadaptive manner. Formally, the applied weighted aggregation gate\noutputs a ğ‘‘-dimensional unified representation for ğ‘¢ğ‘– as follows:\nÎ¨ğ‘– = Softmax(wâŠ¤)Â·\nï£®ï£¯ï£¯ï£¯ï£¯ï£°\nZâŠ¤\nğ‘–,1\n...\nZâŠ¤\nğ‘–,ğ¿\nï£¹ï£ºï£ºï£ºï£ºï£»\n(5)\nwhere w âˆˆRğ‘‘ is the parametric weights for aggregation, the Soft-\nmax activation function is used to normalize the weights. By apply-\ning the weighted aggregation gate, MATN learns the contributions\nof different behavior types and thus can enable the adaptive aggre-\ngation in modeling the cross-type behavior relations.\nAfter obtaining the aggregated user behavior representation, the\nMATN adopts a two-layer feed-forward network with non-linear\nactivation, to capture the complex feature interactions in the latent\nembeddings. Formally the deeply-extracted user representations\nare learned with the following operation:\nÎ›ğ‘– = ğ‘“(W1 Â·Î¨ğ‘– +b1)+Î¨ğ‘–; Î“ğ‘– = ğ‘“(W2 Â·Î›ğ‘– +b2)+Î›ğ‘– (6)\nwhere Wâˆ—âˆˆRğ‘‘Ã—ğ‘‘ and bâˆ—âˆˆRğ‘‘ are transformation and bias vectors\nof the neural network, ğ‘“ is element-wisely applying non-linear\nactivation functions, and residual connections are also employed.\nÎ“ğ‘– âˆˆRğ‘‘ is the final user representation.\n3.4 The Learning Process of MATN\nGiven the user behavior representation aggregated from different\nviews (i.e., behavior type-specific semantics and cross-type behavior\ndependencies), MATN could make predictions on userâ€™s preference\nover items for the target ğ‘™-th type of behavior. In particular, the\nprediction process is performed through a dot product operation\nPr(Xğ‘–,ğ‘—,ğ‘™)= PâŠ¤\nğ‘— Â·Î“ğ‘–, where Pğ‘— âˆˆRğ‘‘ is from a parametric embed-\nding table for all items, and the result Pr(Xğ‘–,ğ‘—,ğ‘™)is a scalar score\nrepresenting ğ‘¢ğ‘–â€™s tendency of interacting with ğ‘¡ğ‘— under behavior ğ‘™.\nInspired by the settings of learning process on top-N recommen-\ndation tasks [23, 51], we leverage the pair-wise loss to model the\nrelative position in ranking-based recommendation scenarios. For\neach training step for user ğ‘¢ğ‘–, we sample a positive interaction set\n{ğ‘¡ğ‘1 ,ğ‘¡ğ‘2 ,...ğ‘¡ğ‘ğ‘ }composed of interacted items with ğ‘¢ğ‘– for the target\nğ‘™-th type of behavior. Here. ğ‘  is defined as the number of the posi-\ntive samples. Correspondingly, the same number of items that have\nno interactions with ğ‘¢ğ‘– in the training set are randomly sampled\nto form the negative interaction set {ğ‘¡ğ‘›1 ,ğ‘¡ğ‘›2 ,...,ğ‘¡ ğ‘›ğ‘ }. Based on the\nabove descriptions, we formally define our loss function over all\nthe samples of all users as below:\nLoss =\nğ¼âˆ‘ï¸\nğ‘–=1\nğ‘ âˆ‘ï¸\nğ‘˜=1\nmax(0,1 âˆ’Pr(Xğ‘–,ğ‘ğ‘˜,ğ‘™)+Pr(Xğ‘–,ğ‘›ğ‘˜,ğ‘™))+ ğœ†âˆ¥Î˜âˆ¥2\nF (7)\nwhere the first term is the pair-wise loss for a positive-negative pair.\nIt expands the signed difference between two predictions, until it\nAlgorithm 1: Learning Process of MATN Framework\nInput: user-item interaction tensor Xâˆˆ Rğ¼Ã—ğ½Ã—ğ¿, target\nbehavior ğ‘™, sample number ğ‘ , maximum epoch\nnumber ğ¸, regularization weight ğœ†, learning rate ğœ‚\nOutput: trained parameters in Î˜\n1 Initialize all parameters in Î˜\n2 for ğ‘’ = 1 to ğ¸do\n3 Draw a mini-batch U from all users {1,2,...,ğ¼ }\n4 Loss = ğœ†Â·âˆ¥Î˜âˆ¥2\nF\n5 for each ğ‘¢ğ‘– âˆˆU do\n6 Sample ğ‘  positive items {ğ‘¡ğ‘1 ,...,ğ‘¡ ğ‘ğ‘ }from Xğ‘–,ğ‘™\n7 Sample ğ‘  negative items {ğ‘¡ğ‘›1 ,...,ğ‘¡ ğ‘›ğ‘ }from Xğ‘–\n8 Compute Î“ğ‘– according to Eq 2 to Eq 6\n9 for ğ‘˜ = 1 to ğ‘  do\n10 Pr(Xğ‘–,ğ‘ğ‘˜,ğ‘™)= PâŠ¤ğ‘ğ‘˜ Â·Î“ğ‘–\n11 Pr(Xğ‘–,ğ‘›ğ‘˜,ğ‘™)= PâŠ¤ğ‘›ğ‘˜ Â·Î“ğ‘–\n12 Loss+= max(0,1 âˆ’(Pr(Xğ‘–,ğ‘ğ‘˜,ğ‘™)âˆ’Pr(Xğ‘–,ğ‘›ğ‘˜,ğ‘™)))\n13 end\n14 end\n15 for each parameter ğœƒ âˆˆÎ˜ do\n16 ğœƒ = ğœƒ âˆ’ğœ‚Â·ğœ•Loss/ğœ•ğœƒ\n17 end\n18 end\n19 return all parameters Î˜\nreaches a big enough scale. The latter term is a weight decay regu-\nlarization term to prevent over-fitting, and ğœ†is the regularization\nweight. The learning process is elaborated in Algorithm 1.\n4 EVALUATION\nIn this section, we perform experiments on different datasets to\ndemonstrate the effectiveness of our MATN. We aim to answer the\nfollowing research questions:\nâ€¢RQ1: Compared to state-of-the-art models, does MATN achieve\nbetter performance in various recommendation applications?\nâ€¢RQ2: What is the impact of the designed modules inMATN? Are\nthe proposed cross-behavior transformer network and attention\nmemory module necessary for improving performance?\nâ€¢RQ3: How is the MATNâ€™s recommendation accuracy w.r.t the\nintegration of different types of behavior?\nâ€¢RQ4: What is the influence of hyperparameter settings in MATN\nfor the recommendation performance?\nâ€¢RQ5: What behavior relational patterns does the proposedMATN\nmodel capture for the final recommendation decision?\nâ€¢RQ6: How is the scalability of the MATN framework?\n4.1 Experiment Settings\n4.1.1 Data Description. We evaluate the model performance on\nthree different types of datasets: (i) MovieLens: a benchmark dataset\nfor movie recommendations; (ii) Yelp: another benchmark dataset\nfor location-based venue recommendations from the online review\nTable 1: Statistics of experimented datasets\nDataset User # Item # Interaction # Interactive Behavior Type\nYelp 19800 22734 1.4 Ã—106 {Tip, Dislike, Neutral, Like}\nML10M 67788 8704 9.9 Ã—106 {Dislike, Neutral, Like}\nE-Commerce 805506 584050 6.4 Ã—107 {Page View, Favorite, Cart, Purchase}\nplatform Yelp; (iii) E-Commerce: a user behavior data from a real-\nworld e-commence platform. Table 1 summarizes the data statistics\nand we present the data details as below:\nMovieLens Data1. It is a widely-used dataset for performance vali-\ndation of various recommendation methods. Following the partition\nstrategy in [18, 24], we differentiate the explicit user-item interac-\ntive behavior into three types in terms of user rating scores ( i.e.,\nranging from 1 (worst) to 5 (best) stars with 0.5 star as increment):\nthe original rating score â‰¤2, > 2 and < 4, â‰¥4 corresponds to the\ndislike, neutral and like user behavior, respectively. In the MovieLens\ndataset, we regard the like interaction as the target behavior and\nother interactions (dislike and neutral) as source behavior, because\nthe positive interactions between users and items may be more\nuseful for capturing userâ€™s preferences in recommendations [20].\nYelp Data2. This is another recommendation benchmark dataset\ncollected from Yelp. We use the same multi-behavior differentiation\nstrategy as the MovieLens data and partition the 5-star range rating\nbehavior into dislike, neutral and like user behavior. In addition\nto the user rating behavior, this data includes an additional tip\nbehavior to indicate that user writes a tip on his/her visited venues.\nSimilar to the MovieLens data, the target behavior in Yelp data is\nalso set as the like interaction and others are set as source behavior.\nE-Commerce Data. Besides the two benchmark datasets for movie\nand location-based venue recommendations, we also evaluate our\nMATN framework in a real-world recommendation scenario with\nexplicit multiple user behavior data from a major online retailing\nplatform. Specifically, this data contains four types of interaction be-\nhavior, i.e., page view , add-to-favorite, add-to-cart and purchase. We\nconsider the purchase behavior as the target one, since the purchase\nis directly related with the conversion rate of recommendation in\nreal-life E-commerce sites [8].\n4.1.2 Evaluation Settings and Metrics. In our experiments, we\nutilize the leave-one-out evaluation strategy which has been widely\nutilized in recommendation literature [11, 12]. Following their eval-\nuation settings, we regard the latest interaction of each user as\nthe test set and use the rest of data for training. For efficient and\nfair evaluation, we follow the common strategy in [14, 32] to asso-\nciate each ground truth item with 99 randomly sampled negative\ninstances which have not interacted with the corresponding user.\nWe leverage two widely-used ranking metrics: Hit Ratio (HR@ ğ‘˜)\nand Normalized Discounted Cumulative Gain (NDCG@ ğ‘˜) [4, 41], to\ninvestigate the ranking performance (top-ğ‘˜ ranked recommended\nitems) of all compared methods. Note that higher HR and NDCG\nscores reflect better recommendation results. In our experiments,\nwe also evaluate the model performance by varying the ğ‘˜ value.\n1https://grouplens.org/datasets/movielens/10m/\n2https://www.yelp.com/dataset/download\n4.1.3 Competitive Baselines. To perform a comprehensive per-\nformance validation, we compare ourMATN with 12 baselines from\nsix research lines, which are elaborated as follows:\nConventional Matrix Factorization-based Recommendation:\nâ€¢BiasMF [15]: This method is built upon the matrix factorization\narchitecture with the incorporation of user and item biases.\nNeural Collaborative Filtering Models for Recommendation :\nâ€¢DMF [47]: It is a deep matrix factorization model which takes\nboth the explicit and implicit feedback as the input.\nâ€¢NCF [10]: NCF aims to supercharge collaborative filtering with\nnon-linear neural networks. We consider three variants of NCF\nw.r.t user-item interaction encoders: i.e., Multilayer perceptron\n(i.e., NCF-M), concatenated element-wise-product branch ( i.e.,\nNCF-N) and the fixed element-wise product (i.e., NCF-G).\nCollaborative Filtering with Auto-Encoder :\nâ€¢AutoRec [27]: It leverages a three-layer autoencoder to map\nuser-item interactions into latent representations.\nâ€¢CDAE [45]: In this autoencoder CF, an adaptive loss is incorpo-\nrated into the embedding projection process for users/items.\nNeural Auto-regressive Recommendation Models :\nâ€¢CF-NADE [53]: It enhances the autoregressive collaborative fil-\ntering with the parameter sharing between different ratings.\nâ€¢CF-UIcA [5]: It is a neural co-autoregressive framework to con-\nsider the structural correlation for both users and items.\nGraph Neural Network Recommendation Models :\nâ€¢ST-GCN [49]: It stacks encoder-decoder blocks using graph con-\nvolutional networks to learn embeddings of users and items.\nâ€¢NGCF [41]: This approach explore the structural knowledge\nwith the message-passing mechanism to capture the high-order\nconnections in the user-item interaction graph.\nRecommendation with Multi-Behavior Learning :\nâ€¢NMTR [7]: It is a multi-task recommendation model which con-\nsiders the behavior correlations in a cascaded manner.\nâ€¢DIPN [8]: This model utilizes bi-directional recurrent network\nand attention mechanism to consider the correlations between\nthe buying or browsing activities.\n4.1.4 Parameter Settings. In the latent learning space of MATN\nframework, we set the hidden state dimensionality ğ‘‘ as 16. In the\nmulti-behavior transformer module, we set the number of atten-\ntion heads for multi-dimensional learning as 2. Furthermore, the\nnumber of memory transformations is set as 8 in our customized\nbehavior-specific context encoding. We implement ourMATN with\nTensorFlow and use Adam optimizer for model optimization with\nthe learning rate and batch size of 1ğ‘’âˆ’3 and 32, respectively. The\ndecay rate of 0.96 is applied for each epoch during the training\nphase. To reduce the overfitting effect, we adopt set weight decay\nas the regularization strategy with the selection from {0.05, 0.01,\n0.005, 0.001, 0}. The depth of our feature extraction module is set\nas 3. For the baselines ( i.e., NCF and NMTR) which employ the\npoint-wise loss, we set the sampling ratio for positive and negative\ninstances from the range of 1 : 1to 1 : 4.\nTable 2: Prediction performance on Yelp (like behavior),\nMovieLens (like behavior) and E-Commerce (purchase be-\nhavior) data, in terms of HR@ğ‘˜ and NDCG@ğ‘˜ (ğ‘˜ = 10).\nModel\nYelp Data MovieLens E-Commerce\nHR Imp NDCG Imp HR Imp NDCG Imp HR Imp NDCG Imp\nBiasMF 0.755 9.4% 0.481 10.2% 0.767 10.4% 0.490 16.1% 0.262 35.1% 0.153 36.6%\nDMF 0.756 9.3% 0.485 9.3% 0.779 8.7% 0.485 17.3% 0.305 16.1% 0.189 10.6%\nNCF-M 0.714 15.7% 0.429 23.5% 0.757 11.9% 0.471 20.8% 0.319 11.0% 0.191 9.4%\nNCF-G 0.755 9.4% 0.487 8.8% 0.787 7.6% 0.502 13.3% 0.290 22.1% 0.167 15.1%\nNCF-N 0.771 7.1% 0.500 6.0% 0.801 5.7% 0.518 9.8% 0.325 8.9% 0.201 4.0%\nAutoRec 0.765 8.0% 0.472 12.3% 0.658 28.7% 0.392 45.2% 0.313 13.1% 0.190 10.0%\nCDAE 0.750 10.1% 0.462 14.7% 0.659 28.5% 0.392 45.2% 0.329 7.6% 0.196 6.6%\nCF-NADE 0.792 4.3% 0.499 6.2% 0.761 11.3% 0.486 17.1% 0.317 11.7% 0.191 9.4%\nCF-UIcA 0.750 10.1% 0.469 13.0% 0.778 8.9% 0.491 15.9% 0.332 6.6% 0.198 5.6%\nST-GCN 0.775 6.6% 0.465 14.0% 0.738 14.8% 0.444 28.2% 0.347 2.0% 0.206 1.5%\nNGCF 0.789 4.7% 0.500 6.0% 0.790 7.2% 0.508 12.0% 0.302 17.2% 0.185 13.0%\nNMTR 0.790 4.6% 0.478 10.9% 0.808 4.8% 0.531 7.2% 0.332 6.6% 0.179 16.8%\nDIPN 0.791 4.4% 0.500 6.0% 0.811 4.4% 0.540 5.4% 0.317 11.7% 0.178 17.4%\nMATN 0.826 â€“ 0.530 â€“ 0.847 â€“ 0.569 â€“ 0.354 â€“ 0.209 â€“\n4.2 Performance Comparison (RQ1)\n4.2.1 Performance on Target Behavior. In the evaluation, we\nfirst perform experiments to separately make recommendations\non venue, movie and online retailing products with three types of\ndatasets and the results are shown in Table 2 (â€œImpâ€ indicates the\nrelatively improvement ratio). We observe the remarkable perfor-\nmance improvement achieved by our MATN in predicting different\ntypes of behaviors. We attribute such improvements to exploration\nof the cross-type behavior dependencies which are neglected by\nmost existing methods, although they attempt to model complex\nuser-item interactive relations with various deep neural encoders\n(e.g., autoencoder, graph neural network, attention mechanism).\nAdditionally, by jointly analyzing the results among the three\ndatasets, we find that the improvement ofMATN on the E-Commerce\ndata is the most significant with the largest data scale. This may be\ncaused by the behavior diversity: the multiple behaviors from the\nE-Commerce site are constructed with four different types of be-\nhavior which may show strong ordinal relations between the target\n(purchase) and source behaviors (e.g., page view â†’add-to-cart â†’\npurchase) in the real-world online retailing systems. The consistent\nimprovement across datasets with different user-item interaction\ndensities, suggests the robustness of MATN in accurately learning\nuser preference under different sparsity degrees.\nLastly, it is worth mentioning that although the correlations\nbetween behavior has been considered in recent recommendation\nsolutions (i.e., NMTR and DIPN), they merely model the singular di-\nmensional cascading correlations between multi-type interactions,\nand cannot comprehensively capture the arbitrary dependencies\nbetween different types of interaction with different items. There-\nfore, such oversimplification on the behavior dependency leads to\nsuboptimal recommendation results.\n4.2.2 Overall Prediction Click Behavior. We also conduct ex-\nperiments to evaluate the recommendation performance of all com-\npared methods by forecasting the overall user-item interaction (i.e.,\nclick behavior), since the accurate predictions on overall interactive\nbehavior (e.g., including all page view, add-to-cart and purchase\nbehavior) could also provide useful insights for recommendation\nscenarios which focus on optimizing the click rate. As shown in Ta-\nble 3, our MATN still achieves the best performance on all datasets\nas compared to various types of baselines. This validation shows\nTable 3: Overall recommendation performance in forecasting click behavior in terms of HR@ğ‘˜ and NDCG@ğ‘˜ (ğ‘˜ = 10).\nData Metric BiasMF DMF NCF-M NCF-G NCF-N AutoRec CDAE CF-NADE CF-UIcA ST-GCN NGCF NMTR DIPN MATN\nYelp HR 0.809 0.801 0.770 0.808 0.812 0.745 0.753 0.771 0.808 0.796 0.810 0.794 0.816 0.848\nNDCG 0.513 0.503 0.464 0.519 0.523 0.456 0.456 0.478 0.512 0.483 0.521 0.473 0.514 0.548\nMovieLens HR 0.727 0.730 0.693 0.745 0.748 0.612 0.613 0.677 0.718 0.688 0.735 0.773 0.776 0.808\nNDCG 0.456 0.461 0.427 0.470 0.473 0.361 0.360 0.421 0.442 0.425 0.468 0.497 0.499 0.535\nE-Commerce HR 0.383 0.399 0.401 0.400 0.409 0.423 0.427 0.486 0.428 0.452 0.470 0.409 0.405 0.535\nNDCG 0.228 0.245 0.239 0.240 0.244 0.257 0.262 0.293 0.257 0.257 0.281 0.236 0.237 0.326\nthe potential of the overall prediction performance of MATN by\njointly considering multi-type behavior of users.\n4.2.3 Ranking Performance v.s. Top- ğ¾ Value. We also eval-\nuate the model ranking performance by varying the ğ¾ value in\nterms of HR@ğ¾ and NDCG@ğ¾. We compare MATN with the best\nperformed method of each baseline categories (see Section 4.1.3\nfor baseline description), and report the results of predicting the\nclick and like behavior on Yelp data in Table 4. We can observe\nthat MATN consistently outperforms other representative baselines\nwith different settings of ğ¾.\nTable 4: Ranking performance evaluation on Yelp dataset\nwith varying Top-K value in terms of HR@K and NDCG@K\nModel Metric Click Like\n@1 @3 @5 @7 @9 @1 @3 @5 @7 @9\nBiasMF HR 0.261 0.519 0.644 0.728 0.799 0.287 0.474 0.626 0.714 0.741\nNDCG 0.261 0.409 0.460 0.490 0.508 0.287 0.378 0.432 0.461 0.474\nNCF-N HR 0.278 0.535 0.661 0.747 0.800 0.260 0.481 0.604 0.695 0.742\nNDCG 0.278 0.426 0.474 0.505 0.519 0.260 0.396 0.444 0.477 0.492\nAutoRec HR 0.227 0.443 0.568 0.650 0.715 0.228 0.455 0.586 0.684 0.732\nNDCG 0.227 0.343 0.398 0.426 0.440 0.228 0.362 0.410 0.449 0.462\nCF-NADE HR 0.236 0.466 0.597 0.682 0.744 0.265 0.508 0.642 0.720 0.784\nNDCG 0.236 0.368 0.423 0.452 0.468 0.265 0.402 0.454 0.478 0.497\nCF-UIcA HR 0.261 0.501 0.640 0.723 0.784 0.235 0.449 0.576 0.659 0.731\nNDCG 0.261 0.390 0.447 0.478 0.500 0.235 0.360 0.412 0.440 0.463\nST-GCN HR 0.231 0.474 0.614 0.704 0.766 0.216 0.445 0.580 0.669 0.744\nNDCG 0.231 0.369 0.426 0.458 0.478 0.216 0.347 0.400 0.430 0.454\nNMTR HR 0.203 0.459 0.608 0.700 0.767 0.214 0.466 0.610 0.700 0.762\nNDCG 0.203 0.352 0.412 0.445 0.465 0.214 0.360 0.419 0.450 0.469\nMATN HR 0.296 0.560 0.693 0.771 0.828 0.279 0.529 0.659 0.741 0.798\nNDCG 0.296 0.447 0.500 0.529 0.545 0.279 0.423 0.477 0.507 0.524\n4.3 Model Ablation Study (RQ2)\nFurthermore, we conduct ablation experiments over a several key\ncomponents of MATN to better understand the component-specific\neffects. Particularly, we introduce the following model variants:\nâ€¢Effect of Multi-Behavior Transformer Network : MATN-T.\nWe do not utilize the multi-behavior transformer network to\ncapture mutual relations between different types of behavior.\nâ€¢Effect of Memory Attention Mechanism : MATN-M. We re-\nmove the memory-augmented attention network in the joint\nMATN model to encode behavior type-specific semantics.\nâ€¢Effect of Gating Mechanism : MATN-G. We replace the de-\nsigned gating mechanism with the simplified average pooling\noperation over all behavior type-specific representations in the\nbehavioral pattern aggregation layer.\nFigure 2 presents the model ablation study results. We summary\nthe following findings (MATN is the default model version).\n(1) The incorporation of mutual dependencies between different\ntypes of interaction behavior over all items, is capable of boosting\nthe performance substantially. It demonstrates the rationality of our\nmulti-head self-attention architecture in learning explicit pair-wise\nrelations between different behavior types.\n(2) MATN is consistently superior to MATN-M, which hence il-\nlustrates the importance of considering context and semantics of\nindividual type of behavior in profiling user preferences.\n(3) The replacement of our gating mechanism (MATN) with average\npooling operation (MATN-G), degrades the modelâ€™s performance. It\nmake sense since MATN-G fails to model the different importance\nacross different types of behavior in making final recommendations.\n4.4 Impact Studies of Multi-Behavior Relation\nIntegration (RQ3)\nTo investigate whether exploiting multi-type interaction behavior\nhelps to achieve better performance, we further perform ablation ex-\nperiments for the purchase prediction task on E-Commerce data, to\nshow the effect of incorporating different types of user-item interac-\ntions in our MATN with four model variants: MATNğ¹â€“without the\nadd-to-favorite behavior; MATNğ¶â€“without the add-to-cart behav-\nior; and MATNğ‘ƒâ€“without the page view behavior. Furthermore, we\ndesign another variant by removing all other types of interactions\nand only contain the purchase behavior MATNğµ.\nFigure 3 shows the evaluation results of different variants under\nvarying top-k settings. We summarize the following findings:\n(1) MATN using all types of interaction behaviors consistently\noutperforms other variants with varying top-k settings, except for\none exception on top-1 prediction with minor performance defect.\nThe results validate that our MATN improve purchase forecasting\nthrough integrating multi-behavior relations.\n(2) MATNğµ using only purchase data yields worst performance,\nwhich shows the positive contribution of the three additional be-\nhavior types ( i.e. page view, add-to-favorite and add-to-cart) in\nhelping with user modeling in the e-commerce scenario.\n(3) Among the three variants that utilize two additional behavior\ntypes (i.e. MATNğ¹, MATNğ¶, MATNğ‘ƒ), MATNğ‘ƒ clearly shows more\nsevere performance degradation compared to the other two. This\nsheds light on the higher importance and effectiveness of utilizing\npage view data in MATN and online shopping recommendation.\n4.5 Hyperparameter Study of MATN (RQ4)\nIn our experiments, we also investigate the impact of different hy-\nperparameter settings in our developed MATN framework. Specif-\nically, we evaluate the model recommendation performance by\nvarying the values of several key hyperparameters, including the\nhidden state dimensionality ğ‘‘, the memory dimension ğ‘€ in our\nmemory attention network, and the number of neural network\nlayers ğ‘ in our deep feature extraction module. The evaluation\nLike Click\n0.7\n0.74\n0.78\n0.82\n0.86HR@10\nMATN\nMATN-G\nMATN-M\nMATN-T\n(a) Yelp-HR@10\nLike Click\n0.4\n0.45\n0.5\n0.55\n0.6NDCG@10\nMATN\nMATN-G\nMATN-M\nMATN-T (b) Yelp-NDCG@10\nLike Click\n0.7\n0.74\n0.78\n0.82\n0.86HR@10\nMATN\nMATN-G\nMATN-M\nMATN-T (c) MovieLens-HR@10\nLike Click\n0.4\n0.44\n0.48\n0.52\n0.56\n0.6\nNDCG@10\nMATN\nMATN-G\nMATN-M\nMATN-T (d) MovieLens-NDCG@10\nPurchase Click\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6HR@10\nMATN\nMATN-G\nMATN-M\nMATN-T (e) E-Commerce-HR@10\nPurchase Click\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35NDCG@10\nMATN\nMATN-G\nMATN-M\nMATN-T (f) E-Commerce-NDCG@10\nFigure 2: Model ablation study of MATN on Yelp, MovieLens and E-Commerce data in terms of HR@ ğ¾ and NDCG@ğ¾.\nHR@1\nMATNB\nMATNP\nMATNF\nMATNC\nHR@3 HR@5 HR@7 HR@9\n0.1\n0.2\n0.3\n0\nMATN\n(a) HR@ k\nNDCG@1 NDCG@3 NDCG@5 NDCG@7 NDCG@9\n0.1\n0.2\n0\nMATNB\nMATNP\nMATNF\nMATNC\nMATN (b) NDCG@ k\nFigure 3: Impact study of multi-behavior relation integra-\ntion on purchase prediction of E-Commerce dataset.\nresults on the Yelp data in predicting both click and like behavior\nare shown in Figure 4. The major findings are summarized as below:\nâ€¢Hidden State Dimensionality ğ‘‘. We can observe that when\nwe increase ğ‘‘ from 4 to 16, the recommendation performance\nbecomes better, but the further increase the ğ‘‘ value (â‰¥32) may\nnot be helpful for the model prediction accuracy. The potential\nreason for this observation is that the large number of latent\nunits could bring a stronger representation capability.\nâ€¢Memory Dimension ğ‘€. Our memory attention network en-\nables the behavior type-specific semantics learning could be per-\nformed fromğ‘€different dimensions. The parameter study results\non memory dimension ğ‘€ indicates that performing the trans-\nformation with more latent learning sub-spaces will benefit the\nrecommendation at the early stage, but the continuous increase\nof ğ‘€ will lead to the overfitting issue.\nâ€¢Feature Extraction Network Depth ğ‘. We further examine\nwhether designing a deep feature extraction network is beneficial\nto the recommendation task. As we can see, stacking two hidden\nlayers is beneficial to the performance, which is attributed to the\nhigh non-linearities brought by more non-linear layers. However,\nthe overfitting phenomenon can be observe when we perform\nmore transformation-based feature interaction operation with\nmore hidden layers (â‰¥3).\n4.6 Case Study on Model Interpretation (RQ5)\nIn this subsection, we perform qualitative analyses to show the\nmodel interpretation of MATN in comprehending user behavior\nrelationships and generate more convincing recommendation. To\nbe specific, we visualize the learned quantitative weights learned\nby our multi-head self-attention mechanism, memory-augmented\nattention network and multiplex relation aggregation layer. Four\ntypical cases (i.e., samp1,...,samp4) are sampled from the prediction\nof overall click behavior and purchase behavior on the E-Commerce\ndataset. From the visualization results, we have the following ob-\nservations:\n(1) page view and purchase behavior could provide more infor-\nmative signals in predicting the click and purchase, respectively.\nThis make sense since the same type of behavior may share closer\nrelationships than other behavior types. (2) In the head-specific\nself-attention layer, the 4 Ã—4 behavior relevance matrix indicates\nacross four types of user behavior. An interesting observation is\nthat: add-to-favorite activity is more like to be correlated with page\nview and purchase, than add-to-favorite. Similar results can be ob-\nserved for add-to-cart. It might indicate that add-to-favorite has\na high co-occurrence probability than others in the real-world e-\ncommerce platform. (3) The memory attention could learn weights\nin an adaptive way which corresponds to the importance score\ngenerated by our gating mechanism. The reason lies in the utiliza-\ntion of ReLU activation in the attention calculation instead of the\nmandatory restriction with Softmax function. Overall, all above\nobservations demonstrate the model interpretation power ofMATN\nin capturing complex behavior relations from different perspective.\n4.7 Scalability Study of MATN (RQ6)\nIn addition to recommendation accuracy, the model efficiency is\nalso an important factor to investigate. In this subsection, we eval-\nuate the computation time cost of our MATN as compared to other\nbaselines. In Table 5, we report the running time of each epoch\nduring the training phase of each compared approach. We can ob-\nserve that our MATN model could achieve comparable performance\nwhen competing with most baselines, especially in dealing with\nthe large-scale user-item interaction data.\nAlthough we lose in the cases when comparing with some of\nthe competitive baselinesâ€“learning user-item interaction represen-\ntations with simple relation encoders (e.g., Multilayer Perceptron,\nvanilla autoencoder), our MATN still exhibits competitive model\nscalability due to the comparable time complexity. However,MATN\ncan show obvious performance superiority over these techniques.\nIn addition, the performance gap (measured by running time) be-\ntween MATN and graph neural network recommendation methods\n(i.e., ST-GCN and NGCF), may stem from the high computational\ncost of graph convolution operation when performing information\naggregation and propagation.\n5 RELATED WORK\nDeep Collaborative Filtering Techniques . Deep learning have\nbeen revolutionizing collaborative filtering techniques and achieve\npromising results in many recommendation scenarios. For example,\n5 10 15 20 25 300.7\n0.75\n0.8\n0.85\nHidden State Dimensionalityğ‘‘\nHR\nClickLike 5 10 15 20 25 300.4\n0.45\n0.5\n0.55\nHidden State Dimensionalityğ‘‘\nNDCG\nClickLike 2 4 6 8 101214160.76\n0.78\n0.8\n0.82\n0.84\n0.86\nMemory Dimensionğ‘€\nHR\nClickLike 2 4 6 8 101214160.48\n0.5\n0.52\n0.54\n0.56\nMemory Dimensionğ‘€\nNDCG\nClickLike 0 1 2 3 4 5 60.78\n0.8\n0.82\n0.84\n0.86\nFeature Extraction Network Depthğ‘\nHR\nClickLike 0 1 2 3 4 5 60.46\n0.48\n0.5\n0.52\n0.54\n0.56\nFeature Extraction Network Depthğ‘\nNDCG\nClickLike\nFigure 4: Hyper-parameter study in terms of HR@10 and NDCG@10\nSelf\nAtt.\nMemo\nAtt.\n(69, 0, 10, 5)\nSamp. 1\n Samp. 2\n(19, 1, 3, 1)\n (50, 12, 39, 50)\nSamp. 3\n Samp. 4\n(20, 2, 7, 6)\n0.0 1.0 / 5.0\nAggregation:\n0.59   0.07   0.17   0.17\nPurchase\n0.07   0.11   0.11   0.71\nClick\nP\nF\nC\nB\nP: page view    F: add-to-favorite    C: add-to-cart    B: purchase\nP\nF\nC\nBP F C B\nHead 1 Head 2\nğœ”8ğœ”1\nFigure 5: Case study of the learned quantitative weights\nfrom key modules in MATN. Pair-wise relations between\nfour types of behavior ( e.g., ğ‘ƒ, ğ¹, ğ¶ and ğµ) are represented\nwith a 4 Ã—4 weight matrix in the multi-head self-attention\nlayer.ğœ”1,...,ğœ”8 indicate the learned weights across 8 memory\ndimensions in the memory attention network. The four rel-\nevance scores encoded by gating mechanism corresponds to\nfour behavior types. Tuples ( e.g., <50,12,39,50>) are numbers\nof behaviors in the order of (P, F, C, B).\nTable 5: Computational time cost (seconds) investigation.\nModels Yelp MovieLens E-Commerce\nBiasMF 34 34 241\nDMF 48 59 231\nNCF-N 33 35 247\nAutoRec 32 37 228\nCDAE 33 37 228\nCF-NADE 32 37 229\nCF-UIcA 51 63 476\nST-GCN 55 65 491\nNGCF 61 70 503\nNMTR 33 35 268\nDIPN 99 134 1062\nMATN 40 49 234\nMulti-layer Perceptron has been integrated into the collaborative fil-\ntering architecture to handle non-linear feature interactions [11, 47].\nSeveral work attempts to utilize encoder-decoder network to map\nexplicit user-item interactions into latent representations, using au-\ntoencoder [27] and its variants [29]. In addition, another research\nline lie in leveraging graph neural network to incorporate user-\nitem graph signals into the recommendation framework, such as\nNGCF [41], STAR-GCN [49] and Multi-GCCF [31]. The major dif-\nference between these methods and ours is that MATN explores the\ncross-behavior interactive knowledge to assist recommendation.\nRelation-aware Recommender Systems . Prior work has made\nsignificant advances to develop recommender systems with the con-\nsideration of various relations between users and items. For exam-\nple, the social-aware recommender systems aim to boost recommen-\ndation performance by exploring userâ€™s social relations based on the\ninformation dissemination [3, 6]. Furthermore, knowledge graph\nhas become another information source from item side to help rec-\nommendation models capture relationships between items [37, 40].\nIn addition to the relation of collaborative similarity, there exist\nwork aiming to consider multiple item relationships (e.g., shared\ndirector or categories) to learn fine-grained item knowledge [46].\nDifferent from these methods which focus on using the exogenous\ninformation from either user or item side, this work explores the\nmultiplicity of pairwise user-item interactions and carefully learns\ntheir underlying inter-dependencies.\nAttention Network for Recommendations . Attention mecha-\nnism has been proven to be effective in differentiating various\nrelations for recommendations [34], such as item transitions [17],\nuser connections [28] and customer group dynamics [36]. To ad-\ndress the limitation of recurrent neural architectures in capturing\nlong-range dependencies without the rigid order assumption, self-\nattention mechanism has been introduced to model correlations\nfrom any pair of positions of input data points [35]. For example,\nSun et al. [30] proposed a bidirectional self-attention framework for\nsequential recommendation. Additionally, multi-head self-attentive\nmodel is introduced to recommended news to users [43]. Our MATN\nframework is motivated by the multi-head self-attentive learning\narchitecture in a sense that a memory augmented transformer is\ndesigned to model multiplex behavior relation dynamics from dif-\nferent types of user-item interactions.\n6 CONCLUSION\nIn this work, we propose MATN, a novel memory augmented trans-\nformer neural architecture which incorporates multiple types of\nuser behavior relationships into a cross-behavior collaborative fil-\ntering framework. We argue that these different types of user-item\ninteractions are usually neglected in conventional methods. MATN\ndemonstrates the state-of-the-art performance on two benchmark\ndatasets and a large-scale user behavior data from a major online\nretailing platform. In addition, via the qualitative analysis of the at-\ntentive weights, we discover that the implicit cross-type behavioral\ndependencies are encoded within the MATN framework.\nNotwithstanding the interesting problem and promising results,\nsome directions exist for future work. We will next incorporate rich\nauxiliary data source ( e.g., user review text information or item\ndescription [52]) to further enhance the current recommendation\nframework. Additionally, another time dimension of the problem\ndeserves more investigation. When multi-type user-item interac-\ntion data arrives in a timely manner, how to best account for it\nin the current MATN framework? One possible direction is adapt-\ning MATN to a time-sensitive model by analyzing the trade-off\nbetween accuracy and complexity.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their constructive feedback.\nThis work was supported by National Nature Science Foundation\nof China (61672241, U1611461), Major Project of National Social Sci-\nence Foundation of China (18ZDA062), Natural Science Foundation\nof Guangdong Province (2016A030308013), Science and Technology\nProgram of Guangdong Province (2019A050510010), and Fundamen-\ntal Research Funds for the Central Universities (x2js-D2192830).\nREFERENCES\n[1] Kristen M Altenburger and Daniel E Ho. 2019. Is Yelp Actually Cleaning Up\nthe Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer\nReviews. In WWW. 2543â€“2550.\n[2] Yukuo Cen, Xu Zou, Jianwei Zhang, Hongxia Yang, Jingren Zhou, and Jie Tang.\n2019. Representation learning for attributed multiplex heterogeneous network.\nIn KDD. 1358â€“1368.\n[3] Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2019. Social attentional\nmemory network: Modeling aspect-and friend-level differences in recommenda-\ntion. In WSDM. 177â€“185.\n[4] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and\nHongyuan Zha. 2018. Sequential recommendation with user memory networks.\nIn WSDM. ACM, 108â€“116.\n[5] Chao Du, Chongxuan Li, Yin Zheng, Jun Zhu, and Bo Zhang. 2018. Collaborative\nfiltering with user-item co-autoregressive models. In AAAI.\n[6] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.\n2019. Graph Neural Networks for Social Recommendation. In WWW. ACM,\n417â€“426.\n[7] Chen Gao, Xiangnan He, Dahua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-\nSeng Chua, and Depeng Jin. 2019. Neural multi-task recommendation from multi-\nbehavior data. In 2019 IEEE 35th International Conference on Data Engineering\n(ICDE). IEEE, 1554â€“1557.\n[8] Long Guo, Lifeng Hua, Rongfei Jia, Binqiang Zhao, Xiaobo Wang, and Bin\nCui. 2019. Buying or Browsing?: Predicting Real-time Purchasing Intent us-\ning Attention-based Deep Network with Multiple Behavior. In KDD. 1984â€“1992.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR. 770â€“778.\n[10] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse\npredictive analytics. In SIGIR. ACM, 355â€“364.\n[11] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In WWW. 173â€“182.\n[12] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast\nmatrix factorization for online recommendation with implicit feedback. In SIGIR.\n549â€“558.\n[13] Chao Huang, Xian Wu, Xuchao Zhang, Chuxu Zhang, Jiashu Zhao, Dawei Yin,\nand Nitesh V Chawla. 2019. Online Purchase Prediction via Multi-Scale Modeling\nof Behavior Dynamics. In KDD. 2613â€“2622.\n[14] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. IEEE, 197â€“206.\n[15] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-\nniques for recommender systems. Computer 8 (2009), 30â€“37.\n[16] Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang,\nGuoliang Kang, Qiwei Chen, Wei Li, and Dik Lun Lee. 2019. Multi-interest\nnetwork with dynamic routing for recommendation at Tmall. In CIKM. 2615â€“\n2623.\n[17] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural attentive session-based recommendation. In CIKM. 1419â€“1428.\n[18] Daryl Lim, Julian McAuley, and Gert Lanckriet. 2015. Top-n recommendation\nwith missing implicit feedback. In Recsys. 309â€“312.\n[19] Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, and Steven CH\nHoi. 2019. Compositional Coding for Collaborative Filtering. In SIGIR. 145â€“154.\n[20] Babak Loni, Roberto Pagano, Martha Larson, and Alan Hanjalic. 2019. Top-\nn recommendation with multi-channel positive feedback using factorization\nmachines. Transactions on Information Systems (TOIS) 37, 2 (2019), 1â€“23.\n[21] Chen Ma, Liheng Ma, Yingxue Zhang, Jianing Sun, Xue Liu, and Mark Coates.\n2020. Memory Augmented Graph Neural Networks for Sequential Recommenda-\ntion. In AAAI.\n[22] Andriy Mnih and et al . 2008. Probabilistic matrix factorization. In NIPS. 1257â€“\n1264.\n[23] Athanasios N Nikolakopoulos and George Karypis. 2019. Recwalk: Nearly un-\ncoupled random walks for top-n recommendation. In WSDM. 150â€“158.\n[24] Vito Claudio Ostuni, Tommaso Di Noia, Eugenio Di Sciascio, and Roberto Mirizzi.\n2013. Top-n recommendations from implicit feedback leveraging linked open\ndata. In Recsys. 85â€“92.\n[25] Jiarui Qin, Kan Ren, Yuchen Fang, Weinan Zhang, and Yong Yu. 2020. Sequen-\ntial Recommendation with Dual Side Neighbor-based Collaborative Relation\nModeling. In WSDM. 465â€“473.\n[26] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2012. BPR: Bayesian personalized ranking from implicit feedback. In UAI.\n[27] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.\nAutorec: Autoencoders meet collaborative filtering. In WWW. ACM, 111â€“112.\n[28] Weiping Song, Zhiping Xiao, Yifan Wang, Laurent Charlin, Ming Zhang, and Jian\nTang. 2019. Session-based social recommendation via dynamic graph attention\nnetworks. In WSDM. 555â€“563.\n[29] Florian Strub, Romaric Gaudel, and JÃ©rÃ©mie Mary. 2016. Hybrid recommender\nsystem based on autoencoders. In DLRS. 11â€“16.\n[30] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder repre-\nsentations from transformer. In CIKM. 1441â€“1450.\n[31] Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming\nTang, and Xiuqiang He. 2019. Multi-Graph Convolution Collaborative Filtering.\nIn ICDM. IEEE, 1306â€“1311.\n[32] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation\nvia convolutional sequence embedding. In WSDM. 565â€“573.\n[33] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Latent relational metric learning\nvia memory-based attention for collaborative ranking. In WWW. 729â€“739.\n[34] Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018. Multi-pointer co-attention\nnetworks for recommendation. In KDD. 2309â€“2318.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS. 5998â€“6008.\n[36] Lucas Vinh Tran, Tuan-Anh Nguyen Pham, Yi Tay, Yiding Liu, Gao Cong, and\nXiaoli Li. 2019. Interact and decide: Medley of sub-attention networks for effective\ngroup recommendation. In SIGIR. 255â€“264.\n[37] Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao,\nWenjie Li, et al . 2019. Knowledge-aware graph neural networks with label\nsmoothness regularization for recommender systems. In KDD. 968â€“977.\n[38] Jianling Wang, Raphael Louca, Diane Hu, Caitlin Cellier, James Caverlee, and\nLiangjie Hong. 2020. Time to Shop for Valentineâ€™s Day: Shopping Occasions and\nSequential Recommendation in E-commerce. In WSDM. 645â€“653.\n[39] Weixun Wang, Junqi Jin, Jianye Hao, Chunjie Chen, Chuan Yu, Weinan Zhang,\nJun Wang, Xiaotian Hao, Yixi Wang, Han Li, et al . 2019. Learning Adaptive\nDisplay Exposure for Real-Time Advertising. In CIKM. 2595â€“2603.\n[40] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:\nKnowledge graph attention network for recommendation. In KDD. 950â€“958.\n[41] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR.\n[42] Hongfa Wen, Xin Liu, Chenggang Yan, Linhua Jiang, Yaoqi Sun, Jiyong Zhang,\nand Haibing Yin. 2019. Leveraging Multiple Implicit Feedback for Personalized\nRecommendation with Neural Network. In AIAM. 1â€“6.\n[43] Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.\n2019. Neural News Recommendation with Multi-Head Self-Attention. In EMNLP.\n6390â€“6395.\n[44] Xian Wu, Baoxu Shi, Yuxiao Dong, Chao Huang, and Nitesh V Chawla. 2019.\nNeural tensor factorization for temporal interaction learning. InWSDM. 537â€“545.\n[45] Yao Wu, Christopher DuBois, Alice X Zheng, et al. 2016. Collaborative denoising\nauto-encoders for top-n recommender systems. In WSDM. ACM, 153â€“162.\n[46] Xin Xin, Xiangnan He, Yongfeng Zhang, Yongdong Zhang, and Joemon Jose.\n2019. Relational Collaborative Filtering: Modeling Multiple Item Relations for\nRecommendation. In SIGIR.\n[47] Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, et al . 2017. Deep\nMatrix Factorization Models for Recommender Systems.. In IJCAI. 3203â€“3209.\n[48] Seongjun Yun, Raehyun Kim, Miyoung Ko, and Jaewoo Kang. 2019. SAIN: Self-\nAttentive Integration Network for Recommendation. In SIGIR. 1205â€“1208.\n[49] Jiani Zhang, Xingjian Shi, Shenglin Zhao, and Irwin King. 2019. STAR-GCN:\nStacked and Reconstructed Graph Convolutional Networks for Recommender\nSystems. IJCAI (2019).\n[50] Shuai Zhang, Yi Tay, Lina Yao, Aixin Sun, and Jake An. 2019. Next item recom-\nmendation with self-attentive metric learning. In AAAI, Vol. 9.\n[51] Lei Zheng, Chaozhuo Li, Chun-Ta Lu, Jiawei Zhang, and Philip S Yu. 2019. Deep\nDistribution Network: Addressing the Data Sparsity Issue for Top-N Recommen-\ndation. In SIGIR. 1081â€“1084.\n[52] Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint deep modeling of users\nand items using reviews for recommendation. In WSDM. 425â€“434.\n[53] Yin Zheng, Bangsheng Tang, Wenkui Ding, and Hanning Zhou. 2016. A neural\nautoregressive approach to collaborative filtering. In ICML.\n[54] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\nrate prediction. In KDD. 1059â€“1068.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ]
}