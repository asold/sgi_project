{
  "title": "Comparing CNN-based and transformer-based models for identifying lung cancer: which is more effective?",
  "url": "https://openalex.org/W4390012371",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5089925207",
      "name": "Lulu Gai",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A5112668595",
      "name": "Mengmeng Xing",
      "affiliations": [
        "Shandong First Medical University",
        "Shandong Provincial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5100690344",
      "name": "Wei Chen",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A5100388188",
      "name": "Yi Zhang",
      "affiliations": [
        "Shandong Maternal and Child Health Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5040144517",
      "name": "Qiao Xu",
      "affiliations": [
        "Shandong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2310252196",
    "https://openalex.org/W1878258539",
    "https://openalex.org/W4221013561",
    "https://openalex.org/W4311710860",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W4224271142",
    "https://openalex.org/W3092760571",
    "https://openalex.org/W2955805844",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W2801920224",
    "https://openalex.org/W3169729987",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4292616310",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3041937667",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3195244249",
    "https://openalex.org/W3012120793",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W6600018615",
    "https://openalex.org/W3012412627",
    "https://openalex.org/W6601258443",
    "https://openalex.org/W2158698691",
    "https://openalex.org/W1677182931"
  ],
  "abstract": "Abstract Lung cancer constitutes the most severe cause of cancer-related mortality. Recent evidence supports that early detection by means of computed tomography (CT) scans significantly reduces mortality rates. Given the remarkable progress of Vision Transformers (ViTs) in the field of computer vision, we have delved into comparing the performance of ViTs versus Convolutional Neural Networks (CNNs) for the automatic identification of lung cancer based on a dataset of 212 medical images. Importantly, neither ViTs nor CNNs require lung nodule annotations to predict the occurrence of cancer. To address the dataset limitations, we have trained both ViTs and CNNs with three advanced techniques: transfer learning, self-supervised learning, and sharpness-aware minimizer. Remarkably, we have found that CNNs achieve highly accurate prediction of a patient’s cancer status, with an outstanding recall (93.4%) and area under the Receiver Operating Characteristic curve (AUC) of 98.1%, when trained with self-supervised learning. Our study demonstrates that both CNNs and ViTs exhibit substantial potential with the three strategies. However, CNNs are more effective than ViTs with the insufficient quantities of dataset.",
  "full_text": "Multimedia Tools and Applications (2024) 83:59253–59269\nhttps://doi.org/10.1007/s11042-023-17644-4\nComparing CNN-based and transformer-based models\nfor identifying lung cancer: which is more effective?\nLulu Gai 1 · Mengmeng Xing 2 · Wei Chen 3 · Yi Zhang 4 · Xu Qiao 1\nReceived: 21 April 2023 / Revised: 16 September 2023 / Accepted: 28 October 2023 /\nPublished online: 20 December 2023\n© The Author(s) 2023\nAbstract\nLung cancer constitutes the most severe cause of cancer-related mortality. Recent evidence\nsupports that early detection by means of computed tomography (CT) scans signiﬁcantly\nreduces mortality rates. Given the remarkable progress of Vision Transformers (ViTs) in\nthe ﬁeld of computer vision, we have delved into comparing the performance of ViTs ver-\nsus Convolutional Neural Networks (CNNs) for the automatic identiﬁcation of lung cancer\nbased on a dataset of 212 medical images. Importantly, neither ViTs nor CNNs require lung\nnodule annotations to predict the occurrence of cancer. To address the dataset limitations,\nwe have trained both ViTs and CNNs with three advanced techniques: transfer learning,\nself-supervised learning, and sharpness-aware minimizer. Remarkably, we have found that\nCNNs achieve highly accurate prediction of a patient’s cancer status, with an outstanding\nrecall (93.4%) and area under the Receiver Operating Characteristic curve (AUC) of 98.1%,\nwhen trained with self-supervised learning. Our study demonstrates that both CNNs and ViTs\nexhibit substantial potential with the three strategies. However, CNNs are more effective than\nViTs with the insufﬁcient quantities of dataset.\nB Yi Zhang\nzhangyisd@139.com\nB Xu Qiao\nqiaoxu@sdu.edu.cn\nLulu Gai\n2927791087llg@gmail.com\nMengmeng Xing\n81115228@qq.com\nWei Chen\nichenwei@sdu.edu.cn\n1 School of Control Science and Engineering, Shandong University, 250100 Jinan, China\n2 Department of Thoracic Surgery, Shandong Provincial Hospital Afﬁliated to Shandong First\nMedical University, Shandong, China\n3 Department of Stomatology, Shandong University, 250012 Jinan, China\n4 Shandong Provincial Maternal and Child Health Care Hospital, Jinan, China\n123\n59254 Multimedia Tools and Applications (2024) 83:59253–59269\nKeywords Lung cancer identiﬁcation · Convolutional neural networks ·\nVision transformers · Self-supervised learning · Sharpness-aware Minimizer\n1 Introduction\nLung cancer stands as the foremost cause of cancer-related fatalities worldwide, with over 1.6\nmillion newly diagnosed cases per year [ 1]. Computed tomography (CT) has been proposed\nas a valuable early screening tool for lung cancer. Typically, lung cancer appears as a nodule\non a CT scan. Early differentiation between non-cancerous (benign) and cancerous (malig-\nnant) nodules holds paramount importance for clinical treatment. Computed-aided diagnosis\n(CAD) systems [ 2] are widely employed for nodule detection. However, nodule classiﬁcation\nremains a challenging task for CAD systems.\nCurrent clinical classiﬁcation algorithms require hundreds of images per patient, making\nit extremely challenging for a radiologist to perform precise and thorough analysis of all the\nimages. To assist with disease diagnosis, radiomics [ 3] have gained popularity in extracting\nfeatures from medical images, thanks to the advancements in computer systems [ 4, 5]. This\nidentiﬁcation scheme typically comprises four stages: preprocessing, segmentation, feature\nextraction, and classiﬁcation. However, accurately segmenting lung lesion annotations for\nfeature extraction in radiomics can be costly for doctors. Given the signiﬁcant advancements\nin deep learning in recent years, we have investigated two deep learning approaches to identify\nlung cancer in this study. One of the key beneﬁts of using deep learning methods is that the\ndeep neural network considers context information from consecutive slices of a patient’s\nlungs, eliminating the need for manual segmentation of lung lesions, which is time-saving\nand convenient. As illustrated in Fig. 1, the three crucial stages in this study involve data\npreprocessing, feature extraction, and classiﬁcation.\nOne of the approaches employed in this study involves the utilization of three-dimensional\nconvolutional neural networks (3-D CNNs), which are considered the leading deep learning\napproach in automatic medical diagnosis [ 6–9]. The other approach utilizes vision transform-\ners (ViTs) [ 10], which have been extensively utilized in natural language processing (NLP)\nand have emerged as a viable alternative to CNNs in computer vision, yielding comparable\nperformance levels to those achieved by CNNs.\nHowever, medical images are signiﬁcantly different from natural images. For example,\nnatural images are two-dimensional (2-D) RGB (Red, Green, Blue) images, while medical\nimages are three-dimensional (3-D) grayscale images. Dosovitskiy et al. [ 10] trained and\ntested ViTs with 2-D natural images such as ImageNet [ 11]. Their results showed that ViTs\ncould only outperform CNNs on very large-scale datasets. For instance, ViTs performed better\nafter pre-training on JFT-300M, a 300 million-record proprietary dataset from Google. Thus,\ninvestigating the classiﬁcation performance of ViTs on medical images is an urgent issue\nthat needs to be addressed. Our research tackles the challenge of a relatively modest medical\ndataset consisting of 212 cases. This is a departure from the large-scale datasets traditionally\nused in ViTs experiments. The exploration of ViTs’ performance on a smaller dataset is a\nnotable contribution, as it reﬂects real-world scenarios where medical data availability is\noften limited. To bridge the performance gap caused by limited data, we trained ViTs and\nCNNs using various methodologies to enhance their performance, as detailed in Section 2.2.\nIn essence, our research ventures into uncharted territory by evaluating the suitability of\nViTs for medical image classiﬁcation, acknowledging the data-speciﬁc challenges and offer-\ning various strategies to address them. It underscores the importance of adapting cutting-edge\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59255\nFig. 1 The main stages of lung cancer identiﬁcation involve taking 3-D CT scans as input, preprocessing the\ndata, and comparing two deep learning approaches based on CNNs and ViTs to generate a prediction indicating\nthe patient’s lung cancer probability\ndeep learning techniques for medical applications, ultimately contributing to the broader ﬁeld\nof lung cancer identiﬁcation.\n2 Materials and methods\n2.1 Model architecture\nWith the rapid development of deep learning in recent years, CNNs have been the controlling\napproach in the domain of computer vision, with applications ranging from classiﬁcation [ 12–\n16], segmentation [ 17–19] and object detection [ 20–23]. In recent years, many deep neural\nnetwork architectures have demonstrated outstanding performance in image classiﬁcation\ntasks, such as VGG [ 24], GoogLeNet [25], ResNet [26], and EfﬁcientNet [ 27]. When contem-\nplating the implementation of a lung cancer identiﬁcation system, EfﬁcientNet3D emerges\nas a commendable option for processing 3-D medical images. This choice is supported by its\ncomparable parameter count to ViT, along with its outstanding performance on the ImageNet\ndataset, establishing it as a state-of-the-art deep learning method.\n2.1.1 EfﬁcientNet\nThe inference accuracy and speed of a CNN-based model are greatly affected by CNN’s\ndepth, width and resolution. Tan and Le [ 27] propose a new scaling method that uniformly\n123\n59256 Multimedia Tools and Applications (2024) 83:59253–59269\nFig. 2 The EfﬁcientNet3D-b0 architecture\nscales all dimentions of depth/width/resolution using a simple yet highly effective compound\ncoefﬁcient. The scaling method equation can be described as:\ndepth : d = αφ\nwidth : w = βφ\nresolution : r = γφ\ns.t.α · β2 · γ2 ≈ 2\nα ≥ 1,β ≥ 1,γ ≥ 1\n(1)\nwhere α, β,a n d γ indicate how to assign these additional resources to the width, depth, and\nresolution of the network, and φ is a user-speciﬁed coefﬁcient that governs how many more\nresources are available for the model scaling.\nFigure 2 shows the architecture of EfﬁcientNet3D-b0 which we used in this work. Efﬁ-\ncientNet3D consists of a baseline network model that is expanded by adding more layers,\nwidening the network, and increasing the input image resolution. Compared to other popular\narchitectures, this mixed-dimension scaling method allows EfﬁcientNet3D to achieve higher\naccuracy with fewer parameters and lower computational costs. EfﬁcientNet has achieved\nstate-of-the-art results on multiple image classiﬁcation tasks, including ImageNet, CIFAR-\n10, and CIFAR-100, and has been widely adopted in many computer vision applications.\nOverall, it is appropriate for the deployment of lung cancer identiﬁcation system.\n2.1.2 Vision transformers\nViTs have recently garnered signiﬁcant interest in the realm of natural image processing,\nserving as a compelling alternative to CNNs. The transformer architecture, a fundamental\ndeep neural network for processing sequential input data, has found widespread use in NLP .\nThe transformer encoder framework can be characterized as a series of similar layers, as\nexempliﬁed in Fig. 3. Each of these layers is comprised of two sub-layers: a multi-head\nself-attention mechanism and a simple multi-layer perceptron (MLP) of fully-connected\nfeed-forward networks. In addition to these sub-layers, a residual connection [ 26] surrounds\neach one, culminating in layer normalization [ 28].\nAttention function A ss h o w ni nF i g .3, the query and key-value pairs can be translated into\nan output by the general attention function, where the query, key, value, and output are all\nvectors. The input of each layer in the encoder includes queries and keys with dimention\nd\nk , and values with dimention dv. It computes the dot products of the query with all keys,\ndivided each by √dk , and computes the value’s dot products with the associated weights to\nget the output after applying a softmax function to get the weights on the value. In practice,\na set of queries are packed together into a matrix Q as well as keys and values with matrices\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59257\nFig. 3 The transformer encoder\narchitecture\nK and V respectively. The output of attention can be represented like:\nQ = W Q ⊙ Xq\nK = W K ⊙ Xk\nV = W V ⊙ Xv (2)\nAttention (Q, K , V ) = sof tmax ( QK T\n√dk\n)V\nwhere Xq , Xk , Xv are the inputs of transformer encoder, ⊙ denotes matrix dot product and\nW Q , W K , W V are the parameter matrices for linear projection. If the Q, K and V are\ncomputed from the same input X, it is known as self-attention. Speciﬁcally, the multi-head\nself-attention can be described as:\nMultiHead (Q, K , V ) = Concat (head1, ...,head h )W O\nwhere headi = Attention (QW Q\ni , KW K\ni , VW V\ni ) (3)\nwhere the linear projections are parameter matrices W Q\ni ∈ Rdmodel ×dk , W K\ni ∈ Rdmodel ×dk ,\nW V\ni ∈ Rdmodel ×dv and W O\ni ∈ Rhdv×dmodel . For example, Deit-S [ 29] takes h = 6 heads and\nfor each head dk = dv = dmodel /h = 64.\nThe ViTs model has gained popularity in natural image processing alongside CNNs, owing\nto its ability to simultaneously examine data from multiple representation subspaces at various\nlocations through multi-head attention. Furthermore, it provides an integrated saliency map\nthat identiﬁes areas of focus by the model, which is difﬁcult to achieve with CNNs [ 30].\n123\n59258 Multimedia Tools and Applications (2024) 83:59253–59269\nFig. 4 The vision transformer architecture. We take 16X16X16 size of tokens in the patch embedding and\nﬂatten them into 1X4096 size of vector as we use 3-D CT scans for the lung cancer identiﬁcation\nAdditionally, the parallel attention heads structure allow for training on large-scale datasets\nand reduce training time.\nAn overview of the ViTs architecture is illustrated in Fig. 4. The ﬁrst two layers are patch\nembedding and position embedding. In our methodology, we initially divided the CT images\ninto patches of size 16X16X16, following the resampling process, which transformed the\nimages into dimensions of 224X224X64. Subsequently, these patches were ﬂattened into\na single 1x4096-sized vector through patch embedding. The representation of the patch\nembedding is illustrated as follows:\np\ni = Flatten(xi )\nqi = F (pi )\n(4)\nwhere xi is the ith patch, Flatten (·) denotes ﬂattening the patch into 1X4096 size and F (·)\ndenotes projecting the vector into 1X384 size.\nPosition embedding In images, the spatial arrangement of pixels is crucial for understand-\ning the content. Unlike text data, where the order of words is explicitly represented by their\npositions in the sequence, images inherently lack this positional information. Position embed-\nding provides ViTs with a way to incorporate spatial relationships between pixels or patches\nin the image. It is typically added to the patch embedding and help the model understand the\nrelative positions of different image elements. To learn the positional information between\ntokens, we add 1-D positional embedding for all the tokens which can be described as:\nZ\nout = concat(qclass ;q1;q2;··· ; q N ) + E pos (5)\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59259\nwhere qclass is class token, qi is patch token and E pos is a learnable matrice which is initialized\nwith zero.\nClass token The class token is a unique token added to the patch embedding that serves as\na global representation of the entire image. In NLP tasks, the transformer model typically\nuses a special token(e.g., [CLS]) to represent the entire sequence or document. Similarly, as\ns h o w ni nF i g .4, the class token represents the overall content of the image. It provides a\nhigh-level context and is crucial for image classiﬁcation tasks where the goal is to assign a\nsingle label to the entire image.\nIn image processing tasks, ViTs have several advantages compared to traditional CNNs:\n1. ViTs provide contextual features by utilizing self-attention mechanisms on input seri-\nalized data, which helps capture relationships between different positions in the input,\nleading to more accurate image processing.\n2. ViTs’ parallelized network architecture allows them to be trained on larger datasets and\nsigniﬁcantly reduces training time. This enables ViTs to efﬁciently handle large-scale\nimage datasets.\n3. ViTs come with built-in attention maps, which provide insights into the regions of interest\nthat the model focuses on when making decisions. This enhances interpretability and\nhelps understand how the model operates.\nApplying ViTs to computer vision has potential advantages as it allows the use of the same\ndeep neural network model for both computer vision and natural language processing tasks,\nfacilitating developments in the multimodal ﬁeld. Therefore, exploring the classiﬁcation\nperformance of ViTs on medical images is of great signiﬁcance, as it is expected to offer\nimproved performance and interpretability, thereby enhancing the analysis and diagnosis of\nmedical images.\n2.2 Training strategies\nViTs have been widely used in the ﬁeld of natural image processing, but their utility in\nthe ﬁeld of medical images is limited since the medical datasets typically have insufﬁcient\nquantities and frequently come with less trustworthy labels. Therefore, in order to enhance\nthe performance of CNNs and ViTs, we additionally investigate three training methodologies\ninclude transfer learning, self-supervised learning and sharpness-aware minimizer.\nThe ﬁrst approach is transfer learning, which entails utilizing pre-existing weights that\nhave undergone training on ImageNet and ﬁne-tuning for optimal performance on the own\ndataset. According to Raghu et al. [ 31], transfer learning has been demonstrated to increase\nthe overall efﬁcacy of the model, as well as to signiﬁcantly reduce the duration of the training\nprocess. However, the scarcity of ample datasets for 3-D medical images imposes constraints\non the application of transfer learning for 3-D CNNs, because ImageNet is of 2-D natural\nimages. As an alternative, we can adopt the use of ViTs, where only the patch embedding and\nposition embedding are initialized from scratch, while the remaining layers are initialized\nwith pre-trained weights from ImageNet.\nRecent advances in self-supervised learning (SSL) have improved the deep neural net-\nwork’s ability to express unlabeled data. For instance, BYOL [ 32] utilizes two neural\nnetworks, namely the online and target networks, which interact and learn from each other,\nand achieves a state-of-the-art performance on ImageNet using a standard ResNet-50 [ 33].\nHua et al. [ 34] propose a progressive hierarchical feature alignment method to solve the\n123\n59260 Multimedia Tools and Applications (2024) 83:59253–59269\nimbalance problems of data and classes. Michieli et al. [ 35] propose an Unsupervised Domain\nAdaptation strategy to learn on labeled synthetic data and unlabeled real data. Their results\nprove the effectiveness of the proposed strategy in adapting a segmentation network trained\non synthetic datasets to real world datasets. The difference between ssl and them is that\nwe do unsupervised training on a real world dataset and then use the learned knowledge\nin supervised learning tasks. DINO [ 36] uses self-distillation to encourage a student and\nteacher network to produce similar representations given inputs of different augmentations,\nand achieves the highest performance on ImageNet and other standard benchmarks with\nViTs. Therefore, we also employ SSL to train all models and assess whether it beneﬁts the\nidentiﬁcation of lung cancer.\nFurthermore, it should be noted that the training of ViTs is sensitive to initialization\nand hyperparameter selection, as pointed out by Touvron et al. [ 29], which may result in\nlow training error but high generalization error during evaluation [ 37, 38]. To address this\nissue, Chen et al. [ 39] propose the use of the Sharpness-Aware Minimizer (SAM) optimiza-\ntion technique to achieve smoother loss landscapes for each network component and higher\nweight norms in ViTs. Additionally, ViTs trained from scratch on ImageNet, with limited\npre-training and data augmentation, have demonstrated superior performance compared to\nsimilarly sized ResNets, aligning with the characteristics of 3-D medical image datasets.\nThus, we also incorporate the SAM optimization method to improve the accuracy of lung\ncancer identiﬁcation.\n2.3 Dataset\nAs outlined in Table 1, our dataset was meticulously collected from the Shandong Provin-\ncial Hospital, afﬁliated with Shandong First Medical University, spanning the period from\nSeptember 2018 to December 2020. This comprehensive dataset comprises a total of 212\ncases, consisting of 130 malignant and 82 benign tumor cases. Figure 5 provides a visual\nrepresentation of the lesions pertaining to each lung cancer category. Within this ﬁgure, the\nred circle is employed to denote the largest lesion in the dataset, based on its diameter.\nTable 1 Detail information about\nthe lung cancer dataset Category Malignant cases Benign cases\nAIS 27 −\nMIA 36 −\nIAC 67 −\nHamartoma − 21\nInﬂammatory pseudotumor − 11\nAtypical hyperplasia − 9\nGranuloma − 5\nBenign tumor − 10\nPneumonia − 26\nTotal 130 82\nAdenocarcinoma In Situ (AIS), Minimally Invasive Adenocarcinoma\n(MIA), Inﬁltrative Adenocarcinoma (IAC) are considered of malignant\ntumor with total 130 cases. Hamartoma, Inﬂammatory pseudotumor,\nAtypical hyperplasia, Granuloma, Benign tumor, Pneumonia are con-\nsidered of benign tumor with total 82 cases\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59261\nFig. 5 The imaging shows of each lung cancer categories. (a) AIS: a ground-glass nodule in the lateral segment\nof the right middle lung lobe. (b) MIA: a ground-glass density foci in the posterior segment of the left upper\nlung lobe. (c) IAC: a ground-glass nodule in the apical segment of the right upper lung lobe. (d) Hamartoma: a\nround high-density nodule in the dorsal segment of the right lower lung lobe. (e) Inﬂammatory pseudotumor:\na nodule in the dorsal segment of the left lower lung lobe. (f) Atypical hyperplasia: a ground-glass density\nnodule in the anterior medial basal segment of the left lower lung lobe. (g) Granuloma: an anterior segment\nnodule in the apical segment of the right upper lung lobe. (h) Benign tumor: a lobular nodule in the basal\nsegment of the right lower lung lobe. (i) Pneumonia: a soft tissue nodule in the lateral segment of the right\nmiddle lung lobe\nEach entry in our dataset corresponds to a CT scan with dimensions of 512 ×512×z,w h e r e\nz represents the number of slices acquired for each scan. It’s important to note that all the\ncases included in our dataset exhibited imaging ﬁndings indicative of lung cancer, warranting\nthe necessity for surgical resection. The lesions observed in these cases exhibited a range in\nterms of their longest diameter, spanning from 0.6 cm to 3 cm. Moreover, it is imperative\nto highlight that each of these lesions underwent surgical removal and was subsequently\nconﬁrmed through pathological examination.\nWith regard to the classiﬁcation of lung tumors in our dataset, they can be categorized as\nfollows:\n• Malignant. If the tumor is malignant, it is an uncontrollable level of cancer tumor and\nis no cure in this category.\n• Benign. If the tumor is benign, it is starting level of cancer tumor and is easy to cure in\nthis category.\n123\n59262 Multimedia Tools and Applications (2024) 83:59253–59269\nFig. 6 Preprocessing CT lung images processing. (a) Original image. (b) Image after undergoing Hounsﬁeld\nscale change, normalization, and zero centering\n3 Experiments\n3.1 Preprocessing\nThe preprocessing of lung CT scans is undertaken with the aim of improving the quality of\nthe scans, thus achieving superior outcomes in the diagnosis of lung cancer. As depicted in\nFig. 6, the Hounsﬁeld scale (HU) of the CT scans was adjusted to fall between -1000 and\n1000, after which normalization was carried out to maintain image values within the range of\n0 to 255. Subsequently, all CT scans were resized to size of 224X224X64, following which\nzero centering was applied to each slice of the scans. The zero centering is described as ( 6):\nx = x − μ\nσ (6)\nwhere μ is the mean intensity of one slice and σ is the standard deviation intensity of one\nslice. A 7 : 3 random split was made for the training and validation. We also used Rando-\nmAnisotropy, random V erticalFlip, random HorizontalFlip, RandomBlur and RandomAfﬁne\nas the medical data augmentations [ 40] before training the models.\n3.2 Training settings\nWe have opted DeiT-S [ 29] as the transformer-based model which is identical to the ViTs but\nhas been implemented in Pytorch [ 41]. In addition, we have chosen EfﬁcientNet3D-b0 [ 27]\nas the 3-D CNN-based model, as Deit-S and EfﬁcientNet3D-b0 have similar efﬁciency and\nparameter counts. The hyperparameters for training are presented in Table 2. Previous stud-\nies have demonstrated the advantages of using “LinearWarmup” [ 42] to train transformers.\nHence, we have utilized “LinearWarmup” of 10 epochs with an initial learning rate of 1X10\n−8\nand “CosineAnnealingLR” [ 43] with a ﬁnal learning rate of 1X10 −6 as the learning rate\nschedulers. Each experiment was conducted on a single host equipped with two TITAN RTX\nGPUs (24GB memory).\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59263\nTable 2 Hyperparameters for training\nMethods Models Epochs Base LR Batch size Weight decay Optimizer\nRandom EfﬁcientNet3D 300 1X 10 −3 16 1X 10 −6 Adam\nDeiT-S 300 1X 10 −4 16 1X 10 −6 Adam\nTransfer learning EfﬁcientNet3D 300 1X 10 −3 16 1X 10 −6 Adam\nDeiT-S 300 1X 10 −4 16 1X 10 −6 Adam\nSSL EfﬁcientNet3D 500 1X 10 −3 81 X 1 0 −6 Adam\nDeiT-S 500 1X 10 −4 81 X 1 0 −6 Adam\nSAM EfﬁcientNet3D 500 1X 10 −3 81 X 1 0 −5 AdamW\nDeiT-S 500 1X 10 −4 81 X 1 0 −5 AdamW\n3.3 Evaluation metrics\nIn order to evaluate the different models against each other, we choose accuracy, precision,\nrecall, F1 score and AUC as the metrics for the lung cancer identiﬁcation. Accuracy is the\npercentage of correct predictions out of all cases evaluated, which is described as:\nAccuracy = (TP + TN )/(TP + TN + FP + FN ) (7)\nwhere TP , TN , FP and FN are true positive, true negative, false positive and false negative\nof the confusion matrix [ 44]. Precision is described as\nPrecision = (TP )/(TP + FP ) (8)\nand it refers to the proportion of truely positive among the number of positive predictions.\nRecall is described as\nRecall = (TP )/(TP + FN ) (9)\nand it describes the proportion of the correctly classiﬁed among the actual positive cases.\nThe F1 score, which ranges from 0 to 1, represents the harmonic mean of recall and precision\nand is described as follows:\nF\n1 = 2 ∗ precision ∗ recall\nprecision + recall (10)\nThe area under the receiver operating characteristic curve, or AUC [ 45], demonstrates\nhow well the probabilities from the positive classes are separated from the negative classes.\nThe ROC curve is plotted by false positive rate (FPR) and true positive rate (TPR) with\nvarious threshold values, where FPR is simply the percentage of erroneous detections that\nthe algorithm is catching and can be calculated as (1 - speciﬁcity). It can be described as:\n1 − Specificity = FPR = FP /(TN + FP ) (11)\nTPR is just the percentage of trues that the algorithm is catching and is also known as\nsensitivity or recall, which can be described as:\nSensiti vity = TPR = Recall = TP /(TP + FN ) (12)\n123\n59264 Multimedia Tools and Applications (2024) 83:59253–59269\nTable 3 ViT and CNNs comparisons using various training methods for the validation measures\nMethods Models Accuracy Precision Recall F1 AUC\nRandom EfﬁcientNet3D 0.891 0.895 0.874 0.882 0.952\nDeiT-S 0.812 0.805 0.817 0.808 0.892\nTransfer learning EfﬁcientNet3D −−− − −\nDeiT-S 0.875 0.874 0.862 0.867 0.953\nSSL EfﬁcientNet3D 0.938 0.934 0.934 0.934 0.981\nDeiT-S 0.875 0.867 0.876 0.870 0.957\nSAM EfﬁcientNet3D 0.953 0.954 0.947 0.950 0.974\nDeiT-S 0.859 0.885 0.827 0.842 0.965\nNotably, as ImageNet only contains 2-D natural images and 3-D CNNs checkpoints are not yet available, we\ndo not include EfﬁcientNet3D with weights pre-trained on ImageNet\n4 Results\nIn Table 3, we compare Deit-S against EfﬁcientNet3D using different training strategies.\nInitially, the AUC (95.2%) of EfﬁcientNet3D greatly exceeded that of Deit-S (89.2%) when\ntrained with randomly initialized weights (Kaiming initialization [ 46]). However, in medical\nimaging recognition tasks, random initialization is no longer the standard practice. Instead,\nthe standard procedure is transfer learning. As shown in Table 3, transfer learning increased\nthe AUC of Deit-S from 89.2% to 95.3%. This suggests that Deit-S beneﬁted signiﬁcantly\nfrom transfer learning for lung cancer identiﬁcation, and achieved similar performance to\nEfﬁcientNet3D.\nWe also compared Deit-S against EfﬁcientNet3D with SSL and SAM in Table 3. Results\nshowed that pre-training EfﬁcientNet3D with SSL increased its AUC from 95.2% to 98.1%,\nand Deit-S from 89.2% to 95.7%. Additionally, SAM improved the AUC of EfﬁcientNet3D\nfrom 95.2% to 97.4% and Deit-S from 89.2% to 96.5%. These ﬁndings suggest that both\nSSL and SAM can successfully enhance the capabilities of ViTs and CNNs for lung cancer\nidentiﬁcation, with ViTs exhibiting more improvement than CNNs under these settings.\nMoreover, Fig. 7 revealed that EfﬁcientNet3D achieved the highest score of AUC (98.1%)\nwhen pre-trained with SSL, and the highest recall score (94.7%) when trained with SAM.\nHowever, ViTs could not achieve the same level of performance as CNNs, even when trained\nwith SSL or SAM.\n5 Discussion\nThe results presented in Table 3 highlight the signiﬁcant impact of different training strategies\non the performance of Deit-S and EfﬁcientNet3D in the task of lung cancer identiﬁcation.\nInitially, when both models were trained with randomly initialized weights, EfﬁcientNet3D\noutperformed Deit-S. However, when Deit-S was ﬁne-tuned using transfer learning with\npre-trained weights from ImageNet, its AUC improved substantially, reaching 95.3%. This\nsuggests that Deit-S can beneﬁt signiﬁcantly from transfer learning in the context of lung\ncancer identiﬁcation and achieve performance comparable to EfﬁcientNet3D.\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59265\nFig. 7 The confusion matrix with the evaluation results of EfﬁcientNet3D\nThe comparison between Deit-S and EfﬁcientNet3D with SSL and SAM further illu-\nminates the impact of advanced training techniques. Pre-training both models with SSL\nresulted in substantial improvements in AUC. Additionally, the incorporation of SAM led\nto enhanced AUC scores for both models, with EfﬁcientNet3D achieving 97.4% and Deit-\nS reaching 96.5%. These ﬁndings indicate that SSL and SAM can effectively enhance the\ncapabilities of both ViTs and CNNs for lung cancer identiﬁcation. Notably, ViTs exhibited a\nmore signiﬁcant improvement in performance compared to CNNs under these settings.\nFigure 7 provides additional insights into the models’ performance, particularly in terms of\nconfusion matrix. EfﬁcientNet3D pre-trained with SSL achieved the highest AUC of 98.1%,\ndemonstrating its potential when leveraged with SSL techniques. On the other hand, when\ntrained with SAM, EfﬁcientNet3D exhibited the highest recall score of 94.7%, indicating\nits superior ability to detect lung cancer cases. However, it is essential to highlight that\nViTs, including Deit-S, did not achieve the same level of performance as CNNs, even when\ntrained with SSL or SAM. This discrepancy in performance could be attributed to the limited\navailability of training data. It aligns with the theory that ViTs, lacking inherent “inductive\nbias”, may struggle to generalize effectively when trained with a restricted dataset.\nIn summary, the results and discussions presented herein underscore the importance of\ntraining strategies in the performance of deep learning models for lung cancer identiﬁcation in\nmedical imaging. Transfer learning, SSL, and SAM have been demonstrated to be valuable\ntools for enhancing the capabilities of both ViTs and CNNs, though the training of ViTs\nis sensitive to initialization and hyperparameter selection. While ViTs show promise, their\nperformance may be limited by data availability, highlighting the need for further research\ninto mitigating this limitation and harnessing the full potential of ViTs in medical image\nanalysis.\n6 Interpretability of transformer\nThe Transformer-based model presents a unique advantage in its inherent interpretability, a\nfeature of paramount signiﬁcance for comprehending its inner workings and the rationale\nbehind its predictions. This interpretability is chieﬂy facilitated through the generation of\n123\n59266 Multimedia Tools and Applications (2024) 83:59253–59269\nFig. 8 Slice of original data and saliency maps of multi-head attention. As we use ViT-S, there are 6 heads in\nmulti-head attention block\nsaliency maps of attention, which effectively highlight the pivotal regions within the input\nimage that contribute signiﬁcantly to the model’s decision-making process. These saliency\nmaps are created by computing the self-attention weight scores associated with a transformer\nencoder layer, where these weight scores essentially quantify the degree of emphasis placed\non each individual image patch in relation to its counterparts.\nTo delve into the speciﬁcs of the saliency map generation process, we speciﬁcally extracted\nthe attention weights from the ﬁnal self-attention block of the transformer encoder, as depicted\nin Fig. 4. These attention weights serve as a testament to the model’s attentional focus and\nare instrumental in crafting the saliency maps. By computing the weighted sum of the feature\nmaps corresponding to each image patch, we effectively obtain a two-dimensional saliency\nmap with dimensions mirroring those of the input image patches.\nAs illustrated in Fig. 8, the resultant saliency maps generated for ViTs when trained with\nSSL reveal some intriguing insights. Notably, each attention head exhibits subtle variations\nin the patches it prioritizes during the decision-making process. However, it is evident that, in\nthe broader context, ViTs tend to concentrate their attention predominantly on the lung region\nwhen making predictions. This phenomenon aligns seamlessly with the distribution of tumor\nregions, which tend to be densely clustered within the lungs. Such nuanced attention maps,\nshowcasing distinct areas of focus across different heads and yet an overarching emphasis\non medically relevant regions, present a remarkable aspect of interpretability unique to ViTs.\nThis level of interpretability is challenging to attain when employing CNNs-based models.\nThis facet of interpretability assumes paramount importance in the context of employ-\ning artiﬁcial intelligence for medical diagnosis. It not only assists in demystifying the\nmodel’s decision-making process but also empowers researchers and medical practitioners\nwith invaluable insights into the reasoning and justiﬁcations underlying the AI model’s pre-\ndictions. This enhanced transparency aids in validating the model’s accuracy and facilitates\nﬁne-tuning and optimization for more robust and reliable medical applications.\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59267\n7 Conclusions\nIn the present study, we undertook the task of developing an automated system for early lung\ncancer identiﬁcation, employing two prominent deep learning approaches: Deit-S and Efﬁ-\ncientNet3D. Moreover, we delved into the utilization of critical training strategies, including\ntransfer learning, self-supervised learning, and sharpness-aware minimizer, to enhance the\nperformance of both ViTs and CNNs. Our research outcomes underscore the efﬁcacy of\nthese techniques in elevating the predictive power of these models, with CNNs achieving the\nhighest AUC of 98.1% when trained with SSL. This achievement highlights the potential of\nCNNs in identifying lung cancer with exceptional accuracy.\nNonetheless, it is crucial to acknowledge that our study also reveals certain limitations\nin the current state of ViTs for medical imaging tasks. Despite their promise, ViTs did not\nmatch the performance levels of CNNs in our experiments. This discrepancy emphasizes\nthe need for further research and development in leveraging ViTs effectively for medical\nimage analysis. It suggests that ViTs might require more substantial data resources or novel\narchitectural adaptations to fully exploit their capabilities in this domain. Looking ahead,\nfuture research efforts should concentrate on training ViTs and CNNs on larger-scale lung\ncancer datasets and other CNN models. The expectation is that, as the dataset size increases,\nViTs will progressively bridge the performance gap with CNNs and potentially surpass them\nin accuracy and robustness. This evolution promises to bring signiﬁcant beneﬁts to medical\npractitioners by enabling more precise and early diagnosis of lung cancer, ultimately saving\nlives through earlier interventions.\nIn conclusion, our study has shed light on the immense potential of deep neural networks,\nparticularly CNNs, for the task of lung cancer identiﬁcation based on medical CT images. We\nhave demonstrated the impact of advanced training strategies and highlighted the ongoing\nchallenges faced by ViTs in this context. The journey towards harnessing the full potential of\nViTs in medical imaging is an exciting frontier that holds the promise of even more accurate\nand accessible healthcare solutions in the future.\nAcknowledgements This study was supported by the Natural Science Foundation of China under\nGrant U1806202, Grant 61533011 and the Natural Science Foundation of Shandong Province of China\nZR2019BF035, ZR2020ZD25, ZR2021QF042, 2022CXGC10501.\nFunding Natural Science Foundation of Shandong Province of China (ZR2021MF057).\nData Availability The datasets generated during and/or analysed during the current study are available from\nthe corresponding author on reasonable request.\nDeclarations\nConﬂict of interest The authors declare that we have no conﬂict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\n123\n59268 Multimedia Tools and Applications (2024) 83:59253–59269\nReferences\n1. Tsao AS, Scagliotti GV , Bunn PA Jr, Carbone DP , Warren GW, Bai C, De Koning HJ, Y ousaf-Khan AU,\nMcWilliams A, Tsao MS (2016) Scientiﬁc advances in lung cancer 2015. J Thor Oncol 11(5):613–638\n2. Fraioli F, Serra G, Passariello R (2010) CAD (computed-aided detection) and CADX (computer aided\ndiagnosis) systems in identifying and characterising lung nodules on chest CT: overview of research,\ndevelopments and new prospects. La Radiol Med 115(3):385–402\n3. Kukreja V , Sakshi (2022) Machine learning models for mathematical symbol recognition: a stem to stern\nliterature analysis. Multimedia Tools Appl 81(20):28651–28687\n4. Vijaya G, Suhasini A, Priya R (2014) Automatic detection of lung cancer in CT images. IJRET: Int J Res\nEng Technol 3(7):182–186\n5. Sakshi, Kukreja V (2023) A dive in white and grey shades of ml and non-ml literature: a multivocal\nanalysis of mathematical expressions. Artif Intell Rev 56(7):7047–7135\n6. Tajbakhsh N, Shin JY , Gurudu SR, Hurst RT, Kendall CB, Gotway MB, Liang J (2016) Convolu-\ntional neural networks for medical image analysis: full training or ﬁne tuning? IEEE Trans Med Imag\n35(5):1299–1312\n7. Aurna NF, Y ousuf MA, Taher KA, Azad A, Moni MA (2022) A classiﬁcation of MRI brain tumor based\non two stage feature level ensemble of deep CNN models. Comput Biol Med 146:105539\n8. Rostami B, Anisuzzaman D, Wang C, Gopalakrishnan S, Niezgoda J, Y u Z (2021) Multiclass wound\nimage classiﬁcation using an ensemble deep CNN-based classiﬁer. Comput Biol Med 134:104536\n9. Deepak S, Ameer P (2019) Brain tumor classiﬁcation using deep CNN features via transfer learning.\nComput Biol Med 111:103345\n10. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer\nM, Heigold G, Gelly S et al (2020) An image is worth 16x16 words: transformers for image recognition\nat scale. arXiv:2010.11929\n11. Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet: a large-scale hierarchical image\ndatabase. Paper presented at the 2009 IEEE conference on computer vision and pattern recognition, pp\n248–255\n12. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classiﬁcation with deep convolutional neural\nnetworks. Adv Neural Inform Process Syst 25\n13. Hershey S, Chaudhuri S, Ellis DP , Gemmeke JF, Jansen A, Moore RC, Plakal M, Platt D, Saurous RA,\nSeybold B (2017) CNN architectures for large-scale audio classiﬁcation. Paper presented at the 2017\nIEEE international conference on acoustics, speech and signal processing (icassp), pp 131–135\n14. Bardou D, Zhang K, Ahmad SM (2018) Lung sounds classiﬁcation using convolutional neural networks.\nArtif Intell Med 88:58–69\n15. Kukreja V , Lodhi S et al (2023) Impact of varying strokes on recognition rate: a case study on handwritten\nmathematical expressions. Int J Comput Digit Sys\n16. Kukreja V (2021) A retrospective study on handwritten mathematical symbols and expressions: classiﬁ-\ncation and recognition. Eng Appl Artif Intell 103:104292\n17. Ronneberger O, Fischer P , Brox T (2015) U-net: convolutional networks for biomedical image segmenta-\ntion. Paper presented at the international conference on medical image computing and computer-assisted\nintervention, pp 234–241\n18. Sakshi Kukreja V (2023) Image segmentation techniques: statistical, comprehensive, semi-automated\nanalysis and an application perspective analysis of mathematical expressions. Archiv Computat Methods\nEng 30(1):457–495\n19. Çiçek Ö, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O (2016) 3d u-net: learning dense volumetric\nsegmentation from sparse annotation. Paper presented at the international conference on medical image\ncomputing and computer-assisted intervention, pp 424–432\n20. Girshick R (2015) Fast R-CNN. In: Proceedings of the IEEE international conference on computer vision,\npp 1440–1448\n21. Ren S, He K, Girshick R, Sun J (2015) Faster R-CNN: towards real-time object detection with region\nproposal networks. Adv Neural Inform Process Syst 28\n22. Redmon J, Divvala S, Girshick R, Farhadi A (2016) Y ou only look once: uniﬁed, real-time object detection.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 779–788\n23. Rosati R, Romeo L, Silvestri S, Marcheggiani F, Tiano L, Frontoni E (2020) Faster R-CNN approach for\ndetection and quantiﬁcation of DNA damage in COMET assay images. Comput Biol Med 123:103912\n24. Simonyan K, Zisserman A (2014) V ery deep convolutional networks for large-scale image recognition.\narXiv:1409.1556\n123\nMultimedia Tools and Applications (2024) 83:59253–59269 59269\n25. Szegedy C, Liu W, Jia Y , Sermanet P , Reed S, Anguelov D, Erhan D, V anhoucke V , Rabinovich A (2015)\nGoing deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp 1–9\n26. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp 770–778\n27. Tan M, Le Q (2019) Efﬁcientnet: rethinking model scaling for convolutional neural networks. In: Inter-\nnational conference on machine learning, pp 6105–6114\n28. Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv:1607.06450\n29. Touvron H, Cord M, Douze M, Massa F, Sablayrolles A, Jégou H (2021) Training data-efﬁcient image\ntransformers & distillation through attention. In: International conference on machine learning, pp 10347–\n10357\n30. V aswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A.N, Kaiser Ł, Polosukhin I (2017)\nAttention is all you need. Adv Neural Inform Process Syst 30\n31. Raghu M, Zhang C, Kleinberg J, Bengio S (2019) Transfusion: understanding transfer learning for medical\nimaging. arXiv:1902.07208\n32. Grill J-B, Strub F, Altché F, Tallec C, Richemond P , Buchatskaya E, Doersch C, Avila Pires B, Guo Z,\nGheshlaghi Azar M (2020) Bootstrap your own latent-a new approach to self-supervised learning. Adv\nNeural Inform Process Syst 33:21271–21284\n33. He K, Zhang X, Ren S, Sun J (2016) Identity mappings in deep residual networks. In: European conference\non computer vision, pp 630–645\n34. Hua Y , Yi D (2021) Synthetic to realistic imbalanced domain adaption for urban scene perception. IEEE\nTrans Ind Inform 18(5):3248–3255\n35. Michieli U, Biasetton M, Agresti G, Zanuttigh P (2020) Adversarial learning and self-teaching techniques\nfor domain adaptation in semantic segmentation. IEEE Trans Intell V eh 5(3):508–518\n36. Caron M, Touvron H, Misra I, Jégou H, Mairal J, Bojanowski P , Joulin A (2021) Emerging properties in\nself-supervised vision transformers. arXiv:2104.14294\n37. Hendrycks D, Dietterich T (2019) Benchmarking neural network robustness to common corruptions and\nperturbations. arXiv:1903.12261\n38. Hendrycks D, Basart S, Mu N, Kadavath S, Wang F, Dorundo E, Desai R, Zhu T, Parajuli S, Guo M (2021)\nThe many faces of robustness: a critical analysis of out-of-distribution generalization. In: Proceedings of\nthe IEEE/CVF international conference on computer vision, pp 8340–8349\n39. Chen X, Hsieh C-J, Gong B (2021) When vision transformers outperform resnets without pretraining or\nstrong data augmentations. arXiv:2106.01548\n40. Pérez-García F, Sparks R, Ourselin S (2021) Torchio: a python library for efﬁcient loading, preprocessing,\naugmentation and patch-based sampling of medical images in deep learning. Comput Methods Programs\nBiomed 106236. https://doi.org/10.1016/j.cmpb.2021.106236\n41. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga\nL et al (2019) Pytorch: an imperative style, high-performance deep learning library. Adv Neural Inform\nProcess Syst 32\n42. Liu L, Jiang H, He P , Chen W, Liu X, Gao J, Han J (2019) On the variance of the adaptive learning rate\nand beyond. arXiv:1908.03265\n43. Loshchilov I, Hutter F (2016) SGDR: Stochastic gradient descent with warm restarts. arXiv:1608.03983\n44. Powers DM (2020) Evaluation: from precision, recall and f-measure to roc, informedness, markedness\nand correlation. arXiv:2010.16061\n45. Fawcett T (2006) An introduction to roc analysis. Pattern Recogn Lett 27(8):861–874\n46. He K, Zhang X, Ren S, Sun J (2015) Delving deep into rectiﬁers: surpassing human-level performance\non imagenet classiﬁcation. In: Proceedings of the IEEE international conference on computer vision, pp\n1026–1034\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7857038974761963
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6954491138458252
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6902192831039429
    },
    {
      "name": "Transfer of learning",
      "score": 0.6335532665252686
    },
    {
      "name": "Lung cancer",
      "score": 0.5990344285964966
    },
    {
      "name": "Deep learning",
      "score": 0.5424280166625977
    },
    {
      "name": "Transformer",
      "score": 0.5228174924850464
    },
    {
      "name": "Machine learning",
      "score": 0.5035111308097839
    },
    {
      "name": "Recall",
      "score": 0.49489015340805054
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4424680471420288
    },
    {
      "name": "Receiver operating characteristic",
      "score": 0.42066165804862976
    },
    {
      "name": "Computed tomography",
      "score": 0.4182835817337036
    },
    {
      "name": "Radiology",
      "score": 0.19977155327796936
    },
    {
      "name": "Medicine",
      "score": 0.15491008758544922
    },
    {
      "name": "Pathology",
      "score": 0.14143431186676025
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154099455",
      "name": "Shandong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210163399",
      "name": "Shandong First Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116517",
      "name": "Shandong Provincial Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156555",
      "name": "Shandong Maternal and Child Health Hospital",
      "country": "CN"
    }
  ],
  "cited_by": 26
}