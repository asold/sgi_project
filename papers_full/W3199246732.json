{
  "title": "Distilling Linguistic Context for Language Model Compression",
  "url": "https://openalex.org/W3199246732",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2977702213",
      "name": "Geondo Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2308216759",
      "name": "Gyeong-Man Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171972029",
      "name": "Eunho Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3101731278",
    "https://openalex.org/W2963350559",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W4288375898",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W2907947679",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W2130519113",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W4288112596",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364–378\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n364\nDistilling Linguistic Context for Language Model Compression\nGeondo Park1 Gyeongman Kim1 Eunho Yang1,2\nKAIST1, Daejeon, South Korea\nAITRICS2, Seoul, South Korea\n{geondopark, gmkim, eunhoy}@kaist.ac.kr\nAbstract\nA computationally expensive and memory in-\ntensive neural network lies behind the re-\ncent success of language representation learn-\ning. Knowledge distillation, a major technique\nfor deploying such a vast language model\nin resource-scarce environments, transfers the\nknowledge on individual word representations\nlearned without restrictions. In this paper, in-\nspired by the recent observations that language\nrepresentations are relatively positioned and\nhave more semantic knowledge as a whole, we\npresent a new knowledge distillation objective\nfor language representation learning that trans-\nfers the contextual knowledge via two types of\nrelationships across representations: Word Re-\nlation and Layer Transforming Relation . Un-\nlike other recent distillation techniques for the\nlanguage models, our contextual distillation\ndoes not have any restrictions on architectural\nchanges between teacher and student. We vali-\ndate the effectiveness of our method on chal-\nlenging benchmarks of language understand-\ning tasks, not only in architectures of various\nsizes, but also in combination with DynaBERT,\nthe recently proposed adaptive size pruning\nmethod.\n1 Introduction\nSince the Transformer, a simple architecture based\non attention mechanism, succeeded in machine\ntranslation tasks, Transformer-based models have\nbecome a new state of the arts that takes over more\ncomplex structures based on recurrent or convo-\nlution networks on various language tasks, e.g.,\nlanguage understanding and question answering,\netc (Devlin et al., 2018; Lan et al., 2019; Liu et al.,\n2019; Raffel et al., 2019; Yang et al., 2019). How-\never, in exchange for high performance, these mod-\nels suffer from a major drawback: tremendous com-\nputational and memory costs. In particular, it is not\npossible to deploy such large models on platforms\nwith limited resources such as mobile and wearable\ndevices, and it is an urgent research topic with im-\npact to keep up with the performance of the latest\nmodels from a small-size network.\nAs the main method for this purpose, Knowl-\nedge Distillation (KD) transfers knowledge from\nthe large and well-performing network (teacher)\nto a smaller network (student). There have been\nsome efforts that distill Transformer-based mod-\nels into compact networks (Sanh et al., 2019; Turc\net al., 2019; Sun et al., 2019, 2020; Jiao et al., 2019;\nWang et al., 2020). However, they all build on the\nidea that each word representation is independent,\nignoring relationships between words that could be\nmore informative than individual representations.\nIn this paper, we pay attention to the fact\nthat word representations from language mod-\nels are very structured and capture certain\ntypes of semantic and syntactic relationships. -\nWord2Vec (Mikolov et al., 2013) and Glove (Pen-\nnington et al., 2014) demonstrated that trained em-\nbedding of words contains the linguistic patterns\nas linear relationships between word vectors. Re-\ncently, Reif et al. (2019) found out that the dis-\ntance between words contains the information of\nthe dependency parse tree. Many other studies also\nsuggested the evidence that contextual word rep-\nresentations (Belinkov et al., 2017; Tenney et al.,\n2019a,b) and attention matrices (Vig, 2019; Clark\net al., 2019) contain important relations between\nwords. Moreover, Brunner et al. (2019) showed\nthe vertical relations in word representations across\nthe transformer layers through word identiﬁability.\nIntuitively, although each word representation has\nrespective knowledge, the set of representations of\nwords as a whole is more semantically meaningful,\nsince words in the embedding space are positioned\nrelatively by learning.\nInspired by these observations, we propose a\nnovel distillation objective, termed Contextual\nKnowledge Distillation (CKD), for language tasks\nthat utilizes the statistics of relationships between\n365\nword representations. In this paper, we deﬁne\ntwo types of contextual knowledge: Word Rela-\ntion (WR) and Layer Transforming Relation (LTR).\nSpeciﬁcally, WR is proposed to capture the knowl-\nedge of relationships between word representations\nand LTR deﬁnes how each word representation\nchanges as it passes through the network layers.\nWe validate our method on General Language\nUnderstanding Evaluation (GLUE) benchmark and\nthe Stanford Question Answer Dataset (SQuAD),\nand show the effectiveness of CKD against the\ncurrent state-of-the-art distillation methods. To val-\nidate elaborately, we conduct experiments on task-\nagnostic and task-speciﬁc distillation settings. We\nalso show that our CKD performs effectively on a\nvariety of network architectures. Moreover, with\nthe advantage that CKD has no restrictions on stu-\ndent’s architecture, we show CKD further improves\nthe performance of adaptive size pruning method\n(Hou et al., 2020) that involves the architectural\nchanges during the training.\nTo summarize, our contribution is threefold:\n• (1) Inspired by the recent observations that\nword representations from neural networks\nare structured, we propose a novel knowledge\ndistillation strategy, Contextual Knowledge\nDistillation (CKD), that transfers the relation-\nships across word representations.\n• (2) We present two types of complementary\ncontextual knowledge: horizontal Word Rela-\ntion across representations in a single layer\nand vertical Layer Transforming Relation\nacross representations for a single word.\n• (3) We validate CKD on the standard lan-\nguage understanding benchmark datasets and\nshow that CKD not only outperforms the state-\nof-the-art distillation methods but boosts the\nperformance of adaptive pruning method.\n2 Related Work\nKnowledge distillation Since recently popu-\nlar deep neural networks are computation- and\nmemory-heavy by design, there has been a long\nline of research on transferring knowledge for the\npurpose of compression. Hinton et al. (2015) ﬁrst\nproposed a teacher-student framework with an ob-\njective that minimizes the KL divergence between\nteacher and student class probabilities. In the ﬁeld\nof natural language processing (NLP), knowledge\ndistillation has been actively studied (Kim and\nRush, 2016; Hu et al., 2018). In particular, after\nthe emergence of large language models based on\npre-training such as BERT (Devlin et al., 2018; Liu\net al., 2019; Yang et al., 2019; Raffel et al., 2019),\nmany studies have recently emerged that attempt\nvarious knowledge distillation in the pre-training\nprocess and/or ﬁne-tuning for downstream tasks\nin order to reduce the burden of handling large\nmodels. Speciﬁcally, Tang et al. (2019); Chia et al.\n(2019) proposed to distill the BERT to train the\nsimple recurrent and convolution networks. Sanh\net al. (2019); Turc et al. (2019) proposed to use the\nteacher’s predictive distribution to train the smaller\nBERT and Sun et al. (2019) proposed a method to\ntransfer individual representation of words. In addi-\ntion to matching the hidden state, Jiao et al. (2019);\nSun et al. (2020); Wang et al. (2020) also utilized\nthe attention matrices derived from the Transformer.\nSeveral works including Liu et al. (2020); Hou et al.\n(2020) improved the performance of other compres-\nsion methods by integrating with knowledge distil-\nlation objectives in the training procedure. In par-\nticular, DynaBERT (Hou et al., 2020) proposed the\nmethod to train the adaptive size BERT using the\nhidden state matching distillation. Different from\nprevious knowledge distillation methods that trans-\nfer respective knowledge of word representations,\nwe design the objective to distill the contextual\nknowledge contained among word representations.\nContextual knowledge of word representations\nUnderstanding and utilizing the relationships\nacross words is one of the key ingredients in lan-\nguage modeling. Word embedding (Mikolov et al.,\n2013; Pennington et al., 2014) that captures the\ncontext of a word in a document, has been tradi-\ntionally used. Unlike the traditional methods of\ngiving ﬁxed embedding for each word, the con-\ntextual embedding methods (Devlin et al., 2018;\nPeters et al., 2018) that assign different embed-\ndings according to the context with surrounding\nwords have become a new standard in recent years\nshowing high performance. Xia and Zong (2010)\nimproved the performance of the sentiment classiﬁ-\ncation task by using word relation, and Hewitt and\nManning (2019); Reif et al. (2019) found that the\ndistance between contextual representations con-\ntains syntactic information of sentences. Recently,\nBrunner et al. (2019) also experimentally showed\nthat the contextual representations of each token\nchange over the layers. Our research focuses on\n366\nknowledge distillation using context information\nbetween words and between layers, and to our best\nknowledge, we are the ﬁrst to apply this context\ninformation to knowledge distillation.\n3 Setup and background\nMost of the recent state-of-the-art language models\nare stacking Transformer layers which consist of\nrepeated multi-head attentions and position-wise\nfeed-forward networks.\nTransformer based networks. Given an input\nsentence with ntokens, X = [x1,x2,...,x n] ∈\nRdi×n, most networks (Devlin et al., 2018; Lan\net al., 2019; Liu et al., 2019) utilize the embed-\nding layer to map an input sequence of sym-\nbol representations X to a sequence of continu-\nous representations E = [e1,...,e n] ∈ Rde×n.\nThen, each l-th Transformer layer of the identical\nstructure takes the previous representations Rl−1\nand produces the updated representations Rl =\n[rl,1,rl,2,...,r l,n] ∈ Rdr×n through two sub-\nlayers: Multi-head Attention (MHA) and position-\nwise Feed Forward Network (FFN). The input at\nthe ﬁrst layer ( l = 1) is simply E. In MHA op-\neration where h separate attention heads are op-\nerating independently, each input token rl−1,i for\neach head is projected into a query qi ∈ Rdq ,\nkey ki ∈ Rdq , and value vi ∈ Rdv , typically\ndk = dq = dv = dr/h. Here, the key vectors\nand value vectors are packed into the matrix forms\nK = [k1,··· ,kn] and V = [v1,··· ,vn], respec-\ntively, and the attention valueai and output of each\nhead oh,i are calculated as followed:\nai = Softmax\n(\nKT ·qi√\ndq\n)\nand oh,i = V ·ai\nThe outputs of all heads are then concatenated\nand fed through the FFN, producing the single word\nrepresentation rl,i. For clarity, we pack attention\nvalues of all words into a matrix form Al,h =\n[a1,a2,..,a n] ∈Rn×n for attention head h.\nKnowledge distillation for Transformer. In\nthe general framework of knowledge distillation,\nteacher network (T) with large capacity is trained\nin advance, and then student network ( S) with\npre-deﬁned architecture but relatively smaller than\nteacher network is trained with the help of teacher’s\nknowledge. Speciﬁcally, given the teacher param-\neterized by θt, training the student parameterized\nby θs aims to minimize two objectives: i) the cross-\nentropy loss LCE between the output of the student\nnetwork S and the true label y and ii) the differ-\nence of some statistics LD between teacher and\nstudent models. Overall, our goal is to minimize\nthe following objective function:\nL(θs) =E\n[\nLCE+λLD\n(\nKt(X; θt),Ks(X; θs)\n)]\nwhere λcontrols the relative importance between\ntwo objectives. Here, K characterizes the knowl-\nedge being transferred and can vary depending on\nthe distillation methods, and LD is a matching loss\nfunction such as l1, l2 or Huber loss. Recent studies\non knowledge distillation for Transformer-based\nBERT can also be understood in this general frame-\nwork. In particular, each distillation methods of\nprevious works are summarized in Appendix A.\n4 Contextual Knowledge Distillation\nWe now present our distillation objective that trans-\nfers the structural or contextual knowledge which\nis deﬁned based on the distribution of word repre-\nsentations. Unlike previous methods distilling each\nword separately, our method transfers the informa-\ntion contained in relationships between words or\nbetween layers, and provides a more ﬂexible way of\nconstructing embedding space than directly match-\ning representations. The overall structure of our\nmethod is illustrated in Figure 1(a). Speciﬁcally,\nwe design two key concepts of contextual knowl-\nedge from language models: Word Relation-based\nand Layer Transforming Relation-based contextual\nknowledge, as shown in Figure 1(b).\n4.1 Word Relation (WR)-based Contextual\nKnowledge Distillation\nInspired by previous studies suggesting that neural\nnetworks can successfully capture contextual rela-\ntionships across words (Reif et al., 2019; Penning-\nton et al., 2014; Mikolov et al., 2013), WR-based\nCKD aims to distill the contextual knowledge con-\ntained in the relationships across words at certain\nlayer. The “relationship” across a set of words can\nbe deﬁned in a variety of different ways. Our work\nfocuses on deﬁning it as the sum of pair-wise and\ntriple-wise relationships. Speciﬁcally, for each in-\nput X with nwords, let Rl = [rl,1,···rl,n] be the\nword representations at layer lfrom the language\nmodel (it could be teacher or student), as described\nin Section 3. Then, the objective of WR-based CKD\n367\n(a)\n (b)\nFigure 1: Overview of our contextual knowledge distillation. (a) In the teacher-student framework, we deﬁne the two\ncontextual knowledge, word relation and layer transforming relation which are the statistics of relation across the words from the\nsame layer (orange) and across the layers for the same word (turquoise), respectively. (b) Given the pair-wise and triple-wise\nrelationships of WR and LTR from teacher and student, we deﬁne the objective as matching loss between them.\nis to minimize the following loss:\nLCKD−WR =\n∑\n(i,j)∈χ2\nwij LD\n(\nφ(rs\ni,rs\nj),φ(rt\ni,rt\nj)\n)\n+ λWR\n∑\n(i,j,k)∈χ3\nwijk LD\n(\nψ(rs\ni,rs\nj,rs\nk),ψ(rt\ni,rt\nj,rt\nk)\n)\n(1)\nwhere χ = {1,...,n }. The function φand ψde-\nﬁne the pair-wise and triple-wise relationships, re-\nspectively and λWR adjust the scales of two losses.\nHere, we suppress the layer index lfor clarity, but\nthe distillation loss for the entire network is sim-\nply summed for all layers. Since not all terms in\nEq. (1) are equally important in deﬁning contextual\nknowledge, we introduce the weight valueswij and\nwijk to control the weight of how important each\npair-wise and triple-wise term is. Determining the\nvalues of these weight values is open as an imple-\nmentation issue, but it can be determined by the\nlocality of words (i.e. wij = 1if |i−j|≤ δand\n0, otherwise), or by attention information A to fo-\ncus only on relationship between related words. In\nthis work, we use the locality of words as weight\nvalues.\nWhile functions φand ψdeﬁning pair-wise and\ntriple-wise relationship also have various possibil-\nities, the simplest choices are to use the distance\nbetween two words for pair-wise φand the angle\nby three words for triple-wise ψ, respectively.\nPair-wise φ via distance. Given a pair of\nword representations (ri,rj) from the same layer,\nφ(ri,rj) could be deﬁned as cosine distance:\ncos (ri,rj) or l2 distance: ∥ri −rj∥2.\nTriple-wise ψ via angle. Triple-wise relation\ncaptures higher-order structure and provides more\nﬂexibility in constructing contextual knowledge.\nOne of the simplest forms for ψis the angle, which\nis calculated as\nψ(ri,rj,rk) = cos∠(ri,rj,rk)\n=\n⟨ ri −rjri −rj\n\n2\n, rk −rjrk −rj\n\n2\n⟩\n(2)\nwhere ⟨·,·⟩denotes the dot product between two\nvectors.\nDespite its simple form, efﬁciently computing\nthe angles in Eq. (2) for all possible triples out\nof n words requires storing all relative represen-\ntations (ri −rj) in a (n,n,d r) tensor1. This in-\ncurs an additional memory cost of O(n2dr). In this\ncase, using locality for wijk in Eq. (1) mentioned\nabove can be helpful; by considering only the\ntriples within a distance of δfrom rj, the additional\nmemory space required for efﬁcient computation\nis O(δndr), which is beneﬁcial for δ≪n. It also\nreduces the computation complexity of comput-\ning triple-wise relation fromO(n3dr) to O(δ2ndr).\n1From the equation ∥ri −rj∥2\n2 = ∥ri∥2\n2 + ∥rj∥2\n2 −\n2⟨ri,rj⟩, computing the pair-wise distance with the right hand\nside of equation requires no additional memory cost.\n368\nMoreover, we show that measuring angles in local\nwindow is helpful in the performance in the experi-\nmental section.\n4.2 Layer Transforming Relation (LTR)\n-based Contextual Knowledge Distillation\nThe second structural knowledge that we propose to\ncapture is on “how each word is transformed as it\npasses through the layers\". Transformer-based lan-\nguage models are composed of a stack of identical\nlayers and thus generate a set of representations for\neach word, one for each layer, with more abstract\nconcept in the higher hierarchy. Hence, LTR-based\nCKD aims to distill the knowledge of how each\nword develops into more abstract concept within\nthe hierarchy. Toward this, given a set of representa-\ntions for a single word inLlayers, [rs\n1,w,··· ,rs\nL,w]\nfor student and [rt\n1,w,··· ,rt\nL,w] for teacher (Here\nwe abuse the notation and {1,...,L }is not nec-\nessarily the entire layers of student or teacher. It\nis the index set of layers which is deﬁned in align-\nment strategy; this time, we will suppress the word\nindex below), the objective of LTR-based CKD is\nto minimize the following loss:\nLCKD−LTR =\n∑\n(l,m)∈ρ2\nwlm LD\n(\nφ(rs\nl,rs\nm),φ(rt\nl,rt\nm)\n)\n+ λLTR\n∑\n(l,m,o)∈ρ3\nwlmo LD\n(\nψ(rs\nl,rs\nm,rs\no),ψ(rt\nl,rt\nm,rt\no)\n)\n(3)\nwhere ρ = {1,...,L }and λLTR again adjust the\nscales of two losses. Here, the composition of\nEq. (3) is the same as Eq. (1), but only the ob-\njects for which the relationships are captured have\nbeen changed from word representations in one\nlayer to representations for a single word in lay-\ners. That is, the relationships among representa-\ntions for a word in different layers can be deﬁned\nfrom distance or angle as in Eq. (2): φ(rl,rm) =\ncos(rl,rm) or ∥rl −rm∥2 and ψ(rl,rm,ro) =\n⟨ rl−rm\n∥rl−rm∥2\n, ro−rm\n∥ro−rm∥2\n⟩.\nAlignment strategy. When the numbers of lay-\ners of teacher and student are different, it is impor-\ntant to determine which layer of the student learns\ninformation from which layer of the teacher. Pre-\nvious works (Sun et al., 2019; Jiao et al., 2019)\nresolved this alignment issue via the uniform (i.e.\nskip) strategy and demonstrated its effectiveness\nexperimentally. For Lt-layered teacher and Ls-\nlayered student, the layer matching function f is\ndeﬁned as\nf(steps ×t) = stept ×t, for t= 0,...,g\nwhere gis the greatest common divisor of Lt and\nLs, stept = Lt/gand steps = Ls/g.\nOverall training objective. The distillation ob-\njective aims to supervise the student network with\nthe help of teacher’s knowledge. Multiple distil-\nlation loss functions can be used during training,\neither alone or together. We combine the proposed\nCKD with class probability matching (Hinton et al.,\n2015) as an additional term. In that case, our overall\ndistillation objective is as follows:\nL= LD\nlogit + λCKD\n(\nLCKD−LTR + LCKD−WR\n)\nwhere λCKD is a tunable parameter to balance the\nloss terms.\n4.3 Architectural Constraints in Distillation\nObjectives\nState-of-the-art knowledge distillation objectives\ncommonly used come with constraints in design-\ning student networks since they directly match\nsome parts of the teacher and student networks\nsuch as attention matrices or word representations.\nFor example, DistilBERT (Sanh et al., 2019) and\nPKD (Sun et al., 2019) match each word represen-\ntation independently using their cosine similarities,∑n\ni=1 cos(rt\nl,i,rs\nl,i), hence the embedding size of\nstudent network should follow that of given teacher\nnetwork. Similarly, TinyBERT (Jiao et al., 2019)\nand MINI-LM (Wang et al., 2020) match the at-\ntention matrices via ∑H\nh=1 KL(At\nl,h,As\nl,h). There-\nfore, we should have the same number of attention\nheads for teacher and student networks (see Ap-\npendix A for more details on diverse distillation\nobjectives).\nIn addition to the advantage of distilling contex-\ntual information, our CKD method has the advan-\ntage of being able to select the student network’s\nstructure more freely without the restrictions that\nappear in existing KD methods. This is because\nCKD matches the pair-wise or triple-wise rela-\ntionships of words from arbitrary networks (stu-\ndent and teacher), as shown in Eq. (1), so it is\nalways possible to match the information of the\nsame dimension without being directly affected\nby the structure. Thanks to this advantage, in the\nexperimental section, we show that CKD can fur-\nther improve the performance of recently proposed\n369\nTable 1: Comparisons for task-agnostic distillation. For the task-agnostic distillation comparison, we do not use task-speciﬁc\ndistillation for a fair comparison. The results of TinyBERT and Truncated BERT are ones reported in Wang et al. (2020). Other\nresults are as reported by their authors. We exclude BERT-of-Theseus since the authors do not consider task-agnostic distillation.\nResults of development set are averaged over 4 runs. “-\" indicates the result is not reported in the original papers and the trained\nmodel is not released. †marks our runs with the ofﬁcially released model by the authors.\nModel #Params CoLA MNLI-(m/-mm) SST-2 QNLI MRPC QQP RTE STS-B Avg(Mcc) (Acc) (Acc) (Acc) (F1) (Acc) (Acc) (Spear)\nBERTBASE(Teacher) 110M 60.4 84.8/84.6 94.0 91.8 90.3 91.4 70.4 89.5 84.1\nTruncated BERT (Sun et al., 2019) 67.5M 41.4 81.2/- 90.8 87.9 82.7 90.4 65.5 - -\nBERTSmall(Turc et al., 2019) 67.5M 47.1 † 81.1/81.7 91.1 87.8 87.9 90.0 63.0 87.5 † 79.7\nTinyBERT (Jiao et al., 2019) 67.5M 42.8 83.5/83.2† 91.6 90.5 88.4 90.6 72.2 88.5† 81.3\nCKD 67.5M 52.7 83.5/83.4 92.4 90.7 89.1 90.8 70.1 89.1 82.4\nTable 2: Comparisons for task-speciﬁc distillation. For a fair comparison, all students are 6/768 BERT models, distilled\nby BERTBASE (12/768) teachers. Other results except for TinyBERT and PKD are as reported by their authors. Results of\ndevelopment set are averaged over 4 runs. “-\" indicates the result is not reported. Average score is computed excluding the\nMNLI-mm accuracy.\nModel #Params CoLA MNLI-(m/-mm) SST-2 QNLI MRPC QQP RTE STS-B Avg(Mcc) (Acc) (Acc) (Acc) (F1) (Acc) (Acc) (Spear)\nBERTBASE(Teacher) 110M 60.4 84.8/84.6 94.0 91.8 90.3 91.4 70.4 89.5 84.1\nPD (Turc et al., 2019) 67.5M - 82.5/83.4 91.1 89.4 89.4 90.7 66.7 - -\nPKD (Sun et al., 2019) 67.5M 45.5 81.3/- 91.3 88.4 85.7 88.4 66.5 86.2 79.2\nTinyBERT (Jiao et al., 2019) 67.5M 53.8 83.1/83.4 92.3 89.9 88.8 90.5 66.9 88.3 81.7\nBERT-of-Theseus (Xu et al., 2020) 67.5M 51.1 82.3/- 91.5 89.5 89.0 89.6 68.2 88.7 81.2\nCKD 67.5M 55.1 83.6 /84.1 93.0 90.5 89.6 91.2 67.3 89.0 82.4\nTable 3: Comparison of task-speciﬁc distillation on SQuAD\ndataset. The results of baselines and ours are reported by\nperforming distillation with their objectives on the top of pre-\ntrained 6-layer BERT (6/768) (Turc et al., 2019).\nModel #Params SQuAD 1.1v\nEM F1\nBERTBASE (Teacher) 110M 81.3 88.6\nPKD(Sun et al., 2019) 67.5M 77.1 85.3\nPD(Turc et al., 2019) 67.5M 80.1 87.0\nTinyBERT(Jiao et al., 2019) 67.5M 80.4 87.2\nCKD 67.5M 81.8 88.7\nDynaBERT (Hou et al., 2020) that involves ﬂexible\narchitectural changes in the training phase.\n5 Experiments\nWe conduct task-agnostic and task-speciﬁc distilla-\ntion experiments to elaborately compare our CKD\nwith baseline distillation objectives. We then report\non the performance gains achieved by our method\nfor BERT architectures of various sizes and insert-\ning our objective for training DynaBERT which can\nrun at adaptive width and depth through pruning\nthe attention heads or layers. Finally, we analyze\nthe effect of each component in our CKD and the\nimpact of leveraging locality δfor wijk in Eq. (1).\nDataset. For task-agnostic distillation which\ncompresses a large pre-trained language model into\na small language model on the pre-training stage,\nwe use a document of English Wikipedia. For eval-\nuating the compressed language model on the pre-\ntraining stage and task-speciﬁc distillation, we use\nthe GLUE benchmark (Wang et al., 2018) which\nconsists of nine diverse sentence-level classiﬁca-\ntion tasks and SQuAD (Rajpurkar et al., 2016).\nSetup. For task-agnostic distillation, we use the\noriginal BERT without ﬁne-tuning as the teacher.\nThen, we perform the distillation on the student\nwhere the model size is pre-deﬁned. We perform\ndistillation using our proposed CKD objective with\nclass probability matching of masked language\nmodeling for 3 epochs while task-agnostic distil-\nlation following the Jiao et al. (2019) and keep\nother hyperparameters the same as BERT pre-\ntraining (Devlin et al., 2018). For task-speciﬁc dis-\ntillation, we experiment with our CKD on top of\npre-trained BERT models of various sizes which\nare released for research in institutions with fewer\ncomputational resources2 (Turc et al., 2019). For\nthe importance weight of each pair-wise and triple-\nwise terms, we leverage the locality of words, in\nthat wij = 1if |i−j|≤ δ and 0, otherwise. For\nthis, we select the δin (10-21). More details includ-\ning hyperparameters are provided in Appendix B.\nThe code to reproduce the experimental results is\navailable at https://github.com/GeondoPark/CKD.\n2https://github.com/google-research/bert\n370\nFigure 2: Task speciﬁc distillation on various sizes of models. We consider diverse cases by changing (a) the network structures,\n(b) the number of parameters and (c) the number of FLOPs. All results are averaged over 4 runs on the development set.\nFigure 3: Boosting the performance of DynaBERT via training with CKD. Comparison between the original DynaBERT and\nCKD-augmented DynaBERT according to (a) the number of parameters and (b) the number of FLOPs. The results are averaged\nover 4 runs on the development set.\n5.1 Main Results\nTo verify the effectiveness of our CKD objective,\nwe compare the performance with previous distil-\nlation methods for BERT compression including\ntask-agnostic and task-speciﬁc distillation. Follow-\ning the standard setup in baselines, we use the\nBERTBASE (12/768)3 as the teacher and 6-layer\nBERT (6/768) as the student network. Therefore,\nthe student models used in all baselines and ours\nhave the same number of parameters (67.5M) and\ninference FLOPs (10878M) and time.\nTask-agnostic Distillation. We compare with\nthree baselines: 1) Truncated BERT which drop\ntop 6 layers from BERTbase proposed in PKD (Sun\net al., 2019), 2) BERTsmall which trained using the\nMasked LM objectives provided in PD (Turc et al.,\n2019), 3) TinyBERT (Jiao et al., 2019) which pro-\n3In notation (a/b), ameans the number of layers and b\ndenotes a hidden size in intermediate layers. The number of\nattention heads is deﬁned as b/64.\npose the individual word representation and atten-\ntion map matching. Since MobileBERT (Sun et al.,\n2020) use the speciﬁcally designed teacher and stu-\ndent network which have 24-layers with an inverted\nbottleneck structure, we do not compare with. Dis-\ntilBERT (Sanh et al., 2019) and MINI-LM (Wang\net al., 2020) use the additional BookCorpus dataset\nwhich is no longer publicly available. Moreover,\nthe authors do not release the code, making it hard\nto reproduce. Thus we do not compare in the main\npaper for a fair comparison. The comparisons with\nthose methods are available in Appendix C. Results\nof task-agnostic distillation on GLUE dev sets are\npresented Table 1. The result shows that CKD sur-\npasses all baselines. Comparing with TinyBERT\nwhich transfers the knowledge of individual repre-\nsentations, CKD outdoes in all scores except for\nthe RTE. These results empirically demonstrate that\ndistribution-based knowledge works better than in-\ndividual representation knowledge.\n371\nTable 4: Ablation study about the impact of each component of CKD. ’- *’ denotes\nto eliminate *, the component of CKD.\nObjectives MNLI-(m/-mm) SST-2 QNLI MRPC QQP STS-B\n(Acc) (Acc) (Acc) (F1) (Acc) (Spear)\nCKD 80.7/80.8 91.4 88.1 88.8 90.3 87.9\n- WR 80.1/80.6 90.6 87.5 88.5 89.7 87.5\n- LTR 79.9/80.3 91.1 87.8 88.3 90.3 87.6\n- WR - LTR 79.2/79.9 89.1 87.4 88.1 89.2 86.8\nFigure 4: Effect of local window size.\nTask-speciﬁc Distillation. Here, we compare\nwith four baselines that do not perform distilla-\ntion in the pre-training: 1) PD (Turc et al., 2019)\nwhich do pre-training with Masked LM and distills\nwith Logit KD in task-speciﬁc ﬁne-tuning process.\n2) PKD (Sun et al., 2019) which uses only 6 layers\nbelow BERTbase, and distillation is also performed\nonly in task-speciﬁc ﬁne-tuning. The GLUE re-\nsults on dev sets of PKD are taken from (Xu et al.,\n2020). 3) TinyBERT (Jiao et al., 2019). For the\nTinyBERT, we also perform distillation only in the\ntask-speciﬁc ﬁne-tuning with their objectives on\nthe top of the pre-trained model provided by Turc\net al. (2019) for a fair comparison. 4) BERT-of-\nTheseus (Xu et al., 2020) which learn a compact\nstudent network by replacing the teacher layers in\na ﬁne-tuning stage. Results of task-speciﬁc distil-\nlation on GLUE dev sets and SQuAD datasets are\npresented in Table 2 and 3, respectively. Note that\nbrieﬂy, the CKD also outperforms all baselines for\nall GLUE datasets and SQuAD dataset except for\nRTE for task-speciﬁc distillation, convincingly ver-\nifying its effectiveness. These results consistently\nsupport that contextual knowledge works better\nthan other distillation knowledge.\n5.2 Effect of CKD on various sizes of models\nFor the knowledge distillation with the purpose of\nnetwork compression, it is essential to work well\nin more resource-scarce environments. To this end,\nwe further evaluate our method on various sizes\nof architectures. For this experiment, we perform\ndistillation on a task-speciﬁc training process on\ntop of various size pre-trained models provided by\nTurc et al. (2019). We compare CKD with three\nbaselines: 1) LogitKD objective used by Sanh et al.\n(2019); Turc et al. (2019). 2) TinyBERT (Jiao et al.,\n2019) objective which includes individual word\nrepresentations and attention matrix matching. 3)\nMINI-LM (Wang et al., 2020) objective which in-\ncludes attention matrix and value-value relation\nmatching. We implement the baselines and runs\nfor task-speciﬁc distillation. We note that MINI-\nLM and TinyBERT objective are applicable only\nto models (*/768) which have the same number of\nattention heads with the teacher model (12/768).\nFigure 2 illustrate that our CKD consistently ex-\nhibits signiﬁcant improvements in the performance\ncompared LogitKD. In addition, for task-speciﬁc\ndistillation, we show that CKD works better than\nall baselines on (*/768) student models. The results\non more datasets are provided in Appendix E.\n5.3 Incorporating with DynaBERT\nDynaBERT (Hou et al., 2020) is a recently pro-\nposed adaptive-size pruning method that can run\nat adaptive width and depth by removing the at-\ntention heads or layers. In the training phase, Dyn-\naBERT uses distillation objectives which consist\nof LogitKD and individual word representations\nmatching to improve the performance. Since the\nCKD objective has no constraints about architec-\nture such as embedding size or the number of atten-\ntion heads, we validate the objective by replacing it\nwith CKD. The algorithm of DynaBERT and how\nto insert CKD are provided in Appendix D. To ob-\nserve just how much distillation alone improves\nperformance, we do not use data augmentation and\nan additional ﬁne-tuning process. We note that ob-\njectives proposed in MINI-LM (Wang et al., 2020)\nand TinyBERT (Jiao et al., 2019) cannot be di-\nrectly applied due to constraints of the number of\nattention heads. As illustrated in Figure 3, CKD\nconsistently outperforms the original DynaBERT\non dynamic model sizes, supporting the claim that\ndistribution-based knowledge is more helpful than\nindividual word representation knowledge. The re-\nsults on more datasets are provided in Appendix E.\n5.4 Ablation Studies\nWe provide additional ablation studies to analyze\nthe impact of each component of the CKD and\nthe introduced locality ( wi,j = δ) in Eq. (1) as\nthe weight of how important each pair-wise and\ntriple-wise term is. For these studies, we ﬁx the\n372\nstudent network with 4-layer BERT (4/512) and\nreport the results as an average of over 4 runs on\nthe development set.\nImpact of each component of CKD. The pro-\nposed CKD transfers the word relation based and\nlayer transforming relation based contextual knowl-\nedge. To isolate the impact on them, we experiment\nsuccessively removing each piece of our objective.\nTable 4 summarizes the results, and we observe that\nWR and LTR can bring a considerable performance\ngain when they are applied together, verifying their\nindividual effectiveness.\nLocality as the importance of relation terms.\nWe introduced the additional weights (wij, wijk) in\nEq. (1) for CKD-WR (and similar ones for CKD-\nLTR) to control the importance of each pair-wise\nand triple-wise term and suggested using the local-\nity for them as one possible way. Here, we verify\nthe effect of locality by increasing the local win-\ndow size (δ) on the SST-2 and QNLI datasets. The\nresult is illustrated in Figure 4. We observe that as\nthe local window size increases, the performance\nimproves, but after some point, the performance is\ndegenerated. From this ablation study, we set the\nwindow size (δ) between 10-21.\n6 Conclusion\nWe proposed a novel distillation strategy that lever-\nages contextual information efﬁciently based on\nword relation and layer transforming relation. To\nour knowledge, we are the ﬁrst to apply this con-\ntextual knowledge which is studied to interpret the\nlanguage models. Through various experiments, we\nshow not only that CKD outperforms the state-of-\nthe-art distillation methods but also the possibility\nthat our method boosts the performance of other\ncompression methods.\nAcknowledgement\nThis work was supported by the National\nResearch Foundation of Korea (NRF) grants\n(2018R1A5A1059921, 2019R1C1C1009192) and\nInstitute of Information & Communications Tech-\nnology Planning & Evaluation (IITP) grants\n(No.2017-0-01779, A machine learning and sta-\ntistical inference framework for explainable artiﬁ-\ncial intelligence, No.2019-0-01371, Development\nof brain-inspired AI with human-like intelligence,\nNo.2019-0-00075, Artiﬁcial Intelligence Graduate\nSchool Program (KAIST)) funded by the Korea\ngovernment (MSIT).\nReferences\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan\nSajjad, and James Glass. 2017. What do neural ma-\nchine translation models learn about morphology?\narXiv preprint arXiv:1704.03471.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2019. On identiﬁability in transformers.\narXiv preprint arXiv:1908.04211.\nYew Ken Chia, Sam Witteveen, and Martin Andrews.\n2019. Transformer to cnn: Label-scarce distilla-\ntion for efﬁcient text classiﬁcation. arXiv preprint\narXiv:1909.03508.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. arXiv preprint\narXiv:1906.04341.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic bert\nwith adaptive width and depth. Advances in Neural\nInformation Processing Systems.\nMinghao Hu, Yuxing Peng, Furu Wei, Zhen Huang,\nDongsheng Li, Nan Yang, and Ming Zhou.\n2018. Attention-guided answer distillation for\nmachine reading comprehension. arXiv preprint\narXiv:1808.07644.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nYoon Kim and Alexander M Rush. 2016. Sequence-\nlevel knowledge distillation. arXiv preprint\narXiv:1606.07947.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n373\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nHaotang Deng, and Qi Ju. 2020. Fastbert: a self-\ndistilling bert with adaptive inference time. arXiv\npreprint arXiv:2004.02178.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint\narXiv:1606.05250.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, pages 8594–8603.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Di-\npanjan Das, et al. 2019b. What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations. arXiv preprint\narXiv:1905.06316.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models. arXiv\npreprint arXiv:1904.02679.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao,\nNan Yang, and Ming Zhou. 2020. Minilm: Deep\nself-attention distillation for task-agnostic compres-\nsion of pre-trained transformers. arXiv preprint\narXiv:2002.10957.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nRui Xia and Chengqing Zong. 2010. Exploring the use\nof word relation features for sentiment classiﬁcation.\nIn Coling 2010: Posters, pages 1336–1344.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compress-\ning bert by progressive module replacing. arXiv\npreprint arXiv:2002.02925.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\n374\nA Explanation of previous methods and\ntheir constraints\nTable 5 present the details of knowledge distilla-\ntion objectives of previous methods and their con-\nstraints.\nDistilBERT (Sanh et al., 2019) uses logit distil-\nlation loss (Logit KD), masked language modeling\nloss, and cosine loss between the teacher and stu-\ndent word representations in the learning process.\nThe cosine loss serves to align the directions of\nthe hidden state vectors of the teacher and student.\nSince the cosine of the two hidden state vectors is\ncalculated in this process, they have the constraint\nthat the embedding size of the teacher and the stu-\ndent model must be the same.\nPKD (Sun et al., 2019) transfers teacher knowl-\nedge to the student with Logit KD and patient loss.\nThe patient loss is the mean-square loss between\nthe normalized hidden states of the teacher and stu-\ndent. To calculate the mean square error between\nthe hidden states, they have a constraint that the\ndimensions of hidden states must be the same be-\ntween teacher and student.\nTinyBERT (Jiao et al., 2019) uses additional\nloss that matches word representations and atten-\ntion matrices between the teacher and student. Al-\nthough they acquire ﬂexibility on the embedding\nsize, using an additional parameter, since the atten-\ntion matrices of the teacher and student are matched\nthrough mean square error loss, the number of at-\ntention heads of the teacher and student must be\nthe same.\nMobileBERT (Sun et al., 2020) utilizes a sim-\nilar objective with TinyBERT (Jiao et al., 2019)\nfor task-agnostic distillation. However, since they\nmatch the hidden states with l2 distance and atten-\ntion matrices with KLdivergence between teacher\nand student, they have restrictions on the size of\nhidden states and the number of attention heads.\nMiniLM (Wang et al., 2020) proposes distilling\nthe self-attention module of the last Transformer\nlayer of the teacher. In self-attention module, they\ntransfer attention matrices such as TinyBERT and\nMobileBERT and Value-Value relation matrices.\nSince they match the attention matrices of the\nteacher and student in a one-to-one correspondence,\nthe number of attention heads of the teacher and\nstudent must be the same.\nThe methods introduced in Table 5 have con-\nstraints by their respective knowledge distillation\nobjectives. However, our CKD method which uti-\nlizes the relation statistics between the word repre-\nsentations (hidden states) has the advantage of not\nhaving any constraints on student architecture.\nB Details of experiment setting\nThis section introduces the experimental setting in\ndetail. We implemented with PyTorch framework\nand huggingface’s transformers package (Wolf\net al., 2019).\nTask-agnostic distillation We use the pre-\ntrained original BERTbase with masked language\nmodeling objective as the teacher and a docu-\nment of English Wikipedia as training data. We\nset the max sequence length to 128 and follow the\npreprocess and WordPiece tokenization of Devlin\net al. (2018). Then, we perform the distillation for\n3 epochs. For the pre-training stage, we use the\nCKD objective with class probability matching of\nmasked language modeling and keep other hyper-\nparameters the same as BERT pre-training (Devlin\net al., 2018).\nTask-speciﬁc distillation Our contextual knowl-\nedge distillation proceeds in the following order.\nFirst, from pre-trained BERTbase, task-speciﬁc ﬁne-\ntuning is conducted to serve as a teacher. Then, pre-\npare the pre-trained small-size architecture which\nserves as a student. In this case, pre-trained mod-\nels of various model sizes provided by Turc et al.\n(2019) are employed. Finally, task-speciﬁc distilla-\ntion with our CKD is performed.\nTo reduce the hyperparameters search cost,λWR\nin Eq. (1) and λLTR in Eq. (3) are used with same\nvalue. For the importance weights introduced for\npair-wise and triple-wise terms, the locality is ap-\nplied only to the importance weight wof the word\nrelation (WR)-based CKD loss. The importance\nweight wof the layer transforming relation (LTR)-\nbased CKD loss is set to 1. In this paper, we report\nthe best result among the following values to ﬁnd\nthe optimal hyperparameters of each dataset:\n• Alpha (α) : 0.7, 0.9\n• Temperature (T) : 3, 4\n• λWR, λLTR : 1, 10, 100, 1000\n• λCKD : 1, 10, 100, 1000\nOther training conﬁgurations such as batch size,\nlearning rate and warm up proportion are used fol-\nlowing the BERT (Devlin et al., 2018).\n375\nTable 5: Overview of distillation objectives used for language model compression and their constraint on architec-\nture. Sk means scaled softmax function across the kth-dimension.\nKnowledge Distillation Objectives Constraint\nDistilBERT (Sanh et al., 2019)\nn∑\ni=1\ncos(rt\nl,i,rs\nl,i), LDLogit Embedding size\nPKD (Sun et al., 2019)\nn∑\ni=1\n[\nMSE(\nrt\nl,i\n∥rt\nl,i∥2\n−\nrs\nl,i\n∥rs\nl,i∥2\n)\n]\n, LDLogit Embedding size\nTinyBERT (Jiao et al., 2019)\nn∑\ni=1\n[\nMSE(rt\nl,i −Wrrs\nl,i)\n]\n,\nH∑\nh=1\n[\nMSE(At\nl,h −As\nl,h)\n]\n, LDLogit Attention head\nMobile-BERT (Sun et al., 2020)\nn∑\ni=1\n[\nMSE(rt\nl,i −rs\nl,i)\n]\n,\nH∑\nh=1\n[\nKL(At\nl,h,As\nl,h\n)]\n, LDLogit\nEmbedding size\nAttention head\nMiniLM (Wang et al., 2020)\nH∑\nh=1\n[\nKL(At\nl,h,As\nl,h\n)]\n,\nH∑\nh=1\n[\nKL(S2(Vt\nl,h ·Vt\nl,h\nT),S2(Vs\nl,h ·Vs\nl,h\nT)]\nAttention head\nTable 6: Full comparison of task-agnostic distillation comparing our CKD against the baseline methods. For the task-agnostic\ndistillation comparison, we do not use task-speciﬁc distillation for a fair comparison. The results of TinyBERT cited as reported\nby Wang et al. (2020). Other results are as reported by their authors. Results of the development set are averaged over 4 runs. “-\"\nmeans the result is not reported and the trained model is not released. †marks our runs with the ofﬁcially released model.\nModel #Params CoLA MNLI-(m/-mm) SST-2 QNLI MRPC QQP RTE STS-B\n(Mcc) (Acc) (Acc) (Acc) (F1) (Acc) (Acc) (Spear)\nBERTBASE (Teacher) 110M 60.4 84.8/84.6 94.0 91.8 90.3 91.4 70.4 89.5\nTruncated BERT (Sun et al., 2019) 67.5M 41.4 81.2/- 90.8 87.9 82.7 90.4 65.5 -\nBERTSmall (Turc et al., 2019) 67.5M 47.1 † 81.1/81.7 91.1 87.8 87.9 90.0 63.0 87.5 †\nDistilBERT (Sanh et al., 2019) 67.5M 51.3 82.2/- 91.3 89.2 87.5 88.5 59.9 86.9\nTinyBERT (Jiao et al., 2019) 67.5M 42.8 83.5/83.2 † 91.6 90.5 88.4 90.6 72.2 88.5†\nMINI-LM (Wang et al., 2020) 67.5M 49.2 84.0/- 92.0 91.0 88.4 91.0 71.5 -\nCKD 67.5M 52.7 83.5/83.4 92.4 90.7 89.1 90.8 70.1 89.1\nC Additional comparison on\ntask-agnostic distillation\nWe report the fair comparison of our method and\nbaselines about the task-agnostic distillation in Sec-\ntion 5.1 of the main paper. Several works (Sanh\net al., 2019; Wang et al., 2020) use the additional\nBookCorpus dataset which is no longer publicly\navailable. Here, we present the full comparison of\nCKD and baselines including DistilBERT (Sanh\net al., 2019) and MINI-LM (Wang et al., 2020).\nAs shown in Table 6, even though we do not use\nthe BookCorpus dataset, we outperform all base-\nlines on four datasets and obtain comparable per-\nformance on the rest of the datasets.\nD Applying CKD to DynaBERT\nIn this section, we describe how we apply our CKD\nobjective to DynaBERT (Hou et al., 2020). Train-\ning DynaBERT consists of three stages: 1) Rewire\nthe model according to the importance and then\n2) Go through the two-stage of adaptive pruning\nwith distillation objective. Since we suppress some\ndetails of DynaBERT for clarity, refer to the pa-\nper (Hou et al., 2020) for more information.\nWe summarize the training procedure of Dyn-\naBERT with CKD in algorithm 1. To fully exploit\nthe capacity, more important attention heads and\nneurons must be shared more across the various\nsub-networks. Therefore, we follow phase 1 in Dyn-\naBERT to rewire the network by calculating the\nloss and estimating the importance of each atten-\ntion head in the Multi-Head Attention (MHA) and\nneuron in the Feed-Forward Network (FFN) based\non gradients. Then, they train the DynaBERT by\naccumulating the gradient varying the width and\ndepth of BERT. In these stages, they utilize distil-\nlation objective which matches hidden states and\nlogits to improve the performance. We apply our\nCKD at these stages by replacing their objective\nwith CKD as shown in algorithm 1 (Blue). Since\nCKD has no restrictions on student’s architecture,\nit can be easily applied.\nE More Results\nDue to space limitations in the main paper, we only\nreport the results on a subset of GLUE datasets for\nexperiments about the effect of model size for CKD\nand boosting the DynaBERT with CKD. Here, we\nreport all datasets of GLUE except for CoLA for\ntwo experiments. We exclude the CoLA dataset\nsince the distillation losses are not converged prop-\nerly in the very small-size models.\nHere, we present the results of three experiments\n376\nAlgorithm 1: Train DynaBERT with CKD\nPhase 1: Rewire the network.\ninput :Development set, trained BERT on downstream task.\nCalculate the importance of attention heads and neurons with gradients.\nRewire the network according to the importance.\nPhase 2: Train DynaBERTW with adaptive width.\ninput :Training set, width multiplier list widthList.\ninitialize a ﬁxed teacher model and a trainable student model with rewired net.\nfor iter= 1,...,T train do\nGet the logits y, hidden states R from teacher model.\nfor width multiplier mw in widthListdo\nGet the logits y(mw), hidden states R(mw) from student model.\nCompute distillation loss.\nLDynaBERT = SCE(y(mw),y) +λ1 ·∑L\nl=0 MSE(R(mw)\nl ,Rl)\nLCKD = SCE(y(mw),y) +λ1 ·LCKD−WR(R(mw),R) +λ2 ·LCKD−LTR(R(mw),R)\nAccumulate gradients L.backward().\nUpdate with the accumulated gradients.\nPhase 3: Train DynaBERT with adaptive width and depth.\ninput :Training set, width multiplier list widthList, depth multiplier list depthList.\ninitialize a ﬁxed DynaBERTW as teacher model and a trainable student model with the DynaBERTW.\nfor iter= 1,...,T train do\nfor width multiplier mw in widthListdo\nGet the logits y(mw), R(mw) from teacher model.\nfor depth multiplier md in depthListdo\nGet the logits y(mw,md), hidden states R(mw,md) from student model.\nCompute distillation loss.\nLDynaBERT = SCE(y(mw,md),y(mw)) +λ1 ·∑\nl,l′∈LS,LT\nMSE(R(mw,md)\nl ,R(mw)\nl′ )\nLCKD = SCE(y(mw,md),y(mw)) +λ1 ·LCKD−WR(R(mw,md),R(mw))\n+λ2 ·LCKD−LTR(R(mw,md),R(mw))\nAccumulate gradients L.backward().\nUpdate with the accumulated gradients.\non additional datasets in order. 1) Effect of CKD\non various sizes of models. 2) Boosting the perfor-\nmance of DynaBERT when CKD is applied.\nEffect of CKD on various sizes of models. Fig-\nure 5 illustrates the performance of task-speciﬁc\ndistillation on various sizes of models. Again, we\nnote that MINI-LM and TinyBERT objectives are\napplicable only to models (*/768), which have the\nsame number of attention heads as the teacher\nmodel (12/768). As shown in Figure 5, our CKD\nconsistently exhibits signiﬁcant improvements in\nthe performance compared LogitKD for all model\nsizes. Compared to TinyBERT and MINI-LM,\nCKD shows higher performance on all datasets\nfor almost all model sizes (*/768).\nIncorporating with DynaBERT Figure 6\nshows the performance of the original DynaBERT\nand when CKD is applied. As illustrated in\nFigure 6, CKD further improves the original\nDynaBERT on dynamic width and depth size,\nconvincingly verifying its effectiveness. The\nresults also present the possibility that our method\nboosts the performance of other compression\nmethods.\n377\nFigure 5: The efﬁciency of various sizes of models for CKD compared to baselines. The performance graph according to\n(a) network structure (b) the number of parameters (c) the number of FLOPs. The results are averaged over 4 runs on the\ndevelopment set.\n378\nFigure 6: Boosting the performance of DynaBERT when CKD is applied. The performance graph for comparison of original\nDynaBERT and CKD according to (a) the number of parameters and (b) the number of FLOPs. The results are averaged over 4\nruns on the development set.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.827109694480896
    },
    {
      "name": "Relation (database)",
      "score": 0.6087780594825745
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5909163951873779
    },
    {
      "name": "Natural language processing",
      "score": 0.5715827941894531
    },
    {
      "name": "Distillation",
      "score": 0.5646665692329407
    },
    {
      "name": "Representation (politics)",
      "score": 0.5334746241569519
    },
    {
      "name": "Context (archaeology)",
      "score": 0.49764230847358704
    },
    {
      "name": "Language model",
      "score": 0.4764150381088257
    },
    {
      "name": "Pruning",
      "score": 0.4379115402698517
    },
    {
      "name": "Machine learning",
      "score": 0.33124905824661255
    },
    {
      "name": "Data mining",
      "score": 0.07402384281158447
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ]
}