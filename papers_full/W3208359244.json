{
  "title": "HRViT: Multi-Scale High-Resolution Vision Transformer",
  "url": "https://openalex.org/W3208359244",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2105054052",
      "name": "Jiaqi Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2734323002",
      "name": "Hyoukjun Kwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229879960",
      "name": "Dilin Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975948561",
      "name": "Ye Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1986365266",
      "name": "Meng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155923623",
      "name": "Yu-Hsin Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106611394",
      "name": "Liangzhen Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119841321",
      "name": "Vikas Chandra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2033707519",
      "name": "David Z. Pan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3167597877",
    "https://openalex.org/W3013594164",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3190216403",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2953139137",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2949892913"
  ],
  "abstract": "Vision transformers (ViTs) have attracted much attention for their superior performance on computer vision tasks. To address their limitations of single-scale low-resolution representations, prior work adapts ViTs to high-resolution dense prediction tasks with hierarchical architectures to generate pyramid features. However, multi-scale representation learning is still under-explored on ViTs, given their classification-like sequential topology. To enhance ViTs with more capability to learn semantically-rich and spatially-precise multi-scale representations, in this work, we present an efficient integration of high-resolution multi-branch architectures with vision transformers, dubbed HRViT, pushing the Pareto front of dense prediction tasks to a new level. We explore heterogeneous branch design, reduce the redundancy in linear layers, and augment the model nonlinearity to balance the model performance and hardware efficiency. The proposed HRViT achieves 50.20% mIoU on ADE20K and 83.16% mIoU on Cityscapes for semantic segmentation tasks, surpassing state-of-the-art MiT and CSWin with an average of +1.78 mIoU improvement, 28% parameter reduction, and 21% FLOPs reduction, demonstrating the potential of HRViT as strong vision backbones.",
  "full_text": "HRViT: Multi-Scale High-Resolution Vision Transformer\nJiaqi Gu2∗, Hyoukjun Kwon1, Dilin Wang1, Wei Ye1, Meng Li1, Yu-Hsin Chen1,\nLiangzhen Lai1, Vikas Chandra1, David Z. Pan2\n1Facebook Inc.,2University of Texas at Austin\njqgu@utexas.edu\nAbstract\nVision transformers (ViTs) have attracted much at-\ntention for their superior performance on computer vi-\nsion tasks. To address their limitations of single-scale\nlow-resolution representations, prior work adapts ViTs\nto high-resolution dense prediction tasks with hierarchi-\ncal architectures to generate pyramid features. How-\never, multi-scale representation learning is still under-\nexplored on ViTs, given their classiﬁcation-like sequen-\ntial topology. To enhance ViTs with more capability\nto learn semantically-rich and spatially-precise multi-\nscale representations, in this work, we present an ef-\nﬁcient integration of high-resolution multi-branch ar-\nchitectures with vision transformers, dubbed HRViT,\npushing the Pareto front of dense prediction tasks to\na new level. We explore heterogeneous branch design,\nreduce the redundancy in linear layers, and augment\nthe model nonlinearity to balance the model perfor-\nmance and hardware eﬃciency. The proposed HRViT\nachieves 50.20% mIoU on ADE20K and 83.16% mIoU\non Cityscapes for semantic segmentation tasks, sur-\npassing state-of-the-art MiT and CSWin with an av-\nerage of +1.78 mIoU improvement, 28% parameter\nreduction, and 21% FLOPs reduction, demonstrating\nthe potential of HRViT as strong vision backbones.\n1. Introduction\nDense prediction vision tasks, e.g., semantic seg-\nmentation, object detection, are critical workloads on\nmodern intelligent computing platforms, e.g., AR/VR\ndevices. Convolutional neural networks (CNNs) have\nrapidly evolved with signiﬁcant improvement in dense\nprediction tasks [1, 4, 16, 18, 21, 24]. Beyond classical\nCNNs, vision transformers (ViTs) have attracted ex-\ntensive interests and showed competitive performance\nin vision tasks [2,3,5,9,10,15,17,23,26,27,29,30,33,35].\n∗Work done during an internship at Facebook Inc.\nBeneﬁting from the self-attention operations, ViTs em-\nbrace strong expressivity with long-distance informa-\ntion interaction. However, ViTs produce single-scale\nand low-resolution representations, which are not com-\npatible with dense prediction workloads that require\nhigh position sensitivity and ﬁne-grained image details.\nRecently, various ViT backbones have been pro-\nposed to adapt to dense prediction tasks. Prior ViT\nbackbones proposed various eﬃcient global/local self-\nattention to extract hierarchical features [5,9,17,25,26,\n29, 32]. A multi-scale ViT (MViT) [11] has been pro-\nposed to learn a hierarchy that progressively expands\nthe channel capacity while reducing the spatial resolu-\ntion. However, they still follow a classiﬁcation-like net-\nwork topology with a sequential or series architecture.\nFor complexity consideration, they gradually down-\nsample the feature maps to extract higher-level low-\nresolution (LR) representations and directly feed each\nstage’s output to the downstream framework. Such se-\nquential structures lack enough cross-scale interaction\nthus cannot generate high-qualityhigh-resolution (HR)\nrepresentations.\nHRNet [24] was proposed to enhance the cross-\nresolution interaction with a multi-branch architecture\nthat maintains all resolutions throughout the network.\nMulti-resolution features are extracted in parallel and\nfused repeatedly to generate high-quality HR repre-\nsentations with richer semantic information. Such a\ndesign concept has achieved great success in various\ndense prediction tasks. Nevertheless, its expressivity\nis limited by small receptive ﬁelds and strong induc-\ntive bias from cascaded convolution operations. Later,\na slimmed Lite-HRNet [31] was put forward with ef-\nﬁcient shuﬄe blocks and channel weighting operators.\nHR-NAS [8] inserted a lightweight transformer path\ninto the residual blocks to extract global information\nand applied the neural architecture search to remove\nthe channel/head redundancies. However, those im-\nproved HRNet designs are still mainly based on the\nconvolutional building blocks, and the demonstrated\n1\narXiv:2111.01236v1  [cs.CV]  1 Nov 2021\nperformance of their tiny models is still far behind the\nSoTA scores of ViT counterparts.\nMigrating the success of HRNet to ViT designs is\nnon-trivial. Given the high complexity of multi-branch\nHR architectures and self-attention operations, sim-\nply replacing all residual blocks in HRNet with trans-\nformer blocks will encounter severe scalability issues.\nThe inherited powerful representability will be over-\nwhelmed by the prohibitive hardware cost without\ncareful architecture-block co-optimization.\nTo enhance ViTs with stronger representability to\ngenerate semantically-rich and position-precise fea-\ntures, in this work, we present HRViT, an eﬃcient\nmulti-scale high-resolution vision transformer back-\nbone speciﬁcally optimized for high-resolution dense\nprediction tasks. Our goal is facilitating eﬃcient multi-\nscale representation learning for vision transformers .\nHRViT is diﬀerent from prior sequential ViTs in several\naspects: 1) our multi-branch HR architecture extracts\nmulti-scale features in parallel with cross-resolution\nfusion to enhance the multi-scale representability of\nViTs; 2) our augmented local self-attention removes\nredundant keys and values for better eﬃciency and en-\nhances the expressivity with extra convolution paths,\nadditional nonlinearity, and auxiliary shortcuts to en-\nhance feature diversity; 3) we adopt mixed-scale convo-\nlutional feedforward networks to fortify the multi-scale\nfeature extraction; 4) our HR convolutional stem and\neﬃcient patch embedding layers maintain more low-\nlevel ﬁne-grained features with reduced hardware cost.\nAlso, distinguished from the HRNet-family, ourHRViT\nfollows a unique heterogeneous branch design to bal-\nance eﬃciency and performance, which is not simply\nan improved HRNet but a new topology of pure ViTs\nmainly constructed by self-attention operators. Our\nmain contributions are as follows,\n• We deeply investigate the multi-scale representa-\ntion learning in ViTs and integrate high-resolution\narchitecture with vision transformers for high-\nperformance dense prediction vision tasks.\n• To enable scalable HR-ViT integration with better\nperformance and eﬃciency trade-oﬀ, we leverage\nthe redundancy in transformer blocks and perform\njoint optimization on key components of HRViT\nwith heterogeneous branch designs.\n• The proposed HRViT achieves 50.20% mIoU on\nADE20K val and 83.16% mIoU on Cityscapes\nval for semantic segmentation tasks, outperform-\ning state-of-the-art (SoTA) MiT and CSWin with\n1.78 higher mIoU, 28% fewer parameters, and 21%\nlower FLOPs, on average.\n2. Proposed HRViT Architecture\nCompared with the surge in sophisticated atten-\ntion operator innovations, the multi-scale representa-\ntion learning of ViTs is much less explored, which is\nfar behind the recent advance in their CNN counter-\nparts. New topology designs create another dimension\nto unleash the potential of ViTs with even stronger vi-\nsion expressivity. An important question that remains\nto be answered is whether the success of HRNet can\nbe eﬃciently migrated to ViT backbones to consolidate\ntheir leading position in high-resolution dense predic-\ntion tasks.\nIn this section, we delve into the multi-scale repre-\nsentation learning in ViTs and introduce a hardware-\neﬃcient integration of the HR architecture and ViTs.\n2.1. Architecture overview\nWe illustrate the architecture of HRViT in Figure 1.\nIt consists of a convolutional stem to reduce spatial\ndimensions while extracting low-level features. Then\nwe construct four progressive transformer stages where\nthe n-th stage contains n parallel multi-scale trans-\nformer branches. Each stage can have one or more\nmodules. Each module starts with a lightweight dense\nfusion layer to achieve cross-resolution interaction and\nan eﬃcient patch embedding block for local feature\nextraction, followed by repeated augmented local self-\nattention blocks ( HRViTAttn) and mixed-scale con-\nvolutional feedforward networks (MixCFN). Unlike se-\nquential ViT backbones that progressively reduce the\nspatial dimension to generate pyramid features, we\nmaintain the HR features throughout the network to\nstrengthen the quality of HR representations via cross-\nresolution fusion.\n2.2. Efﬁcient HR-ViT integration with heteroge-\nneous branch design\nWe design a heterogeneous multi-branch architec-\nture for hardware-eﬃcient multi-scale high-resolution\nViTs. A straightforward choice is to replace all convo-\nlutions in HRNet with self-attentions. However, given\nthe high complexity of multi-branch HRNet and self-\nattention operators, this brute-force combining will\nquickly cause an explosion in memory footprint, pa-\nrameter size, and computational cost. The real chal-\nlenge is that we want to leverage both of the superior\nmulti-scale representability from HR architectures and\nthe superior modeling capacity of transformers , mean-\nwhile, we have to overcome the enormous complexity\nand make it even more hardware-eﬃcient than both of\nthem. Hence careful architecture and block co-design is\ncritical to a scalable and eﬃcient HR-ViT integration.\n2\nStem\nPatch Embed\nTransformer Block\nUpsample\nDownsample\nStage 3Module 1Stage 1 Stage 2 Stage 4\nSkip\nx b\nFigure 1: The overall architecture of our proposed HRViT. It progressively expands to 4 branches. Each stage has\nmultiple modules. Each module contains multiple transformer blocks.\nFeature/Arch. HR (1\n4×,1\n8×) MR ( 1\n16×) LR ( 1\n32×)\nMemory cost High Medium Low\nComputation Heavy Moderate Light\n#Params Small Medium Large\nEﬀ. on class. Not quite useful Important Important\nGranularity Fine Medium Coarse\nReceptive ﬁeld Local Region Global\nWindow size Narrow (s=1,2) Wide (s=7) Wide (s=7)\nDepth Shallow (∼5-6) Deep (20-30) Shallow (∼4)\nTable 1: Qualitative cost and functionality analysis.\nWindow sizes and depth are given for each branch.\nHeterogeneous branch conﬁguration. The ﬁrst\nquestion is how to conﬁgure each branch for a scalable\nHRViT design. Simply assigning the same number of\nblocks with the same local self-attention window size\non each module will make it intractably costly. We\ngive a detailed analysis on the functionality and cost of\neach branch in Table 1, based on which we summarize\na simple design heuristic.\nWe give the parameter count of HRViTAttn and\nMixCFN blocks on the i-th branch (i=1,2,3,4),\nParamsHRViTAttn,i = O(4i−1C2 + 2i−1C),\nParamsMixCFN,i = O(4i−1C2ri + 2i−1Cri).\n(1)\nThe amount of ﬂoating-point operations (FLOPs) is,\nFLOPsHRViTAttn,i =O\n(\nHWC2 + CHW(H+W)si\n4i−1\n)\n,\nFLOPsMixCFN,i = O\n(\nriHWC2 + riHWC\n2i−1\n)\n.\n(2)\nThe ﬁrst and second HR branches (i=1,2) can barely\ngenerate useful high-level features for classiﬁcation but\nhave a high memory and computational cost. On\nthe other hand, they are parameter-eﬃcient and can\nprovide ﬁne-grained detail calibration in segmentation\ntasks. Thus we use a narrow attention window size s\nand use a minimum number of blocks on two HR paths .\nThe most important branch is the third one with a\nmedium resolution (MR). Given its medium hardware\ncost, we can aﬀord a deep branch with a large window\nsize on the MR path to provide large receptive ﬁelds\nand well-extracted high-level features.\nThe lowest resolution (LR) branch contains most pa-\nrameters and is very useful to provide high-level fea-\ntures as coarse segmentation maps. However, its small\nspatial sizes lose too many image details. Therefore, we\nonly put a few blocks with a large window size on the\nLR branch to improve high-level feature quality under\nparameter budgets.\nNearly-even block assignment. Once we decide\nthe total branch depth, a unique question, which does\nnot exist in the sequential ViT variants, is how to\nassign those blocks to each module . In our example\nHRViT, we need to assign 20 blocks to 4 modules on the\n3rd path. To maximize the average depth of the net-\nwork ensemble and help input/gradient ﬂow through\nthe deep transformer branch, we prefer a nearly-even\nparititioning, e.g., 6-6-6-2, to an extremely unbalanced\nassignment, e.g., 17-1-1-1.\n2.3. Efﬁcient HRViT component design\nThen we will give a detailed introduction to the op-\ntimized building blocks and key features of HRViT.\nAugmented cross-shaped local self-attention. To\nachieve high performance with improved eﬃciency, a\nhardware-eﬃcient self-attention operator is necessary.\nWe adopt one of the SoTA eﬃcient cross-shaped self-\nattention [9] as our baseline attention operator. Based\non that, we design our augmented cross-shaped local\nself-attention HRViTAttn. This attention has the fol-\nlowing advantages. (1) Fine-grained attention : Com-\npared with globally-downsampled attentions [25, 29],\nthis one has ﬁne-grained feature aggregation that pre-\nserves detailed information. (2) Approximated global\nview: By using two parallel orthogonal local attentions,\n3\nQ\nK\nA\nV\nMM\n+\nVertical MHSA\n[ | ]\nHorizontal MHSA\nMM\n+\nLN\nHswish+DWConv\nHswish \n+Linear \n+BN\nW\nH\nC\ns\nC/2\nW\nsW\ndk\nsoftmax\nshare \nDiversity-Enhanced Shortcut\n(a)\n1\n2\nK=H/s\n-inf\nSoftmax\n0\nMask\nsW\nsW\nZero-padding\n0\n(b)\nFigure 2: (a) HRViTAttn: augmented cross-shaped\nlocal self-attention with a parallel convolution path and\nan eﬃcient diversity-enhanced shortcut. (b) Window\nzero-padding with attention map masking.\nthis attention can collect global information. (3) Scal-\nable complexity: one dimension of the window is ﬁxed,\nwhich avoids quadratic complexity to image sizes.\nTo balance performance and hardware eﬃciency,\nwe introduce our augmented version, denoted as\nHRViTAttn, with several key optimizations. In Fig-\nure 2(a), we follow the cross-shaped window parti-\ntioning approach in CSWin that separates the input\nx ∈RH×W×C into two parts {xH,xV ∈RH×W×C/2}.\nxH is partitioned into disjoint horizontal windows, and\nthe other half xV is chunked into vertical windows. The\nwindow is set to s×W or H×s. Within each window,\nthe patch is chunked intoKdk-dimensional heads, then\na local self-attention is applied,\nHRViTAttn(x) =BN\n(\nσ(WO[y1,··· ,yk,··· ,yK])\n)\nyk = zk + DWConv\n(\nσ(WV\nk x)\n)\n[z1\nk,··· ,zM\nk ] =zk =\n{\nH-Attnk(x), 1 ≤k<K/ 2\nV-Attnk(x), K/ 2 ≤k≤K\nzm\nk = MHSA(WQ\nk xm,WK\nk xm,WV\nk xm)\n[x1,··· ,xm,··· ,xM ] =x, x m ∈R(H/s)×W×C,\n(3)\nwhere WQ\nk ,WK\nk ,WV\nk ∈Rdk×C are projection matrices\nto generate query Qk, key Kk, and value Vk tensors\nfor the k-th head, WO ∈ RC×C is the output pro-\njection matrix, and σ is Hardswish activation. If the\nimage sizes are not a multiple of window size, e.g.,\ns⌈H/s⌉ > H, we apply zero-padding to inputs x to\nallow a complete K-th window, shown in Figure 2(b).\nThen the padded region in the attention map is masked\nto 0 to avoid incoherent semantic correlation.\nThe original QKV linear layers are quite costly in\ncomputation and parameters. We share the linear pro-\njections for key and value tensors in HRViTAttn to\nsave computation and parameters as follows,\nMHSA(WQ\nk xm,WV\nk xm,WV\nk xm)= softmax\n(Qm\nk (Vm\nk )T\n√dk\n)\nVm\nk ,\n(4)\nBesides, we introduce an auxiliary path with paral-\nlel depth-wise convolution to inject inductive bias to\nfacilitate training. Diﬀerent from the local positional\nencoding in CSWin, our parallel path is nonlinear and\napplied on the entire 4-D feature map WV x without\nwindow-partitioning. This path can be treated as an\ninverted residual module sharing point-wise convolu-\ntions with the linear projection layers in self-attention.\nThis shared path can eﬀectively inject inductive bias\nand reinforce local feature aggregation with marginal\nhardware overhead.\nAs a performance compensation for the above key-\nvalue sharing, we introduce an extra Hardswish func-\ntion to improve the nonlinearity. We also append a\nBatchNorm (BN) layer that is initialized to an iden-\ntity projection to stabilize the distribution for bet-\nter trainability. Recent studies revealed that diﬀerent\ntransformer layers tend to have very similar features\nwhere the shortcut plays a critical role [20]. Inspired\nby the augmented shortcut [22], we add a channel-wise\nprojector as a diversity-enhanced shortcut (DES). The\nmain diﬀerence is that our shortcut has higher non-\nlinearity and does not depend on hardware-unfriendly\nFourier transforms. The projection matrix in our\nDES PC×C is approximated by Kronecker decompo-\nsition P= A\nC\np ×C\np ⊗Bp×p to minimize parameter cost,\nwhere p is optimally set to ⌊\n√\nC⌋. Then we fold x as\n˜x ∈ RHW ×C\np ×p and convert ( A⊗B)x into ( A˜xBT )\nto save computations. We further insert Hardswish\nafter the B projection to increase the nonlinearity,\nDES(x) = A·Hardswish(˜xBT ). (5)\nMixed-scale convolutional feedforward net-\nwork. Inspired by theMixFFN in MiT [29] and multi-\nbranch inverted residual blocks in HR-NAS [8], we de-\nsign a mixed-scale convolutional FFN (MixCFN) by\ninserting two multi-scale depth-wise convolution paths\nbetween two linear layers. After LayerNorm, we ex-\npand the channel by a ratio of r, then split it into two\n4\nH W C\nH W rC/2\nLinear\nLinear\n3 3DWConv\n5 5\nDWConv\nH W rC\n[ | ]\nH W C\nGELU\n+Linear +LN\nFigure 3: MixCFN with multiple depth-wise convolu-\ntion paths to extract multi-scale local information.\nbranches. The 3 ×3 and 5 ×5 depth-wise convolutions\n(DWConv) are used to increase the multi-scale local\ninformation extraction of HRViT. For eﬃciency, we ex-\nploit the channel redundancy by reducing the MixCFN\nexpansion ratio rfrom 4 [17,29] to 2 or 3 with marginal\nperformance loss on medium to large models.\nDownsampling stem. In dense prediction tasks,\nimages are of high resolution, e.g., 1024 ×1024. Self-\nattention operators are known to be expensive as\ntheir complexity is quadratic to image sizes. To ad-\ndress the scalability issue when processing large im-\nages, we down-sample the inputs by 4 ×before feed-\ning into the main body of HRViT. We do not use\nattention operations in the stem since early convolu-\ntions are more eﬀective to extract low-level features\nthan self-attentions [12, 28]. On the other hand, in-\nstead of simply using a stride-4 convolution as in prior\nViTs [5, 17, 29], we follow the design in HRNet and\nuse two stride-2 CONV-BN-ReLU blocks as a stronger\ndownsampling stem to extract C-channel features with\nmore information maintained.\nEﬃcient patch embedding. Before transformer\nblocks in each module, we put a patch embedding block\n(CONV-LayerNorm) on each branch. It is used to\nmatch channels and extract patch information with en-\nhanced inter-patch communication. Unlike in sequen-\ntial architectures that only have 4 embedding layers,\nwe found that the patch embedding layers have a non-\ntrivial hardware cost in the HR architecture since each\nmodule at stage- n will have n embedding blocks. We\nslim it down with a blueprint convolution [13], i.e.,\npoint-wise CONV followed by a depth-wise CONV,\nEffPatchEmbed(x) =LN\n(\nDWConv(PWConv(x))\n)\n. (6)\nCross-resolution fusion layer. The cross-resolution\nfusion layer is critical for HRViT to learn high-quality\nHR representations, shown in Figure 4. To impose\nmore cross-resolution interaction, we borrow the idea\nfrom HRNet [24,31] to insert repeated cross-resolution\nfusion layers at the beginning of each module.\nTo help LR features maintain more image details\nand precise position information , we merge them with\n+\n DWConv\nstride=2,4,8 BatchNorm1 1 Conv2dNearest\nUpsampling\n+\n+\nGELU\nFigure 4: Channel matching, up-scaling, and down-\nsampling in the fusion layer.\ndown-sampled HR features. Instead of using a progres-\nsive convolution-based downsampling path to match\ntensor shapes [24, 31], we employ a direct down-\nsampling path to minimize hardware overhead. In the\ndown-sampling path between the i-th input and j-th\noutput (j >i), we use a depth-wise separable convolu-\ntion with a stride of 2 j−i to shrink the spatial dimen-\nsion and match the output channels. The kernel size\nused in the DWConv is (2 j−i+1) to create patch over-\nlaps. Those HR paths inject more image information\ninto the LR path to mitigate information loss and for-\ntify gradient ﬂows during backpropagation to facilitate\nthe training of LR transformer blocks.\nOn the other hand, the receptive ﬁeld is usually lim-\nited in the HR blocks as we minimize the window size\nand branch depth on HR paths. Hence, we merge\nLR representations into HR paths to help them ob-\ntain higher-level features with a larger receptive ﬁeld .\nSpeciﬁcally, in the up-scaling path ( j < i), we ﬁrst\nincrease the number of channels with a point-wise con-\nvolution and up-scale the spatial dimension via a near-\nest neighbor interpolation with a rate of 2 i−j. When\ni=j, we directly pass the features to the output as a\nskip connection. Note that in HR-NAS [8], the dense\nfusion is simpﬂiﬁed by a sparse fusion module where\nonly neighboring resolutions are merged. This tech-\nnique is not considered inHRViT since it saves marginal\nhardware cost but leads to a noticeable accuracy drop,\nwhich will be shown in the ablation study later.\n2.4. Architectural variants\nDiﬀerent HRViT variants scale both in network\ndepth and width. Table 2 summarizes detailed branch\ndesigns of 3 variants.\nWe follow the aforementioned design guidance and\nput 1 transformer block on HR branches, 20-24 blocks\non the MR branch, and 4-6 blocks on the LR branch.\nWindow sizes are set to 1,2,7,7 for 4 branches. We\nuse relatively large MixCFN expansion ratios in small\nvariants for performance and reduce the ratio to 2 on\n5\nVariant Architecture design Window s MixCFN ratior ChannelC Head dimdk\nHRViT-b1\n1\n2\n7\n7\n4\n4\n4\n4\n32\n64\n128\n256\n16\n32\n32\n32\nHRViT-b2\n1\n2\n7\n7\n2\n3\n3\n3\n48\n96\n240\n384\n24\n24\n24\n24\nHRViT-b3\n1\n2\n7\n7\n2\n2\n2\n2\n64\n128\n256\n512\n32\n32\n32\n32\nTable 2: Architecture variants of HRViT. #blocks is marked in each module. Per branch settings are shown.\nVariant Image Size #Params\n(M) GFLOPs IMNet-1K\ntop-1 acc.\nHRViT-b1 224 19.7 2.7 80.5\nHRViT-b2 224 32.5 5.1 82.3\nHRViT-b3 224 37.9 5.7 82.8\nTable 3: ImageNet-1K pre-training results. GFLOPs\nis measured under an image size of 224×224. #Params\nincludes the classiﬁcation head as used in HR-\nNetV2 [24].\nlarger variants for eﬃciency. We gradually follow the\nscaling rule from CSWin [9] to increase the basic chan-\nnel C for the highest resolution branch from 32 to 64.\n#Blocks and #channels can be ﬂexibly tuned for the\n3rd/4th branch to match a speciﬁc hardware cost.\n3. Experiments\nWe pretrain all models on ImageNet-1K and conduct\nexperiments on ADE20K [36] and Cityscapes [7] for\nsemantic segmentation. We compare the performance\nand eﬃciency of our HRViT with SoTA ViT backbones,\ni.e., Swin [17], Twins [5], MiT [29], and CSWin [9].\n3.1. Semantic segmentation on ADE20K and\nCityscapes\nOn semantic segmentation tasks, HRViT achieves\nthe best performance-eﬃciency Pareto front, surpass-\ning the SoTA MiT and CSWin under diﬀerent set-\ntings. HRViT (b1-b3) outperform the previous SoTA\nSegFormer-MiT (B1-B3) [29] with +3.68, +2.26, and\n+0.80 higher mIoU on ADE20K val, and +3.13,\n+1.81, +1.46 higher mIoU on Cityscapes val.\nImageNet-1K pre-training. All ViT models are\npre-trained on ImageNet-1K. We follow the same pre-\ntraining settings as DeiT [23] and other ViTs [9,17,29].\nWe adopt stochastic depth [14] for all HRViT variants\nwith the max drop rate of 0.1. The drop rate is gradu-\nally increased on the deepest 3rd branch, and other\nshallow branches follow the rate of the 3rd branch\nwithin the same module. We use the HRNetV2 [24]\nclassiﬁcation head in HRViT on ImageNet-1K pre-\ntraining. The pre-training results are in Table 3.\nSettings. We evaluate HRViT for semantic segmen-\ntation on the Cityscapes and ADE20K datasets. We\nemploy a lightweight SegFormer [29] head based on the\nmmsegmentation framework [6]. We follow the training\nsettings of prior work [9,29]. The training image size for\nADE20K and Cityscapes are 512×512 and 1024×1024,\nrespectively. We use an AdamW optimizer for 160 k\niterations using a ’poly’ learning rate schedule, 1,500\nsteps of linear warm-up, an initial learning rate of 6e-\n5, a mini-batch size of 16, and a weight decay rate of\n0.01. The test image size for ADE20K and Cityscapes\nis set to 512 ×2048 and 1024 ×2048, respectively. We\ndo inference on Cityscapes with sliding window test by\ncropping 1024×1024 patches.\nResults on ADE20K. We evaluate diﬀerent ViT\nbackbones in single-scale mean intersection-over-union\n(mIoU), #Params, and GFLOPs. Figure 5 plots the\nPareto curves in the #params and FLOPs space. On\nADE20K val, HRViT outperforms other ViTs with\nbetter performance and eﬃciency trade-oﬀ. For exam-\nple, with the SegFormer head, HRViT-b1 outperforms\nMiT-B1 with 3.68% higher mIoU, 40% fewer parame-\nters, and 8% less computation. Our HRViT-b3 achieves\na higher mIoU than the best CSWin-S but saves 23%\nparameters and 13% FLOPs. Compared with the con-\nvolutional HRNetV2+OCR, our HRViT shows consid-\n6\n32\n36\n40\n44\n48\n52\n0 50 100 150 200 250 300\nADE20K SS mIoU (%)\nGFLOPs\n32\n36\n40\n44\n48\n52\n0 50 100 150\nADE20K SS mIoU (%)\n#Params\nMiT+SegFormer [29]\nCSWin+FPN [9]\nFCN-R50 [18]\nPVT+FPN [25]\nHRNet-W48+OCR [24]\nDeepLabV3+R101 [4]\nSwin [17]\nb1\nb2\nb3 CSWin+SegFormer [9]HRViT \n(ours) Twins [5]\nMiT+SegFormer [29]\nCSWin+FPN [9]\nFCN-R50 [18]\nPVT+FPN [25]\nHRNet-W48+OCR [24] DeepLabV3\n+R101 [4]\nSwin [17]\nb1\nb2\nb3 CSWin+SegFormer [9]HRViT \n(ours)\nTwins [5]\nmIoU\nMiT-B1 42.20\nCSWin-Ti 41.43\nHRViT-b1 45.88\nMiT-B2 46.50\nCSWin-T 47.88\nHRViT-b2 48.76\nMiT-B3 49.40\nCSWin-S 49.93\nHRViT-b3 50.20\nSegFormer Head\nFigure 5: HRViT achieves the best Pareto front compared with other models on ADE20K val.\nSegFormer Head [29]Backbone #Param. (M)↓ GFLOPs↓ mIoU (%)↑\nMiT-B0 [29] 3.8 8.4 76.20\nMiT-B1 [29] 13.7 15.9 78.50\nCSWin-Ti [9] 5.9 11.4 79.16\nHRViT-b1 8.1 14.1 81.63\nMiT-B2 [29] 27.5 62.4 81.00\nCSWin-T [9] 22.4 28.3 81.56\nHRViT-b2 20.8 27.4 82.81\nMiT-B3 [29] 47.3 79.0 81.70\nMiT-B4 [29] 64.1 95.7 82.30\nCSWin-S [9] 37.3 78.1 82.58\nHRViT-b3 28.6 66.3 83.16\nAvg improv. -30.7% -22.3% +2.16\nTable 4: Compare diﬀerent ViT backbones on the\nCityscapes val segmentation dataset. CSWin-Ti is\na slimmed CSWin-T with half channels (64 →32).\nFLOPs are measured on the image size of 512 ×512.\nerable performance advantages with signiﬁcant hard-\nware eﬃciency boost.\nResults on Cityscapes. In Table 4, our small\nmodel HRViT-b1 outperforms MiT-B1 and CSWin-Ti\nby +3.13 and +2.47 higher mIoU, which shows the\nlarger eﬀective width of HR architectures is especially\neﬀective on slim networks.\nWhen training HRViT-b3 on Cityscapes, we set\nthe multi-branch window settings to 1-2-9-9. HRViT-\nb3 outperforms the MiT-b4 with +0.86 higher mIoU,\n55.4% fewer parameters, and 30.7% lower FLOPs.\nCompared with two SoTA ViT backbones, i.e., MiT\nand CSWin, HRViT achieves an average of +2.16\nhigher mIoU with 30.7% fewer parameters and 22.3%\nless computation.\nVariants #Params\n(M)\nFLOPs\n(G)\nIMNet\ntop-1 acc.\nCity\nmIoU\nHRViT-b1 8.1 14.1 80.52 81.63\n−Key-value sharing 8.8 14.7 80.52 81.00\n−Eﬀ. patch embed 9.9 16.5 80.19 81.18\n−MixCFN 7.9 13.6 79.86 80.52\n−Parallel CONV path8.1 14.0 80.06 80.82\n−Nonlinearity/BN 8.1 14.1 80.37 81.12\n−Dense fusion 8.0 14.0 79.95 81.26\n−DES 8.1 14.0 80.36 81.38\n−All block opt. 10.1 16.3 79.79 80.45\nTable 5: Ablation on proposed techniques. Each en-\ntry removes one technique independently. The last one\nremoves all block optimization techniques.\n3.2. Ablation studies\nIn Table 5, we independently remove each technique\nfrom HRViT and evaluate on ImageNet and Cityscapes.\nSharing key-value. When removing key-value shar-\ning, i.e., using independent keys and values, HRViT-b1\nshows the same ImageNet-1K accuracy but at the cost\nof lower Cityscapes segmentation mIoU, 9% more pa-\nrameters, and 4% more computations.\nPatch embedding. We change our eﬃcient patch\nembedding to the CONV-based overlapped patch em-\nbedding. We observe 22% more parameters and 17%\nmore FLOPs without accuracy/mIoU beneﬁts.\nMixCFN. Removing the mixed-scale convolutional\nfeedforward block directly leads to ∼0.66% ImageNet\naccuracy drop and 0.11% Cityscapes mIoU loss with\nmarginal eﬃciency improvement. We can observe that\nthe MixCFN block is an important technique to guar-\nantee our performance.\nParallel CONV path. The embedded inverted\nresidual path in the attention block is very lightweight\nbut contributes 0.46% higher ImageNet accuracy and\n7\nBackbone #Params\n(M)\nFLOPs\n(G)\nIMNet\ntop-1 acc.\nCity\nmIoU\nHRNet18-MiT 8.4 29.3 79.3 80.30\nHRNet18-CSWin 8.1 22.3 79.5 80.95\nHRViT-b1 8.1 14.1 80.5 81.63\nHRNet32-MiT 24.4 52.4 81.1 82.05\nHRNet32-CSWin 23.9 42.2 81.1 82.11\nHRViT-b2 20.8 27.4 82.3 82.81\nHRNet40-MiT 40.1 108.0 82.3 82.10\nHRNet40-CSWin 39.5 96.3 82.4 82.38\nHRViT-b3 28.6 66.3 82.8 83.16\nAvg Improv. -14.4% -38.5% +0.92 +0.89\nTable 6: Compare naive HRNet-ViT variants with\nHRViT on ImageNet-1K and Cityscapes val. Het-\nerogeneous branch designs with optimized blocks in\nHRViT are more eﬃcient and scalable than naive\nHRNet-ViT counterparts.\n0.81% higher mIoU.\nAdditional nonlinearity/BN. The extra Hardswish\nand BN introduce negligible overhead but boost ex-\npressivity and trainability, bringing 0.15% higher\nImageNet-1K accuracy 0.51% higher mIoU on\nCityscapes val.\nDense vs. sparse fusion layers. The sparse fu-\nsion [8] is not eﬀective in HRViT as it saves tiny hard-\nware cost (<1%) but leads to 0.57% accuracy drop and\n0.37% mIoU loss.\nDiversity-enhanced shortcut. The nonlinear\nshortcut (DES) helps improve the feature diversity\nand eﬀectively improves the performance to a higher\nlevel on multiple tasks. Negligible hardware cost is in-\ntroduced due to the high eﬃciency of the Kronecker\ndecomposition-based projector.\nNaive HRNet-ViT vs. HRViT. In Table 6, we di-\nrectly replace residual blocks in HRNetV2 with trans-\nformer blocks as a naive baseline. When comparing\nHRNet-MiT with the sequential MiT, we notice the\nHR variants have comparable mIoUs while signiﬁcantly\nsaving hardware cost. This shows that themulti-branch\narchitecture is indeed helpful to boost the multi-scale\nrepresentability. However, the naive HRNet-ViT over-\nlooks the expensive cost of transformers. Thus it is not\nscalable as the hardware cost quickly outweigh its per-\nformance gain. In contrast, our heterogeneous branches\nand optimized components achieve good control of the\nhardware cost, enhance the model representability, and\nmaintain good scalability.\n4. Related Work\nMulti-scale representation learning. Previous\nCNNs and ViTs progressively down-sample the fea-\nture map to compute the LR representations [4,10,18],\nand recover the HR features via up-sampling, e.g., Seg-\nNet [1], UNet [21], Hourglass [19]. HRNet [24] main-\ntains the HR representations throughout the network\nwith cross-resolution fusion. Lite-HRNet [31] proposes\nconditional channel weighting blocks to exchange in-\nformation across resolutions. HR-NAS [8] searches the\nchannel/head settings for inverted residual blocks and\nthe auxiliary transformer branches. HRFormer [34]\nimproves HRNetV2 by replacing residual blocks with\nSwin transformer blocks. Diﬀerent from the HRNet-\nfamily, HRViT is a pure ViT backbone with a novel\nmulti-branch topology that beneﬁts both from HR ar-\nchitectures and self-attentions. Besides, we explore\nheterogeneous branch design and block optimization\nto boost the hardware eﬃciency.\nMulti-scale ViT backbones. Several multi-scale\nViTs adopt hierarchical architectures to generate pro-\ngressively down-sampled pyramid features, but they\nstill follow the design concept of classiﬁcation net-\nworks with a sequential topology, e.g., PVT [25],\nCrossViT [3], Swin [17], Twins [5], SegFormer [29],\nMViT [11], CSWin [9]. However, there is no informa-\ntion ﬂow from LR to HR features inside the ViT back-\nbone, and the HR features are still very shallow ones\nof relatively low quality. In contrast, HRViT adopts a\nmulti-branch network topology with enhanced multi-\nscale representability and improved eﬃciency.\n5. Conclusion\nIn this paper, we delve into the multi-scale repre-\nsentation learning in vision transformers and present\nan eﬃcient multi-scale high-resolution ViT backbone\ndesign, named HRViT. To fully exploit the potentials\nof ViTs in dense prediction tasks, we enhance ViT\nbackbones with a multi-branch architecture to enable\nhigh-quality HR representation and cross-scale interac-\ntion. To scale up HRViT, we jointly optimize key build-\ning blocks with eﬃcient embedding layers, augmented\ncross-shaped attentions, and mixed-scale convolutional\nfeedforward networks. Our architecture-block co-\ndesign pushes the performance-eﬃciency Pareto front\nto a new level. Extensive experiments show that\nHRViT outperform state-of-the-art vision transformer\nbackbone designs with signiﬁcant performance im-\nprovement with lower hardware cost.\n8\nReferences\n[1] V. Badrinarayanan, A. Kendall, and R. Cipolla.\nSegnet: A deep convolutional encoder-decoder ar-\nchitecture for image segmentation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelli-\ngence, page 2481–2495, 2017.\n[2] Nicolas Carion, Francisco Massa, Gabriel Syn-\nnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-End object detection\nwith transformers. In Proc. ECCV, 2020.\n[3] Chun-Fu Chen, Quanfu Fan, and Rameswar\nPanda. Crossvit: Cross-attention multi-scale\nvision transformer for image classiﬁcation. In\nProc. ICCV, 2021.\n[4] Liang-Chieh Chen, Yukun Zhu, George Pa-\npandreou Florian Schroﬀ, and Hartwig Adam.\nEncoder-decoder with atrous separable convo-\nlution for semantic image segmentation. In\nProc. ECCV, 2018.\n[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo\nZhang, Haibing Ren, Xiaolin Wei, Huaxia Xia,\nand Chunhua Shen. Twins: Revisiting the De-\nsign of Spatial Attention in Vision Transformers.\nIn Proc. NeurIPS, 2021.\n[6] MMSegmentation Contributors. MMSegmenta-\ntion: Openmmlab semantic segmentation toolbox\nand benchmark. https://github.com/open-\nmmlab/mmsegmentation, 2020.\n[7] Marius Cordts, Mohamed Omran, Sebastian\nRamos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt\nSchiele. The cityscapes dataset for semantic ur-\nban scene understanding. In Proc. CVPR, 2016.\n[8] Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng\nWang, Xiaojie Jin, Zhiwu Lu, and Ping Luo. HR-\nNAS: Searching Eﬃcient High-Resolution Neural\nArchitectures with Lightweight Transformers. In\nProc. CVPR, 2021.\n[9] Xiaoyi Dong, Jianmin Bao, Dongdong Chen,\nWeiming Zhang, Nenghai Yu, Lu Yuan, Dong\nChen, and Baining Guo. CSWin Transformer: A\nGeneral Vision Transformer Backbone with Cross-\nShaped Windows, 2021.\n[10] A. Dosovitskiy, L. Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, M. Dehghani, Matthias Minderer, G.\nHeigold, S. Gelly, Jakob Uszkoreit, and N.\nHoulsby. An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale. In\nProc. ICLR, 2021.\n[11] Haoqi Fan, Bo Xiong, Karttikeya Mangalam,\nYanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision trans-\nformers. Arxiv preprint 2104.11227, 2021.\n[12] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron,\nPierre Stock, Armand Joulin, Herv´ e J´ egou, and\nMatthijs Douze. LeViT: a Vision Transformer in\nConvNet’s Clothing for Faster Inference. Arxiv\npreprint 2104.01136, 2021.\n[13] Daniel Haase and Manuel Amthor. Rethink-\ning Depthwise Separable Convolutions: How\nIntra-Kernel Correlations Lead to Improved Mo-\nbileNets. In Proc. CVPR , pages 14588–14597,\n2020.\n[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra,\nand Kilian Q Weinberger. Deep networks with\nstochastic depth. In Proc. ECCV, 2016.\n[15] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Tim-\nofte, and Luc Van Gool. Localvit: Bringing\nlocality to vision transformers. arXiv preprint\narXiv:2104.05707, 2021.\n[16] Iasonas Kokkinos Liang-Chieh Chen, George Pa-\npandreou, Kevin Murphy, and Alan L Yuille.\nSemantic image segmentation with deep convo-\nlutional nets and fully connected CRFs. In\nProc. ICLR, 2015.\n[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan\nWei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin Transformer: Hierarchical Vision Trans-\nformer using Shifted Windows. In Proc. ICCV,\n2021.\n[18] Jonathan Long, Evan Shelhamer, and Trevor Dar-\nrell. Fully convolutional networks for semantic seg-\nmentation. In Proc. CVPR, 2015.\n[19] A. Newell, K. Yang, and J. Deng. Stacked hour-\nglass networks for human pose estimation. In\nProc. ECCV, page 483–499, 2016.\n[20] Maithra Raghu, Thomas Unterthiner, Simon Ko-\nrnblith, Chiyuan Zhang, and Alexey Dosovitskiy.\nDo Vision Transformers See Like Convolutional\nNeural Networks? Arxiv preprint 2108.08810 ,\n2021.\n[21] Olaf Ronneberger, Philipp Fischer, and Thomas\nBrox. U-Net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Com-\nputing and Computer-Assisted Intervention (MIC-\nCAI), May 2015.\n[22] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yip-\ning Deng, Chao Xu, and Yunhe Wang. Aug-\nmented Shortcuts for Vision Transformers. In\nProc. NeurIPS, 2021.\n9\n[23] Hugo Touvron, Matthieu Cord, Matthijs Douze,\nFrancisco Massa, Alexandre Sablayrolles, and\nHerve Jegou. Training data-eﬃcient image trans-\nformers: distillation through attention. In\nProc. ICML, pages 10347–10357, 2021.\n[24] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui\nJiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang,\nWenyu Liu, and Bin Xiao. Deep High-Resolution\nRepresentation Learning for Visual Recognition.\nIEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 43(10):3349–3364, 2021.\n[25] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping\nFan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid Vision Transformer: A\nVersatile Backbone for Dense Prediction without\nConvolutions. In Proc. ICCV, 2021.\n[26] Wenxiao Wang, Lu Yao, Long Chen, Deng Cai,\nXiaofei He, and Wei Liu. CrossFormer: A Versa-\ntile Vision Transformer Based on Cross-scale At-\ntention. arXiv preprint arXiv:2108.00154 , 2021.\n[27] Yuqing Wang, Zhaoliang Xu, Xinlong Wang,\nChunhua Shen, Baoshan Cheng, Hao Shen, , and\nHuaxia Xia. End-to-end video instance segmenta-\ntion with transformers. In Proc. CVPR, 2021.\n[28] Tete Xiao, Mannat Singh, Eric Mintun, Trevor\nDarrell, Piotr Doll´ ar, and Ross Girshick. Early\nConvolutions Help Transformers See Better.Arxiv\npreprint 2106.14881, 2021.\n[29] Enze Xie, Wenhai Wang, Zhiding Yu, Anima\nAnandkumar, Jose M Alvarez, and Ping Luo.\nSegFormer: Simple and Eﬃcient Design for Se-\nmantic Segmentation with Transformers. In\nProc. NeurIPS, 2021.\n[30] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen\nTu. Co-scale conv-attentional image transformers.\nIn Proc. ICCV, 2021.\n[31] Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan,\nLei Zhang, Nong Sang, and Jingdong Wang. Lite-\nHRNet: A Lightweight High-Resolution Network.\nIn Proc. CVPR, 2021.\n[32] Qihang Yu, Yingda Xia, Yutong Bai, Yongyi\nLu, Alan Yuille, and Wei Shen. Glance-\nand-Gaze Vision Transformer. arXiv preprint\narXiv:2106.02277, 2021.\n[33] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu,\nYujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token ViT:\nTraining vision transformers from scratch on ima-\ngenet. arXiv preprint arXiv:2101.11986 , 2021.\n[34] Yuhui Yuan, Rao Fu, Lang Huang, Weihong\nLin, Chao Zhang, Xilin Chen, and Jingdong\nWang. HRFormer: High-Resolution Transformer\nfor Dense Prediction. In Proc. NeurIPS, 2021.\n[35] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin\nXiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision trans-\nformer for high-resolution image encoding. arXiv\npreprint arXiv:2103.15358, 2021.\n[36] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler,\nAdela Barriuso, , and Antonio Torralba. Scene\nparsing through ade20k dataset. In Proc. CVPR,\n2017.\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7320038676261902
    },
    {
      "name": "Transformer",
      "score": 0.6514691710472107
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.6509748697280884
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5978397130966187
    },
    {
      "name": "Segmentation",
      "score": 0.542049765586853
    },
    {
      "name": "FLOPS",
      "score": 0.48239338397979736
    },
    {
      "name": "Machine vision",
      "score": 0.4176372289657593
    },
    {
      "name": "Benchmarking",
      "score": 0.41478103399276733
    },
    {
      "name": "Computer vision",
      "score": 0.4012911319732666
    },
    {
      "name": "Computer engineering",
      "score": 0.35782214999198914
    },
    {
      "name": "Machine learning",
      "score": 0.35308003425598145
    },
    {
      "name": "Engineering",
      "score": 0.1296178698539734
    },
    {
      "name": "Parallel computing",
      "score": 0.11902570724487305
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ]
}