{
    "title": "Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective",
    "url": "https://openalex.org/W4225592884",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3092360317",
            "name": "Emmanuelle Salin",
            "affiliations": [
                "Aix-Marseille Université",
                "Laboratoire d’Informatique et Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A4221468836",
            "name": "Badreddine Farah",
            "affiliations": [
                "Sorbonne Université",
                "Université Sorbonne Paris Nord"
            ]
        },
        {
            "id": "https://openalex.org/A2051736877",
            "name": "Stéphane Ayache",
            "affiliations": [
                "Laboratoire d’Informatique et Systèmes",
                "Aix-Marseille Université"
            ]
        },
        {
            "id": "https://openalex.org/A1869572900",
            "name": "Benoit Favre",
            "affiliations": [
                "Laboratoire d’Informatique et Systèmes",
                "Aix-Marseille Université"
            ]
        },
        {
            "id": "https://openalex.org/A3092360317",
            "name": "Emmanuelle Salin",
            "affiliations": [
                "Laboratoire d’Informatique et Systèmes",
                "Centre National de la Recherche Scientifique",
                "Université de Toulon"
            ]
        },
        {
            "id": "https://openalex.org/A4221468836",
            "name": "Badreddine Farah",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2051736877",
            "name": "Stéphane Ayache",
            "affiliations": [
                "Centre National de la Recherche Scientifique",
                "Laboratoire d’Informatique et Systèmes",
                "Université de Toulon"
            ]
        },
        {
            "id": "https://openalex.org/A1869572900",
            "name": "Benoit Favre",
            "affiliations": [
                "Laboratoire d’Informatique et Systèmes",
                "Centre National de la Recherche Scientifique",
                "Université de Toulon"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3191974319",
        "https://openalex.org/W3024464497",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2251329024",
        "https://openalex.org/W3141280416",
        "https://openalex.org/W3127384563",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W2533598788",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W3038476992",
        "https://openalex.org/W3134873017",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3113067643",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W3105232955",
        "https://openalex.org/W3177487519",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3184784418",
        "https://openalex.org/W3104379732",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3106784008",
        "https://openalex.org/W3131151401",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W3168640669",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W3091588028"
    ],
    "abstract": "In recent years, joint text-image embeddings have significantly improved thanks to the development of transformer-based Vision-Language models. Despite these advances, we still need to better understand the representations produced by those models. In this paper, we compare pre-trained and fine-tuned representations at a vision, language and multimodal level. To that end, we use a set of probing tasks to evaluate the performance of state-of-the-art Vision-Language models and introduce new datasets specifically for multimodal probing. These datasets are carefully designed to address a range of multimodal capabilities while minimizing the potential for models to rely on bias. Although the results confirm the ability of Vision-Language models to understand color at a multimodal level, the models seem to prefer relying on bias in text data for object position and size. On semantically adversarial examples, we find that those models are able to pinpoint fine-grained multimodal differences. Finally, we also notice that fine-tuning a Vision-Language model on multimodal tasks does not necessarily improve its multimodal ability. We make all datasets and code available to replicate experiments.",
    "full_text": "Are Vision-Language Transformers Learning Multimodal Representations? A\nProbing Perspective\nEmmanuelle Salin1, Badreddine Farah2, St´ephane Ayache1, Benoit Favre1\n1Aix Marseille Univ, Universit´e de Toulon, CNRS, LIS, Marseille, France\n2 ´Ecole Sup Galil´ee, Universit´e Sorbonne Paris Nord, France\n{emmanuelle.salin,sephane.ayache,benoit.favre}@lis-lab.fr, badreddine.farah@edu.univ-paris13.fr\nAbstract\nIn recent years, joint text-image embeddings have signifi-\ncantly improved thanks to the development of transformer-\nbased Vision-Language models. Despite these advances, we\nstill need to better understand the representations produced by\nthose models. In this paper, we compare pre-trained and fine-\ntuned representations at a vision, language and multimodal\nlevel. To that end, we use a set of probing tasks to evaluate\nthe performance of state-of-the-art Vision-Language models\nand introduce new datasets specifically for multimodal prob-\ning. These datasets are carefully designed to address a range\nof multimodal capabilities while minimizing the potential for\nmodels to rely on bias. Although the results confirm the abil-\nity of Vision-Language models to understand color at a multi-\nmodal level, the models seem to prefer relying on bias in text\ndata for object position and size. On semantically adversarial\nexamples, we find that those models are able to pinpoint fine-\ngrained multimodal differences. Finally, we also notice that\nfine-tuning a Vision-Language model on multimodal tasks\ndoes not necessarily improve its multimodal ability. We make\nall datasets and code available to replicate experiments.\nIntroduction\nVision-Language (VL) tasks consist in jointly processing a\npicture and a text related to the picture. VL tasks, such as\nvisual question answering, cross modal retrieval or gener-\nation, are notoriously difficult because of the necessity for\nmodels to build sensible multimodal representations that can\nrelate fine-grained elements of the text and the picture. Fol-\nlowing the success of pre-trained transformers for language\nmodeling such as BERT (Devlin et al. 2018), the community\nhas proposed various transformer-based models, such as Vil-\nBERT (Lu et al. 2019), LXMERT (Tan and Bansal 2019),\nVLBERT (Su et al. 2019), UNITER (Chen et al. 2020), OS-\nCAR (Li et al. 2020b), VinVL (Zhang et al. 2021), ViLT\n(Kim, Son, and Kim 2021) or ERNIE-VIL (Yu et al. 2021),\nthat combine representations from both the text and image\nmodalities to reach state-of-the-art results in several multi-\nmodal tasks. Similar models have been developed in the field\nof video-language pre-training, such as ClipBERT (Lei et al.\n2021) and HERO (Li et al. 2020a).\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nWhile the results are impressive, it is important to under-\nstand how multimodal information is encoded in the repre-\nsentations learned by those models, and how affected they\nare by various bias and properties of their training data.\nA few studies have been conducted to better understand\nthose models and their representations. (Cao et al. 2020)\nhave probed attention heads at various layers of the mod-\nels, showing that textual modality is more important than\nvisual modality for model decisions. This prevalence of\nlanguage over vision in multimodal models is not specific\nto transformer-based representation models, as noticed by\n(Goyal et al. 2017a). (Li, Gan, and Liu 2020) have looked\ninto the robustness of the representations to manipulations\nof the input, compared to more traditional models. (Hen-\ndricks and Nematzadeh 2021) relied on probing tasks to\nstudy verb understanding in pre-trained transformer-based\nmodels and determined that models learn less multimodal\nconcepts associated to verbs than to subjects and objects.\nWhile these studies have shed light on some particular as-\npects of transformer-based VL models, they lack a more sys-\ntematic analysis of monomodal biases that impede the nature\nof the learned representations.\nIn that light, we are interested in studying the multimodal\ncapacity of VL representations, and in exploring what in-\nformation is learned and forgotten between pre-training and\nfine-tuning, as we think this could show the current limits\nof the pre-training process. Inspired by probing tasks de-\nveloped in the Natural Language Processing field, we probe\nthree VL models: UNITER, LXMERT and ViLT to answer\nthose questions. We probe both pre-trained and fine-tuned\nmodels. We propose probing tasks and collect associated\ndatasets to evaluate the monomodal and multimodal capa-\nbilities of those models over a range of concepts. We find\nthat UNITER reaches better overall results on the language\nmodality, while ViLT reaches better results on the vision\nmodality. Finally, we notice that while the models show their\nability to identify colors, they do not yet have multimodal\ncapacity to distinguish object size and position. We make\nthe set of monomodal and multimodal probing tasks, as well\nas all software developed for this study, available for further\nresearch1.\n1https://github.com/ejsalin/vlm-probing\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11248\nRelated Work\nVision-Language models Transformer-based VL models\nare typically trained from image/caption pairs. The text is\ntokenized and projected to an embedding space with addi-\ntive position encoding, similar to BERT. The image is usu-\nally encoded using a Faster RCNN (Anderson et al. 2018)\nto extract a sequence of object-region representations with\nadditive position and shape embeddings. However some re-\ncent models, such as ViLT and SOHO (Huang et al. 2021)\nswap the object-based representations for grid-based repre-\nsentations trained from scratch.\nAfter encoding, the representations from each modality\nare passed through a transformer following one of two ar-\nchitectures. The single-stream architecture, used in UNITER\nand ViLT, relies on a single transformer spanning inputs\nfrom both modalities. The dual-stream architecture, used in\nLXMERT, inputs each modality in its own transformer, the\noutputs of which are fed to a cross-modal transformer.\nVL models are pre-trained using text-oriented, image-\noriented and cross modal losses. While the text-oriented\nloss reflects that of language models (Masked Language\nModeling, i.e. MLM), the image loss varies from feature\nregression tasks, to object class prediction tasks such as\nfor VilBERT (Lu et al. 2019). Most models, like ViLT,\nUNITER and LXMERT adopt the Image Text Matching\n(ITM) task for cross-modal pre-training. Some models add\nother pre-training tasks to complete their multimodal knowl-\nedge. In addition, some models rely on other specific pre-\ntraining tasks such as Visual Question Answering (VQA) in\nLXMERT, word-region alignment in UNITER, scene graphs\nin ERNIE-VIL (Yu et al. 2021) or object semantics in OS-\nCAR (Li et al. 2020b).\nIn order to reach state-of-the-art performances on down-\nstream multimodal tasks such as VQA (Goyal et al. 2017b)\nand NLVR2 (Suhr et al. 2018), those models need fine-tuned\non the target task.\nModel probing Probing tasks have been first developed to\nanalyse language models through benchmarks such as Sen-\ntEval (Conneau and Kiela 2018). For example, (Hewitt and\nManning 2019) showed that syntactic parse trees can be in-\nferred from ELMO and BERT representations.\nAlthough explainability has been largely explored in vi-\nsion models, the use of probing tasks is more limited. Re-\ncently, Basaj et al. (2021) have developed a visual prob-\ning framework by constructing visual equivalents to words\nbased on superpixels. It then translated language probing\ntasks such as sentence length and semantic odd man out to\nthe vision modality.\nCao et al. (2020) study how the transformer architec-\nture impacts the learning process of single-stream and dual\nstream models. They observe the role of each layer as\nwell as the fusion of the vision and language modalities\nacross layers. They in particular notice the prevalence of\nthe language modality over the visual modality, which we\nstudy further in our work, by evaluating the multimodal\nnature of representations. Lindstr ¨om et al. (2021) explore\nthe use multimodal probing tasks such as object counting,\nand object identification to study Visual Semantic embed-\ndings. They also notice the importance of linguistic infor-\nmation in multimodal tasks. However, they do not analyse\nVL transformer-based models in their study. Hendricks and\nNematzadeh (2021) rely on probes to study verb understand-\ning in pre-trained transformer-based models. They deter-\nmine that models learn less multimodal concepts associated\nto verbs than to subjects and objects. Similar to our study,\nShekhar et al. (2017) build a dataset to evaluate if the text\nand image information in VL models are both deeply inte-\ngrated. Contrary to this dataset, we do not study the ability\nof a model to differentiate between objects from the same\nsuper-categories, but focus on multimodal concepts such as\ncolor, size and position.\nMethodology\nFramework In this paper, we aim at evaluating VL mod-\nels at a language, vision and multimodal level through their\ntext-image representations. We write V LMpre a pre-trained\ntransformer-based VL model, such as UNITER, LXMERT\nor ViLT. This model can be fine-tuned on a task T, for ex-\nample VQA or NLVR2. Fine-tuning tasks are used to em-\nbed new knowledge in the model, and to evaluate more ded-\nicated semantics or abilities related to a specific task. We\nwrite V LMfine(T) the VL model fine-tuned on task T.\nWe use probing to study the representations of V LMpre\nand V LMfine(T ) models. To evaluate the representations of\nVL models on a probing task p, we build a training dataset\nSp = {(Xj, Yj)}np\nj=1, drawn i.i.d. from Dp\nX×Y where X and\nY relate to the datasets needed to probe the models.\nThe first step of our method is to compute the final layer\nrepresentations V LMpre(X) or V LMfine(T )(X) of an in-\nstance Xj = ( ximage\nj , xcaption\nj ) of Sp through the VL\ntransformer-based model. If the probing task p studies the\ninstance at a global level, we use the representation of the\nclassification token [CLS] as input for p. If p studies the\nrepresentations of each word, the representation of WORD\ntokens are used as input for p.\nThe second step of our method is to use the represen-\ntations Rj of the [CLS] or WORD tokens as input of a\nlinear probing model PMp trained using the {(Rj, Yj)}np\nj=1\ndataset. As V LMpre or V LMfine(T) are not trained on the\nprobing task, the probing model PMp can only rely on lin-\nearly separable information the model has already learned\nto extract during pre-training or fine-tuning. As a result, the\nperformance of PMp will reflect the capability of V LMpre\nand V LMfine(T ) models to extract the information needed\nfor the probing task p.\nThe models V LMand the set of probing tasks P are\ndescribed in the following sections. Figure 1 illustrates the\nmethodology on the object counting task V-ObjCount of P.\nStudying the impact of each modality We also want to\nstudy how much VL models rely on the language and vision\nmodalities when building text-image representations. As a\nresult, for each task p build on Sp, we create another corre-\nsponding task p♣ with mismatched image and caption pairs.\nThe dataset Sp♣ is build using Sp by associating the label\nwith the image (resp. caption) and selecting at random a\n11249\nFigure 1: Probing methodology: The first step is to compute the final layer representations of the image/caption input using the\nchosen Vision Language Model. Then, we use the final layer [CLS] or word token representations Rj to train a linear probing\nmodel on the probing task. This example illustrates the methodology using the visual probing task V-ObjCount and an image\nfrom MS-COCO. This task consists in counting the number of objects in an image using (ximage\nj , xcaption\nj ) as input and Yj as\ntarget. The notation ♣ indicates the corresponding task (V-ObjCount) with mismatched instances (i.e. a caption that does not\nmatch the image and label), which uses (ximage\nj , xcaption\nk ) as input and serves as a baseline.\nmismatched caption (resp. image). For better comparison,\nall models use the same mismatched datasets.\nIf p is a language-oriented task, each instance of p♣ will\nbe (ximage\nk , xcaption\nj , Yj), with the caption corresponding to\nthe label and a wrong image. If the performance of PMp\nis similar to the performance of PMp♣, we can deduce that\nthe representations R♣\nj given by V LMare not affected by\nvisual “bias”.\nSimilarly, when p is a vision-oriented task, each instance\nof p♣ will be (ximage\nj , xcaption\nk , Yj), with the image cor-\nresponding to the label and a wrong caption. If the perfor-\nmance of PMp is similar to the performance of PMp♣, we\ncan deduce the representations R♣\nj given by V LMare not\naffected by linguistic “bias”.\nFor a multimodal probing task p, as we want to study the\npresumed prevalence of language over vision in model de-\ncisions, each instance of p♣ will be (ximage\nk , xcaption\nj , Yj),\nwith the caption corresponding to the label and a mis-\nmatched image. If V LMextracts multimodal information\nrather than only linguistic information, PMp should reach\nbetter performance than PMp♣. This is a way to control if\nPMp♣ only uses textual information or if it also uses multi-\nmodal information.\nProbing Tasks\nThe set of probing tasks P, summarized in Table 1, is com-\nposed of language-oriented tasksL, vision-oriented tasks V ,\nand multimodal tasks M. Each task of the set is built to eval-\nuate the mono-modal or multi-modal performance of V LM\non a specific capability. The tasks consist of regression, bi-\nnary or multiclass classification problems. For each of them,\na linear layer is trained as in (Hewitt and Manning 2019).\nIdeally, one would probe VL models on all properties that\nhave an impact on down-stream tasks or that help under-\nstand their behavior. However, in this paper we restrain our-\nselves to a few representative language, vision and multi-\nmodal properties. We choose and build tasks that are easy\nto implement on new datasets. For the language and vision\nproperties, we use tasks well understood in past work. For\nmultimodal properties, we create new tasks assessing mul-\ntimodal properties we think are especially relevant for VL\nmodels. We explain the tasks and their choice in the follow-\ning section.\nLanguage Probing Tasks: L\nFor language-oriented probing tasks, we choose already ex-\nisting language probing tasks and adapt them to a subset\nof 3,000 instances from Flickr30k (Young et al. 2014). We\nchoose tasks appropriate for the relatively simple structure\nof captioning datasets, and easy to transfer to a new dataset.\n• Part of Speech Tagging (L-Tagging): Part of Speech\nTagging consists in associating a word with its corre-\nsponding part of speech label, such as verb. There are 34\ncategories. This task evaluates the syntactic knowledge\npresent in the representation of individual word tokens.\nAs a result, we train a linear classifier PML-Tagging us-\ning word token representations given byV LM. To create\na gold standard for this task, we annotate the Flickr30k\ndataset using theen\ncore web sm SpaCy tagger (Honni-\nbal and Johnson 2015), which performs at 97% accuracy\non Ontonotes.\n• Bigram Shift (L-BShift): Bigram Shift (Conneau and\nKiela 2018) consists in determining whether two consec-\nutive words in a sentence have been swapped. For exam-\n11250\nInput Total Test Maj.\nTask Description Repr. Type (Metric) Dataset Instances Size (%)\nL-Tagging Part-of-speech tagging Word Multiclass (acc.) Flickr 3,000 1000 24.06\nL-BShift Bigram shift detection [CLS] Binary (acc.) Flickr 3,000 1,000 50.20\nV-Flower Fine-grained classification [CLS] Multiclass (acc.) Flower-102 7,169 1,020 0.98\nV-ObjCount Object counting [CLS] Regression (MSE) MS-COCO 2,424 624 -\nM-Col Color prediction [MASK] Multiclass (acc.) Flickr 3,000 1,000 25.30\nM-Size Size corruption detection [CLS] Binary (acc.) Flickr 2,552 752 50.67\nM-Pos Position corruption detection [CLS] Binary (acc.) Flickr 2,626 826 53.75\nM-Adv Adversarial captions [CLS] Binary (acc.) MS-COCO 700 200 50.00\nTable 1: List of probing tasks.Repr.is the representation vector used as input of the probing task.Instances indicates the number\nof image/caption instances used for the probing task. Maj. is the majority baseline.\nple, in the sentence “Peopleat relaxing the park.”, tokens\nfrom the bigram (“relaxing”, “at”) have been swapped to\ncreate a negative example caption. As this evaluates the\nglobal correctness of a sentence, we use the [CLS] token\nrepresentation given by V LM.\nVision Probing Tasks:V\nIn order to probe the vision capability of the models, we\nselected two tasks: an object counting task to assess if infor-\nmation on the general structure of an image is present in the\nrepresentation, and a fine-grained object classification task\nto evaluate whether the representations also retain informa-\ntion on fine details of objects.\n• Flower identification (V-Flower):This is a fine-grained\nobject classification task which consists in classifying\nflower pictures into 102 categories. We use the 102-\nFlower dataset (Nilsback and Zisserman 2008). As there\nis no caption available for this task, we use an empty cap-\ntion. The linear classifier PMV-Flower uses the representa-\ntion of the [CLS] token.\n• Object Counting (V-ObjCount): We build this object\ncounting task on a subset of 3,000 instances of the MS-\nCOCO dataset (Lin et al. 2014). The labels are created\nby counting the number of objects in its manual annota-\ntions. The linear regression modelPMV-ObjCount also uses\nthe [CLS] token representation. As there can be clues in\nthe caption indicating how many objects are in the image,\nsome multimodal information present in the representa-\ntion can be used for this task, which makes the use of a\nbaseline important.\nMultimodal Probing Tasks: M\nTo evaluate the multimodal information present in V LM\nrepresentations, we focus on concepts which are used to de-\nscribe objects, as those are inherently multimodal. However,\nas evaluating all those properties can be time-consuming, we\nrestrain ourselves to a few important attributes that matter in\nmany downstream applications: color, size and position. We\ncreate datasets to evaluate those attributes.\nAs we cannot evaluate all multimodal properties, we also\nassess the general multimodal competency of models, not\nlinked to a specific property. To that aim, in addition to the\nthree attribute-specific tasks, we create a task that assesses\nTask Text Input\nM-Col Two men standing behind a tall [MASK] fence.\nM-Size Two men standing behind a short black fence.\nM-Pos Two men standing in front of a tall black fence.\nM-Adv Two men running behind a tall black fence.\nFigure 2: Example of modified captions for the multimodal\nprobing tasks, using the caption “Two men standing behind\na tall black fence” as original (Flickr30k).\nhow well the model captures linguistically likely differences\nin multimodal concepts.\nThe creation of the four tasks consists in altering the cap-\ntion of half of the instances to create negative examples and\nevaluating the performance of a model in distinguishing be-\ntween positive and negative examples. The probing datasets\nare carefully designed to avoid textual bias. Figure 2 lists\naltered captions for the multimodal tasks with an example\npicture.\nWe leave as future work the evaluation through other mul-\ntimodal tasks such as specific object properties as shape or\ntexture, and global image properties such as focus, quality\nor emotion.\n• Color Identification (M-Col): This task aims at evalu-\nating precise color understanding, at a multimodal level.\nTo this end, we select 8 common colors that are un-\nlikely to be ambiguous: blue, red, black, white, yellow,\norange, green, purple. A subset of 3,000 instances from\nFlickr30k that contain those colors is used for evaluation.\n11251\nFigure 3: Colors distribution for task M-Col\nWe do not control for text bias, and use the text-only and\nmismatched baselines to analyse the results. For each in-\nstance, a color word is masked with [MASK] in the cap-\ntion. The representation of this token byV LMis used as\ninput of the linear classifier PMM-Col in order to predict\nthe missing color as in the MLM pre-training task. The\ngoal is to check whether V LMrepresentations associate\ntext and visual features to determine the masked color.\nFigure 3 represents the color distribution for the M-Col\ntask.\n• Size Identification (M-Size): This task aims at assessing\nif object size is a multimodal concept included in V LM\nrepresentations. We want to force the probing model to\nuse multimodal cues instead of textual bias for this task,\nso we build the dataset to minimize the possibility of us-\ning only linguistic cues. Instances are selected if their\ncaption contain size adjectives (large, big, long, tall or\nsmall, little, short, narrow). Then, we select among those\ncaptions 56 concrete object categories that are present\nin the test set with opposite size adjectives (i.e. large\nvs small) subject to a relatively balanced prior. To en-\nsure this balance, the least frequent variant represents at\nleast 10% of the occurrences in the subset. For example,\nthere are no examples in the dataset describing a rock\nas “small” while more than 100 describe one as “large”,\nleading this category to be left out. By comparison, if\nwe compare “small” and “large” dogs, 37% of dogs are\n“large”. This ensures that the model has limited ability\nto exploit the object category bias to determine its size.\nWe then manually create negative instances by switching\nthe adjective with its opposite. The resulting dataset is a\nsubset of 2,552 instances of Flickr30k. A linear binary\nclassifier PMM-Size is trained to determine if the caption\nhas been modified, using the [CLS] token representation.\n• Position Identification (M-Pos): This task aims at as-\nsessing if object position is a multimodal concept present\nin V LMrepresentations. For this task, we also minimize\nthe possibility of exploiting linguistic bias. Captions are\nselected based on their use of positional expressions (bot-\ntom, top, inside, outside, left, right, up, down, towards,\naway from, over, under, behind, in front of). Then, we se-\nlect among those captions 16 different contexts where an\nexpression and its opposite are both present in the dataset\nin similar proportions. For example, the top/bottom pair\nis unbalanced since there are 2,362 occurrences of top\nand 161 occurrences bottom in the dataset, while there\nare 177 occurrences of “at the top” and 78 occurrences\nof “at the bottom” which is more balanced. To ensure\nrelative balance, we select expressions where the least\nfrequent variant represents at least 30% of the occur-\nrences in the subset. The negative instances are created\nby switching an expression with its opposite. The result-\ning dataset is a subset of 2,626 instances from Flickr30k.\nWe train a linear binary classifier PMM-Size to determine\nif the caption has been modified, using the [CLS] token\nrepresentation.\n• Adversarial Captions (M-Adv): This task evaluates the\ngeneral multimodal information present in V LMrepre-\nsentations. It consists in determining if a caption matches\nan image, except that the examples are crafted in or-\nder to be challenging. For each caption from an MS-\nCOCO subset, we select words corresponding to visu-\nally relevant grammatical categories (nouns, verbs, ad-\njectives, numbers). For each target word, a likely replace-\nment is selected from the top of the distribution output by\nthe text-only BERT model. This means that the created\ncaptions, although wrong, are believable for a language\nmodel, which minimizes the possibility for multimodal\nmodels to rely on text bias. The adversarial instances are\nmanually screened for semantic and syntactic correctness\nprior to inclusion. As a result, the words replaced in the\ntest set are mainly object related, either related to people\n(15%), or from the 79 other MS-COCO categories (35%)\nor referring to other objects (26%), as well as noun and\nadjectives qualifying objects (10%), verbs (6%), words\nexpressing quantity (6%) and others (2%). Contrary to\nthe other tasks, as BERT is used to generate the adversar-\nial captions, the multimodal concepts that are evaluated\nare diverse. We train a linear binary classifier PMM-Adv\nto determine if the caption has been altered, using the\n[CLS] token representation.\nExperimental Setup\nWe choose three state-of-the-art VL models that dif-\nfer in transformer architecture and pre-training tasks:\nUNITER (single-stream with Faster RCNN visual features),\nLXMERT (dual-stream with Faster RCNN visual features),\nand ViLT (single-stream which does not use Faster RCNN\nvisual features). Although there are alternatives, we choose\nthose models as they are representative of different types of\narchitectures. We list the pre-training tasks of all three mod-\nels in Table 2. The training protocol of those models vary,\nand they are not pre-trained on the same datasets.\nUNITER LXMERT ViLT\nLanguage task Masked Language Modeling\nVision tasks Region Classification n/aFeature Regression\nMultimodal tasks Image-Text Matching\nWRA VQA n/a\nTable 2: Pre-training tasks used by UNITER, LXMERT and\nViLT. Abbreviations are Word-Region Alignment (WRA)\nand Visual Question Answering (VQA).\n11252\nEach V LM is studied as a pre-trained model and as\na fine-tuned model on fine-tuning tasks T = V QAand\nT = NLV R. We choose these tasks as they differ from the\ntasks used for pre-training and therefore require non-trivial\nmodel fine-tuning. They are also very popular when evaluat-\ning VL models and necessitate fine-grained multimodal un-\nderstanding. VQA is a visual question answering task while\nNLVR2 consists in determining whether a sentence is cor-\nrect using a pair of images as input. Our goal is to explore\nthe effect of the tasks T on probing task performances.\nIn addition, we compare the performance of V LM to\nmonomodal baselines BERT, ResNet (He et al. 2016) and\nViT (Dosovitskiy et al. 2020) for a better understanding of\nthe performance that can be reached using a single modality.\nFor all three models, we use the available check-\npoints for V LMpre. For UNITER and LXMERT, we fine-\ntune the models using authors’ instructions, to obtain\nV LMfine(V QA) and V LMfine(NLV R). For ViLT, we use\nthe available checkpoints. For the BERT and ViT baselines,\nwe follow the same instructions as theV LMmodels for the\nrepresentations, which are of dimension 768. For the ResNet\nbaseline, we use the whole final layer representation, which\nis of dimension 2048. We use the pre-trained models from\nPytorch (Paszke et al. 2019) and Hugging Face (Wolf et al.\n2020) for the experiments.\nThe probing model PM is a linear model trained over\n30 epochs for M, V and L-BShift tasks and 50 epochs for\nL-Tagging, with a learning rate of 0.001. We used MSE loss\nto train PMV-ObjCount and report RMSE as a metric to eval-\nuate V-ObjCount, and the cross entropy loss for all other\nprobing tasks, with accuracy as metric. The results of each\nprobing task are averaged over 5 runs. We trained the models\non a cuda75-capable GPU.\nResults\nThis section is organized according to the model analysed\nand to the modality of the probing task.\nPre-trained Models\nLanguage (pre-trained) Table 3 shows the results of the\nV LMpre representations for L probing tasks: part-of-speech\ntagging (L-Tagging) and bigram shift (L-BShift). Results for\nthe L-Tagging task are close for all models. For L-BShift,\nBERT reaches the best results with an accuracy of 86.33, and\nUNITER has a higher performance than the others V LMs.\nWe notice that using wrong images as input for these tasks\nimpacts negatively UNITER.\nUNITER LXMERT ViLT BERT\nL-Tagging 94.66 95.13 96.27 95.57\nL-Tagging♣ 90.86 95.36 96.16 -\nL-BShift 80.89 70.65 72.08 86.33\nL-BShift♣ 76.25 72.22 71.05 -\nTable 3: Language probing: Accuracy of the pre-trained VL\nmodels. ♣ indicates mismatched instances.\nUNITER LXMERT ViL\nT ResNet VIT\nV-Flo\nwer 71.82 75.56 91.34 86.83 99.66\nTable\n4: Vision probing: Accuracy of pre-trained VL models\nfor the fine-grained classification task (V-Flower).\nVision (pre-trained) Tables 4 and 5 show the results of\nV LMpre representations for V probing tasks: fine-grained\nclassification (V-Flower) and object counting (V-ObjCount).\nWe notice that the ViLT reaches significantly better re-\nsults than both UNITER and LXMERT models. V-Flower\nis an image-only task, so we use an empty caption. On the\nV-Flower task, ViLT is better than the ResNet baseline.\nOn the V-ObjCount task, the metric symbolises the aver-\nage object count error for different models. The results show\nthat using the associated caption significantly improves the\nobject counting results. It shows that V LMpre models use\nlinguistic cues for V-ObjCount. The performance of ViLT\ndrops using when using mismatched captions, but it re-\nmains better than the vision-only baseline VIT. UNITER and\nLXMERT, however, barely reach this performance using the\nright caption.\nMultimodality (pre-trained) Table 6 shows the results\nfor the multimodal probing tasks.\nOn the color prediction (M-Col) and adversarial examples\n(M-Adv) tasks, VL models reach much higher results than\nthe monomodal baselines. UNITER and ViLT have better\nperformance than LXMERT for M-Col, with an accuracy of\n86.27 and 85.97, while LXMERT reaches 71.21. UNITER\nalso has significantly better results than the other two mod-\nels for the M-Adv task. We notice that LXMERT has better\nresults when using the wrong image, on the M-Adv ♣ and\nM-Col♣ tasks. It means that the LXMERT performances\nare both lower and more dependent on linguistic cues than\nUNITER and ViLT, which extract more visual information.\nFor the M-Size and M-Pos tasks, UNITER and LXMERT\nyield similar results while ViLT shows the worst results on\nthose tasks. However, all results are close to the monomodal\nbaselines. It seems to show that VL models have a hard time\nextracting visual information related to size and position. On\nthese tasks, it seems that bias in text data is linked to the\nperformances of the models. Thus, the concepts of size and\nposition seem to not be very well understood at a multimodal\nlevel by V LMpre models.\nFine-tuned Models\nLanguage (fine-tuned) Table 7 shows the results of the\nV LMfine(V QA) and V LMfine(NLV R) representations for\nL probing tasks.\nWe notice that fine-tuning negatively impacts\nmodel performance on L-BShift, and especially for\nLXMERT. For L-Tagging, all fine-tuned models ex-\ncept LXMERT fine(NLV R) have similar performances to\npre-trained models.\nThe performance of UNITER for using wrong images are\nthe only ones which show an improvement, reaching the\nlevel of the their respective “normal” task. It seems to show\n11253\nUNITER LXMERT ViLT BERT ResNet VIT\nV-ObjCount 5.49 5.49 4.90 6.27 4.96 5.67\nV-ObjCount♣ 7.20 7.31 5.44 - 4.96 5.67\nTable 5: Vision probing: Square Root of the Mean Square Error (RMSE) for pre-trained VL models on the V-ObjCount task\n(lower is better). ♣ indicates mismatched instances.\nUNITER LXMERT ViLT BERT ViT\nM-Col 86.27 71.21 85.97 37.02 41.19\nM-Col♣ 34.80 39.33 35.69 - 41.19\nM-Size 57.15 58.43 55.45 55.66 51.76\nM-Size♣ 56.06 55.05 52.10 - 51.76\nM-Pos 55.92 54.62 48.95 56.52 52.78\nM-Pos♣ 54.06 54.68 52.37 - 52.78\nM-Adv 79.71 72.60 73.4 53.46 -\nM-Adv♣ 51.92 61.25 56.4 - -\nTable 6: Multimodal probing: Accuracy of pre-trained VL\nmodels. ♣ indicates mismatched instances.\nUNITER LXMERT ViLT\nVQA\nL-Tagging 93.84 94.14 94.79\nL-Tagging♣ 93.80 94.73 94.89\nL-BShift 79.48 65.40 69.43\nL-BShift♣ 76.92 62.74 68.32\nNLVR\nL-Tagging 94.37 88.44 95.60\nL-Tagging♣ 94.38 88.49 95.50\nL-BShift 72.74 57.10 67.18\nL-BShift♣ 72.34 57.82 67.10\nTable 7: Language probing: Accuracy of the fine-tuned VL\nmodels. ♣ indicates mismatched instances, gray cells show\nbetter performance than their V LMpre counterpart.\nthat the gap in performance of UNITER pre for mismatched\ninstances is due to a specificity of its the pre-training proto-\ncol.\nThe lower performances for the NLVR fine-tuned models\ncould be due to the fact that the NLVR task is used to hav-\ning two images as input, contrary to pre-training and probing\ntasks. The lower performance of fine-tuned LXMERT mod-\nels could show that LXMERT forgets more easily than other\nmodels the linguistic knowledge it has learned through pre-\ntraining.\nVision (fine-tuned) Tables 8 and 9 show the results of\nV LMfine(V QA) and V LMfine(NLV R) for the V probing\ntasks. For the V-Flower task, we notice an improvement\nof the fine-tuned UNITER models compared to the pre-\ntrained models. ViLT performances were already high, and\ndecreased slightly. However, LXMERT only improves with\nVQA fine-tuning.\nOn the V-ObjCount task, UNITER and LXMERT also\nshow improvements. UNITER fine-tuned models reach\nthe performance of the ResNet baseline with 4.98 for\nUNITERfine(V QA). However, LXMERTfine(NLV R) is also\nworse than the pre-trained model for this task. Additionally,\nUNITER LXMERT ViLT\nV-Flower VQA 82.91 78.80 93.11\nNLVR 82.78 74.23 91.23\nTable 8: Vision probing: Accuracy of fine-tuned VL models\nfor the V-Flower task. Gray cells show better performance\nthan their V LMpre counterpart.\nUNITER LXMERT ViLT\nVQA V-ObjCount 4.98 5.13 5.20\nV-ObjCount♣ 6.49 6.85 5.87\nNLVR V-ObjCount 4.95 5.65 4.92\nV-ObjCount♣ 6.22 6.94 5.50\nTable 9: Vision probing: RMSE for fine-tuned VL models\non the V-ObjCount task. ♣ indicates mismatched instances,\ngray cells show better performance than their V LMpre\ncounterpart.\nthe results using the wrong caption also improve, showing\nthat the increase in performance relies partly on a better ex-\ntraction of visual information.\nFine-tuning improves the vision performance of UNITER\nand, to a lesser extent, LXMERT. This seems to show that\nVQA and NLVR rely on visual information that is not lin-\nearly accessible within the pre-trained models. On the the\nother hand, it seems that fine-tuning does not improve the\nvisual capacity of ViLT, which was already similar in term\nof performance to the visual baselines for the pre-trained\nmodel. It shows that the vision performances of UNITER\nand LXMERT pre-trained models seem to be lacking, which\ncould point out that the visual pre-training of the those mod-\nels is a limiting factor. Our hypothesis is that it is easier to\nextract information from the textual input than the Faster\nRCNN features, making UNITER and LXMERT rely more\non text than image.\nMultimodality (fine-tuned) Table 10 shows the re-\nsults for the multimodal probing tasks. On the color\n(M-Col) and adversarial (M-Adv) tasks, we notice that\nfine-tuned UNITER and ViLT models have slightly lower\nperformances than their pre-trained counterpart, while\nLXMERT shows generally an increase in performance, ex-\ncept LXMERTfine(NLV R) for the M-Adv task. Indeed, for\nLXMERT especially, VQA fine-tuning leads to better per-\nformances than NLVR fine-tuning. UNITER remains the\noverall best model for those tasks, despite the improvement\nof LXMERT.\nFor the size (M-Size) and position (M-Pos) tasks, we no-\ntice a slight increase in performance for all models. This is\nmore noticeable for the M-Size task, while M-Pos results re-\n11254\nUNITER LXMERT ViLT\nVQA\nM-Col 83.39 82.60 81.23\nM-Col♣ 35.75 37.18 33.49\nM-Size 64.23 60.85 58.62\nM-Size♣ 55.96 56.60 55.24\nM-Pos 57.55 56.25 53.43\nM-Pos♣ 55.27 54.51 52.84\nM-Adv 78.37 74.90 70.07\nM-Adv♣ 49.42 61.06 52.27\nNLVR\nM-Col 82.52 78.00 83.18\nM-Col♣ 36.17 37.02 33.83\nM-Size 63.19 59.28 54.20\nM-Size♣ 59.28 54.57 53.17\nM-Pos 56.99 56.23 52.80\nM-Pos♣ 54.42 55.09 53.31\nM-Adv 77.50 68.46 68.12\nM-Adv♣ 53.17 53.46 57.42\nTable 10: Multimodal probing: Accuracy of fine-tuned VL\nmodels. ♣ indicates mismatched instances, gray cells show\nbetter performance than their V LMpre counterpart.\nmain close to the mismatched image baseline. ViLT has the\nworst results on those tasks. The improvement on these tasks\ncould be due to the fact that fine-tuning datasets are more fo-\ncused on the concepts of size and position than pre-training\ndatasets. These results seem to show that the models, and\nUNITER in particular, manage to extract additional visual\ninformation relevant to size, while they keep using linguistic\nclues for position.\nDiscussion\nLanguage-oriented probing seems to show that VL models\nhave slightly worse syntactic understanding than language-\nonly models such as BERT. This could be due to the less\nvaried syntactic structure of the captioning datasets used for\npre-training. UNITER shows overall better performances.\nVision-oriented probing seems to show that visual pre-\ntraining is a limiting factor for VL models based on Faster-\nRCNN features as UNITER and LXMERT show signifi-\ncantly worse performance than ViLT. We think that the mod-\nels rely on textual information because they cannot extract\naccurate visual information from the representation. This\nlimiting factor is consistent with what has been found in\nother studies, such as VinVL (Zhang et al. 2021), which\nshows that a better object detection model leads to better\ndownstream tasks results.\nMultimodal probing shows that pre-trained VL mod-\nels are able to capture some multimodal information, with\nUNITER reaching the best performances. While ViLT has\nshown better results on vision probing than UNITER, this\nhas not translated to the multimodal probing tasks. In par-\nticular, the weaker performance in the M-Adv task could be\ndue to the the absence of object prediction task, which could\nlimit the semantic understanding of objects for ViLT. How-\never, concepts related to object size and position are still\nnot well understood by those models. These are harder to\ngrasp because they are relatively subjective and depend on\nthe context and annotator. For those concepts, the models\nstill almost exclusively rely on linguistic cues, resulting in a\nperformance drop when they cannot rely on textual bias. In\nadditional ablation studies, we use non-curated size and po-\nsition datasets to see how the models perform when there\nare more linguistic clues. We notice that on this dataset,\nUNITER pre-trained representations reach an accuracy of\n71.66 on the M-Size probing task, and of 65.69 when us-\ning wrong images. For the M-Pos probing task, the model\nreaches 73.18 using the right images and 72.68 using the\nmismatched images. This shows that using linguistic cues is\nhelpful on these tasks on less controlled datasets. The per-\nformance of the position task seem to show that visual in-\nformation regarding this concept is even less accessible in\nrepresentations than size-related information. It could show\nthat the current visual pre-training is not enough to under-\nstand the positional relationship between objects at a multi-\nmodal level. This is especially true for ViLT, which shows\nthe worst performances on those tasks.\nContrary to our expectations, fine-tuning does not neces-\nsarily lead to better cross modal probing performance. The\nimprovements in performance on probing tasks are specific\nand not consistent from one model to another. This seems\nto point out that architecture and model pre-training are par-\nticularly important to understand multimodal concepts, and\nthat concepts that are not well understood by a pre-trained\nmodel will generally not have much improvement with fine-\ntuned models.\nFinally, our results seem to show that for some con-\ncepts, multimodal performance is dependent on the pres-\nence of textual biases in the dataset, which makes creating\ncontrolled datasets especially important. However, the re-\nliance of a model on linguistic clues for training does not\nalways help improve multimodal performance. On the con-\ntrary, LXMERT models which rely the most on linguistic\nclues for the M-Adv task will not necessarily show the best\nperformance for this task.\nConclusion\nWe evaluate Vision-Language models: UNITER, LXMERT\nand ViLT using probing tasks. We find that although they\nextract slightly less syntactic information than language-\nonly models. Additionally, we find that Faster-RCNN fea-\ntures seem to be a limiting factor for visual performances.\nAs for their multimodal capability, UNITER manages to ex-\ntract better multimodal information on some concepts, such\nas color. However, all models have trouble understanding\nless objective concepts, such as position and size. We notice\nfor those tasks an over-reliance of VL models on linguis-\ntic clues. This highlights the importance of using more con-\ntrolled datasets to evaluate multimodal performance, with-\nout allowing the models to learn linguistic bias for visual in-\nformation. For future work, it would be interesting to adapt\nVL pre-training for better multimodal performance on fine-\ngrained multimodal concepts such as position and size. We\nmake available the datasets for further experiments.\n11255\nAcknowledgments\nThis work was performed using HPC resources from\nGENCI–IDRIS (Grant 2021-[101693]).\nThe project leading to this publication has received fund-\ning from Excellence Initiative of Aix-Marseille - A*MIDEX\n(Archimedes Institute AMX-19-IET-009), a French ”In-\nvestissements d’Avenir” Programme.\nReferences\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 6077–6086.\nBasaj, D.; Oleszkiewicz, W.; Sieradzki, I.; G ´orszczak, M.;\nRychalska, B.; Trzcinski, T.; and Zielinski, B. 2021. Ex-\nplaining Self-Supervised Image Representations with Visual\nProbing. In International Joint Conference on Artificial In-\ntelligence.\nCao, J.; Gan, Z.; Cheng, Y .; Yu, L.; Chen, Y .-C.; and Liu, J.\n2020. Behind the scene: Revealing the secrets of pre-trained\nvision-and-language models. In European Conference on\nComputer Vision, 565–580. Springer.\nChen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan,\nZ.; Cheng, Y .; and Liu, J. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on com-\nputer vision, 104–120. Springer.\nConneau, A.; and Kiela, D. 2018. SentEval: An Evalua-\ntion Toolkit for Universal Sentence Representations. arXiv\npreprint arXiv:1803.05449.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and\nParikh, D. 2017a. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 6904–6913.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and\nParikh, D. 2017b. Making the V in VQA Matter: Elevating\nthe Role of Image Understanding in Visual Question An-\nswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR).\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHendricks, L. A.; and Nematzadeh, A. 2021. Prob-\ning Image-Language Transformers for Verb Understanding.\narXiv preprint arXiv:2106.09141.\nHewitt, J.; and Manning, C. D. 2019. A structural probe for\nfinding syntax in word representations. InProceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 4129–\n4138.\nHonnibal, M.; and Johnson, M. 2015. An improved non-\nmonotonic transition system for dependency parsing. In\nProceedings of the 2015 conference on empirical methods\nin natural language processing, 1373–1378.\nHuang, Z.; Zeng, Z.; Huang, Y .; Liu, B.; Fu, D.; and Fu, J.\n2021. Seeing Out of tHe bOx: End-to-End Pre-training for\nVision-Language Representation Learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 12976–12985.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. arXiv preprint arXiv:2102.03334.\nLei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.;\nand Liu, J. 2021. Less is more: Clipbert for video-and-\nlanguage learning via sparse sampling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7331–7341.\nLi, L.; Chen, Y .-C.; Cheng, Y .; Gan, Z.; Yu, L.; and Liu,\nJ. 2020a. Hero: Hierarchical encoder for video+ lan-\nguage omni-representation pre-training. arXiv preprint\narXiv:2005.00200.\nLi, L.; Gan, Z.; and Liu, J. 2020. A closer look at the ro-\nbustness of vision-and-language pre-trained models. arXiv\npreprint arXiv:2012.08673.\nLi, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.;\nWang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar:\nObject-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision , 121–\n137. Springer.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLindstr¨om, A. D.; Bensch, S.; Bj ¨orklund, J.; and Drewes,\nF. 2021. Probing Multimodal Embeddings for Linguis-\ntic Properties: the Visual-Semantic Case. arXiv preprint\narXiv:2102.11115.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vil-\nbert: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. arXiv preprint\narXiv:1908.02265.\nNilsback, M.-E.; and Zisserman, A. 2008. Automated flower\nclassification over a large number of classes. In 2008 Sixth\nIndian Conference on Computer Vision, Graphics & Image\nProcessing, 722–729. IEEE.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\n11256\nHigh-Performance Deep Learning Library. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox, E.;\nand Garnett, R., eds., Advances in Neural Information Pro-\ncessing Systems 32, 8024–8035. Curran Associates, Inc.\nShekhar, R.; Pezzelle, S.; Klimovich, Y .; Herbelot, A.; Nabi,\nM.; Sangineto, E.; and Bernardi, R. 2017. Foil it! find\none mismatch between image and language caption. arXiv\npreprint arXiv:1705.01359.\nSu, W.; Zhu, X.; Cao, Y .; Li, B.; Lu, L.; Wei, F.; and Dai, J.\n2019. Vl-bert: Pre-training of generic visual-linguistic rep-\nresentations. arXiv preprint arXiv:1908.08530.\nSuhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.; and Artzi,\nY . 2018. A corpus for reasoning about natural language\ngrounded in photographs. arXiv preprint arXiv:1811.00491.\nTan, H.; and Bansal, M. 2019. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu, J.;\nXu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest, Q.; and\nRush, A. M. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning: System Demonstrations, 38–45. Online: Association for\nComputational Linguistics.\nYoung, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.\nFrom image descriptions to visual denotations: New simi-\nlarity metrics for semantic inference over event descriptions.\nTACL, 2: 67–78.\nYu, F.; Tang, J.; Yin, W.; Sun, Y .; Tian, H.; Wu, H.; and\nWang, H. 2021. ERNIE-ViL: Knowledge Enhanced Vision-\nLanguage Representations Through Scene Graph. In AAAI.\nZhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.;\nChoi, Y .; and Gao, J. 2021. Vinvl: Revisiting visual rep-\nresentations in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5579–5588.\n11257"
}