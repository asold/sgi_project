{
  "title": "Regularizing and Optimizing LSTM Language Models",
  "url": "https://openalex.org/W2743945814",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4301100015",
      "name": "Merity, Stephen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281371573",
      "name": "Keskar, Nitish Shirish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2952507584",
    "https://openalex.org/W1588607462",
    "https://openalex.org/W2951605425",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2175402905",
    "https://openalex.org/W2565031282",
    "https://openalex.org/W2619516334",
    "https://openalex.org/W2963304263",
    "https://openalex.org/W2950297649",
    "https://openalex.org/W2436219157",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2605488176",
    "https://openalex.org/W2096388912",
    "https://openalex.org/W2742448943",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1811750039",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W1164749991",
    "https://openalex.org/W2952436057",
    "https://openalex.org/W2619808423",
    "https://openalex.org/W2622263826"
  ],
  "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
  "full_text": "arXiv:1708.02182v1  [cs.CL]  7 Aug 2017\nRegularizing and Optimizing LSTM Language Models\nStephen Merity 1 Nitish Shirish Keskar 1 Richard Socher 1\nAbstract\nRecurrent neural networks (RNNs), such as long\nshort-term memory networks (LSTMs), serve as\na fundamental building block for many sequence\nlearning tasks, including machine translation,\nlanguage modeling, and question answering. In\nthis paper, we consider the speciﬁc problem of\nword-level language modeling and investigate\nstrategies for regularizing and optimizing LSTM-\nbased models. W e propose the weight-dropped\nLSTM which uses DropConnect on hidden-to-\nhidden weights as a form of recurrent regulariza-\ntion. Further, we introduce NT -ASGD, a vari-\nant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined us-\ning a non-monotonic condition as opposed to be-\ning tuned by the user. Using these and other reg-\nularization strategies, we achieve state-of-the-art\nword level perplexities on two data sets: 57.3 on\nPenn Treebank and 65.8 on WikiT ext-2. In ex-\nploring the effectiveness of a neural cache in con-\njunction with our proposed model, we achieve an\neven lower state-of-the-art perplexity of 52.8 on\nPenn Treebank and 52.0 on WikiT ext-2.\n1. Introduction\nEffective regularization techniques for deep learning hav e\nbeen the subject of much research in recent years. Given\nthe over-parameterization of neural networks, general-\nization performance crucially relies on the ability to\nregularize the models sufﬁciently. Strategies such as\ndropout (\nSrivastava et al. , 2014) and batch normalization\n(Ioffe & Szegedy , 2015) have found great success and are\nnow ubiquitous in feed-forward and convolutional neural\nnetworks. Naïvely applying these approaches to the case\nof recurrent neural networks (RNNs) has not been highly\nsuccessful however. Many recent works have hence been\nfocused on the extension of these regularization strategie s\nto RNNs; we brieﬂy discuss some of them below .\n1 Salesforce Research, Palo Alto, USA. Correspondence to:\nStephen Merity <smerity@salesforce.com>.\nA naïve application of dropout ( Srivastava et al. , 2014)\nto an RNN’s hidden state is ineffective as it disrupts\nthe RNN’s ability to retain long term dependencies\n(\nZaremba et al. , 2014). Gal & Ghahramani (2016) propose\novercoming this problem by retaining the same dropout\nmask across multiple time steps as opposed to sampling\na new binary mask at each timestep. Another approach\nis to regularize the network through limiting updates to\nthe RNN’s hidden state. One such approach is taken by\nSemeniuta et al. (2016) wherein the authors drop updates\nto network units, speciﬁcally the input gates of the LSTM,\nin lieu of the units themselves. This is reminiscent of zone-\nout (\nKrueger et al. , 2016) where updates to the hidden state\nmay fail to occur for randomly selected neurons.\nInstead of operating on the RNN’s hidden states, one can\nregularize the network through restrictions on the recur-\nrent matrices as well. This can be done either through\nrestricting the capacity of the matrix (\nArjovsky et al. ,\n2016; Wisdom et al. , 2016; Jing et al. , 2016) or through\nelement-wise interactions ( Balduzzi & Ghifary , 2016;\nBradbury et al. , 2016; Seo et al. , 2016).\nOther forms of regularization explicitly act upon activa-\ntions such as batch normalization (\nIoffe & Szegedy , 2015),\nrecurrent batch normalization ( Cooijmans et al. , 2016), and\nlayer normalization ( Ba et al. , 2016). These all introduce\nadditional training parameters and can complicate the trai n-\ning process while increasing the sensitivity of the model.\nIn this work, we investigate a set of regularization strateg ies\nthat are not only highly effective but which can also be used\nwith no modiﬁcation to existing LSTM implementations.\nThe weight-dropped LSTM applies recurrent regulariza-\ntion through a DropConnect mask on the hidden-to-hidden\nrecurrent weights. Other strategies include the use of\nrandomized-length backpropagation through time (BPTT),\nembedding dropout, activation regularization (AR), and\ntemporal activation regularization (T AR).\nAs no modiﬁcations are required of the LSTM implemen-\ntation these regularization strategies are compatible wit h\nblack box libraries, such as NVIDIA cuDNN, which can\nbe many times faster than naïve LSTM implementations.\nEffective methods for training deep recurrent networks\nhave also been a topic of renewed interest. Once a model\nRegularizing and Optimizing LSTM Language Models\nhas been deﬁned, the training algorithm used is required\nto not only ﬁnd a good minimizer of the loss function but\nalso converge to such a minimizer rapidly. The choice of\nthe optimizer is even more important in the context of reg-\nularized models since such strategies, especially the use\nof dropout, can impede the training process. Stochastic\ngradient descent (SGD), and its variants such as Adam\n(\nKingma & Ba , 2014) and RMSprop ( Tieleman & Hinton ,\n2012) are amongst the most popular training methods.\nThese methods iteratively reduce the training loss through\nscaled (stochastic) gradient steps. In particular, Adam ha s\nbeen found to be widely applicable despite requiring less\ntuning of its hyperparameters. In the context of word-level\nlanguage modeling, past work has empirically found that\nSGD outperforms other methods in not only the ﬁnal loss\nbut also in the rate of convergence. This is in agreement\nwith recent evidence pointing to the insufﬁciency of adap-\ntive gradient methods (\nWilson et al. , 2017).\nGiven the success of SGD, especially within the language\nmodeling domain, we investigate the use of averaged SGD\n(ASGD) (\nPolyak & Juditsky , 1992) which is known to have\nsuperior theoretical guarantees. ASGD carries out itera-\ntions similar to SGD, but instead of returning the last itera te\nas the solution, returns an average of the iterates past a cer -\ntain, tuned, threshold T . This threshold T is typically tuned\nand has a direct impact on the performance of the method.\nW e propose a variant of ASGD where T is determined on\nthe ﬂy through a non-monotonic criterion and show that it\nachieves better training outcomes compared to SGD.\n2. Weight-dropped LSTM\nW e refer to the mathematical formulation of the LSTM,\nit = σ(W ixt + Uiht− 1)\nft = σ(W f xt + Uf ht− 1)\not = σ(W oxt + Uoht− 1)\n˜ct = tanh(W cxt + Ucht− 1)\nct = it ⊙˜ct + ft ⊙+˜ct− 1\nht = ot ⊙tanh(ct)\nwhere [W i, W f , W o, U i, U f , U o] are weight matrices, xt\nis the vector input to the timestep t, ht is the current ex-\nposed hidden state, ct is the memory cell state, and ⊙is\nelement-wise multiplication.\nPreventing overﬁtting within the recurrent connections of\nan RNN has been an area of extensive research in language\nmodeling. The majority of previous recurrent regulariza-\ntion techniques have acted on the hidden state vector ht− 1,\nmost frequently introducing a dropout operation between\ntimesteps, or performing dropout on the update to the mem-\nory state ct. These modiﬁcations to a standard LSTM pre-\nvent the use of black box RNN implementations that may\nbe many times faster due to low-level hardware-speciﬁc op-\ntimizations.\nW e propose the use of DropConnect (\nW an et al. , 2013)\non the recurrent hidden to hidden weight matrices which\ndoes not require any modiﬁcations to an RNN’s formu-\nlation. As the dropout operation is applied once to the\nweight matrices, before the forward and backward pass,\nthe impact on training speed is minimal and any standard\nRNN implementation can be used, including inﬂexible but\nhighly optimized black box LSTM implementations such\nas NVIDIA ’s cuDNN LSTM.\nBy performing DropConnect on the hidden-to-hidden\nweight matrices [Ui, U f , U o, U c] within the LSTM, we can\nprevent overﬁtting from occurring on the recurrent connec-\ntions of the LSTM. This regularization technique would\nalso be applicable to preventing overﬁtting on the recurren t\nweight matrices of other RNN cells.\nAs the same weights are reused over multiple timesteps,\nthe same individual dropped weights remain dropped for\nthe entirety of the forward and backward pass. The result\nis similar to variational dropout, which applies the same\ndropout mask to recurrent connections within the LSTM\nby performing dropout on ht− 1, except that the dropout\nis applied to the recurrent weights. DropConnect could\nalso be used on the non-recurrent weights of the LSTM\n[W i, W f , W o] though our focus was on preventing over-\nﬁtting on the recurrent connection.\n3. Optimization\nSGD is among the most popular methods for training deep\nlearning models across various modalities including com-\nputer vision, natural language processing, and reinforce-\nment learning. The training of deep networks can be posed\nas a non-convex optimization problem\nmin\nw\n1\nN\nN∑\ni=1\nfi(w),\nwhere fi is the loss function for the ith data point, w are\nthe weights of the network, and the expectation is taken\nover the data. Given a sequence of learning rates, γk, SGD\niteratively takes steps of the form\nwk+1 = wk −γk ˆ∇f(wk), (1)\nwhere the subscript denotes the iteration number and the\nˆ∇denotes a stochastic gradient that may be computed on a\nminibatch of data points. SGD demonstrably performs well\nin practice and also possesses several attractive theoreti cal\nproperties such as linear convergence (\nBottou et al. , 2016),\nsaddle point avoidance ( Panageas & Piliouras , 2016) and\nRegularizing and Optimizing LSTM Language Models\nbetter generalization performance ( Hardt et al. , 2015). For\nthe speciﬁc task of neural language modeling, tradition-\nally SGD without momentum has been found to outperform\nother algorithms such as momentum SGD (\nSutskever et al. ,\n2013), Adam ( Kingma & Ba , 2014), Adagrad ( Duchi et al. ,\n2011) and RMSProp ( Tieleman & Hinton , 2012) by a sta-\ntistically signiﬁcant margin.\nMotivated by this observation, we investigate averaged\nSGD (ASGD) to further improve the training process.\nASGD has been analyzed in depth theoretically and many\nsurprising results have been shown including its asymp-\ntotic second-order convergence (\nPolyak & Juditsky , 1992;\nMandt et al. , 2017). ASGD takes steps identical to equa-\ntion ( 1) but instead of returning the last iterate as the solu-\ntion, returns 1\n(K− T +1)\n∑ K\ni=T wi, where K is the total num-\nber of iterations and T < K is a user-speciﬁed averaging\ntrigger.\nAlgorithm 1 Non-monotonically Triggered ASGD (NT -\nASGD)\nInputs: Initial point w0, learning rate γ, logging interval L,\nnon-monotone interval n.\n1: Initialize k ←0, t ←0, T ←0, logs ←[]\n2: while stopping criterion not met do\n3: Compute stochastic gradient ˆ∇f(wk) and take SGD\nstep ( 1).\n4: if mod(k, L ) = 0and T = 0then\n5: Compute validation perplexity v.\n6: if t > n and v > min\nl∈{ t− n,···,t}\nlogs[l] then\n7: Set T ←k\n8: end if\n9: Append v to logs\n10: t ←t + 1\n11: end if\n12: end while\nreturn\n∑ k\ni=T wi\n(k− T +1)\nDespite its theoretical appeal, ASGD has found limited\npractical use in training of deep networks. This may be in\npart due to unclear tuning guidelines for the learning-rate\nschedule γk and averaging trigger T . If the averaging is\ntriggered too soon, the efﬁcacy of the method is impacted,\nand if it is triggered too late, many additional iterations m ay\nbe needed to converge to the solution. In this section, we\ndescribe a non-monotonically triggered variant of ASGD\n(NT -ASGD), which obviates the need for tuning T . Fur-\nther, the algorithm uses a constant learning rate throughou t\nthe experiment and hence no further tuning is necessary for\nthe decay scheduling.\nIdeally, averaging needs to be triggered when the SGD it-\nerates converge to a steady-state distribution (\nMandt et al. ,\n2017). This is roughly equivalent to the convergence of\nSGD to a neighborhood around a solution. In the case of\nSGD, certain learning-rate reduction strategies such as th e\nstep-wise strategy analogously reduce the learning rate by\na ﬁxed quantity at such a point. A common strategy em-\nployed in language modeling is to reduce the learning rates\nby a ﬁxed proportion when the performance of the model’s\nprimary metric (such as perplexity) worsens or stagnates.\nAlong the same lines, one could make a triggering decision\nbased on the performance of the model on the validation\nset. However, instead of averaging immediately after the\nvalidation metric worsens, we propose a non-monotonic\ncriterion that conservatively triggers the averaging when\nthe validation metric fails to improve for multiple cycles;\nsee Algorithm\n1. Given that the choice of triggering is irre-\nversible, this conservatism ensures that the randomness of\ntraining does not play a major role in the decision. Anal-\nogous strategies have also been proposed for learning-rate\nreduction in SGD (\nKeskar & Saon , 2015).\nWhile the algorithm introduces two additional hyperparam-\neters, the logging interval L and non-monotone interval n,\nwe found that setting L to be the number of iterations in\nan epoch and n = 5 worked well across various models\nand data sets. As such, we use this setting in all of our NT -\nASGD experiments in the following section and demon-\nstrate that it achieves better training outcomes as compare d\nto SGD.\n4. Extended regularization techniques\nIn addition to the regularization and optimization tech-\nniques above, we explored additional regularization tech-\nniques that aimed to improve data efﬁciency during training\nand to prevent overﬁtting of the RNN model.\n4.1. V ariable length backpropagation sequences\nGiven a ﬁxed sequence length that is used to break a data\nset into ﬁxed length batches, the data set is not efﬁciently\nused. T o illustrate this, imagine being given 100 elements\nto perform backpropagation through with a ﬁxed backprop-\nagation through time (BPTT) window of 10. Any element\ndivisible by 10 will never have any elements to backprop\ninto, no matter how many times you may traverse the data\nset. Indeed, the backpropagation window that each element\nreceives is equal to i mod 10where i is the element’s in-\ndex. This is data inefﬁcient, preventing 1\n10 of the data set\nfrom ever being able to improve itself in a recurrent fash-\nion, and resulting in 8\n10 of the remaining elements receiving\nonly a partial backpropagation window compared to the full\npossible backpropagation window of length 10.\nT o prevent such inefﬁcient data usage, we randomly select\nthe sequence length for the forward and backward pass in\ntwo steps. First, we select the base sequence length to be\nRegularizing and Optimizing LSTM Language Models\nseq with probability p and seq\n2 with probability 1 −p, where\np is a high value approaching 1. This spreads the start-\ning point for the BPTT window beyond the base sequence\nlength. W e then select the sequence length according to\nN(seq, s ), where seq is the base sequence length and s is\nthe standard deviation. This jitters the starting point suc h\nthat it doesn’t always fall on a speciﬁc word divisible by\nseq or seq\n2 . From these, the sequence length more efﬁciently\nuses the data set, ensuring that when given enough epochs\nall the elements in the data set experience a full BPTT win-\ndow , while ensuring the average sequence length remains\naround the base sequence length for computational efﬁ-\nciency.\nDuring training, we rescale the learning rate depending\non the length of the resulting sequence compared to the\noriginal speciﬁed sequence length. The rescaling step is\nnecessary as sampling arbitrary sequence lengths with a\nﬁxed learning rate favors short sequences over longer ones.\nThis linear scaling rule has been noted as important for\ntraining large scale minibatch SGD without loss of accu-\nracy (\nGoyal et al. , 2017) and is a component of unbiased\ntruncated backpropagation through time ( T allec & Ollivier ,\n2017).\n4.2. V ariational dropout\nIn standard dropout, a new binary dropout mask is sampled\neach and every time the dropout function is called. New\ndropout masks are sampled even if the given connection\nis repeated, such as the input x0 to an LSTM at timestep\nt = 0 receiving a different dropout mask than the input\nx1 fed to the same LSTM at t = 1. A variant of this,\nvariational dropout (\nGal & Ghahramani , 2016), samples a\nbinary dropout mask only once upon the ﬁrst call and then\nto repeatedly use that locked dropout mask for all repeated\nconnections within the forward and backward pass.\nWhile we propose using DropConnect rather than varia-\ntional dropout to regularize the hidden-to-hidden transit ion\nwithin an RNN, we use variational dropout for all other\ndropout operations, speciﬁcally using the same dropout\nmask for all inputs and outputs of the LSTM within a given\nforward and backward pass. Each example within the mini-\nbatch uses a unique dropout mask, rather than a single\ndropout mask being used over all examples, ensuring di-\nversity in the elements dropped out.\n4.3. Embedding dropout\nFollowing\nGal & Ghahramani (2016), we employ embed-\nding dropout. This is equivalent to performing dropout on\nthe embedding matrix at a word level, where the dropout is\nbroadcast across all the word vector’s embedding. The re-\nmaining non-dropped-out word embeddings are scaled by\n1\n1− pe\nwhere pe is the probability of embedding dropout. As\nthe dropout occurs on the embedding matrix that is used\nfor a full forward and backward pass, this means that all\noccurrences of a speciﬁc word will disappear within that\npass, equivalent to performing variational dropout on the\nconnection between the one-hot embedding and the embed-\nding lookup.\n4.4. W eight tying\nW eight tying (\nInan et al. , 2016; Press & W olf , 2016) shares\nthe weights between the embedding and softmax layer, sub-\nstantially reducing the total parameter count in the model.\nThe technique has theoretical motivation (\nInan et al. , 2016)\nand prevents the model from having to learn a one-to-one\ncorrespondence between the input and output, resulting in\nsubstantial improvements to the standard LSTM language\nmodel.\n4.5. Independent embedding size and hidden size\nIn most natural language processing tasks, both pre-\ntrained and trained word vectors are of relatively low\ndimensionality—frequently between 100 and 400 dimen-\nsions in size. Most previous LSTM language models tie\nthe dimensionality of the word vectors to the dimensional-\nity of the LSTM’s hidden state. Even if reducing the word\nembedding size was not beneﬁcial in preventing overﬁt-\nting, the easiest reduction in total parameters for a langua ge\nmodel is reducing the word vector size. T o achieve this, the\nﬁrst and last LSTM layers are modiﬁed such that their in-\nput and output dimensionality respectively are equal to the\nreduced embedding size.\n4.6. Activation Regularization (AR) and T emporal\nActivation Regularization (T AR)\nL2-regularization is often used on the weights of the net-\nwork to control the norm of the resulting model and reduce\noverﬁtting. In addition, L2 decay can be used on the in-\ndividual unit activations and on the difference in outputs\nof an RNN at different time steps; these strategies labeled\nas activation regularization (AR) and temporal activation\nregularization (T AR) respectively (\nMerity et al. , 2017). AR\npenalizes activations that are signiﬁcantly larger than 0 as\na means of regularizing the network. Concretely, AR is\ndeﬁned as\nα L 2(m ⊙ht)\nwhere m is the dropout mask, L2(·) = ∥·∥2, ht is the out-\nput of the RNN at timestep t, and α is a scaling coefﬁcient.\nT AR falls under the broad category of slowness regulariz-\ners (\nHinton, 1989; Földiák, 1991; Luciw & Schmidhuber ,\n2012; Jonschkowski & Brock , 2015) which penalize the\nmodel from producing large changes in the hidden state.\nRegularizing and Optimizing LSTM Language Models\nUsing the notation from AR, T AR is deﬁned as\nβ L 2(ht −ht+1)\nwhere β is a scaling coefﬁcient. As in Merity et al. (2017),\nthe AR and T AR loss are only applied to the output of the\nﬁnal RNN layer as opposed to being applied to all layers.\n5. Experiment Details\nFor evaluating the impact of these approaches, we perform\nlanguage modeling over a preprocessed version of the Penn\nTreebank (PTB) (\nMikolov et al. , 2010) and the WikiT ext-2\n(WT2) data set ( Merity et al. , 2016).\nPTB: The Penn Treebank data set has long been a central\ndata set for experimenting with language modeling. The\ndata set is heavily preprocessed and does not contain capita l\nletters, numbers, or punctuation. The vocabulary is also\ncapped at 10,000 unique words, quite small in comparison\nto most modern datasets, which results in a large number\nof out of vocabulary (OoV) tokens.\nWT2: WikiT ext-2 is sourced from curated Wikipedia ar-\nticles and is approximately twice the size of the PTB data\nset. The text is tokenized and processed using the Moses\ntokenizer (\nKoehn et al. , 2007), frequently used for machine\ntranslation, and features a vocabulary of over 30,000 words .\nCapitalization, punctuation, and numbers are retained in\nthis data set.\nAll experiments use a three-layer LSTM model with 1150\nunits in the hidden layer and an embedding of size 400. The\nloss was averaged over all examples and timesteps. All em-\nbedding weights were uniformly initialized in the interval\n[−0. 1, 0. 1] and all other weights were initialized between\n[− 1\n√\nH , 1√\nH ], where H is the hidden size.\nFor training the models, we use the NT -ASGD algorithm\ndiscussed in the previous section for 750 epochs with L\nequivalent to one epoch and n = 5. W e use a batch size\nof 80 for WT2 and 40 for PTB. Empirically, we found rel-\natively large batch sizes (e.g., 40-80) performed better than\nsmaller sizes (e.g., 10-20) for NT -ASGD. After comple-\ntion, we run ASGD with T = 0 and hot-started w0 as a\nﬁne-tuning step to further improve the solution. For this\nﬁne-tuning step, we terminate the run using the same non-\nmonotonic criterion detailed in Algorithm 1.\nW e carry out gradient clipping with maximum norm 0. 25\nand use an initial learning rate of 30 for all experiments. W e\nuse a random BPTT length which is N(70, 5) with proba-\nbility 0. 95 and N(35, 5) with probability 0. 05. The values\nused for dropout on the word vectors, the output between\nLSTM layers, the output of the ﬁnal LSTM layer, and em-\nbedding dropout where (0. 4, 0. 3, 0. 4, 0. 1) respectively. For\nthe weight-dropped LSTM, a dropout of 0. 5 was applied to\nthe recurrent weight matrices. For WT2, we increase the\ninput dropout to 0. 65 to account for the increased vocabu-\nlary size. For all experiments, we use AR and T AR values\nof 2 and 1 respectively, and tie the embedding and soft-\nmax weights. These hyperparameters were chosen through\ntrial and error and we expect further improvements may be\npossible if a ﬁne-grained hyperparameter search were to be\nconducted. In the results, we abbreviate our approach as\nA WD-LSTM for ASGD W eight-Dropped LSTM.\n6. Experimental Analysis\nW e present the single-model perplexity results for both our\nmodels (A WD-LSTM) and other competitive models in T a-\nble\n1 and 2 for PTB and WT2 respectively. On both data\nsets we improve the state-of-the-art, with our vanilla LSTM\nmodel beating the state of the art by approximately 1 unit\non PTB and 0.1 units on WT2.\nIn comparison to other recent state-of-the-art models, our\nmodel uses a vanilla LSTM.\nZilly et al. (2016) propose the\nrecurrent highway network, which extends the LSTM to al-\nlow multiple hidden state updates per timestep.\nZoph & Le\n(2016) use a reinforcement learning agent to generate an\nRNN cell tailored to the speciﬁc task of language model-\ning, with the cell far more complex than the LSTM.\nIndependently of our work,\nMelis et al. (2017) apply ex-\ntensive hyperparameter search to an LSTM based lan-\nguage modeling implementation, analyzing the sensitivity\nof RNN based language models to hyperparameters. Un-\nlike our work, they use a modiﬁed LSTM, which caps the\ninput gate it to be min(1 −ft, i t), use Adam with β1 = 0\nrather than SGD or ASGD, use skip connections between\nLSTM layers, and use a black box hyperparameter tuner for\nexploring models and settings. Of particular interest is th at\ntheir hyperparameters were tuned individually for each dat a\nset compared to our work which shared almost all hyperpa-\nrameters between PTB and WT2, including the embedding\nand hidden size for both data sets. Due to this, they used\nless model parameters than our model and found shallow\nLSTMs of one or two layers worked best for WT2.\nLike our work,\nMelis et al. (2017) ﬁnd that the underly-\ning LSTM architecture can be highly effective compared\nto complex custom architectures when well tuned hyperpa-\nrameters are used. The approaches used in our work and\nMelis et al. (2017) may be complementary and would be\nworth exploration.\n7. Pointer models\nIn past work, pointer based attention models have been\nshown to be highly effective in improving language mod-\neling (\nMerity et al. , 2016; Grave et al. , 2016). Given such\nRegularizing and Optimizing LSTM Language Models\nModel Parameters V alidation T est\nMikolov & Zweig (2012) - KN-5 2M‡ − 141. 2\nMikolov & Zweig (2012) - KN5 + cache 2M‡ − 125. 7\nMikolov & Zweig (2012) - RNN 6M‡ − 124. 7\nMikolov & Zweig (2012) - RNN-LDA 7M‡ − 113. 7\nMikolov & Zweig (2012) - RNN-LDA + KN-5 + cache 9M‡ − 92. 0\nZaremba et al. (2014) - LSTM (medium) 20M 86. 2 82 . 7\nZaremba et al. (2014) - LSTM (large) 66M 82. 2 78 . 4\nGal & Ghahramani (2016) - V ariational LSTM (medium) 20M 81. 9 ±0. 2 79 . 7 ±0. 1\nGal & Ghahramani (2016) - V ariational LSTM (medium, MC) 20M − 78. 6 ±0. 1\nGal & Ghahramani (2016) - V ariational LSTM (large) 66M 77. 9 ±0. 3 75 . 2 ±0. 2\nGal & Ghahramani (2016) - V ariational LSTM (large, MC) 66M − 73. 4 ±0. 0\nKim et al. (2016) - CharCNN 19M − 78. 9\nMerity et al. (2016) - Pointer Sentinel-LSTM 21M 72. 4 70 . 9\nGrave et al. (2016) - LSTM − − 82. 3\nGrave et al. (2016) - LSTM + continuous cache pointer − − 72. 1\nInan et al. (2016) - V ariational LSTM (tied) + augmented loss 24M 75. 7 73 . 2\nInan et al. (2016) - V ariational LSTM (tied) + augmented loss 51M 71. 1 68 . 5\nZilly et al. (2016) - V ariational RHN (tied) 23M 67. 9 65 . 4\nZoph & Le (2016) - NAS Cell (tied) 25M − 64. 0\nZoph & Le (2016) - NAS Cell (tied) 54M − 62. 4\nMelis et al. (2017) - 4-layer skip connection LSTM (tied) 24M 60. 9 58 . 3\nA WD-LSTM - 3-layer LSTM (tied) 24M 60. 0 57 . 3\nA WD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 24M 53. 9 52 . 8\nT able 1. Single model perplexity on validation and test sets for the P enn Treebank language modeling task. Parameter numbers wit h ‡\nare estimates based upon our understanding of the model and w ith reference to Merity et al. (2016). Models noting tied use weight tying\non the embedding and softmax weights. Our model, A WD-LSTM, s tands for ASGD W eight-Dropped LSTM.\nModel Parameters V alidation T est\nInan et al. (2016) - V ariational LSTM (tied) ( h = 650) 28M 92. 3 87 . 7\nInan et al. (2016) - V ariational LSTM (tied) ( h = 650) + augmented loss 28M 91. 5 87 . 0\nGrave et al. (2016) - LSTM − − 99. 3\nGrave et al. (2016) - LSTM + continuous cache pointer − − 68. 9\nMelis et al. (2017) - 1-layer LSTM (tied) 24M 69. 3 65 . 9\nMelis et al. (2017) - 2-layer skip connection LSTM (tied) 24M 69. 1 65 . 9\nA WD-LSTM - 3-layer LSTM (tied) 33M 68. 6 65 . 8\nA WD-LSTM - 3-layer LSTM (tied) + continuous cache pointer 33M 53. 8 52 . 0\nT able 2. Single model perplexity over WikiT ext-2. Models noting tied use weight tying on the embedding and softmax weights. Our\nmodel, A WD-LSTM, stands for ASGD W eight-Dropped LSTM.\nRegularizing and Optimizing LSTM Language Models\nsubstantial improvements to the underlying neural lan-\nguage model, it remained an open question as to how ef-\nfective pointer augmentation may be, especially when im-\nprovements such as weight tying may act in mutually ex-\nclusive ways.\nThe neural cache model (\nGrave et al. , 2016) can be added\non top of a pre-trained language model at negligible cost.\nThe neural cache stores the previous hidden states in mem-\nory cells and then uses a simple convex combination of\nthe probability distributions suggested by the cache and th e\nlanguage model for prediction. The cache model has three\nhyperparameters: the memory size (window) for the cache,\nthe coefﬁcient of the combination (which determines how\nthe two distributions are mixed), and the ﬂatness of the\ncache distribution. All of these are tuned on the validation\nset once a trained language model has been obtained and\nrequire no training by themselves, making it quite inexpen-\nsive to use. The tuned values for these hyperparameters\nwere (2000, 0. 1, 1. 0) for PTB and (3785, 0. 1279, 0. 662)\nfor WT2 respectively.\nIn T ables 1 and 2, we show that the model further improves\nthe perplexity of the language model by as much as 6 per-\nplexity points for PTB and 11 points for WT2. While this\nis smaller than the gains reported in\nGrave et al. (2016),\nwhich used an LSTM without weight tying, this is still a\nsubstantial drop. Given the simplicity of the neural cache\nmodel, and the lack of any trained components, these re-\nsults suggest that existing neural language models remain\nfundamentally lacking, failing to capture long term depen-\ndencies or remember recently seen words effectively.\nT o understand the impact the pointer had on the model,\nspeciﬁcally the validation set perplexity, we detail the co n-\ntribution that each word has on the cache model’s overall\nperplexity in T able 3. W e compute the sum of the total dif-\nference in the loss function value (i.e., log perplexity) be -\ntween the LSTM-only and LSTM-with-cache models for\nthe target words in the validation portion of the WikiT ext-2\ndata set. W e present results for the sum of the difference as\nopposed to the mean since the latter undesirably overem-\nphasizes infrequently occurring words for which the cache\nhelps signiﬁcantly and ignores frequently occurring words\nfor which the cache provides modest improvements that cu-\nmulatively make a strong contribution.\nThe largest cumulative gain is in improving the handling\nof <unk> tokens, though this is over 11540 instances. The\nsecond best improvement, approximately one ﬁfth the gain\ngiven by the <unk> tokens, is for Meridian, yet this word\nonly occurs 161 times. This indicates the cache still helps\nsigniﬁcantly even for relatively rare words, further demon -\nstrated by Churchill, Blythe, or Sonic. The cache is not\nbeneﬁcial when handling frequent word categories, such as\npunctuation or stop words, for which the language model is\nW ord Count ∆ loss W ord Count ∆ loss\n. 7632 -696.45 <unk> 11540 5047.34\n, 9857 -687.49 Meridian 161 1057.78\nof 5816 -365.21 Churchill 137 849.43\n= 2884 -342.01 - 67 682.15\nto 4048 -283.10 Blythe 97 554.95\nin 4178 -222.94 Sonic 75 543.85\n<eos> 3690 -216.42 Richmond 101 429.18\nand 5251 -215.38 Starr 74 416.52\nthe 12481 -209.97 Australian 234 366.36\na 3381 -149.78 Pagan 54 365.19\n\" 2540 -127.99 Asahi 39 316.24\nthat 1365 -118.09 Japanese 181 295.97\nby 1252 -113.05 Hu 43 285.58\nwas 2279 -107.95 Hedgehog 29 266.48\n) 1101 -94.74 Burma 35 263.65\nwith 1176 -93.01 29 92 260.88\nfor 1215 -87.68 Mississippi 72 241.59\non 1485 -81.55 German 108 241.23\nas 1338 -77.05 mill 67 237.76\nat 879 -59.86 Cooke 33 231.11\nT able 3. The sum total difference in loss (log perplexity) that a\ngiven word results in over all instances in the validation da ta set\nof WikiT ext-2 when the continuous cache pointer is introduc ed.\nThe right column contains the words with the twenty best im-\nprovements (i.e., where the cache was advantageous), and th e left\ncolumn the twenty most deteriorated (i.e., where the cache w as\ndisadvantageous).\nlikely well suited. These observations motivate the design\nof a cache framework that is more aware of the relative\nstrengths of the two models.\n8. Model Ablation Analysis\nIn T able\n4, we present the values of validation and test-\ning perplexity for different variants of our best-performi ng\nLSTM model. Each variant removes a form of optimization\nor regularization.\nThe ﬁrst two variants deal with the optimization of the lan-\nguage models while the rest deal with the regularization.\nFor the model using SGD with learning rate reduced by 2\nusing the same nonmonotonic fashion, there is a signiﬁ-\ncant degradation in performance. This stands as empirical\nevidence regarding the beneﬁt of averaging of the iterates.\nUsing a monotonic criterion instead also hampered perfor-\nmance. Similarly, the removal of the ﬁne-tuning step ex-\npectedly also degrades the performance. This step helps\nimprove the estimate of the minimizer by resetting the\nmemory of the previous experiment. While this process of\nﬁne-tuning can be repeated multiple times, we found little\nbeneﬁt in repeating it more than once.\nThe removal of regularization strategies paints a similar\npicture; the inclusion of all of the proposed strategies\nRegularizing and Optimizing LSTM Language Models\nPTB WT2\nModel V alidation T est V alidation T est\nA WD-LSTM (tied) 60. 0 57 . 3 68.6 65.8\n– ﬁne-tuning 60. 7 58 . 8 69. 1 66 . 0\n– NT -ASGD 66.3 63.7 73.3 69.7\n– variable sequence lengths 61.3 58.9 69.3 66.2\n– embedding dropout 65.1 62.7 71.1 68.1\n– weight decay 63.7 61.0 71.9 68.7\n– AR/T AR 62.7 60.3 73.2 70.1\n– full sized embedding 68.0 65.6 73.7 70.7\n– weight-dropping 71.1 68.9 78.4 74.9\nT able 4. Model ablations for our best LSTM models reporting results o ver the validation and test set on Penn Treebank and WikiT ext -2.\nAblations are split into optimization and regularization v ariants, sorted according to the achieved validation perpl exity on WikiT ext-2.\nwas pivotal in ensuring state-of-the-art performance. The\nmost extreme perplexity jump was in removing the hidden-\nto-hidden LSTM regularization provided by the weight-\ndropped LSTM. Without such hidden-to-hidden regular-\nization, perplexity rises substantially, up to 11 points.\nThis is in line with previous work showing the neces-\nsity of recurrent regularization in state-of-the-art mode ls\n(\nGal & Ghahramani , 2016; Inan et al. , 2016).\nW e also experiment with static sequence lengths which we\nhad hypothesized would lead to inefﬁcient data usage. This\nalso worsens the performance by approximately one per-\nplexity unit. Next, we experiment with reverting to match-\ning the sizes of the embedding vectors and the hidden\nstates. This signiﬁcantly increases the number of param-\neters in the network (to 43M in the case of PTB and 70M\nfor WT2) and leads to degradation by almost 8 perplexity\npoints, which we attribute to overﬁtting in the word em-\nbeddings. While this could potentially be improved with\nmore aggressive regularization, the computational over-\nhead involved with substantially larger embeddings likely\noutweighs any advantages. Finally, we experiment with the\nremoval of embedding dropout, AR/T AR and weight decay.\nIn all of the cases, the model suffers a perplexity increase\nof 2– 6 points which we hypothesize is due to insufﬁcient\nregularization in the network.\n9. Conclusion\nIn this work, we discuss regularization and optimization\nstrategies for neural language models. W e propose the\nweight-dropped LSTM, a strategy that uses a DropConnect\nmask on the hidden-to-hidden weight matrices, as a means\nto prevent overﬁtting across the recurrent connections. Fu r-\nther, we investigate the use of averaged SGD with a non-\nmonontonic trigger for training language models and show\nthat it outperforms SGD by a signiﬁcant margin. W e in-\nvestigate other regularization strategies including the u se\nof variable BPTT length and achieve a new state-of-the-art\nperplexity on the PTB and WikiT ext-2 data sets. Our mod-\nels outperform custom-built RNN cells and complex reg-\nularization strategies that preclude the possibility of us ing\noptimized libraries such as the NVIDIA cuDNN LSTM.\nFinally, we explore the use of a neural cache in conjunc-\ntion with our proposed model and show that this further\nimproves the performance, thus attaining an even lower\nstate-of-the-art perplexity. While the regularization an d op-\ntimization strategies proposed are demonstrated on the tas k\nof language modeling, we anticipate that they would be\ngenerally applicable across other sequence learning tasks .\nReferences\nArjovsky, M., Shah, A., and Bengio, Y . Unitary evolution\nrecurrent neural networks. In International Conference\non Machine Learning , pp. 1120–1128, 2016.\nBa, J., Kiros, J., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016.\nBalduzzi, D. and Ghifary, M. Strongly-typed recurrent neu-\nral networks. arXiv preprint arXiv:1602.02218 , 2016.\nBottou, L., Curtis, F . E., and Nocedal, J. Optimization\nmethods for large-scale machine learning. arXiv preprint\narXiv:1606.04838 , 2016.\nBradbury, J., Merity, S., Xiong, C., and Socher, R.\nQuasi-Recurrent Neural Networks. arXiv preprint\narXiv:1611.01576 , 2016.\nCooijmans, T ., Ballas, N., Laurent, C., and Courville, A. C.\nRecurrent batch normalization. CoRR, abs/1603.09025,\n2016.\nDuchi, J., Hazan, E., and Singer, Y . Adaptive subgradient\nmethods for online learning and stochastic optimization.\nRegularizing and Optimizing LSTM Language Models\nJournal of Machine Learning Research , 12(Jul):2121–\n2159, 2011.\nFöldiák, P . Learning invariance from transformation se-\nquences. Neural Computation , 3(2):194–200, 1991.\nGal, Y . and Ghahramani, Z. A theoretically grounded appli-\ncation of dropout in recurrent neural networks. In NIPS,\n2016.\nGoyal, P ., Dollár, P ., Girshick, R., Noordhuis, P .,\nW esolowski, L., Kyrola, A., Tulloch, A., Jia, Y ., and He,\nK. Accurate, large minibatch sgd: Training imagenet in\n1 hour. arXiv preprint arXiv:1706.02677 , 2017.\nGrave, E., Joulin, A., and Usunier, N. Improving neural\nlanguage models with a continuous cache. arXiv preprint\narXiv:1612.04426 , 2016.\nHardt, M., Recht, B., and Singer, Y . Train faster, generaliz e\nbetter: Stability of stochastic gradient descent. arXiv\npreprint arXiv:1509.01240 , 2015.\nHinton, G. E. Connectionist learning procedures. Artiﬁcial\nintelligence, 40(1-3):185–234, 1989.\nInan, H., Khosravi, K., and Socher, R. T ying W ord V ectors\nand W ord Classiﬁers: A Loss Framework for Language\nModeling. arXiv preprint arXiv:1611.01462 , 2016.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerat-\ning deep network training by reducing internal covariate\nshift. In ICML, 2015.\nJing, L., Shen, Y ., Dub ˇcek, T ., Peurifoy, J., Skirlo, S.,\nT egmark, M., and Solja ˇci´c, M. Tunable Efﬁcient Uni-\ntary Neural Networks (EUNN) and their application to\nRNN. arXiv preprint arXiv:1612.05231 , 2016.\nJonschkowski, R. and Brock, O. Learning state represen-\ntations with robotic priors. Auton. Robots , 39:407–428,\n2015.\nKeskar, N. and Saon, G. A nonmonotone learning rate\nstrategy for sgd training of deep neural networks. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2015 IEEE International Conference on , pp. 4974–4978.\nIEEE, 2015.\nKim, Y ., Jernite, Y ., Sontag, D., and Rush, A. M. Character-\naware neural language models. In Thirtieth AAAI Con-\nference on Artiﬁcial Intelligence , 2016.\nKingma, D. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\nKoehn, P ., Hoang, H., Birch, A., Callison-Burch, C., Fed-\nerico, M., Bertoldi, N., Cowan, B., Shen, W ., Moran, C.,\nZens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst,\nE. Moses: Open source toolkit for statistical machine\ntranslation. In ACL, 2007.\nKrueger, D., Maharaj, T ., Kramár, J., Pezeshki, M., Bal-\nlas, N., Ke, N., Goyal, A., Bengio, Y ., Larochelle, H.,\nCourville, A., et al. Zoneout: Regularizing RNNss by\nrandomly preserving hidden activations. arXiv preprint\narXiv:1606.01305 , 2016.\nLuciw , M. and Schmidhuber, J. Low complexity proto-\nvalue function learning from sensory observations with\nincremental slow feature analysis. Artiﬁcial Neural Net-\nworks and Machine Learning–ICANN 2012 , pp. 279–\n287, 2012.\nMandt, S., Hoffman, M. D., and Blei, D. M. Stochastic gra-\ndient descent as approximate bayesian inference. arXiv\npreprint arXiv:1704.04289 , 2017.\nMelis, G., Dyer, C., and Blunsom, P . On the State of the\nArt of Evaluation in Neural Language Models. arXiv\npreprint arXiv:1707.05589 , 2017.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\nPointer Sentinel Mixture Models. arXiv preprint\narXiv:1609.07843 , 2016.\nMerity, S., McCann, B., and Socher, R. Revisiting acti-\nvation regularization for language rnns. arXiv preprint\narXiv:1708.01009 , 2017.\nMikolov, T . and Zweig, G. Context dependent recurrent\nneural network language model. SLT, 12:234–239, 2012.\nMikolov, T ., Karaﬁát, M., Burget, L., Cernocký, J., and\nKhudanpur, S. Recurrent neural network based language\nmodel. In INTERSPEECH, 2010.\nPanageas, I. and Piliouras, G. Gradient descent converges\nto minimizers: The case of non-isolated critical points.\nCoRR, abs/1605.00405 , 2016.\nPolyak, B. and Juditsky, A. Acceleration of stochastic ap-\nproximation by averaging. SIAM Journal on Control and\nOptimization, 30(4):838–855, 1992.\nPress, O. and W olf, L. Using the output embed-\nding to improve language models. arXiv preprint\narXiv:1608.05859 , 2016.\nSemeniuta, S., Severyn, A., and Barth, E. Recurrent\ndropout without memory loss. In COLING, 2016.\nSeo, M., Min, S., Farhadi, A., and Hajishirzi, H. Query-\nReduction Networks for Question Answering. arXiv\npreprint arXiv:1606.04582 , 2016.\nRegularizing and Optimizing LSTM Language Models\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overﬁtting. Journal of Machine\nLearning Research , 15:1929–1958, 2014.\nSutskever, I., Martens, J., Dahl, G., and Hinton, G. On\nthe importance of initialization and momentum in deep\nlearning. In International conference on machine learn-\ning, pp. 1139–1147, 2013.\nT allec, C. and Ollivier, Y . Unbiasing truncated backprop-\nagation through time. arXiv preprint arXiv:1705.08209 ,\n2017.\nTieleman, T . and Hinton, G. Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magni-\ntude. COURSERA: Neural networks for machine learn-\ning, 4(2):26–31, 2012.\nW an, L., Zeiler, M., Zhang, S., LeCun, Y , and Fergus, R.\nRegularization of neural networks using dropconnect. In\nProceedings of the 30th international conference on ma-\nchine learning (ICML-13) , pp. 1058–1066, 2013.\nWilson, A. C, Roelofs, R., Stern, M., Srebro, N., and Recht,\nB. The marginal value of adaptive gradient methods\nin machine learning. arXiv preprint arXiv:1705.08292 ,\n2017.\nWisdom, S., Powers, T ., Hershey, J., Le Roux, J., and Atlas,\nL. Full-capacity unitary recurrent neural networks. In\nAdvances in Neural Information Processing Systems , pp.\n4880–4888, 2016.\nZaremba, W ., Sutskever, I., and V inyals, O. Recur-\nrent neural network regularization. arXiv preprint\narXiv:1409.2329, 2014.\nZilly, J. G., Srivastava, R. K., Koutník, J., and Schmid-\nhuber, J. Recurrent highway networks. arXiv preprint\narXiv:1607.03474 , 2016.\nZoph, B. and Le, Q. V . Neural architecture search with re-\ninforcement learning. arXiv preprint arXiv:1611.01578 ,\n2016.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9769314527511597
    },
    {
      "name": "Perplexity",
      "score": 0.961635947227478
    },
    {
      "name": "Computer science",
      "score": 0.8108229041099548
    },
    {
      "name": "Language model",
      "score": 0.7302290201187134
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7182955741882324
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6744685173034668
    },
    {
      "name": "Machine translation",
      "score": 0.6354992389678955
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6117985844612122
    },
    {
      "name": "Monotonic function",
      "score": 0.5554130673408508
    },
    {
      "name": "Word (group theory)",
      "score": 0.46800723671913147
    },
    {
      "name": "Deep neural networks",
      "score": 0.4558720290660858
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4461551010608673
    },
    {
      "name": "Natural language processing",
      "score": 0.44535496830940247
    },
    {
      "name": "Artificial neural network",
      "score": 0.4276200830936432
    },
    {
      "name": "Mathematics",
      "score": 0.08176213502883911
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Dependency (UML)",
      "score": 0.0
    }
  ],
  "institutions": []
}