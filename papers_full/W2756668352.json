{
  "title": "Towards Quantum Language Models",
  "url": "https://openalex.org/W2756668352",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2805618889",
      "name": "Ivano Basile",
      "affiliations": [
        "Scuola Normale Superiore"
      ]
    },
    {
      "id": "https://openalex.org/A2002991233",
      "name": "Fabio Tamburini",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1563364586",
    "https://openalex.org/W4301014524",
    "https://openalex.org/W16967297",
    "https://openalex.org/W2555456875",
    "https://openalex.org/W2288817436",
    "https://openalex.org/W32507011",
    "https://openalex.org/W2143331952",
    "https://openalex.org/W1564720605",
    "https://openalex.org/W2250180457",
    "https://openalex.org/W2407936315",
    "https://openalex.org/W1608367484",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4297797495",
    "https://openalex.org/W2091917610",
    "https://openalex.org/W2951605425",
    "https://openalex.org/W1779452081",
    "https://openalex.org/W1526481195",
    "https://openalex.org/W2963042606",
    "https://openalex.org/W1549285799",
    "https://openalex.org/W1510667201",
    "https://openalex.org/W2581719241",
    "https://openalex.org/W3102194711",
    "https://openalex.org/W1582774210",
    "https://openalex.org/W3115277010",
    "https://openalex.org/W176510440",
    "https://openalex.org/W1981706894",
    "https://openalex.org/W2295119550",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3127686677",
    "https://openalex.org/W2175402905",
    "https://openalex.org/W2026369565",
    "https://openalex.org/W120524294",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W1525068081",
    "https://openalex.org/W1806891645",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W2977321575",
    "https://openalex.org/W3100514093"
  ],
  "abstract": "This paper presents a new approach for building Language Models using the Quantum Probability Theory, a Quantum Language Model (QLM). It mainly shows that relying on this probability calculus it is possible to build stochastic models able to benefit from quantum correlations due to interference and entanglement. We extensively tested our approach showing its superior performances, both in terms of model perplexity and inserting it into an automatic speech recognition evaluation setting, when compared with state-of-the-art language modelling techniques.",
  "full_text": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1840–1849\nCopenhagen, Denmark, September 7–11, 2017.c⃝2017 Association for Computational Linguistics\nTowards Quantum Language Models\nIvano Basile\nScuola Normale Superiore, Pisa, Italy\nivano.basile@sns.it\nFabio Tamburini\nFICLIT - University of Bologna, Italy\nfabio.tamburini@unibo.it\nAbstract\nThis paper presents a new approach\nfor building Language Models using the\nQuantum Probability Theory, a Quantum\nLanguage Model (QLM). It mainly shows\nthat relying on this probability calculus it\nis possible to build stochastic models able\nto beneﬁt from quantum correlations due\nto interference and entanglement. We ex-\ntensively tested our approach showing its\nsuperior performances, both in terms of\nmodel perplexity and inserting it into an\nautomatic speech recognition evaluation\nsetting, when compared with state-of-the-\nart language modelling techniques.\n1 Introduction\nQuantum Mechanics Theory (QMT) is one of the\nmost successful theories in modern science. De-\nspite its effectiveness in the physics realm, the at-\ntempts to apply it in other domains remain quite\nlimited, excluding, of course, the large quantity of\nstudies regarding Quantum Information Process-\ning on quantum computers.\nOnly in recent years some scholars tried to em-\nbody principles derived from QMT into their spe-\nciﬁc ﬁelds, for example, by the Information Re-\ntrieval community (Zuccon et al., 2009; Melucci\nand van Rijsbergen, 2011; Gonz´alez and Caicedo,\n2011; Melucci, 2015) and in the domain of cog-\nnitive sciences and decision making (Khrennikov,\n2010; Busemeyer and Bruza, 2012; Aerts et al.,\n2013). In the machine learning ﬁeld (Arjovsky\net al., 2016; Wisdom et al., 2016; Jing et al., 2017)\nhave used unitary evolution matrices for building\ndeep neural networks obtaining interesting results,\nbut we have to observe that their works do not ad-\nhere to QMT and use unitary evolution operators\nin a way not allowed by QMT. In recent years, also\nthe Natural Language Processing (NLP) commu-\nnity started to look at QMT with interest and some\nstudies using it have already been presented (Bla-\ncoe et al., 2013; Liu et al., 2013; Tamburini, 2014;\nKartsaklis et al., 2016).\nLanguage models (LM) are basic tools in NLP\nused in various applications, such as Automatic\nSpeech Recognition (ASR), machine translation,\npart-of-speech tagging, etc., and were traditionally\nmodeled by using N-grams and various smoothing\ntechniques. Among the dozen of tools for comput-\ning N-gram LM, we will refer to CMU-SLM (with\nGood-Turing smoothing) (Clarkson and Rosen-\nfeld, 1997) and IRSTLM (with Linear Witten-Bell\nsmoothing) (Federico et al., 2008); the latter is the\ntool used in Kaldi (Povey et al., 2011b), one of the\nmost powerful and used open-source ASR pack-\nage that we will use for some of the experiments\npresented in the following sections.\nIn recent years new techniques from the Neural\nNetworks (NN) domain have been introduced in\norder to enhance the performances of such models.\nElman recurrent NN, as used in the RNNLM tool\n(Mikolov et al., 2010, 2011), or Long Short-Term\nMemory NN, as in the tool LSTMLM (Soutner\nand M¨uller, 2015), produce state-of-the-art perfor-\nmances for current language models.\nThis paper presents a different approach for\nbuilding LM based on quantum probability the-\nory. Actually, we present a QLM applicable only\nto problems deﬁned on a small set of different to-\nkens. This is a “proof-of-concept” study and our\nmain aim is to show the potentialities of such ap-\nproach rather than building a complete application\nfor solving this problem for any setting.\nThe paper is organized as follows: we provide\nbackground on Quantum Probability Theory in\nSection 2 followed by the description of our pro-\nposed Quantum Language Model in Section 3. We\nthen discuss some numerical issues mainly related\n1840\nto the optimisation procedure in Section 4, and in\nSection 5 we present the experiments we did to\nvalidate our approach. In Section 6 we discuss our\nresults and draw some provisional conclusions.\n2 Quantum Probability Theory\nIn QMT the state of a system is usually described,\nin the most general case, by using density matrices\nover an Hilbert space H. More speciﬁcally, a den-\nsity matrix ρ is a positive semideﬁnite Hermitian\nmatrix of unit trace, namely ρ†= ρ, Tr(ρ) = 1,\nand it is able to encode all the information about\nthe state of a quantum system1.\nThe measurable quantities, or observables, of\nthe quantum system are associated to Hermitian\nmatrices O deﬁned on H. The axioms of QMT\nspecify how one can make predictions about the\noutcome of a measurement using a density matrix:\n•the possible outcomes of a projective mea-\nsurement of an observable Oare its eigenval-\nues {λj};\n•the probability that the outcome of the mea-\nsurement is λj is P(λj) = Tr( ρΠλj ) =\nTr(Πλj ρ), where Πλj is the projector on the\neigenspace of O associated to λj. Note that\nin the following we will use some proper-\nties of these kind of measurements, namely\nΠ†\nλj = Πλj and Π2\nλj = Πλj ;\n•after the measurement the system state col-\nlapses in the following fashion: if the out-\ncome of the measurement was λj, the col-\nlapse is\nρ′= Πλj ρΠλj\nTr(Πλj ρΠλj )\nwhere the denominator is needed for trace\nnormalization;\n•time evolution of states using a ﬁxed time\nstep is described by a unitary matrix U over\nH, i.e. U†U = I, where I is the identity ma-\ntrix. Given a state ρt, at a speciﬁc time t,\nthe system evolution without measurements\nmodiﬁes the state as:\nρt+1 = UρtU†.\nSee for example (Nielsen and Chuang, 2010)\nor (Vedral, 2007) for a complete introduction on\nQPT.\n1†marks the conjugate transpose of a vector/matrix and\nTr(·) is the trace of a matrix.\n3 Quantum Language Models\nIn this section we describe our approach to build\nQLM that can compute probabilities for the oc-\ncurrence of a sequence w = ( w1,w2,...,w n) of\nlength n, composed using N different symbols,\nthe vocabulary containing all the words in the\nmodel, i.e. for every symbol w in the sequence\nw∈{0,...,N −1}. We deﬁne a set of orthogonal\nN-dimensional vectors{ew : w∈{0,...,N −1}},\nspanning the complex space H= CN; to measure\nthe probability of a symbol w, collapsing the state\nover the space spanned by ew, we use the projec-\ntor Πw = ewe†\nw. Note that all the words in the\nvocabulary have been encoded as numbers corre-\nsponding to the N dimensions of the vector space\nH.\nOur method is sequential, from QMT point of\nview, in the sense that we use a quantum system\nthat produces a single symbol upon measurement.\nThe basic idea is that the probabilistic informa-\ntion for a given sequence w = (w1,w2,...,w n) is\nencoded in the density matrix that results from the\nfollowing process:\n•Inititalisation\nCond.Prob.: P(w1; ρ0,U) = Tr(ρ0Πw1 )\nProjection: ρ′\n1 = Πw1 ρ0Πw1\nTr(Πw1 ρ0Πw1 )\nEvolution: ρ1 = Uρ′\n1U†\n•Recurrence (i= 2,..,n )\nCond.Prob.: P(wi|w1,...,w i−1; ρ0,U) =\nTr(ρi−1Πwi )\nProjection: ρ′\ni = Πwi ρi−1Πwi\nTr(Πwi ρi−1Πwi )\nEvolution: ρi = Uρ′\niU†\n•Termination\nP(w|ρ0,U) = P(w1; ρ0,U) ·\nn∏\ni=2\nP(wi|w1,...,w i−1; ρ0,U)\nThe total probability P(w|ρ0,U) for the given\nsequence is thus obtained, in the termination\nstep, by multiplying the conditional probability\nP(wi|w1,...,w i−1; ρ0,U) for each word in the se-\nquence.\nWe then use the initial density matrixρ0 and the\ntime evolution unitary matrix U as parameters to\noptimise the perplexity Γ, evaluated on a training\n1841\ncorpus of sequences S,\nΓ(ρ0,U) = exp\n(\n−1\nC\n∑\nw∈S\nlog P(w|ρ0,U)\n)\nwhich quantiﬁes the uncertainty of the model. C\nis the number of tokens in the corpus.\nMinimising Γ is equivalent of learning a model\nby ﬁxing all the model parameters, a typical pro-\ncedure in the machine learning domain.\n3.1 Ancillary system\nThe problem with this setup is that the ‘quan-\ntum effects’ are completely washed out by the\nmeasurements on the system by using projec-\ntors. The resulting expression for the probability\nP(w|ρ0,U) for a sequence w is identical to that\nobtained using a classical Markov model.\nTo solve this issue, our approach is to avoid the\ncomplete collapse of the state after each symbol\nmeasurement using a common technique in QMT:\nwe introduce an ancillary system described by a\nﬁctitious D-dimensional Hilbert space,Hancilla =\nCD, and we couple the original system to the an-\ncillary system. The resulting DN-dimensional\nHilbert space is\nH2 ≡Hancilla ⊗H = CDN\nwhere ⊗denotes the Kronecker product for matri-\nces and D can be seen as a free hyper-parameter\nof the model. On this new space the projectors are\nnow given by Π(2)\nw = ID ⊗Πw, where ID is the\nD-dimensional identity matrix.\nThe advantage of using this method is that the\ntime evolution for the coupled system creates non-\ntrivial correlations between the two entangled sys-\ntems such that measuring and collapsing the sym-\nbol state keeps some information about the whole\nsequence stored in the ancillary part of the state.\nThis information is then reshufﬂed into the symbol\nstate via time evolution, resulting in a ‘memory ef-\nfect’ that takes the whole sequence of symbols into\naccount, thereby extending the idea behind the N-\ngrams approach. Larger D values will results in\nmore memory of this system and, of course, in a\nlarger number of parameters to learn.\n3.2 System evolution\nWe need to specify the system evolution for our\ncoupled system. The simplest approach is to use a\nunitary DN ×DN matrix U that acts on the en-\ntangled Hilbert space as shown before; it can be\nspeciﬁed by (DN)2 real parameters with a suit-\nable parametrization (Spengler et al., 2010) that\nensures the unitarity of U. However, in our pre-\nliminary experiments this approach resulted in an\ninsufﬁcient ‘memory’ capability for the QLM and\nin a very complex and slow minimisation proce-\ndure.\nA different approach could be introduced by us-\ning a speciﬁc unitary matrix for each word, but this\nwould lead to an enormous amount of parameters\nto learn with the optimization procedure.\nThere are a lot of techniques in NLP to repre-\nsent single words with dense vectors (see for ex-\nample (Mikolov et al., 2013) for the so calledword\nembeddings). Following this idea, we can repre-\nsent every symbol in our system with a speciﬁc p-\ndimensional vector trained using one of the avail-\nable techniques w ↦→(α1(w),...,α p(w)) or ﬁxed\nrandomly.\nWe then work with a set ofpDN ×DN unitary\nmatrices U = (U1,...,U p), one for each compo-\nnent of the word vector, that are used to dynami-\ncally build a different system evolution matrix for\neach word in this way:\nV(w) ≡\np∏\ni=1\nUαi(w)\ni\nThis results in p(DN)2 complex or 2p(DN)2 real\nparameters to be learned.\nEssentially, we treat the words in our problem\nin different ways: the evolution operator for each\nword V(w) is build by using a combination of the\noperators U deﬁned for each word-vector compo-\nnent, while, considering the system projection, we\ntreat each word as one basis vector for the space\nH.\nNote that the choice to use a set {V(w)}of\noperators, one for each word w, does not violate\nthe linearity of quantum mechanics: let K be the\nquantum operation\nK(ρ) =\n∑\nw\nV(w)Π(2)\nw ρΠ(2)\nw V†(w)\ndeﬁned using projectors and evolution matrices.\nThen K is a valid (i.e. a Completely Positive\nTrace-preserving) evolution map that exactly re-\nproduces our results in the sequence of evolutions\nand collapses.\nThe number of evolutionary operators is a trade-\noff: as we said before, deﬁning only one op-\nerator U resulted in a poor performance of the\n1842\nproposed method in all the relevant experiments,\nwhile deﬁning an operator for each word would\nproduce too many parameters to be learned. The\ntrade-off that we chose is to use one operator for\neach word-vector component, and build the set\n{V(w)}from them as described above while pre-\nserving unitarity.\nWith regard to the initial density matrix ρ0, we\nhave to deﬁne it combining the initial density ma-\ntrix of our system, ρs\n0, and the initial density ma-\ntrix of the ancilla, ρa\n0. We deﬁned ρs\n0 as a diagonal\nN ×N matrix containing the classical Maximum\nLikelihood probability Estimation to have a spe-\nciﬁc symbol at the ﬁrst sequence position:\nρs\n0 = 1\n|S|\n∑\nw∈S\nΠw1\nwhere S is again the set of all sequences in the\ntraining set and w1 is the ﬁrst word in each se-\nquence w. With regard to the ancilla system we\ndo not know anything about it and thus we have to\ndeﬁne ρa\n0 as the D×Ddiagonal matrix\nρa\n0 = ID\nTr(ID) .\nConsequently we can deﬁne ρ0 as\nρ0 = ρa\n0 ⊗ρs\n0 .\n3.3 The ﬁnal model\nPutting all the ingredients together, we can ﬁ-\nnally write down the formula for the probability\nP(w|ρ0,U) for a sequence w in the QLM speci-\nﬁed by ρ0 and U. The product of conditional prob-\nabilities simpliﬁes because of the normalising de-\nnominators added at each collapse and time evolu-\ntion step. The result is:\nP(w|ρ0,U) = Tr(Π(2)\nwn ...V†(w2)Π(2)\nw2 V†(w1)\nΠ(2)\nw1 ρΠ(2)\nw1 V(w1)Π(2)\nw2 V(w2)...Π(2)\nwn )\n(1)\nUsing the fact that projectors have many zero en-\ntries one can also re-express this trace of the prod-\nuct of DN×DN matrices in terms of the trace of\nthe product of D×D matrices. The formula for\nP(w|ρ0,U) then simpliﬁes to our ﬁnal result\nP(w|ρ0,U) = Tr(T†RT) (2)\nwhere the matricesRand T are deﬁned as follows:\n•in terms of entries Ri,j with indices i,j =\n0,...,D −1, the matrix Ris given by\nRi,j = [ρ0]Ni+w1,Nj+w1 .\nNote that only the value of ﬁrst symbol in the\nsequence, w1, enters in the expression. This\nis to be expected sinceRderives from the ini-\ntial density matrix ρ0;\n•analogously, the matrix T that encodes\nthe chain of combined collapses and time\nevolutions is given by the product T =\nT(2)T(3)...T(n), where the matrices T(k) are\ngiven in entries, with indicesi,j = 0,...,D −\n1, by\nT(k)\ni,j = [V(wk−1)]Ni+wk−1,Nj+wk .\nThese matrices can be pre-calculated for ev-\nery pair of the involved symbols, so that\nthe calculation of P(w|ρ0,U) for all the se-\nquences will be very fast.\nThe detailed calculation for obtaining the equation\n(2) can be found in the supplementary material.\n4 Optimisation and Numerical Issues\nIn order to optimise the parameters U we numer-\nically minimise the perplexity Γ computed on a\ngiven training corpus of sequences S. This re-\nquires that the matrices U remain strictly unitary\nat every step of the minimisation procedure and it\ncan be accomplished in various ways.\nThe most straightforward way is to employ\nan explicit parametrization for unitary matrices,\nas was done in (Spengler et al., 2010). Due\nto the transcendental functions employed in this\nparametrisation, this approach resulted in a func-\ntional form for Γ that has proven to be very chal-\nlenging to minimise efﬁciently in our experiments.\nA more elegant and efﬁcient approach is to con-\nsider the entries of U as parameters (thereby en-\nsuring a polynomial functional form for Γ) and\nto employ techniques of differential geometry to\nkeep the parameters from leaving the unitary sub-\nspace at each minimisation step. This can be done\nusing a modiﬁcation of the approach outlined in\n(Tagare, 2011) that considers the unitary matri-\nces subspace as a manifold, the Stiefel manifold\nU(DN). It is then possible to project the gradient\n∇f of a generic function f(M) of the matrix vari-\nable M on the tangent space of the Stiefel mani-\nfold and build a line search algorithm that sweeps\n1843\nout curves on this manifold so that at each point\nthe parameters are guaranteed to form a unitary\nmatrix.\nIn our case we have multiple unitary matrices\nU = ( U1,...,U p). This simply results having\ncurves deﬁned on U (DN)p, parametrised by a p-\ndimensional vector ofDN×DN unitary matrices.\n4.1 Formula for the gradient\nTo implement the curvilinear search method de-\nscribed in (Tagare, 2011) one needs an expression\nfor the gradient G = ( G1,...,G p) of the proba-\nbility function. This gradient is organised in a p-\ndimensional vector of DN ×DN matrices, such\nthat the component Gj is obtained by computing\nthe matrix derivative of P(w|ρ0,U) with respect\nto Uj either analytically or by applying some nu-\nmerical estimate of the gradients, for example by\nusing ﬁnite differences. The latter method, when\nworking with thousands or millions of variables\ncan be very time consuming and, usually, an ex-\nplicit analytic formula for the gradient accelerates\nconsiderably all the required processing.\nA lengthy analytic computation results in an ex-\nplicit result. Firstly, we introduce the following\nobjects:\n•The spectral decomposition of Uj, given by\nUj = SjDjS†\nj, guaranteed to exist by the\nspectral theorem. Sj is unitary and the di-\nagonal matrix Dj contains the eigenvalues\n(uj1,...,u jDN) of Uj, j = 1,...,p .\n•The DN ×DN matrices Cj(α) deﬁned, in\nentries, by\n[Cj(α)]ab = ujaα −ujbα\nuja −ujb\nif uja ̸= ujb\n[Cj(α)]ab = αujaα−1 if uja = ujb\nwhere uis the complex conjugate of u.\n•The D×DN matrices Qk given in entries by\n(Qk)jA = δNj+wk,A\nwhere j = 0,...,D −1, A= 0,...,DN −1.\n•The lesser and greater products associated to\nthe construction of system evolution matrices\nV<j(w) =\nj−1∏\ni=1\nUαi(w)\ni\nV>j(w) =\nn∏\ni=j+1\nUαi(w)\ni .\nWith these ingredients, the resulting formula for\nthe components Gj of the gradient is\nGj = 2Sj\nn∑\nk=2\n{[\nS†\nj\n(\nV<j(wk−1)†QT\nk−1\n(k−1∏\nl=2\nT(l)\n)†\nRT\n( n∏\nl=k+1\nT(l)\n)†\nQkV>j(wk−1)†\n)\nSj\n]\n·Cj(αj(wk−1))\n}\nS†\nj\n(3)\nwhere ·denotes the element-wise matrix product.\nAgain, all the detailed calculations for obtaining\nthe analytic expression (3) for the gradient Gj can\nbe found in the supplementary material.\nUsing Tagare’s method we can project the gra-\ndient onto the Stiefel manifold and build a curvi-\nlinear search algorithm for the minimisation.\nTo achieve this aim, Tagare proposed an\nArmijo-Wolfe line search inserted into a simple\ngradient descent procedure. We developed an ex-\ntension of this algorithm combining the minimiza-\ntion over the Steifel manifold technique with a\nMor´e-Thuente (1994) line search and a Conju-\ngate Gradient minimisation algorithm that uses the\nPolak-Ribi`ere method for the combination of gra-\ndients and search directions (Nocedal and Wright,\n2006). All the experiments presented in the next\nsection were performed using these methods.\nThe minimisation uses random mini-batches\nthat increase their size during the training: they\nstart with approximately one tenth of the training\nset dimension and increase to include all the in-\nstances using a parametrised logistic function. As\nstopping criterion we used the minimum of the\nperplexity function over the validation set as sug-\ngested in (Bengio, 2012; Prechelt, 2012) for other\nmachine learning techniques.\n5 Experiments and Results\n5.1 Data\nThe TIMIT corpus is a read speech corpus\ndesigned to provide speech data for acoustic-\nphonetic studies and for the development and eval-\nuation of automatic speech recognition systems\n(Garofolo et al., 1990). It contains broadband\nrecordings of 630 speakers of eight major dialects\n1844\nof American English and includes time-aligned or-\nthographic, phonetic and word transcriptions as\nwell as a 16-bit, 16 kHz speech waveform ﬁle for\neach utterance.\nIn the speech community, the TIMIT corpus\nis the base for a standard phone-recognition task\nwith speciﬁc evaluation procedures described in\ndetail in (Lopes and Perdigao, 2011). We stick\ncompletely to this evaluation to test the effective-\nness of our proposed model adopting, among the\nother procedures, the same splitting between the\ndifferent data sets: the training set contains 3696\nutterances (140225 phones), the validation set 400\nutterances (15057 phones) and the test set 192 ut-\nterances (7215 phones).\n5.2 Evaluation Results\nWe tested the proposed model by setting up two\ndifferent evaluations: the ﬁrst is an intrinsic evalu-\nation of LM performances in terms of global per-\nplexity on the TIMIT testset; the second is an\nextrinsic evaluation in which we replace the LM\ntools provided with the Kaldi ASR toolkit (Povey\net al., 2011b) with our model in order to check the\nﬁnal system performances in a phone-recognition\ntask and comparing them with the other state-of-\nthe-art LM techniques brieﬂy introduced in Sec-\ntion 1.\n5.2.1 Intrinsic evaluation\nThe ﬁrst experiment consisted in an evaluation of\nmodels perplexity (PPL) on the TIMIT testset. We\ncompared the QLM model with two N-gram im-\nplementations, namely CMU-SLM (Clarkson and\nRosenfeld, 1997) and IRSTLM (Federico et al.,\n2008), and two recurrent NN models able to\nproduce state-of-the-art results in language mod-\nelling, the RNNLM (Mikolov et al., 2010, 2011)\nand the LSTMLM (Soutner and M ¨uller, 2015)\npackages.\nTable 1 shows the results of the intrinsic evalu-\nation. With regard to RNNLM and LSTMLM re-\nsults, only the best hyper-parameters combination\nafter a lot of experiments, optimizing them on the\nvalidation set, has been inserted into the Table.\nWith regard to QLM, all the presented ex-\nperiments are based on artiﬁcial word vectors\nproduced randomly using values from the set\n{−1,0,1}instead of real word embeddings. Ev-\nery word vector is different from the others and we\ndecided not to use real embeddings in order to test\nthe core QMT method without adding the contex-\nModel Parameters PPL\nCMU-SLM 2-gram 15.49\n(Good-Turing 3-gram 14.28\nsmoothing) 4-gram 15.62\n5-gram 17.33\nIRSTLM 2-gram 15.47\n(linear Witten- 3-gram 14.07\nBell smoothing) 4-gram 15.55\n5-gram 17.53\nRNNLM 280 neurons 13.32\nLSTMLM 25 neurons, 1 layer 13.17\nQLM N=48, p=4, D=10 13.44\nN=48, p=4, D=20 13.15\nN=48, p=4, D=30 13.10\nN=48, p=4, D=40 12.99\nTable 1: Perplexity (PPL) of the tested language-\nmodelling techniques on the TIMIT testset. All\nthe QLM results in bold face are better than the\nother systems we tested.\ntual information, contained in word embeddings,\nthat could have helped our approach to obtain bet-\nter performances, at least in principle.\n5.2.2 Extrinsic evaluation\nThe “TIMIT recipe” contained in the Kaldi dis-\ntribution2 reproduces exactly the same evalua-\ntion settings described in (Lopes and Perdigao,\n2011) for a phone recognition task based on this\ncorpus. Moreover, Kaldi provides some n-best\nrescoring scripts that apply RNNLM hypothesis\nrescoring and interpolate the results with the stan-\ndard N-gram model results used in the evaluation.\nWe slightly modiﬁed these scripts to work with\nLSTMLM and QLM in order to test different mod-\nels using the same setting. This allowed us to re-\nplace the LM used in Kaldi and experiment with\nall the systems evaluated in the previous section.\nTable 2 outlines the results we obtained replac-\ning the LM technique into Kaldi ASR package\nw.r.t. the different ASR systems that the TIMIT\nrecipe implements. These systems are built on top\nof MFCC, LDA, MLLT, fMLLR with CMN3 fea-\ntures (see (Povey et al., 2011b; Rath et al., 2013)\nfor all acronyms references and a complete feature\n2https://github.com/kaldi-asr/kaldi\n3MFCC: Mel-Frequency Cepstral Coefﬁcients; LDA:\nLinear Discriminant Analysis; MLTT: Maximum Likelihood\nLinear Transform; fMLLR: feature space Maximum Likeli-\nhood Linear Regression; SAT: Speaker Adapted Training, i.e.\ntrain on fMLLR-adapted features; CMN: Cepstral Mean Nor-\nmalization.\n1845\nor recipe descriptions).\nFor this extrinsic evaluation we used the best\nmodels we obtained in the previous experiments\ninterpolating their log-probability results for each\nutterance with the original bigram (or trigram)\nlog-probability using a linear model with a ratio\n0.25/0.75 between the original N-gram LM and\nthe tested one as suggested in the standard Kaldi\nrescoring script. For this test we rescored the\n10,000-best hypothesis.\nWe have to say that in this experiment we were\nnot trying to build the best possible phone recog-\nniser, but simply to compare the relative perfor-\nmances of the analysed LM techniques showing\nthe effectiveness of QLM when used in a real ap-\nplication. Thus absolute Phone Error Rate is not\nso important here and it can be certainly possible\nto devise recognisers with better performances by\napplying more sophisticated techniques. For ex-\nample (Peddinti et al., 2015) presented a method\nfor lattice rescoring in Kaldi that exhibits better\nperformances than the n-best rescoring we used to\ninterpolate between n-grams and the tested mod-\nels, but modifying it in order to test LSTMLM\nand QLM presented a lot of problems and thus\nwe decided to use the simpler n-best approach.\nFor completeness, the last column of Table 2 out-\nlines the results obtained using this lattice rescor-\ning method with RNNLM as described in (Ped-\ndinti et al., 2015).\n6 Discussion and conclusions\nWe presented a new technique for building LM\nbased on QMT, and its probability calculus, test-\ning it extensively both with intrinsic and extrinsic\nevaluation methods.\nThe PPL results for the intrinsic evaluation,\noutlined in Table 1, show a clear superiority\nof the proposed method when compared with\nstate-of-the-art techniques such as RNNLM and\nLSTMLM. It is interesting to note that even using\nD = 20, that means a system containing a quar-\nter of parameters, therefore much less ‘memory’,\nw.r.t. the system with D = 40, we obtain a PPL\nperformance better than the other methods.\nWith regard to the second experiment we made,\nan extrinsic evaluation where we replaced the LM\nof an ASR system with the LM produced by all the\ntested methods (see Table 2), QLM consistently\nexhibits the best performances for all the tested\nASR systems from the Kaldi “TIMIT recipe”. De-\nspite using a n-best technique in this evaluation\nfor hypothesis rescoring, that is known to perform\nworse than the lattice rescoring method proposed\nin (Peddinti et al., 2015), the QLM performances\nare even better than this method.\nThe approach we have presented in this paper\nis not without problems: the number of different\nword types in the considered language has to be\nsmall in order to keep the model computationally\ntractable. Even if the code we used in the evalu-\nations is analytically highly optimised, the train-\ning of this model is rather slow and requires rele-\nvant computational resources even for small prob-\nlems. On the contrary, inference is very quick,\nfaster than the RNNLM and LSTMLM packages\nwe tested.\nThe main research question that drove this work\nwas to verify if the distinguishing properties of\nquantum probability theory, namely interference\nand system entaglement that could allow the an-\ncilla to have a “potentially inﬁnite” memory, were\nenough to build stochastic systems more power-\nful than those built using classical probabilities\nor those built using recurrent NN. Our main aim\nwas not to build a complete model to handle all\npossible LM scenarios, but to present a “proof-of-\nconcept” study to test the potentialities of this ap-\nproach. For this reason we tried to keep the model\nas simple as possible using orthogonal projectors:\nfor measuring probabilities, projecting the system\nstate, each word is mapped onto a single basis vec-\ntor and the dimension of the system Hilbert space,\nN, is equal to the number of different words.\nGiven the matrix dimensions that we have to man-\nage when we add the ancilla, DN×DN, this set-\nting does not scale to real LM problems (e.g. the\nBrown corpus), even though the calculations are\nperformed using D×Dsubmatrices, but allowed\nus to successfully verify the research question. For\nthe same reason out-of-vocabulary words cannot\nbe handled in this model because there are no ba-\nsis vectors assigned to them.\nIn order to overcome these limitations, this\nwork can be extended by using generalized quan-\ntum measurements projectors (POVM) and by us-\ning a different structure for the system Hilbert\nspace: instead of mapping each word onto a sin-\ngle basis vector we can span this space using as\nbasis the same p-basis vectors used to deﬁne the\nV matrices. In this way we will project the system\nstate on a generic word vector built as a superposi-\n1846\nKaldi IRSTLM N-Best rescoring Lattice\nASR 2-gram IRSTLM 2-gram LM interp. with: rescoring\nSystem RNNLM LSTMLM QLM RNNLM\ntri1 26.32 25.74 25.09 24.59 25.70\ntri2 24.14 23.34 23.23 23.05 23.17\ntri3 21.55 21.07 21.22 20.35 20.85\nSGMM2 19.15 18.99 18.52 18.23 18.75\nDan NN 22.27 22.20 22.26 21.80 22.05\nKaldi IRSTLM N-Best rescoring Lattice\nASR 3-gram IRSTLM 3-gram LM interp. with: rescoring\nSystem RNNLM LSTMLM QLM RNNLM\ntri1 25.64 25.39 24.86 24.59 25.42\ntri2 23.16 23.13 22.90 22.65 22.97\ntri3 20.80 20.57 20.68 20.04 20.68\nSGMM2 18.64 18.41 18.48 18.23 18.27\nDan NN 21.72 21.90 21.95 21.34 21.48\nTable 2: Phone-recognition performances, in terms of Phone Error Rate, for the TIMIT dataset and\nthe different Kaldi ASR models, rescoring the 10,000-best solutions with the tested LM techniques in-\nterpolated with the IRSTLM bigrams and trigrams LM (the standard LM used in Kaldi). In boldface\nthe best performing system and in italics the second best. Kaldi ASR systems descriptions: tri1 = a\ntriphone model using 13 dim. MFCC+ ∆+∆∆; tri2 = tri1+LDA+MLLT; tri3 = tri2+SAT; SGMM2 =\nSemi-supervised Gaussian Mixture Model (Huang and Hasegawa-Johnson, 2010; Povey et al., 2011a);\nDan NN = DNN model by (Zhang et al., 2014; Povey et al., 2015).\ntion on the p-basis. Such improvement would re-\nduce dramatically the dimensions of the matrices\nto Dp×Dp potentially mitigating the computa-\ntional issue. Moreover, this would solve also the\nproblem of out-of-vocabulary words allowing for\na proper management of the large set of different\nwords typical of real applications.\nWe are still working on these improvements and\nwe will hope to get a complete model soon.\nWith this contribution we would like to raise\nalso some interest in the community to analyse\nand develop more effective techniques, both on\nthe modelling and minimisation/learning sides, to\nallow to build real world application based on\nthis framework. QMT and its probability calculus\nseem to be promising methodologies to enhance\nthe performances of our systems in NLP and cer-\ntainly deserve further investigations.\nAcknowledgments\nWe acknowledge the CINECA 4 award no.\nHP10C7XVUO under the ISCRA initiative, for\nthe availability of HPC resources and support.\n4https://www.cineca.it/en\nReferences\nDiederik Aerts, Jan Broekaert, Liane Gabora, and\nSandro Sozzo. 2013. Quantum structure and hu-\nman thought. Behavioral and Brain Sciences ,\n36(3):274276.\nMartin Arjovsky, Amar Shah, and Yoshua Bengio.\n2016. Unitary evolution recurrent neural networks.\nIn Proceedings of the 33rd International Conference\non International Conference on Machine Learning -\nICML’16, pages 1120–1128.\nYoshua Bengio. 2012. Practical recommendations for\ngradient-based training of deep architectures. In\nGr´egoire Montavon, Genevi `eve B. Orr, and Klaus-\nRobert M ¨uller, editors, Neural Networks: Tricks of\nthe Trade: Second Edition, pages 437–478. Springer\nBerlin Heidelberg, Berlin, Heidelberg.\nWilliam Blacoe, Elham Kasheﬁ, and Mirella Lapata.\n2013. A quantum-theoretic approach to distribu-\ntional semantics. In Proceedings of Human Lan-\nguage Technologies: Conference of the North Amer-\nican Chapter of the Association of Computational\nLinguistics, Atlanta, Georgia, pages 847–857.\nJerome R. Busemeyer and Peter D. Bruza. 2012.Quan-\ntum Models of Cognition and Decision . Cambridge\nUniversity Press, New York, NY .\nPhilip Clarkson and Ronald Rosenfeld. 1997. Statis-\ntical language modeling using the cmu-cambridge\n1847\ntoolkit. In Proceedings of EUROSPEECH ’97 ,\npages 2707–2710. ISCA.\nMarcello Federico, Nicola Bertoldi, and Mauro Cet-\ntolo. 2008. IRSTLM: an open source toolkit for\nhandling large scale language models. In INTER-\nSPEECH 2008, 9th Annual Conference of the Inter-\nnational Speech Communication Association, Bris-\nbane, Australia, pages 1618–1621.\nJohn Garofolo, Lori Lamel, William Fisher, Jonathan\nFiscus, David Pallett, Nancy Dahlgren, and Vic-\ntor Zue. 1990. Darpa timit acoustic-phonetic con-\ntinuous speech corpus cd-rom. DARPA, TIMIT\nAcoustic-Phonetic Continuous Speech Corpus CD-\nROM.\nFabio A. Gonz´alez and Juan C. Caicedo. 2011. Quan-\ntum latent semantic analysis. In Advances in Infor-\nmation Retrieval Theory, LNCS, 6931, pages 52–63.\nJui Ting Huang and Mark Hasegawa-Johnson. 2010.\nSemi-supervised training of gaussian mixture mod-\nels by conditional entropy minimization. In Pro-\nceedings of the 11th Annual Conference of the In-\nternational Speech Communication Association, IN-\nTERSPEECH 2010, pages 1353–1356.\nLi Jing, Yichen Shen, Tena Dubcek, John Peurifoy,\nScott A. Skirlo, Max Tegmark, and Marin Soljacic.\n2017. Tunable efﬁcient unitary neural networks\n(EUNN) and their application to RNN. In Thirty-\nfourth International Conference on Machine Learn-\ning - ICML2017.\nDimitrios Kartsaklis, Martha Lewis, and Laura Rimell.\n2016. Proceedings of the 2016 Workshop on Seman-\ntic Spaces at the Intersection of NLP , Physics and\nCognitive Science, volume 221. Electronic Proceed-\nings in Theoretical Computer Science.\nAndrei Y . Khrennikov. 2010. Ubiquitous Quantum\nStructure: From Psychology to Finance . Springer-\nVerlag Berlin Heidelberg.\nDing Liu, Xiaofang Yang, and Minghu Jiang. 2013. A\nnovel classiﬁer based on quantum computation. In\nProceedings of the 51st Annual Meeting of the As-\nsociation for Computational Linguistics, Soﬁa, Bul-\ngaria, pages 484–488.\nCarla Lopes and Fernando Perdigao. 2011. Phoneme\nrecognition on the timit database. In Ivo Ipsic, edi-\ntor, Speech Technologies. InTech, Rijeka.\nMassimo Melucci. 2015. Introduction to Information\nRetrieval and Quantum Mechanics . The Informa-\ntion Retrieval Series 35. Springer-Verlag Berlin Hei-\ndelberg.\nMassimo Melucci and Keith van Rijsbergen. 2011.\nQuantum mechanics and information retrieval. In\nMassimo Melucci and Ricardo Baeza-Yates, editors,\nAdvanced Topics in Information Retrieval , pages\n125–155. Springer Berlin Heidelberg, Berlin, Hei-\ndelberg.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient Estimation of Word Repre-\nsentations in Vector Space. In Proc. of Workshop at\nICLR.\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, pages 1045–1048.\nTom´aˇs Mikolov, Stefan Kombrink, Anoop Deoras,\nLuk´aˇs Burget, and Jan ˇCernock´y. 2011. Rnnlm -\nrecurrent neural network language modeling toolkit.\nIn Proceedings of ASRU 2011, pages 1–4.\nJorge J. Mor ´e and David J. Thuente. 1994. Line\nsearch algorithms with guaranteed sufﬁcient de-\ncrease. ACM Trans. Math. Softw., 20(3):286–307.\nMichael A. Nielsen and Isaac L. Chuang. 2010. Quan-\ntum Computation and Quantum Information: 10th\nAnniversary Edition. Cambridge University Press.\nJ. Nocedal and S. J. Wright. 2006. Numerical Opti-\nmization, 2nd edition. Springer, New York.\nVijayaditya Peddinti, Guoguo Chen, Vimal Manohar,\nTom Ko, Daniel Povey, and Sanjeev Khudanpur.\n2015. JHU aspire system: Robust LVCSR with\ntdnns, ivector adaptation and RNN-LMS. In 2015\nIEEE Workshop on Automatic Speech Recognition\nand Understanding, ASRU 2015, Scottsdale, AZ,\nUSA, pages 539–546.\nDaniel Povey, Luk ´aˇs Burget, Mohit Agarwal, Pinar\nAkyazi, Feng Kai, Arnab Ghoshal, Ond ˇrej Glem-\nbek, Nagendra Goel, Martin Karaﬁ ´at, Ariya Ras-\ntrow, Richard C. Rose, Petr Schwarz, and Samuel\nThomas. 2011a. The subspace gaussian mixture\nmodel-a structured model for speech recognition.\nComput. Speech Lang., 25(2):404–439.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr\nSchwarz, Jan Silovsky, Georg Stemmer, and Karel\nVesely. 2011b. The kaldi speech recognition toolkit.\nIn IEEE 2011 Workshop on Automatic Speech\nRecognition and Understanding . IEEE Signal Pro-\ncessing Society.\nDaniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur.\n2015. Parallel training of dnns with natural gradient\nand parameter averaging. In International Confer-\nence on Learning Representations - ICLR2015.\nLutz Prechelt. 2012. Early stopping — but when? In\nGr´egoire Montavon, Genevi `eve B. Orr, and Klaus-\nRobert M ¨uller, editors, Neural Networks: Tricks of\nthe Trade: Second Edition , pages 53–67. Springer\nBerlin Heidelberg, Berlin, Heidelberg.\n1848\nShakti P. Rath, Daniel Povey, Karel Vesel ´y, and\nJan Cernock ´y. 2013. Improved feature processing\nfor deep neural networks. In Proceedings of the\n14th Annual Conference of the International Speech\nCommunication Association - INTERSPEECH2013,\nLyon, France, pages 109–113.\nDaniel Soutner and Lud ˇek M ¨uller. 2015. In Adrian-\nHoria Dediu, Carlos Mart ´ın-Vide, and Kl´ara Vicsi,\neditors, Statistical Language and Speech Process-\ning: Third International Conference, SLSP 2015,\nBudapest, Hungary, November 24-26, 2015, Pro-\nceedings, pages 267–274. Springer International\nPublishing.\nChristoph Spengler, Marcus Huber, and Beatrix C\nHiesmayr. 2010. A composite parameterization\nof unitary groups, density matrices and subspaces.\nJournal of Physics A: Mathematical and Theoreti-\ncal, 43(38):385306.\nH.D. Tagare. 2011. Notes on optimization on Stiefel\nmanifolds. Technical report, Technical report, Yale\nUniversity.\nFabio Tamburini. 2014. Are quantum classiﬁers\npromising? In Proceedings of the First Italian Con-\nference on Computational Linguistics CLiC-it 2014,\nPisa, Pisa University Press, pages 360–364.\nVlatko Vedral. 2007. Introduction to Quantum Infor-\nmation Science. Oxford University Press, USA.\nScott Wisdom, Thomas Powers, John Hershey,\nJonathan Le Roux, and Les Atlas. 2016. Full-\ncapacity unitary recurrent neural networks. In D. D.\nLee, M. Sugiyama, U. V . Luxburg, I. Guyon, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 29 , pages 4880–4888. Curran\nAssociates, Inc.\nXiaohui Zhang, Jan Trmal, Daniel Povey, and San-\njeev Khudanpur. 2014. Improving deep neural net-\nwork acoustic models using generalized maxout net-\nworks. In Acoustics, Speech and Signal Processing\n(ICASSP), 2014 IEEE International Conference on,\npages 215–219. IEEE.\nGuido Zuccon, Leif A. Azzopardi, and Keith van Ri-\njsbergen. 2009. The quantum probability ranking\nprinciple for information retrieval. In Leif et al. Az-\nzopardi, editor, Advances in Information Retrieval\nTheory, LNCS, 5766, pages 232–240.\n1849",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9259734153747559
    },
    {
      "name": "Computer science",
      "score": 0.7217456698417664
    },
    {
      "name": "Language model",
      "score": 0.7033859491348267
    },
    {
      "name": "Quantum entanglement",
      "score": 0.6933871507644653
    },
    {
      "name": "Quantum",
      "score": 0.49205106496810913
    },
    {
      "name": "Theoretical computer science",
      "score": 0.42000776529312134
    },
    {
      "name": "Quantum probability",
      "score": 0.4104580879211426
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3586500287055969
    },
    {
      "name": "Quantum process",
      "score": 0.1210736632347107
    },
    {
      "name": "Quantum mechanics",
      "score": 0.11256897449493408
    },
    {
      "name": "Quantum dynamics",
      "score": 0.09070250391960144
    },
    {
      "name": "Physics",
      "score": 0.08009892702102661
    }
  ]
}