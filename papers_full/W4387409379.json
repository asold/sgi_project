{
  "title": "Swinv2-Imagen: hierarchical vision transformer diffusion models for text-to-image generation",
  "url": "https://openalex.org/W4387409379",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100750027",
      "name": "Ruijun Li",
      "affiliations": [
        "Auckland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5044008966",
      "name": "Weihua Li",
      "affiliations": [
        "Auckland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5039924074",
      "name": "Yi Yang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100657666",
      "name": "H. Wei",
      "affiliations": [
        "University of Tasmania"
      ]
    },
    {
      "id": "https://openalex.org/A5028333907",
      "name": "Jianhua Jiang",
      "affiliations": [
        "Jilin University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A5029548157",
      "name": "Quan Bai",
      "affiliations": [
        "University of Tasmania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3080642835",
    "https://openalex.org/W3006538026",
    "https://openalex.org/W4226278310",
    "https://openalex.org/W3212516020",
    "https://openalex.org/W2965289598",
    "https://openalex.org/W2054829889",
    "https://openalex.org/W2884367402",
    "https://openalex.org/W6600234944",
    "https://openalex.org/W6601055912",
    "https://openalex.org/W2963184176",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W2966792645",
    "https://openalex.org/W3034667500",
    "https://openalex.org/W2963163163",
    "https://openalex.org/W3174194560",
    "https://openalex.org/W4312282373",
    "https://openalex.org/W3035500781",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W6777078701",
    "https://openalex.org/W4200498145",
    "https://openalex.org/W2077069816",
    "https://openalex.org/W2250378130",
    "https://openalex.org/W3035665735",
    "https://openalex.org/W3035750252",
    "https://openalex.org/W3004349648",
    "https://openalex.org/W2807697862",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W3107848485",
    "https://openalex.org/W2992478697",
    "https://openalex.org/W2965833116",
    "https://openalex.org/W4312561757",
    "https://openalex.org/W3087257704",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W2971865858",
    "https://openalex.org/W4290878206",
    "https://openalex.org/W3209047863",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2774320778",
    "https://openalex.org/W3030515889",
    "https://openalex.org/W2905338897",
    "https://openalex.org/W2594833348",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3143894246",
    "https://openalex.org/W3174525637",
    "https://openalex.org/W4312438583",
    "https://openalex.org/W4312911498",
    "https://openalex.org/W4312977351",
    "https://openalex.org/W4312388283",
    "https://openalex.org/W4225495512",
    "https://openalex.org/W2928133111",
    "https://openalex.org/W3102554291"
  ],
  "abstract": "Abstract Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google’s Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
  "full_text": "ORIGINAL ARTICLE\nSwinv2-Imagen: hierarchical vision transformer diffusion models\nfor text-to-image generation\nRuijun Li1 • Weihua Li1 • Yi Yang2 • Hanyu Wei3 • Jianhua Jiang4 • Quan Bai3\nReceived: 7 December 2022 / Accepted: 5 September 2023 / Published online: 6 October 2023\n/C211 The Author(s) 2023\nAbstract\nRecently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of\nstudies, immediately presenting new study opportunities for image generation. Google’s Imagen follows this research trend\nand outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language\nmodel for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efﬁcient\nUNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-\nImagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorpo-\nrating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved\nin the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-\nTransformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN\nconvolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using\nthree real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed\nSwinv2-Imagen model outperforms several popular state-of-the-art methods.\nKeywords Text-to-image synthesis /C1Diffusion models /C1Scene graph /C1Graph neural network /C1UNet\n1 Introduction\nPeople tend to describe rich and detailed pictures of scenes\nthrough language, and the ability to generate images from\nthese descriptions can facilitate creative applications in\nvarious life contexts, including art design and multimedia\ncontent creation [1, 2]. This fact has inspired researchers to\ndesign models of text-to-image comparative learning to\nassist people with making decisions quickly in speciﬁc\nscenarios, such as presentation and advertising design\n[3, 4]. In recent years, diffusion models have attracted the\nattention of many scholars due to their promising perfor-\nmance in image generation. Within this framework,\nDALL-E 2 [ 5] and Imagen [ 6] have become successful\ngenerative models for image generation.\nImagen is currently one of the greatest image generation\nmodels. Its most signiﬁcant distinguishing feature is its\nimmensity, which is reﬂected, in particular, by its utilisa-\ntion of a large text encoder, i.e. T5 [7]. T5 is pre-trained on\na sizable plain text corpus. It turns out that T5 is very\neffective for enhancing image ﬁdelity and image-text\nalignment [ 6]. However, using T5 alone to obtain text\nYi Yang, Hanyu Wei, Jianhua Jiang and Quan Bai have\ncontributed equally to this work.\n& Ruijun Li\nzjc0233@autuni.ac.nz\n& Weihua Li\nweihua.li@aut.ac.nz\nYi Yang\nyyang@hfut.edu.cn\nHanyu Wei\nhanyu.wei@utas.edu.au\nJianhua Jiang\njjh@jlufe.edu.cn\nQuan Bai\nquan.bai@utas.edu.au\n1 Auckland University of Technology, Auckland 1010, New\nZealand\n2 Hefei University of Technology, Hefei 230601, China\n3 University of Tasmania, Hobart 7005, Australia\n4 Jilin University of Finance and Economics, Changchun,\nChina\n123\nNeural Computing and Applications (2024) 36:17245–17260\nhttps://doi.org/10.1007/s00521-023-09021-x(0123456789().,-volV)(0123456789().,-volV)\nembeddings cannot guarantee that the model learns\nimportant text features, such as semantic layout. Besides\nvisual elements, the semantic layout is recognised as an\nimportant factor in guiding text-to-image synthesis [8]. Our\nexperimental results provide evidence for this claim.\nFurthermore, very few research works are dedicated to\naddressing the UNet issue of Imagen. The diffusion model\nof Imagen relies on the Efﬁcient-UNet, which suffers from\nthe limitations of CNN convolution operations. CNN are\ngood at extracting the low-level features and elements of\nvisual structure, such as colour, contour, texture and shape\n[9]. However, CNN focuses on the consistency of these\nlow-level features under transformations, such as transla-\ntion [ 10] and rotation [ 11]. This is also the main reason\nwhy CNNs are widely used in object detection [ 12]. In\nother words, while the convolutional ﬁlters are good at\ndetecting key points, object boundaries and other basic\nunits that constitute the visual elements, it fails to extract\nfeatures efﬁciently in terms of global and layout. For text-\nto-image synthesis tasks, it is signiﬁcant to consider how to\naccurately extract the complex relationships between\nobjects from the limited text. The Transformer is more\nnatural and efﬁcient than CNN in processing this demand.\nThis is mainly because the attention in the Transformer can\neffectively mine the relationships between text features,\nallowing the model not only focuses on local information\nbut also has a diffusion mechanism to ﬁnd expressions\nfrom the local to global layout [ 13, 14].\nTo solve the aforementioned drawbacks of Imagen, in\nthis paper, we propose a diffusion text-to-image generation\nmodel called Swinv2-Imagen. The proposed model is\nbased on a Hierarchical Visual Transformer and Scene\nGraph incorporating layout information. Speciﬁcally, the\nsemantic layout is generated via semantic scene graphs,\nenabling Swinv2-Imagen to parse the layout information in\nthe text description effectively. In this paper, we adopt\nStanford Scene Graph Parser [ 15] to obtain the Scene\nGraph from the text. Subsequently, the entity and rela-\ntionship embeddings are extracted using a frozen Graph\nConvolution Network (GCN) [ 15]. The image generation\nprocess appears conditional on text, object and relationship\nembeddings. The layout representation with global\nsemantic information ensures the realism of the generated\nimages. In addition, the diffusion models are developed\nbased on Swinv2-Unet, a variant of Swin Transformer v2\n[16], which allows the model to learn features from local to\nglobal. Finally, we evaluate our model on the MSCOCO,\nCUB and MM-CelebA-HQ datasets. The results show that\nthe proposed model outperforms the current best generative\nmodel, Imagen, on MSCOCO. The ablation experiments\nreveal that the addition of semantic layouts is effective in\nimproving the semantic understanding of the model.\nThe key contributions of this paper are summarised\nbelow.\n1. We leverage scene graphs to extract entity and\nrelational embeddings to improve local and layout\ninformation representation of text for a more accurate\nunderstanding of the text and realistic image\ngeneration;\n2. We propose Swinv2-UNet as a novel diffusion model\narchitecture. The model leverages attention to explore\nthe relationship between features, allowing the diffu-\nsion model to focus on different granularities of\nfeatures at different moments, from local to global;\n3. We fuse the scene graph with the diffusion model, and\nthe experimental results demonstrate that the resulting\nimages not only generate the objects speciﬁed in the\ntext, but also additional objects based on speciﬁc words\n(e.g. kitchen);\n4. We achieve a new state-of-the-art FID result\n(FID=7.21) on the MSCOCO dataset compared to the\nlatest generative models. Better results are also\nobtained on both the CUB (FID=9.78) and MM\nCelebA-HQ (FID=10.31).\nThe rest of the paper is organised as follows. In Sect. 2,\nrelated works are reviewed. In Sect. 3, we elaborate on the\nproposed Swinv2-Imagen model. In Sect. 4, we conduct\nextensive experiments to evaluate the performance of the\nproposed model and perform an ablation study to evaluate\nthe contributions of each key component of our model.\nFinally, we conclude this paper in Sect. 5, and discuss\nfuture research directions.\n2 Related work\n2.1 Diffusion models\nText-to-Image synthesis is a typical application of multi-\nmodal and cross-modal comparative learning. In the ﬁeld\nof image generation, most models mainly fall into two\ncategories, i.e. the GAN-based generation models [ 17–22]\nand the diffusion-based models [ 23–27]. The former has\nbeen developed over the last few years and widely used in\nmany scenarios, such as medical and image restoration.\nThe latter has demonstrated outstanding performance over\nthe GAN models, acknowledged as state-of-the-art deep\ngenerative models [ 6, 28, 29].\nDiffusion models and GAN generative models are\nessentially comparable, both being a process of gradually\nremoving noise. However, in contrast to GAN, the diffu-\nsion models do not suffer from training instability and\nmodel collapse. The diffusion model transforms the data\ndistribution into random noise and reconstructs data\n17246 Neural Computing and Applications (2024) 36:17245–17260\n123\nsamples with the same distribution [ 6, 30]. The diffusion\nmodel demonstrates outstanding performance for a number\nof tasks, such as multimodal modelling. Many contempo-\nrary text-to-image synthesis models, e.g. DALL-E 2 [ 5],\nImagen [ 6] and GLID [ 25], are constructed based on the\ndiffusion model. They cascade multiple diffusion models to\nimprove the image generation quality step by step. DALL-\nE 2 uses a priori diffusion model and CLIP Latents to\nprocess the text. In contrast, Imagen discards the priori\nmodel and replaces it with a large pre-trained text encoder,\ni.e. T5. Although the T5 model leveraged in the Imagen\nmodel improves the understanding of the text, it does not\nensure that the model understands the semantic layout of\nthe text, especially in complex sentences containing mul-\ntiple objects and relationships. As a result, the model will\nnot be able to reproduce some entities or will lose some\nentity relationships. Therefore, we attempt to model the\nglobal semantic layout by adding a scene graph in the text\nprocessing. Furthermore, Imagen builds its diffusion model\nbased on Efﬁcient-Unet. Efﬁcient Unet is not the best\nchoice in image generation tasks, because it contains\nmultiple CNN blocks and leads to a limited view within the\nCNN kernel window.\n2.2 Scene graph and graph representation\nlearning\nA sentence’s nature is a linear data structure, where one\nword follows another [ 15]. Usually, when a sentence is\ncomplex with multiple objects, it is time-consuming to\nanalyse the sentence directly, and the accuracy of the text-\nimage alignment is not guaranteed. Complex sentences\noften incorporate rich scene information. Mapping this\ninformation into a scene graph can provide an intuitive\nunderstanding of the relationships between objects in a\nsentence [31]. Previous studies reveal that the performance\nof multimodal models, such as text-to-image synthesis, is\nsigniﬁcantly dependent on mining visual relationships [32].\nScene graphs can provide a high level of understanding\nregarding scene information [ 15]. Therefore, the scene\ngraph is recognised as a useful representation of images\nand text. Speciﬁcally, each node in a scene graph repre-\nsents an object, such as a person or an event, and each\nobject has multiple attributes, such as shape. The rela-\ntionships between objects are denoted by the edges\nbetween nodes, which can be an action or a position [ 33].\nRecently, the scene graphs have been used extensively for\ntasks such as text-based image retrieval [ 34, 35], semantic\nsegmentation [ 36, 37], visual question answering [ 38],\nimage captioning [ 39–42] and image generation\n[15, 31, 43, 44]. The recently proposed dynamic scene\ngraph generation also demonstrates the prospects of scene\ngraphs in video monitoring, autonomous driving, and other\nﬁelds related to video processing and generation [ 45].\nIn addition, there is no way for an image generation\nmodel to manipulate graph-like data such as scene graphs\ndirectly, so scene graphs are usually used in conjunction\nwith graph representation learning [46]. The main objective\nof graph representation learning is to extract node and edge\ncontexts from the scene graph and map them to a set of\nembeddings. Graph representation learning methods can\ncurrently be classiﬁed into two types, i.e. machine learning\nbased on Random-Walk and deep learning Graph Convo-\nlution-based methods [ 46]. Node2vec [ 47] is a typical\nrepresentative model of the former. It is based on Skip-\nGram [ 48] theory to learn the embedding of nodes on a\ngraph and optimises the sampling method. It is proposed in\nrelated studies [ 49] that two sampling methods, Breadth-\nFirst Search (BFS) and Depth First Search (DFS), are\nmainly included when sampling neighbouring nodes in a\ngraph. BFS requires that each sampled node is a direct\nneighbour of that node. This sampling method results in a\ngraph representation that is more concerned with local\ninformation. In contrast, DFS, where each node is sampled\nto increase the distance to the initial node as much as\npossible, produces a graph representation that focuses more\non global information. Random-Walk-based representation\nlearning [50] comprises multiple stages, each with different\noptimisation goals, which is a typical non-end-to-end\nmodel. Graph convolution-based methods, e.g. Graph\nconvolution neural networks [ 15], are able to learn both\nnode feature information and structural information via an\nend-to-end way. It focuses on both local information and\nglobal structural features. Graph Convolutional Neural\nNetworks (GCNs) have gained signiﬁcant attention in\nrecent studies as powerful tools for analysing graph-\nstructured data, such as social networks and database\ntables [51]. For the text-to-image synthesis task, generation\nmodels leverage GCNs to capture semantic relationships\nbetween textual descriptions and visual features, enabling\nmore accurate image generation. Recently, many studies\nhave been conducted to enhance graph-based neural net-\nworks. Speciﬁcally, a novel augmentation method called\nGraphENS has been proposed to address the issue of\noverﬁtting to neighbour sets of minor class nodes [ 52, 53].\nThey proposed a saliency-based node mixing method to\nleverage the abundant class-generic attributes of other\nnodes while preventing the injection of class-speciﬁc fea-\ntures. To mitigate the negative impact of oversampling on\nmessage passing, they restricted the message passing only\nto the incoming edges of the oversampled nodes.\nNeural Computing and Applications (2024) 36:17245–17260 17247\n123\n2.3 UNet\nUNet is an encoder–decoder architecture, which is scalable\nin structure [ 54]. The encoding stage of the UNet consists\nof four downsamples. Symmetrically, its decoding stage is\nalso upsampled four times, restoring the result of the\nencoder to the resolution of the original image. In contrast\nto Fully Convolutional Networks (FCN) [ 55], UNet\nupsamples four times and uses a jump connection in the\nencoder and decoder of the corresponding convolution\nblocks. The jump connection ensures that the ﬁnal recov-\nered feature map incorporates more low-level semantic\nfeatures and features at different scales are well fused,\nallowing for multi-scale prediction. In addition, the four\ntimes upsampling also allows the segmentation map to\nrecover information such as edges more ﬁnely. However,\nUNet also has some shortcomings. For example, UNet ??\n[56, 57] argues that it is inappropriate to directly combine\nthe shallow features from the encoder with the deeper\nfeatures from the decoder in UNet. Direct fusion would\npotentially lead to semantic gaps. Furthermore, UNet 3 ?\n[58] maximises the scope of model information fusion and\ncirculation. Each decoder layer in the UNet 3 ? fuses\nsmall-scale and same-scale feature maps from the encoder\nwith larger-scale feature maps from the decoder, which\ncapture both ﬁne-grained and coarse-grained semantics at\nfull scale.\nMany researchers develop a set of UNet variants by\nimproving and optimising the original UNet. For example,\nResUNet [59] and DenseUNet [ 60] are inspired by Resid-\nual and Dense connections, respectively; each sub-module\nof the UNet is replaced with a form having a Residual\nconnection and a Dense connection. There are variants, e.g.\nMultiResU-Net [61] and R2 UNet [62]. All of these models\nare constructed using multiple convolutional blocks. With\nthe advent of the Transformer, researchers begin to develop\nthe UNet base on the Transformer, such as Swin-UNet\n[63]. While Swin-UNet mitigates the limitations of CNN\nconvolutional operations, it is likely to suffer from training\ninstability due to the use of the Swin-Transformer block.\nSwin-Transformer v2 [ 16] is an improvement on Swin-\nTransformer, which is effective in avoiding training\ninstability and is easier to scale.\nInspired by these research works, we propose a Swinv2-\nImagen model that leverages scene graphs as auxiliary\nmodules to help the model understand the text semantics\nmore comprehensively. In addition, Swinv2-Unet is\napplied to build the diffusion models so that our model is\nbased on the full Transformer implementation. As a result,\nit effectively addresses the limitations of CNN convolution\noperations, theoretically enabling the synthesis of images\nbetter than baselines.\n3 Swinv2-Imagen\nThe overall architecture of the proposed Swinv2-Imagen\nmodel is shown in Fig.1. It takes text descriptions as input and\nuses scene graphs to guide downstream image generation\nmore accurately and efﬁciently. The upstream comprises two\nsub-modules: the text encoder, which maps the text input to a\ntext embedding sequence and the scene graph generator sub-\nmodule. The scene graph generator includes a Scene Graph\nparser and a frozen Graph Neural Network, which aims to\nrepresent objects and relationships in a text with a graph\nstructure. The downstream consists of a set of conditional\ndiffusion models, integrating the intermediate embeddings in\nthe upstream and generating high-ﬁdelity images step by step.\nThe input of the model is a text-picture pair. Firstly, the\ntext is encoded by T5 tokenizers and input to the embedding\nlayer to get the initial text embedding. Next, it goes through\nthe T5 encoder (n-layer T5 Block) to obtain Text\nEmbeddings.\nMeanwhile, the scene graph parser extracts the scene\ngraph from the text, and the frozen GCN (m-layer Graph\nTriple Convolution) obtains the corresponding Object and\nRelation embeddings. Finally, the Conditional embeddings\nare obtained by concatenating the Text embeddings, Object\nembeddings and Relation embeddings in this order. The\nConditional embeddings are used as conditional input for\nsubsequent super-resolution image generation. In the fol-\nlowing subsections, we describe the main components of\nSwinv2-Imagen in detail.\n3.1 Pre-trained frozen text encoders\nIt is widely acknowledged that a robust semantic text enco-\nder is essential for text-to-image synthesis models and plays\na crucial role in analysing the complexity and composition of\ntextual input [6]. Previously, language models were mainly\nbuilt on RNN architectures. However, since the emergence\nof the Transformer, a number of transformer-based pre-\ntrained language models have been developed, such as GPT\n[64–66], BERT [ 67] and T5 [ 7]. The traditional Imagen\nmodel is compared against popular text encoders, BERT,\nCLIP and T5-XXX, by freezing parameters. The existing\nresearch results prove the promising performance of T5-\nXXX in terms of both image-text alignment and image\nﬁdelity [6]. Therefore, we adopt the T5 large language model\nfor text encoding in the proposed model.\n17248 Neural Computing and Applications (2024) 36:17245–17260\n123\nFig. 1 Overall architecture of\nSwinv2-Imagen. The text is\npassed through both a frozen T5\nEncoder and a scene graph. The\nscene graph mines complex\nentity relationships explicitly,\nensuring that the model\nunderstands the text semantics\naccurately\nNeural Computing and Applications (2024) 36:17245–17260 17249\n123\n3.2 Scene graph and frozen graph convolutional\nneural network\nThis sub-module aims to extract entity and relationship\nfeatures from the text to enhance the text understanding of\nthe model. We adopt a Scene Graph parser to represent text\nas a scene graph, followed by a frozen GCN to extract the\nentity and relational embeddings for the image generation\nwith diffusion models. Scene graphs with graph neural\nnetworks [ 15] have been proven to be highly effective in\nextracting object relationships from the text. As shown in\nFig. 2, Swinv2-Imagen constructs a scene graph for the text\nand is followed by a graph neural network to extract the\nentities and relationships from the scene graph. For any\ngiven text description, the corresponding scene graph is\nrepresented as follows: ( O, E), where O ¼\nðo\n1; o2; o3; /C1/C1/C1; onÞ denotes each object in the sentence, i.e.\nsubject and object, and E is a collection of edges of the\nform ðoi; r; ojÞ, where r 2 R, R refers to a collection of\nrelationships. In the end, object and relation embeddings\nare constructed, which are used to assist the T5 model in\nanalysing and understanding the text more\ncomprehensively.\nThe input to the graph convolution is a scene graph,\nhaving each node and edge represented as a vector with\ndimension D\nin, i.e. vi; vr 2 RDin . In the graph convolution\nsub-module, these vectors are adopted to compute output\nvectors with dimension D\nout for each node and edge, i.e.\nv0\ni; v0\nr 2 RDout . Three functions, gs, go and gp are used to\ncalculate the object features vectors and relation vectors of\noutput. They take a triplet as input, i.e. vi; vr; vj\n/C0/C1\n. In the\nscene graph, given an edge vr, the two associated objects,\nvi and vj, are determined. Thus, the output relationship\nvector v0\nr can be simply expressed as:\nv0\nr ¼ gp vi; vr; vj\n/C0/C1\nð1Þ\nIn contrast, the calculation of output object vectors v0\ni,i s\nmore complicated. Generally, an object is associated with\ntwo or more relations. Therefore, the output vector of an\nentity oi is calculated by considering all the vectors directly\nconnected to the object, i.e. vj, and the corresponding\nrelationship vectors, vr. The function gs in Eq. ( 2) is used\nto compute all vectors starting at node oi and function go in\nEq. (3) is used to compute all vectors ending at node oi.\nAfterwards, these vectors are collected into lists Vs\ni and Vo\ni .\nVs\ni ¼f gs vi; vr; vj\n/C0/C1\n: oi; r; oj\n/C0/C1\n2 Egð 2Þ\nVo\ni ¼f go vj; vr; vi\n/C0/C1\n: oj; r; oi\n/C0/C1\n2 Egð 3Þ\nThen, the output vector v\n0\ni for the entity oi is expressed as\nfollows:\nv0\ni ¼ hV s\ni [ Vo\ni\n/C0/C1\n; ð4Þ\nwhere h denotes a function that pools all vectors in lists Vs\ni\nand Vo\ni to a single output vector [ 15].\n3.3 Image generator\nThe image generator is composed of three diffusion models\nlocated downstream. In the diffusion model, a hidden\nvariable z is obtained by adding noise to the image for\nT times. After forward and backward diffusion, a basic 64 *\n64 image can be learned. The basic image is input to the\nﬁrst Swinv2-Unet to generate a 256 * 256 image. Finally,\nthe image goes to the second Swinv2-Unet super-resolution\ngeneration, producing a 1024 * 1024 high-deﬁnition image.\nThe diffusion model can be described as an Encoder–\nDecoder architecture. It ﬁrst adds Gaussian noise ð/C15 Þ to the\noriginal image ðx\n0 /C24 qðx0ÞÞ in an iterative manner, the\nnumber of iterations being T (T is timestep, usually\nT ¼ 1000). When T tends to inﬁnity, i.e. ðT !1 Þ, the\nimage is nearly a random Gaussian noise distribution xT .\nFig. 2 Process of Object and Relation embeddings extraction. The\ninput to the model is a sentence, which is ﬁrst parsed by the scene\ngraph parser into a graph structure (scene graph). Each node\nrepresents an object in the text and each edge represents a relationship\nbetween objects. Finally, all the nodes and edges are parsed by the\ngraph neural network into an object embedding and a relation\nembedding, respectively\n17250 Neural Computing and Applications (2024) 36:17245–17260\n123\nThis process is called forward diffusion and can be thought\nof as an encoder. The model then learns how to recover the\nnoise distribution ðxT Þ to the original image ðx0 /C24 qðx0ÞÞ by\ngradually removing the noise from xT . The process is called\nreverse diffusion and can be thought of as a decoder\n[28, 29].\nIn the forward diffusion, the result at timestep t is\nmainly related to the outcome at moment t /C0 1 and the\nadded noise /C15 t, i.e.\nxt ¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1 /C0 bt\np\nxt/C0 1 þ\nﬃﬃﬃﬃ\nbt\np\n/C15 t qðxtjxt/C0 1Þ/C24 N ð\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1 /C0 bt\np\n; btÞ;\nð5Þ\nqðx1:T jx0Þ¼\nYT\ni¼1\nqðxtjxt/C0 1Þ ð6Þ\nwhere bt I prefer to understand as a linear weight value. At\ndifferent timestep, xt/C0 1 and /C15 t have different effects on the\nresult. When T is small, e.g. t ¼ 1, xt/C0 1 has a greater impact\non the result and adds little noise. Conversely, when t is\nlarge, e.g. t ¼ 900, more noise is added and the contribu-\ntion to the result is larger than xt/C0 1.\nThe distribution of the noise added at each timestep in\nthe forward process is identical, i.e. /C15 1;/C15 2; :::::: /C24 Nð0; IÞ.\nThus, we can compute the result at any timestep xt directly\nfrom x0, i.e.\nxt ¼ ﬃﬃﬃﬃ/C22at\np xo þ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1 /C0 /C22at\np\n/C15 t ð7Þ\nwhere a ¼ 1 /C0 b, /C22at ¼ Qt\ni¼1 ai.\nReverse diffusion is an image generation process. The\nGaussian noise xT /C24 N ð0; IÞ will be taken as input to infer\nand reconstruct the true sample by sampling from distri-\nbution qðx\nt/C0 1jxtÞ, i.e.\nxt/C0 1 ¼ 1ﬃﬃﬃﬃat\np xt /C0 1 /C0 atﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1 /C0 /C22at\np\n/C18/C19\nfhðxt; tÞ ð8Þ\nwhere fhðxt; tÞ is a function used to predict the noise /C15\nadded in the forward diffusion. This is mainly because it is\ndifﬁcult to infer the true distribution of the image directly\nfrom the random noise x\nT . In other words, the objective of\nthe diffusion generation model is to evaluate the difference\nbetween the predicted noise data and the true added noise\ndata, i.e.\npðx\nt/C0 1jxtÞ¼j j /C15 /C0 fhðxt; tÞjj: ð9Þ\nIn contrast to Imagen, we focus on improving super-reso-\nlution diffusion models. We introduce a new UNet variant\nto our super-resolution diffusion model, called Swinv2-\nUNet. The Swin Transformer Block is replaced with the\nSwin Transformer v2 Block based on the original Swin-\nUnet [ 63], the complete structure of which is shown in\nFig. 3.\nA distinctive feature of Swinv2-Unet compared to Swin-\nUnet is the replacement of the dotðK; QÞ operation with\ncosine normalisation [ 68] in the attention part, which\nmakes the attention output more stable. Given two vectors,\nQ and K, the cosine normalisation could be expressed as\nfollows:\nCosineðQ; KÞ¼\nP\niðqikiÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP\niðqiÞ2\nq ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ P\niðkiÞ2\nq : ð10Þ\nThe DBlock and UBlock of Swinv2-UNet consist of the\nSwin Transformer v2 block, which comprises LayerNorm\n(LN) layers, multi-headed self-attention modules, Residual\nconnections and a 2-layer MLP with GELU nonlinearity.\nThe Swin Transformer v2 block could be represented as\nfollows:\n^z\nlþ1 ¼ LNðAttnðzlÞÞ þ zl ð11Þ\nzlþ1 ¼ MLPðLNð^zlþ1ÞÞ þ ^zlþ1; ð12Þ\nwhere zl and zlþ1 denote the input and output of the\nTransformer v2 block, respectively. ^zlþ1 is an intermediate\nvariable. ? denotes the residual connection or skip\nconnection.\nFig. 3 UNet architecture of the super-resolution sub-module. The\narchitecture includes an encoder(downsampling), bottleneck, and\ndecoder(upsampling). Skip connections are used between the encoder\nand decoder. All components are built based on the Swin Transformer\nv2 block\nNeural Computing and Applications (2024) 36:17245–17260 17251\n123\nThe attention of Swinv2 is expressed as follows:\nAttnðQ; K; VÞ¼ SoftMax CosineðQ; KÞ\ns þ B\n/C18/C19\n; ð13Þ\nwhere Q, K, and V denote the matrix of query, key and\nvalue, respectively. Cosine() refers to a function that cal-\nculates the scaled cosine similarity of Q and K. s denotes a\nlearnable scalar, usually greater than 0.01. B is a matrix of\nrelative position bias.\nFigure 4 illustrates the network structure of the Swinv2-\nUnet DBlock, which is the basic component of the down-\nsampling path under the encoding–decoding structure of\nUNet. Firstly, the DBlock combines the pooled text\nembeddings, object embeddings and relation embeddings\ninto a conditional embedding input to the cross-attention\nlayer. Next, it is followed by the Swinv2-Transformer v2\nblocks for (num_block-1) times feature extraction.\nFigure 5 shows the network structure of the Swinv2-\nUnet UBlock, which is the basic component of the\nupsampling path on the UNet encoder–decoder. The\ninputs to the UBlock include the output of the previous\nUBlock layer and the corresponding DBlock. The\nDBlock and UBlock are connected using skip connec-\ntions [ 57]. Subsequently, the conditional embedding\ninputs are also introduced to the cross-attention layer.\nSimilar to the DBlock, this layer is followed by the\nSwinv2-Transformer v2 blocks for (num_block-1) times\nfeature extraction.\nThe encoder is presented as a stacking of DBlocks and\nPatch Merging. In the encoder, images are fed into ﬁve\nconsecutive DBlocks for learning, where the feature\ndimension and resolution are maintained. Meanwhile,\nPatch Merging performs Token Merging and increases the\nfeature dimension to four times the original dimension.\nNext, we apply a linear layer to standardise the feature\ndimension to twice the original dimension. The process is\nrepeated four times in the encoder.\nSimilar to UNet, skip connections are used to integrate\nthe multi-scale features of the encoder with the upsampled\nfeatures. We connect shallow and deep features to min-\nimise the loss of spatial information due to downsampling.\nThe next layer is a linear layer where the dimensionality of\nthe connected features is kept the same dimensionality as\nthat of the upsampled features.\nThe decoder is a symmetric decoder corresponding to\nthe encoder. For this reason, unlike the Patch Merging used\nin the encoder, we use Patch Expanding in the decoder to\nupsample the extracted features. The Patch Expanding\nreshapes the feature maps of adjacent dimensions into a\nhigher resolution feature map (2 /C2 upsampling) and\naccordingly reduces the number of feature dimensions to\nhalf the original dimensionality.\n4 Experiments\nIn this section, we perform extensive experiments to\nevaluate the proposed Swinv2-Imagen model by using the\nMSCOCO, CUB and Multi-modalCelebA-HQ (MM Cel-\nebA-HQ) datasets. Firstly, a brief description of the data-\nsets is given. Secondly, we compare the performance of the\nSwinv2-Imagen model with state-of-the-art generative\nmodels. Finally, we conduct ablation experiments to\ncompare the contributions of each module.\n4.1 Setup\n4.1.1 Datasets\nThe Microsoft Common Objects in Context 2014 (MS\nCOCO-2014) [ 69], the Caltech-UCSD Birds-200-2011\n(CUB-200-2011) [ 70] and MM CelebA-HQ [ 20] datasets\nare utilised in this research. Three datasets cover both\nsimple (CUB) and complex (MSCOCO) datasets. The use\nof the MM CelebA-HQ dataset is mainly because most\nFig. 4 Swinv2-Unet DBlock\n17252 Neural Computing and Applications (2024) 36:17245–17260\n123\ngenerative models such as CogView and Craiyon, produce\ndistorted and less realistic faces.\n• MSCOCO1 was released in 2014. It is a collection of\n164K images, which have been partitioned into the\ntraining set (82K), validation set (41K) and testing set\n(41K). The dataset is complex because most of the\nimages possess at least two objects.\n• CUB2 contains 12K bird images of 200 subcategories,\n6K for training and 6K for testing. It is a simple dataset,\nhaving only one object per image.\n• MM CelebA-HQ3 is a large-scale face image dataset. It\nis a collection of 30K high-resolution face images. The\ndataset is used widely to train and evaluate algorithms\nfor text-image generation and text-guided image\nmanipulation.\n4.1.2 Evaluation metrics\nWe adopt Fre ´chet Inception Distance (FID) [ 71] and\nInception Score (IS) [ 8] as evaluation metrics. Both are\nacknowledged as standard metrics for evaluating the image\ngeneration model. Speciﬁcally, IS examines both the clar-\nity and diversity of the resulting images. The higher the IS,\nthe better the quality of the generated images. FID calcu-\nlates the difference between the generated image and the\noriginal image. The smaller the difference, the better the\ngenerated image is.\n4.1.3 Baselines\n• PCCM-GAN [ 72] (Photographic Text-to-Image Gen-\neration with Pyramid Contrastive Consistency Model)\nis a typical multi-stage generative model. Its main\ninnovations include the introduction of stack attention\nand the lateral connection of the PCCM. The two\nmodules enhance the generative model to simultane-\nously extract semantic information from both global\nand local aspects, ensuring that the generated images\nare semantically consistent.\n• DM-GAN [ 17] (Dynamic Memory Generative Adver-\nsarial Networks for Text-to-Image Synthesis) is also a\nmulti-stage generative model. It uses a memory module\nand a gate mechanism in the image reﬁnement process.\nThe aim is to re-extract important information from the\nimage as an aid when the generated image is not as\ngood as expected.\n• SDGAN [ 73] (Semantics Disentangling for Text-to-\nImage Generation) consists of two modules, i.e.\nSiamese and semantic conditioned batch normalisation,\nto extract high-level and low-level semantic features\nrespectively.\n• CogView [74] is based on the Transformer architecture.\nIts input is a text-image pair. The text and image\nfeatures are combined and passed to the GPT language\nmodel for autoregressive training.\n• GLIDE [ 25] is a large-scale image generation model\nbased on diffusion models with 3.5 billion model\nparameters.\n• DALL-E 2 [5] is also based on diffusion models. One of\nits highlights is the use of a priori model built on the\ndiffusion models. Its inputs are also text and corre-\nsponding images. The text is ﬁrst passed through the\npriori model and a corresponding image vector is\ngenerated. The image is passed through the CLIP\nmodule which also generates an image vector to\nsupervise the result of the priori model.\n• LAFITE [ 75] is a variant of generative adversarial\nnetworks. It leverages the CLIP model to extract\nfeatures from images and text, ensuring text-image\nconsistency.\n• Imagen [6] is a text-to-image synthesising model based\non the diffusion model. It passes text through a large\npre-trained T5 language model and generates high-\nﬁdelity images through cascading diffusion model\nblocks.\n4.1.4 Training parameters\nWe apply an Imagen-like training strategy, i.e. training the\nbase model and then the super-resolution model twice. The\nAdam optimiser is adopted, having a learning rate of 1e-4.\nWe give 10,000 linear warm-up steps with a batch size of 8\nand training epochs of 1000. The loss function is Mean\nSquared Error (MSE), formulated as follows.\nFig. 5 Swinv2-Unet UBlcok\n1 https://cocodataset.org/.\n2 https://deepai.org/dataset/cub-200-2011.\n3 https://github.com/weihaox/Multi-Modal-CelebA-HQ-Dataset.\nNeural Computing and Applications (2024) 36:17245–17260 17253\n123\nMSEðI; KÞ¼ 1\nM /C2 N\nXM/C0 1\ni¼0\nXN/C0 1\nj¼0\n½Iði; jÞ/C0 Kði; jÞ/C1382; ð14Þ\nwhere M and N denote the total number of pixels in the real\nimage I and the generated image K, respectively. A smaller\nMSE implies that the generated image is closer to the real\nimage.\n4.2 Experimental results\nIn this subsection, we evaluate the proposed model by\ncomparing it against a few state-of-the-art generative\nmodels.\n4.2.1 Performance evaluation\nTable 1 demonstrates the results of the quantitative com-\nparison. The proposed model is compared against 10\npopular generative models, including GAN and diffusion\nmodels. It is evident that the proposed Swinv2-Imagen\nmodel outperforms the baselines on all three datasets.\nParticularly, on the MSCOCO dataset, Swinv2-Imagen\nsigniﬁcantly outperforms the GAN-based generative model\nand slightly surpasses the Imagen, achieving an FID of\n7.21. It can be seen from Fig. 6 that our model has\nachieved the best result in terms of FID. However, our\nmodel, in IS metric, is lower than SDGAN and LAFITE.\nOne possible reason for this result is that IS is not very\nrobust in evaluating classes that differ signiﬁcantly from\nthe ImageNet [ 78] and is more sensitive to data perturba-\ntions. This is also the main reason why this metric is not\nwidely used in most diffusion-based generation methods,\nsuch as DALLE2 and Imagen.\n4.2.2 Qualitative analysis\nFigure 7 shows examples of images generated by our\nproposed model on MSCOCO, CUB and MM CelebA-HQ.\nIt can be seen that our model understands the text very\nwell. For example, given the text input, ‘Food cooks in a\npot on a stove in a kitchen’, the resulting picture not only\ncontains the food, the stove and the pot, but also places\nthese objects to the exact location. More importantly, based\non the word ‘kitchen’, the model also generates other\ncommon kitchen objects, such as spoons and storage\nshelves. This shows that our model understands the text\naccurately and comprehensively.\nFigure 8 illustrates the qualitative comparison of the\nproposed model and the GAN-based, diffusion-based\ngenerative models, i.e. DM-GAN [17], DF-GAN [79], VQ-\nDiffusion [ 80]. Compared to diffusion-based models, the\nGAN-based models lose many detailed features in the\ngenerated results. For example, the bird’s eyes are very\nblurred in the third image in the ﬁrst row and the second\nimage in the second row. One possible reason for this result\nis that the diffusion model improves the generalisation\nability of the model by iterating over the image several\ntimes, with each iteration perturbing the image slightly (by\nadding randomly noisy data to the image). GANs, on the\nother hand, usually rely on continuous optimisation over\nlarge amounts of data in order to generate high-quality\nimages. Compared to VQ-Diffusion, which is a diffusion-\nbased model, our results are more realistic and contain\nmore ﬁne-grained features. Particularly, the blue birds in\nthe third column generated by our method are better than\nthat generated by VQ-Diffusion. One possible reason for\nthis result is that VQ-diffusion is a typical two-stage gen-\nerative model [ 80]. First is the vector quantisation stage,\nTable 1 Experimental results of\nvaried models for Text-To-\nImage synthesis\nModel MSCOCO CUB MM CelebA-HQ\nFID # IS \" FID # IS \" FID #\nPCCM-GAN [72] 33.59 26.52 22.15 4.65 14.52\nDM-GAN [17] 32.64 30.49 16.09 4.75 131.05\nSDGAN [73] 29.35 35.69 29.3 4.64 15.1\nDALL-E [76] 27.5 17.9 56.1 2.65 12.54\nCogView [74] 27.1 18.2 N/A N/A N/A\nGLIDE [25] 12.24 30.47 N/A N/A 9.69\nDALL-E 2 [ 5] 10.39 N/A N/A N/A N/A\nLAFITE [75] 8.12 32.34 10.48 5.97 12.54\nMake-A-Scene [77] 7.55 N/A 22.5 N/A N/A\nImagen [6] 7.27 N/A N/A N/A N/A\nSwinv2-Imagen 7.21 31 .46 9 .78 8 .44 10 .31\nThe bold values indicate the best results\nSymbols \" and # indicate the higher the best and the lower the best, respectively. N/A means that the\nindicator is not used in the article\n17254 Neural Computing and Applications (2024) 36:17245–17260\n123\nwhere the original image is represented as a set of high-\ndimensional vectors and mapped into a discretised space\nusing a vector quantisation model. Second is the diffusion\ncomputation stage, where a discretised sequence of vectors\nis used as the initial state, which is iterated over several\ntimes using a diffusion model. Finally, the resulting dis-\ncretised vector sequence is transformed into an image. The\nvectorisation results of the ﬁrst stage will directly affect the\nquality of the generation results. By comparison, our pro-\nposed model is designed as an end-to-end architecture that\noptimises the entire generation process holistically. Our\nmodel eliminates the need for intermediate stages, facili-\ntating better optimisation and faster convergence. In addi-\ntion, our model also outperforms other generation models\nin terms of text-image alignment. The text description of\nthe ﬁrst column requires the bird’s breast to be white, but\nthis feature seems to be grey in the results of other models,\nespecially DM-GAN. In summary, by comparing with\nother GAN-based and diffusion-based generation models,\nit can be seen that our model synthesises ﬁne-grained and\ndetailed images on CUB.\nFigure 9 presents the qualitative comparison between\nour model and LAFITE [ 75] on MSCOCO. Intuitively, our\nresults are more colourful and saturated. For example, in\nthe ﬁrst and fourth columns, our bus and city street include\nmore colours and the images are brighter. Furthermore, our\nmodel is also better for text understanding. In the third\ncolumn, the room should include two colours, white and\nbeige, however, in the LAFITE result, there are just white\nwalls and a white cupboard. There is not any trace of the\nbeige features. In contrast, our generated room contains the\ntwo colours required by the text, and the overall layout is\nmore realistic. Finally, our model is also better regarding\nimage quality. The tops of the bus and room generated by\nLAFITE are distorted and the results are generally blurred.\nOur model has a signiﬁcant advantage over LAFITE in\ngenerating objects such as buildings, buses, trees, etc.\nAlthough the two models are very close in terms of FID\nand IS in the quantitative analysis in Table 1, our model is\nsuperior in terms of the quality of the generated images.\n4.3 Ablation study\nIn order to improve the performance of the generation\nmodels, we introduce two new modules to Imagen, i.e.\nscene graph and Swinv2-Unet. These are the main inno-\nvations of the article. In this subsection, two ablation\nexperiments are conducted on MSCOCO to investigate the\ncontributions of the scene graph module and Swinv2-Unet,\nrespectively. The choice to experiment on MSCOCO is\nbased on two considerations. Firstly, each image in\nMSCOCO contains multiple objects, which is more com-\nplex than CUB dataset. Theoretically, it allows a better\nevaluation on the effect of each module. Secondly, the\nmain baseline we referenced, Imagen, is only experimented\non MSCOCO. The aim of Experiment 1 is to evaluate the\ncontribution of the scene graph module. We add only the\nscene graph, and the diffusion model is still built using\nEfﬁcient-Unet, which is called Imagen_sg. Experiment 2 is\ndesigned to evaluate the performance of the Swinv2-Unet.\nWe constructed a new diffusion model using our improved\nSwinv2-Unet and replace Imagen’s super-resolution dif-\nfusion models with it, which is called Swinv2-Imagen_su.\nThe result of Experiment 1 supports our conjecture that\nmerely using a T5 encoder does not sufﬁciently learn the\nsemantic information of the text, as mentioned in the\nintroduction. Experiment 2 shows that the diffusion model\nconstructed with the Transformer outperforms the CNN-\nFig. 6 FID and IS on\nMSCOCO. Smaller FID is\nbetter, larger IS is better. 0\nmeans that the model does not\nuse this evaluation metric\nNeural Computing and Applications (2024) 36:17245–17260 17255\n123\nconstructed diffusion model in the image generation task. It\nalso can be seen from Table 2 that the FIDs of the Ima-\ngen_sg and Swinv2-Image_su are very close. This intu-\nitively reveals that the two submodules almost contribute\nequally to the FID.\n5 Conclusion and future work\nIn this paper, we propose a novel text-to-image synthesis\nmodel based on Imagen, called the Swinv2-Imagen, which\nintegrates the Transformer and Scene Graph. The improved\nsliding window-based hierarchical visual Transformer\nFig. 7 Generated examples by proposed model on COCO, CUB and\nMM CelebA-HQ. The resulting images generate not only the objects\nrequested in the sentence but also additional objects based on special\nwords(e.g. kitchen). For example, in the ﬁrst image, the resulting\nimage includes food, a pot, and a stove (required in the text) and some\nspoons and rice cookers (common kitchen items)\n17256 Neural Computing and Applications (2024) 36:17245–17260\n123\n(Swin Transformer v2) avoids the local view of CNN\nconvolution operations. It improves the efﬁciency and\neffectiveness of the Transformer applied to image genera-\ntion. In addition, we introduce a Scene Graph in the text\nprocessing stage. Feature vectors of entities and relation-\nships are extracted from the Scene Graph and incorporated\ninto the diffusion model. These additional feature vectors\nimprove the quality of generated images. Swinv2-Imagen\nproduces 1024 /C2 1024 samples with unprecedented ﬁdelity\nwith these novel components.\nFurthermore, it has also recently been noted that\nautoregressive models can produce diverse and high-qual-\nity images from text. Thus, we plan to consider combining\nautoregressive and diffusion models for image generation\nand determine the best opportunities to combine their\nstrengths.\nFig. 8 Comparison with GAN-based and diffusion models on CUB-200 dataset. For each method, we present three captions and the\ncorresponding generated images. Our resulting images are more detailed in colour and higher in quality than the popular GAN models\nNeural Computing and Applications (2024) 36:17245–17260 17257\n123\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions.\nData availability The data that support the ﬁndings of this study are\nopenly available in [Microsoft COCO] at ( https://cocodataset.org/.);\n[CUB-200-2011] at ( https://deepai.org/dataset/cub-200-2011.) and\n[Multi-Modal-CelebA-HQ] at ( https://github.com/weihaox/Multi-\nModal-CelebA-HQ-Dataset.).\nDeclarations\nConflict of interest The authors declare that they have no known\ncompeting financial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Kim D, Joo D, Kim J (2020) Tivgan: text to image to video\ngeneration with step-by-step evolutionary generator. IEEE\nAccess 8:153113–153122\n2. Li R, Wang N, Feng F, Zhang G, Wang X (2020) Exploring\nglobal and local linguistic representations for text-to-image syn-\nthesis. IEEE Trans Multimed 22(12):3075–3087\n3. Mathesul S, Bhutkar G, Rambhad A (2021) Attngan: realistic\ntext-to-image synthesis with attentional generative adversarial\nnetworks. In: IFIP conference on human-computer interaction,\npp 397–403. Springer\n4. Park DH, Azadi S, Liu X, Darrell T, Rohrbach A (2021)\nBenchmark for compositional text-to-image synthesis. In: Neur-\nIPS datasets and benchmarks\n5. Ramesh A, Dhariwal P, Nichol A, Chu C, Chen M (2022)\nHierarchical text-conditional image generation with clip latents.\nArXiv arXiv:2204.06125\n6. Saharia C, Chan W, Saxena S, Li L, Whang J, Denton EL,\nGhasemipour SKS, Ayan BK, Mahdavi SS, Lopes RG, Salimans\nT, Ho J, Fleet DJ, Norouzi M (2022) Photorealistic text-to-image\ndiffusion models with deep language understanding. ArXiv\narXiv:2205.11487\n7. Raffel C, Shazeer NM, Roberts A, Lee K, Narang S, Matena M,\nZhou Y, Li W, Liu PJ (2020) Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer. ArXiv arXiv:\n1910.10683\n8. Li W, Zhang P, Zhang L, Huang Q, He X, Lyu S, Gao J (2019)\nObject-driven text-to-image synthesis via adversarial training. In:\n2019 IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), pp 12166–12174\n9. Ganar AN, Gode C, Jambhulkar SM (2014) Enhancement of\nimage retrieval by using colour, texture and shape features. In:\n2014 International Conference on Electronic Systems, Signal\nProcessing and Computing Technologies, pp. 251–255. IEEE\n10. Kauderer-Abrams E (2017) Quantifying translation-invariance in\nconvolutional neural networks. arXiv preprint arXiv:1801.01450\nFig. 9 Comparison with LAFITE on MSCOCO dataset\nTable 2 Ablation study of Swinv2-Imagen model\nModel Scene graph Swinv2-UNet FID #\nImagen 7.27\nImagen_sg YES 7.24\nSwinv2-Imagen_su YES 7.23\nSwinv2-Imagen YES YES 7.21\n17258 Neural Computing and Applications (2024) 36:17245–17260\n123\n11. Chidester B, Do MN, Ma J (2018) Rotation equivariance and\ninvariance in convolutional neural networks. arXiv preprint\narXiv:1805.12301\n12. Zhao Z-Q, Zheng P, Xu S-T, Wu X (2019) Object detection with\ndeep learning: a review. IEEE Trans Neural Netw Learn Syst\n30(11):3212–3232\n13. Li J, Yan Y, Liao S, Yang X, Shao L (2021) Local-to-global self-\nattention in vision transformers. arXiv preprint arXiv:2107.04735\n14. Liang C, Wang W, Zhou T, Miao J, Luo Y, Yang Y (2022) Local-\nglobal context aware transformer for language-guided video\nsegmentation. arXiv preprint arXiv:2203.09773\n15. Johnson J, Gupta A, Fei-Fei L (2018) Image generation from\nscene graphs. 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 1219–1228\n16. Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang\nZ, Dong L, Wei F, Guo B (2022) Swin transformer v2: Scaling up\ncapacity and resolution. In: 2022 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR), 11999–12009\n17. Zhu M, Pan P, Chen W, Yang Y (2019) Dm-gan: dynamic\nmemory generative adversarial networks for text-to-image syn-\nthesis. In: 2019 IEEE/CVF conference on computer vision and\npattern recognition (CVPR), 5795–5803\n18. Zhu B, Ngo C-W (2020) Cookgan: Causality based text-to-image\nsynthesis. 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 5518–5526\n19. Zhang H, Xu T, Li H, Zhang S, Wang X, Huang X, Metaxas DN\n(2019) Stackgan ??: realistic image synthesis with stacked\ngenerative adversarial networks. IEEE Trans Pattern Anal Mach\nIntell 41:1947–1962\n20. Xia W, Yang Y, Xue J, Wu B (2021) Tedigan: text-guided\ndiverse face image generation and manipulation. In: 2021 IEEE/\nCVF conference on computer vision and pattern recognition\n(CVPR), pp 2256–2265\n21. Crowson K, Biderman SR, Kornis D, Stander D, Hallahan E,\nCastricato L, Raff E (2022) Vqgan-clip: open domain image\ngeneration and editing with natural language guidance. ArXiv\narXiv:2204.08583\n22. Cheng J, Wu F, Tian Y, Wang L, Tao D (2020) Rifegan: rich\nfeature generation for text-to-image synthesis from prior knowl-\nedge. In: IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), pp 10908–10917\n23. Ho J, Jain A, Abbeel P (2020) Denoising diffusion probabilistic\nmodels. ArXiv arXiv:2006.11239\n24. Ho J, Saharia C, Chan W, Fleet DJ, Norouzi M, Salimans T\n(2022) Cascaded diffusion models for high ﬁdelity image gen-\neration. J Mach Learn Res 23:47–14733\n25. Nichol A, Dhariwal P, Ramesh A, Shyam P, Mishkin P, McGrew\nB, Sutskever I, Chen M (2022) Glide: towards photorealistic\nimage generation and editing with text-guided diffusion models.\nIn: ICML\n26. Rombach R, Blattmann A, Lorenz D, Esser P, Ommer B (2022)\nHigh-resolution image synthesis with latent diffusion models. In:\n2022 IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), pp 10674–10685\n27. Song J, Meng C, Ermon S (2021) Denoising diffusion implicit\nmodels. ArXiv arXiv:2010.02502\n28. Dhariwal P, Nichol A (2021) Diffusion models beat gans on\nimage synthesis. ArXiv arXiv:2105.05233\n29. Yang L, Zhang Z, Hong S, Xu R, Zhao Y, Shao Y, Zhang W,\nYang M-H, Cui B (2022) Diffusion models: a comprehensive\nsurvey of methods and applications. ArXiv arXiv:2209.00796\n30. Cao HK, Tan C, Gao Z, Chen G, Heng P-A, Li SZ (2022) A\nsurvey on generative diffusion model. ArXiv arXiv:2209.02646\n31. Mittal G, Agrawal S, Agarwal A, Mehta S, Marwah T (2019)\nInteractive image generation using scene graphs. arXiv preprint\narXiv:1905.03743\n32. Zhu G, Zhang L, Jiang Y, Dang Y, Hou H, Shen P, Feng M, Zhao\nX, Miao Q, Shah SAA (2022) Bennamoun: scene graph genera-\ntion: a comprehensive survey. ArXiv arXiv:2201.00443\n33. Chang X, Ren P, Xu P, Li Z, Chen X, Hauptmann AG (2021) A\ncomprehensive survey of scene graphs: generation and applica-\ntion. IEEE Trans Pattern Anal Mach Intell 45:1–26\n34. Johnson J, Krishna R, Stark M, Li L-J, Shamma DA, Bernstein\nMS, Fei-Fei L (2015) Image retrieval using scene graphs. In:\n2015 IEEE conference on computer vision and pattern recogni-\ntion (CVPR), pp 3668–3678\n35. Schuster S, Krishna R, Chang AX, Fei-Fei L, Manning CD\n(2015) Generating semantically precise scene graphs from textual\ndescriptions for improved image retrieval. In: VL@EMNLP\n36. Taghanaki SA, Abhishek K, Cohen JP, Cohen-Adad J, Hamarneh\nG (2020) Deep semantic segmentation of natural and medical\nimages: a review. Artif Intell Rev 54:137–178\n37. Jaritz M, Vu T-H, de Charette R, Wirbel E ´ ,P e´rez P (2020)\nxmuda: cross-modal unsupervised domain adaptation for 3d\nsemantic segmentation. In: 2020 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR), 12602–12611\n38. Li L, Gan Z, Cheng Y, Liu J (2019) Relation-aware graph\nattention network for visual question answering. In: 2019 IEEE/\nCVF international conference on computer vision (ICCV),\npp 10312–10321\n39. Gao L, Wang B, Wang W (2018) Image captioning with scene-\ngraph based semantic concepts. In: Proceedings of the 2018 10th\ninternational conference on machine learning and computing\n40. Yang X, Tang K, Zhang H Cai J (2019) Auto-encoding scene\ngraphs for image captioning. In: 2019 IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR),\npp 10677–10686\n41. Zhong Y, Wang L, Chen J, Yu D, Li Y (2020) Comprehensive\nimage captioning via scene graph decomposition. ArXiv\narXiv:\n2007.11731\n42. Gu J, Joty SR, Cai J, Zhao H, Yang X, Wang G (2019) Unpaired\nimage captioning via scene graph alignments. In: 2019 IEEE/\nCVF international conference on computer vision (ICCV),\n10322–10331\n43. Li Y, Ma T, Bai Y, Duan N, Wei S, Wang X (2019) Pastegan: a\nsemi-parametric method to generate image from scene graph.\nAdv Neural Inf Process Syst 32\n44. Zhao B, Meng L, Yin W, Sigal L (2019) Image generation from\nlayout. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp 8584–8593\n45. Li Y, Yang X, Xu C (2022) Dynamic scene graph generation via\nanticipatory pre-training. In: Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npp 13874–13883\n46. Hamilton WL (2020) Graph representation learning. Synthesis\nlectures on artiﬁcial intelligence and machine learning\n47. Grover A, Leskovec J (2016) node2vec: Scalable feature learning\nfor networks. In: Proceedings of the 22nd ACM SIGKDD inter-\nnational conference on knowledge discovery and data mining\n48. Mikolov T, Chen K, Corrado GS, Dean J (2013) Efﬁcient esti-\nmation of word representations in vector space. In: ICLR\n49. Chen F, Wang YC, Wang B, Kuo C-CJ (2020) Graph represen-\ntation learning: a survey. APSIPA Trans Signal Inf Process 9\n50. Hamilton WL, Ying R, Leskovec J (2017) Representation\nlearning on graphs: methods and applications. ArXiv arXiv:1709.\n05584\n51. Chen J, Ye G, Zhao Y, Liu S, Deng L, Chen X, Zhou R, Zheng K\n(2022) Efﬁcient join order selection learning with graph-based\nrepresentation. In: Proceedings of the 28th ACM SIGKDD con-\nference on knowledge discovery and data mining, pp 97–107\nNeural Computing and Applications (2024) 36:17245–17260 17259\n123\n52. Park J, Song J, Yang E (2021) Graphens: Neighbor-aware ego\nnetwork synthesis for class-imbalanced node classiﬁcation. In:\nInternational conference on learning representations\n53. Ghorbani M, Kazi A, Baghshah MS, Rabiee HR, Navab N (2022)\nRa-gcn: graph convolutional network for disease prediction\nproblems with imbalanced data. Med Image Anal 75:102272\n54. Ronneberger O, Fischer P, Brox T (2015) U-net: convolutional\nnetworks for biomedical image segmentation. ArXiv arXiv:1505.\n04597\n55. Shelhamer E, Long J, Darrell T (2015) Fully convolutional net-\nworks for semantic segmentation. 2015 IEEE conference on\ncomputer vision and pattern recognition (CVPR), pp 3431–3440\n56. Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J (2018) Unet ??:\na nested u-net architecture for medical image segmentation. Deep\nlearning in medical image analysis and multimodal learning for\nclinical decision support : 4th international workshop, DLMIA\n2018, and 8th International workshop, ML-CDS 2018, held in\nconjunction with MICCAI 2018, Granada, Spain, S... 11045,\n3–11\n57. Zhou Z, Siddiquee MMR, Tajbakhsh N, Liang J (2020) Unet ??:\nredesigning skip connections to exploit multiscale features in\nimage segmentation. IEEE Trans Med Imaging 39:1856–1867\n58. Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y, Han X,\nChen Y-W, Wu J (2020) Unet 3?: A full-scale connected unet for\nmedical image segmentation. ICASSP 2020 - 2020 IEEE inter-\nnational conference on acoustics, speech and signal processing\n(ICASSP), pp 1055–1059\n59. Zhang Z, Liu Q, Wang Y (2018) Road extraction by deep residual\nu-net. IEEE Geosci Remote Sens Lett 15:749–753\n60. Cai S, Tian Y, Lui H, Zeng H, Wu Y, Chen G (2020) Dense-unet:\na novel multiphoton in vivo cellular image segmentation model\nbased on a convolutional neural network. Quant Imaging Med\nSurg 10(6):1275–1285\n61. Ibtehaz N, Rahman MS (2020) Multiresunet: rethinking the u-net\narchitecture for multimodal biomedical image segmentation.\nNeural Netw Off J Int Neural Netw Soc 121:74–87\n62. Alom MZ, Hasan M, Yakopcic C, Taha TM, Asari VK (2018)\nRecurrent residual convolutional neural network based on u-net\n(r2u-net) for medical image segmentation. ArXiv arXiv:1802.\n06955\n63. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, Wang M\n(2021) Swin-unet: Unet-like pure transformer for medical image\nsegmentation. ArXiv arXiv:2105.05537\n64. Radford A, Narasimhan K (2018) Improving language under-\nstanding by generative pre-training\n65. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I\n(2019) Language models are unsupervised multitask learners\n66. Brock A, Donahue J, Simonyan K (2019) Large scale gan training\nfor high ﬁdelity natural image synthesis. ArXiv arXiv:1809.\n11096\n67. Devlin J, Chang M-W, Lee K, Toutanova K (2019) Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding. In: NAACL\n68. Luo C, Zhan J, Wang L, Yang Q (2018) Cosine normalization:\nUsing cosine similarity instead of dot product in neural networks.\nArXiv arXiv:1702.05870\n69. Cho K, van Merrienboer B, C ¸ aglar Gu¨lc¸ehre Bahdanau D, Bou-\ngares F, Schwenk H, Bengio Y (2014) Learning phrase repre-\nsentations using rnn encoder–decoder for statistical machine\ntranslation. In: EMNLP\n70. Ho J (2022) Classiﬁer-free diffusion guidance. ArXiv arXiv:\n2207.12598\n71. Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S\n(2017) Gans trained by a two time-scale update rule converge to a\nlocal nash equilibrium. In: NIPS\n72. Qi Z, Sun J, Qian J, Xu J, Zhan S (2021) Pccm-gan: photographic\ntext-to-image generation with pyramid contrastive consistency\nmodel. Neurocomputing 449:330–341\n73. Zhang H, Koh JY, Baldridge J, Lee H, Yang Y (2021) Cross-\nmodal contrastive learning for text-to-image generation. 2021\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (CVPR), pp 833–842\n74. Ding M, Yang Z, Hong W, Zheng W, Zhou C, Yin D, Lin J, Zou\nX, Shao Z, Yang H, Tang J (2021) Cogview: Mastering text-to-\nimage generation via transformers. In: NeurIPS\n75. Zhou Y, Zhang R, Chen C, Li C, Tensmeyer C, Yu T, Gu J, Xu J,\nSun T (2022) Towards language-free training for text-to-image\ngeneration. 2022 IEEE/CVF conference on computer vision and\npattern recognition (CVPR), pp 17886–17896\n76. Ramesh A, Pavlov M, Goh G, Gray S, Voss C, Radford A, Chen\nM, Sutskever I (2021) Zero-shot text-to-image generation. ArXiv\narXiv:2102.12092\n77. Gafni O, Polyak A, Ashual O, Sheynin S, Parikh D, Taigman Y\n(2022) Make-a-scene: scene-based text-to-image generation with\nhuman priors. ArXiv arXiv:2203.13131\n78. Barratt ST, Sharma R (2018) A note on the inception score.\nArXiv arXiv:1801.01973\n79. Tao M, Tang H, Wu F, Jing X-Y, Bao B-K, Xu C (2022) Df-gan:\nA simple and effective baseline for text-to-image synthesis. In:\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition (CVPR), pp. 16515–16525\n80. Gu S, Chen D, Bao J, Wen F, Zhang B, Chen D, Yuan L, Guo B\n(2022) Vector quantized diffusion model for text-to-image syn-\nthesis. In: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR), pp. 10696–10706\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n17260 Neural Computing and Applications (2024) 36:17245–17260\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8308401107788086
    },
    {
      "name": "Transformer",
      "score": 0.6237159967422485
    },
    {
      "name": "Artificial intelligence",
      "score": 0.545012354850769
    },
    {
      "name": "Graph",
      "score": 0.5117430686950684
    },
    {
      "name": "Image (mathematics)",
      "score": 0.49244388937950134
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.45011553168296814
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.413467675447464
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39316773414611816
    },
    {
      "name": "Theoretical computer science",
      "score": 0.20963630080223083
    },
    {
      "name": "Artificial neural network",
      "score": 0.1326349675655365
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}