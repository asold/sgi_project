{
  "title": "A Financial Service Chatbot based on Deep Bidirectional Transformers",
  "url": "https://openalex.org/W3010649878",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2048236213",
      "name": "Yu Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2038465042",
      "name": "Chen, Yuxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223826475",
      "name": "Zaidi, Hussain",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2101609803",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W2951394837",
    "https://openalex.org/W2971130081",
    "https://openalex.org/W1719489212",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2884877436",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2985479581",
    "https://openalex.org/W2252215182",
    "https://openalex.org/W2161466446",
    "https://openalex.org/W2108677974",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1567512734",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2111051539",
    "https://openalex.org/W2167433878",
    "https://openalex.org/W2106279089",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2951266961",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2910171846",
    "https://openalex.org/W2771740485",
    "https://openalex.org/W2896860804",
    "https://openalex.org/W2906579211",
    "https://openalex.org/W2810469995",
    "https://openalex.org/W3123298421",
    "https://openalex.org/W2810840719",
    "https://openalex.org/W2550472410",
    "https://openalex.org/W2904875922",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2772604077",
    "https://openalex.org/W2912168444",
    "https://openalex.org/W2431605385",
    "https://openalex.org/W2751124354",
    "https://openalex.org/W2950517871",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2164411961"
  ],
  "abstract": "We develop a chatbot using Deep Bidirectional Transformer models (BERT) to handle client questions in financial investment customer service. The bot can recognize 381 intents, and decides when to say \"I don't know\" and escalates irrelevant/uncertain questions to human operators. Our main novel contribution is the discussion about uncertainty measure for BERT, where three different approaches are systematically compared on real problems. We investigated two uncertainty metrics, information entropy and variance of dropout sampling in BERT, followed by mixed-integer programming to optimize decision thresholds. Another novel contribution is the usage of BERT as a language model in automatic spelling correction. Inputs with accidental spelling errors can significantly decrease intent classification performance. The proposed approach combines probabilities from masked language model and word edit distances to find the best corrections for misspelled words. The chatbot and the entire conversational AI system are developed using open-source tools, and deployed within our company's intranet. The proposed approach can be useful for industries seeking similar in-house solutions in their specific business domains. We share all our code and a sample chatbot built on a public dataset on Github.",
  "full_text": "A V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nShi Yu1 Yuxin Chen1 Hussain Zaidi 1\nAbstract\nWe develop a chatbot using Deep Bidirectional\nTransformer models (BERT) (Devlin et al., 2019)\nto handle client questions in ﬁnancial investment\ncustomer service. The bot can recognize 381 in-\ntents, and decides when to say I don’t knowand\nescalates irrelevant/uncertain questions to human\noperators. Our main novel contribution is the\ndiscussion about uncertainty measure for BERT,\nwhere three different approaches are systemati-\ncally compared on real problems. We investigated\ntwo uncertainty metrics, information entropy and\nvariance of dropout sampling in BERT, followed\nby mixed-integer programming to optimize de-\ncision thresholds. Another novel contribution\nis the usage of BERT as a language model in\nautomatic spelling correction. Inputs with acci-\ndental spelling errors can signiﬁcantly decrease\nintent classiﬁcation performance. The proposed\napproach combines probabilities from masked lan-\nguage model and word edit distances to ﬁnd the\nbest corrections for misspelled words. The chat-\nbot and the entire conversational AI system are\ndeveloped using open-source tools, and deployed\nwithin our company’s intranet. The proposed ap-\nproach can be useful for industries seeking similar\nin-house solutions in their speciﬁc business do-\nmains. We share all our code and a sample chatbot\nbuilt on a public dataset on Github.\n1. Introduction\nSince their ﬁrst appearances decades ago (Weizenbaum,\n1966) (Colby et al., 1971), Chatbots have always been\nmarking the apex of Artiﬁcial Intelligence as forefront of\nall major AI revolutions, such as human-computer interac-\ntion, knowledge engineering, expert system, Natural Lan-\nguage Processing, Natural Language Understanding, Deep\nLearning, and many others. Open-domain chatbots, also\nknown as Chitchat bots, can mimic human conversations\n*Equal contribution 1The Vanguard Group, Malvern, PA, USA.\nCorrespondence to: Shi Yu <shi.yu@hotmail.com>.\nUnder Review, Copyright 2020 by the authors.\nto the greatest extent in topics of almost any kind, thus\nare widely engaged for socialization, entertainment, emo-\ntional companionship, and marketing. Earlier generations\nof open-domain bots, such as Mitsuku(Worswick, 2019)\nand ELIZA(Weizenbaum, 1966), relied heavily on hand-\ncrafted rules and recursive symbolic evaluations to capture\nthe key elements of human-like conversation. New advances\nin this ﬁeld are mostly data-driven and end-to-end systems\nbased on statistical models and neural conversational models\n(Gao et al., 2018) aim to achieve human-like conversations\nthrough more scalable and adaptable learning process on\nfree-form and large data sets (Gao et al., 2018), such as\nMILABOT(Serban et al., 2017), XiaoIce(Zhou et al., 2018),\nReplika(Fedorenko et al., 2017), Zo(Microsoft, 2019), and\nMeena(Adiwardana et al., 2020).\nUnlike open-domain bots, closed-domain chatbots are de-\nsigned to transform existing processes that rely on human\nagents. Their goals are to help users accomplish speciﬁc\ntasks, where typical examples range from order placement\nto customer support, therefore they are also known as task-\noriented bots (Gao et al., 2018). Many businesses are ex-\ncited about the prospect of using closed-domain chatbots to\ninteract directly with their customer base, which comes with\nmany beneﬁts such as cost reduction, zero downtime, or no\nprejudices. However, there will always be instances where a\nbot will need a humans input for new scenarios. This could\nbe a customer presenting a problem it has never expected\nfor (Larson et al., 2019), attempting to respond to a naughty\ninput, or even something as simple as incorrect spelling. Un-\nder these scenarios, expected responses from open-domain\nand closed-domain chatbots can be very different: a success-\nful open-domain bot should be ”knowledgeable, humourous\nand addictive”, whereas a closed-domain chatbot ought to\nbe ”accurate, reliable and efﬁcient”. One main difference\nis the way of handling unknown questions. A chitchat bot\nwould respond with an adversarial question such as Why do\nyou ask this?, and keep the conversation going and deviate\nback to the topics under its coverage (Sethi, 2019). A user\nmay ﬁnd the chatbot is out-smarting, but not very helpful in\nsolving problems. In contrast, a task-oriented bot is scoped\nto a speciﬁc domain of intents, and should terminate out-of-\nscope conversations promptly and escalate them to human\nagents.\nThis paper presents A V A (A Vanguard Assistant), a task-\narXiv:2003.04987v1  [cs.CL]  17 Feb 2020\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nClient Phone \nAgent\nExpert\nSentence Completion \nModel\nIntent Classification\nModel\nInformation Retrieval \nand Question \nAnswering\nHelp Documents \nRepoChatbot\nrelevant\nIrrelevant (human escalation)\nHuman expert support\nConversational AI support\nFuture self-provisioning \ninteraction\nFigure 1.End-to-end conceptual diagram of A V A\noriented chatbot supporting phone call agents when they\ninteract with clients on live calls. Traditionally, when phone\nagents need help, they put client calls on hold and consult\nexperts in a support group. With a chatbot, our goal is to\ntransform the consultation processes between phone agents\nand experts to an end-to-end conversational AI system. Our\nfocus is to signiﬁcantly reduce operating costs by reduc-\ning the call holding time and the need of experts, while\ntransforming our client experience in a way that eventually\npromotes client self-provisioning in a controlled environ-\nment. Understanding intents correctly and escalating irrel-\nevant intents promptly are keys to its success. Recently,\nNLP community has made many breakthroughs in context-\ndependent embeddings and bidirectional language models\nlike ELMo, OpenAI, GPT, BERT, RoBERTa, DistilBERT,\nXLM, XLNet (Dai & Le, 2015; Peters et al., 2017; Devlin\net al., 2019; Peters et al., 2018a; Lample & Conneau, 2019;\nPeters et al., 2018b; Howard & Ruder, 2018; Yang et al.,\n2019; Liu et al., 2019; Tang et al., 2019). In particular,\nthe BERT model (Devlin et al., 2019) has become a new\nNLP baseline including sentence classiﬁcation, question an-\nswering, named-entity recognition and many others. To our\nknowledge there are few measures that address prediction\nuncertainties in these sophisticated deep learning structures,\nor explain how to achieve optimal decisions on observed\nuncertainty measures. The off-the-shelf softmax outputs of\nthese models are predictive probabilities, and they are not a\nvalid measure for the conﬁdence in a networks predictions\n(Gal & Ghahramani, 2016; Maddox et al., 2019; Pearce\net al., 2018; Shridhar et al., 2019), which are important\nconcerns in real-world applications (Larson et al., 2019).\nOur main contribution in this paper is applying advances in\nBayesian Deep Learning to quantify uncertainties in BERT\nintent predictions. Formal methods like Stochastic Gradient\n(SG)-MCMC (Li et al., 2016; Rao & Frtunikj, 2018; Welling\n& Teh, 2011; Park et al., 2018; Maddox et al., 2019; Seedat\n& Kanan, 2019), variational inference (Blundell et al., 2015;\nGal & Ghahramani, 2016; Graves, 2011; Hern´andez-Lobato\n& Adams, 2015) extensively discussed in literature may\nrequire modifying the network. Re-implementation of the\nentire BERT model for Bayesian inference is a non-trivial\ntask, so here we took the Monte Carlo Dropout (MCD)\napproach (Gal & Ghahramani, 2016) to approximate varia-\ntional inference, whereby dropout is performed at training\nand test time, using multiple dropout masks. Our dropout\nexperiments are compared with two other approaches (En-\ntropy and Dummy-class), and the ﬁnal implementation is\ndetermined among the trade-off between accuracy and efﬁ-\nciency.\nWe also investigate the usage of BERT as a language\nmodel to decipher spelling errors. Most vendor-based chat-\nbot solutions embed an additional layer of service, where\ndevice-dependent error models and N-gram language mod-\nels(Lin et al., 2012) are utilized for spell checking and\nlanguage interpretation. At representation layer, Word-\npiece model(Schuster & Nakajima, 2012) and Byte-Pair-\nEncoding(BPE) model(Gage, 1994; Sennrich et al., 2016)\nare common techniques to segment words into smaller units,\nthus similarities at sub-word level can be captured by NLP\nmodels and generalized on out-of-vocabulary(OOV) words.\nOur approach combines efforts of both sides: words cor-\nrected by the proposed language model are further tokenized\nby Wordpiece model to match pre-trained embeddings in\nBERT learning.\nDespite all advances of chatbots, industries like ﬁnance and\nhealthcare are concerned about cyber-security because of the\nlarge amount of sensitive information entered during chatbot\nsessions. Task-oriented bots often require access to criti-\ncal internal systems and conﬁdential data to ﬁnish speciﬁc\ntasks. Therefore, 100% on-premise solutions that enable\nfull customization, monitoring, and smooth integration are\npreferable than cloud solutions. In this paper, the proposed\nchatbot is designed using RASA open-source version and\ndeployed within our enterprise intranet. Using RASA’s con-\nversational design, we hybridize RASA’s chitchat module\nwith the proposed task-oriented conversational systems de-\nveloped on Python, Tensorﬂow and Pytorch. We believe our\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\napproach can provide some useful guidance for industries\ncontemplate adopting chatbot solutions in their business\ndomains.\n2. Background\nRecent breakthroughs in NLP research are driven by two\nintertwined directions: Advances in distributed representa-\ntions, sparked by the success of word embeddings (Mikolov\net al., 2010; 2013), character embeddings (Kim et al., 2015;\ndos Santos & Gatti, 2014; Dos Santos & Zadrozny, 2014),\ncontextualized word embeddings (Peters et al., 2018a; Rad-\nford & Sutskever, 2018; Devlin et al., 2019), have success-\nfully tackled the curse of dimensionality in modeling com-\nplex language models. Advances of neural network archi-\ntecture, represented by CNN (Collobert & Weston, 2008;\nCollobert et al., 2011), RNN(Elman, 1990), Attention Mech-\nanism (Bahdanau et al., 2015), and Transformer as seq2seq\nmodel with parallelized attentions (Vaswani et al., 2017),\nhave deﬁned the new state of the art deep learning models\nfor NLP.\nPrincipled uncertainty estimation in regression (V . Kuleshov\n& Ermon, 2018), reinforcement learning (et al., 2016) and\nclassiﬁcation (et al., 2017) are active areas of research with\na large volume of work. The theory of Bayesian neural\nnetworks (Neal, 1995; MacKay, 1992) provides the tools\nand techniques to understand model uncertainty, but these\ntechniques come with signiﬁcant computational costs as\nthey double the number of parameters to be trained. Gal and\nGhahramani (Gal & Ghahramani, 2016) showed that a neu-\nral network with dropout turned on at test time is equivalent\nto a deep Gaussian process and we can obtain model uncer-\ntainty estimates from such a network by multiple-sampling\nthe predictions of the network at test time. Non-Bayesian ap-\nproaches to estimating the uncertainty are also shown to pro-\nduce reliable uncertainty estimates (B. Lakshminarayanan,\n2017); our focus in this paper is on Bayesian approaches. In\nclassiﬁcation tasks, the uncertainty obtained from multiple-\nsampling at test time is an estimate of the conﬁdence in the\npredictions similar to the entropy of the predictions. In this\npaper, we compare the threshold for escalating a query to\na human operator using model uncertainty obtained from\ndropout-based chatbot against setting the threshold using\nthe entropy of the predictions. We choose dropout-based\nBayesian approximation because it does not require changes\nto the model architecture, does not add parameters to train,\nand does not change the training process as compared to\nother Bayesian approaches. We minimize noise in the data\nby employing spelling correction models before classifying\nthe input. Further, the labels for the user queries are human\ncurated with minimal error. Hence, our focus is on quan-\ntifying epistemic uncertainty in A V A rather than aleatoric\nuncertainty (Kendall & Gal, 2017). We use mixed-integer\noptimization to ﬁnd a threshold for human escalation of a\nuser query based on the mean prediction and the uncertainty\nof the prediction. This optimization step, once again, does\nnot require modiﬁcations to the network architecture and\ncan be implemented separately from model training. In\nother contexts, it might be fruitful to have an integrated\nescalation option in the neural network (Geifman, 2019),\nand we leave the trade-offs of integrated reject option and\nnon-Bayesian approaches for future work.\nSimilar approaches in spelling correction, besides those\nmentioned in Section 1, are reported in Deep Text Corrector\n(Atpaino, 2017) that applies a seq2seq model to automat-\nically correct small grammatical errors in conversational\nwritten English. Optimal decision threshold learning under\nuncertainty is studied in (Lepora, 2016) as Reinforcement\nlearning and iterative Bayesian optimization formulations.\n3. System Overview and Data Sets\n3.1. Overview of the System\nFigure 1 illustrates system overview of A V A. The proposed\nconversational AI will gradually replace the traditional\nhuman-human interactions between phone agents and inter-\nnal experts, and eventually allows clients self-provisioning\ninteraction directly to the AI system. Now, phone agents in-\nteract with A V A chatbot deployed on Microsoft Teams in our\ncompany intranet, and their questions are preprocessed by\na Sentence Completion Model (introduced in Section 6) to\ncorrect misspellings. Then, inputs are classiﬁed by an intent\nclassiﬁcation model (Section 4 & 5), where relevant ques-\ntions are assigned predicted intent labels, and downstream\ninformation retrieval and questioning answering modules\nare triggered to extract answers from a document reposi-\ntory. Irrelevant questions are escalated to human experts\nfollowing the decision thresholds optimized using methods\nintroduced in section 5. This paper only discusses the Intent\nClassiﬁcation model and the Sentence Completion model.\n3.2. Data for Intent Classiﬁcation Model\nTraining data for A V A’s intent classiﬁcation model is col-\nlected, curated, and generated by a dedicated business team\nfrom interaction logs between phone agents and the expert\nteam. The whole process takes about one year to ﬁnish.\nIn total 22,630 questions are selected and classiﬁed to 381\nintents, which compose the relevant questions set for the\nintent classiﬁcation model. Additionally, 17,395 questions\nare manually synthesized as irrelevant questions, and none\nof them belongs to any of the aforementioned 381 intents.\nEach relevant question is hierarchically assigned with three\nlabels from Tier 1 to Tier 3. In this hierarchy, there are 5\nunique Tier-1 labels, 107 Tier-2 labels, and 381 Tier-3 la-\nbels. Our intent classiﬁcation model is designed to classify\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nT1 label T2 label T3 label Questions\nAccount Maintenance\nCall Authentication Type 2 Am I allowed to give the client their Social security number?\nCall Authentication Type 5 Do the web security questions need to be reset by the client if their web access is blocked?\nWeb Reset Type 1 How many security questions are required to be asked to reset a clients web security questions?\nAccount Permission Call Authentication Type 2 How are the web security questions used to authenticate a client?\nAgent Incapactiated Type 3 Is it possible to set up Agent Certiﬁcation for an Incapacitated Person on an Individual Roth 401k?\nTAX FAQ Miscellaneous What is Do I need my social security number on the 1099MISC form?\nTransfer of Asset Unlike registrations Type 2 Does the client need to provide special documentation if they want to transfer from one account to another account?\nBrokerage Transfer Type 3 Is there a list of items that need to be included on a statement to transfer an account?\nBanking Add Owner Type 4 Once a bank has been declined how can we authorize it?\nAdd/Change/Delete Type 3 Does a limited agent have authorization to adjust bank info?\nIrrelevant\n- - How can we get into an account with only one security question?\n- - Am I able to use my Roth IRA to set up a margin account?\n- - What is the best place to learn about Vanguard’s investment philosophy?\nTable 1.Example questions used in A V A intent classiﬁcation model training\nrelevant input questions into 381 Tier 3 intents and then trig-\ngers downstream models to extract appropriate responses.\nThe ﬁve Tier-1 labels and the numbers of intents include in\neach label are: Account Maintenance (9074), Account Per-\nmissions (2961), Transfer of Assets (2838), Banking (4788),\nTax FAQ(2969). At Tier-1, general business issues across\nintents are very different, but at Tier-3 level, questions are\nquite similar to each other, where differences are merely at\nthe speciﬁc responses. Irrelevant questions, compared to\nrelevant questions, have two main characteristics:\n• Some questions are relevant to business intents but\nunsuitable to be processed by conversational AI. For\nexample, in Table 1, question ”How can we get into\nan account with only one security question?” is re-\nlated to Call Authentication in Account Permission, but\nits response needs further human diagnosis to collect\nmore information. These types of questions should be\nescalated to human experts.\n• Out of scope questions. For example, questions like\n”What is the best place to learn about Vanguard’s in-\nvestment philosophy?” or ”What is a hippopotamus?”\nare totally outside the scope of our training data, but\nthey may still occur in real world interactions.\n3.3. Textual Data for Pretrained Embeddings and\nSentence Completion Model\nInspired by the progress in computer vision, transfer learn-\ning has been very successful in NLP community and has\nbecome a common practice. Initializing deep neural net-\nwork with pre-trained embeddings, and ﬁne-tune the models\ntowards task-speciﬁc data is a proven method in multi-task\nNLP learning. In our approach, besides applying off-the-\nshelf embeddings from Google BERT and XLNet, we also\npre-train BERT embeddings using our company’s propri-\netary text to capture special semantic meanings of words\nin the ﬁnancial domain. Three types of textual datasets are\nused for embeddings training:\n• Sharepoint text: About 3.2G bytes of corpora scraped\nfrom our company’s internal Sharepoint websites, in-\ncluding web pages, word documents, ppt slides, pdf\ndocuments, and notes from internal CRM systems.\n• Emails: About 8G bytes of customer service emails\nare extracted.\n• Phone call transcriptions: We apply AWS to transcribe\n500K client service phone calls, and the transcription\ntext is used for training.\nAll embeddings are trained in case-insensitive settings. At-\ntention and hidden layer dropout probabilities are set to 0.1,\nhidden size is 768, attention heads and hidden layers are\nset to 12, and vocabulary size is 32000 using SentencePiece\ntokenizer. On AWS P3.2xlarge instance each embeddings\nis trained for 1 million iterations, and takes about one week\nCPU time to ﬁnish. More details about parameter selection\nfor pre-training are avaialble in the github code. The same\npre-trained embeddings are used to initialize BERT model\ntraining in intent classiﬁcation, and also used as language\nmodels in sentence completion.\nModel Performance\nBERT small + Sharepoint Embeddings 0.944\nBERT small + Google Embeddings 0.949\nBERT large + Google Embeddings 0.954\nXLNet Large + Google Embeddings 0.927\nLSTM with Attention + Word2Vec 0.913\nLSTM + Word2Vec 0.892\nLogistic Regression + TFIDF 0.820\nXgboost + TFIDF 0.760\nNaive Bayes + TFIDF 0.661\nTable 2.Comparison of intent classiﬁcation performance. BERT\nand XLNet models were all trained for 30 epochs using batch size\n16.\n4. Intent Classiﬁcation Performance on\nRelevant Questions\nUsing only relevant questions, we compare various popular\nmodel architectures to ﬁnd one with the best performance\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nFigure 2.Comparison of test set accuracy using different Embed-\ndings and Batch Sizes\non 5-fold validation. Not surprisingly, BERT models gen-\nerally produce much better performance than other models.\nLarge BERT (24-layer, 1024-hidden, 16-heads) has a slight\nimprovement over small BERT (12-layer, 768-hidden, 12-\nheads), but less preferred because of expensive computa-\ntions. To our surprise, XLNet, a model reported outperform-\ning BERT in mutli-task NLP, performs 2 percent lower on\nour data.\nBERT models initialized by proprietary embeddings con-\nverge faster than those initialized by off-the-shelf embed-\ndings (Figure 2.a). And embeddings trained on company’s\nsharepoint text perform better than those built on Emails and\nphone-call transcriptions (Figure 2.b). Using larger batch\nsize (32) enables models to converge faster, and leads to\nbetter performance.\n5. Intent Classiﬁcation Performance including\nIrrelevant Questions\nWe have shown how BERT model outperforming other mod-\nels on real datasets that only contain relevant questions. The\ncapability to handle 381 intents simultaneously at 94.5%\naccuracy makes it an ideal intent classiﬁer candidate in a\nchatbot. This section describes how we quantify uncer-\ntainties on BERT predictions and enable the bot to detect\nirrelevant questions. Three approaches are compared:\n• Predictive-entropy: We measure uncer-\ntainty of predictions using Shannon entropy\nH = −∑K\nk=1 pik log pik where pik is the prediction\nprobability of i-th sample to k-th class. Here, pik\nis softmax output of the BERT network (B. Laksh-\nminarayanan, 2017). A higher predictive entropy\ncorresponds to a greater degree of uncertainty. Then,\nan optimally chosen cut-off threshold applied on\nentropies should be able to separate the majority of\nin-sample questions and irrelevant questions.\n• Drop-out: We apply Monte Carlo (MC) dropout by\ndoing 100 Monte Carlo samples. At each inference\niteration, a certain percent of the set of units to drop\nout. This generates random predictions, which are in-\nterpreted as samples from a probabilistic distribution\n(Gal & Ghahramani, 2016). Since we do not employ\nregularization in our network, τ−1 in Eq. 7 in Gal and\nGhahramani (Gal & Ghahramani, 2016) is effectively\nzero and the predictive variance is equal to the sample\nvariance from stochastic passes. We could then investi-\ngate the distributions and interpret model uncertainty\nas mean probabilities and variances.\n• Dummy-class: We simply treat escalation questions\nas a dummy class to distinguish them from original\nquestions. Unlike entropy and dropout, this approach\nrequires retraining of BERT models on the expanded\ndata set including dummy class questions.\n5.1. Experimental Setup\nAll results mentioned in this section are obtained using\nBERT small + sharepoint embeddings (batch size 16). In\nEntropy and Dropout approaches, both relevant questions\nand irrelevant questions are split into ﬁve folds, where four\nfolds (80%) of relevant questions are used to train the BERT\nmodel. Then, among that 20% held-out relevant questions\nwe further split them into ﬁve folds, where 80% of them\n(equals to 16% of the entire relevant question set) are com-\nbined with four folds of irrelevant questions to learn the\noptimal decision variables. The learned decision variables\nare applied on BERT predictions of the remaining 20%\n(906) of held-out relevant questions and held-out irrelevant\nquestions (4000), to obtain the test performance. In dummy\nclass approach, BERT model is trained using four folds of\nrelevant questions plus four folds of irrelevant questions,\nand tested on the same amount of test questions as Entropy\nand Dropout approaches.\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\n0 1 2 3 4 5 6\nEntropy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nEscalation questions\nTest questions\n(a) Predictive Entropy distributions on\nrelevant (orange) and escalation (blue)\nquestions\n1000 2500 5000 7500 10000\nnumber of escalation questions\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94test accuracy\n = 0.5\n  = 0.6\n  = 0.7\n  = 0.8\n  = 0.9\n(b) Test Accuracy when adding Escala-\ntion Questions in optimization procedure\n1000 2500 5000 7500 10000\nnumber of escalation questions\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00optimal entropy cutoff\n = 0.5\n  = 0.6\n  = 0.7\n  = 0.8\n  = 0.9\n(c) Optimal Entropy cut-off value when\nadding Escalation Questions in optimiza-\ntion procedure\nFigure 3.Optimizing entropy threshold to detect irrelevant questions. As shown in (a), in-sample test questions and irrelevant questions\nhave very different distributions of predictive entropies. Subﬁgure (b) shows how test accuracies, evaluated using decision variablesb\nsolved by (1) on BERT predictions on test data, change when different numbers of irrelevant questions involved in training. Subﬁgure (c)\nshows the impact of δon the optimized thesholds when number of irrelevant questions increase optimization.\n5.2. Optimizing Entropy Decision Threshold\nTo ﬁnd the optimal threshold cutoff b, we consider the fol-\nlowing Quadratic Mixed-Integer programming problem\nmin\nx,b\n∑\ni,k(xik −lik)2\ns.t. xik = 0 if Ei ≥b, for kin 1,...,K\nxik = 1 if Ei ≥b, for k= K+ 1\nxik ∈{0,1}∑K+1\nk=1 xik = 1 ∀iin 1,...,N\nb≥0\n(1)\nto minimize the quadratic loss between the predictive as-\nsignments xik and true labels lik. In (1), iis sample index,\nand kis class (intent) indices. xik is N ×(K+ 1)binary\nmatrix, and lik is also N ×(K + 1), where the ﬁrst K\ncolumns are binary values and the last column is a uniform\nvector δ, which represents the cost of escalating questions.\nNormally δ is a constant value smaller than 1, which en-\ncourages the bot to escalate questions rather than making\nmistaken predictions. The ﬁrst and second constraints of (1)\nforce an escalation label when entropy Ei ≥b. The third\nand fourth constraints restrict xik as binary variables and\nensure the sum for each sample is 1. Experimental results\n(Figure 3) indicate that (1) needs more than 5000 escalation\nquestions to learn a stabilized b. The value of escalation\ncost δhas a signiﬁcant impact on the optimal bvalue, and\nin our implementation is set to 0.5.\n5.3. Monte Carlo Drop-out\nIn BERT model, dropout ratios can be customized at encod-\ning, decoding, attention, and output layer. A combinatorial\nsearch for optimal dropout ratios is computationally chal-\nlenging. Results reported in the paper are obtained through\nsimpliﬁcations with the same dropout ratio assigned and\nvaried on all layers. Our MC dropout experiments are con-\nducted as follows:\n1 Change dropout ratios in encod-\ning/decoding/attention/output layer of BERT\n2 Train BERT model on 80% of relevant questions for\n10 or 30 epochs\n3 Export and serve the trained model by Tensorﬂow serv-\ning\n4 Repeat inference 100 times on questions, then average\nthe results per each question to obtain mean probabil-\nities and standard deviations, then average the devia-\ntions for a set of questions.\nAccording to the experimental results illustrated in Figure\n4, we make three conclusions: (1) Epistemic uncertainty\nestimated by MCD reﬂects question relevance: when inputs\nare similar to the training data there will be low uncertainty,\nwhilst data is different from the original training data should\nhave higher epistemic uncertainty. (2) Converged models\n(more training epochs) should have similar uncertainty and\naccuracy no matter what drop ratio is used. (3) The number\nof epochs and dropout ratios are important hyper-parameters\nthat have substantial impacts on uncertainty measure and\npredictive accuracy and should be cross-validated in real\napplications.\nWe use mean probabilities and standard deviations obtained\nfrom models where dropout ratios are set to 10% after 30\nepochs of training to learn optimal decision thresholds. Our\ngoal is to optimize lowerboundcand upperbound d, and des-\nignate a question as relevant only when the mean predictive\nprobability Pik is larger than cand standard deviation Vik is\nlower than d. Optimizing cand d, on a 381-class problem,\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\n(a) Intent accuracy at different drop\nout ratios\n(b) Uncertainties on questions after\ntraining 10 epochs\n(c) Uncertainties on questions after\ntraining 30 epochs\nFigure 4.Classiﬁcation accuracy and uncertainties obtained from Monte Carlo Dropout\nEntropy Dropout Dummy Class\nnumber of irrelevant\nquestions in training 1000 5000 8000 10000 100 1000 2000 3000 1000 5000 8000 10000\noptimal entropy cutoff b 2.36 1.13 0.85 0.55 - - - - - - - -\noptimal mean prob cutoff c - - - - 0.8172 0.6654 0.7921 0.0459 - - - -\noptimal std. cutoff d - - - - 0.1533 0.0250 0.0261 0.0132 - - - -\nmean accuracy in 381 classes 91.9% 88.3% 85.6% 81.7% 88.41% 80.13% 80.24% 74.72% 94.2% 93.7% 87.7% 82%\naccuracy of the dummy class 79.25% 91.2% 93.25% 95.2% 86.69% 91.83% 91.95% 92.57% 73.6% 94.5% 99.4% 99.6%\nprecision (binary classiﬁcation) 51.4% 70.2% 74.7% 79.8% 90.7% 68.8% 68.9% 63.7% 81% 95.3% 99.5% 99.6%\nrecall (binary classiﬁcation) 96.7% 91.3% 88.1% 83.5% 93.9% 82.7% 83.2% 84.7% 99.7% 98.7% 92.6% 86%\nF1 score (binary classiﬁcation) 0.671 0.794 0.808 0.816 0.738 0.751 0.754 0.727 0.894 0.967 0.959 0.923\nTable 3.Performance cross comparison of three approaches evaluated on test data of same size (906 relevant questions plus 4000 irrelevant\nquestions). Precision/Recall/F1 scores were calculated assuming relevant questions are true positives. In entropy and dropout optimization\nprocesses, δis set to 0.5. Other delta values for dropout approach are listed in appendix.\nis much more computationally challenging than learning\nentropy threshold because the number of constraints is pro-\nportional to class number. As shown in (2), we introduce\ntwo variables αand βto indicate the status of mean prob-\nability and deviation conditions, and the ﬁnal assignment\nvariables xis the logical AND of αand β. Solving (2) with\nmore than 10k samples is very slow (shown in Appendix),\nso we use 1500 original relevant questions, and increase\nthe number of irrelevant questions from 100 to 3000. For\nperformance testing, the optimized cand dare applied as\ndecision variables on samples of BERT predictions on test\ndata. Performance from dropout are presented in Table 3\nand Appendix. Our results showed decision threshold opti-\nmized from (2) involving 2000 irrelevant questions gave the\nbest F1 score (0.754), and we validated it using grid search\nand conﬁrmed its optimality (shown in appendix).\nmin\nx,c,d\n∑\ni,k(xik −lik)2\ns.t. αik =\n{\n0 if Pik ≤c, for kin 1,...,K\n1 if otherwise\nβik =\n{\n0 if Vik ≥d, for kin 1,...,K\n1 if otherwise\nxik = 0 if αik = 0OR βik = 0\nxik = 1 if αik = 1AND βik = 1∑K+1\nk xik = 1 ∀iin 1,...,N\n1 ≥c≥0\n1 ≥d≥0\n(2)\n5.4. Dummy-class Classiﬁcation\nOur third approach is to train a binary classiﬁer using both\nrelevant questions and irrelevant questions in BERT. We use\na dummy class to represent those 17,395 irrelevant ques-\ntions, and split the entire data sets, including relevant and\nirrelevant, into ﬁve folds for training and test.\nPerformance of dummy class approach is compared with\nEntropy and Dropout approaches (Table 3). Deciding an\noptimal number of irrelevant questions involved in threshold\nlearning is non-trivial, especially for Entropy and Dummy\nclass approaches. Dropout doesn’t need as many irrelevant\nquestions as entropy does to learn optimal threshold, mainly\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nbecause the number of constraints in (2) is proportional to\nthe class number (381), so the number of constraints are\nlarge enough to learn a suitable threshold on small samples\n(To support this conclusion, we present extensive studies\nin Appendix on a 5-class classiﬁer using Tier 1 intents).\nDummy class approach obtains the best performance, but\nits success assumes the learned decision boundary can be\ngeneralized well to any new irrelevant questions, which is\noften not valid in real applications. In contrast, Entropy and\nDropout approaches only need to treat a binary problem in\nthe optimization and leave the intent classiﬁcation model in-\ntact. The optimization problem for entropy approach can be\nsolved much more efﬁciently, and is selected as the solution\nfor our ﬁnal implementation.\nIt is certainly possible to combine Dropout and Entropy\napproach, for example, to optimize thresholds on entropy\ncalculated from the average mean of MCD dropout predic-\ntions. Furthermore, it is possible that the problem deﬁned\nin (2) can be simpliﬁed by proper reformulation, and can be\nsolved more efﬁciently, which will be explored in our future\nworks.\n6. Sentence Completion using Language\nModel\n6.1. Algorithm\nWe assume misspelled words are all OOV words, and we\ncan transform them as [MASK] tokens and use bidirec-\ntional language models to predict them. Predicting masked\nword within sentences is an inherent objective of a pre-\ntrained bidirectional model, and we utilize the Masked Lan-\nguage Model API in the Transformer package (Hugging-\nFace, 2017) to generate the ranked list of candidate words\nfor each [MASK] position. The sentence completion algo-\nrithm is illustrated in Algorithm 1.\n6.2. Experimental Setup\nFor each question, we randomly permutate two characters\nin the longest word, the next longest word, and so on. In\nthis way, we generate one to three synthetic misspellings in\neach question. We investigate intent classiﬁcation accuracy\nchanges on these questions, and how our sentence comple-\ntion model can prevent performance changes. All models\nare trained using relevant data (80%) without misspellings\nand validated on synthetic misspelled test data. Five set-\ntings are compared: (1) No correction: classiﬁcation per-\nformance without applying any auto-correction; (2) No LM:\nAuto-corrections made only by word edit distance with-\nout using Masked Language model; (3) BERT Sharepoint:\nAuto-corrections made by Masked LM using pre-trained\nsharepoint embeddings together with word edit distance;\n(4) BERT Email: Auto-corrections using pretrained email\nembeddings together with word edit distance; (5) BERT\nGoogle: Auto-corrections using pretrained Google Small\nuncased embedding data together with word edit distance.\nWe also need to decide what is an OOV , or, what should be\nincluded in our vocabulary. After experiments, we set our\nvocabulary as words from four categories: (1) All words in\nthe pre-trained embeddings; (2) All words that appear in\ntraining questions; (3) Words that are all capitalized because\nthey are likely to be proper nouns, fund tickers or service\nproducts; (4) All words start with numbers because they\ncan be tax forms or speciﬁc products (e.g., 1099b, 401k,\netc.). The purposes of including (3) and (4) is to avoid\nauto-correction on those keywords that may represent sig-\nniﬁcant intents. Any word falls outside these four groups\nis considered as an OOV . During our implementation, we\nkeep monitoring OOV rate, deﬁned as the ratio of OOV\noccurrences to total word counts in recent 24 hours. When\nit is higher than 1%, we apply manual intervention to check\nchatbot log data.\nWe also need to determine two additional parametersM, the\nnumber of candidate tokens prioritized by masked language\nmodel and B, the beam size in our sentence completion\nmodel. In our approach, we set M and Bto the same value,\nand it is benchmarked from 1 to 10k by test sample accuracy.\nNotice that when M and B are large, and when there are\nmore than two OOVs, Beam Search becomes very inefﬁ-\ncient in Algorithm 1. To simplify this, instead of ﬁnding the\noptimal combinations of candidate tokens that maximize\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\n(a) Accuracy - Single OOV\n (b) Accuracy - Two OOVs\n (c) Accuracy - Three OOVs\n (d) Accuracy per beam size\nFigure 5.As expected, misspelled words can signiﬁcantly decrease intent classiﬁcation performance. The same BERT model that achieved\n94% on clean data, dropped to 83.5% when a single OOV occured in each question. It further dropped to 68% and 52%, respectively,\nwhen two and three OOVs occured. In all experiments, LM models proved being useful to help correcting words and reduce performance\ndrop, while domain speciﬁc embeddings trained on Vanguard Sharepoint and Email text outperform off-the-shelf Google embeddings.\nThe beam size B(M) was benchmarked as results shown in subﬁgure (d), and was set to 4000 to generate results in subﬁgure (a) to (c).\nthe joint probability arg max∏d\ni=1 pi, we assume they are\nindependent and apply a simpliﬁed Algorithm (shown in\nAppendix) on single OOV separately. An improved ver-\nsion of sentence completion algorithm to maximize joint\nprobability will be our future research. We haven’t consider\nsituations when misspellings are not OOV in our paper. To\ndetect improper words in a sentence may need evaluation of\nmetrics such as Perplexity or Sensibleness and Speciﬁcity\nAverage (SSA)(Adiwardana et al., 2020), and will be our\nfuture goals.\n6.3. Results\nAccording to the experimental results illustrated in Figure 5,\npre-trained embeddings are useful to increase the robustness\nof intent prediction on noisy inputs. Domain-speciﬁc em-\nbeddings contain much richer context-dependent semantics\nthat helps OOVs get properly corrected, and leads to better\ntask-oriented intent classiﬁcation performance. Benchmark\nshows B≥4000 leads to the best performance for our prob-\nlem. Based on this, we apply sharepoint embeddings as the\nlanguage model in our sentence completion module.\n7. Implementation\nThe chatbot has been implemented fully inside our company\nnetwork using open source tools including RASA(Bocklisch\net al., 2017), Tensorﬂow, Pytorch in Python enviornment.\nAll backend models (Sentence Completion model, Intent\nClassiﬁcation model and others) are deployed as REST-\nFUL APIs in AWS Sagemaker. The front-end of chatbot is\nlaunched on Microsoft Teams, powered by Microsoft Bot-\nframework and Microsoft Azure directory, and connected to\nbackend APIs in AWS environment. All our BERT model\ntrainings, including embeddings pretraining, are based on\nBERT Tensorﬂow running on AWS P3.2xlarge instance.\nThe optimization procedure uses Gurobi 8.1 running on\nAWS C5.18xlarge instance. BERT language model API in\nsentence completion model is developed using Transformer\n2.1.1 package on PyTorch 1.2 and Tensorﬂow 2.0.\nDuring our implementation, we further explore how the\nintent classiﬁcation model API can be served in real appli-\ncations under budget. We gradually reduce the numbers\nof attention layer and hidden layer in the original BERT\nSmall model (12 hidden layers, 12 attention heads) and\ncreate several smaller models. By reducing the number of\nhidden layers and attention layers in half, we see a remark-\nable 100% increase in performance (double the throughput,\nhalf the latency) with the cost of only 1.6% drop in intent\nclassiﬁcation performance.\nModel Performance Throughput Avg. Latency\n12A-12H 0.944 8.9/s 1117 ms\n6A-12H 0.941 9.0/s 1108 ms\n12A-9H 0.934 11.8/s 843 ms\n3A-9H 0.933 12.0/s 831 ms\n3A-12H 0.930 9.1/s 1097 ms\n6A-6H 0.928 18.1/s 552 ms\nTable 4.Benchmark of intent classiﬁcation API performance\nacross different models in real application. Each model is tested\nusing 10 threads, simulating 10 concurrent users, for a duration of\n10 minutes. In this test, models are not served as Monte Carlo sam-\npling, so the inference is done only once. All models are hosted on\nidentical AWS m5.4xlarge CPU instances. As seen, the simplest\nmodel (6A-6H, 6 attention layers and 6 hidden layers) can have\ndouble throughput rate and half latency than the original BERT\nsmall model, and the accuracy performance only drops 1.6%. The\nperformance is evaluated using JMeter at client side, and APIs are\nserved using Domino Lab 3.6.17 Model API. Throughput indicates\nhow many API responses being made per second. Latency is mea-\nsured as time elapse between request sent till response received at\nclient side.\n8. Conclusions\nOur results demonstrate that optimized uncertainty thresh-\nolds applied on BERT model predictions are promising to\nescalate irrelevant questions in task-oriented chatbot im-\nplementation, meanwhile the state-of-the-art deep learning\narchitecture provides high accuracy on classifying into a\nlarge number of intents. Another feature we contribute is\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nthe application of BERT embeddings as language model to\nautomatically correct small spelling errors in noisy inputs,\nand we show its effectiveness in reducing intent classiﬁca-\ntion errors. The entire end-to-end conversational AI system,\nincluding two machine learning models presented in this\npaper, is developed using open source tools and deployed\nas in-house solution. We believe those discussions provide\nuseful guidance to companies who are motivated to reduce\ndependency on vendors by leveraging state-of-the-art open\nsource AI solutions in their business.\nWe will continue our explorations in this direction, with\nparticular focuses on the following issues: (1) Current ﬁne-\ntuning and decision threshold learning are two separate parts,\nand we will explore the possibility to combine them as a\nnew cost function in BERT model optimization. (2) Dropout\nmethodology applied in our paper belongs to approximated\ninference methods, which is a crude approximation to the ex-\nact posterior learning in parameter space. We are interested\nin a Bayesian version of BERT, which requires a new archi-\ntecture based on variational inference using tools like TFP\nTensorﬂow Probability. (3) Maintaining chatbot produc-\ntion system would need a complex pipeline to continuously\ntransfer and integrate features from deployed model to new\nversions for new business needs, which is an uncharted\nterritory for all of us. (4) Hybridizing ”chitchat” bots, us-\ning state-of-the-art progresses in deep neural models, with\ntask-oriented machine learning models is important for our\npreparation of client self-provisioning service.\n9. Acknowledgement\nWe thank our colleagues in Vanguard CAI (ML-DS team\nand IT team) for their seamless collaboration and support.\nWe thank colleagues in Vanguard Retail Group (IT/Digital,\nCustomer Care) for their pioneering effort collecting and\ncurating all the data used in our approach. We thank Robert\nFieldhouse, Sean Carpenter, Ken Reeser and Brain Heck-\nman for the fruitful discussions and experiments.\nReferences\nAdiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N.,\nThoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G.,\nLu, Y ., and Le, Q. V . Towards a human-like open-domain\nchatbot, 2020.\nAtpaino. Deep-text-corrector. https://github.com/\natpaino/deep-text-corrector, 2017.\nB. Lakshminarayanan, A. Prtizel, C. B. Simple and scalable\npredictive uncertainty estimation using deep ensembles.\npp. 6405, 2017.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. In3rd\nInternational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, 2015.\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,\nD. Weight uncertainty in neural network. In Proceedings\nof the 32nd ICML, pp. 1613–1622, 2015.\nBocklisch, T., Faulkner, J., Pawlowski, N., and Nichol, A.\nRasa: Open source language understanding and dialogue\nmanagement. CoRR, abs/1712.05181, 2017. URL http:\n//arxiv.org/abs/1712.05181.\nColby, K. M., Weber, S., and Hilf, F. D. Artiﬁcial paranoia.\nArtiﬁcial Intelligence, 2(1):125, January 1971. ISSN\n0004-3702.\nCollobert, R. and Weston, J. A uniﬁed architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. In Proceedings of the 25th ICML, pp.\n160167, 2008.\nCollobert, R., Weston, J., Bottou, L., Karlen, M.,\nKavukcuoglu, K., and Kuksa, P. P. Natural language\nprocessing (almost) from scratch. JMLR, 12, 2011. ISSN\n1532-4435.\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\nIn In Proceedings of Advances in Neural Information\nProcessing Systems 28, pp. 3079–3087. 2015.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the NACL, Vol 1, pp. 4171–4186, June 2019.\ndos Santos, C. and Gatti, M. Deep convolutional neural\nnetworks for sentiment analysis of short texts. In Pro-\nceedings of the 25th International Conference on Compu-\ntational Linguistics, pp. 69–78, Dublin, Ireland, August\n2014.\nDos Santos, C. N. and Zadrozny, B. Learning character-level\nrepresentations for part-of-speech tagging. In Proceed-\nings of the 31st International Conference on Machine\nLearning - Volume 32, ICML14, pp. II1818II1826, 2014.\nElman, J. L. Finding structure in time. COGNITIVE SCI-\nENCE, 14(2):179–211, 1990.\net al., C. G. On calibration of modern neural networks.\nvolume 70, 2017.\net al., M. G. Bayesian reinforcement learning. Foundations\nand Trends in Machine Learning, 8(5-6), 2016.\nFedorenko, D. G., Smetanin, N., and Rodichev, A. Avoiding\necho-responses in a retrieval-based conversation system.\nConference on Artiﬁcial Intelligence and Natural Lan-\nguage, pp. 91–97, 2017.\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nGage, P. A new algorithm for data compression. C Users J.,\n12(2):2338, February 1994. ISSN 0898-9788.\nGal, Y . and Ghahramani, Z. Dropout as bayesian approxi-\nmation: representing model uncertainty in deep learning.\nvolume 48, pp. 1050, 2016.\nGao, J., Galley, M., and Li, L. Neural approaches to conver-\nsational ai. In SIGIR ’18, 2018.\nGeifman, Y . Selectivenet: A deep neural network with an\nintegrated reject option. 2019.\nGraves, A. Practical variational inference for neural net-\nworks. In Advances in Neural Information Processing\nSystems 24, pp. 2348–2356. 2011.\nHern´andez-Lobato, J. M. and Adams, R. P. Probabilistic\nbackpropagation for scalable learning of bayesian neural\nnetworks. In Proceedings of the 32nd ICML, Vol 37 ,\nICML15, pp. 18611869, 2015.\nHoward, J. and Ruder, S. Universal language model ﬁne-\ntuning for text classiﬁcation. In Proceedings of the 56th\nAnnual Meeting of the ACL , pp. 328–339, Melbourne,\nAustralia, July 2018.\nHuggingFace. Transformers. https://github.com/\nhuggingface/transformers, 2017.\nKendall, A. and Gal, Y . What uncertainties do we need in\nbayesian deep learning for computer vision? volume 30,\n2017.\nKim, Y ., Jernite, Y ., Sontag, D. A., and Rush, A. M.\nCharacter-aware neural language models. In Proceed-\nings of the Thirtieth AAAI Conference on Artiﬁcial Intel-\nligence, pp. 27412749, 2015.\nLample, G. and Conneau, A. Cross-lingual language model\npretraining. CoRR, abs/1901.07291, 2019.\nLarson, S., Mahendran, A., Peper, J. J., Clarke, C., Lee, A.,\nHill, P., Kummerfeld, J. K., Leach, K., Laurenzano, M. A.,\nTang, L., and Mars, J. An evaluation dataset for intent\nclassiﬁcation and out-of-scope prediction. In Proceedings\nof the 9th EMNLP-IJCNLP, November 2019.\nLepora, N. F. Threshold learning for optimal decision mak-\ning. In Advances in Neural Information Processing Sys-\ntems 29, pp. 3763–3771. 2016.\nLi, C., Stevens, A., Chen, C., Pu, Y ., Gan, Z., and Carin,\nL. Learning weight uncertainty with stochastic gradient\nmcmc for shape classiﬁcation. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pp.\n5666–5675, June 2016.\nLin, Y ., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman,\nW., and Petrov, S. Syntactic annotations for the google\nbooks ngram corpus. In Proceedings of the ACL 2012\nSystem Demonstrations, pp. 169174, USA, 2012.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,\nO., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta:\nA robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019.\nMacKay, D. A practical bayesian framework for backpropa-\ngation networks. volume 4, 1992.\nMaddox, W., Garipov, T., Izmailov, P., Vetrov, D. P., and\nWilson, A. G. A simple baseline for bayesian uncertainty\nin deep learning. Neural Information Processing Systems\n(NeurIPS), 2019.\nMicrosoft. Zo. https://www.zo.ai, 2019.\nMikolov, T., Karaﬁt, M., Burget, L., Cernock, J., and Khu-\ndanpur, S. Recurrent neural network based language\nmodel. volume 2, pp. 1045–1048, 01 2010.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean,\nJ. Distributed representations of words and phrases and\ntheir compositionality. CoRR, abs/1310.4546, 2013.\nNeal, R. Bayesian learning for neural networks. PhD thesis,\n1995.\nPark, C., Kim, J., Ha, S. H., and Lee, J. Sampling-based\nbayesian inference with gradient uncertainty. CoRR,\nabs/1812.03285, 2018.\nPearce, T., Zaki, M., Brintrup, A., and Neely, A. Uncer-\ntainty in neural networks: Bayesian ensembling. ArXiv,\nabs/1810.05546, 2018.\nPeters, M., Ammar, W., Bhagavatula, C., and Power, R.\nSemi-supervised sequence tagging with bidirectional lan-\nguage models. In Proceedings of the 55th ACL, pp. 1756–\n1765, Vancouver, Canada, July 2017.\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. Deep contextualized word\nrepresentations. In Proceedings of the 2018 Conference\nof the NAACL, pp. 2227–2237, New Orleans, Louisiana,\nJune 2018a.\nPeters, M. E., Neumann, M., Zettlemoyer, L., and Yih, W.\nDissecting contextual word embeddings: Architecture\nand representation. CoRR, abs/1808.08949, 2018b.\nRadford, A. and Sutskever, I. Improving language under-\nstanding by generative pre-training. In arxiv, 2018.\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nRao, Q. and Frtunikj, J. Deep learning for self-driving cars:\nChances and challenges. In 2018 IEEE/ACM 1st Inter-\nnational Workshop on Software Engineering for AI in\nAutonomous Systems (SEFAIAS), pp. 35–38, Los Alami-\ntos, CA, USA, may 2018. IEEE Computer Society.\nSchuster, M. and Nakajima, K. Japanese and korean voice\nsearch. In 2012 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pp. 5149–\n5152, March 2012.\nSeedat, N. and Kanan, C. Towards calibrated and scalable\nuncertainty representations for neural networks. ArXiv,\nabs/1911.00104, 2019.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. In Proceed-\nings of the 54th ACL, pp. 1715–1725, Berlin, Germany,\nAugust 2016.\nSerban, I. V ., Sankar, C., Germain, M., Zhang, S., Lin,\nZ., Subramanian, S., Kim, T., Pieper, M., Chandar, S.,\nKe, N. R., Mudumba, S., de Br ´ebisson, A., Sotelo, J.,\nSuhubdy, D., Michalski, V ., Nguyen, A., Pineau, J.,\nand Bengio, Y . A deep reinforcement learning chat-\nbot. CoRR, 2017. URL http://arxiv.org/abs/\n1709.02349.\nSethi, S. The state of chatbots in 2019.\n2019. URL https://hackernoon.com/\nthe-state-of-chatbots-in-2019-d97f85f2294b .\nShridhar, K., Laumann, F., and Liwicki, M. A comprehen-\nsive guide to bayesian convolutional neural network with\nvariational inference. CoRR, abs/1901.02731, 2019.\nTang, R., Lu, Y ., Liu, L., Mou, L., Vechtomova, O., and Lin,\nJ. Distilling task-speciﬁc knowledge from BERT into\nsimple neural networks. CoRR, abs/1903.12136, 2019.\nV . Kuleshov, N. F. and Ermon, S. Accurate uncertainties for\ndeep learning using calibrated regression. 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NIPS, 2017.\nWeizenbaum, J. Elizaa computer program for the study\nof natural language communication between man and\nmachine. 9(1):3645, January 1966. ISSN 0001-0782.\nWelling, M. and Teh, Y . W. Bayesian learning via stochastic\ngradient langevin dynamics. In Proceedings of the 28th\nInternational Conference on International Conference on\nMachine Learning, ICML11, pp. 681688, Madison, WI,\nUSA, 2011. Omnipress. ISBN 9781450306195.\nWorswick, S. Mitsuku. http://www.mitsuku.com,\n2019.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J. G., Salakhut-\ndinov, R., and Le, Q. V . Xlnet: Generalized autore-\ngressive pretraining for language understanding. CoRR,\nabs/1906.08237, 2019.\nZhou, L., Gao, J., Li, D., and Shum, H. The design and\nimplementation of xiaoice, an empathetic social chatbot.\nCoRR, abs/1812.08989, 2018.\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nA. Appendix\nAll extended materials and source code related to this paper\nare avaliable on https://github.com/cyberyu/\nava Our repo is composed of two parts: (1) Extended mate-\nrials related to the main paper, and (2) Source code scripts.\nTo protect proprietary intellectual property, we cannot share\nthe question dataset and proprietary embeddings. We use\nan alternative data set from Larson et al., “An Evaluation\nDataset for Intent Classiﬁcation and Out-of-Scope Predic-\ntion”, EMNLP-IJCNLP 2019, to demonstrate the usage of\ncode.\nA.1. Additional Results for the Main Paper\nSome extended experimental results about MC dropout and\noptimization are presented on github.\nA.1.1. H ISTOGRAM OF UNCERTAINTIES BY DROPOUT\nRATIOS\nWe compare histograms of standard deviations observed\nfrom random samples of predictions. The left side contains\nhistograms generated by 381-class intent models trained for\n10 epochs, with dropout ratio varied from 10 percent to 90\npercent. The right side shows histograms generated by 381\nclass models trained for 30 epochs.\nA.1.2. U NCERTAINTY COMPARISON BETWEEN\n381- CLASS VS 5-CLASS\nTo understand how uncertainties change vs. the number\nof classes in BERT, we train another intent classiﬁer using\nonly Tier 1 labels. We compare uncertainty and accuracy\nchanges at different dropout rates between the original 381-\nclass problem and the new 5-class problem.\nA.1.3. G RID SEARCH FOR OPTIMAL THRESHOLD ON\nDROPOUT\nInstead of using optimization, we use a grid search to ﬁnd\noptimal combinations of average probability threshold and\nstandard deviation threshold. The search space is set as a\n100 x 100 grid on space [0,0] to [1,1], where thresholds vary\nby step of 0.01 from 0 to 1. Applying thresholds to outputs\nof BERT predictions give us classiﬁcations of relevance vs.\nirrelevance questions, and using the same combination of\ntest and irrelevant questions we visualize the F1 score in\ncontour map shown on github repo.\nA.1.4. O PTIMAL THRESHOLD LEARNING ON DROPOUT\n381 CLASSES VS 5 CLASSES\nUsing the same optimization process mentioned in equation\n(2) of the main paper, we compare the optimal results (also\nCPU timing) learned from 381 classes vs. 5 classes.\nA.1.5. S IMPLE ALGORITHM FOR SENTENCE\nCOMPLETION MODEL\nWhen multiple OOVs occur in a sentence, in order to avoid\nthe computational burden using large beamsize to ﬁnd the\noptimal joint probabilities, we assume all candidate words\nfor OOVs are independent, and apply Algorithm 2 one by\none to correct the OOVs.\nA.2. Intent Classiﬁcation Source Code\nA.2.1. BERT EMBEDDINGS MODEL PRETRAINING\nThe jupyter notebook for pretraining embeddings is\nat https://github.com/cyberyu/ava/blob/\nmaster/scripts/notebooks/BERT_PRETRAIN_\nAva.ipynb. Our script is adapted from Denis An-\ntyukhov’s blog “Pre-training BERT from scratch with cloud\nTPU”. We set the VOC SIZE to 32000, and use Sentence-\nPiece tokenizer as approximation of Google’s WordPiece.\nThe learning rate is set to 2e-5, training batch size is 16,\ntraining setps set to 1 million, MAX SEQ LENGTH set to\n128, and MASKED LM PROB is set to 0.15.\nTo ensure the embeddings is training at the right architecture,\nplease make sure the bert conﬁg.json ﬁle referred in the\nscript has the right numbers of hidden and attention layers.\nA.2.2. BERT MODEL TRAINING AND EXPORTING\nThe jupyter notebook for BERT intent classiﬁcation\nmodel training, validation, prediciton and exporting\nis at https://github.com/cyberyu/ava/\nblob/master/scripts/notebooks/BERT_\nrun_classifier_Ava.ipynb. The main script\nrun classiﬁer inmem.py is tweaked from the default\nBERT script run classiﬁer.py, where a new function serv-\ning input fn(): is added. To export that model in the same\ncommand once training is ﬁnished, the ’–do export=true’\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\nneed be set True, and the trained model will be exported to\ndirectory speciﬁed in ’–export dir’ FLAG.\nA.2.3. M ODEL SERVING API S CRIPT\nWe create a jupyter notebook to demonstrate\nhow exported model can be served as in-memory\nclassiﬁer for intent classiﬁcation, located at\nhttps://github.com/cyberyu/ava/scripts/\nnotebooks/inmemory_intent.ipynb. The script\nwill load the entire BERT graph in memory from exported\ndirectory, keep them in memory and provide inference\nresults on new questions. Please notice that in “getSess()”\nfunction, users need to specify the correct exported\ndirectory, and the correct embeddings vocabulary path.\nA.2.4. M ODEL INFERENCE WITH DROPOUT SAMPLING\nWe provide a script that performs Monte Carlo dropout\ninference using in-memory classiﬁer. The script assumes\nthree groups of questions are saved in three separate ﬁles:\ntraining.csv, test.csv, irrelevant.csv. Users need to specify\nthe number of random samples, and prediction probabilities\nresults are saved as corresponding pickle ﬁles. The script is\navailable at https://github.com/cyberyu/ava/\nscripts/dropout_script.py\nA.2.5. V ISUALIZATION OF MODEL ACCURACY AND\nUNCERTAINTY\nThe visualization notebook https://github.com/\ncyberyu/ava/scripts/notebooks/BERT_\ndropout_visualization.ipynb uses output pickle\nﬁles from the previous script to generate histogram\ndistribution ﬁgures and ﬁgures 4(b) and (c).\nA.3. Threshold Optimization Source Code\nA.3.1. T HRESHOLD FOR ENTROPY\nOptimization script ﬁnding best threshold for entropy\nis available at https://github.com/cyberyu/\nava/blob/master/scripts/optimization/\noptimize_entropy_threshold.py. The script\nrequires Python 3.6 and Gurobi 8.1.\nA.3.2. T HRESHOLD FOR MEAN PROBABILITY AND\nSTANDARD DEVIATION\nOptimization script ﬁnding best mean probability thresh-\nold and standard deviation threshold is available at\nhttps://github.com/cyberyu/ava/blob/\nmaster/scripts/optimization/optimize_\ndropout_thresholds.py\nA.4. Sentence Completion Source Code\nThe complete Sentence Completion RESTFUL API\ncode is in https://github.com/cyberyu/ava/\nscripts/sentence_completion/serve.py.\nThe model depends on BertForMaskedLM func-\ntion from Transformer package (ver 2.1.1) to gen-\nerate token probabilities. We use transformers-cli\n(https://huggingface.co/transformers/\nconverting_tensorflow_models.html) to con-\nvert our early pretrained embeddings to PyTorch formats.\nThe input parameters for API are:\n• Input sentence. The usage can be three cases:\n– The input sentence can be noisy (containing mis-\nspelled words) that require auto-correction. As\nshown in the example, the input sentence has\nsome misspelled words.\n– Alternatively, it can also be a masked sentence, in\nthe form of Does it require [MASK] signature for\nIRA signup. [MASK] indicates the word needs\nto be predicted. In this case, the predicted words\nwill not be matched back to input words. Every\nMASKED word will have a separate output of\ntop M predict words. But the main output of the\ncompleted sentence is still one (because it can\nbe combined with misspelled words and cause a\nlarge search) .\n– Alternatively, the sentence can be a complete sen-\ntence, which only needs to be evaluated only for\nPerplexity score. Notice the score is for the entire\nsentence. The lower the score, the more usual the\nsentence is.\n• Beamsize: This determines how many alternative\nchoices the model needs to explore to complete\nthe sentence. We have three versions of functions,\npredict oov v1, predict oov v2 and predict oov v3.\nWhen there are multiple [MASK] signs in a sentence,\nand beamsize is larger than 100, v3 function is used\nas independent correction of multiple OOVs. If beam-\nsize is smaller than 100, v2 is used as joint-probability\nbased correction. If a sentence has only one [MASK]\nsign, v1 (Algorithm 2 in Appendix) is used.\n• Customized V ocabulary: The default vocabulary is the\nencoding vocabulary when the bidirectional language\nmodel was trained. Any words in the sentence that do\nnot occur in vocabulary will be treated as OOV , and\nwill be predicted and matched. If you want to avoid\npredicting unwanted words, you can include them in\nthe customized vocabulary. For multiple words, com-\nbine them with — and the algorithm will split them\ninto list. It is possible to turn off this customized vo-\nA V A: A Financial Service Chatbot based on Deep Bidirectional Transformers\ncabulary during runtime, which simply just put None\nin the parameters.\n• Ignore rule: Sometimes we expect the model to ignore\na range of words belonging to speciﬁc patterns, for\nexample, all words that are capitalized, all words that\nstart with numbers. They can be speciﬁed as ignore\nrules using regular expressions to skip processing them\nas OOV words. For example, expression ”[A-Z]+” tells\nthe model to ignore all uppercase words, so it will not\ntreat ‘IRA’ as an OOV even it is not in the embeddings\nvocabulary (because the embeddings are lowercased).\nTo turn this function off, use None as the parameter.\nThe model returns two values: the completed sentence, and\nits perplexity score.\nA.5. RASA Server Source Code\nThe proposed chatbot utilizes RASA’s open framework to\nintegrate RASA’s “chitchat” capability with our proposed\ncustomized task-oriented models. To achieve this, we set\nup an additional action endpoint server to handle dialogues\nthat trigger customized actions (sentence completion+intent\nclassiﬁcation), which is speciﬁed in actions.py ﬁle. Dia-\nlogue management is handled by RASA’s Core dialogue\nmanagement models, where training data is speciﬁed in sto-\nries.md ﬁle. So, in RASA dialogue model.py ﬁle run core\nfunction, the agent loads two components: nlu interpreter\nand action endpoint.\nThe entire RASA project for chatbot is shared under\nhttps://github.com/cyberyu/ava/bot. Please\nfollow the github guidance in README ﬁle to setup the\nbackend process.\nA.6. Microsoft Teams Setup\nOur chatbot uses Microsoft Teams as front-end to connect to\nRASA backend. We realize setting up MS Teams smoothly\nis a non-trivial task, especially in enterprise controlled env-\niornment. So we shared detailed steps on Github repo.\nA.7. Connect MS Teams to RASA\nAt RASA side, the main tweak to allow MS Team connec-\ntion is at dialogue model.py ﬁle. The BotFrameworkInput\nlibrary needs to be imported, and the correct app id and\napp password speciﬁed in MS Teams setup should be as-\nsigned to initialize RASA InputChannel.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7707699537277222
    },
    {
      "name": "Chatbot",
      "score": 0.6960997581481934
    },
    {
      "name": "Spelling",
      "score": 0.5519405007362366
    },
    {
      "name": "Transformer",
      "score": 0.5266138315200806
    },
    {
      "name": "Language model",
      "score": 0.5163025259971619
    },
    {
      "name": "Intranet",
      "score": 0.45963531732559204
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4310843050479889
    },
    {
      "name": "Natural language processing",
      "score": 0.3422865867614746
    },
    {
      "name": "World Wide Web",
      "score": 0.2841220796108246
    },
    {
      "name": "The Internet",
      "score": 0.227695494890213
    },
    {
      "name": "Engineering",
      "score": 0.11125895380973816
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}