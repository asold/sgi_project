{
    "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
    "url": "https://openalex.org/W4283815156",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5101458399",
            "name": "Jialin Tian",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5009074046",
            "name": "Xing Xu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5074492050",
            "name": "Fumin Shen",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5100397616",
            "name": "Yang Yang",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A5052993469",
            "name": "Heng Tao Shen",
            "affiliations": [
                "Peng Cheng Laboratory",
                "University of Electronic Science and Technology of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6749820799",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3101029400",
        "https://openalex.org/W2962945761",
        "https://openalex.org/W3023628276",
        "https://openalex.org/W1975771248",
        "https://openalex.org/W1974647172",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6743731764",
        "https://openalex.org/W3100436313",
        "https://openalex.org/W6760640297",
        "https://openalex.org/W2997918867",
        "https://openalex.org/W2603445054",
        "https://openalex.org/W2931049280",
        "https://openalex.org/W6743520202",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W2332729875",
        "https://openalex.org/W6758474948",
        "https://openalex.org/W3003735286",
        "https://openalex.org/W6749978463",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2786817236",
        "https://openalex.org/W3027758526",
        "https://openalex.org/W6746034047",
        "https://openalex.org/W2888960966",
        "https://openalex.org/W3111984153",
        "https://openalex.org/W2962856082",
        "https://openalex.org/W3005971801",
        "https://openalex.org/W2885402090",
        "https://openalex.org/W3035705029",
        "https://openalex.org/W2988772405",
        "https://openalex.org/W4309845474",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W3112919685",
        "https://openalex.org/W2922509574",
        "https://openalex.org/W2903978117",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2962898354",
        "https://openalex.org/W3128099838",
        "https://openalex.org/W4297665946",
        "https://openalex.org/W4300089971",
        "https://openalex.org/W2751862348",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W4297940714",
        "https://openalex.org/W4298395628",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2962990451",
        "https://openalex.org/W2963081964",
        "https://openalex.org/W3191326487",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W3118062200"
    ],
    "abstract": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
    "full_text": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere\nLearning for Zero-Shot Sketch-Based Image Retrieval\nJialin Tian1, Xing Xu1*, Fumin Shen1, Yang Yang1, and Heng Tao Shen1,2\n1Center for Future Media and School of Computer Science and Engineering\nUniversity of Electronic Science and Technology of China, China\n2Peng Cheng Laboratory, China\ntian.garin@gmail.com; xing.xu@uestc.edu.cn; fumin.shen@gmail.com; dlyyang@gmail.com; shenhengtao@hotmail.com\nAbstract\nIn this paper, we study the zero-shot sketch-based image re-\ntrieval (ZS-SBIR) task, which retrieves natural images related\nto sketch queries from unseen categories. In the literature,\nconvolutional neural networks (CNNs) have become the de-\nfacto standard and they are either trained end-to-end or used\nto extract pre-trained features for images and sketches. How-\never, CNNs are limited in modeling the global structural in-\nformation of objects due to the intrinsic locality of convo-\nlution operations. To this end, we propose a Transformer-\nbased approach called Three-Way Vision Transformer (TVT)\nto leverage the ability of Vision Transformer (ViT) to model\nglobal contexts due to the global self-attention mechanism.\nGoing beyond simply applying ViT to this task, we propose a\ntoken-based strategy of adding fusion and distillation tokens\nand making them complementary to each other. Speciﬁcally,\nwe integrate three ViTs, which are pre-trained on data of each\nmodality, into a three-way pipeline through the processes of\ndistillation and multi-modal hypersphere learning. The distil-\nlation process is proposed to supervise fusion ViT (ViT with\nan extra fusion token) with soft targets from modality-speciﬁc\nViTs, which prevent fusion ViT from catastrophic forget-\nting. Furthermore, our method learns a multi-modal hyper-\nsphere by performing inter- and intra-modal alignment with-\nout loss of uniformity, which aims to bridge the modal gap be-\ntween modalities of sketch and image and avoid the collapse\nin dimensions. Extensive experiments on three benchmark\ndatasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demon-\nstrate the superiority of our TVT method over the state-of-\nthe-art ZS-SBIR methods.\nIntroduction\nSketch-based image retrieval (SBIR) (Eitz et al. 2010;\nSaavedra, Barrios, and Orand 2015) is a practical problem\nthat the sketch is used as a query to retrieve relevant images\nfrom the gallery. The conventional SBIR scenario assumes\nthat training and testing data come from the distributions of\nthe same categories. Many methods (Sangkloy et al. 2016;\nLiu et al. 2017a) have achieved satisfying performance in\nthis scenario with the help of a large number of annotated\nsamples. However, annotating samples is labor-intensive and\ntime-consuming, as well as these methods perform poorly on\n*Corresponding author\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ndata of unseen classes. Consequently, there has been some\nwork (Shen et al. 2018; Dey et al. 2019) focused on studying\nthe SBIR problem in the zero-shot setting, which assumes\nthat the training class set and the test class set are disjoint.\nSo zero-shot SBIR (ZS-SBIR) is a more challenging prob-\nlem for the inherent modal gap as well as the semantic gap\nbrought by the zero-shot setting.\nSo far, most existing ZS-SBIR methods largely rely on\nconvolutional neural networks (CNNs),i.e., they either ﬁne-\ntune the pre-trained CNNs to extract features and then build\nprojection models to learn a shared embedding space (Dutta\nand Akata 2019; Hwang et al. 2020), or train the whole\nmodel in an end-to-end manner (Liu et al. 2019; Wang et al.\n2021). In spite of the excellent representational power of\nCNNs, these methods are also limited in modeling the global\nstructural information due to the inherent local nature of\nconvolution operations. However, global structural informa-\ntion is essential for the ZS-SBIR task since the only infor-\nmation that the image and sketch together contain is the\nglobal structural information of the object. Vision Trans-\nformer (ViT) has demonstrated that it is a advanced alter-\nnative to the CNN framework, with the global structural in-\nformation modeling capability and exceptional transferabil-\nity. In particular, ViT pre-trained in a self-supervised man-\nner (e.g., DINO (Caron et al. 2021)) surprisingly shows a\nsegmentation property. The sufﬁxes of DINO variants in-\ndicate the model size and input patch size, where DINO-\nS/8 (DINO-B/16) means the “Small” (“Base”) variant with\n8×8 (16×16) patch size (as shown in Fig. 1). We can see\nthat the DINO model explicitly learns the object boundaries\nof images and sketches (Fig. 1(a)), ignoring occlusions and\nbackgrounds. The effect of the segmentation property is also\nreﬂected in the retrieval tasks (Fig. 1(b)): DINO-S/8 outper-\nforms ResNet-50 (He et al. 2016) by a margin in both intra-\nand inter-modal retrieval tasks on unseen data of Sketchy\n(Yelamarthi et al. 2018).\nMotivated by the above observations, we take the ﬁrst\nstep in this paper towards utilizing the global structure mod-\neling capability of ViT for the ZS-SBIR task. Speciﬁcally,\nwe propose a novel approach named Three-Way Vision\nTransformer (TVT), which integrates two modality-speciﬁc\nViTs and a fusion ViT into a three-way pipeline by a token-\nbased strategy. As the general framework of our proposed\nTVT model shown in Fig. 2, the modality-speciﬁc ViTs,\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2370\n(a) Visualization of Attention Maps\n(b) Evaluation of Pre-trained Models on Different Retrieval Tasks\nFigure 1: Illustration of (a) the visualization of the attention\nmaps of images and sketches in the self-attention modules\nfrom the last layer of the latest pre-trained model DINO-\nS/8 (Caron et al. 2021), and (b) the comparison between\npre-trained ResNet-50 and DINO variants under different\ntasks with unseen data of Sketchy (non-overlapping with Im-\nageNet).\npre-trained in a self-supervised manner, is to provide global\nstructural information speciﬁc to each image and sketch for\nfusion ViT. In addition, fusion ViT is the model that an extra\nfusion token is added to interact with other tokens through\nthe self-attention mechanism, whose output is mapped into\na common hypersphere to alleviate the modal gap. Besides,\nwe design a novel method for multi-modal hypersphere\nlearning. The representations of each class are required to\nbe well clustered regardless of modalities (both inter- and\nintra-modal alignment), but representations of each modal-\nity are only individually encouraged to approach the uniform\ndistribution (intra-modal uniformity). As a result, the distri-\nbutions of each class for both modalities are overlapped on\nthe hypersphere, while avoiding collapse in dimensions. Fi-\nnally, the fusion ViT is optimized to perform multi-modal\nhypersphere learning through the fusion token and preserve\nglobal structural information through the distillation tokens.\nExtensive experiments on three benchmark datasets of ZS-\nSBIR verify the superiority of our TVT method.\nWe summarize the main contributions of this work as:\n• To the best of our knowledge, we are the ﬁrst to model\nthe global structural information in the ﬁeld of ZS-SBIR\nusing Vision Transformer, which is critical for the align-\nment between sketches and images.\n• We propose a novel Three-Way Vision Transformer\nmethod termed TVT based on the distillation tokens and\nfusion token. These two types of tokens play the same\nrole as the normal class token, except that the former is\nused for knowledge distillation and the latter for elimi-\nnating the modal gap.\n• We devise a novel multi-modal hypersphere learning pro-\ncess that effectively leverages the representational power\nof the hypersphere by inter- and intra-modal alignment\nand intra-modal uniformity.\nRelated Work\nZero-Shot Sketch-Based Image Retrieval. ZS-SBIR is a\nchallenging task that simultaneously addresses the inherent\nmodal gap and the semantic gap brought by the zero-shot\nsetting. Pioneer work (Shen et al. 2018) ﬁrst studied the\nSBIR problem under the zero-shot setting by cross-modal\nlearning (Shen et al. 2021; Xu et al. 2020b). The subsequent\nwork mainly used ﬁne-tuned pre-trained CNNs to extract\nfeatures and then built projection models to learn a joint em-\nbedding space (Xu et al. 2021, 2020a) with the help of se-\nmantic information, including the generative adversarial net-\nwork (Dutta and Akata 2019), the adversarial network with\nGradient Reversal Layer (Dey et al. 2019), the content-style\ndisentanglement model (Dutta and Biswas 2019), and so on.\n(Yelamarthi et al. 2018; Hwang et al. 2020) adopted varia-\ntional auto-encoder (V AE) to learn latent embedding space\nbut without semantic information. Unlike the above meth-\nods, (Liu et al. 2019) presented a framework that trains CSE-\nResNet-50 (Lu et al. 2018) with knowledge distillation in an\nend-to-end manner, through which features are extracted and\ncross-modal retrieval is conducted. (Wang et al. 2021) im-\nproved this model by tackling the large intra-class diversity\nof sketches with a category-speciﬁc memory bank. However,\nall these methods largely rely on CNNs and are consequently\nlimited in modeling global structural information, which is\ngreatly important for ZS-SBIR. In this paper, we take the\nﬁrst step to use ViT’s global structure modeling capability\nfor the ZS-SBIR task.\nVision Transformer. The architecture of Transformer was\nﬁrstly introduced by (Vaswani et al. 2017) for machine\ntranslation and has currently become the de-facto standard\nfor its tremendous success. Subsequently, several attempts\n(Hu, Shen, and Sun 2018; Wang et al. 2018b; Li et al.\n2019; Ramachandran et al. 2019; Zhang et al. 2020) have\nbeen devoted to adapting the mechanism of Transformer to\nCNNs. More recently, (Dosovitskiy et al. 2021) proposed\na convolution-free method that directly applies Transformer\nto the sequence of image patches, which achieved state-of-\nthe-art results on the image recognition task. (Touvron et al.\n2021) subsequently addressed the problem of ViT requiring\nhuge amounts of data and computation from the perspective\nof knowledge distillation, producing competitive results by\ntraining on ImageNet (Deng et al. 2009) solely. (Caron et al.\n2021) investigated the impact of self-supervised pre-training\nfor ViT and the resulting model showed a superior segmen-\ntation property and performed particularly well with ak-NN\nclassiﬁer alone. In this paper, we leverage the segmentation\nproperty to align sketches and images by captured global\nstructural information.\nRepresentation Learning on the Hypersphere. (Liu\net al. 2017b; Davidson et al. 2018; Xu and Durrett 2018;\nWang et al. 2018a) have shown that learning presentations\non hypersphere performs better than Euclidean space since\nangular information preserves key semantics rather than\nthe magnitude. (Sablayrolles et al. 2019) presented a dif-\nferential entropy regularizer derived from the estimator by\n(Kozachenko and Leonenko 1987), which was subsequently\napplied to image retrieval with contrastive loss by (El-Nouby\n2371\nMLPToken\nImage\nSketch\nImage\nViT\nFusion\nViT\nSketch\nViT\nInput Objective\nFusion ViT Vision \nTransformer\n(a) Fusion ViT (b) Three-Way Vision Transformer\nStop Gradient\nStop Gradient\nProjection + Positional Embedding\nLayer Norm\nMulti-Head \nSelf-Attention \nLayer Norm\nFFN\nPatch Token\nDistillation \nToken\nFusion Token\nFigure 2: The illustration of basic architectures of (a) our proposed fusion ViT with an additional fusion token and (b) Three-\nWay Vision Transformer (TVT), respectively. Images and sketches are fed into the fusion ViT and modality-speciﬁc ViTs,\nwhich are pre-trained in a self-supervised manner. The output of the distillation token of fusion ViT is to predict those of\nmodality-speciﬁc ViTs, while the output of the fusion token is to learn a common hypersphere. For clarity, we have drawn the\ndistillation token of fusion ViT and the associated MLP (multi-layer perceptron) symmetrically twice.\net al. 2021). (Wang and Isola 2020) analyzed the behav-\nior of contrastive learning theoretically and experimentally\nand argued that optimizing contrastive loss is equivalent to\noptimizing the two properties of alignment and uniformity.\nInspired by this research, we propose multi-modal hyper-\nsphere learning to perform intra- and inter-modal alignment\nwith intra-modal uniformity.\nProposed Method\nProblem Deﬁnition\nWe ﬁrst describe the deﬁnition of zero-shot sketch-based\nimage retrieval. The goal of this task is to train a model\non the training images and sketches from seen classes and\nthen apply it to extract common representations of unseen\ndata for retrieval. The training dataset of seen classes is de-\nnoted as Ds = {Is;Ss}, where Is and Ss represent sets\nof natural images and sketches from seen classes Ys, re-\nspectively. Mathematically, they are formulated as Is =\n{(xI\ni;yi)|yi ∈Ys}N1\ni=1 and Ss = {(xS\nj;yj)|yj ∈Ys}N2\nj=1,\nwhere N1 and N2 mean the cardinality of the Is and Ss,\nrespectively. Similarly, the test dataset can be consistently\ndeﬁned as Du = {Iu;Su}for unseen categories Yu. Note\nthat under the zero-shot scenario, the scope of seen and un-\nseen classes are disjoint, i.e., Ys ∩Yu = \u001e. This setting\nimplies that we need to improve the generalization of the\nmodel trained on limited data.\nNetwork Architecture\nThe overall framework of the proposed TVT method is il-\nlustrated in Fig.2. The DINO model g = h◦f is composed\nof a ViT backbonef and an additional projection head h(an\nMLP), whose output is a K-D vector treated as probabili-\nties to achieve self-distillation training. In this way, our TVT\nmodel consists of two modality-speciﬁc ViTs (fI and fS for\nmodalities of image and sketch, respectively) with their cor-\nresponding projection heads (hI and hS) and a fusion ViT\n(fF) with two projection heads (h D and hF for the distil-\nlation and fusion token, respectively). For brevity, we here-\nafter use gI, gS, gD, and gF to denote the joint operations of\ncorresponding fand h. Then, we integrate them into a three-\nway pipeline through the processes of distillation and hyper-\nsphere learning. The distillation process allows the fusion\nViT to reconcile the outputs of hD with those of hI and hS\nto prevent catastrophic forgetting. Furthermore, the hyper-\nsphere learning process aims to learn good representations\nby performing inter- and intra-modal alignment without loss\nof uniformity on the unit hypersphere. It is implemented by\na token-based strategy that adds a new fusion token to the\ninitial embedding, as shown in Fig. 2(a). The detailed proce-\ndure of our TVT method will be described in the remainder\nof this section.\nImage ViT\nFirstly, let us brieﬂy review the mechanism of ViT. It con-\nsists of alternating L layers of multi-head self-attention\n(MSA) and Feed-Forward Network (FFN) blocks. Both\nMSA and FFN layers contain “pre-norm” layer normaliza-\ntion and are paralleled with skip connections. ViT takes as\ninput a sequence of image patches of ﬁxed resolution n×n.\nThese patches are then linearly projected and added a learn-\n2372\nable positional embedding to form a sequence of vector-\nshaped tokens. An extra learnable class token is incorporated\ninto the sequence to aggregate information from other tokens\nsuch that it serves as a global image description. We refer to\n(Vaswani et al. 2017) for the basic theory of Transformer and\n(Dosovitskiy et al. 2021) for its adaptation to vision tasks.\nIn this paper, we choose the DINO-S/8 variant (Caron\net al. 2021) as the basic architecture for the sake of its excel-\nlent performance (shown in Fig. 1) and compact model size,\nwhich is even less than ResNet-50 in terms of parameters\ncount. Since the class token of DINO is not attached to any\nlabel nor supervision and is instead used for distillation, we\nrenamed it to distillation token to avoid ambiguity.\nSketch ViT\nSince the original DINO-S/8 model is not trained on\nsketches data, we ﬁrstly ﬁne-tune it on sketches of seen cat-\negories in the same self-supervised manner as DINO. No-\ntably, the labels of sketches are excluded from this process to\navoid the loss of the pre-trained model’s generalization. This\nis because the sketch only abstractly depicts the structural\ninformation of the object, without the complex textures and\nbackground variations like an image. If we ﬁne-tune the pre-\ntrained model based on the supervised signal, this inevitably\nresults in modality-speciﬁc overﬁtting, which is detrimental\nto the subsequent training process.\nMore speciﬁcally, we utilize the multi-crop strategy to\ngenerate a set V of various views for each sketch, which\nconsists of two global views (xS\ng;1 and xS\ng;2) with a resolution\nof 2242 and ten local views with a resolution of 962. Then,\nwe build the teacher-student architecture that the teacher\nand student are both initialized from the same pre-trained\nweights. Speciﬁcally, the optimization follows the “local-to-\nglobal” strategy by feeding all views of V into the student\nwhile only feeding xS\ng;1 and xS\ng;2 into the teacher. We denote\nby Zt, \u001ct, and \u0012t (Zs, \u001cs, and \u0012s) the logits, temperature,\nand parameters for the teacher (student) and  the softmax\noperation. Finally, the objective can be formulated as:\nmin\n\u0012s\n∑\nx∈{xS\ng;1;xS\ng;2}\n∑\nx02V\nx06=x\nKL ( (Zt(x)=\u001ct) ; (Zs(x′) =\u001cs)) ;\n(1)\nwhere \u0012t = \u0010\u0012t + (1−\u0010)\u0012s is updated by the exponential\nmoving average of \u0012s and taken as the sketch ViT for the\nsubsequent training.\nFusion ViT\nDistillation through Tokens. After obtaining two\nmodality-speciﬁc ViTs with associated heads, we start to\ntrain fusion ViT with supervision from them. Since the two\nmodality-speciﬁc ViTs are pre-trained in a self-supervised\nmanner, they are encouraged to discover global structural\ninformation speciﬁc to each image and sketch. However, the\nfusion ViT aims to reduce the modal gap between images\nand sketches of the same category, which will inevitably\nrequire the model to pay more attention to the more dis-\ncriminative local structures shared by the whole category,\ngradually forgetting the structural information speciﬁc\nto each instance. Therefore, we avoid this catastrophic\nforgetting phenomenon by knowledge distillation. Given\na batch of N images, we reconcile the probability vectors\ngiven by hD with those of hI through the distillation tokens,\nwhich is formulated as follows:\nLI\nKD =\nN∑\ni=1\nKL\n(\n \n(\ngI(xI\ni)=\u001ct\n)\n; \n(\ngD(xI\ni)=\u001cs\n))\n; (2)\nwhere \u001ct, \u001cs, and  are the same as deﬁned previously. Simi-\nlarly, LS\nKD is the knowledge distillation loss for the modality\nof sketch. Then, we deﬁne LKD as follows:\nLKD = LI\nKD + LS\nKD: (3)\nIn this way, we prevent our model from reducing the rich\nvisual information to a limited number of concepts se-\nlected from the thousands of object classes acquired by pre-\ntraining.\nInter- and Intra-Modal Alignment. As shown in Fig.\n2(b), the fusion tokens of images and sketches are jointly\nprojected into a unit hypersphere in which the images and\nsketches of the same class are expected to be well clustered.\nWhen all classes are well clustered, they are linearly sep-\narable in the hypersphere space. Therefore, we classify the\nsamples using a linear classiﬁer:\nLCLS = −E [log P(yi |gF(xi); \u0012c)] ; (4)\nwhere xi can be an image or a sketch, \u0012c is the parameters\nof the shared classiﬁer. Consequently, LCLS can perform\nintra-modal alignment as well as inter-modal alignment by\nthe shared classiﬁer. We also propose a center alignment loss\nthat explicitly requires the distributions of sketches and im-\nages to overlap on the hypersphere:\nc∗\nyi = \u0015c∗\nyi + (1−\u0015)\nNyi∑\nj=1\n[\ngF\n(\nx∗\nj\n)]\n;\nc∗\nyi = c∗\nyi\nc∗yi\n\n2\n; ∗∈{I;S };\nLCA =\n∑\ni∈Ys\n(cI\ni −cS\ni )2:\n(5)\nHere \u0015is the weight of the exponential moving average.Nyi\nis the number of samples x∗\nj with the label yi in the batch.\nI and S indicate the modalities of image and sketch. The\ncenters are l2-normalized to map back to the hypersphere.\nEquipped with LCLS and LCA, we align the distributions\nof the bi-modal data from both inter-modal and intra-modal\naspects.\nIntra-Modal Uniformity. Both alignment and uniformity\nare key properties of representations in the hypersphere,\nwhere uniformity implies an efﬁcient use of the represen-\ntational power of the hypersphere. Speciﬁcally, we adopt the\naverage Gaussian potential to encourage the uniformity of\nsketches or images:\nGt(x∗\ni;x∗\nj; gF) =e−t∥gF (x\u0003\ni )−gF (x\u0003\nj )∥2\n2 ;\nL∗\nUNI = logE\n[\nGt(x∗\ni;x∗\nj; gF)\n]\n; ∗∈{I;S};\nLUNI = LI\nUNI + LS\nUNI;\n(6)\n2373\nAlgorithm 1: Overall training procedure of TVT.\nPhase 1: Fine-tuning sketch ViT.\nInput: Ss = {(xS\nj;yj)|yj ∈Ys}N2\nj=1, batch size N, expo-\nnential moving average \u0010.\nOutput: Model Parameters \u0012t\n1: Build a teacher-student architecture (\u0012t and \u0012s).\n2: repeat\n3: Sample a batch of sketches.\n4: Update \u0012s using Adam optimizer with Eq. 1.\n5: \u0012t = \u0010\u0012t + (1−\u0010)\u0012s.\n6: until Reach maximum iterations.\n7: Take \u0012t as parameters of Sketch ViT.\nPhase 2: Training TVT.\nInput: Is = {(xS\ni ;yi)|yi ∈Ys}N1\ni=1, Ss = {(xS\nj;yj)|yj ∈\nYs}N2\nj=1, batch sizeN, image ViTgI, sketch ViTgS, learning\nrate \u0016, hyper-parameters \u0015, \u00151, \u00152.\nOutput: Model parameters \u0012fF , \u0012hF , \u0012hD .\n1: Build the three-way pipeline.\n2: repeat\n3: Sample a batch of images and sketches.\n4: Compute the objective L← Eq. 7.\n5: \u0012fF ← \u0012fF −0:1 ∗\u0016O\u0012fF\nL.\n6: \u0012h\u0003 ← \u0012h\u0003−\u0016O\u0012h\u0003L, ∗∈{F;D}.\n7: until Reach maximum iterations.\n8: Take trained fusion ViT to conduct ZS-SBIR.where t is a ﬁxed parameter. It is worth noting that LUNI\nis separately applied on the distribution of each modal-\nity, rather than constraining all representations regardless of\nmodalities. Such a design is reasonable because the distri-\nbutions of both modalities on the hypersphere are expected\nto approach the uniform distribution, but with overlapping\npositions learned by inter- and intra-modal alignment.\nOverall Objective. Finally, the overall objective of the fu-\nsion ViT is the linear combination of the four losses as:\nL= LKD + LCLS + \u00151LCA + \u00152LUNI; (7)\nwhere \u00151 and \u00152 are the hyper-parameters of center align-\nment loss and uniformity loss. The training procedure of our\nTVT method is shown in Algorithm 1.\nExperiments\nExperimental Setup\nDatasets. We verify the effectiveness of our TVT method\non three benchmark datasets of SBIR, i.e., Sketchy (Sangk-\nloy et al. 2016), TU-Berlin (Eitz et al. 2010), and Quick-\nDraw (Dey et al. 2019). Sketchy is originally composed of\n75,471 sketches and 12,500 natural images from 125 classes.\nThen (Liu et al. 2017a) extended this dataset with addi-\ntional 60,502 images, so yielding in total 73,002 images.\nTU-Berlin consists of sketches of 250 categories, with 80\nsketches each. It is extended by the collection of 204,489\nimages provided by (Liu et al. 2017a).QuickDraw contains\n330,000 sketches and 204,000 images from 110 classes,\nwhich makes it the largest dataset among three datasets with\nthe most abstract sketches drawn by the amateur.\nEvaluation Setting. There are two kinds of seen and un-\nseen class divisions for Sketchy: the one proposed by (Liu\net al. 2017a) randomly selects 25 classes as unseen classes,\nwhile the one proposed by (Yelamarthi et al. 2018) selects\nclasses that do not overlap with ImageNet categories as un-\nseen classes. For simplicity, we refer to the former one as\nSketchy and the latter one as Sketchy-NO. TU-Berlin is\nsimilar to Sketchy in that 30 randomly selected classes are\nused as unseen classes. However, QuickDraw is similar to\nSketchy-NO in that it selects 30 classes that do not overlap\nwith ImageNet categories as unseen classes. The output of\nthe fusion token is taken as the retrieval feature. In addition,\nwe binarize the real features by iterative quantization (ITQ)\n(Gong et al. 2012) for comparison. The cosine and hamming\ndistance metrics are used to compute the similarities for real\nand binary embedding, respectively.\nImplementation Details. We implement TVT with the\npopular PyTorch toolkit. For our network architecture, the\nViTs consist of 12 Transformer blocks (an MSA and an\nFFN) with 6 heads in multi-head self-attention. The pro-\njection heads contain three fully connected layers with di-\nmensions [2048, 2048, 256] followed by l2 normalization\nand an additional output layer for distillation (ﬁxed 65536-\nD) or classiﬁcation. We train the model in 50 epochs with\nAdam optimizer with weight decay that is initially 0.04 and\nis ramped up to 0.4 by a cosine schedule. The batch size of\n512 samples is distributed over two GPUs with 16 steps of\ngradient accumulation. The base value of the learning rate\u0016\nis set to 0.0005 * (batch size/256). \u0016is raised linearly to the\nbase value during the ﬁrst 5 epochs and is decayed to 1e-6 by\na cosine schedule as well. Especially, the learning rates are\nset to 0.1*\u0016for the ViTs but \u0016for the projection heads. The\ntemperature \u001cs is always 0.1 while\u001ct increases linearly from\n0.04 to 0.07 during the initial 5 epochs. We follow the data\naugmentations of DINO (Caron et al. 2021), which consist\nof color jittering, Gaussian blur, and solarization. The ﬁxed\nt in Eq. 6 is set to 2 according to (Wang and Isola 2020).\nWhat’s more, sketch ViT and class centers are updated with\n\u0010= 0.996 and\u0015= 0.9, respectively. Finally,\u00151 and \u00152 are set\nto 2.0 and 0.5 in all experiments, unless speciﬁed otherwise.\nFurther implementation codes and additional experimental\nanalyses can be found in the supplementary material.\nComparison with the State-of-the-Arts\nWe compare our TVT method with 10 state-of-the-art meth-\nods relevant to the ZS-SBIR task, including CAAE (Yela-\nmarthi et al. 2018), CV AE (Yelamarthi et al. 2018), SEM-\nPCYC (Dutta and Akata 2019), Dey et al. (Dey et al. 2019),\nSAKE (Liu et al. 2019), IIAE (Hwang et al. 2020), LCALE\n(Lin et al. 2020), OCEAN (Zhu et al. 2020), and DSN (Wang\net al. 2021). We report the results on Sketchy-NO, Sketchy,\nand TU-Berlin in Table 1 and the results on QuickDraw in\nTable 2. Since IIAE and DSN are the two latest competi-\ntive approaches, we implement them according to their pub-\nlic codes and instructions, and we report their results on\nSketchy-NO, in addition to IIAE on TU-Berlin.\nAs we can see, our TVT method shows a consistent\nand signiﬁcant improvement over all of the state-of-the-art\n2374\nMethods Dim Sketchy-NO Sketchy TU-Berlin\nmAP@200 Prec@200 mAP@all Prec@100 mAP@all Prec@100\nCAAE (ECCV’2018) 4096 0.156 0.260 0.196 0.284 - -\nCV AE (ECCV’2018) 4096 0.225 0.333 - - 0.005 0.001\nZSIH (CVPR’2018) 64 - - 0.254 0.340 0.220 0.291\nSEM-PCYCb (CVPR’2019) 64 - - 0.344 0.399 0.293 0.392\nSEM-PCYC (CVPR’2019) 64 - - 0.349 0.463 0.297 0.426\nDey et al. (CVPR’2019) 256 0.369 0.370 - - 0.110 0.121\nSAKEb (ICCV’2019) 64 0.356 0.477 0.364 0.487 0.359 0.481\nSAKE (ICCV’2019) 512 0.497 0.598 0.547 0.692 0.475 0.599\nLCALE (AAAI’2020) 64 - - 0.476 0.583 - -\nOCEAN (ICME’2020) 64 - - 0.462 0.590 0.333 0.467\nIIAE (NeurIPS’2020) 64 0.373 0.485 0.573 0.659 0.412 0.503\nDSNb (IJCAI’2021) 64 0.367 0.481 0.436 0.553 0.385 0.497\nDSN (IJCAI’2021) 512 0.501 0.597 0.583 0.704 0.481 0.586\nTVTb (Ours) 64 0.447 0.554 0.553 0.727 0.396 0.606\nTVT (Ours) 384 0.531 0.618 0.648 0.796 0.484 0.662\nTable 1: Comparison of our method and 10 compared approaches on Sketchy and TU-Berlin. The subscript “b” denotes results\nobtained by binary hashing codes, and “-” means that the results are not reported in the original papers. “Sketchy-NO” is short\nfor Sketchy with non-overlapping classes. The best and second-best results are marked in bold and underlined, respectively.\n(SOTA) methods. Most of ZS-SBIR methods only experi-\nmented on Sketchy and TU-Berlin, which share the same\nway of randomly selecting unseen classes. Speciﬁcally, on\nthese two datasets, TVT consistently beats the SOTA (DSN)\nwith 11.1% and 0.5% improvements of mAP@all scores,\nrespectively. However, few of them experimented on more\nrealistic and challenging datasets: Sketchy-NO and Quick-\nDraw guarantee that the unseen classes do not overlap\nwith ImageNet, in addition to QuickDraw being a very\nlarge dataset. On Sketchy-NO, our approach improves the\nmAP@200 score from 0.501 to 0.531 compared with DSN.\nMoreover, on the large-scale QuickDraw, it achieves a huge\nimprovement of almost 100% mAP@all score. Given the\nlarge-scale nature of these datasets and the limitation of the\nﬁxed class splits, these results effectively prove that the dra-\nmatic improvement of our method is not by chance or by\nsplit bias. Compared with hashing methods, our method also\ngets the best results. When we compare the results using\nmetrics that consider only top k candidates, the improvement\nachieved by our method is more pronounced. On Sketchy\nand TU-Berlin, our approach surpasses DSN with 13.1%\nand 13.0% improvements of Prec@100 scores, respectively.\nOn QuickDraw, it gains increases of 112.2% and 330.9%\nMethods QuickDraw\nmAP@all mAP@200 Prec@200\nCV AE 0.003 0.006 0.003\nDey et al. 0.075 0.090 0.068\nTVT (Ours) 0.149 0.191 0.293\nTable 2: Overall comparison of TVT and 2 compared ap-\nproaches on large-scale QuickDraw. The best results are\nshown in bold.\nof mAP@200 and Prec@200 scores, respectively. These re-\nsults mean that the true positive examples have a higher\nprobability of appearing in the top 100 (or 200) retrieved\nresults, which is well suited to the retrieval task.\nAll these comparisons can demonstrate that our method\ncan effectively align intra- and inter-modal distributions\nwithout loss of uniformity and then achieves satisfactory\ngeneralization on unseen classes.\nFurther Analysis on TVT\nAblation Study. We ﬁrst investigate the effect of each loss\nterm in Eq. 7 by ablating it in Eq. 7 in the training phase. The\nresults of these variants, the full TVT and pre-trained DINO-\nS/8 on Sketchy and TU-Berlin are shown in Table 3, where\n“w/o” means the ablating behavior.\nFrom the comparison of these models, we can draw the\nfollowing conclusions: 1) TVT w/o LCLS performs worse\nthan the other variants as it fails to consider inter-modal\nalignment. However, it is better than DINO-S/8, demonstrat-\ning that the center alignment and three-way training pipeline\ncan align inter-modal distributions to some extent. 2) The\nperformance of TVT w/o LKD shows that learning by fo-\nModels Sketchy TU-Berlin\nDINO-S/8 0.101 0.084\nTVT w/o LCLS 0.286 0.244\nTVT w/o LKD 0.599 0.452\nTVT w/o LCA 0.630 0.476\nTVT w/o LUNI 0.634 0.479\nFull TVT 0.648 0.484\nTable 3: Ablation results (mAP@all) for each loss term on\nSketchy and TU-Berlin. The best results are shown in bold.\n2375\nparachute\nhot_air_balloon\nlighter\nbread\nQuery Top 10 Retrieved Candidates\nFigure 3: Retrieval examples of ZS-SBIR results on unseen\ndata of TU-Berlin.\ncusing only on the fusion ViT will inevitably lead to catas-\ntrophic forgetting, which clariﬁes the need for three-way\ntraining through distillation tokens. 3) The results of TVT\nw/o LCA indicate that explicitly required overlap of class\ncenters on the hypersphere facilitates the elimination of the\nmodal gap. 4) The results of TVT w/o LUNI suggest that\nuniformity effectively prevents the reduction of generaliza-\ntion caused by dimensional collapse on the hypersphere. 5)\nThe full model achieves the best results with both advan-\ntages of knowledge distillation and hypersphere learning.\nQualitative Analysis. Fig. 3 shows the top 10 retrieved\ncandidates of sketches queries, where correct and incorrect\ncandidates are marked with checkmarks and crosses, respec-\ntively. Our model successfully retrieves the correct candi-\ndates in most cases, except for some structurally similar in-\ncorrect candidates. For example, the hot air balloons (penul-\ntimate row) are so similar to the parachutes in structure and\nbackground that they are retrieved incorrectly.\nFig. 4 visualizes the distributions of seen and unseen data\nof Sketchy by t-SNE (Van der Maaten and Hinton 2008). We\ncan see that the seen data are well clustered together regard-\nless of modalities, but with a certain degree of uniformity.\nBesides, all classes are separated by proper distances. The\nunseen data are not involved in the training, but they are also\nable to cluster together at relatively small distances based on\nthe classes.\nAnalysis on Parameter Sensitivity. As shown in Fig.\n5, we analyze the effect of center alignment and unifor-\nmity with varying hyper-parameters \u00151 and \u00152 in Eq. 7 on\nSketchy and TU-Berlin. We can observe that the effect of\ncenter alignment is less inﬂuenced by \u00151 and reach the peak\nat \u00151 = 2. However, the effect of uniformity shows a dif-\nferent trend: it accelerates the deterioration of the retrieval\nresults when \u00152 grows too large. It indicates the different\nimportance of alignment and uniformity.\n(a) Seen data\n (b) Unseen data\nFigure 4: The t-SNE visualization for seen and unseen data\nof Sketchy, where the colored circles (•) and upper trian-\ngles (N) represent images and sketches (zoom in for better\nviewing), respectively.\nConclusions\nIn this paper, we took the ﬁrst step to leverage ViT to model\nthe global structure of objects, which is essential for ZS-\nSBIR. We ﬁrstly proposed a novel yet effective Three-Way\nVision Transformer that integrates modality-speciﬁc ViTs\nand our proposed fusion ViT into a three-way pipeline.\nThen, we trained the fusion ViT by a devised token-based\nstrategy with distillation, which aims to prevent catastrophic\nforgetting, and multi-modal hypersphere learning, which en-\ncourages the representations to be well clustered without\nloss of uniformity according to their class. We conducted ex-\ntensive experiments on three benchmark datasets to demon-\nstrate the superiority of our approach and establish new\nstate-of-the-art performance. In the future, we will investi-\ngate the performance of our approach on other multi-modal\nmulti-view datasets.\nAcknowledgments\nThis work was supported in part by Meituan; Sichuan Sci-\nence and Technology Program, China (No. 2019ZDZX0008,\n2020YFS0057); National Natural Science Foundation of\nChina under Grants (No. 61976049, 62072080 and\nU20B2063). Jialin Tian was with the internship in Meituan\nwhen this work was performed.\n0.0 0.25 0.5 1.0 2.0 4.0 16.0\nweights\n50\n55\n60\n65mAP@all (%)\n1\n2\n(a) Sketchy\n0.0 0.25 0.5 1.0 2.0 4.0 16.0\nweights\n40\n42\n44\n46\n48\n50mAP@all (%)\n1\n2 (b) TU-Berlin\nFigure 5: The mAP@all scores on Sketchy and TU-Berlin\nwith different values of \u00151 and \u00152 for center alignment and\nuniformity, respectively.\n2376\nReferences\nCaron, M.; Touvron, H.; Misra, I.; J´egou, H.; Mairal, J.; Bo-\njanowski, P.; and Joulin, A. 2021. Emerging Properties in\nSelf-Supervised Vision Transformers. arXiv:2104.14294.\nDavidson, T. R.; Falorsi, L.; De Cao, N.; Kipf, T.; and\nTomczak, J. M. 2018. Hyperspherical Variational Auto-\nEncoders. 34th Conference on Uncertainty in Artiﬁcial In-\ntelligence (UAI-18).\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDey, S.; Riba, P.; Dutta, A.; Llados, J.; and Song, Y .-Z. 2019.\nDoodle to search: Practical zero-shot sketch-based image re-\ntrieval. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2179–2188.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv:2010.11929.\nDutta, A.; and Akata, Z. 2019. Semantically tied paired cy-\ncle consistency for zero-shot sketch-based image retrieval.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 5089–5098.\nDutta, T.; and Biswas, S. 2019. Style-Guided Zero-Shot\nSketch-based Image Retrieval. In British Machine Vision\nConference 2019, 209–213.\nEitz, M.; Hildebrand, K.; Boubekeur, T.; and Alexa, M.\n2010. An evaluation of descriptors for large-scale image re-\ntrieval from sketched feature lines. Computers & Graphics,\n34(5): 482–498.\nEl-Nouby, A.; Neverova, N.; Laptev, I.; and J ´egou, H.\n2021. Training Vision Transformers for Image Retrieval.\narXiv:2102.05644.\nGong, Y .; Lazebnik, S.; Gordo, A.; and Perronnin, F. 2012.\nIterative quantization: A procrustean approach to learning\nbinary codes for large-scale image retrieval. IEEE transac-\ntions on pattern analysis and machine intelligence, 35(12):\n2916–2929.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHu, J.; Shen, L.; and Sun, G. 2018. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7132–7141.\nHwang, H.; Kim, G.-H.; Hong, S.; and Kim, K.-E. 2020.\nVariational Interaction Information Maximization for Cross-\ndomain Disentanglement. Advances in Neural Information\nProcessing Systems, 33.\nKozachenko, L.; and Leonenko, N. N. 1987. Sample esti-\nmate of the entropy of a random vector.Problemy Peredachi\nInformatsii, 23(2): 9–16.\nLi, X.; Wang, W.; Hu, X.; and Yang, J. 2019. Selective ker-\nnel networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 510–519.\nLin, K.; Xu, X.; Gao, L.; Wang, Z.; and Shen, H. T. 2020.\nLearning Cross-Aligned Latent Embeddings for Zero-Shot\nCross-Modal Retrieval. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, 11515–11522.\nLiu, L.; Shen, F.; Shen, Y .; Liu, X.; and Shao, L. 2017a.\nDeep sketch hashing: Fast free-hand sketch-based image re-\ntrieval. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2862–2871.\nLiu, Q.; Xie, L.; Wang, H.; and Yuille, A. L. 2019. Semantic-\naware knowledge preservation for zero-shot sketch-based\nimage retrieval. In Proceedings of the IEEE International\nConference on Computer Vision, 3662–3671.\nLiu, W.; Zhang, Y .-M.; Li, X.; Liu, Z.; Dai, B.; Zhao, T.; and\nSong, L. 2017b. Deep Hyperspherical Learning. In NIPS,\n3953–3963.\nLu, P.; Huang, G.; Fu, Y .; Guo, G.; and Lin, H. 2018. Learn-\ning large euclidean margin for sketch-based image retrieval.\narXiv preprint arXiv:1812.04275.\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Lev-\nskaya, A.; and Shlens, J. 2019. Stand-Alone Self-Attention\nin Vision Models. In Wallach, H.; Larochelle, H.; Beygelz-\nimer, A.; d'Alch ´e-Buc, F.; Fox, E.; and Garnett, R., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 32. Curran Associates, Inc.\nSaavedra, J. M.; Barrios, J. M.; and Orand, S. 2015. Sketch\nbased Image Retrieval using Learned KeyShapes (LKS). In\nProceedings of the British Machine Vision Conference 2015,\nvolume 1, 1–11.\nSablayrolles, A.; Douze, M.; Schmid, C.; and J ´egou, H.\n2019. Spreading vectors for similarity search. In Interna-\ntional Conference on Learning Representations.\nSangkloy, P.; Burnell, N.; Ham, C.; and Hays, J. 2016. The\nsketchy database: learning to retrieve badly drawn bunnies.\nACM Transactions on Graphics (TOG), 35(4): 1–12.\nShen, H. T.; Liu, L.; Yang, Y .; Xu, X.; Huang, Z.; Shen, F.;\nand Hong, R. 2021. Exploiting Subspace Relation in Seman-\ntic Labels for Cross-Modal Hashing. IEEE Transactions on\nKnowledge and Data Engineering, 33(10): 3351–3365.\nShen, Y .; Liu, L.; Shen, F.; and Shao, L. 2018. Zero-shot\nsketch-image hashing. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 3598–\n3607.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of Machine Learning Research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\n2377\nWang, H.; Wang, Y .; Zhou, Z.; Ji, X.; Gong, D.; Zhou, J.; Li,\nZ.; and Liu, W. 2018a. Cosface: Large margin cosine loss\nfor deep face recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 5265–\n5274.\nWang, T.; and Isola, P. 2020. Understanding contrastive rep-\nresentation learning through alignment and uniformity on\nthe hypersphere. In International Conference on Machine\nLearning, 9929–9939. PMLR.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018b. Non-\nlocal neural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 7794–\n7803.\nWang, Z.; Wang, H.; Yan, J.; Wu, A.; and Deng, C. 2021.\nDomain-Smoothing Network for Zero-Shot Sketch-Based\nImage Retrieval. arXiv:2106.11841.\nXu, J.; and Durrett, G. 2018. Spherical Latent Spaces for\nStable Variational Autoencoders. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Lan-\nguage Processing.\nXu, X.; Lin, K.; Yang, Y .; Hanjalic, A.; and Shen, H. 2021.\nJoint Feature Synthesis and Embedding: Adversarial Cross-\nmodal Retrieval Revisited. IEEE Transactions on Pattern\nAnalysis Machine Intelligence.\nXu, X.; Lu, H.; Song, J.; Yang, Y .; Shen, H. T.; and\nLi, X. 2020a. Ternary Adversarial Networks With Self-\nSupervision for Zero-Shot Cross-Modal Retrieval. IEEE\nTransactions on Cybernetics, 50(6): 2400–2413.\nXu, X.; Wang, T.; Yang, Y .; Zuo, L.; Shen, F.; and Shen,\nH. T. 2020b. Cross-Modal Attention With Semantic Con-\nsistence for Image-Text Matching. IEEE Trans. Neural Net-\nworks Learn. Syst., 31(12): 5412–5425.\nYelamarthi, S. K.; Reddy, S. K.; Mishra, A.; and Mittal, A.\n2018. A zero-shot framework for sketch based image re-\ntrieval. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 300–317.\nZhang, H.; Wu, C.; Zhang, Z.; Zhu, Y .; Lin, H.; Zhang,\nZ.; Sun, Y .; He, T.; Mueller, J.; Manmatha, R.; Li, M.;\nand Smola, A. 2020. ResNeSt: Split-Attention Networks.\narXiv:2004.08955.\nZhu, J.; Xu, X.; Shen, F.; Lee, R. K.-W.; Wang, Z.; and Shen,\nH. T. 2020. Ocean: A Dual Learning Approach For Gener-\nalized Zero-Shot Sketch-Based Image Retrieval. In 2020\nIEEE International Conference on Multimedia and Expo\n(ICME), 1–6. IEEE.\n2378"
}