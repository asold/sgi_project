{
  "title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",
  "url": "https://openalex.org/W4389519033",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126740995",
      "name": "Kaiyan Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1903586662",
      "name": "Ning Ding",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2899379175",
      "name": "Qi Biqing",
      "affiliations": [
        "Tsinghua University",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132717116",
      "name": "Xuekai Zhu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2532862397",
      "name": "Xinwei Long",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2110030736",
      "name": "Bowen Zhou",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4307206164",
    "https://openalex.org/W4385565413",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4287022992",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3035617116",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4310999088",
    "https://openalex.org/W4286892891",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W3210129272",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W3034487470",
    "https://openalex.org/W4317601315",
    "https://openalex.org/W4401042420",
    "https://openalex.org/W2777662428",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385573069",
    "https://openalex.org/W4327526719",
    "https://openalex.org/W2799069271",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4308558317",
    "https://openalex.org/W3101284630",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W4320165905",
    "https://openalex.org/W4385570319",
    "https://openalex.org/W3104136798",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4376167570",
    "https://openalex.org/W3202088367",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W4281806276",
    "https://openalex.org/W4289147263",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4287854684",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W4224950634",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W2996074092",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W3173528555",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W3136363192",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W4379958490",
    "https://openalex.org/W4282961290",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4390189971",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3199348444",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3123806455",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3018102029"
  ],
  "abstract": "Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9612–9637\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without\nFull Large Language Model\nKaiyan Zhang1, Ning Ding1,2, Biqing Qi1,2,3, Xuekai Zhu1\nXinwei Long1, Bowen Zhou1,2∗\n1 Department of Electronic Engineering, Tsinghua University, Beijing, China\n2 Frontis.AI, Beijing, China\n3 School of Astronautics, Harbin Institute of Technology, Harbin, China\nzhang-ky22@mails.tsinghua.edu.cn\nzhoubowen@tsinghua.edu.cn\nAbstract\nInstruction tuning has recently been recognized\nas an effective way of aligning Large Language\nModels (LLMs) to enhance their generalization\nability across various tasks. However, when\ntuning publicly accessible, centralized LLMs\nwith private instruction data, privacy concerns\nare inevitable. While direct transfer of parame-\nterized modules between models is a plausible\napproach to address this, its implications and\neffectiveness need further exploration. This\npaper focuses on Offsite-Tuning (OFT), a rep-\nresentative technique that transfers transformer\nblocks between centralized LLMs and down-\nstream emulators. Given the limited under-\nstanding of the underlying mechanism of OFT,\nwe perform an empirical analysis on LLMs\nfrom the perspectives of representation and\nfunctional similarity. Interestingly, our find-\nings reveal a unique modular structure within\nthe layers of LLMs that appears to emerge as\nthe model size expands. Simultaneously, we\nnote subtle but potentially significant changes\nin representation and intermediate predictions\nacross the layers. Inspired by these observa-\ntions, we propose CRaSh, involving Clustering,\nRemoving, and Sharing, a training-free strat-\negy to derive improved emulators from LLMs.\nCRaSh significantly boosts performance of\nOFT with billions of parameters. Furthermore,\nwe investigate the optimal solutions yielded\nby fine-tuning with and without full model\nthrough the lens of loss landscape. Our findings\ndemonstrate a linear connectivity among these\noptima falling over the same basin, thereby\nhighlighting the effectiveness of CRaSh and\nOFT. The source code is publicly available at\nhttps://github.com/TsinghuaC3I/CRaSh.\n1 Introduction\nNowadays, there is a growing interest in large lan-\nguage models (LLMs) such as PaLM (Chowdhery\net al., 2022), LLaMA (Touvron et al., 2023) and\n∗Corresponding author\n0 3 6 9 12\n129630\nLayer\n125M\n0 5 10 15 20\n20151050\n350M\n0 6 12 18 24\n24181260\n1.3B\n0 8 16 24 32\nLayer\n32241680\nLayer\n6.7B\n0 10 20 30 40\nLayer\n403020100\n13B\n0 12 24 36 48\nLayer\n483624120\n30B\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: Emergence of modular structure of repre-\nsentations of LLMs. The table displays representation\nsimilarity among layers of OPT models (Zhang et al.,\n2022) on ARC dataset (Clark et al., 2018) with instruc-\ntion format. The lighter colors indicate higher similarity\nand same color scale is used in all plots.\nGPT-4 (OpenAI, 2023) due to their potential to-\nwards advanced intelligent systems. By employing\ntechniques like prompt learning (Ding et al., 2022a;\nWei et al., 2022b) and instruction tuning (Sanh\net al., 2022; Wei et al., 2022a; Wang et al., 2022b,c),\nthe behavior of LLMs can be aligned with human\nintent using a small amount of data. (Taori et al.,\n2023; Chiang et al., 2023; Zhou et al., 2023).\nHowever, the centralization of LLMs poses a\nsignificant challenge concerning the trade-off be-\ntween high performance and user data (Li et al.,\n2022a). For instance, OpenAI provides fine-tuning\nAPIs* that allows users to upload personal data for\nfurther fine-tuning of davinci models family, which\nhas gained popularity in the industry, like Ope-\nnAI GPT-4 (OpenAI, 2023), Google Bard †, and\nAnthropic Claude‡. Safeguarding the privacy of\nboth LLMs and downstream user data is an urgent\nconcern. One viable approach involves the direct\ntransfer of parameterized modules between mod-\nels, such as Federated Learning (FL) (McMahan\n*https://platform.openai.com/docs/guides\n†https://bard.google.com/\n‡https://www.anthropic.com/product\n9612\net al., 2017; Lin et al., 2022) and Split Learning\n(SL) (Vepakomma et al., 2018; Thapa et al., 2022),\nwhere limited exploration are conducted on LLMs\nwith billions of parameters. Recently, Xiao et al.\n(2023) propose Offsite-Tuning (OFT) for transfer\nlearning that operates independently of full LLMs\nwith sizes exceeding 1B. OFT entails compressing\nthe LLM into a smaller model known as emulator\nby layer dropping, followed by fine-tuning the em-\nulator using user data. Finally, the parameterized\nmodules are transferred from emulator and seam-\nlessly integrated into LLM in a single turn. Despite\nthe promising results obtained by OFT, there is still\nlimited understanding of its underlying mechanism.\nIn this paper, we conduct a detailed analysis of\nLLMs from the perspective of representation and\nfunctional similarity (Kornblith et al., 2019; Bel-\nrose et al., 2023) to enhance our understanding of\nOFT. Through comparing the similarity of hidden\nstates across layers, we observeemergence of mod-\nular structureof representations within LLMs. As\ndepicted in Figure 1, models with a size less than\n10B exhibit uniform representations across all lay-\ners. However, for models of size 13B and 30B,\nmodular structure becomes apparent in the repre-\nsentation similarities. For the 13B model, high\nsimilarities are observed between layers 5 and 20,\nforming a light-colored block-diagonal structure\n(i.e., modular structure), as well as between layers\n25 and 40. Additionally, we note subtle changes\nin the representation between adjacent layers. We\nfurther analyze functional similarity to explore the\nintermediate predictions of each layer, which en-\nhances these findings.\nBuilding upon our findings, we propose a com-\npletely training-free strategy to enhance fine-tuning\nwithout relying on full model. This strategy con-\nsists of three steps, namely Clustering, Removing,\nand Sharing (CRaSh). In the initial step, we cluster\nadjacent layers based on their similarity using vari-\nous hierarchical clustering algorithms (Murtagh\nand Contreras, 2012). We then remove layers\nwithin the same cluster to obtain an emulator. The\nremaining layers are shared as a supplement to the\nremoved layers. Finally, selected layers of the emu-\nlator are updated using downstream data and trans-\nferred to be seamlessly integrated into LLMs, re-\nsulting in improved performance. Through the uti-\nlization of CRaSh, we significantly enhance the per-\nformance of fine-tuning LLMs without full models\nacross multiple datasets. In order to comprehend\nthe relationship of optimal solutions in CRaSh, we\nvisualize the optima using loss surfaces and mode\nconnectivity (Li et al., 2018; Frankle et al., 2020).\nThis study offers valuable insights into the effec-\ntiveness of CRaSh. In summary, our main contri-\nbutions can be summarized as follows:\n• We discover emergence ofmodular structure\nwithin layers along with size of model in-\ncrease in decoder-only models (e.g., OPT and\nLLaMA), which shows clusters of similar rep-\nresentations across layers (Section. 2).\n• We propose CRaSh, a training-free strategy\nto enhance layer dropping compression (Sec-\ntion. 3). CRaSh improves the performance of\nfine-tuning without full model and even out-\nperforms knowledge distillation on multiple\ndatasets (Section. 4).\n• We analyze the optima of fine-tuning with\nand without full model through the lens of\nloss surface and mode connectivity, which\nshows the two minima fall over the same basin\nand are connected linearly in parameter space.\nThis observation explains the effectiveness of\nCRaSh (Section. 5).\n2 Empirical Analysis\nIn this section, we analyze the inherent similar-\nity of layers in LLMs from two complementary\nperspectives: (i) representation similarity (Sec-\ntion. 2.1), which examines the differences in activa-\ntions among intermediate layers, and (ii) functional\nsimilarity (Section. 2.2), specifically the variations\nin predictions among intermediate layers.\n2.1 Representation Similarity\nGiven a LLM, which produces word representa-\ntions (i.e., hidden states) at each layer and ag-\ngregates them into sentence representations. We\nmeasure the similarity of sentence representations\namong layers using linear centered kernel align-\nment (CKA) (Kornblith et al., 2019). CKA em-\nphasizes the distributivity of information. If two\nlayers exhibit similarity across all their neurons,\nthe similarity will be higher, even if individual neu-\nrons do not have similar matching pairs or are not\nwell represented by all neurons in the other layer.\nThe representation similarity reveals correlations\nbetween layers within LLMs. The inherent sim-\nilarity in the representation of layers across two\n9613\n0 3 6 9 12\n129630\n125M\n0 5 10 15 20\n20151050\n350M\n0 6 12 18 24\n24181260\n1.3B\n0 8 16 24 32\n32241680\n6.7B\n0 10 20 30 40\n403020100\n13B\n0 12 24 36 48\n483624120\n30B\n0.25\n0.50\n0.75\n1.00\n0 3 6 9 12\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0Similarity\n0 5 10 15 20\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n0 6 12 18 24\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n0 8 16 24 32\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n0 10 20 30 40\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n0 12 24 36 48\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 2: This table presents the representation similarity among all layers (shown above the heatmaps) and the\nsimilarity between adjacent layers (represented by the line charts) on the Wikitext corpus (Merity et al., 2017).\ndatasets are demonstrated in Figure 1 and Figure 2.\nWe provide a detailed explanation of our two main\nfindings as follows:\nFinding 1: Emergence of modular structure.\nAs shown in Figure 1 and Figure 2, we observe\nthat as the number of parameters increases, distinct\nblocks or modules with high similarity emerge in\nthe representations of different layers. We refer\nto these blocks of high-similarity representations\nas modular structure. The emergence of modular\nstructure can be seen as a self-organizing behav-\nior during the pre-training of LLMs, where the\ninternal representations gradually differentiate into\nmodules with specific functions or semantics. This\nphenomenon has been briefly examined in previous\nstudies (Phang et al., 2021; Merchant et al., 2020;\nChen et al., 2021; Chiang et al., 2020). To the best\nof our knowledge, no study has investigated this\nquestion on LLMs with billions of parameters.\nAdditionally, our initial findings indicate that\nthere are no evident clusters of layers in pre-trained\nmodels with millions of parameters, which may\nindicate insufficient capacity to comprehend tasks\nin zero-shot setting. However, when the model\nsize reaches a certain threshold, referred to as mod-\nular point, modular structure emerges in LLMs,\nresulting in the formation of distinct clusters. Our\nexperiments have shown that the specific modular\npoint varies across tasks. Furthermore, we observe\nthat the modular point is larger for harder tasks, but\nsmaller for easier tasks, such as language model-\ning. For example, in the case of the ARC dataset (a\nquestion answering task with instruction format),\nmodular point is observed to be 10B, as depicted\nin Figure 1. On the other hand, for the Wikitext\ncorpus (a language modeling task), modular point\nis found to be 1.3B, as illustrated in Figure 2.\nFinding 2: Subtle changes in representation\namong layers. In addition to modular structure,\nwe can also observe high similarity along the di-\nagonal of the similarity matrix. This indicates that\nintermediate layers may exhibit significant similar-\nity to their adjacent layers. We directly examine the\nrepresentation similarity between adjacent layers\nin Figure 2, where point at xi means similarity be-\ntween layer i and layer i + 1. In addition to the low\nsimilarity observed in the bottom layers, starting\nfrom about the 5th layer, there is a significantly\nhigher similarity compared to adjacent layers.\nFurthermore, we broaden the scope of these find-\nings to include more LLMs and datasets, uncover-\ning a consistent trend of modular structure emer-\ngence. The details of results, as well as implemen-\ntation details, are provided in Appendix A.1.\n2.2 Functional Similarity\nIn addition, a complementary perspective to rep-\nresentation similarity involves extracting specific\nconcepts from the hidden states. For example, we\ncan convert the hidden states at each intermediate\nlayer into a probability distribution over the vocab-\nulary (nostalgebraist, 2020; Belrose et al., 2023).\nThis approach facilitates a deeper comprehension\nof the functional aspects of the layers. For the anal-\nysis of functional similarity, we utilize the state-\nof-the-art tool Tuned-Lens§ to obtain predictions\nfrom the intermediate layers. Additional details are\nprovided in Appendix A.2.\nFinding 3: Removing layers within a cluster\nmaximizes behavior invariance.In the sub-figure\nlabeled 1⃝full model of Figure 3, we observe\nsubtle changes as the depth increases and note that\nadjacent layers display a high similarity in hidden\npredictions, resulting in the formation of clusters.\n§https://github.com/AlignmentResearch/tuned-lens\n9614\n_the \\n . _Ring _to , _them\n_the \\n \\n . _the _the _them\n_the \\n - . _the _the _them\n_and \\n - . _the _the _them\n_and \\n - _Ring _the _the _is\n_and \\n - \\n _the _the _is\n_and \\n - \\n _the _the .\n_and \\n - _is _the _the .\n\\n \\n - _of _be _the \\n\n_and \\n - \\n _be _the _to\n_and \\n - \\n _be _the _to\n\\n I - _in _be _out _to\n\\n And - _to _be _out _to\n\\n And - _to _be _out _out\n\\n And - _to _be _the _into\n\\n And - _to _be _the _out\n\\n and _hand _to _be _them _to\n\\n and _hand _to _bind _them _all\n\\n and _hand _to _bind _them _all\n\\n And _ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bring _them _all\n\\n One _Ring _to _bring _them _all\n\\n One _Ring _to _bring _them _all\n, \\n One  _Ring   _to    _bring     _them      \ninput\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\noutput\n_the \\n . _Ring _to , _them\n_the \\n \\n . _the _the _them\n_the \\n - . _the _the _them\n_and \\n - . _the _the _them\n_and \\n - _Ring _the _the _is\n_and \\n - \\n _the _the _is\n_and \\n - \\n _the _the .\n_and \\n - _is _the _the .\n\\n \\n _of , _be _the \\n\n\\n \\n - _of _be _the \\n\n\\n \\n - _of _be _the _all\n_ \\n - _of _be _out _out\n_ \\n _way _of _be _out _out\n\\n _ - _is _be _them _into\n\\n \\n - _to _be _the _out\n_and I _way _to _be _them _to\n\\n And _Ring _to _be _them _all\n\\n The _ring _to _be _them _all\n\\n One _Ring _to _be _them _all\n\\n One _Ring _to _be _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bring _them _all\n\\n One _Ring _to _bring _them _all\n, \\n One  _Ring   _to    _bring     _them      \ninput\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\noutput\n_the \\n . _Ring _to , _them\n_the \\n \\n . _the _the _them\n_the \\n - . _the _the _them\n_and \\n _of _Ring _to _the _them\n\\n \\n _of \\n _to _the .\n\\n \\n _of _of _the _the _is\n\\n \\n _of \\n _the _the \\n\n\\n \\n - \\n _the _the .\n\\n \\n - \\n _the _the .\n\\n I - _of _the _the ,\n\\n I - _of _be _the .\n\\n I - , _be _the ,\n\\n _ - _to _be _the _into\n\\n and - _to _be _the _to\n\\n and _hand _to _be _them _to\n\\n and _hand _to _be _them _all\n\\n and _ _to _bind _them _all\n\\n And _ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bring _them _all\n\\n One _Ring _to _bring _them _all\n, \\n One  _Ring   _to    _bring     _them      \ninput\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\noutput\n_the \\n . _Ring _to , _them\n_the \\n \\n . _the _the _them\n_the \\n - . _the _the _them\n_and \\n - . _the _the _them\n_and \\n - _Ring _the _the _is\n_and \\n - \\n _the _the _is\n_and \\n - \\n _the _the .\n_and \\n - _is _the _the .\n\\n \\n - _of _be _the \\n\n_and \\n - \\n _be _the _to\n_and \\n - \\n _be _the _to\n\\n I - _in _be _out _to\n\\n And - _to _be _out _to\n\\n And - _to _be _out _out\n\\n And - _to _be _the _into\n\\n And - _to _be _the _out\n\\n and _hand _to _be _them _to\n\\n and _hand _to _bind _them _all\n\\n and _hand _to _bind _them _all\n\\n And _ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bind _them _all\n\\n One _Ring _to _bring _them _all\n, \\n One  _Ring   _to    _bring     _them      \ninput\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\noutput\n0.2\n0.4\n0.6\n0.8\nMax Probability (probs)\n0.2\n0.4\n0.6\n0.8\nMax Probability (probs)\n0.2\n0.4\n0.6\n0.8\n1\nMax Probability (probs)\n0.2\n0.4\n0.6\n0.8\n1\nMax Probability (probs)\n① full model ② uniform (removing layer 8, 16) ③ low-cluster (removing layer 3, 5)④ high-cluster (removing layer 20, 22)\nFigure 3: This figure shows results of Tuned-lens on OPT-1.3B and three variants of layer dropping. The entire\ninput text consists of \"One Ring to rule them all,\\n One Ring to find them,\\n One Ring to bring them all\\n and in the\ndarkness bind them\", where tokens from position 14 to 21 are indicated.\nTo assess the redundancy of layers, we eliminate\nmultiple layers from the cluster based on represen-\ntation similarity in Figure 2 and compare the pre-\ndictions when these layers are uniformly dropped.\nBased on the results, we also observe all the re-\nmoval strategies yield predictions that exhibit over-\nall similarity to the internal layers of the full model,\nshowing only minor differences in probability, as\nindicated by subtle variations in color shades. In\ncomparison to the 1⃝full model, the 2⃝uniform\nstrategy exhibits notable differences in the middle\nlayers of token one, high layers of token four, and\nthe last two tokens, indicating the presence of im-\npure predictions in these positions. Conversely,\nthe 3⃝low-cluster and 4⃝high-cluster strate-\ngies display closer alignment with the full model\nat these locations. This phenomenon leads to the\nconclusion that adjacent layers with high represen-\ntation similarity also demonstrate analogous func-\ntional similarity. Furthermore, removing layers\nfrom these clusters results in maximum behavior\ninvariance compared to uniformly dropping them.\n3 Methodology\nEmpirical analysis of LLMs can inspire fine-tuning\napproaches, contributing to privacy protection for\nboth centralized LLMs and user data. In this sec-\ntion, we begin by revisiting Offsite-Tuning (OFT),\na representative method for fine-tuning LLMs with-\nout full model. Subsequently, we introduce our\nproposed CRaSh method inspired by Section. 2.\n3.1 Revisit Offsite-Tuning\nIn this section, we provide a brief introduction\nto the settings and evaluation metrics of Offsite-\nTuning (OFT) (Xiao et al., 2023). The primary con-\ncern of OFT is to maintain the privacy of both the\nLLM and user data. Specifically, the data owner is\nunable to share their labeled training data with the\nLLM owner, and vice versa, the LLM owner cannot\nshare their model with the data owner.\nAs shown in Figure 4a, OFT involves compress-\ning the LLM into a smaller emulator by layer drop-\nping and fine-tuning it using privacy data (i.e.,\n3⃝Emulator Fine-tuning ). Subsequently, the\nblock weights are transferred and plug-in the LLM\nfor inference (i.e., 4⃝Plug-in). The main objective\nof OFT is for the plug-in to exceed the performance\nof both full zero-shot on the LLM and fine-tuning\non the emulator ( 4⃝> 1⃝, 3⃝), thereby ensuring the\noverall effectiveness of OFT. Additionally, OFT\naims to optimize the performance of the plug-in\nto closely approximate the results achieved by full\nfine-tuning on the LLM ( 4⃝≈ 2⃝).\nThe key to OFT lies in identifying an emula-\ntor that closely resembles the original LLM and\nsubsequently performing fine-tuning to replace the\nLLM. Drawing from our findings on the empirical\nanalysis of LLMs in Section 2, we propose CRaSh,\nwhich is designed to optimize layer dropping and\nyield superior sub-layer emulators from LLMs, as\nillustrated in the following sections.\n3.2 CRaSh\nIn this section, we provide a detailed description of\nthe three steps of CRaSh, namely Clustering, Re-\nmoving, and Sharing as presented in Figure 4b. The\nconcept of Clustering and Sharing are supported\nby Finding 1, which suggests that layers within a\ncluster share similar functions. Additionally, Find-\ning 2 and Finding 3 support the idea of removing\nlayers from clusters, as it helps minimize changes\nin representations and functions.\nClustering Drawing inspiration from the pres-\nence of layer clusters in representations and the\ngradual change in layer behavior in Section. 2, it\nis observed that adjacent layers within a cluster\nmay have similar functions. These layers can be\n9615\nQuestion: What star sign is Jamie Lee Curtis?Answer:ScorpioQuestion: what character did natalie portman play in star wars?Answer:Padmé Amidala\n…\nQuestion: What star sign is Jamie Lee Curtis?Answer:Scorpio\nQuestion: what character did natalie portman play in star wars?Answer:Padmé Amidala…\nBlockN’\nBlock2’Block1’\nBlockN-1’…\nEmulator\n③EmulatorFine-tuning<train>①FullZero-Shot\n②FullFine-tuning\nQuestion: what is the name of justin bieber brother?Answer:\nJazmyn Bieber\nQuestion: what is the name of justin bieber brother?Answer:Jazmyn Bieber\n BlockNBlockN-1BlockN-2\nBlock4Block3Block2Block1\nBlockN-3…\nFullLLM\nBlockN’\nBlock1’\nTransfer\nTransfer\nCompress\n④Plug-in<test>\n(a) Fine-tuning and inference strategy.\nBlockiBlocki BlockiFreezedandsharedlayerRemovedlayerFine-tunedlayerBlockiFreezedlayer\nBlockNBlockN-1BlockN-2\nBlock4Block3Block2Block1\nBlockN-3…\nBlockNBlockN-1BlockN-2\nBlock4Block3Block2Block1\nBlockN-3…\nBlockNBlockN-1BlockN-2\nBlock4Block3Block2Block1\nBlockN-3…\nBlockNBlockN-1\nBlock4\nBlock2Block1\nBlockN-3…\nBlockN-1\nBlock2\nStep1,ClusteringStep2,RemovingStep3,Sharing\n (b) Overview of CRaSh Strategy\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n2423222120191817161514131211109876543210\n0.4\n0.6\n0.8\n1.0 (c) Adjacent Clustering\nFigure 4: Overview of Offsite-Tuning and CRaSh strategy.\ngrouped together into a single cluster and subse-\nquently replaced by the cluster center. As shown\nin Figure 4c, we propose a process for clustering\nthe layers of LLMs based on the similarity of repre-\nsentations between adjacent layers. Initially, each\nindividual layer is considered as a separate clus-\nter. Subsequently, the closest clusters are selected\nand merged into a new cluster based on the CKA\nmetric of their output representations. This step\nis repeated until the desired number of clusters is\nreached. This process bears resemblance to hierar-\nchical clustering (Murtagh and Contreras, 2012),\nthe key distinction lies in the fact that we exclu-\nsively cluster the adjacent layers.\nRemoving Following the clustering of interme-\ndiate layers, only layers located at the cluster cen-\nters are retained, while the rest within the clusters\nare removed. Subsequently, a selection of layers\nis chosen for fine-tuning, where parameters of the\nbottom and top n layers are learnable in OFT (Xiao\net al., 2023) (with n = 2). Considering the signifi-\ncance of knowledge contained in the middle layers\nfor downstream tasks, we uniformly select a set of\nn layers from the remaining layers after removal.\nThe exploration of skill layers (Sajjad et al., 2023;\nJordao et al., 2023) based on module cruciality is\nleft as a potential area for future research.\nSharing In the original OFT approach, the\nmodel with the remaining layers is considered as\nan emulator and utilized for fine-tuning on down-\nstream user data. This allows users to experiment\nwith different strategies to optimize the emulator’s\nperformance. For downstream fine-tuning, we pro-\npose a straightforward strategy to enhance the per-\nformance of emulator. Considering the emergent\nabilities of LLMs, certain features, such as chain-\nof-thought (Wei et al., 2022b), may not be present\nin shallow layer models. Drawing inspiration from\nthe concept of layer sharing (Lan et al., 2020),\nwe implement layer sharing within the remaining\nmodel to achieve optimal results.\n4 Experiments\n4.1 Setup\nDataset. We evaluate CRaSh on three tasks\nand eight datasets which are used in Offsite-\nTuning (Xiao et al., 2023), including Multi-\nChoice QA: OpenBookQA (Mihaylov et al., 2018),\nPIQA (Bisk et al., 2020), SciQ (Welbl et al.,\n2017), RACE (Lai et al., 2017); Closed-Book\nQA: ARC-Easy/Challenge (Clark et al., 2018),\nWebQuestion (Berant et al., 2013); and Sentence\nCompletion: HellaSwag (Zellers et al., 2019).\nTo enhance zero-shot performance, we organize\nsource and target text in instruction format as\nlm-evaluation-harness (Gao et al., 2021). We\nutilize it to evaluate our models and report the ac-\ncuracy on all benchmarks. For clustering step, we\nutilize data from the same task as support dataset\nto maintain the privacy of target dataset, including\nBoolQ (Clark et al., 2019), TriviaQA (Joshi et al.,\n2017), and CoPA (Wang et al., 2019). We show\ndetails about statistic of datasets in Appendix B.1.\nModels. We perform empirical analysis on a\nrange of models, including OPT (from 125M to\n30B) (Zhang et al., 2022) and LLaMA (from 7B to\n30B) (Touvron et al., 2023). Due to limited com-\nputational resources, we primarily conduct main\nexperiments on OPT-1.3b, with plans to scale up to\nOPT-6.7B and LLaMA-7B. As our primary base-\nlines, we consider models from OFT (Xiao et al.,\n2023), including a knowledge distillation emula-\ntor and a uniform 2-8-2 configuration for OPT-\n1.3b, 2-18-2 for OPT-6.7B and LLaMA-7B. Here,\nl-c-r denotes setting the parameters of bottom l\nand top r layers as learnable, while c layers are\nuniformly selected from original LLM and kept\n9616\nSetting OpenBookQA ARC-E ARC-C WebQs PIQA SciQ RACE HellaSwag\nFull Large Language Model\nZero-shot (ZS) 23.4% 56.9% 23.5% 4.6% 71.6% 84.4% 34.2% 41.5%\nFine-tuning (FT) 31.4% 61.3% 27.7% 31.2% 75.2% 92.5% 37.0% 42.7%\nKnowledge Distillation (Continual Training)\nEmulator ZS 19.4% 53.9% 21.5% 1.3% 68.7% 80.9% 33.0% 35.1%\nEmulator FT 24.8% 58.1% 26.1% 24.3% 71.6% 92.2% 38.6% 37.0%\nPlug-in (Xiao et al., 2023) 29.0% 59.4% 27.8% 26.2% 74.5% 92.9% 38.9% 43.3%\nUniform Strategy (Training-free)\nEmulator ZS 13.8% 34.9% 19.0% 0.0% 58.4% 49.8% 22.7% 27.0%\nEmulator FT 24.6% 50.4% 21.2% 21.8% 69.3% 89.4% 36.5% 32.7%\nPlug-in (base) 26.4% 58.3% 23.0% 21.4% 72.7% 90.8% 37.9% 41.2%\nwith uniform learnable layers\nEmulator FT 24.2% 51.1% 24.1% 23.7% 69.3% 89.3% 36.9% 33.8%\nPlug-in 27.6% 58.8% 24.8% 16.4% 72.6% 92.1% 39.7% 41.2%\nCRaSh Strategy (Training-free)\nEmulator ZS 14.0% 35.9% 18.5% 4.7% 57.0% 84.3% 34.2% 25.9%\nEmulator FT 25.0% 50.0% 21.5% 21.8% 68.9% 88.9% 38.9% 33.6%\nPlug-in (our) 30.2%↑4.8 60.0%↑1.7 24.8%↑1.8 23.7%↑2.3 73.2%↑0.5 93.1%↑2.3 39.9%↑2.0 41.9%↑0.7\nw/o layer sharing\nEmulator ZS 14.0% 35.9% 18.5% 0.0% 57.0% 43.3% 23.6% 25.9%\nEmulator FT 23.8% 50.0% 21.5% 24.3% 68.8% 89.6% 36.2% 31.2%\nPlug-in 25.2% 57.7% 24.6% 17.7% 71.7% 92.2% 39.1% 41.3%\nTable 1: Results of CRaSh on OPT-1.3B. The values in red font indicate an increase compared to the uniform\nstrategy, highlighting the superiority of CRaSh. Remarkably, CRaSh outperforms the strategy with knowledge\ndistillation (KD) on several datasets. It is worth noting that KD can further enhance the performance of CRaSh.\nfrozen during fine-tuning. We present implementa-\ntion details about experiments in Appendix B.2.\n4.2 Experimental results\nWe present the main results in Table 1 and provide\nan analysis as follows.\nOFT and CRaSh does work well. CRaSh satis-\nfies the condition of OFT, where the performance of\nplug-in is better than the zero-shot performance of\nfull model and the fine-tuning performance of emu-\nlator. The results prove that OFT effectively works\nfor fine-tuning without full model and is benefi-\ncial for protecting the privacy of LLMs. Addition-\nally, due to over-parameterization of LLMs (Agha-\njanyan et al., 2021; Ding et al., 2022b), it may not\nbe necessary to optimize all parameters. There-\nfore, the performance of plug-in can surpass that\nof directly fine-tuning on LLMs, particularly on\ndatasets like SciQ and RACE.\nCRaSh is better than uniformly dropping\nstartegy. Compared to the uniform layer drop-\nping strategy used in OFT (Xiao et al., 2023),\nCRaSh is an effective method to boost performance\nthat requires low additional cost and is completely\ntraining-free, where lifting effects are indicated in\nred font. Additionally, CRaSh can outperform the\nknowledge distillation models in OFT settings on\nseveral datasets, such as achieving 1.2% improve-\nments on OpenBookQA and 1.0% on RACE. The\nknowledge distillation method obtains the emulator\nby continuously pre-training it on the first block\nof the Pile corpus (Gao et al., 2020), which aims\nto align the intermediate representation between\nemulator and full model. Therefore, CRaSh is com-\nplementary to knowledge distillation, which can\nfurther improve performance by leveraging the im-\nproved emulator initialization provided by CRaSh.\nCRaSh works better while scaling up model.\nAs depicted in Table 2, CRaSh remains effective\neven when scaling up the model from 1B to 7B.\nThe benefits and effectiveness of CRaSh are not\nrestricted to specific model sizes, making it a valu-\nable strategy for enhancing OFT outcomes across\nmodels of varying scales. Meanwhile, as the depth\nincreases, layers are more prone to redundancy in\nLLMs. Therefore, by employing a clustering step,\nCRaSh eliminates redundant network layers and\neffectively improves the performance of OFT com-\npared to the uniform strategy.\n4.3 Ablation study\nImpact of clustering and sharing steps. This sec-\ntion discusses the significance of clustering and\nsharing steps in CRaSh. We compare the results\nwith and without the clustering step, as shown in\nTable 1. In OFT (Xiao et al., 2023), the top and\n9617\nSetting OpenBookQA ARC-E ARC-C WebQs PIQA SciQ RACE HellaSwag\nOPT-6.7B\nFull ZS 27.6% 65.6% 30.6% 8.8% 76.2% 90.1% 38.2% 50.5%\nEmulator ZS 21.4% 55.6% 23.9% 1.5% 57.0% 84.1% 31.1% 28.4%\nEmulator FT 29.0% 60.1% 31.1% 22.1% 75.6% 88.4% 36.2% 43.4%\nPlug-in (Xiao et al., 2023) 33.8% 66.8% 33.9% 23.9% 77.7% 91.9% 44.1% 52.1%\nPlug-in (CRaSH) 38.8%↑5.0 70.7%↑3.9 36.3%↑2.4 26.1%↑2.2 78.0%↑0.3 95.3%↑4.2 45.2%↑1.1 53.4%↑1.3\nLLaMA-7B\nFull ZS 28.2% 67.3% 38.2% 0.0% 78.3% 89.7% 40.0% 56.4%\nEmulator ZS 15.0% 44.3% 23.5% 0.0% 65.7% 57.6% 30.2% 36.2%\nEmulator FT 25.4% 60.0% 28.8% 25.7% 73.6% 91.8% 40.8% 45.0%\nPlug-in (Uniform) 33.0% 69.6% 39.0% 27.3% 78.8% 93.5% 44.0% 57.4%\nPlug-in (CRaSH) 34.6%↑1.6 71.3%↑1.7 41.8%↑2.8 29.8%↑2.5 80.0%↑1.2 95.1%↑1.6 45.6%↑1.6 58.4%↑1.0\nTable 2: Results on OPT-6.7B and LLaMA-7B. CRaSh continues to perform effectively as the model size scales up,\nand even achieves further improvement, benefiting from the emergence ofmodular structure.\nDataset OpenBookQA ARC-E ARC-C WebQs\nThe Wikitext 27.8% 58.5% 24.0% 22.0%\nSupport Task 30.2% 60.0% 24.8% 23.7%\nDownstream Task 29.4% 59.6% 25.1% 24.1%\nTable 3: The table presents the plug-in results of CRaSh,\nconsidering different data types for the clustering step.\n2 4 6 8 10 12 14 16 18 20 22 24 26 28\nNumber of layers for emulator\n0.15\n0.20\n0.25\n0.30\n0.35Accuracy\n emulator zero-shot\nemulator fine-tuning\nfull plug-in\nfull zero-shot\nfull fine-tuning\nFigure 5: The accuracy varies as the number of layers\nto be dropped changes on OPT-6.7B model.\nbottom two layers are set to be learnable, while\nthe four layers to be updated are uniformly cho-\nsen in our CRaSh strategy. Hence, we run the\nUniform Strategy in \"with uniform learnable lay-\ners\" and compare it with CRaSh Strategy without\nlayer sharing (e.g., \"w/o layer sharing\"). In cases\nwhere only the layers of the emulator differ, we\nfind that the clustering step provides slight utility\ncompared to the uniform strategy. This trend may\nbe attributed to the potential loss of hidden informa-\ntion caused by layer dropping. However, this loss\ncan be mitigated by incorporating layer sharing and\nfine-tuning, resulting in improved performance of\nthe final CRaSh approach. The results in Table 1\nalso indicate the significance of layer sharing steps,\nas they enable the emulator to have sufficient depth\nto handle challenging tasks.\nImpact of data type for clustering. In our main\nexperiments, we take into account the privacy of\ndownstream data. Therefore, we solely utilize data\nfrom the support task for clustering, which shares\na similar task type with the downstream task. As\npresented in Table 3, we compare this approach\nwith using public general task and directly down-\nstream dataset in order to examine the impact of\ndata type on clustering and the resulting plug-in\nperformance. By avoiding direct use of the down-\nstream task, which may compromise privacy, we\nachieve comparable performance by utilizing data\nsolely from a similar support task. We leave it\nfor future work to explore the identification of the\nmost relevant support task for clustering based on\ndownstream task information.\nNumber of layers for emulator. To ensure a\nfair comparison, we set the number of layers of\nemulator to 12 and 22 in main experiments, respec-\ntively, for 1B and 7B LLMs. To investigate the\nimpact of an extensive range of layers, we varied\nthe number of clusters from 2 to the maximum\nnumber of total layers. The performance of plug-in\nincreases in accordance with the number of layers\nfor the emulator, as depicted in Figure 5. Addition-\nally, we observed that the performance of plug-in\ncan achieve a comparable effect to full fine-tuning\nwhen the layers comprise only 50% ∼60% of the\nLLMs. However, there is still room for further ex-\nploration when transferring fewer layers of LLMs.\n5 Discussion\nLoss landscape and mode connectivity. In or-\nder to comprehend the effectiveness of CRaSh, we\nconduct an analysis of the relationship between op-\ntimal minima derived from plug-in and full model\nfine-tuning. Firstly, based on Figure 6a, we observe\nthat the initialization resides in a low basin and per-\n9618\n4\n3\n2\n1\n01234\n4\n3\n2\n1\n0\n1\n2\n3\n4\nloss\nInit\nCRaSh\nFull\nopenbookqa\n4\n6\n8\n10\n121\n012\n1\n0\n1\n2\nInit\nCRaSh\nFull\n4\n3\n2\n1\n01234\n4\n3\n2\n1\n0\n1\n2\n3\n4\nloss\nInit\nCRaSh\nFull\narc_easy\n2.5\n5.0\n7.5\n10.0\n12.51\n012\n1\n0\n1\n2\nInit\nCRaSh\nFull\n(a) Loss surface\n0.0 0.2 0.4 0.6 0.8 1.0\ncoefficient\n4.0\n4.5\n5.0\n5.5loss\nopenbookqa\ninterpolation\nOFT & Full\nCRaSh & Full\n0.0 0.2 0.4 0.6 0.8 1.0\ncoefficient\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2loss\narc_easy\ninterpolation\nOFT & Full\nCRaSh & Full (b) Model interpolation\nFigure 6: (a) The initialization weights and optima obtained through CRaSh and full model fine-tuning are located\nwithin the same basin. (b) Model interpolation on weights from CRaSh, OFT, and full fine-tuing.\nforms well across various datasets, which benefits\nfrom the over-parameterization and generalization\nof LLMs. Consequently, by optimizing the LLM\nat a relatively low cost on the target dataset, it can\neffectively perform on this dataset. This observa-\ntion highlights the efficacy of delta-tuning (Ding\net al., 2022b) and offsite-tuning. Secondly, the so-\nlutions derived from CRaSh and full fine-tuning\nreside within the same basin. Utilizing the clus-\ntering and sharing steps, CRaSh exhibits a closer\nproximity to full fine-tuning compared to OFT (re-\nfer to Figure 6b). Finally, through the interpolation\nof various solutions, we observe their mode con-\nnectivity in the parameter space, wherein CRaSh\nexhibits a smoother transition towards full fine-\ntuning compared to OFT, which undergoes more\ndramatic changes. Based on the visualization of the\nloss landscape, there is potential to enhance CRaSh\nfor better performance in future directions.\nParamter-Efficient Fine-tuning (PEFT).\nPEFT has gained popularity as a method for\nadapting LLMs with minimal parameter updates\nor additions. For instance, in the case of LoRA,\nonly 590K parameters need to be updated for\nOPT-1.3B, while OFT requires updating 201M\nparameters and the full model requires updating\n1208M parameters. By applying LoRA to the\ntransferred layers, CRaSh achieves parameter- and\nresource-efficient fine-tuning. Further details on\nthis topic can be found in Appendix C.2.\nReconstruct full model from emulator. When\ntransmitting the emulator downstream, an impor-\ntant consideration emerges: given the emulators,\nhow challenging would it be to reconstruct the orig-\ninal model? The core question is the attainable\nperformance level with just the emulator. Our find-\nings, detailed in Section 4, discuss the complexi-\nties involved in reconstruction: (1) Layer sharing\nin CRaSh is viewed as the most effective recon-\nstruction technique. However, it does not repli-\ncate the performance of the original model entirely.\nAs indicated in Table 2, the emulator fine-tuned\nwith CRaSh does not achieve the full model’s zero-\nshot performance. But, when integrated, there’s a\nmarked improvement in performance across mul-\ntiple datasets, especially ARC-E/C, PIQA, RACE,\nand HellaSwag. (2) A crucial factor in the recon-\nstruction challenge is the number of transferred\nlayers; fewer transferred layers complicate the re-\nconstruction. Figure 5 demonstrates that accuracy\nvaries with the number of layers omitted. Interest-\ningly, emulators with merely 6 layers (2 frozen and\n4 fine-tuned, compared to the 32 layers in the full\nmodel) continue to boost the primary LLM’s per-\nformance when incorporated. Undoubtedly, CRaSh\ncould be enhanced by incorporating federated learn-\ning technologies, including homomorphic encryp-\ntion (Lee et al., 2022; Chen et al., 2022) and dif-\nferential privacy (Yue et al., 2021), making the\noriginal model’s reconstruction more challenging.\n6 Related Work\nDelta-tuning (Ding et al., 2022b) methods effi-\nciently update a subset of parameters compared\nto the entire LLMs (Houlsby et al., 2019; Ben Za-\nken et al., 2022; Hu et al., 2022; Dettmers et al.,\n2023). However, these methods require feeding\ndata into the entire LLMs, which is not resource-\nefficient and raises concerns about the privacy of\nLLMs. Black-box tuning (Sun et al., 2022b,a), as\napplied to centralized LLMs, attempts to learn pa-\nrameters based on input or output text (Cui et al.,\n2022), which helps protect LLMs but poses risks\nto user data. Directly manipulating model pa-\nrameters instead of transferring data has found\nwide applications in federated learning (McMa-\n9619\nhan et al., 2017), split learning (Vepakomma et al.,\n2018; Thapa et al., 2022), distributed gradient de-\nscent (Huo et al., 2018; Xu et al., 2020; Ni et al.,\n2023), branch-train-interpolation (Li et al., 2022b;\nWortsman et al., 2022), and collaborative machine\nlearning development (Raffel, 2023; Kandpal et al.,\n2023). These methods either involve model sharing\nbetween servers and clients or require multi-turn\ncommunication to achieve coverage, which poses\nlimitations when applied to LLMs with billions of\nparameters. Offsite-Tuning (Xiao et al., 2023) is\na notable method that addresses these challenges\nby selectively sharing layers of LLMs with bil-\nlions of parameters between servers and clients in\na single turn. On the other hand, previous studies\nhave focused on gaining insights into the interme-\ndiate representation of neural networks for better\nfine-tuning (Kornblith et al., 2019; Merchant et al.,\n2020; Wu et al., 2020; Raghu et al., 2021). In terms\nof transformers, Phang et al. (2021) observed clus-\ntering of layer representations in fine-tuned BERT\nmodels(Devlin et al., 2019), supporting the notion\nof layer redundancy (Dalvi et al., 2020). In contrast,\nwe find that clustering also emerges in pre-trained\nLLMs as the model size increases, which helps\nOffsite-Tuning on LLMs. A detailed introduction\nto related work is presented in Appendix D.\n7 Conclusion\nIn this paper, we uncover block structure of repre-\nsentation within the intermediate layers of LLMs,\nindicating the clustering of layers in depth models.\nBased on these observations, we propose a com-\npletely training-free strategy to enhance fine-tuning\nwithout full model. The strategy consists of three\nsteps: Clustering, Removing, and Sharing (CRaSh).\nCRaSh boosts the performance of Offsite-Tuning\nfor LLMs on various datasets. Further analysis of\nthe loss surface and mode connectivity provides\ninsights into the effectiveness of CRaSh.\nLimitations\nThis paper primarily focuses on fine-tuning LLMs\nwithout using the full model, thereby safeguard-\ning the privacy of both centralized model and data.\nOur empirical analysis on LLMs has inspired a\nstraightforward yet effective improvement that out-\nperforms previous methods.\nHowever, We have only conducted experiments\nusing two types of LLMs (OPT and LLAMA), leav-\ning a wide range of other LLMs unexplored (such\nas Pythia). Due to limited computing resources,\nour main experiments were conducted on LLMs\nwith a model size of less than 10B.\nAlthough we hypothesize that larger models with\nredundant layers and CRaSh may lead to improved\nperformance, further exploration is necessary to\nvalidate this hypothesis. We utilize representation\nsimilarity as a clustering metric, and although it\ndemonstrates satisfactory performance in our ex-\nperiments, we have encountered challenges and ob-\nserved instability. Consequently, we intend to inves-\ntigate functional similarity as an alternative, which\nmay necessitate additional preprocessing time.\nGiven the complexity involved in validating the\nplug-in, further research on behavior predicting is\nrequired to enhance CRaSh (which tends to be a\nheuristic strategy) and transform it into an auto-\nmated learnable strategy (such as reinforcement\nlearning). For future work, we plan to do analy-\nsis using more similarity methods and broaden the\napplication of this strategy to encompass a wider\nrange of instructional data and LLMs.\nEthics Statement\nOur research on Offsite-Tuning (OFT) and the pro-\nposed CRaSh strategy aligns with the ethical guide-\nlines outlined by the ACL Ethics Policy. We recog-\nnize the importance of addressing privacy concerns\nwhen fine-tuning publicly accessible, centralized\nLarge Language Models (LLMs) with private in-\nstruction data.\nThe primary objective of our work is to enhance\nthe generalization ability of LLMs across various\ntasks while safeguarding the privacy of both the\nLLMs and the instruction data. We acknowledge\nthe potential risks associated with the direct transfer\nof parameterized modules between models and the\nneed for further exploration to fully understand its\nimplications and effectiveness.\nOur research adheres to ethical principles by\npromoting privacy protection, transparency, and\nresponsible use of LLMs. We are committed to\ncontinuous ethical evaluation and will contribute to\nthe ongoing discourse on the ethical implications\nof language model fine-tuning.\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (No. 2022ZD0119101). We\nextend our gratitude to the anonymous reviewers\nfor their insightful feedback.\n9620\nReferences\nArmen Aghajanyan, Sonal Gupta, and Luke Zettle-\nmoyer. 2021. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 7319–7328,\nOnline. Association for Computational Linguistics.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1–9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nSteven Bills, Nick Cammarata, Dan Mossing, Henk\nTillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. 2023. Lan-\nguage models can explain neurons in language mod-\nels.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7432–\n7439. AAAI Press.\nBoli Chen, Yao Fu, Guangwei Xu, Pengjun Xie,\nChuanqi Tan, Mosha Chen, and Liping Jing. 2021.\nProbing BERT in hyperbolic spaces. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained BERT networks. In Advances in Neural In-\nformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nTianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong,\nBinxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li,\nand Furu Wei. 2022. THE-X: Privacy-preserving\ntransformer inference with homomorphic encryption.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 3510–3520, Dublin,\nIreland. Association for Computational Linguistics.\nXuxi Chen, Tianlong Chen, Weizhu Chen, Ahmed Has-\nsan Awadallah, Zhangyang Wang, and Yu Cheng.\n2023. Dsee: Dually sparsity-embedded efficient tun-\ning of pre-trained language models.\nCheng-Han Chiang, Sung-Feng Huang, and Hung-yi\nLee. 2020. Pretrained language model embryology:\nThe birth of ALBERT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6813–6828, On-\nline. Association for Computational Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv\npreprint, abs/1803.05457.\nAdrián Csiszárik, Péter Korösi-Szabó, Ákos K. Matszan-\ngosz, Gergely Papp, and Dániel Varga. 2021. Similar-\nity and matching of neural network representations.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 5656–5668.\nGanqu Cui, Wentao Li, Ning Ding, Longtao Huang,\nZhiyuan Liu, and Maosong Sun. 2022. Decoder tun-\ning: Efficient language understanding as decoding.\nArXiv preprint, abs/2212.08408.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\n9621\nLanguage Processing (EMNLP), pages 4908–4926,\nOnline. Association for Computational Linguistics.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Haitao Zheng, and Maosong Sun.\n2022a. OpenPrompt: An open-source framework\nfor prompt-learning. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 105–113,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022b. Delta tuning: A comprehensive study of\nparameter efficient methods for pre-trained language\nmodels.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel\nRoy, and Michael Carbin. 2020. Linear mode con-\nnectivity and the lottery ticket hypothesis. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 3259–3269. PMLR.\nElias Frantar and Dan Alistarh. 2023. Sparsegpt: Mas-\nsive language models can be accurately pruned in\none-shot.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nMitchell Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing BERT: Studying the effects of\nweight pruning on transfer learning. In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 143–155, Online. Association for Com-\nputational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nZhouyuan Huo, Bin Gu, Qian Yang, and Heng Huang.\n2018. Decoupled parallel backpropagation with con-\nvergence guarantee. In Proceedings of the 35th In-\nternational Conference on Machine Learning, ICML\n2018, Stockholmsmässan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pages 2103–2111. PMLR.\nArtur Jordao, George Correa de Araujo, Helena\nde Almeida Maia, and Helio Pedrini. 2023. When\nlayers play the lottery, all tickets win at initialization.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nNikhil Kandpal, Brian Lester, Mohammed Muqeeth,\nAnisha Mascarenhas, Monty Evans, Vishal Baskaran,\nTenghao Huang, Haokun Liu, and Colin Raffel. 2023.\nGit-theta: A git extension for collaborative develop-\nment of machine learning models. ArXiv preprint,\nabs/2306.04529.\n9622\nMax Klabunde, Tobias Schumacher, Markus Strohmaier,\nand Florian Lemmerich. 2023. Similarity of neural\nnetwork models: A survey of functional and represen-\ntational measures. ArXiv preprint, abs/2305.06329.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey E. Hinton. 2019. Similarity of neural\nnetwork representations revisited. In Proceedings of\nthe 36th International Conference on Machine Learn-\ning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\nfornia, USA, volume 97 of Proceedings of Machine\nLearning Research, pages 3519–3529. PMLR.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nGaram Lee, Minsoo Kim, Jai Hyun Park, Seung-\nwon Hwang, and Jung Hee Cheon. 2022. Privacy-\npreserving text classification on BERT embeddings\nwith homomorphic encryption. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3169–3175,\nSeattle, United States. Association for Computational\nLinguistics.\nBo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei,\nJinfeng Yi, and Bowen Zhou. 2022a. Trustworthy ai:\nFrom principles to practices.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada, pages\n6391–6401.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A Smith, and Luke Zettle-\nmoyer. 2022b. Branch-train-merge: Embarrassingly\nparallel training of expert language models. ArXiv\npreprint, abs/2208.03306.\nBill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin\nWang, Yufen Hua, Christophe Dupuy, Rahul Gupta,\nMahdi Soltanolkotabi, Xiang Ren, and Salman Aves-\ntimehr. 2022. FedNLP: Benchmarking federated\nlearning methods for natural language processing\ntasks. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 157–175,\nSeattle, United States. Association for Computational\nLinguistics.\nYuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu,\nYanan Cao, Weiping Wang, and Jie Zhou. 2022.\nLearning to win lottery tickets in BERT transfer via\ntask-agnostic mask training. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5840–5857, Seat-\ntle, United States. Association for Computational\nLinguistics.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Agüera y Arcas. 2017.\nCommunication-efficient learning of deep networks\nfrom decentralized data. In Proceedings of the 20th\nInternational Conference on Artificial Intelligence\nand Statistics, AISTATS 2017, 20-22 April 2017, Fort\nLauderdale, FL, USA , volume 54 of Proceedings\nof Machine Learning Research , pages 1273–1282.\nPMLR.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and\nIan Tenney. 2020. What happens to BERT embed-\ndings during fine-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 33–44,\nOnline. Association for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nJack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023.\nLanguage models implement simple word2vec-style\nvector arithmetic.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embed-\ndings for semantic search.\nFionn Murtagh and Pedro Contreras. 2012. Algorithms\nfor hierarchical clustering: an overview. Wiley Inter-\ndisciplinary Reviews: Data Mining and Knowledge\nDiscovery, 2(1):86–97.\nThao Nguyen, Maithra Raghu, and Simon Kornblith.\n2021. Do wide and deep networks learn the same\nthings? uncovering how neural network represen-\ntations vary with width and depth. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\n9623\nZanlin Ni, Yulin Wang, Jiangwei Yu, Haojun Jiang,\nYue Cao, and Gao Huang. 2023. Deep incubation:\nTraining large models by divide-and-conquering.\nnostalgebraist. 2020. interpreting gpt: the logit lens.\nLessWrong.\nOpenAI. 2023. Gpt-4 technical report.\nJason Phang, Haokun Liu, and Samuel R. Bowman.\n2021. Fine-tuned transformers show clusters of simi-\nlar representations across layers. In Proceedings of\nthe Fourth BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP, pages 529–\n538, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nColin Raffel. 2023. Building machine learning models\nlike open source software. Communications of the\nACM, 66(2):38–40.\nMaithra Raghu, Thomas Unterthiner, Simon Kornblith,\nChiyuan Zhang, and Alexey Dosovitskiy. 2021. Do\nvision transformers see like convolutional neural net-\nworks? In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 12116–12128.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav\nNakov. 2023. On the effect of dropping layers of\npre-trained transformer models. Computer Speech\n&amp Language, 77:101429.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for Com-\nputational Linguistics.\nTianxiang Sun, Zhengfu He, Hong Qian, Yunhua Zhou,\nXuanjing Huang, and Xipeng Qiu. 2022a. BBTv2:\nTowards a gradient-free future with large language\nmodels. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3916–3930, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022b. Black-box tuning\nfor language-model-as-a-service. In International\nConference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n20841–20855. PMLR.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nLst: Ladder side-tuning for parameter and mem-\nory efficient transfer learning. ArXiv preprint ,\nabs/2206.06522.\nChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, Ping Luo, and Ngai Wong. 2022.\nCompression of generative pre-trained language mod-\nels via quantization. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 4821–\n4836, Dublin, Ireland. Association for Computational\nLinguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nChandra Thapa, Mahawaga Arachchige Pathum\nChamikara, Seyit Camtepe, and Lichao Sun. 2022.\nSplitfed: When federated learning meets split learn-\ning. In Thirty-Sixth AAAI Conference on Artificial\nIntelligence, AAAI 2022, Thirty-Fourth Conference\non Innovative Applications of Artificial Intelligence,\nIAAI 2022, The Twelveth Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2022 Vir-\ntual Event, February 22 - March 1, 2022, pages 8485–\n8493. AAAI Press.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. ArXiv preprint,\nabs/2302.13971.\nPraneeth Vepakomma, Otkrist Gupta, Tristan Swedish,\nand Ramesh Raskar. 2018. Split learning for health:\nDistributed deep learning without sharing raw patient\ndata. ArXiv preprint, abs/1812.00564.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\n9624\nXiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou,\nZhiyuan Liu, and Juanzi Li. 2022a. Finding skill\nneurons in pre-trained transformer-based language\nmodels. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 11132–11152, Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022b. Self-instruct: Aligning lan-\nguage model with self generated instructions. ArXiv\npreprint, abs/2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022c. Super-NaturalInstructions: General-\nization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5085–5109, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint, abs/2201.11903.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 94–106, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nMitchell Wortsman, Suchin Gururangan, Shen Li, Ali\nFarhadi, Ludwig Schmidt, Michael Rabbat, and Ari S\nMorcos. 2022. lo-fi: distributed fine-tuning without\ncommunication. ArXiv preprint, abs/2210.11948.\nJohn Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Similar-\nity analysis of contextual word representation mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4638–4655, Online. Association for Computational\nLinguistics.\nGuangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-\ntuning: Transfer learning without full model. arXiv.\nAn Xu, Zhouyuan Huo, and Heng Huang. 2020. On\nthe acceleration of deep learning model parallelism\nwith staleness. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020 , pages\n2085–2094. IEEE.\nXiang Yue, Minxin Du, Tianhao Wang, Yaliang Li,\nHuan Sun, and Sherman S. M. Chow. 2021. Dif-\nferential privacy for text analytics via natural text\nsanitization. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3853–3866, Online. Association for Computational\nLinguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nMinjia Zhang and Yuxiong He. 2020. Accelerating\ntraining of transformer-based language models with\nprogressive layer dropping. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\nArXiv preprint, abs/2305.11206.\n9625\nA Empirical Analysis\nA.1 Representation Similarity Details\nTo compute the similarity of representations be-\ntween pairs of layers within LLMs, we employ\nthe technique known as Centered Kernel Analysis\n(CKA), as introduced by Kornblith et al. (2019).\nFor every layer within LLMs, the hidden state out-\nputs encompass characteristics derived from the\ninput tokens with the shape of (N, L, H), denoted\nas hi. Here, N signifies the batch size, L denotes\nthe input length, and H represents the hidden size.\nAfter getting features of all input tokens, a pool-\ning strategy is used to get sentence embedding for\nsimilarity computing, where max pooling is taken.\nWe evaluate CKA between each pair of layers in\nthe LLM to be compared. For layer i and j, which\ndenoted as si and sj, both with the shape of(N, H),\nthe linear CKA is given by:\nCKA(si, sj) = ||sT\ni sj||2\nF\n||sT\nj sj||F ||sT\ni si||F\nwhere ||·||F denotes the Frobenius norm.\nIn contrast to BERT-style models, which incorpo-\nrate a CLS token for sentence embedding, GPT-style\nmodels focus solely on preceding tokens. There-\nfore, the last token within GPT models often holds\nthe most significant semantic representation. Nev-\nertheless, according to the findings in the experi-\nmental study by Muennighoff (2022), employing\nthe weighted mean pooling strategy, which assigns\nmore importance to later tokens, leads to supe-\nrior results in sentence-level semantic expression\nand ensures stability even as the depth of layers\nincreases. Consequently, we adopt the weighted\nmean pooling strategy to obtain the sentence vector\nwithin the hidden layers:\nRi =\nL∑\ni=k\nwkhk, wk = i∑L\nk=1 k\nwhere hk is the kth hidden state and R means the\nsentence embedding of ith layer.\nWe organize the source and target text samples\ninto an instruction format, as illustrated in Table 5\nand Table 6.\nFor CKA, representation similarity is computed\nacross all input samples. Due to computational\nconstraints, we use a set of 512 samples for com-\nputation, which has been found to be sufficient and\nstable for analyzing the modular structure.\nAdditionally, we provide results on more\ndatasets and models in Figure 11, Figure 12, and\nFigure 15. While we employ weighted mean pool-\ning to obtain sentence representations, we also in-\nclude results using mean pooling in Figure 13 and\nFigure 14.\nA.2 Functional Similarity Details\nLogits-Lens was proposed by (nostalgebraist, 2020)\nas a method to gain insights into the internal work-\nings of GPT-2, with a specific focus on analyzing\nthe logits, which represent the raw outputs gen-\nerated by the model before applying the softmax\nfunction to obtain probabilities. Logits-Lens aims\nto examine the logits at various layers of GPT mod-\nels in order to enhance understanding of prediction\nprocess in LLMs. Through the inspection of these\nlogits, researchers and developers can gain valuable\ninsights into decision-making process of LLMs and\npotentially discover underlying patterns or biases.\nDue to the instability of Logits-Lens, which fails\nto function effectively in larger and deeper mod-\nels, (Belrose et al., 2023) introduces Tuned-Lens,\nwhere each layer of the model is trained using an\naffine transformation on a pre-training corpus.\nB Main Experimental Details\nB.1 Dataset Details\nWe present statistical results of both downstream\ntasks (target tasks) and support tasks in Table 4.\nB.2 Implementation Details\nFor empirical analysis, we randomly select 512\nsamples from the evaluation set of each dataset\nfor CKA computation. Additionally, we perform\nthe clustering step by considering only layers be-\ntween the lth bottom layers and the last rth top lay-\ners, taking into account the coarse changes among\nthem. However, we ensure that the layers of the\nemulator remain the same as in Offsite-Tuning.\nWe perform a learning rate tuning process on a\ngrid of values and report the runs with the high-\nest emulator performance, where {1e −4, 2e −\n4, 3e −4, 2e −5, 5e −5, 8e −5}for OPT-1.3B\nand {5e −6, 8e −6, 1e −5, 2e −5, 5e −5}for\nOPT-6.7B and LLaMA-7B. We also select the best\nadapter layers in both settings, with and without\nsharing, indicating that the sharing step is an op-\ntional component for local clients. Furthermore,\nthe decision to repeat fine-tuned layers is dependent\non the available local resources. The experiments\n9626\nTask Multi-Choice QA Closed-Book QA SentComp\nDataset OpenBookQA PIQA SciQ RACE BoolQ ARC-E ARC-C WebQs TriviaQA HellaSwag CoPA\nDomain Sci.Edu Physical Sci.Edu Edu.Exam Gen. Gen. Sci.Edu Gen. Gen. Gen. Gen.\nTrain.Size 4,957 16,113 11,679 62,445 9,427 2,251 1,119 3,589 87,622 39,905 400\nEval.Size 500 1,838 1,000 3,451 3,270 570 299 189 11,313 10,042 100\nTest.Size 500 3,084 1,000 3,498 3,245 2,376 1,172 2,032 10,832 10,003 500\nAvg.Context 13.10 14.36 110.27 407.90 143.96 28.02 31.15 15.54 24.23 49.18 8.68\nAvg.Target 4.87 24.28 3.68 8.68 2.0 5.93 7.28 4.24 4.48 29.52 7.34\nAvg.Total 17.97 38.64 113.95 416.58 145.96 33.95 38.43 19.78 28.71 78.69 16.03\nTable 4: Statistics are collected for all datasets, with the last one of each task serving as a support task for\nclustering (e.g., BoolQ, TriviaQA, and CoPA). Abbreviations: SentComp = Sentence Completion, Sci.Edu = Science\nEnducation, Edu.Exam = Enducation and Examination, Gen. = General domains.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\nlayer\n81632641282565121024\ndataset size\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nF F F\n F F F\n F F\nFigure 7: This figure presents clustering results on OPT-\n1.3B with the Wikitext Corpus, showcasing the influence\nof varying example numbers (8 to 1024). Adjacent\nlayers within the same cluster are color-coded (red or\nblue). Emulator layers are marked with F and √, where\nthe latter indicates learnable.\nare conducted using the NVIDIA RTX 3090 GPUs\nfor OPT-1.3B and A6000 GPUs for OPT-6.7B and\nLLaMA-7B.\nC Additional Experiments and Analysis\nC.1 Impact of data size for clustering\nAs illustrated in Section. 3.2, the CKA metric used\nin clustering step is computed based on the similar-\nity of representation in intermediate layers.\nAccording to previous studies (Wu et al., 2020;\nCsiszárik et al., 2021; Phang et al., 2021), increas-\ning the amount of data used results in higher accu-\nracy for the CKA metric. However, due to limited\ncomputing resources, it is challenging to load all\nexamples into memory once for computation. To\nreduce memory consumption during computation,\nNguyen et al. (2021) proposed computing linear\nCKA by averaging HSIC scores based on mini-\nbatches. However, being limited with computing\nresource, it’s hard to feed all examples into mem-\nory for computing. In contrast to their approach,\nwe focus on the clustering process itself and ana-\nlyze the results using varying sizes of examples, as\nshown in Figure 7. Our findings indicate that the\nclustering results tend to stabilize as the data size\nincreases up to 128. Based on these findings, down-\nstream users can now upload only a small amount\nof data relevant to their task, resulting in improved\nclustering.\nC.2 Combined with parameter-efficient\ntuning\nOFT is independent of parameter-efficient tuning\nmethods, including Adapter (Houlsby et al., 2019),\nBitFit (Ben Zaken et al., 2022), and LoRA (Hu\net al., 2022). For example, in the case of LoRA,\nonly 590K parameters need to be updated for OPT-\n1.3B, compared to 201M for OFT and 1208M for\nthe full model. This demonstrates the parameter-\nand resource-efficiency achieved through OFT.\nBased on results presented in Figure 8, we observe\nthat CRaSh is also compatible with LoRA and ex-\nhibits superior performance. Nevertheless, there is\nstill a noticeable performance decline on various\ndatasets that requires further exploration in future\nstudies.\nC.3 Sharing among different layers\nLayer sharing is a beneficial approach to en-\nhance model capacity when faced with limited re-\nsources (Lan et al., 2020). The downstream user\nhas the option to either share weights among layers\nor not, depending on the emulator’s performance.\nIn order to investigate the impact of weight sharing\nacross different layers, we substitute each layer in\nLLMs with another layer. As illustrated in Figure 9,\nthe heatmap above displays the loss on the valida-\ntion dataset when replacing layer i with layer j.\nThe diagonal represents the original LLMs, where\nthe loss is the lowest, and locations near the di-\nagonal also exhibit low loss. Additionally, in the\nheatmap below, we compare the representations\nfrom the last layer of the original LLM and the\nlayer sharing models, which demonstrates a similar\n9627\nopenbookqa arc-easy arc-challenge web-question\n0\n10\n20\n30\n40\n50\n60Plug-in Accuracy\nModel\nFull\nFull+LoRA\nUniform\nUniform+LoRA\nCRaSh\nCRaSh+LoRA\nFigure 8: The table presents the plug-in accuracy for\nfour datasets when combining LoRA with different\nstrategies. It is evident that CRaSh performs well with\nLoRA compared to the uniform strategy. Although\nLoRA significantly reduces the number of learnable pa-\nrameters by optimizing the model in a low-rank space,\nit negatively impacts performance in almost all datasets.\nphenomenon. These analyses collectively indicate\nthat weights sharing between adjacent layers can\nmaintain comparable performance to the primary\nmodel. This finding supports the effectiveness of\nthe layer sharing steps in CRaSh.\nC.4 Comparing layer clustering to module\ncruciality\nAs demonstrated in Figure 10, we showcase mod-\nule cruciality through the process of rewinding and\nremoving layers. The findings reveal that the ma-\njority of important neurons are concentrated in the\nmiddle and high layers of the model. This sup-\nports our decision to uniformly select learnable\nlayers. However, further investigation is required\nfor a more comprehensive understanding of this\nphenomenon.\nD Related Works\nIn this section, we first review the adaptation meth-\nods for LLMs, then introduce a technique called\nfine-tuning without full models. After that, we\ndiscuss the compression methods for LLMs. Fi-\nnally, we present similarity methods for interpret-\ning LLMs to enhance adaptation.\nParameter- and resource-efficient fine-tuning\nhas emerged as a popular method for adapting\nLLMs. Instead of directly updating pre-trained\nmodels for downstream tasks, these methods focus\non updating or adding a minimal number of pa-\nrameters, sometimes not requiring the involvement\nof the entire parameter set. Regarding parameter-\nefficient fine-tuning, Houlsby et al. (2019) intro-\nduced task-specific modules called adapters, such\nas bottleneck networks, into LLMs. Taking into\naccount the intrinsic space of LLMs (Aghajanyan\net al., 2021), LoRA (Hu et al., 2022) optimizes\nLLMs in a low-rank space, where additional ma-\ntrices are updated and can be directly added to the\noriginal parameter matrix. Except for inserting new\nparameters, two other approaches, BitFit (Ben Za-\nken et al., 2022) and skill neurons (Wang et al.,\n2022a), aim to identify task-related parameters\nwithin LLMs and update them to incorporate task-\nspecific knowledge while keeping the remaining\nmodel parameters frozen. Another approach is\nblack-box tuning (BBT) (Sun et al., 2022b,a),\nwhich involves LLMs that are inaccessible to users.\nIn this method, the continuous prompt prepended\nto the input text is optimized using derivative-free\noptimization techniques. However, in these meth-\nods, the full models are required to participate in\nforward propagation, and they do not provide any\nassistance in protecting the privacy of LLMs and\ndata. Another approach is resource-efficient fine-\ntuning, which involves evolving only a subset of\nparameters during the fine-tuning process (Sung\net al., 2022; Chen et al., 2023). These methods\nsolely focus on transferring knowledge in one di-\nrection to downstream tasks and do not facilitate\nthe transfer of updated weights back to the original\nmodel.\nFine-tuning without using full models is moti-\nvated by the need for privacy protection, which\nis not adequately addressed by current transfer\nlearning methods. Federated learning (FL) (McMa-\nhan et al., 2017) involves distinguishing server and\nclient models, where training is performed locally\non the client and then aggregated. However, the\nclient and server retain the same model, which com-\npromises the privacy of LLMs. Another approach\nis decoupled learning, which decomposes the end-\nto-end optimization problem of neural training into\nsmaller subproblems. This objective is achieved\nthrough various techniques, such as distributed gra-\ndient descent (Huo et al., 2018; Xu et al., 2020),\nmodel assembly (Ni et al., 2023), branch-train-\ninterpolation (Li et al., 2022b; Wortsman et al.,\n2022), and collaborative development of machine\nlearning (Raffel, 2023; Kandpal et al., 2023). How-\never, these methods primarily focus on training\nfrom the beginning rather than during the fine-\ntuning phase or risk leaking the full model. To ad-\n9628\nDiscussion,LayerSharing\n16\narc_challengearc_easy\nopenbookqa\nweb_questions\n(i,j)àreplacelayeriwithlayerj<sharinglayerjwithlayeri>\n Above:lossonvalidationsetBelow:representationsimilarityoflastlayer\nWhydoeslayersharingwork?\nFigure 9: Analysis on layer sharing. (i, j) means replacing layer i with layer j, i.e. sharing layer j with layer i.\nAbove: loss on validation set. Below: representation similarity compared to last layer.\nFigure 10: Given a full finetuned model, we evaluate\nspecific layer importance via rewinding weights of layer\nto initialization (above) and directly removing layer\nfrom finetuned model (below).\ndress this issue, Xiao et al. (2023) propose Offsite-\nTuning, a novel approach that involves transferring\ncompressed versions of LLMs to downstream tasks\nand performing fine-tuning on privacy-sensitive\nuser data. The updated weights are then transferred\nback and incorporated into LLMs. This method\nshows promise for protecting the privacy of LLMs\nand user data. This work is highly relevant to our\nresearch, and we propose a training-free strategy to\nenhance offsite-tuning by leveraging the similari-\nties among neural networks.\nCompression methods for LLMs To address\nresource-constrained scenarios, various technolo-\ngies such as knowledge distillation (Hinton et al.,\n2015; Sun et al., 2019; Gordon et al., 2020), prun-\ning (Chen et al., 2020; Liu et al., 2022), and quan-\ntization (Tao et al., 2022) have been employed in\npre-trained models such as BERT, T5, and GPT-2.\nHowever, with the increasing size of LLMs, such as\nGPT-3 which consists of 96 layers, knowledge dis-\ntillation becomes challenging. With the increasing\npopularity of post-quantization, methods such as\nGPTQ (Frantar et al., 2023) and QLoRA (Dettmers\net al., 2023) have emerged to reduce the computa-\ntional resources required by LLMs. However, these\nmethods still encounter challenges related to loss\nof precision and model leakage, posing threats to\nthe privacy and safety of LLMs. SparseGPT (Fran-\ntar and Alistarh, 2023) demonstrated, for the first\ntime, the possibility of pruning large-scale gener-\native pretrained transformer (GPT) family models\nto achieve at least 50% sparsity in a single opera-\n9629\ntion. In comparison to the aforementioned methods,\nLayerDrop is an effective technique for reducing\nthe parameters of LLMs while maintaining com-\nparable performance. Fan et al. (2020) explored\nthe use of LayerDrop to reduce the depth of trans-\nformers, but this approach relies on a specialized\ntraining strategy. Building on this, Zhang and He\n(2020) proposed the concept of layer drop to ex-\npedite training. These studies primarily focus on\ntraining with layer drop during the training pro-\ncess rather than during post-training. Sajjad et al.\n(2023) investigated various strategies for imple-\nmenting layer drop on pre-trained models such as\nBERT, XLNet, and GPT-2, with a particular empha-\nsis on the application of heuristic rules. Building\nupon the concept of \"lottery tickets\" (Frankle and\nCarbin, 2019), which involves discovering sparse\nsubnetworks within trained dense networks that can\nachieve comparable accuracy, Jordao et al. (2023)\nconfirmed the presence of winning tickets when\nlayers are pruned in vision models. Insufficient\nexploration has been conducted on the application\nof layer dropping in large-depth neural networks\nsuch as LLMs.\nNeural network similarity primarily encom-\npasses representation similarity and functional sim-\nilarity. Representation similarity refers to the simi-\nlarity of hidden states across all layers, while func-\ntional similarity indicates that two models exhibit\nthe same behavior on the given inputs (Klabunde\net al., 2023). By measuring the similarity of neu-\nral network representations, we can gain insights\ninto the reasons behind variations in model be-\nhavior. Kornblith et al. (2019) revisited previous\nmethods for comparing neural network represen-\ntations and introduced centered kernel alignment\n(CKA), a technique that measures the relationship\nbetween representations without being affected by\nhigh dimensionality or the number of data points.\nExtensive exploration has been conducted in the\nfield of computer vision. In comparison to CNN\nmodels, Raghu et al. (2021) discovered significant\ndifferences in the representation structure between\nVision Transformers (ViTs) and convolutional net-\nworks. ViTs exhibit highly similar representations\nacross all layers, whereas ResNet models demon-\nstrate lower similarity between lower and higher\nlayers, indicating the absence of a block structure\nin the representation of transformer models. In the\nstudy of large language models, researchers have\nanalyzed representation similarity across different\npre-trained models, comparing them within various\nmodel families (Wu et al., 2020), and have inves-\ntigated the dynamics of embeddings during fine-\ntuning (Merchant et al., 2020). Phang et al. (2021)\ndiscovered the clustering of layer representations\nin fine-tuned transformers, providing evidence for\nthe presence of layer redundancy in LLMs (Dalvi\net al., 2020). As the size increases, we observe that\nclustering also occurs in original pre-trained mod-\nels, not just in specialized models. Recognizing the\nwide applicability of this concept in various fields,\nrecent studies have aimed to gain insights into the\nfunctional aspects of LLMs. nostalgebraist (2020)\ndirectly mapped the hidden states of intermediate\nlayers to the final classification layer to obtain hid-\nden predictions. Considering the phenomenon of\nrepresentation drift within hidden layers, especially\nin lower layers, Belrose et al. (2023) propose tuned-\nlens, a method that trains projections for each layer\nusing a small amount of pre-trained corpus. These\nmethods indicate that the functional behaviors of\nhidden layers also undergo subtle changes as the\ndepth of the layers increases. Building upon these\nobservations, Merullo et al. (2023) discovered that\nlanguage models implement simple word2vec-style\nvector arithmetic (Mikolov et al., 2013), which\nsheds light on the prediction process of LLMs. In-\nstead of using the transformation layer for decod-\ning, Bills et al. (2023) attempted to interpret GPT-2\nhidden states using GPT-4 (OpenAI, 2023), open-\ning up the possibility of automatically aligning the\nbehavior of models with AI.\n9630\nOpenBookQA\nWhich organism cannot specialize?\nprotozoa\nA person can grow cabbage in January with the help of what product?\nGreen house\nPIQA\nQuestion: To fight Ivan Drago in Rocky for sega master system.\nAnswer:\nYou have to defeat Apollo Creed and Clubber Lang first.\nQuestion: Make outdoor pillow.\nAnswer:\nBlow into trash bag and tie with rubber band.\nSciQ\nA wetland is an area that is wet for all or part of the year. Wetlands are home to certain types of plants. Question: What\nis an area of land called that is wet for all or part of the year?\nAnswer:\nwetland\nQuestion: Surface waters are heated by the radiation from?\nAnswer:\nthe sun\nRACE\nArticle: I am a psychologist. I first met Timothy, a quiet, overweight eleven-year-old boy, when his mother brought him\nto me to discuss his declining grades. A few minutes with Timothy were enough to confirm that his self-esteem and\ngeneral happiness were falling right along with _ . I asked about Timothy’s typical day. He awoke every morning at six\nthirty so he could reach his school by eight and arrived home around four thirty each afternoon. He then had a quick\nsnack, followed by either a piano lesson or a lesson with his math tutor. He finished dinner at 7 pm, and then he sat down\nto do homework for two to three hours. Quickly doing the math in my head, I found that Timothy spent an average of\nthirteen hours a day at a writing desk.\nWhat if Timothy spent thirteen hours a day at a sewing machine instead of a desk? We would immediately be shocked,\nbecause that would be called children being horribly mistreated. Timothy was far from being mistreated, but the mountain\nof homework he faced daily resulted in a similar consequence –he was being robbed of his childhood. In fact, Timothy\nhad no time to do anything he truly enjoyed, such as playing video games, watching movies, or playing board games\nwith his friends. Play, however, is a crucial part of healthy child development. It affects children’s creativity, their social\nskills, and even their brain development. The absence of play, physical exercise, and freefrom social interaction takes a\nserious toll on many children. It can also cause significant health problems like childhood obesity, sleep problems and\ndepression.\nExperts in the field recommend the minutes children spend on their homework should be no more than ten times the\nnumber of their grade level. As a fifthgrader, Timothy should have no more than fifty minutes a day of homework\n(instead of three times that amount). Having an extra two hours an evening to play, relax, or see a friend would soundly\nbenefit any child’s life quality.\nQuestion: According to the passage, how long should a thirdgrader spend a day doing homework?\nAnswer:\nNo more than thirty minutes.\nBoolQ <support set>\nPhantom pain – Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or\nan organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb\ndeficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\nQuestion: is pain experienced in a missing body part or paralyzed area?\nAnswer:\nyes\nTable 5: Instructions format of Multi-Choice QA task.\n9631\nARC-E\nQuestion: Which technology was developed most recently?\nAnswer:\ncellular telephone\nQuestion: A student hypothesizes that algae are producers. Which question will best help the student determine if this is\ncorrect?\nAnswer:\nDo algae use sunlight to make food?\nARC-C\nQuestion: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What\nshould they do so they can repeat their investigation?\nAnswer:\nRecord the details of the investigation.\nQuestion: High-pressure systems stop air from rising into the colder regions of the atmosphere where water can condense.\nWhat will most likely result if a high-pressure system remains in an area for a long period of time?\nAnswer:\ndrought\nWebQS\nQuestion: what is the name of justin bieber brother?\nAnswer:\nJazmyn Bieber\nQuestion: what character did natalie portman play in star wars?\nAnswer:\nPadmé Amidala\nTriviaQA <support set>\nQuestion: What star sign is Jamie Lee Curtis?\nAnswer:\nScorpio\nQuestion: Which Lloyd Webber musical premiered in the US on 10th December 1993?\nAnswer:\nSunset Boulevard\nHellaSwag\nRoof shingle removal: A man is sitting on a roof. He\nstarts pulling up roofing on a roof.\nClean and jerk: A lady walks to a barbell. She bends down and grabs the pole. The lady\nstands and lifts the weight over her head.\nCoPA <support set>\nThe man turned on the faucet therefore\nwater flowed from the spout.\nThe girl found a bug in her cereal therefore\nshe lost her appetite.\nTable 6: Instructions format of Closed-Book QA and Sentece Completion task.\n9632\n0 3 6 9 12\n129630\n125m, openbookqa\n0 5 10 15 20\n20151050\n350m, openbookqa\n0 6 12 18 24\n24181260\n1.3b, openbookqa\n0 8 16 24 32\n32241680\n6.7b, openbookqa\n0 10 20 30 40\n403020100\n13b, openbookqa\n0 12 24 36 48\n483624120\n30b, openbookqa\n0 3 6 9 12\n129630\n125m, arc_challenge\n0 5 10 15 20\n20151050\n350m, arc_challenge\n0 6 12 18 24\n24181260\n1.3b, arc_challenge\n0 8 16 24 32\n32241680\n6.7b, arc_challenge\n0 10 20 30 40\n403020100\n13b, arc_challenge\n0 12 24 36 48\n483624120\n30b, arc_challenge\n0 3 6 9 12\n129630\n125m, web_questions\n0 5 10 15 20\n20151050\n350m, web_questions\n0 6 12 18 24\n24181260\n1.3b, web_questions\n0 8 16 24 32\n32241680\n6.7b, web_questions\n0 10 20 30 40\n403020100\n13b, web_questions\n0 12 24 36 48\n483624120\n30b, web_questions\n0 3 6 9 12\n129630\n125m, piqa\n0 5 10 15 20\n20151050\n350m, piqa\n0 6 12 18 24\n24181260\n1.3b, piqa\n0 8 16 24 32\n32241680\n6.7b, piqa\n0 10 20 30 40\n403020100\n13b, piqa\n0 12 24 36 48\n483624120\n30b, piqa\n0 3 6 9 12\n129630\n125m, sciq\n0 5 10 15 20\n20151050\n350m, sciq\n0 6 12 18 24\n24181260\n1.3b, sciq\n0 8 16 24 32\n32241680\n6.7b, sciq\n0 10 20 30 40\n403020100\n13b, sciq\n0 12 24 36 48\n483624120\n30b, sciq\n0 3 6 9 12\n129630\n125m, race\n0 5 10 15 20\n20151050\n350m, race\n0 6 12 18 24\n24181260\n1.3b, race\n0 8 16 24 32\n32241680\n6.7b, race\n0 10 20 30 40\n403020100\n13b, race\n0 12 24 36 48\n483624120\n30b, race\n0 3 6 9 12\n129630\n125m, hellaswag\n0 5 10 15 20\n20151050\n350m, hellaswag\n0 6 12 18 24\n24181260\n1.3b, hellaswag\n0 8 16 24 32\n32241680\n6.7b, hellaswag\n0 10 20 30 40\n403020100\n13b, hellaswag\n0 12 24 36 48\n483624120\n30b, hellaswag\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 11: The table displays the representation similarity among layers of OPT models (Zhang et al., 2022) with\ndifferent sizes, across five datasets with weighted mean pooling strategy. The rows in the table represent the layer\nsimilarity of each dataset across the 125M, 1.3B, 6.7B, and 13B OPT models. Notably, the characteristics of the\nWikitext dataset differ from the rest because it is a language modeling task similar to the pre-training objective.\n(Phang et al., 2021) introduced that fine-tuned transformers exhibit clusters of similar representations across layers,\nwhich explains the emerging block structure of the Wikitext dataset even in small language models. In contrast, the\nremaining four datasets are associated with question answering tasks, which pose challenges for smaller models.\nConsequently, the representation of these tasks demonstrates abnormal similarity in the intermediate layers of the\n125M OPT model, suggesting a relatively lower level of comprehension in it.\n9633\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, openbookqa\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, openbookqa\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, openbookqa\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, openbookqa\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, openbookqa\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, openbookqa\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, arc_challenge\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, arc_challenge\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, arc_challenge\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, arc_challenge\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, arc_challenge\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, arc_challenge\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, web_questions\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, web_questions\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, web_questions\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, web_questions\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, web_questions\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, web_questions\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, piqa\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, piqa\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, piqa\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, piqa\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, piqa\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, piqa\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, sciq\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, sciq\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, sciq\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, sciq\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, sciq\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, sciq\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, race\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, race\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0 1.3b, race\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, race\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, race\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, race\n0 3 6 9 12\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, hellaswag\n0 5 10 15 20\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0 350m, hellaswag\n0 6 12 18 24\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n1.3b, hellaswag\n0 8 16 24 32\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n6.7b, hellaswag\n0 10 20 30 40\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n13b, hellaswag\n0 12 24 36 48\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n30b, hellaswag\nFigure 12: The table displays the representation similarity among layers of OPT models (Zhang et al., 2022) with\ndifferent sizes, across five datasets with weighted mean pooling strategy.\n9634\n0 3 6 9 12\n129630\n125m, openbookqa\n0 5 10 15 20\n20151050\n350m, openbookqa\n0 6 12 18 24\n24181260\n1.3b, openbookqa\n0 8 16 24 32\n32241680\n6.7b, openbookqa\n0 10 20 30 40\n403020100\n13b, openbookqa\n0 12 24 36 48\n483624120\n30b, openbookqa\n0 3 6 9 12\n129630\n125m, arc_challenge\n0 5 10 15 20\n20151050\n350m, arc_challenge\n0 6 12 18 24\n24181260\n1.3b, arc_challenge\n0 8 16 24 32\n32241680\n6.7b, arc_challenge\n0 10 20 30 40\n403020100\n13b, arc_challenge\n0 12 24 36 48\n483624120\n30b, arc_challenge\n0 3 6 9 12\n129630\n125m, web_questions\n0 5 10 15 20\n20151050\n350m, web_questions\n0 6 12 18 24\n24181260\n1.3b, web_questions\n0 8 16 24 32\n32241680\n6.7b, web_questions\n0 10 20 30 40\n403020100\n13b, web_questions\n0 12 24 36 48\n483624120\n30b, web_questions\n0 3 6 9 12\n129630\n125m, piqa\n0 5 10 15 20\n20151050\n350m, piqa\n0 6 12 18 24\n24181260\n1.3b, piqa\n0 8 16 24 32\n32241680\n6.7b, piqa\n0 10 20 30 40\n403020100\n13b, piqa\n0 12 24 36 48\n483624120\n30b, piqa\n0 3 6 9 12\n129630\n125m, sciq\n0 5 10 15 20\n20151050\n350m, sciq\n0 6 12 18 24\n24181260\n1.3b, sciq\n0 8 16 24 32\n32241680\n6.7b, sciq\n0 10 20 30 40\n403020100\n13b, sciq\n0 12 24 36 48\n483624120\n30b, sciq\n0 3 6 9 12\n129630\n125m, race\n0 5 10 15 20\n20151050\n350m, race\n0 6 12 18 24\n24181260\n1.3b, race\n0 8 16 24 32\n32241680\n6.7b, race\n0 10 20 30 40\n403020100\n13b, race\n0 12 24 36 48\n483624120\n30b, race\n0 3 6 9 12\n129630\n125m, hellaswag\n0 5 10 15 20\n20151050\n350m, hellaswag\n0 6 12 18 24\n24181260\n1.3b, hellaswag\n0 8 16 24 32\n32241680\n6.7b, hellaswag\n0 10 20 30 40\n403020100\n13b, hellaswag\n0 12 24 36 48\n483624120\n30b, hellaswag\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 13: The table displays the representation similarity among layers of OPT models (Zhang et al., 2022) with\ndifferent sizes, across five datasets with simple mean pooling strategy.\n9635\n0 3 6 9 12\n0.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, openbookqa\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, openbookqa\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, openbookqa\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, openbookqa\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, openbookqa\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, openbookqa\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, arc_challenge\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, arc_challenge\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, arc_challenge\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, arc_challenge\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, arc_challenge\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, arc_challenge\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, web_questions\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0350m, web_questions\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, web_questions\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, web_questions\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, web_questions\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, web_questions\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, piqa\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, piqa\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, piqa\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, piqa\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, piqa\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, piqa\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, sciq\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, sciq\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, sciq\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, sciq\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, sciq\n0 12 24 36 480.2\n0.4\n0.6\n0.8\n1.0\n30b, sciq\n0 3 6 9 120.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, race\n0 5 10 15 200.2\n0.4\n0.6\n0.8\n1.0 350m, race\n0 6 12 18 240.2\n0.4\n0.6\n0.8\n1.0\n1.3b, race\n0 8 16 24 320.2\n0.4\n0.6\n0.8\n1.0\n6.7b, race\n0 10 20 30 400.2\n0.4\n0.6\n0.8\n1.0\n13b, race\n0 12 24 36 48\n0.2\n0.4\n0.6\n0.8\n1.0\n30b, race\n0 3 6 9 12\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0Similarity\n125m, hellaswag\n0 5 10 15 20\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0 350m, hellaswag\n0 6 12 18 24\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n1.3b, hellaswag\n0 8 16 24 32\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n6.7b, hellaswag\n0 10 20 30 40\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n13b, hellaswag\n0 12 24 36 48\nLayer\n0.2\n0.4\n0.6\n0.8\n1.0\n30b, hellaswag\nFigure 14: The table displays the representation similarity among layers of OPT models (Zhang et al., 2022) with\ndifferent sizes, across five datasets with simple mean pooling strategy.\n9636\n0 8 16 24 32\n32241680\n7b, wikitext\n0 10 20 30 40\n403020100\n13b, wikitext\n0 15 30 45 60\n604530150\n30b, wikitext\n0 8 16 24 32\n32241680\n7b, arc_easy\n0 10 20 30 40\n403020100\n13b, arc_easy\n0 15 30 45 60\n604530150\n30b, arc_easy\n0 8 16 24 32\n32241680\n7b, arc_challenge\n0 10 20 30 40\n403020100\n13b, arc_challenge\n0 15 30 45 60\n604530150\n30b, arc_challenge\n0 8 16 24 32\n32241680\n7b, piqa\n0 10 20 30 40\n403020100\n13b, piqa\n0 15 30 45 60\n604530150\n30b, piqa\n0 8 16 24 32\n32241680\n7b, sciq\n0 10 20 30 40\n403020100\n13b, sciq\n0 15 30 45 60\n604530150\n30b, sciq\n0 8 16 24 32\n32241680\n7b, race\n0 10 20 30 40\n403020100\n13b, race\n0 15 30 45 60\n604530150\n30b, race\n0 8 16 24 32\n32241680\n7b, hellaswag\n0 10 20 30 40\n403020100\n13b, hellaswag\n0 15 30 45 60\n604530150\n30b, hellaswag\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) Weighted mean pooling on LLAMA.\n0 8 16 24 32\n32241680\n7b, wikitext\n0 10 20 30 40\n403020100\n13b, wikitext\n0 15 30 45 60\n604530150\n30b, wikitext\n0 8 16 24 32\n32241680\n7b, arc_easy\n0 10 20 30 40\n403020100\n13b, arc_easy\n0 15 30 45 60\n604530150\n30b, arc_easy\n0 8 16 24 32\n32241680\n7b, arc_challenge\n0 10 20 30 40\n403020100\n13b, arc_challenge\n0 15 30 45 60\n604530150\n30b, arc_challenge\n0 8 16 24 32\n32241680\n7b, piqa\n0 10 20 30 40\n403020100\n13b, piqa\n0 15 30 45 60\n604530150\n30b, piqa\n0 8 16 24 32\n32241680\n7b, sciq\n0 10 20 30 40\n403020100\n13b, sciq\n0 15 30 45 60\n604530150\n30b, sciq\n0 8 16 24 32\n32241680\n7b, race\n0 10 20 30 40\n403020100\n13b, race\n0 15 30 45 60\n604530150\n30b, race\n0 8 16 24 32\n32241680\n7b, hellaswag\n0 10 20 30 40\n403020100\n13b, hellaswag\n0 15 30 45 60\n604530150\n30b, hellaswag\n0.2\n0.4\n0.6\n0.8\n1.0\n (b) Mean pooling on LLAMA.\nFigure 15: This table illustrates the presence of a modular structure in LLAMA models. We observe that weighted\nmean pooling performs well on LLAMA models, while direct mean pooling may lead to information loss and\nabnormal structures, particularly in the case of 30B models.\n9637",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6691264510154724
    },
    {
      "name": "Cluster analysis",
      "score": 0.6107699871063232
    },
    {
      "name": "Crash",
      "score": 0.5260926485061646
    },
    {
      "name": "Modular design",
      "score": 0.44737210869789124
    },
    {
      "name": "Representation (politics)",
      "score": 0.4350104033946991
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2952198088169098
    },
    {
      "name": "Programming language",
      "score": 0.08985310792922974
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}