{
    "title": "Multimodal Transformer for Multimodal Machine Translation",
    "url": "https://openalex.org/W3034773362",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2136007140",
            "name": "Shaowei Yao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101284925",
            "name": "Xiao-jun Wan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963909453",
        "https://openalex.org/W2509282593",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2903343986",
        "https://openalex.org/W2963988211",
        "https://openalex.org/W2950207430",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2896234464",
        "https://openalex.org/W2593341061",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2805516822",
        "https://openalex.org/W2950886580",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W2581101319",
        "https://openalex.org/W2889903020",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2963331233"
    ],
    "abstract": "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4346–4350\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4346\nMultimodal Transformer for Multimodal Machine Translation\nShaowei Yao, Xiaojun Wan\nCenter for Data Science, Peking University\nWangxuan Institute of Computer Technology, Peking University\nThe MOE Key Laboratory of Computational Linguistics, Peking University\n{yaosw,wanxiaojun}@pku.edu.cn\nAbstract\nMultimodal Machine Translation (MMT) aims\nto introduce information from other modality,\ngenerally static images, to improve the transla-\ntion quality. Previous works propose various\nincorporation methods, but most of them do\nnot consider the relative importance of multi-\nple modalities. In MMT, equally treating text\nand images may encode too much irrelevant in-\nformation from images which may introduce\nnoise. In this paper, we propose the multi-\nmodal self-attention in Transformer to solve\nthe issues above. The proposed method learns\nthe representations of images based on the\ntext, which avoids encoding irrelevant informa-\ntion in images. Experiments and visualization\nanalysis demonstrate that our model beneﬁts\nfrom visual information and substantially out-\nperforms previous works and competitive base-\nlines in terms of various metrics.\n1 Introduction\nMultimodal machine translation (MMT) is a novel\nmachine translation (MT) task which aims at de-\nsigning better translation systems using context\nfrom an additional modality, usually images (See\nFigure 1). It initially organized as a shared task\nwithin the First Conference on Machine Transla-\ntion (Specia et al., 2016; Elliott et al., 2017; Bar-\nrault et al., 2018). Current works focus on the\ndataset named Multi30k (Elliott et al., 2016), a\nmultilingual extension of Flickr30k dataset with\ntranslations of the English image descriptions into\ndifferent languages.\nPrevious works propose various incorporation\nmethods. Calixto and Liu (2017) utilize global im-\nage features to initialize the encoder/decoder hid-\nden states of RNN. Elliott and K´ad´ar (2017) model\nthe source sentence and reconstruct the image repre-\nsentation jointly via multi-task learning. Recently,\nIve et al. (2019) propose a translate-and-reﬁne ap-\nFigure 1: An Example for Multimodal Machine Trans-\nlation.\nproach using two-stage decoder based on Trans-\nformer (Vaswani et al., 2017). Calixto et al. (2019)\nput forward a latent variable model to learn the in-\nteraction between visual and textual features. How-\never, in multimodal tasks the different modalities\nusually are not equally important. For example, in\nMMT the text is obviously more important than\nimages. Although the image carries richer infor-\nmation, it also contains more irrelevant content.\nIf we directly encode the image features, it may\nintroduce a lot of noise.\nTo address the issues above, we propose the mul-\ntimodal Transformer. The proposed model does not\ndirectly encode image features. Instead, the hidden\nrepresentations of images are induced from the text\nunder the guide of image-aware attention. Mean-\nwhile, we introduce a better way to incorporate\ninformation from other modality based on a graph\nperspective of Transformer. Experimental results\nand visualization show that our model can make\ngood use of visual information and substantially\noutperforms the current state of the art.\n2 Methodology\nOur model is adapted from Transformer and it is\nalso an encoder-decoder architecture, consisting of\nstacked encoder and decoder layers. The focus of\nour work is to build a powerful encoder to incorpo-\nrate the information from other modality. Thus, we\nwill ﬁrst begin with an introduction to the incorpo-\n4347\nration method. Then we will detail the multimodal\nself-attention. The ﬁnal representations of text and\nimages are sent to the sequence decoder to generate\nthe target text.\n2.1 Incorporating Method\nThe method of incorporating information from\nother modality is based on a graph perspective\nof Transformer. The core of Transformer is self-\nattention which employs the multi-head mecha-\nnism. Each attention head operates on an input se-\nquence x= (x1,...,x n) of nelements where xi ∈\nRd, and computes a new sequence z= (z1,...,z n)\nof the same length where z∈Rd:\nzi =\nn∑\nj=1\nαij\n(\nxjWV )\n(1)\nwhere αij is weight coefﬁcient computed by a soft-\nmax function:\nαij = softmax\n((\nxiWQ)(\nxjWK)T\n√\nd\n)\n(2)\nWV ,WQ,WV ∈Rd×d are layer-speciﬁc trainable\nparameter matrices.\nThus we can see that each word representation\nis induced from all the other words. If we consider\nevery word to be a node, then Transformer can be\nregarded as a variant of GNN which treats each\nsentence as a fully-connected graph with words\nas nodes (Battaglia et al., 2018; Yao et al., 2020).\nIn traditional MT tasks, the source sentence graph\nonly contains nodes with text information. If we\nwant to incorporate information from other modal-\nity, we should add the nodes with other modality\ninformation into the source graph. Therefore, as\nthe words are local semantic representations of the\nsentence, we extract the spatial features which are\nthe semantic representations of local spatial regions\nof the image. We add the spatial features of the\nimage as pseudo-words in the source sentence and\nfeed it into the multimodal self-attention layer.\n2.2 Multimodal Self-attention\nAs stated before, in MMT the text and images are\nnot equally important. Directly encoding images\nwhich contain a lot of irrelevant content may intro-\nduce noise. Therefore, we propose the multimodal\nself-attention to encode multimodal information.\nIn multimodal self-attention, the hidden represen-\ntations of the image are induced from text under\nFigure 2: Multimodal self-attention\nthe guide of image-aware attention which provides\na latent adaptation from the text to the image. A\nvisual representation is illustrated in Figure 2.\nFormally, we consider two modalities textand\nimg, with two entries from each of them denoted\nby xtext ∈Rn×d and ximgWimg ∈Rp×d, respec-\ntively. The output of multimodal self-attention is\ncomputed as follows:\nci =\nn∑\nj=1\n˜αij\n(\nxtext\nj WV )\n(3)\nwhere ˜αij is weight coefﬁcient computed by a soft-\nmax function:\n˜αij = softmax\n\n\n(\n˜xiWQ)(\nxtext\nj WK\n)T\n√\nd\n\n (4)\nwhere c∈R(n+p)×d is the hidden representation\nof words and the image. At last layer, cis fed into\nsequence decoder to generate target sequence. We\ncan see that the hidden representations of the image\nis only induced from words but under the guide\nof image-aware attention. The extracted spatial\nfeatures of the image are not directly encoded in\nthe model. Instead, they adjust the attention of\neach word to compute the hidden representations\nof the image. In each encoder layer we also employ\n4348\nresidual connections between each layer as well as\nlayer normalization. And the decoder are followed\nthe standard implemention of Transformer.\n3 Experiment\n3.1 Baselines and Metrics\nWe compare the performance of our model with pre-\nvious kinds of models: (1) sequence-to-sequence\nmodel only trained on text data (LSTM, Trans-\nformer). (2) Previous works trained on both text\nand image data. We evaluated the translation qual-\nity of our model in terms of BLEU (Papineni et al.,\n2002) and METEOR (Denkowski and Lavie, 2014),\nwhich have been used in most previous works.\n3.2 Datasets\nWe build and test our model on the Multi30k\ndataset (Elliott et al., 2016), which consists of two\nmultilingual expansions of the original Flickr30k\ndataset referred to as M30kT and M30KC, respec-\ntively. Multi30k contains 30k images, and for each\nof the images, M30k T has one of its English de-\nscription manually translated into German by a\nprofessional translator. M30K C has ﬁve English\ndescriptions and ﬁve German descriptions, but the\nGerman descriptions were crowdsourced indepen-\ndently from their English versions. The training,\nvalidation, test sets of Multi30k contain 29k, 1014\nand 1k instances respectively. We use M30kT as\nthe original training data and M30kC for building\nadditional back-translated training data following\nCalixto et al. (2019). We present our experiment\nresults on English-German (En-De) Test2016. We\nuse LSTM trained on the textual part of the M30KT\ndataset (De-En, the original 29k sentences) without\nimages to build a back-translation model (Sennrich\net al., 2016), and then apply this model to translate\n145k monolingual German description in M30kC\ninto English as additional training data. This part\nof data we refer to as back-translated data.\n3.3 Settings\nWe preprocess the data by tokenizing and lower-\ncasing. Word embeddings are initialized using\npretrained 300-dimensional Glove vectors. we ex-\ntract spatial image features from the last convolu-\ntional layer of ResNet-50. The spatial features are\n7 ×7 ×2048-dimensional vectors which are repre-\nsentations of local spatial regions of the image.\nOur encoder and decoder have both 6 layers with\n300-dimensional word embeddings and hidden\nModel BLEU4 METEOR\nLSTM 36.8 54.9\nTransformer 37.8 55.3\nIMGD (Calixto and Liu, 2017) 37.3 55.1\nNMTSRC+IMG(Calixto et al., 2017)36.5 55.0\nTransformer+Att (Ive et al., 2019)36.9 54.5\nDel+obj (Ive et al., 2019) 38.0 55.6\nVMMTF (Calixto et al., 2019) 37.6 56.0\nOurs 38.7 55.7\n+ back-translated data\nIMGD (Calixto and Liu, 2017) 38.5 55.9\nNMTSRC+IMG(Calixto et al., 2017)37.1 54.5\nVMMTF (Calixto et al., 2019) 38.4 56.3\nOurs 39.5 56.9\nTable 1: Comparison results on the Multi30k test set.\nThe best baseline results are underlined. Bold high-\nlights statistically signiﬁcant improvements.\nstates. We employ 10 heads here and dropout=0.1.\nWe used Adam optimizer (Kingma and Ba, 2014)\nwith β1 = 0.9, β2 = 0.98 and minibatches of size\n32 or 128 (depends on if add the back-translated\ndata). Meanwhile, we increase learning rate lin-\nearly for the ﬁrst warmup steps, and decrease it\nthereafter proportionally to the inverse square root\nof the step number. We used warmup steps =\n8000. The similar learning rate schedule is adopted\nin (Vaswani et al., 2017).\n3.4 Results\nThe results of all methods are shown in Table 1. We\ncan see our Transformer baseline has comparable\nresults compared to most previous works, When\ntrained on the original data, our model substantially\noutperforms the SoTA according to BLEU and gets\na competitive result according to METEOR. More-\nover, we note that our model surpasses the text-only\nbaseline by above 1 BLEU points. It demonstrates\nthat our model beneﬁts a lot from the visual modal-\nity.\nTo further investigate our model performance on\nmore data, we also train the models with additional\nback-translated data, and the comparison results\nare shown in the lower part of Table 1. We can\nsee that almost all models get improved with the\nadditional training data, but our model obtains the\nmost improvements and achieving new SoTA re-\nsults on all metrics. It demonstrates that our model\nwill perform better on the larger dataset.\n4349\nFigure 3: Translation cases and Visualization. Colored words represent some of the improvements.\n3.5 Visualization Analysis\nFigure 3 depicts translations for two cases in the\ntest set. Colors highlight improvement. Further-\nmore, we visualize the contributions of different\nlocal regions of the image in different attention\nheads, which shows our model can focus on the\nappropriate regions of the image. For example, our\nmodel pays more attention to the building and the\nperson in the ﬁrst case, and thus the model under-\nstands that the person is working on the building\nrather than just standing there. In the second case,\nmost attention heads attend to the balance beam\nand the jean dress of the girl, avoiding errors in the\ntranslation.\n3.6 Ablation Study\nTo further study the inﬂuence of the individual com-\nponents in our model, we conduct ablation experi-\nments to better understand their relative importance.\nThe results are presented in Table 2. Firstly, we\ninvestigate the effect of multimodal self-attention.\nAs shown in the second columns (replace with self-\nattention) in Table 2. If we simply concatenate the\nword vectors with the image features and then per-\nform self-attention, we will lose 0.6 BLEU score\nand 0.4 METEOR score. Inspired by Elliott (2018),\nwe further examine the utility of the image by the\nadversarial evaluation. When we replace all input\nimages with a blank picture, the performance of\nthe model drops a lot. When we replace all input\nimages with a random image (the context of image\ndoes not match the description in the sentence pair),\nthe model performs even worse than the text-only\nmodel. The image here is actually a noise which\ndistracts the translation.\nBLEU4 MEMTEOR\nFull Model 38.7 55.7\n- replace with self-attention 38.1 55.3\n- replace with blank images 37.1 54.8\n- replace with random images 36.7 54.5\nTable 2: Inﬂuence of different components in our\nmodel.\n4 Conclusion\nIn this paper, we propose the multimodal self-\nattention to consider the relative importance be-\ntween different modalities in the MMT task. The\nhidden representations of less important modality\n(image) are induced from the important modality\n(text) under the guide of image-aware attention.\nThe experiments and visualization show that our\nmodel can make good use of multimodal infor-\nmation and get better performance than previous\nworks.\nThere are various multimodal tasks where mul-\ntiple modalities have different relative importance.\nIn future work, we would like to investigate the\neffectiveness of our model in these tasks.\n4350\nAcknowledgments\nThis work was supported by National Natural Sci-\nence Foundation of China (61772036), MSRA Col-\nlaborative Research Program, and Key Laboratory\nof Science, Technology and Standard in Press In-\ndustry (Key Laboratory of Intelligent Press Media\nTechnology). We thank the anonymous reviewers\nfor their helpful comments. Xiaojun Wan is the\ncorresponding author.\nReferences\nLo¨ıc Barrault, Fethi Bougares, Lucia Specia, Chiraag\nLala, Desmond Elliott, and Stella Frank. 2018. Find-\nings of the third shared task on multimodal machine\ntranslation. In Proceedings of the Third Conference\non Machine Translation: Shared Task Papers, pages\n304–323.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst,\nAlvaro Sanchez-Gonzalez, Vinicius Zambaldi, Ma-\nteusz Malinowski, Andrea Tacchetti, David Raposo,\nAdam Santoro, Ryan Faulkner, et al. 2018. Rela-\ntional inductive biases, deep learning, and graph net-\nworks. arXiv preprint arXiv:1806.01261.\nIacer Calixto and Qun Liu. 2017. Incorporating global\nvisual features into attention-based neural machine\ntranslation. In EMNLP, pages 992–1003.\nIacer Calixto, Qun Liu, and Nick Campbell. 2017.\nDoubly-attentive decoder for multi-modal neural\nmachine translation. In ACL, pages 1913–1924.\nIacer Calixto, Miguel Rios, and Wilker Aziz. 2019. La-\ntent variable model for multi-modal translation. In\nACL, pages 6392–6405.\nMichael Denkowski and Alon Lavie. 2014. Meteor uni-\nversal: Language speciﬁc translation evaluation for\nany target language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation, pages\n376–380.\nDesmond Elliott. 2018. Adversarial evaluation of mul-\ntimodal machine translation. In EMNLP, pages\n2974–2978.\nDesmond Elliott, Stella Frank, Lo ¨ıc Barrault, Fethi\nBougares, and Lucia Specia. 2017. Findings of the\nsecond shared task on multimodal machine transla-\ntion and multilingual image description. In Proceed-\nings of the Second Conference on Machine Transla-\ntion, pages 215–233.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30K: Multilingual English-\nGerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–\n74.\nDesmond Elliott and ´Akos K ´ad´ar. 2017. Imagination\nimproves multimodal translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing, pages 130–141.\nJulia Ive, Pranava Madhyastha, and Lucia Specia. 2019.\nDistilling translations with visual awareness. In\nACL, pages 6525–6538.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In ICLR.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL, pages 311–\n318.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In ACL, pages 86–96.\nLucia Specia, Stella Frank, Khalil Simaan, and\nDesmond Elliott. 2016. A shared task on multi-\nmodal machine translation and crosslingual image\ndescription. In Proceedings of the First Conference\non Machine Translation: Volume 2, Shared Task Pa-\npers, pages 543–553.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nShaowei Yao, Tianming Wang, and Xiaojun Wan.\n2020. Heterogeneous graph transformer for graph-\nto-sequence learning. In Proceedings of the Associ-\nation for Computational Linguistics (ACL)."
}