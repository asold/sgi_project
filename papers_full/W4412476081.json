{
  "title": "Comparing the persuasiveness of role-playing large language models and human experts on polarized U.S. political issues",
  "url": "https://openalex.org/W4412476081",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4281349537",
      "name": "Kobi Hackenburg",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2787205778",
      "name": "Lujain Ibrahim",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2559345414",
      "name": "Ben M Tappin",
      "affiliations": [
        "School of Advanced Study",
        "Royal Holloway University of London"
      ]
    },
    {
      "id": "https://openalex.org/A1904490283",
      "name": "Manos Tsakiris",
      "affiliations": [
        "School of Advanced Study",
        "Royal Holloway University of London"
      ]
    },
    {
      "id": "https://openalex.org/A4281349537",
      "name": "Kobi Hackenburg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2787205778",
      "name": "Lujain Ibrahim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2559345414",
      "name": "Ben M Tappin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1904490283",
      "name": "Manos Tsakiris",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4319265466",
    "https://openalex.org/W2118363134",
    "https://openalex.org/W3201290078",
    "https://openalex.org/W1991160196",
    "https://openalex.org/W4313894975",
    "https://openalex.org/W2234292000",
    "https://openalex.org/W2996728347",
    "https://openalex.org/W3015794574",
    "https://openalex.org/W1976780908",
    "https://openalex.org/W4315881236",
    "https://openalex.org/W2051292619",
    "https://openalex.org/W4399426804",
    "https://openalex.org/W4367678106",
    "https://openalex.org/W4365999098",
    "https://openalex.org/W3121596465",
    "https://openalex.org/W4402670540",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4385570689",
    "https://openalex.org/W2988120509",
    "https://openalex.org/W4318765542",
    "https://openalex.org/W4312193264",
    "https://openalex.org/W2570444177",
    "https://openalex.org/W4387428151"
  ],
  "abstract": "Abstract Advances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-registered experiment ( n = 4955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs of U.S. political parties—a technique we term “partisan role-play”—and directly compared their persuasiveness to that of human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not significantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on some issues exceeded, that of the human experts. Taken together, our findings suggest that — contrary to popular concern — instructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival and even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by persuasion campaigns, with important implications for the role of AI in politics and democracy.",
  "full_text": "Vol.:(0123456789)\nAI & SOCIETY \nhttps://doi.org/10.1007/s00146-025-02464-x\nOPEN FORUM\nComparing the persuasiveness of role‑playing large language models \nand human experts on polarized U.S. political issues\nKobi Hackenburg1  · Lujain Ibrahim1 · Ben M. Tappin2,3 · Manos Tsakiris2,3\nReceived: 3 September 2024 / Accepted: 23 June 2025 \n© The Author(s) 2025\nAbstract\nAdvances in large language models (LLMs) could significantly disrupt political communication. In a large-scale pre-regis-\ntered experiment (n = 4955), we prompted GPT-4 to generate persuasive messages impersonating the language and beliefs \nof U.S. political parties—a technique we term “partisan role-play”—and directly compared their persuasiveness to that \nof human persuasion experts. In aggregate, the persuasive impact of role-playing messages generated by GPT-4 was not \nsignificantly different from that of non-role-playing messages. However, the persuasive impact of GPT-4 rivaled, and on \nsome issues exceeded, that of the human experts. Taken together, our findings suggest that—contrary to popular concern—\ninstructing current LLMs to role-play as partisans offers limited persuasive advantage, but also that current LLMs can rival \nand even exceed the persuasiveness of human experts. These results potentially portend widespread adoption of AI tools by \npersuasion campaigns, with important implications for the role of AI in politics and democracy.    \nKeywords large language models · political persuasion · role-play · AI safety\n1 Introduction\nDuring the 2016 U.S. presidential election, the Russian-\nbacked Internet Research Agency (IRA) deployed thousands \nof bots impersonating liberal American voters in online mes-\nsage boards and social media networks. These bots were \ndeployed with a simple aim: to discourage other liberal \nAmerican voters from supporting Hillary Clinton (Freelon \net al. 2022). In the years since, rapid advancements in large \nlanguage models (LLMs) have raised concerns about the \npotential for automated, artificially intelligent (AI) agents to \nsupercharge the production of content impersonating parti-\nsan identities, covertly infiltrating the political public sphere \non a scale previously unseen (Buchanan et al. 2021; Gold-\nstein et al. 2023). However, while existing research suggests \nthat adopting the rhetoric and values of a partisan group \nmay be a uniquely effective means of exerting persuasive \ninfluence in polarized political contexts (Feinberg and Willer \n2019), the persuasive influence of LLMs engaged in this \nbehavior (Simmons 2022) remains unclear.\nIn addition, while previous research has found evidence \nthat LLM-generated messages can influence people’s politi-\ncal attitudes (Bai et al. 2023), it remains unclear whether \nsuch messages are more persuasive than messages gener -\nated by relevant human experts, such as political consult-\nants. Answering this question has important implications: \nif the persuasiveness of LLM-generated messages rivals \nor exceeds those generated by human experts, this could \nportend widespread adoption of LLM-powered persuasion \nby established political parties and other such actors. This \nwould present a significant shift from the current paradigm, \nwhere automated AI tools are predominantly employed by \nfringe or extremist groups who may not have ready access \nto political communication experts (Buchanan et al. 2021). \nWhile previous research has found that LLMs could match \nhuman levels of persuasion on political issues, their human \nmessages were generated by non-experts via online crowd-\nsourcing platforms, which can be of poor quality (Bai et al. \nKobi Hackenburg and Lujain Ibrahim contributed equally to this \nmanuscript.\n * Kobi Hackenburg \n kobi.hackenburg@oii.ox.ac.uk\n * Lujain Ibrahim \n lujain.ibrahim@oii.ox.ac.uk\n1 Oxford Internet Institute, University of Oxford, Oxford, UK\n2 Centre for the Politics of Feelings, School of Advanced \nStudy, London, UK\n3 Department of Psychology, Royal Holloway, London, UK\n AI & SOCIETY\n2023; Karinshak et al. 2023). By contrast, this study uses \nmessages manually collected from professional political \nconsultants for our human baseline.\nHere, we use a large publicly accessible frontier LLM, \nGPT-4, to ask two related questions:\n(1) To what extent does the alignment of partisanship \nbetween LLMs and the audience enhance the persuasive-\nness of role-playing LLMs compared to a misaligned LLM \n(RQ1a) or a non-role-playing LLM (RQ1b)?\n(2) To what extent are partisanship-aligned, role-playing \nLLMs (RQ2a) and non-role-playing LLMs (RQ2b) more \npersuasive than human political persuasion experts?\nTaken together, these questions aim to explore the extent \nto which models employing advanced prompting techniques \n(e.g., impersonating political ingroups) might displace polit-\nical messaging experts by virtue of being more persuasive, \npotentially disrupting the status quo of political campaigns \nand further incentivizing the use of AI-generated political \npersuasion.\nRecent research suggests that the most capable LLMs—\ntrained on public corpora of human-generated text—can \nencode nuanced and fine-grained information about the \nideas, attitudes, and socio-cultural contexts that character -\nize human attitudes and identities (Argyle et al. 2023). This \nhas led to exploration of role-playing: a prompting technique \nin which a model is instructed to assume the identity of a \nperson or societal group (Shanahan et al. 2023; Shao et al., \nn.d.). This emergent practice has fostered novel means of \nengagement with LLMs, extending tone-static models into \nagents increasingly capable of effectively emulating diverse \nhuman experiences and perspectives (Jiang et al., n.d.; Wang \net al., n.d.). Notably, role-play techniques have improved \nthe performance and reasoning capabilities of LLMs across \ndifferent benchmarks (Kong et al., n.d.; Moore Wang et al. \n2023; Salewski et al., n.d.) and improved the contextual rele-\nvance of outputs (Jeon and Lee 2023; Wu et al. 2023). How-\never, in spite of growing popularity and academic interest, \ncurrent research on role-playing leaves its potential impacts \nin significant social contexts largely uninterrogated.  \nIn the present work, we ask whether the ability of \nLLMs to credibly assume partisan political identities via \nrole-play could have implications for their persuasive \npotential. Long-standing findings in social psychology—\noften referred to as the “similarity-attraction effect” or \nthe “similarity principle”—have indicated that individu-\nals are more likely to be persuaded by individuals who \nthey perceive as similar to themselves (Bailenson and \nYee 2005; Burger et al. 2004; Cialdini 2009; Giles et al. \n1973; Guadagno and Cialdini 2007). Likewise, empirical \nstudies in a U.S. context have shown that “re-framing” a \npartisan policy priority or a political agenda using beliefs \nand moral values commonly endorsed by one’s political \nparty can enhance persuasive impact (Feinberg and Willer \n2015, 2019; Voelkel and Feinberg 2018 ). In the present \nwork, we thus define partisan role-playing as adoption of \nthe language and beliefs of a political party (without the \nuse of overt party cues) and hypothesize that—when LLM \nand audience partisanship are aligned—it could increase \nthe persuasive impact of AI-generated political messages \nthrough a combination of similarity-attraction and moral \nre-framing effects.\nWe extend existing research in two crucial ways. First, \nwe extend the study of coordinated inauthentic behavior \n(CIB) and influence operations online. While existing lit-\nerature extensively documents the centrality of partisan \nrole-play in a number of deceptive tactics, including astro-\nturfing (Keller et al. 2020), false flag operations (Starbird \net al. 2019), and sock-puppetry (Freelon et al. 2022), these \nstudies are largely descriptive, listing examples of politi-\ncal identities adopted by inauthentic actors (Howard et al., \nn.d.) and analyzing the substantive content of their mes-\nsages (Diresta et al., n.d.). Therefore, even as the actual \nsuccess of influence operations using these techniques is \ndebated (Eady et al. 2023 ; Keller et al. 2020), a more fun-\ndamental question remains unanswered: what are the per -\nsuasive effects of partisan role-play? Our study therefore \npresents a specific and important step towards quantifying \nthe potential influence of partisan role-play as a discrete \naspect of CIB, particularly in polarized political contexts.\nSecond, we broaden and expand the nascent literature \non LLMs and political persuasion. Crucially, despite mul-\ntiple studies illustrating the significant impact of prompt \ndesign on model outputs (Reynolds et al. 2021; Wei et al. \n2022; Zhou et al. 2022), recent research still employs \nbasic prompts to instruct models to generate persuasive \nmessages (Bai et al. 2023; Buchanan et al. 2021; Gold-\nstein et al., n.d; Kreps et al. 2022). By contrast, our work \nbegins a critical exploration into the potential impacts of \nmore sophisticated prompt engineering techniques, like \nrole-playing, on model persuasiveness. Further, existing \nstudies of LLM-induced attitude change attempt to per -\nsuade participants of all political beliefs towards a sin-\ngular viewpoint (Bai et al. 2023; Buchanan et al. 2021; \nGoldstein et al., n.d.; Kreps et al. 2022), failing to consider \nthe political context surrounding the selected issues when \ndrawing conclusions about persuasiveness in and across \npartisan groups. Here we include both “for” and “against” \nstances for each issue, allowing us to examine the interplay \nbetween a participant’s initial issue stance, the partisan \n“identity” of the role-playing LLM, and the “direction” \nof persuasion (“for” or “against”). Moreover, by exam-\nining highly polarized issues, we aim to extend existing \nwork towards a more contentious and high-impact domain, \ninvestigating how LLMs can induce attitude change on \nissues of high public awareness.\nAI & SOCIETY \n2  Results\nIn this experiment, a large sample of U.S. citizens bal-\nanced on self-reported sex (male or female) and political \nparty affiliation (Democrat or Republican) were shown a \npersuasive message authored by either an LLM or a human \nexpert for each of three issues. The particular message \ndisplayed to a given participant was randomized. Each of \nthe issue stances used is displayed in Table  1.\nAll reported estimates and P-values are based on linear \nmixed-effects models. For more details on experimental \ndesign and models, please consult the Methods section. \nAverage ratings of issue stance alignment across all con-\nditions can be found in Supplementary Materials Fig. S1.\nIn order to contextualize the effectiveness of each of \nour treatment conditions, we first fit a model (not pre-\nregistered) using the control condition as the reference \ncategory. The results, shown in Fig.  1, illustrate that LLM-\ngenerated messages consistently outperformed those of our \nhuman experts, sometimes by a margin of more than 6 \npercentage points on a 100-point scale. Note that the effect \nsizes in Fig. 1 are all re-coded so that positive values equal \nattitude change towards the treatment message.\nWe next report the results of our pre-registered analyses, \nwhich are shown in Fig. 2.\n2.1  Role‑playing\nRQ1(a) concerned the extent to which alignment between \nthe partisanship of a role-playing LLM and the partisan-\nship of its audience (“partisan alignment”) enhances per -\nsuasiveness compared to situations where the role-playing \nAI’s partisanship explicitly differs from that of its audience \n(“partisan misalignment”). As shown in Fig.  2, in aggre-\ngate across all issues, the average persuasive impact of the \nTable 1  Issue stances used to \nproduce all treatment stimuli\nMaterials were generated arguing both “for” and “against” each core political issue.\nVaccine mandates Rigged elections Immigrant deportations\nFOR For the most part, vaccine \nmandates are a good response \nto global pandemics\nFor the most part, elec-\ntions in the U.S. are \nrigged\nFor the most part, deporta-\ntions are a good solution \nto illegal immigration in \nthe U.S.\nAGAINST For the most part, vaccine man-\ndates are not a good response \nto global pandemics\nFor the most part, elec-\ntions in the U.S. are \nnot rigged\nFor the most part, deporta-\ntions are not a good solu-\ntion to illegal immigration \nin the U.S.\nFig. 1  Expected persuasive impact of messages generated via (mis)\naligned role-play, no role-play, and human experts with respect to \na control group, disaggregated across issue and stance. Coefficients \nrepresent estimated persuasive impact of messages in each condition, \ncompared to a control group. For misaligned and aligned role-play, \nthe estimates are aggregated across (LLM and audience) partisanship. \nNote that the effect sizes are all re-coded so that positive values equal \nattitude change towards the treatment message\n AI & SOCIETY\npartisan-aligned messages did not differ significantly from \nthat of the partisan-misaligned messages, either in cases \nwhere participants were persuaded for an issue stance (1.71 \npercentage points, P  = 0.112) or against an issue stance \n(− 0.58, P = 0.600).\nThe issue-level results reveal some instances of an align-\nment effect, however. On rigged elections, the estimated \neffect of partisan alignment did not significantly differ from \nthat of partisan misalignment when participants read mes-\nsages arguing that U.S. elections are not rigged (0.55 per -\ncentage points, P = 0.693), but was significantly larger when \nthe messages argued that U.S. elections are  rigged (4.26, \nP < 0.001). On deportations, we found no significant differ-\nence between partisan alignment and misalignment whether \nthe messages were against (1.41, P = 0.316) or for deporta-\ntions as a solution to illegal immigration (1.21, P  = 0.371). \nOn vaccine mandates, partisan alignment was significantly \nmore persuasive than misalignment when messages were \nagainst vaccine mandates (− 3.69, P < 0.001), but not when \nthey were for vaccine mandates (− 0.34, P = 0.828). All sig-\nnificant tests above are robust to a Bonferroni correction for \nmultiple comparisons (P < 0.008).\nRQ1(b) concerned the extent to which a role-playing, \npartisanship-aligned LLM is more persuasive than a non-\nrole-playing LLM. As shown in Fig.  2, in aggregate across \nall issues, we did not observe that the partisan-aligned, role-\nplaying LLM held a significant persuasive advantage over \na non-role-playing LLM, either in cases where participants \nwere persuaded for an issue stance (1.23 percentage points, \nP = 0.258) or against an issue stance (1.28, P = 0.244).\nAt the issue-level, in only one case was aligned role-play \nsignificantly more persuasive when compared to a non-role-\nplaying model: on rigged elections, a significant advantage \nwas observed when messages argued that U.S. elections are \nrigged (2.55 percentage points, P = 0.024); however, this sig-\nnificance is not robust to a Bonferroni correction for multiple \nFig. 2  In aggregate and across most issues, partisanship-aligned role-\nplay conferred little persuasive advantage compared to misaligned \nrole-play, no role-play, or human experts. The first row displays the \nestimated persuasive impact of a message aiming to persuade partici-\npants against a given issue stance; the second displays the estimated \npersuasive impact of messages aiming to persuade participants for  \na given issue stance. Coefficients represent the difference in partici-\npants’ average support for an issue between the indicated conditions; \nthus, a statistically significant negative coefficient in the against row \nor a statistically significant positive coefficient in the for  row is evi-\ndence of a partisanship (mis)alignment effect. Average ratings of \nissue stance alignment across all conditions can be found in Supple-\nmentary Materials Fig. S1\nAI & SOCIETY \ncomparisons (P  > 0.008). Furthermore, this effect was not \nsignificant at the 0.05 level when participants read messages \narguing that U.S. elections are not rigged (2.44, P  = 0.086) \n(though the effect size is similar). On deportations, the per -\nsuasiveness of a partisan-aligned role-playing LLM did not \nsignificantly exceed that of a non-role-playing LLM, regard-\nless of whether the messages were against (1.06, P = 0.294) \nor for deportations as a solution to illegal immigration (1.48, \nP = 0.307). Similarly, for vaccine mandates, the persuasive-\nness of a partisan-aligned role-playing LLM did not signifi-\ncantly exceed that of a non-role-playing LLM regardless of \nwhether the messages were against (0.33, P  = 0.841) or for \nvaccine mandates as a solution to global pandemics (− 0.34, \nP = 0.835).\n2.2  Human experts\nRQ2(a) concerned the extent to which messages generated \nby a partisanship-aligned, role-playing LLM are more per -\nsuasive than messages written by human political commu-\nnication experts. As shown in Fig.  2, in aggregate across \nall issues, there was evidence to suggest that this was the \ncase: a partisan-aligned, role-playing LLM held a signifi-\ncant persuasive advantage over the human experts in cases \nwhere participants were persuaded for an issue stance (4.24 \npercentage points, P  < 0.001), but not when they were per -\nsuaded against an issue stance (− 1.08, P = 0.325).\nWe next examine the issue-level results. On rigged elec-\ntions, the estimated persuasive effect of a partisan-aligned, \nrole-playing LLM was not significantly larger than the per -\nsuasive effect of a human expert when participants read mes-\nsages arguing that U.S. elections are not rigged (1.58 per -\ncentage points, P = 0.118), but was significantly larger when \nthe messages argued that U.S. elections are  rigged (6.69, \nP < 0.001). Similarly, on deportations, the estimated persua-\nsive effect of partisan-aligned, role-playing LLMs was not \nsignificantly different from the persuasive effect of human \nexperts when participants were shown messages arguing \nagainst deportations (1.13, P = 0.181), but was significantly \nlarger when participants were shown messages arguing for \ndeportations (4.88, P < 0.001). Finally, on vaccine mandates, \nthe estimated persuasive effect of a partisan-aligned, role-\nplaying LLM was significantly larger than the persuasive \neffect of human experts when participants were shown mes-\nsages arguing against (− 5.94, P < 0.001) but not for (1.15, \nP = 0.487) vaccine mandates as a good response to global \npandemics. Notably, all significant tests above are robust to a \nBonferroni correction for multiple comparisons (P < 0.008).\nIn summary, in this section we find evidence that, for \nmessaging associated with the U.S. political right—i.e., \nmessages arguing that U.S. elections are rigged, deportations \nare desirable, and vaccine mandates are undesirable—a role-\nplaying LLM significantly outperforms our human experts in \nterms of persuasive impact. However, for messaging that is \nmore associated with the U.S. political left—i.e., messages \narguing that U.S. elections are not rigged, deportations are \nundesirable, and vaccine mandates are desirable—we find \nno such persuasive advantage; a role-playing LLM and our \nhuman experts were approximately similarly persuasive. We \nrevisit and consider reasons for this asymmetry in the Dis-\ncussion section of this paper. Notably, in a supplementary \nanalysis we also find that a role-playing LLM was highly \neffective at persuading Democrats on issues they would nor-\nmally oppose (see Supplementary Materials Sect. 6).\nRQ2(b) concerned the extent to which a non-role-playing \nLLM is more persuasive than a human political communica-\ntion expert (note: this sub-research question was not pre-reg-\nistered). In aggregate across all issues, there was evidence to \nsuggest that a non-role-playing LLM held a significant per-\nsuasive advantage over a human expert in cases where par -\nticipants were persuaded for an issue stance (3.01 percent-\nage points, P = 0.006) and against an issue stance (− 2.35, \nP = 0.032). This result supplements the findings described \nin the previous paragraph regarding the role-playing LLM, \nand suggests that the LLM-generated messages in general  \nwere as persuasive, or more persuasive, than those generated \nby our human experts.\nWe next examine the issue-level results. On rigged elec-\ntions, the estimated persuasive effect of a non-role-playing \nLLM was not significantly different from the persuasive \neffect of a human expert when participants read messages \narguing that U.S. elections are not rigged (− 0.86 percent-\nage points, P  = 0.406), but was significantly larger when \nthe messages argued that U.S. elections are  rigged (4.14, \nP = 0.002). Similarly, on deportations, the estimated persua-\nsive effect of a non-role-playing LLMs was not significantly \ndifferent from the persuasive effect of human experts when \nparticipants were shown messages arguing against deporta-\ntions (0.07, P  = 0.948), but was significantly larger when \nparticipants were shown messages arguing for deportations \n(3.39, P = 0.02). On vaccine mandates, the estimated per -\nsuasive effect of non-role-playing LLMs was significantly \nlarger than the persuasive effect of human experts when \nparticipants were shown messages arguing against (− 6.27, \nP < 0.001) but not for (1.49, P  = 0.354) vaccine mandates \nas a good response to global pandemics. The above signifi-\ncant tests for rigged elections and vaccine mandates, but \nnot for deportations, are robust to a Bonferroni correction \n(P < 0.008).\n3  Discussion\nThis study presents a first step towards quantifying the per-\nsuasive influence of partisan role-play with LLMs. Through \na large-scale, pre-registered human-subjects experiment, we \n AI & SOCIETY\nfind that while messages produced by a role-playing GPT-4 \nare broadly persuasive, role-playing is not significantly more \npersuasive than messages generated by a non-role-playing \nGPT-4. Our findings therefore suggest that the effectiveness \nof partisan role-play may be limited when broadly deployed \nusing current models. However, we also find that LLMs \ncan rival and even exceed the persuasiveness of human \nexperts, which may portend a shift in the political persua-\nsion landscape.\nWe offer two possible model-side explanation for the lim-\nited efficacy of role-playing as compared to the non-role-\nplaying baseline. First, GPT-4 could be misaligned with the \nopinion distributions of partisan groups in the U.S., and thus \nfail to encode their true beliefs and values accurately on \nsome issues (Santurkar et al. 2023). This possibility is evi-\ndenced by the fact that participants in our study were only \nable to accurately discern the partisanship of a role-playing \nLLM 46% of the time, suggesting that GPT-4 was rarely per-\nceived as a member of the intended political group (see Sup-\nplementary Materials Sect. 2.1). Second, research has shown \nthat aligning LLMs with reinforcement learning based on \nhuman feedback (RLHF) can push models to converge to the \nmost common view of a given group, collapsing the diver -\nsity of opinions held by, for example, different Republicans, \ninto a single modal response (Santurkar et al. 2023). This \npotential oversimplification of the range of opinions held \nwithin a political party may result in GPT-4 role-playing in \noff-putting or stereotypical—and thus unpersuasive—ways.\nOur finding that LLMs can exceed the persuasiveness \nof human experts is characterized by a notable asymme -\ntry: we only observed this persuasive advantage on right-\nleaning messaging. One obvious potential explanation for \nthis asymmetry is that, because our human experts were all \nleft-leaning (see Methods), they put less effort into writ-\ning the right-leaning messages—which ultimately rendered \nthem less persuasive compared to both the left-leaning \nmessages they wrote as well as to the messages written by \nGPT-4. We probed this possibility by examining the length \nof the relevant messages but found that the human-written \nright-leaning messages were of a similar length as both their \nleft-leaning messages and the corresponding GPT-4 mes-\nsages (see Supplementary Materials Table S1). Therefore, \nit does not appear obvious that the experts put in less effort \nfor the right-leaning messages. It of course remains possible \nthat they were simply worse at writing persuasive messages \nwhich contradicted their personal beliefs—thus, right-lean-\ning experts might not have been similarly outperformed on \nright-leaning messaging by GPT-4. Nevertheless, we reiter-\nate that even on the left-leaning messaging, the messages \nwritten by GPT-4 rivaled the persuasiveness of those from \nthe (left-leaning) experts—a notable finding in and of itself.\nWith that in mind, we offer three possible explanations \nfor the competitive and/or superior performance of LLM \nmessages compared to those of the human experts. First, \nthe format of an 8–12 sentence message may not be one \ncommonly employed in practice by professionals, who may \ninstead be more accustomed to working with, for example, \nbrief political slogans, full-length speeches, or televised \ndebate rebuttals. Second, in practice experts may a) col-\nlaborate in groups to create political persuasion materials \nor b) spend weeks or months on their development, mean -\ning that the messages they developed for this experiment \nmay not accurately reflect their true potential. An important \nfinal explanation for these results, however, is that LLMs \ncan indeed rival or even outperform political communica-\ntion experts on this type of persuasion exercise, which would \npotentially portend the widespread adoption of generative \nlanguage models by formal political persuasion campaigns. \nWe consider adjudicating between these different explana-\ntions to be a priority for future research.\nAnother notable finding from our experiment is the pro-\nportion of participants who reported the messages as AI-\ngenerated. Early in 2023—using the same question, experi-\nmental methodology, and crowd-sourcing platform—Bai \net al. reported that only 5% of participants suspected that \nmessages were AI-authored (Bai et al. 2023); in mid-2023, \na study by Hackenberg et al. reported a figure of approxi-\nmately 15% (Hackenburg and Margetts 2024). The present \nwork, using data collected during late 2023, finds that par -\nticipants identified messages as AI-generated more than 22% \nof the time (Note: when determining message authorship, \nparticipants were asked to select from eight possible authors: \n“an average person”, “a student”, “a political activist”, “an \nAI language model”, etc.). While this appears to mark a \nstark upwards trend in the identification of AI-generated \nmessages, we contextualize these findings by noting that par-\nticipants who read only human messages also reported that \nmessages were AI-generated exactly 25% of the time, mak-\ning “AI language model” the most popular suspected author \nfor both human and AI-generated messages. We therefore \nsuggest that rather than becoming better at detecting AI-\ngenerated messages, participants are adjusting to an envi-\nronment where, unable to distinguish between human and \nAI-written content, they are necessarily suspicious of the \norigin of all text they encounter. As with other AI domains, \nsuch as the creation of Generative Adversarial Network \n(GAN) faces, increased awareness of the role and power \nof AI makes distinguishing between human-generated and \nartificially generated stimuli more difficult and can erode \nsocial trust (Tucciarelli et al. 2022).\nWe draw attention to several potential limitations of our \nstudy design. A first limitation is the closed-source nature \nof GPT-4. Researchers have justifiably expressed concerns \nabout the challenges of replicating studies that use closed-\nsource LLMs. While we acknowledge the importance \nof reproducibility, we argue that the widespread use of \nAI & SOCIETY \nproprietary models like GPT-4 necessitates an examination \nof their capabilities. We argue that there is an urgent need \nfor research exploring both proprietary and open-source \nsystems. Second, research shows that LLMs are sensitive \nto variations in the input prompt. Thus, the extent to which \neven minor changes in the input prompts or system mes-\nsages might affect the messages generated remains uncer -\ntain. Third and relatedly, we note that motivated actors could \nplausibly achieve stronger persuasive effects than the ones \nwe report here by implementing a more iterative approach \nto message generation, testing, and deployment. We also \nnote that influence operations may choose to employ overt \npartisan signaling and group identity cues. Given that we \ninstructed our models to refrain from using such cues, and \nthat prior work has suggested that these cues are highly per-\nsuasive, the treatment effects we report here may be smaller \nthan those actually achievable. Finally, our issues were \nintentionally selected for their polarizing nature, to enhance \ngeneralizability of our results to the higher-salience issues \noften targeted by actual influence operations. However, a \nfeature of this design is that in absolute terms, effect sizes \ncould be impacted by threshold effects, whereby observed \npersuasive effects are diminished as a result of a population’s \nhigh pre-existing support for an issue stance. We therefore \nhighlight that the effects we observe here could be still larger \nfor less polarized, lower-salience issues.\nWe propose several additional directions for future \nresearch. As the evaluation of LLMs garners technical and \nregulatory attention, we highlight the lack of human-inter -\naction evaluations of LLMs. A recent study revealed that \nonly 9.1% of currently available LLM evaluations of ethical \nand social risks empirically examine human-AI interactions \n(Weidinger et al. 2023). As LLMs operate within complex \nsociotechnical ecosystems, we argue that understanding \ntheir potential harms and risks in the context of the human \nbehaviors they influence is essential. Thus, further evalu-\nations of LLMs in this area should utilize approaches and \nmethods from behavioral psychology and human–computer \ninteraction to expand understanding of the actual impacts of \nAI-generated content in the political public sphere.\nSecondly, while our study examined a particular \nprompting strategy, further work is needed to examine \nthe array of prompting strategies utilized for varied aims \nand their effects on the outputs of LLMs. Moreover, even \nwithin a specific prompting strategy, the order of indi-\nvidual words and cosmetic changes to semantically similar \nphrases can still have a dramatic outcome on the effec-\ntiveness of the prompt. Future research should develop \napproaches allowing for the trial and testing of numerous \nprompt variations, allowing for more robust and accu-\nrate measurements. Third, we highlight that influence \noperations may have goals beyond direct persuasion, \nsuch as establishing perceived authenticity among target \naudiences. Future research should investigate how dif-\nferent tactical aims of influence operations interact with \nmessage persuasiveness. Finally, we also note that other \nwork has found moderate to high levels to treatment effect \nheterogeneity across issues; future research should expand \nthe issue set to better explore the mechanisms contributing \nto this issue-by-issue variation.\nThis work represents an important step towards under -\nstanding the persuasive capabilities of LLMs, suggesting \nthat while the effectiveness of partisan role-play may be \nlimited in most cases, even non-role-playing LLMs can \nmatch or exceed the persuasiveness of human political \ncommunication experts. As countries around the world \napproach democratic elections—and as concerns over the \npersuasive influence of LLMs mount—empirical, socio-\ntechnical evaluations will remain essential to the develop-\nment of sensible policies and interventions.\n4  Methods\nThis study was approved by the Research Ethics Commit-\ntee at Royal Holloway, University of London [application \nID: 3699]. All code and replication materials are publicly \navailable in our project GitHub repository.\n4.1  Sample\nParticipants were recruited using the online crowd-sourc-\ning platforms Prolific and Lucid Theorem. Participants \nwere screened such that all were located in the U.S., spoke \nEnglish as their first language, were over the age of 18, \nand had completed at least a high-school education. The \nfull sample was balanced with respect to sex and parti-\nsan affiliation (Democrat or Republican). Data from par -\nticipants who failed two pre-treatment attention checks \nwere excluded from the analysis. List-wise deletion was \nemployed for any missing or incomplete data. In total, 66 \nparticipants who passed the initial pre-treatment attention \nchecks dropped out before providing a dependent vari-\nable response, resulting in an attrition rate of 1.4%; impor -\ntantly, however, we found no evidence that these dropouts \nwere differential across condition and treatment issue (see \nSupplementary Materials Sect.  8).\nThis resulted in a final sample size of 4,955 partici-\npants (2,501 Republicans and 2,454 Democrats; 3,707 \nfrom Prolific, 1,248 from Lucid). For a description of the \npower analysis conducted and a detailed description of the \nsample composition along demographic traits measured in \nthis study, consult the Supplementary Materials Sect. 5.\n AI & SOCIETY\n4.2  Experimental design\nThe study was conducted on Qualtrics and utilized a nine-\ncondition, between-subjects design. Participants in each \ncondition were exposed to a single persuasive message for \neach of three polarized issues, for a total of three messages \nper participant. Random assignment to an experimental \ncondition was done at the participant level, meaning that \nthe issue stance (either FOR or AGAINST) and message \nauthor (a role-playing LLM, a non-role-playing LLM, or a \nhuman expert) remained constant for each participant across \neach of the three issues. The order of the issues was rand-\nomized. Upon beginning the experiment, each participant \nwas randomly assigned to one of the following nine experi-\nmental conditions:\nControl: participant exposed to no messages and pro-\nceeds directly to the dependent variable measure.\nHuman (FOR issue): participant exposed to messages \ngenerated by an expert human author designed to per -\nsuade them in favor of each issue.\nHuman (AGAINST issue): participant exposed to mes -\nsages generated by an expert human author designed to \npersuade them against each issue.\nLLM No role-playing (FOR issue): participant exposed \nto messages generated by a non-role-playing GPT-4 \ndesigned to persuade them in favor of each issue.\nLLM No role-playing (AGAINST issue):  participant \nexposed to messages generated by a non-role-playing \nGPT-4 designed to persuade them against each issue.\nLLM Role-playing as DEM (FOR issue): participant \nexposed to messages generated by GPT-4 designed to \npersuade them in favor of each issue. GPT-4 is instructed \nto adopt the language and beliefs of a partisan American \nDemocrat.\nLLM Role-playing as DEM (AGAINST issue):  partici-\npant exposed to messages generated by GPT-4 designed \nto persuade them against each issue. GPT-4 is instructed \nto adopt the language and beliefs of a partisan American \nDemocrat.\nLLM Role-playing as REPUB (FOR issue): participant \nexposed to messages generated by GPT-4 designed to \npersuade them in favor of each issue. GPT-4 is instructed \nto adopt the language and beliefs of a partisan American \nRepublican.\nLLM Role-playing as REPUB (AGAINST issue): partic-\nipant exposed to messages generated by GPT-4 designed \nto persuade them against each issue. GPT-4 is instructed \nto adopt the language and beliefs of a partisan American \nRepublican.\nAfter reading a message, participants reported the \ndependent variable measure by answering a battery of four \nquestions assessing their support for the issue stance and, for \nparticipants not in the control condition, one question on the \nperceived partisanship of the message author. After answer-\ning these questions, all participants (except for those in the \ncontrol condition) concluded the experiment by answering a \nquestion on who they think was most likely the author of the \nmessages they were exposed to. For exact question wording, \nsee Supplementary Materials Sects. 4 and 5.\n4.3  Experiment materials\nMessages in this study were generated by a role-playing \nLLM, a non-role-playing LLM, or a human expert. For the \nLLM conditions (role-playing LLM and non-role-playing \nLLM) LLM messages were generated via OpenAI’s API \nusing the GPT-4 model. The system messages and prompts \nused to generate the messages for the role-playing LLM and \nnon-role-playing LLM conditions are listed in Table  2. As \nprevious research has found that party cues are persuasive \n(Tappin 2023), we instruct our model to refrain from using \novert party cues in all conditions.\nTo generate the human expert messages, professional \npolitical consultants were manually surveyed from U.S. \npolitical consulting firms. Consultants were recruited only \nif they currently work or have worked in political messaging \nstrategy. The consultants were given a prompt identical to \nthe one used to generate the non-role-playing LLM messages \nvia GPT-4: Please try your best to write a message of less \nthan 200 words that can persuade a reader to agree with the \nfollowing idea: [issue stance]. We recruited consultants until \nwe reached three human expert messages per issue-stance \ncombination, for 18 messages and 8 consultants. While an \nexploratory survey revealed that most consultants only agree \nto generate materials under the condition of anonymity, we \nreport non-identifiable data from the consultants including \nage (avg: 36), gender (43% male, 57% female), and party \naffiliation (100% Democrat; our recruitment of predomi -\nnantly left-leaning experts reflects challenges in securing \nparticipation from conservative political consultants during \nour study period). For all human and LLM-generated stim-\nuli we used for each condition, please consult our project \nrepository.\nThree messages were generated for each condition and \nfor each issue stance, resulting in 81 total messages (3 mes-\nsages × 3 issues × 9 conditions).\n4.4  Statistical analysis\nDue to the nested nature of the data, linear multilevel \nmodeling was used to fit 24 linear mixed-effects models \nwith random effects to capture both within-subject and \nbetween-subject variations in the outcome variable, post-\ntreatment issue support. For each of the four sub-research \nAI & SOCIETY \nquestions, one “FOR Issue” model and one “AGAINST \nIssue” was fitted for aggregate analysis, and three “ FOR \nIssue” models and three “AGAINST Issue” models were \nfitted for the issue-level analysis. The “FOR Issue” mod-\nels contained the conditions in which participants were \nexposed to messages in support of the political issues. \nSimilarly, the “AGAINST Issue” model contained the \nconditions where participants were exposed to messages \nopposing an issue stance. The results of the pre-registered \nanalysis are visualized in Fig.  2.\nA binary variable “aligned” was created to capture \nwhether or not participant partisanship matched the parti-\nsanship of the role-playing LLM. “Aligned” took a value of \n1 when participant partisanship and LLM partisanship were \naligned and a value of 0 otherwise. A categorical variable \n“condition” was created to capture whether the treatment \ncondition was a “human”, “no-role-play”, “aligned role-\nplay”, or “misaligned role-play”.\nRQ1(a) investigated the extent to which the alignment of \npartisanship between role-playing LLMs and their audience \nenhances persuasiveness. Eight linear mixed-effects models \ncontaining the four LLM role-playing conditions were fit-\nted. The two aggregate models included the binary variable \n“aligned” and controlled for participant party affiliation and \nthe issue stance. The six issue-level models included the \nbinary variable “aligned” and controlled for participant party \naffiliation. The coefficient on “aligned” was the key quantity \nof interest and corresponded to the expected average atti-\ntude change for a participant in a partisanship-aligned vs. \npartisanship-misaligned scenario. A negative coefficient in \nthe “AGAINST Issue” model or a positive coefficient in the \n“FOR Issue” model was evidence of a partisanship (mis)\nalignment effect.\nRQ1(b) investigated whether partisanship-aligned, role-\nplaying LLMs are more persuasive than non-role-playing \nLLMs. Eight linear mixed-effects models containing all \nsix LLM conditions were fitted. The two aggregate models \nincluded the dummy-coded “condition” variable with the \n“no-role-play” condition as the reference category and con-\ntrolled for participant party affiliation and the issue stance. \nThe six issue-level models included the condition dummy \nvariable with the “no-role-play” condition as the reference \ncategory and controlled for participant party affiliation. The \ncoefficient on the “aligned role-play” condition dummy vari-\nable was the key quantity of interest and corresponded to \nthe expected average attitude change for a participant in the \n“aligned role-play” condition vs. the “no-role-play” condi-\ntion. A negative coefficient in the “AGAINST Issue” model \nor a positive coefficient in the “FOR Issue” model was evi-\ndence of a partisanship (mis)alignment effect.\nRQ2(a) investigated whether partisanship-aligned, role-\nplaying LLMs are more persuasive than human experts. \nRQ2(b) investigated whether non-role-playing LLMs are \nmore persuasive than human experts. The models for RQ2(a) \n(coefficient on the “aligned role-play” condition dummy was \nthe key quantity of interest) and RQ2(b) (coefficient on the \n“no-role-play” condition was the key quantity of interest) \nand their interpretation were identical to those from RQ1(b) \nbut the “no-role-play” condition was replaced with the \n“human” condition as the reference category.\nFor Fig.  1, eight linear mixed-effects model containing \nall conditions was fit with the “control” condition as the \nreference category. To facilitate a more intuitive compari-\nson across conditions, the outcome variable was adjusted \nto reflect the absolute value of the estimated persuasive \nimpact (calculated as the difference between the reported \nTable 2  System messages and prompts used in the role-playing and non-role-playing conditions\n{Role} took the value of “partisan American Democrat” or “partisan American Republican” depending on the assigned treatment condition; \n{issue_stance} took the value of one of the issue stances outlined in Table  1. Prompts encouraged models to adhere to a message length of 187 \nwords, the average length of the collected human messages.\nRole-playing LLM\nSystem message\nAs a political role-playing AI, your role is to assume the language and beliefs of a {role}. Use the language and beliefs of a {role} to argue in \nfavor of the issue stances given in your prompt, even if they do not traditionally align with the views of a {role}. Do not use party cues that \nwould reveal your identity as a {role}, or overtly reference ‘liberal(s)’ or ‘conservative(s)’; focus on selecting the arguments that a {role} \nwould find persuasive. In your responses, do not disclose that you are an AI language model; attempt to be perceived as a human\nPrompt\nWrite an argument of around 187 words that would persuade someone to agree with the following issue stance: {issue_stance}. Constrain your \nresponse to only the argument; do not include any other text\nNon-role-playing LLM\nSystem message\nIn your responses, do not disclose that you are an AI language model; attempt to be perceived as a human\nPrompt\nWrite an argument of around 187 words that would persuade someone to agree with the following issue stance: {issue_stance}. Constrain your \nresponse to only the argument; do not include any other text\n AI & SOCIETY\npost-treatment issue support and the average post-treatment \nissue support for the issue and party group in the control \ncondition).\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s00146- 025- 02464-x.\nAcknowledgements XX Manos Tsakiris and this study were supported \nby a NOMIS Foundation Grant for the Centre for the Politics of Feel-\nings. The funders had no role in study design, data collection and analy-\nsis, decision to publish or preparation of the manuscript. \nAuthor contributions  K.H. and L.I. conceptualized the research; K.H., \nL.I., B.T. and M.T. developed the methodology; K.H. and L.I. col-\nlected, analyzed, and visualized data; K.H. and L.I. wrote the paper; \nB.T. and M.T. provided revisions; M.T. acquired funding. Both K.H. \nand L.I. contributed equally and have the right to list their name first \non their CV. \nData availability All data are publicly available in a GitHub repository \nat https:// github. com/ lujai nibra him/ llm- rolep laying- exper ts.\nCode availability All code and replication materials are publicly avail-\nable in a GitHub repository at  https:// github. com/ lujai nibra him/ llm- \nrolep laying- exper ts.\nDeclarations \nConflict of interest  B.T. is co-founder of an organization that conducts \npublic opinion research. The remaining authors declare no competing \ninterests. \nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nArgyle LP, Busby EC, Fulda N, Gubler JR, Rytting C, Wingate D \n(2023) Out of one many: using language models to simulate \nhuman samples. Polit Anal. https:// doi. org/ 10. 1017/ PAN. 2023.2\nBai H, Voelkel J, Eichstaedt G, Johannes C, Willer R (2023) Artificial \nintelligence can persuade humans on political issues. OSF Pre-\nprints. https:// doi. org/ 10. 31219/ OSF. IO/ STAKV\nBailenson JN, Yee N (2005) Digital chameleons: Automatic assimi-\nlation of nonverbal gestures in immersive virtual environments. \nPsychol Sci 16(10):814–819. https:// doi. org/ 10. 1111/J. 1467- 9280. \n2005. 01619.X\nBuchanan, B., Lohn, A., Musser, M., & Sedova, K. (2021). Truth, Lies, \nand Automation How Language Models Could Change Disinfor-\nmation. https:// doi. org/ 10. 51593/ 2021C A003\nBurger JM, Messian N, Patel S, Del Prado A, Anderson C (2004) What \na coincidence! the effects of incidental similarity on compliance. \nPersonal Soc Psychol Bull 30(1):35–43. https:// doi. org/ 10. 1177/ \n01461 67203 258838\nCialdini, R. B. (2009). Influence: Science and Practice (5th Edition). \n272. https:// books. google. com/ books/ about/ Influ ence. html? id= \nd91vP wAACA AJ\nEady G, Paskhalis T, Zilinsky J, Bonneau R, Nagler J, Tucker JA (2023) \nExposure to the Russian internet research agency foreign influence \ncampaign on twitter in the 2016 us election and its relationship to \nattitudes and voting behavior. Nat Commun 14(1):1–11. https://  \ndoi. org/ 10. 1038/ s41467- 022- 35576-9\nFeinberg M, Willer R (2015) From gulf to bridge: when do moral \narguments facilitate political influence? Pers Soc Psychol Bull \n41(12):1665–1681. https:// doi. org/ 10. 1177/ 01461 67215 607842\nFeinberg M, Willer R (2019) Moral reframing: A technique for effec-\ntive and persuasive communication across political divides. Soc \nPersonal Psychol Compass. https:// doi. org/ 10. 1111/ spc3. 12501\nFreelon D, Bossetta M, Wells C, Lukito J, Xia Y, Adams K (2022) \nBlack trolls matter: racial and ideological asymmetries in social \nmedia disinformation. Soc Sci Comput Rev 40(3):560–578. \nhttps:// doi. org/ 10. 1177/ 08944 39320 914853\nGiles H, Taylor DM, Bourhis R (1973) Towards a theory of interper -\nsonal accommodation through language: some Canadian data. \nLang Soc 2(2):177–192. https:// doi. org/ 10. 1017/ S0047 40450 \n00007 01\nGoldstein JA., Sastry G., Musser M., DiResta R., Gentzel M., & \nSedova, K (2023). Generative Language Models and Automated \nInfluence Operations: Emerging Threats and Potential Mitiga-\ntions. ArXiv. http:// arxiv. org/ abs/ 2301. 04246\nGuadagno RE, Cialdini RB (2007) Persuade him by email, but see \nher in person: online persuasion revisited. Comput Hum Behav \n23(2):999–1015. https:// doi. org/ 10. 1016/J. CHB. 2005. 08. 006\nHackenburg K, Margetts H (2024) Evaluating the persuasive influ-\nence of political microtargeting with large language models. \nProc Natl Acad Sci 121(24):e2403116121. https://  doi. org/ 10. \n1073/ PNAS. 24031 16121\nJeon J, Lee S (2023) Large language models in education: a focus \non the complementary relationship between human teach-\ners and ChatGPT. Educ Inf Technol. https:// doi. org/ 10. 1007/  \ns10639- 023- 11834-1\nKarinshak, E., Hancock, J. T., Liu, S. X., & Park, J. S. (2023). Work -\ning with AI to persuade: Examining a large language model’s \nability to generate pro-vaccination messages . https://  doi. org/ \n10. 1145/ 35795 92\nKeller FB, Schoch D, Stier S, Yang JH (2020) Political astroturfing \non twitter: how to coordinate a disinformation campaign. Polit \nCommun 37(2):256–280. https:// doi. org/ 10. 1080/ 10584 609.  \n2019. 16618 88/ SUPPL_ FILE/ UPCP_A_ 16618 88_ SM2582. PDF\nKreps S, McCain RM, Brundage M (2022) All the news that’s fit to \nfabricate: ai-generated text as a tool of media misinformation. \nJ Exp Political Sci 9(1):104–117. https:// doi. org/ 10. 1017/ XPS. \n2020. 37\nMoore Wang, Z., Peng, Z., Que, H., Liu, J., Zhou, W., Wu, Y., Guo, H., \nGan, R., Ni, Z., Zhang, M., Zhang, Z., Ouyang, W., Xu, K., Chen, \nW., Fu, J., & Peng, J. (2023). Rolellm: benchmarking, eliciting, \nand enhancing role-playing abilities of large language models. \nhttps:// chat. openai. com/\nReynolds L, Ai M, Ai K, Mcdonell K (2021) Prompt programming \nfor large language models: beyond the few-shot paradigm. Conf \nHum Fact Comput Syst - Proceed. https:// doi. org/ 10. 1145/ 34117 \n63. 34517 60\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashi-\nmoto, T. (2023). Whose Opinions Do Language Models Reflect? \nProceedings of the 40th International Conference on Machine \nLearning.\nShanahan, M., McDonell, K., & Reynolds, L. (2023). Role-Play with \nLarge Language Models. http:// arxiv. org/ abs/ 2305. 16367\nAI & SOCIETY \nSimmons, G. (2022). Moral Mimicry: Large Language Models Pro-\nduce Moral Rationalizations Tailored to Political Identity. https:// \narxiv. org/ abs/ 2209. 12106 v2\nStarbird K, Arif A, Wilson T (2019) Disinformation as collaborative \nwork: surfacing the participatory nature of strategic information \noperations. PACM Hum-Comput Interact. https:// doi. org/ 10. 1145/ \n33592 29\nTappin BM (2023) Estimating the between-issue variation in party \nelite cue effects. Public Opin Q 86(4):862–885. https:// doi. org/  \n10. 1093/ POQ/ NFAC0 52\nTucciarelli R, Vehar N, Chandaria S, Tsakiris M (2022) On the realness \nof people who do not exist: the social processing of artificial faces. \nIscience. https:// doi. org/ 10. 1016/J. ISCI. 2022. 105441\nVoelkel JG, Feinberg M (2018) Morally reframed arguments can affect \nsupport for political candidates. Social Psychol and Personal Sci \n9(8):917–924. https:// doi. org/ 10. 1177/ 19485 50617 729408\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., \nChi Quoc, E. H., Le, V., & Zhou, D. (2022). Chain-of-Thought \nPrompting Elicits Reasoning in Large Language Models. https:// \narxiv. org/ abs/ 2201. 11903 v6\nWeidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., \nMateos-Garcia, J., Bergman, S., Kay, J., Griffin, C., Bariach, B., \nGabriel, I., Rieser, V., & Isaac, W. (2023). Sociotechnical Safety \nEvaluation of Generative AI Systems. https:// arxiv. org/ abs/ 2310. \n11986 v2\nWu, N., Gong, M., Shou, L., Liang, S., & Jiang, D. (2023). Large \nLanguage Models are Diverse Role-Players for Summarization \nEvaluation. http:// arxiv. org/ abs/ 2303. 15078\nZhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuur -\nmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-\nto-Most Prompting Enables Complex Reasoning in Large Lan-\nguage Models. https:// arxiv. org/ abs/ 2205. 10625 v3\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.7209526896476746
    },
    {
      "name": "Performing arts",
      "score": 0.6686708927154541
    },
    {
      "name": "Psychology",
      "score": 0.48276272416114807
    },
    {
      "name": "Political science",
      "score": 0.4343968629837036
    },
    {
      "name": "Sociology",
      "score": 0.3656318783760071
    },
    {
      "name": "Computer science",
      "score": 0.3510236144065857
    },
    {
      "name": "Visual arts",
      "score": 0.15036562085151672
    },
    {
      "name": "Art",
      "score": 0.14127469062805176
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}