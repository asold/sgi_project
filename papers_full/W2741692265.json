{
  "title": "Comparing Character-level Neural Language Models Using a Lexical Decision Task",
  "url": "https://openalex.org/W2741692265",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2567870273",
      "name": "Gaël Le Godais",
      "affiliations": [
        "Université Paris Sciences et Lettres",
        "École des hautes études en sciences sociales"
      ]
    },
    {
      "id": "https://openalex.org/A817205692",
      "name": "Tal Linzen",
      "affiliations": [
        "Université Paris Sciences et Lettres",
        "École des hautes études en sciences sociales"
      ]
    },
    {
      "id": "https://openalex.org/A2121369328",
      "name": "Emmanuel Dupoux",
      "affiliations": [
        "École des hautes études en sciences sociales",
        "Université Paris Sciences et Lettres"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2014307400",
    "https://openalex.org/W2098192529",
    "https://openalex.org/W2185277953",
    "https://openalex.org/W1990948551",
    "https://openalex.org/W303185113",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1989462718",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963251942",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2168979204",
    "https://openalex.org/W4362220304",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2740606810",
    "https://openalex.org/W2037387434",
    "https://openalex.org/W2138723686",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2020220682",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W1917177419",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2885588803",
    "https://openalex.org/W1548013675"
  ],
  "abstract": "International audience",
  "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 125–130,\nValencia, Spain, April 3-7, 2017.c⃝2017 Association for Computational Linguistics\nComparing Character-level Neural Language Models Using a Lexical\nDecision Task\nGa¨el Le Godais1,3 Tal Linzen1,2 Emmanuel Dupoux1\n1LSCP, CNRS, EHESS and ENS, PSL Research University 2IJN 3ENSIMAG\ngael.le-godais@orange.fr,{tal.linzen,emmanuel.dupoux}@gmail.com\nAbstract\nWhat is the information captured by neural\nnetwork models of language? We address\nthis question in the case of character-level\nrecurrent neural language models. These\nmodels do not have explicit word repre-\nsentations; do they acquire implicit ones?\nWe assess the lexical capacity of a network\nusing the lexical decision task common in\npsycholinguistics: the system is required\nto decide whether or not a string of charac-\nters forms a word. We explore how accu-\nracy on this task is affected by the architec-\nture of the network, focusing on cell type\n(LSTM vs. SRN), depth and width. We\nalso compare these architectural properties\nto a simple count of the parameters of the\nnetwork. The overall number of parame-\nters in the network turns out to be the most\nimportant predictor of accuracy; in partic-\nular, there is little evidence that deeper net-\nworks are beneﬁcial for this task.\n1 Introduction\nNeural networks have rapidly become ubiquitous\nin natural language processing systems, but our\nability to understand those networks has not kept\npace: we typically have little understanding of a\ntypical neural network beyond its accuracy on the\ntask it was trained to do. One potential way to gain\ninsight into the ability of a trained model is to eval-\nuate it on an interpretable auxiliary task that is dis-\ntinct from the task that the network was trained on:\na network that performs a particular auxiliary task\nsuccessfully is likely to have internal representa-\ntions that encode the information relevant for that\ntask (Adi et al., 2017; Mikolov et al., 2013). Lin-\nguistics and psycholinguistics offer a rich reper-\ntoire of tasks that have been used for decades to\nstudy the components of the human mind; it is nat-\nural to use these tasks to understand the abilities\nof artiﬁcial neural networks (Dunbar et al., 2015;\nLinzen et al., 2016).\nThe present work takes up character-level neu-\nral network language models. Such models\nhave been surprisingly competitive in applica-\ntions, even though they do not explicitly represent\nwords (Chung et al., 2016; Kim et al., 2016). Our\ngoal is to shed light on the ability of character-\nlevel models to implicitly learn a lexicon. We\nuse a task designed to investigate humans lexical\nprocesses. This task is based on a simple ques-\ntion: how well can the subject distinguish real\nwords from character strings that do not belong\nto the language (nonwords)? Since character-level\nlanguage models deﬁne a probability distribution\nover all character strings, we can perform this\ntask in a particularly straightforward way: given a\nword and a nonword that are matched on low-level\nproperties such as length and character bigram fre-\nquency, we expect the probability of the word to be\nhigher than the probability of the nonword.\nWe systematically explore how the performance\nof the network on this task is affected by three ar-\nchitectural parameters. First, we vary the depth\nof the network (number of layers); second, we\nvary the number of units in each layer; and ﬁnally,\nwe compare simple recurrent networks (SRN)\nto networks with long short-term memory cells\n(LSTM). We ﬁnd that the main factor that deter-\nmines the lexical capacity of the network is the to-\ntal number of parameters rather than any one of\nthese architectural properties.\n2 Lexical decision\nThe lexical decision task is widely used in cog-\nnitive psychology to probe human lexical repre-\nsentations (Meyer and Schvaneveldt, 1971; Balota\n125\net al., 2006). In the standard version of the task,\nwhich we refer to as yes/no lexical decision, the\nsubject is presented with a string of characters—\ne.g., horse in one trial or porse in another—and\nis requested to indicate whether or not the string\nmakes up a word. A large array of properties of\nthe word (or nonword) have been found to inﬂu-\nence human performance on the task, measured\nin accuracy and reaction time; most famously, hu-\nmans recognize frequent words more quickly and\naccurately than infrequent ones.\nOur goal is to administer the lexical decision\ntask to a character-level language model. Such a\nlanguage model should assign a higher probability\nto words than to nonwords. At ﬁrst blush, it ap-\npears straightforward to perform the task by ﬁxing\na probability threshold and classifying all of the\nstrings whose probability falls above this threshold\nas words and all of the strings that fall below it as\nnonwords. In preliminary experiments, however,\nwe found it difﬁcult to deﬁne such a threshold. At\na minimum, the probability assigned by the model\nto strings strongly depends on their length, so nor-\nmalization for length is essential (see Lau et al.\n(2016) for discussion); even after normalization,\nhowever, it remained challenging to set a thresh-\nold distinguishing words from nonwords.\nInstead of the standard yes/no lexical decision\ntask, then, we use a forced choice variant of the\ntask (Baddeley et al., 1993). In this version, two\nstrings are simultaneously presented, one of which\nis always a word and the other always a nonword;\nsubjects are instructed to select the item that they\nbelieve is a word. The advantage of this setup is\nthat we can match each word with a nonword that\nis maximally similar to it in length or any other\nproperties that may be relevant, thus avoiding\ncomplicated probability normalization schemes.\n3 Models\nWe tested two types of recurrent units: the classic\nElman (1990) architecture, which we refer to as\nsimple recurrent network (SRN), and Long Short-\nTerm Memory units, or LSTM (Hochreiter and\nSchmidhuber, 1997). Since each LSTM unit con-\ntains several gates and a memory cell, it has ap-\nproximately four times as many connections as an\nSRN unit, and therefore four times as many pa-\nrameters.\nThe ﬁrst layer of each network is a character\nembedding. This layer is followed by one or more\nrecurrent layers with atanh nonlinearity, each fol-\nlowed by a batch normalization layer (Ioffe and\nSzegedy, 2015). A pair of ‘view’ layers then re-\nshape the tensor with a linear transformation be-\ntween them, yielding predicted scores for each el-\nement of the vocabulary. Finally, the output is pro-\nduced by a softmax layer that gives a probability\ndistribution over the next character.\nHow many parameters does each network have?\nLet n be its number of recurrent layers, V the size\nof the vocabulary (all possible characters), D the\nsize of the character embedding, and H the num-\nber of units per layer. Table 1 shows the number\nof parameters in each layer:\nLayer Parameters\nCharacter embedding layer V D\nFirst SRN layer H(D + H + 1)\nFirst LSTM layer 4H(D + H + 1)\nAdditional SRN layer H(2H + 1)\nAdditional LSTM layer 4H(2H + 1)\nBatch normalization layers H\nFirst ‘view’ H\nLinear transformation HV\nSecond ‘view’ V\nTable 1: Number of parameters in each of the com-\nponents of the model.\nIn addition to the RNNs, we test two simple\nbaselines: a bigram and a unigram model of the\ntraining set. The goal of these baselines is to eval-\nuate the nonwords: if a unigram model can reli-\nably distinguish nonwords from words, the non-\nwords are not sufﬁciently challenging; this could\nhappen, for example, if the nonwords tend to have\nrare characters such as Q or Z.\n4 Methods\nCorpus: We trained our language models on a\nsubset of the movie book project corpus (Zhu\net al., 2015); the subset contained approximately\n50M characters (10M words). The corpus was\nlowercased by its creators. We split the corpus\ninto training, validation and test sets ( 80%, 10%\nand 10% of the data, respectively); this test set\nwas used only to calculate perplexity (see below).\nThe vocabulary we used to test our network in the\nlexical decision task only included words that oc-\n126\n(a)\n16 32 64 128\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●●\n●\n●\n●\n●●●●\n●\n●●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●●\n●●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\n●\n60%\n80%\n100%\n1 2 3 1 2 3 1 2 3 1 2 3\nNumber of layers\nAccuracy\nCell type\n●\n●\nSRN\nLSTM (b)\nSRN\nLSTM\nRandom guessing\n60%\n80%\n100%\n214 215 216 217 218\nNumber of parameters\nAccuracy\nFigure 1: Accuracy as a function of the complexity of the network. The dashed line represents chance\naccuracy (50%). Each dot represents a single run.) (a) Detailed results by architecture, number of units\nper layer (16, 32, 64 or 128) and number of layers. (b) Relationship between accuracy and total number\nof parameters (on a logarithmic scale).\ncurred in the training set.1\nNonword generation: We generated nonwords\nusing a slightly modiﬁed version of Wuggy\n(Keuleers and Brysbaert, 2010); we refer the\nreader to the original paper and our published code\nfor further details.\nThe algorithm takes a list of words as its input\nand outputs a matching nonword for each word of\nthe list. Matching is performed using a phono-\ntactic grammar of the lexicon. This phonotactic\ngrammar is based on a segmentation of the words\ninto syllables and subsyllabic elements (onset, nu-\ncleus and coda). A syllabiﬁcation dictionary splits\nthe words into a sequence of syllables. Each syl-\nlable is then segmented into subsyllabic elements\nusing a grammar of legal subsyllabic sequences.\nEach subsyllabic element is represented by a tuple\nthat records its letters, position in the word, total\nnumber of subsyllabic elements in the word and\nthe subsyllabic element that follows it. The ﬁrst\nthree elements of the tuples form a “link”. The\nfrequency of a link is computed from the lexicon,\nalong with its possible next subsyllabic elements.\nThis makes up a “bigram chain” that describes the\nphonotactics of the lexicon. For a given word, a\nnonword is generated by the bigram chain with pa-\nrameters as similar as possible as the input word.\n1A network may be able to correctly perform a lexical\ndecision on words to which it has not been exposed if those\nwords follow the word formation rules of the language (e.g.,\nFrenchify); we are exploring this issue in ongoing work.\nSuch parameters deﬁned by the bigram chain can\nbe, but are not limited to, the total length of the\nword and the transition probabilities between its\nsubsyllabic elements.\nTask: The RNN deﬁnes a probability distribu-\ntion over character strings. We performed the\nforced choice task by calculating the probability\nof the word and the probability of the nonword,\nand selecting the string that had a higher proba-\nbility; trials in which the probability of nonword\nwas higher were considered to be errors. To en-\nsure that we were computing the probability of a\nword rather than a preﬁx or sufﬁx (e.g., cat as a\npreﬁx of category), we added spaces on either side\nof the word; e.g., we computed the probability of\n‘ cat ’ rather than ‘cat’. We transformed the train-\ning corpus accordingly, to ensure that all words en-\ncountered during training contribute to the lexical\ndecision, including words preceded or followed by\na punctuation mark or a sentence boundary.\nExperiments: We trained networks with all\ncombinations of unit type (LSTM or SRN), width\n(16, 32, 64 or 128 hidden units per layer) and\ndepth (one, two or three hidden layers). To es-\ntimate the impact of random initialization on the\nresults, we trained six networks with each combi-\nnation of parameters.2\nWe used a slightly modiﬁed version of Justin\n2Our code can be found at https://github.com/\nbootphon/char_rnn_lexical_decision.\n127\nLSTM\nSRN\nRandom guessing\n60%\n80%\n100%\n2.30 2.64 3.03 3.48\nPerplexity\nAccuracy\nFigure 2: The relationship between character-level\nperplexity and lexical decision accuracy. Each\npoint represent a single ﬁtted model.\nJohnson’s Torch implementation of character-level\nRNNs.3 To prevent overﬁtting, the networks\nwere trained using early stopping based on vali-\ndation set loss. They were optimized using Adam\n(Kingma and Ba, 2015) with a learning rate of\n2e−3. The number of distinct characters was\n95, and the dimension of the character embed-\ndings was 64. During training, the networks op-\nerated over minibatches of size 50 and sequences\nof length 50.\n5 Results\nThe accuracy of the unigram and bigram baselines\nwas 49.6% and 52.1% respectively, very close to\nchance accuracy (50%). This suggests that the\nnonwords we generated were sufﬁciently difﬁcult\nto distinguish from the words. The results of the\nRNNs we trained are shown in Figure 1a. All\nof the three architectural parameters affected per-\nformance in the task: networks with LSTM cells\nperformed better than SRNs with the same num-\nber of units and layers. Increasing the number\nof units per layer was beneﬁcial across the board.\nAdditional layers improved performance as well,\nthough the addition of the third layer was often\nless beneﬁcial than the addition of the second one.\nGiven a ﬁxed budget of units, it was more useful\nto deploy them in a wide and shallow network than\na narrow and deep network (e.g., an SRN with 32\nhidden units in one layer outperformed an SRN\nwith 16 hidden units in two layers).\n3https://github.com/jcjohnson/\ntorch-rnn\nHow much of the advantage of LSTMs is due to\nthe fact that they have more parameters per unit?\nFigure 1b plots the accuracy of the same networks,\nthis time against the log-transformed number of\nparameters. While there remains a slight advan-\ntage for LSTMs over SRNs, especially as the num-\nber of parameters increases, it is evident that the\nnumber of parameters is an excellent predictor of\nthe performance of the network. Of course, since\nthe dependencies that the network needs to model\nto perform the lexical decision task are relatively\nshort, this task may not bring out the competitive\nadvantage of LSTMs, which are argued to excel in\ntasks that require long dependencies.\nWe plot the relationship between the perplexity\nof the language model and its accuracy in the lex-\nical decision task in Figure 2. This relationship is\nnot entirely surprising, given that low perplexity\nindicates that the model assigns high likelihood to\nthe character sequences that occurred in the test\nset, which are of course much more likely to be\nwords than nonwords. The two measures are far\nfrom being identical, however. Perplexity incor-\nporates the model’s ability to predict dependencies\nacross words; this is not the case for lexical deci-\nsion, where performance may in fact be hindered\nby irrelevant contextual information, as it is for hu-\nmans (McDonald and Shillcock, 2001). Perplexity\nalso weights accurate prediction of frequent words\nmuch more highly than infrequent words. Given\nthese differences, the measures could potentially\ndiverge in subsets of the lexicon.\n6 Discussion\nThe lexical capacity measure that we have pro-\nposed assigns the same weight to rare and frequent\nwords. As such, it may provide an alternative eval-\nuation metric for character-based language mod-\nels, supplementing the more standard measure of\nperplexity, which is biased in favor of frequent\nwords and conﬂates lexical knowledge with longer\ndependencies across words.\nOne advantage of the evaluation metric we have\nproposed is that it is in principle possible to com-\npare it to human performance. This contrasts\nwith perplexity, which does not map onto any task\nthat can be given to humans, especially when the\nmodel is at the character level. For example, our\npreliminary analyses showed that the model makes\nmore errors on low-frequency than high-frequency\nwords, a pattern that is qualitatively similar to hu-\n128\nmans (Ratcliff et al., 2004).\nSome challenges remain, however, before a\nquantitative comparison before humans and neu-\nral network language models can be performed.\nExisting large-scale human behavioral datasets are\nbased on a speeded yes/no version of the task, in\nwhich participants are instructed to make a lex-\nical decision on a single string of characters as\nquickly as possible (Balota et al., 2007), whereas\nour evaluation is based on the forced choice task\nand does not incorporate time pressure. A be-\nhavioral dataset with the paradigm we have used\nshould be easy to collect using crowdsourcing. Al-\nternatively, direct comparison to existing human\ndatasets could be made possible by developing re-\nliable ways to map language model probabilities\nonto timed yes/no lexical decisions; our initial ex-\nperiments suggest that some nontrivial challenges\nwould need to be overcome before this direction\ncan be pursued.\nOur work is related to early work that aimed\nto measure the phonotactic knowledge of recur-\nrent networks (Stoianov et al., 1998; Stoianov\nand Nerbonne, 2000). This idea was developed\nby Testolin et al. (2016), who use the lexical de-\ncision task to measure the orthographic knowl-\nedge of various neural networks and n-gram mod-\nels. The Naive Discriminative Learner (Baayen\net al., 2011), which can be seen as a simple non-\nrecurrent neural network, has been used to model\nhuman lexical decision reaction times. Finally, our\nwork is related to work on syntax that evaluated\nwhether a word-level language model assigns a\nhigher probability to an grammatical sentence than\nto a minimally different ungrammatical one (Lau\net al., 2016; Linzen et al., 2016; Sennrich, 2017).\nIn summary, the main result of this study is that\nwith a sufﬁcient number of parameters character-\nlevel neural networks are able to perform lexical\ndecisions with high levels of performance, despite\nnot being trained on this task. A second important\nresult is that the main predictor of lexical decision\naccuracy was the total number of parameters in the\nnetwork; we found no evidence that deep networks\nare superior to shallow and wide ones on this task.\nAcknowledgements\nWe thank Emmanuel Keuleers for his assistance\nwith the Wuggy nonword generator. This research\nwas supported by the European Research Coun-\ncil (grant ERC-2011-AdG 295810 BOOTPHON)\nand the Agence Nationale pour la Recherche\n(grants ANR-10-IDEX-0001-02 PSL and ANR-\n10-LABX-0087 IEC).\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. In International Conference on Learning\nRepresentations.\nR. Harald Baayen, Petar Milin, Dusica F. Djurdjevi ´c,\nPeter Hendrix, and Marco Marelli. 2011. An amor-\nphous model for morphological processing in visual\ncomprehension based on naive discriminative learn-\ning. Psychological Review, 118(3):438–481.\nAlan Baddeley, Hazel Emslie, and Ian Nimmo-Smith.\n1993. The spot-the-word test: A robust estimate of\nverbal intelligence based on lexical decision. British\nJournal of Clinical Psychology, 32(1):55–65.\nDavid A. Balota, Melvin J. Yap, and Michael J.\nCortese. 2006. Visual word recognition: The jour-\nney from features to meaning (a travel update). In\nHandbook of psycholinguistics, pages 285–375.\nDavid. A. Balota, Melvin. J. Yap, Michael J. Cortese,\nKeith A. Hutchison, Brett Kessler, Bjorn Loftis,\nJames H. Neely, Douglas L. Nelson, Greg B. Simp-\nson, and Rebecca Treiman. 2007. The En-\nglish lexicon project. Behavior Research Methods,\n39(3):445–459.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Ben-\ngio. 2016. A character-level decoder without ex-\nplicit segmentation for neural machine translation.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1693–1703. Association for\nComputational Linguistics.\nEwan Dunbar, Gabriel Synnaeve, and Emmanuel\nDupoux. 2015. Quantitative methods for compar-\ning featural representations. In Proceedings of the\n18th International Congress of Phonetic Sciences.\nJeffrey L. Elman. 1990. Finding structure in time.\nCognitive Science, 14(2):179–211.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. In Proceedings\nof the 32nd International Conference on Machine\nLearning.\nEmmanuel Keuleers and Marc Brysbaert. 2010.\nWuggy: A multilingual pseudoword generator. Be-\nhavior Research Methods, 42(3):627–633.\n129\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Thirtieth AAAI Conference on Ar-\ntiﬁcial Intelligence, pages 2741–2749, Phoenix, AZ.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2016. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nScott A. McDonald and Richard C. Shillcock. 2001.\nRethinking the word frequency effect: The ne-\nglected role of distributional information in lexical\nprocessing. Language and Speech, 44(3):295–323.\nDavid E. Meyer and Roger W. Schvaneveldt. 1971. Fa-\ncilitation in recognizing pairs of words: Evidence of\na dependence between retrieval operations. Journal\nof Experimental Psychology, 90(2):227–234.\nTomas Mikolov, Wen-Tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 746–751, Atlanta,\nGeorgia, June. Association for Computational Lin-\nguistics.\nRoger Ratcliff, Pablo Gomez, and Gail McKoon. 2004.\nA diffusion model account of the lexical decision\ntask. Psychological Review, 111(1):159–182.\nRico Sennrich. 2017. How grammatical is character-\nlevel neural machine translation? Assessing MT\nquality with contrastive translation pairs. In Pro-\nceedings of the Conference of the European Chapter\nof the Association for Computational Linguistics.\nIvelin Stoianov and John Nerbonne. 2000. Explor-\ning phonotactics with simple recurrent networks. In\nWalter Daelemans, editor, Proceedings of Compu-\ntational Linguistics in the Netherlands, 2000 , vol-\nume 29, pages 51–68.\nIvelin Stoianov, Huub Bouma, and John Nerbonne.\n1998. Modeling the phonotactic structure of natural\nlanguage words with simple recurrent networks. In\nHans van Halteren Peter-Arno Coppen and Lisanne\nTeunissen, editors, Proceedings of Computational\nLinguistics in the Netherlands, 1997 , pages 77–95.\nAmsterdam: Rodopi.\nAlberto Testolin, Ivelin Stoianov, Alessandro Sperduti,\nand Marco Zorzi. 2016. Learning orthographic\nstructure with sequential generative neural networks.\nCognitive Science, (3):579–606.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision , pages\n19–27.\n130",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7920000553131104
    },
    {
      "name": "Task (project management)",
      "score": 0.7335845232009888
    },
    {
      "name": "Psycholinguistics",
      "score": 0.7172105312347412
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6603148579597473
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6373577117919922
    },
    {
      "name": "Natural language processing",
      "score": 0.6368265748023987
    },
    {
      "name": "Artificial neural network",
      "score": 0.562740683555603
    },
    {
      "name": "Word (group theory)",
      "score": 0.553481936454773
    },
    {
      "name": "Lexical decision task",
      "score": 0.5018467903137207
    },
    {
      "name": "Language model",
      "score": 0.4841340482234955
    },
    {
      "name": "String (physics)",
      "score": 0.41699764132499695
    },
    {
      "name": "Linguistics",
      "score": 0.18572917580604553
    },
    {
      "name": "Cognition",
      "score": 0.100766122341156
    },
    {
      "name": "Psychology",
      "score": 0.08233892917633057
    },
    {
      "name": "Mathematics",
      "score": 0.07003912329673767
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210151031",
      "name": "Laboratoire de Sciences Cognitives et Psycholinguistique",
      "country": "FR"
    }
  ],
  "cited_by": 18
}