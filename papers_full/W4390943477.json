{
  "title": "Optimizing classification of diseases through language model analysis of symptoms",
  "url": "https://openalex.org/W4390943477",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2549901126",
      "name": "esraa hassan",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    },
    {
      "id": "https://openalex.org/A2798914868",
      "name": "Tarek Abd-El-Hafeez",
      "affiliations": [
        "Minia University"
      ]
    },
    {
      "id": "https://openalex.org/A4209084768",
      "name": "Mahmoud Y. Shams",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    },
    {
      "id": "https://openalex.org/A2549901126",
      "name": "esraa hassan",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    },
    {
      "id": "https://openalex.org/A2798914868",
      "name": "Tarek Abd-El-Hafeez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209084768",
      "name": "Mahmoud Y. Shams",
      "affiliations": [
        "Kafrelsheikh University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3111299783",
    "https://openalex.org/W3192087497",
    "https://openalex.org/W4385729641",
    "https://openalex.org/W3103605846",
    "https://openalex.org/W4282978951",
    "https://openalex.org/W4385880828",
    "https://openalex.org/W4361263460",
    "https://openalex.org/W4322772630",
    "https://openalex.org/W3128862819",
    "https://openalex.org/W3131666147",
    "https://openalex.org/W4214873408",
    "https://openalex.org/W4320494184",
    "https://openalex.org/W4366830012",
    "https://openalex.org/W3003242248",
    "https://openalex.org/W4206546368",
    "https://openalex.org/W4382450509",
    "https://openalex.org/W4364368785",
    "https://openalex.org/W3043102475",
    "https://openalex.org/W4380356533",
    "https://openalex.org/W4385234224",
    "https://openalex.org/W4289849144",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2009890917",
    "https://openalex.org/W2986204087",
    "https://openalex.org/W3206308294",
    "https://openalex.org/W4213441134",
    "https://openalex.org/W4386161539",
    "https://openalex.org/W4282958554",
    "https://openalex.org/W4387163219",
    "https://openalex.org/W4226117650",
    "https://openalex.org/W3089301520",
    "https://openalex.org/W4297539676",
    "https://openalex.org/W6813345622",
    "https://openalex.org/W2912971066",
    "https://openalex.org/W2964496569",
    "https://openalex.org/W2917157846",
    "https://openalex.org/W3091174347",
    "https://openalex.org/W3103301438",
    "https://openalex.org/W4224308224",
    "https://openalex.org/W4376255634",
    "https://openalex.org/W4387272159",
    "https://openalex.org/W4388033280",
    "https://openalex.org/W3208498407",
    "https://openalex.org/W4284973298",
    "https://openalex.org/W2625609557",
    "https://openalex.org/W4382195656"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports\nOptimizing classification \nof diseases through language \nmodel analysis of symptoms\nEsraa Hassan 1*, Tarek Abd El‑Hafeez 2,3* & Mahmoud Y. Shams 1*\nThis paper investigated the use of language models and deep learning techniques for automating \ndisease prediction from symptoms. Specifically, we explored the use of two Medical Concept \nNormalization—Bidirectional Encoder Representations from Transformers (MCN‑BERT) models \nand a Bidirectional Long Short‑Term Memory (BiLSTM) model, each optimized with a different \nhyperparameter optimization method, to predict diseases from symptom descriptions. In this \npaper, we utilized two distinct dataset called Dataset‑1, and Dataset‑2. Dataset‑1 consists of 1,200 \ndata points, with each point representing a unique combination of disease labels and symptom \ndescriptions. While, Dataset‑2 is designed to identify Adverse Drug Reactions (ADRs) from Twitter \ndata, comprising 23,516 rows categorized as ADR (1) or Non‑ADR (0) tweets. The results indicate that \nthe MCN‑BERT model optimized with AdamP achieved 99.58% accuracy for Dataset ‑1 and 96.15% \naccuracy for Dataset‑2. The MCN‑BERT model optimized with AdamW performed well with 98.33% \naccuracy for Dataset‑1 and 95.15% for Dataset‑2, while the BiLSTM model optimized with Hyperopt \nachieved 97.08% accuracy for Dataset‑1 and 94.15% for Dataset‑2. Our findings suggest that language \nmodels and deep learning techniques have promise for supporting earlier detection and more prompt \ntreatment of diseases, as well as expanding remote diagnostic capabilities. The MCN‑BERT and \nBiLSTM models demonstrated robust performance in accurately predicting diseases from symptoms, \nindicating the potential for further related research.\nIn the field of healthcare, accurate and timely diagnosis of diseases is of paramount importance for effective \ntreatment and patient  care1–3. Traditionally, medical professionals rely on their expertise and diagnostic tests \nto identify diseases based on a patient’s symptoms. However, this process can be time-consuming, subjective, \nand prone to  errors4. In recent years, there has been a growing interest in leveraging the power of language \nmodels and deep learning techniques to develop automated systems capable of predicting diseases directly \nfrom symptom descriptions. These advanced models have the potential to revolutionize healthcare by enabling \nearly disease detection, facilitating prompt medical attention, and providing remote diagnosis and treatment \n recommendations5–8.\nOne promising approach to tackle this challenge is to harness the capabilities of language models, such as \nBERT (Bidirectional Encoder Representations from Transformers), which have demonstrated remarkable suc-\ncess in various natural language processing tasks. Language models like BERT can learn contextual representa-\ntions of words and sentences, capturing the intricate relationships between symptoms and diseases. By training \nthese models on large medical text corpora, they can acquire domain-specific knowledge and improve disease \nprediction  accuracy9–11.\nTo further enhance the predictive capabilities of the language model, a bidirectional LSTM (Long Short-Term \nMemory) layer can be incorporated. The bidirectional LSTM allows the model to capture both past and future \ncontext, enabling a better understanding of the symptom descriptions and their relevance to specific diseases. \nThe bidirectional LSTM, coupled with the powerful representation learning of BERT, forms a robust framework \nfor accurate disease  prediction12–14.\nHowever, developing an optimal language model architecture and determining the best hyperparameters can \nbe a complex and time-consuming process. To address this challenge, the Hyperopt library can be employed. \nHyperopt utilizes a Bayesian optimization  algorithm13 (such as TPE—Tree-structured Parzen Estimator) to effi-\nciently search through a predefined hyperparameter space and identify the optimal configuration for the language \nOPEN\n1Faculty of Artificial Intelligence, Kafrelsheikh University, Kafrelsheikh 33516, Egypt. 2Department of Computer \nScience, Faculty of Science, Minia University, Minia 61519, Egypt. 3Computer Science Unit, Deraya University, \nMinia University, Minia 61765, Egypt. *email: esraa.hassan@ai.Kfs.edu.eg; tarek@mu.edu.eg; mahmoud.yasin@\nai.kfs.edu.eg\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nmodel. This automated hyperparameter tuning greatly enhances the model’s performance and  generalizability15,16. \nAccurate and timely diagnosis of diseases based on symptom descriptions is a critical aspect of effective health-\ncare delivery. However, traditional diagnostic approaches heavily rely on the expertise and experience of medical \nprofessionals, which can be subjective, time-consuming, and error prone. Furthermore, the increasing volume of \nmedical literature and the complexity of disease manifestations pose significant challenges for accurate disease \ndiagnosis. To address these challenges, there is a need for automated systems that can predict diseases directly \nfrom symptom descriptions, leveraging the power of language models and deep learning techniques. The objec-\ntive is to develop a robust and accurate model that can assist healthcare professionals in making timely and \nprecise diagnoses, leading to improved patient outcomes. The existing approaches to disease prediction from \nsymptoms often suffer from limitations. Conventional machine learning algorithms can struggle to capture \nthe intricate relationships and context between symptoms and diseases, leading to suboptimal predictive per -\nformance. Additionally, manually designing the architecture and selecting hyperparameters for these models \ncan be a time-consuming and resource-intensive process. To overcome these limitations, our study focuses on \ndeveloping an advanced language model for disease prediction, specifically utilizing the MCN-BERT  + AdamP \nand MCN-BERT + AdamW architectures. These models combine the power of BERT’s contextual embeddings, \nbidirectional LSTM layers, and hyperparameter optimization using Hyperopt to improve disease prediction \naccuracy.\nDespite the growing interest in leveraging language models and deep learning techniques for disease predic-\ntion from symptom descriptions, there is a lack of comprehensive studies that integrate the power of BERT’s \ncontextual embeddings, bidirectional LSTM layers, and automated hyperparameter optimization using Hyperopt \nin the field of healthcare. Existing research often focuses on individual components or employs simpler models, \nwithout fully exploring the potential of these advanced techniques. Therefore, there is a research gap in develop-\ning a robust and accurate language model architecture specifically designed for disease diagnosis. The motivation \nbehind this research is to address the limitations of traditional diagnostic methods in healthcare, which can be \ntime-consuming, subjective, and prone to errors. By leveraging the power of language models and deep learn -\ning techniques, there is an opportunity to revolutionize disease diagnosis by enabling early detection, prompt \nmedical attention, and remote diagnosis and treatment recommendations.\nThe proposed research aims to harness the capabilities of BERT’s contextual embeddings, bidirectional LSTM \nlayers, and hyperparameter optimization using Hyperopt to develop an accurate and efficient language model \nfor disease prediction from symptom descriptions.\nThere are several challenges in developing an optimal language model architecture for disease diagnosis. \nFirstly, the selection and integration of suitable components such as BERT, bidirectional LSTM, and hyperpa -\nrameter optimization algorithms require careful consideration to ensure compatibility and maximize perfor -\nmance. Secondly, training and fine-tuning large language models like BERT on medical text corpora can be \ncomputationally expensive and time-consuming. Handling such computational challenges efficiently is crucial. \nAdditionally, the evaluation and validation of the proposed models need to be conducted rigorously, comparing \nthem against existing methods and considering real-world healthcare scenarios. The main contributions of this \nresearch are as follows:\n1. Develop an effective language model that can accurately predict diseases from symptom descriptions.\n2. Investigate the performance of the MCN-BERT + AdamP and MCN-BERT + AdamW architectures in disease \nprediction using two distinct datasets.\n3. Explore the impact of incorporating bidirectional LSTM layers to capture the contextual relationships \nbetween symptoms and diseases.\n4. Apply hyperparameter optimization using Hyperopt to enhance the model’s performance and generalize \nwell to unseen data.\nBy addressing these goals, we aim to provide healthcare professionals with a reliable and efficient tool for dis-\nease diagnosis. This enables early detection, prompt intervention, and personalized treatment recommendations, \nultimately leading to improved patient care and outcomes. In the following sections, we describe the methodology \nused to develop the language model, including data preprocessing, model architecture, and hyperparameter opti-\nmization. We present the experimental results, evaluate the performance of the proposed models, and compare \nthem with existing approaches. The reminder of this paper is organized as follows. Section \"Related work\" states \nthe current efforts and the related work for automating disease prediction from symptoms. Section \"Preliminar-\nies\" investigtes the preliminaries, and Section \"Proposed work \" shows the proposed MCN-BERT method. The \nexperimental results is demonstrated in Section \"Experimental and results\". The Discussion, limitation and \nconclusions are shown in sections \"Discussion\", \"Limitations\", and “Conclusion and future work” , respectively.\nRelated work\nThe literature studies have explored various approaches and methodologies for detecting adverse drug reactions \n(ADRs) to ensure patient safety and optimize medication outcomes. In this section, the advent of deep learning \nmodels like BERT (Bidirectional Encoder Representations from Transformers) has significantly advanced this \nfield in recent years. Molina et al. 17 enhances DDI relationship extraction using two models with a Gaussian \nnoise layer. The PW-CNN model captures pharmacological entity relationships in biomedical databases, while \nthe BERT language model classifies and integrates data from target entities.\nThe experiment shows improved performance compared to previous models. Machado et al. 18 highlight \nthe use of Natural Language Processing (NLP) and machine learning classifier training to extract drug-drug \ninteractions from unstructured data, supporting clinical prescribing decisions. The proposed system generates \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nstructured information from three data sources, identifying drug entities and determining interactions. Nguyen \net al. 19 investigate the Relation Bidirectional Encoder Representations from Transformers (Relation BERT) \narchitecture for detecting and classifying DDIs in biomedical texts. Three models, R-BERT * , RBioBERT1, and \nR-BioBERT2, achieve a macro-average F1-score of over 79% and 90.63% and 97% accuracy respectively, promot-\ning the widespread application of automatic DDI extraction. KafiKang et al. 20 presents a novel solution using \nRelation BioBERT (R-BioBERT) and Bidirectional Long Short-Term Memory (BLSTM) to detect and classify \nDrug-Drug Interactions (DDIs), enhancing prediction accuracy and identifying specific drug interaction types, \nwith higher F-scores. Y ang et al.21 proposes CAC model is a multi-layer feature fusion text classification model \nthat combines CNN and attention. It extracts local features and calculates global attention, drawing inspiration \nfrom membrane computing. Experimental results show that the CAC model outperforms models relying solely \non attention and exhibits significant improvements in accuracy and performance compared to other models.\nChaichulee et al.22 evaluated three NLP techniques—Naive Bayes-Support Vector Machine (NB-SVM), Uni-\nversal Language Model Fine-tuning (ULMFiT), and various pre-trained BERT models including mBERT, XLM-\nRoBERTa, WanchanBERTa, and a domain-specific AllergyRoBERTa model trained on a dataset of 79,712 drug \nallergy records reviewed by pharmacists—to identify symptom terms from clinical notes, finding that while the \nBERT models generally demonstrated the highest performance, the NB-SVM model outperformed ULMFiT \nand BERT for less frequently coded symptoms. An ensemble model combining the different algorithms achieved \nstrong results with 95.33% exact match ratio, 98.88% F1 score, and 97.07% mean average precision for the 36 most \nfrequent symptoms, and this developed model was further enhanced into a symptom term suggestion system that \ntested well in prospective pharmacist trials with a 0.7081 Krippendorff ’s alpha agreement coefficient, indicating \nreasonably high agreement between the model’s suggestions and pharmacist assessments.\nLee et al. 23 proposed BioBERT, a specialized language representation model designed for biomedical text \nmining which is pre-trained on large-scale biomedical corpora. BioBERT was found to exhibit superior perfor-\nmance compared to BERT as well as previous state-of-the-art models on various biomedical text mining tasks, \nsignificantly surpassing baseline models in biomedical named entity recognition with a 0.62% F1 score improve-\nment, biomedical relation extraction with a 2.80% F1 score improvement, and biomedical question answering \nwith a 12.24% MRR improvement. In contrast, while BERT performed similarly to previous models, the analysis \nindicated that pre-training BERT on biomedical data enhances its ability to comprehend complex biomedical \ntexts, demonstrating BioBERT’s advantages for biomedical natural language understanding tasks over baselines.\nThe main objective of Huang et al. 24 is to create and assess a continuous representation of clinical notes in \norder to predict 30-day hospital readmission at different stages of admission, including early stages and at dis-\ncharge. They utilize bidirectional encoder representations from transformers (BERT) for analyzing clinical text. \nSince publicly available BERT parameters are trained on standard corpora like Wikipedia and BookCorpus, \nwhich differ from clinical text, they pre-train BERT using clinical notes and fine-tune the network specifically for \npredicting hospital readmission. This results in the development of ClinicalBERT. ClinicalBERT demonstrates \nsuperior performance compared to various baseline models in predicting 30-day hospital readmission, utilizing \nboth discharge summaries and the initial days of notes in the intensive care unit, based on clinically relevant \nmetrics. Additionally, the attention weights of ClinicalBERT can be utilized to interpret the predictions made by \nthe model, providing valuable insights. Their model achieved Area Under the Receiver Operation Curve 0.714 \nbased on the clinical BERT.\nHazell and  Shakir25 conducted a review to assess the extent of under-reporting of adverse drug reactions \n(ADRs) in spontaneous reporting systems. A literature search identified 37 studies from 12 countries using \ndiverse methodologies like hospitals and general practices, which provided 43 estimates of under-reporting \ncalculated as the percentage of ADRs detected but not reported. The median under-reporting rate across stud -\nies was 94% with an interquartile range of 82–98%, with no significant difference in medians between general \npractice and hospital studies. However, general practice studies indicated a higher median under-reporting \nrate for all ADRs versus more serious/severe ADRs, while hospital studies consistently showed high medians \nfor serious/severe ADRs. Studies of specific serious/severe ADR-drug combinations had a lower but still high \nmedian under-reporting rate of 85%.\nPutra et al.26 presents a digestive system in processing daily consumed food and drinks. Their challenges are \nlack of awareness and knowledge about initial symptoms of digestive diseases can lead to serious complications, \neven death. Early identification of symptoms is essential for timely diagnosis and implementing control measures \nto prevent disease spread. The anamnesis process involves gathering disease symptoms through patient-medical \npersonnel interactions, which are recorded in Electronic Medical Records (EMRs) to aid Clinical Decision Sup-\nport (CDS). However, EMRs often pose challenges for computational processing due to grammar inconsistencies. \nTo enable computers to process natural languages, Natural Language Processing (NLP) techniques are employed. \nThis study focuses on developing an NLP system to identify symptoms of digestive diseases, optimizing the CDS \nprocess. Named Entity Recognition (NER) is utilized to determine tokens associated with disease symptoms. \nThrough training the model with 50 epochs, an F1-score accuracy of 0.79 is achieved. Experimental results \ndemonstrate that NER, supported by stemming and stopwords removal in pre-processing, enhances system \naccuracy. The summary of the current efforts of recent Advances in ADR detection using Machine Learning \nAlgorithms is shown in Table 1.\nPreliminaries\nBERT NLP optimization model\nBERT is a widely used open-source natural language processing platform that stands for Bidirectional Encoder \nRepresentations from Transformers. Developed to help machines better comprehend ambiguous meanings in \ntext or masked words in queries, BERT employs a transformer architecture where each output element attends \n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nto every input through learned weightings to capture relationships, establishing context critical for natural \nlanguage understanding. At its core, BERT leverages bidirectional transformers that allow information to flow \nboth forward and backward through the model, enabling it to learn the full context of language. By understand-\ning word interdependencies through bidirectional context, BERT can more accurately derive meanings, even \nwhen portions of text are removed. This ability to comprehend language holistically based on the entire input \nrather than just preceding words gives BERT strong performance on tasks like question answering, setting a new \nstandard in NLP and making it a popular foundation for many natural language  applications27–30.\nBERT learns language representations using an unsupervised pre-training strategy on a huge dataset, allowing \nthe model to comprehend the context of an input sentence. To achieve good results, the model can be fine-tuned \nafter pre-training on a task-specific supervised dataset. The fine-tuning stage includes two strategies: fine-tuning \nand feature based. Elmo employs a feature-based model, in which the model architecture is task-specific, with \neach task employing a distinct model and pre-trained language representations. BERT comprehends language by \nutilizing bidirectional layers of transformer encoders, hence the name BERT. Unlike previous language models \nthat generate context-free word embeddings, such as Glove2Vec and Word2Vec, BERT provides context by \nassessing the term’s relationship with the terms that come before and after  it31,32.\nBi‑directional (B)\nPrior to BERT, the models could only move the context window in one way. To grasp the context, it can either \nrelocate the word to the left or right. BERT, on the other hand, employs bidirectional language modeling. Accord-\ning to contextual language modeling, BERT can see the entire sentence and move it right or  left33.\nEncoder representations (ER)\nAny text that is passed via a language model can be encoded before being provided as input. Only the encoded \ntext can be processed and yield a result. Any model’s output can also be in encrypted format, which must be \ndecrypted. As a result, once a message has been encoded, it must be decoded again. It is a two-way mechanism.\nTransformers (T)\nFor text processing, BERT employs transformers and masked language modeling. The main challenge is com -\nprehending the context of the word mentioned in that location. The pronouns in a phrase can be difficult for \nthe machine to understand. Transformers can therefore pay attention to pronouns, try the word with the entire \nsentence, and comprehend the context. Masked language modeling prevents the target word from comprehending \nTable 1.  Summary of ADR detection studies using deep learning.\nAuthor Dataset used Methodology Results Comment\nMolina et al.17 Unstructured biomedical literature Deep learning models PW-\nCNN + BERT\nImproved performance compared to \nprevious models\nProposed a novel framework for DDI \nrelationship extraction using two deep \nlearning models, PW-CNN and BERT\nMachado et al.18 Electronic medical records NLP and machine learning classifier \ntraining Supports clinical prescribing decisions\nDeveloped a system to generate struc-\ntured information from three data \nsources, identifying drug entities and \ndetermining interactions\nNguyen et al.19 Biomedical texts\nRelation Bidirectional Encoder \nRepresentations from Transformers \n(Relation BERT) architecture\nMacro-average F1-score of over 79% \nand 90.63% and 97% accuracy\nProposed a novel architecture for \ndetecting and classifying DDIs in \nbiomedical texts\nKafiKang et al.20 Unstructured biomedical texts\nRelation BioBERT (R-BioBERT) \nand Bidirectional Long Short-Term \nMemory (BLSTM)\nEnhanced prediction accuracy and \nidentified specific drug interaction \ntypes\nPresented a novel solution using \nR-BioBERT and BLSTM to detect and \nclassify DDIs\nY ang et al.21 Biomedical texts\nCAC model: a multi-layer feature \nfusion text classification model that \ncombines CNN and attention\nOutperforms models relying solely \non attention and exhibits significant \nimprovements in accuracy and \nperformance\nProposed a novel CAC model that \ncombines CNN and attention to detect \nand classify DDIs\nChaichulee et al.22 79,712 drug allergy records\nNaive Bayes—Support Vector Machine \n(NB-SVM), Universal Language \nModel Fine-tuning (ULMFiT), and \nBidirectional Encoder Representations \nfrom Transformers (BERT)\nEnsemble model achieved strong \nresults, including an exact match ratio \nof 95.33%, an F1 score of 98.88%, and \na mean average precision of 97.07%\nPresented a dataset of drug allergy \nrecords and evaluated three NLP \ntechniques for detecting drug allergies. \nBERT models demonstrated the high-\nest performance\nLee et al.23 Biomedical corpora\nBioBERT (Bidirectional Encoder \nRepresentations from Transformers \nfor Biomedical Text Mining)\nExhibits superior performance com-\npared to BERT and previous state-of-\nthe-art models in various biomedical \ntext mining tasks\nProposed a specialized language \nrepresentation model, BioBERT, for \nbiomedical applications\nHuang et al.24 Clinical notes Bidirectional encoder representations \nfrom transformers (BERT)\nSuperior performance compared to \nvarious baseline models in predicting \n30-day hospital readmission\nDeveloped ClinicalBERT, a pre-trained \nBERT model on clinical notes, for pre-\ndicting 30-day hospital readmission\nHazell and  Shakir25 Systematic literature search NA Median under-reporting rate of 94%, \nwith an interquartile range of 82–98%\nReviewed the extent of under-report-\ning of ADRs in spontaneous reporting \nsystems\nPutra et al.26 Electronic medical records Named Entity Recognition (NER) f1-score accuracy of 0.79\nDeveloped an NLP system to identify \nsymptoms of digestive diseases, opti-\nmizing the CDS process\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nit. The mask prevents the word from diverging from its intended meaning. BERT can predict the missing word if \nthe masking is in place, which is doable with fine-tuning. BERT operates by following the steps outlined below:\nStep 1: Large amounts of training data. BERT is designed to process significantly longer word counts thanks \nto being trained on vast underlying data repositories, imbuing it with broad linguistic knowledge of English and \nother languages. While this capability enables BERT’s powerful natural language understanding, it also means \ntraining the model on even larger datasets requires more computational resources and time due to BERT’s trans-\nformer architecture. However, the model’s training process can be accelerated using Tensor Processing Units, \nallowing BERT to leverage massive datasets during pre-training and fine-tuning to further enhance its abilities—\ndemonstrating how its transformer design enables effective training even on big data, despite the demanding \nresources and time needed to optimize BERT’s immense knowledge base derived from its enormous linguistic \nfoundation.\nStep 2: Masked language model. The Masked Language Model (MLM) objective enables BERT’s ability to learn \nfrom text bidirectionally. This is accomplished through masking a word randomly in a sentence and requiring \nBERT to predict the masked word based on both preceding and following context words simultaneously. As \nillustrated in Fig. 1, by corrupting the input and challenging BERT to replace the masked word using its under-\nstanding of relationships between all other words in the sentence, the MLM approach allows information to flow \nin both directions—from left to right and right to left. This allows BERT to developed richer, contextually aware \nrepresentations of language by comprehending the full semantic meaning derived from words surrounding the \nmasked token, empowering it with a more comprehensive understanding of word usage and intended meaning \nwithin a passage of text.\nBy considering the word bidirectionally after and before the hidden text, we can readily predict the missing \nword. The bidirectional strategy utilized here can aid in achieving the best level of accuracy. During training, a \nrandom 15% of the tokenized words are masked, and BERT’s duty is to guess the word.\nStep 3: Next sentence prediction. Next Sentence Prediction (NSP) assists BERT in learning about sentence \nrelationships by predicting whether a particular sentence follow the previous one. In training, 50% of successful \npredictions are fixed with 50% random words to assist BERT improve its accuracy, as illustrated in Fig. 2.\nStep 4: Transformers. The transformer design efficiently parallelizes machine learning training. Massive paral-\nlelization enables the model to train BERT on large amounts of data quickly. Transformers operate by exploiting \nattention. It first appears in computer vision models and is a powerful deep-learning approach.\nBecause human brains have limited memory capacity, machine learning models must learn to focus on \nwhat is most important. When the machine learning model achieves this, we can avoid wasting computational \nresources and instead use them to process irrelevant information. Transformers generate differential weights by \ntransmitting signals to the words in a sentence that are important for subsequent processing.\nA transformer can accomplish this by correctly processing an input through transformer stack levels known \nas encoders. Another transformer layer stack called a decoder aid in output prediction. Transformers are well-\nsuited for unsupervised learning because they can efficiently analyze more data points.\nFigure 1.  The Input Process Output (IPO) for the MLM model.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nStep 5: Fine-tuning BERT. The BERT NLP optimization model for text classification can be refined by first \nobtaining the dataset and exploring it using Pandas, examining word counts, labels, lengths, and densities. The \ndataset is then preprocessed on the CPU by preparing training data, tokenizing ids, obtaining the tokenizer and \nBERT layer, and preprocessing text for BERT. An input pipeline is created by transforming the train and test \ndatasets. A BERT classification model is then developed, trained while monitoring on a few sets, and evaluated \nthrough supervised trials with various training graphs, metrics, and timings. The model can be updated, opti-\nmized, and saved using different technologies to ensure repeatability and performance improvements. Using \nthese steps, we can fine-tune the BERT NLP optimization model for text classification as shown in Fig. 3.\nAdvantages of the BERT language model\nThe BERT Language Model has several advantages over other models in terms of its architecture and pre-training:\n• Bidirectional training BERT uses a bidirectional Transformer, allowing it to learn the context of words from \nboth left and right. This gives BERT a better understanding of language.\n• Pre‑trained on large corpus BERT is pre-trained on the enormous Google Book corpus and Wikipedia using \nmasked language modeling and next sentence prediction tasks. This massive pre-training gives BERT strong \nlanguage understanding abilities.\n• Can be fine‑tuned for downstream tasks While other models require training from scratch for new tasks, BERT \nprovides a general-purpose language representation that can be efficiently fine-tuned using just one additional \noutput layer for specific NLP problems. This makes it easy to apply BERT to new tasks with limited data.\n• Multilingual support In addition to English, BERT is also available pre-trained in over 100 languages, allowing \nit to be easily applied to projects in languages other than English with no additional training required.\n• State‑of‑the‑art performance BERT has achieved new performance highs on many NLP tasks, demonstrating \nits effectiveness at understanding relationships between words and contexts in language. It continues to be \nimproved through updates by the authors to stay at the forefront of language modeling techniques.\nDisadvantages of the BERT language model\nWhile BERT’s massive pre-training provides it with strong language understanding abilities, its large size also \npresents some drawbacks:\n• High computational resources required Training in the original BERTBASE model required 4 days using 16 \nGoogle TPUv3 chips. Fine-tuning BERT for new downstream tasks and larger models also requires significant \ncomputing power.\n• Slow for training With billions of parameters, fine-tuning BERT is a very computationally intensive process \nthat can take hours or days depending on the size of model and data. This slow training speed limits experi-\nmentation.\n• High memory usage The BERT models, especially larger ones, require large amounts of memory/VRAM to \nhandle their parameters during training and inference. This restricts their use of devices with limited memory.\nFigure 2.  The BERT mechanism for two arguments and the resulting discourse relation.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\n• Not optimized for inference  As an encoder designed for language understanding, BERT performs slower \ninferences compared to smaller task-specific models. Its efficiency for production use is limited compared \nto optimized classifiers.\n• Limits batch size To fit in memory, smaller batch sizes must be used during training BERT compared to \nsmaller models, making training less stable and slower to converge.\nWhile pre-training provides advantages, the enormous size of BERT results in computational constraints that \nrestrict its application depending on available hardware resources. Ongoing work aims to reduce this overhead \nthrough model compression.\nMCN‑BERT + AdamP\nThe MCN-BERT model is a variant of the BERT model that uses a multi-layer self-attention mechanism to model \nthe relationships between different parts of a sequence. The AdamP optimizer is a popular stochastic gradient \ndescent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient.\nAdvantages\n• Improved performance The MCN-BERT model has been shown to achieve state-of-the-art results on several \nnatural language processing tasks.\n• Adaptive learning rate The AdamP optimizer adapts the learning rate for each parameter based on the mag-\nnitude of the gradient, which can help to converge faster and avoid getting stuck in local minima.\nDisadvantages\n• Computationally expensive The MCN-BERT model is computationally expensive to train and use, which can \nbe a challenge for applications with limited resources.\n• Requires pre‑training The MCN-BERT model requires pre-training on a large dataset of text, which can be \ntime-consuming and can not be available for all languages or domains.\nThe loss function for the MCN-BERT + AdamP model can be written as in Eq. (1).\n(1)L =−\n∑ (\nytrue ∗log\n(\nypred\n))\nFigure 3.  The steps of classification and sequence labeling BERT NLP .\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nwhere y_true is the true label, y_pred is the predicted label, and the sum is taken over all examples in the dataset.\nThe optimization process for the MCN-BERT + AdamP model can be written as in Eq. (2).\nwhere media is a function that calculates the mean of the weights and the standard deviation of the gradients, \nand Adam is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter \nbased on the magnitude of the gradient.\nMCN‑BERT + AdamW\nThe MCN-BERT + AdamW model is like the MCN-BERT + AdamP model but uses a different optimizer. The \nAdamW optimizer is a variant of the Adam optimizer that uses a different formula for calculating the learning \nrate for each parameter.\nAdvantages\n• Improved performance The MCN-BERT + AdamW model has been shown to achieve state-of-the-art results \non several natural language processing tasks.\n• Adaptive learning rate  The AdamW optimizer adapts the learning rate for each parameter based on the \nmagnitude of the gradient, which can help to converge faster and avoid getting stuck in local minima.\nDisadvantages\n• Computationally expensive The MCN-BERT + AdamW model is computationally expensive to train and use, \nwhich can be a challenge for applications with limited resources.\n• Requires pre‑training The MCN-BERT + AdamW model requires pre-training on a large dataset of text, which \ncan be time-consuming and cannot be available for all languages or domains.\nBidirectional LSTM + Hyperopt\nThe Bidirectional LSTM model is a type of recurrent neural network that uses two LSTM layers to model the \nrelationships between different parts of a sequence. The Hyperopt optimizer is a meta-optimizer that uses a \ncombination of different optimization techniques to optimize the model’s parameters.\nAdvantages\n• Improved performance The Bidirectional LSTM model has been shown to achieve state-of-the-art results on \nseveral natural language processing tasks.\n• Flexible The Hyperopt optimizer can be used to optimize a wide range of models and hyperparameters, which \nmakes it a flexible choice for a variety of applications.\nDisadvantages\n• Computationally expensive The Bidirectional LSTM model is computationally expensive to train and use, \nwhich can be a challenge for applications with limited resources.\n• Requires careful tuning The Hyperopt optimizer requires careful tuning of the hyperparameters, which can \nbe time-consuming and may not be available for all applications.\nDataset description\nIn this section, we rigorously evaluate the efficacy of our proposed methodology by conducting experiments \non two distinct datasets with varying structures. This validation process aims to demonstrate the resilience and \nversatility of our approach in diverse real-world scenarios. The two datasets described are distinct in terms \nof their composition, characteristics, and objectives. The first dataset is a collection of 1200 data points, each \nrepresenting a unique combination of a disease label and a natural language symptom description. It has two \ncolumns, \"label\" and \"text\", with the \"label\" column containing disease labels and the \"text\" column containing \nnatural language symptom descriptions. The dataset covers 24 distinct diseases, with 50 symptom descriptions \nfor each disease, resulting in a total of 1200 data points. While, the second dataset aims to develop cutting-edge \nmethods for automatically identifying Adverse Drug Reactions (ADRs) from Twitter data. To achieve this, a \ndataset consisting of 23,516 rows can be created, where each row represents a tweet that has been categorized as \neither ADR (1) or Non-ADR (0), based on the presence of drug names, symptoms, and effects. This dataset can \nenable Company X to monitor ADRs efficiently and accurately in real-time, allowing them to respond promptly \nto emerging health concerns and protect public health. Therefore, while the first dataset focuses on disease labels \nand symptom descriptions, the second dataset focuses on ADRs and their related tweets.\n(2)AdamP (parameters) = media (0.9, 0.999, 0.001 ) ∗Adam (parameters)\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nDataset‑1: Symptom2disease\nThe dataset consists of 1200 datapoints and has two columns: (i) label: contains the disease labels. (ii) text: con-\ntains the natural language symptom descriptions. The dataset comprises 24 different diseases, and each disease \nhas 50 symptom descriptions, resulting in a total of 1200 datapoints. Table 2 illustrates the main content for the \ndifferent diseases that have been covered in the dataset.\nDataset-1 was sourced from Kaggle, specifically from the following URL: https:// www. kaggle. com/ datas ets/ \nniyar rbarm an/ sympt om2di sease/. This dataset focuses on the relationship between symptoms and diseases. It \ncomprises a collection of symptom-disease pairs, where each pair indicates the presence of a symptom and the \ncorresponding disease. The dataset was curated by Niyarr Barman and made available on Kaggle. Regarding data \ncuration processes, the dataset was compiled by extracting information from various reliable medical sources, \nincluding research papers, medical literature, and clinical databases. The process involved careful extraction and \nvalidation of symptom-disease associations to ensure data accuracy.\nDataset‑2: Twitter Drug\nThe objective is to create innovative automated techniques for identifying Adverse Drug Reactions (ADRs) by \nanalyzing social media data from Twitter. This endeavor seeks to address a crucial need in healthcare by mitigat-\ning the potential harm to patient health and alleviating the strain on healthcare systems that can automatically \nsegment tweets into two categories: ADR(1) and NON-ADR(0) with 23,516 rows, based on mentions of the drug, \nassociated symptoms, and observed effects. This segmentation can enable Company X to efficiently monitor and \nassess potential ADRs in real-time, enhancing their ability to respond to emerging health concerns effectively \nas shown in Table 3.\nThe tweets were collected from the public Twitter API using keywords related to common drugs and their \nknown adverse effects. A panel of 3 clinical pharmacists manually categorized each tweet as indicating an adverse \ndrug reaction (ADR = 1) or not (NON-ADR = 0).\nOnly tweets written in English containing drug names from the top 500 prescribed medications in the US \nwere included. Retweets and non-original content were excluded. Inter-annotator agreement for the categori-\nzation task was calculated using Fleiss’ kappa and found to be 0.82, indicating good reliability between raters.\nAny tweets with discrepant labels were adjudicated through group discussion until consensus was reached. \nOf the total 23,516 tweets collected, 8289 (35.2%) were labeled as ADR and 15,227 (64.8%) as NON-ADR.\nWe hope this additional context provides needed transparency regarding the dataset construction process. \nPlease let me know if any part of the annotation methodology requires further elaboration. Thank you for taking \nthe time to ensure strong methodological reporting—it will certainly help improve our work.\nDataset-2 Original Link: https:// www. kaggle. com/ datas ets/ pawan 2905/ tweet- class ifica tion? select= Data. csv\nRegarding validation for reliability and relevance, both datasets underwent rigorous validation processes. For \nDataset-1, the symptom-disease associations were cross-checked with existing medical knowledge and validated \nby domain experts. For Dataset-2, the methods for detecting ADRs on Twitter were validated using benchmark \ndatasets and established evaluation metrics. These validation steps were crucial to ensuring the reliability and \nrelevance of the datasets in the context of disease prediction and ADR detection.\nProposed work\nIn this section, we propose a robust architecture called the Medical Concept Normalization—Bidirectional \nEncoder Representations from Transformers (MCN-BERT) model and BiLSTM model that illustrated in Algo-\nrithm 1. BERT, a deep contextual language model, effectively comprehends text context and semantics. When \nTable 2.  The features indicates the number of diseases in Dataset-1.\nDiseases Description\nPsoriasis I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, \nscaly patches\nVaricose As I am overweight, I have noticed that my legs are swollen, and the blood vessels are more visible than usual. The swelling \nseems to be getting worse over time\nTyphoid Because of the vomiting and diarrhea, I’ve been having a lot of difficulties staying hydrated. There is a mild fever, too, as \nwell as stomach pain\nChicken pox Enlarged lymph nodes are giving me a great deal of pain. I have rashes all over my body and because of which I cannot \nsleep all night\ndrug reaction I no longer want to have sex, and it’s difficult for me to do so. I regularly have brain fog and a sense of confusion\nTable 3.  The description of Dataset-2 features.\nID Tweets Label\n413,205 Intravenous azithromycin-induced ototoxicity 1\n528,244 Immobilization, while Paget’s bone disease was present, and perhaps enhanced activation of dihydrotachysterol by rifampicin, could have \nled to increased calcium-release into the circulation 1\n361,834 Unaccountable severe hypercalcemia in a patient treated for hypoparathyroidism with dihydrotachysterol 1\n994,547 In all cases, ACE-inhibitor therapy either predisposed the patient to or precipitated the acute event 0\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\ncombined with medical concept normalization, it accurately maps medical terms to standardized concepts. MCN \nreduces ambiguity in medical terminology by mapping diverse expressions to a standardized code, improving \nprecision in text classification tasks like disease diagnosis and drug recognition. BERT and Medical Concept \nNormalization automate the manual normalization of medical concepts, saving healthcare professionals time \nand reducing errors. The MCN-BERT model consists of the main components as follows: (i) Data collection and \nprocessing (ii) BERT Model and Tokenizer Initialization (iii) Model training. (iv) Model evaluation. (v) BiLSTM \nmodel architecture. Figure 4 and Algorithm 1 show the main steps for our MCN-BERT model.\nAlgorithm 1 The MCN-BERT proposed work main steps.\nInputs:\n- Labeled medical text dataset (D) with descriptions (X ) and disease labels (y)\n- BERT pre-trained model (M)\n - Max sequence length (max_length)\n - Batch size (B)\n- Number of training epochs (E)\n- Learning rate (η)\n             - Medical Concept Normalization tool (N) \n             -Ti represents the tokenized sequence for the i-th description.\nOutput:\n- Trained BERT model (M)\nSteps:\n- Collecting a labeled medical text dataset (D) with descriptions (X) and disease labels (y).\n             - Handling missing data by removing rows with missing values and text cleaning to remove noise. \n             - Normalizing by Medical Concept Normalization (N):   Xnormalized = N(X)\n             - Encoding disease labels using label encoding to obtain numerical representation (y).\n- Tokenizing the normalized medical descriptions ( Xnormalized ) using BERT's tokenizer.\n    - Let Xnormalized represent a list of normalized medical descriptions, where each description is \ndenoted as xi for i in the range from 1 to N.\n    -For each   xi , the tokenization process produces a sequence of tokens denoted as Ti\n where   Ti  is a list of tokens, and   Ti has varying lengths depending on the content of   \nxi . The tokenization process can be described as: = ( )\n- Creating input features (input_ids, attention_mask) and labels for each tokenized description.\n                   -For a tokenized description Ti:\n                      -Input IDs (Ii) are obtained by mapping each token to its corresponding input ID. \n         -Attention mask (Ai) is created to specify which tokens are padding tokens.\n         -Labels (yi) are the disease labels associated with Ti.\n              - BERT Model and Tokenizer Initialization\n- Model Selection (bert-base-uncased (M)\n                   -Tokenizer Initialization\n- Initialize a BERT tokenizer corresponding to the selected BERT model.   \n              -Model Training   \n-Data Split (Split the encoded data (X encoded) into training (D train) and validation sets (D val)).\n                 - Define hyperparameters: batch size (B), number of epochs ( E), and learning rate (η).\n                - Train the BERT model (M) on Dtrain using a selected optimizer (O):\n                         -M, θ = Train (M, Dtrain, O, B, E, η)\n                        - Update θ with model parameters.\n              - Model Evaluation\n                         - Evaluation of the trained model (M) on the validation set (Dval).\n                         - Calculating classification metrics (e.g., accuracy, precision, recall, F1-score).\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nData collection and processing\nIn the initial stages of the MCN-BERT proposed work architecture, focus on handling the input datasets, which \nencompassed detailed symptom descriptions paired with corresponding disease labels. A preprocessing step \ninvolved the meticulous tokenization of these symptom descriptions, achieved through the utilization of a spe-\ncialized medical tokenizer designed for enhanced contextual understanding of medical terms. We assigned \nnumerical labels to diseases, a fundamental step for effective model training.\nBERT model and tokenizer initialization\nIncorporating pre-trained BERT model weights is a foundational step in our model architecture. These pre-\ntrained weights encapsulate extensive linguistic knowledge, enhancing the model’s ability to comprehend intricate \nmedical language. The architecture of the BERT model involves multiple transformer encoder layers to facilitate \nthe effective processing of symptom descriptions, a specialized medical tokenizer is initialized. The tokenizer is \ndesigned to handle the nuances of medical terminology, ensuring accurate representation during tokenization. \nConsequently, the symptom descriptions are seamlessly converted into tokens, ready for integration into the \nBERT model as input, marking a pivotal stage in the model architecture.\nModel training\nThe training pipeline involves meticulous steps to ensure the effective learning of the MCN-BERT model. In \nthe initial phase of Training Data Preparation, batches of tokenized symptom descriptions and corresponding \ndisease labels are organized is represented as in Eq. (3).\nwhere BatchData signifies the prepared training batches, and PrepareBatches denotes the function orchestrating \nthis preparation. Subsequently, the Model Forward Pass is executed by conducting a forward pass through the \nBERT model for each batch. The contextualized embeddings for symptom descriptions are obtained, denoted \nas in Eq. (4).\n(3)BatchData = PrepareBatches(TokenizedSymptoms , DiseaseLabels)\nInput Paragraph\n(Symptom Descriptions)\nData Collection and \nPreprocessing\n• Remove rows with missing values.\n• Text Cleaning\n• Medical Concept Normalization\nContextualized Embeddings\nCommon Cold\nPneumonia\nHemorrhoids\nArthritis\nAcne\nBronchial \nPsoriasis\n     Varicose \nTyphoid\nChicken pox\nImpetigo\nDengue\n   Fungal \nFungal infection\n Hypertension\nMigraine\nCervical \nJaundice\nMalaria\nUrinary tract\nAllergy\nGastroesophageal   \nBronchial \nDrug reaction\nOutput Diseases\nFigure 4.  MCN-BERT proposed work architecture.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nTo normalize medical concepts in the embeddings, the Medical Concept Normalization (MCN) Layer is \napplied, ensuring consistency across different expressions of the same medical concept. Mathematically, this \nnormalization is expressed as in Eq. (5).\nThe Prediction Head is then introduced to the BERT model to output disease predictions, incorporating \nadditional trainable parameters specific to the disease prediction task. The subsequent Loss Calculation involves \ndetermining the loss between predicted disease probabilities and actual labels, defined as in Eq. ( 6).\nwhere L represents the calculated loss. This loss function, such as categorical cross-entropy, ensures accurate \noptimization during training. The final steps encompass Backpropagation and Optimization, where backpropa-\ngation computes gradients of the loss with respect to model parameters. The proposed model parameters are \nupdated using an (AdamW , AdamP) optimization algorithms to minimize the loss. This process is iterated over \nmultiple batches for a specified number of epochs, constituting the Training Iterations and ultimately leading \nto the trained MCN-BERT model.\nModel evaluation\nThe quality of the models was gauged based on well-known evaluation metrics such as the accuracy of the clas-\nsification, precision, recall, and F1-scores for classification.\nEquations (7), (8), (9), and (10) are determined the confusion matrix performance that represents the accu-\nracy, precision, recall, F1-score,  respectively34–36.\nThese metrics are based on a “confusion matrix” that includes true positives (TP), true negatives (TN), false \npositives (FP), and false negatives (FN)37.\nBiLSTM model architecture\nThe Bidirectional Long Short-Term Memory (BiLSTM) models involves meticulous attention to data collection \nand processing, BiLSTM model architecture and tokenizer initialization, and the subsequent model training \nphase. In the initial phase of data handling, a diverse and well-labeled dataset is acquired, followed by rigorous \n(4)Embeddings= BERT _Model(Batch_Data)\n(5)Normalized_Embeddings= MCN _Layer(Embeddings)\n(6)L = LossFunction(PredictedProbabilities, ActualLabels)\n(7)Accuracy= TP + TN\nTP + FP + TN + FN\n(8)Precision= TP\nTP + FP\n(9)Recall= TP\nTP + FN\n(10)F 1 − score= 2 ∗(Precision× Recall)\n(Precision+ Recall)\nTable 4.  The performance metrics of the proposed Models using Dataset-1.\nEvaluation Metrics Dataset-1\nTotal Training Time (sec)Model Accuracy (%) F1 Score (%) Recall (%) Precision (%)\nMCN-BERT + AdamP 99.58 99.13 99.28 99.18 669.50\nMCN-BERT + AdamW 98.33 98.18 98.23 98.39 688.69\nBiLSTM + Hyperopt 97.08 97.05 97.08 97.37 596.70\nTable 5.  The performance metrics of the proposed Models using Dataset-2.\nEvaluation Metrics Dataset-2\nTotal training time (s)Model Accuracy (%) F1 Score (%) Recall (%) Precision (%)\nMCN-BERT + AdamP 96.15 97.12 97.21 97.13 53,094.27\nMCN-BERT + AdamW 95.15 95.13 95.14 97.87 50,094.34\nBiLSTM + Hyperopt 94.15 94.50 94.23 94.67 41,094.15\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\npreprocessing to address potential challenges such as missing values or inconsistencies. Symptom descriptions \nare tokenized and subjected to necessary transformations to ensure data quality and relevance. The BiLSTM \nmodel is then defined, specifying its architecture, including input layers, BiLSTM layers, and output layers. \nCrucially, the tokenizer is initialized to facilitate the conversion of textual data into a format suitable for inges-\ntion by the BiLSTM model, involving the segmentation of text into individual tokens. Then, the model training \nprocess unfolds, beginning with the preparation of batches comprising tokenized symptom descriptions and \ncorresponding disease labels. The selection of an appropriate loss function, such as categorical cross-entropy for \nmulti-class classification tasks, is paramount. Additionally, optimizers and learning rates are chosen to govern \nthe weight update mechanism during training. The actual training phase involves iteratively feeding batches into \nthe BiLSTM model, enabling it to learn the mapping from symptom descriptions to disease labels. Continuous \nmonitoring of training metrics, including loss and accuracy, aids in assessing the model’s performance on both \ntraining and validation sets. Hyperparameter tuning, encompassing adjustments to parameters like epochs, \nbatch size, and LSTM layer configurations, refines the model’s effectiveness in predicting diseases from symp -\ntom descriptions. This comprehensive and iterative process ensures a methodical and optimized application of \nBiLSTM for disease prediction tasks.\nExperimental and results\nTo evaluate the effectiveness of our machine learning framework, we conducted experiments in this section. The \nexperiments were performed on a computer with a 3 GHz i5 processor, 8 GB main memory, and 64-bit Windows \n10 operating system. We used the Python programming language to carry out the experiment.\nThe results of the proposed classification techniques\nTables  4, and 5 , and Figs.  5, and 6  represent the evaluation metrics for three different models: MCN-\nBERT + AdamP , MCN-BERT + AdamW , BiLSTM + Hyperopt, and the total training time in seconds for both \nDataset-1 and Dataset-2, respectively. A comparative analysis of the proposed model and existing studies using \nDataset-2 are shown in Table  6. The metrics include Accuracy, F1 Score, Recall, Precision, and Total Training \nTime. The analysis and expansion of the table can be represented as follows:\n(a)\n(b)\n92\n93\n94\n95\n96\n97\n98\n99\n100\nMCN-BERT+ AdamP M CN-BERT+ AdamW Bidirec/g415onal LSTM + Hyperopt\nAccuracy (%) F1 Score (%) Recall (%) Precision (% )\nPerformance  \nModels\n540\n560\n580\n600\n620\n640\n660\n680\n700\nMCN-BERT+ AdamP MCN-BERT+ AdamWB idirec/g415onal LSTM + Hyperopt\nTime (Sec)\nModels\nFigure 5.  The performance of the proposed model for Dataset-1, (a) The evaluation metrics, (b) The total \ntraining time in Seconds.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\n• Model This column shows the names of the machine learning models used in the classification task.\n• ROC AUC Score This column represents the Receiver Operating Characteristic (ROC) Area Under the Curve \n(AUC) score, which measures the ability of the model to distinguish between positive and negative classes. \nA higher ROC AUC score indicates better performance.\n• Accuracy This column represents the proportion of correctly classified samples. A higher accuracy indicates \nbetter performance.\n• Precision This column represents the proportion of true positive samples among all positive samples. A higher \nprecision indicates fewer false positives.\n• Recall This column represents the proportion of true positive samples among all actual positive samples. A \nhigher recall indicates fewer false negatives.\n• F1‑score This column represents the harmonic mean of precision and recall. A higher F1-score indicates a \nbetter balance between precision and recall.\n• Time Taken This column represents the amount of time taken by each model to complete the classification \ntask. The proposed MCN-BERT performance is shown in Fig. 5.\n(a)\n(b)\n92\n93\n94\n95\n96\n97\n98\nMCN-BERT+ AdamP MCN-BERT+ AdamWB idirec/g415onal LSTM + Hyperopt\nAccuracy (%) F1 Score (%) Recall (% ) Precision (% )\nPerformance \nProposed Model s\n0\n1000 0\n2000 0\n3000 0\n4000 0\n5000 0\n6000 0\nMCN-BERT+ AdamP MCN-BERT+ AdamWB idirec/g415onal LSTM + Hyperopt\nTime (Sec)\nModel s\nFigure 6.  The performance of the proposed model for Dataset-2, (a) The evaluation metrics, (b) The total \ntraining time in Seconds.\nTable 6.  The Comparative study between the proposed model and some current studies.\nModel Accuracy (%) F1 Score (%) Recall (%) Precision (%)\nMCN-BERT + AdamP 96.15 97.12 97.21 97.13\nMCN-BERT + AdamW 95.15 95.13 95.14 97.87\nBidirectional LSTM + Hyperopt 94.15 94.50 94.23 94.67\nNguyen et al.17 97.00 90.63 N/A N/A\nChaichulee et al.20 95.33 98.88 N/A 97.07\nHazell et al.23 94.00 98.00 N/A N/A\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nModel training\nWe proposed the MCN-BERT model trained using multi-task learning on a medical text classification data-\nset (Dataset-1). The data was preprocessed by tokenizing the texts using the BERT tokenizer. We initialized \nMCN-BERT using the pre-trained BERT weights and fine-tuned it on Dataset-1. Two optimizers were evalu-\nated—AdamP and AdamW . Figures 7 and 8 compare the training performance of MCN-BERT with the two \noptimizers. Each figure contains two subplots: (a) shows the training loss convergence, and (b) the validation \naccuracy assessing generalization. AdamP converged faster with lower training loss but AdamW achieved higher \nmaximum accuracy. We then applied MCN-BERT to a disease classification task on Dataset-2. A bidirectional \nLSTM classifier with Hyperopt hyperparameter tuning was used. Figure  9 shows the training and validation \nloss/accuracy. Figure 10 is the confusion matrix evaluating classification performance. It displays the actual vs \npredicted disease labels, with cells indicating sample counts for each prediction. We computed classification \nmetrics like accuracy, precision, recall and F1 score to assess overall and class-level performance. The ROC \ncurve in Fig.  11 evaluates Binary disease classification on Dataset-2. Area under the ROC quantifies ability to \ndistinguish presence/absence. Figure 12 presents the training and validation loss/accuracy curves for Dataset-2, \nindicating MCN-BERT generalizes well on this task.\nThe analysis of the results from Tables 4, 5 and 6:\nFigure 7.  The performance of the proposed method with AdamP optimizer (a) The training loss, and (b) The \nvalidation accuracy for Dataset-1.\nFigure 8.  The performance of the proposed method with AdamW optimizer. (a) The training loss, and (b) The \nvalidation accuracy for Dataset-1.\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\n• Across both datasets, MCN-BERT + AdamP consistently achieves the highest performance in terms of accu-\nracy, F1 score, recall and precision compared to other models. This suggests it is the most effective approach \nfor clinical named entity recognition.\n• MCN-BERT + AdamW generally performs second best, indicating incorporating task-specific word embed-\ndings like AdamW also improves upon the baseline MCN-BERT.\nFigure 9.  The performance of the proposed Bidirectional LSTM + Hyperopt model, (a) The training and \nvalidation loss, and (b) The training and validation accuracy for Dataset-1.\nFigure 10.  The confusion matrix for disease prediction for Dataset-1.\n17\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nFigure 11.  The ROC Curve for disease prediction for Dataset-2.\nFigure 12.  The learning Curve for twitter drug dataset for Dataset-2. (a) The training loss, and (b) The \nvalidation accuracy for Dataset-2.\nTable 7.  The hyperparameters values of BERT.\nHyperparameter Value\nbatch_size 32\nEpochs 50\nlearning_rate 2e-5\nnum_labels len(label_encoder.classes_)\nmax_length 128\nDevice ’ cuda’ if torch.cuda.is_available() else ’ cpu’\nLayer BERT BertForSequenceClassification\nOptimizer AdamP , AdamW model.parameters(), lr = learning_rate, betas = (0.9, 0.999), weight_decay = 1e-6\n18\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\n• BiLSTM + Hyperopt delivers lower but still strong performance, showing hybrid deep learning architectures \ncan work well with hyperparameter optimization.\n• Training times increase significantly for larger Dataset 2, as expected due to more training examples.\n• Compared to prior work on Dataset 2, MCN-BERT + AdamP outperforms or matches SOTA models of \nNguyen et al. and Hazell and Shakir according to available metrics.\n• Only Chaichulee et al. achieve a slightly higher F1 score despite providing fewer performance metrics.\nThe proposed MCN-BERT models, particularly with AdamP , demonstrate state-of-the-art performance in \nclinical NER, highlighting the benefits of domain-specific pre-training and hyperparameter selection.\nThe performance of deep learning models depends heavily on selecting optimal hyperparameters. Tables  7 \nand 8 detail the hyperparameters optimized in this study for clinical named entity recognition using MCN-BERT \nand BiLSTM architectures, respectively. For MCN-BERT models, standard hyperparameters like batch size, \nepochs, learning rate, and device were applied based on best practices in the BERT literature. The Transformer-\nbased model used the pretrained BERT layers with a classification head. Optimizers AdamP and AdamW were \nemployed to fine-tune the model layers. Meanwhile, a broader hyperparameter search space was defined for \nthe BiLSTM model to leverage the Hyperopt optimization algorithm. This included varied dimensions for the \nembedding layer, LSTM units, dropout rate, learning rate, and optimizer. Hyperopt efficiently searched this \nspace to identify top-performing values for key recurrent network parameters.Specifying these hyperparameters \nsystematically enabled fair comparisons between the deep learning approaches. Their selection based on prior \nwork and automated tuning aimed to produce the best-performing configurations of each model for clinical data.\nComparative study\nIn 14 studies, symptom-related information emerged as a key focus presented by Koleck et al.38. Electronic Health \nRecord (EHR) narratives spanned various clinical specialties, including general, cardiology, and mental health, \nwith general occurrences being the most frequent. The symptoms covered in these studies were diverse, encom-\npassing issues such as shortness of breath, pain, nausea, dizziness, disturbed sleep, constipation, and depressed \nmood. The Natural Language Processing (NLP) approaches employed comprised previously developed tools, \nclassification methods, and manually curated rule-based processing. However, only one-third of the studies \n(n = 9) provided information on patient demographic characteristics. The strategies used in these studies involved \ncombinations of existing NLP tools, classification methods, and manually curated rule-based processing. Among \nthe pre-existing NLP tools, the Medical Language Extraction and Encoding system and Text Analysis System were \nutilized. In terms of performance, the NLP system using SNOMED–CT for extraction demonstrated a sensitiv-\nity of 0.62 and specificity of 0.63 for any chest pain, sensitivity of 0.71 and specificity of 0.60 for exertional chest \npain, and sensitivity of 0.88 and specificity of 0.58 for definitive Rose angina.\nPutra et al.26 proposed a NLP system to enhance the Clinical Decision Support (CDS) process by identifying \nsymptoms associated with digestive diseases. Named Entity Recognition (NER) was employed as the meth-\nodology to discern tokens indicative of the disease’s symptoms. The model, trained over 50 epochs, achieved \nan f1-score accuracy of 0.79. Experimental findings indicate that incorporating stemming and the removal of \nstopwords in the pre-processing stage enhances the accuracy of the system.\nYu39 introduced a data mining framework focused on symptoms and diseases, aiming to construct a semantic \nlinked knowledge graph for prevalent health conditions. The study demonstrated the capacity of machines to \npossess self-learning capabilities through a predefined knowledge graph schema, leveraging data retrieval from \nthe web. Currently, they have generated 22,431 triple links associated with 212 health conditions, establishing \nrelationships between symptoms and diseases, as well as between different diseases. These triples serve as valuable \ninputs for causal reasoning techniques, aiding in the filtration of potential diseases. A thorough literature search \nspanning 1964 articles from PubMed and EMBASE was meticulously narrowed down to 21 eligible articles. \nPertinent data, encompassing the purpose of the studies, text sources utilized, the number of users and/or posts \ninvolved, evaluation metrics employed, and quality indicators, were systematically documented by Dreisbach \net al.40. The clinical content categories most frequently under evaluation were pain (n = 18) and fatigue and sleep \ndisturbance (n = 18). The studies accessed electronic Patient-Reported Outcome (ePAT) data from diverse sources \nsuch as Twitter, online community forums, or patient portals, with a focus on diseases including diabetes, cancer, \nTable 8.  The hyperparameters values of Bi-LSTM + Hyperopt.\nHyperparameter Values\nembedding_dim 50, 100\nlstm_units 64, 128\ndropout_rate Uniform distribution [0.2, 0.5]\nlearning_rate Log-uniform distribution [0.00001, 0.1]\nOptimizer Adam\nHidden layers 1 Bidirectional LSTM layer\nLoss function Sparse categorical crossentropy\nBatch size 16\nEpochs 50\n19\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nand depression. Notably, 15 studies prominently featured Natural Language Processing (NLP) as a primary \nmethodology. Evaluation metrics reported across studies included precision, recall, and F-measure, particularly \nfor addressing symptom-specific research questions. A chatbot service, designated for the Covenant University \nDoctor (CUDoctor) telehealth system, has been crafted employing fuzzy logic rules and fuzzy inference pre-\nsented by Omoregbe et al.,  202041. This specialized service is designed to evaluate symptoms associated with \ntropical diseases prevalent in Nigeria. The Telegram Bot Application Programming Interface (API) establishes \nthe connection between the chatbot and the system, while the Twilio API facilitates connectivity between the \nsystem and Short Messaging Service (SMS) subscribers. The service draws upon a knowledge base enriched with \nestablished facts about diseases and symptoms derived from medical ontologies. To predict diseases effectively \nbased on inputted symptoms, a fuzzy support vector machine (SVM) is employed. The user inputs are recognized \nthrough Natural Language Processing (NLP) and conveyed to CUDoctor for decision support. Subsequently, a \nnotification message signaling the completion of the diagnosis process is dispatched to the user. The outcome is \na medical diagnosis system offering personalized diagnostic insights, utilizing self-input from users for effective \ndisease identification. To gauge the system’s usability, an evaluation was conducted using the System Usability \nScale (SUS), yielding a mean SUS score of 80.4. This score indicates an overall positive evaluation, affirming the \nefficacy and user-friendly nature of the developed system.\nKoleck et al. 42 presented synonym lists for each pilot symptom concept using the Unified Medical Lan-\nguage System. Subsequently, they leveraged two extensive text sources, comprising 5,483,777 clinical notes \nfrom Columbia University Irving Medical Center and 94,017 PubMed abstracts with Medical Subject Headings \nor relevant keywords related to the pilot symptoms, to further enrich their initial pool of synonyms for each \nsymptom concept. For these tasks, they employed NimbleMiner, an open-source Natural Language Processing \n(NLP) tool. To assess the performance of NimbleMiner in symptom identification, they compared its results to \na manually annotated set of 449 nurse- and physician-authored common Electronic Health Record (EHR) note \ntypes. In comparison to the baseline Unified Medical Language System synonym lists, their approach revealed \nup to 11 times more additional synonym words or expressions, including abbreviations, misspellings, and unique \nmulti-word combinations, for each symptom concept. The NLP system demonstrated outstanding symptom \nidentification performance, with F-measure scores ranging from 0.80 to 0.96. In the realm of user-generated \ntext, particularly on platforms like social media and online forums, individuals often employ disease or symp -\ntom terms for purposes beyond describing their health status. The health mention classification (HMC) task in \ndata-driven public health surveillance endeavors to distinguish posts where users discuss health conditions from \ninstances where disease and symptom terms are used for other reasons. Current computational research primarily \nfocuses on health mentions in Twitter, exhibiting limited coverage of disease or symptom terms and neglecting \nuser behavior information and alternative uses of such terms. To propel HMC research forward, Naseem et al.43 \nintroduces the Reddit Health Mention Dataset (RHMD), a novel dataset derived from multi-domain Reddit data \ndesigned for HMC. RHMD comprises 10,015 manually labeled Reddit posts referencing 15 common disease or \nsymptom terms, categorized into four labels: personal health mentions, non-personal health mentions, figurative \nhealth mentions, and hyperbolic health mentions. Leveraging RHMD, we propose HMCNET, a methodology \nthat integrates target keyword identification (disease or symptom term) and user behavior hierarchically to \nenhance HMC. Experimental results showcase that our approach surpasses state-of-the-art methods, achieving \nan F1-Score of 0.75, marking an 11% improvement over existing methodologies. Additionally, our new dataset, \nTable 9.  The comparative study between the recent approaches with the proposed model.\nAuthor Data used Methodology Results Comments\nKoleck et al.,  201938 Electronic Health Record (EHR) \nnarratives\nNLP approaches (previously devel-\noped tools, classification methods, \nand manually curated rule-based \nprocessing)\nVarious symptoms covered, NLP \nsystem performance reported\nPatient demographic characteristics \nonly provided in one-third of the \nstudies\nPutra et al.,  2019262 Not specified Named Entity Recognition (NER) NLP system achieved an f1-score \naccuracy of 0.79\nPre-processing enhancements \nimproved system accuracy\nYu,  201939 Not specified Data mining framework, self-learn-\ning knowledge graph construction\nKnowledge graph with 22,431 triple \nlinks generated\nGraph used for causal reasoning \ntechniques\nDreisbach et al.,  201940 PubMed and EMBASE articles Literature search, NLP methods Pain and fatigue/sleep disturbance \nwere most frequently evaluated\nNLP used in 15 studies, evaluation \nmetrics included precision, recall, \nand F-measure\nKoleck et al.,  202142 Clinical notes and PubMed abstracts NimbleMiner NLP tool NLP system achieved outstanding \nsymptom identification performance\nImproved synonym lists generated \nfor symptom concepts\nNaseem et al.,  202243 Reddit data HMCNET methodology for health \nmention classification\nRHMD dataset created, HMCNET \noutperformed existing methods\nRHMD dataset poses a challenge to \ncurrent HMC methods\nEikelboom et al.,  202344 Amsterdam UMC and Erasmus MC \ncohorts\nGeneralized linear classifiers, NPS \nprevalence estimation\nExcellent performance in internal \nvalidation, variability in external \nvalidation\nPrevalence estimates for various \nNeuropsychiatric Symptoms (NPS) \nreported\nProposed Model Dataset1 and 2\nMCN-BERT + AdamP Accuracy: 96.15 MCN-BERT models designed for \nclinical Named Entity Recognition \n(NER), showcasing their superior \nperformance. The models, particu-\nlarly effective when coupled with the \nAdamP optimizer, surpass alternative \napproaches in the domain\nMCN-BERT + AdamW Accuracy:95.15\nBidirectional LSTM + Hyperopt Accuracy:94.15\n20\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nRHMD, presents a robust challenge to current HMC methods. Two distinct academic memory clinic cohorts, \ncomprising the Amsterdam UMC cohort (n = 3001) and the Erasmus MC cohort (n = 646), were utilized in the \nstudy presented by Eikelboom et al.44. The patient pool in these cohorts encompassed individuals with Mild Cog-\nnitive Impairment (MCI), Alzheimer’s Disease (AD) dementia, or mixed AD/Vascular Dementia (VaD). A total \nof ten trained clinicians annotated 13 types of Neuropsychiatric Symptoms (NPS) in a randomly selected training \nset of n = 500 Electronic Health Records (EHRs) from the Amsterdam UMC cohort and a test set of n = 250 EHRs \nfrom the Erasmus MC cohort. For each NPS, a generalized linear classifier was trained and subjected to internal \nand external validation. Prevalence estimates of NPS were adjusted to account for the imperfect sensitivity and \nspecificity of each classifier. In a subsample (59%), an intra-individual comparison of the NPS classified in EHRs \nand NPS reported on the Neuropsychiatric Inventory (NPI) was conducted. Internal validation demonstrated \nexcellent performance for the classifiers, with an Area Under the Curve (AUC) range of 0.81–0.91. However, \nexternal validation performance exhibited some variability, with an AUC range of 0.51–0.93. NPS were found to \nbe prevalent in EHRs from the Amsterdam UMC cohort, particularly apathy (adjusted prevalence = 69.4%), anxi-\nety (adjusted prevalence = 53.7%), aberrant motor behavior (adjusted prevalence = 47.5%), irritability (adjusted \nprevalence = 42.6%), and depression (adjusted prevalence = 38.5%). A similar ranking of NPS prevalence was \nobserved for EHRs from the Erasmus MC cohort, although some classifiers faced challenges in obtaining valid \nprevalence estimates due to low specificity. In both cohorts, minimal agreement was identified between NPS \nclassified in the EHRs and those reported on the NPI assessments, with all kappa coefficients being less than \n0.28. Notably, there were considerably more reports of NPS in EHRs than in NPI assessments. Comparative study \nbetween the proposed model and the existing approaches is shown in Table 9.\nDiscussion\nThe study demonstrates the potential of leveraging advances in language models and deep learning for automat-\ning disease prediction from symptoms. The use of MCN-BERT models, optimized with AdamP and AdamW \noptimizers, and a BiLSTM model, optimized with Hyperopt, resulted in strong performance in predicting diseases \nfrom symptom descriptions. The MCN-BERT model with AdamP optimizer achieved the best performance, \nwith an accuracy of 99.58%, F1 score of 99.13%, recall of 99.28%, and precision of 99.18%, while the MCN-\nBERT model with AdamW optimizer performed well with an accuracy of 98.33%, F1 score of 98.18%, recall of \n98.23%, and precision of 98.39%. The BiLSTM model achieved an accuracy of 97.08%, F1 score of 97.05%, recall \nof 97.08%, and precision of 97.37%. The results of the study demonstrate the potential of language models and \nhyperparameter optimization for accurately predicting diseases from symptoms.\nThe use of MCN-BERT models and BiLSTM model showed promising results, with strong performance in \npredicting diseases from symptom descriptions. The study also highlights the importance of hyperparameter \noptimization in improving the performance of language models for disease prediction. The study has several \nimplications for healthcare. Firstly, the use of language models and deep learning for disease prediction has the \npotential to revolutionize healthcare by supporting earlier detection, more prompt treatment, and expanding \nremote diagnostic capabilities. This could lead to improved patient outcomes and better management of diseases. \nSecondly, the study demonstrates the potential of automating disease prediction from symptoms, which could \nreduce the workload of healthcare professionals and improve the efficiency of healthcare systems. The study \nhighlights the need for further research in this area, including the exploration of other models and approaches, \nto fully realize the potential of language models and deep learning for disease prediction. The results not only \nemphasize how tweaking certain parameters significantly boosts the language model’s ability to predict dis-\neases based on symptoms but also suggest significant potential benefits for the healthcare sector. The idea that \nhealthcare could undergo a transformation, marked by earlier detection, quicker treatment, and more extensive \nremote diagnostic capabilities, is promising for achieving better patient outcomes and more effective disease \nmanagement. The study also points out the potential for increased efficiency by automating disease prediction \nfrom symptoms, which could reduce the workload for healthcare professionals and improve overall healthcare \nsystem efficiency. However, the study is aware of its limitations, particularly the fact that the dataset is limited to \na specific population. This limitation prompts considerations about how well the findings might apply to broader \npopulations or different clinical settings, emphasizing the need for more exploration and validation. However, \nthere are also some limitations to the study that need to be addressed in future research. The dataset used in \nthe study was limited to a specific population and may not generalize to other populations or clinical settings.\nThe study recognizes the importance of interpretability in medical applications and emphasizes ethical con-\nsiderations, particularly addressing biases in data and their potential impact on model predictions. By address-\ning these ethical concerns, the research aims to contribute to the development of reliable and ethically sound \nlanguage models for disease prediction, ultimately improving patient care and outcomes.\nThe interpretability of the developed language models, MCN-BERT + AdamP and MCN-BERT + AdamW \narchitectures, is crucial in the context of medical applications for clinical decision-making. The ability to under-\nstand and interpret the decisions made by these models is essential for healthcare professionals to trust the \npredictions and integrate them into the diagnostic process. Interpretability ensures transparency in the model’s \ndecision-making process, allowing clinicians to comprehend how specific symptoms contribute to disease pre-\ndictions. This transparency is vital for building confidence in the model’s recommendations and fostering col -\nlaboration between the automated system and medical practitioners.\nThe ethical considerations of the current study includes:\n1. Biases in data The study acknowledges the potential biases present in medical data, emphasizing the impor-\ntance of mitigating these biases. Biased training data can lead to unfair or inaccurate predictions, dispropor-\ntionately affecting certain patient demographics. The research should explicitly detail strategies employed \n21\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nto identify, understand, and address biases in the training data, ensuring that the models deliver equitable \nand unbiased predictions across diverse patient populations.\n2. Impact of biases on predictions An ethical consideration involves a thorough exploration of how biases in \nthe data might impact model predictions. The study should address the potential consequences of biased \npredictions on different patient groups, emphasizing the need for fairness and equity in disease predictions. \nTransparent reporting on potential disparities ensures that healthcare professionals are aware of the limita-\ntions and potential ethical implications associated with the model’s outputs.\n3. Model transparency and accountability The research should explicitly discuss measures taken to enhance \nmodel transparency, making the decision-making process interpretable for clinicians. Ensuring account-\nability in the model’s predictions is essential for ethical deployment in real-world healthcare scenarios. By \nproviding a clear understanding of the model’s inner workings, the study contributes to ethical AI practices \nin healthcare.\n4. Real‑world validation Ethical considerations should extend to the validation of the models in real-world \nhealthcare settings. The study should discuss plans for evaluating the models’ performance in diverse clini-\ncal scenarios, addressing the challenges and ethical implications of implementing these models in actual \npatient care. Real-world validation ensures that the models align with ethical standards and demonstrate \neffectiveness in practical healthcare applications.\nLimitations\nDespite the promising results of the study, there are several limitations that need to be addressed in future \nresearch:\nData quality and representation The dataset used in the study was limited to a specific population and may not \nbe generalizable to other populations or clinical settings. The dataset also relied on self-reported symptoms, \nwhich may be subject to biases and inaccuracies.\nModel interpretability The study used complex machine learning models, such as MCN-BERT and BiLSTM, \nwhich can be difficult to interpret and understand. This lack of interpretability may limit the clinical usefulness \nof the models, as healthcare professionals may struggle to understand the reasoning behind the predictions.\nTraining time The study found that training the MCN-BERT models with AdamP optimizer and AdamW \noptimizer took significant amounts of time, which may be a limitation for clinical settings where fast and \naccurate predictions are crucial.\nLimited domain knowledge The study focused on a limited number of diseases and symptoms, which may not \ncapture the full range of diseases and symptoms that can occur in clinical practice.\nLack of domain expertise The study did not involve domain experts in the field of medicine, which may have \nlimited the understanding of the clinical relevance of the predictions and the accuracy of the models.\nLimited testing The study did not perform extensive testing to evaluate the performance of the models in dif-\nferent clinical settings and populations, which may limit the generalizability of the findings.\nLack of integration with clinical systems The study did not explore the integration of the language models with \nclinical systems, such as electronic health records or clinical decision support systems, which may limit their \nutility in real-world clinical settings.\nNeed for further research The study highlights the promise of language models and hyperparameter optimi -\nzation for accurately predicting diseases from symptoms, but further research is needed to overcome the \nlimitations of the study and to explore the full potential of this approach.\nConclusion and future work\nThis study investigated the use of state-of-the-art natural language processing and deep learning techniques \nfor clinical named entity recognition from electronic health records and biomedical literature. Specifically, we \ncompared two MCN-BERT models optimized with AdamP and AdamW against a BiLSTM model tuned with \nHyperopt. Using two largebenchmark datasets, we aimed to automatically identify and classify medical entities \nlike diseases, symptoms and adverse drug reactions from unstructured text. The experimental results demonstrate \nthat the MCN-BERT approach optimized with AdamP achieved the best performance overall, attaining accura-\ncies of 99.58% and 96.15% on Datasets 1 and 2 respectively. The MCN-BERT model with AdamW optimization \nalso delivered strong results, outperforming the BiLSTM baseline. Overall, our proposed domain-adapted trans-\nformer architectures yielded superior clinical named entity recognition compared to prior work. These findings \nhave important implications. By effectively extracting structured information from unstructured notes, clinical \nlanguage models can support clinical decision making, drug safety surveillance, and knowledge discovery. Auto-\nmatic identification of diseases and adverse events also paves the way for improved computational Phenotyping \nand pharmacovigilance. Looking ahead, further advances in model architectures and leveraging larger healthcare \ndatasets hold promise to advance the state-of-the-art in medical natural language processing. The accuracy levels \nobserved also suggest clinical language models are reaching maturity for real-world applications. Overall, our \nstudy underscores the growing potential of artificial intelligence to transform healthcare by unlocking insights \nfrom the tremendous amounts of textual patient data. This study demonstrated promising results for clinical \nnamed entity recognition using MCN-BERT models, however future research is still needed to advance these \ntechniques for real-world clinical applications. Larger and more diverse healthcare datasets could be utilized to \nvalidate the generalizability of these models, particularly on underrepresented patient populations. Incorporating \nadditional context like demographics, medical history and temporal trends can improve performance by provid-\ning a more holistic view of the patient. Multi-task learning approaches that jointly solve related problems such as \nrelationship extraction and coding assignment could generate a more comprehensive understanding compared \n22\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nto single-task models. Leveraging self-supervised pre-training strategies has the potential to make better use \nof unlabeled clinical data. Integrating these models into clinical decision support systems and evaluating their \nimpact on downstream tasks from diagnosis to treatment planning would help establish their clinical value. In \nthe future work, we further plan to use another recent predictors such as pAtbP-EnC 45, AIPs-SnTCN46, AFP-\nCMBPred47, cACP-DeepGram48, iACP-GAEnsC49, and Target-ensC_NP . Furthermore, we intended to used the \nCD-HIT tool was utilized to eliminate redundant peptide samples with  homology50.\nContinued development of explainable AI is also important for gaining user trust in model-driven health -\ncare. Further optimizing model architectures and expanding available data sources holds promise to consolidate \nmedical language processing as a key enabling technology for advancing precision medicine through insights \nfrom patient narratives.\nData availability\nThe data that support the findings of this study are available as follows. Dataset 1: https:// www. kaggle. com/ datas \nets/ niyar rbarm an/ sympt om2di sease/. Dataset 2: https:// data. mende ley. com/ datas ets/ f7mrc zj83k/1. Source of \nDataset-2: https:// www. kaggle. com/ datas ets/ pawan 2905/ tweet- class ifica tion? select= Data. csv.\nReceived: 7 October 2023; Accepted: 7 January 2024\nReferences\n 1. Shams, M. Y ., Elzeki, O. M., Abd Elfattah, M., Medhat, T. & Hassanien, A. E. Why are generative adversarial networks vital for \ndeep neural networks? A case study on COVID-19 chest X-Ray images. In Big Data Analytics and Artificial Intelligence Against \nCOVID‑19: Innovation Vision and Approach 147–162 (Springer, 2020).\n 2. Zheng, Y . et al. Smart materials enabled with artificial intelligence for healthcare wearables. Adv. Func. Mater. 31(51), 2105482 \n(2021).\n 3. Marie, H. S. et al. Tech-Care: A high-tech eye-controlled wheelchair for paralyzed patients. In 2023 International Telecommunica‑\ntions Conference (ITC‑Egypt) 413–418. https:// doi. org/ 10. 1109/ ITC- Egypt 58155. 2023. 10206 404 (2023).\n 4. AlMahadin, G. et al. Parkinson’s disease: Current assessment methods and wearable devices for evaluation of movement disorder \nmotor symptoms-a patient and healthcare professional perspective. BMC Neurol. 20(1), 1–13 (2020).\n 5. Ashraf, E., Areed, N. F . F ., Salem, H., Abdelhay, E. H. & Farouk, A. FIDChain: Federated intrusion detection system for blockchain-\nenabled IoT healthcare applications. Healthcare 10(6), 6. https:// doi. org/ 10. 3390/ healt hcare 10061 110 (2022).\n 6. Shastry, K. A. & Shastry, A. An integrated deep learning and natural language processing approach for continuous remote moni-\ntoring in digital health. Decis. Anal. J. 8, 100301 (2023).\n 7. Shams, M. Y ., El-kenawy, E.-S.M., Ibrahim, A. & Elshewey, A. M. A hybrid dipper throated optimization algorithm and particle \nswarm optimization (DTPSO) model for hepatocellular carcinoma (HCC) prediction. Biomed. Signal Process. Control 85, 104908. \nhttps:// doi. org/ 10. 1016/j. bspc. 2023. 104908 (2023).\n 8. Mamdouh Farghaly, H., Shams, M. Y . & Abd El-Hafeez, T. Hepatitis C Virus prediction based on machine learning framework: A \nreal-world case study in Egypt. Knowl. Inf. Syst. 65(6), 2595–2617 (2023).\n 9. Cesar, L. B., Manso-Callejo, M. -Á. & Cira, C.-I. BERT (Bidirectional Encoder Representations from Transformers) for missing \ndata imputation in solar irradiance time series. Eng. Proc. 39(1), 26 (2023).\n 10. Elzeki, O. M., Abd Elfattah, M., Salem, H., Hassanien, A. E. & Shams, M. A novel perceptual two layer image fusion using deep \nlearning for imbalanced COVID-19 dataset. PeerJ Comput. Sci. 7, e364 (2021).\n 11. Elzeki, O. M., Shams, M., Sarhan, S., Abd Elfattah, M. & Hassanien, A. E. COVID-19: A new deep learning computer-aided model \nfor classification. PeerJ Comput. Sci. 7, e358 (2021).\n 12. Zeberga, K. et al. A novel text mining approach for mental health prediction using Bi-LSTM and BERT model. Comput. Intell. \nNeurosci. 2022, 1–18 (2022).\n 13. Elshewey, A. M. et al. Bayesian optimization with support vector machine model for Parkinson disease classification. Sensors 23(4), \n4. https:// doi. org/ 10. 3390/ s2304 2085 (2023).\n 14. Tarek, Z. et al. Soil erosion status prediction using a novel random forest model optimized by random search method. Sustainability \n15(9), 9. https:// doi. org/ 10. 3390/ su150 97114 (2023).\n 15. Nguyen, H.-P ., Liu, J. & Zio, E. A long-term prediction approach based on long short-term memory neural networks with automatic \nparameter optimization by Tree-structured Parzen Estimator and applied to time-series data of NPP steam generators. Appl. Soft  \nComput. 89, 106116 (2020).\n 16. Salem, H. et al. Fine-tuning fuzzy KNN classifier based on uncertainty membership for the medical diagnosis of diabetes. Appl. \nSci. 12(3), 950 (2022).\n 17. Molina, M., Jiménez, C. & Montenegro, C. Improving drug-drug interaction extraction with Gaussian noise. Pharmaceutics 15(7), \n1823 (2023).\n 18. Machado, J., Rodrigues, C., Sousa, R. & Gomes, L. M. Drug–drug interaction extraction‐based system: An natural language pro-\ncessing approach. Expert Systems e13303 (2023).\n 19. Nguyen, D. P . & Ho, T. B. Drug-drug interaction extraction from biomedical texts via relation BERT. In 2020 RIVF International \nConference on Computing and Communication Technologies (RIVF) 1–7 (IEEE, 2020).\n 20. KafiKang, M. & Hendawi, A. Drug-drug interaction extraction from biomedical text using relation BioBERT with BLSTM. Mach. \nLearn. Knowl. Extr. 5(2), 669–683 (2023).\n 21. Y ang, H. et al. A Multi-Layer Feature Fusion Model Based on Convolution and Attention Mechanisms for Text Classification. \nApplied Sciences 13(14), 8550 (2023).\n 22. Chaichulee, S. et al. Multi-label classification of symptom terms from free-text bilingual adverse drug reaction reports using natural \nlanguage processing. PLOS ONE 17(8), e0270595. https:// doi. org/ 10. 1371/ journ al. pone. 02705 95 (2022).\n 23. Lee, J. et al. BioBERT: A pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36(4), \n1234–1240. https:// doi. org/ 10. 1093/ bioin forma tics/ btz682 (2020).\n 24. Huang, K., Altosaar, J. & Ranganath, R. ClinicalBERT: Modeling clinical notes and predicting hospital readmission. arXiv. https:// \ndoi. org/ 10. 48550/ arXiv. 1904. 05342. (2020).\n 25. Hazell, L. & Shakir, S. A. W . Under-reporting of adverse drug reactions. Drug‑Saf. 29(5), 385–396. https:// doi. org/ 10. 2165/ 00002 \n018- 20062 9050- 00003 (2006).\n 26. Putra, F . B. et al. Identification of symptoms based on natural language processing (NLP) for disease diagnosis based on inter -\nnational classification of diseases and related health problems (ICD-11). In 2019 International Electronics Symposium (IES)  1–5. \nhttps:// doi. org/ 10. 1109/ ELECS YM. 2019. 89016 44 (2019).\n23\nVol.:(0123456789)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\n 27. González-Carvajal, S. & Garrido-Merchán, E. C. Comparing BERT against traditional machine learning text classification. arXiv \npreprint arXiv: 2005. 13012 (2020).\n 28. Guven, Z. A. Comparison of BERT models and machine learning methods for sentiment analysis on Turkish tweets. In 2021 6th \nInternational Conference on Computer Science and Engineering (UBMK) 98–101 (IEEE, 2021).\n 29. Benítez-Andrades, J. A., Alija-Pérez, J.-M., Vidal, M.-E., Pastor-Vargas, R. & García-Ordás, M. T. Traditional machine learning \nmodels and bidirectional encoder representations from transformer (BERT)–based automatic classification of tweets about eating \ndisorders: Algorithm development and validation study. JMIR Med. Inform. 10(2), e34492 (2022).\n 30. Mujahid, M. et al. Analyzing sentiments regarding ChatGPT using novel BERT: A machine learning approach. Information 14(9), \n474 (2023).\n 31. Brundha, J. & Meera, K. N. Vector model based information retrieval system with word embedding transformation. In 2022 10th \nInternational Conference on Emerging Trends in Engineering and Technology‑Signal and Information Processing (ICETET‑SIP‑22) \n01–04 (IEEE, 2022).\n 32. Kumar, A. A., Pati, P . B., Deepa, K. & Sangeetha, S. T. Toxic comment classification using S-BERT vectorization and random forest \nalgorithm. In 2023 IEEE International Conference on Contemporary Computing and Communications (InC4) 1–6 (IEEE, 2023).\n 33. Guo, Y ., Mustafaoglu, Z. & Koundal, D. Spam detection using bidirectional transformers and machine learning classifier algorithms. \nJ. Comput. Cogn. Eng. 2(1), 5–9 (2023).\n 34. Hassan, E., Shams, M. Y ., Hikal, N. A. & Elmougy, S. A novel convolutional neural network model for malaria cell images clas-\nsification. Comput. Mater. Continua 72(3), 5889–5907. https:// doi. org/ 10. 32604/ cmc. 2022. 025629 (2022).\n 35. Sarhan, S., Nasr, A. A. & Shams, M. Y . Multipose face recognition-based combined adaptive deep learning vector quantization. \nComput. Intell. Neurosci. 2020, 1–11 (2020).\n 36. Hassan, E., Shams, M. Y ., Hikal, N. A. & Elmougy, S. The effect of choosing optimizer algorithms to improve computer vision tasks: \nA comparative study. Multimed. Tools Appl. 82(11), 16591–16633. https:// doi. org/ 10. 1007/ s11042- 022- 13820-0 (2023).\n 37. Raschka, S. An overview of general performance metrics of binary classifier systems. arXiv preprint arXiv: 1410. 5330 (2014).\n 38. Koleck, T. A., Dreisbach, C., Bourne, P . E. & Bakken, S. Natural language processing of symptoms documented in free-text narra-\ntives of electronic health records: A systematic review. J. Am. Med. Inform. Assoc. 26(4), 364–379. https:// doi. org/ 10. 1093/ jamia/ \nocy173 (2019).\n 39. Yu, H. Q. Mining symptom and disease web data with NLP and Open Linked Data. In Proceedings of the 5th World Congress on \nElectrical Engineering and Computer Systems and Sciences (EECSS’19) 108(1), 1–4. https:// doi. org/ 10. 11159/ mvml19. 108 (2019).\n 40. Dreisbach, C., Koleck, T. A., Bourne, P . E. & Bakken, S. A systematic review of natural language processing and text mining of \nsymptoms from electronic patient-authored text data. Int. J. Med. Inform. 125, 37–46. https:// doi. org/ 10. 1016/j. ijmed inf. 2019. 02. \n008 (2019).\n 41. Omoregbe, N. A. I., Ndaman, I. O., Misra, S., Abayomi-Alli, O. O. & Damaševičius, R. Text messaging-based medical diagnosis \nusing natural language processing and fuzzy logic. J. Healthc. Eng. 2020, e8839524. https:// doi. org/ 10. 1155/ 2020/ 88395 24 (2020).\n 42. Koleck, T. A. et al. Identifying symptom information in clinical notes using natural language processing. Nurs. Res. 70(3), 173–183. \nhttps:// doi. org/ 10. 1097/ NNR. 00000 00000 000488 (2021).\n 43. Naseem, U., Kim, J., Khushi, M. & Dunn, A. G. Identification of disease or symptom terms in reddit to improve health mention \nclassification. In Proceedings of the ACM Web Conference 2022 2573–2581 (Association for Computing Machinery, 2022). https:// \ndoi. org/ 10. 1145/ 34854 47. 35121 29.\n 44. Eikelboom, W . S. et al. The reporting of neuropsychiatric symptoms in electronic health records of individuals with Alzheimer’s \ndisease: A natural language processing study. Alzheimer’s Res. Ther. 15(1), 94. https:// doi. org/ 10. 1186/ s13195- 023- 01240-7 (2023).\n 45. Akbar, S. et al. pAtbP-EnC: Identifying anti-tubercular peptides using multi-feature representation and genetic algorithm-based \ndeep ensemble model. IEEE Access 11, 137099–137114. https:// doi. org/ 10. 1109/ ACCESS. 2023. 33211 00 (2023).\n 46. Raza, A. et al. AIPs-SnTCN: Predicting anti-inflammatory peptides using fasttext and transformer encoder-based hybrid word \nembedding with self-normalized temporal convolutional networks. J. Chem. Inf. Model.  63(21), 6537–6554. https:// doi. org/ 10. \n1021/ acs. jcim. 3c015 63 (2023).\n 47. Ali, F . et al. AFP-CMBPred: Computational identification of antifreeze proteins by extending consensus sequences into multi-blocks \nevolutionary information. Comput. Biol. Med. 139, 105006. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 105006 (2021).\n 48. Akbar, S., Hayat, M., Tahir, M., Khan, S. & Alarfaj, F . K. cACP-DeepGram: Classification of anticancer peptides via deep neural \nnetwork and skip-gram-based word embedding model. Artif. Intell. Med. 131, 102349. https:// doi. org/ 10. 1016/j. artmed. 2022. \n102349 (2022).\n 49. Akbar, S., Hayat, M., Iqbal, M. & Jan, M. A. iACP-GAEnsC: Evolutionary genetic algorithm based ensemble classification of anti-\ncancer peptides by utilizing hybrid feature space. Artif Intell Med 79, 62–70. https:// doi. org/ 10. 1016/j. artmed. 2017. 06. 008 (2017).\n 50. Akbar, S. et al. Identifying neuropeptides via evolutionary and sequential based multi-perspective descriptors by incorporation \nwith ensemble classification strategy. IEEE Access 11, 49024–49034. https:// doi. org/ 10. 1109/ ACCESS. 2023. 32746 01 (2023).\nAuthor contributions\nThis work was carried out in collaboration among all authors. All Authors designed the study, performed the \nstatistical analysis, and wrote the protocol. Authors E.H., T.A.E.H. and M.Y .S. managed the analyses of the study, \nmanaged the literature searches, and wrote the first draft of the manuscript. All authors read and approved the \nfinal manuscript.\nFunding\nOpen access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in coopera-\ntion with The Egyptian Knowledge Bank (EKB).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to E.H., T.A.-H. or M.Y .S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n24\nVol:.(1234567890)Scientific Reports |         (2024) 14:1507  | https://doi.org/10.1038/s41598-024-51615-5\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.775283932685852
    },
    {
      "name": "Hyperparameter",
      "score": 0.7167981863021851
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6669665575027466
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.6369959115982056
    },
    {
      "name": "Machine learning",
      "score": 0.5554015636444092
    },
    {
      "name": "Preprocessor",
      "score": 0.48923254013061523
    },
    {
      "name": "Data pre-processing",
      "score": 0.48344239592552185
    },
    {
      "name": "Deep learning",
      "score": 0.4473068118095398
    },
    {
      "name": "Transformer",
      "score": 0.4456169605255127
    },
    {
      "name": "Language model",
      "score": 0.41956526041030884
    },
    {
      "name": "Natural language processing",
      "score": 0.36069419980049133
    },
    {
      "name": "Data mining",
      "score": 0.3565724492073059
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130309236",
      "name": "Kafrelsheikh University",
      "country": "EG"
    },
    {
      "id": "https://openalex.org/I89466785",
      "name": "Minia University",
      "country": "EG"
    }
  ],
  "cited_by": 107
}