{
  "title": "Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors",
  "url": "https://openalex.org/W3196706143",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5068324214",
      "name": "Ryo Nagata",
      "affiliations": [
        null,
        "Konan University",
        "RIKEN",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5078592584",
      "name": "Manabu Kimura",
      "affiliations": [
        null,
        "Konan University",
        "RIKEN",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5053883037",
      "name": "Kazuaki Hanawa",
      "affiliations": [
        null,
        "Konan University",
        "RIKEN",
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946119234",
    "https://openalex.org/W2971167679",
    "https://openalex.org/W2977788613",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W3103010876",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970868759",
    "https://openalex.org/W3119589059",
    "https://openalex.org/W2953083125",
    "https://openalex.org/W3175560839",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W3212044575",
    "https://openalex.org/W2970968076",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2741494657",
    "https://openalex.org/W2946535868",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W3029219243",
    "https://openalex.org/W2252042690",
    "https://openalex.org/W2170527467",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3035010485",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3175649109",
    "https://openalex.org/W2964020763"
  ],
  "abstract": "In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with respect to training data size in the BERT-based method than in the non-language model method. This suggests that (i) the BERT-based method should have a good knowledge of the grammar required to recognize certain types of error and that (ii) it can transform the knowledge into error detection rules by fine-tuning with few training samples, which explains its high generalization ability in grammatical error detection. We further show with pseudo error data that it actually exhibits such nice properties in learning rules for recognizing various types of error. Finally, based on these findings, we discuss a cost-effective method for detecting grammatical errors with feedback comments explaining relevant grammatical rules to learners.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 4107 - 4118\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nExploring the Capacity of a Large-scale Masked Language Model to\nRecognize Grammatical Errors\nRyo Nagata\nKonan University / Japan\nnagata-acl2022@ ml.hyogo-u.ac.jp.\nManabu Kimura\nGRAS Group, Inc. / Japan\nmanabu.kimura@gras-group.co.jp\nKazuaki Hanawa\nRIKEN / Japan\nTohoku University / Japan\nkazuaki.hanawa@riken.jp\nAbstract\nIn this paper, we explore the capacity of a\nlanguage model-based method for grammat-\nical error detection in detail. We ﬁrst show\nthat 5 to 10% of training data are enough\nfor a BERT-based error detection method to\nachieve performance equivalent to what a non-\nlanguage model-based method can achieve with\nthe full training data; recall improves much\nfaster with respect to training data size in the\nBERT-based method than in the non-language\nmodel method. This suggests that (i) the BERT-\nbased method should have a good knowledge of\nthe grammar required to recognize certain types\nof error and that (ii) it can transform the knowl-\nedge into error detection rules by ﬁne-tuning\nwith few training samples, which explains its\nhigh generalization ability in grammatical error\ndetection. We further show with pseudo error\ndata that it actually exhibits such nice proper-\nties in learning rules for recognizing various\ntypes of error. Finally, based on these ﬁndings,\nwe discuss a cost-effective method for detecting\ngrammatical errors with feedback comments\nexplaining relevant grammatical rules to learn-\ners.\n1 Introduction\nRecent studies have shown that masked language\nmodels pre-trained on a large corpus (hereafter,\nsimply language models) achieve tremendous im-\nprovements over a wide variety of natural language\nprocessing tasks with ﬁne-tuning. These results\nsuggest that they are also effective in recognizing\nerroneous words and phrases, the task known as\ngrammatical error detection. There has been, how-\never, much less work on this aspect of grammatical\nerror detection than in other tasks. One can argue\nthat since language models are trained on language\ndata produced by native speakers of a language\n(speciﬁcally, English in this paper), they might not\nwork well on the target language data produced\nby non-native speakers of that language. In other\nwords, English language models do not know at\nall about grammatical errors made by non-native\nspeakers. Even apart from grammatical errors,\nthe target language is different from the canon-\nical English, meaning that it contains unnatural\nphrases and characteristic language usages that na-\ntive speakers do not normally use, as Nagata and\nWhittaker (2013) demonstrate. These differences\nmight affect performance of language model-based\nmethods in grammatical error detection.\nActually, researchers have reported on perfor-\nmance of language models on grammatical error\ndetection and correction. Cheng and Duan (2020)\nand Kaneko and Komachi (2019) have shown that\nBERT (Devlin et al., 2019)-based methods improve\ngrammatical error detection performance in Chi-\nnese and English, respectively. Kaneko et al. (2020)\nand Didenko and Shaptala (2019) have shown a\nsimilar tendency in grammatical error correction.\nWhile these studies empirically prove the effective-\nness of language models in grammatical error detec-\ntion and correction, the questions of why and where\nlanguage models beneﬁt error detection/correction\nmethods are left unanswered.\nIn this paper, we explore this aspect of language\nmodels in grammatical error detection to better an-\nswer the research questions. We ﬁrst show that a\nBERT-based method incredibly quickly learns to\nrecognize various types of error as summarized in\nFigure 1; it achieves only with 5 to 10% of train-\ning data an F1.0 that a non-language model-based\nmethod can achieve with the full training data (the\ndetails will be described in Sect. 4). This implies\nthat the BERT-based method (i) should have a good\nknowledge of the grammar required to recognize\ncertain types of error and (ii) can transform it into\nerror detection rules by ﬁne-tuning with very few\n4107\n0 5000 10000 15000 20000 25000 30000\n# of sentences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF\n1.0\nFCE ALL ERROR\nB LSTM\nBER T -based (w/o BER T t#a n)\nBER T -based (w/ BER T t#a n)\n100\n100\n100\n27380\n27380\n27380\nFigure 1: Detection Performance in Relation with Training Size (FCE).\ntraining samples. Following this, we further show\nwith real and pseudo error data why and where it\ngains in error detection, revealing the insights of\nlanguage model-based methods. For instance, we\nshow that the BERT-based method trained on few\ninstances of a transitive verb with a preposition\n(e.g., *discuss about) can detect the same type of\nerror in other verbs (e.g., *approach to and *attend\nin). Finally, based on these ﬁndings, we discuss\na cost-effective method for detecting grammatical\nerrors with feedback comments explaining relevant\ngrammatical rules to learners.\n2 Related Work\nRei (2017) shows it is useful for neural error de-\ntection models to introduce a secondary language\nmodel objective together with the main error de-\ntection objective. Rei and Yannakoudakis (2017)\ncompare several other auxiliary training objectives\nincluding Part-Of-Speech (POS) tagging and error\ntype identiﬁcation and ﬁnd that the language model\nobjective is the most effective. This line of work\nsuggests that grammatical error detection beneﬁts\nfrom language modeling, although these studies\nuse BiLSTM-based language models instead of\nmasked language models trained on a large corpus.\nAs mentioned in Sect. 1, several researchers\nhave applied masked language models including\nBERT to grammatical error detection and correc-\ntion. Cheng and Duan (2020) and Kaneko and\nKomachi (2019) show that error detection methods\ngain in recall and precision with the use of lan-\nguage models. Bell et al. (2019) use BERT-based\ncontextual embeddings for grammatical error detec-\ntion and compare it with other types of contextual\nembedding. They show the BERT-based contex-\ntual embeddings are effective in almost all error\ntypes provided by ERRANT (Bryant et al., 2017)\nalthough BERT is not ﬁne-tuned in their study.\nYuan et al. (2021) compare BERT, XLNet (Yang\net al., 2019), ELECTRA (Clark et al., 2020) in\ngrammatical error detection to show their effective-\nness in grammatical error detection1. Kaneko et al.\n(2020) and Didenko and Shaptala (2019) also show\nperformance improvements in grammatical error\ncorrection. To strengthen the ﬁndings of these pre-\nvious studies, we will explore why and where error\ndetection methods beneﬁt from language models,\nrevealing their generalization ability, in the follow-\ning sections.\nThere has been a long history of studies that\ninvestigate the linguistic knowledge of language\nmodels including the work by Li et al. (2021); Et-\ntinger (2020); Warstadt et al. (2020) to name a few.\nA popular approach is to test whether a language\nmodel assigns higher likelihood to the appropriate\nword than an inappropriate one, given context. The\nlinguistic knowledge to be explored ranges from\nsyntactic/semantic knowledge to common sense.\nThese studies mostly use (i) synthetic test data:\nsentences that are generated synthetically by using\na certain kind of template or (ii) perturbed test data:\nsentences that are generated by perturbing a natural\ncorpus. Our work is different from these previous\nstudies in two points: (i) to our best knowledge,\nwe examine linguistic phenomena that have never\nbeen explored before in the conventional studies\n(e.g., subjects marked with a preposition and errors\ninvolving the usages of transitive and intransitive\nverbs); (ii) we use a real learner corpus with real\n1ELECTRA is not a language model. It however contains\nan architecture similar to that of a language model.\n4108\nerrors as our test data.\nMita and Yanaka (2021) examine if an encoder-\ndecoder neural network for grammatical error cor-\nrection (not BERT-based) can learn the knowl-\nedge of grammar through the task of grammati-\ncal error correction. They target ﬁve error types:\nsubject-verb agreement, verb form, word order, ad-\njective/adverb comparison, noun number. They use\nboth synthetic and real learner data. They report a\nnegative answer to the research question except for\nword order errors. They also report that their model\nlearns the knowledge to detect the target errors in\ntheir synthetic data. However, there is still room for\ndebate in this argument because error positions tend\nto be rather obvious in their synthetic data (e.g., ad-\njective forms erroneously used as adverbs almost\nalways appear at the end of a sentence). Our study\nexpands and deepens their ﬁndings for a wider va-\nriety of error types that are much more difﬁcult to\ndetect (in that it requires a much wider range of\nlinguistic knowledge including POS, lexical, and\nsyntactic knowledge).\n3 Data and Methods\n3.1 Real and Pseudo Error Data\nIn this paper, we use two kinds of data: real and\npseudo data. Real data are error-annotated learner\ncorpora while pseudo error data are automatically\ngenerated by perturbing a native English corpus.\nFor the real data, we use four English learner\ncorpora: the First Certiﬁcate of English error\ndetection dataset (FCE) (Yannakoudakis et al.,\n2011); NUS Corpus of Learner English (NU-\nCLE) (Dahlmeier et al., 2013); BEA-2019 shared\ntask dataset (BEA) (Bryant et al., 2019); the In-\nternational Corpus Network of Asian Learners of\nEnglish with feedback comments (ICNALE) (Na-\ngata et al., 2020). We use the data splits provided\nby the creator except for ICNALE where we ran-\ndomly split the essays into training, development,\nand test sets in the ratios of 85%, 7.5%, and 7.5%,\nrespectively. Table 1 shows their statistics2.\nICNALE provides information about errors in\npreposition use and their corresponding feedback\ncomments. We use it to investigate in detail why\nand where language model-based methods gain an\nadvantage. Their essay topics are controlled; they\nare written on either (a) It is important for college\nstudents to have a part-time job. or (b) Smoking\nshould be completely banned at all the restaurants\nin the country., which hereafter will be referred to\nas PART-TIME JOBand SMOKING, respectively.\nEach essay is manually annotated with errors in\npreposition use and their corresponding feedback\ncomments. For example, the major errors in the\ncorpus include deverbal prepositions (e.g.,*include\n→including), intransitive verbs with a direct object\n(e.g., *agree it →agree with it), a verb phrase used\nas a noun phrase (*Learn English is difﬁcult. →To\nlearn/Learning English is difﬁcult.), and compari-\nson between a phrase and a clause (e.g., *because\nan error →because of an error); see the work (Na-\ngata et al., 2020) for the details.\nTo investigate the relationship between the num-\nber of training sentences and detection perfor-\nmance, we randomly sample 100, 300, 500, 1,000,\n3,000, 5,000, 10,000, and all sentences, resulting\nin eight sets of training data for each corpus3. Note\nthat these training, development, and test sets con-\n2The data development is still ongoing in the work (Nagata,\n2019). For this work, we used data that had not been open to\nthe public yet from the developer.\n3In the sub-corpora A and C in BEA, only seven sets are\nused because they consist of less than 10,000 sentences.\nSplit Training Development Test\nStatistics sents tokens errors sents tokens errors sents tokens errors\nFCE 27,380 435,768 41,277 2,129 33,720 3,335 2,581 40,498 4,374\nICNALE\nPART-TIME JOB 12,163 205,355 2,439 1,129 18,276 244 1,042 17,192 222\nSMOKING 12,312 201,304 2,342 1,160 18,242 230 1,023 17,318 212\nBEA\nA 9,244 160,818 24,520 1,014 17,417 2,566 1,014 18,106 2,801\nB 11,410 207,252 20,580 1,261 22,435 2,261 1,261 22,806 2,362\nC 9,410 179,156 8,649 1,020 19,035 990 1,020 20,392 1,052\nNUCLE 16,969 433,787 38,723 2,120 54,799 4,019 2,120 56,804 4,406\nTable 1: Statistics on Real Datasets.\n4109\ntain error-free sentences.\nFor the pseudo error data, we use the 1998-2000\nNew York Times in the AQUAINT Corpus of En-\nglish News Text (Graff, 2002) as a base corpus.\nWe automatically generate erroneous sentences by\ninjecting errors into them (one error per sentence).\nWe ﬁrst obtain chunks and parses by using spaCy4.\nHere, we only use sentences whose lengths are\nlonger than three tokens and shorter than 26 to\nobtain reliable chunks and parses. We then add,\nremove, or replace a word in the sentences based\non the analyses.\nWhile we target all errors labeled as errors in the\nreal data, we only target the following ﬁve error\ntypes in the pseudo error data:\nPrepositional inﬁnitive: to-inﬁnitive with other\nprepositions than to.\n(e.g., a book to read →*a book for read)\nSubject verb: Verb phrases used as a subject\n(e.g., *Learn English is difﬁcult.)\nPreposition + subject: Subjects used with a\npreposition\n(e.g., *In the restaurant serves good food.)\nTransitive verb + preposition: Transitive verbs\nused with a preposition\n(e.g., *We discussed about it.)\nIntransitive verb + object: Intransitive verbs tak-\ning a direct object\n(e.g., *We agree it.)\nThese ﬁve error types are selected with the fol-\nlowing two criteria: (i) they are major errors in\nICNALE; (ii) we can write a software program to\ngenerate pseudo errors based on chunks and parses.\nFor example, we can ﬁnd the subject of a sentence\nfrom its parse and then can add a randomly-chosen\npreposition before the subject noun phrase (e.g.,\nThe restaurant serves good food. →*In the restau-\nrant serves good food.). We randomly choose one\nof the following ﬁve prepositions: at, about, to,\nin, and with for addition and replacement; an ex-\nception is that we only use for for “Prepositional\ninﬁnitive” (e.g., a book to read →*a book for\nread), which often appears in ICNALE. Similarly,\nwe can extract pairs of a verb and its direct object\nfrom parses and then can add one of the prepo-\nsitions before the direct object noun phrase as in\ndiscuss the matter →*discuss about the matter .\nWe select the following transitive verbs as our tar-\ngets: in training/development data: answer, attend,\n4https://spacy.io/\ndiscuss, inhabit, mention, oppose, and resemble;\nin test data: approach, consider, enter, marry,\nobey, reach, visit. Similarly, we select the fol-\nlowing intransitive verbs: in training/development\ndata: agree, belong, disagree, and relate; in test\ndata: apply, graduate, listen, specialize, worry. It\nshould be emphasized that there is no overlap of\nthe target transitive/intransitive verbs in the train-\ning/development and test data.\nFrom the resulting pseudo error data, we ran-\ndomly sample 2k(1 ≤k ≤10) sentences for each\nerror type, resulting in ten sets of training data (e.g.,\nwhen k = 1, the set comprises two instances of\neach error type, ten instances in total). We use these\ntraining sets to estimate the relationship between\nthe number of training sentences and detection per-\nformance. For a validation set, we randomly sam-\nple 200 sentences for each error type. Similarly,\nwe use a test set consisting of 200 sentences ran-\ndomly sampled for each error type plus another\n200 error-free sentences. The validation and test\nsets are ﬁxed regardless of the training data.\n3.2 Grammatical Error Detection Methods\nThis subsection describes the three methods to be\nexplored and compared. Before looking into them,\nlet us deﬁne grammatical error detection formally.\nGrammar error detection can be solved as a token\nclassiﬁcation problem5. We will denote a sequence\nof words and its length by w1, . . . , wi . . . , wN and\nN, respectively. We will denote the corresponding\nsequence of labels by l1, . . . , li, . . . , lN where li\ncorresponds to the label of wi. We assume two\nsets of labels: (i) either C or E denoting correct\nor erroneous in the real data, respectively; and (ii)\nK labels for K error types plus C for correct in\nthe pseudo error data. Then, grammatical error\ndetection is deﬁned as a problem of predicting the\noptimal label sequence given w1, . . . , wi . . . , wN .\nWe use neural networks to predict the optimal\nlabel sequence. In this paper, training is repeated\nﬁve times with different (but ﬁxed) random seeds.\nThe reported performance values (i.e., recall, pre-\ncision, and F1.0) are averaged over the ﬁve runs.\nTraining epochs are 50 for the real data or ten for\nthe pseudo error data at the maximum and we adopt\n5More generally, it can also be solved as a sequence la-\nbeling problem using for example CRF. However, Rei (2017)\nshows that the grammatical error detection task does not bene-\nﬁt from CRF. We observed the same tendency in our datasets.\nAccordingly, we solve it as a token classiﬁcation problem\n(without CRF).\n4110\nthe epoch achieving the best F1.0 on the develop-\nment set. Other major hyper parameters are shown\nin Appendix A.\n3.2.1 BERT-based Method\nThe BERT-based method takes as input a word\nsequence w1, . . . , wi . . . , wN and conducts the fol-\nlowing procedures:\n(1) Subword: convert all wi into their correspond-\ning subwords: s1, . . . , sj . . . , sM . Note that\nthe total number of all subwords are generally\ndifferent from that of all words in the input\nword sequence.\n(2) Encode: encode all sj into BERT embeddings\nbj by:\nbj = BERT(sj) (1)\nwhere BERT(·) denotes BERT taking sub-\nwords as input and outputs their correspond-\ning embedding vectors ofh-dimension (specif-\nically, h = 768 for ‘bert-base’) from the\nﬁnal layer. We use ‘bert-base-uncased’ for\nBERT(·).\n(3) Token classiﬁcation: output the optimal la-\nbels by:\nli = arg maxsoftmax(Wbj) (2)\nwhere W is a k ×h weight matrix where k\nis either 2 or K + 1(the number of different\nlabels). To take care of the difference in the\nlengths of the input word sequence and the\ncorresponding subword sequence, only the\nﬁrst subword of each word is considered in\nprediction.\n3.2.2 Methods to Be Compared\nFor comparison, we select a BiLSTM-based error\ndetection method. Basically, it follows the above\nsteps (1) to (3), but uses BiLSTM as an encoder in\nplace of BERT. Also, the input word sequence is\nturned into a sequence of embedding vectors where\neach embedding vector consists of the concatena-\ntion of the conventional word embedding and a\ncharacter-based embedding. The character-based\nembedding is obtained by another BiLSTM taking\nthe characters of each word following the work (Ak-\nbik et al., 2018). The concatenated embeddings are\nput into the encoder BiLSTM to produce vectors\nfor prediction in step (3). Speciﬁcally, we use the\nimplementation FLAIR (Akbik et al., 2019). We\nwill refer to this method for comparison as the\nBiLSTM-based method hereafter.\nWe also investigate how effective the ﬁne-tuning\nof BERT is. Namely, the BERT part of the BERT-\nbased method is ﬁxed during training and only the\noutput layer is adjusted by the training data. We\nwill refer to this method as the BERT-based method\nwithout BERT training hereafter.\n4 Performance on Real Data\nFigure 1 (see the second page) to Figure 3 show\nthe relationship between the number of training\nsentences and F1.0 in FCE6, NUCLE, and BEA\nwith its corresponding precision-recall curves, re-\nspectively. All F1.0 graphs show the high general-\nization ability of the BERT-based method. They\nalso show that the BERT-based method exhibits a\nperformance saturation at a very early stage (1,000-\n3,000 training sentences). It is worthwhile to point\nout the fact that the F1.0 curves for all proﬁciency\nlevels (A, B, and C) 7 in BEA (Figure 3) exhibit\nthe same trend as the other corpora although the\nabsolute performances differ depending on the lev-\nels. These results empirically conﬁrm that the high\ngeneration ability of the BERT-based method holds\nin various writer populations.\nUnlike the BERT-based method, the BiLSTM-\nbased method improves steadily as the number of\ntraining sentences increases although even with the\nfull training data, it only achieves an F1.0 that the\nBERT-based method can achieve with only 500\nor less training sentences. Also, the BERT-based\nmethod without BERT training does not perform\nwell at all. This is probably because it requires\nmuch more degrees of freedom in terms of the\nnetwork parameters to learn rules for detecting a\nwide variety of grammatical errors, which have a\ncertain degree of complexity.\nTo investigate the results from a different point\nof view, let us now consider precision-recall curves\n6The best performance of the BERT-based method in\nFCE is a recall and precision of 0.455 and 0.628, respec-\ntively. These numbers are considerably lower than those of\nthe state-of-the-art Yuan et al. (2021) report; they show that\nthe ELECTRA-based method achieves the best recall and pre-\ncision of 0.505 and 0.821, respectively (c.f., recall: 0.480,\nprecision: 0.757 for their BERT-based method). Note that\nthey use the large models whereas we use the bert-base model,\nwhich should be part of the reason for the performance dif-\nferences. While our results do not achieve the best results,\nthey explain well why large-scale masked language models\nperform well in grammatical error detection.\n7The proﬁciency levels A, B, and C in BEA, which corre-\nsponds to those in CEFR, becomes higher in this order.\n4111\n0 5000 10000 15000 20000\n# of sentences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0F1.0\nNUCLE ALL ERROR\nB LSTM\nBERT-based (w/o BERT t#a n)\nBERT-based (w/ BERT t#a n)\n\u0012\u0011\u0011\n\u0012\u0011\u0011\n\u0012\u0011\u0011 \u0012\u0017\u001a\u0017\u001a\n\u0012\u0017\u001a\u0017\u001a\n\u0012\u0017\u001a\u0017\u001a\nFigure 2: Detection Performance in Relation with Training Size (NUCLE).\n\u0012\u0011\u0011\n\u0012\u0011\u0011\n\u0012\u0011\u0011\n\u0012\u0011\u0011\u0012\u0011\u0011 \u0012\u0011\u0011 \u0012\u0011\u0011\n\u0012\u0011\u0011\n\u0012\u0011\u0011\n\u001a\r\u0013\u0015\u0015\n\u0012\u0012\r\u0015\u0012\u0011\n\u001a\r\u0015\u0012\u0011\n\u001a\r\u0013\u0015\u0015\n\u0012\u0012\r\u0015\u0012\u0011\n\u001a\r\u0015\u0012\u0011\n\u001a\r\u0013\u0015\u0015\n\u001a\r\u0015\u0012\u0011\u0012\u0012\r\u0015\u0012\u0011\nFigure 3: Detection Performance in Relation with Training Size (BEA). Essays in BEA are classiﬁed into three\ncategories A, B, and C corresponding to the CEFR levels. The categories are referred to by the labels such as\n“A:BERT” and “B:BERT” in the legend.\n(the right graphs; the numbers at the edges of\neach curve correspond to the numbers of mini-\nmum and maximum training sentences and the\nplots are lined in minimum to maximum order). All\nprecision-recall curves show that only the BERT-\nbased method quickly improves in recall as the\nnumber of training sentences increases while the\nBERT-based and BiLSTM-based methods both im-\nprove in precision. In other words, only the BERT-\nbased method learns to recognize various error\ntypes with little exposure to error examples.\nFigure 4 shows recalls per detailed error types in\nFCE8 where error types are automatically obtained\nby using ERRANT. Figure 4 shows that the BERT-\nbased method quickly achieves a good recall in\nSPELL (spelling errors) while the BiLSTM-based\n8PUNCT, OTHER and those whose frequency is less than\n150 are excluded in Figure 4.\nmethod shows a much milder rise. This is not sur-\nprising considering that BERT is trained on large\nnative corpora. Namely, it virtually has a large\nvocabulary list and knows about English spellings\nwell. More interestingly, it exhibits a performance\nsaturation at a very early stage (500-3,000 training\nsentences) in all errors, resulting in log-like-shape\ncurves while the BiLSTM-based method improves\nrather linearly (except for SPELL). Even more in-\nterestingly, it shows a sharp rise in recall in DET\n(determiner errors) and NOUN:NUM (errors in\nnoun number). The notion of noun countability\nwith POS plays an important role in detecting these\ntwo types of error as in *I am student/countable.\nand an evidence/uncountable. This suggests that\nBERT contains some kind of knowledge corre-\nsponding to noun countability and POS (singu-\nlar/plural nouns). More generally, from these, one\n4112\n0 5000 10000 15000 20000 25000\n# of senten#es\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRe#all\nFCE ERROR T PED RECALL:BER T -based (w/ BER T tra.)\nDET\nSPELL\nPREP\nVERB\nVERB:TENSE\nNOUN\nVERB:FORM\nNOUN:NUM\nOR TH\nPRON\n0 5000 10000 15000 20000 25000\n# o  se#te#ces\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR ecall\nFCE ERROR TYPED RECALL:BiLSTM\nDET\nSPELL\nPREP\nVERB\nVERB:TENSE\nNOUN\nVERB:FORM\nNOUN:NUM\nOR TH\nPRON\nFigure 4: Detection Performance (Recall) per Error Type in Relation with Training Size (FCE).\n0 2000 4000 6000 8000 10000 12000\n# of #e te ce#\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF\n1.0\nP AR T - TIME JOB\nBiLSTM\nBER T -ba#ed (w/o BER T trai )\nBER T -ba#ed (w/ BER T trai )\n100\n100\n12163\n100\n12163\n12163\nFigure 5: Detection Performance in Relation with Training Size (ICNALE, in-domain test data).\ncan expect that it has broad knowledge about lin-\nguistic properties, which we will observe presently\nin the experiments with ICNALE.\nLet us then turn to ICNALE where we only target\nerrors concerning preposition use (meaning that the\nother errors are unmarked and that if other errors\nare detected, they are treated as false positives).\nHere, we only present performance on PART-TIME\nJOB due to the space limitation; the results for\nSMOKING exhibit a very similar tendency, which\ncan be found in Appendix B.\nFigure 5 and Figure 6 show performance in\nPART-TIME JOB in ICNALE; in Figure 5, all\nmodels are trained on essays on PART-TIME JOB\nand tested on (test) essays on the same topic (in-\ndomain setting) while in Figure 6, they are trained\non SMOKING and tested on PART-TIME JOB\n(out-of-domain setting). In both settings, 300 to\n500 training sentences are again enough for the\nBERT-based method to rival the BiLSTM-based\nmethod with the full training data, which exhibits\nagain a linear improvement in F1.0. Also, only the\nBERT-based method quickly improves in recall.\nA closer look at the detection results reveals\nthat many of the errors require linguistic knowl-\nedge including POS, syntactic relations, and lexi-\ncal properties such as transitive/intransitive verbs.\nFor example, “Preposition + subject” errors, which\nwere introduced in Subsect. 3.1, require the notions\nof POS such as verbs and syntactic relations such\nas subjects. Similarly, the distinction between tran-\nsitive and intransitive verbs play a crucial role in\n“transitive verb + preposition” and “intransitive verb\n+ object”. The fact that the BERT-based method\ncan detect these types of errors with a few training\ninstances suggest that it has an access to grammar-\nlike knowledge and that it can turn the knowledge\ninto error detection rules by ﬁne-tuning. We will\nexplore this in more detail in the following section.\nThese results also shed light on an important\naspect of the BERT-based method in practice.\nNamely, a cost-effective way of developing an er-\nror detection system based on BERT would be to\ncreate a small amount of training sentences (e.g.,\n1,000) for each essay topic; according to the above\n4113\n0 2000 4000 6000 8000 10000 12000\n# of sentences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF\n1.0\nT est:P AR T - TIME JOB  T ra#n:SMOKING\nB#LSTM\nBER T -base  (w/o BER T (rain )\nBER T -based  (w/ BER T (rain )\n100\n100\n100 12312\n12312\n12312\nFigure 6: Detection Performance in Relation with Training Size: (ICNALE, out-of-domain test data).\nﬁgures, the gain would be much smaller after 1,000\ntraining sentences. Of course, the results are only\nfor a simple BERT-based method. No one knows\nhow differently the performance curves grow with\ndifferent architectures and/or with a much larger\nset of training instances. It will be interesting to\ninvestigate these points for future work.\n5 Performance on Synthetic Data\nIn the previous section, we have seen that the BERT-\nbased method has a much higher generalization\nability in grammatical error detection. To analysis\nthis phenomenon in detail, we now turn to detection\nperformance of the BERT-based method on the\npseudo error data. As described in Subsect. 3.1, we\ntrain it on the ten sets of training data (i.e., 2, 4,···,\n1024 training sentences for each error type) and test\nthe trained models on the ﬁxed test set. Doing so,\nwe estimate the relationship between the number of\ntraining sentences and detection performance for\neach error type.\nFigure 7 shows the relationship between the size\nof training data and F1.0 for each error type where\nthe size is measured by the number of sentences.\nFigure 7 reveals that the BERT-based method al-\nready recognizes some of the target errors at early\nstages (even with two or four training sentences).\nPerformance goes much higher even with eight\ntraining sentences in most of the error types with\nan exception of the error type “Intransitive verb\n+ object”. For instance, the BERT-based method\nrecognizes more than half of the “Preposition +\nsubject” errors with a precision of 0.800 only with\neight training instances. This implies again that\nBERT has certain knowledge about the notions of\nPOS such as verbs and syntactic relations such as\nsubjects; otherwise, it would be difﬁcult to achieve\n0\n0.2\n0.4\n0.6\n0.8\n1\n2 4 8 16 32 64 128 256 512 1024\nF1.0\nNumber of training sentences\nIntransitive verb + direct object\nSubject verb\nPrepositional infinitive\nPreposition + subject\nTransitive verb + preposition\nFigure 7: Detection Performance (F1.0) per Error Type\nin Relation with Training Size: Synthetic Data.\na similar performance in this type of error consider-\ning that the noun phrase of a subject and its position\nin the sentence considerably vary depending on the\ntarget sentence.\nThe same argument applies to “transitive verb\n+ preposition” and “intransitive verb + object”. It\nshould be emphasized that the BERT-based method\nhas to detect errors in verbs that never9 appear in\nthe training data; recall that there is no overlap of\nthe target transitive/intransitive verbs in the train-\ning and test data. In other words, the BERT-based\nmethod can recognize unseen erroneous combina-\ntions, for example, *visited in Atlanta (transitive\nverb + preposition type) and *specialized environ-\n9Strictly, some of the verbs may appear in the training\nsentences for the other error types. However, they never appear\nin the erroneous phrases. Also, they do not appear at all when\nthe training size is small.\n4114\nmental litigation (intransitive verb + direct object\nerror type) after just seeing *mention in, discussed\nabout and *were related drugs and belongs Lon’s\ngrandmother. These training and test sentences\nhave almost nothing in common except that they\nare the combinations of transitive/intransitive verbs\nand prepositions/objects. Besides, the fact that\ncorrect combinations of other verbs and preposi-\ntions/objects often appear in the test data makes\nthe task even more difﬁcult without the knowledge\nof POS and syntactic relations. These ﬁndings\nsupport the hypotheses that BERT has linguistic\nknowledge and that it can turn the knowledge into\nerror detection rules by ﬁne-tuning.\n6 Exploration for Cost-Effective Error\nDetection with Feedback Comments\nThe ﬁndings we have obtained so far bring out\nthe possibility that one can implement with few\ntraining instances a system that accurately detects\ngrammatical errors and recognizes their detailed er-\nror types. For example, manually or automatically,\nby creating few instances of the erroneous combina-\ntion of transitive verbs and prepositions as we saw\nin the previous sections (e.g., *discuss about), one\ncan develop a system detecting the same type of\nerror in other transitive verbs and prepositions (e.g.,\n*attend in it ). With the detailed error types, the\nsystem can also output feedback comments to the\nuser such as Transitive verbs do not take a prepo-\nsition. Instead, they take a direct object instead of\njust indicating them as preposition errors.\nAs a pilot study, we trained the BERT-based\nmethod on the pseudo error data and tested it on\nthe real (learner) data to examine the above pos-\nsibility. To achieve it, we manually annotated the\nreal data with the target ﬁve error types based on\nthe feedback comments available in ICNALE.\nFigure 8 shows the results. It reveals that the\nBERT-based method trained on the pseudo data\ndoes not perform on the real data as well as on the\npseudo data. Performance growths stop at an early\nstage (around eight training sentences). A possible\nreason for this is that in the real data, multiple\nerrors often appear in a sentence. Also, multiple\nerrors in a sentence can range over multiple types\nof error. Besides, the error rate is much lower in\nthe real data than in the pseudo error data where\none error occurs per sentence except 200 error-free\nsentences (although multiple types of error appear\nin the whole data set). These conditions make the\n0\n0.2\n0.4\n0.6\n0.8\n1\n2 4 8 16 32 64 128 256 512 1024\nF1.0\nNumber of training sentences\nIntransitive verb + direct object\nSubject verb\nPrepositional infinitive\nPreposition + subject\nTransitive verb + preposition\nFigure 8: Detection Performance ( F1.0) on per Error\nType in Relation with Training Size: Trained on Syn-\nthetic Data; Tested on Real Data (PART-TIME JOB and\nSMOKING merged).\ntask much more difﬁcult on the real data, as Flachs\net al. (2019) demonstrate.\nHaving said that, the results shown in Figure 7\nstill encourage us to develop language model-based\nsystems with a small amount of in-domain train-\ning data in order to detect grammatical errors with\ndetailed error types. One possible way to achieve\nit is to sample sentences from unannotated essays\nwritten on the target topic, and then to annotate\nthem with the speciﬁc error types that the devel-\noper wants to give feedbacks to learners. This\nwill naturally mitigate the problems caused by the\nmultiple-type multiple error situation and the er-\nror rate difference. One can also manually create\nsample error sentences (and their correct versions).\n7 Conclusions\nIn this paper, we have explored the capacity of a\nlarge-scale masked language model to recognize\ngrammatical errors. Our ﬁndings are summarized\nin the following three points: (1) Experiments with\nthe real learner data show that a BERT-based error\ndetection method has a much higher generalization\nability in grammatical error detection than a non-\nlanguage model-based method, and the ﬁrst per-\nformance saturation comes at the point of around\n1,000-3,000 training instances; (2) It starts to rec-\nognize the target errors involving a wide variety of\ngrammatical knowledge with very few instances of\nthem; (3) The high generalization ability brings out\nits potential for developing systems that detect and\nexplain grammatical errors with very few training\ninstances.\n4115\nAcknowledgments\nThis work was partly supported by Japan Science\nand Technology Agency (JST), PRESTO Grant\nNumber JPMJPR1758, Japan. This work was\npartly conducted by using computational resource\nof AI Bridging Cloud Infrastructure (ABCI) pro-\nvided by National Institute of Advanced Industrial\nScience and Technology (AIST).\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018.\nContextual string embeddings for sequence label-\ning. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1638–\n1649, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nSamuel Bell, Helen Yannakoudakis, and Marek Rei.\n2019. Context is key: Grammatical error detection\nwith contextual word representations. In Proceedings\nof the Fourteenth Workshop on Innovative Use of\nNLP for Building Educational Applications , pages\n103–115. Association for Computational Linguistics.\nChristopher Bryant, Mariano Felice, Øistein E. Ander-\nsen, and Ted Briscoe. 2019. The BEA-2019 shared\ntask on grammatical error correction. In Proceedings\nof 14th Workshop on Innovative Use of NLP for Build-\ning Educational Applications, pages 52–75, Florence,\nItaly. Association for Computational Linguistics.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of error\ntypes for grammatical error correction. In Proceed-\nings of 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 793–805.\nYong Cheng and Mofan Duan. 2020. Chinese gram-\nmatical error detection based on BERT model. In\nProceedings of 6th Workshop on Natural Language\nProcessing Techniques for Educational Applications,\npages 108–113. Association for Computational Lin-\nguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learning\nRepresentations.\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.\n2013. Building a large annotated corpus of learner\nEnglish: The NUS corpus of learner English. In Pro-\nceedings of 8th Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 22–31,\nAtlanta, Georgia. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nBohdan Didenko and Julia Shaptala. 2019. Multi-\nheaded architecture based on BERT for grammatical\nerrors correction. In Proceedings of 14th Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 246–251.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nSimon Flachs, Ophélie Lacroix, Marek Rei, Helen Yan-\nnakoudakis, and Anders Søgaard. 2019. A simple\nand robust approach to detecting subject-verb agree-\nment errors. In Proceedings of 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2418–2427. Association for Computational Linguis-\ntics.\nDavid Graff. 2002. The aquaint corpus of english news\ntext.\nMasahiro Kaneko and Mamoru Komachi. 2019. Multi-\nhead multi-layer attention to deep language represen-\ntations for grammatical error detection.\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun\nSuzuki, and Kentaro Inui. 2020. Encoder-decoder\nmodels can beneﬁt from pre-trained masked language\nmodels in grammatical error correction. In Proceed-\nings of 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4248–4254. Asso-\nciation for Computational Linguistics.\nBai Li, Zining Zhu, Guillaume Thomas, Yang Xu, and\nFrank Rudzicz. 2021. How is BERT surprised? lay-\nerwise detection of linguistic anomalies. In Proceed-\nings of 59th Annual Meeting of the Association for\nComputational Linguistics and 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4215–4228, Online.\nMasato Mita and Hitomi Yanaka. 2021. Do grammatical\nerror correction models realize grammatical gener-\nalization? In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4116\n4554–4561. Association for Computational Linguis-\ntics.\nRyo Nagata. 2019. Toward a task of feedback comment\ngeneration for writing learning. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3206–3215, Hong Kong,\nChina. Association for Computational Linguistics.\nRyo Nagata, Kentaro Inui, and Shin’ichiro Ishikawa.\n2020. Creating corpora for research in feedback com-\nment generation. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n340–345, Marseille, France. European Language Re-\nsources Association.\nRyo Nagata and Edward Whittaker. 2013. Reconstruct-\ning an Indo-European family tree from non-native\nEnglish texts. In Proceedings of 51st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1137–1147, Soﬁa,\nBulgaria. Association for Computational Linguistics.\nMarek Rei. 2017. Semi-supervised multitask learning\nfor sequence labeling. In Proceedings of 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2121–2130.\nMarek Rei and Helen Yannakoudakis. 2017. Auxiliary\nobjectives for neural error detection models. In Pro-\nceedings of 12th Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 33–43.\nAssociation for Computational Linguistics.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of 33rd Con-\nference on Neural Information Processing Systems,\npages 5753–5763.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading ESOL texts. In Proceedings of 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies, pages\n180–189.\nZheng Yuan, Shiva Taslimipoor, Christopher Davis, and\nChristopher Bryant. 2021. Multi-class grammati-\ncal error detection for correction: A tale of two sys-\ntems. In Proceedings of 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8722–8736, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\n4117\nA Hyper parameter settings\nTable 2 shows major hyper parameters used in the\nexperiments. Note that when we use the pseudo\nerror data for training, the number of training sen-\ntences can be as small as ten, and we use a rather\nsmall batch of ﬁve; otherwise we use 32.\nBatch size 5 or 32\nOptimization Adam with decoupled weight\ndecay regularization\nLearning rate 5e-5, (0.9, 0.999)\nEpsilon 1e-8\nWeight decay 1e-2\nTable 2: Major Hyper parameters.\nB Performance on SMOKING in\nICNALE\nFigure 9 and Figure 10 show performance in\nSMOKING in ICNALE. We can see the same ten-\ndency as in Figures 5 and 6.\n0 2000 4000 6000 8000 10000 12000\n# of #e te ce#\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF\n1.0\nSMOKING\nBiLSTM\nBER T -ba#ed (w/o BER T trai )\nBER T -ba#ed (w/ BER T trai )\n100\n100\n100\n12312\n12312\n12312\nFigure 9: Detection Performance in Relation with Training Size (ICNALE, in-domain test data).\n0 2000 4000 6000 8000 10000 12000\n# of sentences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF\n1.0\nT est:SMOKING  T ra#n:P AR T - TIME JOB\nB#LSTM\nBER T -base  (w/o BER T (rain )\nBER T -based  (w/ BER T (rain )\n100100\n100 12163\n12163\n12163\nFigure 10: Detection Performance in Relation with Training Size: (ICNALE, out-of-domain test data).\n4118",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8120107650756836
    },
    {
      "name": "Grammar",
      "score": 0.6879304051399231
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6388806104660034
    },
    {
      "name": "Generalization",
      "score": 0.6076381206512451
    },
    {
      "name": "Natural language processing",
      "score": 0.555457592010498
    },
    {
      "name": "Generalization error",
      "score": 0.5081799030303955
    },
    {
      "name": "Language model",
      "score": 0.46496492624282837
    },
    {
      "name": "Error detection and correction",
      "score": 0.4432622194290161
    },
    {
      "name": "Machine learning",
      "score": 0.43149271607398987
    },
    {
      "name": "Speech recognition",
      "score": 0.3645356297492981
    },
    {
      "name": "Algorithm",
      "score": 0.21555903553962708
    },
    {
      "name": "Linguistics",
      "score": 0.15550833940505981
    },
    {
      "name": "Artificial neural network",
      "score": 0.14832890033721924
    },
    {
      "name": "Mathematics",
      "score": 0.08673813939094543
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I15991598",
      "name": "Konan University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210110652",
      "name": "RIKEN",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ],
  "cited_by": 4
}