{
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "url": "https://openalex.org/W3170863103",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5053932343",
            "name": "Hangbo Bao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5051688016",
            "name": "Dong Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5014662947",
            "name": "Furu Wei",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3156665996",
        "https://openalex.org/W3009561768",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2326925005",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3101821705",
        "https://openalex.org/W3095121901",
        "https://openalex.org/W2971074500",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3122325173",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2548228487",
        "https://openalex.org/W2963799213",
        "https://openalex.org/W2883725317",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3107668149",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W2321533354",
        "https://openalex.org/W2887997457",
        "https://openalex.org/W2962742544",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3113747735",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3160566314",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2995181141",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2798991696",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2948433173",
        "https://openalex.org/W3020482686",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W3120857301",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2507296351",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3034255912",
        "https://openalex.org/W2971155163"
    ],
    "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",
    "full_text": "BEIT: BERT Pre-Training of Image Transformers\nHangbo Bao‚Ä†‚àó, Li Dong ‚Ä°, Songhao Piao ‚Ä†, Furu Wei ‚Ä°\n‚Ä†Harbin Institute of Technology\n‚Ä°Microsoft Research\nhttps://aka.ms/beit\nAbstract\nWe introduce a self-supervised vision representation model BEIT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT [DCLT19] developed in the natural language processing area, we propose a\nmasked image modelingtask to pretrain vision Transformers. SpeciÔ¨Åcally, each\nimage has two views in our pre-training, i.e., image patches (such as16√ó16 pixels),\nand visual tokens (i.e., discrete tokens). We Ô¨Årst ‚Äútokenize‚Äù the original image into\nvisual tokens. Then we randomly mask some image patches and fed them into\nthe backbone Transformer. The pre-training objective is to recover the original\nvisual tokens based on the corrupted image patches. After pre-training BEIT, we\ndirectly Ô¨Åne-tune the model parameters on downstream tasks by appending task\nlayers upon the pretrained encoder. Experimental results on image classiÔ¨Åcation\nand semantic segmentation show that our model achieves competitive results with\nprevious pre-training methods.\n1 Introduction\nTransformer [ VSP+17] has achieved promising performance in computer vision [ DBK+20,\nTCD+20]. However, empirical studies show that vision Transformers require more training data than\nconvolutional neural networks. In order to solve the data-hungry issue [LSB+21], self-supervised\npre-training is a promising solution to leverage large-scale image data. Several strands of methods\nhave been explored for vision Transformers, such as contrastive learning [CXH21, XLY+21], and\nself-distillation [CTM+21].\nConcurrently, BERT [ DCLT19] has achieved great success in natural language processing. Its\nmasked language modeling task Ô¨Årst randomly masks some proportion of tokens within a text, and\nthen recovers the masked tokens based on the Transformer encoding results of the corrupted text.\nMotivated by BERT, we turn to the denoising auto-encoding idea to pretrain vision Transformers,\nwhich has not been well studied by the vision community. It is challenging to directly apply BERT-\nstyle pre-training for image data. First of all, there is no pre-exist vocabulary for vision Transformer‚Äôs\ninput unit, i.e., image patches. So we cannot simply employ a softmax classiÔ¨Åer to predict over all\npossible candidates for masked patches. In contrast, the language vocabulary, such as words and\nBPE [SHB16], is well-deÔ¨Åned and eases auto-encoding prediction. A straightforward alternative\nis regarding the task as a regression problem, which predicts the raw pixels of masked patches.\nHowever, such pixel-level recovery task tends to waste modeling capability on pre-training short-\nrange dependencies and high-frequency details [RPG+21]. Our goal is to overcome the above issues\nfor pre-training of vision Transformers.\nIn this work, we introduce a self-supervised vision representation model BEIT, which stands for\nBidirectional Encoder representation from Image Transformers. Inspired by BERT, we propose a\npre-training task, namely, masked image modeling (MIM). As shown in Figure 1, MIM uses two\n‚àóContribution during internship at Microsoft. Correspondence to: Li Dong<lidong1@microsoft.com>, Furu\nWei<fuwei@microsoft.com>\narXiv:2106.08254v2  [cs.CV]  3 Sep 2022\n123 234 456 567\n987 876 765 543\n112 223 334 445\n211 322 433 544\n+ + + + + + + + + + + + + + + ++\nBEIT EncoderBlockwise\nMasking\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 160\nFlatten\nTokenizer\n Decoder\nPosition \nEmbedding\nPatch \nEmbedding\nOriginal \nImage\nImage \nPatches\nVisual Tokens\nùê°2\nL ùê°3\nL ùê°6\nL ùê°7\nL ùê°14\nL\nMasked Image Modeling Head\nReconstructed \nImage\nUnused During \nPre-Training\n234 456 876 765 322\n[S] [M] [M] [M] [M] [M]\nFigure 1: Overview of BEIT pre-training. Before pre-training, we learn an ‚Äúimage tokenizer‚Äù via\nautoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according\nto the learned vocabulary. During pre-training, each image has two views, i.e., image patches, and\nvisual tokens. We randomly mask some proportion of image patches (gray patches in the Ô¨Ågure) and\nreplace them with a special mask embedding [M]. Then the patches are fed to a backbone vision\nTransformer. The pre-training task aims at predicting the visual tokens of the original image based\non the encoding vectors of the corrupted image.\nviews for each images, i.e., image patches, and visual tokens. We split the image into a grid of patches\nthat are the input representation of backbone Transformer. Moreover, we ‚Äútokenize‚Äù the image to\ndiscrete visual tokens, which is obtained by the latent codes of discrete V AE [RPG+21]. During\npre-training, we randomly mask some proportion of image patches, and feed the corrupted input to\nTransformer. The model learns to recover the visual tokens of the original image, instead of the raw\npixels of masked patches.\nWe perform self-supervised learning and then Ô¨Åne-tune the pretrained BEIT on two downstream\ntasks, i.e., image classiÔ¨Åcation, and semantic segmentation. Experimental results indicate that BEIT\noutperforms both from-scratch training and previous strong self-supervised models. Moreover, BEIT\nis complementary to supervised pre-training. Performance of BEIT can be further improved by\nintermediate Ô¨Åne-tuning with ImageNet labels. Ablation studies show that our proposed techniques\nare critical to the effectiveness of BERT-style pre-training for image data. Apart from performance,\nthe improvements of convergence speed and stability of Ô¨Åne-tuning reduce training costs on end tasks.\nIn addition, we demonstrate that self-supervised BEIT can learn reasonable semantic regions via\npre-training, unleashing the rich supervision signals contained in images.\nOur contributions are summarized as follows:\n‚Ä¢ We propose a masked image modeling task to pretrain vision Transformers in a self-supervised\nmanner. We also provide a theoretical explanation from the perspective of variational autoencoder.\n‚Ä¢ We pretrain BEIT and conduct extensive Ô¨Åne-tuning experiments on downstream tasks, such as\nimage classiÔ¨Åcation, and semantic segmentation.\n‚Ä¢ We present that the self-attention mechanism of self-supervised BEIT learns to distinguish\nsemantic regions and object boundaries, although without using any human annotation.\n2 Methods\nGiven an input image x, BEIT encodes it to contextualized vector representations. As shown\nin Figure 1, BEIT is pretrained by the masked image modeling (MIM) task in a self-supervised\n2\nlearning manner. MIM aims at recovering the masked image patches based on encoding vectors. For\ndownstream tasks (such as image classiÔ¨Åcation, and semantic segmentation), we append task layers\nupon pretrained BEIT and Ô¨Åne-tune the parameters on the speciÔ¨Åc datasets.\n2.1 Image Representations\nThe images have two views of representations in our method, namely, image patch, and visual tokens.\nThe two types serve as input and output representations during pre-training, respectively.\n2.1.1 Image Patch\nThe 2D image is split into a sequence of patches [ DBK+20], so that a standard Transformer can\ndirectly accept image data. Formally, we reshape the image x ‚ààRH√óW√óC into N = HW/P 2\npatches xp ‚ààRN√ó(P2C), where Cis the number of channels, (H,W ) is the input image resolution,\nand (P,P ) is the resolution of each patch. The image patches {xp\ni}N\ni=1 are Ô¨Çattened into vectors\nand are linearly projected, which is similar to word embeddings in BERT [DCLT19]. Image patches\npreserve raw pixels and are used as input features in BEIT.\nIn our experiments, we split each 224 √ó224 image into a 14 √ó14 grid of image patches, where each\npatch is 16 √ó16.\n2.1.2 Visual Token\nSimilar to natural language, we represent the image as a sequence of discrete tokens obtained by an\n‚Äúimage tokenizer‚Äù, instead of raw pixels. SpeciÔ¨Åcally, we tokenize the image x ‚ààRH√óW√óC into\nz = [z1,...,z N] ‚ààVh√ów, where the vocabulary V= {1,..., |V|}contains discrete token indices.\nFollowing [RPG+21], we use the image tokenizer learned by discrete variational autoencoder (dV AE).\nThere are two modules during visual token learning, namely, tokenizer and decoder. The tokenizer\nqœÜ(z|x) maps image pixels x into discrete tokens z according to a visual codebook (i.e., vocabulary).\nThe decoder pœà(x|z) learns to reconstruct the input image x based on the visual tokens z. The\nreconstruction objective can be written as Ez‚àºqœÜ(z|x)[log pœà(x|z)]. Because the latent visual tokens\nare discrete, the model training is non-differentiable. Gumbel-softmax relaxation [JGP17, MMT17]\nis employed to train the model parameters. Moreover, a uniform prior is put on qœÜ during dV AE\ntraining. Refer to [RPG+21] for more training details of the image tokenizer.\nWe tokenize each image to a 14 √ó14 grid of visual tokens. Notice the number of visual tokens and\nthe number of image patches for one image are the same. The vocabulary size is set to |V|= 8192.\nIn our work, we directly use the publicly available2 image tokenizer described in [RPG+21]. We also\ncompare it with a re-implemented tokenizer in Appendix C.\n2.2 Backbone Network: Image Transformer\nFollowing ViT [DBK+20], we use the standard Transformer [VSP+17] as the backbone network. So\nthe results can be directly compared with previous work in terms of the network architecture.\nThe input of Transformer is a sequence of image patches {xp\ni}N\ni=1. The patches are then linearly\nprojected to obtain patch embeddings Exp\ni, where E ‚àà R(P2C)√óD. Moreover, we prepend a\nspecial token [S] to the input sequence. We also add standard learnable 1D position embeddings\nEpos ‚ààRN√óD to patch embeddings. The input vectors H0 = [e[S],Exp\ni,..., Exp\nN] +Epos is fed\ninto Transformer. The encoder contains Llayers of Transformer blocks Hl = Transformer(Hl‚àí1),\nwhere l= 1,...,L . The output vectors of the last layer HL = [hL\n[S],hL\n1 ,..., hL\nN] are used as the\nencoded representations for the image patches, where hL\ni is the vector of the i-th image patch.\n2.3 Pre-Training BE IT: Masked Image Modeling\nWe propose a masked image modeling(MIM) task. We randomly mask some percentage of image\npatches, and then predict the visual tokens that are corresponding to the masked patches.\n2https://github.com/openai/DALL-E\n3\nFigure 1 shows the overview of our method. As presented in Section 2.1, given an input image\nx, we split it into N image patches ({xp\ni}N\ni=1), and tokenize it to N visual tokens ({zi}N\ni=1). We\nrandomly mask approximately 40% image patches, where the masked positions are denoted as\nM‚àà{ 1,...,N }0.4N. Next we replace the masked patches with a learnable embedding e[M] ‚ààRD.\nThe corrupted image patches xM= {xp\ni : i /‚ààM}N\ni=1\n‚ãÉ{e[M] : i ‚ààM}N\ni=1 are then fed into the\nL-layer Transformer as described in Section 2.2. The Ô¨Ånal hidden vectors {hL\ni }N\ni=1 are regarded as\nencoded representations of the input patches. For each masked position {hL\ni : i‚ààM}N\ni=1, we use a\nsoftmax classiÔ¨Åer to predict the corresponding visual tokenspMIM(z‚Ä≤|xM) = softmaxz‚Ä≤ (WchL\ni +bc),\nwhere xMis the corrupted image, Wc ‚ààR|V|√óD, and bc ‚ààR|V|. The pre-training objective is to\nmaximize the log-likelihood of the correct visual tokens zi given the corrupted image:\nmax\n‚àë\nx‚ààD\nEM\n[‚àë\ni‚ààM\nlog pMIM(zi|xM)\n]\n(1)\nwhere Dis the training corpus, Mrepresents randomly masked positions, and xMis the corrupted\nimage that is masked according to M.\nAlgorithm 1 Blockwise Masking\nInput: N(= h√ów) image patches\nOutput: Masked positions M\nM‚Üê{}\nrepeat\ns‚ÜêRand(16,0.4N ‚àí|M|) ‚äøBlock size\nr‚ÜêRand(0.3, 1\n0.3 ) ‚äøAspect ratio of block\na‚Üê‚àös¬∑r; b‚Üê\n‚àö\ns/r\nt‚ÜêRand(0, h‚àía) ; l‚ÜêRand(0, w‚àíb)\nM‚ÜêM ‚ãÉ{(i,j) :i‚àà[t,t + a),j ‚àà[l,l + b)}\nuntil |M|>0.4N ‚äø Masking ratio is40%\nreturn M\nRather than randomly choosing patches\nfor the masked positions M, we employ\nblockwise masking in our work. As sum-\nmarized in Algorithm 1, a block of image\npatches is masked each time. For each\nblock, we set the minimum number of\npatches to 16. Then we randomly choose\nan aspect ratio for the masking block. We\nrepeat the above two steps until obtaining\nenough masked patches, i.e., 0.4N, where\nN is the total number of image patches,\nand 0.4 is masking ratio.\nThe MIM task is greatly inspired by masked language modeling [DCLT19], which is one of the most\nsuccessful pre-training objective in natural language processing. Moreover, blockwise (or n-gram)\nmasking is also widely applied in BERT-like models [ JCL+20, BDW+20, RSR+20]. However,\ndirectly using pixel-level auto-encoding (i.e., recovering the pixels of masked patches) for vision pre-\ntraining pushes the model to focus on short-range dependencies and high-frequency details [RPG+21].\nBEIT overcomes the above issue by predicting discrete visual tokens, which summarizes the details\nto high-level abstractions. Ablation studies in Section 3.3 show that our proposed method signiÔ¨Åcantly\noutperforms pixel-level auto-encoding.\n2.4 From the Perspective of Variational Autoencoder\nThe BEIT pre-training can be viewed as variational autoencoder [KW14] training. Let xdenote the\noriginal image, Àúxthe masked image, and zthe visual tokens. Considering the evidence lower bound\n(ELBO) of the log-likelihood p(x|Àúx), i.e., recovering the original image from its corrupted version:\n‚àë\n(xi,Àúxi)‚ààD\nlog p(xi|Àúxi) ‚â•\n‚àë\n(xi,Àúxi)‚ààD\n(\nEzi‚àºqœÜ(z|xi)[log pœà(xi|zi)]\nÓ¥ô Ó¥òÓ¥ó Ó¥ö\nVisual Token Reconstruction\n‚àíDKL[qœÜ(z|xi),pŒ∏(z|Àúxi)]\n)\n(2)\nwhere (1) qœÜ(z|x) denotes the image tokenizer that obtains visual tokens; (2) pœà(x|z) decodes the\noriginal image given input visual tokens; (3) pŒ∏(z|Àúx) recovers the visual tokens based on the masked\nimage, which is our MIM pre-training task.\nWe learn the model following a two-stage procedure similar to [vdOVK17, RvdOV19]. In the Ô¨Årst\nstage, we obtain the image tokenizer as a discrete variational autoencoder [RPG+21]. SpeciÔ¨Åcally,\nthe Ô¨Årst stage minimizes the reconstruction loss ‚àíEzi‚àºqœÜ(z|xi)[log pœà(xi|zi)] with an uniform prior\nas described in Equation (2). In the second stage, we learn the prior pŒ∏ while keeping qœÜ and\npœà Ô¨Åxed. We simplify qœÜ(z|xi) to a one-point distribution with the most likely visual tokens\nÀÜzi = arg maxzqœÜ(z|xi). Then Equation (2) can be rewritten as:\n‚àë\n(xi,Àúxi)‚ààD\n(\nEzi‚àºqœÜ(z|xi)[log pœà(xi|zi)]\nÓ¥ô Ó¥òÓ¥ó Ó¥ö\nStage 1: Visual Token Reconstruction\n+ log pŒ∏(ÀÜzi|Àúxi)Ó¥ô Ó¥òÓ¥ó Ó¥ö\nStage 2: Masked Image Modeling\n)\n(3)\n4\nwhere the second term is our BEIT pre-training objective.\n2.5 Pre-Training Setup\nThe network architecture of BEIT follows that of ViT-Base [DBK+20] for a fair comparison. We\nuse a 12-layer Transformer with 768 hidden size, and 12 attention heads. The intermediate size of\nfeed-forward networks is 3072. We employ the default 16 √ó16 input patch size. We directly borrow\nthe image tokenizer trained by [RPG+21]. The vocabulary size of visual tokens is 8192.\nWe pretrain BEIT on the training set of ImageNet-1K [ RDS+15], which contains about 1.2M\nimages. Our augmentation policy includes random resized cropping, horizontal Ô¨Çipping, color\njittering [WXYL18]. Notice that we do not use the labels for self-supervised learning. We use the\n224 √ó224 resolution in our experiments. So the input is split to 14 √ó14 image patches, and the same\namount of visual tokens. We randomly mask at most 75 patches (i.e., roughly 40% of total image\npatches).\nThe pre-training runs for about 500k steps (i.e., 800 epochs) with 2k batch size. Adam [LH19] with\nŒ≤1 = 0.9,Œ≤2 = 0.999 is employed for optimization. The learning rate is set to 1.5e-3, with a warmup\nof 10 epochs, and cosine learning rate decay. The weight decay is 0.05. We employ stochastic\ndepth [HSL+16] with a 0.1 rate, and disable dropout. The 500k training steps take about Ô¨Åve days\nusing 16 Nvidia Telsa V100 32GB GPU cards.\nWe Ô¨Ånd that proper initialization is important to stabilize Transformer, especially for large-scale pre-\ntraining. We Ô¨Årst randomly initialize all the parameters within a small range, such as [‚àí0.02,0.02].\nThen, for the l-th Transformer layer, we rescale the output matrices (i.e., the last linear projection\nwithin each sub-layer) of the self-attention module and the feed-forward network by 1‚àö\n2l.\n2.6 Fine-Tuning BE IT on Downstream Vision Tasks\nAfter pre-training BEIT, we append a task layer upon the Transformer, and Ô¨Åne-tune the parameters\non downstream tasks, like BERT. We take image classiÔ¨Åcation and semantic segmentation as examples\nin our work. It is straightforward to leverage the pre-training-then-Ô¨Åne-tuning paradigm on other\nvision tasks with BEIT.\nImage classiÔ¨Åcation. For image classiÔ¨Åcation tasks, we directly employ a simple linear clas-\nsiÔ¨Åer as the task layer. SpeciÔ¨Åcally, we use average pooling to aggregate the representa-\ntions, and feed the global to a softmax classiÔ¨Åer. The category probabilities are computed\nas softmax(avg({hL\ni }N\ni=1Wc)), where hL\ni is the Ô¨Ånal encoding vector of the i-th image patch,\nWc ‚ààRD√óC is a parameter matrix, and Cis the number of labels. We maximize the likelihood of\nlabeled data by updating the parameters of BEIT and the softmax classiÔ¨Åer.\nSemantic segmentation. For semantic segmentation, we follow the task layer used in SETR-\nPUP [ZLZ+20]. To be speciÔ¨Åc, we use pretrained BEIT as a backbone encoder, and incorporate\nseveral deconvolution layers as decoder to produce segmentation. The model is also end-to-end\nÔ¨Åne-tuned similar to image classiÔ¨Åcation.\nIntermediate Ô¨Åne-tuning. After self-supervised pre-training, we can further train BEIT on a data-\nrich intermediate dataset (i.e., ImageNet-1K in our work), and then Ô¨Ånetune the model on the target\ndownstream tasks. Such intermediate Ô¨Åne-tuning is the common practice of BERT Ô¨Åne-tuning in\nNLP [PPL+20]. We directly follow the method for BEIT.\n3 Experiments\nWe conduct full Ô¨Åne-tuning experiments on image classiÔ¨Åcation and semantic segmentation. Moreover,\nwe present various ablation studies for pre-training and analyze the representations learned by BEIT.\nWe also report linear probes on ImageNet in Appendix D.\n5\n3.1 Image ClassiÔ¨Åcation\nThe image classiÔ¨Åcation task classiÔ¨Åes input images to various categories. We evaluate BEIT on the\nILSVRC-2012 ImageNet dataset [RDS+15] with 1k classes and 1.3M images. We directly follow\nthe most of hyperparameters of DeiT [TCD+20] in our Ô¨Åne-tuning experiments for a fair comparison.\nWe reduce Ô¨Åne-tuning epochs compared with training from scratch, as BEIT has been pre-trained.\nAccordingly, we use a larger learning rate with layer-wise decay. The detailed hyperparameters are\nsummarized in Appendix H.\nTable 1 reports top-1 accuracy on image classiÔ¨Åcation. We compare BEIT with vision Transformers\ntrained by random initialization, supervised pre-training, and previous self-supervised learning\nmethods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is\nconducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google‚Äôs\nin-house 300M images.\nCompared with the models trained by random initialization, we Ô¨Ånd that pre-trained BEIT signiÔ¨Å-\ncantly improves performance on both datasets. BEIT improves the performance on ImageNet, which\nshows the effectiveness under the rich-resource setting.\nMoreover, we compare BEIT with previous state-of-the-art self-supervised methods for Transformer,\nsuch as DINO [CTM+21], and MoCo v3 [ CXH21]. Our proposed method outperforms previous\nmodels on ImageNet Ô¨Åne-tuning. Among them, iGPT-1.36B [CRC+20] uses much more parameters\n(i.e., 1.36B vs 86M), and ViT-JFT300M [DBK+20] is pretrained on larger corpus (i.e., 300M vs\n1.3M), while others pretrain ViT-Base on ImageNet-1K. iGPT-1.36B and ViT-JFT300M are the\nmost comparable methods, which also follows auto-encoding pre-training for vision Transformer.\nSpeciÔ¨Åcally, iGPT uses clustered image tokens as both input and output for image GPT or image\nBERT. In contrast, we use image patches as input to preserve raw pixels, and employ discrete visual\ntokens as a prediction bottleneck. ViT-JFT300 predicts the mean, 3-bit color of each masked patch,\nrather than visual tokens learned by discrete V AE. We also pretrain the self-supervised tasks ofBEIT\nand DINO in a multi-task learning manner, which is presented in Appendix E.\nIn addition, we evaluate our proposed method with intermediate Ô¨Åne-tuning. In other words, we Ô¨Årst\npretrain BEIT in a self-supervised manner, and then Ô¨Åne-tune the pretrained model on ImageNet with\nlabeled data. The results show that BEIT is complementary to supervised pre-training, achieving\nadditional gain after intermediate Ô¨Åne-tuning on ImageNet.\nFine-tuning to 384 √ó384 resolution. After Ô¨Åne-tuning with resolution 224 √ó224, we additionally\nÔ¨Åne-tune the model on384√ó384 images by 10 more epochs. We follow the standard higher-resolution\nsetting of DeiT [TCD+20], except using fewer epochs. Notice that we keep patch size the same for\nboth 224 √ó224 and 384 √ó384 images. So the input sequence length of Transformers becomes longer\nfor higher resolutions. Table 1 shows that higher resolution improves the BEIT results by 1+ points\non ImageNet. More importantly, BEIT384 pretrained on ImageNet-1K even outperforms supervised\npre-training ViT384 that uses ImageNet-22K, when they use the same input resolution.\nScaling up to larger size. We further scale up BEIT to the large size (same as ViT-L). As shown in\nTable 1, ViT384-L is worse than ViT384 on ImageNet, when training from scratch. The results veriÔ¨Åes\nthe data-hungry issue of vision Transformers. Supervised pre-training on ImageNet-22K partially\nrelieves the issue, where ViT384-L Ô¨Ånally outperforms ViT384 by 1.2. In comparison, BEIT-L is\nbetter than BEIT by 2.0, and BEIT384-L outperforms BEIT384 by 1.7. In other words, the beneÔ¨Åts\nof scaling up BEIT from base to large are greater than supervised pre-training with ImageNet-22K.\nMore importantly, comparing between BEIT384 with ViT384 that conducts supervised pre-training\non ImageNet-22K, the improvements of BEIT become greater along with scaling the size from base\n(i.e., 0.6) to large (i.e., 1.1). The results suggest that BEIT tends to help more for extremely larger\nmodels (such as 1B, or 10B), especially when labeled data are insufÔ¨Åcient3 to conduct supervised\npre-training4 for such large models.\n3[ZKHB21] report that supervised pre-training of a 1.8B-size vision Transformer requires billions of labeled\nimages.\n4Appendix B shows that BEIT Ô¨Åne-tuned on ImageNet-22K (14M) can match the performance of supervised\npre-training on Google‚Äôs in-house JFT-3B [ZKHB21], while using 214x less labels. We also demonstrate that\nlarge-size BEIT Ô¨Åne-tuned on 70M labeled images can achieve 89.5% top-1 accuracy on ImageNet and 58.4%\nmIoU on ADE20K, creating new state-of-the-art results for large-size vision Transformers.\n6\nModels Model Size Resolution ImageNet\nTraining from scratch (i.e., random initialization)\nViT384-B [DBK+20] 86M 3842 77.9\nViT384-L [DBK+20] 307M 3842 76.5\nDeiT-B [TCD+20] 86M 2242 81.8\nDeiT384-B [TCD+20] 86M 3842 83.1\nSupervised Pre-Training on ImageNet-22K (using labeled data)\nViT384-B [DBK+20] 86M 3842 84.0\nViT384-L [DBK+20] 307M 3842 85.2\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled data)\niGPT-1.36B‚Ä†[CRC+20] 1.36B 2242 66.5\nViT384-B-JFT300M‚Ä°[DBK+20] 86M 3842 79.9\nMoCo v3-B [CXH21] 86M 2242 83.2\nMoCo v3-L [CXH21] 307M 2242 84.1\nDINO-B [CTM+21] 86M 2242 82.8\nBEIT-B (ours) 86M 2242 83.2\nBEIT384-B (ours) 86M 3842 84.6\nBEIT-L (ours) 307M 2242 85.2\nBEIT384-L (ours) 307M 3842 86.3\nTable 1: Top-1 accuracy on ImageNet-1K. We evaluate base- (‚Äú-B‚Äù) and large-size (‚Äú-L‚Äù) models at\nresolutions 224 √ó224 and 384 √ó384. ‚Ä†: iGPT-1.36B contains 1.36 billion parameters, while others\nare base-size models. ‚Ä°: ViT384-B-JFT300M is pretrained with the ‚Äúmasked patch prediction‚Äù task\non Google‚Äôs in-house300M images, while others use ImageNet.\n50 100 150 200 250 300\nEpochs\n60\n65\n70\n75\n80T op-1 Acc.\nDeiT (Training from scratch)\nBEiT (Fine-tuning)\nTable 2: Convergence curves of training\nDeiT from scratch and Ô¨Åne-tuning BEIT on\nImageNet-1K.\nModels ADE20K\nSupervised Pre-Training on ImageNet 45.3\nDINO [CTM+21] 44.1\nBEIT (ours) 45.6\nBEIT + Intermediate Fine-Tuning (ours) 47.7\nTable 3: Results of semantic segmentation on\nADE20K. We use SETR-PUP [ZLZ+20] as the task\nlayer and report results of single-scale inference.\nConvergence curves. Figure 2 compares the convergence curves of the training-from-scratch and\npre-training-then-Ô¨Åne-tuning paradigms. We Ô¨Ånd that Ô¨Åne-tuning BEIT not only achieves better\nperformance, but also converging much faster than training DeiT from scratch. Moreover, Ô¨Åne-tuning\nBEIT can reach reasonable numbers within very few epochs.\n3.2 Semantic Segmentation\nSemantic segmentation aims to predict a corresponding class for each pixel of the input image. We\nevaluate BEIT on the ADE20K benchmark [ZZP+19] with 25K images and 150 semantic categories.\nWe report the metric of mean Intersection of Union (mIoU) averaged over all semantic categories. As\npresented in Section 2.6, we directly follow the task layer and the most of hyperparameters described\nin SETR-PUP [ZLZ+20]. On ADE20K, we use Adam [LH19] as the optimizer. The learning rate is\nset to 1e-3 with layer-wise decay similar to image classiÔ¨Åcation. We conduct Ô¨Åne-tuning for 160K\nsteps. The batch size is 16. The detailed hyperparameters are described in Appendix I.\nAs shown in Table 3, we compare BEIT with supervised pre-training that relies on labeled data\nof ImageNet. We Ô¨Ånd that our proposed method achieves better performance than supervised pre-\ntraining, although BEIT does not require manual annotations for pre-training. Moreover, we employ\n7\nModels ImageNet ADE20K\nBEIT (300 Epochs) 82.86 44.65\n‚àíBlockwise masking 82.77 42.93\n‚àíVisual tokens (i.e., recover masked pixels) 81.04 41.38\n‚àíVisual tokens ‚àíBlockwise masking 80.50 37.09\n+ Recover 100% visual tokens 82.59 40.93\n‚àíMasking + Recover 100% visual tokens 81.67 36.73\nPretrain longer (800 epochs) 83.19 45.58\nTable 4: Ablation studies for BEIT pre-training on image classiÔ¨Åcation and semantic segmentation.\nintermediate Ô¨Åne-tuning for BEIT on ImageNet, i.e., we Ô¨Årst Ô¨Åne-tune pretrained BEIT on ImageNet,\nand then Ô¨Åne-tune the model on ADE20K. The results indicate that intermediate Ô¨Åne-tuning further\nimproves BEIT on semantic segmentation.\n3.3 Ablation Studies\nWe conduct ablation studies to analyze the contributions of each component in BEIT. The models\nare evaluated on image classiÔ¨Åcation (i.e., ImageNet) and semantic segmentation (i.e., ADE20K). We\nset the default pre-training steps to 300 epochs for the ablation studies, which is 37.5% of the total\nsteps used in the previous experiments.\nTable 4 reports the results of various model variants. First, we ablate blockwise masking by randomly\nsample masked positions. We Ô¨Ånd that blockwise masking is beneÔ¨Åcial on both tasks, especially on\nsemantic segmentation. Second, we ablate the usage of visual tokens by predicting the raw pixels of\nmasked patches, i.e., the pre-training task becomes a pixel regression problem to recover masked\npatches. Our proposed masked image modeling task signiÔ¨Åcantly outperforms naive pixel-level\nauto-encoding. Compared with the results in Table 1, the ablation result is worse than training vision\nTransformer from scratch on two tasks. The results indicate that the prediction of visual tokens is the\nkey ingredient of BEIT. Third, we ablate the usage of visual tokens and blockwise masking together.\nWe Ô¨Ånd that blockwise masking is even more helpful for pixel-level auto-encoding, which relieves\nthe suffering of short-distance dependency. Forth, recovering all the visual tokens harms performance\non downstream tasks. Fifth, we compare BEIT with different training steps. Pre-training the model\nlonger can further improve performance on downstream tasks.\n3.4 Analysis of Self-Attention Map\nWe show that the self-attention mechanism inBEIT can separate objects, even though our pre-training\ndoes not rely on any manual annotation at all. Similar properties are also observed by [CTM+21].\nThe probing images are taken from the MS COCO [ LMB+14] corpus to avoid appearing in the\npre-training data.\nAs shown in Figure 2, we plot the self-attention map for different reference points within an image.\nThe visualizations are produced by attention scores computed via query-key product in the last layer.\nFor each reference point, we use the corresponding patch as query, and show which patch it attends\nto. After pre-training, BEIT learns to distinguish semantic regions using self-attention heads, without\nany task-speciÔ¨Åc supervision. The property partially indicates the reason why BEIT is able to help\ndownstream tasks. Such knowledge acquired by BEIT potentially improves the generalization ability\nof Ô¨Åne-tuned models, especially on small-scale datasets.\n4 Related Work\nSelf-supervised visual representation learning. Various methods have been introduced over the\nyears to pretrain vision models in a self-supervised manner. Pioneering works design clever pretext\ntasks, such as predicting the patch orderings [NF16], colorization [ZIE16], and predicting rotation\nangles [KG18]. In addition, [TLL19] propose to mask some patches within an image, and classify\nwhether the masked patches are real or fake for each masked position. The method is similar to the\n8\nFigure 2: Self-attention map for different reference points. The self-attention mechanism in BEIT is\nable to separate objects, although self-supervised pre-training does not use manual annotations.\nmasked version of Jigsaw pre-training [ NF16]. The recent strand of research follows contrastive\nparadigm [WXYL18, OLV18, HFLM+19, BHB19, HFW+20, CKNH20, CFGH20]. The models\ntypically regard various data augmentations as different views of an image, and then make the\nrepresentations of positive pairs similar while pushing negative pairs away. In order to obtain enough\ninformative negative samples in contrastive learning, the methods usually rely on large memory\nbanks [WXYL18, HFW+20] or large batch size [CKNH20]. BYOL [GSA+20] and SimSiam [CH20]\nfurther eliminate the requirement of negative samples, using various techniques to avoid representation\ncollapse. Another strand of methods use clustering to organize image examples [CBJD18, ARV20,\nCMM+20, LZXH21].\nSelf-supervised vision Transformers. Pre-training vision Transformers has received signiÔ¨Åcant\nattention recently due to the data-hungry issue. iGPT [ CRC+20] Ô¨Årst creates a 9-bit color palette\nby k-means clustering RGB pixels, and then uses the clustered tokens to represent images. Next\niGPT uses the tasks of BERT and GPT to pretrain Transformers. In comparison, our proposed\nmethod uses image patches as input without losing pixel-level information. Moreover, our visual\ntokens are obtained by discrete V AE instead of clustering. ViT [DBK+20] conducts a preliminary\nexploration with the masked patch prediction task, which predicts the 3-bit mean color of the masked\npatches. [ DBK+20] also report that pixel-level auto-encoding performs worse, although it is the\nmost straightforward translation of BERT from NLP to CV . Rather than using heuristically designed\npre-training tasks, our proposed model leverages visual tokens learned by discrete V AE, which not\nonly achieves better performance but also is better theoretically motivated. Apart from masked\nauto-encoding, other mainstream research works use contrastive learning [CXH21, XLY+21], and\nself-distillation [CTM+21]. In comparison, BEIT can achieve several times of improvement in terms\nof pre-training throughput (Appendix E), and memory consumption. The advantages make BEIT\nappealing to scale up vision Transformers.\n5 Conclusion\nWe introduce a self-supervised pre-training framework for vision Transformers, achieving strong\nÔ¨Åne-tuning results on downstream tasks, such as image classiÔ¨Åcation, and semantic segmentation.\nWe show that the proposed method is critical to make BERT-like pre-training (i.e., auto-encoding\nwith masked input) work well for image Transformers. We also present the intriguing property\nof automatically acquired knowledge about semantic regions, without using any human-annotated\ndata. In the future, we would like to scale up BEIT pre-training in terms of data size and model\n9\nsize. Moreover, we will conduct multimodal pre-training in a more uniÔ¨Åed way, using the similar\nobjectives and the shared architecture for texts and images.\nAcknowledgement We would like to acknowledge Yue Cao, Han Hu, Hang Hua, Jingdong\nWang, Zheng Zhang for the helpful discussions, and Yaru Hao for some analysis experiments\nusing [HDWX20].\nReferences\n[ARV20] Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simulta-\nneous clustering and representation learning. In International Conference on Learning\nRepresentations (ICLR), 2020.\n[BDW+20] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang,\nJianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-\nmasked language models for uniÔ¨Åed language model pre-training. In Proceedings of\nthe 37th International Conference on Machine Learning, ICML 2020, volume 119 of\nProceedings of Machine Learning Research, pages 642‚Äì652. PMLR, 2020.\n[BHB19] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations\nby maximizing mutual information across views. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates, Inc., 2019.\n[CBJD18] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clus-\ntering for unsupervised learning of visual features. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 132‚Äì149, 2018.\n[CFGH20] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with\nmomentum contrastive learning. preprint arXiv:2003.04297, 2020.\n[CH20] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning.\npreprint arXiv:2011.10566, 2020.\n[CKNH20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A\nsimple framework for contrastive learning of visual representations. preprint\narXiv:2002.05709, 2020.\n[CMM+20] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and\nArmand Joulin. Unsupervised learning of visual features by contrasting cluster as-\nsignments. In Advances in Neural Information Processing Systems, volume 33, pages\n9912‚Äì9924. Curran Associates, Inc., 2020.\n[CRC+20] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and\nIlya Sutskever. Generative pretraining from pixels. In Hal Daum√© III and Aarti\nSingh, editors, Proceedings of the 37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning Research, pages 1691‚Äì1703. PMLR,\n13‚Äì18 Jul 2020.\n[CTM+21] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bo-\njanowski, and Armand Joulin. Emerging properties in self-supervised vision transform-\ners. arXiv preprint arXiv:2104.14294, 2021.\n[CXH21] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-\nsupervised vision transformers. ArXiv, abs/2104.02057, 2021.\n[DBK+20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\nat scale. preprint arXiv:2010.11929, 2020.\n10\n[DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-\ntraining of deep bidirectional transformers for language understanding. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, pages 4171‚Äì4186. Association\nfor Computational Linguistics, 2019.\n[GSA+20] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre H Richemond,\nElena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R√©mi Munos, and Michal Valko.\nBootstrap your own latent: A new approach to self-supervised learning. In NeurIPS,\n2020.\n[HDWX20] Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting\ninformation interactions inside Transformer. arXiv preprint arXiv:2004.11207, 2020.\n[HFLM+19] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bach-\nman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual\ninformation estimation and maximization. In International Conference on Learning\nRepresentations, 2019.\n[HFW+20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum\ncontrast for unsupervised visual representation learning. In CVPR, 2020.\n[HSL+16] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep\nnetworks with stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max\nWelling, editors,Computer Vision ‚Äì ECCV 2016, pages 646‚Äì661, Cham, 2016. Springer\nInternational Publishing.\n[JCL+20] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and\nOmer Levy. SpanBERT: Improving pre-training by representing and predicting spans.\nTransactions of the Association for Computational Linguistics, 8:64‚Äì77, 2020.\n[JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-\nsoftmax. In 5th International Conference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,\n2017.\n[KG18] Nikos Komodakis and Spyros Gidaris. Unsupervised representation learning by pre-\ndicting image rotations. In International Conference on Learning Representations\n(ICLR), 2018.\n[KH09] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images.\nMaster‚Äôs thesis, Department of Computer Science, University of Toronto, 2009.\n[KW14] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd\nInternational Conference on Learning Representations, ICLR 2014, 2014.\n[LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Interna-\ntional Conference on Learning Representations, 2019.\n[LLC+21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted\nwindows. arXiv preprint arXiv:2103.14030, 2021.\n[LMB+14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr Doll√°r, and C Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In European conference on computer vision, pages 740‚Äì755. Springer, 2014.\n[LSB+21] Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, and Marco De Nadai.\nEfÔ¨Åcient training of visual transformers with small datasets. In Thirty-Fifth Conference\non Neural Information Processing Systems, 2021.\n11\n[LZXH21] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive\nlearning of unsupervised representations. In International Conference on Learning\nRepresentations, 2021.\n[MMT17] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete Distribution: A\nContinuous Relaxation of Discrete Random Variables. In International Conference on\nLearning Representations, 2017.\n[NF16] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations\nby solving jigsaw puzzles. In European conference on computer vision, pages 69‚Äì84.\nSpringer, 2016.\n[OLV18] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\ncontrastive predictive coding. preprint arXiv:1807.03748, 2018.\n[PPL+20] Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang,\nRichard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman.\nIntermediate-task transfer learning with pretrained language models: When and why\ndoes it work? In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Linguistics, July 2020.\n[RDS+15] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C\nBerg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015.\n[RPG+21] A. Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Rad-\nford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv,\nabs/2102.12092, 2021.\n[RSR+20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a uniÔ¨Åed text-to-text transformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67, 2020.\n[RvdOV19] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-Ô¨Ådelity\nimages with VQ-V AE-2. In Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\n[SHB16] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of\nrare words with subword units. In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 1715‚Äì1725,\nBerlin, Germany, August 2016. Association for Computational Linguistics.\n[TCD+20] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv√© J√©gou. Training data-efÔ¨Åcient image transformers & distillation\nthrough attention. preprint arXiv:2012.12877, 2020.\n[TCS+21] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√©\nJ√©gou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[TLL19] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. SelÔ¨Åe: Self-supervised pretraining\nfor image embedding. arXiv preprint arXiv:1906.02940, 2019.\n[vdOVK17] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete repre-\nsentation learning. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, NIPS‚Äô17, page 6309‚Äì6318, Red Hook, NY , USA,\n2017. Curran Associates Inc.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle\nGuyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N.\nVishwanathan, and Roman Garnett, editors,Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages 5998‚Äì6008, 2017.\n12\n[WXYL18] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature\nlearning via non-parametric instance discrimination. In CVPR, 2018.\n[XLY+21] Zhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu.\nSelf-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553,\n2021.\n[XLZ+18] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. UniÔ¨Åed perceptual\nparsing for scene understanding. In ECCV, 2018.\n[ZIE16] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In\nECCV, 2016.\n[ZKHB21] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision\ntransformers. arXiv preprint arXiv:2106.04560, 2021.\n[ZLZ+20] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,\nYanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking\nsemantic segmentation from a sequence-to-sequence perspective with transformers.\nCoRR, abs/2012.15840, 2020.\n[ZZP+19] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and\nAntonio Torralba. Semantic understanding of scenes through the ADE20K dataset. Int.\nJ. Comput. Vis., 127(3):302‚Äì321, 2019.\n13\nA Architecture Variants of Vision Transformer\nWe use the standard vision Transformer (ViT) in the experiments for fair comparisons. In addition,\nwe Ô¨Ånd that LayerScale [TCS+21] and relative position bias [BDW+20, RSR+20] improve ViTs on\ndownstream tasks. We employ the same setting as in Section 3.3 for ablation studies, which pretrains\nbase-size models for 300 epochs on ImageNet-1K.\nAs shown in Table 5, both LayerScale and relative position bias improve performance on ImageNet\nclassiÔ¨Åcation and ADE20K semantic segmentation. We denote the improved architecture as BEIT+\nand use it for the experiments in Appendix B. We empirically notice that vanilla Transformer is the\nmost stable when scaling up the model to billions of parameters, so we do not use LayerScale for\nextra-large models.\nArchitecture ImageNet ADE20K\nViT (used in this paper) 82.86 44.86\nViT+LayerScale 83.00 45.43\nViT+LayerScale+Relative Position Bias 83.22 45.70\nTable 5: Ablation studies of architecture variants on image classiÔ¨Åcation and semantic segmentation.\nFor ADE20K, we use UperNet [XLZ+18] as the task layer, and report mIoU scores of single-scale\ninference.\nB Comparison with Large-Scale Supervised Pre-Training\nWe compare with state-of-the-art supervised pre-training at scale. In addition to using ImageNet-1K\nfor fair comparisons with previous work, we pretrain BEIT on ImageNet-22K to boost performance.\nWe employ the architecture improvements (i.e., LayerScale, and relative position bias) as described\nin Appendix A, which is denoted as BEIT+ in Table 6 and Table 7. We follow the same pre-training\nsetup as in Section 2.5, except we pretrain 150 epochs on ImageNet-22K. After self-supervised\npre-training, we conduct intermediate Ô¨Åne-tuning on ImageNet-22K for 90 epochs. Moreover, we use\nan in-house dataset that has about 70M labeled images as a drop-in replacement of ImageNet-22K.\nModels Model\nSize\nLabeled\nData Size\nImageNet\n3842 5122\nSupervised Pre-Training on ImageNet-22K (using labeled data)\nViT-B [DBK+20] 86M 14M 84.0 -\nViT-L [DBK+20] 307M 14M 85.2 85.30\nViT-H [DBK+20] 632M 14M 85.1 -\nSupervised Pre-Training on Google JFT-300M (using labeled data)\nViT-B [DBK+20] 86M 300M 84.2 -\nViT-L [DBK+20] 307M 300M 87.1 87.76\nViT-H [DBK+20] 632M 300M 88.0 88.55\nSupervised Pre-Training on Google JFT-3B (using labeled data)\nViT-B [ZKHB21] 86M 3000M 86.6 -\nViT-L [ZKHB21] 307M 3000M 88.5 -\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K\nBEIT-B+ (ours) 86M 14M 86.8 -\nBEIT-L+ (ours) 307M 14M 88.4 88.6\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M\nBEIT-L+ (ours) 307M 70M 89.3 89.5\nTable 6: Top-1 accuracy on ImageNet-1K Ô¨Åne-tuning. We evaluate models at resolutions3842 and\n5122.\n14\nTable 6 compares BEIT with previous state-of-the-art supervised pre-training [DBK+20, ZKHB21]\non ImageNet Ô¨Åne-tuning. Rather than heavily relying on extremely large-size labeled data (such as\nGoogle‚Äôs in-house JFT-300M and JFT-3B), we demonstrate thatBEIT pre-training can catch up with\nonly ImageNet-22k (14M). SpeciÔ¨Åcally, BEIT-L Ô¨Åne-tuned on ImageNet-22K achieves comparable\nperformance with ViT-L trained on Google JFT-3B. Moreover,BEIT-L obtains 89.5% top-1 accuracy\non ImageNet after intermediate Ô¨Åne-tuning on an in-house 70M dataset. The results indicate that\nBEIT pre-training greatly reduces the required labeling efforts and advances the new state of the art\nfor large-size vision Transformers.\nAs shown in Table 7, we report the Ô¨Åne-tuning results on the ADE20K semantic segmentation\nbenchmark. Following Swin [LLC+21], we use the same task layer (i.e., UperNet) and evaluate the\nmodels at the resolution 640 √ó640. The BEIT-L model obtains state-of-the-art performance on\nADE20K.\nModels mIoU ( %) Multi-Scale mIoU ( %)\nSupervised Pre-Training on ImageNet-22K (using labeled data)\nSwin-B [LLC+21] 50.0 51.7\nSwin-L [LLC+21] 52.1 53.5\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-22K\nBEIT-B+ (ours) 53.6 54.2\nBEIT-L+ (ours) 56.7 57.0\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on In-House-70M\nBEIT-L+ (ours) 57.9 58.4\nTable 7: Performance comparison on the ADE20K semantic segmentation. We follow Swin-\nL [LLC+21] to use UperNet [XLZ+18] as the task layer and evaluate at resolution 640 √ó640.\nC Ablation Studies of Image Tokenizer\nFor comparison, we re-train the image tokenizer on ImageNet-1K. The reimplementation is based\non https://github.com/lucidrains/DALLE-pytorch. We use the same codebook size 8K as\nin DALL-E [RPG+21]. Then we plug the tokenizer into our pre-training process. We follow the\nsame experimental setup of ablation studies as in Section 3.3. Table 8 shows that our reimplemented\ntokenizer obtains comparable reconstruction loss and ImageNet Ô¨Åne-tuning performance compared\nwith the off-the-shelf DALL-E tokenizer.\nImage Tokenizer Reconstruction Error ImageNet\nDALL-E Tokenizer [RPG+21] 0.0856 82.86\nOur reimplementation 0.0880 82.70\nTable 8: Top-1 accuracy on ImageNet-1K using different image tokenizers during pre-training. For\nimage reconstruction, we report mean absolute error of normalized RGB values. The reimplemented\nimage tokenizer is trained on ImageNet-1K without labels.\nD Linear Probes on ImageNet\nWe evaluate linear probes on ImageNet for various pretrained vision Transformers. We compare\nBEIT with two main strands of work, namely discriminative and generative self-supervised learning.\nThe Ô¨Årst one applies discriminative learning for pre-training, such as contrastive learning [CXH21],\nand self distillation [ CTM+21]. The above methods typically learn to aggregate the image-level\nfeatures into a global vector, which is relatively suitable for linear probing. In contrast, the second\nstrand of methods, such as iGPT [ CRC+20] and ours, usually do not pretrain such global feature\naggregation, which tends to make linear probes difÔ¨Åcult.\nFollowing iGPT [CRC+20], we use average pooling to aggregate the hidden states of each image\npatches, and add the probing layer at the middle layer of Transformer instead of always at the Ô¨Ånal\n15\nlayer. Similarly, we Ô¨Ånd that the best layer lies in 9-th layer for BEIT-B, and 14-th layer for BEIT-L.\nTo be speciÔ¨Åc, we use AdamW [LH19] to update the linear probe layer for 50 epochs. The learning\nrate is 4e-3 with cosine decay. The batch size is 1024. The weight decay is set to 1e-4. We follow\ndata augmentation used in DINO [CTM+21], which uses random resize crops and horizontal Ô¨Çips\naugmentation during training and evaluates on central crops.\nModels Model Size Accuracy\nDiscriminative self-supervised learning\nDINO-B [CTM+21] 86M 78.2\nMoCo v3-B [CXH21] 86M 76.7\nMoCo v3-L [CXH21] 307M 77.6\nGenerative self-supervised learning\niGPT-L [CRC+20] 1362M 65.2\niGPT-XL [CRC+20] 6801M 68.7\niGPT-XL [CRC+20] 6801M 72.0 ‚àó\nBEIT-B (ours) 86M 56.7\nBEIT-L (ours) 307M 73.5\nTable 9: Linear probing accuracy on ImageNet. ‚Äú ‚àó‚Äù denotes that iGPT-XL uses concatenation of Ô¨Åve\nlayers for linear probing, while others use the features of single layer.\nAs shown in Table 9, we evaluate linear probes on ImageNet-1K for self-supervised learning. Overall,\ndiscriminative methods perform better than generative pre-training on linear probing. Linear probes\nkeep the Transformer parameters Ô¨Åxed and only update the linear layer. So the pre-training of global\naggregation of image-level features is beneÔ¨Åcial to linear probing in DINO and MoCo v3, although\nfull Ô¨Åne-tuning eliminates the gap. Moreover, the results indicate that increasing the model size from\nbase (86M) to large (304M) signiÔ¨Åcantly improves accuracy for our proposed method. In contrast,\nthe gap between base- and large-size MoCo v3 is smaller. We also Ô¨Ånd that BEIT outperforms iGPT\nby a large margin even using much fewer parameters.\nE Multi-Task Pre-Training with DINO\nWe train the pre-training tasks of BEIT and DINO [CTM+21] together in a multi-task manner. As\nshown in Table 10, augmenting masked image modeling with DINO improves semantic segmentation\non ADE20K, and obtains comparable results on ImageNet classiÔ¨Åcation. Moreover, BEIT is more\nefÔ¨Åcient in terms of pre-training speed, as DINO has two copies of Transformer parameters for\nself-distillation and multi-crop augmentation [CMM+20]. For the throughput comparisons between\nBEIT and BEIT+DINO, we set batch size to the same. Because BEIT is also more memory-efÔ¨Åcient,\nwe can use larger batch size to fully utilize GPU cards, which obtains greater speedup in practice\nthan the reported numbers.\nModels ImageNet ADE20K Pre-Training Throughput\nDINO (400 Epochs) 82.8 44.08 -\nBEIT (300 Epochs) 82.9 44.65 4.2x\nBEIT + DINO (300 Epochs) 82.9 46.85 1.0x\nTable 10: We train the pre-training tasks of BEIT and DINO [CTM+21] in the way of multi-task\nlearning. We report the performance by Ô¨Åne-tuning on ImageNet-1K image classiÔ¨Åcation and\nADE20K semantic segmentation. For ADE20K, we use SETR-PUP [ZLZ+20] as the task layer and\nreport the mIoU score of single-scale inference. The pre-training throughput measures the speed,\nwhere larger numbers indicate faster pre-training.\n16\nF Image ClassiÔ¨Åcation on CIFAR-100\nIn addition to ImageNet classiÔ¨Åcation, we conduct Ô¨Åne-tuning experiments on the CIFAR-100 [KH09]\nbenchmark with 100 classes and 60k images. The experimental setup is the same as in Section 3.1.\nTable 11 reports the top-1 accuracy on CIFAR-100. Notably, on the smaller CIFAR-100 dataset, ViT\ntrained from scratch only reaches 48.5% accuracy [CXH21]. In comparison, BEIT achieves 90.1%\nwith the help of pre-training. The results indicate that BEIT can greatly reduce the requirement\nof annotation efforts. BEIT also outperforms MoCo v3. Moreover, intermediate Ô¨Åne-tuning on\nImageNet-1K further improves the results on CIFAR-100.\nModels CIFAR-100\nTraining from scratch (i.e., random initialization)\nViT384 [DBK+20] 48.5*\nSupervised Pre-Training on ImageNet-1K (using labeled data)\nViT384 [DBK+20] 87.1\nDeiT [TCD+20] 90.8\nSelf-Supervised Pre-Training on ImageNet-1K (without labeled data)\nDINO [CTM+21] 91.7\nMoCo v3 [CXH21] 87.1\nBEIT (ours) 90.1\nSelf-Supervised Pre-Training, and Intermediate Fine-Tuning on ImageNet-1K\nBEIT (ours) 91.8\nTable 11: Top-1 accuracy of image classiÔ¨Åcation on CIFAR-100. The models are at resolution\n224 √ó224, except ViT384 uses 384 √ó384. The results, unless otherwise indicated, are all obtained\nby base-size models. *: result is taken from [CXH21].\nG Hyperparameters for Pre-Training\nHyperparameters Base Size Large Size\nLayers 12 24\nHidden size 768 1024\nFFN inner hidden size 3072 4096\nAttention heads 12 16\nAttention head size 64\nPatch size 16 √ó16\nTraining epochs 800\nBatch size 2048\nAdam œµ 1e-8\nAdam Œ≤ (0.9, 0.999)\nPeak learning rate 1.5e-3\nMinimal learning rate 1e-5\nLearning rate schedule Cosine\nWarmup epochs 10\nGradient clipping 3.0 1.0\nDropout \u0017\nStoch. depth 0.1\nWeight decay 0.05\nData Augment RandomResizeAndCrop\nInput resolution 224 √ó224\nColor jitter 0.4\nTable 12: Hyperparameters for pre-training BE IT on ImageNet-1K.\n17\nH Hyperparameters for Image ClassiÔ¨Åcation Fine-Tuning\nHyperparameters CIFAR-100 ImageNet-1K\nBase Size Base Size Large Size\nPeak learning rate {2e-3, 3e-3, 4e-3, 5e-3}\nFine-tuning epochs 150 100 50\nBatch size 512 1024 1024\nWarmup epochs 20 20 5\nLayer-wise learning rate decay 0.65 0.65 0.75\nAdam œµ 1e-8\nAdam Œ≤ (0.9, 0.999)\nMinimal learning rate 1e-6\nLearning rate schedule Cosine\nRepeated Aug \u0013 \u0013 \u0017\nWeight decay 0.3 0.05 0.05\nLabel smoothing Œµ 0.1\nStoch. depth 0.1\nDropout \u0017\nGradient clipping \u0017\nErasing prob. \u0017 0.25 0.25\nInput resolution 224 √ó224\nRand Augment 9/0.5\nMixup prob. 0.8\nCutmix prob. 1.0\nTable 13: Hyperparameters for Ô¨Åne-tuning BE IT on ImageNet-1K and CIFAR-100.\nI Hyperparameters for ADE20K Semantic Segmentation Fine-Tuning\nHyperparameters Base Size\nPeak learning rate 1e-3\nFine-tuning steps 160K\nBatch size 16\nAdam œµ 1e-8\nAdam Œ≤ (0.9, 0.999)\nLayer-wise learning rate decay 0.65\nMinimal learning rate 0\nLearning rate schedule Linear\nWarmup steps 1500\nDropout \u0017\nStoch. depth 0.1\nWeight decay 0.05\nInput resolution 512 √ó512\nPosition embedding interpolate bilinear\nTable 14: Hyperparameters for Ô¨Åne-tuning BE IT on ADE20K.\n18"
}