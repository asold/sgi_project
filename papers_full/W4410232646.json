{
  "title": "Evaluating large language model workflows in clinical decision support for triage and referral and diagnosis",
  "url": "https://openalex.org/W4410232646",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5107554741",
      "name": "Farieda Gaber",
      "affiliations": [
        "Max Delbrück Center",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A5113389909",
      "name": "Maqsood Shaik",
      "affiliations": [
        "Max Delbrück Center"
      ]
    },
    {
      "id": "https://openalex.org/A5047394907",
      "name": "Fabio Allega",
      "affiliations": [
        "Agostino Gemelli University Polyclinic"
      ]
    },
    {
      "id": "https://openalex.org/A4290006404",
      "name": "Agnes Julia Bilecz",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2131432264",
      "name": "Felix Busch",
      "affiliations": [
        "Klinikum rechts der Isar"
      ]
    },
    {
      "id": "https://openalex.org/A2667981281",
      "name": "Kelsey Goon",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2576545573",
      "name": "Vedran Franke",
      "affiliations": [
        "Max Delbrück Center"
      ]
    },
    {
      "id": "https://openalex.org/A2305467968",
      "name": "Altuna Akalin",
      "affiliations": [
        "Max Delbrück Center"
      ]
    },
    {
      "id": "https://openalex.org/A5107554741",
      "name": "Farieda Gaber",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113389909",
      "name": "Maqsood Shaik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5047394907",
      "name": "Fabio Allega",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290006404",
      "name": "Agnes Julia Bilecz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131432264",
      "name": "Felix Busch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2667981281",
      "name": "Kelsey Goon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2576545573",
      "name": "Vedran Franke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2305467968",
      "name": "Altuna Akalin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4399759212",
    "https://openalex.org/W4390876225",
    "https://openalex.org/W4386399156",
    "https://openalex.org/W2418968714",
    "https://openalex.org/W4213248739",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4390311718",
    "https://openalex.org/W4389712789",
    "https://openalex.org/W4386409641",
    "https://openalex.org/W4392099631",
    "https://openalex.org/W4389042140",
    "https://openalex.org/W4390719175",
    "https://openalex.org/W4388376970",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W6922196192",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W6922249992",
    "https://openalex.org/W6884907361",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4395050972",
    "https://openalex.org/W4400955423",
    "https://openalex.org/W4402940475",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4386554921",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4379598302",
    "https://openalex.org/W4403309919",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W6870071885",
    "https://openalex.org/W4392711451"
  ],
  "abstract": "Abstract Accurate medical decision-making is critical for both patients and clinicians. Patients often struggle to interpret their symptoms, determine their severity, and select the right specialist. Simultaneously, clinicians face challenges in integrating complex patient data to make timely, accurate diagnoses. Recent advances in large language models (LLMs) offer the potential to bridge this gap by supporting decision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM versions and an LLM-based workflow incorporating retrieval-augmented generation (RAG) on a curated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care database. Our findings show that these LLMs are capable of providing personalized insights into likely diagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may also support clinicians in refining diagnoses and decision-making, offering a promising approach to improving patient outcomes and streamlining healthcare delivery.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01684-1\nEvaluating large language model\nworkﬂows in clinical decision support for\ntriage and referral and diagnosis\nCheck for updates\nFarieda Gaber1,2,6, Maqsood Shaik1,6, Fabio Allega3, Agnes Julia Bilecz4, Felix Busch5,K e l s e yG o o n4,\nVedran Franke1 &A l t u n aA k a l i n1\nAccurate medical decision-making is critical for both patients and clinicians. Patients often struggle to\ninterpret their symptoms, determine their severity, and select the right specialist. Simultaneously,\nclinicians face challenges in integrating complex patient data to make timely, accurate diagnoses.\nRecent advances in large language models (LLMs) offer the potential to bridge this gap by supporting\ndecision-making for both patients and healthcare providers. In this study, we benchmark multiple LLM\nversions and an LLM-based workﬂow incorporating retrieval-augmented generation (RAG) on a\ncurated dataset of 2000 medical cases derived from the Medical Information Mart for Intensive Care\ndatabase. Ourﬁndings show that these LLMs are capable of providing personalized insights into likely\ndiagnoses, suggesting appropriate specialists, and assessing urgent care needs. These models may\nalso support clinicians in reﬁning diagnoses and decision-making, offering a promising approach to\nimproving patient outcomes and streamlining healthcare delivery.\nClinical decision-making is a fundamentally complex process that relies on\nclinicians applying their knowledge and experience1 while considering\nnumerous factors and integrating vast amounts of data to assess patient\nsymptoms, determine the severity of their condition, and choose the most\nappropriate next steps. This process typically involves combining infor-\nmation from various sources, such as symptoms, vital signs, patient medical\nhistory, and various examinations, to arrive at an accurate and timely\ndiagnosis. The ability to correctly interpret this information and make well-\nf o u n d e dd e c i s i o n si sc r u c i a lf o ri m p r oving patient outcomes. In a saturated\nhealthcare system with increasing amounts and complexity of patient data,\nfewer healthcare professionals face the challenge of meeting increasing\npatient demands for fast, accurate, and personalized care. Especially in high-\npressure environments like emergency departments, the fast pace and\ncomplexity of decision-making can contribute to delays or errors in triaging,\ndiagnosis and treatment, ultimately leading to suboptimal care.\nRecent advancements in large language models (LLMs) have demon-\nstrated signiﬁcant potential to transform variousﬁelds, including clinical\ndecision-support\n2,3. While LLMs have shown promise in structured envir-\nonments, such as medical licensing exams and clinical vignettes4,5,t h e i r\napplication in real-world, open-ended clinical scenarios remains an\nemerging area of research. Powerful models could increase diagnostic\naccuracy, optimize triage processes and improve patient management. For\nexample, LLMs could assist in prioritizing patients based on symptoms and\nvital signs, distinguishing between urgent and non-urgent cases, thereby\nreducing waiting times and improvingcare delivery. This capability is\nespecially crucial in emergency departments (EDs), where accurate triage\n(level of severity of a patient’s condition) assessment is vital for patient\nprioritization. Errors in this process— whether under-triage (assigning\nlower urgency than needed) or over-triage (assigning higher urgency)—\nsigniﬁcantly impact patient outcomes and resource allocation.\nTrauma systems have set the goal to minimize under triage and accept\nah i g h e rr a t eo fo v e rt r i a g et or e d u c emortality rate caused by under triage,\nwith goals set at≤5% and ≤35%\n6, respectively. A review ofﬁeld triage\nperformance showed 14% to 34% under triaged cases across all ages7,w h i c h\ncan result in delayed treatment for patients requiring immediate care,\npotentially worsening their outcomes.On the other hand, over-triage rates\nwere shown to be between 12% and 31%7, leading to the waste of critical\nresources and increased waiting times for other patients. In this context,\nLLMs might mitigate both under-triage and over-triage, thereby improving\nresource allocation and overall patient outcomes.\n1Berlin Institute for Medical Systems Biology (BIMSB), Max Delbrück Center for Molecular Medicine, Berlin, Germany.2Department of Computer Science,\nHumboldt-Universität zu Berlin, Berlin, Germany.3Department of Radiation Oncology, Fondazione Policlinico Universitario Agostino Gemelli, Rome, Italy.\n4Department of Obstetrics and Gynecology/Section of Gynecologic Oncology, University of Chicago, Chicago, IL, USA.5Institute for Diagnostic and Interventional\nRadiology, TUM School of Medicine and Health, TUM University Hospital Rechts der Isar, Technical University of Munich, Munich, Germany.6These authors\ncontributed equally: Farieda Gaber, Maqsood Shaik. e-mail: altuna.akalin@mdc-berlin.de\nnpj Digital Medicine|           (2025) 8:263 1\n1234567890():,;\n1234567890():,;\nBeyond assisting clinicians, LLMs could help patients manage their\nown healthcare decisions. These models have the potential to guide patients\nin interpreting their symptoms, recommend appropriate specialists and\ndetermine the best course of action. However, while the capabilities of LLMs\nare promising, their real-world application in dynamic and unstructured\nclinical environments remains an area of active research and development.\nWhile the scope of current LLM research in healthcare focuses on\ndiagnosing speciﬁc diseases or targeting particular medical specialties,\nwhich are necessary and hold signiﬁcant promise\n8–14, it misses the broader\ntask of predicting diagnoses to support comprehensive clinical decision-\nmaking in more general, fast-paced environments. Other studies employ\nmodels that are required to choose a diagnosis from a simpliﬁed set of binary\nor multiple-choice options testing human competencies within particular\ndomains\n15–18 which reduces the complexity of real-world clinical decision-\nmaking. In practice, clinicians are frequently faced with vague or unclear\nsymptoms, incomplete information, and unlike in controlled studies, they\ndo not have the convenience of selecting from multiple-choice options.\nInstead, they must rely on their clinical judgment and experience to navigate\nuncertainty and arrive at a diagnosis.\nIn this study, we aimed to benchmark multiple LLM workﬂows on\ntheir ability to predict key aspects of clinical care: triage level in the form of\nthe Emergency Severity Index (ESI)\n19, patient to medical specialty referral,\nand diagnosis based on symptoms (also referred to as history of present\nillness), patient information and initial vitals. The workﬂow is illustrated in\nFig. 1. Using a dataset of 2000 real-world cases from the Medical Infor-\nmation Mart for IntensiveCare (MIMIC-IV) database\n20–22,w ee v a l u a t et h e\nperformance of several LLMs, speciﬁcally multiple versions of the Claude\nfamily23,24, as well as a retrieval-augmented generation (RAG) agentic\nworkﬂow designed to mimic the clinical decision-making process.\nThis paper systematically evaluates the potential and limitations of\nthese models in supporting clinicians with complex decision-making,\nshowing promising results in their ability to assist effectively. With the\nincreasing digitization of healthcare, the integration of AI-powered tools\npresents a promising opportunity to enhance clinical workﬂows and\nstreamline patient-centered care. Such advancements will beneﬁtb o t h\nclinicians and patients.\nResults\nCurated MIMIC-ED dataset and model evaluation approach\nWe created a curated dataset using the fully de-identiﬁed MIMIC-IV ED\ndataset22,25, consisting of electronic health records, together with the\nMIMIC-IV Notes22,26 to simulate clinical decision-making in an emergency\ndepartment setting. Both datasets are modules from MIMIC-IV20,22.D e t a i l s\nabout the dataset and the preprocessing can be found in the Methods: Data\nPreprocessing. From the processed data, we extracted 2000 medical cases\ncovering a wide range of medical conditions. Figure2a displays the dis-\ntribution of triage levels in the emergency department (ED), while Fig.2b\nshows the specialties managing these cases, occurring more than 30 times.\nAs expected in the ED, there were few triage level 4 and no triage level 5 cases\n(less severe), with most classiﬁed as triage level 3, followed by triage level 2,\nand a smaller number as triage level 1. This dataset has the advantage of not\nbeing directly publicly available, which makes it ideal for evaluating LLMs\nthat otherwise tend to use publicly available test sets as part of their train-\ning data.\nModel selection and RAG-assisted LLM\nWe tested three models from the Claude family - Claude 3.5 Sonnet, Claude\n3 Sonnet, and Claude 3 Haiku - due to their superior performance across\nmultiple benchmarks, excellingin contextual understanding, efﬁciency, and\nhandling specialized queries23,24 (see Methods: Model Selection for details).\nAdditionally to the stand-aloneLLMs, we developed a retrieval-\naugmented generation (RAG) assisted LLM. Generally, a RAG method\ncombines two components: a retrieversystem that extracts information\nfrom an external domain-speciﬁc knowledge source for the given query and\nan LLM that merges the retrieved context with the input query. The addition\nof extra information reduces hallucinations in the output and to allow for\nmore precise and informed domain speciﬁco u t p u t s\n27.I nt h i ss t u d y ,t h e\nRAG-assisted LLM is used to enhance the performance of Claude 3.5\nSonnet, and utilizes a knowledge base of 30 million PubMed abstracts to\nimprove its output with domain-speciﬁc context. A more in-depth expla-\nnation of the RAG workﬂow used in this paper can be found in the\n“Methods” section.\nDue to privacy regulations surrounding the MIMIC-IV dataset, which\nprohibit its use with external application programming interfaces (APIs)\nlike those provided by OpenAI (e.g., GPT-4), we utilized AWS Privatelink to\nprivately connect to the Claude models supported by AWS services. More\ndetails are provided in the Methods: Model Selection. For each model we\ndifferentiated between two user types: general users, typically patients who\nprovide only personal information and symptoms (referred to as the‘history\nof present illness’in the dataset), and clinicians in the ED, who can addi-\ntionally retrieve initial clinical data, such as temperature, heart rate,\nrespiratory rate, oxygen saturation, and blood pressure. While we recognize\nthat making a deﬁnitive diagnosis requires further input, such as physical\nexams or laboratory tests, our approach seeks to replicate the decision-\nFig. 1 | Workﬂow for clinical decision support using LLM.This workﬂow shows\nusing LLMs in clinical decision support for referral, triage and diagnosis. The\nworkﬂow begins with the input - general user input and clinical user input - followed\nby prompts engineered to predict triage, specialty and diagnosis. The predictions are\ngenerated using the Claude family models and a RAG-assisted Claude 3.5 Sonnet\nmodel. These predictions are then evaluated and compared with the ground truth to\nassess the performance of the LLMs.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 2\nmaking process both for patients feeling ill at home and those arriving at the\nED. This distinction allowed us to explore the capabilities of LLMs in both\nhome settings, where users report symptoms, and ED settings, where pre-\nliminary clinical data is available.\nLLM performance in triage\nIn the context of emergency care, triage or acuity as it is mentioned in the\nMIMIC-IV-ED dataset refers to the severity of a patient’s condition and is\ncommonly assessed using the Emergency Severity Index (ESI)19.T h i s\nstandardized triage tool classiﬁes patients intoﬁve levels based on the\nurgency of their treatment needs, allowing healthcare providers to prioritize\ncare more effectively. The levels range from ESI 1, which indicates patients\nrequiring immediate life-saving interventions, to ESI 5, which represents\ncases where treatment can be safely delayed. The description for each level\ncan be found in the Supplementary Table 1. This classiﬁcation system plays\na crucial role in emergency department operations, helping clinicians to\nallocate resources efﬁciently and address critical cases with minimal delay.\nIn our study, we assess the model’s capabilities to predict patient triage\nlevel for two user scenarios: a general user providing only symptom-based\ninformation and a clinician with additional access to initial clinical data.\nThis evaluation aims to determine whether the models can be effectively\nintegrated into the decision-making process as aﬁrst-pass aid to assist and\nhelp prioritize in triaging patients in the ED in real-time. The speciﬁc\nprompting details used for these cases can be found in the Methods:\nPrompts.\nThe results were assessed based once on exact match accuracy, where\nthe predicted triage level matched the actual value, and a triage range\naccuracy, where predictions were considered correct if they were exactly or\nonly one triage level higher than the actual level, except for the triage level 1,\nwhich has to be predicted as 1. The latter method compensates for the\nvariability between different clinical judgements, arising from personal\nexperience and knowledge, while onlyaccepting triage level assignment if\nthe LLM assigns a one level higher triage level than the ground truth. This\napproach reduces the risks of undertriaging while avoiding to overwhelm\nthe system with exaggerated cautious overtriaged patients.\nModels incorporating vital signs generally performed better in pre-\ndicting the triage level than thoseusing symptoms alone. RAG-Assisted\nLLM showed the highest exact match accuracy in both conditions. The\naddition of clinical data had a modest but positive effect on performance\nacross all models and more recent models outperformed simpler ones.\nUnder the triage range accuracy metric, Claude 3.5 Sonnet out-\nperformed all other models. All the results are presented in Fig.3 and\nTable 1.F i g u r e4 presents the confusion matrices for the two models that\nperformed best in each accuracy evaluation. These matrices provide addi-\ntional insight into the models’behavior. It is important to note that while no\nmodel achieved high accuracy in predicting the most severe triage levels,\nnone of the models confused the most critical cases with the least serious\nones, and vice versa. This is a crucialﬁnding, as it indicates that the models\nhad difﬁculty accurately predicting cases at the extreme ends of triage\nseverity, but they consistently recognized the difference between life-\nthreatening cases and those of lower urgency.\nThe improvement between the general user and clinical user models\ncan be observed in Table2, which shows a performance increase in triage\nlevel prediction across all models. This highlights how the LLM’s predictions\nimprove when provided with more detailed information, similar to how a\nclinician makes more accurate decisions when given initial vitals. However,\nthis improvement is not as apparent in the triage range evaluation. When\nthe model misclassiﬁed the triage level, it is usually within the range of one\nlevel more severe. A slight decline in triage range accuracy was noted across\nmost cases, except for Claude 3 Haiku, which struggled strongly to process\nthe additional information fromthe initial vitals effectively.\nPredicting appropriate medical specialty referrals from\npatient data\nWe aimed to evaluate whether LLMs can assist in the specialty referral\nprocess. Accurate identiﬁcation of the appropriatespecialty for a patient is\ncritical in ensuring they receive the most effective and timely treatment,\nwhich also reduces healthcare costs by minimizing unnecessary referrals.\nSince the MIMIC-IV-ED and MIMIC-IV-Notes datasets don’tc o n t a i n\nexact information on the medical specialist the primary care doctors can\nconsult or refer the patient to, we used Claude 3.5 Sonnet to create a ground\ntruth by predicting the most likely specialist for each of the diagnoses of each\npatient. More details on this process and the used prompt can be found in\nMethods: Prompts. To validate this approach, we asked four clinicians to\nreview a subset of the created ground truth to determine whether the\nassigned ground truth specialties appropriately matched the corresponding\ndiagnoses. Additional information on the results and the acceptability of the\nLLM-generated ground truth among clin i c i a n si sp r o v i d e di nt h es u b s e c t i o n\n“Clinician Validation of LLM-Generated Specialties.” Further details about\nthe clinician evaluation process can be found in the subsection“Reader\nStudy” in the“Methods” section.\nWe evaluated the ability of LLMs to predict specialties in our two\nscenarios, the general user and clinical user models. For each scenario, we\nasked the model to predict the top three specialties that would handle the\nFig. 2 | Triage levels and specialty distributions. aShows the distribution of the triage level with the quantities assigned to each level andb distribution of medical specialties\npredicted from the diagnosis list, reﬂecting the specialties to which patient cases would be referred and consulted.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 3\npatient’s case based on the symptoms and the patient info, for the general\nuser and adding the initial vitals for the clinical user. More insights on the\ntwo evaluation frameworks and the two user scenarios can be read upon in\nthe Methods: Specialty evaluation Framework.\nIn theﬁrst evaluation, which checked each of the top three predicted\nspecialties individually if it matches the specialties in the ground truth,\nClaude 3.5 Sonnet slightly outperformed the other models. However, the\nperformance differences among all models were minimal, with all models\nshowing similar accuracy across both general and clinical user scenarios.\nFor the evaluation that focused on checking if at least one of the top\nthree predicted specialties is predicted correctly, Claude 3.5 Sonnet had the\nhighest performance, while overall performance differences remained small\nacross all models. The results are illustrated in Fig.5 and Table1.\nThe improvement shown in Table2 between the general user and\nclinician user models was most evident in the Claude 3.5 Sonnet model, with\nonly minimal improvements seen in the RAG-assisted LLM and Claude 3\nHaiku. In contrast, Claude 3 Sonnet experienced a negative impact when\nprovided with additional information about the initial vitals. Predicting the\nappropriate specialty relies on several factors. Symptoms need to be clear\nand accurate, but it’sc o m m o nf o rs y m p t o m st of a l lu n d e rt h ee x p e r t i s eo f\nmultiple specialists, and often additional tests are required to narrow down\nthe appropriate referral. In th i ss t u d y ,t h es p e c i a l t yw a sd eﬁned by the\npatient’s discharge primary diagnosis, meaning the diagnosis was made after\nseveral tests and possibly after days ofobservation. As a result, the addition\nof initial vitals may not signiﬁcantly inﬂuence specialty prediction, as more\ndetailed information becomes available only later in the patient’sc a r e .\nThe evaluation of specialty frequencies, which can be found in the\nSupplementary Fig. 7 shows that in the best model, clinical user Claude 3.5\nSonnet, general surgery, emergency medicine, infectious diseases, and\ninternal medicine are overrepresented, while the underrepresentation of\northopedics is nearly balanced by the higher occurrence of orthopedic\nsurgery. The same tendencies can be seen in the other models.\nThe performance of LLMs in predicting the specialties shows that\nLLMs are generally well-suited to assist in medical referrals by offering a\nvariety of relevant specialty options.\nEvaluating LLM workﬂows for diagnostic accuracy\nIn the process of clinical decision-making, we evaluated whether LLMs can\nassist in predicting the diagnosis ordiagnoses a patient might have. We\nc o n d u c t e dt h i se v a l u a t i o ni no u rt w osettings like described before, the\ngeneral user and clinical user setting. More on the evaluation framework can\nbe found in the Methods: Diagnosis Evaluation Framework. To compare the\npredicted diagnoses to the ground truth diagnoses, we used Claude 3.5\nSonnet as a judge. Additionally, to validate this approach of using an LLM as\nFig. 3 | Triage level accuracy: exact match vs. range evaluation.This ﬁgure\ncompares the model performance in predicting triage levels. The graph shows the\naccuracy [%] for two evaluation methods: exact match (left) and range evaluation\n(right). Results are displayed for both evaluation types, illustrating the differences in\nmodel accuracy across the different LLMs for triage level prediction.\nTable 1 | Model performance comparison across tasks and evaluation methods\nUser setting Model Triage level Specialty Diagnosis Average\nExact match Range Matched At least one Matched At least one\nGeneral User RAG-Assisted LLM 64.10 78.20 77.12 86.35 69.43 80.85 76.01\nClaude 3.5 Sonnet 62.20 82.80 78.26 88.05 70.22 82.00 77.26\nClaude 3 Sonnet 58.35 74.40 78.10 87.70 70.17 81.55 75.05\nClaude 3 Haiku 57.70 71.80 77.86 87.10 67.39 79.60 73.58\nClinical User RAG-Assisted LLM 65.75 77.15 77.28 86.45 69.77 81.70 76.35\nClaude 3.5 Sonnet 64.40 82.40 78.86 88.55 70.26 82.10 77.76\nClaude 3 Sonnet 61.65 74.55 77.72 87.15 70.51 82.05 75.61\nClaude 3 Haiku 59.00 66.15 78.02 87.05 67.46 79.30 72.83\nPerformance is presented as accuracy [%] on all tasks and with all evaluation methods. A bold value indicates the best-performing model and an underlined value indicates the second-best-performing\nmodel, determined separately within each user setting (general or clinical user) and within each evaluation method (exact match/matched or range/at least one) for each prediction task (triage level, specialty\nor diagnosis).\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 4\nan evaluator, we asked four clinicians to review a subset of the data and\ncompare the predicted diagnoses to the ground truth diagnoses. A detailed\nexplanation of this process can be found in the subsection“Reader Study” in\nthe “Methods” section, and the results are presented in the subsection\n“Inter-Rater Agreement on Diagnosis Evaluation” in the“Results” section.\nI no u re v a l u a t i o no fL L M s’ability to assist in predicting patient diag-\nnoses, we found small differences in performance between models. In the\nﬁrst evaluation, in which each diagnosis was compared to the ground truth,\nClaude 3.5 Sonnet and Claude 3 Sonnet performed equally well for the\ngeneral and clinical user setting. In the second evaluation, where the goal was\nto predict at least one correct diagnosis for each patient, all models\ndemonstrated stronger performance. All results are presented in Fig.6 and\nin Table1.\nImprovements in the clinical user model over the general user model is\nparticularly notable for the RAG-assisted LLM, as shown in Table2.T h i s\nsuggests that the knowledge provided to the LLM during the RAG workﬂow\nhas enhanced its diagnostic skills, particularly in interpreting and utilizing\ncurrent initial vitals. Predicting or deﬁning a diagnosis, like specialty referral,\nrequires a signiﬁcant amount of information, much of which is difﬁcult to\ngather upon a patient’s arrival to the ED. This complexity underscores the\nchallenges of early diagnosis in such fast-paced settings, where many crucial\ndetails are still emerging.\nFig. 4 | Confusion matrices for triage level prediction.This ﬁgure presents con-\nfusion matrices for the two best-performing models (RAG-Assisted Claude 3.5\nSonnet and Claude 3.5 Sonnet) in different user settings (General User and Clinical\nUser) and evaluation methods (exact match and range evaluation).a and b show the\nconfusion matrices for the general user setting andc and d show the confusion\nmatrices for the clinical user setting. Diagonal values represent correct predictions in\nthe exact match evaluation, while marked predictions indicate correct values for the\ntriage range evaluation.\nTable 2 | Clinical vs. general user settings\nModel Triage Level Specialty Diagnosis Average\nExact Match Range Matched At Least One Matched At Least One\nRAG-Assisted LLM 1.65 –1.05 0.16 0.10 0.34 0.85 0.34\nClaude 3.5 Sonnet 2.20 –0.40 0.60 0.50 0.04 0.10 0.51\nClaude 3 Sonnet 3.30 0.15 -0.38 –0.55 0.34 0.50 0.56\nClaude 3 Haiku 1.30 –5.65 0.16 –0.05 0.07 –0.30 –0.75\nPerformance improvement for each model from general user to clinical user setting.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 5\nIntra-model agreement\nThe agreement between the models can be seen as a measure of quality, as\nhigh agreement indicates that similar patterns and trends are captured in the\nresponses, indicating robustness and reliability of the predictions. The ana-\nlysis of inter-model agreement for the diagnosis data was omitted as this data\nhas the highest variability and therefore requires assessment by an LLM judge.\nComparisons of inter-model agreement across the triage and specialty\ndatasets are shown in Table3, with full results available in Supplementary\nTable 2. The intra-model agreement analysis showed the highest con-\nsistency between the general user model and the clinical model for all\nmodels. This suggests that the different inputs to the same model do not\nsigniﬁcantly alter or improve responses, but also that the models - parti-\ncularly Claude 3.5 Sonnet and RAG-assisted LLM - show consistent per-\nformance across different user settings.\nRAG-Assisted LLM demonstrated the highest average agreement\nacross all models, closely followed by Claude 3.5 Sonnet, while the highest\nsingle inter-model agreement was between Claude 3.5 Sonnet and RAG-\nassisted LLM.\nThe high agreement between the models underlines their consistency\nin many cases, but the variation in agreement suggests that different models\ncorrectly classify different cases. This indicates that if we could determine\nwhich model excels at speciﬁc classiﬁcations, we could potentially reduce the\noverall error rate by a signiﬁcant margin.\nClinician validation of LLM-generated specialties\nTo evaluate the accuracy and reliabilityof the LLM-generated specialties, we\nasked four clinicians to independently review a subset of the dataset. Each\nclinician assessed the LLM-generated specialty and its appropriateness for a\ngiven diagnosis. They categorized each specialty as either Correct, Partially\nCorrect, Reasonable but Suboptimal or Incorrect. Two clinicians reviewed\nthe same subset to provide an independent evaluation and objectivity. A\nmore detailed description can be read upon in the Methods“Reader Study”.\nFig. 5 | Specialty prediction accuracy: matched vs. at least one specialty match\nevaluation. This ﬁgure compares the model performance in predicting medical\nspecialties. The graph shows the accuracy [%] for two evaluation methods: matched\nspecialty (left) and atleast one specialty match (right). Results are displayed for both\nevaluation types, illustrating the differences in model accuracy across the different\nLLMs for specialty prediction tasks.\nFig. 6 | Diagnosis prediction accuracy: matched vs. at least one diagnosis match\nevaluation. This ﬁgure compares the model performance in predicting medical\ndiagnoses. The graph shows the accuracy [%] for two evaluation methods: matched\ndiagnosis (left) and atleast one diagnosis match (right). Results are displayed for both\nevaluation types, illustrating the differences in model accuracy across the different\nLLMs for diagnosis prediction tasks.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 6\nThe results presented in Table4 show the clinicians’positive assessment\nof the LLM’s performance in creating the specialty ground truth. On average\nacross all clinicians, a signiﬁcant 81.5% of the predictions were rated as\nCorrect, which shows the LLM’sp r o m i s i n ga b i l i t yt oa l ign closely with clinical\nexpectations. Partially Correct and Reasonable but Suboptimal predictions\nare considered as acceptable choices. When the accurate and acceptable\ncategories are combined, the acceptability rises to 97.03%, showing that\nalmost all recommendations are clinically relevant. Notably, the overall error\nrate was exceptionally low at just 2.63%. However, a closer look reveals that\nClinician 2, with a higher error rate of 9.15% compared to the average error\nrate of 0.68% for the other clinicians, rated predictions more stringently. This\nhighlights the variations in human evaluations, which is likely inﬂuenced by\ndifferences in experience and individual judgment. These differences can also\nbe observed when comparing the two confusion matrices of the two clinician\npairs, provided in the Supplementary Fig. 8 and 9.\nThese results suggest that LLMs perform well in generating a ground\ntruth specialty, aligning closely with clinical expectations with high\nacceptability. The low error rate highlights their reliability, though the\nvariations in clinician evaluations reﬂect differences in judgment among\nclinicians. LLMs have strong potentials for specialty recommendations,\nwhile human oversight remains important for complex cases.\nInter-rater-agreement on diagnosis evaluation\nTo trust the LLMs predictions in clinical decision-making, it is essential to\nevaluate how well their outputs align with human evaluation. Therefore, we\naimed to assess the inter-rater agreement between the two LLMs, Claude 3.5\nSonnet and the RAG-assisted LLM, and clinicians in comparing predicted\ndiagnoses to ground truth diagnoses.Four clinicians participated in the\nevaluation, where each pair of clinicians assessed the same subset to ensure\nindependent evaluations and to compare their assessments. They categor-\nized the LLM-predicted diagnoses into four levels: Exact Match, Clinically\nEquivalent, Clinically Related, or Incorrect. The LLMs provided a binary\nevaluation, deciding whether the predicted diagnosis exactly matched the\nreal diagnosis or fell within a broader category related to the real diagnosis.\nMore details about the reader study and the LLM as a judge can be found in\n“Methods”.\nTo measure alignment, we mapped the LLMs’binary outputs to the\nclinician ratings. True was matched with Exact Match or Clinically\nEquivalent, while False was aligned with Clinically Related or Incorrect.\nEach subset was reviewed by two clinicians, allowing us to evaluate agree-\nment under two conditions: Union and Intersection. The results are pre-\nsented in Table5.\nThe evaluation of LLM evaluating predicted diagnoses shows a strong\nalignment with human evaluation, particularly in collaborative clinical\nscenarios, referred here as the union scenario. The union scenario means an\nLLM evaluation was considered correct if at least one of the two clinicians\nagreed with its evaluation (e.g., marking a“True” as Exact Match or\nClinically Equivalent). This approachhighlights the collaborative nature of\nclinical practice, where different perspectives are required to enhance\ndecision-making and arrive at a more comprehensive understanding.\nUnder this scenario, Claude 3.5 Sonnet achieved a high accuracy of 95.62%,\nwhile the RAG-assisted LLM followed closely with 94.91%. These results\nemphasize the models’ability to align with the reasoning of at least one\nh u m a ne x p e r ti nm o s tc a s e s .\nThe Intersection scenario repres ents stricter conditions, where\nan agreement among clinicians is required, such as during critical\nmultidisciplinary team discuss ions. In this scenario, an LLM eva-\nluation was considered correct only if both clinicians agreed with its\nevaluation (e.g., both rated a “True” as Exact Match or Clinically\nEquivalent). The results here show lower but still signiﬁcant accuracy,\nwith Claude 3.5 Sonnet achieving 70.74% and the RAG-assisted LLM\nreaching 69.86%. This outcome shows the challenges of getting full\nagreement among human experts.\nThese results demonstrate that LLMs perform well in collaborative and\nﬂexible clinical scenarios, as shown by the high accuracy in the union con-\ndition. However, the stricter Intersection results reveal the challenges of\nachieving full agreement, even among human experts, due to differences in\nperspectives and levels of experience. This variation can also be seen in the\nindividual inter-rater accuracy between clinicians and the LLMs, which\nranged from 76% to 90% for both Claude 3.5 Sonnet and the RAG-assisted\nLLM. These differences highlight the variability in how clinicians assess LLM-\nevaluated answers and interpret predictions. These differences can also be\no b s e r v e di nt h ee v a l u a t i o nr e s u l t sb e t ween the two clinician pairs, as shown in\nthe confusion matrices provided in the Supplementary Fig. 10 and 11. While\no n ep a i rs h o w e dh i g h e ra g r e e m e n ti nt h e“Exact Match” category, the other\npair demonstrated a more distributedmatching across different criteria.\nThese ﬁndings suggest that while LLMs align well with human evaluation,\nespecially in collaborative settings,there is still room for improvement in\nachieving full consensus. The results also highlight the potential value of using\nLLMs as complementary tools in multidisciplinary discussions, where diverse\nperspectives can enhance decision-making processes.\nDiscussion\nAdvances in large language models (LLMs) are beginning to reshape how\nclinicians approach medical decision-making. These models have already\nproven useful in more structured tasks, like medical licensing exams, but\nhow they can be used in real-world patient care is still being studied. We\nTable 3 | Average inter-model agreement of the triage level and specialty predictions\nCategory RAG-Assisted LLM Claude 3.5 Sonnet Claude 3 Sonnet Claude 3 Haiku Average\nBetween General and Clinical User 85.94 89.74 84.95 83.54 86.04\nTo RAG-Assisted LLM - 83.88 76.64 74.34 78.29\nTo Claude 3.5 Sonnet 83.88 - 77.19 73.35 78.14\nTo Claude 3 Sonnet 76.64 77.19 - 75.65 76.49\nTo Claude 3 Haiku 74.34 73.35 75.65 - 74.44\nAverage inter-model agreement [%] for different categories over triage level and specialty. The“Between general user and clinical user” category shows the average agreement between the corresponding\ngeneral user model and clinical user model, while the other categories show the average agreement to a certain model of the same type (general user to general user and clinical user to clinical user).\nAgreement to the same model is omitted to avoid distorting the average.\nTable 4 | Clinician validation of LLM-generated specialties:\naccuracy, acceptability, and error rates\nClinician Accurate\n[%]\nAcceptable\n[%]\nAccurate &\nAcceptable [%]\nError\nRate [%]\nClinician 1 93.77 6.23 100 0\nClinician 2 82.05 8.79 90.84 9.15\nClinician 3 81.91 17.06 98.98 1.02\nClinician 4 68.94 30.34 98.98 1.02\nAverage 81.5 15.53 97.03 2.63\nAverage Accuracy [%], Acceptability [%], Combined Accuracy and Acceptability [%], and Error Rate\n[%] for the ground truth specialties generated by the LLM, as evaluated by clinicians. The results are\nshown for each clinician and the overall average across all clinicians.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 7\nexplored the potential of LLMs with andwithout RAG assistance, to support\nclinical decision-making by benchmarking their performance on 2000 real-\nworld medical cases from the MIMIC-IV-ED dataset. We wanted to assess\ntheir ability to predict diagnoses, recommend specialists, and determine the\nurgency of care. Our results highlight both the promise and limitations of\nLLMs in the clinical decision process, offering insights into their potential\nrole in healthcare.\nOur results suggest that LLMs and the RAG-assisted LLM can support\nclinical decision-making, but their effectiveness varies depending on the\ntask. Claude 3.5 Sonnet generally performed slightly better across most\ntasks, but the RAG-assisted LLM offered an important advantage: the ability\nto use external, trusted references. This feature helps reduce the risk of\nhallucinations\n28–31 and adds a layer of fact-checking32,33, which is crucial in\nclinical settings where accuracy is crucial. The RAG-assisted LLM, com-\npared to its base model Claude 3.5 Sonnet, showed a different pattern of\nimprovement when using the clinical user setting (with additional patient\nvitals data), as demonstrated in Table2. The RAG-assisted LLM beneﬁted\nsigniﬁcantly from the extra vital information in the triage level and diagnosis\ntasks, though less so in the specialty task. In contrast, Claude 3.5 Sonnet\nshowed improvements in the triage level and specialty tasks but gained less\nfrom the vital signs in the diagnosis task.\nThe RAG workﬂow allows the model to incorporate external sources\nfrom a research context, helping to provide a more informed perspective on\nthe input. We hypothesize that the available external information likely\nemphasizes the relationship between vital signs and triage level or diagnosis,\nbut not as much between vital signs and the corresponding referral specialty.\nTherefore, with the background knowledge provided by the RAG workﬂow,\nit makes sense that this model beneﬁts more from additional vital signs in\nthe domains of triage level and diagnosis. This also suggests that the RAG\nworkﬂow improves the model’s performance in cases where current\nresearchﬁndings are particularly relevant.\nThis is further highlighted by the RAG-assisted LLM’ss t r o n gp e r f o r -\nmance in terms of exact accuracy on the triage level data with vital signs\ninformation, which is likely to be well-represented in available research\nresources. However, this does not necessarily help the model with range\naccuracy, as research sources are unlikely to guide the model in predicting\nmore severe over less severe.\nHowever, the beneﬁt of incorporating more clinical information was\nnot seen in simpler models like Claude 3 Haiku, and only minimal gains\nwere observed for Claude 3 Sonnet when predicting specialties. This is in line\nwith previousﬁndings that LLMs struggle with nuanced clinical data, like\ninterpreting abnormal lab results or subtle symptoms\n8. It also explains why\nnone of the models achieved high accuracy in predicting the most severe\ntriage patients, as these models are not equipped to follow numeric-based\nguidelines effectively. More advancedmodels, like Claude 3.5, showed they\nare better at handling these complexities.\nAccuracy in the medical domain remains a signiﬁcant challenge for\nLLMs, particularly in predicting triage levels, as all models showed both\nover-triaging and under-triaging. Assigning a triage level in a real-world\nsetting demands a high degree of clinical judgement and careful\nconsideration of the patient’s conditions and resources needed. For the\ntriage prediction, we acknowledge that using clinical notes may introduce\nbias since triage determination is typically made prior to the patient being\nfully assessed by a physician.\nTo limit the bias, we extracted the HPI, through our data processing,\nattempting to include only the reason for a patient’s admission andﬁrst\nimpression by a clinician as the earliest recording of the patient’ss y m p t o m s\ndocumented in the physician’s notes. Ideally, one would use real-time audio\nor video recordings of the initial patient contact with medical personnel—\nsuch data is unfortunately not available. While this study is purely an aca-\ndemic evaluation of various LLMs’ performance, the difﬁculty LLMs\ndemonstrate in assigning accurate severity levels - in particular at the\nextreme ends - highlights their current limitations in independently\nhandling these tasks. Nonetheless, the performance on this task remains a\nmeaningful indicator of the overall quality of LLMs, their potential as an\nassisting tool and their promise for future clinical applications.\nFuture work could focus on improving LLMs’numerical handling of\nlaboratory values to enhance their abilityto interpret clinical data accurately.\nAdditionally, a deeper examination of how clinicians make decisions when\ndeﬁning ESI levels is essential. This couldprovide valuable insights to improve\nLLMs to better replicate clinician performance in real-world scenarios.\nA critical aspect of utilizing LLMs inclinical decision-making is the\nimportance of prompt design. In our study, we experimented with various\nprompts to guide the models effectively, and it became evident that how a\ntask is framed signiﬁcantly impacts the quality of the results\n34,35.W h i l ew e\nobserved promising outcomes, it is clear that a more focused approach to\nprompt engineering would be highly beneﬁcial\n36–40, particularly when\ncombined with the context of external sources provided by the RAG\nworkﬂow. One interesting observation was the differences in performance\nbetween the LLM models. The models did not always agree on their pre-\ndictions, which points to both a limitation and an opportunity. The results\non intra-model agreement reveal that the models do not completely overlap\nin their predictions, suggesting that they might function as a“mixture of\nexperts” when combined. Leveraging this diversity in predictions could lead\nto improved outcomes by utilizing the strengths of each model in different\ncontexts. Additionally, higher agreement between models can be seen as a\nmeasure of quality, as it indicates that similar patterns and trends are being\ncaptured, contributing to the robustness and reliability of the predictions.\nA similar pattern of variability and complementarity observed in the\nintra-model agreement was also reﬂected in our evaluations involving\nclinician reviews. It highlights how differences in perspectives and expertise\ncan inﬂuence decision-making. In the review of LLM-generated specialties,\nclinicians showed variation in theirassessments, shaped by individual\njudgment and experience. Similarly,in the evaluation of LLM-predicted\ndiagnoses, the union condition demonstrated strong alignment with the\nLLM-judge, emphasizing the collaborative nature of clinical environments\nwhere different viewpoints can complement each other. However, the\nstricter intersection condition revealed the challenges of achieving full\nagreement among clinicians. This shows the complexity of consensus in\nmedical decision-making.\nTable 5 | Clinician validation of llm-generated diagnoses: inter-rater agreement\nClinician Inter-Rater Accuracy [%] Inter-Rater Union Accuracy [%] Inter-rater intersection accuracy [%]\nClaude 3.5 sonnet RAG-assisted LLM Claude 3.5 sonnet RAG-assisted LLM Claude 3.5 Sonnet RAG-assisted LLM\nClinician 1 90.43 90.11 95.22 95.37 75.44 73.84\nClinician 2 80.22 79.11\nClinician 3 76.51 77.78 96.03 94.44 66.03 65.87\nClinician 4 85.56 82.52\nAverage 83.18 82.38 95.62 94.91 70.74 69.86\nAverage Inter-Rater-Agreement Accuracy [%] for Claude 3.5 Sonnet and RAG-assisted LLM compared to clinician evaluations. Inter-Rater Accuracy isreported for each clinician, along with combined\nvalues for the Union and Intersection evaluations. For the Inter-Rater Union and Intersection, a single value is reported for each pair of clinicians: one value for the Union and one value for the Intersection of\nClinicians 1 and 2, and similarly for Clinicians 3 and 4.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 8\nFinally, while our study establishes benchmark tasks and resources for\nclinical decision-making, the next step will involve reﬁning the RAG-based\nmodel and similar approaches, and focusing on integrating them more\neffectively into clinician workﬂows. Beyond helping healthcare providers,\nthese models can also beneﬁt patients directly. For those experiencing\nsymptoms at home, LLMs can provide an initial assessment, giving patients\nan indication for the severity of their condition and recommending which\nspecialist to visit. This empowers patients to make more informed decisions\nabout their care.\nWe do not propose a direct clinical deployment, but it is still relevant to\nmention that any clinical deployment would need to address signiﬁcant\nregulatory concerns regarding AI in clinical tasks, especially those involving\nLLMs for referral, triage, and diagnosis. These speciﬁct a s k sf a l lu n d e r\n“determining eligibility for healthcare” (5a) and “emergency healthcare\npatient triage systems” (5 d) and are consequently classiﬁed as high-risk in\nAnnex III of the EU Artiﬁcial Intelligence Act (AIA)\n41. As such, clinical\nimplementation must comply with the AIA’s requirements for robust\nvalidation (Art. 57), potentially including multiple external validation sets\nand assessments of performance on edge cases. Moreover, the Act mandates\ncontinuous post-market monitoring (Art. 72) and reporting of serious\nincidents (Art. 73), highlighting the ongoing nature of AI system validation\nbeyond initial approval.\nNevertheless, it is impossible to rule out“leaky deployment” of LLM\nmodels, where physicians would start using openly available models as\nhelper systems in their clinical routine. Therefore, open and strict bench-\nmarking of LLM performance on various sets of clinical tasks is of utmost\nimportance for both the medical community and general populace.\nWhile our study focused on Claude models, it is relevant to consider\nhow other advanced LLMs might perform in similar clinical decision-\nmaking contexts. Newer reasoning-focused models, such as the OpenAI\nmodels with their“Deep Research” function or Claude 3.7 Sonnet\n42 with its\n“extended thinking mode”, could offer substantial advantages in handling\ncomplex medical cases. The rapid development of these models highlights\nthe need for objective benchmarks and continuous evaluation to ensure\ntheir reliability, and usefulness in clinical applications.\nThe results presented here are primarily of academic interest, pro-\nviding a ﬁrst highly needed benchmarking of LLMs in the AI-assisted\nclinical-decision-making process, as we believe these systems require further\nreﬁnement and validation before any potential clinical deployment. Our\nstudy shows that LLMs cannot replaceclinicians in independently per-\nforming complex medical decision-making. However, they demonstrate\npotential as supportive tools, assisting clinicians by providing relevant\ninsights and information.\nThis highlights how LLMs are more suitable for alternative use cases,\nsuch as educational resources for inexperienced clinicians, supplementary\nresources for patients, or background safety screenings. Ultimately, their\neffective integration into healthcarewill rely on thorough testing, ongoing\nimprovements, and well-deﬁned roles within clinical workﬂows.\nMethods\nData preprocessing\nOur goal was to develop a model capable of predicting the specialty, triage\nlevel, and diagnosis for patients in an emergency department (ED) setting or\nthose experiencing symptoms at home. Since we aimed to evaluate the\ndifference in model performance based on whether the information was\nentered by the patient themselves or a clinician, we designed our dataset\naccordingly. For the general user, we required two main inputs: a description\nof the patient’s symptoms and some basic patient information. For the\nclinical user we added the initial vitals signs, such as temperature, heart rate,\nrespiratory rate, oxygen saturation, and blood pressure, which can be\nmeasured upon arrival at the ED.\nWe processed and created our curated dataset using the MIMIC-IV ED\ndataset\n22,25 in conjunction with the MIMIC-IV Notes22,26 dataset, both\nmodules from MIMIC-IV20–22, to support clinical decision-making in an\nemergency department setting. The MIMIC-IV ED dataset contains\nextensive information from patients admitted to the emergency depart-\nment, while the Notes module provides valuable unstructured clinical notes\nof these patients.\nThe data processing pipeline is presented in Fig.7.F i r s t ,w em e r g e dt h e\nnecessary data tables from each source. Triage information was obtained\nfrom the MIMIC-IV-ED“triage” ﬁle, while the patient demographics such\nas race, and gender were extracted from the“edstays” ﬁle. The age speciﬁ-\ncally was extracted from the MIMIC-IV“patients” ﬁle. The initial vital signs\nwere extracted from the MIMIC-IV-ED“triage” ﬁle, and the unstructured\nclinical notes were extracted from the MIMIC-IV-Note“discharge” ﬁle.\nInitially, we extracted relevant discharge notes from MIMIC-IV-Note\ndataset and linked them with the patient records from the MIMIC-IV-ED\n2.2 “edstays” ﬁle using thestay_id. We then merged the triage information\nand the patient demographics - gender, race and age) from the respective\nﬁles, and integrated the initial vital signs. During this merging process, we\ndropped duplicate subject entries, removed cases with missing triage\ninformation, andﬁltered the records to retain only those with sequence\nnumber (seq_num) equal to 1. This ensures the uniqueness of the ED stays.\nWe also excluded patients that had died.\nA separate preprocessing step was applied for the unstructured clinical\nnotes. Speciﬁcally, we selected only the patients that had a history of present\nillness in the unstructured notes. We extracted the history of present illness\nparagraph from the discharge notes- discarding any other information\nincluded in the notes. We further selected only cases with HPIs that had a\nstring length between 50 and 2000 characters, to avoid getting too short or\ntoo long HPIs. We additionally removed any entries that mentioned“ED”or\n“ER”, as these references did not include any necessary information\nregarding the patient’ss y m p t o m so rh o wt h ep a t i e n tw a sf e e l i n g .\nAdditionally to extracting the HPI, we extracted the diagnoses list for\neach patient from the clinical notes. These lists were typically divided into\nprimary and secondary diagnoses. For our evaluation, we used only the\nprimary diagnoses and discarded cases that had more than 15 primary\ndiagnoses, as most cases had up to 3 diagnoses. This approach ensures that\nthe dataset accurately reﬂects patient information and vital signs at the time\nof emergency department triage, offering a comprehensive view of early-\nstage clinical decision-making.\nPrompts\nWe created a series of prompts to guide the LLM in performing speciﬁc\nclinical tasks. These included predicting the triage level, predicting the spe-\ncialty and diagnosis both together as they are both related and complement\neach other. Additionally, we used the prompt creating a ground truth referral\nspecialist, and using the LLM as a judge to compare predicted diagnoses with\nthe true diagnoses. To decide on thesep r o m p t s ,w ee x p e r i m e n t e dw i t h\nseveral variations of the prompts on a subset of data that was not included in\nour evaluation to reﬁne the prompts to our tasks. To ensure consistent and\nreliable outputs, we set the temperature parameter to zero during these\nexperiments. We observed that the results were identical across runs, with no\nvariations. Based on this observation, and given the cost constraints of\nrunning the LLM multiple times, we decided to run the predictions only once\nfor the ﬁnal evaluation. Additionally, our goal is to evaluate the LLM’s\nperformance in a scenario that mimics a clinical environment, where a\nclinician would typically rely on the LLM’s ﬁrst output rather than running it\nmultiple times. By focusing on theﬁrst output, we aimed to test the reliability\nand practical usability of the LLM in such a setting.\nEach prompt begins by setting the system’s role, such as,“You are an\nexperienced healthcare professional with expertise in medical and clinical\ndomains”, followed by clear task instructions. We also provided the data\nnecessary for each task and speciﬁed how the LLM should format its\nresponses, ensuring concise answers within predeﬁned tags. The different\nprompts can be seen in the Supplementary Fig. 1-6.\nModel selection\nTo comply with privacy regulations restricting the use of the MIMIC-IV\ndataset with external APIs like OpenAI’sG P T - 4 oa n dt h eC l a u d ef a m i l y\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 9\nmodels, we employed AWS Privatelink to securely connect to the Claude\nmodels hosted on AWS. This kind of evaluation reduces the likelihood that\nthe data has been previously seen by the LLM models, which cannot be\nguaranteed when using publicly available datasets.\nClaude 3.5 Sonnet, Claude 3 Sonnet, and Claude 3 Haiku are advanced\nLLMs developed to enhance natural language understanding, with\nimprovements in performance and efﬁciency across multiple benchmarks\nover their predecessors, including GPT-4o, GPT-4T, Gemini 1.5 Pro and\nLlama 3 400B\n23. They excel in contextual understanding, efﬁciency, and\ntheir ability to handle specialized queries. This makes them well-suited for\napplications in clinical decision-making, where precision and adaptability\nare essential.\nClaude 3 Haiku is the fastest and most compact model in Anthropic’s\nClaude 3 family. It excels in tasks where it requires quick analysis and\nresponse times\n24, making this feature suitable for the clinical-decision\nprocess.\nClaude 3 Sonnet is a balanced combination of speed and intelligence,\noffering signiﬁcant improvement in reasoning and accuracy. This model is\nversatile, handling complex text generation, analysis and reasoning24.\nClaude 3.5 Sonnet is built on the foundations of Claude 3 Sonnet with\nfurther enhancement in speed and intelligence. It excels in different tasks\nlike reasoning and question answering, while being faster and cost-efﬁcient\nr e l a t i v et ot h ep r e v i o u sm o d e l s .I th a ss h o w nc o m p e t i t i v eo rs u p e r i o rp e r -\nformance in a variety of language-based tasks23.\nRAG-assisted LLM\nA RAG-assisted LLM approach involves two components: a retrieval\nmechanism that gathers the relevant information corresponding to the\nquery from a speciﬁc external knowledge base, and a language model that\nintegrates the retrieved information with the query to produce a response\nthat is both grounded in the external knowledge base and tailored to the\nspeciﬁcs of the given query. This method has shown improvements in both\naccuracy and reliability, which signiﬁcantly reduces false or misleading\ninformation, referred to as hallucination, and produces more factual,\ncontext-aware outputs\n28–31.I nt h i ss t u d y ,t h ef r a m e w o r ki si m p l e m e n t e d\nusing Claude 3.5 Sonnet as the LLM component and incorporates a multi-\ns t e pp r o c e s sw h e r et h eL L Mp l a y sak e yr o l ei nr eﬁning and enhancing query\nprocessing and answer generation. The workﬂow is represented in Fig.8.\nThe workﬂow starts with a query decomposition, breaking down the\npatient’s query into smaller queries. This process allows RAG systems to\nbreak down the input into its smaller key components and retrieve the most\nrelevant information for each component. This idea is supposed to mimic\nthe natural way humans approach understanding by breaking down com-\nplex information into smaller parts to focus on each element individually.\nThe knowledge base supporting this workﬂow consists of 30 million\nPubMed abstracts, which have been converted into embedding vectors and\nstored as a knowledge high-dimensional vector database. This allows the\nsystem to measure semantic similarity by comparing these vectors to those\nin the knowledge vector database. By identifying the closest matches, the\nsystem retrieves the most relevant information for the given query.\nThe LLM uses the retrieved information alongside the query to gen-\nerate a response that is supported by theretrieved data, while also providing\nthe source PubMed references for further review. An additional layer in the\nworkﬂow tries to enhance the performancethrough iterative loops of cri-\ntique, reﬁnement, and retrieval. In these loops, the LLM evaluates the\ngenerated responses, identiﬁes gaps or inaccuracies, and reﬁnes the output\nFig. 7 | Flow diagram for the data preprocessing.\nThis ﬁgure illustrates our data preprocessing pipe-\nline. From Physionet we utilized MIMIC-IV-ED 2.2\nand MIMIC-IV-Note. The necessary data tables\nfrom each data source were merged. Next, the\nmerged data undergoes processing and cleaning.\nFinally, we process the clinical notes to extract the\nrelevant information - history of present illness and\nprimary diagnoses.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 10\nas needed. We used an LLM to evaluate the output and determine whether it\nwas sufﬁcient for the given query. This iterative process intends to achieve\nhigher accuracy alignment with the query, to create more precise and reli-\nable outputs.\nTriage level evaluation framework\nThe triage level is based on the Emergency Severity Index (ESI)19,w h i c h\nconsists ofﬁve levels, as outlined in the Supplementary Table 1. We evaluate\nthe model’s triage level predictions usingtwo different assessment frame-\nworks. Theﬁrst is a straightforward comparison between the predicted\ntriage level and the ground truth, with accuracy as the metric. The second\nevaluation framework uses a triage range approach, accounting for the\nvariability in clinical judgment when assigning triage levels. The ESI is\ntypically determined by a clinician assigning a score based on their assess-\nment of a patient’s condition. Although there are deﬁned levels within the\nESI system, ranging from 1 to 5, the assignment of these levels can vary due\nto the clinician’s intuition and experience. In some cases, clinicians may lean\non the side of caution, assigning a more severe level to avoid the risk of\npatient deterioration or the possibility of misclassifying a patient as less\ncritical than they actually are. To account for this variability, our evaluation\nallows someﬂexibility in model predictions. If the real triage level value is 1,\nthe model must predict 1, as immediate life-saving intervention is required.\nFor a real value of 2, the model can predict either 1 or 2, ensuring patients\nneeding urgent care aren’t harmed by overclassiﬁcation. Similarly, if the real\nvalue is 3, the model can predict 2 or 3, and so on— up to a real value of 5,\nwhere the model can predict either 4 or 5.\nSpecialty evaluation framework\nTo assess the performance of LLMs inrecommending appropriate medical\nspecialties, we developed two distinct evaluation scenarios: one tailored for\nthe general users and another for clinical users. In each scenario, the models\ngenerated the top three specialty recommendations based on the available\npatient information. For the general user case, this input consisted of a\ndescription of the symptoms and basicpatient information, while for the\nclinical user case, the input was augmented with the patient’s initial vital\nsigns. For the general user setting, we implemented this evaluation with the\nGerman healthcare system in mind, where patients can choose any specialist\nwithout prior consultation with primarycare or emergency care specialists.\nFor the clinical user, we designed the evaluation to assist primary care\ndoctors in referring patients to a specialist or seeking consultation as needed.\nSince the MIMIC-IV-ED and MIMIC-IV-Notes do not include\ninformation on whether a consultation is necessary - and we could not\ncompensate for this missing detail - we put our focus on evaluating the\nquestion “which specialist would be most helpful given the symptoms at\nhand.” As the datasets lack exact information on the medical specialist each\npatient visited, we used Claude 3.5 Sonnet to predict the most likely spe-\ncialist for each diagnosis for each case, given that patients often present with\nmultiple diagnoses rather than just one, thereby establishing the ground\ntruth for this study.\nPredicting a single specialist would be insufﬁcient and unfair to the\nmodel when comparing its performance to the ground truth consisting of\nseveral specialties. In fact, it’s not uncommon for a patient to suffer from\nseveral medical conditions simultaneously, each requiring attention. To\naddress this complexity, we chose to predict the top three specialists for each\ncase. An Example is provided in Table6.\nThis approach provides a more realistic comparison and offers clin-\nicians and patients multiple possibilities to consider, reducing the risk of bias\ntoward a single diagnosis. Ultimately, the LLM serves as a support tool,\nproviding valuable insights, while the clinician makes theﬁnal, informed\ndecision based on both the LLM’s recommendations and their own\nexpertise.\nDiagnosis evaluation framework\nAs mentioned in the specialty evaluation previously, patients often come in\nwith more than one diagnosis. To reﬂect this, we predicted a top three list of\ndiagnoses for each case. We then compared each of these predictions to the\nactual diagnoses. Notably, we examined the time from admission to release\nand conﬁrmed that all cases used in our evaluation had a stay duration of less\nthan one day. This minimizes the possibility to include diagnoses that might\narise from later during hospitalization.\nTo make the comparison more accurate, we used an LLM judge to\ndecide if the predicted diagnosis either matched the ground truth orﬁti n t oa\nbroader category of one of the actual diagnoses. This way, we accounted for\ndifferences in wording while still ensuring a fair evaluation. Additionally, on\na subset of the dataset, we involved four clinicians who compared the pre-\ndicted diagnoses to the ground truth diagnoses and reviewed them. More\nd e t a i l sa b o u tt h i sp r o c e s sc a nb ef o u n di nt h es u b s e c t i o n“Reader Study”.\nTable 6 | Example diagnoses, ground truth and predicted\nspecialty\nPrimary Diagnoses Ground Truth Specialties\n(Claude 3.5 Sonnet)\nPredicted Specialties\n(Claude 3.5 Sonnet)\n Alcohol withdrawal\n Pancreatitis\n Thrombocytopenia\n Schizoaffective\n HIV\n Addiction Medicine\n Gastroenterology\n Hematology\n Psychiatry\n Infectious Disease\n Gastroenterology\n Hepatology\n Addiction Medicine\nExample of a case with primary diagnoses, their corresponding created ground truth, and the\npredicted specialties for the case\nFig. 8 | Workﬂow of the RAG-assisted LLM.The\nworkﬂow starts with query decomposition, breaking\ndown patient queries into smaller chunks. These\nchunks are embedded and undergo a semantic\nsimilarity comparison with the embeddings of 30\nmillion PubMed abstracts to extract the most rele-\nvant information. The retrieved information is then\ncombined with the query, and the LLM generates\nresponses supported by the source references. An\niterative critique-reﬁnement loop further enhances\nthe outputs by identifying gaps, reﬁning responses,\nand ensuring alignment with the query.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 11\nWe employed two evaluation methods for assessing the model’sp e r -\nformance in predicting the correct specialty. Theﬁrst method evaluated\nwhether each predicted specialty appeared in the ground truth list. For each\npatient, we counted how many specialties were correctly predicted and then\ndivided that number by the length of the shorter list, either the ground truth\nor the prediction list.\nFor example, if the ground truth for a patient included only one entry, a\ncardiologist, and the model predicted three specialists— one cardiologist,\none general medicine, andone electrophysiologist— only the cardiologist\nwould be considered correct. Although general medicine and electro-\nphysiology could also be relevant in some cases, our evaluation was speci-\nﬁcally set to match the ground truth. This ties into a point discussed in the\npaper, where we explore how a single diagnosis might be managed by\nmultiple specialists, a factor we plan to address in future work.\nIn this example, since only the cardiologist was correctly predicted, the\npatient would receive one point, which is then divided by the length of the\nshorter list (in this case, one, as the ground truth had only one entry). So, the\nscore for this patient would be 1. If the ground truth had included two\nspecialties, and the model only correctly predicted one out of three, the score\nwould be 0.5. The total points across all patients were then summed and\ndivided by the total number of patients to calculate the overall accuracy.\nThe second evaluation framework was simpler, focusing on whether at\nleast one of the predicted specialties appeared in the ground truth list. If any\none of the model’s predicted specialties matched one of the ground truth\nspecialties, the prediction for that patient was considered successful.\nLLM judge\nFor our study, we utilized LLMs to evaluate and compare the accuracy of\npredicted diagnoses for a given set of patient cases. This evaluation aimed to\nassess the model’s diagnostic capabilities by comparing the predicted\ndiagnoses with those listed in the patient’s medical records. The prompt for\nthe evaluation can be found in the Methods: Prompts.\nT h em o d e lw a sg i v e nt h et r u el i s to fd i a g n o s e sf o re a c hp a t i e n t ,a l o n g\nwith three predicted diagnoses.\nI tw a st h e na s k e dt od e t e r m i n ei ft h epredicted diagnosis matched any of\nthe primary diagnoses by focusing on semantic equivalence and meaning, or\nif it fell under a broader category related to the real diagnosis. Since LLMs may\nuse different phrasing for the same concept, which string-matching algo-\nrithms could miss, the model was askedto evaluate whether the predicted\ndiagnosis matched the real one or was related to it in a broader sense. If it did,\nthe model returned“True”, ensuring that only diagnoses with the same or\nrelated meanings were marked as such. Otherwise, it returned“False”.\nSimilar methodologies have been explored successfully in recent\nresearch, showing that LLMs can effectively perform human-like evalua-\ntions in various tasks, including text summarization, quality assessments,\nand chat assistant evaluations, withresults aligning closely to human\njudgments\n43–45.T h e s eﬁndings support the use of LLMs as reliable tools for\ntasks like our diagnostic comparison evaluation.\nMoreover, insights from the paper“Self-Recognition in Language\nModels”46 further argue that LLMs do not speciﬁcally prefer their own\nresponses over those from other models. When asked to choose among\nseveral answers, the study showed that weaker models tend to select the one\nthey consider as best, rather than their own, demonstrating that LLMs\nprioritize answer quality over origin. As a result, high-quality models are\nmore likely to recognize their own outputs as good— not out of bias, but\nbecause of their focus on quality. This reinforces the idea that LLMs can\nperform evaluations without self-preference. Importantly, we did not use\nthe LLM to compare outputs across models, which could risk introducing\nbias. Instead, the LLM evaluator compar e de a c ho ft h et o pt h r e ep r e d i c t e d\ndiagnoses directly to the ground truth, determining whether they aligned in\nmeaning or category. By focusing only on direct comparisons between\npredictions and the ground truth, we aimed to minimize self-bias and ensure\nan objective evaluation process.\nWhile promising, the reliability and interpretability of LLMs as eva-\nluation tools in real-world clinical environments still need further validation\nand reﬁnement to ensure their safe and effective use. To address this, a\nsubset of the dataset was validated by four clinicians, which is described in\nthe subsection“Reader Study” in the“Methods” section. The results of this\nvalidation are detailed in the subsection“Inter-Rater Agreement on Diag-\nnosis Evaluation” in the“Results” section.\nReader study\nIn this study, we asked four cliniciansfrom different institutions to review\nthe performance of an LLM in generating and predicting clinical specialties\nand diagnoses. The clinicians come from diverse medical backgrounds,\nensuring a broad perspective in the evaluation process with several years of\nexperience. We included one clinician afﬁliated with Policlinico Gemelli in\nRome, Italy, another with the Radiology Department at Klinikum rechts der\nIsar in Munich, Germany, and two clinicians are based at the University of\nChicago in the United States.\nThe revision aimed to assess how well the LLM performed the fol-\nlowing two tasks:ﬁrst, creating a ground truth specialty based on the given\ndiagnosis, and second, predicting diagnoses for each patient. We selected a\nsubset of 400 out of the 2000 cases from the dataset. Each clinician was\nassigned 200 cases, with Clinicians 1 and 2 reviewing the same subset, and\nClinicians 3 and 4 reviewing a different subset. This setup allowed for\nindependent evaluations of the same cases by each pair, improving objec-\ntivity as much as possible.\nFor theﬁrst task, the clinicians evaluated the LLM generated ground\ntruth specialty for each diagnosis. The clinicians assessed the accuracy of\nthese predictions by categorizing theminto the following four levels: Cor-\nrect, where the prediction matched the specialty a clinician would select for\nthe diagnosis; Partially Correct, where the prediction was relevant but not\nideal, such as suggesting a generalist or related specialty; Reasonable but\nSuboptimal, where the prediction wasvalid but less optimal, demonstrating\na plausible but less precise understanding of the diagnosis; and Incorrect,\nwhere the prediction had no logical connection to the diagnosis.\nFor the second task, we used a subset from the outputs of the clinical\nuser setting of Claude 3.5 Sonnet and the RAG-assisted LLM. For each\nmodel the clinicians compared the LLM predicted diagnoses with the\nground truth diagnoses and categorized them as follows: Exact Match,\nwhere the prediction matched the ground truth diagnosis exactly; Clinically\nEquivalent, where the prediction conveyed the same clinical condition as the\nground truth but used slightly different terminology or scope; Clinically\nRelated, where the prediction referred to a related condition relevant to\nclinical reasoning but diverged from the ground truth; and Incorrect, where\nthe prediction was clinically unrelated to the ground truth.\nThe goal of this evaluation is to demonstrate that the LLM performs\nwell in predicting both the specialtyand the diagnosis, with a high level of\nacceptance among clinicians. In addition to predicting diagnoses, the LLM\nwas also used to compare and evaluate these predicted diagnoses against the\nground truth. Eventually add here that also here the clinicians review\nshowed a well performance fo the llms.\nThis dual role highlights the LLM’s ability not only to generate outputs\nbut also to assess its own performance. Theseﬁndings show the potential of\nLLMs to assist in clinical decision-making and evaluation processes. By\nproviding a cost-effective and time-efﬁcient solution, LLMs could serve as a\nvaluable tool to support clinicians and offer a reliable second opinion in\nmedical practice.\nIntra-model agreement\nWe evaluated the agreement between models by comparing the predictions\nof different variants of the eight models, consisting of the RAG-assisted\nmodel and the three Claude language models with general user and clinical\nuser settings each. Agreement was calculated separately for triage level\npredictions and specialty predictions and is symmetrical. Therefore, the\nresults for both datasets are shown in the Supplementary Table 2, where the\nupper triangular matrix shows the intra-model agreement for triage and the\nlower triangular matrix for specialty, excluding self-comparisons (i.e., per-\nfect agreement with the same model).\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 12\nWe evaluated and highlighted thetwo highest agreement values\nbetween model pairs for each dataset (specialty and triage) and for each of\nthe three model user setting subgroups (general user to general user, general\nuser to clinical user, clinical user to clinical user).\nData availability\nCore data is available athttps://physionet.org/content/mimic-iv-note/2.2/.\nData processing scripts and processed data are available athttps://github.\ncom/BIMSBbioinfo/medLLMbenchmark.\nCode availability\nThe code to process data is available athttps://github.com/BIMSBbioinfo/\nmedLLMbenchmark.\nReceived: 27 September 2024; Accepted: 27 April 2025;\nReferences\n1. Sutriningsih, A., Wahyuni, C. U. & Haksama, S. Factors affecting\nemergency nurses’perceptions of the triage systems.J. Pub. Health\nRes. 9,8 5–87 (2020).\n2. Ma, M. D. et al. CliBench: Multifaceted evaluation of Large Language\nModels in clinical decisions on diagnoses, procedures, lab tests orders and\nprescriptions.arXivhttps://doi.org/10.48550/arXiv.2406.09923(2024).\n3. Testolin, A. Can neural networks do arithmetic? A survey on the\nelementary numerical skills of state-of-the-art deep learning models.\nAppl. Sci. (Basel)14, 744 (2023).\n4. Abbas, A., Rehman, M. S. & Rehman, S. S. Comparing the\nperformance of popular large language models on the National Board\nof Medical Examiners sample questions.Cureus 16, e55991 (2024).\n5. Brin, D. et al. How large language models perform on the United States\nmedical licensing examination: A systematic review.medRxiv https://\ndoi.org/10.1101/2023.09.03.23294842 (2023)\n6. Resources Optimal Care Injured Patient. 6th.American Col- lege\nSurgeons (2014).\n7. Lupton, J. R. et al. Under-triage and over-triage using the Field Triage\nGuidelines for injured patients: A systematic review.Prehosp. Emerg.\nCare 27,3 8–45 (2023).\n8. Hager, P. et al. Evaluating and mitigating limitations of large language\nmodels in clinical decision making.Nat. Med.30, 2613–2622 (2024).\n9. Mori, Y., Izumiyama, T., Kanabuchi, R., Mori, N. & Aizawa, T. Large\nlanguage model may assist diagnosis of SAPHO syndrome by bone\nscintigraphy. Mod. Rheumatol.34, 1043–1046 (2024).\n10. Kwon, T. et al. Large language models are clinical reasoners:\nReasoning-aware diagnosis framework with prompt-generated\nrationales. ACM AAAI Conf. AI38, 18417–18425 (2023).\n11. Daher, M. et al. Breaking barriers: can ChatGPT compete with a\nshoulder and elbow specialist in diagnosis and management?JSES\nInt. 7, 2534–2541 (2023).\n12. Madadi, Y. et al. ChatGPT assisting diagnosis of neuro-\nophthalmology diseases based on case reports.JNO https://doi.org/\n10.1097/WNO.000000000000227 (2023).\n13. Delsoz, M. et al. Performance of ChatGPT in diagnosis of corneal eye\ndiseases. Cornea 43, 664–670 (2023).\n14. Sorin, V. et al. GPT-4 multimodal analysis on ophthalmology clinical\ncases including text and images.medRxiv https://doi.org/10.1101/\n2023.11.24.23298953 (2023)\n15. Eriksen, A. V., Möller, S. & Ryg, J. Use of GPT-4 to diagnose complex\nclinical cases.NEJM AI1,1 –3 (2023).\n16. Ueda, D. et al. Evaluating GPT-4-based ChatGPT’\ns clinical potential\non the NEJM quiz.BMC Digit Health2, 4 (2024).\n17. Han, T. et al. Comparative analysis of GPT-4Vision, GPT-4 and open\nsource LLMs in clinical diagnostic accuracy: A benchmark against\nhuman expertise.medRxiv https://doi.org/10.1101/2023.11.03.\n23297957 (2023)\n18. Harsha, N. et al. Can generalist foundation models outcompete\nspecial-purpose tuning? Case study in medicine.arXiv https://doi.\norg/10.48550/arXiv.2311.16452 (2023).\n19. Gilboy, N., Tanabe, T., Travers, D. & Rosenau, M.Emergency Severity\nIndex (ESI): Triage Tool Emergency Department Care, Version 4\n(2011).\n20. Johnson, A. et al.MIMIC-IV. PhysioNethttps://doi.org/10.13026/\nHXP0-HG59 (2024).\n21. Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health\nrecord dataset.Sci. Data10, 1 (2023).\n22. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and PhysioNet:\ncomponents of a new research resource for complex physiologic\nsignals. Circulation 101, E215–E220 (2000).\n23. Claude 3.5 Sonnet.https://www.anthropic.com/news/claude-3-5-\nsonnet.\n24. Introducing the next generation of Claude.https://www.anthropic.\ncom/news/claude-3-family.\n25. Johnson, A. et al. MIMIC-IV-ED.PhysioNet https://doi.org/10.13026/\n5NTK-KM72 (2023).\n26. Johnson, A., Pollard, T., Horng, S., Celi, L. A. & Mark, R. MIMIC-IV-\nNote: Deidentiﬁed free-text clinical notes.PhysioNet https://doi.org/\n10.13026/1N74-NE17 (2023).\n27. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks.Adv. Neural Inf. Process. Syst.33, 9459–9474\n(2020).\n28. Kresevic, S. et al. Optimization of hepatological clinical guidelines\ninterpretation by large language models: a retrieval augmented\ngeneration-based framework.NPJ Digit. Med.7, 102 (2024).\n29. Shuster, K., Poff, S., Chen, M., Kiela, D. & Weston, J. Retrieval\nAugmentation Reduces Hallucination in Conversation.ACL Find.\nAssoc. Comput. Linguist.: EMNLP2021, 3784–3803 (2021).\n30. Ayala, O. & Bechard, P. Reducing hallucination in structured outputs\nvia Retrieval-Augmented Generation.ACL Proc. Conf. North Am.\nChapter Assoc. Comput. Linguist.: Hum. Lang. Technol.6, 228–238\n(2024).\n31. Wang, D. et al. Enhancement of the performance of large language\nmodels in diabetes education through retrieval-augmented\ngeneration: Comparative study.J. Med. Internet Res.26, e58041\n(2024).\n32. Park, Y. J., Jerng, S. E., Yoon, S. & Li, J. 1.5 million materials narratives\ngenerated by chatbots.Sci. Data11, 1060 (2024).\n33. Khaliq, M. A., Chang, P. Y.-C., Ma, M., Pﬂugfelder, B. & Miletić,F .\nRAGAR, your falsehood radar: RAG-augmented reasoning for\npolitical fact-checking using multimodal large language models.ACL\nProc. FEVER Workshop7, 280–296 (2024).\n34. Lester, B., Al-Rfou, R. & Constant, N. The power of scale for\nparameter-efﬁcient prompt tuning.ACL Conf. Empir. Methods Nat.\nLang. Process, 3045–3059 (2021).\n35. Yang, C. et al. Large Language Models as Optimizers.arXiv https://\ndoi.org/10.48550/arXiv.2309.03409 (2023).\n36. Savage, T., Nayak, A., Gallo, R., Rangan, E. & Chen, J. H. Diagnostic\nreasoning prompts reveal the potential for large language model\ninterpretability in medicine.NPJ Digit. Med.7, 20 (2024).\n37. Giray, L. Prompt engineering with ChatGPT: A guide for academic\nwriters. Ann. Biomed. Eng.51, 2629–2633 (2023).\n38. Bansal, P. Prompt engineering importance and applicability with\ngenerative AI.J. Comput. Commun.12,1 4–23 (2024).\n39. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large\nLanguage Models are Zero-Shot Reasoners.arXiv Acm. NIPS'2238,\n22199–22213 (2022).\n40. Wang, L. et al. Prompt engineering in consistency and reliability with\nthe evidence-based guideline for LLMs.NPJ Digit. Med.7, 41 (2024).\n41. Annex III: High-Risk AI Systems Referred to in Article 6(2).https://\nartiﬁcialintelligenceact.eu/annex/3/.\n42. Claude 3.7 Sonnet.https://www.anthropic.com/claude/sonnet.\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 13\n43. Gao, M. et al. Human-like Summarization Evaluation with ChatGPT.\narXiv https://doi.org/10.48550/arXiv.2304.02554 (2023).\n44. Chiang, C.-H. & Lee, H.-Y. Can large language models be an\nalternative to human evaluations?ACL Proc. 61st Annu. Meet. Assoc.\nComput. Linguist.1, 15607–15631 (2023).\n45. Zheng, L. et al. Judging LLM-as-a-judge with MT-bench and Chatbot\nArena. arXiv Acm. NIPS'2337, 46595–46623 (2023).\n46. Davidson, T. R. et al. Self-Recognition in Language Models.ACL Find.\nAssoc. Comput. Linguist.: EMNLP2024, 12032–12059 (2024).\nAcknowledgements\nWe thank Akalin lab members for comments on the manuscript. This\nresearch received no speciﬁcf u n d i n gf o rt h i sw o r k .\nAuthor contributions\nA.A. conceptualized and planned the project. F.G. and M.S. jointly executed\nall of the computational analyses. FG made all theﬁgures, completed\nbenchmarks and wrote the initial draft of the manuscript. F.A., A.B., F.B. and\nK.G. performed the clinician expert validation on LLM-related tasks. MS\ndesigned and implemented the RAG-based workﬂow. V.F., M.S., A.A., and\nF.G. edited the manuscript. A.A. acquired funding and supervised the\nproject.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01684-1\n.\nCorrespondenceand requests for materials should be addressed to\nAltuna Akalin.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons Attribution\n4.0 International License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other\nthird party material in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use\nis not permitted by statutory regulation or exceeds the permitted use, you will\nneed to obtain permission directly from the copyright holder. To view a copy of\nthis licence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01684-1 Article\nnpj Digital Medicine|           (2025) 8:263 14",
  "topic": "Medical diagnosis",
  "concepts": [
    {
      "name": "Medical diagnosis",
      "score": 0.8326944708824158
    },
    {
      "name": "Triage",
      "score": 0.7779930830001831
    },
    {
      "name": "Workflow",
      "score": 0.7601598501205444
    },
    {
      "name": "Referral",
      "score": 0.6456409096717834
    },
    {
      "name": "Decision support system",
      "score": 0.6177802085876465
    },
    {
      "name": "Health care",
      "score": 0.4820500314235687
    },
    {
      "name": "Clinical decision making",
      "score": 0.46141964197158813
    },
    {
      "name": "Clinical decision support system",
      "score": 0.45645755529403687
    },
    {
      "name": "MEDLINE",
      "score": 0.4340018630027771
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.42481717467308044
    },
    {
      "name": "Medical emergency",
      "score": 0.40309545397758484
    },
    {
      "name": "Computer science",
      "score": 0.3604696989059448
    },
    {
      "name": "Medicine",
      "score": 0.3407615125179291
    },
    {
      "name": "Nursing",
      "score": 0.2193470001220703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19020116329193115
    },
    {
      "name": "Family medicine",
      "score": 0.17412960529327393
    },
    {
      "name": "Pathology",
      "score": 0.08739641308784485
    },
    {
      "name": "Database",
      "score": 0.0860600471496582
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I205582932",
      "name": "Max Delbrück Center",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802557943",
      "name": "Agostino Gemelli University Polyclinic",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802619606",
      "name": "Klinikum rechts der Isar",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 24
}