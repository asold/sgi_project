{
  "title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study",
  "url": "https://openalex.org/W4394892516",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3080804641",
      "name": "Arun James Thirunavukarasu",
      "affiliations": [
        "University of Cambridge",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A4322225663",
      "name": "Shathar Mahmood",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2261677931",
      "name": "Andrew Malem",
      "affiliations": []
    },
    {
      "id": null,
      "name": "William Paul Foster",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2981255448",
      "name": "Rohan Sanghera",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2635899234",
      "name": "Refaat Hassan",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2103954715",
      "name": "Sean Zhou",
      "affiliations": [
        "West Suffolk NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2149920119",
      "name": "Shiao Wei Wong",
      "affiliations": [
        "Manchester University NHS Foundation Trust",
        "Manchester Royal Eye Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2239391237",
      "name": "Yee-Ling Wong",
      "affiliations": [
        "Manchester University NHS Foundation Trust",
        "Manchester Royal Eye Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2232737292",
      "name": "Yu Jeat Chong",
      "affiliations": [
        "Birmingham and Midland Eye Centre"
      ]
    },
    {
      "id": "https://openalex.org/A5109628756",
      "name": "Abdullah Shakeel",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2981721495",
      "name": "Yin‐Hsi Chang",
      "affiliations": [
        "Chang Gung Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3039616060",
      "name": "Benjamin Kye Jyn Tan",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A1974947386",
      "name": "Nikhil Jain",
      "affiliations": [
        "Luton and Dunstable University Hospital NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2481264266",
      "name": "Ting Fang Tan",
      "affiliations": [
        "Singapore Eye Research Institute",
        "Singapore National Eye Center"
      ]
    },
    {
      "id": "https://openalex.org/A1985247770",
      "name": "Saaeha Rauz",
      "affiliations": [
        "University of Birmingham",
        "Birmingham and Midland Eye Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2619557440",
      "name": "Daniel Shu Wei Ting",
      "affiliations": [
        "Singapore National Eye Center",
        "Singapore Eye Research Institute",
        "University of Nottingham",
        "University of Birmingham",
        "Birmingham and Midland Eye Centre",
        "Stanford University",
        "Duke-NUS Medical School",
        "Smith-Kettlewell Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2619557440",
      "name": "Daniel Shu Wei Ting",
      "affiliations": [
        "Duke-NUS Medical School",
        "Smith-Kettlewell Eye Research Institute",
        "University of Birmingham",
        "Birmingham and Midland Eye Centre",
        "University of Nottingham",
        "Singapore National Eye Center",
        "Stanford University",
        "Singapore Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2794574940",
      "name": "Darren Shu Jeng Ting",
      "affiliations": [
        "University of Nottingham",
        "Singapore National Eye Center",
        "Birmingham and Midland Eye Centre",
        "Stanford University",
        "Singapore Eye Research Institute",
        "Smith-Kettlewell Eye Research Institute",
        "University of Birmingham",
        "Duke-NUS Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A2794574940",
      "name": "Darren Shu Jeng Ting",
      "affiliations": [
        "Stanford University",
        "Duke-NUS Medical School",
        "Smith-Kettlewell Eye Research Institute",
        "Singapore Eye Research Institute",
        "Singapore National Eye Center",
        "Birmingham and Midland Eye Centre",
        "University of Birmingham",
        "University of Nottingham"
      ]
    },
    {
      "id": "https://openalex.org/A3080804641",
      "name": "Arun James Thirunavukarasu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4322225663",
      "name": "Shathar Mahmood",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2261677931",
      "name": "Andrew Malem",
      "affiliations": [
        "Emirates Foundation"
      ]
    },
    {
      "id": null,
      "name": "William Paul Foster",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2981255448",
      "name": "Rohan Sanghera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2635899234",
      "name": "Refaat Hassan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103954715",
      "name": "Sean Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149920119",
      "name": "Shiao Wei Wong",
      "affiliations": [
        "Manchester Royal Eye Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2239391237",
      "name": "Yee-Ling Wong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232737292",
      "name": "Yu Jeat Chong",
      "affiliations": [
        "Manchester Royal Eye Hospital",
        "Birmingham and Midland Eye Centre",
        "Sandwell & West Birmingham Hospitals NHS Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5109628756",
      "name": "Abdullah Shakeel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2981721495",
      "name": "Yin‐Hsi Chang",
      "affiliations": [
        "Chang Gung Memorial Hospital",
        "Linkou Chang Gung Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3039616060",
      "name": "Benjamin Kye Jyn Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1974947386",
      "name": "Nikhil Jain",
      "affiliations": [
        "Luton and Dunstable University Hospital NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2481264266",
      "name": "Ting Fang Tan",
      "affiliations": [
        "Singapore National Eye Center",
        "Singapore Eye Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1985247770",
      "name": "Saaeha Rauz",
      "affiliations": [
        "University of Birmingham",
        "Birmingham and Midland Eye Centre",
        "Sandwell & West Birmingham Hospitals NHS Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2619557440",
      "name": "Daniel Shu Wei Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2794574940",
      "name": "Darren Shu Jeng Ting",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4364378939",
    "https://openalex.org/W6853867480",
    "https://openalex.org/W4317910576",
    "https://openalex.org/W1966976587",
    "https://openalex.org/W4376114558",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W2397288399",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2091254174",
    "https://openalex.org/W4367394076",
    "https://openalex.org/W4229015090",
    "https://openalex.org/W4308885870",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W6852868368",
    "https://openalex.org/W4377011132",
    "https://openalex.org/W4386572315",
    "https://openalex.org/W4220692188",
    "https://openalex.org/W4385833575",
    "https://openalex.org/W4381309843",
    "https://openalex.org/W4388836341",
    "https://openalex.org/W3140124691",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4383301639",
    "https://openalex.org/W4382278861",
    "https://openalex.org/W4378232928",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4386596026"
  ],
  "abstract": "Large language models (LLMs) underlie remarkable recent advanced in natural language processing, and they are beginning to be applied in clinical contexts. We aimed to evaluate the clinical potential of state-of-the-art LLMs in ophthalmology using a more robust benchmark than raw examination scores. We trialled GPT-3.5 and GPT-4 on 347 ophthalmology questions before GPT-3.5, GPT-4, PaLM 2, LLaMA, expert ophthalmologists, and doctors in training were trialled on a mock examination of 87 questions. Performance was analysed with respect to question subject and type (first order recall and higher order reasoning). Masked ophthalmologists graded the accuracy, relevance, and overall preference of GPT-3.5 and GPT-4 responses to the same questions. The performance of GPT-4 (69%) was superior to GPT-3.5 (48%), LLaMA (32%), and PaLM 2 (56%). GPT-4 compared favourably with expert ophthalmologists (median 76%, range 64–90%), ophthalmology trainees (median 59%, range 57–63%), and unspecialised junior doctors (median 43%, range 41–44%). Low agreement between LLMs and doctors reflected idiosyncratic differences in knowledge and reasoning with overall consistency across subjects and types ( p &gt;0.05). All ophthalmologists preferred GPT-4 responses over GPT-3.5 and rated the accuracy and relevance of GPT-4 as higher ( p &lt;0.05). LLMs are approaching expert-level knowledge and reasoning skills in ophthalmology. In view of the comparable or superior performance to trainee-grade ophthalmologists and unspecialised junior doctors, state-of-the-art LLMs such as GPT-4 may provide useful medical advice and assistance where access to expert ophthalmologists is limited. Clinical benchmarks provide useful assays of LLM capabilities in healthcare before clinical trials can be designed and conducted.",
  "full_text": "RESEA RCH ARTICL E\nLarge language models approach expert-level\nclinical knowledge and reasoning in\nophthalmology: A head-to-head cross-\nsectional study\nArun James Thirunavukarasu\nID\n1,2\n*, Shathar Mahmood\n1\n, Andrew Malem\n3\n, William\nPaul Foster\n1,4\n, Rohan Sanghera\nID\n1\n, Refaat Hassan\n1\n, Sean Zhou\n5\n, Shiao Wei Wong\n6\n, Yee\nLing Wong\n6\n, Yu Jeat Chong\n7\n, Abdullah Shakeel\n1\n, Yin-Hsi Chang\n8\n, Benjamin Kye Jyn Tan\n9\n,\nNikhil Jain\n10\n, Ting Fang Tan\n11\n, Saaeha Rauz\nID\n7,12\n, Daniel Shu Wei Ting\n11,13,14\n, Darren Shu\nJeng Ting\n7,12,15\n*\n1 University of Cambridge School of Clinical Medicine, Cambridge , United Kingdom , 2 Oxford University\nClinical Academic Graduate School, University of Oxford , Oxford, United Kingdom, 3 Eye Institute, Cleveland\nClinic Abu Dhabi, Abu Dhabi Emirate , United Arab Emirate s, 4 Departme nt of Physiolog y, Developm ent and\nNeurosc ience, University of Cambridge , Cambridge , United Kingdom, 5 West Suffolk NHS Founda tion Trust,\nBury St Edmund s, United Kingdom , 6 Manchester Royal Eye Hospit al, Manchester University NHS\nFounda tion Trust, Manchester, United Kingdom, 7 Birmin gham and Midland Eye Centre, Sandwel l and West\nBirmingha m NHS Founda tion Trust, Birmin gham, United Kingdom , 8 Department of Ophthalm ology, Chang\nGung Memorial Hospital, Linkou Medical Center, Taoyuan, Taiwan, 9 Yong Loo Lin School of Medicine,\nNational University of Singapore , Singapore , 10 Bedfordshir e Hospitals NHS Foundation Trust, Luton and\nDunstab le, United Kingdom , 11 Singapore Eye Researc h Institute, Singapore National Eye Centre,\nSingapore , Singapore , 12 Academic Unit of Ophthalm ology, Institute of Inflammation and Ageing, Universit y\nof Birmin gham, Birmingha m, United Kingdom, 13 Duke-NUS Medical School, Singapore , Singapore ,\n14 Byers Eye Institute, Stanford Univers ity, Palo Alto, Californi a, United States of America, 15 Academic\nOphthalm ology, School of Medicine, University of Nottingham, Nottingham , United Kingdom\n* ajt205@c antab.ac.uk (AJT); ting.dar ren@gmai l.com (DSJT)\nAbstract\nLarge language models (LLMs) underlie remarkable recent advanced in natural language\nprocessing, and they are beginning to be applied in clinical contexts. We aimed to evaluate\nthe clinical potential of state-of-the-art LLMs in ophthalmolo gy using a more robust bench-\nmark than raw examination scores. We trialled GPT-3.5 and GPT-4 on 347 ophthalmolo gy\nquestions before GPT-3.5, GPT-4, PaLM 2, LLaMA, expert ophthalmologists, and doctors\nin training were trialled on a mock examination of 87 questions. Performance was analysed\nwith respect to question subject and type (first order recall and higher order reasoning).\nMasked ophthalmologis ts graded the accuracy, relevance, and overall preference of GPT-\n3.5 and GPT-4 responses to the same questions. The performance of GPT-4 (69%) was\nsuperior to GPT-3.5 (48%), LLaMA (32%), and PaLM 2 (56%). GPT-4 compared favourably\nwith expert ophthalmologists (median 76%, range 64–90%), ophthalmology trainees\n(median 59%, range 57–63%), and unspecialised junior doctors (median 43%, range 41–\n44%). Low agreement between LLMs and doctors reflected idiosyncratic differences in\nknowledge and reasoning with overall consistency across subjects and types (p>0.05). All\nophthalmologists preferred GPT-4 responses over GPT-3.5 and rated the accuracy and rel-\nevance of GPT-4 as higher (p<0.05). LLMs are approaching expert-leve l knowledge and\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 1 / 16\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Thirunavu karasu AJ, Mahmoo d S, Malem\nA, Foster WP, Sanghera R, Hassan R, et al. (2024)\nLarge language models approach expert-level\nclinical knowledge and reasoning in\nophthalmo logy: A head-to-head cross-sec tional\nstudy. PLOS Digit Health 3(4): e0000341. https://\ndoi.org/10.13 71/journal.pdi g.0000341\nEditor: Man Luo, Mayo Clinic Scottsdal e, UNITED\nSTATES\nReceived: July 31, 2023\nAccepted: February 26, 2024\nPublished: April 17, 2024\nCopyright: © 2024 Thirunavukar asu et al. This is an\nopen access article distributed under the terms of\nthe Creative Commons Attributio n License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All data are available\nas suppleme ntary information, excluding\ncopyrighte d material from the textbook used for\nexperimen ts.\nFunding: DSWT is supported by the National\nMedical Research Council, Singapore (NMCR/\nHSRG/008 7/2018; MOH-000655 -00; MOH-\n001014-00) , Duke-NUS Medical School (Duke-\nNUS/RSF/2 021/0018; 05/FY202 0/EX/15-A58) , and\nAgency for Science, Technology and Research\nreasoning skills in ophthalmology . In view of the comparable or superior performance to\ntrainee-grade ophthalmolo gists and unspecialise d junior doctors, state-of-the-art LLMs\nsuch as GPT-4 may provide useful medical advice and assistance where access to expert\nophthalmologists is limited. Clinical benchmarks provide useful assays of LLM capabilities\nin healthcare before clinical trials can be designed and conducted.\nAuthor summary\nLarge language models (LLMs) are the most sophisticated form of language-based artifi-\ncial intelligence. LLMs have the potential to improve healthcare, and experiments and tri-\nals are ongoing to explore potential avenues for LLMs to improve patient care. Here, we\ntest state-of-the-art LLMs on challenging questions used to assess the aptitude of eye doc-\ntors (ophthalmologists) in the United Kingdom before they can be deemed fully qualified.\nWe compare the performance of these LLMs to fully trained ophthalmologists as well as\ndoctors in training to gauge the aptitude of the LLMs for providing advice to patients\nabout eye health. One of the LLMs, GPT-4, exhibits favourable performance when com-\npared with fully qualified and training ophthalmologists; and comparisons with its prede-\ncessor model, GPT-3.5, indicate that this superior performance is due to improved\naccuracy and relevance of model responses. LLMs are approaching expert-level ophthal-\nmological knowledge and reasoning, and may be useful for providing eye-related advice\nwhere access to healthcare professionals is limited. Further research is required to explore\npotential avenues of clinical deployment.\nIntroduction\nGenerative Pre-trained Transformer 3.5 (GPT-3.5) and 4 (GPT-4) are large language mod-\nels (LLMs) trained on datasets containing hundreds of billions of words from articles,\nbooks, and other internet sources [1, 2]. ChatGPT is an online chatbot which uses GPT-3.5\nor GPT-4 to provide bespoke responses to human users’ queries [3]. LLMs have revolution-\nised the field of natural language processing, and ChatGPT has attracted significant atten-\ntion in medicine for attaining passing level performance in medical school examinations\nand providing more accurate and empathetic messages than human doctors in response to\npatient queries on a social media platform [3,4,5,6]. While GPT-3.5 performance in more\nspecialised examinations has been inadequate, GPT-4 is thought to represent a significant\nadvancement in terms of medical knowledge and reasoning [3,7,8]. Other LLMs in wide use\ninclude Pathways Language Model 2 (PaLM 2) and Large Language Model Meta AI 2\n(LLaMA 2) [3], [9, p. 2], [10].\nApplications and trials of LLMs in ophthalmological settings has been limited despite\nChatGPT’s performance in questions relating to ‘eyes and vision’ being superior to other sub-\njects in an examination for general practitioners [7,11]. ChatGPT has been trialled on the\nNorth American Ophthalmology Knowledge Assessment Program (OKAP), and Fellowship of\nthe Royal College of Ophthalmologists (FRCOphth) Part 1 and Part 2 examinations. In both\ncases, relatively poor results have been reported for GPT-3.5, with significant improvement\nexhibited by GPT-4 [12,13,14,15,16]. However, previous studies are afflicted by two important\nissues which may affect their validity and interpretability. First, so-called ‘contamination’,\nwhere test material features in the pretraining data used to develop LLMs, may result in\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 2 / 16\n(A20H4g2 141; H20C6a 0032). DSJT is supporte d\nby a Medical Research Council / Fight for Sight\nClinical Research Fellowship (MR/T001674/1 ).\nThese funders were not involved in the conception,\nexecution, or reporting of this review.\nCompeting interests : AM is a member of the Panel\nof Examiners of the Royal College of\nOphthalm ologists and performs unpaid work as an\nFRCOphth examiner. DSWT holds a patent on a\ndeep learning system to detect retinal disease.\nDSJT authored the book used in the study and\nreceives royalty from its sales. The other authors\nhave no competing interests to declare.\ninflated performance as models recall previously seen text rather than using clinical reasoning\nto provide an answer. Second, examination performance in and of itself provides little infor-\nmation regarding the potential of models to contribute to clinical practice as a medical-assis-\ntance tool [3]. Clinical benchmarks are required to understanding the meaning and\nimplications of scores in ophthalmological examinations attained by LLMs and are a necessary\nprecursor to clinical trials of LLM-based interventions.\nHere, we used FRCOphth Part 2 examination questions to gauge the ophthalmological\nknowledge base and reasoning capability of LLMs using fully qualified and currently training\nophthalmologists as clinical benchmarks. These questions were not freely available online,\nminimising the risk of contamination. The FRCOphth Part 2 Written Examination tests the\nclinical knowledge and skills of ophthalmologists in training using multiple choice questions\nwith no negative marking and must be passed to fully qualify as a specialist eye doctor in the\nUnited Kingdom.\nMethods\nQuestion extraction\nFRCOphth Part 2 questions were sourced from a textbook for doctors preparing to take the\nexamination [17]. This textbook is not freely available on the internet, making the possibil-\nity of its content being included in LLMs’ training datasets unlikely [1]. All 360 multiple-\nchoice questions from the textbook’s six chapters were extracted, and a 90-question mock\nexamination from the textbook was segregated for LLM and doctor comparisons. Two\nresearchers matched the subject categories of the practice papers’ questions to those defined\nin the Royal College of Ophthalmologists’ documentation concerning the FRCOphth Part 2\nwritten examination. Similarly, two researchers categorised each question as first order\nrecall or higher order reasoning, corresponding to ‘remembering’ and ‘applying’ or ‘analys-\ning’ in Bloom’s taxonomy, respectively [18]. Disagreement between classification decisions\nwas resolved by a third researcher casting a deciding vote. Questions containing non-plain\ntext elements such as images were excluded as these could not be inputted to the LLM\napplications.\nTrialling large language models\nEvery eligible question was inputted into ChatGPT (GPT-3.5 and GPT-4 versions; OpenAI,\nSan Francisco, California, United States of America) between April 29 and May 10, 2023. The\nanswers provided by GPT-3.5 and GPT-4 were recorded and their whole reply to each ques-\ntion was recorded for further analysis. If ChatGPT failed to provide a definitive answer, the\nquestion was re-trialled up to three times, after which ChatGPT’s answer was recorded as ‘null’\nif no answer was provided. Correct answers (‘ground truth’) were defined as the answers pro-\nvided by the textbook and were recorded for every eligible question to facilitate calculation of\nperformance. Upon their release, Bard (Google LLC, Mountain View, California, USA) and\nHuggingChat (Hugging Face, Inc., New York City, USA) were used to trial PaLM 2 (Google\nLLC) and LLaMA (Meta, Menlo Park, California, USA) respectively on the portion of the text-\nbook corresponding to a 90-question examination, adhering to the same procedures between\nJune 20 and July 2, 2023.\nClinical benchmarks\nTo gauge the performance, accuracy, and relevance of LLM outputs, five expert ophthalmolo-\ngists who had all passed the FRCOphth Part 2 (E1-E5), three trainees (residents) currently in\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 3 / 16\nophthalmology training programmes (T1-T3), and two unspecialised (i.e. not in ophthalmol-\nogy training) junior doctors (J1-J2) first answered the 90-question mock examination inde-\npendently, without reference to textbooks, the internet, or LLMs’ recorded answers. As with\nthe LLMs, doctors’ performance was calculated with reference to the correct answers provided\nby the textbook. After completing the examination, ophthalmologists graded the whole output\nof GPT-3.5 and GPT-4 on a Likert scale from 1–5 (very bad, bad, neutral, good, very good) to\nqualitatively appraise accuracy of information provided and relevance of outputs to the ques-\ntion used as an input prompt. For these appraisals, ophthalmologists were blind to the LLM\nsource (which was presented in a randomised order) and to their previous answers to the same\nquestions, but they could refer to the question text and correct answer and explanation pro-\nvided by the textbook. Procedures are comprehensively described in the protocol issued to the\nophthalmologists (S1 Protocol).\nOur null hypothesis was that LLMs and doctors would exhibit similar performance, sup-\nported by results in a wide range of medical examinations [3, 6]. Prospective power analysis\nwas conducted which indicated that 63 questions were required to identify a 10% superior per-\nformance of an LLM to human performance at a 5% significance level (type 1 error rate) with\n80% power (20% type 2 error rate). This indicated that the 90-question examination in our\nexperiments was more than sufficient to detect ~10% differences in overall performance. The\nwhole 90-question mock examination was used to avoid over- or under-sampling certain ques-\ntion types with respect to actual FRCOphth papers. To verify that the mock examination was\nrepresentative of the FRCOphth Part 2 examination, expert ophthalmologists were asked to\nrate the difficulty of questions used here in comparison to official examinations on a 5-point\nLikert scale (“much easier”, “somewhat easier”, “similar”, “somewhat more difficult”, “much\nmore difficult”).\nStatistical analysis\nPerformance of doctors and LLMs were compared using chi-squared (χ\n2\n) tests. Agreement\nbetween answers provided by doctors and LLMs was quantified through calculation of Kappa\nstatistics, interpreted in accordance with McHugh’s recommendations [19]. To further explore\nthe strengths and weaknesses of the answer providers, performance was stratified by question\ntype (first order fact recall or higher order reasoning) and subject using a chi-squared or Fish-\ner’s exact test where appropriate. Likert scale data corresponding to the accuracy and relevance\nof GPT-3.5 and GPT-4 responses to the same questions were analysed with paired t-tests with\nthe Bonferroni correction applied to mitigate the risk of false positive results due to multiple-\ntesting—parametric testing was justified by a sufficient sample size [20]. A chi-squared test\nwas used to quantify the significance of any difference in overall preference of ophthalmolo-\ngists choosing between GPT-3.5 and GPT-4 responses. Statistical significance was concluded\nwhere p < 0.05. For additional contextualisation, examination statistics corresponding to\nFRCOphth Part 2 written examinations taken between July 2017 and December 2022 were col-\nlected from Royal College of Ophthalmologists examiners’ reports [21]. These statistics facili-\ntated comparisons between human and LLM performance in the mock examination with the\nperformance of actual candidates in recent examinations. Failure cases where all LLMs pro-\nvided an incorrect answer were appraised qualitatively to explore any specific weaknesses of\nthe technology.\nStatistical analysis was conducted in R (version 4.1.2; R Foundation for Statistical Comput-\ning, Vienna, Austria), and figures were produced in Affinity Designer (version 1.10.6; Serif\nLtd, West Bridgford, Nottinghamshire, United Kingdom).\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 4 / 16\nResults\nQuestions sources\nOf 360 questions in the textbook, 347 questions (including 87 of the 90 questions from the\nmock examination chapter) were included [17]. Exclusions were all due to non-text elements\nsuch as images and tables which could not be inputted into LLM chatbot interfaces. The distri-\nbution of question types and subjects within the whole set and mock examination set of ques-\ntions is summarised in Table 1 and S1 Table alongside performance.\nGPT-4 represents a significant advance on GPT-3.5 in ophthalmological knowledge and\nreasoning. Overall performance over 347 questions was significantly higher for GPT-4\n(61.7%) than GPT-3.5 (48.41%; χ\n2\n= 12.32, p<0.01), with results detailed in S1 Fig and S1\nTable. ChatGPT performance was consistent across question types and subjects (S1 Table).\nFor GPT-4, no significant variation was observed with respect to first order and higher order\nquestions (χ\n2\n= 0.22, p = 0.64), or subjects defined by the Royal College of Ophthalmologist s\n(Fisher’s exact test over 2000 iterations, p = 0.23). Similar results were observed for GPT-3.5\nwith respect to first and second order questions (χ\n2\n= 0.08, p = 0.77), and subjects (Fisher’s\nexact test over 2000 iterations, p = 0.28). Performance and variation within the 87-question\nmock examination was very similar to the overall performance over 347 questions, and subse-\nquent experiments were therefore restricted to that representative set of questions.\nGPT-4 compares well with other LLMs, junior and trainee doctors and ophthalmology\nexperts. Performance in the mock examination is summarised in Fig 1—GPT-4 (69%) was\nthe top-scoring model, performing to a significantly higher standard than GPT-3.5 (48%; χ\n2\n=\n7.33, p < 0.01) and LLaMA (32%; χ\n2\n= 22.77, p < 0.01), but statistically similarly to PaLM 2\n(56%) despite a superior score (χ\n2\n= 2.81, p = 0.09). LLaMA exhibited the lowest examination\nscore, significantly weaker than GPT-3.5 (χ\n2\n= 4.58, p = 0.03) and PaLM-2 (χ\n2\n= 10.01,\np < 0.01) as well as GPT-4.\nThe performance of GPT-4 was statistically similar to the mean score attained by expert\nophthalmologists (Fig 1; χ\n2\n= 1.18, p = 0.28). Moreover, GPT-4’s performance exceeded the\nmean mark attained across FRCOphth Part 2 written examination candidates between 2017–\n2022 (66.06%), mean pass mark according to standard setting (61.31%), and the mean official\nmark required to pass the examination after adjustment (63.75%), as detailed in S2 Table. In\nindividual comparisons with expert ophthalmologists, GPT-4 was equivalent in 3 cases (χ\n2\ntests, p > 0.05, S3 Table), and inferior in 2 cases (χ\n2\ntests, p < 0.05; Table 2). In comparisons\nwith ophthalmology trainees, GPT-4 was equivalent to all three ophthalmology trainees (χ\n2\ntests, p > 0.05; Table 2). GPT-4 was significantly superior to both unspecialised trainee doctors\n(χ\n2\ntests, p < 0.05; Table 2). Doctors were anonymised in analysis, but their ophthalmological\nexperience is summarised in S3 Table. Unsurprisingly, junior doctors (J1-J2) attained lower\nscores than expert ophthalmologists (E1-E5; t = 7.18, p < 0.01), and ophthalmology trainees\n(T1-T3; t = 11.18, p < 0.01), illustrated in Fig 1. Ophthalmology trainees approached expert-\nlevel scores with no significant difference between the groups (t = 1.55, p = 0.18). None of the\nother LLMs matched any of the expert ophthalmologists, mean mark of real examination can-\ndidates, or FRCOphth Part 2 pass mark.\nExpert ophthalmologists agreed that the mock examination was a faithful representation of\nactual FRCOphth Part 2 Written Examination papers with a mean and median score of 3/5\n(range 2-4/5).\nLLM strengths and weaknesses are similar to doctors. Agreement between answers\ngiven by LLMs, expert ophthalmologists, and trainee doctors was generally absent (0 � κ <\n0.2), minimal (0.2 � κ < 0.4), or weak (0.4 � κ < 0.6), with moderate agreement only\nrecorded for one pairing between the two highest performing ophthalmologists (Fig 2; κ =\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 5 / 16\nTable 1. Examinati on characteris tics and granular performa nce data. Questi on subject and type distributions presented alongside scores attaine d by LLMs (GPT-3.5, GPT-4, LLaMA, and PaLM\n2), expert ophthalmol ogists (E1-E5), ophthalmol ogy trainees (T1-T3), and unspecial ised junior doctors (J1-J2). Median scores do not necessari ly sum to the overall median score, as fractiona l scores\nare impossi ble.\nQuestion source Question subset Large language model performance Ophthalmologist performance Trainee performance Junior performance\nGPT-3.5 GPT-4 LLaMA PaLM 2 E1 E2 E3 E4 E5 Median T1 T2 T3 Median J1 J2 Median\nMock examination Overall (N = 87) 42 (48%) 60 (69%) 28 (32%) 49 (56%) 78 (90%) 56 (64%) 66 (76%) 60 (69%) 72 (83%) 66 (76%) 63 (72%) 57 (66%) 59 (68%) 59 (68%) 38 (44%) 36 (41%) 37 (43%)\nFirst order fact recall (n = 59) 29 (49%) 43 (73%) 19 (32%) 34 (58%) 52 (88%) 40 (68%) 47 (80%) 37 (63%) 46 (78%) 46 (78%) 44 (75%) 41 (69%) 41 (69%) 41 (69%) 24 (42%) 25 (42%) 25 (42%)\nHigher order reasoning (n = 28) 13 (46%) 17 (61%) 9 (32%) 15 (54%) 26 (93%) 16 (57%) 19 (68%) 23 (82%) 26 (93%) 23 (82%) 19 (68%) 16 (57%) 18 (64%) 18 (64%) 14 (50%) 11 (39%) 13 (46%)\nCataract (n = 3) 0 (0%) 1 (33%) 1 (33%) 1 (33%) 3 (100%) 2 (67%) 3 (100%) 3 (100%) 3 (100%) 3 (100%) 2 (67%) 3 (100%) 2 (67%) 2 (67%) 2 (67%) 1 (33%) 2 (67%)\nCornea and external eye (n = 8) 3 (38%) 4 (50%) 3 (38%) 1 (13%) 7 (88%) 5 (63%) 5 (63%) 7 (88%) 7 (88%) 7 (88%) 8 (100%) 6 (75%) 7 (88%) 7 (88%) 4 (50%) 4 (50%) 4 (50%)\nEthics (n = 1) 1 (100%) 0 (0%) 1 (100%) 0 (0%) 1 (100%) 1 (100%) 1 (100%) 1 (100%) 0 (0%) 1 (100%) 1 (100%) 1 (100%) 1 (100%) 1 (100%) 0 (0%) 1 (100%) 1 (100%)\nGenetics (n = 6) 3 (50%) 5 (83%) 0 (0%) 4 (67%) 4 (67%) 4 (67%) 3 (50%) 3 (50%) 3 (50%) 3 (50%) 4 (67%) 1 (17%) 2 (33%) 2 (33%) 0 (0%) 2 (33%) 1 (17%)\nGlaucoma (n = 3) 1 (33%) 2 (67%) 2 (67%) 2 (67%) 3 (100%) 2 (67%) 3 (100%) 3 (100%) 3 (100%) 3 (100%) 3 (100%) 1 (33%) 2 (67%) 2 (67%) 0 (0%) 1 (33%) 1 (33%)\nGuidelines (n = 3) 1 (33%) 2 (67%) 0 (0%) 0 (0%) 2 (67%) 1 (33%) 2 (67%) 2 (67%) 3 (100%) 2 (67%) 3 (100%) 2 (67%) 3 (100%) 3 (100%) 3 (100%) 1 (33%) 2 (67%)\nNeuro-imaging (n = 0) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA) 0 (NA)\nNeuro-ophthalmology (n = 12) 8 (67%) 11 (92%) 4 (33%) 8 (67%) 12 (100%) 7 (58%) 9 (75%) 6 (50%) 10 (83%) 9 (75%) 8 (67%) 8 (67%) 8 (67%) 8 (67%) 8 (67%) 5 (42%) 7 (58%)\nOphthalmic investigations (n = 7) 3 (43%) 4 (57%) 2 (29%) 3 (43%) 6 (86%) 4 (57%) 7 (100%) 5 (71%) 7 (100%) 6 (86%) 4 (57%) 3 (43%) 6 (86%) 4 (57%) 3 (43%) 2 (29%) 3 (43%)\nOrbit and oculoplastics (n = 5) 3 (60%) 2 (40%) 2 (40%) 2 (40%) 5 (100%) 4 (80%) 4 (80%) 2 (40%) 3 (60%) 4 (80%) 4 (80%) 4 (80%) 3 (60%) 4 (80%) 0 (0%) 2 (40%) 1 (20%)\nOrthoptic investigations (n = 2) 0 (0%) 2 (100%) 1 (50%) 2 (100%) 2 (100%) 1 (50%) 1 (50%) 1 (50%) 2 (100%) 1 (50%) 1 (50%) 2 (100%) 1 (50%) 1 (50%) 2 (100%) 0 (0%) 1 (50%)\nPaediatric ophthalmology (n = 5) 1 (20%) 1 (20%) 2 (40%) 3 (60%) 5 (100%) 2 (40%) 4 (80%) 4 (80%) 5 (100%) 4 (80%) 2 (40%) 3 (60%) 2 (40%) 2 (40%) 1 (20%) 1 (20%) 1 (20%)\nPharmacology (n = 5) 3 (60%) 3 (60%) 1 (20%) 4 (80%) 4 (80%) 4 (80%) 4 (80%) 4 (80%) 3 (60%) 4 (80%) 2 (40%) 3 (60%) 4 (80%) 3 (60%) 2 (40%) 2 (40%) 2 (40%)\nResearch (n = 1) 1 (100%) 1 (100%) 0 (0%) 1 (100%) 1 (100%) 0 (0%) 0 (0%) 1 (100%) 1 (100%) 1 (100%) 1 (100%) 1 (100%) 0 (0%) 1 (100%) 0 (0%) 0 (0%) 0 (0%)\nRetina (n = 10) 5 (50%) 8 (80%) 3 (30%) 8 (80%) 8 (80%) 8 (80%) 7 (70%) 5 (50%) 7 (70%) 7 (70%) 6 (60%) 8 (80%) 7 (70%) 7 (70%) 4 (40%) 4 (40%) 4 (40%)\nStatistics (n = 2) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 1 (50%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%)\nStrabismus (n = 2) 1 (50%) 1 (50%) 0 (0%) 1 (50%) 2 (100%) 1 (50%) 1 (50%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 1 (50%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%)\nTrauma (n = 2) 0 (0%) 2 (100%) 0 (0%) 0 (0%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 2 (100%) 1 (50%) 0 (0%) 1 (50%)\nUveitis and oncology (n = 10) 6 (60%) 9 (90%) 4 (40%) 7 (70%) 9 (90%) 6 (60%) 8 (80%) 8 (80%) 9 (90%) 8 (80%) 8 (80%) 6 (60%) 5 (50%) 6 (60%) 4 (40%) 6 (60%) 5 (50%)\nhttps://doi.org/10 .1371/journal.p dig.0000341. t001\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 6 / 16\nFig 1. FRCOpht h Part 2 performa nce of LLMs and doctors of variable experti se. Examination performan ce in the 87-question mock examination used to\ntrial LLMs (GPT-3.5, GPT-4, LLaMA, and PaLM 2), expert ophthal mologists (E1-E5), ophthalmol ogy trainees (T1-T3), and unspec ialised junior doctors\n(J1-J2). Dotted lines depict the mean performance of expert ophthalmolog ists (66/87; 76%), ophthal mology trainees (60/87; 69%), and unspecialise d junior\ndoctors (37/87; 43%). The performan ce of GPT-4 lay within the range of expert ophthalmol ogists and ophthalmol ogy trainees.\nhttps://doi.org/10 .1371/journal.p dig.0000341. g001\nTable 2. GPT-4 compares favoura bly with LLMs and doctors. Results of pair-wis e comparison s of examinat ion per-\nformanc e between GPT-4 and the other answer provid ers. Significant ly greater performance for GPT-4 is highlighted\ngreen, significantly inferior performance for GPT-4 is highlighted orange. GPT-4 was superior to all other LLMs and\nunspecial ised junior doctors, and equivalent to most expert ophthal mologists and all ophthalmolog y trainees.\nAnswer provider Score (max = 87) χ2 p value\nGPT-4 60 Reference\nGPT-3.5 42 7.68 0.01\nLLaMA 28 23.54 <0.01\nPaLM 2 49 2.97 0.08\nE1 78 11.35 <0.01\nE2 56 0.41 0.52\nE3 66 1.04 0.31\nE4 60 0.00 1.00\nE5 72 4.52 0.03\nT1 63 0.25 0.62\nT2 57 0.23 0.63\nT3 59 0.03 0.87\nJ1 38 11.31 <0.01\nJ2 36 13.39 <0.01\nhttps://d oi.org/10.1371/j ournal.pdig. 0000341.t0 02\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 7 / 16\n0.64) [19]. Disagreement was primarily the result of general differences in knowledge and rea-\nsoning ability, illustrated by strong negative correlation between Kappa statistic (quantifying\nagreement) and difference in examination performance (Pearson’s r = -0.63, p < 0.01). Answer\nproviders with more similar scores exhibited greater agreement overall irrespective of their\ncategory (LLM, expert ophthalmologist, ophthalmology trainee, or junior doctor).\nStratification analysis was undertaken to identify any specific strengths and weaknesses of\nLLMs with respect to expert ophthalmologists and trainee doctors (Table 1 and S4 Table). No\nsignificant difference between performance in first order fact recall and higher order reasoning\nquestions was observed among any of the LLMs, expert ophthalmologists, ophthalmology\ntrainees, or unspecialised junior doctors (S4 Table; χ\n2\ntests, p > 0.05). Similarly, only J1 (junior\ndoctor yet to commence ophthalmology training) exhibited statistically significant variation in\nperformance between subjects (S4 Table; Fisher’s exact tests over 2000 iterations, p = 0.02); all\nother doctors and LLMs exhibited no significant variation (Fisher’s exact tests over 2000\nFig 2. Heat map of Kappa statistics quantifyi ng agreemen t between answers given by LLMs, expert\nophthalmo logists, and trainee doctors. Agreeme nt correlates strongly with overall performan ce and stratificati on\nanalysis found no particular question type or subject was associated with better performan ce of LLMs or doctors,\nindicating that LLM knowled ge and reasoning ability is general across ophthalmolog y rather than restricted to\nparticular subspecial ties or question types.\nhttps://d oi.org/10.1371/j ournal.pdig. 0000341.g002\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 8 / 16\niterations, p > 0.05). To explore whether consistency was due to an insufficient sample size,\nsimilar analyses were run for GPT-3.5 and GPT-4 performance over the larger set of 347 ques-\ntions (S1 Table; S4 Table). As with the mock examination, no significant differences in perfor-\nmance across question types (S4 Table; χ\n2\ntests, p > 0.05) or subjects (S4 Table; Fisher’s exact\ntests over 2000 iterations, p > 0.05) were observed.\nLLM examination performance translates to subjective preference indicated by expert\nophthalmologists. Ophthalmologists’ appraisal of GPT-4 and GPT-3.5 outputs indicated a\nmarked preference for the former over the latter, mirroring objective performance in the\nmock examination and over the whole textbook. GPT-4 exhibited significantly (t-test with\nBonferroni correction, p < 0.05) higher accuracy and relevance than GPT-3.5 according to all\nfive ophthalmologists’ grading (Table 3). Differences were visually obvious, with GPT-4 exhib-\niting much higher rates of attaining the highest scores for accuracy and relevance than GPT-\n3.5 (Fig 3). This superiority was reflected in ophthalmologists’ qualitative preference indica-\ntions: GPT-4 responses were preferred to GPT-3.5 responses by every ophthalmologist with\nstatistically significant skew in favour of GPT-4 (χ\n2\ntest, p < 0.05; Table 3).\nFailure cases exhibit no association with subject, complexity, or human answers. The\nLLM failure cases—where every LLM provided an incorrect answer—are summarised in\nTable 4. While errors made by LLMs were occasionally similar to those made by trainee oph-\nthalmologists and junior doctors, this association was not consistent (Table 4). There was no\npreponderance of ophthalmological subject or first or higher order questions in the failure\ncases, and questions did not share a common theme, sentence structure, or grammatical con-\nstruct (Table 4). Examination questions are redacted here to avoid breaching copyright and\nprevent future LLMs accessing the test data during pretraining but can be provided on request.\nDiscussion\nHere, we present a clinical benchmark to gauge the ophthalmological performance of LLMs,\nusing a source of questions with very low risk of contamination as the utilised textbook is not\nfreely available online [17]. Previous studies have suggested that ChatGPT can provide useful\nresponses to ophthalmological queries, but often use online question sources which may have\nfeatured in LLMs’ pretraining datasets [7, 12, 15, 22]. In addition, our employment of multiple\nLLMs as well as fully qualified and training doctors provides novel insight into the potential\nand limitations of state-of-the-art LLMs through head-to-head comparisons which provide\nclinical context and quantitative benchmarks of competence in ophthalmology. Subsequent\nresearch may leverage our questions and results to gauge the performance of new LLMs and\napplications as they emerge.\nWe make three primary observations. First, performance of GPT-4 compares well to expert\nophthalmologists and ophthalmology trainees, and exhibits pass-worthy performance in an\nTable 3. GPT-4 responses are preferred to GPT-3.5 responses by expert ophthalmolo gists. t-test results with Bonferron i correction applied showing the superior accu-\nracy and relevance of GPT-4 responses relative to GPT-3. 5 responses in the opinion of five fully trained ophthalmolog ists (positive mean differences favour GPT-4), and χ\n2\ntest showing that GPT-4 response s were preferred to GPT-3.5 response s by every ophthalmol ogist in their blinded qualitativ e apprais als.\nGrader Accurac y Relevance Overall preference\nMean differen ce t statistic p value Mean difference t statistic p value GPT-4:GPT- 3.5 χ2 p value\nE1 0.60 3.78 <0.01 0.54 4.38 <0.01 61:26 14.08 <0.01\nE2 0.93 4.89 <0.01 0.80 5.16 <0.01 65:22 21.25 <0.01\nE3 0.74 5.65 <0.01 1.27 7.53 <0.01 63:24 17.48 <0.01\nE4 0.84 4.18 <0.01 0.46 3.15 0.01 65:22 21.25 <0.01\nE5 0.59 4.15 <0.01 0.51 5.76 <0.01 72:15 37.35 <0.01\nhttps://do i.org/10.1371/j ournal.pdig. 0000341.t00 3\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 9 / 16\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 10 / 16\nFRCOphth Part 2 mock examination. PaLM 2 did not attain pass-worthy performance or\nmatch expert ophthalmologists’ scores but was within the spread of trainee doctors’ perfor-\nmance. LLMs are approaching human expert-level knowledge and reasoning in ophthalmol-\nogy, and significantly exceed the ability of non-specialist clinicians (represented here by\nunspecialised junior doctors) to answer ophthalmology questions. Second, clinician grading of\nmodel outputs suggests that GPT-4 exhibits improved accuracy and relevance when compared\nwith GPT-3.5. Development is producing models which generate better outputs to ophthalmo-\nlogical queries in the opinion of expert human clinicians, which suggests that models are\nbecoming more capable of providing useful assistance in clinical settings. Third, LLM perfor-\nmance was consistent across question subjects and types, distributed similarly to human per-\nformance, and exhibited comparable agreement between other LLMs and doctors when\ncorrected for differences in overall performance. Together, this indicates that the ophthalmo-\nlogical knowledge and reasoning capability of LLMs is general rather than limited to certain\nsubspecialties or tasks. LLM-driven natural language processing seems to facilitate similar—\nalthough idiosyncratic—clinical knowledge and reasoning to human clinicians, with no obvi-\nous blind spots precluding clinical use.\nSimilarly dramatic improvements in the performance of GPT-4 relative to GPT-3.5 have\nbeen reported in the context of the North American Ophthalmology Knowledge Assessment\nProgram (OKAP) [13, 15]. State-of-the-art models exhibit far more clinical promise than their\npredecessors, and expectations and development should be tailored accordingly. Results from\nthe OKAP also suggest that improvement in performance is due to GPT-4 being more well-\nrounded than GPT-3.5 [13]. This increases the scope for potential applications of LLMs in\nophthalmology, as development is eliminating weaknesses rather than optimising in narrow\ndomains. This study shows that well-rounded LLM performance compares well with expert\nophthalmologists, providing clinically relevant evidence that LLMs may be used to provide\nmedical advice and assistance. Further improvement is expected as multimodal foundation\nmodels, perhaps based on LLMs such as GPT-4, emerge and facilitate compatibility with\nimage-rich ophthalmological data [3, 23, 24].\nFig 3. Accuracy and relevan ce of GPT-3.5 and GPT-4 in response to ophthalmo logical questio ns. Accuracy (A) and relevance (B)\nratings were provided by five expert ophthalmol ogists for ChatGPT (powered by GPT-3.5 and GPT-4) responses to 87 FRCOp hth\nPart 2 mock examin ation questions . In every case, the accuracy and relevance of GPT-4 is significan tly superior to GPT-3.5 (t-test\nwith Bonferron i correct applied, p < 0.05). Pooled scores for accuracy (C) and relevance (D) from all five raters are presented in the\nbottom two plots, with GPT-3.5 (left bars) compared directly with GPT-4 (right bars).\nhttps:// doi.org/10.1371 /journal.pdig .0000341.g003\nTable 4. LLMs do not exhibit consisten t weakness es. Summary of LLM failure cases, where all models provid ed an incorrec t answer to the FRCOphth Part 2 mock\nexaminat ion question. No associations were found with human answers, complexity , subject, theme, sentence structur e, or grammatic constructs.\nQuestion Order Correct Category GPT-3.5 GPT-4 LLaMA PaLM 2 E1 E2 E3 E4 E5 T1 T2 T3 J1 J2\n6 2 A Cornea and external eye D D C D A A A A A A D A D D\n15 2 B Glaucoma D C D D B A B B B B C C C D\n20 1 C Cataract B B D D C C C C C C C C C D\n24 1 D Uveitis and oncology C C C C D D D D C D C C A C\n47 1 D Orbit and oculop lastics A A A ? D D D A D D D B A D\n51 2 A Strabismus B B B B A B B A A A D A A A\n55 2 C Paediatri c ophthalmo logy D B B B C B C C C B C B B D\n82 1 B Ophthalm ic investigati ons A A ? D B B B B B D B B D B\n86 2 A Guidelines B B B C A B A A A A A A A D\nhttps://do i.org/10.1371/j ournal.pdig. 0000341.t00 4\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 11 / 16\nLimitations\nThis study was limited by three factors. First, examination performance is an unvalidated indi-\ncator of clinical aptitude. We sought to ameliorate this limitation by employing expert ophthal-\nmologists, ophthalmology trainees, and unspecialised junior doctors answering the same\nquestions as clinical benchmarks; and compared LLM performance to real cohorts of candi-\ndates in recent FRCOphth examinations. However, it remains an issue that comparable perfor-\nmance to clinical experts in an examination does not necessarily demonstrate that an LLM can\ncommunicate with patients and practitioners or contribute to clinical decision making accu-\nrately and safely. Early trials of LLM chatbots have suggested that LLM responses may be\nequivalent or even superior to human doctors in terms of accuracy and empathy, and experi-\nments using complicated case studies suggest that LLMs operate well even outside typical pre-\nsentations and more common medical conditions [4,25,26]. In ophthalmology, GPT-3.5 and\nGPT-4 have been shown to be capable of providing precise and suitable triage decisions when\nqueried with eye-related symptoms [22,27]. Further work is now warranted in conventional\nclinical settings.\nSecond, while the study was sufficiently powered to detect a less than 10% difference in\noverall performance, the relatively small number of questions in certain categories used for\nstratification analysis may mask significant differences in performance. Testing LLMs and cli-\nnicians with more questions may help establish where LLMs exhibit greater or lesser ability in\nophthalmology. Furthermore, researchers using different ways to categorise questions may be\nable to identify specific strengths and weaknesses of LLMs and doctors which could help guide\ndesign of clinical LLM interventions.\nFinally, experimental tasks were ‘zero-shot’ in that LLMs were not provided with any exam-\nples of correctly answered questions before it was queried with FRCOphth questions from the\ntextbook. This mode of interrogation entails the maximal level of difficulty for LLMs, so it is\nconceivable that the ophthalmological knowledge and reasoning encoded within these models\nis actually even greater than indicated by results here [1]. Future research may seek to fine-\ntune LLMs by using more domain-specific text during pretraining and fine-tuning, or by pro-\nviding examples of successfully completed tasks to further improve performance in that clini-\ncal task [3].\nFuture directions\nAutonomous deployment of LLMs is currently precluded by inaccuracy and fact fabrication.\nOur study found that despite meeting expert standards, state-of-the-art LLMs such as GPT-4 do\nnot match top-performing ophthalmologists [28]. Moreover, there remain controversial ethical\nquestions about what roles should and should not be assigned to inanimate AI models, and to\nwhat extent human clinicians must remain responsible for their patients [3]. However, the\nremarkable performance of GPT-4 in ophthalmology examination questions suggests that\nLLMs may be able to provide useful input in clinical contexts, either to assist clinicians in their\nday-to-day work or with their education or preparation for examinations [3,13,14,27]. Further\nimprovement in performance may be obtained by specific fine-tuning of models with high qual-\nity ophthalmological text data, requiring curation and deidentification [29]. GPT-4 may prove\nespecially useful where access to ophthalmologists is limited: provision of advice, diagnosis, and\nmanagement suggestions by a model with FRCOphth Part 2-level knowledge and reasoning\nability is likely to be superior to non-specialist doctors and allied healthcare professionals work-\ning without support, as their exposure to and knowledge of eye care is limited [27,30,31].\nHowever, close monitoring is essential to avoid mistakes caused by inaccuracy or fact fabri-\ncation [32]. Clinical applications would also benefit from an uncertainty indicator reducing\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 12 / 16\nthe risk of erroneous decisions [7]. As LLM performance often correlates with the frequency of\nquery terms’ representation in the model’s training dataset, a simple indicator of ‘familiarity’\ncould be engineered by calculating the relative frequency of query term representation in the\ntraining data [7,33]. Users could appraise familiarity to temper their confidence in answers\nprovided by the LLM, perhaps reducing error. Moreover, ophthalmological applications\nrequire extensive validation, preferably with high quality randomised controlled trials to con-\nclusively demonstrate benefit (or lack thereof) conferred to patients by LLM interventions\n[34]. Trials should be pragmatic so as not to inflate effect sizes beyond what may generalise to\npatients once interventions are implemented at scale [34,35]. In addition to patient outcomes,\npractitioner-related variables should also be considered: interventions aiming to improve effi-\nciency should be specifically tested to ensure that they reduce rather than increase clinicians’\nworkload [3].\nConclusion\nAccording to comparisons with expert and trainee doctors, state-of-the-art LLMs are\napproaching expert-level performance in advanced ophthalmology questions. GPT-4 attains\npass-worthy performance in FRCOphth Part 2 questions and exceeds the scores of some expert\nophthalmologists. As top-performing doctors exhibit superior scores, LLMs do not appear\ncapable of replacing ophthalmologists, but state-of-the-art models could provide useful advice\nand assistance to non-specialists or patients where access to eye care professionals is limited\n[27,28]. Further research is required to design LLM-based interventions which may improve\neye health outcomes, validate interventions in clinical trials, and engineer governance struc-\ntures to regulate LLM applications as they begin to be deployed in clinical settings [36].\nSupporting information\nS1 Fig. ChatGPT performance in questions taken from the whole textbook. Mosaic plot\ndepicting the overall performance of ChatGPT versions powered by GPT-3.5 and GPT-4 in\n360 FRCOphth Part 2 written examination questions. Performance was significantly higher\nfor GPT-4 than GPT-3.5, and was close to mean human examination candidate performance\nand pass mark set by standard setting and after adjustment.\n(EPS)\nS1 Table. Question characteristics and performance of GPT-3.5 and GPT-4 over the whole\ntextbook. Similar observations were noted here to the smaller mock examination used for sub-\nsequent experiments. GPT-4 performs to a significantly higher standard than GPT-3.5\n(XLSX)\nS2 Table. Examination statistics corresponding to FRCOphth Part 2 written examinations\nsat between July 2017-December 2022.\n(XLSX)\nS3 Table. Experience of expert ophthalmologists (E1-E5), ophthalmology trainees (T1-T3),\nand unspecialised junior doctors (J1-J2) involved in experiments.\n(XLSX)\nS4 Table. Results of statistical tests of variation in performance between question subjects\nand types, for each trialled LLM, expert ophthalmologist, and trainee doctor. Statistically\nsignificant results are highlighted in green.\n(XLSX)\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 13 / 16\nS1 Protocol. Procedures followed by ophthalmologists to grade the output of GPT-3.5 and\nGPT-4 in terms of accuracy, relevance, and rater-preference of model outputs.\n(PDF)\nAcknowledgmen ts\nThe authors extend their thanks to Mr Arunachalam Thirunavukaras u (Betsi Cadwaladr Uni-\nversity Health Board) for his advice and assistance with recruitment.\nAuthor Contributions\nConceptualization: Arun James Thirunavukarasu, Darren Shu Jeng Ting.\nData curation: Arun James Thirunavukarasu, Shathar Mahmood, Andrew Malem, William\nPaul Foster, Rohan Sanghera, Refaat Hassan, Sean Zhou, Shiao Wei Wong, Yee Ling Wong,\nYu Jeat Chong, Abdullah Shakeel, Yin-Hsi Chang, Benjamin Kye Jyn Tan, Nikhil Jain, Ting\nFang Tan.\nFormal analysis: Arun James Thirunavukarasu, Darren Shu Jeng Ting.\nFunding acquisition: Arun James Thirunavukarasu, Daniel Shu Wei Ting, Darren Shu Jeng\nTing.\nInvestigation: Arun James Thirunavukarasu, Shathar Mahmood, Andrew Malem, William\nPaul Foster, Rohan Sanghera, Refaat Hassan, Sean Zhou, Shiao Wei Wong, Yee Ling Wong,\nYu Jeat Chong, Abdullah Shakeel, Yin-Hsi Chang, Benjamin Kye Jyn Tan, Nikhil Jain, Ting\nFang Tan.\nMethodology: Arun James Thirunavukaras u, Darren Shu Jeng Ting.\nProject administration: Arun James Thirunavukarasu, Nikhil Jain, Daniel Shu Wei Ting,\nDarren Shu Jeng Ting.\nResources: Arun James Thirunavukarasu.\nSoftware: Arun James Thirunavukarasu.\nSupervision: Arun James Thirunavukarasu, Darren Shu Jeng Ting.\nValidation: Arun James Thirunavukarasu.\nVisualization: Arun James Thirunavukarasu .\nWriting – original draft: Arun James Thirunavukarasu , William Paul Foster, Yin-Hsi Chang,\nDarren Shu Jeng Ting.\nWriting – review & editing: Arun James Thirunavukarasu, Shathar Mahmood, Andrew\nMalem, William Paul Foster, Rohan Sanghera, Sean Zhou, Shiao Wei Wong, Yee Ling\nWong, Yu Jeat Chong, Abdullah Shakeel, Yin-Hsi Chang, Benjamin Kye Jyn Tan, Nikhil\nJain, Ting Fang Tan, Saaeha Rauz, Darren Shu Jeng Ting.\nReferences\n1. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language Models are Few-Sho t\nLearners. In: Advances in Neural Informatio n Processin g Systems [Internet]. Curran Associates, Inc.;\n2020 [cited 2023 Jan 30]. p. 1877–901. Availab le from: https://paper s.nips.c c/paper/202 0/hash/\n1457c0d 6bfcb49674 18bfb8ac142f 64a-Abstract. html\n2. OpenAI. GPT-4 Technical Report [Internet]. arXiv; 2023 [cited 2023 Apr 11]. Available from: http://arx iv.\norg/abs/2 303.08774\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 14 / 16\n3. Thirunav ukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language models\nin medicine. Nat Med. 2023 Jul 17; 29:1930–4 0. https://doi.or g/10.1038/ s41591-023- 02448-8 PMID:\n37460753\n4. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing Physician and Artificial\nIntelligenc e Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. JAMA\nInternal Medicine [Internet ]. 2023 Apr 28 [cited 2023 Apr 28]; Available from: https://doi.or g/10.1001/\njamaintern med.2023. 1838 PMID: 37115527\n5. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How Does ChatGPT Perform on\nthe United States Medical Licensing Examinati on? The Implications of Large Language Models for\nMedical Education and Knowled ge Assessme nt. JMIR Med Educ. 2023; 9(10168451 8):e45312.\n6. Kung TH, Cheatham M, Medenil la A, Sillos C, Leon LD, Elepaño C, et al. Perform ance of ChatGP T on\nUSMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health.\n2023 Feb 9; 2(2):e0000 198. https://doi.or g/10.137 1/journal.pd ig.0000198 PMID: 36812645\n7. Thirunav ukarasu AJ, Hassan R, Mahmood S, Sanghera R, Barzangi K, Mukashfi ME, et al. Trialling a\nLarge Language Model (ChatGPT) in General Practice With the Applied Knowled ge Test: Observa-\ntional Study Demonstrating Opportuni ties and Limitation s in Primary Care. JMIR Medical Educatio n.\n2023 Apr 21; 9(1):e4659 9. https://d oi.org/10.219 6/46599 PMID: 37083633\n8. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabiliti es of GPT-4 on Medical Challenge Prob-\nlems [Internet] . arXiv; 2023 [cited 2023 Mar 26]. Availab le from: http://arxiv .org/abs/230 3.13375\n9. Google. PaLM 2 Technic al Report [Interne t]. 2023 [cited 2023 May 11]. Available from: https://ai.\ngoogle/sta tic/documen ts/palm2techr eport.pdf\n10. Touvron H, Martin L, Stone K. Llama 2: Open Foundation and Fine-Tu ned Chat Models [Internet]. 2023.\nAvailable from: https://ai.m eta.com/resea rch/publ ications/lla ma-2-open- foundation -and-fine-tune d-\nchat-mode ls/\n11. Ting DSJ, Tan TF, Ting DSW. ChatGP T in ophthalm ology: the dawn of a new era? Eye (Lond). 2023\nJun 27; https://doi.or g/10.1038/ s41433-023- 02619-4 PMID: 373697 64\n12. Antaki F, Touma S, Milad D, El-Khoury J, Duval R. Evaluating the Perform ance of ChatGP T in Ophthal-\nmology: An Analysis of its Succes ses and Shortco mings. Ophthalmolog y Science [Interne t]. 2023 May\n4 [cited 2023 May 8];0(0). Available from: https://www .ophthalm ologyscience.o rg/artic le/S2666-9 145\n(23)00056 -8/fulltex t https://doi.or g/10.1016/ j.xops.2023 .100324 PMID: 37334036\n13. Teebagy S, Colwell L, Wood E, Yaghy A, Faustina M. Improved Perform ance of ChatGP T-4 on the\nOKAP Exam: A Comp arative Study with ChatGP T-3.5 [Internet ]. medRxiv; 2023 [cited 2023 Apr 23].\np. 2023.04 .03.23287957. Availab le from: https://ww w.medrxiv. org/content/ 10.1101/2023.0 4.03.\n23287957v 1\n14. Raimondi R, Tzoumas N, Salisbury T, Di Simplicio S, Romano MR. Comp arative analysis of large lan-\nguage models in the Royal College of Ophthalmo logists fellowship exams. Eye. 2023 May 9;1–4.\nhttps://doi.or g/10.103 8/s41433 -023-02563 -3 PMID: 37161074\n15. Mihalache A, Popovic MM, Muni RH. Perform ance of an Artifici al Intelligence Chatbot in Ophthalm ic\nKnowled ge Assessme nt. JAMA Ophthalmolog y. 2023 Jun 1; 141(6):589 –97. https://doi.or g/10.100 1/\njamaoph thalmol.20 23.1144 PMID: 37103928\n16. Thirunav ukarasu AJ. ChatGPT cannot pass FRCOphth examinatio ns: implication s for ophthal mology\nand large language model artificial intelligence. Eye News [Interne t]. 2023 Apr 26 [cited 2023 Apr 26];\n30(1). Available from: https://www .eyenews.u k.com/featu res/ophthalm ology/post/cha tgpt-cann ot-pass-\nfrcophth-e xamination s-implicatio ns-for-ophth almolog y-and-larg e-language -model-artifi cial-intellige nce\n17. Ting DSJ, Steel D. MCQs for FRCOph th Part 2. Oxford University Press; 2020. 253 p.\n18. Adams NE. Bloom’s taxonomy of cognitive learning objectives. J Med Libr Assoc. 2015 Jul; 103\n(3):152–3. https://doi.o rg/10.3163/15 36-5050.10 3.3.010 PMID: 262135 09\n19. McHugh ML. Interrater reliability: the kappa statistic . Biochemia Medica. 2012 Oct 15; 22(3):276– 82.\nhttps://doi.or g/10.101 6/j.jocd.20 12.03.005 PMID: 230920 60\n20. Sullivan GM, Artino AR. Analyzing and Interpreting Data From Likert- Type Scales. J Grad Med Educ.\n2013 Dec; 5(4):541–2 . https:// doi.org/10.43 00/JGME -5-4-18 PMID: 24454995\n21. Part 2 Written FRCOp hth Exam [Internet]. The Royal College of Ophth almologist s. [cited 2023 Jan 30].\nAvailable from: https://www .rcophth.a c.uk/exami nations/rco phth-exam s/part-2-writt en-frcophth- exam/\n22. Tsui JC, Wong MB, Kim BJ, Maguire AM, Scoles D, VanderBee k BL, et al. Appropria teness of ophthal-\nmic symptoms triage by a popula r online artificia l intelligence chatbot. Eye. 2023 Apr 29;1–2. https://doi.\norg/10.1038/ s41433-023- 02556-2 PMID: 371206 56\n23. Nath S, Marie A, Ellershaw S, Korot E, Keane PA. New meaning for NLP: the trials and tribulations of\nnatural language processing with GPT-3 in ophthalm ology. British Journal of Ophthalmolog y. 2022 Jul\n1; 106(7):889 –92. https:/ /doi.org/10.11 36/bjop hthalmol-2022 -321141 PMID: 355235 34\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 15 / 16\n24. Kline A, Wang H, Li Y, Dennis S, Hutch M, Xu Z, et al. Multimodal machine learning in precision health:\nA scoping review. npj Digit Med. 2022 Nov 7; 5(1):1–14.\n25. Kulkarni PA, Singh H. Artificial Intelligenc e in Clinical Diagnosis : Opportuni ties, Challeng es, and Hype.\nJAMA [Internet]. 2023 Jul 6 [cited 2023 Jul 7]; Availab le from: https://doi. org/10.1001/j ama.2023 .11440\n26. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical\nknowledge. Nature. 2023 Jul 12;1–9.\n27. Waisberg E, Ong J, Zaman N, Kamran SA, Sarker P, Tavakkoli A, et al. GPT-4 for triaging ophthalm ic\nsymptoms. Eye. 2023 May 25;1–2. https:// doi.org/10.10 38/s4143 3-023-025 95-9 PMID: 37231187\n28. Thirunav ukarasu AJ. Large language models will not replace healthcare profession als: curbing popular\nfears and hype. J R Soc Med. 2023; 116(5):181 –2. https://doi.or g/10.1177/0 14107682311 73123 PMID:\n37199678\n29. Tan TF, Thirunavu karasu AJ, Campbell JP, Keane PA, Pasqua le LR, Abramo ff MD, et al. Generativ e\nArtificial Intelligenc e through ChatGP T and Other Large Language Models in Ophthalmolog y: Clinical\nApplications and Challeng es. Ophthalm ology Science. 2023 Sep 5; 3(4):10039 4. https:// doi.org/10.\n1016/j.xop s.2023.10 0394 PMID: 3788575 5\n30. Alsaedi MG, Alhujaili HO, Fairaq GS, Alwdaan SA, Alwadan RA. Emergent Ophthalm ic Disease Knowl-\nedge among Non-Ophthalm ologist Healthca re Professio nals in the Western Region of Saudi Arabia:\nCross-Sectio nal Study. The Open Ophthalm ology Journal [Internet]. 2022 Mar 25 [cited 2023 Jul 12];\n16(1). Available from: https://openo phthalmo logyjournal.c om/VOLUME /16/ELOC ATOR/\ne1874364122 03160/FU LLTEXT/\n31. Tan TF, Thirunavu karasu AJ, Jin L, Lim J, Poh S, Teo ZL, et al. Artificial intelligenc e and digital health in\nglobal eye health: opportunitie s and challeng es. The Lancet Global Health. 2023 Sep 1; 11(9):e143 2–\n43. https://doi. org/10.1016/S 2214-109X (23)00323 -6 PMID: 37591589\n32. Bakken S. AI in health: keeping the human in the loop. Journal of the American Medical Informatics\nAssociation . 2023 Jul 1; 30(7):1225 –6. https://doi. org/10.1093/j amia/ocad0 91 PMID: 37337923\n33. Biderman S, Schoelko pf H, Anthony Q, Bradley H, O’Brien K, Hallahan E, et al. Pythia: A Suite for Ana-\nlyzing Large Language Models Across Training and Scaling [Interne t]. arXiv; 2023 [cited 2023 May 6].\nAvailable from: http://arxiv.or g/abs/230 4.01373\n34. Thirunav ukarasu AJ. How Can the Clinical Aptitud e of AI Assistants Be Assayed? Journal of Medical\nInternet Researc h. 2023 Dec 5; 25(1):e516 03. https:// doi.org/10.21 96/5160 3 PMID: 38051572\n35. Tossaint-S choenmaker s R, Versluis A, Chavannes N, Talboom- Kamp E, Kasteleyn M. The Challeng e\nof Integrat ing eHealth Into Health Care: Systemat ic Literature Review of the Donabedi an Model of\nStructure, Process, and Outcome. J Med Internet Res. 2021 May 10; 23(5):e271 80. https:/ /doi.org/10.\n2196/27180 PMID: 33970123\n36. Mesko ´ B, Topol EJ. The imperati ve for regulatory oversight of large language models (or generative AI)\nin healthcare. npj Digit Med. 2023 Jul 6; 6(1):1–6.\nPLOS DIGI TAL HEALT H\nLarge language models approach expert- level ophthalm ological knowledge and reasoni ng\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000034 1 April 17, 2024 16 / 16",
  "topic": "Relevance (law)",
  "concepts": [
    {
      "name": "Relevance (law)",
      "score": 0.530494213104248
    },
    {
      "name": "Medicine",
      "score": 0.5087376236915588
    },
    {
      "name": "Family medicine",
      "score": 0.3720795512199402
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2799945731",
      "name": "West Suffolk NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2801704036",
      "name": "Manchester Royal Eye Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210092852",
      "name": "Manchester University NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210117485",
      "name": "Birmingham and Midland Eye Centre",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I3020100970",
      "name": "Chang Gung Memorial Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2800375139",
      "name": "Luton and Dunstable University Hospital NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210116917",
      "name": "Singapore Eye Research Institute",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I2799299286",
      "name": "Singapore National Eye Center",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I79619799",
      "name": "University of Birmingham",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210126319",
      "name": "Duke-NUS Medical School",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I61628605",
      "name": "Smith-Kettlewell Eye Research Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I142263535",
      "name": "University of Nottingham",
      "country": "GB"
    }
  ],
  "cited_by": 59
}