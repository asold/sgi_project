{
    "title": "Few-Shot Segmentation via Cycle-Consistent Transformer",
    "url": "https://openalex.org/W3166448862",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2036896606",
            "name": "Zhang, Gengwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2381148516",
            "name": "Kang Guo-liang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1979858332",
            "name": "Yang Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2390531409",
            "name": "Wei, Yunchao",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034749803",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W2798376494",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W3108187451",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963863119",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W3144077317",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W2474531669",
        "https://openalex.org/W2990230185",
        "https://openalex.org/W3106518474",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2981787211",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3106906018",
        "https://openalex.org/W2963599420",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963078159",
        "https://openalex.org/W3149878926",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2893918048",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W2600144439",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3033502887",
        "https://openalex.org/W2766453196",
        "https://openalex.org/W3047258141",
        "https://openalex.org/W1507506748",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2799124825",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2983850069",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2964105864",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3108189450"
    ],
    "abstract": "Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot segmentation task. We design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods. Specifically, on Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art methods by 5.6% and 7.1% respectively.",
    "full_text": "Few-Shot Segmentation via Cycle-Consistent\nTransformer\nGengwei Zhang1,2âˆ—, Guoliang Kang 3, Yi Yang4, Yunchao Wei5,6â€ \n1 Baidu Research\n2 ReLER, Centre for Artiï¬cial Intelligence, University of Technology Sydney\n3 University of Texas, Austin\n4 CCAI, College of Computer Science and Technology, Zhejiang University\n5 Institute of Information Science, Beijing Jiaotong University\n6 Beijing Key Laboratory of Advanced Information Science and Network\n{zgwdavid, kgl.prml, wychao1987, yee.i.yang}@gmail.com\nAbstract\nFew-shot segmentation aims to train a segmentation model that can fast adapt to\nnovel classes with few exemplars. The conventional training paradigm is to learn\nto make predictions on query images conditioned on the features from support\nimages. Previous methods only utilized the semantic-level prototypes of support\nimages as the conditional information. These methods cannot utilize all pixel-wise\nsupport information for the query predictions, which is however critical for the\nsegmentation task. In this paper, we focus on utilizing pixel-wise relationships\nbetween support and query images to facilitate the few-shot segmentation task.\nWe design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate\npixel-wise support features into query ones. CyCTR performs cross-attention\nbetween features from different images, i.e. support and query images. We observe\nthat there may exist unexpected irrelevant pixel-level support features. Directly\nperforming cross-attention may aggregate these features from support to query and\nbias the query features. Thus, we propose using a novel cycle-consistent attention\nmechanism to ï¬lter out possible harmful support features and encourage query\nfeatures to attend to the most informative pixels from support images. Experiments\non all few-shot segmentation benchmarks demonstrate that our proposed CyCTR\nleads to remarkable improvement compared to previous state-of-the-art methods.\nSpeciï¬cally, on Pascal-5i and COCO-20i datasets, we achieve 67.5% and 45.6%\nmIoU for 5-shot segmentation, outperforming previous state-of-the-art method by\n5.6% and 7.1% respectively.\n1 Introduction\nRecent years have witnessed great progress in semantic segmentation [19, 4, 47]. The success can be\nlargely attributed to large amounts of annotated data [48, 17]. However, labeling dense segmentation\nmasks are very time-consuming [45]. Semi-supervised segmentation [15, 39, 38] has been broadly\nexplored to alleviate this problem, which assumes a large amount of unlabeled data is accessible.\nHowever, semi-supervised approaches may fail to generalize to novel classes with very few exemplars.\nIn the extreme low data regime, few-shot segmentation [26, 35] is introduced to train a segmentation\nmodel that can quickly adapt to novel categories.\nâˆ—Part of this work was done when Gengwei Zhang was an intern at Baidu Research.\nâ€ Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.02320v4  [cs.CV]  8 Mar 2022\nð‘ð‘ \nð‘ð‘ž\nPool\nCos/Add\nð‘ð‘ \nð‘ð‘ž\nEM/Cluster\nCos/Add\nð‘ð‘ \nð‘ð‘ž\nGraph-\nAttention\nselect\nð‘ð‘ \nð‘ð‘ž\nCycle-Consistent\nTransformer Encoder\n(a) (b) (c) (d)\nfg/bg\nprototype\nfg/bg\nprototypes\nfg pixels \nflatten\nflatten\nð¼ð‘ \nð¼ð‘ž\nð¼ð‘ \nð¼ð‘ž\nð‘€ð‘ \nð‘€ð‘ \nð¼ð‘ \nð¼ð‘ž\nð¼ð‘ \nð¼ð‘ž\nð‘€ð‘ ð‘€ð‘ \nFigure 1: Different learning frameworks for few-shot segmentation, from the perspective of ways to\nutilize support information. (a) Class-wise mean pooling based method. (b) Clustering based method.\n(c) Foreground pixel attention method. (d) Our Cycle-Consistent TRansformer (CyCTR) framework\nthat enables all beneï¬cial support pixel-level features (foreground and background) to be considered.\nMost few-shot segmentation methods follow a learning-to-learn paradigm where predictions of query\nimages are made conditioned on the features and annotations of support images. The key to the\nsuccess of this training paradigm lies in how to effectively utilize the information provided by support\nimages. Previous approaches extract semantic-level prototypes from support features and follow a\nmetric learning [29, 7, 35] pipeline extending from PrototypicalNet [28]. According to the granularity\nof utilizing support features, these methods can be categorized into two groups, as illustrated in\nFigure 1: 1) Class-wise mean pooling [ 35, 46, 44] (Figure 1(a)). Support features within regions\nof different categories are averaged to serve as prototypes to facilitate the classiï¬cation of query\npixels. 2) Clustering [18, 41] (Figure 1(b)). Recent works attempt to generate multiple prototypes\nvia EM algorithm or K-means clustering [ 41, 18], in order to extract more abundant information\nfrom support images. These prototype-based methods need to â€œcompress\" support information into\ndifferent prototypes ( i.e. class-wise or cluster-wise), which may lead to various degrees of loss\nof beneï¬cial support information and thus harm segmentation on query image. Rather than using\nprototypes to abstract the support information, [43, 34] (Figure 1(c)) propose to employ the attention\nmechanism to extract information from support foreground pixels for segmenting query. However,\nsuch methods ignore all the background support pixels that can be beneï¬cial for segmenting query\nimage, and incorrectly consider partial foreground support pixels that are quite different from the\nquery ones, leading to sub-optimal results.\nBackground pointsForeground points\nð’‘ðŸ\nð’‘ðŸ\nsupport image query image\nð’‘ðŸ‘\nFigure 2: The motivation of our proposed method.\nMany pixel-level support features are quite differ-\nent from the query ones, and thus may confuse the\nattention. We incorporate cycle-consistency into\nattention to ï¬lter such confusing support features.\nNote that the confusing support features may come\nfrom foreground and background.\nIn this paper, we focus on equipping each query\npixel with relevant information from support im-\nages to facilitate the query pixel classiï¬cation.\nInspired by the transformer architecture [ 32]\nwhich performs feature aggregation through at-\ntention, we design a novel Cycle-Consistent\nTransformer (CyCTR) module (Figure 1(d)) to\naggregate pixel-wise support features into query\nones. Speciï¬cally, our CyCTR consists of two\ntypes of transformer blocks: the self-alignment\nblock and the cross-alignment block. The self-\nalignment block is employed to encode the query\nimage features by aggregating its relevant con-\ntext information, while the cross-alignment aims\nto aggregate the pixel-wise features of support\nimages into the pixel-wise features of query\nimage. Different from self-alignment where\nQuery3, Key and Value come from the same\nembedding, cross-alignment takes features from query images as Query, and those from support\nimages as Key and Value. In this way, CyCTR provides abundant pixel-wise support information for\npixel-wise features of query images to make predictions.\nMoreover, we observe that due to the differences between support and query images,e.g., scale, color\nand scene, only a small proportion of support pixels can be beneï¬cial for the segmentation of query\nimage. In other words, in the support image, some pixel-level information may confuse the attention\nin the transformer. Figure 2 provides a visual example of a support-query pair together with the label\n3To distinguish from the phrase \"query\" in few-shot segmentation, we use \"Query\" with capitalization to\nnote the query sequence in the transformer.\n2\nmasks. The confusing support pixels may come from both foreground pixels and background pixels.\nFor instance, point p1 in the support image located in the plane afar, which is indicated as foreground\nby the support mask. However, the nearest point p2 in the query image (i.e. p2 has the largest feature\nsimilarity with p1) belongs to a different category, i.e. background. That means, there exists no query\npixel which has both high similarity and the same semantic label with p1. Thus, p1 is likely to be\nharmful for segmenting \"plane\" and should be ignored when performing the attention. To overcome\nthis issue, in CyCTR, we propose to equip the cross-alignment block with a novel cycle-consistent\nattention operation. Speciï¬cally, as shown in Figure 2, starting from the feature of one support\npixel, we ï¬nd its nearest neighbor in the query features. In turn, this nearest neighbor ï¬nds the most\nsimilar support feature. If the starting and the end support features come from the same category, a\ncycle-consistency relationship is established. We incorporate such an operation into attention to force\nquery features only attend to cycle-consistent support features to extract information. In this way, the\nsupport pixels that are far away from query ones are not considered. Meanwhile, cycle-consistent\nattention enables us to more safely utilize the information from background support pixels, without\nintroducing much bias into the query features.\nIn a nutshell, our contributions are summarized as follows: (1) We tackle few-shot segmentation from\nthe perspective of providing each query pixel with relevant information from support images through\npixel-wise alignment. (2) We propose a novel Cycle-Consistent TRansformer (CyCTR) to aggregate\nthe pixel-wise support features into the query ones. In CyCTR, we observe that many support\nfeatures may confuse the attention and bias pixel-level feature aggregation, and propose incorporating\ncycle-consistent operation into the attention to deal with this issue. (3) Our CyCTR achieves state-of-\nthe-art results on two few-shot segmentation benchmarks, i.e., Pascal-5i and COCO-20i. Extensive\nexperiments validate the effectiveness of each component in our CyCTR.\n2 Related Work\n2.1 Few-Shot Segmentation\nFew-shot segmentation [26] is established to perform segmentation with very few exemplars. Recent\napproaches formulate few-shot segmentation from the view of metric learning [ 29, 7, 35]. For\ninstance, [ 7] ï¬rst extends PrototypicalNet [ 28] to perform few-shot segmentation. PANet [ 35]\nsimpliï¬es the framework with an efï¬cient prototype learning framework. SG-One [46] leverage the\ncosine similarity map between the single support prototype and query features to guide the prediction.\nCANet [44] replaces the cosine similarity with an additive alignment module and iteratively reï¬nes\nthe network output. PFENet [30] further designs an effective feature pyramid module and leverages\na prior map to achieve better segmentation performance. Recently, [ 41, 18, 43] point out that only\na single support prototype is insufï¬cient to represent a given category. Therefore, they attempt to\nobtain multiple prototypes via EM algorithm to represent the support objects and the prototypes are\ncompared with query image based on cosine similarity [ 18, 41]. Besides, [ 43, 34] attempt to use\ngraph attention networks [ 33, 40] to utilize all foreground support pixel features. However, they\nignore all pixels in the background region by default. Besides, due to the large difference between\nsupport and query images, not all support pixels will beneï¬t ï¬nal query segmentation. Recently, some\nconcurrent works propose to learn dense matching through Hypercorrelation Squeeze Networks [22]\nor mining latent classes [ 42] from the background region. Our work aims at mining information\nfrom the whole support image, but exploring to use the transformer architecture and from a different\nperspective, i.e., reducing the noise in the support pixel-level features.\n2.2 Transformer\nTransformer and self-attention were ï¬rstly introduced in the ï¬elds of machine translation and natural\nlanguage processing [6, 32], and are receiving increasing interests recently in the computer vision\narea. Previous works utilize self-attention as additional module on top of existing convolutional\nnetworks, e.g., Nonlocal [36] and CCNet [ 14]. ViT [ 8] and its following work [ 31] demonstrate\nthe pure transformer architecture can achieve state-of-the-art for image recognition. On the other\nhand, DETR [3] builds up an end-to-end framework with a transformer encoder-decoder on top of\nbackbone networks for object detection. And its deformable vairents [51] improves the performance\nand training efï¬ciency. Besides, in natural language processing, a few works [ 2, 5, 27] have been\n3\nintroduced for long documents processing with sparse transformers. In these works, each Query\ntoken only attends to a pre-deï¬ned subset of Key positions.\n2.3 Cycle-consistency Learning\nOur work is partially inspired by cycle-consistency learning [ 50, 9] that is explored in various\ncomputer vision areas. For instance, in image translation, CycleGAN [ 50] uses cycle-consistency\nto align image pairs. It is also effective in learning 3D correspondence [ 49], consistency between\nvideo frames [37] and association between different domains [16]. These works typically constructs\ncycle-consistency loss between aligned targets ( e.g., images). However, the simple training loss\ncannot be directly applied to few-shot segmentation because the test categories are unseen from the\ntraining process and no ï¬netuning is involved during testing. In this work, we incorporate the idea of\ncycle-consistency into transformer to eliminate the negative effect of confusing or irrelevant support\npixels.\n3 Methodology\n3.1 Problem Setting\nFew-shot segmentation aims at training a segmentation model that can segment novel objects with\nvery few annotated samples. Speciï¬cally, given dataset Dtrain and Dtest with category set Ctrain\nand Ctest respectively, where Ctrain âˆ©Ctest = âˆ…, the model trained on Dtrain is directly used to\ntest on Dtest. In line with previous works [ 30, 35, 44], episode training is adopted in this work\nfor few-shot segmentation. Each episode is composed of ksupport images Is and a query image\nIq to form a k-shot episode {{Is}k,Iq}, in which all {Is}k and Iq contain objects from the same\ncategory. Then the training set and test set are represented by Dtrain = {{Is}k,Iq}Ntrain and\nDtest = {{Is}k,Iq}Ntest, where Ntrain and Ntest is the number of episodes for training and test\nset. During training, both support masks Ms and query masks Mq are available for training images,\nand only support masks are accessible during testing.\n3.2 Revisiting of Transformer\nFollowing the general form in [32], a transformer block is composed of alternating layers of multi-head\nattention (MHA) and multi-layer perceptron (MLP). LayerNorm (LN) [1] and residual connection [12]\nare applied at the end of each block. Specially, an attention layer is formulated as\nAtten(Q,K,V ) = softmax(QKT\nâˆš\nd\n)V, (1)\nwhere [Q; K; V] = [WqZq; WkZkv; WvZkv], in which Zq is the input Query sequence, Zkv is the\ninput Key/Value sequence, Wq,Wk,Wv âˆˆRdÃ—d denote the learnable parameters, dis the hidden\ndimension of the input sequences and we assume all sequences have the same dimension dby default.\nFor each Query element, the attention layer computes its similarities with all Key elements. Then the\ncomputed similarities are normalized via softmax, which are used to multiply the Value elements to\nachieve the aggregated outputs. When Zq = Zkv, it functions as self-attention mechanism.\nThe multi-head attention layer is an extention of attention layer, which performshattention operations\nand concatenates consequences together. Speciï¬cally,\nMHA(Q,K,V ) = [head1,..., headh], (2)\nwhere headm = Atten( Qm,Km,Vm) and the inputs [Qm,Km,Vm] are the mth group from\n[Q,K,V ] with dimension d/h.\n3.3 Cycle-Consistent Transformer\nOur framework is illustrated in Figure 3(a). Generally, an encoder of our Cycle-Consistent TRans-\nformer (CyCTR) consists of a self-alignment transformer block for encoding the query features and a\ncross-alignment transformer block to enable the query features to attend to the informative support\nfeatures. The whole CyCTR module stacks Lencoders.\n4\nð‘ð‘˜ð‘£\nð‘ð‘˜ð‘£\nð‘ð‘ž\nð‘Šð‘£\nð‘Šð‘˜\nð‘Šð‘ž\nCycle-Consistent Muti-head Attention (h=1)\nMatMul\nð´\nð‘ð‘ \nð»ð‘žð‘Šð‘ž\n0 0 1\n1\n0\n0\n1\n0 1 1\nâˆš\nÃ— âˆš\n2 2 0\nCyC\nMatMul\nsoftmax\nflatten\n(ð»ð‘žð‘Šð‘ž Ã—ð‘‘)\nð¿Ã—Encoders\nquery\nfeat.\nsupp.\nfeat.\nð»ð‘ž\nð‘Šð‘ž\nð‘‘\nð»ð‘ \nð‘Šð‘ \nð‘‘\nflatten & \nsample\n(ð‘ð‘  Ã—ð‘‘)\nð‘ð‘ž\nð‘ ð‘’ð‘™ð‘“\nð‘ð‘˜ð‘£\nð‘ ð‘’ð‘™ð‘“\nð‘ð‘˜ð‘£\nð‘ ð‘’ð‘™ð‘“\nMHA N MLP N\nSelf-alignment Block\nð‘ð‘˜ð‘£\nð‘ð‘Ÿð‘ \nð‘ð‘˜ð‘£\nð‘ð‘Ÿð‘ \nð‘ð‘žð‘ð‘Ÿð‘ \nCyC-\nMHA N MLP N\nCross-alignment Block\nsupp.\nmask\nflatten & sample\n(ð‘ð‘ )\nð»ð‘ \nð‘Šð‘ \nargmax\nargmax\nindexing\nindexing\nCycle-Consistency\nð‘–â‹† 0 0âˆ’âˆžð‘©\nð´\nLayer Normalization Multi-head AttentionElement-wise AdditionMulti-layer PerceptronN MHAMLP\nð‘—â‹†\nð‘€ð‘ \nð‘€ð‘ \nð‘€ð‘ \noutput \nquery\nfeat.\nFigure 3: Framework of our proposed Cycle-Consistent TRansformer (CyCTR). Each encoder of\nCyCTR consists of two transformers blocks, i.e., the self-alignment block for utilizing global context\nwithin the query feature map and the cross-alignment block for aggregate information from support\nimages. In the cross-alignment block, we introduce the multi-head cycle-consistent attention (shown\non the right, with the number of heads h= 1 for simplicity). The attention operation is guided by the\ncycle-consistency among query and support features.\nSpeciï¬cally, for the given query feature Xq âˆˆRHqÃ—WqÃ—d and support feature Xs âˆˆRHsÃ—WsÃ—d, we\nï¬rst ï¬‚atten them into 1D sequences (with shape HW Ã—d) as inputs for transformer, in which a token\nis represented by the feature zâˆˆRd at one pixel location. The self-alignment block only takes the\nï¬‚attened query feature as input. As context information of each pixel has been proved beneï¬cial\nfor segmentation [4, 47], we adopt the self-alignment block to pixel-wise features of query image to\naggregate their global context information. We donâ€™t pass support images through the self-alignment\nblock, as we mainly focus on the segmentation performance of query images. Passing through the\nsupport images which donâ€™t coordinate with the query mask may do harm to the self-alignment on\nquery images.\nIn contrast, the cross-alignment block performs attention between query and support pixel-wise\nfeatures to aggregate relevant support features into query ones. It takes the ï¬‚attened query feature\nand a subset of support feature (the sampling procedure is discussed latter) with size Ns â‰¤HsWs as\nKey/Value sequenceZkv.\nWith these two blocks, it is expected to better encoder the query features to facilitate the subsequent\npixel-wise classiï¬cation. When stacking Lencoders, the output of the previous encoder is fed into\nthe self-alignment block. The outputs of self-alignment block and the sampled support features are\nthen fed into the cross-alignment block.\n3.3.1 Cycle-Consistent Attention\nAccording to the aforementioned discussion, the pure pixel-level attention may be confused by\nexcessive irrelevant support features. To alleviate this issue, as shown in Figure 3(b), a cycle-\nconsistent attention operation is proposed. We ï¬rst go through the proposed approach for 1-shot case\nfor presentation simplicity and then discuss it in the multiple shot setting.\nFormally, an afï¬nity map A = QKT\nâˆš\nd ,A âˆˆRHqWqÃ—Ns is ï¬rst calculated to measure the corre-\nspondence between all query and support pixels. Then, for an arbitrary support pixel/token j\n(j âˆˆ{0,1,...,N s âˆ’1}, Ns is the number of support pixels), its most similar query pixel/token iâ‹† is\nobtained by\niâ‹† = argmax\ni\nA(i,j), (3)\nwhere iâˆˆ{0,1,...,H qWq âˆ’1}denotes the spatial index of query pixels. Since the query mask is\nnot accessible, the label of query pixel iâ‹† is unknown. However, we can in turn ï¬nd its most similar\nsupport pixel jâ‹† in the same way:\njâ‹† = argmax\nj\nA(iâ‹†,j). (4)\n5\nGiven the sampled support label Ms âˆˆRNs, cycle-consistency is satisï¬ed if Ms(j) = Ms(jâ‹†).\nPrevious work [16] attempts to encourage the feature similarity between cycle-consistent pixels to\nimprove the modelâ€™s generalization ability within the same set of categories. However, in few-shot\nsegmentation, the goal is to enable the model to fast adapt to novel categories rather than making the\nmodel ï¬t better to training categories. Thus, we incorporate the cycle-consistency into the attention\noperation to encourage the cycle-consistent cross-attention. First, by traversing all support tokens, an\nadditive bias B âˆˆRNs is obtained by\nBj =\n{\n0, ifMs(j) = Ms(jâ‹†)\nâˆ’âˆž, ifMs(j) Ì¸= Ms(jâ‹†)\n,\nwhere j âˆˆ{0,1,...,N s}. Then, for a single query token Zq(i) âˆˆRd at location i, the support\ninformation is aggregated by\nCyCAtten(Qi,Ki,Vi) = softmax(A(i) + B)V, (5)\nwhere iâˆˆ{0,1,...,H qWq}and Ais obtained by QKT\nâˆš\nd . In the forward process, Bis element-wise\nadded with the afï¬nity A(i) for Zq(i) to aggregate support features. In this way, the attention weight\nfor the cycle-inconsistent support features become zero, implying that these irrelevant information\nwill not be considered. Besides, the cycle-consistent attention implicitly encourages the consistency\nbetween the most relevant query and support pixel-wise features through backpropagation. Note\nthat our method aims at removing support pixels with certain inconsistency, rather than ensuring all\nsupport pixels to form cycle-consistency, which is impossible without knowing the query ground\ntruth labels.\nWhen performing self-attention in the self-alignment block, there may also exist the same issue, i.e.\nthe query token may attend to irrelevant or even harmful features (especially when background is\ncomplex). According to our cycle-consistent attention, each query token should receive information\nfrom more consistent pixels than aggregating from all pixels. Due to the lack of query mask\nMq, it is impossible to establish the cycle-consistency among query pixels/tokens. Inspired by\nDeformableAttention [51], the consistent pixels can be obtained via a learnable way as âˆ† = f(Q+\nCoord) and A\nâ€²\n= g(Q+ Coord), where âˆ† âˆˆRHpWpÃ—P is the predicted consistent pixels, in which\neach element Î´âˆˆRP in âˆ† represents the relative offset from each pixel and P represents the number\nof pixels to aggregate. And A\nâ€²\nâˆˆRHqWqÃ—P is the attention weights. Coord âˆˆRHqWqÃ—d is the\npositional encoding [24] to make the prediction be aware of absolute position, and f(Â·) and g(Â·) are\ntwo fully connected layers that predict the offsets4 and attention weights. Therefore, the self-attention\nwithin the self-alignment transformer block is represented as\nPredAtten(Qr,Vr) =\nPâˆ‘\ng\nsoftmax(A\nâ€²\n)(r,g)Vr+âˆ†(r,g) , (6)\nwhere râˆˆ{0,1,...,H qWq}is the index of the ï¬‚attened query feature, both Qand V are obtained by\nmultiplying the ï¬‚attened query feature with the learnable parameter.\nGenerally speaking, the cycle-consistent transformer effectively avoids the attention being biased by\nirrelevant features to beneï¬t the training of few-shot segmentation.\nMask-guided sparse sampling andK-shot Setting: Our proposed cycle-consistency transformer\ncan be easily extended to K-shot setting where K >1. When multiple support feature maps are\nprovided, all support features are ï¬‚attened and concatenated together as input. As the attention is\nperformed at the pixel-level, the computation load will be high if the number of support pixels/tokens\nis large, which is usually the case under K-shot setting. In this work, we apply a simple mask-\nguided sampling strategy to reduce the computation complexity and make our method more scalable.\nConcretely, given the k-shot support sequence Zs âˆˆRkHsWsÃ—d and the ï¬‚attened support masks\nMs âˆˆRkHsWs, the support pixels/tokens are obtained by uniformly sampling Nfg tokens (Nfg <=\nNs\n2 , where Ns â‰¤kHsWs) from the foreground regions and Ns âˆ’Nfg tokens from the background\nregions in all support images. With a proper Ns, the sampling operation reduces the computational\ncomplexity, and makes our algorithm more scalable with the increase of spatial size of support\nimages. Additionally, this strategy helps balance the foreground-background ratio and also implicitly\nconsiders different sizes of various object regions in support images.\n4The offsets are predicted as 2d coordinates and transformed into 1d coordinates.\n6\nTable 1: Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on\nPASCAL-5i using the mIoU (%) evaluation metric. Best results are shown in bold.\nMethod Backbone 1-shot 5-shot\n50 51 52 53 Mean 50 51 52 53 Mean\nPANet [35]\nVgg-16\n42.3 58.0 51.1 41.2 48.1 51.8 64.6 59.8 46.5 55.7\nFWB [23] 47.0 59.6 52.6 48.3 51.9 50.9 62.9 56.5 50.1 55.1\nSG-One [46] 40.2 58.4 48.4 38.4 46.3 41.9 58.6 48.6 39.4 47.1\nRPMM [41] 47.1 65.8 50.6 48.5 53.0 50.0 66.5 51.9 47.6 54.0\nCANet [44]\nRes-50\n52.5 65.9 51.3 51.9 55.4 55.5 67.8 51.9 53.2 57.1\nPGNet [43] 56.0 66.9 50.6 50.4 56.0 57.7 68.7 52.9 54.6 58.5\nRPMM [41] 55.2 66.9 52.6 50.7 56.3 56.3 67.3 54.5 51.0 57.3\nPPNet [18] 47.8 58.8 53.8 45.6 51.5 58.4 67.8 64.9 56.7 62.0\nPFENet [30] 61.7 69.5 55.4 56.3 60.8 63.1 70.7 55.8 57.9 61.9\nCyCTR (Ours) Res-50 65.7 71.0 59.5 59.7 64.0 69.3 73.5 63.8 63.5 67.5\nFWB [23]\nRes-101\n51.3 64.5 56.7 52.2 56.2 54.9 67.4 62.2 55.3 59.9\nDAN [34] 54.7 68.6 57.8 51.6 58.2 57.9 69.0 60.1 54.9 60.5\nPFENet [30] 60.5 69.4 54.4 55.9 60.1 62.8 70.4 54.9 57.6 61.4\nCyCTR (Ours) Res-101 67.2 71.1 57.6 59.0 63.7 71.0 75.0 58.5 65.0 67.4\n3.4 Overall Framework\nFollowing previous works [30, 35, 44], both query and support images are ï¬rst feed into a shared\nbackbone (e.g., ResNet [12]) which is initialized with weights pretrained from ImageNet [ 25] to\nobtain general image features. Similar to [ 30], middle-level query features (the concatenation of\nquery features from the 3rd and the 4th blocks of ResNet) are processed by a 1Ã—1 convolution to\nreduce the hidden dimension. The high-level query features (from the 5th block) are used to generate\na prior map (the prior map is generated by calculating the pixel-wise similarity between query and\nsupport features, details can be found in the supplementary materials) and then are concatenated\nwith the middle-level query features. The average masked support feature is also concatenated to\nprovide global support information. The concatenated features are processed by a 1Ã—1 convolution.\nThe output query features are then fed into our proposed CyCTR encoders. The output of CyCTR\nencoders is fed into a classiï¬er to obtain the ï¬nal segmentation results. The classiï¬er consists of\na 3Ã—3 convolutional layer, a ReLU layer and a 1 Ã—1 convolutional layer. More details about our\nnetwork structure can be found in the supplementary materials.\n4 Experiments\n4.1 Dataset and Evaluation Metric\nWe conduct experiments on two commonly used few-shot segmentation datasets, Pascal- 5i [10]\n(which is combined with SBD [11] dataset) and COCO-20i [17], to evaluate our method. For Pascal-\n5i, 20 classes are separated into 4 splits. For each split, 15 classes are used for training and 5 classes\nfor test. At the test time, 1,000 pairs that belong to the testing classes are sampled from the validation\nset for evaluation. In COCO-20i, we follow the data split settings in FWB [23] to divide 80 classes\nevenly into 4 splits, 60 classes for training and test on 20 classes, and 5,000 validation pairs from the\n20 classes are sampled for evaluation. Detailed data split settings can be found in the supplementary\nmaterials. Following common practice [ 30, 35, 46], the mean intersection over union (mIoU) is\nadopted as the evaluation metric, which is the averaged value of IoU of all test classes. We also report\nthe foreground-background IoU (FB-IoU) for comparison.\n4.2 Implementation Details\nIn our experiments, the training strategies follow the same setting in [ 30]: training for 50 epochs\non COCO-20i and 200 epochs on Pascal- 5i. Images are resized and cropped to 473 Ã—473 for\nboth datasets and we use random rotation from âˆ’10â—¦to 10â—¦as data augmentation. Besides, we\nuse ImageNet [25] pretrained ResNet [12] as the backbone network and its parameters (including\nBatchNorms) are freezed. For the parameters except those in the transformer layers, we use the\ninitial learning rate 2.5 Ã—10âˆ’3, momentum 0.9, weight decay 1 Ã—10âˆ’4 and SGD optimizer with\n7\nTable 2: Comparison with other state-of-the-art methods for 1-shot and 5-shot segmentation on\nCOCO-20i using the mIoU (%) evaluation metric. Best results are shown in bold.\nMethod Backbone 1-shot 5-shot\n200 201 202 203 Mean 200 201 202 203 Mean\nFWB [23] Res-101 19.9 18.0 21.0 28.9 21.2 19.1 21.5 23.9 30.1 23.7\nPPNet [18] Res-50 28.1 30.8 29.5 27.7 29.0 39.0 40.8 37.1 37.3 38.5\nRPMM [41] Res-50 29.5 36.8 29.0 27.0 30.6 33.8 42.0 33.0 33.3 35.5\nPFENet [30] Res-101 34.3 33.0 32.3 30.1 32.4 38.5 38.6 38.2 34.3 37.4\nCyCTR (Ours) Res-50 38.9 43.0 39.6 39.8 40.3 41.1 48.9 45.2 47.0 45.6\npoly learning rate decay [4]. The mini batch size on each gpu is set to 4. Experiments are carried out\non Tesla V100 GPUs. For Pascal-5i, one model is trained on a single GPU, while for COCO-20i, one\nmodel is trained with 4 GPUs. We construct our baseline as follows: as stated in Section 3.4, the\nmiddle-level query features from backbone network are concatenated and merged with the global\nsupport feature and the prior map. This feature is processed by two residule blocks and input to\nthe same classiï¬er as our method. Dice loss [ 21] is used as the training objective. Besides, the\nmiddle-level query feature is averaged using the ground truth and concatenated with support feature\nto predict the support segmentation map, which produces an auxiliary loss for aligning features.\nThe same settings are also used in our method except that we use our cycle-consistent transformer\nto process features rather than the residule blocks. For the proposed cycle-consistent transformer,\nwe set the number of sampled support tokens Ns to 600 for 1-shot and 5 Ã—600 for 5-shot setting.\nThe number of sampled tokens is obtained according to the averaged number of foreground pixels\namong Pascal-5i training set. For the self-attention block, the number of points P is set to 9. For\nother hyper-parameters in transformer blocks, we use L = 2 transformer encoders. We set the\nhidden dimension of MLP layer to 3 Ã—256 and that of input to 256. The number of heads for all\nattention layers is set to 8 for Pascal-5i and 1 for COCO-20i. Parameters in the transformer blocks are\noptimized with AdamW [20] optimizer following other transformer works [3, 8, 31], with learning\nrate 1 Ã—10âˆ’4 and weight decay 1 Ã—10âˆ’2. Besides, we use Dropout with the probability 0.1 in all\nattention layers.\n4.3 Comparisons with State-of-the-Art Methods\nIn Table 1 and Table 2, we compare our method with other state-of-the-art few-shot segmentation\napproaches on Pascal-5i and COCO-20i respectively. It can be seen that our approach achieves new\nTable 3: Comparison with other methods using\nFB-IoU (%) on Pascal- 5i for 1-shot and 5-shot\nsegmentation.\nMethod Backbone FB-IoU (%)\n1-shot 5-shot\nA-MCG [13] Res-101 61.2 62.2\nDAN [34] Res-101 71.9 72.3\nPFENet [30] Res-101 72.9 73.5\nCyCTR (Ours) Res-101 73.0 75.4\nstate-of-the-art performance on both Pascal-5i\nand COCO-20i. Speciï¬cally, on Pascal- 5i, to\nmake fair comparisons with other methods, we\nreport results with both ResNet-50 and ResNet-\n101. Our CyCTR achieves 64.0% mIoU with\nResNet-50 backbone and 63.7% mIoU with\nResNet-101 backbone for 1-shot segmentation,\nsigniï¬cantly outperforming previous state-of-\nthe-art results by 3.2% and 3.6% respectively.\nFor 5-shot segmentation, our CyCTR can even\nsurpass state-of-the art methods by 5.6% and\n6.0% mIoU when using ResNet-50 and ResNet-\n101 backbones respectively. For COCO-20i results in Table 2, our method also outperforms other\nmethods by a large margin due to the capability of the transformer to ï¬t more complex data. Besides,\nTable 3 shows the comparison using FB-IoU on PASCAL-5i for 1-shot and 5-shot segmentation, our\nmethod also obtains the state-of-the-art performance.\n4.4 Ablation Studies\nTo provide a deeper understanding of our proposed method, we show ablation studies in this section.\nThe experiments are performed on Pascal-5i 1-shot setting with ResNet-50 as the backbone network,\nand results are reported in terms of mIoU.\n8\nTable 4: Ablation studies that validate the effectiveness of each component in our Cycle-Consistent\nTRansformer. The ï¬rst result is obtained by our baseline (see Section 4.2 for details).\nself-alignment cross-alignment CyCTR (pred) CyCTR (fg. only) CyCTR mIoU (%)\n59.3\nâœ“ 62.5\nâœ“ âœ“ 62.9\nâœ“ âœ“ âœ“ 62.6\nâœ“ âœ“ âœ“ 63.0\nâœ“ âœ“ âœ“ 63.5\n4.4.1 Component-Wise Ablations\nWe perform ablation studies regarding each component of our CyCTR in Table 4. The ï¬rst line is the\nresult of our baseline, where we use two residual blocks to merge features as stated in Section 4.2.\nFor all ablations in Table 4, the hidden dimension is set to 128 and two transformer encoders are used.\nThe mIoU results are averaged over four splits. Firstly, we only use the self-alignment block that only\nencodes query features. The support information in this case comes from the concatenated global\nsupport feature and the prior map used in [44]. It can already bring decent results, showing that the\ntransformer encoder is effective for modeling context for few-shot segmentation. Then, we utilize the\ncross-alignment block but only with the vanilla attention operation in Equation 1. The mIoU increases\nby 0.4%, indicating that pixel-level features from support can provide additional performance gain.\nBy using our proposed cycle-consistent attention module, the performance can be further improved\nby a large margin, i.e. 0.6% mIoU compared to the vanilla attention. This result demonstrates\nour cycle-consistent attentionâ€™s capability to suppress possible harmful information from support.\nBesides, we assume some background support features may also beneï¬t the query segmentation and\ntherefore use the cycle-consistent transformer to aggregate pixel-level information from background\nsupport features as well. Comparing the last two lines in Table 4, we show that our way of utilizing\nbeneï¬cial background pixel-level support information brings 0.5% mIoU improvement, validating\nour assumption and the effectiveness of our proposed cycle-consistent attention operation.\nBesides, one may be curious about whether the noise can also be removed by predicting the ag-\ngregation position like the way in Equation 6 for aggregating support features to query. Therefore,\nwe use predicted aggregation instead of the cycle-consistent attention in the cross-alignment block,\nas denoted by CyCTR(pred) in Table 4. It does beneï¬t the few-shot segmentation by aggregating\nuseful information from support but is 0.9% worse than the proposed cycle-consistent attention. The\nreason lies in the dramatically changing support images under few-shot segmentation testing. The\ncycle-consistency is better than the learnable way as it can globally consider the varying conditional\ninformation from both query and support.\n4.4.2 Effect of Model Capacity\nTable 5: Effect of varying (a) number of encoders\nLand (b) hidden dimensions d. When varying L,\ndis ï¬xed to 128; while varying d, Lis ï¬xed to 2.\n#Encoder mIoU (%)\n1 62.4\n2 63.5\n3 63.7\n(a)\n#Dim mIoU (%)\n128 63.5\n256 64.0\n384 63.9\n(b)\nWe can stack more encoders or increase the hid-\nden dimension of encoders to increase its capac-\nity and validate the effectiveness of our CyCTR.\nThe results with different numbers of encoders\n(denoted as L) or hidden dimensions (denoted\nas d) are shown in Table 5a and 5b. While in-\ncreasing Lor dwithin a certain range, CyCTR\nachieves better results. We chose L= 2 as our\ndefault choice for accuracy-efï¬ciency trade-off.\n4.5 Qualitative results\nIn Figure 4, we show some qualitative results generated by our model on Pascal- 5i. Our cycle-\nconsistent attention can improve the segmentation quality by suppressing possible harmful information\nfrom support. For instance, without cycle-consistency, the model misclassiï¬es trousers as â€œcowâ€ in\nthe ï¬rst row, babyâ€™s hair as â€œcatâ€ in the second row, and a fraction of mountain as â€œcarâ€ in the third\nrow, while our model rectiï¬es these part as background. However, in the ï¬rst row, our CyCTR still\n9\nSupport Query Ours without\nCycle-consistency Ours\nFigure 4: Qualitative results on Pascal-5i. From left to right, each column shows the examples of:\nSupport image with mask region in red; Query image with ground truth mask region in blue; Result\nproduced by the model without cycle-consistency in CyCTR; Result produced by our method.\nsegments part of the trousers as \"cow\" and the right boundary of the segmentation mask is slightly\nworse than the model without cycle-consistency. The reason comes from the extreme differences\nbetween query and support, i.e. the support image shows a \"cattle\" but the query image contains a\nmilk cow. The cycle-consistency may over-suppress the positive region in support images. Solving\nsuch issue may be a potential direction to investigate to improve our method further.\n5 Conclusion\nIn this paper, we design a CyCTR module to deal with the few-shot segmentation problem. Different\nfrom previous practices that either adopt semantic-level prototype(s) from support images or only\nuse foreground support features to encode query features, our CyCTR utilizes all pixel-level support\nfeatures and can effectively eliminate aggregating confusing and harmful support features with the\nproposed novel cycle-consistency attention. We conduct extensive experiments on two popular\nbenchmarks, and our CyCTR outperforms previous state-of-the-art methods by a signiï¬cant margin.\nWe hope this work can motivate researchers to utilize pixel-level support features to design more\neffective algorithms to advance the few-shot segmentation research.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, pages 213â€“229. Springer, 2020.\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE transactions on pattern analysis and machine intelligence ,\n40(4):834â€“848, 2017.\n[5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n10\n[7] Nanqing Dong and Eric P Xing. Few-shot semantic segmentation with prototype learning. In\nBMVC, volume 3-4, 2018.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[9] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman.\nTemporal cycle-consistency learning. InProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1801â€“1810, 2019.\n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.\nThe pascal visual object classes (voc) challenge. International journal of computer vision ,\n88(2):303â€“338, 2010.\n[11] Bharath Hariharan, Pablo ArbelÃ¡ez, Ross Girshick, and Jitendra Malik. Simultaneous detection\nand segmentation. In European Conference on Computer Vision, pages 297â€“312. Springer,\n2014.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition\n(CVPR), pages 770â€“778, 2016.\n[13] Tao Hu, Pengwan Yang, Chiliang Zhang, Gang Yu, Yadong Mu, and Cees GM Snoek. Attention-\nbased multi-context guiding for few-shot semantic segmentation. In Association for the Ad-\nvancement of Artiï¬cial Intelligence (AAAI), volume 33, pages pp. 8441â€“8448, 2019.\n[14] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu.\nCcnet: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 603â€“612, 2019.\n[15] Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weakly-\nsupervised semantic segmentation network with deep seeded region growing. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pages 7014â€“7023, 2018.\n[16] Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, and Alexander G Hauptmann. Pixel-\nlevel cycle association: A new perspective for domain adaptive semantic segmentation. In\nNeurIPS, 2020.\n[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollÃ¡r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision (ECCV), 2014.\n[18] Yongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xuming He. Part-aware prototype network\nfor few-shot semantic segmentation. In European Conference on Computer Vision , pages\n142â€“158. Springer, 2020.\n[19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for se-\nmantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3431â€“3440, 2015.\n[20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[21] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation. In 2016 fourth international conference\non 3D vision (3DV), pages 565â€“571. IEEE, 2016.\n[22] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmenta-\ntion. arXiv preprint arXiv:2104.01538, 2021.\n[23] Khoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 622â€“631,\n2019.\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. In International Conference on Machine Learning, pages\n4055â€“4064. PMLR, 2018.\n11\n[25] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115(3):211â€“252, 2015.\n[26] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for\nsemantic segmentation. In British Machine Vision Conference (BMVC), 2018.\n[27] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James T\nKwok. Sparsebert: Rethinking the importance analysis in self-attention. arXiv preprint\narXiv:2102.12871, 2021.\n[28] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in neural information processing systems (NeurIPS), 2017.\n[29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 1199â€“1208, 2018.\n[30] Z Tian, H Zhao, M Shu, Z Yang, R Li, and J Jia. Prior guided feature enrichment network for\nfew-shot segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n[31] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervÃ© JÃ©gou. Training data-efï¬cient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, pages 6000â€“6010, 2017.\n[33] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n[34] Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang, Xianbin Cao, and Xiantong Zhen.\nFew-shot semantic segmentation with democratic attention networks. In European Conference\non Computer Vision (ECCV), 2020.\n[35] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot\nimage semantic segmentation with prototype alignment. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 9197â€“9206, 2019.\n[36] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n7794â€“7803, 2018.\n[37] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-\nconsistency of time. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2566â€“2576, 2019.\n[38] Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan.\nObject region mining with adversarial erasing: A simple classiï¬cation to semantic segmentation\napproach. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1568â€“1576, 2017.\n[39] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S Huang.\nRevisiting dilated convolution: A simple approach for weakly-and semi-supervised seman-\ntic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 7268â€“7277, 2018.\n[40] Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, and Liang\nLin. Bidirectional graph reasoning network for panoptic segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9080â€“9089, 2020.\n[41] Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for\nfew-shot semantic segmentation. In European Conference on Computer Vision, pages 763â€“778.\nSpringer, 2020.\n[42] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. Mining latent classes for few-shot\nsegmentation. arXiv preprint arXiv:2103.15402, 2021.\n12\n[43] Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo, Qingyao Wu, and Rui Yao. Pyramid\ngraph networks with connection attentions for region-based one-shot semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 9587â€“9595,\n2019.\n[44] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic\nsegmentation networks with iterative reï¬nement and attentive few-shot learning. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5217â€“5226,\n2019.\n[45] Shiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei, and Yao Zhao. Interactive object\nsegmentation with inside-outside guidance. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12234â€“12244, 2020.\n[46] Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S Huang. Sg-one: Similarity guidance\nnetwork for one-shot semantic segmentation. IEEE Transactions on Cybernetics, 50(9):3855â€“\n3865, 2020.\n[47] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition (CVPR), 2017.\n[48] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nScene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633â€“641, 2017.\n[49] Tinghui Zhou, Philipp Krahenbuhl, Mathieu Aubry, Qixing Huang, and Alexei A Efros. Learning\ndense correspondence via 3d-guided cycle consistency. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 117â€“126, 2016.\n[50] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks. InProceedings of the IEEE international\nconference on computer vision, pages 2223â€“2232, 2017.\n[51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020.\nA More Details\nA.1 Implementation\nBackbone\nBackbone\nC\nexpand\nexpand\nC\nprior\nmask\nSupport image\nQuery image\nSupport mask\npool\n1x1\nconv\n1x1\nconv\n1x1\nconv\n1x1\nconv\nCycle-consistent\nTransformer\nconv\npredict\nð» Ã—ð‘Š Ã—ð‘‘\nð» Ã—ð‘Š Ã—ð‘‘\nð» Ã—ð‘Š Ã—2ð‘‘ ð» Ã—ð‘Š Ã—ð‘‘\nð» Ã—ð‘Š Ã—(2ð‘‘+1) ð» Ã—ð‘Š Ã—ð‘‘\nð»ð‘Š Ã—ð‘‘\nflatten & sample\nð»ð‘Š Ã—ð‘‘\nflatten\nreshape\nð» Ã—ð‘Š Ã—ð‘‘\nQuery prediction\nflatten & sample\nElement-wise multiplication C Concatenation\nFigure 5: The network structure used in our experiments. The backbone network ï¬rst extracts features\nfor query and support images. To enable the pixel-wise comparison in transformer, the averaged\nforeground support feature is expanded and concatenated with both query and support features. Our\nCycle-Consistent TRansformer (CyCTR) takes the ï¬‚attened query and support features as well as the\nï¬‚attened support mask as input and produces the encoded query feature for prediction.\nThe overall network architecture used in our experiments is shown in Figure 5. Following the common\npractice [30, 35, 44], query and support image are ï¬rst feed into a shared backbone network to obtain\ngeneral image features. Similar to [30], the backbone network is pretrained on ImageNet [25] and\nthen completely kept ï¬xed during few-shot segmentation training. Following [18, 30, 41], we use\n13\ndilated version of ResNet [12] as the backbone network. Besides, middle-level features are processed\nby a 1x1 convolution to reduce the hidden dimension and high-level features are used to generate a\nprior map that concatenated with the middle-level feature. In details, the middle-level feature consists\nof the concatenation of features from the 3rd and the 4th block of ResNet (total 5 blocks including\nthe stem block) with shape HÃ—WÃ—(512 + 1024)and is feed into a 1 Ã—1 convolution to reduce the\ndimension to HÃ—W Ã—d, where dis the hidden dimension that can be adjusted in our experiments.\nThe high-level feature (from the 5th block of ResNet) with shape HÃ—W Ã—2048 is used to generate\nthe prior mask as in [30], which compute the pixel-wise similarity between the query and support\nhigh-level features and keep the maximum similarity at each pixel and normalize (using min-max\nnormalization) the similarity map to the range of [0,1]. To enable the pixel-wise comparison, we also\nconcatenate the mask averaged support feature to both query and support feature and processed by a\n1x1 convolution before inputting into the transformer. The ï¬nal segmentation result is obtained by\nreshaping the output sequence back to spatial dimensions and predicted by a small convolution head\nthat is consisted of one 3x3 convolution, one ReLU activation, and a 1x1 convolution. Dice loss [21]\nis used as the training objective.\nBaseline setup: For the baseline of our method, we use two residual blocks [ 12] to merge the\nquery feature. The support information comes from the concatenated support global feature and\nthe prior map. During training, the foreground middle-level query feature from backbone network\nis averaged and concatenated with the middle-level support feature to predict the support mask for\nfeature alignment. This auxiliary supervision is included in all of our experiments.\nA.2 Dataset Settings\nIn this Table 6 and Table 7, we provide the detailed split settings for datasets (Pascal5i and COCO-\n20i) used in our experiments, which follow the split settings proposed in [23].\nSplit Test classes\nPASCAL-50 aeroplane, bicycle, bird, boat, bottle\nPASCAL-51 bus, car, cat, chair, cow\nPASCAL-52 diningtable, dog, horse, motorbike, person\nPASCAL-53 potted plant, sheep, sofa, train, tv/monitor\nTable 6: Data split for PASCAL-5i, which follows the 4-fold cross-validation. Each row contains 5\nclasses for test and the rest 15 classes in the PASCAL dataset are used for training.\nSplit Test classes\nCOCO-200\nPerson, Airplane, Boat, Park meter, Dog, Elephant, Backpack, Suitcase, Sports ball,\nSkateboard, W. glass, Spoon, Sandwich, Hot dog, Chair, D. table, Mouse, Microwave,\nFridge, Scissors,\nCOCO-201 Bicycle, Bus, T.light, Bench, Horse, Bear, Umbrella, Frisbee, Kite, Surfboard, Cup, Bowl,\nOrange, Pizza, Couch, Toilet, Remote, Oven, Book, Teddy,\nCOCO-202 Car, Train, Fire H., Bird, Sheep, Zebra, Handbag, Skis, B. bat, T. racket, Fork, Banana,\nBroccoli, Donut, P. plant, TV , Keyboard, Toaster, Clock, Hairdrier,\nCOCO-203 Motorcycle, Truck, Stop, Cat, Cow, Giraffe, Tie, Snowboard, B. glove, Bottle, Knife, Apple,\nCarrot, Cake, Bed, Laptop, Cellphone, Sink, Vase, Toothbrush,\nTable 7: Data split for COCO-20i, which follows the 4-fold cross-validation. Each row contains 20\nclasses for test and the rest classes in the COCO dataset are used for training.\nB More Visualizations\nWe provide more visualizations in Figure 6. We also provide the visualization of cycle-consistency\nrelationships. In the ï¬rst row, only a small part of the foreground region is activated while most\nforeground regions are valid in the second row. And in the second row, pixels on the \"person\" are\nshown in gray, which indicates that these pixels may have a negative impact on segmenting \"cat\".\n14\nSupport Query Ours without\nCycle-consistency OursCycle-consistency\nFigure 6: More Qualitative results on Pascal- 5i. The cycle-consistency is visualized in the 2ed\ncolumn, in which red points are cycle-consistent foreground pixels, blue points are cycle-consistent\nbackground pixels, and gray points are cycle-inconsistent pixels. Best viewed in color and with\nzoom-in.\n15"
}