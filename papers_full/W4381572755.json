{
    "title": "Auditing large language models: a three-layered approach",
    "url": "https://openalex.org/W4381572755",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2461069441",
            "name": "Jakob Mökander",
            "affiliations": [
                "Princeton University",
                "University of Oxford",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3160123256",
            "name": "Jonas Schuett",
            "affiliations": [
                "Centre for the Governance of AI",
                "Goethe University Frankfurt"
            ]
        },
        {
            "id": "https://openalex.org/A3055715833",
            "name": "Hannah Rose Kirk",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A1977953962",
            "name": "Luciano Floridi",
            "affiliations": [
                "University of Bologna",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2461069441",
            "name": "Jakob Mökander",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3160123256",
            "name": "Jonas Schuett",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3055715833",
            "name": "Hannah Rose Kirk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1977953962",
            "name": "Luciano Floridi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2106534692",
        "https://openalex.org/W2332851105",
        "https://openalex.org/W3130234976",
        "https://openalex.org/W3017343191",
        "https://openalex.org/W2962059918",
        "https://openalex.org/W3134395196",
        "https://openalex.org/W2765330044",
        "https://openalex.org/W3001807593",
        "https://openalex.org/W4223475929",
        "https://openalex.org/W4288058319",
        "https://openalex.org/W3199424257",
        "https://openalex.org/W2898883664",
        "https://openalex.org/W2604790242",
        "https://openalex.org/W4223510915",
        "https://openalex.org/W4223506650",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W6811129797",
        "https://openalex.org/W6811340617",
        "https://openalex.org/W3172556138",
        "https://openalex.org/W4283170666",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4306176220",
        "https://openalex.org/W4312050653",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W4309182913",
        "https://openalex.org/W4283157303",
        "https://openalex.org/W4296413526",
        "https://openalex.org/W4226404919",
        "https://openalex.org/W4238858058",
        "https://openalex.org/W2015198362",
        "https://openalex.org/W4287900772",
        "https://openalex.org/W4223908421",
        "https://openalex.org/W6782465632",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4225479391",
        "https://openalex.org/W4306808908",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W3173783648",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W6600042607",
        "https://openalex.org/W3188785232",
        "https://openalex.org/W4312213844",
        "https://openalex.org/W4375949262",
        "https://openalex.org/W4281264299",
        "https://openalex.org/W4200303997",
        "https://openalex.org/W3139137532",
        "https://openalex.org/W2897042519",
        "https://openalex.org/W3212368439",
        "https://openalex.org/W4380319827",
        "https://openalex.org/W4322626329",
        "https://openalex.org/W3211982349",
        "https://openalex.org/W3006708286",
        "https://openalex.org/W1968241292",
        "https://openalex.org/W3154536706",
        "https://openalex.org/W3002093512",
        "https://openalex.org/W3163154359",
        "https://openalex.org/W2985543011",
        "https://openalex.org/W3181414820",
        "https://openalex.org/W2998516762",
        "https://openalex.org/W2956792022",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4294754000",
        "https://openalex.org/W4285288507",
        "https://openalex.org/W3023001449",
        "https://openalex.org/W3200802389",
        "https://openalex.org/W3159435787",
        "https://openalex.org/W4220929609",
        "https://openalex.org/W4301186400",
        "https://openalex.org/W3215475614",
        "https://openalex.org/W4307819507",
        "https://openalex.org/W3210785036",
        "https://openalex.org/W3127162919",
        "https://openalex.org/W4205948809",
        "https://openalex.org/W6756281437",
        "https://openalex.org/W4283168787",
        "https://openalex.org/W2944704506",
        "https://openalex.org/W3092010601",
        "https://openalex.org/W2588936615",
        "https://openalex.org/W2993252589",
        "https://openalex.org/W2807113735",
        "https://openalex.org/W2897439270",
        "https://openalex.org/W4212915308",
        "https://openalex.org/W2799244840",
        "https://openalex.org/W3175487198",
        "https://openalex.org/W6838461927",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W3009613839",
        "https://openalex.org/W2294076101",
        "https://openalex.org/W3184097711",
        "https://openalex.org/W4245587674",
        "https://openalex.org/W4307697543",
        "https://openalex.org/W4311992568",
        "https://openalex.org/W2780153414",
        "https://openalex.org/W3101447603",
        "https://openalex.org/W4319601582",
        "https://openalex.org/W3112689365",
        "https://openalex.org/W3033511014",
        "https://openalex.org/W3123554940",
        "https://openalex.org/W4229447062",
        "https://openalex.org/W4283155630",
        "https://openalex.org/W4283167130",
        "https://openalex.org/W3212464620",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3104423855",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W3172794097",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W3171654528",
        "https://openalex.org/W3208206463",
        "https://openalex.org/W4285787421",
        "https://openalex.org/W6836672048",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W3148308966",
        "https://openalex.org/W4288083800",
        "https://openalex.org/W3213241618",
        "https://openalex.org/W3093211917",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W2911227954",
        "https://openalex.org/W3048824105",
        "https://openalex.org/W4404815083",
        "https://openalex.org/W3135484384",
        "https://openalex.org/W2962704634",
        "https://openalex.org/W4239801659",
        "https://openalex.org/W2953522645",
        "https://openalex.org/W1534502583",
        "https://openalex.org/W4283156811",
        "https://openalex.org/W2738428139",
        "https://openalex.org/W4401115532",
        "https://openalex.org/W3213782894",
        "https://openalex.org/W4224222525",
        "https://openalex.org/W4312353889",
        "https://openalex.org/W2884074583",
        "https://openalex.org/W2403984353",
        "https://openalex.org/W4224307573",
        "https://openalex.org/W2972657613",
        "https://openalex.org/W2963532322",
        "https://openalex.org/W4287887133",
        "https://openalex.org/W3168584517",
        "https://openalex.org/W2925224691",
        "https://openalex.org/W3212327893",
        "https://openalex.org/W4385573644",
        "https://openalex.org/W3212281239",
        "https://openalex.org/W4283076002",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W3160920566",
        "https://openalex.org/W3182976505",
        "https://openalex.org/W2995006168",
        "https://openalex.org/W4281723109",
        "https://openalex.org/W4308875527",
        "https://openalex.org/W2255792231",
        "https://openalex.org/W4225411436",
        "https://openalex.org/W4321074112",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W3162439866",
        "https://openalex.org/W1536594450",
        "https://openalex.org/W4379177239",
        "https://openalex.org/W4312624993",
        "https://openalex.org/W3152436735",
        "https://openalex.org/W3217449589",
        "https://openalex.org/W2767394981",
        "https://openalex.org/W4236606073",
        "https://openalex.org/W2081446015",
        "https://openalex.org/W2297259217",
        "https://openalex.org/W2482146702",
        "https://openalex.org/W3211595942",
        "https://openalex.org/W4288859400",
        "https://openalex.org/W2902634493",
        "https://openalex.org/W4237497717",
        "https://openalex.org/W4282832127",
        "https://openalex.org/W6600560973",
        "https://openalex.org/W2942730552",
        "https://openalex.org/W4292223708",
        "https://openalex.org/W2880383342",
        "https://openalex.org/W3102619004",
        "https://openalex.org/W4327952037",
        "https://openalex.org/W4225115543",
        "https://openalex.org/W597036898",
        "https://openalex.org/W2079283960",
        "https://openalex.org/W1975675278",
        "https://openalex.org/W2794015279",
        "https://openalex.org/W2338991735",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3174269049",
        "https://openalex.org/W3198659451",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W4244120138",
        "https://openalex.org/W2028860426",
        "https://openalex.org/W4287391981",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W3193717459",
        "https://openalex.org/W4244751360",
        "https://openalex.org/W3125569121",
        "https://openalex.org/W4230816331",
        "https://openalex.org/W3098379356",
        "https://openalex.org/W1977675760",
        "https://openalex.org/W3186551952",
        "https://openalex.org/W3133874049",
        "https://openalex.org/W3100279624",
        "https://openalex.org/W3125809767",
        "https://openalex.org/W4210477483",
        "https://openalex.org/W3212020324",
        "https://openalex.org/W4206524579",
        "https://openalex.org/W3105871743",
        "https://openalex.org/W2594153394"
    ],
    "abstract": "Abstract Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective mechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to remain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only of our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the methodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from technical, ethical, and legal perspectives.",
    "full_text": "Vol.:(0123456789)1 3\nAI and Ethics (2024) 4:1085–1115 \nhttps://doi.org/10.1007/s43681-023-00289-2\nORIGINAL RESEARCH\nAuditing large language models: a three‑layered approach\nJakob Mökander1,2  · Jonas Schuett3,4  · Hannah Rose Kirk1  · Luciano Floridi1,5 \nReceived: 17 February 2023 / Accepted: 18 April 2023 / Published online: 30 May 2023 \n© The Author(s) 2023\nAbstract\nLarge language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread \nuse of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing \nas a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, \nlegal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by \nLLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address \nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby \ngovernance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training \nbut prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We \nshow how audits, when conducted in a structured and coordinated manner on all three levels, can be a feasible and effective \nmechanism for identifying and managing some of the ethical and social risks posed by LLMs. However, it is important to \nremain realistic about what auditing can reasonably be expected to achieve. Therefore, we discuss the limitations not only \nof our three-layered approach but also of the prospect of auditing LLMs at all. Ultimately, this article seeks to expand the \nmethodological toolkit available to technology providers and policymakers who wish to analyse and evaluate LLMs from \ntechnical, ethical, and legal perspectives.\nKeywords Artificial intelligence · Auditing · Ethics · Foundation models · Governance · Large language models · Natural \nlanguage processing · Policy · Risk management\n1 Introduction\nAuditing is a governance mechanism that technology provid-\ners and policymakers can use to identify and mitigate risks \nassociated with artificial intelligence (AI) systems [1 -5].1 \nAuditing is characterised by a systematic and independ-\nent process of obtaining and evaluating evidence regard-\ning an entity's actions or properties and communicating the \nresults of that evaluation to relevant stakeholders [6]. Three \nideas underpin the promise of auditing as an AI governance \nmechanism: that procedural regularity and transparency \ncontribute to good governance [7, 8]; that proactivity in the \ndesign of AI systems helps identify risks and prevent harm \nbefore it occurs [9, 10]; and, that the operational independ-\nence between the auditor and the auditee contributes to the \nobjectivity and professionalism of the evaluation [11, 12].\n * Jakob Mökander \n jakob.mokander@oii.ox.ac.uk\n Jonas Schuett \n jonas.schuett@governance.ai\n Hannah Rose Kirk \n hannah.kirk@oii.ox.ac.uk\n Luciano Floridi \n luciano.floridi@oii.ox.ac.uk\n1 Oxford Internet Institute, University of Oxford, Oxford, UK\n2 Center for Information Technology Policy, Princeton \nUniversity, Princeton, USA\n3 Centre for the Governance of AI, Oxford, UK\n4 Faculty of Law, Goethe University Frankfurt, \nFrankfurt am Main, Germany\n5 Department of Legal Studies, University of Bologna, \nBologna, Italy\n1 The term governance mechanism refers to the set of activities and \ncontrols used by various parties in society to exert influence and \nachieve normative ends [275].\n1086 AI and Ethics (2024) 4:1085–1115\n1 3\nPrevious work on AI auditing has focused on ensuring \nthat specific applications meet predefined, often sector-spe-\ncific, requirements. For example, researchers have developed \nprocedures for how to audit AI systems used in recruitment \n[13], online search [14], image classification [15], and medi-\ncal diagnostics [16, 17]. However, the capabilities of AI sys-\ntems tend to become ever more general. In a recent article, \nBommasani et al. [18] coined the term foundation models  \nto describe models that can be adapted to a wide range of \ndownstream tasks. While foundation models are not neces-\nsarily new from a technical perspective, 2 they differ from \nother AI systems insofar as they have proven to be effective \nacross many different tasks and display emergent capabilities \nwhen scaled [19]. The rise of foundation models also reflects \na shift in how AI systems are designed and deployed, since \nthese models tend to be trained and released by one actor and \nsubsequently adapted for a wide range of different applica-\ntions by a plurality of other actors.\nFrom an AI auditing perspective, foundation models \npose significant challenges. For example, it is difficult to \nassess the risks that AI systems pose independent of the \ncontext in which they are deployed. Moreover, how to allo-\ncate responsibility between technology providers and down-\nstream developers when harms occur remains unresolved. \nTaken together, the capabilities and training processes of \nfoundation models have outpaced the development of tools \nand procedures to ensure that these are ethical, legal, and \ntechnically robust.3 This implies that, while application-level \naudits have an important role in AI governance, they must be \ncomplemented with new forms of supervision and control.\nThis article addresses that gap by focusing on a subset of \nfoundation models, namely large language models (LLMs). \nLLMs start from a source input, called the prompt, to gener-\nate the most likely sequences of words, code, or other data \n[20]. Historically, different model architectures have been \nused in natural language processing (NLP), including proba-\nbilistic methods [21]. However, most recent LLMs—includ-\ning those we focus on in this article—are based on deep \nneural networks trained on a large corpus of texts. Examples \nof such LLMs include GPT-3 [22], GPT-4 [23], PaLM [24], \nLaMDA [25], Gopher [26] and OPT [27]. Once an LLM has \nbeen pre-trained, it can be adapted (with or without fine-\ntuning4) to support various applications, from spell-checking \n[28] to creative writing [29].\nDeveloping LLM auditing procedures is an important \nand timely task for two reasons. First, LLMs pose many \nethical and social challenges, including the perpetuation of \nharmful stereotypes, the leakage of personal data protected \nby privacy regulations, the spread of misinformation, pla-\ngiarism, and the misuse of copyrighted material [30- 33]. In \nrecent months, the scope of impact from these harms has \nbeen dramatically scaled by unprecedented public visibility \nand growing user bases of LLMs. For example, ChatGPT \nattracted over 100 million users just two months after its \nlaunch [34]. The urgency of addressing those challenges \nmakes developing a capacity to audit LLMs’ characteris-\ntics along different normative dimensions (such as privacy, \nbias, safety, etc.) a critical task in and of itself [35]. Second, \nLLMs can be considered proxies for other foundation mod-\nels.5 Consider CLIP [36], a vision-language model trained \nto predict which text caption accompanied an image, as an \nexample. CLIP too displays emergent capabilities, can be \nadapted for multiple downstream applications, and faces \nsimilar governance challenges as LLMs. The same holds \nof text2image models such as DALL·E 2 [37]. Developing \nfeasible and effective procedures for how to audit LLMs is \ntherefore likely to offer transferable lessons on how to audit \nother foundation models and even more powerful generative \nsystems in the future.6\nThe main contribution offered in this article is a novel \nblueprint for how to audit LLMs. Specifically, we propose a \nthree-layered approach, whereby governance audits (of tech-\nnology providers that design and disseminate LLMs), model \n2 Foundation models are typically based on deep neural networks and \nself-supervised learning, two approaches that have existed for dec-\nades [18]. That said, the rise of foundation models has been enabled \nby more recent developments, including: the advancement of new \nnetwork architectures, like transformers [276]; the increase in com-\npute resources and improvements in hardware capacity [277]; the \navailability of large scale datasets, e.g., through ImageNet [278] or \nCommonCrawl [279]; and the application of these increased compute \nresources with larger datasets for model pre-training [280].\n3 The European Commission’s Ethics Guidelines for Trustworthy \nAI stipulate that AI systems should be legal, ethical, and technically \nrobust [281]. That normative standard includes safeguards against \nboth immediate and long-term concerns, e.g., those related to data \nprivacy and discrimination and those related to the safety and control \nof highly capable and autonomous AI systems, respectively.\n4 To fine-tune LLMs for specific tasks, an additional dataset of in-\ndomain examples can be used to adapt the final layers of a pre-trained \nmodel. In some cases, developers apply reinforcement learning \n(RL)—a feedback driven training paradigm whereby LLMs learn to \nadjust their behaviour to maximise a reward function [282]; espe-\ncially reinforcement learning from human feedback (RLHF)—where \nthe reward function is estimated based on human ratings of model \noutputs [50-52]. Alternatively, LLMs can be adapted to specific tasks \nwith no additional training data and frozen weights—via in-context \nlearning or prompt-based demonstrations [283].\n5 In some cases, the ability to utilize other modalities is integrated \ninto single-modal LLMs: DeepMind’s Flamingo model [284] fuses \nan LLM with visual embeddings to exploit its strong existing perfor -\nmance on text-based tasks.\n6 Following Jonathan Zittrain [285], we define ‘generative technolo-\ngies’ as technologies that allow third-parties to innovate upon them \nwithout any gatekeeping. Colloquially, ‘generative AI’ sometimes \nrefers to systems that can output content (images, text, audio, or code) \n[286], but that is not how we use the term in this article.\n1087AI and Ethics (2024) 4:1085–1115 \n1 3\naudits (of LLMs after pre-training but prior to their release), \nand application audits (of applications based on LLMs) \ncomplement and inform each other. Figure 1 (see Sect. 4.1) \nprovides an overview of this three-layered approach. As we \ndemonstrate throughout this article, many tools and methods \nalready exist to conduct audits at each individual level. How-\never, the key message we seek to stress is that, to provide \nmeaningful assurance for LLMs, audits conducted on the \ngovernance, model, and application levels must be combined \ninto a structured and coordinated procedure. Figure  2 (see \nSect. 4.5) illustrates how outputs from audits on one level \nbecome inputs for which audits on other levels must account. \nTo the best of our knowledge, our blueprint for how to audit \nLLMs is the first of its kind, and we hope it will inform both \ntechnology providers’ and policymakers’ efforts to ensure \nthat LLMs are legal, ethical, and technically robust.\nIn the process of introducing and discussing our three-\nlayered approach, the article also offers two secondary \ncontributions. First, it makes seven claims about how LLM \nauditing procedures should be designed to be feasible and \neffective in practice. Second, it identifies the conceptual, \ntechnical, and practical limitations associated with audit-\ning LLMs. Together, these secondary contributions lay a \ngroundwork that other researchers and practitioners can \nbuild upon when designing new, more refined, LLM audit-\ning procedures in the future.\nOur efforts tie into an extensive research agenda and \nongoing policy formation process. AI labs like Cohere, Ope-\nnAI, and AI21 have expressed interest in understanding what \nit means to develop LLMs responsibly [38], and DeepMind, \nMicrosoft, and Anthropic have highlighted the need for new \ngovernance mechanisms to address the social and ethical \nchallenges that LLMs pose [30, 39, 40]. Individual parts of \nour proposal (e.g., those related to model evaluation [ 24] \nand red teaming [41, 42])7 have thus already started to be \nimplemented across the industry, although not always in a \nstructured manner or with full transparency. Policymakers, \ntoo, are interested in ensuring that societies benefit from \nLLMs while managing the associated risks. Recent exam-\nples of proposed AI regulations include the EU AI Act [43] \nand the US Algorithmic Accountability Act of 2022 [44]. \nThe blueprint for auditing LLMs outlined in this article nei-\nther seeks to replace existing best practices for training and \ntesting LLMs nor to foreclose forthcoming AI regulations. \nInstead, it complements them by demonstrating how gov -\nernance, model, and application audits—when conducted \nin a structured and coordinated manner—can help ensure \nthat LLMs are designed and deployed in ethical, legal, and \ntechnically robust ways.\nA further remark is needed to narrow down this article’s \nscope. Our three-layered approach concerns the procedure  \nof LLM audits and answers questions about what should be \naudited, when, and according to which criteria. Of course, \nwhen designing a holistic auditing ecosystem, several addi-\ntional considerations exist, e.g., who should conduct the \naudit and how to ensure post-audit action [12]. While such \nconsiderations are important, they fall outside the scope of \nthis article. How to design an institutional ecosystem to audit \nLLMs is a non-trivial question that we have neither the space \nnor the capacity to address here. That said, the policy pro-\ncess required to establish an LLM auditing ecosystem will \nlikely be gradual and involve negotiations between numer -\nous actors, including AI labs, policymakers, and civil rights \ngroups. For this reason, our early blueprint for how to audit \nLLMs is intentionally limited in scope to not forego but \nrather to initiate this policy formation process by eliciting \nstakeholder reactions.\nThe remainder of this article proceeds as follows: Sect. 2 \nhighlights the ethical and social risks posed by LLMs and \nestablishes the need to audit them. In doing so, it situates \nour work in relation to recent technological and societal \ndevelopments. Section  3 reviews previous literature on AI \nauditing to identify transferable best practices, discusses \nthe properties of LLMs that undermine existing AI auditing \nprocedures, and derives seven claims for how LLM audit-\ning procedures should be designed to be feasible and effec-\ntive. Section 4 outlines our blueprint for how to audit LLMs, \nintroducing a three-layered approach that combines govern-\nance, model, and application audits. The section explains \nin detail why these three types of audits are needed, what \nthey entail, and the outputs they should produce. Section  5 \ndiscusses the limitations of our three-layered approach and \ndemonstrates that any attempt to audit LLMs will face sev -\neral conceptual, technical, and practical constraints. Finally, \nSect. 6 concludes by discussing the implications of our find-\nings for technology providers, policymakers, and independ-\nent auditors.\n2  The need to audit LLMs\nThis section summarises previous research on LLMs and \ntheir ethical and social challenges. It aims to situate our \nwork in relation to recent technological and societal devel-\nopments, stress the need for auditing procedures that capture \nthe risks LLMs pose, and address potential objections to our \napproach.\n7 A ‘red team’ is a group of people authorised to emulate an adver -\nsarial attack on a system to identify and exploit its vulnerabilities \n[287]. The objective of red teaming is thus to gather information that \nin turn can be used to improve the system’s robustness.\n1088 AI and Ethics (2024) 4:1085–1115\n1 3\n2.1  The opportunities and risks of LLMs\nAlthough LLMs represent a major advance in AI research, \nthe idea of building text-processing machines is not new. \nSince the 1950s, NLP researchers and practitioners have \nbeen developing software that can analyse, manipulate, and \ngenerate natural language [45]. Until the 1980s, most NLP \nsystems used logic-based rules and focused on automating \nthe structural analysis of language needed to enable machine \ntranslation and speech recognition [46]. More recently, the \nadvent of deep learning, advances in neural architectures \nsuch as transformers, growth in computational power and \nthe availability of internet-scraped training data have revo-\nlutionised the field [47] by permitting the creation of LLMs \nthat can approximate human performance on some bench-\nmarks [48, 49]. Further advances in instruction-tuning and \nreinforcement learning from human feedback have improved \nmodel capabilities to predict user intent and respond to natu-\nral language requests [50-52].\nLLMs’ core training task is to produce the most likely \ncontinuation of a text sequence [53]. Consequently, LLMs \ncan be used to recognise, summarise, translate, and gener -\nate texts, with near human-like performance on some tasks \n[54]. Exactly when a language model becomes ‘large’ is a \nmatter of debate—referring to either more trainable param-\neters [55], a larger training corpus [56] or a combination of \nthese. For our purposes, it is sufficient to note that LLMs are \nhighly adaptable to various downstream applications, requir-\ning fewer in-domain labelled examples than traditional deep \nlearning systems [57]. This means that LLMs can more eas-\nily be adapted for specific tasks, such as diagnosing medical \nconditions [58], generating code [59, 60] and translating lan-\nguages [61]. Previous research has demonstrated that LLMs \ncan perform well on a task with few-shot or zero-shot rea-\nsoning [22, 62].8 Moreover, a scaling law has been identified \nwhereby the training error of an LLM falls off as a power of \ntraining set size, model size or both [63]. Simply scaling the \nmodel can thus result in emergent gains on a wide array of \ntasks [64], though those gains are non-uniform, especially \nfor complex mathematical or logical reasoning domains \n[26]. Finally, while some pre-trained models are protected \nby paywalls or siloed within companies, many LLMs are \naccessible via open-source libraries such as HuggingFace, \ndemocratising the gains from deep language modelling and \nallowing non-experts to use it in their applications [65].\nAlongside such opportunities, however, the use of LLMs \nis coupled with ethical challenges [31, 32]. As recent con-\ntroversies surrounding ChatGPT [66] have shown, LLMs \nare prone to give biased or incorrect answers to user queries \n[67]. More generally, a recent article by Weidinger et al. \n[30] suggests that the risks associated with LLM include \nthe following:\n(1) Discrimination. LLMs can introduce representational \nand allocational harms by perpetuating social stereo-\ntypes and biases;\n(2) Information hazards. LLMs may compromise privacy \nby leaking private information and inferring sensitive \ninformation;\n(3) Misinformation hazards. LLMs producing misleading \ninformation can lead to less well-informed users and \nerode trust in shared information;\n(4) Malicious use. LLMs can be co-opted by users with bad \nintent, e.g., to generate personalised scams or large-\nscale fraud;\n(5) Human–computer interaction harms. Users may over-\nestimate the capabilities of LLMs that appear human-\nlike and use them in unsafe ways; and\n(6) Automation and environmental harms. Training and \noperating LLMs require lots of computing power, \nincurring high environmental costs.\nEach of these risk areas constitutes a vast and complex \nfield of research. Providing a comprehensive overview of \neach field’s nuances is beyond this paper’s scope. Instead, \nwe take Weidinger et al.’s summary of the ethical and social \nrisks associated with LLMs as a starting point for pragmatic \nproblem-solving.\n2.2  The governance gap\nFrom a governance perspective, LLMs pose both methodo-\nlogical and normative challenges. As previously mentioned, \nfoundation models—like LLMs—are typically developed \nand adopted in two stages. Firstly, a model is pre-trained \nusing self-supervised learning on a large, unstructured text \ncorpus scraped from the internet. Pre-training captures the \ngeneral language representations required for many tasks \nwithout explicitly labelled data. Secondly, the weights or \nbehaviours of this pre-trained model can be adapted on a far \nsmaller dataset of labelled, task-specific, examples. 9 That \nmakes it methodologically difficult to assess LLMs inde -\npendent of the context in which they will be deployed [18].\nFurthermore, although performance is predictable at a \ngeneral level, performance on specific tasks, or at scale, \ncan be unpredictable [40]. Crucially, even well-functioning \n8 An LLM is considered a ‘zero-shot’ reasoner if employed for a \ncompletely unseen task and a ‘few-shot’ reasoner if only a small sam-\nple of demonstrations are given for a previously unseen task.\n9 The term ‘adapted’ here encompasses multiple existing methods for \neliciting specific model behaviours, including fine-tuning, reinforce-\nment learning with human feedback and in-context learning.\n1089AI and Ethics (2024) 4:1085–1115 \n1 3\nLLMs force AI labs and policymakers to face hard questions, \nsuch as who should have access to these technologies and \nfor which purposes [68 ]. Of course, the challenges posed \nby LLMs are not necessarily distinct from those associated \nwith classical NLP or other ML-based systems. However, \nLLMs' widespread use and generality make those challenges \ndeserving of urgent attention. For all these reasons, analys-\ning LLMs from ethical perspectives requires innovation in \nrisk assessment tools, benchmarks, and frameworks [69].\nSeveral governance mechanisms designed to ensure that \nLLMs are legal, ethical, and safe have been proposed or \npiloted [70]. Some are technically oriented, including the \npre-processing of training data, the fine-tuning of LLMs \non data with desired properties, and procedures to test the \nmodel at scale pre-deployment [ 42, 69]. Others seek to \naddress the ethical and social risks associated with LLMs \nthrough sociotechnical mitigation strategies, e.g., creating \nmore diverse developer teams [71], human-in-the-loop pro-\ntocols [72] and qualitative evaluation tools based on ethno-\ngraphic methods [73]. Yet others seek to ensure transparency \nin AI development processes, e.g., through a structured use \nof model cards [74, 75], datasheets [76], system cards [77], \nand the watermarking of system outputs [78].10\nTo summarise, while LLMs have shown impressive per-\nformance across a wide range of tasks, they also pose signifi-\ncant ethical and social risks. Therefore, the question of how \nLLMs should be governed has attracted much attention, with \nproposals ranging from structured access protocols designed \nto prevent malicious use [68] to hard regulation prohibit-\ning the deployment of LLMs for specific purposes [ 79]. \nHowever, the effectiveness and feasibility of these govern-\nance mechanisms have yet to be substantiated by empirical \nresearch. Moreover, given the multiplicity and complexity \nof the ethical and social risks associated with LLMs, we \nanticipate that policy responses will need to be multifaceted \nand incorporate several complementary governance mecha-\nnisms. As of now, technology providers and policymakers \nhave only started experimenting with different governance \nmechanisms, and how LLMs should be governed remains \nan open question [80].\n2.3  Calls for audits\nAgainst the backdrop of the technological and regulatory \nlandscape surveyed in this section, auditing should be \nunderstood as one of several governance mechanisms dif-\nferent stakeholders can employ to ensure and demonstrate \nthat LLMs are legal, ethical, and technically robust. It is \nimportant to stress that auditing LLMs is not a hypothetical \nidea but a tangible policy option that has been proposed by \nresearchers, technology providers, and policymakers alike. \nFor instance, when coining the term foundation models, \nBommasani et al. [18] suggested that ‘such models should \nbe subject to rigorous testing and auditing procedures’. \nMoreover, in an open letter concerning the risks associated \nwith LLMs and other foundation models, OpenAI’s CEO \nSam Altman stated that ‘it’s important that efforts like ours \nsubmit to independent audits before releasing new systems’ \n[81]. Finally, the European Commission is considering clas-\nsifying LLMs as ‘high-risk AI systems’ [82]. 11 This would \nimply that technology providers designing LLMs have to \nundergo ‘conformity assessments with the involvement of an \nindependent third-party’, i.e., audits by another name [83].\nDespite widespread calls for LLM auditing, central ques-\ntions concerning how LLMs can and should be audited have \nyet to be systematically explored. This article addresses that \ngap by outlining a procedure for auditing LLMs. The main \nargument we advance can be summarised as follows. What \nauditing means varies between different academic disci-\nplines and industry contexts [84]. However, three strands \nof auditing research and practice are particularly relevant \nwith respect to ensuring good governance of LLMs. The first \nstems from IT audits, whereby auditors assess the adequacy \nof technology providers’ software development processes \nand quality management procedures [85]. The second strand \nstems from model testing and verification within the com-\nputer sciences, whereby auditors assess the properties of \ndifferent computational models [86]. The third strand stems \nfrom product certification procedures, whereby auditors test \nconsumer goods for legal compliance and technical safety \nbefore they go to market [87]. As we argue throughout this \npaper, it is necessary to combine auditing tools and pro-\ncedural best practices from each of these three strands to \nidentify and manage the social and ethical risks LLMs pose. \nTherefore, our blueprint for auditing LLMs combines gov -\nernance audits of technology providers, model audits of \nLLMs, and application audits of downstream products and \nservices built on top of LLMs. The details of this ‘three-\nlayered approach’ are outlined in Sect. 4.\n2.4  Addressing initial objections\nBefore proceeding any further, it is useful to consider some \nreasonable objections to the prospect of auditing LLMs—as \nwell as potential responses to these objections. First, one \nmay argue that there is no need to audit LLMs per se and \n10 A watermark is a hidden pattern in a text that is imperceptible to \nhumans but makes it algorithmically identifiable as synthetic.\n11 It is still uncertain how the EU AI Act should be interpreted. The \ncurrent formulation states that LLMs that may be used for high-risk \napplications should be considered high-risk [288].\n1090 AI and Ethics (2024) 4:1085–1115\n1 3\nthat auditing procedures should be established at the applica-\ntion level instead. Although audits on the application level \nare important, the objection presents a false dichotomy: \nquality and accountability mechanisms can and should be \nestablished at different stages of supply chains. Moreover, \nwhile some risks can only be addressed at the application \nlevel, others are best managed upstream. It is true that many \nfactors, including some beyond the technology provider’s \ncontrol, determine whether a specific technological artefact \ncauses harm [ 88]. However, technology providers are still \nresponsible for taking proportional precautions regarding \nreasonably foreseeable risks during the product life cycle \nstages that they do control. For this reason, we propose that \napplication audits should be complemented with governance \naudits of the organisations that develop LLMs. The same \nlogic underpins the EU’s AI liability directive [89]. Our \nproposal is thereby compatible with the emerging European \nAI regulations.\nSecond, identifying and mitigating all LLM-related risks \nat the technology level may not be possible. As we explain \nin Sect. 5, this is partly because different normative values \nmay conflict and require trade-offs [90-92]. Using individu-\nals’ data, for example, may permit improved personalisation \nof language models, but compromise privacy [93]. Moreover, \nconcepts like ‘fairness’ or ‘transparency’ hide deep norma-\ntive disagreements [94]. Different definitions of fairness (like \ndemographic parity and counterfactual fairness) are mutually \nexclusive [95-97], and prioritising between competing defini-\ntions remains a political question. However, while audits can-\nnot ensure that LLMs are ‘ethical’ in any universal sense, they \nnevertheless contribute to good governance in several ways. \nFor example, audits can help technology providers identify \nrisks and potentially prevent harm, shape the continuous (re-\ndesign) of LLMs, and inform public discourse concerning tech \npolicy. Bringing all this together, our blueprint for how to audit \nLLMs focuses on making implicit choices and tensions visible, \ngiving voice to different stakeholders, and generating resolu-\ntions that—even when imperfect—are, at least, more explicit \nand publicly defensible [98].\nThird, one may contend that designing LLM auditing \nprocedures is difficult. We agree and would add that this dif-\nficulty has both practical and conceptual components. Dif-\nferent stages in the software development life cycle (includ-\ning curating training data and the pre-training/fine-tuning of \nmodel weights) overlap in messy and iterative ways [99]. For \nexample, open-source LLMs are continuously re-trained and \nre-uploaded on collaborative platforms (like HuggingFace) \npost-release. That creates practical problems concerning when \nand where audits should be mandated. Yet the conceptual chal-\nlenges run even more deeply. For instance, what constitutes \ndisinformation and hate speech are contested questions [100]. \nDespite widespread agreement that LLMs should be ‘truthful’ \nand ‘fair’, such notions are hard to operationalise. Because \nthere exists no universal condition of validity that applies \nequally to all kinds of utterances [101], it is hard to establish a \nnormative baseline against which LLMs can be audited.\nHowever, these difficulties are not reasons for abstaining \nfrom developing LLM auditing procedures. Instead, they are \nhealthy reminders that it cannot be assumed that one single \nauditing procedure will capture all LLM-related ethical risks \nor be equally effective in all contexts [102]. The insufficiency \nand limited nature of auditing as a governance mechanism is \nnot an argument against its complementary usefulness. With \nthose caveats highlighted, we now review previous work on \nAI auditing. The aim of the next section is thus to explore \nthe merits and limitations of existing AI auditing procedures \nwhen applied to LLMs and, ultimately, identify transferable \nbest practices.\n3  The merits and limits of existing AI \nauditing procedures\nIn this section, we provide an overview of previous work.12 \nIn doing so, we introduce auditing as an AI governance \nmechanism, highlight the properties of LLMs that under -\nmine the feasibility and effectiveness of existing AI audit-\ning procedures, and derive and defend seven claims about \nhow LLM auditing procedures should be designed. Taken \ntogether, this section provides the theoretical justification for \nthe LLM auditing blueprint outlined in Sect.  4.\n3.1  AI auditing\nIn the broadest sense, auditing refers to an independent \nexamination of any entity, conducted with a view to express \nan opinion thereon [103]. Auditing can be conceived as a \ngovernance mechanism because it can be used to monitor \nconduct and performance [104] and has a long history of \npromoting procedural regularity and transparency in areas \nlike financial accounting and worker safety [105]. The idea \nbehind AI auditing is thus simple: just like financial trans-\nactions can be audited for correctness, completeness, and \nlegality, so can the design and use of AI systems be audited \nfor technical robustness, legal compliance, or adherence with \npre-defined ethics principles.\nAI auditing is a relatively recent field of study, sparked \nin 2014 by Sandvig et al.’s article Auditing Algorithms [ 1]. \nHowever, auditing intersects with almost every aspect of AI \ngovernance, from the documentation of design procedures \nto model testing and verification [106]. AI auditing is thus \nboth a multifaceted practice and a multidisciplinary field of \n12 See Appendix 1 for the methodology used to conduct this litera-\nture review.\n1091AI and Ethics (2024) 4:1085–1115 \n1 3\nresearch, harbouring contributions from computer science \n[107, 108], law [109, 110], media and communication stud-\nies [1, 111], and organisation studies [112, 113].\nDifferent researchers have defined AI auditing in dif-\nferent ways. For example, it is possible to distinguish \nbetween narrow and broad conceptions of AI auditing. \nThe former is impact-oriented and focuses on probing \nand assessing the outputs of AI systems for different input \ndata [114]. The latter is process-oriented and focuses on \nassessing the adequacy of technology providers’ software \ndevelopment processes and quality management systems \n[115]. This article takes the broad perspective, defining \nAI auditing as a systematic and independent process of \nobtaining and evaluating evidence regarding an entity's \nactions or properties and communicating the results of that \nevaluation to relevant stakeholders. Note that the entity \nin question, i.e., the audit’s subject, can be either an AI \nsystem, an organisation, a process, or any combination \nthereof [116].\nDifferent actors can employ AI auditing for differ -\nent purposes [117]. In some cases, policymakers man-\ndate audits to ensure that AI systems used within their \njurisdiction meet specific legal standards. For example, \nNew York City’s AI Audit Law (NYC Local Law 144) \nrequires independent auditing of companies utilising AI \nsystems to inform employment-related decisions [118]. In \nother cases, technology providers commission AI audits \nto mitigate technology-related risks, calling on profes-\nsional services firms like PwC, Deloitte, KPMG, and EY \n[119-122]. In yet other cases, other stakeholders conduct \nAI audits to inform citizens about the conduct of specific \ncompanies. 13\nThe key takeaway from this brief overview is that while \nAI auditing is a widespread practice, both the design and \npurpose of different AI auditing procedures vary. Moreover, \nprocedures to audit LLMs and other foundation models have \nyet to be developed. Therefore, it is useful to consider the \nmerits and limitations of existing AI auditing procedures \nwhen applied to LLMs.\n3.2  Seven claims about auditing LLMs\nAs demonstrated above, a wide range of AI auditing proce-\ndures have already been developed.14 However, not all audit-\ning procedures are equally effective in handling the risks \nposed by LLMs. Nor are they equally likely to be imple-\nmented, due to factors including technical limitations, insti-\ntutional access, and administrative costs [3]. In what follows, \nwe discuss some key distinctions that inform the design of \nauditing procedures and defend seven claims about making \nsuch designs feasible and effective for LLMs.\nTo start with, it is useful to distinguish between compli-\nance audits and risk audits. The former compares an entity’s \nactions or properties to predefined standards or regulations. \nThe latter asks open-ended questions about how a system \nworks to identify and control risks. When conducting risk \naudits of LLMs, auditors can draw on well-established pro-\ncedures, including standards for AI risk management [123, \n124] and guidance on how to assess and evaluate AI sys-\ntems [112, 125-129]. In contrast, compliance audits require \na normative baseline against which AI systems can be evalu-\nated. However, LLM research is a quickly developing field in \nwhich standards and regulations have yet to emerge. Moreo-\nver, the fact that LLMs are adaptable to many downstream \napplications [40] undermines the feasibility of auditing pro-\ncedures designed to ensure compliance with sector-specific \nnorms and regulations. This leads us to our first claim:\nClaim 1  AI auditing procedures focusing on compliance \nalone are unlikely to provide adequate assurance for LLMs.\nOur blueprint for how to audit LLMs outlined in Sect.  4 \naccounts for Claim 1 by incorporating elements of both risk \naudits (at governance and model levels) and compliance \naudits (at the application level).\nFurther, it is useful to distinguish between external and \ninternal audits . The former is conducted by independent \nthird-parties and the latter by an internal function report-\ning directly to its board [130]. External audits help address \nconcerns regarding accuracy in self-reporting [1 ], so they \ntypically underpin formal certification procedures [ 131]. \nHowever, they are constrained by limited access to inter -\nnal processes [9 ]. For internal audits, the inverse is true: \nwhile constituting an essential step towards informed \nmodel design decisions [132], they run an increased risk \nof collusion between the auditor and the auditee [133]. \nMoreover, without third-party accountability, decision-\nmakers may ignore audit recommendations that threaten \ntheir business interests [134 ]. The risks stemming from \nmisaligned incentives are especially stark for technologies \nwith rapidly increasing capabilities and for companies fac-\ning strong competitive pressures [ 135]. Both conditions \napply to LLMs, undermining the ability of internal auditing \nprocedures to provide meaningful assurance in this space. \nThis observation, combined with the need to manage the \nsocial and ethical risks posed by LLMs surveyed in Sect. 2, \nleads us to assert that:\n13 AI auditing procedures have not only been developed by academic \nresearchers and private companies but also by non-profit organisa-\ntions like ForHumanity [302]  and  the Algorithmic Justice League \n[289].\n14 For more comprehensive overviews of available AI auditing tools \nand procedures, see [6, 114, 117].\n1092 AI and Ethics (2024) 4:1085–1115\n1 3\nClaim 2 External audits are required to ensure that LLMs \nare ethical, legal, and technically robust, as well as to hold \ntechnology providers accountable in case of irregularities \nof incidents.\nAs we explain in Sect.  4, each step in our blueprint for \nhow to audit LLMs should be conducted by independent \nthird-party auditors. However, external audits come with \ntheir own challenges, including how to access information \nthat is protected by privacy or IP rights [12, 136]. This is \nespecially challenging in the case of LLMs since some are \nonly accessible via an application programming interface \n(API) and others are not published at all. Determining the \nauditor’s level of access is thus an integral part of designing \nLLM auditing procedures.\nKoshiyama et al. [10] proposed a typology that distin-\nguishes between different access levels. At lower levels, \nauditors have no direct access to the model but base their \nevaluations on publicly available information about the \ndevelopment process. At middle levels, auditors have access \nto the computational model itself, meaning they can manipu-\nlate its parameters and review its task objectives. At higher \nlevels, auditors have access equivalent to the system devel-\noper to all the details encompassing a system, i.e., full access \nto organisational processes, actual input and training data, \nand information about how and why the system was initially \ncreated. In Sect. 4, we use this typology to indicate the level \nof access auditors need to conduct audits at the governance, \nmodel, and application levels.\nThe question about access leads us to a further distinction \nmade in the AI auditing literature, i.e., between adversarial \nand collaborative audits. Adversarial audits are conducted \nby independent actors to assess the properties or impact \nan AI system has—without privileged access to its source \ncode or technical design specifications [1 , 114]. Collabora-\ntive audits see technology providers and external auditors \nworking together to assess and improve the process that \nshapes future AI systems’ design and safeguards [115, 116]. \nWhile the former primarily aims to expose harms, the latter \nseeks to provide assurance. Previous research has shown \nthat audits are most effective when technology providers and \nindependent auditors collaborate towards the common goal \nof identifying and managing risks [11]. This implies that:\nClaim 3  To be feasible and effective in practice, procedures \nto audit LLM require active collaboration between technol-\nogy providers and independent auditors.\nAccounting for Claim 3, this article focuses on collabora-\ntive audits. All steps in our three-layered approach outlined \nin Sect. 4 demand that technology providers provide external \nauditors with the access they need and proactively feed their \nown know-how into the process. After all, evaluating LLMs \nrequires resources and technical expertise that technology \nproviders are best positioned to provide.\nMoving on, it is also useful to distinguish between gov -\nernance audits and technology audits. The former focus on \nthe organisation designing or deploying AI systems and \ninclude assessments of software development and quality \nmanagement processes, incentive structures, and the allo-\ncation of roles and responsibilities [85]. The latter focus on \nassessing a technical system’s properties, e.g., reviewing \nthe model architecture, checking its consistency with pre-\ndefined specifications, or repeatedly querying an algorithm \nto understand its workings and potential impact [114]. Some \nLLM-related risks can be identified and mitigated at the \napplication level. However, other issues are best addressed \nupstream, e.g., those concerning the sourcing of training \ndata. This implies that, to be feasible and effective:\nClaim 4 Auditing procedures designed to assess and miti-\ngate the risks posed by LLMs must include elements of both \ngovernance and technology audits.\nOur blueprint for how to audit LLMs satisfies this claim \nin the following way. The governance audits we propose aim \nto assess the processes whereby LLMs are designed and dis-\nseminated, the model audits focus on assessing the technical \nproperties of pre-trained LLMs, and the application audits \nfocus on assessing the technical properties of applications \nbuilt on top of LLMs.\nHowever, both governance audits and technology audits \nhave limitations. During governance audits, for example, it \nis not possible to anticipate upfront all the risks that emerge \nas AI systems interact with complex environments over time \n[102, 137]. Further, not all ethical tensions stem from tech-\nnology design alone, as some are intrinsic to specific tasks \nor applications [138]. While these limitations of governance \naudits are well-known, LLMs introduce new challenges \nfor technology audits, which have historically focused on \nassessing systems designed to fill specific functions in well-\ndefined contexts, e.g., improving image analysis in radiol-\nogy [139] or detecting corporate fraud [140]. Because LLMs \nenable many downstream applications, traditional auditing \nprocedures are not equipped to capture the full range social \nand ethical risks they pose. While existing best practices \nin governance auditing appear applicable to organisations \ndesigning or deploying LLMs, that is not true for technology \naudits. In short:\nClaim 5 The methodological design of technology audits \nwill require significant modifications to identify and assess \nLLM-related risks.\nAs mentioned above, our blueprint for how to audit LLMs \nincorporates elements of technology audits on both the \n1093AI and Ethics (2024) 4:1085–1115 \n1 3\nmodel and the application levels. To understand why that \nis necessary to identify and mitigate the ethical risks posed \nby LLMs, we must first distinguish between different types \nof technology audits.\nPrevious work on technology audits distinguish between \nfunctionality, model, and impact audits [ 141]. Functional-\nity audits focus on the rationale underpinning AI systems \nby asking questions about intentionality, e.g., what is this \nsystem’s purpose [142]? Model audits review the system’s \ndecision-making logic. For symbolic AI systems, 15 that \nentails reviewing the source code. For sub-symbolic AI sys-\ntems, including LLMs, it entails asking how the model was \ndesigned, what data it was trained on, and how it performs \non different benchmarks. Finally, impact audits investigate \nthe types, severity, and prevalence of effects from an AI \nsystem’s outputs on individuals, groups, and the environ-\nment [143]. These approaches are not mutually exclusive \nbut rather highly complementary [116]. Still, technology \nproviders that design and disseminate LLMs have limited \ninformation about the future deployment of their systems \nby downstream developers and end-users. This leads us to \nour sixth claim:\nClaim 6  Model audits will play a key role in identifying and \ncommunicating LLMs’ limitations, thereby informing system \nredesign, and mitigating downstream harm.\nThis claim constitutes a key justification for the three-\nlayered approach to LLM auditing proposed in this article. \nAs highlighted in Sect. 4, governance audits and application \naudits are both well-established practices in systems engi-\nneering and software development. Hence, it is precisely \nby adding structured and independent audits on the model \nlevel that our blueprint for auditing LLMs complements and \nenhances existing governance structures.\nFinally, within technology audits, it is important to dis-\ntinguish between ex-ante and ex-post audits, which take \nplace before and after a system is deployed, respectively. \nThe former can identify and prevent some harms before they \noccur while informing downstream users about the model’s \nappropriate, intended applications. Considerable literature \nalready exists within computer science on techniques such \nas red teaming [41, 42], model fooling [144], functional test-\ning [145] and template-based stress-testing [146], which all \nplay important roles during technology audits of LLMs. \nHowever, ex-ante audits cannot fully capture all the risks \nassociated with systems that continue to ‘learn’ by updating \ntheir internal decision-making logic [147].16 This limitation \napplies to all learning systems but is particularly relevant \nfor LLMs that display emergent capabilities [148]. 17 Ex-\npost audits can be divided into snapshot audits (which occur \nonce or on regular occasions) and continuous audits (which \nmonitor performance over time). Most existing AI audit-\ning procedures are snapshots. 18 Like ex-ante audits, how -\never, snapshots are unable to provide meaningful assurance \nregarding LLMs as they display emergent capabilities and, \nin some cases, can learn as they are fed new data. This leads \nto our final claim:\nClaim 7 LLM auditing procedures must include elements \nof continuous ex- post monitoring to meet their regulatory \nobjectives.\nIn our blueprint, continuous ex-post monitoring is one of \nthe activities conducted at the application level. However, \nas detailed in Sect.  4.5, audits on the different levels are \nstrongly interconnected. For example, continuous monitor -\ning of LLM-based applications presupposes that technology \nproviders have established ex-post monitoring plans—which \ncan only be verified by audits at the governance level. Invert-\nedly, technology providers rely on feedback from audits at \nthe application level to continue improving their software \ndevelopment and quality management procedures.\nTo summarise, much can be learned from existing AI \nauditing procedures. However, LLMs display several proper-\nties that undermine the feasibility of such procedures. Spe-\ncifically, LLMs are adaptable to a wide range of downstream \napplications, display emergent capabilities, and can, in some \ncases, continue to learn over time. As this section has shown, \nthat means that neither functionality audits (which hinge on \nthe evaluation of the purpose of a specific application) nor \nimpact audits (which hinge on the ability to observe a spe-\ncific system’s actual impact) alone can provide meaningful \nassurance against the social and ethical risks LLMs pose. \nIt also means that ex-ante audits must be complemented by \ncontinuous post-market monitoring of outputs from LLM-\nbased applications.\nIn this section, we have built on these and other insights \nto derive and defend seven claims about how auditing pro-\ncedures should be designed to account for the governance \nchallenges LLMs pose. These seven claims provided our \n15 Symbolic AI systems are based on explicit methods like first-order \nlogic and decision trees. Sub-symbolic systems rely on establishing \ncorrelations through statistical methods like Bayesian learning and \nback-propagation [290].\n16 In their unfrozen states, all LLMs can learn as they are fed new \ndata. However, once a model has been ‘fixed’, it does not update and \nsimply uses new input data to make predictions.\n17 Emergence implies that an entity can have properties its parts do \nnot individually possess, and that randomness can generate orderly \nstructures [291].\n18 The post-market monitoring mandated by the proposed EU AI Act \n[43] is a rare example of continuous auditing.\n1094 AI and Ethics (2024) 4:1085–1115\n1 3\nstarting point when designing the three-layered approach \nfor auditing LLMs that will be outlined in Sect. 4. However, \nwe maintain that these claims are more general and could \nserve as guardrails for other attempts to design auditing pro-\ncedures for all foundation models.\n4  Auditing LLMs: a three‑layered approach\nThis section offers a blueprint for auditing LLMs that satis-\nfies the seven claims in Sect.  3 about how to structure such \nprocedures. While there are many ways to do that, our pro-\nposal focuses on a limited set of activities that are (i) jointly \nsufficient to identify LLM-related risks, (ii) practically \nfeasible to implement, and (iii) have a justifiable cost–ben-\nefit ratio. The result is the three-layered approach outlined \nbelow.\n4.1  A blueprint for LLM auditing\nAudits should focus on three levels. First, technology pro-\nviders developing LLMs should undergo governance audits \nthat assess their organisational procedures, accountability \nstructures and quality management systems. Second, LLMs \nshould undergo model audits, assessing their capabilities \nand limitations after initial training but before adaptation \nand deployment in specific applications. Third, downstream \napplications using LLMs should undergo continuous appli-\ncation audits that assess the ethical alignment and legal \ncompliance of their intended functions and their impact over \ntime. Figure 1 illustrates the logic of our approach.\nSome clarifications are needed to flesh out our blueprint. \nTo begin with, governance, model and application audits \nonly provide effective assurance when coordinated. This is \nbecause the affordances and limitations of audits conducted \nat the three levels differ in ways that make them critically \ncomplementary. For example, as Sect. 3 showed, LLM audits \nmust include elements of both process- and performance-ori-\nented auditing (Claim 4). In our three-layered approach, the \ngovernance audits are process-oriented, whereas the model \nand application audits are performance-oriented. Moreover, \nfeasible and effective LLM auditing procedures must include \naspects of continuous, ex-post assessments (Claim 7). In our \nblueprint, these elements are incorporated at the application \nlevel. But this is just two examples. As we discuss what gov-\nernance, model and applications audits entail in this section, \nwe also make highlight how they, when combined, satisfies \nall seven claims listed in Sect. 3.\nWhile the three types of audits included in our blueprint \nare individually necessary, their boundaries overlap and can \nFig. 1  Blueprint for how to audit LLMs: A three-layered approach\n1095AI and Ethics (2024) 4:1085–1115 \n1 3\nbe drawn in multiple ways. For example, the collection and \npre-processing of training data ties into software develop-\nment practices. Hence, reviewing organisational procedures \nfor obtaining and curating training data is legitimate dur -\ning holistic governance audits. However, the characteristics \nLLMs display during model audits may also reflect biases \nin their training data [149, 150].19 Reviewing such data is, \ntherefore, often necessary during the model audits too [151, \n152]. Nevertheless, the conceptual distinction between gov-\nernance, model and application audits remains useful when \nidentifying varied risks that LLMs pose.\nIt is theoretically possible to add further layers to our \nblueprint. For example, downstream developers could also \nbe made subject to process-oriented governance audits. \nBut such audits would be difficult to implement, given that \nmany decentralised actors build applications on top of \nLLMs. The combination of governance, model, and appli-\ncation audits, we argue, strikes a balance between covering \na sufficiently large part of the development and deployment \nlifecycle to identify LLM-related risks, on the one hand, \nand being practically feasible to implement, on the other. \nRegardless of how many layers are included, however, the \nsuccess of our blueprint relies on responsible actors at each \nlevel who actively want to or are incentivised to ensure \ngood governance.\nFinally, to provide meaningful assurance, audits on all \nthree levels should be external (Claim 2 ) yet collaborative \n(Claim 3). In practice, this implies that independent third \nparties not only seek to verify claims made by technology \nproviders but also work together with them to identify and \nmitigate risks and shape the design of future LLMs. As \nmentioned in the introduction, the question of who should \nconduct the audits falls outside the scope of this article. That \nsaid, reasonable concerns about how independent collabo-\nrative audits really are can be raised regardless of who is \nconducting the audit. In Sect.  5, we discuss this and other \nlimitations.\nWith those clarifications in mind, we will now present \nthe details of our three-layered approach. The following \nthree subsections discuss governance, model, and applica-\ntion audits respectively, focusing on why each is needed, \nwhat each entails, and what outputs each should produce.\n4.2  Governance audits\nTechnology providers working on LLMs should undergo \ngovernance audits that assess their organisational pro-\ncedures, incentive structures, and management systems. \nOverwhelming evidence shows that such features influence \nthe design and deployment of technologies [4 ]. Moreover, \nresearch has demonstrated that risk-mitigation strategies \nwork best when adopted transparently, consistently, and \nwith executive-level support [153, 154]. Technology pro-\nviders are responsible for identifying the risks associated \nwith their LLMs and are uniquely well-positioned to man-\nage some of those risks. Therefore, it is crucial that their \norganisational procedures and governance structures are \nadequate.\nGovernance audits have a long history in areas like IT gov-\nernance [85, 155, 156] and systems and safety engineering \n[157-159]. Tasks include assessing internal governance struc-\ntures, product development processes and quality manage-\nment systems [115] to promote transparency and procedural \nregularity, ensure that appropriate risk management systems \nare in place [160], and spark deliberation regarding ethical \nand social implications throughout the software development \nlifecycle. Governance audits can also improve accountability, \ne.g., publicising their results prevents companies from cover-\ning up undesirable outcomes and incentivises better behav -\niour [136]. Thus defined, governance audits incorporate ele-\nments of both compliance audits, regarding completeness and \ntransparency of documentation, and risk audits, regarding the \nadequacy of the risk management system (Claim 1).\nSpecifically, we argue that governance audits of LLM \nproviders should focus on three tasks:20\n(1) Reviewing the adequacy of organisational governance \nstructures to ensure that model development processes \nfollow best practices and that quality management sys-\ntems can capture LLM-specific risks. While technology \nproviders have in-house quality management experts, \nconfirmation bias may prevent them from recognising \ncritical flaws; involving external auditors addresses \nthat issue [161]. Nevertheless, governance audits are \nmost effective when auditors and technology provid -\ners collaborate to identify risks [162]. Therefore, it is \nimportant to distinguish accountability from blame at \nthis stage of an audit.\n(2) Creating an audit trail of the LLM development pro-\ncess to provide chronological documentary evidence \nof the development of an LLM’s capabilities, including \ninformation about its intended purpose, design specifi-\ncations and choices, as well as how it was trained and \ntested through the generation of model cards [74] and \nsystem cards [77]. 21 This includes the structured use \n19 The link between model characteristics and biases in the training \ndata can sometimes be counterintuitive [292]. In some cases, biased \ndatasets can help models recognise bias and steer away from it with \nthe help of reinforcement learning and human feedback [293].\n20 Governance audits could examine many tasks, and prioritization \nmay vary depending on the sector and jurisdiction. Hence, the three \ntasks we propose are merely a minimum baseline.\n21 Meta AI’s detailed notes on the OPT model provide an exemplar of \nmodel training documentation [294].\n1096 AI and Ethics (2024) 4:1085–1115\n1 3\nof datasheets [76] to document how the datasets used \nto train and validate LLMs were sources, labelled, \nand curated. The creation of such audit trails serves \nseveral related purposes. Stipulating design specifica-\ntions upfront facilitates checking system adherence to \njurisdictional requirements downstream [157]. Moreo-\nver, information concerning intended use cases should \ninform licensing agreements with downstream devel-\nopers [163], thereby restricting the potential for harm \nthrough malicious use. Finally, requiring providers to \ndocument and justify their design choices sparks ethical \ndeliberation by making trade-offs explicit.\n(3) Mapping roles and responsibilities within organisa -\ntions that design LLMs to facilitate the allocation of \naccountability for system failures. LLMs’ adaptability \ndownstream does not exculpate technology providers \nfrom all responsibility. Some risks are ‘reasonably \nforeseeable’. In the adjacent field of machine learning \n(ML) image recognition, a study found that commer -\ncial gender classification systems were less accurate \nfor darker-skinned females than lighter-skin males [15]. \nAfter the release of these findings, all technology pro-\nviders speedily improved the accuracy of their mod-\nels, suggesting that the problem was not intrinsic, but \nresulted from inadequate risk management. Mapping \nthe roles and responsibilities of different stakeholders \nimproves accountability and increases the likelihood of \nimpact assessments being structured rather than ad-hoc, \nthus helping identify and mitigate harms proactively.\nTo conduct these three tasks, auditors primarily require \nwhat Koshiyama et al. [10] refer to as white-box auditing. \nThis is the highest level of access and suggests that the audi-\ntor knows how and why an LLM was developed. In practice, \nit implies privileged access to facilities, documentation, and \npersonnel, which is standard practice in governance audits \nin other fields. For example, IT auditors have full access to \nmaterial and reports related to operational processes and \nperformance metrics [85]. It also implies access to the input \ndata, learning procedures, and task objectives used to train \nLLMs. White-box auditing requires that nondisclosure and \ndata-sharing agreements are in place, which adds to the \nlogistical burden of governance audits. However, granting \nsuch a high level of access is especially important from an \nAI safety perspective because, in addition to auditing LLMs \nbefore market deployment, governance audits should also \nevaluate organisational safeguards concerning high-risk \nprojects that providers may prefer not to discuss publicly.\nThe results of governance audits should be provided in \nformats tailored to different audiences. The primary audience \nis the management and directors of the LLM provider. Audi-\ntors should provide a full report that directly and transparently \nlists and discusses the vulnerabilities of existing governance \nstructures. Such reports may recommend actions, but taking \nactions remains the provider’s responsibility. Usually, such \naudit reports are not made public. However, some evidence \nobtained during governance audits can be curated for two \nsecondary audiences: law enforcers and developers of down-\nstream applications. In some jurisdictions, hard legislation may \ndemand that technology providers follow specific requirements. \nFor instance, the proposed EU AI Act required providers to \nregister high-risk AI systems with a centralised database [43] \nor implement a risk management system [164]. In such cases, \nreports from independent governance audits can help provid-\ners demonstrate adherence to legislation. Reports from govern-\nance audits also help developers of downstream applications to \nunderstand an LLM’s intended purpose,  risks, and limitations.\nBefore concluding this discussion, it is useful to reflect \non how governance audits contribute to relieving some of \nthe social and ethical risks LLMs pose. As mentioned in \nSect.  2, Weidinger et al. [30] listed six broad risk areas: \ndiscrimination, information hazards, misinformation haz-\nards, malicious use, human–computer interaction harm, and \nautomation and environmental harms. Governance audits \naddress some of these directly. By assessing the adequacy \nof the governance structures surrounding LLMs, including \nlicencing agreements [163] and structured access protocols \n[68], governance audits help reduce the risk of malicious \nuse. Further, some information hazards stem from the pos-\nsibility of extracting sensitive information from LLMs via \nadversarial attacks [165]. By reviewing the process whereby \ntraining datasets were sourced, labelled, and curated, as well \nas the strategies and techniques used during the model train-\ning process—such as differential privacy [ 166] or secure \nfederated learning [167]—governance audits can minimise \nthe risk of LLMs leaking sensitive information. However, for \nmost of the risk areas listed by Weidinger et al. [30], gov -\nernance audits have only an indirect impact insofar as they \ncontribute to transparency about the limitations and intended \npurposes of LLMs. Hence, risks areas like discrimination, \nmisinformation hazards, and human–computer interaction \nharms are better addressed by model and application audits.\n4.3  Model audits\nBefore deployment, LLMs should be subject to model \naudits that assess their capabilities and limitations (Claim \n6). Model audits share some features with governance audits. \nFor instance, both happen before an LLM is adapted for \nspecific applications. However, model audits do not focus \non organisational procedures but on LLMs’ capabilities and \ncharacteristics. Specifically, they should identify an LLM’s \nlimitations to (i) inform the continuous redesign the system, \nand (ii) communicate its capabilities and limitations to exter-\nnal stakeholders. These two tasks use similar methodologies, \nbut they target different audiences.\n1097AI and Ethics (2024) 4:1085–1115 \n1 3\nThe first task—limitation identification—aims primarily \nto support organisations that develop LLMs with bench-\nmarks or other data points that inform internal model rede-\nsigning and retraining efforts [168]. Model audits’ results \nshould also inform API license agreements, helping prevent \napplications in unintended use cases [163 ] and restricting \nthe distribution of dangerous capabilities [68]. The second \ntask—communicating capabilities and limitations—aims to \ninform the design of specific applications built on top of \nLLMs by downstream developers. Such communication can \ntake different forms, e.g., interactive model cards [169], spe-\ncific language model risk cards [75], and information about \nthe initial training dataset [ 170, 171], to help downstream \ndevelopers adapt the model appropriately.\nIn Sect.  3, we argued that the way technology audits \nare being conducted requires modifications to address the \ngovernance challenges associated with LLMs (Claim 5 ). \nIn what follows, we demonstrate that evaluating an LLM’s \ncharacteristics independent of an intended use case is chal-\nlenging but not impossible. 22 To do so, auditors can use \ntwo distinct approaches. The first involves identifying and \nassessing intrinsic characteristics. For example, the training \ndataset can be assessed for completeness and consistency \nwithout reference to specific use cases [112]. However, it \nis often expensive and technically challenging to interro-\ngate large datasets [172]. The second involves employing \nan indirect approach that tests the model across multiple \npotential downstream use cases, links the results to differ -\nent characteristics, and assesses the aggregated results using \ndifferent weighting techniques. That second approach may \nprove more fruitful when assessing an LLM’s performance.\nNevertheless, selecting the characteristics to focus on dur-\ning model audits remains challenging. Given such audits’ \npurpose, we recommend examining characteristics that are \n(i) socially and ethically relevant, i.e., can be directly linked \nto the social and ethical risks posed by LLMs; (ii) predict-\nably transferable , i.e., impact the nature of downstream \napplications; and (iii) meaningfully operationalisable, i.e., \ncan be assessed with the available tools and methods.\nKeeping those criteria in mind, we posit that model audits \nshould focus on (at least) the performance, robustness, infor-\nmation security and truthfulness of LLMs. As other char -\nacteristics may meet the three criteria listed above, those \nfour characteristics are just examples highlighting the role \nof model audits in our three-layered approach. The list of \nrelevant model characteristics can be amended as required \nwhen developing specific auditing procedures. With those \ncaveats out of the way, we now proceed to discuss how four \nexample characteristics can be assessed during model audits:\n(1) Performance, i.e., how well the LLM functions on vari-\nous tasks. Standardised benchmarks can help assess \nan LLM’s performance by comparing it to a human \nbaseline. For example, GLUE [ 173] aggregates LLM \nperformance across multiple tasks into a single report-\nable metric. Such benchmarks have been criticised for \noverestimating performance over a narrow set of capa-\nbilities and quickly becoming saturated, i.e., rapidly \nconverging on the performance of non-expert humans, \nleaving limited space for valuable comparisons. \nTherefore, it is crucial to evaluate LLMs’ performance \nagainst many tasks or benchmarks, and sophisticated \ntools and methods have been proposed for that purpose, \nincluding SuperGLUE [49], which is more challenging \nand ‘harder to game’ with narrow LLM capabilities, \nand BIG-bench [64], which can assess LLM’s perfor -\nmance on tasks that appear beyond their current capa-\nbilities. These benchmarks are particularly relevant for \nmodel audits because they were primarily developed \nto evaluate pre-trained models, without task-specific \nfine-tuning.\n(2) Robustness,  i.e., how well the model reacts to unex -\npected prompts or edge cases. In ML, robustness indi-\ncates how well an algorithm performs when faced with \nnew, potentially unexpected (i.e., out-of-domain) input \ndata. LLMs lacking robustness introduce, at least, two \ndistinct risks [174]. First, the risk of critical system \nfailures if, for example, an LLM performs poorly for \nindividuals, unlike those represented in the training \ndata [175]. Second, the risk of adversarial attacks [176, \n177]. Therefore, researchers and developers have cre-\nated tools and methods to assess LLMs’ robustness, \nincluding adversarial methods like red teaming [58], \nevaluation toolkits like the Robustness Gym [ 178], \nbenchmark datasets like ANLI [ 179], and open-source \nplatforms for model-and-human-in-the-loop testing like \nDynabench [180]. Particularly relevant for our purposes \nis AdvGLUE [181], which evaluates LLMs’ vulnerabili-\nties to adversarial attacks in different domains using \na multi-task benchmark. By quantifying robustness, \nAdvGLUE facilitates comparisons between LLMs and \ntheir various affordances and limitations. However, \nrobustness can be operationalised in different ways, \ne.g., group robustness, which measures a model’s \nperformance across different sub-populations [182]. \nTherefore, model audits should employ multiple tools \nand methods to assess robustness.\n(3) Information security, i.e., how difficult it is to extract \ntraining data from the LLM. Several LLM-related \nrisks can be understood as ‘information hazards’ [30], \n22 A wide range of tools and methods to evaluate LLMs already \nexists. For an overview, see the report Holistic Evaluation of Lan-\nguage Models published by researchers at the Center for Research on \nFoundation Models [35].\n1098 AI and Ethics (2024) 4:1085–1115\n1 3\nincluding the risk of compromising privacy by leaking \npersonal data. As demonstrated by [165], adversarial \nagents can perform training data extraction attacks  \nto recover personal information like names and social \nsecurity numbers. However, not all LLMs are equally \nvulnerable to such attacks. The memorisation of train-\ning data can be minimised through differentially private \ntraining techniques [183], but their application gener -\nally reduces accuracy [184] and increases training time \n[151]. Promisingly, it is possible to assess the extent to \nwhich an LLM has unintentionally memorised rare or \nunique training data sequences using metrics such as \nexposure [185]. Testing strategies, like exposure, can \nbe employed at the model level, although that requires \nauditors to have access to the LLM and its training cor-\npus. Still, assessing LLMs’ information security during \nmodel audits does not address all information hazards \nbecause some risk of correctly inferring sensitive infor-\nmation about users can only be audited on an applica-\ntion level.\n(4) Truthfulness, i.e., to what extent the LLM can dis-\ntinguish between the real world and possible worlds. \nSome LLM-related risks stem from their capacity to \nprovide false or misleading information, which creates \nless well-informed users and potentially erodes public \ntrust in shared information [30]. Statistical methods \nstruggle to distinguish between factually correct ver -\nsus plausible but factually incorrect information. That \nproblem is exacerbated by the fact that many LLM \ntraining practices, like imitating human text on the web \nor optimising for clicks, are unlikely to create truthful \nAI [186].23 However, during model audits, our concern \nis not developing truthful AI but evaluating truthful-\nness. Such audits should focus on evaluating overall \ntruthfulness, not the truthfulness of an individual state-\nment. Yet that does not preclude focusing on multiple \naspects, e.g., how frequent falsehoods are on average, \nand how bad worst-case falsehoods are. One bench-\nmark that measures truthfulness is TruthfulQA [ 187], \nwhich generates a percentage score using 817 questions \nspanning 38 application domains, including healthcare \nand politics. When evaluating an LLM with the help of \nTruthfulQA, auditors would get a percentage score on \nhow truthful the model is. However, even a strong per-\nformance on TruthfulQA does not imply that an LLM \nwill be truthful in a specialised domain. Nevertheless, \nsuch benchmarks offer helpful tools for model audits.\nThese four characteristics pertain to pre-trained LLMs. \nHowever, model audits should also review training datasets. \nIt is well-known that training data gaps or biases create \nmodels that perform poorly on different datasets [ 188]. \nTraining LLMs with biased or incomplete data can cause \nrepresentational and allocational harms [189]. Therefore, a \nrecent European Parliament report [152] discussed mandat-\ning third-party audits of AI-training datasets. Technology \nproviders should prepare for such suggestions potentially \nbecoming legal requirements.\nDespite these technical and legal considerations, training \ndatasets are often collected with little curation, supervision, \nor foresight [190]. While curating ‘unbiased’ datasets may \nbe impossible, disclosing how a dataset was assembled can \nsuggest its potential biases [191]. Model auditors can use \nexisting tools and methods that interrogate biases in LLMs’ \npre-trained word embeddings, such as the metrics DisCo  \n[192], SEAT [193] or CAT  [194]. So-called data statements \n[195] can provide developers and users with the context \nrequired to understand specific models’ potential biases. \nData representativeness criterion [196] can determine how \nrepresentative 24 a training dataset is, and manual datasets \naudits can be supplemented with automatic analysis [ 197]. \nThe Text Characterisation Toolkit [198] permits automatic \nanalysis of how dataset properties impact model behaviour. \nWhile the availability of such tools is encouraging, it is \nimportant to remain realistic about what dataset audits can \nachieve. Model audits do not aim to ensure that LLMs are \nethical in any global sense. Instead, they contribute to better \nprecision in claims about an LLM’s capabilities and inform \nthe design of downstream applications.\nModel audits require auditors to have privileged access to \nLLMs and their training datasets. In the typology provided \nby Koshiyama et al. [10], this corresponds to medium-level \naccess, whereby auditors have access to an LLM equiva-\nlent to its developer, meaning they can manipulate model \nparameters and review learning procedures and task objec-\ntives. Such access is required to assess LLMs' capabilities \naccurately during model audits. However, in contrast to \nwhite-box audits, the access model auditors enjoy is limited \nto the technical system and does not extend to technology \nproviders’ organisational processes.\nSome of the characteristics tested for during model \naudits correspond directly to the social and ethical risks \nLLMs pose. For example, model audits entail evaluating \nLLMs according to characteristics like information secu-\nrity and truthfulness, which correspond to information \nhazards and misinformation hazards, respectively, in Wei-\ndinger et al.’s taxonomy [30]. Yet it should be noted that \nour proposed model audits only focus on a few characteris-\ntics of LLMs. That is because the criterion of meaningful \n23 Alternative techniques that are better suited for developing truthful \nAI include bootstrapping, adversarial training [295] and transparent \nAI [296].\n24 The term ‘representativeness’ has different meaning in statistics, \npolitics, and machine learning, ranging from a proportionate match \nbetween sample and population to a more general sense of inclusive-\nness [297].\n1099AI and Ethics (2024) 4:1085–1115 \n1 3\noperationalisability sets a high bar: not all risks associated \nwith LLMs can be addressed at the model level. Consider \ndiscrimination as an example. Model audits can expose the \nroot causes of some discriminatory practices, such as biases \nin training datasets that reflect historic injustices. However, \nwhat constitutes unjust discrimination is context-dependent \nand varies between jurisdictions. That problematises saying \nanything meaningful about risks like unjust discrimination \non a model level [199]. While important, that observation \ndoes not argue against model audits but for complementary \napproaches like application audits, as discussed next.\n4.4  Application audits\nProducts and services built using LLMs should undergo \napplication audits that assess the legality of their intended \nfunctions and how they will impact users and societies. \nUnlike governance and model audits, application audits \nfocus on actors employing LLMs in downstream applica-\ntions. Such audits are well-suited to ensure compliance with \nnational and regional legislation, sector-specific standards, \nand organisational ethics principles.\nApplication audits have two components: functionality \naudits, which evaluate applications using LLMs based on \ntheir intended and operational goals, and impact audits, \nwhich evaluate applications based on their impacts on dif-\nferent users, groups, and the natural environment. As dis-\ncussed in Sect. 3.2, both functionality and impact audits are \nwell-established practices [200]. Next, we consider how they \ncan be combined into procedures for auditing applications \nbased on LLMs.\nDuring functionality audits, auditors should check \nwhether the intended purpose of a specific application is \n(1) legal and ethical in and of itself and (2) aligned with \nthe intended use of the LLM in question. The first check is \nfor legal and ethical compliance, i.e., the adherence to the \nlaws, regulations, guidelines, and specifications relevant to \na specific application [201], as well as to voluntary ethics \nprinciples [202] or codes of conduct [203]. The purpose of \nthese compliance checks is straightforward: if an applica-\ntion is unlawful or unethical, the performance of its LLM \ncomponent is irrelevant, and the application should not be \npermitted on the market.\nThe second check within functionality audits aim to \naddress the risks stemming from developers overstating or \nmisrepresenting a specific application’s capabilities [204]. \nTo do so, functionality audits build on—and accounts \nfor outputs from—audits on other levels. During govern -\nance audits, technology providers are obliged to define the \nintended and disallowed use cases of their LLMs. During \nmodel audits, the limitations of LLMs are documented to \ninform their adaptation downstream. Using such informa-\ntion, functionality audits should ensure that downstream \napplications are aligned with a given LLM’s intended use \ncases in ways that take account of the model’s limitations. \nFunctionality audits thus combines the elements of compli-\nance and risks audit needed to provide assurance for LLMs \n(Claim 1).\nDuring impact audits, auditors disregard an application’s \nintended purpose and technological design to focus only on \nhow its outputs impact different user groups and the environ-\nment. The idea behind impact audits is simple: every system \ncan be understood in terms of its inputs and outputs [142]. \nHowever, despite that simplicity, implementing impact \naudits is notoriously hard. AI systems and their environ-\nments co-evolve in non-linear ways [137]. Therefore, the \nlink between an LLM-based application’s intended purpose \nand its actual impact may be neither intuitive nor consistent \nover time. Moreover, it is difficult to track impacts stem-\nming from indirect causal chains [205, 206]. Consequently, \nestablishing which direct and indirect impacts are considered \nlegally and socially relevant remains a context-dependent \nquestion which must be resolved on a case-by-case basis. \nThe application must be redesigned or terminated if the \nimpact is considered unacceptable.\nImportantly, impact audits should include both pre-\ndeployment (ex-ante) assessments and post-deployment \n(ex-post) monitoring (Claim 7 ).25 The former leverages \neither empirical evidence or plausible scenarios, depending \non how well-defined the application is and the predictability \nof the environments in which it will operate. For example, \napplications can be tested in sandbox environments [ 207] \nthat mimic real-world environments and allow developers \nand policymakers to understand the potential impact before \nan application goes to market. When used for ML-based \nsystems, sandboxes have proven safe harbours in which to \ndetect and mitigate biases [208]. However, real-world envi-\nronments often differ from training and testing environments \nin unforeseen ways [209]. Hence, pre-deployment assess-\nments of LLM-based applications must also use analytical \nstrategies to anticipate the application’s impact, e.g., ethical \nimpact assessments [ 110, 210, 211] and ethical foresight \nanalysis [153].\nPre-deployment impact assessments and post-deployment \nmonitoring are both individually necessary. As policymakers \nare well-aware, capturing the full range of potential harms \nfrom LLM-based applications requires auditing procedures \nto include elements of continuous oversight (again, see \nClaim 7). For example, the EU AI Act requires technol-\nogy providers to document and analyse high-risk AI sys-\ntems’ performance throughout their life cycles [43]. Meth-\nodologically, post-deployment monitoring can be done in \n25 This structure mirrors the ‘conformity assessments’ and ‘post-mar-\nket monitoring plans’ proposed in the EU AI Act [83].\n1100 AI and Ethics (2024) 4:1085–1115\n1 3\ndifferent ways, e.g., periodically reviewing the output from \nan application and comparing it to relevant standards. Such \nprocedures can also be automated, e.g., by using oversight \nprograms [212] that continuously monitor and evaluate sys-\ntem outputs and alert or intervene if they transgress prede-\nfined tolerance spans. Such monitoring can be done by both \nprivate companies and government agencies [213]. Overall, \napplication audits seek to ensure that ex-ante testing and \nimpact assessments have been conducted following existing \nbest practices; that post-market plans have been established \nto enable continuous monitoring of system outputs; and that \nprocedures are in place to mitigate or report different types \nof failure modes.\nBy focusing on individual use cases, application audits \nare well-suited to alerting stakeholders to risks that require \nmuch contextual information to understand and address. This \nincludes risks related to discrimination and human–com-\nputer interaction harms in Weidinger et al.’s taxonomy [30]. \nApplication audits help identify and manage such risks in \nseveral ways. For example, quantitative assessments link -\ning prompts with outputs can give a sense of what kinds \nof language an LLM is propagating and how appropriate \nthat communication style and content is in different settings \n[214, 215]. Moreover, qualitative assessments (e.g., those \nbased on interviews and ethnographic methods) can provide \ninsights into users’ lived experiences of interacting with an \nLLM [73].\nHowever, despite those methodological affordances, it \nremains difficult to define some forms of harm in any global \nsense [216]. For example, several studies have documented \nsituations in which LLMs propagate toxic language [ 150, \n217], but the interpretation of toxicity and the materialisa -\ntion of its harms vary across cultural, social, or political \ngroups [218-220]. Sometimes, ‘detoxifying’ an LLM may be \nincompatible with other goals and potentially suppress texts \nwritten about or by marginalised groups [221]. Moreover, \ncertain expressions might be acceptable in one setting but \nnot in another. In such circumstances, the most promising \nway forward is to audit not LLMs themselves but down-\nstream applications—thereby ensuring that each applica-\ntion’s outputs adhere to contextually appropriate conversa-\ntional conventions [101].\nAnother example concerns harmfulness, i.e., the extent \nto which an LLM-based application inflicts representational, \nallocational or experiential harms. 26 An LLM that lacks \nrobustness or performs poorly for some social groups may \npermit unjust discrimination [30] or violate capability fair -\nness [222] when informing real-world allocational decisions \nlike hiring. Multiple benchmarks exist to assess model ste-\nreotyping of social groups, including CrowS-Pairs [ 223], \nStereoSet [194] or Winogender [224]. To assess risks from \nexperiential harms, quantitative assessments of LLM outputs \ngive a sense of the language it is propagating. For example, \n[150] have developed the RealToxicityPrompts benchmark \nto assess the toxicity of a generated completion.27 However, \nthe tools mentioned above are only examples. The main \npoint here is that representational, allocational and experi-\nential harms associated with LLMs are best assessed at the \napplication level through functionality and impact audits as \ndescribed in this section.\nTo conduct application audits, lower levels of access are \nsufficient. For example, to make quantitative assessments to \ndetermine the relationship between inputs and outputs, it is \nsufficient that auditors have what Koshiyama et al. [10] refer \nto as black-box model access or, in some cases, input data \naccess. Similarly, to audit LLM-based applications for legal \ncompliance and ethical alignment, auditors do not require \ndirect access to the underlying model but can rely on pub-\nlicly available information—including the claims technol -\nogy providers and downstream developers make about their \nsystems and the user instructions attached to them.\nWe contend that governance audits and model audits \nshould be obligatory for all technology providers design-\ning and disseminating LLMs. However, we recommend that \napplication audits should be employed more selectively. \nFurther, although application audits may form the basis for \ncertification [225], auditing does not equal certification. Cer-\ntification requires predefined standards against which a prod-\nuct or service can be audited and institutional arrangements \nto ensure the certification process’s integrity [131]. Even \nwhen not related to certification, application audits’ results \nshould be publicly available (at least in summary form). \nRegistries publishing such results incentivise companies to \ncorrect behaviour, inform enforcement actions and help cure \ninformational asymmetries in technology regulation [12].\n4.5  Connecting the dots\nIn order to make a real difference to the ways in which LLMs \nare designed and used, governance, model, and application \naudits must be connected into a structured process. In prac-\ntice, this means that outputs from audits on one level become \ninputs for audits on other levels. Model audits, for instance, \nproduce reports summarising LLMs’ properties and limi-\ntations, which should inform application audits that verify \nwhether a model’s known limitations have been considered \nwhen designing downstream applications. Similarly, ex-\npost application audits produce output logs documenting \n26 [298] distinguish between representational harms (portraying some \ngroups more favourably than others) and allocation harms (allocating \nresources or opportunities unfairly by social group).\n27 This benchmark relies on PerspectiveAPI to score ‘toxicity’, which \nis a limitation given that system’s weaknesses [145, 299, 300].\n1101AI and Ethics (2024) 4:1085–1115 \n1 3\nthe impact that different applications have in applied set-\ntings. Such logs should inform LLMs’ continuous redesign \nand revisions of their accompanying model cards. Finally, \ngovernance audits must check the extent to which technol-\nogy providers’ software development processes and quality \nmanagement systems include mechanisms to incorporate \nfeedback from application audits. Figure  2 illustrates how \ngovernance, model, and application audits are intercon-\nnected in our blueprint.\nEach step in our three-layered approach should involve \nindependent third-party auditors (Claim 2 ). However, two \ncaveats are required here. First, it need not be the same \norganisation conducting audits on all three levels as each \nrequires different competencies. Governance audits require \nunderstanding corporate governance [226] and soft skills \nlike stakeholder communication. Model audits are highly \ntechnical and require knowledge about evaluating ML \nmodels, operationalising different normative dimensions, \nand visualising model characteristics. Application auditors \ntypically need domain-specific expertise. All these compe-\ntencies may not be found within one organisation.\nSecond, as institutional arrangements vary between juris-\ndictions and sectors, the best option may be to leverage the \ncapabilities of institutions operating within a specific geog-\nraphy or industry to perform various elements of govern-\nance, model, and application audits. For example, medical \ndevices are already subject to various testing and certifica-\ntion procedures before being launched. Hence, application \naudits for new medical devices incorporating LLMs could be \nintegrated with such procedures. In part, this is already hap-\npening. The US Food and Drug Administration (FDA) has \nproposed a regulatory framework for modifying ML-based \nsoftware as a medical device [227]. The point is that dif-\nferent independent auditors can perform the three different \ntypes of audits outlined here and that different institutional \narrangements may be preferable in different jurisdictions or \nsectors.\nFig. 2  Outputs from audits on one level become inputs for audits on other levels\n1102 AI and Ethics (2024) 4:1085–1115\n1 3\n5  Limitations and avenues for further \nresearch\nThis section highlights three limitations of our work that \napply to any attempt to audit LLMs: one conceptual, one \ninstitutional and one practical. First, model audits pose \nconceptual problems related to construct validity. Second, \nan institutional ecosystem to support independent third-\nparty audits has yet to emerge. Third, not all LLM-related \nsocial and ethical risks can be practically addressed on the \ntechnology level. We consider these limitations in turn, dis-\ncuss potential solutions, and provide directions for future \nresearch.\n5.1  Lack of methods and metrics to operationalise \nnormative concepts\nOne bottleneck to developing effective auditing procedures \nis the difficulty of operationalising normative concepts like \nrobustness and truthfulness [228]. A recent case study found \nthat organisations' lack of standardised evaluation metrics is \na crucial challenge when implementing AI auditing proce-\ndures [229, 230]. The problem is rooted in construct validity, \ni.e., the extent to which a given metric accurately measures \nwhat it is supposed to [231]. Construct validity problems \nprimarily arise in our blueprint from attempts to operational-\nise characteristics like performance, robustness, information \nsecurity and truthfulness during model audits.\nConsider truthfulness as an example. LLMs do not require \na model of the real world. Instead, they compress vast num-\nbers of conditional probabilities by picking up on language \nregularities [232, 233]. Therefore, they have no reason to \nfavour any reality but can select from various possible worlds, \nprovided each is internally coherent [ 234].28 However, dif-\nferent epistemological positions disagree about the extent to \nwhich this way of sensemaking is unique to LLMs or, indeed, \na problem at all. Simplifying to the extreme, realists believe \nin objectivity and the singularity of truth, at least insofar as \nthe natural world is concerned [235]. In contrast, relativists \nbelieve that truth and falsity are products of context-dependent \nconventions and assessment frameworks [236]. Numerous \ncompromise positions can be found on the spectrum between \nthose poles. However, tackling pressing social issues cannot \nawait the resolution of long-standing philosophical disa-\ngreements. Indeed, courts settle disagreements daily based \non pragmatist operationalisations of concepts like truth and \nfalsehood in keeping with the pragmatic maxim that theories \nshould be judged by their success when applied practically to \nreal-world situations [237].\nFollowing that reasoning, we argue that refining prag-\nmatist operationalisations of concepts like truthfulness and \nrobustness do more to promote fairness, accountability, \nand transparency in using LLM than either dogmatic or \nsceptical alternatives [238]. However, developing metrics \nto capture the essence of thick normative concepts is dif-\nficult and entails many well-known pitfalls. Reductionist \nrepresentations of normative concepts generally bear little \nresemblance to real-life considerations, which tend to be \nhighly contextual [239]. Moreover, different operationalisa-\ntions of the same normative concept (like ‘fairness’) cannot \nbe satisfied simultaneously [240]. Finally, the quantifica-\ntion of normative concepts can itself have subversive or \nundesired consequences [241, 242]. As Goodhart’s Law \nreminds us, a measure ceases to be a good metric once it \nbecomes a target.\nThe operationalisation of characteristics like perfor -\nmance, robustness, information security and truthfulness \ndiscussed in Sect.  4 is subject to the above limitations. \nResolving all construct validity problems may be impossi-\nble, but some ways of operationalising normative concepts \nare better than others for evaluating an LLM’s characteris-\ntics. Consequently, an important avenue for further research \nis developing new methods to operationalise normative con-\ncepts in ways that are verifiable and maintain high construct \nvalidity.\n5.2  Lack of an institutional ecosystem\nA further limitation is that our blueprint does not decisively \nidentify who should conduct the audits it recommends. This \nis a limitation, since any auditing procedure will only be as \ngood as the institution delivering it [243]. However, we have \nleft the question open for two reasons. First, different institu-\ntional ecosystems intended to support audits and conformity \nassessments of AI systems are currently emerging in differ-\nent jurisdictions and sectors [244]. Second, our blueprint \nis flexible enough to be adopted by any external auditor. \nHence, the feasibility and effectiveness of our approach do \nnot hinge on the question of institutional design.\nThat said, the question of who audits whom is important, \nand much can be learned from auditing in other domains. \nFive institutional arrangements for structuring independent \naudits are particularly relevant to our purposes. Audits of \nLLMs can be conducted by:\n(1) Private service providers, chosen by and paid for by \nthe technology provider (equivalent to the role account-\ning firms play during financial audits or business ethics \naudits [245]).\n(2) A government agency, centrally administered and paid \nfor by government, industry, or a combination of both \n28 LLMs favour the statistically most likely reality given their train-\ning data. Yet any training data necessarily constitute a reduction of \nreality that supports some interpretations but obscures others [301].\n1103AI and Ethics (2024) 4:1085–1115 \n1 3\n(equivalent to the FDA’s role in approving food and \ndrug substances [246]).29\n(3) An industry body, operationally independent yet funded \nthrough fees from its member companies (equivalent to \nthe British Safety Council’s role in audits of workers’ \nhealth and safety [247]).\n(4) Non-profit organisations, operationally independent \nand funded through public grants and voluntary dona-\ntions (equivalent to the Rainforest Alliance role in \nauditing forestry practices [248]).\n(5) An international organisation, administered and funded \nby its member countries (equivalent to the International \nAtomic Energy Agency’s role in auditing nuclear medi-\ncine practices [249]).\nEach of these arrangements has its own set of affordances \nand constraints. Private service providers, for example, are \nunder constant pressure to innovate, which can be beneficial \ngiven the fast-moving nature of LLM research. However, \nprivate providers’ reliance on good relationships with tech-\nnology providers to remain in business increases the risk of \ncollusion [250]. Therefore, some researchers have called for \nmore government involvement, including an ‘FDA for algo-\nrithms’ [251]. Establishing a government agency to review \nand approve high-risk AI systems could ensure the uniform-\nity and independence of pre-market audits but might stifle \ninnovation and cause longer lead times. Moreover, while \nthe FDA enjoys a solid international reputation [252], not \nall jurisdictions would consider the judgement of an agency \nwith a national or regional mandate legitimate.\nThe lack of an institutional ecosystem to implement and \nenforce the LLM auditing blueprint outlined in this article is \na limitation. Without clear institutional arrangements, claims \nthat an AI system has been audited are difficult to verify and \nmay exacerbate harms [133]. Further research could use-\nfully investigate the feasibility and effectiveness of different \ninstitutional arrangements for conducting and enforcing the \nthree types of audits proposed.\n5.3  Not all risks from LLMs can be addressed \non the technology level\nOur blueprint for auditing LLMs has been designed to con-\ntribute to good governance. However, it cannot eliminate \nthe risks associated with LLMs for three reasons. First, most \nrisks cannot be reduced to zero [125]. Hence, the question is \nnot whether residual risks exist but how severe and socially \nacceptable they are [253]. Second, some risks stem from \ndeliberate misuse, creating an offensive-defensive asym-\nmetry wherein responsible actors constantly need to guard \nagainst all possible vulnerabilities while malicious agents \ncan cause harm by exploiting a single vulnerability [254]. \nThird, as we will expand on below, not all risks associated \nwith LLMs can be addressed on the technology level.\nWeidinger et al. [30] list over 20 risks associated with \nLLMs divided into six broad risk areas. In Sect. 4, we high-\nlighted how our three-layered approach helps identify and \nmitigate some of these risks. To recap, governance audits \ncan help protect against risks associated with malicious use; \nmodel audits can help identify and manage information and \nmisinformation hazards; and application audits can help pro-\ntect against discrimination as well as experiential harms. Of \ncourse, these are just examples. Audits at each level con-\ntribute, directly or indirectly, to addressing many different \nrisks. However, not all the risks listed by Weidinger et al. \nare captured by our blueprint. Consider automation harm \nas an example. Increasing the capabilities of LLMs to com-\nplete tasks that would otherwise require human intelligence \nthreatens to undermine creative economies [255]. While \nsome highly potent LLMs may remove the basis for some \nprofessions that employ many people today—such as trans-\nlators or copywriters—that is not a failure on the part of the \ntechnology. The alternative of building less capable LLMs \nis counterproductive since abstaining from technology usage \ngenerates significant social and economic opportunity costs \n[256].\nThe problem is not necessarily change per se but its speed \nand how the fruits of automation are distributed [257, 258]. \nHence, problems related to changing economic environments \nmay be better addressed through social and political reform \nrather than audits of specific technologies. It is important to \nremain realistic about auditing’s capabilities and not fall into \nthe trap of overpromising when introducing new govern-\nance mechanisms [259]. However, the fact that no auditing \nprocedures can address all risks associated with LLMs does \nnot diminish their merits. Instead, it points towards another \nimportant avenue for further research: how can and should \nsocial and political reform complement technically oriented \nmechanisms in holistic efforts to govern LLMs?\n6  Conclusion\nSome of the features that make LLMs attractive also create \nsignificant governance challenges. For instance, the potential \nto adapt LLMs to a wide range of downstream applications \nundermines system verification procedures that presuppose \nwell-defined demand specifications and predictable oper -\nating environments. Consequently, our analysis in Sect.  3 \nconcluded that existing AI auditing procedures are not \n29 Alternative models of government involvement exist. For example, \naudits may be conducted or sanctioned by a government agency like \nthe National Institute of Standards and Technology (NIST) in the US \nor by the same notified bodies that the European Commission [43] \nhas tasked with performing conformity assessments of high-risk AI \nsystems in the EU.\n1104 AI and Ethics (2024) 4:1085–1115\n1 3\nwell-equipped to assess whether the checks and balances put \nin place by technology providers and downstream developers \nare sufficient to ensure good governance of LLMs.\nIn this article, we have attempted to bridge that gap by \noutlining a blueprint for how to audit LLMs. In Sect.  4, we \nintroduced a three-layered approach, whereby governance, \nmodel and application audits inform and complement each \nother. During governance audits, technology providers’ \naccountability structures and quality management systems \nare evaluated for robustness, completeness, and adequacy. \nDuring model audits, LLMs’ capabilities and limitations are \nassessed along several dimensions, including performance, \nrobustness, information security, and truthfulness. Finally, \nduring application audits, products and services built on top \nof LLMs are first assessed for legal compliance and subse-\nquently evaluated based on their impact on users, groups, \nand the natural environment.\nTechnology providers and policymakers have already \nstarted experimenting with some of the auditing activities \nwe propose. Consequently, auditors can leverage a wide \nrange of existing tools and methods, such as impact assess-\nments, benchmarking, model evaluation, and red teaming, \nto conduct governance, model, and application audits. That \nsaid, the feasibility and effectiveness of our three-layered \napproach hinge on two factors. First, only when conducted \nin a combined and coordinated fashion can governance, \nmodel and application audits enable different stakeholders \nto manage LLM-related risks. Hence, audits on the three \nlevels must be connected in a structured process. Govern-\nance audits should ensure that providers have mechanisms \nto take the output logs generated during application audits \ninto account when redesigning LLMs. Similarly, application \naudits should ensure that downstream developers take the \nlimitations identified during model audits into account when \nbuilding on top of a specific LLM. Second, audits at each \nlevel must be conducted by an independent third-party to \nensure that LLMs are ethical, legal, and technically robust. \nThe case for independent audits rests not only on concerns \nabout the misaligned incentives that technology providers \nmay face but also on concerns about the rapidly increasing \ncapabilities of LLMs [260].\nHowever, even when implemented under ideal circum-\nstances, audits will not solve all tensions or protect against \nall risks of harm associated with LLMs. So, it is important \nto remain realistic about what auditing can achieve and the \nmain limitations of our approach discussed in Sect.  5 are \nworth reiterating. To begin with, the feasibility of model \naudits hinges on the construct validity of the metrics used to \nassess characteristics like robustness and truthfulness. This \nis a limitation because such normative concepts are notori-\nously difficult to operationalise. Further, our blueprint for \nhow to audit LLMs does not specify who should conduct \nthe audits it posits. No auditing procedure is stronger than \nthe institutions backing it. Hence, the fact that an ecosys-\ntem of actors capable of implementing our blueprint has \nyet to emerge constrains its effectiveness. Finally, not all \nrisks associated with LLMs arise from processes that can \nbe addressed through auditing. Some tensions are inherently \npolitical and require continuous management through public \ndeliberation and structural reform.\nAcademics and industry researchers can contribute to \novercoming these limitations by focusing on two avenues \nfor further research. The first is to develop new methods \nand metrics to operationalise normative concepts in ways \nthat are verifiable and maintain a high degree of construct \nvalidity. The second is to disentangle further the sources of \ndifferent types of risks associated with LLMs. Such research \nwould advance our understanding of how political reform \ncan complement technically oriented mechanisms in holistic \nefforts to govern LLMs.\nPolicymakers can facilitate the emergence of an insti-\ntutional ecosystem capable of carrying out and enforcing \ngovernance, model, and application audits of LLMs. For \nexample, policymakers can encourage and strengthen private \nsector auditing initiatives by creating standardised evalua-\ntion metrics [261], harmonising AI regulation [262], facili-\ntating knowledge sharing [263] or rewarding achievements \nthrough monetary incentives [256]. Policymakers should \nalso update existing and proposed AI regulations in line with \nour three-layered approach to address LLM-related risks. \nFor example, while the EU AI Act’s conformity assessments \nand post-market monitoring plans mirror application audits, \nthe proposed regulation does not contain mechanisms akin \nto governance and model audits [83]. Without amendments, \nsuch regulations are unlikely to generate adequate safeguards \nagainst the risks associated with LLMs.\nOur findings most directly concern technology providers \nas they are primarily responsible for ensuring that LLMs are \nlegal, ethical, and technically robust. Such providers have \nboth moral and material reasons to subject themselves to \nindependent audits, including the need to manage financial \nand legal risks [264] and build an attractive brand [265]. \nSo, what ought technology providers do? To start with, they \nshould subject themselves to governance audits and their \nLLMs to model audits. That would create a demand for inde-\npendent auditing and accreditation bodies and help spark \nmethodological innovation in governance and model audits. \nMid-term, Technology providers should also demand that \nproducts and services built on top of their LLMs undergo \napplication audits. That could be done through structured \naccess procedures, whereby permission for using an LLM \nis conditional on such terms. In the long-term, like-minded \ntechnology providers should establish, and fund, an inde-\npendent industry body that conducts or commissions govern-\nance, model, and application audits.\n1105AI and Ethics (2024) 4:1085–1115 \n1 3\nTaking a long-term perspective, our three-layered \napproach holds lessons for how to audit more capable and \ngeneral future AI systems. This article has focused on LLMs \nbecause they have broad societal impacts via widespread \napplications already today. However, elements of the gov -\nernance challenges—including generativity, emergence, \nlack of grounding, and lack of access—have some general \napplicability to other ML-based systems [266, 267]. Hence, \nwe anticipate that our blueprint can inform the design proce-\ndures for auditing other generative, ML-based technologies.\nThat said, the long-term feasibility and effectiveness of \nour blueprint for how to audit LLMs may also be under -\nmined by future developments. For example, governance \naudits make sense when only a limited number of actors have \nthe ability and resources to train and disseminate LLMs. \nThe democratisation of AI capabilities—either through \nthe reduction of entry barriers or a turn to business models \nbased on open-source software—would challenge this sta-\ntus quo [268]. Similarly, if language models become more \nfragmented or personalised [93], there will be many user-\nspecific branches or instantiations of a single LLM which \nwould make model audits more complex to standardise. As a \nresult, while maintaining the usefulness of our three-layered \napproach, we acknowledge that it will need to be continu-\nously revised in response to the changing technological and \nregulatory landscape.\nIt is worth concluding with some words of caution. \nOur blueprint is not intended to replace existing govern-\nance mechanisms but to complement and interlink them by \nstrengthening procedural transparency and regularity. Rather \nthan being adopted wholesale by technology providers and \npolicymakers, we hope that our three-layered approach can \nbe adopted, adjusted, and expanded to meet the governance \nneeds of different stakeholders and contexts.\nAppendix 1: methodology\nBefore describing our methodology, something should be \nsaid about our research approach. According to the pragma-\ntist tradition, research is only legitimate when applied, i.e., \ngrounded in real-world problems [269]. As established in \nSect. 2, there is a need to develop new governance mecha-\nnisms that different stakeholders can use to identify and miti-\ngate the risks associated with LLMs. In this article, we take \na pragmatist stance when exploring how auditing procedures \ncan be designed so they are feasible and effective in practice.\nDesigning procedures to audit LLMs is an art, not a sci-\nence. In a policy context, applied research concerns the \nevaluation of different governance design decisions or poli-\ncies in relation to a desired outcome [270]. From a prag-\nmatist point of view, however, a mark of quality in applied \npolicy research is that questions are answered in ways that \nare actionable [237]. That implies that researchers must \nsometimes go beyond an evaluation of existing options to \nprescribe new solutions. While there is no guarantee that the \nbest course of action will be found, researchers can ensure \nrigour by systematically building on previous research and \nby incorporating input from different stakeholders.\nMindful of those considerations, the following method-\nology was used to develop our blueprint for how to audit \nLLMs. Note that while the five steps below exhaust the range \nof research activities that went into this study, the sequential \npresentation is a gross simplification. In reality, the research \nprocess was messy and iterative, with several of the steps \noverlapping both thematically and chronologically.\nFirstly, we mapped existing auditing procedures designed \nto identify the risks associated with different AI systems \nthrough a systematised literature review [271]. In doing so, \nwe searched five databases (Google Scholar, Scopus, SSRN, \nWeb of Science and arXiv) for articles related to the auditing \nof AI systems. Keywords for the search included (“auditing, \n“evaluation” OR “assessment”) AND (“fairness”, “truthful-\nness”, “transparency” OR “robustness”) AND (“language \nmodels”, “artificial intelligence” OR “algorithms”). How -\never, not all relevant auditing procedures have been devel-\noped by academic researchers. Hence, we used a snowball-\ning technique [ 272], i.e., tracking the citations of already \nincluded articles, to identify auditing procedures developed \nby private service providers, national or regional policymak-\ners and industry associations. A total of 126 documents were \nincluded in this systematised literature review.\nSecondly, we identified the elements underpinning these \nprocedures. This resulted in a typology that distinguishes \nbetween different types of audits, e.g., risk and compliance \naudits; internal and external audits; ex-ante and ex-post \naudits; as well as between functionality, code, and impact \naudits. The space of possible auditing procedures consists \nof all unique combinations between these different elements.\nThirdly, we generated a list of key claims about how \nauditing procedures for LLMs should be designed so that \nthey are feasible and effective in practice. To do this, we \nconducted a gap analysis between the governance challenges \nposed by LLMs on the one hand and the theoretical affor -\ndances of existing AI auditing procedures on the other. Our \nanalysis resulted in seven key claims about how auditing \nprocedures should be designed in order to capture the full \nrange of risks posed by LLMs. Those claims are presented \nand discussed in Sect. 3.\nFourthly, we created a draft blueprint for how to audit \nLLMs by identifying the smallest set of auditing proce-\ndures that satisfied our seven key claims. In practice, not all \nauditing procedures are equally effective in identifying the \nrisks posed by LLMs. Besides, some auditing procedures \nserve similar functions. Although some redundancy is an \nimportant feature in safety engineering, too much overlap \n1106 AI and Ethics (2024) 4:1085–1115\n1 3\nbetween different auditing regimes can be counterproductive \nin so far as roles and responsibilities become less clear and \nscarce resources are being consumed that could otherwise \nhave been more effectively invested elsewhere. This step \nthus consisted of reducing the theoretical space of possible \nauditing procedures into a limited set of activities that are (1) \njointly sufficient to identify the full range of risks associated \nwith LLMs, (2) practically feasible to implement, and (3) \nseem to have a justifiable cost–benefit ratio.\nFifthly and finally, we sought to refine and validate our \ndraft blueprint by triangulating findings [273] from different \nsources. For example, we sought input from a diverse set of \nstakeholders. In total, we conducted over 20 semi-structured \ninterviews [274] with, and received feedback from, research-\ners, professional auditors, AI developers at frontier labs and \npolicymakers in different jurisdictions. The final blueprint \noutlined in Sect. 4 is the result of those consultations.\nAcknowledgments The authors would like to thank the following peo-\nple for their helpful comments on earlier versions of this manuscript: \nMarkus Anderljung, Matthew Salganik, Arvind Narayanan, Deep Gan-\nguli, Katherine Lee, Toby Shevlane, Varun Rao, and Lennart Heim. \nIn the process of writing the article, the authors also benefited from \nconversations with and input from Allan Dafoe, Ben Garfinkel, Anders \nSandberg, Emma Bluemke, Andreas Hauschke, Owen Larter, Sebastien \nKrier, Mihir Kshirsagar, and Jade Leung. The article is much better \nfor it.\nAuthor contributions JM is the first author of this paper. JS, HRK, and \nLF contributed equally to the paper.\nFunding JM’s doctoral research at the Oxford Internet Institute is sup-\nported through a studentship provided by AstraZeneca. JM also con-\nducted part of this research during a paid Research Fellowship at the \nCentre for Governance of AI.\nData availability We confirm that the research presented in this article \nis our own, that it has not been submitted to any other journal, and \nthat all sources we used have been credited both in the text and in the \nbibliography. As part of this research, we have not collected or stored \nany personal or sensitive data.\nDeclarations \nConflict of interest The authors have no conflicts of interest to declare.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Sandvig, C., Hamilton, K., Karahalios, K., Langbort, C.: Audit-\ning algorithms. In: ICA 2014 Data and Discrimination Preconfer-\nence, pp. 1–23 (2014). https:// doi. org/ 10. 1109/ DEXA. 2009. 55\n 2. Diakopoulos, N.: Algorithmic accountability: journalistic investi-\ngation of computational power structures. Digit. J. 3(3), 398–415 \n(2015). https:// doi. org/ 10. 1080/ 21670 811. 2014. 976411\n 3. Mökander, J., Floridi, L.: Ethics—based auditing to develop \ntrustworthy AI. Minds Mach. (Dordr) 0123456789, 2–6 (2021). \nhttps:// doi. org/ 10. 1007/ s11023- 021- 09557-8\n 4. Brundage, M., et al.: Toward trustworthy AI development: \nmechanisms for supporting verifiable claims. ArXiv, no. \n2004.07213[cs.CY])., 2020, [Online]. http:// arxiv. org/ abs/ 2004. \n07213\n 5. Raji, I.D., Buolamwini, J.: Actionable auditing: Investigating \nthe impact of publicly naming biased performance results of \ncommercial AI products. In: AIES 2019—Proceedings of the \n2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. \n429–435, (2019). https:// doi. org/ 10. 1145/ 33066 18. 33142 44\n 6. Mökander, J., Morley, J., Taddeo, M., Floridi, L.: Ethics-based \nauditing of automated decision-making systems: nature, scope, \nand limitations. Sci. Eng. Ethics (2021). https:// doi. org/ 10. 1007/ \ns11948- 021- 00319- 4ORIG INAL\n 7. Cobbe, J., Lee, M.S.A., Singh, J.: Reviewable automated \ndecision-making: a framework for accountable algorithmic \nsystems. In: FAccT 2021—Proceedings of the 2021 ACM \nConference on Fairness, Accountability, and Transparency, \npp. 598–609 (2021). https:// doi. org/ 10. 1145/ 34421 88. 34459 21\n 8. Floridi, L.: Infraethics–on the conditions of possibility of \nmorality. Philos. Technol. 30(4), 391–394 (2017). https:// doi. \norg/ 10. 1007/ s13347- 017- 0291-1\n 9. Raji, I.D. et al.: Closing the AI accountability gap: Defining \nan end-to-end framework for internal algorithmic auditing. \nIn: FAT* 2020—Proceedings of the 2020 Conference on Fair -\nness, Accountability, and Transparency, pp. 33–44, 2020, doi: \nhttps:// doi. org/ 10. 1145/ 33510 95. 33728 73\n 10. Koshiyama, A., Kazim, E., Treleaven, P.: Algorithm auditing: \nmanaging the legal, ethical, and technological risks of artifi-\ncial intelligence, machine learning, and associated algorithms. \nIEEE 55(4), 40–50 (2022). https:// doi. org/ 10. 1109/ MC. 2021. \n30672 25\n 11. Power, M.: The Audit Society: Rituals of Verification. Oxford \nUniversity Press, Oxford (1997)\n 12. Raji, I.D., Xu, P., Honigsberg, C., Ho, D.: Outsider oversight: \ndesigning a third party audit ecosystem for AI governance. In: \nAIES 2022 - Proceedings of the 2022 AAAI/ACM Conference \non AI, Ethics, and Society, pp. 557–571, Jul. 2022, https:// doi. \norg/ 10. 1145/ 35140 94. 35341 81\n 13. Kazim, E., Koshiyama, A.S., Hilliard, A., Polle, R.: Systematiz-\ning audit in algorithmic recruitment. J. Intell. 9(3), 1–11 (2021). \nhttps:// doi. org/ 10. 3390/ jinte llige nce90 30046\n 14. Robertson, R.E., Jiang, S., Joseph, K., Friedland, L., Lazer, \nD., Wilson, C.: Auditing partisan audience bias within Google \nsearch. In: Proc ACM Hum Comput Interact, vol. 2, no. CSCW, \n2018, https:// doi. org/ 10. 1145/ 32744 17\n 15. Buolamwini, J., Gebru, T.: Gender shades: intersectional accu-\nracy disparities in commercial gender classification. In: Confer-\nence on Fairness, Accountability, and Transparency, 2018, pp. \n1–15. https:// doi. org/ 10. 2147/ OTT. S1269 05\n 16. Oakden-Rayner, L., et al.: Validation and algorithmic audit of \na deep learning system for the detection of proximal femoral \nfractures in patients in the emergency department: a diagnostic \naccuracy study. Lancet Digit. Health 4 (5), e351–e358 (2022). \nhttps:// doi. org/ 10. 1016/ S2589- 7500(22) 00004-8\n1107AI and Ethics (2024) 4:1085–1115 \n1 3\n 17. Liu, X., Glocker, B., McCradden, M.M., Ghassemi, M., Den-\nniston, A.K., Oakden-Rayner, L.: The medical algorithmic audit. \nLancet Digit. Health 4(5), e384–e397 (2022). https:// doi. org/ 10. \n1016/ S2589- 7500(22) 00003-6\n 18. Bommasani, R., et al.: On the opportunities and risks of founda-\ntion models. ArXiv, Aug. 2021, [Online]. http:// arxiv. org/ abs/ \n2108. 07258\n 19. Bommasani, R., Liang, P.: Reflections on foundation models. \nHAI, 2021. https:// hai. stanf ord. edu/ news/ refle ctions- found ation- \nmodels. Accessed 13 Feb 2023\n 20. Floridi, L., Chiriatti, M.: GPT-3: its nature, scope, limits, and \nconsequences. Minds Mach. 30(4), 681–694 (2020). https:// doi. \norg/ 10. 1007/ s11023- 020- 09548-1\n 21. Rosenfeld, R.: Two decdes of statistical language modeling where \ndo we go form here? Where do we go from here? Proc. IEEE \n88(8), 1270–1275 (2000). https:// doi. org/ 10. 1109/5. 880083\n 22. Brown, T.B., et al.: Language models are few-shot learners. Adv. \nNeural Inf. Process. Syst. (2020). https:// doi. org/ 10. 48550/ arxiv. \n2005. 14165\n 23. OpenAI.: GPT-4 Technical Report. Mar. 2023. [Online]. https:// \narxiv. org/ abs/ 2303. 08774 v3. Accessed 12 Apr 2023\n 24. Chowdhery, A., et al.: PaLM: scaling language modeling with \npathways. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. 2204. \n02311\n 25. Thoppilan, R., et al.: LaMDA: language models for dialog appli-\ncations. ArXiv (2022)\n 26. Rae, J.W. et al.: Scaling language models: methods, analysis & \ninsights from training Gopher. ArXiv (2022)\n 27. S. Zhang et al., “OPT: Open Pre-trained Transformer Language \nModels,” ArXiv, May 2022, [Online]. Available: http:// arxiv. org/ \nabs/ 2205. 01068\n 28. Hu, Y., Jing, X., Ko, Y., Rayz, J.T.: Misspelling correction with \npre-trained contextual language model. In:  2020 IEEE 19th \nInternational Conference on Cognitive Informatics & Cognitive \nComputing (ICCICC), pp. 144–149. (2020)\n 29. Hsieh, K.: Transformer poetry: poetry classics reimagined by \nartificial intelligence. San Francisco: Paper Gains Publishing, \n2019. [Online]. https:// paper gains. co/ pdfs/ Trans former_ Poetry- \n978-1- 73416 47-0- 1. pdf. Accessed 20 Jan 2023\n 30. Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.S., Mel-\nlor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., Biles, \nC., Brown, S., Kenton, Z., Hawkins, W., Stepleton, T., Birhane, \nA., Hendricks, A.L., Rimell, L., Isaac, W., Haas, J., Legassick, \nS., Irving, G., Gabriel, I.: Taxonomy of risks posed by language \nmodels. In: 2022 ACM Conference on Fairness, Accountabil -\nity, and Transparency (FAccT '22). Association for Computing \nMachinery, pp. 214–229. New York, NY, USA. (2022). https://  \ndoi. org/ 10. 1145/ 35311 46. 35330 88\n 31. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: \nOn the dangers of stochastic parrots: Can language models be too \nbig? In: FAccT 2021—Proceedings of the 2021 ACM Confer -\nence on Fairness, Accountability, and Transparency, Association \nfor Computing Machinery, Inc, Mar. 2021, pp. 610–623. https:// \ndoi. org/ 10. 1145/ 34421 88. 34459 22\n 32. Shelby, R., et al.: Sociotechnical harms: scoping a taxonomy for \nharm reduction. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. \n2210. 05791\n 33. Perez, E., et al.: Discovering language model behaviors with \nmodel-written evaluations. ArXiv, Dec. 2022. [Online]. https://  \narxiv. org/ abs/ 2212. 09251 v1. Accessed 22 Mar 2023\n 34. Curry, D.: ChatGPT Revenue and Usage Statistics (2023)—Busi-\nness of Apps. BusinessofApps, 2023. [Online]. https:// www. busin \nessof apps. com/ data/ chatg pt- stati stics/. Accessed 2 Apr 2023\n 35. Liang, P., et al.: Holistic evaluation of language models; holistic \nevaluation of language models. ArXiv, 2022. [Online]. https://  \narxiv. org/ pdf/ 2211. 09110. pdf. Accessed 13 Feb 2023\n 36. Radford, A., et al.: Learning transferable visual models from nat-\nural language supervision. In: Proceedings of the 38th Interna-\ntional Conference on Machine Learning, 2021. [Online]. https:// \ngithub. com/ OpenAI/ CLIP. Accessed 20 Jan 2023\n 37. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M, Ope-\nnAI.: Hierarchical text-conditional image generation with CLIP \nlatents. 2022. https:// doi. org/ 10. 48550/ arxiv. 2204. 06125\n 38. OpenAI.: Best practices for deploying language models. Website, \n2022. https:// openai. com/ blog/ best- pract ices- for- deplo ying- langu \nage- models/. Accessed 20 Jan 2023\n 39. Peyrard, M., et al.: Invariant language modeling. ArXiv (2021). \nhttps:// doi. org/ 10. 48550/ arxiv. 2110. 08413\n 40. Ganguli, D., et al.: Predictability and surprise in large generative \nmodels. In: ACM International Conference Proceeding Series, \npp. 1747–1764, Jun. 2022. https:// doi. org/ 10. 1145/ 35311 46. \n35332 29.\n 41. Ganguli, D., et al.: Red teaming language models to reduce \nharms: methods, scaling behaviors, and lessons learned. ArXiv, \n2022. [Online]. https:// github. com/ anthr opics/ hh- rlhf. Accessed \n2 Apr 2023\n 42. Perez, E., et al.: Red teaming language models with language \nmodels. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. 2202.  \n03286\n 43. European Commission: Proposal for regulation of the European \nparliament and of the council—Laying down harmonised rules \non artificial intelligence (artificial intelligence act) and amending \ncertain Union legislative acts (2021). https:// eur- lex. europa. eu/ \nlegal- conte nt/ EN/ TXT/? uri= celex% 3A520 21PC0 206\n 44. Office of U.S. Senator Ron Wyden.: Algorithmic Accountability \nAct of 2022. In: 117th Congress 2D Session, 2022, https:// doi. \norg/ 10. 1016/ S0140- 6736(02) 37657-8\n 45. Joshi, A.K.: Natural language processing. Science (1979) \n253(5025), 1242–1249 (1991). https:// doi. org/ 10. 1126/ SCIEN \nCE. 253. 5025. 1242\n 46. Hirschberg, J., Manning, C.D.: Advances in natural language \nprocessing. Science (1979) 349(6245), 261–266 (2015). https://  \ndoi. org/ 10. 1126/ SCIEN CE. AAA86 85/ ASSET/ D33AB 763- \nA443- 444C- B766- A6B69 883BF D7/ ASSETS/ GRAPH IC/ 349_ \n261_ F5. JPEG\n 47. Chernyavskiy, A., Ilvovsky, D., Nakov, P.: Transformers: ‘The \nEnd of History’ for Natural Language Processing?,” Lecture \nNotes in Computer Science (including subseries Lecture Notes \nin Artificial Intelligence and Lecture Notes in Bioinformatics), \nvol. 12977 LNAI, pp. 677–693, 2021. https:// doi. org/ 10. 1007/ \n978-3- 030- 86523-8_ 41/ TABLES/5\n 48. Adiwardana, D., et al.: Towards a human-like open-domain Chat-\nbot. ArXiv (2020). https:// doi. org/ 10. 48550/ arxiv. 2001. 09977\n 49. Wang, A., et al.: SuperGLUE: a stickier benchmark for general-\npurpose language understanding systems.In: NIPS’19, 2019. \nhttps:// doi. org/ 10. 5555/ 34542 87. 34545 81\n 50. Bai, Y., et al.: Training a helpful and harmless assistant with \nreinforcement learning from human feedback. ArXiv, Apr. 2022. \n[Online]. https:// arxiv. org/ abs/ 2204. 05862 v1. Accessed 2 Apr \n2023\n 51. Stiennon, N., et al. Learning to summarize from human feedback. \nAdv Neural Inf Process Syst. vol. 2020-December, Sep. 2020. \n[Online]. https:// arxiv. org/ abs/ 2009. 01325 v3. Accessed 2 Apr \n2023\n 52. Ouyang, L., et al.: Training language models to follow instruc-\ntions with human feedback. ArXiv, Mar. 2022. [Online]. https:// \narxiv. org/ abs/ 2203. 02155 v1. Accessed 2 Apr 2023\n 53. Arcas, B.A.Y.: Do large language models understand us? Dae-\ndalus 151(2), 183–197 (2022). https:// doi. org/ 10. 1162/ daed_a_ \n01909\n1108 AI and Ethics (2024) 4:1085–1115\n1 3\n 54. Suzgun, M., et al.: Challenging BIG-bench tasks and whether \nchain-of-thought can solve them. ArXiv, Oct. 2022. [Online]. \nhttps:// arxiv. org/ abs/ 2210. 09261 v1. Accessed 2 Apr 2023\n 55. Villalobos, P., Sevilla, J., Besiroglu, T., Heim, L., Ho, A., \nHobbhahn, M.: Machine learning model sizes and the parameter \ngap. ArXiv, Jul. 2022, [Online]. http:// arxiv. org/ abs/ 2207. 02852\n 56. Hoffmann, J. et al.: Training compute-optimal large language \nmodels. ArXiv, Mar. 2022, [Online]. http:// arxiv. org/ abs/ 2203. \n15556\n 57. Bowman, S.R.: Eight things to know about large language mod-\nels. Apr. 2023. [Online]. https:// arxiv. org/ abs/ 2304. 00612 v1 \nAccessed 12 Apr 2023\n 58. Rasmy, L., Xiang, Y., Xie, Z., Tao, C., Zhi, D.: Med-BERT: \npretrained contextualized embeddings on large-scale structured \nelectronic health records for disease prediction. npj Digit. Med. \n4(1), 1–13 (2021). https:// doi. org/ 10. 1038/ s41746- 021- 00455-y\n 59. Wang, Y., Wang, W., Joty, S., Hoi, S.C.H.: CodeT5: Identifier-\naware unified pre-trained encoder-decoder models for code \nunderstanding and generation. In: EMNLP 2021—2021 Con-\nference on Empirical Methods in Natural Language Processing, \nProceedings, pp. 8696–8708, 2021. https:// doi. org/ 10. 18653/ V1/ \n2021. EMNLP- MAIN. 685\n 60. Chen, M., et al.: Evaluating large language models trained on \ncode. ArXiv. [Online]. https:// www. github. com/ openai/ human- \neval (2021). Accessed 20 Jan 2023\n 61. Wang, S., Tu, Z., Tan, Z., Wang, W., Sun, M., Liu, Y.: Language \nmodels are good translators. ArXiv (2021). https:// doi. org/ 10.  \n48550/ arxiv. 2106. 13627\n 62. Kojima, T., Shane Gu, S., Reid, M., Matsuo, Y., Iwasawa, Y., \nGoogle Research: Large language models are zero-shot reason-\ners. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. 2205. 11916\n 63. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, \nB., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D.: Scal -\ning laws for neural language models (2020). https:// doi. org/ 10. \n48550/ arXiv. 2001. 08361\n 64. Brown, A.R., Kluska, A., Nie, A., Gupta, A., Venkatesh, A., \nGholamidavoodi, A., et al.: Beyond the imitation game: Quan-\ntifying and extrapolating the capabilities of language models \n(2022). arXiv: 2206. 04615\n 65. Kirk, H.R., et al.: Bias out-of-the-box: an empirical analysis of \nintersectional occupational biases in popular generative language \nmodels. NeurlIPS, 2021. [Online]. https:// github. com/ oxai/ inter \nsecti onal_ gpt2. Accessed 20 Jan 2023\n 66. Azaria, A.: ChatGPT Usage and Limitations. HAL, Dec. 2022. \n[Online]. https:// hal. scien ce/ hal- 03913 837. Accessed 2 Apr 2023\n 67. Borji, A., Ai, Q.: A categorical archive of ChatGPT failures. Feb. \n2023. [Online]. https:// arxiv. org/ abs/ 2302. 03494 v7. Accessed 22 \nMar 2023\n 68. Shevlane, T.: Structured access. In: Bullock, J., Chen, Y.-C., \nHimmelreich, J., Hudson, V.M., Korinek, A., Young, M., Zhang, \nB. (eds.) The Oxford Handbook of AI Governance. Oxford Uni-\nversity Press (2022). https:// doi. org/ 10. 1093/ oxfor dhb/ 97801 \n97579 329. 013. 39\n 69. Tamkin, A., Brundage, M., Clark, J., Ganguli, D.: Understanding \nthe capabilities, limitations, and societal impact of large language \nmodels. ArXiv, Feb. 2021, [Online]. http:// arxiv. org/ abs/ 2102. \n02503\n 70. Avin, S., et al.: Filling gaps in trustworthy development of AI. \nScience 374(6573), 1327–1329 (2021). https:// doi. org/ 10. 1126/ \nSCIEN CE. ABI71 76\n 71. PAI.: Researching Diversity, Equity, and Inclusion in the Field \nof AI-Partnership on AI. Website, 2020. https:// partn ershi ponai. \norg/ resea rching- diver sity- equity- and- inclu sion- in- the- field- of- \nai/. Accessed 20 Jan 2023\n 72. Wang, Z.J., Choi, D., Xu, S., Yang, D.: Putting humans in the \nnatural language processing loop: a survey. In: Proceedings of \nthe First Workshop on Bridging Human–Computer Interaction \nand Natural Language Processing, pp. 47–52 (2021)\n 73. Marda, V., Narayan, S.: On the importance of ethnographic meth-\nods in AI research. Nat. Mach. Intell. 3 (3. Nature Research), \n187–189 (2021). https:// doi. org/ 10. 1038/ s42256- 021- 00323-0\n 74. Mitchell, M., et  al.: Model cards for model reporting. In: \nFAT* 2019—Proceedings of the 2019 Conference on Fairness, \nAccountability, and Transparency, pp. 220–229, Jan. 2019. \nhttps:// doi. org/ 10. 1145/ 32875 60. 32875 96\n 75. Derczynski, L., et al.: Assessing language model deployment \nwith risk cards. [Online]. https:// arxiv. org/ abs/ 2303. 18190 v1 \n(2023). Accessed 2 Apr 2023\n 76. Gebru, T., et al.: Datasheets for datasets. Commun. ACM 64(12), \n86–92 (2021). https:// doi. org/ 10. 1145/ 34587 23\n 77. MetaAI.: System Cards, a new resource for understanding how \nAI systems work. Website. https:// ai. faceb ook. com/ blog/ system- \ncards-a- new- resou rce- for- under stand ing- how- ai- syste ms- work/ \n(2023). Accessed 20 Jan 2023\n 78. Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., Gold-\nstein, T.: A watermark for large language models (2023). arXiv: \n2301. 10226\n 79. Hacker, P., Engel, A., Mauer, M.: Regulating ChatGPT and other \nlarge generative AI models (2023). arXiv: 2302. 02337\n 80. Engler, A.: Early thoughts on regulating generative AI like Chat-\nGPT. In: Brookings TechTank. https:// www. brook ings. edu/ blog/ \ntecht ank/ 2023/ 02/ 21/ early- thoug hts- on- regul ating- gener ative- ai- \nlike- chatg pt/ (2023). Accessed 2 Apr 2023\n 81. Altman, S.: Planning for AGI and beyond. OpenAI Blog. \n[Online]. https:// openai. com/ blog/ plann ing- for- agi- and- beyond# \nfn1 (2023). Accessed 24 Mar 2023\n 82. Helberger, N., Diakopoulos, N.: ChatGPT and the AI Act. Inter-\nnet Policy Rev. (2023). https:// doi. org/ 10. 14763/ 2023.1. 1682\n 83. Mökander, J., Axente, M., Casolari, F., Floridi, L.: Conform-\nity assessments and post-market monitoring: a guide to the role \nof auditing in the Proposed European AI regulation. Minds \nMach. (Dordr) 32(2), 241–268 (2022). https:// doi. org/ 10. 1007/ \ns11023- 021- 09577-4\n 84. Lee, T.-H., Azham, M.A.: The evolution of auditing: An analysis \nof the historical development. [Online]. https:// www. resea rchga \nte. net/ publi cation/ 33925 1518 (2008). Accessed 10 Feb 2023\n 85. Senft, S., Gallegos, F.: Information Technology Control and \nAudit, 3rd edn. CRC Press/Auerbach Publications, Boca Raton \n(2009)\n 86. Dai, W., Berleant, D.: Benchmarking contemporary deep learn-\ning hardware and frameworks: A survey of qualitative metrics. \nIn: Proceedings—2019 IEEE 1st International Conference on \nCognitive Machine Intelligence, CogMI 2019, pp. 148–155, Dec. \n2019. https:// doi. org/ 10. 1109/ COGMI 48466. 2019. 00029.\n 87. Voas, J., Miller, K.: Software certification services: encouraging \ntrust and reasonable expectations. In: IEEE Computer Society, \npp. 39–44. [Online]. https:// ieeex plore. ieee. org/ stamp/ stamp. jsp? \narnum ber= 17173 42 (2016). Accessed 2 Apr 2023\n 88. Dean, S., Gilbert, T.K., Lambert, N., Zick, T.: Axes for Socio-\ntechnical Inquiry in AI Research. IEEE Trans. Technol. Soc. \n2(2), 62–70 (2021). https:// doi. org/ 10. 1109/ tts. 2021. 30740 97\n 89. European Commission.: AI liability directive. In: Proposal for \na Directive of the European Parliament and of the Council on \nadapting non-contractual civil liability rules to artificial intelli-\ngence, pp. 1–29. [Online]. https:// ec. europa. eu/ commi ssion/ sites/ \nbeta- polit ical/ files/ polit ical- guide lines- next- commi ssion_ en. pdf \n(2022). Accessed 21 Jan 2023\n 90. Berlin, I.: The pursuit of the ideal. In: The Crooked Timber \nof Mankind: Chapters in the History of Ideas, 1988, pp. 1–20. \n[Online]. https:// www- jstor- org. ezpro xy- prd. bodle ian. ox. ac. uk/ \nstable/ j. ctt2t t8nd.6# metad ata_ info_ tab_ conte nts. Accessed 20 \nJan 2023\n1109AI and Ethics (2024) 4:1085–1115 \n1 3\n 91. Gabriel, I.: Artificial intelligence, values, and alignment. Minds \nMach. (Dordr) 30(3), 411–437 (2020). https:// doi. org/ 10. 1007/ \ns11023- 020- 09539-2\n 92. Goodman, B.: Hard choices and hard limits in artificial intel-\nligence. In: AIES 2021-Proceedings of the 2021 AAAI/ACM \nConference on AI, Ethics, and Society, pp. 112–120, Jul. 2021, \nhttps:// doi. org/ 10. 1145/ 34617 02. 34625 39\n 93. Kirk, H.R., Vidgen, B., Röttger, P., Hale, S.A.: Personalisation \nwithin bounds: A risk taxonomy and policy framework for the \nalignment of large language models with personalised feedback. \nArXiv (2023)\n 94. Mittelstadt, B.: Principles alone cannot guarantee ethical AI. Nat. \nMach. Intell. 1 (11), 501–507 (2019). https:// doi. org/ 10. 1038/ \ns42256- 019- 0114-4\n 95. Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, \nA.: a survey on bias and fairness in machine learning. ACM \nComput. Surv. (CSUR) (2021). https:// doi. org/ 10. 1145/ 34576 07\n 96. Kleinberg, J.: Inherent trade-offs in algorithmic fairness. ACM \nSIGMETRICS Perform. Eval. Rev. 46(1), 40–40 (2018). https:// \ndoi. org/ 10. 1145/ 32920 40. 32196 34\n 97. Kusner, M., Loftus, J., Russell, C., Silva, R.: Counterfactual \nfairness. In: 31st Conference on Neural Information Processing \nSystems. [Online]. https:// obama white house. archi ves. gov/ blog/ \n2016/ 05/ 04/ big- risks- big- oppor tunit ies- inter secti on- big- data \n(2017). Accessed 20 Jan 2023\n 98. Whittlestone, J., Alexandrova, A., Nyrup, R., Cave, S.: The role \nand limits of principles in AI ethics: Towards a focus on tensions. \nIn: AIES 2019—Proceedings of the 2019 AAAI/ACM Confer -\nence on AI, Ethics, and Society, pp. 195–200 (2019). doi: https:// \ndoi. org/ 10. 1145/ 33066 18. 33142 89\n 99. Gururangan, S, et al.: Don’t stop pretraining: adapt language \nmodels to domains and tasks. In: Proceedings of the 58th Annual \nMeeting of the Association for Computational Linguistics, pp. \n8342–8360, Jul. 2020, https:// doi. org/ 10. 18653/ V1/ 2020. ACL- \nMAIN. 740\n 100. O’Neill, O.: A Philosopher Looks at Digital Communication. \nCambridge University Press, Cambridge (2021)\n 101. Kasirzadeh, A., Gabriel, I.: In conversation with Artificial Intel-\nligence: aligning language models with human values. Minds \nMach. (Dordr) (2023). https:// doi. org/ 10. 48550/ arxiv. 2209.  \n00731\n 102. Steed, R., Panda, S., Kobren, A., Wick, M.: Upstream mitiga-\ntion is not all you need: testing the bias transfer hypothesis in \npre-trained language models. In: Proceedings of the Annual \nMeeting of the Association for Computational Linguistics, vol. \n1, pp. 3524–3542 (2022). https:// doi. org/ 10. 18653/ V1/ 2022. \nACL- LONG. 247\n 103. Gupta, K.: Comtemporary Auditing,” p. 1095. [Online]. https://  \nbooks. google. com/ books/ about/ Conte mpora ry_ Audit ing. html? \nid= neDFW DyUWu QC (2004). Accessed 18 Feb 2023\n 104. Flint, D.: Philosophy and Principles of Auditing: An Introduc-\ntion. Macmillan Education, Basingstoke (1988)\n 105. LaBrie, R.C., Steinke, G.H.: Towards a framework for ethical \naudits of AI algorithms. In: 25th Americas Conference on Infor-\nmation Systems, AMCIS 2019, pp. 1–5 (2019)\n 106. Stodt, J., Reich, C.: Machine learning development audit frame-\nwork: assessment and inspection of risk and quality of data, \nmodel and development process. Int. J. Comput. Inform. Eng. \n15(3), 187–193, (2021)\n 107. Adler, P., et al.: Auditing black-box models for indirect influence. \nKnowl. Inf. Syst. 54(1), 95–122 (2018). https:// doi. org/ 10. 1007/ \ns10115- 017- 1116-3\n 108. Kearns, M., Neel, S., Roth, A., Wu, Z.S.: Preventing fairness \ngerrymandering: Auditing and learning for subgroup fairness. \nIn: 35th International Conference on Machine Learning, ICML \n2018, vol. 6, pp. 4008–4016 (2018)\n 109. Laux, J., Wachter, S., Mittelstadt, B.: Taming the few: platform \nregulation, independent audits, and the risks of capture created \nby the DMA and DSA. Comput. Law Secur. Rev. 43, 105613 \n(2021). https:// doi. org/ 10. 1016/j. clsr. 2021. 105613\n 110. Selbst, A.D.: An institutional view of algorithmic impact \nassessments. Harv. J. Law Technol., vol. 35, 2021, [Online]. \nhttps:// papers. ssrn. com/ sol3/ papers. cfm? abstr act_ id= 38676 34. \nAccessed 10 Feb 2023\n 111. Bandy, J.: Problematic machine behavior: a systematic literature \nreview of algorithm audits. Proc. ACM Hum. Comput. Interact. \n5(1), 1–34 (2021). https:// doi. org/ 10. 1145/ 34491 48\n 112. Floridi, L., Holweg, M., Taddeo, M., Amaya Silva, J., Mökander, \nJ., Wen, Y.: capAI—a procedure for conducting conformity \nassessment of AI systems in line with the EU Artificial Intel-\nligence Act. SSRN (2022). https:// doi. org/ 10. 2139/ ssrn. 40640 91\n 113. Minkkinen, M., Laine, J., Mäntymäki, M.: Continuous auditing \nof artificial intelligence: a conceptualization and assessment of \ntools and frameworks. Digit. Soc. 1(3), 21 (2022). https:// doi. org/ \n10. 1007/ s44206- 022- 00022-2\n 114. Metaxa, D., et al.: Auditing algorithms. Found. Trends Human-\nComput. Interact. 14(4), 272–344 (2021). https:// doi. org/ 10. \n1561/ 11000 00083\n 115. Berghout, E., Fijneman, R., Hendriks, L., de Boer, M., \nButijn, B.J.: Advanced digital auditing. In: Progress in IS. \nCham: Springer Nature, (2023). https:// doi. org/ 10. 1007/  \n978-3- 031- 11089-4\n 116. Mökander, J., Axente, M.: Ethics-based auditing of auto-\nmated decision-making systems : intervention points and \npolicy implications. AI Soc (2021). https:// doi. org/ 10. 1007/  \ns00146- 021- 01286-x\n 117. Brown, S., Davidovic, J., Hasan, A.: The algorithm audit: \nscoring the algorithms that score us. Big Data Soc 8 (1), \n205395172098386 (2021). https:// doi. org/ 10. 1177/ 20539 51720 \n983865\n 118. Gibson Dunn.: New York City Proposes Rules to Clarify Upcom-\ning Artificial Intelligence Law for Employers. https:// www. gibso \nndunn. com/ new- york- city- propo ses- rules- to- clari fy- upcom ing- \nartifi  cial- intel ligen ce- law- for- emplo yers/ (2023). Accessed 2 Apr \n2023\n 119. PwC: PwC ethical AI framework (2020). https:// www. pwc. com/ \ngx/ en/ issues/ data- and- analy tics/ artifi  cial- intel ligen ce/ what- is- \nrespo nsible- ai. html. Accessed 10 Feb 2023\n 120. Deloitte.: Deloitte Introduces Trustworthy AI Framework to \nGuide Organizations in Ethical Application of Technology. Press \nrelease. https:// www2. deloi tte. com/ us/ en/ pages/ about- deloi tte/ \nartic les/ press- relea ses/ deloi tte- intro duces- trust worthy- ai- frame \nwork. html (2020). Accessed 18 Sep 2020)\n 121. KPMG.: KPMG offers ethical AI Assurance using CIO Strategy \nCouncil standards. Press release. https:// home. kpmg/ ca/ en/ home/ \nmedia/ press- relea ses/ 2020/ 11/ kpmg- offers- ethic al- ai- assur ance- \nusing- ciosc- stand ards. html (2020). Accessed 10 Nov 2021\n 122. EY.: Assurance in the age of AI,” 2018, [Online]. https:// assets. \ney. com/ conte nt/ dam/ ey- sites/ ey- com/ en_ gl/ topics/ digit al/ ey- \nassur ance- in- the- age- of- ai. pdf. Accessed 12 Feb 2023\n 123. NIST: AI Risk Management Framework: Second Draft Notes \nfor Reviewers: Call for comments and contributions. National \nInstitute of Standards and Technology (2022)\n 124. ISO.: ISO/IEC 23894-Information technology—Artificial intel-\nligence—Guidance on risk management. International Organi-\nzation for Standardization. https:// www. iso. org/ stand ard/ 77304. \nhtml, (2023). Accessed 20 Jan 2023\n 125. NIST.: Risk management guide for information technology sys-\ntems recommendations of the National Institute of Standards \nand Technology. National Institute of Standards and Technol-\nogy. [Online]. https:// www. hhs. gov/ sites/ defau lt/ files/ ocr/ priva \n1110 AI and Ethics (2024) 4:1085–1115\n1 3\ncy/ hipaa/ admin istra tive/ secur ityru le/ nist8 00- 30. pdf (2002). \nAccessed 20 Jan 2023\n 126. VDE.: VDE SPEC 900012 V1.0 (en). Verband Der Elektrotech-\nnik. [Online]. www. vde. com (2022). Accessed 20 Jan 2023\n 127. ICO.: Guidance on the AI auditing framework: Draft guidance \nfor consultation. Information Commissioner’s Office, [Online]. \nhttps:// ico. org. uk/ media/ about- the- ico/ consu ltati ons/ 26172 19/ \nguida nce- on- the- ai- audit ing- frame work- draft- for- consu ltati on. \npdf (2020). Accessed 12 Feb 2023\n 128. Institute of Internal Auditors: The IIA’s Artificial Intelligence \nAuditing Framework. The Institute of Internal Auditors-Global \nPerspectives (2018)\n 129. ISO.: ISO/IEC 38507:2022-Information technology—Govern-\nance of IT—Governance implications of the use of artificial \nintelligence by organizations. International Organization for \nStandardization. https:// www. iso. org/ stand ard/ 56641. html? \nbrowse= tc (2022). Accessed 20 Jan 2023\n 130. Institute of Internal Auditors. About Internal Audit. The Institute \nof Internal Auditors. https:// www. theiia. org/ en/ about- us/ about- \ninter nal- audit/ (2022). Accessed 20 Jan 2023\n 131. Yanisky-Ravid, S., Hallisey, S.K.: Equality and privacy by \ndesign: a new model of artificial data transparency via auditing, \ncertification, and safe harbor regimes. Fordham Urban Law J. \n46(2), 428-486, (2019).\n 132. Saleiro, P., et al.: Aequitas: a bias and fairness audit toolkit. \nArXiv, no. 2018, 2018, [Online]. http:// arxiv. org/ abs/ 1811. 05577\n 133. Costanza-Chock, S., Raji, I.D., Buolamwini, j.: Who Audits the \nAuditors? Recommendations from a field scan of the algorithmic \nauditing ecosystem; Who Audits the Auditors? Recommenda-\ntions from a field scan of the algorithmic auditing ecosystem. \nIn:FAccT’22, vol. 22 (2022). https:// doi. org/ 10. 1145/ 35311 46. \n35332 13\n 134. Slee, T.: The incompatible incentives of private-sector AI. In: \nThe Oxford Handbook of Ethics of AI, Oxford University Press, \npp. 106–123 (2020). https:// doi. org/ 10. 1093/ OXFOR DHB/ 97801 \n90067 397. 013.6\n 135. Naudé, W., Dimitri, N.: The race for an artificial general intel-\nligence: implications for public policy. AI Soc. 35(2), 367–379 \n(2020). https:// doi. org/ 10. 1007/ S00146- 019- 00887-X/ METRI \nCS\n 136. Engler, A.C.: Outside auditors are struggling to hold AI \ncompanies accountable. FastCompany. https:// www. fastc  \nompany. com/ 90597 594/ ai- algor ithm- audit ing- hirev ue (2021). \nAccessed 20 Jan 2023\n 137. Lauer, D.: You cannot have AI ethics without ethics. AI and \nEthics 0123456789, 1–5 (2020). https:// doi. org/ 10. 1007/  \ns43681- 020- 00013-4\n 138. Danks, D., London, A.J.: Regulating autonomous systems: \nbeyond standards. IEEE Intell Syst 32(1), 88–91 (2017). \nhttps:// doi. org/ 10. 1109/ MIS. 2017.1\n 139. Mahajan, V., Venugopal, V.K., Murugavel, M., Mahajan, \nH.: The algorithmic audit: working with vendors to validate \nradiology-AI algorithms—how we do it. Acad. Radiol. 27(1), \n132–135 (2020). https:// doi. org/ 10. 1016/j. acra. 2019. 09. 009\n 140. Zerbino, P., Aloini, D., Dulmin, R., Mininno, V.: Process-min-\ning-enabled audit of information systems: methodology and an \napplication. Expert Syst. Appl. 110, 80–92 (2018). https:// doi.  \norg/ 10. 1016/j. eswa. 2018. 05. 030\n 141. Mittelstadt, B.: Auditing for transparency in content personali-\nzation systems. Int. J. Commun. 10(June), 4991–5002 (2016)\n 142. Kroll, J.A.: The fallacy of inscrutability. Philos. Trans. R. Soc. \nA Math. Phys. Eng. Sci. (2018). https:// doi. org/ 10. 1098/ rsta.  \n2018. 0084\n 143. OECD.: OECD Framework for the Classification of AI sys-\ntems. Paris. [Online]. https:// doi. org/ 10. 1787/ cb6d9 eca- en  \n(2022) Accessed 11 Apr 2022\n 144. Xu, X., Chen, X., Liu, C., Rohrbach, A., Darrell, T., Song, D.: \nFooling vision and language models despite localization and \nattention mechanism. In: 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 4951–4961, \nJun. 2018. https:// doi. org/ 10. 1109/ CVPR. 2018. 00520\n 145. Röttger, P., Vidgen, B., Nguyen, D., Waseem, Z., Margetts, \nH., Pierrehumbert, J.B.: HateCheck: functional tests for hate \nspeech detection models. In: Proceedings of the 59th Annual \nMeeting of the Association for Computational Linguistics and \nthe 11th International Joint Conference on Natural Language \nProcessing (Volume 1: Long Papers), pp. 41–58 (2021). https://  \ndoi. org/ 10. 18653/ V1/ 2021. ACL- LONG.4\n 146. Aspillaga, C., Carvallo, A., Araujo, V.: Stress Test evaluation \nof transformer-based models in natural language understand-\ning tasks. In: Proceedings of the 12th Conference on Language \nResources and Evaluation, pp. 11–16. [Online]. https:// github.  \ncom/ (2020). Accessed 20 Jan 2023\n 147. Dignum, V.: Responsible autonomy. In: Proceedings of the \nInternational Joint Conference on Autonomous Agents and \nMultiagent Systems, AAMAS, vol. 1, p. 5, (2017). .24963/\nijcai.2017/655\n 148. Wei, J. et al.: Emergent abilities of large language models. \nArXiv (2022)\n 149. Sheng, E., Chang, K.W., Natarajan, P., Peng, N.: The woman \nworked as a babysitter: on biases in language generation. In: \n2019 Conference on Empirical Methods in Natural Language \nProcessing, pp. 3407–3412 (2019). https:// doi. org/ 10. 18653/  \nV1/ D19- 1339.\n 150. Gehman, S., Gururangan, S., Sap, M., Choi, Y., Smith, N.A.: \nRealToxicityPrompts: evaluating neural toxic degeneration in \nlanguage models. In: Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pp. 3356–3369, Sep. 2020, \n[Online]. http:// arxiv. org/ abs/ 2009. 11462\n 151. Song, C., Shmatikov, V.: Auditing data provenance in text-gener-\nation models. In: KDD’19, 2019. https:// doi. org/ 10. 1145/ 32925 \n00. 33308 85\n 152. EPRS: Auditing the quality of datasets used in algorithmic deci-\nsion-making systems. European Parliamentary Research Service \n(2022). https:// doi. org/ 10. 2861/ 98930 \n 153. Floridi, L., Strait, A.: Ethical foresight analysis: what it is and \nwhy it is needed? Minds Mach. (Dordr) 30(1), 77–97 (2020). \nhttps:// doi. org/ 10. 1007/ s11023- 020- 09521-y\n 154. Hodges, C.: Ethics in business practice and regulation. In: Law \nand Corporate Behaviour : Integrating Theories of Regulation, \nEnforcement, Compliance and Ethics, pp. 1–21 (2015). https://  \ndoi. org/ 10. 5040/ 97814 74201 124\n 155. ISO.: ISO/IEC 38500:2015—Information technology—Gov -\nernance of IT for the organization. International Organization \nfor Standardization. https:// www. iso. org/ stand ard/ 62816. html \n(2015). Accessed 20 Jan 2023).\n 156. Iliescu, F.-M.: Auditing IT Governance. In: Informatica Eco-\nnomica, 14(1), 93–102. [Online]. https:// www. proqu est. com/ \ndocvi ew/ 14332 36144/ fullt extPDF/ A2EAF E83CB FA461 APQ/1? \naccou ntid= 13042 & force dol= true (2010). Accessed 20 Jan 2023\n 157. Falco, G., et al.: Governing AI safety through independent audits. \nNat. Mach. Intell. 3(7), 566–571 (2021). https:// doi. org/ 10. 1038/ \ns42256- 021- 00370-7\n 158. Leveson, N.: Engineering a safer world: systems thinking applied \nto safety. In: Engineering systems. Cambridge: MIT Press (2011)\n 159. Dobbe, R.I.J.: System safety and artificial intelligence. In: The \nOxford Handbook of AI Governance, p. C67.S1-C67.S18, Oct. \n2022. https:// doi. org/ 10. 1093/ OXFOR DHB/ 97801 97579 329. \n013. 67\n 160. Schuett, J.: Three lines of defense against risks from AI. (2022). \nhttps:// doi. org/ 10. 48550/ arxiv. 2212. 08364\n1111AI and Ethics (2024) 4:1085–1115 \n1 3\n 161. Bauer, J.: The necessity of auditing artificial intelligence. SSRN \n577, 1–16 (2016)\n 162. Chopra, A.K., Singh, M.P.: Sociotechnical systems and ethics in \nthe large. In: AIES 2018—Proceedings of the 2018 AAAI/ACM \nConference on AI, Ethics, and Society, pp 48–53 (2018). https:// \ndoi. org/ 10. 1145/ 32787 21. 32787 40\n 163. Contractor, D., et al.: Behavioral use licensing for responsible \nAI. In” 2022 ACM Conference on Fairness, Accountability, \nand Transparency, New York, NY, USA: ACM, Jun. 2022, pp. \n778–788. https:// doi. org/ 10. 1145/ 35311 46. 35331 43.\n 164. Schuett, J.: Risk management in the artificial intelligence act. \nEur. J. Risk Regul. (2023). https:// doi. org/ 10. 1017/ ERR. 2023.1\n 165. Carlini, N., et al.: Extracting training data from large language \nmodels. In: Proceedings of the 30th USENIX Security Sympo-\nsium, 2021. [Online]. https:// www. usenix. org/ confe rence/ useni \nxsecu rity21/ prese ntati on/ carli ni- extra cting. Accessed 20 Jan \n2023\n 166. Dwork, C.: Differential privacy. In: Lecture Notes in Computer \nScience (including subseries Lecture Notes in Artificial Intel-\nligence and Lecture Notes in Bioinformatics), vol. 4052 LNCS, \npp. 1–12 (2006). https:// doi. org/ 10. 1007/ 11787 006_1/ COVER\n 167. Kaissis, G.A., Makowski, M.R., Rückert, D., Braren, R.F.: \nSecure, privacy-preserving and federated machine learning \nin medical imaging. Nat. Mach. Intell. 2 (6), 305–311 (2020). \nhttps:// doi. org/ 10. 1038/ s42256- 020- 0186-1\n 168. Bharadwaj, K.B.P., Kanagachidambaresan, G.R.: Pattern rec-\nognition and machine learning. (2021). https:// doi. org/ 10. 1007/ \n978-3- 030- 57077-4_ 11\n 169. Crisan, A., Drouhard, M., Vig, J., Rajani, N.: Interactive model \ncards: a human-centered approach to model documentation. In: \nACM International Conference Proceeding Series, vol. 22, pp. \n427–439, Jun. 2022. https:// doi. org/ 10. 1145/ 35311 46. 35331 08\n 170. Pushkarna, M., Zaldivar, A., Kjartansson, O.: Data cards: pur -\nposeful and transparent dataset documentation for responsible \nAI. In: ACM International Conference Proceeding Series, pp. \n1776–1826, Jun. 2022. https:// doi. org/ 10. 1145/ 35311 46. 35332 \n31\n 171. Jernite, Y. et al.: Data governance in the age of large-scale data-\ndriven language technology. In 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, New York, NY, USA: \nACM, Jun. 2022, pp. 2206–2222. https:// doi. org/ 10. 1145/ 35311 \n46. 35346 37\n 172. Paullada, A., Raji, I.D., Bender, E.M., Denton, E., Hanna, A.: \nData and its (dis)contents: a survey of dataset development and \nuse in machine learning research. Patterns 2(11), 100336 (2021). \nhttps:// doi. org/ 10. 1016/J. PATTER. 2021. 100336\n 173. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, \nS.R.: GLUE: a multi-task benchmark and analysis platform \nfor natural language understanding. In: EMNLP 2018 - 2018 \nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting \nNeural Networks for NLP, Proceedings of the 1st Workshop, pp. \n353–355 (2018). https:// doi. org/ 10. 18653/ V1/ W18- 5446.\n 174. Rudner, T.J., Toner, H.: Key concepts in AI Safety: robustness \nand adversarial examples. In: CSET Issue Brief (2021)\n 175. Sohoni, N.S., Dunnmon, J.A., Angus, G., Gu, A., Ré, C.: No Sub-\nclass Left Behind: Fine-Grained Robustness in Coarse-Grained \nClassification Problems. In: 34th Conference on Neural Informa-\ntion Processing Systems, Nov. 2020, [Online]. http:// arxiv. org/ \nabs/ 2011. 12945\n 176. Garg, S., Ramakrishnan, G.: BAE: BERT-based adversarial \nexamples for text classification. In: EMNLP 2020 - 2020 Con-\nference on Empirical Methods in Natural Language Processing, \nProceedings of the Conference, pp. 6174–6181, (2020). https://  \ndoi. org/ 10. 18653/ V1/ 2020. EMNLP- MAIN. 498\n 177. Li, L., Ma, R., Guo, Q., Xue, X., Qiu, X.: BERT-ATTACK: \nadversarial attack against BERT using BERT. In: EMNLP \n2020—2020 Conference on Empirical Methods in Natural Lan-\nguage Processing, Proceedings of the Conference, pp. 6193–\n6202, 2020. https:// doi. org/ 10. 18653/ V1/ 2020. EMNLP- MAIN. \n500.\n 178. Goel, K., Rajani, N.. Vig, J.. Taschdjian, Z., Bansal, M., Ré, \nC.: Robustness gym: unifying the NLP evaluation landscape. \nIn: NAACL-HLT 2021—2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Demonstrations, pp. 42–55 \n(2021). https:// doi. org/ 10. 18653/ V1/ 2021. NAACL- DEMOS.6.\n 179. Nie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., Kiela, \nD.: Adversarial NLI: A new benchmark for natural language \nunderstanding. In: Proceedings of the 58th Annual Meeting of \nthe Association for Computational Linguistics, pp. 4885–4901, \nJul. 2020. https:// doi. org/ 10. 18653/ V1/ 2020. ACL- MAIN. 441\n 180. Kiela, D., et al.: Dynabench: rethinking benchmarking in NLP. \nIn: NAACL-HLT 2021—2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Proceedings of the Confer -\nence, pp. 4110–4124, (2021). https:// doi. org/ 10. 18653/ V1/ 2021. \nNAACL- MAIN. 324\n 181. Wang, B., et al.: Adversarial GLUE: a multi-task benchmark for \nrobustness evaluation of language models. In: NeurIPS 2021, \nNov. 2021. https:// doi. org/ 10. 48550/ arxiv. 2111. 02840.\n 182. Zhang, M., Ré, C.: Contrastive adapters for foundation model \ngroup robustness. In: ICML 2022 Workshop on Spurious Cor -\nrelations, Jul. 2022. https:// doi. org/ 10. 48550/ arxiv. 2207. 07180\n 183. McMahan, H.B., Ramage, D., Talwar, K., Zhang, L.: Learning \ndifferentially private recurrent language models. In: ICLR 2018 \nConference Blind Submission. Feb. 24, 2018\n 184. Jayaraman, B., Evans, D.: Evaluating differentially private \nmachine learning in practice. In: Proceedings of the 28th USE-\nNIX Security Symposium, Feb. 2019, [Online]. http:// arxiv.  \norg/ abs/ 1902. 08874\n 185. Carlini, N., Brain, G., Liu, C., Erlingsson, Ú., Kos, J., Song, D.: \nThe secret sharer: evaluating and testing unintended memoriza-\ntion in neural networks. In: Proceedings of the 28th USENIX \nSecurity Symposium, 2019, [Online]. https:// www. usenix. org/ \nconfe rence/ useni xsecu rity19/ prese ntati on/ carli ni. Accessed 10 \nFeb 2023\n 186. Evans, O., Cotton-Barratt, O., Finnveden, L., Bales, A., Balwit, \nA., Wills, P., et al.: Truthful AI: developing and governing AI \nthat does not lie (2021). arXiv: 2110. 06674\n 187. Lin, S., Openai, J.H., Evans, O.: TruthfulQA: measuring how \nmodels mimic human falsehoods. In: Proceedings of the 60th \nAnnual Meeting of the Association for Computational Lin -\nguistics, vol. 1, pp. 3214–3252, Jun. 2022. https:// doi. org/ 10.  \n18653/ V1/ 2022. ACL- LONG. 229\n 188. Nejadgholi., I., Kiritchenko, S.: On cross-dataset generalization \nin automatic detection of online abuse. In: Proceedings of the \nFourth Workshop on Online Abuse and Harms, pp. 173–183 \n(2020). https:// doi. org/ 10. 18653/ v1/ P17\n 189. Caliskan, A., Bryson, J.J., Narayanan, A.: Semantics derived \nautomatically from language corpora contain human-like \nbiases. Science (1979) 356(6334), 183–186 (2017). https://  \ndoi. org/ 10. 1126/ SCIEN CE. AAL42 30/ SUPPL_ FILE/ CALIS \nKAN- SM. PDF\n 190. Jo, E.S., Gebru, T.: Lessons from archives: Strategies for col-\nlecting sociocultural data in machine learning. In: FAT* 2020 - \nProceedings of the 2020 Conference on Fairness, Accountability, \nand Transparency, pp. 306–316, Jan. 2020. doi: https://  doi. org/ \n10. 1145/ 33510 95. 33728 29\n 191. Dodge, J., et al.: Documenting large Webtext corpora: a case \nstudy on the colossal clean crawled corpus. In: EMNLP 2021 \n- 2021 Conference on Empirical Methods in Natural Language \n1112 AI and Ethics (2024) 4:1085–1115\n1 3\nProcessing, Proceedings, pp. 1286–1305 (2021). https:// doi. org/ \n10. 18653/ V1/ 2021. EMNLP- MAIN. 98\n 192. Webster, K., et al.: Measuring and reducing gendered correlations \nin pre-trained models. ArXiv (2020). https:// doi. org/ 10. 48550/ \narxiv. 2010. 06032\n 193. May, C., Wang, A., Bordia, S., Bowman, S.R., Rudinger, R.: \nOn measuring social biases in sentence encoders. In: NAACL \nHLT 2019—2019 Conference of the North American Chapter \nof the Association for Computational Linguistics: Human Lan-\nguage Technologies - Proceedings of the Conference, vol. 1, pp. \n622–628 (2019). https:// doi. org/ 10. 18653/ V1/ N19- 1063\n 194. Nadeem, M., Bethke, A., Reddy, S.: StereoSet: Measuring stereo-\ntypical bias in pretrained language models. In: Proceedings of the \n59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural \nLanguage Processing, pp. 5356–5371. [Online]. https://stereoset \n(2021). Accessed 20 Jan 2023\n 195. Bender, E.M., Friedman, B.: Data statements for natural language \nprocessing: toward mitigating system bias and enabling better \nscience. Trans. Assoc. Comput. Linguist. 6 , 587–604 (2018). \nhttps:// doi. org/ 10. 1162/ TACL_A_ 00041\n 196. Schat, E., van de Schoot, R., Kouw, W.M., Veen, D., Mendrik, \nA.M.: The data representativeness criterion: Predicting the per -\nformance of supervised classification based on data set similar -\nity. PLoS ONE 15(8), e0237009 (2020). https:// doi. org/ 10. 1371/ \nJOURN AL. PONE. 02370 09\n 197. Kreutzer, J., et al.: Quality at a glance: an audit of web-crawled \nmultilingual datasets. Trans. Assoc. Comput. Linguist. 10, 50–72 \n(2022). https:// doi. org/ 10. 1162/ TACL_A_ 00447/ 109285\n 198. Simig, D., et al.: Text Characterization Toolkit (TCT). In: Pro-\nceedings of the 2nd Conference of the Asia-Pacific Chapter of the \nAssociation for Computational Linguistics and the 12th Interna-\ntional Joint Conference on Natural Language Processing: System \nDemonstrations. pp. 72–87. [Online]. https:// aclan tholo gy. org/ \n2022. aacl- demo.9 (2022). Accessed 21 Jan 2023\n 199. Hancox-Li, L., Kumar, I.E.: Epistemic values in feature impor -\ntance methods: Lessons from feminist epistemology. In: FAccT \n2021—Proceedings of the 2021 ACM Conference on Fairness, \nAccountability, and Transparency, pp. 817–826, Mar. 2021. \nhttps:// doi. org/ 10. 1145/ 34421 88. 34459 43.\n 200. Dash, A., Mukherjee, A., Ghosh, S.: A network-centric frame-\nwork for auditing recommendation systems. In: Proceedings—\nIEEE INFOCOM, vol. April, pp. 1990–1998 (2019). https:// doi. \norg/ 10. 1109/ INFOC OM. 2019. 87374 86.\n 201. Idowu, S.O.: Legal compliance. In: Idowu, S.O., Capaldi, N., Zu, \nL., Gupta, A.D. (eds.) Encyclopedia of Corporate Social Respon-\nsibility, pp. 1578–1578. Springer. Berlin (2013). https:// doi. org/ \n10. 1007/ 978-3- 642- 28036-8_ 100980\n 202. Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI eth-\nics guidelines. Nat. Mach. Intell. 1 (9), 389–399 (2019). https://  \ndoi. org/ 10. 1038/ s42256- 019- 0088-2\n 203. Green, R.M., Donovan, A.: The methods of business ethics. The \nOxford Handbook of Business Ethics (2009). https:// doi. org/ 10. \n1093/ OXFOR DHB/ 97801 95307 955. 003. 0002\n 204. Raji, I.D., Kumar, I.E., Horowitz, A., Selbst, A.: The fallacy of \nAI functionality. In: ACM International Conference Proceeding \nSeries, pp. 959–972, Jun. 2022. https:// doi. org/ 10. 1145/ 35311 \n46. 35331 58\n 205. Rahwan, I.: Society-in-the-loop: programming the algorithmic \nsocial contract. Ethics Inf. Technol. 20(1), 5–14 (2018). https://  \ndoi. org/ 10. 1007/ s10676- 017- 9430-8\n 206. Dafoe, A.: AI governance: a research agenda, no. July 2017 \n(2017). https:// doi. org/ 10. 1176/ ajp. 134.8. aj134 8938.\n 207. Truby, J., Brown, R.D., Ibrahim, I.A., Parellada, O.C.: A sand-\nbox approach to regulating high-risk artificial intelligence \napplications. Eur. J. Risk Regul. 13(2), 270–294 (2022). https:// \ndoi. org/ 10. 1017/ ERR. 2021. 52\n 208. Akpinar, N.-J., et al.: A sandbox tool to bias(Stress)-test fairness \nalgorithms. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. 2204. \n10233\n 209. Zinda, N.: Ethics auditing framework for trustworthy AI: lessons \nfrom the IT audit literature. In: Mökander J., Ziosi, M. (eds.) The \n2021 Yearbook of the Digital Ethics Lab. Springer Cham (2021). \nhttps:// doi. org/ 10. 1007/ 978-3- 031- 09846-8\n 210. Mantelero, A.: AI and Big Data: A blueprint for a human rights, \nsocial and ethical impact assessment. Comput. Law Secur. Rev. \n34(4), 754–772 (2018). https:// doi. org/ 10. 1016/j. clsr. 2018. 05. \n017\n 211. Reisman, D., Schultz, J., Crawford, K., Whittaker, M.: Algo -\nrithmic impact assessments: A practical framework for public \nagency accountability. AI Now Institute, no. April, p. 22, 2018, \n[Online]. https:// ainow insti tute. org/ aiare port2 018. pdf. Accessed \n10 Feb 2023\n 212. Etzioni, A., Etzioni, O.: AI assisted ethics. Ethics Inf. \nTechnol. 18(2), 149–156 (2016). https://  doi.  org/  10. 1007/  \ns10676- 016- 9400-6\n 213. Whittlestone, J., Clarke, S.: AI challenges for society and eth-\nics. In: Bullock, J., Chen, Y.-C., Himmelreich, J., Hudson, V.M., \nKorinek, A., Young, M., Zhang, B. (eds.) The Oxford Handbook \nof AI Governance. Oxford University Press (2022). https:// doi.  \norg/ 10. 1093/ oxfor dhb/ 97801 97579 329. 013.3\n 214. Karan, M., Šnajder, j.: Preemptive toxic language detection in \nWikipedia comments using thread-level context. In: Proceed-\nings of the Third Workshop on Abusive Language Online, pp. \n129–134, Sep. 2019. https:// doi. org/ 10. 18653/ V1/ W19- 3514\n 215. Gao, L., Huang, R.: Detecting online hate speech using context \naware models. In: Proceedings of the International Conference \nRecent Advances in Natural Language Processing, RANLP 2017, \npp. 260–266, Nov. 2017, https:// doi. org/ 10. 26615/ 978- 954- 452- \n049-6_ 036\n 216. Delobelle, P., Tokpo, E.K., Calders, T., Berendt, B.: Measuring \nfairness with biased rulers: a comparative study on bias met-\nrics for pre-trained language models. In: NAACL 2022—2022 \nConference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, \nProceedings of the Conference, pp. 1693–1706. (2022). https://  \ndoi. org/ 10. 18653/ V1/ 2022. NAACL- MAIN. 122.\n 217. Nozza, D., Bianchi, F., Hovy, D.: HONEST: measuring hurt-\nful sentence completion in language models. In: NAACL-HLT \n2021—2021 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Proceedings of the Conference, pp. 2398–2406 \n(2021). https:// doi. org/ 10. 18653/ V1/ 2021. NAACL- MAIN. 191.\n 218. Costello, M., Hawdon, J., Bernatzky, C., Mendes, K.: Social \ngroup identity and perceptions of online hate. Sociol. Inq. 89(3), \n427–452 (2019). https:// doi. org/ 10. 1111/ SOIN. 12274\n 219. Sap, M., Swayamdipta, S., Vianna, L., Zhou, X., Choi, Y., Smith, \nN. A.: Annotators with attitudes: how annotator beliefs and iden-\ntities bias toxic language detection. In: NAACL 2022—2022 \nConference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, \nProceedings of the Conference, pp. 5884–5906 (2022). https://  \ndoi. org/ 10. 18653/ V1/ 2022. NAACL- MAIN. 431\n 220. Kirk, H.R., Birhane, A., Vidgen, B., Derczynski, L.: Handling \nand Presenting Harmful Text in NLP Research. Findings of \nthe Association for Computational Linguistics: EMNLP 2022 \n(2022). https:// aclan tholo gy. org/ 2022. findi ngs- emnlp. 35/\n 221. Welbl, J., et al.: Challenges in detoxifying language models. In: \nFindings of the Association for Computational Linguistics, Find-\nings of ACL: EMNLP 2021, pp. 2447–2469, Sep. 2021, https://  \ndoi. org/ 10. 48550/ arxiv. 2109. 07445\n1113AI and Ethics (2024) 4:1085–1115 \n1 3\n 222. Rauh, M., et al.: Characteristics of harmful text: towards rigorous \nbenchmarking of language models. ArXiv (2022). https:// doi. org/ \n10. 48550/ arxiv. 2206. 08325\n 223. Nangia, N., Vania, C., Bhalerao, R., Bowman, S.R.: CrowS-pairs: \na challenge dataset for measuring social biases in masked lan-\nguage models. In: EMNLP 2020—2020 Conference on Empiri-\ncal Methods in Natural Language Processing, Proceedings of the \nConference, pp. 1953–1967 (2020). https:// doi. org/ 10. 18653/ V1/ \n2020. EMNLP- MAIN. 154\n 224. Rudinger, R.: GitHub - rudinger/winogender-schemas: Data \nfor evaluating gender bias in coreference resolution systems. \nGitHub. https:// github. com/ rudin ger/ winog  ender- schem as \n(2019). Accessed 25 Jan 2023\n 225. Cihon, P., Kleinaltenkamp, M.J., Schuett, J., Baum, S.D.: AI \nCertification: advancing ethical practice by reducing information \nasymmetries. IEEE Trans. Technol. Soc. 2 (4), 200–209 (2021). \nhttps:// doi. org/ 10. 1109/ tts. 2021. 30775 95\n 226. Cihon, P., Schuett, J., Baum, S.D.: Corporate governance of arti-\nficial intelligence in the public interest. Information 12(7), 1–30 \n(2021). https:// doi. org/ 10. 3390/ info1 20702 75\n 227. FDA.: Artificial intelligence and machine learning in software \nas a medical device. In: U.S. Food & Drug Administration, \n2021. https:// www. fda. gov/ medic al- devic es/ softw are- medic al- \ndevice- samd/ artifi  cial- intel ligen ce- and- machi ne- learn ing- softw \nare- medic al- device (2021). Accessed 20 Jan 2023\n 228. Jacobs, A.Z., Wallach, H.: Measurement and fairness. In: FAccT \n2021—Proceedings of the 2021 ACM Conference on Fairness, \nAccountability, and Transparency, vol. 11, no. 21, pp. 375–385, \nMar. 2021, https:// doi. org/ 10. 1145/ 34421 88. 34459 01.\n 229. Mökander, J., Floridi, L.: Operationalising AI governance \nthrough ethics-based auditing: an industry case study. AI and \nEthics (2022). https:// doi. org/ 10. 1007/ s43681- 022- 00171-7\n 230. Mökander, J., Sheth, M., Gersbro-Sundler, M., Blomgren, P., \nFloridi, L.: Challenges and best practices in corporate AI gov -\nernance: Lessons from the biopharmaceutical industry. Front. \nComput. Sci. (2022). https:// doi. org/ 10. 3389/ fcomp. 2022. 10683 \n61\n 231. Smith, E.: Research design. In: H. Reis, H., Judd, C. (eds.) \nHandbook of Research Methods in Social and Personality Psy -\nchology, pp. 27–48 (2014) [Online]. https://  doi. org/ 10. 1017/ \nCBO97 80511 996481. 006\n 232. Sobieszek, A., Price, T.: Playing games with Ais: the limits \nof GPT-3 and similar large language models. Minds Mach. \n(Dordr) 32(2), 341–364 (2022). https:// doi. org/ 10. 1007/  \ns11023- 022- 09602-0\n 233. Floridi, L.: AI as Agency without Intelligence: on ChatGPT, \nlarge language models, and other generative models. SSRN. \n[Online]. https:// papers. ssrn. com/ sol3/ papers. cfm? abstr act_ \nid= 43587 89 (2022). Accessed 13 Feb 2023\n 234. Reynolds, L., Ai, M., Ai, K., Mcdonell, K.: Prompt program-\nming for large language models: beyond the few-shot para-\ndigm. In: Conference on Human Factors in Computing Sys-\ntems-Proceedings, May 2021, https:// doi. org/ 10. 1145/ 34117  \n63. 34517 60\n 235. Hacking, I.: Representing and Intervening: Introductory Topics \nin the Philosophy of Natural Science. Cambridge University \nPress, Cambridge (1983)\n 236. Rorty, R.: Pragmatism as Anti-authoritarianism. Harvard Uni -\nversity Press, Cambridge (2021)\n 237. Legg, C., Hookway, C.: Pragmatism. In Stanford encyclopedia \nof philosophy. PhilPapers (2020). https:// plato. stanf ord. edu/ \nentri es/ pragm atism/. Accessed 20 Feb 2020\n 238. Watson, D.S., Mökander, J.: In defense of sociotechnical prag-\nmatism. In: Mazzi, F. (ed.) The 2022 Yearbook of the Digital \nGovernance Research Group. Springer (2023). https:// doi. org/ \n10. 1007/ 978-3- 031- 28678-0\n 239. Lee, M.S.A., Floridi, L., Singh, J.: “ormalising Trade-offs \nbeyond algorithmic fairness: lessons from ethical philosophy \nand welfare economics. In: The 2021 Yearbook of the Digital \nEthics Lab, pp. 157–182. Cham: Springer (2022). https:// doi.  \norg/ 10. 1007/ 978-3- 031- 09846-8_ 11\n 240. Friedler, S.A., Scheidegger, C., Venkatasubramanian, S.: The \n(Im)possibility of fairness. Commun ACM 64(4), 136–143 \n(2021). https:// doi. org/ 10. 1145/ 34339 49\n 241. Islam, G., Greenwood, M.: The metrics of ethics and the eth-\nics of metrics. J. Bus. Ethics (2021). https:// doi. org/ 10. 1007/  \ns10551- 021- 05004-x\n 242. Cugueró-Escofet, N., Rosanas, J.M.: The ethics of metrics: over-\ncoming the dysfunctional effects of performance measurements \nthrough justice. J. Bus. Ethics 140(4), 615–631 (2017). https://  \ndoi. org/ 10. 1007/ S10551- 016- 3049-2/ TABLES/2\n 243. Boddington, P.: Towards a code of ethics for artificial intelli-\ngence. In: Artificial Intelligence: Foundations, Theory, and Algo-\nrithms. Switzerland: Springer Cham, (2017)\n 244. Minkkinen, M., Zimmer, M.P., Mäntymäki, M.: Towards Ecosys-\ntems for Responsible AI: Expectations, Agendas and Networks in \nEU Documents. Springer International Publishing (2021). https:// \ndoi. org/ 10. 1007/ 978-3- 030- 85447-8 \n 245. Schöppl, N., Taddeo, M., Floridi, L.: Ethics auditing: lessons \nfrom business ethics for ethics auditing of AI. In: Mökander J., \nZiosi, M. (eds.) The 2021 Yearbook of the Digital Ethics Lab, \npp. 209–227. Springer, Cham (2021). https:// doi. org/ 10. 1007/ \n978-3- 031- 09846-8\n 246. FDA.: Inspection classification database. U.S. Food & Drug \nAdministration. https:// www. fda. gov/ inspe ctions- compl iance- \nenfor cement- and- crimi nal- inves tigat ions/ inspe ction- class ifica \ntion- datab ase (2022). Accessed 20 Jan 2023\n 247. British Safety Council.: About the British Safety Council. Web-\nsite. https:// www. brits afe. org/ about- us/ intro ducing- the- briti sh- \nsafety- counc il/ about- the- briti sh- safety- counc il/ (2023). Accessed \n20 Jan 2023\n 248. Rainforest Alliance.: Our approach. Website. https:// www. rainf \norest-  allia  nce. org/ appro  ach/?_  ga=2. 13719  1288.  95390  5227. \n16581 39559- 11302 50530. 16581 39559 (2023). Accessed 20 Jan \n2023\n 249. IAEA.: Quality management audits in nuclear medicine prac-\ntices. IAEA Human Health Series, vol. 33. [Online]. http:// www. \niaea. org/ Publi catio ns/ index. html (2015). Accessed 20 Jan 2023\n 250. Duflo, E., Greenstone, M., Pande, R., Ryan, N.: Truth-telling by \nthird-party auditors and the response of polluting firms: experi-\nmental evidence from India. Q. J. Econ. 128(4), 1499–1545 \n(2013). https:// doi. org/ 10. 1093/ QJE/ QJT024\n 251. Tutt, A.: An FDA for Algorithms. Adm. Law. Rev. 69(1), 83–123 \n(2017). https:// doi. org/ 10. 2139/ ssrn. 27479 94\n 252. Carpenter, D.: Reputation and power: organizational image and \npharmaceutical regulation at the FDA. In: Reputation and Power: \nOrganizational Image and Pharmaceutical Regulation at the \nFDA, pp. 1–802, (2014). https:// doi. org/ 10. 5860/ choice. 48- 3548\n 253. Fraser, H.L., Bello y Villarino, J.-M.: Where residual risks reside: \na comparative approach to Art 9(4) of the European Union’s Pro-\nposed AI Regulation. SSRN Electron. J. (2021). https:// doi. org/ \n10. 2139/ SSRN. 39604 61\n 254. van Merwijk, C.: An AI defense-offense symmetry thesis. Less-\nWrong. https:// www. lessw rong. com/ posts/ dPe87 urYGQ PA4gD \nEp/ an- ai- defen se- offen se- symme try- thesis (2022). Accessed 20 \nJan 2023\n 255. Du Sautoy, M.: The Creativity Code : Art and Innovation in the \nAge of A First US edition. Fourth Estate, Cambridge (2019)\n 256. Floridi, L., et al.: AI4People—an ethical framework for a good \nAI society: opportunities, risks, principles, and recommenda -\ntions. Minds Mach (Dordr) 28(4), 689–707 (2018). https:// doi.  \norg/ 10. 1007/ s11023- 018- 9482-5\n1114 AI and Ethics (2024) 4:1085–1115\n1 3\n 257. Frey, C.B.: The Technology Trap : Capital, Labor, and Power in \nthe Age of Automation. Princeton University Press, Princeton \n(2019)\n 258. Mökander, J., Floridi, L.: From algorithmic accountability to \ndigital governance. Nat. Mach. Intell. (2022). https:// doi. org/ 10. \n1038/ s42256- 022- 00504-5\n 259. Sloane, M.: The Algorithmic Auditing Trap. [Online]. https://  \noneze ro. medium. com/ the- algor ithmic- audit ing- trap- 9a6f2 d4d46 \n1d (2021). Accessed 20 Jan 2023\n 260. Ziegler, D. M., Nix, S., Chan, L., Bauman, T., Schmidt-Nielsen, \nP., Lin, T., et al.: Adversarial training for high-stakes reliability. \nAdv. Neural Inf. Process. Syst. 35, 9274–9286 (2022). arXiv:  \n2205. 01663\n 261. Keyes, O., Durbin, M., Hutson, J.: A mulching proposal: Analys-\ning and improving an algorithmic system for turning the elderly \ninto high-nutrient slurry. In: Conference on Human Factors in \nComputing Systems-Proceedings, May 2019, https:// doi. org/ 10. \n1145/ 32906 07. 33104 33\n 262. Mökander, J., Juneja, P., Watson, D.S., Floridi, L.: The US Algo-\nrithmic Accountability Act of 2022 vs. The EU Artificial Intel-\nligence Act: what can they learn from each other? Minds Mach \n(Dordr) (2022). https:// doi. org/ 10. 1007/ s11023- 022- 09612-y\n 263. Epstein, Z., et al.: Turingbox: An experimental platform for the \nevaluation of AI systems. In: IJCAI International Joint Confer -\nence on Artificial Intelligence, vol. 2018-July, pp. 5826–5828 \n(2018). https:// doi. org/ 10. 24963/ ijcai. 2018/ 851\n 264. EPRS: A Governance Framework for Algorithmic Accountabil-\nity and Transparency. European Parliamentary Research Service \n(2019). https:// doi. org/ 10. 2861/ 59990 \n 265. EIU.: Staying ahead of the curve—the business case for responsi-\nble AI. The Economist Intelligence Unit. https:// www. eiu. com/n/ \nstayi ng- ahead- of- the- curve- the- busin ess- case- for- respo nsible- ai/, \n(2020). Accessed 7 Oct 2020\n 266. Mondal, S., Das, S., Vrana, V.G.: How to bell the cat? A theo-\nretical review of generative artificial intelligence towards digital \ndisruption in all walks of life. Technologies 11(2), 44 (2023). \nhttps:// doi. org/ 10. 3390/ TECHN OLOGI ES110 20044\n 267. Muller, M., Chilton, L.B., Kantosalo, A., Lou Maher, M., Martin, \nC.P., Walsh, G.: GenAICHI: generative AI and HCI. In: Con -\nference on Human Factors in Computing Systems-Proceedings, \nApr. 2022, https:// doi. org/ 10. 1145/ 34911 01. 35037 19\n 268. Rao, A.S.: Democratization of AI. A double-edged sword. \nToward Data Science. https:// towar  dsdat ascie nce. com/ democ \nratiz ation- of- ai- de155 f0616 b5 (2020). Accessed 22 Mar 2023\n 269. Salkind, N.J.: Encyclopedia of Research Design. SAGE, Los \nAngeles (2010)\n 270. Haas, P.J., Springer, J.F.: Applied policy research: concepts \nand cases. In: Garland Reference Library of Social Science ; \nv. 1051. New York: Garland Pub (1998)\n 271. Grant, M.J., Booth, A.: A typology of reviews: an analysis of \n14 review types and associated methodologies. Health Info \nLibr J 26(2), 91–108 (2009). https:// doi. org/ 10. 1111/j. 1471-  \n1842. 2009. 00848.x\n 272. Wohlin, C.: Guidelines for snowballing in systematic literature \nstudies and a replication in software engineering. In: EASE ’14 \n(2014). https:// doi. org/ 10. 1145/ 26012 48. 26012 68.\n 273. Frey, B.B.: The SAGE Encyclopedia of Educational Research, \nMeasurement, and Evaluation, vol. 4. SAGE Publications, \nIncorporated, Thousand Oaks (2018)\n 274. Adams, W.C.: Conducting semi-structured interviews. In: Hand-\nbook of Practical Program Evaluation: Fourth Edition, pp. 492–\n505. (2015). https:// doi. org/ 10. 1002/ 97811 19171 386. CH19.\n 275. Baldwin, R., Cave, M.: Understanding Regulation: Theory, Strat-\negy, and Practice. Oxford University Press, Oxford (1999)\n 276. Vaswani, A., et al.: Attention is all you need. Adv Neural Inf \nProcess Syst, vol. 2017-December, pp. 5999–6009, Jun. 2017. \n[Online]. https:// arxiv. org/ abs/ 1706. 03762 v5. Accessed 12 Apr \n2023\n 277. Smith-Goodson, P.: “NVIDIA’s New H100 GPU Smashes Artifi-\ncial Intelligence Benchmarking Records. Forbes, 2022. [Online]. \nhttps:// www. forbes. com/ sites/ moori nsigh ts/ 2022/ 09/ 14/ nvidi \nas- new- h100- gpu- smash es- artifi  cial- intel ligen ce- bench marki \nng- recor ds/? sh= 5e8dc a9ce7 28. Accessed 2 Apr 2023\n 278. Russakovsky, O., et al.: ImageNet large scale visual recognition \nchallenge. Int J Comput Vis 115(3), 211–252 (2015). https:// doi. \norg/ 10. 1007/ S11263- 015- 0816-Y/ FIGUR ES/ 16\n 279. Luccioni, A., Viviano, J.D.: What’s in the Box? An analysis of \nundesirable content in the common crawl corpus. In: Proceedings \nof the 59th Annual Meeting of the Association for Computational \nLinguistics and the 11th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers), pp. 182–189 \n(2021). https:// doi. org/ 10. 18653/ V1/ 2021. ACL- SHORT. 24.\n 280. Han, X., et al.: Pre-trained models: past, present and future. AI \nOpen 2, 225–250 (2021). https:// doi. org/ 10. 1016/J. AIOPEN. \n2021. 08. 002\n 281. European Commission.: Ethics guidelines for trustworthy AI. \nAI HLEG, pp. 2–36, [Online]. https:// ec. europa. eu/ futur ium/ en/ \nai- allia nce- consu ltati on/ guide lines# Top (2019). Accessed 10 Feb \n2023\n 282. Korbak, T., Elsahar, H., Kruszewski, G., Dymetman, M.: On \nreinforcement learning and distribution matching for fine-tuning \nlanguage models with no catastrophic forgetting. ArXiv, Jun. \n2022. [Online]. https:// arxiv. org/ abs/ 2206. 00761 v2. Accessed 2 \nApr 2023\n 283. Min, S., et al.: Rethinking the role of demonstrations: what makes \nin-context learning work?, In: Proceedings of the 2022 Confer -\nence on Empirical Methods in Natural Language Processing, pp. \n11048–11064. [Online]. https:// aclan tholo gy. org/ 2022. emnlp- \nmain. 759 (2022). Accessed 2 Apr 2023\n 284. Alayrac, J.-B., et al.: Flamingo: a visual language model for few-\nshot learning. ArXiv (2022). https:// doi. org/ 10. 48550/ arxiv. 2204. \n14198\n 285. Zittrain, J.L.: The generative internet. Connect. Q. J. 13(4), \n75–118 (2014). https:// doi. org/ 10. 11610/ CONNE CTIONS. 13.4. \n05\n 286. OpenAI.: Generative models. Blog post. https:// openai. com/ blog/ \ngener ative- models/ (2016). Accessed 25 Jan 2023\n 287. NITS.: Red Team (Glossary). Computer Security Resource \nCenter. https:// csrc. nist. gov/ gloss ary/ term/ red_ team (2023). \nAccessed 2 Apr 2023\n 288. Bertuzzi, L.: AI Act: EU Parliament’s crunch time on high-risk \ncategorisation, prohibited practices. In: EURACTIV, 2023. \nhttps:// www. eurac tiv. com/ secti on/ artifi  cial- intel ligen ce/ news/ \nai- act- eu- parli aments- crunch- time- on- high- risk- categ orisa tion- \nprohi bited- pract ices/. Accessed 24 Mar 2023\n 289. AJL.: Algorithmic Justice League-Unmasking AI harms and \nbiases. https:// www. ajl. org/ (2023). Accessed 2 Apr 2023\n 290. Russell, S.J., Norvig, P.: Artificial Intelligence : A Modern \nApproach, 3rd edn. Pearson, New Delhi (2015)\n 291. Corning, P.A.: The re-emergence of emergence, and the causal \nrole of synergy in emergent evolution. Synthese 185(2), 295–317 \n(2010). https:// doi. org/ 10. 1007/ s11229- 010- 9726-2\n 292. Molnar, C.: Interpretable machine learning. a guide for making \nblack box models explainable.. Book, p. 247 (2021), [Online]. \nhttps:// chris tophm. github. io/ inter preta ble- ml- book. Accessed 10 \nFeb 2023\n 293. Christiano, P.F., Leike, J., Brown, T.B., Martic, M., Legg, S., \nAmodei, D.: Deep reinforcement learning from human prefer -\nences. Adv Neural Inf. Process. Syst. 30 (2017)\n 294. Zang, S.: metaseq/projects/OPT/chronicles at main · facebookre-\nsearch/metaseq · GitHub. GitHub. https:// github. com/ faceb ookre \n1115AI and Ethics (2024) 4:1085–1115 \n1 3\nsearch/ metas eq/ tree/ main/ proje cts/ OPT/ chron icles (2022). \nAccessed 25 Jan 2023\n 295. Hubinger, E.: Relaxed adversarial training for inner alignment—\nAI Alignment Forum. In: Alignment Forum. https:// www. align \nmentf orum. org/ posts/ 9Dy5Y RaoCx H9zuJ qa/ relax ed- adver sarial- \ntrain ing- for- inner- align ment (2019). Accessed 20 Jan 2023\n 296. Weller, A.: Challenges for transparency. In: 2017 ICML Work -\nshop on Human Interpretability in Machine (2017). https:// openr \neview. net/ forum? id= SJR9L 5MQ-\n 297. Chasalow, K., Levy, K.: Representativeness in statistics, politics, \nand machine learning,” FAccT 2021 - Proceedings of the 2021 \nACM Conference on Fairness, Accountability, and Transparency, \npp. 77–89, Jan. 2021, doi: https:// doi. org/ 10. 48550/ arxiv. 2101. \n03827.\n 298. Blodgett, S.L., Barocas III, S.H.D., Wallach, H.: Language \n(Technology) is power: a critical survey of ‘Bias’ in NLP. In: \nProceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics, pp. 5454–5476, Jul. 2020, https:// doi. \norg/ 10. 18653/ V1/ 2020. ACL- MAIN. 485.\n 299. Kirk, H.R., Vidgen, B., Röttger, P., Thrush, T., Hale, S. A.: \nHatemoji: a test suite and adversarially-generated dataset for \nbenchmarking and detecting emoji-based hate. In: NAACL \n2022-2022 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Proceedings of the Conference, pp. 1352–1368 \n(2022), https:// doi. org/ 10. 18653/ V1/ 2022. NAACL- MAIN. 97.\n 300. Kumar, D., et al.: Designing toxic content classification for a \ndiversity of perspectives. In: Proceedings of the Seventeenth \nSymposium on Usable Privacy and Security. https:// data. esrg.  \nstanf ord. edu/ study/ toxic ity- persp ectiv es (2021). Accessed 25 Jan \n2023\n 301. Cantwell Smith, B.: The promise of artificial intelligence: reck -\noning and judgment. MIT Press, Cambridge (2019)\n 302. ForHumanity. Independent audit of AI systems (2023). https://  \nforhu manity. center/ indep endent- audit- of- ai- syste ms/. Accessed \n12 Feb 2023\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}