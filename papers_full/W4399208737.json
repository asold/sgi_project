{
    "title": "Social AI Agents Too Need to Explain Themselves",
    "url": "https://openalex.org/W4399208737",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Basappa, Rhea",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Tekman, Mustafa",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2114999453",
            "name": "Lu Hong",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Faught, Benjamin",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Kakar, Sandeep",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4288954566",
            "name": "Goel, Ashok K.",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2014858765",
        "https://openalex.org/W3032178096",
        "https://openalex.org/W6605034069",
        "https://openalex.org/W4399209637",
        "https://openalex.org/W2118957105",
        "https://openalex.org/W2618829310",
        "https://openalex.org/W4389607477",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W6699011579",
        "https://openalex.org/W3093718041",
        "https://openalex.org/W2963847595",
        "https://openalex.org/W2945976633",
        "https://openalex.org/W2784193494",
        "https://openalex.org/W2908743506",
        "https://openalex.org/W2482473633",
        "https://openalex.org/W1997650798",
        "https://openalex.org/W1592509827",
        "https://openalex.org/W2999765337",
        "https://openalex.org/W4321786089",
        "https://openalex.org/W3104847483"
    ],
    "abstract": null,
    "full_text": "Self-Explanation in Social AI Agents\nRhea Basappa, Mustafa Tekman, Hong Lu, Benjamin Faught, Sandeep Kakar,\nand Ashok K. Goel\nGeorgia Institute of Technology, Atlanta GA 30332, USA\n{rb324,mtekman3,bfaught3,skakar6,ag25}\n@gatech.edu, hlu07@tufts.edu\nAbstract. Social AI agents interact with members of a community,\nthereby changing the behavior of the community. For example, in online\nlearning, an AI social assistant may connect learners and thereby en-\nhance social interaction. These social AI assistants too need to explain\nthemselves in order to enhance transparency and trust with the learners.\nWe present a method of self-explanation that uses introspection over\na self-model of an AI social assistant. The self-model is captured as a\nfunctional model that specifies how the methods of the agent use knowl-\nedge to achieve its tasks. The process of generating self-explanations uses\nChain of Thought to reflect on the self-model and ChatGPT to provide\nexplanations about its functioning. We evaluate the self-explanation of\nthe AI social assistant for completeness and correctness. We also report\non its deployment in a live class.\nKeywords: Social AI · XAI · Self-Explanation · Self-Models · Gen-\nerative AI · Combining Knowledge-Based and Generative AI\n1 Introduction\nLearning at scale, and particularly online learning at scale, offers many well-\nestablished benefits such as geographically distributed and self-paced asyn-\nchronous learning that meets the reskilling and upskilling needs of working\nlearners and learning workers. However, learning at scale, and again particu-\nlarly online learning at scale, also have several well-known drawbacks such as\nlack of social presence, i.e., the ability of learners to establish and maintain a\nsense of connectedness both with one another and with the instructor [1].\nSAMI (Social Agent Mediated Interaction) is an AI social assistant that helps\nstudents in large online classes form social connections by introducing them to\none another based on shared characteristics and interests [2–4]. This is posited\nto increase social presence in an online class environment [3]. However, students\ninteracting with SAMI often have questions regarding its inner workings [4].\nKnowing how SAMI works internally may help students build trust in its recom-\nmendations. Thus, the specific research question for us in this paper becomes:\nHow might an AI social assistant, such as SAMI, provide an explanation of its\ninner workings to online students?\narXiv:2501.13945v1  [cs.CL]  19 Jan 2025\n2 R. Basappa et al.\nWe present a computational technique for self-explanation in SAMI. Our self-\nexplanation technique consists of several parts. First, we view self-explanation\nas a process of question answering in which a user provides the AI agent input\nin natural language (English), the agent then introspects on its knowledge of its\nown reasoning and then produces an answer back to this question also in natural\nlanguage (English). Second, this introspection requires the AI agent to have a\nself-model of its goals, knowledge, and methods. We use the Task, Method and\nKnowledge (TMK) framework [5–7] for representing this self-model. Third, we\nreplace logical propositions in the traditional TMK models with short descrip-\ntions in English while still retaining their task-method-knowledge decomposition.\nFourth, we conduct a similarity search on the input question and the English de-\nscriptions in TMK model to find the relevant snippets for answering the question.\nFifth, we use Chain of Thought [8] to walk step-by-step over the TMK model\nto generate prompts into ChatGPT to produce an answer from the identified\nsnippets. Thus, the self-explanation model of SAMI combines the strengths of\ngenerative AI (training over a very large corpus and the ability to address a large\nvariety of natural language tasks) with that of knowledge-based AI (knowledge\nrepresentation and organization at multiple levels of abstraction).\n2 Related Work\nSelf-explanation has re-emerged as an important topic in AI. Muller et al.\n(2019) [9] provide a fairly comprehensive and a very useful summary of the\nhistory of AI research on self-explanation. Confalonieri et al. (2021) [10] present\nanother and more recent take on the history. The need for interpretability of\nthe representations and processing in modern neural networks is one of the\nmain reasons for the resurgence of interest in self-explanation in AI agents [11].\nRudin [12] advocates the construction and use only of AI agents capable of self-\ninterpretation and self-explanation. Tulli & Aha [13] provide a recent collection\nof papers on self-explanation.\nIt is useful here to distinguish between two kinds of AI assistants: AI assis-\ntants that interact with individual humans and AI assistants that enable inter-\naction among humans. In the context of AI in learning and teaching, teaching\nassistants such as Jill Watson [14, 15] that answer a student’s questions are an\nexample of the former; AI social assistants such as SAMI [2–4] that help fos-\nter interactions among students are an example of the latter. The latter class\nof assistants exemplify the paradigm of “computers are social actors” [16]. It is\nimportant to note that self-explanation in social assistants is as important as it\nis in personal teaching and learning assistants.\nOne of the key ideas to emerge out of this early research on explanation\nwas the importance of explicit representation of knowledge of the design of an\nAI assistant [17, 18]. An explicit representation of the design knowledge of an\nAI assistant enables the generation of explanations of the tasks it accomplishes,\nthe domain knowledge it uses, as well as the method that use the knowledge\nto achieve the tasks. This raised the question of how this design knowledge can\nSelf-Explanation in Social AI Agents 3\nbe identified, acquired, represented, stored, accessed, and used for generating\nexplanations [7]. One possible answer was to endow the AI agent with meta-\nknowledge of its own design [19] and enable the agent to generate explanations\nthrough introspection on its meta-knowledge.\n3 Computational Architecture and Process for\nSelf-Explanation\n3.1 SAMI, A Social AI Agent\nSAMI accesses the self-introduction posts of students in an online discussion\nforum and extracts information such as their location, hobbies and academic\ninterests. Using this, SAMI builds a knowledge graph for each student. It then\nuses the knowledge graph to ’match’ students who share one or more similarities.\nSAMI communicates its recommendations of matches to the online students who\nelect to contact the recommended matches [2–4].\n3.2 Semantic Representation of SAMI\nFrom its code base, we manually create a symbolic representation of SAMI in\nthe Task-Method-Knowledge (TMK) framework [5–7]. Briefly, the TMK model\nspecifies SAMI’s tasks (or goals), methods (or mechanisms for achieving the\ngoals) and the domain knowledge of the environment. The TMK is organized\nhierarchically. The top-level task specifies SAMI’s method for accomplishing it;\nthe method specifies the finite state machine for accomplishing the task in terms\nof a sequence of information states and state transitions. The state transitions\nare annotated by either subtasks or domain knowledge. This decomposition con-\ntinues until all leaf nodes in the TMK model are primitive tasks that can be\ndirectly accomplished by the available domain knowledge. Figure 1 illustrates\nthe hierarchical organization of the TMK model of SAMI. Figure 2 illustrates\nthe state-transition specification of a method in the TMK model in detail. Having\nbuilt the TMK model of SAMI, we manually translate the logical propositions\nin the TMK model into brief natural language descriptions to obtain a semantic\nrepresentation of SAMI. This “semantic representation” becomes the self-model\nof SAMI that empowers the self-explanation technique.\n3.3 Self-Explanation Technique\nThe self-explanation technique utilizes the information from SAMI’s TMK self-\nmodel to provide explanations about its inner workings. As Figure 3 illustrates,\nthe technique has has three main stages: Classification, Localization and Reason-\ning. When a question is asked, the Classifier first analyses the question to deter-\nmine which of the pre-defined classes the question belongs to. These classes are\n‘mmodel’, ‘kmodel’, ‘multimodel’ (and ‘can’t answer’) and are used to determine\nwhich part(s) of the self-model of SAMI are later used in the self-explanation\n4 R. Basappa et al.\nFig. 1.The organization of the TMK model of SAMI. Outer rectangles represents\nhigh-level goals. The inner rectangles represents methods, with the circles within them\nrepresenting sub-tasks and the arrows between them representing transitions. (We know\nthat the text in this figure is not readable. Our goal here is to convey the hierarchical\ndecomposition of the TMK model of SAMI, along with its size and complexity.\nSelf-Explanation in Social AI Agents 5\nFig. 2.This figure illustrates the state-transition specification of one method (‘RG\nprocess’) in the TMK model of SAMI. The circles within the method represents the\nindividual tasks. The arrow connecting the tasks represent the state by state transitions\nwithin this method.\npipeline. A ‘kmodel’ classification would lead to using information only from\nthe knowledge part of the self-model. An ‘mmodel’ classification would lead to\nlocalizing the relevant task and method within the task and method parts. With\na ‘multimodel’ classification, a similarity search would be conducted to find the\nrelevant pieces of information from all knowledge, method, and task parts of the\nself-model. Lastly, any question deemed as not being relevant to SAMI would\nbe classified as ‘can’t answer’ . The Classifier employs LangChain 1 to create a\nprompt that uses pre-written templates describing each of these classifications,\nalong with the question to be answered. This prompt is then sent to ChatGPT2,\nwhich returns a value for the classification, along with a complexity ‘k’ value\nwhich is used to control the verbosity of the final answer in later stages.\nFig. 3.The computational architecture and process of the self-explanation technique\n1 LangChain Official Documentation\n2 Open AI’s gpt3.5-turbo-instruct model has been used\n6 R. Basappa et al.\nNext, the Localizer conducts the similarity search to find the most relevant\nk pieces of information within the sub-model(s) identified as relevant by the\nclassifier. The Localizer uses the FAISS library 3 to do a similarity search on\nthe input question and the natural language descriptions in the relevant sub-\nmodel(s). The hierarchical organization of the TMK model (see Figure 1) helps\nin this localization.\nIn the final stage, if the relevant items identified by the Localizer include\na method, the Reasoner uses Chain of Thought to walk step-by-step over the\nspecification of the identified method including the subtasks in the method spec-\nification (see Figure 2). This enables the answer to include descriptions of task\nannotations on the state transitions within a method that might have led to\na particular outcome of SAMI. The Reasonser uses LangChain to construct\nprompts to ChatGPT to compose the final answer.\n4 Evaluation\nCorrectness and Completeness Study Design: To evaluate the self-\nexplanation method, we used high-level, non context-dependent questions taken\ndirectly from XAI question banks [20, 21] such as “What is the source of the\ndata?” [20], “How often does the system make mistakes?” [20] and “What is\nthe scope of the output data?” [21]. Additionally, we modified some questions so\nthat they become more relevant to SAMI. For example, “What are the results of\nother people using the system?” [21] was adapted as “What is the result of other\nstudents opting-in to use SAMI?” .\nIn total, 57 questions were borrowed and adapted from the question banks.\nAdditionally, 9 questions specific to SAMI were created. These include questions\nsuch as “What is a match?” and “How do you find matches for students?” .\nThe self-explanation model of SAMI was asked each of these 66 questions and\nSAMI developers assessed each explanation for correctness and completeness.\nWe considered the definition of correctness as “nothing but the truth” [22] and\ncompleteness as “the whole truth” [22]. For correctness, three categories - yes,\npartial and no - were noted, and for completeness, two categories - complete and\nincomplete - were used.\nResult of the Correctness and Completeness Study:Table 1 summarizes\nthe completeness and correctness scores for each category of questions. The self-\nexplanation method provided correct answers to 49 out of 66 questions; 37 of the\n49 answers were both correct and complete. For example, for the question,“What\nis a match?” , the self-explanation method provided the answer “A match is a\nstudent recommended by SAMI to the user who shares one or several similarities\nwith the user. This information is based on the task, method and goals of the\nSocial AI agent provided, which contains information about objects and their\n3 https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-\nefficient-similarity-search/\nSelf-Explanation in Social AI Agents 7\nTable 1.Results of categorising all 66 questions that were used to evaluate the self-\nexplanation method, a representative question for each category along with their adap-\ntation and corresponding completeness and correctness results.\nCategory # of\nQuestions\nExample Question Completeness\nScores\nCorrectness\nScores\nInput 4 What kind of data does SAMI learn from? Complete: 2/4\nIncomplete: 2/4\nCorrect: 3/4\nPartially Correct: 1/4\nIncorrect: 0/4\nOutput 22\nHow can I best utilise the output of the system?\nHow can I best utilise SAMI’s output?\nHow can I best utilize the matches given by SAMI?\nComplete: 16/22\nIncomplete: 6/22\nCorrect: 14/22\nPartially Correct: 2/22\nIncorrect: 6/22\nHow (global) 17\nIs location used or not used in matchmaking?\nAre hobbies used or not used in matchmaking?\nAre courses taken used or not used in matchmaking?\nIs specialization used or not used in matchmaking?\nComplete: 12/17\nIncomplete: 5/17\nCorrect: 14/17\nPartially Correct: 2/17\nIncorrect: 1/17\nWhy not 1 Why would two students not be matches? Complete: 1/1\nIncomplete: 0/1\nCorrect: 1/1\nPartially Correct: 0/1\nIncorrect: 0/1\nOthers 10\nWhat are the results of other people using the system?\nWhat are the result of other students opting-in to SAMI?\nWould I be affected if other students use or not use SAMI?\nHow will I be affected if other students use or not use SAMI?\nComplete: 10/10\nIncomplete: 0/10\nCorrect: 9/10\nPartially Correct: 1/10\nIncorrect: 0/10\nOthers (context) 3 Who is responsible for this system? Complete: 2/3\nIncomplete: 1/3\nCorrect: 0/3\nPartially Correct: 2/3\nIncorrect: 1/3\nSAMI Specific\nQuestions\n9 Why am I matched with a particular student?Complete: 6/9\nIncomplete: 3/9\nCorrect: 8/9\nPartially Correct: 0/9\nIncorrect: 1/9\n8 R. Basappa et al.\nproperties that SAMI tracks in its system. The task, method and goals of the agent\nstates that a match is a student recommended by SAMI, and this recommendation\nis based on shared similarities between the user and the recommended student.\nThese similarities could include location, hobbies, specialization, classes taken,\nand timezone”. Since this explanation did not contain any incorrect information\nnor did it miss any other relevant correct information, it was assessed to be both\ncorrect and complete.\nThe question “What are the different types of responses you are capable of\ngiving?” was answered with “I am capable of giving Individual Responses, which\nare recommendations for matches based on the information provided by the user.\nThese responses are based on the entities I extract from the user’s introduc-\ntion post, such as location, hobbies, specialization, classes taken, and timezone. ”.\nWhilst this is correct, it is incomplete because SAMI is capable of providing\nindividual, summary and feedback responses. Saying it only provides individual\nresponse makes the answer incomplete. 12 correct answers were incomplete. Some\nanswers given by the self-explanation method were partially correct. Such par-\ntially correct answers were given to 8 of the 66 questions. The question, “What\nare your limitations?” received an answer as, “I am a software system called\nSAMI, and my main limitation is that I can only mediate social interactions\namong students who have opted in to use me. I do this by extracting entities\nsuch as location, hobbies, specialization, classes taken, and timezone from their\nintroduction post and connecting them to others based on similar entities. How-\never, I am limited to only students who have opted in and have included the\n#connectme optin tag in their introduction post in the ED online discussion fo-\nrum. Additionally, I am limited to only students who are enrolled in one of the\nfive specializations offered by Georgia Institute of Technology for its Computer\nScience Masters students. ”. This answer is evaluated to be partially correct be-\ncause even though the first part around opt-in limitation is correct, the second\npart around being only limited to students in one of the five specializations is\nincorrect. On the other hand, the answer is complete as it’s not missing any\nother relevant information.\nIncorrect answers were given to only 9 out of the 66 questions. 6 of these\n9 incorrect answers were to questions belonging to the ‘Output’ category - this\ncould be attributed to the fact that there’s no output specified in SAMI’s self-\nmodel, since it does not have a static output. For example,“What does the output\nof SAMI mean?” received an answer as, “The output of SAMI refers to the result\nor outcome of the system’s task, which is to mediate social interactions among\nstudents who have opted in to use SAMI. However, according to the given task,\nmethod and goals of the Social AI agent, the output for this specific task is not\nspecified, so it is not possible to provide a specific answer. It is recommended to\nask other questions that may be answered with the information provided in the\ntask, method and goals of the Social AI agent” . This answer was evaluated to\nbe incorrect (and incomplete) as it doesn’t correctly explain what the output of\nSAMI means.\nSelf-Explanation in Social AI Agents 9\nIn total, 48 out of 66 answers were complete, with 18 being incomplete.\nQuestions like “What does a match mean?” , “What type of matches does SAMI\ngive?” and “What kind of output does SAMI give?” received complete answers,\nwhilst questions such as “What data is the system not using?” [20] and “What\nis system’s overall logic?” [20] received incomplete explanations. We expect the\ncompleteness of SAMI’s self-explanations to improve with the completeness of\nits TMK self-model.\nPrecision study design10 questions out of the 66 questions used in correct-\nness and completeness study were randomly selected. Each of these questions\nwere asked 100 times to the self-explanation method of SAMI. These questions\nare:\n1. Why would two students not be matches?\n2. What is the system’s overall logic? [20]\n3. How will the matches change?\n4. How can I get better matches?\n5. Why am I matched with a particular student?\n6. How can I best utilise the matches given by SAMI?\n7. What kind of output does SAMI give?\n8. What is the result of other students opting-in to SAMI?\n9. What does the output of SAMI mean?\n10. What is the source of your information? [21]\nResult of precision studyOut of these 10 questions, questions 4 and 9 re-\nceived only one answer. So the self-explanation method of SAMI is precise for\nthese two questions. Questions 1, 3, 7 and 10 got three different answers each.\nQuestion 6 and 5 got 4 different answers. Question 2 got 8 different answers and\nquestion 8 got 9 different answers. However, for each of these questions, each of\nthe different answers were repeated different number of times. Moreover, for a\nsingle question, the answers didn’t appear to be very different from each other.\nTo test this, we used ‘similarity’ function in “en-core-web-g” model of spacy\nand Figure 4 notes how similar each answer was to the other. By this we see\nthat the self-explanation model is able to provide nearly the same answer to the\nsame question regardless of how many times the question is asked. The minimal\ndifference in answer could be attributed to natural language (English). There is\nlittle to no difference in the actual meaning of the answer. Therefore, we can\nconclude that our self-explanation model provides precise answers.\n4.1 Ablation Study\nAblation study designAn ablation study was carried out to further evaluate\nthe self-explanation model’s performance – in particular, to examine the possible\neffects of removing all or parts of the information provided by the self-model to\nthe self-explanation method. As part of this, we ask the same 66 questions to the\n10 R. Basappa et al.\nFig. 4.Results of running similarity checks between each answer pair received for\neach question. Similarity scores are shown on the x-axis and y-axis shows the question\nnumber\nSelf-Explanation in Social AI Agents 11\nself-explanation method 6 times, each time removing a further part of the self-\nmodel that is available to the self-explanation method. We refer to each of these\nsteps as ‘levels’ . To enable this, we ‘layer’ up the TMK representation of the AI\nsocial assistant based on the hierarchy of tasks (shown in Figure 5). For the first\nthree levels of this study, we control the amount of information available based\non this layering. For the remaining three levels, we remove the knowledge part of\nthe self-model as well as any description related to the inner workings of the AI\nsocial assistant from any prompts, until we are left with no additional knowledge\nbeing available. With this last level, the self-explanation method essentially relies\npurely on the reasoning being provided by the generative AI part of it via the\nlarge language model, which is not enhanced with any additional knowledge from\nthe self-model.\nIn the end, the self-explanation method was provided the below information\nwith each level of degradation:\n1. Last three task and method layers of the self-model were removed - leaving\nonly the first four layers.\n2. Last five task and method layers of the self-model were removed - leaving\nonly the first two layers\n3. All but the outermost layer of the self-model were removed.\n4. All task and method layers, as well as the knowledge of self-model were\nremoved.\n5. Only a one-sentence description of the system’s overall task (roughly corre-\nsponding to layer 0) was provided in the prompt.\n6. All information from the self-model as well as from any prompts were re-\nmoved.\nResults of ablation study After running the model with differing amount\nof information as described above, each answer was compared to the answer\nreceived by running the model with all possible information for the same ques-\ntions. Then, a similarity score was assigned (using the ‘spacy’ method as de-\nscribed above). Our results show the contribution of the information from the\nself-model (TMK based semantic representation) and how the answers increase\nin similarity with each additional layer, providing additional piece of informa-\ntion. This also shows us that whilst reasoning alone can provide satisfactory\nanswers, each additional information contributes to the answer. To test the sig-\nnificance of the difference between each levels, pairwise t-tests have been carried\nout. The results show that apart from the difference between Levels 2 and 3,\nthere is a statistically significant difference with p values obtained below 0.05.\nLevels 2 and 3 only differ by removing the first level below the top, and even\nthough a difference can be observed, this is not statistically significant. This can\nbe interpreted as this one layer not providing important information as much as\nthe other parts of the self-model.\n12 R. Basappa et al.\nFig. 5.The result of breaking down the representation of the self-model into layers.\nLayer 0 is shown in color green. The rectangles representing some high-level goals as\nwell as some methods shown in color yellow denote layer 1 and those in color orange\ndenote layers 2 and 3. Finally, we have used color blue on the rectangles to denote\nlayers 4, 5 and 6. (We provide the figure as a reference to give a sense of the systematic\ndegradation of the task element of the TMK model - the figure is otherwise, illegible.)\nFig. 6.Similarity scores of each question with each level of degradation as described\nabove. Similarity is checked against the answers received with the complete model for\nthe same questions.\nSelf-Explanation in Social AI Agents 13\nTable 2.Significance Check of Degradation of each Level\nResult of Pair-wise t-tests\nPair p-value Significance Level\nLevel 1 to Level 2 0.02 **\nLevel 2 to Level 3 0.24\nLevel 3 to Level 4 0.02 **\nLevel 4 to Level 5 0.01 **\nLevel 5 t0 Level 6 0.02 **\n4.2 Deployment in live classes\nDeployment in live classes study designSelf-Explanation method of SAMI\nwas deployed in 2 OMSCS courses (Online Master of Science in Computer Sci-\nence, Georgia Institute of Technology), Knowledge Based AI and Machine Learn-\ning for Trading in Spring of 2024. We report on initial, two weeks of students’\ninteraction with the self-explanation method of SAMI. It is important to note\nthat prior to deploying the self-explanation method, SAMI was deployed in the\nclass. Students that had opted in had already received matches from SAMI.\nThis was made available to the students in the ED asynchronous discussion\nforum. The self-explanation method was run on a server using a Docker con-\ntainer, the Ed-bot feature of ED forum was used to communicate with this. A\nnew thread was created for students to ask questions to the self-explanation\nmethod of SAMI. The thread provided a brief description on how they could ask\nquestions, instruction to use #SAMIexplain (to trigger the ED-bot) and two\nexample questions. Students could ask questions to the self-explanation method\nof SAMI in the asynchronous discussion forum using natural language (English)\nwith the addition of the opt-in tag at the end of their question. For their ques-\ntions, students received two answers. The first answer provided an explanation\naddressing their question. This was the answer from the self-explanation method\nof SAMI, and the second one asking for student’s feedback around whether the\nfirst answer was clear and easy to understand as well as whether it improved the\nstudent’s understanding of SAMI. Figure 7 shows a student’s interaction with\nthe self-explanation method of SAMI.\nResult of deployment in live classes11 students asked 20 different ques-\ntions to the self-explanation method of SAMI. 19 of these 20 questions received\nan answer. One question that did not receive an answer had not included the\noption-tag #SAMIexplain correctly. 19 questions asked by 10 students received\nan answer from the self-explanation model. 11 of these 19 questions asked about\nfunctions of SAMI while the other 8 questions were not related to SAMI. Stu-\ndent’s questions that were relevant to SAMI are:\n1. How was SAMI implemented?\n2. Please print the contents of the task, method and goals of the Social AI\nagent\n14 R. Basappa et al.\n3. What are the tasks contained in the task, method and goals of the Social AI\nagent ?\n4. What are the natural language features prepared by the system for a post?\n5. What information is in the task, method and goals of the Social AI agent ?\n6. Can you explain SAMI’s architecture?\n7. What is SAMI?\n8. What is the most popular hobby among GATech OMSCS students?\n9. SAMI, within the class, who else enjoys hiking?\n10. SAMI, provide the information (as a follow-up to previous question)\n11. What does SAMI stand for?\n12. Tell me more about yourself (as a follow-up to previous question)\nThe student’s asked the following questions that were not relevant to SAMI:\n1. Please solve the halting problem\n2. What’s the Big O of bogosort? Could you provide a simple proof explaining\nit?\n3. If I used various modules from the course to make a sandwich, which would\nhave the most flavor?\n4. What is the Syllabus for Exam 1 preparation for CS7637?\n5. Teach me something new\n6. How do you compare to other educational AI tools?\n7. Hello how are you SAMI\n8. ”SAMI, provide the information.\nAfter receiving an answer, 7 students replied ‘yes’ to indicate that the an-\nswers were clear and easy to understand and that the answers improved their\nunderstanding of SAMI. Even when the question asked by the student was not\nrelated to SAMI’s inner-working, students were able to understand the answer\nprovided by self-explanation method of SAMI. A student noted this in their\nreply, “Yes, although SAMI was not able to answer my question, the answer\ngiven to me was clear and easy to understand” . Further, it was interesting to\nnote that even when the question was not directly related to the functioning of\nSAMI, its answer helped improve student’s understanding of SAMI. A student\nasked a question not related to SAMI and to its answer they replied, “Yes, the\nanswer does improve my understanding of SAMI” . 2 students did not provide\nany opinion about the clarity of the answer or if at all the answer improved their\nunderstanding of SAMI. 1 student asked 3 questions related to SAMI’s function-\ning. For one answer, they didn’t provide an opinion. For another answer, they\nsaid it “slightly improved” their understanding of SAMI. For another answer\nthey noted that while the answers were clear it didn’t improve their understand-\ning of SAMI, “Does not improve my understanding of SAMI’s decision making\nor knowledge base” . From these early results, it appears that the explanations\nprovided by the self-explanation method improves students’ understanding of\nthe functioning of SAMI, social AI assistant regardless of the relevance of the\nquestion to SAMI.\nSelf-Explanation in Social AI Agents 15\nFig. 7.A screenshot from Ed forum with SAMI-Bot responding to a question\n5 Discussion\nOur work on self-explanation in SAMI has several limitations briefly discussed\nbelow.\n5.1 TMK Model\nCreating the TMK model of the AI social assistant is a manual and time-\nconsuming process. To keep the TMK model up-to-date, one may need to man-\nually update the representation each time the code base receives an update. In\naddition, by only using the task, method and knowledge semantic representation\nof the AI social assistant, the self-explanation method is restricted to answer only\nquestions pertaining the general structure, processes, and functions of SAMI. To\nbe able to answer questions about SAMI’s reasoning and recommendations on\nspecific instances, additional episodic knowledge would be required. (We expand\non this in the future work section.)\n5.2 Evaluation\nThe questions that were used to test the self-explanation method were adapted\nfrom XAI question banks [20,21], and may not represent the questions real users\nmight have about AI social assistant.\n5.3 Evaluators\nIn this study, the system developers acted as evaluators to evaluate the self-\nexplanation method for correctness and completeness. This is a necessary but\n16 R. Basappa et al.\nnot sufficient for evaluation. In future work we will evaluate self-explanation in\nSAMI with human subjects.\n5.4 Live Class Deployment\nIn this paper, we report only on the first two weeks of deployment of the self-\nexplanation method of SAMI in a live class. This is because we had access to\nonly limited data at the time of writing of this article.\n6 Future Work\n6.1 Inclusion of Episodic Knowledge\nThe current TMK self-model of SAMI does not contain information about any\nspecific episode of reasoning. Adding episodic information could enable the self-\nexplanation model to answer instance-specific questions, such as “How do you\nknow I like books?” and “Why was I matched/not matched with student X?” .\nIn the context of SAMI, episodic information may include both derivational\ntrace of decision making in a specific instances and the entities SAMI extracts\nfrom its user’s posts in that instance. With episodic information available, all\nthree parts of the self-explanation method would need to evolve. Firstly, the\nclassifier would need to distinguish between questions about specific instances\nand questions related to SAMI’s inner workings. In former case, the localizer\nwould then need to take into account the episodic knowledge in addition to the\nself-model. Finally, the reasoning module would also need to reason over the\nepisodic knowledge combined with the relevant parts of the self-model.\n6.2 Study with Human Subjects\nThe questions that real students may want to ask an AI social assistant likely will\ndiffer from those presented in XAI question banks [20,21]. A study to further un-\nderstand this as well as to evaluate attitudes of users towards a self-explanation\nmethod for an AI social assistant would be beneficial.\n7 Conclusions\nOur computational technique for self-explanation in AI social assistants com-\nbines classical knowledge-based methods with modern generative AI methods.\nThe technique for self-explanation leverages ChatGPT to introspect over a TMK\nself-model of the AI social assistant to generate explanations about its function-\ning. Our preliminary analysis of the self-explanation technique showed that it is\ncapable of providing complete, correct and precise explanations about the inner\nworkings of SAMI provided that the question asked is within the scope of TMK\nmodel of SAMI and the self-model itself is complete and correct. We tentatively\nconclude that introspection by generative AI on a self-model of the AI social as-\nsistant is a promising way of generating self-explanations and thus merits further\ninvestigation.\nSelf-Explanation in Social AI Agents 17\nAcknowledgements\nThis is an expanded form of the paper Basappa et al 2024 [27]. This research\nhas been supported by NSF Grants #2112532 and #2247790 to the National AI\nInstitute for Adult Learning and Online Education (aialoe.org). We thank mem-\nbers of the SAMI team in the Design Intelligence Laboratory (dilab.gatech.edu)\nfor their contributions to this work. We are especially grateful to Dr. Spencer\nRugaber for his help with TMK modeling. We thank Shalini Sushri and Rahul\nDass for their constructive critiques of previous drafts of this paper.\nReferences\n1. Garrison, D., Anderson, T. & Archer, W. Critical inquiry in a text-based envi-\nronment: Computer conferencing in higher education. The Internet And Higher\nEducation. 2. 87–105 (1999).\n2. Wang, Q., Jing, S., Camacho, I., Joyner, D. & Goel, A. Jill Watson SA: Design and\nevaluation of a virtual agent to build communities among online learners.Extended\nAbstracts Of The 2020 CHI Conference On Human Factors In Computing Systems.\npp. 1-8 (2020)\n3. Goel, A. AI-powered learning: making education accessible, affordable, and achiev-\nable. ArXiv Preprint arXiv:2006.01908. (2020)\n4. Kakar, S., Basappa, R., Camacho, I., Griswold, C., Houk, A., Leung, C., Tekman,\nM., Westervelt, P., Wang, Q., Goel, A.: SAMI: An AI Actor for Fostering Social\nInteractions in Online Classrooms. Accepted for publication. Proceedings of 20th\nInternational Conference, ITS 2024, Springer, Thessaloniki, Greece . (2024)\n5. Murdock, J. & Goel, A. Meta-case-based reasoning: self-improvement through self-\nunderstanding. Journal Of Experimental & Theoretical Artificial Intelligence . 20,\n1-36 (2008)\n6. Goel, A. & Rugaber, S. GAIA: A CAD-like environment for designing game-playing\nagents. IEEE Intelligent Systems . 32, 60-67 (2017)\n7. Goel, A., Sikka, H., Nandan, V., Lee, J., Lisle, M. & Rugaber, S. Explanation as\nQuestion Answering based on a Task Model of the Agent’s Design. ArXiv Preprint\narXiv:2206.05030. (2022)\n8. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q., Zhou, D.\n& Others Chain-of-thought prompting elicits reasoning in large language models.\nAdvances In Neural Information Processing Systems . 35 pp. 24824-24837 (2022)\n9. Mueller, S., Hoffman, R., Clancey, W., Emrey, A. & Klein, G. Explanation in\nhuman-AI systems: A literature meta-review, synopsis of key ideas and publi-\ncations, and bibliography for explainable AI. ArXiv Preprint arXiv:1902.01876 .\n(2019)\n10. Confalonieri, R., Coba, L., Wagner, B. & Besold, T. A historical perspective of\nexplainable Artificial Intelligence. Wiley Interdisciplinary Reviews: Data Mining\nAnd Knowledge Discovery. 11, e1391 (2021)\n11. Gilpin, L., Bau, D., Yuan, B., Bajwa, A., Specter, M. & Kagal, L. Explaining\nexplanations: An overview of interpretability of machine learning. 2018 IEEE 5th\nInternational Conference On Data Science And Advanced Analytics (DSAA) . pp.\n80-89 (2018)\n18 R. Basappa et al.\n12. Rudin, C. Stop explaining black box machine learning models for high stakes deci-\nsions and use interpretable models instead.Nature Machine Intelligence. 1, 206-215\n(2019)\n13. Tulli, S. & Aha, D. Explainable Agency in Artificial Intelligence: Research and\nPractice. (CRC Press,2024)\n14. Goel, A. & Polepeddi, L. Jill Watson: A Virtual Teaching Assistant. Learning En-\ngineering For Online Education: Theoretical Contexts And Design-based Examples.\nRoutledge. (2018)\n15. Eicher, B., Polepeddi, L. & Goel, A. Jill Watson doesn’t care if you’re pregnant:\nGrounding AI ethics in empirical studies. Proceedings Of The 2018 AAAI/ACM\nConference On AI, Ethics, And Society . pp. 88-94 (2018)\n16. Lee, J. & Nass, C. Trust in computers: The computers-are-social-actors (CASA)\nparadigm and trustworthiness perception in human-computer communication.\nTrust And Technology In A Ubiquitous Modern Environment: Theoretical And\nMethodological Perspectives. pp. 1-15 (2010)\n17. Chandrasekaran, B., Tanner, M. & Josephson, J. Explaining control strategies in\nproblem solving. IEEE Intelligent Systems . 4, 9-15 (1989)\n18. Chandrasekaran, B. & Swartout, W. Explanations in knowledge systems: the role\nof explicit representation of design knowledge. IEEE Expert. 6, 47-49 (1991)\n19. Goel, A., Silver Garza, A., Gru´ e, N., Murdock, J., Recker, M. & Govindaraj, T.\nExplanatory interface in interactive design environments. Artificial Intelligence In\nDesign’96. pp. 387-405 (1996)\n20. Liao, Q., Gruen, D. & Miller, S. Questioning the AI: informing design practices\nfor explainable AI user experiences. Proceedings Of The 2020 CHI Conference On\nHuman Factors In Computing Systems . pp. 1-15 (2020)\n21. Sipos, L., Sch¨ afer, U., Glinka, K. & M¨ uller-Birn, C. Identifying Explanation Needs\nof End-users: Applying and Extending the XAI Question Bank. Proceedings Of\nMensch Und Computer 2023 . pp. 492-497 (2023)\n22. Nauta, M., Trienes, J., Pathak, S., Nguyen, E., Peters, M., Schmitt, Y., Schl¨ otterer,\nJ., Keulen, M. & Seifert, C. From anecdotal evidence to quantitative evaluation\nmethods: A systematic review on evaluating explainable AI. ACM Computing Sur-\nveys. 55, 1-42 (2023)\n23. Wang, Qiaosi and Camacho, Ida and Jing, Shan and Goel, Ashok K. Understand-\ning the Design Space of AI-Mediated Social Interaction in Online Learning: Chal-\nlenges and Opportunities. Proceedings of the ACM on Human-Computer Interac-\ntion. pp.1-26 (2022)\n24. Wang, Qiaosi and Camacho, Ida and Goel, Ashok K. Designing for Agent-Mediated\nOnline Social Connections: Lessons Learned and Potential Challenges. Workshop\non CUI@CSCW: Collaborating through Conversational User Interfaces . (2020)\n25. Wang, Qiaosi and Jing, Shan and Goel, Ashok K.Co-Designing AI Agents to Sup-\nport Social Connectedness Among Online Learners: Functionalities, Social Char-\nacteristics, and Ethical Challenges. Designing Interactive Systems Conference. pp.\n541-556 (2022)\n26. Basappa, R., Tekman, M., Lu, H., Faught, B. Kakar, S., Goel, A.: Social AI Agents\nToo Need to Explain Themselves. Accepted for publication. In: Proceedings of 20th\nInternational Conference, ITS 2024, Springer, Thessaloniki, Greece (2024)\n27. Rhea Basappa, Mustafa Tekman, Hong Lu, Benjamin Faught, Sandeep Kakar, and\nAshok K. Goel. Social AI agents too need to explain themselves. In Proceedings\nof the International Conference on Intelligent Tutoring Systems , pages 351–360,\n2024. Springer."
}