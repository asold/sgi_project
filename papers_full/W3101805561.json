{
  "title": "A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder",
  "url": "https://openalex.org/W3101805561",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2953784871",
      "name": "Hengzhi Pei",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2116385994",
      "name": "Hang Yan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964194354",
    "https://openalex.org/W3094020375",
    "https://openalex.org/W2341695656",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964269722",
    "https://openalex.org/W2507296208",
    "https://openalex.org/W2250739653",
    "https://openalex.org/W2963355640",
    "https://openalex.org/W25062297",
    "https://openalex.org/W3015673211",
    "https://openalex.org/W2801805865",
    "https://openalex.org/W2772987967",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2899395607",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W2400932086",
    "https://openalex.org/W3088186095",
    "https://openalex.org/W2296194829",
    "https://openalex.org/W2952054097",
    "https://openalex.org/W2896649846",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2903674902",
    "https://openalex.org/W2962885853",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2251362855",
    "https://openalex.org/W2963997155",
    "https://openalex.org/W2741390671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2922018002"
  ],
  "abstract": "Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different tasks, which are learned together under the multi-task learning framework. In this paper, we propose a concise but effective unified model for MCCWS, which is fully-shared for all the criteria. By leveraging the powerful ability of the Transformer encoder, the proposed unified model can segment Chinese text according to a unique criterion-token indicating the output criterion. Besides, the proposed unified model can segment both simplified and traditional Chinese and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2887–2897\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n2887\nA Concise Model for Multi-Criteria Chinese Word Segmentation with\nTransformer Encoder\nXipeng Qiu∗, Hengzhi Pei, Hang Yan, Xuanjing Huang\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\nSchool of Computer Science, Fudan University\n{xpqiu, hzpei16, hyan19, xjhuang}@fudan.edu.cn\nAbstract\nMulti-criteria Chinese word segmentation\n(MCCWS) aims to exploit the relations among\nthe multiple heterogeneous segmentation cri-\nteria and further improve the performance of\neach single criterion. Previous work usually\nregards MCCWS as different tasks, which are\nlearned together under the multi-task learn-\ning framework. In this paper, we propose a\nconcise but effective uniﬁed model for MC-\nCWS, which is fully-shared for all the cri-\nteria. By leveraging the powerful ability of\nthe Transformer encoder, the proposed uniﬁed\nmodel can segment Chinese text according to\na unique criterion-token indicating the output\ncriterion. Besides, the proposed uniﬁed model\ncan segment both simpliﬁed and traditional\nChinese and has an excellent transfer capabil-\nity. Experiments on eight datasets with differ-\nent criteria show that our model outperforms\nour single-criterion baseline model and other\nmulti-criteria models. Source codes of this pa-\nper are available on Github1.\n1 Introduction\nChinese word segmentation (CWS) is a prelimi-\nnary step to process Chinese text. The mainstream\nCWS methods regard CWS as a character-based se-\nquence labeling problem, in which each character\nis assigned a label to indicate its boundary infor-\nmation. Recently, various neural models have been\nexplored to reduce efforts of the feature engineer-\ning (Chen et al., 2015a,b; Qun et al., 2020; Wang\nand Xu, 2017; Kurita et al., 2017; Ma et al., 2018).\nRecently, Chen et al. (2017) proposed multi-\ncriteria Chinese word segmentation (MCCWS)\nto effectively utilize the heterogeneous resources\nwith different segmentation criteria. Speciﬁcally,\nthey regard each segmentation criterion as a single\n∗Corresponding author.\n1https://github.com/acphile/MCCWS\nCorpora Lin Dan won the championship\nCTB 林丹 赢得 总冠军\nPKU 林 丹 赢得 总 冠军\nMSRA 林丹 赢得 总 冠军\nTable 1: Illustration of different segmentation criteria.\ntask under the framework of multi-task learning,\nwhere a shared layer is used to extract the criteria-\ninvariant features, and a private layer is used to\nextract the criteria-speciﬁc features.\nHowever, it is unnecessary to use a speciﬁc pri-\nvate layer for each criterion. These different criteria\noften have partial overlaps. For the example in Ta-\nble 1, the segmentation of “林丹(Lin Dan)” is the\nsame in CTB and MSRA criteria, and the segmen-\ntation of “总|冠军(the championship)” is the same\nin PKU and MSRA criteria. All these three crite-\nria have the same segmentation for the word “ 赢\n得(won)”. Although these criteria are inconsistent,\nthey share some partial segmentation. Therefore,\nit is interesting to use a uniﬁed model for all the\ncriteria. At the inference phase, a criterion-token is\ntaken as input to indicate the predict segmentation\ncriterion. Following this idea, Gong et al. (2018)\nused multiple LSTMs and a criterion switcher at\nevery position to automatically switch the routing\namong these LSTMs. He et al. (2019) used a shared\nBiLSTM to deal with all the criteria by adding two\nartiﬁcial tokens at the beginning and end of an input\nsentence to specify the target criterion. However,\ndue to the long-range dependency problem, BiL-\nSTM is hard to carry the criterion information to\neach character in a long sentence.\nIn this work, we propose a concise uniﬁed model\nfor MCCWS task by integrating shared knowledge\nfrom multiple segmentation criteria. Inspired by\nthe success of the Transformer (Vaswani et al.,\n2017), we design a fully shared architecture for\nMCCWS, where a shared Transformer encoder is\n2888\n[CTB] 林 丹 赢 得 总 冠 军\nB E B E B M E\nUnified Model\n(a) CTB\n[PKU] 林 丹 赢 得 总 冠 军\nS S B E S B E\nUnified Model\n(b) PKU\nFigure 1: Uniﬁed model for MCCWS. “[ ·]” is a spe-\ncial token indicating the output criterion. The label\n{B,M,E,S }of each character indicates it is the begin,\nmiddle, end of a word, or a word with single character.\nused to extract the criteria-aware contextual fea-\ntures, and a shared decoder is used to predict the\ncriteria-speciﬁc labels. An artiﬁcial token is added\nat the beginning of the input sentence to deter-\nmine the output criterion. The similar idea is also\nused in the ﬁeld of machine translation, Johnson\net al. (2017) used a single model to translate be-\ntween multiple languages. Figure 1 illustrates our\nmodel. There are two reasons to use the Trans-\nformer encoder for MCCWS. The primary reason\nis its neatness and ingenious simplicity to model\nthe criterion-aware context representation for each\ncharacter. Since the Transformer encoder uses self-\nattention mechanism to capture the interaction each\ntwo tokens in a sentence, each character can im-\nmediately perceive the information of the criterion-\ntoken as well as the context information. The sec-\nondary reason is that the Transformer encoder has\npotential advantages in capturing the long-range\ncontext information and having a better parallel\nefﬁciency than the popular LSTM-based encoders.\nFinally, we exploit the eight segmentation criteria\non the ﬁve simpliﬁed Chinese and three traditional\nChinese corpora. Experiments show that the pro-\nposed model is effective in improving the perfor-\nmance of MCCWS.\nThe contributions of this paper could be summa-\nrized as follows.\n• We proposed a concise uniﬁed model for MC-\nCWS based on Transformer encoder, which\nadopts a single fully-shared model to segment\nsentences with a given target criterion. It is\nattractive in practice to use a single model to\nproduce multiple outputs with different crite-\nria.\n• By a thorough investigation, we show the fea-\nsibility of using a uniﬁed CWS model to seg-\nment both simpliﬁed and traditional Chinese\n(see Sec. 4.3). We think it is a promising\ndirection for CWS to exploit the collective\nknowledge of these two kinds of Chinese.\n• The learned criterion embeddings reﬂect the\nrelations between different criteria, which\nmake our model have better transfer capability\nto a new criterion (see Sec. 4.4) just by ﬁnd-\ning a new criterion embedding in the latent\nsemantic space.\n• It is a ﬁrst attempt to train the Transformer\nencoder from scratch for CWS task. Although\nwe mainly address its conciseness and suit-\nability for MCCWS in this paper and do not\nintend to optimize a speciﬁc Transformer en-\ncoder for the single-criterion CWS (SCCWS),\nwe prove that the Transformer encoder is also\nvalid for SCCWS. The potential advantages\nof the Transformer encoder are that it can ef-\nfectively extract the long-range interactions\namong characters and has a better parallel abil-\nity than LSTM-based encoders.\n2 Background\nIn this section, we ﬁrst brieﬂy describe the back-\nground knowledge of our work.\n2.1 Neural Architecture for CWS\nUsually, CWS task could be viewed as a character-\nbased sequence labeling problem. Speciﬁcally,\neach character in a sentence X = {x1,...,x T}is\nlabelled as one of y∈L = {B,M,E,S }, indicat-\ning the begin, middle, end of a word, or a word with\nsingle character. The aim of CWS task is to ﬁgure\nout the ground truth of labels Y∗= {y∗\n1,...,y ∗\nT}:\nY∗= arg max\nY∈LT\np(Y|X). (1)\nRecently, various neural models have been\nwidely used in CWS and can effectively reduce\nthe efforts of feature engineering. The modern ar-\nchitecture of neural CWS usually consists of three\ncomponents:\nEmbedding Layer: In neural models, the ﬁrst\nstep is to map discrete language symbols into dis-\ntributed embedding space. Formally, each char-\nacter xt is mapped as ext ∈Rde, where de is a\nhyper-parameter indicating the size of character\nembedding.\n2889\nX\nPrivate\nEncoder\nPrivate\nDecoder\nY\n(a) SCCWS\nX\nPrivate\nEncoder\nShared\nEncoder\nPrivate\nDecoder\nY (b) MTL-based MCCWS\nm X\nShared\nEncoder\nShared\nDecoder\nY (c) Uniﬁed MCCWS\nFigure 2: Architectures of SCCWS and MCCWS. The shaded components are shared for different criteria.\nEncoding Layer: The encoding layer is to ex-\ntract the contextual features for each character.\nFor example, a prevalent choice for the encod-\ning layer is the bi-directional LSTM (BiLSTM)\n(Hochreiter and Schmidhuber, 1997), which could\nincorporate information from both sides of se-\nquence.\nht = BiLSTM(ext ,− →h t−1,← −h t+1,θe), (2)\nwhere − →h t and ← −h t are the hidden states at step tof\nthe forward and backward LSTMs respectively, θe\ndenotes all the parameters in the BiLSTM layer.\nBesides BiLSTM, CNN is also alternatively used\nto extract features.\nDecoding Layer: The extracted features are\nthen sent to conditional random ﬁelds (CRF) (Laf-\nferty et al., 2001) layer or multi-layer perceptron\n(MLP) for tag inference.\nWhen using CRF as decoding layer, p(Y|X) in\nEq (1) could be formalized as:\np(Y|X) = Ψ(Y|X)∑\nY′∈Ln Ψ(Y′|X), (3)\nwhere Ψ(Y|X) is the potential function. In ﬁrst\norder linear chain CRF, we have:\nΨ(Y|X) =\nn∏\nt=2\nψ(X,t,y t−1,yt), (4)\nψ(x,t,y ′,y) = exp(δ(X,t)y + by′y), (5)\nwhere by′y ∈R is trainable parameters respective\nto label pair (y′,y), score function δ(X,t) ∈R|L|\ncalculates scores of each label for tagging the t-th\ncharacter:\nδ(X,t) = W⊤\nδ ht + bδ, (6)\nwhere ht is the hidden state of encoder at step\nt, Wδ ∈ Rdh×|L| and bδ ∈ R|L| are trainable\nparameters.\nWhen using MLP as decoding layer, p(Y|X) in\nEq (1) is directly predicted by a MLP with softmax\nfunction as output layer.\np(yt|X) = MLP(ht,θd), ∀t∈[1,T] (7)\nwhere θd denotes all the parameters in MLP layer.\nMost current state-of-the-art CWS models (Chen\net al., 2015a; Xu and Sun, 2016; Liu et al., 2016;\nYang et al., 2018; Qun et al., 2020) mainly focus on\nsingle-criterion CWS (SCCWS). Figure 2a shows\nthe architecture of SCCWS.\n2.2 MCCWS with Multi-Task Learning\nTo improve the performance of CWS by exploiting\nmultiple heterogeneous criteria corpora, Chen et al.\n(2017) utilize the multi-task learning framework to\nmodel the shared information among these different\ncriteria.\nFormally, assuming that there are M corpora\nwith heterogeneous segmentation criteria, we refer\nDm as corpus mwith Nm samples:\nDm = {(X(m)\nn ,Y (m)\nn )}Nm\nn=1, (8)\nwhere X(m)\nn and Y(m)\nn denote the n-th sentence and\nthe corresponding label in corpus mrespectively.\nThe encoding layer introduces a shared encoder\nto mine the common knowledge across multiple\ncorpora, together with the original private encoder.\nThe architecture of MTL-based MCCWS is shown\nin Figure 2b.\nConcretely, for corpusm, a shared encoder and a\nprivate encoder are ﬁrst used to extract the criterion-\nagnostic and criterion-speciﬁc features.\nH(s) =encs(eX; θ(s)\ne ), (9)\nH(m) =encm(eX; θ(m)\ne ), ∀m∈[1,M] (10)\nwhere eX = {ex1,··· ,exT}denotes the embed-\ndings of the input characters x1,··· ,xT, encs(·)\nrepresents the shared encoder and encm(·) repre-\nsents the private encoder for corpus m; θ(s)\ne and\n2890\nθ(m)\ne are the shared and private parameters respec-\ntively. The shared and private encoders are usually\nimplemented by the RNN or CNN network.\nThen a private decoder is used to predict\ncriterion-speciﬁc labels. For the m-th corpus, the\nprobability of output labels is\npm(Y|X) = decm([H(s); H(m)]; θ(m)\nd ), (11)\nwhere decm(·) is a private CRF or MLP decoder\nfor corpus m(m∈[1,M]), taking the shared and\nprivate features as inputs; θ(m)\nd is the parameters of\nthe m-th private decoder.\nObjective The objective is to maximize the log\nlikelihood of true labels on all the corpora:\nJseg(Θm,Θs) =\nM∑\nm=1\nNm∑\nn=1\nlog pm(Y(m)\nn |X(m)\nn ; Θm,Θs),\n(12)\nwhere Θm = {θ(m)\ne ,θ(m)\nd }and Θs = {E,θ(s)\ne }de-\nnote all the private and shared parameters respec-\ntively; E is the embedding matrix.\n3 Proposed Uniﬁed Model\nIn this work, we propose a more concise architec-\nture for MCCWS, which adopts the Transformer\nencoder (Vaswani et al., 2017) to extract the con-\ntextual features for each input character. In our pro-\nposed architecture, both the encoder and decoder\nare shared by all the criteria. The only difference\nfor each criterion is that a unique token is taken as\ninput to specify the target criterion, which makes\nthe shared encoder to capture the criterion-aware\nrepresentation. Figure 2 illustrates the difference\nbetween our proposed model and the previous mod-\nels. A more detailed architecture for MCCWS is\nshown in Figure 3.\n3.1 Embedding Layer\nGiven a sentence X = {x1,...,x T}, we ﬁrst map\nit into a vector sequence where each token is a\ndmodel dimensional vector. Besides the standard\ncharacter embedding, we introduce three extra em-\nbeddings: criterion embedding, bigram embedding,\nand position embedding.\n1) Criterion Embedding: Firstly, we add a\nunique criterion-token at the beginning of X to\nindicate the output criterion. For the m-th criterion,\nthe criterion-token is [m]. We use e[m] to denote its\nembedding. Thus, the model can learn the relations\nm x1 x2 x3 x4 Input\nEmbedding\nEncoder\nDecoder\nOutput\nh0 h1 h2 h3 h4\n˜h0 ˜h1 ˜h2 ˜h3 ˜h4\nT ransformer Encoder\nh0 h1 h2 h3 h4\n˜h0 ˜h1 ˜h2 ˜h3 ˜h4\ny1 y2 y3 y4\nCRF/MLP\nFigure 3: Proposed Model for MCCWS.\nbetween different criteria in the latent embedding\nspace.\n2) Bigram Embedding: Based on (Chen et al.,\n2015b; Shao et al., 2017; Zhang et al., 2018), the\ncharacter-level bigram features can signiﬁcantly\nbeneﬁt the task of CWS. Following their settings,\nwe also introduce the bigram embedding to aug-\nment the character-level unigram embedding. The\nrepresentation of character xt is\ne′\nxt = FC(ext ⊕ext−1xt ⊕extxt+1), (13)\nwhere e denotes the d-dimensional embedding vec-\ntor for the unigram and bigram, ⊕is the con-\ncatenation operator, and FC is a fully connected\nlayer to map the concatenated character embed-\nding with the dimension 3d into the embedding\ne′\nxt ∈Rdmodel.\n3) Position Embedding: To capture the order\ninformation of a sequence, a position embedding\nPE is used for each position. The position embed-\nding can be learnable parameters or predeﬁned. In\nthis work, we use the predeﬁned position embed-\nding following (Vaswani et al., 2017). For the t-th\ncharacter in a sentence, its position embedding is\ndeﬁned by\nPEt,2i = sin(t/100002i/dmodel), (14)\nPEt,2i+1 = cos(t/100002i/dmodel), (15)\nwhere idenotes the dimensional index of position\nembedding.\nFinally, the embedding matrix of the sequence\nX = {x1,··· ,xT}with criterion mis formulated\nas\nH = [e[m] + PE0; e′\nx1 + PE1; ··· ; e′\nxT + PET], (16)\nwhere H ∈R(T+1)×dmodel, (T + 1) and dmodel\nrepresent the length and the dimension of the input\nvector sequence.\n2891\n3.2 Encoding Layer\nIn sequence modeling, RNN and CNN often suf-\nfer from the long-term dependency problem and\ncannot effectively extract the non-local interac-\ntions in a sentence. Recently, the fully-connected\nself-attention architecture, such as Transformer\n(Vaswani et al., 2017), achieves great success in\nmany NLP tasks.\nIn this work, we adopt the Transformer encoder\nas our encoding layer, in which several multi-head\nself-attention layers are used to extract the contex-\ntual feature for each character.\nGiven a sequence of vectors H ∈\nR(T+1)×dmodel, a single-head self-attention\nprojects H into three different matrices: the\nquery matrix Q ∈ R(T+1)×dk, the key ma-\ntrix K ∈ R(T+1)×dk and the value matrix\nV ∈ R(T+1)×dv, and uses scaled dot-product\nattention to get the output representation.\nQ,K,V = HWQ,HW K,HW V (17)\nAttn(Q,K,V ) = softmax(QKT\n√dk\n)V, (18)\nwhere the matrices WQ ∈ Rdmodel×dk,WK ∈\nRdmodel×dk,WV ∈Rdmodel×dv are learnable pa-\nrameters and softmax(·) is performed row-wise.\nThe Transformer encoder consists of several\nstacked multi-head self-attention layers and fully-\nconnected layers. Assuming the input of the multi-\nhead self-attention layer is H, its output ˜H is cal-\nculated by\nZ =layer-norm\n(\nH+ MultiHead(H)\n)\n, (19)\n˜H =layer-norm\n(\nZ+ FFN(Z)\n)\n, (20)\nwhere layer-norm(·) represents the layer normal-\nization (Ba et al., 2016) .\nAll the tasks with the different criteria use the\nsame encoder. Nevertheless, with the different\ncriterion-token [m], the encoder can effectively ex-\ntract the criterion-aware representation for each\ncharacter.\n3.3 Decoding Layer\nIn the standard multi-task learning framework, each\ntask has its private decoder to predict the task-\nspeciﬁc labels. Different from the previous work,\nwe use a shared decoder for all the tasks since we\nhave extracted the criterion-aware representation\nfor each character. In this work, we use CRF as the\ndecoder since it is slightly better than MLP (see\nSec. 4.2).\nWith the fully-shared encoder and decoder, our\nmodel is more concise than the shared-private ar-\nchitectures (Chen et al., 2017; Huang et al., 2019).\n4 Experiments\nDatasets We use eight CWS datasets from\nSIGHAN2005 (Emerson, 2005) and SIGHAN2008\n(Jin and Chen, 2008). Among them, the AS,\nCITYU, and CKIP datasets are in traditional Chi-\nnese, while the MSRA, PKU, CTB, NCC, and\nSXU datasets are in simpliﬁed Chinese. Except\nwhere otherwise stated, we follow the setting of\n(Chen et al., 2017; Gong et al., 2018), and translate\nthe AS, CITYU and CKIP datasets into simpliﬁed\nChinese. We do not balance the datasets and ran-\ndomly pick 10% examples from the training set\nas the development set for all datasets. Similar to\nthe previous work (Chen et al., 2017), we prepro-\ncess all the datasets by replacing the continuous\nLatin characters and digits with a unique token,\nand converting all digits, punctuation and Latin let-\nters to half-width to deal with the full/half-width\nmismatch between training and test set.\nWe have checked the annotation schemes of dif-\nferent datasets, which are just partially shared and\nno two datasets have the same scheme. According\nto our statistic, the averaged overlap is about 20.5%\nfor 3-gram and 4.4% for 5-gram.\nTable 2 gives the details of the eight datasets\nafter preprocessing. For training and development\nsets, lines are split into shorter sentences or clauses\nby punctuations, in order to make a faster batch.\nPre-trained Embedding Based on on (Chen\net al., 2015b; Shao et al., 2017; Zhang et al., 2018),\nn-gram features are of great beneﬁt to Chinese\nword segmentation and POS tagging tasks. Thus\nwe use unigram and bigram embeddings for our\nmodels. We ﬁrst pre-train unigram and bigram\nembeddings on Chinese Wikipedia corpus by the\nmethod proposed in (Ling et al., 2015), which im-\nproves standard word2vec by incorporating token\norder information.\nHyper-parameters We use Adam opti-\nmizer (Kingma and Ba, 2014) with the same\nwarmup strategy as (Vaswani et al., 2017). The\ndevelopment set is used for parameter tuning. All\nthe models are trained for 100 epochs. Pre-trained\nembeddings are ﬁxed for the ﬁrst 80 epochs and\nthen updated during the following epochs. After\n2892\nTable 2: Details of the eight datasets after preprocessing. “Word Types” represents the number of unique word.\n“Char Types” is the number of unique characters. “OOV Rate” is Out-Of-V obulary rate.\nCorpora Words# Chars# Word Types Char Types OOV\nSighan05\nMSRA\nTrain 2.4M 4.0M 75.4K 5.1K\nTest 0.1M 0.2M 11.9K 2.8K 1.32%\nAS\nTrain 5.4M 8.3M 128.8K 5.8K\nTest 0.1M 0.2M 18.0K 3.4K 2.20%\nPKU\nTrain 1.1M 1.8M 51.2K 4.6K\nTest 0.1M 0.2M 12.5K 2.9K 2.06%\nCITYU\nTrain 1.1M 1.8M 43.4K 4.2K\nTest 0.2M 0.4M 23.2K 3.6K 3.69%\nSighan08\nCTB\nTrain 0.6M 1.0M 40.5K 4.2K\nTest 0.1M 0.1M 11.9K 2.9K 3.80%\nCKIP\nTrain 0.7M 1.1M 44.7K 4.5K\nTest 0.1M 0.1M 14.2K 3.1K 4.29%\nNCC\nTrain 0.9M 1.4M 53.3K 5.3K\nTest 0.2M 0.2M 20.9K 3.9K 3.31%\nSXU\nTrain 0.5M 0.8M 29.8K 4.1K\nTest 0.1M 0.2M 11.6K 2.8K 2.60%\nEmbedding Size d 100\nHidden State Size dmodel 256\nTransformer Encoder Layers 6\nAttention Heads 4\nBatch Size 256\nDropout Ratio 0.2\nWarmup Steps 4000\nTable 3: Hyper-Parameter Settings\neach training epoch, we test the model on the dev\nset, and models with the highest F1 in the dev set\nare used in the test set. Table 3 shows the detailed\nhyperparameters.\n4.1 Overall Results\nTable 4 shows the experiment results of the pro-\nposed model on test sets of eight CWS datasets.\nWe ﬁrst compare our Transformer encoder with\nthe previous models in the single-criterion sce-\nnario. The comparison is presented in the upper\nblock of Table 4. Since Switch-LSTMs (Gong\net al., 2018) is designed form MCCWS, it is just\nslight better than BiLSTM in single-criterion sce-\nnario. Compared to the LSTM-based encoders, the\nTransformer encoder brings a noticeable improve-\nment compared to (Chen et al., 2017; Gong et al.,\n2018), and gives a comparable performance to (Ma\net al., 2018). In this work, we do not intend to\nprove the superiority of the Transformer encoder\nover LSTM-based encoders in the single-criterion\nscenario. Our purpose is to build a concise uniﬁed\nmodel based on Transformer encoder for MCCWS.\nIn the multi-criteria scenario, we compare our\nuniﬁed model with the BiLSTM (Chen et al., 2017)\nand Switch-LSTMs (Gong et al., 2018). The lower\nblock of Table 4 displays the contrast. Firstly, al-\nthough different criteria are trained together, our\nuniﬁed model achieves better performance besides\nCTB. Compared to the single-criterion scenario,\n0.42 gain in average F1 score is obtained by\nthe multi-criteria scenario. Moreover, our uniﬁed\nmodel brings a signiﬁcant improvement of 5.05\nin OOV recall. Secondly, compared to previous\nMCCWS models, our uniﬁed model also achieves\nbetter average F1 score. Especially, our uniﬁed\nmodel signiﬁcantly outperforms the uniﬁed BiL-\nSTM (He et al., 2019), which indicates the Trans-\nformer encoder is more effective in carrying the\ncriterion information than BiLSTM. The reason\nis that the Transformer encoder can model the in-\nteraction of the criterion-token and each character\ndirectly, while BiLSTM needs to carry the crite-\nrion information step-by-step from the two ends\nto the middle of the input sentence. The criterion\ninformation could be lost for the long sentences.\nThere are about 200 sentences are shared by\nmore than one datasets with different segmentation\nschemes, but it is not much harder to correctly\nsegment them. Their F1 score is 96.84.\nFigure 4 visualizes the 2D PCA projection of the\nlearned embeddings of eight different criteria. Gen-\nerally, the eight criteria are mapped into dispersed\npoints in the embedding space, which indicates\n2893\nModels MSRA AS PKU CTB CKIP CITYU NCC SXU Avg.\nSingle-Criterion Models\nStacked BiLSTM (Ma et al., 2018) F 97.4 96.2 96.1 96.7 - 97.2 - - -\nBiLSTM (Chen et al., 2017) F 95.84 94.2 93.3 95.3 93.06 94.07 92.17 95.17 94.14\nSwitch-LSTMs (Gong et al., 2018) F 96.46 94.51 95.74 97.09 92.88 93.71 92.12 95.57 94.76\nTransformer Encoder F 98.07 96.06 96.39 96.41 95.66 96.32 95.57 97.08 96.45\nTransformer Encoder OOV 73.75 73.05 72.82 82.82 79.05 83.72 71.81 77.95 76.87\nMulti-Criteria Models\nBiLSTM (Chen et al., 2017) F 96.04 94.64 94.32 96.18 94.26 95.55 92.83 96.04 94.98\nSwitch-LSTMs (Gong et al., 2018) F 97.78 95.22 96.15 97.26 94.99 96.22 94.12 97.25 96.12\nUniﬁed BiLSTM (He et al., 2019) F 97.2 95.4 96.0 96.7 - 96.1 - 96.4 -\nOur Uniﬁed Model F 98.05 96.44 96.41 96.99 96.51 96.91 96.04 97.61 96.87\nOur Uniﬁed Model OOV 78.92 76.39 78.91 87 82.89 86.91 79.3 85.08 81.92\nTable 4: Overall results on eight CWS datasets. F and OOV indicate the F1 score and OOV recall, respectively.\nThe upper block consists of single-criterion models. Since Stacked BiLSTM (Ma et al., 2018) is a strong SOTA\nmodel, the other comparable CWS models are omitted for brevity. The lower block consists of multi-criteria\nmodels.\nModels MSRA AS PKU CTB CKIP CITYU NCC SXU Avg.\nUniﬁed Model 98.05 96.44 96.41 96.99 96.51 96.91 96.04 97.61 96.87\nw/o CRF 98.02 96.42 96.41 96.9 96.59 96.87 95.96 97.5 96.83\nw/o bigram 97.41 96 96.25 96.71 96 96.31 94.62 96.84 96.27\nw/o pre-trained emb. 97.51 96.06 96.02 96.47 96.22 95.99 94.82 96.76 96.23\nTable 5: Ablation experiments.\nthat each criterion is different from others. Among\nthem, MSRA is obviously different from others. A\npossible reason is that the named entity is regarded\nas a whole word in the MSRA criterion, which is\nsigniﬁcantly distinguishing with other criteria.\nFigure 4: Visualization of the criterion embeddings.\n4.2 Ablation Study\nTable 5 shows the effectiveness of each component\nin our model.\nThe ﬁrst ablation study is to verify the effective-\nness of the CRF decoder, which is popular in most\nCWS models. The comparison between the ﬁrst\ntwo lines indicates that with or without CRF does\nnot make much difference. Since a model with\nCRF takes a longer time to train and inference, we\nsuggest not to use CRF in Transformer encoder\nmodels in practice.\nThe other two ablation studies are to evaluate\nthe effect of the bigram feature and pre-trained\nembeddings. We can see that their effects vary in\ndifferent datasets. Some datasets are more sensitive\nto the bigram feature, while others are more sensi-\ntive to pre-trained embeddings. In terms of average\nperformance, the bigram feature and pre-trained\nembeddings are important and boost the perfor-\nmance considerably, but these two components do\nnot have a clear winner.\n4.3 Joint Training on both simpliﬁed and\nTraditional Corpora\nIn the above experiments, the traditional Chinese\ncorpora (AS, CITYU, and CKIP) are translated into\nsimpliﬁed Chinese. However, it might be more at-\ntractive to jointly train a uniﬁed model directly on\nthe mixed corpora of simpliﬁed and traditional Chi-\nnese without translation. As a reference, the single\nmodel has been used to translate between multi-\nple languages in the ﬁeld of machine translation\n(Johnson et al., 2017).\nTo thoroughly investigate the feasibility of this\nidea, we study four different settings to train our\nmodel on simpliﬁed and traditional Chinese cor-\npora.\n1. The ﬁrst setting (“8Simp”) is to translate all\n2894\nModels MSRA AS PKU CTB CKIP CITYU NCC SXU Avg. F1\n8Simp 98.05 96.44 96.41 96.99 96.51 96.91 96.04 97.61 96.87\n8Trad 97.98 96.39 96.49 96.99 96.49 96.86 95.98 97.48 96.83\n5Simp, 3Trad 98.03 96.52 96.6 96.94 96.38 96.8 96.02 97.55 96.86\n8 Simp, 8 Trad 98.04 96.41 96.43 96.99 96.54 96.85 96.08 97.52 96.86\nTable 6: Joint training on both the simpliﬁed and traditional Chinese corpus.\n苹果(apple) 蘋果(apple) 爱好(hobby) 愛好(hobby) 担心(worry) 擔心(worry)\n坚果(nut) 微軟(Microsoft) 热爱(love) 熱愛(love) 关心(care) 關心(care)\n谷歌(Google) 黃油(butter) 兴趣(interest) 爱好(hobby) 怀疑(doubt) 顧慮(misgiving)\n华为(Huawei) 現貨(goods in stock) 愛好(hobby) 興趣(interest) 顾虑(misgiving) 懷疑(doubt)\n黄油(butter) 果凍(jelly) 梦想(dream) 夢想(dream) 担忧(concern) 擔憂(concern)\n鲜果(fresh fruit) 京東(JD) 爱玩(Playful) 愛玩(playful) 责怪(blame) 憂慮(anxiety)\n微软(Microsoft) 賣家(seller) 痴迷(addict) 喜愛(adore) 伤心(sad) 責怪(blame)\n诺基(Nokia) 苹果(apple) 乐趣(pleasure) 習慣(habbit) 嫌弃(disfavour) 傷心(sad)\n蘋果(Apple) 售後(after-sales) 喜爱(adore) 樂趣(pleasure) 忧虑(anxiety) 担心(worry)\nTable 7: Qualitative analysis for the joint embedding space of simpliﬁed and traditional Chinese. Given the target\nbigram, we list its top 8 similar bigrams. The bigram with red color indicates it is traditional Chinese.\nthe corpora into simpliﬁed Chinese. For the\npre-trained embeddings, we use the simpliﬁed\nChinese Wikipedia dump to pre-train the un-\nigram and bigram embeddings. This way is\nthe same as the previous experiments.\n2. The second setting (“8Trad”) is to translate\nall the corpora into traditional Chinese. For\nthe pre-trained embeddings, we ﬁrst convert\nthe Wikipedia dump into traditional Chinese\ncharacters, then we use this converted corpus\nto pre-train unigram and bigram embeddings.\n3. The third setting (“5Simp, 3Trad”) is to keep\nthe original characters for ﬁve simpliﬁed Chi-\nnese corpora and three traditional Chinese cor-\npora without translation. The uniﬁed model\ncan take as input the simpliﬁed or traditional\nChinese sentences. In this way, we pre-train\nthe joint simpliﬁed and traditional Chinese\nembeddings in a joint embedding space. We\nmerge the Wikipedia corpora used in “8Trad”\nand “8Simp” to form a mixed corpus, which\ncontains both the simpliﬁed and traditional\nChinese characters. The unigram and bigram\nembeddings are pre-trained on this mixed cor-\npus.\n4. The last setting (“8Simp, 8Trad”) is to si-\nmultaneously train our model on both the\neight simpliﬁed Chinese corpora in “8Simp”\nand the eight traditional Chinese corpora in\n“8Trad”. The pre-trained word embeddings\nare the same as “5Simp, 3Trad”.\nTable 6 shows that there does not exist too much\ndifference between different settings. This inves-\ntigation indicates it is feasible to train a uniﬁed\nmodel directly on two kinds of Chinese characters.\nTo better understand the quality of the learned\njoint embedding space of the simpliﬁed and tradi-\ntional Chinese, we conduct a qualitative analysis\nto illustrate the most similar bigrams for a target\nbigram. Similar bigrams are retrieved based on\nthe cosine similarity calculated using the learned\nembeddings. As shown in Table 7, the traditional\nChinese bigrams are similar to their simpliﬁed Chi-\nnese counterparts, and vice versa. The results show\nthat the simpliﬁed and traditional Chinese bigrams\nare aligned well in the joint embedding space.\n4.4 Transfer Capability\nSince except for the criterion embedding, the other\nparts of the uniﬁed model are the same for differ-\nent criteria, we want to exploit whether a trained\nuniﬁed model can be transferred to a new criterion\nonly by learning a new criterion embedding with\nfew examples.\nWe use the leave-one-out strategy to evaluate the\ntransfer capability of our uniﬁed model. We ﬁrst\ntrain a model on seven datasets, then only learn\nthe new criterion embedding with a few training\ninstances from the left dataset. This scenario is\nalso discussed in (Gong et al., 2018), and Figure\n5 presents their and our outcomes (averaged F1\nscore). There are two observations: Firstly, for the\ndifferent number of samples, the transferred model\nalways largely outperforms the models learned\n2895\n200 400 600 800 1,000\n75\n80\n85\n90\n95\nNumber of Training Samples\nAveragedF1score\nOurs(trans)\nSwitch-LSTMs(trans)\nOurs\nSwitch-LSTMs\nFigure 5: Evaluation of the transfer capability. Switch-\nLSTMs and Ours are models trained on the given\ninstances from scratch. Switch-LSTMs (trans) and\nOurs(trans) are models learned in transfer fashion.\nfrom scratch. We believe this indicates that learn-\ning a new criterion embedding is an effective way\nto transfer a trained uniﬁed model to a new crite-\nrion. Secondly, our model also has superior trans-\nferability than Switch-LSTMs (Ours(trans) versus\nSwitch-LSTMs(trans)).\n5 Related Work\nThe previous work on the MCCWS can be catego-\nrized into two lines.\nOne line is multi-task based MCCWS. Chen et al.\n(2017) proposed a multi-criteria learning frame-\nwork for CWS, which uses a shared layer to extract\nthe common underlying features and a private layer\nfor each criterion to extract criteria-speciﬁc fea-\ntures. Huang et al. (2019) proposed a domain adap-\ntive segmenter to capture diverse criteria based on\nBidirectional Encoder Representations from Trans-\nformer (BERT) (Devlin et al., 2018).\nAnother line is uniﬁed MCCWS. Gong et al.\n(2018) presented Switch-LSTMs to segment sen-\ntences, which consists of several LSTM layers,\nand uses a criterion switcher at every position to\nchange the routing among these LSTMs automati-\ncally. However, the complexity of the model makes\nSwitch-LSTMs hard to be applied in practice. He\net al. (2019) used a shared BiLSTM by adding two\nartiﬁcial tokens at the beginning and end of an input\nsentence to specify the output criterion. However,\ndue to the long-range dependency problem, BiL-\nSTM is hard to carry the criterion information to\neach character in a long sentence.\nCompared to the above two uniﬁed models, we\nuse the Transformer encoder in our uniﬁed model,\nwhich can elegantly model the criterion-aware con-\ntext representation for each character. With the\nTransformer, we just need a special criterion-token\nto specify the output criterion. Each character can\ndirectly attend the criterion-token to be aware of the\ntarget criterion. Thus, we can use a single model\nto produce different segmented results for differ-\nent criteria. Different from (Huang et al., 2019),\nwhich uses the pre-trained Transformer BERT and\nseveral extra projection layers for different criteria,\nour model is a fully-shared and more concise.\n6 Conclusion and Future Work\nWe propose a concise uniﬁed model for MCCWS,\nwhich uses the Transformer encoder to extract\nthe criterion-aware representation according to a\nunique criterion-token. Experiments on eight cor-\npora show that our proposed model outperforms\nthe previous models and has a stronger transfer ca-\npability. The conciseness of our model makes it\neasy to be applied in practice.\nIn this work, we only adopt the vanilla Trans-\nformer encoder since we just want to utilize its self-\nattention mechanism to model the criterion-aware\ncontext representation for each character neatly.\nTherefore, it is promising for future work to look\nfor the more effective adapted Transformer encoder\nfor CWS task or to utilize the pre-trained models\n(Qiu et al., 2020), such as BERT-based MCCWS\n(Ke et al., 2020). Besides, we are also planning\nto incorporate other sequence labeling tasks into\nthe uniﬁed model, such as POS tagging and named\nentity recognition.\nAcknowledgements\nThis work was supported by the National Natural\nScience Foundation of China (No. 62022027 and\n61976056), Science and Technology on Parallel\nand Distributed Processing Laboratory (PDL).\nReferences\nLei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. CoRR, abs/1607.06450.\nXinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing\nHuang. 2015a. Gated recursive neural network for\nChinese word segmentation. In Proceedings of An-\nnual Meeting of the Association for Computational\nLinguistics.\nXinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,\nand Xuanjing Huang. 2015b. Long Short-Term\n2896\nMemory Neural Networks for Chinese Word Seg-\nmentation. In EMNLP, pages 1197–1206.\nXinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing\nHuang. 2017. Adversarial multi-criteria learning for\nChinese word segmentation. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 1193–1203.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nT. Emerson. 2005. The second international Chinese\nword segmentation bakeoff. In Proceedings of the\nFourth SIGHAN Workshop on Chinese Language\nProcessing, pages 123–133. Jeju Island, Korea.\nJingjing Gong, Xinchi Chen, Tao Gui, and Xipeng\nQiu. 2018. Switch-LSTMs for multi-criteria\nchinese word segmentation. arXiv preprint\narXiv:1812.08033.\nHan He, Lei Wu, Hua Yan, Zhimin Gao, Yi Feng, and\nGeorge Townsend. 2019. Effective neural solution\nfor multi-criteria word segmentation. In Smart Intel-\nligent Computing and Applications, pages 133–142.\nSpringer.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nWeipeng Huang, Xingyi Cheng, Kunlong Chen,\nTaifeng Wang, and Wei Chu. 2019. Toward\nfast and accurate neural chinese word segmenta-\ntion with multi-criteria learning. arXiv preprint\narXiv:1903.04190.\nG. Jin and X. Chen. 2008. The fourth international\nchinese language processing bakeoff: Chinese word\nsegmentation, named entity recognition and chinese\npos tagging. In Sixth SIGHAN Workshop on Chinese\nLanguage Processing, page 69.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nZhen Ke, Liang Shi, Erli Meng, Bin Wang, Xipeng Qiu,\nand Xuanjing Huang. 2020. Uniﬁed multi-criteria\nchinese word segmentation with bert. arXiv preprint\narXiv:2004.05808.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nShuhei Kurita, Daisuke Kawahara, and Sadao Kuro-\nhashi. 2017. Neural joint model for transition-based\nChinese syntactic analysis. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics, volume 1, pages 1204–1214.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning.\nWang Ling, Chris Dyer, Alan W Black, and Isabel\nTrancoso. 2015. Two/too simple adaptations of\nword2vec for syntax problems. In Proceedings of\nthe 2015 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1299–1304.\nYijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, and\nTing Liu. 2016. Exploring segment representations\nfor neural segmentation models. arXiv preprint\narXiv:1604.05499.\nJi Ma, Kuzman Ganchev, and David Weiss. 2018.\nState-of-the-art Chinese word segmentation with Bi-\nLSTMs. arXiv preprint arXiv:1808.06511.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nSCIENCE CHINA Technological Sciences.\nNuo Qun, Hang Yan, Xipeng Qiu, and Xuan-\njing Huang. 2020. Chinese word segmenta-\ntion via BiLSTM+Semi-CRF with relay node.\nJournal of Computer Science and Technology ,\n35(5):1115–1126.\nYan Shao, Christian Hardmeier, Jörg Tiedemann, and\nJoakim Nivre. 2017. Character-based joint segmen-\ntation and pos tagging for chinese using bidirec-\ntional rnn-crf. arXiv preprint arXiv:1704.01314.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nChunqi Wang and Bo Xu. 2017. Convolutional neu-\nral network with word embeddings for Chinese word\nsegmentation. arXiv preprint arXiv:1711.04411.\nJingjing Xu and Xu Sun. 2016. Dependency-based\ngated recursive neural network for chinese word seg-\nmentation. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 567–572.\nJie Yang, Yue Zhang, and Shuailong Liang. 2018. Sub-\nword encoding in lattice LSTM for Chinese word\nsegmentation. arXiv preprint arXiv:1810.12594.\n2897\nMeishan Zhang, Nan Yu, and Guohong Fu. 2018. A\nsimple and effective neural model for joint word seg-\nmentation and POS tagging. IEEE/ACM Transac-\ntions on Audio, Speech and Language Processing\n(TASLP), 26(9):1528–1538.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8402923941612244
    },
    {
      "name": "Encoder",
      "score": 0.7177808284759521
    },
    {
      "name": "Exploit",
      "score": 0.6651808023452759
    },
    {
      "name": "Segmentation",
      "score": 0.6515378355979919
    },
    {
      "name": "Transformer",
      "score": 0.6310815215110779
    },
    {
      "name": "Security token",
      "score": 0.627249538898468
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5736693143844604
    },
    {
      "name": "Text segmentation",
      "score": 0.5265805125236511
    },
    {
      "name": "Unified Model",
      "score": 0.451347291469574
    },
    {
      "name": "Word (group theory)",
      "score": 0.424416184425354
    },
    {
      "name": "Language model",
      "score": 0.4154300391674042
    },
    {
      "name": "Machine learning",
      "score": 0.3802684545516968
    },
    {
      "name": "Natural language processing",
      "score": 0.3476623296737671
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}