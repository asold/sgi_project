{
  "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning",
  "url": "https://openalex.org/W4414281281",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4221592771",
      "name": "Guo, Daya",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2629811825",
      "name": "Yang Dejian",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2230759314",
      "name": "Zhang Haowei",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2745080376",
      "name": "Song Junxiao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2065075791",
      "name": "Wang Peiyi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2625063975",
      "name": "Zhu, Qihao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A4202137999",
      "name": "Xu, Runxin",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2231939982",
      "name": "Zhang, Ruoyu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2563679360",
      "name": "Ma Shirong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2350559191",
      "name": "Bi Xiao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2151428886",
      "name": "Zhang Xiaokang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A3177944885",
      "name": "Yu Xingkai",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2100338995",
      "name": "Wu, Yu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2749552771",
      "name": "Wu Z F",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2370936511",
      "name": "Gou, Zhibin",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2356729452",
      "name": "Shao Zhi-hong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2888866385",
      "name": "Li Zhuoshu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2579418010",
      "name": "Gao Zi-Yi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1929470351",
      "name": "Liu Ai-xin",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2139990583",
      "name": "Xue Bing",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2744718974",
      "name": "Wang Bingxuan",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Wu, Bochao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2351385867",
      "name": "Feng Bei",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2714641065",
      "name": "Lu, Chengda",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2188937280",
      "name": "Zhao Cheng-gang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2356700191",
      "name": "Deng, Chengqi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2095838001",
      "name": "Ruan Chong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A3163909274",
      "name": "Dai, Damai",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2225349808",
      "name": "Chen De-li",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Ji, Dongjie",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Li, Erhang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Lin, Fangyun",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Dai, Fucong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2369097918",
      "name": "Luo, Fuli",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2393809180",
      "name": "Hao Guang-Bo",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2352177326",
      "name": "Chen Guanting",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2187166751",
      "name": "Li Guowei",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A3156158384",
      "name": "Zhang H",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2765724757",
      "name": "Xu Hanwei",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2313917949",
      "name": "Ding Hong-hui",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Gao, Huazuo",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2121154354",
      "name": "Qu Hui",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1885612739",
      "name": "Li Hui",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2230856064",
      "name": "Guo Jianzhong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2694617570",
      "name": "Li, Jiashi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2361579609",
      "name": "Chen Jing-chang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2387332632",
      "name": "Yuan Jing-yang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Tu, Jinhao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2352779430",
      "name": "Qiu Jun-jie",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2232244647",
      "name": "Li Junlong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Cai, J. L.",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A3181420650",
      "name": "Ni Jiaqi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2032091031",
      "name": "Liang Jian",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2027462004",
      "name": "Chen Jin",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2222638671",
      "name": "Dong Kai",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2101290657",
      "name": "Hu Kai",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A4221579903",
      "name": "You, Kaichao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2742411014",
      "name": "Gao Kaige",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2375724665",
      "name": "Guan Kang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2229279800",
      "name": "Huang Ke-xin",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2352908875",
      "name": "Yu Kuai",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2365527316",
      "name": "Wang Le-an",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Zhang, Lecong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2030402057",
      "name": "Zhao Liang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2355651644",
      "name": "Wang Litong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2232632504",
      "name": "Zhang Li-yue",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1985236295",
      "name": "Xu Lei",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Xia, Leyi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2205806586",
      "name": "Zhang Mingchuan",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2060343572",
      "name": "Zhang Ming-hua",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2355091046",
      "name": "Tang MingHui",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2363920746",
      "name": "Zhou Ming-xu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1979955179",
      "name": "Li Meng",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2353163994",
      "name": "Wang Miao-jun",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1976672934",
      "name": "Li Mingming",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2097166894",
      "name": "Tian Ning",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2225754328",
      "name": "Huang PanPan",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A1800355547",
      "name": "Zhang Peng",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2482220044",
      "name": "Wang Qiancheng",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2349852716",
      "name": "Chen Qin-yu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2230482807",
      "name": "Du Qiushi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Ge, Ruiqi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2288838861",
      "name": "Zhang Ruisong",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2637276651",
      "name": "Pan Rui-zhe",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2392265231",
      "name": "Wang Runji",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A4256829276",
      "name": "Chen, R. J.",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Jin, R. L.",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2351626814",
      "name": "Chen Ruyi",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Lu, Shanghao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2370640471",
      "name": "Zhou Shang-yan",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": null,
      "name": "Chen, Shanhuang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2231880883",
      "name": "Ye Shengfeng",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2138180727",
      "name": "Wang Shiyu",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2604525444",
      "name": "YU Shuiping",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2358080230",
      "name": "Zhou Shunfeng",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2318209098",
      "name": "Pan Shuting",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A4288397904",
      "name": "Li, S. S.",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2130719219",
      "name": "Zhou Shuang",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2562040385",
      "name": "Wu Shaoqing",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    },
    {
      "id": "https://openalex.org/A2104965822",
      "name": "Yun Tao",
      "affiliations": [
        "Institute on Taxation and Economic Policy"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4405903187",
    "https://openalex.org/W4391631327",
    "https://openalex.org/W4411119490",
    "https://openalex.org/W4388718089",
    "https://openalex.org/W4411119426",
    "https://openalex.org/W4388926373",
    "https://openalex.org/W4412944996",
    "https://openalex.org/W4394708144",
    "https://openalex.org/W4399836654",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W6949517105",
    "https://openalex.org/W6949480412",
    "https://openalex.org/W4387321091"
  ],
  "abstract": null,
  "full_text": "Nature | Vol 645 | 18 September 2025 | 633\nArticle\nDeepSeek-R1 incentivizes reasoning in LLMs \nthrough reinforcement learning\nGeneral reasoning represents a long-standing and formidable challenge in artificial \nintelligence (AI). Recent breakthroughs, exemplified by large language models \n(LLMs)1,2 and chain-of-thought (CoT) prompting3, have achieved considerable \nsuccess on foundational reasoning tasks. However, this success is heavily contingent \non extensive human-annotated demonstrations and the capabilities of models are \nstill insufficient for more complex problems. Here we show that the reasoning \nabilities of LLMs can be incentivized through pure reinforcement learning (RL), \nobviating the need for human-labelled reasoning trajectories. The proposed RL \nframework facilitates the emergent development of advanced reasoning patterns, \nsuch as self-reflection, verification and dynamic strategy adaptation. Consequently, \nthe trained model achieves superior performance on verifiable tasks such as \nmathematics, coding competitions and STEM fields, surpassing its counterparts \ntrained through conventional supervised learning on human demonstrations. \nMoreover, the emergent reasoning patterns exhibited by these large-scale models \ncan be systematically used to guide and enhance the reasoning capabilities of \nsmaller models.\nReasoning capability, the cornerstone of human intelligence, enables \ncomplex cognitive tasks ranging from mathematical problem-solving \nto logical deduction and programming. Recent advances in AI have \ndemonstrated that LLMs can exhibit emergent behaviours, including \nreasoning abilities, when scaled to a sufficient size4,5. However, achiev-\ning such capabilities in pre-training typically demands substantial \ncomputational resources. In parallel, a complementary line of research \nhas demonstrated that LLMs can be effectively augmented through CoT \nprompting. This technique, which involves either providing carefully \ndesigned few-shot examples or using minimalistic prompts such as \n“Let’s think step by step”3,6, enables models to produce intermediate \nreasoning steps, thereby substantially enhancing their performance \non complex tasks. Similarly, further performance gains have been \nobserved when models learn high-quality, multistep reasoning tra-\njectories during the post-training phase2,7. Despite their effectiveness, \nthese approaches exhibit notable limitations. Their dependence on \nhuman-annotated reasoning traces slows scalability and introduces \ncognitive biases. Furthermore, by constraining models to replicate \nhuman thought processes, their performance is inherently capped \nby the human-provided exemplars, which prevents the exploration \nof superior, non-human-like reasoning pathways.\nT o tackle these issues, we aim to explore the potential of LLMs for \ndeveloping reasoning abilities through self-evolution in a RL frame-\nwork, with minimal reliance on human labelling efforts. Specifically, \nwe build on DeepSeek-V3 Base8 and use Group Relative Policy Optimi-\nzation (GRPO)9 as our RL framework. The reward signal is only based \non the correctness of final predictions against ground-truth answers, \nwithout imposing constraints on the reasoning process itself. Nota-\nbly, we bypass the conventional supervised fine-tuning (SFT) phase \nbefore RL training. This design choice originates from our hypothesis \nthat human-defined reasoning patterns may limit model exploration, \nwhereas unrestricted RL training can better incentivize the emergence \nof new reasoning capabilities in LLMs. Through this process, detailed \nin the next section, our model (referred to as DeepSeek-R1-Zero) natu-\nrally developed diverse and sophisticated reasoning behaviours. T o \nsolve reasoning problems, the model exhibits a tendency to generate \nlonger responses, incorporating verification, reflection and the explo-\nration of alternative approaches within each response. Although we \ndo not explicitly teach the model how to reason, it successfully learns \nimproved reasoning strategies through RL.\nAlthough DeepSeek-R1-Zero demonstrates excellent reasoning capa-\nbilities, it faces challenges such as poor readability and language mixing, \noccasionally combining English and Chinese in a single CoT response. \nFurthermore, the rule-based RL training stage of DeepSeek-R1-Zero is \nnarrowly focused on reasoning tasks, resulting in limited performance \nin broader areas such as writing and open-domain question answering. \nT o address these challenges, we introduce DeepSeek-R1, a model trained \nthrough a multistage learning framework that integrates rejection sam-\npling, RL and supervised fine-tuning, detailed in the ‘DeepSeek-R1’ sec-\ntion. This training pipeline enables DeepSeek-R1 to inherit the reasoning \ncapabilities of its predecessor, DeepSeek-R1-Zero, while aligning model \nbehaviour with human preferences through further non-reasoning data.\nT o enable broader access to powerful AI at a lower energy cost, we \nhave distilled several smaller models and made them publicly available. \nThese distilled models exhibit strong reasoning capabilities, surpassing \nthe performance of their original instruction-tuned counterparts. We \nbelieve that these instruction-tuned versions will also greatly contribute \nto the research community by providing a valuable resource for under-\nstanding the mechanisms underlying long CoT reasoning models and \nfor promoting the development of more powerful reasoning models. \nWe release DeepSeek-R1-Zero, DeepSeek-R1, data samples and distilled \nmodels to the public as described in the ‘Code availability’ section.\nhttps://doi.org/10.1038/s41586-025-09422-z\nReceived: 14 February 2025\nAccepted: 17 July 2025\nPublished online: 17 September 2025\nOpen access\n Check for updates\nA list of authors and their affiliations appears at the end of the paper.\n634 | Nature | Vol 645 | 18 September 2025\nArticle\nDeepSeek-R1-Zero\nT o implement large-scale RL of DeepSeek-R1-Zero, we use a highly \nefficient RL pipeline. Specifically, we use GRPO9 as our RL algorithm, \ndescribed in Methods section ‘GRPO’ . Furthermore, we use a rule-based \nreward system to compute accuracy and format rewards, with detailed \nmethodologies outlined in Methods section ‘Reward design’ . Further-\nmore, our high-performance RL infrastructure is described in Sup -\nplementary Information, section 2.1, ensuring scalable and efficient \ntraining.\nSpecifically, we apply the RL technique on the DeepSeek-V3 Base to \ntrain DeepSeek-R1-Zero. During training, we design a straightforward \ntemplate to require DeepSeek-R1-Zero to first produce a reasoning \nprocess, followed by the final answer. The prompt template is written \nas below.\n“ A conversation between User and Assistant. The User asks a question \nand the Assistant solves it. The Assistant first thinks about the reasoning \nprocess in the mind and then provides the User with the answer. The \nreasoning process and answer are enclosed within <think>...</think> \nand <answer>...</answer> tags, respectively, that is, <think> reasoning \nprocess here </think><answer> answer here </answer>. User: prompt. \nAssistant:” , in which the prompt is replaced with the specific reason-\ning question during training. We intentionally limit our constraints to \nthis structural format, avoiding any content-specific biases to ensure \nthat we can accurately observe the natural progression of the model \nduring the RL process.\nFigure 1a shows the performance trajectory of DeepSeek-R1-Zero \non the American Invitational Mathematics Examination (AIME) 2024 \nbenchmark throughout the RL training process, in which the \naverage pass@1 score on AIME 2024 shows a marked increase, \njumping from an initial value of 15.6% to 77.9%. Also, by using the \nself-consistency decoding 10, the performance of the model can be \nfurther improved, achieving an accuracy of 86.7%. This performance \ngreatly surpasses the average performance across all human com -\npetitors of the AIME. Besides the maths competitions, as shown in \nSupplementary Fig. 8, DeepSeek-R1-Zero also achieves remark -\nable performance in coding competitions and graduate-level biol -\nogy, physics and chemistry problems. These results underscore \nthe effectiveness of RL in enhancing the reasoning capabilities of  \nLLMs.\nAs well as the progressive enhancement of reasoning capabilities \nduring training, DeepSeek-R1-Zero also demonstrates self-evolutionary \nbehaviour with RL training. As shown in Fig.  1b, DeepSeek-R1-Zero \nexhibits a steady increase in thinking time throughout training, driven \nonly by intrinsic adaptation rather than external modifications. Mak-\ning use of long CoT, the model progressively refines its reasoning, \ngenerating hundreds to thousands of tokens to explore and improve \nits problem-solving strategies.\nThe increase in thinking time helps with the autonomous develop-\nment of sophisticated behaviours. Specifically, DeepSeek-R1-Zero \nincreasingly exhibits advanced reasoning strategies such as reflective \nreasoning and systematic exploration of alternative solutions provided \nin Extended Data Fig. 1a, substantially boosting its performance on \nverifiable tasks such as maths and coding. Notably, during training, \nDeepSeek-R1-Zero exhibits an ‘aha moment’ , shown in Table 1, character-\nized by a sudden increase in the use of the word ‘wait’ during reflections, \nprovided in Extended Data Fig. 1b. This moment marks a distinct change \nin reasoning patterns and clearly shows the self-evolution process of \nDeepSeek-R1-Zero.\nThe self-evolution of DeepSeek-R1-Zero underscores the power and \nbeauty of RL: rather than explicitly teaching the model how to solve a \nproblem, we simply provide it with the right incentives and it autono-\nmously develops advanced problem-solving strategies. This serves as \na reminder of the potential of RL to unlock higher levels of capabilities \nin LLMs, paving the way for more autonomous and adaptive models \nin the future.\nDeepSeek-R1\nAlthough DeepSeek-R1-Zero exhibits strong reasoning capabilities, \nit faces several issues. DeepSeek-R1-Zero struggles with challenges \nsuch as poor readability and language mixing, as DeepSeek-V3 Base \nis trained on several languages, especially English and Chinese. T o \naddress these issues, we develop DeepSeek-R1, whose pipeline is illus-\ntrated in Fig. 2. In the initial stage, we collect thousands of cold-start \ndata that exhibit a conversational, human-aligned thinking process, \nas detailed in Supplementary Information, section 2.3.2. RL training \nis then applied with hyperparameters in Methods section ‘Training \ndetails of the first RL stage’ , data details in Supplementary Infor -\nmation, section 2.3.1, to improve the model performance with the \n0 2,000 4,000 6,000 8,000 10,000\nSteps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\na DeepSeek-R1-Zero AIME accuracy during training\nr1-zero-pass@1\nr1-zero-cons@16\nHuman participant\n0 2,000 4,000 6,000 8,000 10,000\nSteps\n0\n2,500\n5,000\n7,500\n10,000\n12,500\n15,000\n17,500\n20,000\nAverage length per response\nb DeepSeek-R1-Zero average length per response during training\nFig. 1 | Accuracy and output length of DeepSeek-R1-Zero throughout the \ntraining process.  a, AIME accuracy of DeepSeek-R1-Zero during training. AIME \ntakes a mathematical problem as input and a number as output, illustrated in \nExtended Data Table 1. pass@1 and cons@16 are described in Supplementary \nInformation, section 4.1. The baseline is the average score achieved by human \nparticipants in the AIME competition. b , The average response length of \nDeepSeek-R1-Zero on the training set during the RL process. DeepSeek-R1-Zero \nnaturally learns to solve reasoning tasks with more thinking time. Note that a \ntraining step refers to a single policy update operation.\nNature | Vol 645 | 18 September 2025 | 635\nconversational thinking process and language consistency. Subse-\nquently, we apply rejection sampling and SFT once more. This stage \nincorporates both reasoning and non-reasoning datasets into the SFT \nprocess, as detailed in Supplementary Information, section 2.3.3, ena-\nbling the model to not only excel in reasoning tasks but also demon-\nstrate advanced writing capabilities. T o further align the model with \nhuman preferences, we implement a secondary RL stage designed \nto enhance the helpfulness and harmlessness of the model while \nsimultaneously refining its reasoning capabilities. The reward model \nis described in Methods section ‘Reward design’ and RL hyperpa -\nrameters are in Methods section ‘Training details of the second RL \nstage’ . The total training cost is listed in Supplementary Information,  \nsection 2.4.4.\nWe evaluate our models on MMLU11, MMLU-Redux12, MMLU-Pro13, \nDROP14, C-Eval15, IFEval16, FRAMES17, GPQA Diamond18, SimpleQA19, \nC-SimpleQA20, CLUEWSC21, AlpacaEval 2.0 (ref.  22), Arena-Hard 23, \nSWE-bench Verified24, Aider-Polyglot25, LiveCodeBench26 (2024-08–\n2025-01), Codeforces27, Chinese National High School Mathematics \nOlympiad (CNMO 2024)28 and AIME 2024 (ref. 29). The details of these \nbenchmarks are provided in Supplementary Tables 15–29.\nTable 2 summarizes the performance of DeepSeek-R1 across several \ndevelopmental stages, as outlined in Fig. 2 . A comparison between \nDeepSeek-R1-Zero and DeepSeek-R1 Dev1 reveals substantial improve-\nments in instruction-following, as evidenced by higher scores on the \nIF-Eval and Arena-Hard benchmarks. However, owing to the limited \nsize of the cold-start dataset, Dev1 exhibits a partial degradation in \nreasoning performance compared with DeepSeek-R1-Zero, most \nnotably on the AIME benchmark. By contrast, DeepSeek-R1 Dev2 \ndemonstrates marked performance enhancements on benchmarks \nthat require advanced reasoning skills, including those focused on \ncode generation, mathematical problem solving and STEM-related \ntasks. Benchmarks targeting general-purpose tasks, such as Alpac -\naEval 2.0, show marginal improvement. These results indicate that \nreasoning-oriented RL considerably enhances reasoning capabili -\nties while exerting limited influence on user-preference-oriented \nbenchmarks.\nDeepSeek-R1 Dev3 integrates both reasoning and non-reasoning \ndatasets into the SFT pipeline, thereby enhancing the proficiency of \nthe model in both reasoning and general language-generation tasks. \nCompared with Dev2, DeepSeek-R1 Dev3 achieves notable performance \nimprovements on AlpacaEval 2.0 and Aider-Polyglot, attributable to the \ninclusion of large-scale non-reasoning corpora and code-engineering \ndatasets. Finally, comprehensive RL training on DeepSeek-R1 Dev3 \nusing mixed reasoning-focused and general-purpose data produced \nthe final DeepSeek-R1. Marginal improvements occurred in code \nand mathematics benchmarks, as substantial reasoning-specific RL \nwas done in previous stages. The primary advancements in the final \nDeepSeek-R1 were in general instruction-following and user-preference \nbenchmarks, with AlpacaEval 2.0 improving by 25% and Arena-Hard \nby 17%.\nWe also compare DeepSeek-R1 with other models in Supplementary \nInformation, section 4.2. Model safety evaluations are provided in \nSupplementary Information, section 4.3. A comprehensive analysis of \nevaluation is provided in Supplementary Information, section 5, includ-\ning a comparison with DeepSeek-V3, performance evaluations on both \nfresh test sets, a breakdown of mathematical capabilities by category \nand an investigation of test-time scaling behaviour. Supplementary \nInformation, section 6 shows that the strong reasoning capability can \nbe transferred to smaller models.\nTable 1 | An interesting ‘aha moment’ of an intermediate \nversion of DeepSeek-R1-Zero\nQuestion: If a > 1, then the sum of the real solutions of aa xx+=−−  is equal to\nResponse: <think>\nTo solve the equation aa xx−+ = , let’s start by squaring both …\n−+ =− +=aa xx aa xx() .22 2/uni21D2\nRearrange to isolate the inner square root term:\n/uni21D2/uni21D2−= +− += +− −+ −=ax ax aa xx ax xa xx aa() 2( )2 () 022 22 22 42 2\n…\nWait, wait. Wait. That’s an aha moment I can flag here.\nLet’s reevaluate this step by step to identify whether the correct sum can be …\nWe started with the equation:\n−+ =aa xx\nFirst, let’s square both sides:\na ax xa xa x22−+ =+ =−/uni21D2\nNext, I could square both sides again, treating the equation: …\n…\nThe model learns to rethink using an anthropomorphic tone. This is also an aha moment for \nus, allowing us to witness the power and beauty of RL.\nDeepSeek-V3\nBase\nCold-start long CoT\nSFT\nReasoning prompts\nRule-based reward \nand language consistency\nRL\nDeepSeek-R1\nDev2\nDeepSeek-V3\nNon-reasoning\nReasoning\nSFT\nDeepSeek-V3\nBase\nDiverse prompts\nRule-based reward\nand preference reward\nRL\nDeepSeek-R1\nSampling\nModels\nPrompts + responses\nTraining algorithms\nPrompts\nRewards\nPost-processing\nDeepSeek-R1\nDev1\nDeepSeek-R1\nDev3\nDeepSeek-V3\nBase\nReasoning prompts\nAccuracy and format\nRL\nDeepSeek-R1\n-Zero\nSampling\nAccuracy\nand format\nReasoning prompts\nDeepSeek-V3\n+ human\nReﬁneFilter\nFig. 2 | The multistage pipeline of DeepSeek-R1. A detailed background on DeepSeek-V3 Base and DeepSeek-V3 is provided in Supplementary Information, \nsection 1.1. The models DeepSeek-R1 Dev1, Dev2 and Dev3 represent intermediate checkpoints in this pipeline.\n636 | Nature | Vol 645 | 18 September 2025\nArticle\nEthics and safety statement\nWith the advancement in the reasoning capabilities of DeepSeek-R1, \nwe deeply recognize the potential ethical risks. For example, R1 can \nbe subject to jailbreak attacks, leading to the generation of dangerous \ncontent such as explosive manufacturing plans, whereas the enhanced \nreasoning capabilities enable the model to provide plans with better \noperational feasibility and executability. Besides, a public model is \nalso vulnerable to further fine-tuning that could compromise inherent \nsafety protections.\nIn Supplementary Information, section 4.3, we present a compre-\nhensive safety report from several perspectives, including perfor -\nmance on open-source and in-house safety evaluation benchmarks, \nand safety levels across several languages and against jailbreak \nattacks. These comprehensive safety analyses conclude that the \ninherent safety level of the DeepSeek-R1 model, compared with other \nstate-of-the-art models, is generally at a moderate level (compara -\nble with GPT-4o (2024-05-13)30). Besides, when coupled with the risk \ncontrol system, the safety level of the model is increased to a superior  \nstandard.\nConclusion, limitation and future work\nWe present DeepSeek-R1-Zero and DeepSeek-R1, which rely on \nlarge-scale RL to incentivize model reasoning behaviours. Our results \ndemonstrate that pre-trained checkpoints inherently have substan-\ntial potential for complex reasoning tasks. We believe that the key to \nunlocking this potential lies not in large-scale human annotation but \nin the provision of hard reasoning questions, a reliable verifier and \nsufficient computational resources for RL. Sophisticated reasoning \nbehaviours, such as self-verification and reflection, seemed to emerge \norganically during the RL process.\nEven if DeepSeek-R1 achieves frontier results on reasoning \nbenchmarks, it still faces several capability limitations, as outlined  \nbelow.\nStructure output and tool use\nAt present, the structural output capabilities of DeepSeek-R1 remain \nsuboptimal compared with existing models. Moreover, DeepSeek-R1 \ncannot make use of tools, such as search engines and calculators, to \nimprove the performance of output. However, as it is not hard to build \na RL environment for structure output and tool use, we believe that the \nissue will be addressed in the next version.\nT oken efficiency\nUnlike conventional test-time computation scaling approaches, such \nas majority voting or Monte Carlo tree search (MCTS), DeepSeek-R1 \ndynamically allocates computational resources during inference \naccording to the complexity of the problem at hand. Specifically, it \nuses fewer tokens to solve simple tasks but generating more tokens \nfor complex tasks. Nevertheless, there remains room for further opti-\nmization in terms of token efficiency, as instances of excessive rea-\nsoning—manifested as overthinking—are still observed in response \nto simpler questions.\nLanguage mixing\nDeepSeek-R1 is at present optimized for Chinese and English, which \nmay result in language-mixing issues when handling queries in other \nlanguages. For instance, DeepSeek-R1 might use English for reasoning \nand responses, even if the query is in a language other than English \nor Chinese. We aim to address this limitation in future updates. The \nlimitation may be related to the base checkpoint, DeepSeek-V3 Base, \nwhich mainly uses Chinese and English, so that it can achieve better \nresults with the two languages in reasoning.\nPrompting engineering\nWhen evaluating DeepSeek-R1, we observe that it is sensitive to \nprompts. Few-shot prompting consistently degrades its performance. \nTherefore, we recommend that users directly describe the problem and \nspecify the output format using a zero-shot setting for optimal results.\nTable 2 | Experimental results at each stage of DeepSeek-R1\nBenchmark (metric) R1-Zero R1 Dev1 R1 Dev2 R1 Dev3 R1\nEnglish MMLU (EM) 88.8 89.1 91.2 91.0 90.8\nMMLU-Redux (EM) 85.6 90.0 93.0 93.1 92.9\nMMLU-Pro (EM) 68.9 74.1 83.8 83.1 84.0\nDROP (3-shot F1) 89.1 89.8 91.1 88.7 92.2\nIF-Eval (Prompt Strict) 46.6 71.7 72.0 78.1 83.3\nGPQA Diamond (Pass@1) 75.8 66.1 70.7 71.2 71.5\nSimpleQA (Correct) 30.3 17.8 28.2 24.9 30.1\nFRAMES (Acc.) 82.3 78.5 81.8 81.9 82.5\nAlpacaEval 2.0 (LC-winrate) 24.7 50.1 55.8 62.1 87.6\nArena-Hard (GPT-4-1106) 53.6 77.0 73.2 75.6 92.3\nCode LiveCodeBench (Pass@1-COT) 50.0 57.5 63.5 64.6 65.9\nCodeforces (Percentile) 80.4 84.5 90.5 92.1 96.3\nCodeforces (Rating) 1,444 1,534 1,687 1,746 2,029\nSWE-bench Verified (Resolved) 43.2 39.6 44.6 45.6 49.2\nAider-Polyglot (Acc.) 12.2 6.7 25.6 44.8 53.3\nMaths AIME 2024 (Pass@1) 77.9 59.0 74.0 78.1 79.8\nMATH-500 (Pass@1) 95.9 94.2 95.9 95.4 97.3\nCNMO 2024 (Pass@1) 88.1 58.0 73.9 77.3 78.8\nChinese CLUEWSC (EM) 93.1 92.8 92.6 91.6 92.8\nC-Eval (EM) 92.8 85.7 91.9 86.4 91.8\nC-SimpleQA (Correct) 66.4 58.8 64.2 66.9 63.7\nNumbers in bold denote that the performance is statistically significant (t-test with P < 0.01).\nNature | Vol 645 | 18 September 2025 | 637\nSoftware-engineering tasks\nOwing to the long evaluation times, which affect the efficiency of \nthe RL process, large-scale RL has not been applied extensively in \nsoftware-engineering tasks. As a result, DeepSeek-R1 has not demon-\nstrated a huge improvement over DeepSeek-V3 on software-engineering \nbenchmarks. Future versions will address this by implementing rejec-\ntion sampling on software-engineering data or incorporating asyn -\nchronous evaluations during the RL process to improve efficiency.\nBeyond specific capability limitations, the pure RL methodology \nitself also presents inherent challenges:\nReward hacking\nThe success of pure RL depends on reliable reward signals. In this study, \nwe ensure reward reliability through a reasoning-domain rule-based \nreward model. However, such dependable reward models are diffi-\ncult to construct for certain tasks, such as writing. If the reward signal \nis assigned by a model instead of predefined rules, it becomes more \nsusceptible to exploitation as training progresses, which means that \nthe policy model may find shortcuts to hack the reward model. Con-\nsequently, for complex tasks that cannot be effectively evaluated by \na reliable reward model, scaling up pure RL methods remains an open \nchallenge.\nIn this work, for tasks that cannot obtain a reliable signal, DeepSeek-R1 \nuses human annotation to create supervised data and only conducts \nRL for hundreds of steps. We hope that, in the future, a robust reward \nmodel can be obtained to address such issues.\nWith the advent of pure RL methods such as DeepSeek-R1, the \nfuture holds immense potential for solving any task that can be effec-\ntively evaluated by a verifier, regardless of its complexity for humans. \nMachines equipped with such advanced RL techniques are poised to \nsurpass human capabilities in these domains, driven by their ability \nto optimize performance iteratively through trial and error. However, \nchallenges remain for tasks for which constructing a reliable reward \nmodel is inherently difficult. In such cases, the lack of a robust feedback \nmechanism may slow progress, suggesting that future research should \nfocus on developing innovative approaches to define and refine reward \nstructures for these complex, less verifiable problems.\nFurthermore, making use of tools during the reasoning process \nholds notable promise. Whether it is using tools such as compilers or \nsearch engines to retrieve or compute necessary information or using \nexternal tools such as biological or chemical reagents to validate final \nresults in the real world, this integration of tool-augmented reason -\ning could greatly enhance the scope and accuracy of machine-driven \nsolutions.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-025-09422-z.\n1. Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural Information \nProcessing Systems 33 (eds Larochelle, H. et al.) (ACM, 2020).\n2. OpenAI et al. GPT4 technical report. Preprint at https://doi.org/10.48550/arXiv.2303.08774 \n(2024).\n3. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models. In \nAdvances in Neural Information Processing Systems 35 (eds Koyejo, S. et al.) 24824–24837 \n(ACM, 2022).\n4. Wei, J. et al. Emergent abilities of large language models. In Transactions on Machine \nLearning Research (eds Kamath, G. et al.) (2022).\n5. Kaplan, J. et al. Scaling laws for neural language models. Preprint at https://doi.org/ \n10.48550/arXiv.2001.08361 (2020).\n6. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language models are zero-shot \nreasoners. In Advances in Neural Information Processing Systems 35 (eds Oh, A. H. et al.) \n22199–22213 (ACM, 2022).\n7. Chung, H. W. et al. Scaling instruction-finetuned language models. J. Mach. Learn. Res. \n25, 1–53 (2024).\n8. DeepSeek-AI et al. DeepSeek-V3 technical report. Preprint at https://doi.org/10.48550/\narXiv.2412.19437 (2025).\n9. Shao, Z. et al. DeepSeekMath: pushing the limits of mathematical reasoning in open \nlanguage models. Preprint at https://doi.org/10.48550/arXiv.2402.03300 (2024).\n10. Wang, X. et al. Self-consistency improves chain of thought reasoning in language \nmodels. In 11th International Conference on Learning Representations (ICLR, 2023).\n11. Hendrycks, D. et al. Measuring massive multitask language understanding.  \nIn 9th International Conference on Learning Representations (ICLR, 2021).\n12. Gema, A. P. et al. Are we done with MMLU? In Proc. 2025 Conference of the Nations of the \nAmericas Chapter of the Association for Computational Linguistics: Human Language \nTechnologies (eds Chiruzzo, L. et al.) Vol. 1 (Long Papers), 5069–5096 (ACL, 2025).\n13. Wang, Y. et al. MMLU-Pro: a more robust and challenging multi-task language  \nunderstanding benchmark. In Advances in Neural Information Processing Systems 37  \n(eds Globersons, A. et al.) 95266–95290 (ACM, 2024).\n14. Dua, D. et al. DROP: a reading comprehension benchmark requiring discrete reasoning \nover paragraphs. In Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies Vol. 1  \n(Long and Short Papers) (eds Burstein, J. et al.) 2368–2378 (ACL, 2019).\n15. Huang, Y. et al. C-EVAL: a multi-level multi-discipline Chinese evaluation suite  \nfor foundation models. In Advances in Neural Information Processing Systems 36  \n(eds Oh, A. et al.) 62991–63010 (ACM, 2023).\n16. Zhou, J. et al. Instruction-following evaluation for large language models. Preprint at \nhttps://doi.org/10.48550/arXiv.2311.07911 (2023).\n17. Krishna, S. et al. Fact, fetch, and reason: a unified evaluation of retrieval-augmented \ngeneration. In Proc. 2025 Conference of the Nations of the Americas Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies Vol. 1 (Long \nPapers) 4745–4759 (ACL, 2025).\n18. Rein, D. et al. GPQA: a graduate-level Google-proof Q&A benchmark. Preprint at https://\ndoi.org/10.48550/arXiv.2311.12022 (2023).\n19. OpenAI. Introducing SimpleQA; https://openai.com/index/introducing-simpleqa/ (2024).\n20. He, Y. et al. Chinese SimpleQA: a Chinese factuality evaluation for large language \nmodels. In Proc. 63rd Annual Meeting of the Association for Computational Linguistics \nVol. 1 (Long Papers), 19182–19208 (ACL, 2025).\n21. Xu, L. et al. CLUE: a Chinese Language Understanding Evaluation benchmark. In Proc. \n28th International Conference on Computational Linguistics (eds Scott, D. et al.) 4762–4772 \n(International Committee on Computational Linguistics, 2020).\n22. Dubois, Y., Galambosi, B., Liang, P. & Hashimoto, T. B. Length-controlled AlpacaEval: a \nsimple way to debias automatic evaluators. Preprint at https://doi.org/10.48550/arXiv. \n2404.04475 (2025).\n23. Li, T. et al. From crowdsourced data to high-quality benchmarks: Arena-Hard and \nBenchBuilder pipeline. Preprint at https://doi.org/10.48550/arXiv.2406.11939 (2024).\n24. OpenAI. Introducing SWE-bench verified; https://openai.com/index/introducing-swe- \nbench-verified/ (2024).\n25. Aider. Aider LLM leaderboards; https://aider.chat/docs/leaderboards/ (2024).\n26. Jain, N. et al. LiveCodeBench: holistic and contamination free evaluation of large \nlanguage models for code. In 13th International Conference on Learning Representations \n(ICLR, 2024).\n27. Mirzayanov, M. Codeforces; https://codeforces.com/ (2025).\n28. Chinese Mathematical Society (CMS). Chinese National High School Mathematics \nOlympiad; https://www.cms.org.cn/Home/comp/comp/cid/12.html (2024).\n29. Mathematical Association of America. American Invitational Mathematics Examination; \nhttps://maa.org/maa-invitational-competitions (2024).\n30. OpenAI. Hello GPT-4o; https://openai.com/index/hello-gpt-4o/ (2024).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025\nDaya Guo1, Dejian Yang1, Haowei Zhang1, Junxiao Song1, Peiyi Wang1, Qihao Zhu1, Runxin Xu1, \nRuoyu Zhang1, Shirong Ma1, Xiao Bi1, Xiaokang Zhang1, Xingkai Yu1, Yu Wu1, Z. F. Wu1, \nZhibin Gou1, Zhihong Shao1, Zhuoshu Li1, Ziyi Gao1, Aixin Liu1, Bing Xue1, Bingxuan Wang1, \nBochao Wu1, Bei Feng1, Chengda Lu1, Chenggang Zhao1, Chengqi Deng1, Chong Ruan1, \nDamai Dai1, Deli Chen1, Dongjie Ji1, Erhang Li1, Fangyun Lin1, Fucong Dai1, Fuli Luo1,2, \nGuangbo Hao1, Guanting Chen1, Guowei Li1, H. Zhang1, Hanwei Xu1, Honghui Ding1, \nHuazuo Gao1, Hui Qu1, Hui Li1, Jianzhong Guo1, Jiashi Li1, Jingchang Chen1, Jingyang Yuan1, \nJinhao Tu1,3, Junjie Qiu1, Junlong Li1, J. L. Cai1, Jiaqi Ni1, Jian Liang1, Jin Chen1, Kai Dong1, \nKai Hu1,4, Kaichao You1, Kaige Gao1, Kang Guan1, Kexin Huang1,5, Kuai Yu1, Lean Wang1, \nLecong Zhang1, Liang Zhao1, Litong Wang1, Liyue Zhang1, Lei Xu1, Leyi Xia1, \nMingchuan Zhang1, Minghua Zhang1, Minghui Tang1, Mingxu Zhou1, Meng Li1, \nMiaojun Wang1, Mingming Li1, Ning Tian1, Panpan Huang1, Peng Zhang1, Qiancheng Wang1, \nQinyu Chen1, Qiushi Du1, Ruiqi Ge1, Ruisong Zhang1, Ruizhe Pan1, Runji Wang1, R. J. Chen1, \n638 | Nature | Vol 645 | 18 September 2025\nArticle\nR. L. Jin1, Ruyi Chen1, Shanghao Lu1, Shangyan Zhou1, Shanhuang Chen1, Shengfeng Ye1, \nShiyu Wang1, Shuiping Yu1, Shunfeng Zhou1, Shuting Pan1, S. S. Li1, Shuang Zhou1, \nShaoqing Wu1, Tao Yun1, Tian Pei1, Tianyu Sun1, T . Wang1, Wangding Zeng1, Wen Liu1, \nWenfeng Liang1 ✉, Wenjun Gao1, Wenqin Yu1,5, Wentao Zhang1, W. L. Xiao1, Wei An1, \nXiaodong Liu1, Xiaohan Wang1, Xiaokang Chen1, Xiaotao Nie1, Xin Cheng1, Xin Liu1, Xin Xie1, \nXingchao Liu1, Xinyu Yang1, Xinyuan Li1,5, Xuecheng Su1, Xuheng Lin1, X. Q. Li1, Xiangyue Jin1, \nXiaojin Shen1, Xiaosha Chen1, Xiaowen Sun1, Xiaoxiang Wang1, Xinnan Song1, Xinyi Zhou1, \nXianzu Wang1, Xinxia Shan1, Y . K. Li1, Y . Q. Wang1, Y . X. Wei1, Yang Zhang1, Yanhong Xu1, \nYao Li1, Yao Zhao1, Yaofeng Sun1, Yaohui Wang1, Yi Yu1, Yichao Zhang1, Yifan Shi1, \nYiliang Xiong1, Ying He1, Yishi Piao1, Yisong Wang1, Yixuan Tan1, Yiyang Ma1, Yiyuan Liu1, \nYongqiang Guo1, Yuan Ou1, Yuduan Wang1, Yue Gong1,5, Yuheng Zou1, Yujia He1,5, \nYunfan Xiong1, Yuxiang Luo1, Yuxiang You1, Yuxuan Liu1, Yuyang Zhou1, Y . X. Zhu1, \nYanping Huang1, Yaohui Li1, Yi Zheng1, Yuchen Zhu1, Yunxian Ma1, Ying Tang1, Yukun Zha1, \nYuting Yan1, Z. Z. Ren1, Zehui Ren1, Zhangli Sha1, Zhe Fu1, Zhean Xu1, Zhenda Xie1, \nZhengyan Zhang1, Zhewen Hao1, Zhicheng Ma1, Zhigang Yan1, Zhiyu Wu1, Zihui Gu1, Zijia Zhu1, \nZijun Liu1,6, Zilin Li1, Ziwei Xie1, Ziyang Song1,7, Zizheng Pan1, Zhen Huang1, Zhipeng Xu1, \nZhongyu Zhang1 & Zhen Zhang1\n1DeepSeek-AI Team, Hangzhou, China. 2Present address: Individual Researcher, Beijing, China. \n3Present address: Jianping High School, Shanghai, China. 4Present address: University of \nScience and Technology of China, Hefei, China. 5Present address: Peking University, Beijing, \nChina. 6Present address: Tsinghua University, Beijing, China. 7Present address: Citadel \nSecurities, Hong Kong SAR, China. ✉e-mail: wenfeng.liang@deepseek.com\nMethods\nGRPO\nGRPO9 is the RL algorithm that we use to train DeepSeek-R1-Zero and \nDeepSeek-R1. It was originally proposed to simplify the training process \nand reduce the resource consumption of proximal policy optimization \n(PPO)31, which is widely used in the RL stage of LLMs32. The pipeline of \nGRPO is shown in Extended Data Fig. 2.\nFor each question q, GRPO samples a group of outputs {o1, o2,…, oG} \nfrom the old policy πθold and then optimizes the policy model π θ by \nmaximizing the following objective:\nE\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJ\n∑\n(1)\nθq PQ oπ Oq\nG\nπo q\nπo q A πo q\nπo q ϵϵ A\nβπ π\n() =[ ~( ), {} ~( )]\n1 min ()\n() ,c lip ()\n() ,1 −, 1+\n−( ),\nii\nG\nθ\ni\nG\nθi\nθi\ni\nθi\nθi\ni\nKL θ\nGRPO =1\n=1\nref\nold\noldo ld\nD ππ πo q\nπo q\nπo q\nπo q() = ()\n() −l og ()\n() −1 , (2)θ\ni\nθi\ni\nθi\nKL ref\nrefr ef\nin which πref is a reference policy, ϵ and β are hyperparameters and Ai \nis the advantage, computed using a group of rewards {r1, r2,…, rG} cor-\nresponding to the outputs in each group:\nA rr rr\nrr r= −m ean({, ,, })\nstd({, ,, }) . (3)i\niG\nG\n12\n12\n/uni22EF\n/uni22EF\nWe give a comparison of GRPO and PPO in Supplementary Informa-\ntion, section 1.3.\nReward design\nThe reward is the source of the training signal, which decides the direc-\ntion of RL optimization. For DeepSeek-R1-Zero, we use rule-based \nrewards to deliver precise feedback for data in mathematical, coding and \nlogical reasoning domains. For DeepSeek-R1, we extend this approach \nby incorporating both rule-based rewards for reasoning-oriented data \nand model-based rewards for general data, thereby enhancing the \nadaptability of the learning process across diverse domains.\nRule-based rewards. Our rule-based reward system mainly consists of \ntwo types of reward: accuracy rewards and format rewards.\nAccuracy rewards evaluate whether the response is correct. For \nexample, in the case of maths problems with deterministic results, \nthe model is required to provide the final answer in a specified format \n(for example, within a box), enabling reliable rule-based verification of \ncorrectness. Similarly, for code competition prompts, a compiler can \nbe used to evaluate the responses of the model against a suite of prede-\nfined test cases, thereby generating objective feedback on correctness.\nFormat rewards complement the accuracy reward model by enforc-\ning specific formatting requirements. In particular, the model is incen-\ntivized to encapsulate its reasoning process within designated tags, \nspecifically <think> and </think>. This ensures that the thought process \nof the model is explicitly delineated, enhancing interpretability and \nfacilitating subsequent analysis.\nReward =R eward+ Reward (4)rule accf ormat\nThe accuracy, reward and format reward are combined with the \nsame weight. Notably, we abstain from applying neural reward mod-\nels—whether outcome-based or process-based-to reasoning tasks. \nThis decision is predicated on our observation that neural reward \nmodels are susceptible to reward hacking during large-scale RL. \nMoreover, retraining such models necessitates substantial computa-\ntional resources and introduces further complexity into the training \npipeline, thereby complicating the overall optimization process.\nModel-based rewards. For general data, we resort to reward models \nto capture human preferences in complex and nuanced scenarios. We \nbuild on the DeepSeek-V3 pipeline and use a similar distribution of pref-\nerence pairs and training prompts. For helpfulness, we focus exclusively \non the final summary, ensuring that the assessment emphasizes the use \nand relevance of the response to the user while minimizing interference \nwith the underlying reasoning process. For harmlessness, we evaluate \nthe entire response of the model, including both the reasoning process \nand the summary, to identify and mitigate any potential risks, biases or \nharmful content that may arise during the generation process.\nHelpful reward model. For helpful reward model training, we first gen-\nerate preference pairs by prompting DeepSeek-V3 using the Arena-Hard \nprompt format, listed in Supplementary Information, section 2.2, for \nwhich each pair consists of a user query along with two candidate \nresponses. For each preference pair, we query DeepSeek-V3 four times, \nrandomly assigning the responses as either Response A or Response B \nto mitigate positional bias. The final preference score is determined \nby averaging the four independent judgments, retaining only those \npairs for which the score difference (Δ) exceeds 1 to ensure meaningful \ndistinctions. Furthermore, to minimize length-related biases, we ensure \nthat the chosen and rejected responses of the whole dataset have com-\nparable lengths. In total, we curated 66,000 data pairs for training the \nreward model. The prompts used in this dataset are all non-reasoning \nquestions and are sourced either from publicly available open-source \ndatasets or from users who have explicitly consented to share their \ndata for the purpose of model improvement. The architecture of our \nreward model is consistent with that of DeepSeek-R1, with the addition \nof a reward head designed to predict scalar preference scores.\nReward =R M( Response ,R esponse) (5)helpfulh elpful AB\nThe helpful reward models were trained with a batch size of 256, a \nlearning rate of 6 × 10−6 and for a single epoch over the training dataset. \nThe maximum sequence length during training is set to 8,192 tokens, \nwhereas no explicit limit is imposed during reward model inference.\nSafety reward model. T o assess and improve model safety, we curated \na dataset of 106,000 prompts with model-generated responses anno-\ntated as ‘safe’ or ‘unsafe’ according to predefined safety guidelines. \nUnlike the pairwise loss used in the helpfulness reward model, the \nsafety reward model was trained using a pointwise methodology to \ndistinguish between safe and unsafe responses. The training hyper-\nparameters are the same as the helpful reward model.\nReward =R M( Response) (6)safety safety\nFor general queries, each instance is categorized as belonging to either \nthe safety dataset or the helpfulness dataset. The general reward, \nRewardgeneral, assigned to each query corresponds to the respective \nreward defined in the associated dataset.\nTraining details\nTraining details of DeepSeek-R1-Zero. T o train DeepSeek-R1-Zero, we \nset the learning rate to 3 × 10−6, the Kullback–Leibler (KL) coefficient \nto 0.001 and the sampling temperature to 1 for rollout. For each ques-\ntion, we sample 16 outputs with a maximum length of 32,768 tokens \nbefore the 8.2k step and 65,536 tokens afterward. As a result, both \nthe performance and response length of DeepSeek-R1-Zero exhibit a \nsubstantial jump at the 8.2k step, with training continuing for a total \nof 10,400 steps, corresponding to 1.6 training epochs. Each training \nstep consists of 32 unique questions, resulting in a training batch size \nof 512 per step. Every 400 steps, we replace the reference model with \nArticle\nthe latest policy model. T o accelerate training, each rollout generates \n8,192 outputs, which are randomly split into 16 minibatches and trained \nfor only a single inner epoch.\nTraining details of the first RL stage. In the first stage of RL, we set the \nlearning rate to 3 × 10−6, the KL coefficient to 0.001, the GRPO clip ratio \nϵ to 10 and the sampling temperature to 1 for rollout. For each question, \nwe sample 16 outputs with a maximum length of 32,768. Each training \nstep consists of 32 unique questions, resulting in a training batch size \nof 512 per step. Every 400 steps, we replace the reference model with \nthe latest policy model. T o accelerate training, each rollout gener-\nates 8,192 outputs, which are randomly split into 16 minibatches and \ntrained for only a single inner epoch. However, to mitigate the issue of \nlanguage mixing, we introduce a language consistency reward during \nRL training, which is calculated as the proportion of target language \nwords in the CoT.\nReward =\nNum(Words)\nNum(Words) (7)language\ntarget\nAlthough ablation experiments in Supplementary Information, \nsction 2.6 show that such alignment results in a slight degradation in \nthe performance of the model, this reward aligns with human prefer-\nences, making it more readable. We apply the language consistency \nreward to both reasoning and non-reasoning data by directly adding \nit to the final reward.\nNote that the clip ratio plays a crucial role in training. A lower value \ncan lead to the truncation of gradients for a large number of tokens, \nthereby degrading the performance of the model, whereas a higher \nvalue may cause instability during training. Details of RL data used in \nthis stage are provided in Supplementary Information, section 2.3.\nTraining details of the second RL stage. Specifically, we train the \nmodel using a combination of reward signals and diverse prompt dis-\ntributions. For reasoning data, we follow the methodology outlined in \nDeepSeek-R1-Zero, which uses rule-based rewards to guide learning \nin mathematical, coding and logical reasoning domains. During the \ntraining process, we observe that CoT often exhibits language mixing, \nparticularly when RL prompts involve several languages. For general \ndata, we use reward models to guide training. Ultimately, the inte -\ngration of reward signals with diverse data distributions enables us \nto develop a model that not only excels in reasoning but also assigns \npriority to helpfulness and harmlessness. Given a batch of data, the \nreward can be formulated as\nReward =R eward+ Reward +R eward (8)reasoning general language\nin which\nReward =R eward (9)reasoningr ule\nReward =R eward+ Reward (10)general reward_model format\nThe second stage of RL retains most of the parameters from the \nfirst stage, with the key difference being a reduced temperature of \n0.7, as we find that higher temperatures in this stage lead to incoher-\nent generation. The stage comprises a total of 1,700 training steps, \nduring which general instruction data and preference-based rewards \nare incorporated exclusively in the final 400 steps. We find that more \ntraining steps with the model-based preference reward signal may lead \nto reward hacking, which is documented in Supplementary Informa-\ntion, section 2.5.\nData availability\nWe provide the data samples used in our rejection sampling and RL \nprompts at https://github.com/deepseek-ai/DeepSeek-R1 (https://\ndoi.org/10.5281/zenodo.15753193)33. Comprehensive statistics and \ndetails of our complete data-generation methodology are presented \nin Supplementary Information, section 2.3.\nCode availability\nTrained weights of DeepSeek-R1-Zero and DeepSeek-R1 are available \nunder an MIT license at https://github.com/deepseek-ai/DeepSeek-R1 \n(https://doi.org/10.5281/zenodo.15753193)33. The inference script is \nreleased at https://github.com/deepseek-ai/DeepSeek-V3 (https://\ndoi.org/10.5281/zenodo.15753347)34. Neural networks were developed \nwith PyT orch35 and the distributed framework is based on our internal \nframework HAI-LLM (https://www.high-flyer.cn/en/blog/hai-llm). The \ninference framework is based on vLLM36. Data analysis used Python \nv.3.8 (https://www.python.org/), NumPy v.1.23.1 (https://github.com/\nnumpy/numpy), Matplotlib v.3.5.2 (https://github.com/matplotlib/\nmatplotlib) and T ensorBoard v.2.9.1 (https://github.com/tensorflow/\ntensorboard).\n \n31. Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal policy optimization \nalgorithms. Preprint at https://doi.org/10.48550/arXiv.1707.06347 (2017).\n32. Ouyang, L. et al. Training language models to follow instructions with human feedback. \nIn Advances in Neural Information Processing Systems 35 (eds Koyejo, S. et al.) 27730–27744 \n(ACM, 2022).\n33. Nano et al. deepseek-ai/DeepSeek-R1: v1.0.0. Zenodo https://doi.org/10.5281/zenodo. \n15753192 (2025).\n34. Yu, X. et al. deepseek-ai/DeepSeek-V3: v1.0.0. Zenodo https://doi.org/10.5281/zenodo. \n15753346 (2025).\n35. Paszke, A. et al. PyTorch: an imperative style, high-performance deep learning library.  \nIn Advances in Neural Information Processing Systems 32 (eds Wallach, H. M. et al.) \n8026–8037 (ACM, 2019).\n36. Kwon, W. et al. Efficient memory management for large language model serving with \nPagedAttention. In Proc. ACM SIGOPS 29th Symposium on Operating Systems Principles \n611–626 (ACM, 2023).\nAcknowledgements The research is supported by DeepSeek-AI.\nAuthor contributions All authors have contributed to the publication, being variously involved \nin collecting and curating data, designing the experiments and building the LLM training \nframework. The authors also participated in implementing and testing the experimental \nset-up, refining the RL process and performing the analysis of results. The scientific findings \nwere discussed and approved by all contributors. This article was written by a subgroup of \nauthors designated by the collaboration and it underwent an internal collective review process. \nAll authors reviewed and approved the final version of the manuscript. Core contributors: D.G., \nD.Y., H.Z., J.S., R.Z., R.X., Q.Z., S.M., P.W., X.B., X.Z., X.Y., Y.W., Z.F.W., Z.G., Z.S., Z.L., Z.G. These \nauthors, designated as core contributors, contributed equally to this work, and are presented \nin alphabetical order. The remaining authors have also contributed meaningfully to the study, \nand their names are likewise presented in alphabetical order.\nCompeting interests The authors declare no competing interests and will not file patents \nrelated to the content of this manuscript.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-025-09422-z.\nCorrespondence and requests for materials should be addressed to Wenfeng Liang.\nPeer review information Nature thanks Edward Beeching, Yarin Gal, José Hernández-Orallo, \nDaphne Ippolito, Subbarao Kambhampati, Lewis Tunstall, Yiming Zhang and Lexin Zhou for \ntheir contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permissions information is available at http://www.nature.com/reprints.\nExtended Data Fig. 1 | Evolution of reasoning-related linguistic features  \nin model outputs across training steps.  a, Frequency of representative \nreflective terms in model-generated outputs throughout the training process. \nReflective terms—including ‘wait’, ‘mistake’, ‘however’, ‘but’, ‘retry’, ‘error’, \n‘verify’, ‘wrong’, ‘evaluate’ and ‘check’—were identified and curated by a panel \nof three human experts. Each expert independently proposed a set of words \nindicative of reflective reasoning, which were subsequently consolidated \nthrough consensus into a final vocabulary list. b , Frequency of the term ‘wait’  \nin model outputs over the course of training. This term was virtually absent \nduring the initial training stages, appeared sporadically between steps 4,000 \nand 7,000 and exhibited a marked increase in frequency after step 8,000. \nThese trends suggest the emergence of temporal reasoning or self-monitoring \nbehaviour as training progresses.\nArticle\nExtended Data Fig. 2 | Illustration of the proposed GRPO for RL-based \ntraining.  In the proposed framework, a LLM is used as a policy model to \ngenerate responses {o 1, o2,…, oG} conditioned on a given query q . Each response \nwithin the group is evaluated by a reward model—either learned (model-based) \nor manually specified (rule-based)—to assign a scalar reward signal. Subsequently, \nGRPO computes the relative advantages of each group member based on their \nassigned rewards. Rather than relying on an explicit value function, as in PPO, \nGRPO directly estimates advantages from the intra-group reward distribution. \nThe policy parameters are then updated to maximize the expected reward \nwhile simultaneously minimizing divergence from a reference policy, typically \nquantified through the KL divergence. By eliminating the need for a separate \nvalue network, GRPO offers a simplified yet effective alternative to traditional \nactor-critic methods such as PPO.\nExtended Data Table 1 | An illustrative example from the AIME dataset\nThe presented question is sourced from the 2024 AIME. The model is tasked with solving the problem and formatting its answer in a required format (for example, ANSWER). For evaluation,  \na rule-based grading system is used to determine correctness. The output of the model is considered correct if and only if it exactly matches the ground-truth solution.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155796",
      "name": "Shanghai Jinyuan Senior High School",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 53
}