{
  "title": "First Automatic Fongbe Continuous Speech Recognition System: Development of Acoustic Models and Language Models",
  "url": "https://openalex.org/W2524544624",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A4298469877",
      "name": "Frejus Laleye",
      "affiliations": [
        "Université du littoral côte d'opale",
        "Institute of Mathematics and Physics"
      ]
    },
    {
      "id": "https://openalex.org/A2237966808",
      "name": "Laurent Besacier",
      "affiliations": [
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A562923353",
      "name": "Eugène C. Ezin",
      "affiliations": [
        "Institute of Mathematics and Physics"
      ]
    },
    {
      "id": "https://openalex.org/A2265958234",
      "name": "Cina Motamed",
      "affiliations": [
        "Université du littoral côte d'opale"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2091746061",
    "https://openalex.org/W6607426509",
    "https://openalex.org/W2345799635",
    "https://openalex.org/W1928758465",
    "https://openalex.org/W1898164463",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2612280934",
    "https://openalex.org/W2132959327",
    "https://openalex.org/W2319056086",
    "https://openalex.org/W2022763053",
    "https://openalex.org/W2572097499",
    "https://openalex.org/W72757270",
    "https://openalex.org/W3152004739",
    "https://openalex.org/W3151268685",
    "https://openalex.org/W92696887"
  ],
  "abstract": "International audience",
  "full_text": "First Automatic Fongbe Continuous Speech\nRecognition System: Development of Acoustic\nModels and Language Models\nFréjus A. A. LAleye ∗‡, Laurent Besacier †, Eugène C. Ezin ‡ and Cina Motamed ∗\n∗Laboratoire d’Informatique Signal et Image de la Côte d’Opa le\nUniversité du Littoral Côte d’Opale, France.\n50 rue F . Buisson\nBP 719, 62228 Calais Cedex\nEmail: {laleye, motamed}@lisic.univ-littoral.fr\n†Laboratoire LIG- Univ. Grenoble Alpes - BP 53,\nEmail: laurent.besacier@imag.fr\n‡Unité de Recherche en Informatique et Sciences Appliquées\nInstitut de Mathématiques et de Sciences Physiques\nUniversité d’Abomey-Calavi Bénin,\nBP 613 Porto-Novo.\nEmail: {frejus.laleye, eugene.ezin}@imsp-uac.org\nAbstract—This paper reports our efforts toward an ASR\nsystem for a new under-resourced language (Fongbe). The aim\nof this work is to build acoustic models and language models f or\ncontinuous speech decoding in Fongbe. The problem encounte red\nwith Fongbe (an African language spoken especially in Benin ,\nT ogo, and Nigeria) is that it does not have any language resou rces\nfor an ASR system. As part of this work, we have ﬁrst collected\nFongbe text and speech corpora that are described in the\nfollowing sections. Acoustic modeling has been worked out a t\na graphemic level and language modeling has provided two\nlanguage models for performance comparison purposes. W e al so\nperformed a vowel simpliﬁcation by removing tones diacriti cs in\norder to investigate their impact on the language models.\nI. I N T RO D U CT IO N\nA\nUTOMA TIC Speech Recognition (ASR) is a technology\nthat allows a computer to identify the words spoken\nby a person in microphone. Speech recognition technology\nis changing the way information is accessed, tasks are ac-\ncomplished and business is done. The growth of speech\napplications over the past years has been remarkable [1]. AS R\napplications have been successfully achieved for most west ern\nlanguages such as English, French, Italian etc., for Asian\nlanguages such as Chinese, Japanese, Indian etc, because of\nthe large quantity and the availability of linguistic resou rces of\nthese languages [2]. This technology is less prevalent in Af rica\ndespite its 2,000 languages because of lack or unavailabili ty\nof these resources for most African languages (vernacular f or\nmost). Also, for the most of the time, these are not written\nlanguages (no formal grammar, limited number of dictionari es,\nfew linguists). Despite the shortcomings, some have been\ninvestigated and now have the linguistic resources to build\na speech recognition systems. For example, in the context\nof a project entitled ALFF A 1 , the authors in [3] developed\nASR systems for 4 sub-saharan african languages (Swahili,\nHausa, Amharic and W olof). Another language of W est Africa\n(Y oruba) spoken mainly in Nigeria, in Benin and neighboring\ncountries has been also investigated for an ASR system.\n[4] provides a brief review of research progress on Y or ´ ub` a\nAutomatic Speech Recognition.\nOur main objective in this paper is to introduce a ﬁrst ASR\nsystem for an under-resourced language, Fongbe. Fongbe is\na vernacular language spoken primarily in Benin, by more\nthan 50% of the population, in T ogo and in Nigeria. It’s an\nunder-resourced because it lacks linguistics resources (s peech\ncorpus and text data) and very few websites provide textual\ndata. Building these resources, acoustic models and langua ge\nmodels for Fongbe ASR becomes a challenging task. For this,\nwe used Kaldi toolkit 2 that has allowed us to train our acoustic\nmodels on speech data that we have collected ourselves. For\nthe language modeling, we used SRILM toolkit 3 to built\ntrigram language models that we trained on collected text\ndata. T o enhance performance of our ASR, we subsequently\ntransformed the vowels by normalizing different tones of\nFongbe. Experiments have shown a signiﬁcant improvement\nin the results given by the world error rates (WER).\nThe remainder of this paper is organized as follows. The\nnext section describes the target under-resourced languag e that\nis Fongbe. Section 3 describes how text and speech corpora\nhave been collected. Section 4 and 5 focus respectively on\nlanguage modeling and acoustic modeling. Section 6 present s\n1 http://alffa.imag.fr\n2 kaldi.sourceforge.net/\n3 www .speech.sri.com/projects/srilm/\nProceedings of the Federated Conference on Computer Science\nand Information Systems pp. 477–482\nDOI: 10.15439/2016F153\nACSIS, V ol. 8. ISSN 2300-5963\n978-83-60810-90-3/$25.00 c⃝ 2016, IEEE 477\nand comments the experimental results of WER that we\nobtained. Section 7 concludes this paper and presents futur e\nwork.\nII. D E S CRIP T IO N O F FO N G BE L A N G UAG E\nFongbe language is the majority language of Benin, which\nis spoken by more than 50% of Benin’s population, including\n8 million speakers and also spoken in Nigeria and T ogo. The\nFongbe people are the largest ethnic group in Benin. Fongbe\nis part of the Gbe dialect cluster and is spoken mainly in\nBenin [6]. It is quite widespread in the media and is used\nin schools, including adult literacy. The Fongbe group is on e\nof the ﬁve Gbe dialect. J. Greenberg classiﬁes Fongbe in the\nKwa languages group in the Niger-Congo branch of the large\nfamily Niger-Kordofan [5]. It is written ofﬁcially in Benin\nwith an alphabet derived from the Latin writting since 1975.\nIt has a complex tonal system, with two lexical tones, high\nand low , which may be modiﬁed by means of tonal processes\nto drive three further phonetic tones: rising low-high, fal ling\nhigh-low and mid [6]. The use of diacritical marks to transcr ibe\nthe different tones of the language is essential even if they\nare not always marked since Fongbe is originally a spoken\nlanguage. The Fongbe’s vowel system is well suited to the\nvocalic timbre as it was designed by the ﬁrst Phoneticians.\nIt includes twelve timbres: 7 oral vowels with 4 degrees of\naperture and 5 nasal vowels with 3 degrees of aperture. Its\nconsonant system includes 22 phonemes.\nScientiﬁc studies on the Fongbe started in 1963 with the\npublication of Fongbe-French dictionary [7]. Since 1976,\nseveral linguists have worked on the language and many paper s\nwere published on the linguistic aspects of Fongbe. Unlike\nmost of the western languages (English, French, Spanish,\netc) and some Asian languages (Chinese, Japanese, etc) and\nAfrican (W olof, Swahili, shrugged, etc.) the Fongbe langua ge\nsuffers from a very signiﬁcant lack of linguistic resources\nin digital form (text corpus and speech) despite the many\nlinguistic works (phonology, lexicon and syntax).\nIII. C O L L E CT IO N O F L A N G UAG E RE S O U RCE S\nThe development of automatic continuous speech recogni-\ntion system is made from a large amount of data which must\ncontain both speech signals (for the acoustic modeling of th e\nsystem) and also text data (for the language model of the\nsystem). It becomes a challenge and very difﬁcult when it is\nan under-resourced language that still doesn’t possess the se\ndigital resources. In this section we describe the methodol ogy\nused to collect texts and audio signals of Fongbe language fo r\nbuilding of the recognition system.\nA. Speech corpus\nAs an audio corpus is not available for Fongbe, we pro-\nceeded to the speech signals collection to build the audio\ndata for the system. W e thus conducted the tedious task of\nrecording the texts pronounced by native speakers (includi ng\n8 women and 20 men) of Fongbe in a noiseless environment.\nW e have recorded at 16Khz 28 native speakers who have\nspoken around 1500 phrases (from daily living) grouped into 3\ncategories. A category is read by several speakers and conta ins\ntexts that are different from contents of other categories. These\nrecordings were made with an android application referred t o\nas LigAikuma [8] which is developed by GET ALP group of\nGrenoble’s Computer Science Laboratory. Overall, there ar e\naround 10 hours of speech data that have been collected. Firs t,\nwe split the data by categories leading to a ﬁrst conﬁguratio n\nFC1: 2 categories for training (8 hours) and 1 category for\ntesting (2 hours). Next, we split the data by speakers leadin g to\na second conﬁguration FC2: 20 speakers (8 hours) for trainin g\nand 8 speakers (2 hours) for testing. W e split the data this wa y\nﬁrstly to make sure that category appear in test data will not\nappear in training and secondly, to reduce the chance of havi ng\nspeakers overlapping between training and testing.\nT ABLE I\nCO N T E N T S O F FO N G B E S P E E C H C O R P U S .\nSpeech\nsegments\nPhrases Duration Categories Speakers\nFC1 - conﬁg\nTrain data 8,234 879 7h\n35mn\nC2 &\nC3\n25\nT est data 2,168 542 1h\n45mn\nC1 4\nFC2 - conﬁg\nTrain data 8,651 1,421 8h C1, C2\n& C3\n21\nT est data 1,751 1,410 2h C1, C2\n& C3\n7\nB. T ext corpus\nT o build a language model we need to have a text corpus\ncontaining thousands of words of the given language. The\nstandard way most commonly used to build a text corpus\nis the collection of texts from websites. As we have shown\nin previous sections, Fongbe is an under-resourced languag e\nand thus has a very limited number of websites compared\nto languages such as W olof, Hausa, and above all Arabic,\nFrench and English that have a very large wide coverage on\nthe internet and do not suffer from lack of textual data. So,\nbased on the few websites that provide texts in Fongbe, we\nused RLA T [9] to crawl text from these websites covering few\ntexts from everyday life and many texts of the Bible translat ed\ninto Fongbe. RLA T enables us to crawl text from a given web-\npage with different link depths. For improving the quantity of\ntexts obtained from HTML links of websites, we have added\nto our corpus some texts obtained from PDF ﬁles that cover\nmany of Fongbe citations, songs and the Universal Declarati on\nof Human Rights. After extracting all text content in web pag es\nand pdf ﬁle, we conducted to a cleaning and normalization of\nthe texts:\n1) remove all HTML tags and codes,\n2) remove empty lines and punctuations,\n3) conversion of texts to Unicode,\n4) remove pages and lines from other languages than\nFongbe,\n478 PROCEEDINGS OF THE FEDCSIS. GDA ´NSK, 2016\n5) transcription of special characters and numbers,\n6) delete duplicate lines.\nIn total, we obtained nearly 10,130 words to build our vocabu -\nlary dictionary and a corpus which contains 34,653 sentence s\ncollected from the few documents written in Fongbe that are\nactually available. In table II, we list the websites used to\nextract text for two language models (LM1 and LM2) and from\nwhich we selected 1,500 utterances (source 1) for recording\nspeech data for the training and testing set.\nT ABLE II\nCO N T E N T S O F T E X T C O R P U S .\nSource\nW ebsites T ext utterances\n1 http://www .fonbe.fr variety of texts in\ndaily life\n1,500\n2 http://unicode.org/\nudhr/d/udhr_fon.txt\nUniversal Decla-\nration of Human\nRights\n92\n3 http://ipedef-fongbe.\norg/\nEducational\ntexts, songs and\ntales\n2,200\n4 http://www .\nvodoo-beninbrazil.\norg/fon.html\nEducational\nT exts\n1,055\n5 https://www .bible.\ncom/fr/bible/813/dan\nThe Bible 29,806\nIV . L A N G UAG E M O D E L IN G\nStatistical language models (LM) are employed in various\nnatural language processing applications, such as machine\ntranslation, information retrieval or automatic speech re cogni-\ntion. they describe relations between words (or other token s),\nthus enabling to choose most probable sequences. This prove s\nto be especially useful in speech recognition, where acoust ical\nmodels usually produce a number of hypotheses, and re-\nranking them according to a language model can substantiall y\nimprove recognition rates [10] T o compare the performance o f\nour Fongbe recognition system, we built two language models\n(LM) using the same text corpus. The ﬁrst language model\n(LM1) is built with the original texts after normalization a nd\ncontain different tonal vowels. The use of tonal vowels impl ies\nthat the system has to handle 26 vowels (with accented char-\nacters) considered as different tones instead of the 12 init ial\nvowels. The second language model is built with the original\ntexts that we modiﬁed by performing a second normalization\non different tonal vowels from text corpus. The normalizati on\nwas made by removing the tones from vowels and replacing\naccented characters by single characters. The result is tha t we\nhave new entries with their transcriptions in our vocabular y\ndictionary. For example, the original word ax´ Os´ u, which means\nking will become in the dictionary axOsu. T able III summarizes\nthe various changes made to the vowels.\nW e used SRILM toolkit to train the two languages models.\nLM1 and LM2 were trained on 995,338 words (10,095 uni-\ngrams) by using the training data from text corpus (1,054,72 4\nwords, 33,153 sentences) without utterances used for the\nspeech corpus (5,490 words and 1,500 sentences removed).\nLM1 was trained with the original texts while LM2 was\nT ABLE III\nVOW E L N O R M A L I Z AT I O N .\nT onal vowels\nNormalization\n´ a /a/\n` a /a/\n˜ a /a/\n´ o /o/\n` o /o/\n˜ o /o/\n´ e /e/\n` e /e/\n˜ e /e/\n´ u /u/\n` u /u/\n˜ u /u/\n´ i /i/\n` i /i/\n˜i /i/\n´ E /E/\n` E /E/\n˜ E /E/\n´ O /O/\n` O /O/\n˜ O /O/\ntrained with the modiﬁed texts by vowel normalization. T o\nrepresent the uncertainty of our language models, we calcul ate\nthe perplexity values of all the utterance transcriptions f rom\nspeech corpus that are not contained in the various text\ncorpus and which represents our test data to evaluate the\nperformance of the two language models. T able V shows the\nperplexity values. The vowel normalization after the origi nal\ntext modiﬁcation has positive impact on the quality of the\nlanguage model by reducing in the OOV from 9.1% to 4.96%.\nThis leads to observe a signiﬁcant perplexity improvement\nwith LM2 compared to LM1. Final system has been built using\na lexicon which contains 10,130 unique grapheme words. As\nin [12], [11], we used grapheme as modeling unit to create\nour own lexicon because. An example of its content obtained\nafter text pre-processing is shown in T able IV.\nT ABLE IV\nE X A M P L E O F L E X I C O N ’ S C O N T E N T\nW ord Graphemes\nOriginal text ax´ Os´ uãuãu a x ´ O s ´ u ã u ã u\nh˜ agb´ E h ˜ ag b ´ E\nV owel nor-\nmalization\naxOsuãuãu a x O s u ã u ã u\nhaagbE h a a g b E\nT ABLE V\nL A N G UAG E MO D E L C O M PA R I S O N U S I N G T H E P E R P L E X I T Y .\nLM V ocab\n(words)\nOOV PPL\nLM1 10,130 9.1% 591\nLM2 8,244 4.96% 138\nV . A CO U S T IC M O D E L IN G\nIn this section, we describe the methods that we used for\ntraining and testing our 2 conﬁgurations (FC1 and FC2) and\nFR ´EJUS LALEYE ET AL.: FIRST AUTOMA TIC FONGBE CONTINUOUS SPEECH RECOGNITION SYSTEM 479\npresent in the next section the obtained results. The record ings\nand their transcriptions are used for acoustic modeling. Th e\nAcoustics models (AMs) are trained and tested on acoustic\ndata from both FC1 and FC2 by using Kaldi acoustic modeling\nscripts that we have adapted to produce Kaldi scripts for\nFongbe. W e not only explored AM training methods but\nalso experimented the impact of presence of tones in the\nutterances transcription from speech corpus by using LM1\n(with tones) or LM2 (no tones). Thus, FC1 and FC2 training\nare performed not only with the same scripts but also by using\nboth pronunciation dictionary. The pronunciation diction ary\nbased grapheme that is used with LM1 contains 49 graphemes\nwhile the dictionary used with LM2 contains 28 graphemes.\nThe models are trained with 13 MFCC (Mel-Frequency\nCesptral Coefﬁcients) features whose coefﬁcients are trip led\nwith the ∆+∆∆ by computing the ﬁrst and second derivatives\nfrom MFCC coefﬁcients. W e also computed other feature\ntransformation techniques such as LDA (Linear Discriminan t\nAnalysis) and MLL T (Maximum Likelihood Linear Trans-\nform) which gain substantial improvement over ∆+∆∆ trans-\nformation. Subsequently, we also applied speaker Adaptati on\nwith feature-space Maximum Likelihood Linear Regression\n(fMLLR). Refer to the papers [13] and [14] for details on\nthe theory of these transformation techniques implemented in\nKaldi ASR. Figure 1 and T able VI show the hierarchy of the\nacoustics models that we trained in our experiments. In this\nhierarchy, we started by training monophone model using the\nMFCC features and we ended up training of SGMM using\nfMMI transformed features. The intermediate triphone mode ls\nare also trained as shown in Figure 1. For decoding, we used\nthe different trained acoustics models with the utterances from\nthe test data. For each trained acoustic model we used the sam e\nspeech parametrization and feature transformation method as\nwas used for the given acoustic model at training time.\nT ABLE VI\nAC O U S T I C S MO D E L S . C O M B I N E * R E P L AC E D\nC O M B I N E _T R I 3B_F MM I _I N D I RE C T _S G MM 2_5 B 2_ M MI _B0.1\nTraining method\nScript\nMonophone mono\nTriphone tri1\n∆ + ∆∆ tri2a\nLDA + MLL T tri2b\nLDA + MLL T + SA T + FMLLR tri3b\nLDA + MLL T + SA T + FMLLR\n+ fMMI\ntri3b_fmmi_a\nLDA + MLL T + SA T + FMLLR\n+ MMI\ntri3b_mmi_b0.1\nLDA + MLL T + SA T + FMLLR\n+ fMMI + MMI\ntri3b_fmmi_indirect\nLDA + MLL T + SGMM sgmm2_5b2\nLDA + MLL T + SGMM + MMI sgmm2_5b2_mmi_b0.1\nLDA + MLL T + SGMM + fMMI\n+ MMI\ncombine*\nVI. E X P E RIM E N TA L RE S U LT S\nThe experiments focus on comparing the quality of ASR\nhypothesis measured by WER on AMs trained by different\nmethods. T o obtain the best path, we followed the standard\nmono\ntri1\ntri2a\ntri2b\ntri3b\ntri3b_fmmi_a tri3b_mmi_b01\ntri3b_fmmi_indirect\nsgmm2_5b2\ncombine\nsgmm2_5b2_mmi_b01\nFig. 1. Hierarchy of trained 2coustics models\nKALDI procedures and report the best WER. The experiments\nwere performed ﬁrst on LM1 built with the original texts and\nusing both speech data conﬁgurations. Then we conducted\nexperiments based on the same procedures on LM2 including\ntexts without diacritics. The interest is to measure the imp act\nof using diacritics in language modelling from the results\ngiven by the WER. W e also showed how the data speech\nconﬁguration inﬂuence the quality of AMs measured by WER.\nA. Results before vowel normalization\nIn this subsection we present the results of different acous tic\ntraining methods according to data speech conﬁguration. T a ble\nVII presents AMs results for LM1.\nFrom the results in table VII, we can see that the monophone\nAM has the worst WER while the best performances are\nachieved with the sgmm2_5b2 AM for FC1-conﬁg and the\nsgmm2_5b2_mmi for FC2-conﬁg. W e can thus notice that\nthe monophone AM is typically used for the initialization\nof triphone models. The quality of speech recognition varie s\naccording to the used discriminative training method. The\nLDA+MLL T is more effective feature transformation than\nusing ∆ + ∆∆ features. There are subtle performance dif-\nferences among the discriminatively trained acoustic mode l.\nThe WER on both speech data conﬁguration for ﬁxed LM1\nis around 44%. This can be explained by the complexity of\nFongbe language for modelling the diacritics and the qualit y\nof language model used (LM1). The perplexity reported in\ntable V justiﬁes this assertion. Figure VI-A shows the curve\nperformances of the acoustic training methods for both spee ch\ndata conﬁguration.\nB. Results after vowel normalization\nT able VIII presents two WERs of different acoustic training\nmethods according to data speech conﬁguration. In the secon d\n480 PROCEEDINGS OF THE FEDCSIS. GDA ´NSK, 2016\nT ABLE VII\nW E R O F L M 1-B A S E D A S R ( W I T H D I AC R I T I C S ) F O R D I FFE R E N T\nT R A I N I N G M O N O P H O N E A N D T R I P H O N E M E T H O D S\nSpeech data conﬁg/ method WER %\nFC1-conﬁg\nMonophone (a) 69.44\nTriphone (b) 69.13\n∆ + ∆∆ (c) 70.21\nLDA + MLL T (d) 65.7\nLDA + MLL T + SA T + FMLLR (e) 54.96\nLDA + MLL T + SA T + FMLLR + fMMI (f) 55.36\nLDA + MLL T + SA T + FMLLR + MMI (g) 51.11\nLDA + MLL T + SA T + FMLLR + fMMI + MMI (h) 55.60\nLDA + MLL T + SGMM (i) 44.04\nLDA + MLL T + SGMM + MMI (j) 47.11\nLDA + MLL T + SGMM + fMMI + MMI (k) 49.83\nFC2-conﬁg\nMonophone (a) 71.97\nTriphone (b) 60.37\n∆ + ∆∆ (c) 59.74\nLDA + MLL T (d) 57.52\nLDA + MLL T + SA T + FMLLR (e) 51.47\nLDA + MLL T + SA T + FMLLR + fMMI (f) 53.06\nLDA + MLL T + SA T + FMLLR + MMI (g) 52.75\nLDA + MLL T + SA T + FMLLR + fMMI + MMI (h) 52.37\nLDA + MLL T + SGMM (i) 49.85\nLDA + MLL T + SGMM + MMI (j) 44.09\nLDA + MLL T + SGMM + fMMI + MMI (k) 44.17\nFig. 2. Inﬂuence of speech data conﬁguration on speech recog nition quality .\nLM2 is ﬁxed and only speech data and acoustic models vary . The letter in\nabscissa represent acoustic training methods labelled in t able VI-A\ncolumn (LM2-Based ASR), we have included the WER results\nof ASR performed after vowel normalization (without diacri t-\nics).\nColunm of LM2-Based ASR in T able VIII also shows that\ntriphone models signiﬁcantly improve the monophone model\nperformance. The tri2b+SA T+FMLLR acoustic model adapted\nto speaker from feature-space Maximum Likelihood Linear\nRegression reduced the WER by 6% absolute for both speech\ndata conﬁguration. The WER on FC1-conﬁg is lower than 20%\nfor discriminative methods based on tri3b. For FC2-conﬁg,\nthese acoustic training methods reduced the WER by 20%.\nThe best results are coming from the training for Subspace\nGaussian Mixture Models (SGMM), with an overall WER\nof 14.83% for FC1-conﬁg and 28.93% for FC2-conﬁg. The\nspeech data divided by speakers helps us to obtain a relative\ngain of 14% with the best ﬁnal WER of 14.83%. This leads us\nto choose AM training methods using SGMM for performance\nT ABLE VIII\nW E R O F L M 2-B A S E D A S R ( W I T H O U T D I AC R I T I C S ) A N D L M 1’-B A S E D\nA S R ( R E M OV I N G O F D I AC R I T I C S F RO M H Y P OT H E S E S A N D R E F E R E N C E S\nO F L M 1-B A S E D A S R).\nSpeech data conﬁg/ method LM2-\nBased\nASR\nLM1’-\nBased\nASR\nFC1-conﬁg\nMonophone (a) 36.36 59.05\nTriphone (b) 28.19 46.8\n∆ + ∆∆ (c) 28.21 46.98\nLDA + MLL T (d) 24.4 41.52\nLDA + MLL T + SA T + FMLLR (e) 17.83 29.29\nLDA + MLL T + SA T + FMLLR + fMMI (f) 19.72 31.34\nLDA + MLL T + SA T + FMLLR + MMI (g) 18.93 35.59\nLDA + MLL T + SA T + FMLLR + fMMI + MMI (h) 18.26 35.44\nLDA + MLL T + SGMM (i) 15.23 20.56\nLDA + MLL T + SGMM + MMI (j) 15.3 20.68\nLDA + MLL T + SGMM + fMMI + MMI (k) 14.83 21.39\nFC2-conﬁg\nMonophone (a) 52.26 57.89\nTriphone (b) 38.72 47.47\n∆ + ∆∆ (c) 38.58 46.39\nLDA + MLL T (d) 35.34 42.45\nLDA + MLL T + SA T + FMLLR (e) 30.74 35.63\nLDA + MLL T + SA T + FMLLR + fMMI (f) 35.36 37.46\nLDA + MLL T + SA T + FMLLR + MMI (g) 32.38 36.19\nLDA + MLL T + SA T + FMLLR + fMMI + MMI (h) 32.94 37.52\nLDA + MLL T + SGMM (i) 31.64 31.58\nLDA + MLL T + SGMM + MMI (j) 31.36 32.75\nLDA + MLL T + SGMM + fMMI + MMI (k) 28.93 32.02\ncomparison among FC1-conﬁg and FC2-conﬁg. Figure VI-B\nshows the evolution of WER depending on acoustic models\nwith LM2.\nIt is therefore remarkable that the language model LM2\ngives very satisfactory decoding results compared to LM1\nstandard (with diacritics). Adding diacritics in text corp us be-\nfore language modelling maked the speech recognition syste m\nless efﬁcient by increasing the WER by 44.04% compared to\n15.23% (performance without diacritics). While diacritic s add\ninformation, which should help the recognition system, it a lso\nincreases OOV rate and perplexity of the language model (see\ntable V).\nFig. 3. Inﬂuence of speech data conﬁguration on speech recog nition quality .\nLM2 is ﬁxed and only speech data and acoustic models vary . The letter in\nabscissa represent acoustic training methods labelled in t able VI-B\nFor further, we performed an effective comparison of ASR\nperformance without diacritics. T o do this, we removed the\nFR ´EJUS LALEYE ET AL.: FIRST AUTOMA TIC FONGBE CONTINUOUS SPEECH RECOGNITION SYSTEM 481\ndiacritics from the outputs (hypotheses) and references of ASR\nsystem built with LM1 (LM1’-Based ASR). The obtained\nresults for this evaluation are included in the third column\nof T able VIII. These results can be compared to results\nobtained with LM2-Based ASR system (second column). This\ncomparison leads us to assert that the removing of diacritic s\nfor different models is more effective and provides an efﬁci ent\nASR system.\nVII. C O N CL U S IO N\nIn this work we introduced the ﬁrst system of Fongbe\ncontinuous speech recognition by training different acous tic\nmodels using Kaldi scripts and different language models\nusing SRILM toolkit. W e also demonstrated the effect of\ntones on the quality of the recognition system. This leads\nus to conclude that with the current state of our system, the\nlanguage modelling without diacritics improves signiﬁcan tly\nthe recognition performances by decreasing the WER by\n15.23% for speech data divided by speakers and 28.93%\nfor speech data divided by category. Using the Kaldi recipe\nand the language resources we provide, researcher can build\na Fongbe recognition system with the same WER obtained\nin this paper. For future work, ﬁrstly we will enhance the\nspeech and text data and introduce other training technique s\nto further improve the performance of this ﬁrst system of\nFongbe recognition. Secondly, we will investigate the Fong be\nre-diacritization in the context of Speech recognition.\nRE F E RE N CE S\n[1] J. K. T amgno and E. Barnard and C. Lishou and M. Richomme, W olof\nSpeech Recognition Model of Digits and Limited-V ocabulary Based\non HMM and T oolKit , in. 14th International Conference on Computer\nModelling and Simulation (UKSim), pp. 389–395, 2012 UKSim.\n[2] Besacier, L., Barnard, E., Karpov , A., and Schultz, T . (2 014). Automatic\nspeech recognition for under-resourced languages: A surve y . Speech\nCommunication, 56:85–100.\n[3] E. Gauthier and L.Besacier and S. V oisin and M. Melese and U. P . Elin-\ngui Collecting Resources in Sub-Saharan African Languages for Auto-\nmatic Speech Recognition: a Case Study of W olof in. 10th edition of\nthe Language Resources and Evaluation Conference, 23-28 Ma y 2016,\nSlovenia.\n[4] S. A. M. Y usof and A. F . Atanda and M. Hariharan, A review of\nY or´ ub` a Automatic Speech Recognition , in. System Engineering and\nT echnology (ICSET), IEEE 3rd International Conference on, pp. 242–\n247, Aug.2013.\n[5] J. Greenberg, Languages of Africa , La Haye Mouton, pp. 177, 1966.\n[6] C. Lefebvre and A-M. Brousseau, A grammar of F onge , De Gruyter\nMouton, PP . 608, December 2001.\n[7] A. B. AKOHA, Syntaxe et lexicologie du F on-gbe: Bénin , Ed.\nL ’harmattan, pp. 368, January 2010.\n[8] Blachon, D., Gauthier, E., Besacier, L., Kouarata, G.-N ., Adda-Decker,\nM., and Rialland, A. (2016). Parallel speech collection for under-\nresourced language studies using the LIG-Aikuma mobile dev ice app.\nIn Proceedings of SLTU (Spoken Language T echnologies for Unde r-\nResourced Languages), Y ogyakarta, Indonesia.\n[9] A. W . Black and T . Schultz, Rapid Language Adaptation T ools and\nT echnologies for Multilingual Speech Processing, in Automatic Speech\nRecognition & Understanding, IEEE W orkshop, pp. 51, 2009.\n[10] Sebastian Dziadzio, Aleksandra Nabozny , Aleksander S mywinski-Pohl\nand Bartosz Ziolko, Comparison of Language Models Trained on\nWritten T exts and Speech Transcripts in the Context of Automatic Speech\nRecognition, in Proc. Proceedings of the IEEE Federated Conference on\nComputer Science and Information Systems, 5, pp. 193-197, P ologne\n2015.\n[11] S. Seng and S. Sam and V . Bac Le and B. Bigi and L. Besacier, Which\nunits for acoustic and language modeling for Khmer automati c speech\nrecognition?, SL TU 2008.\n[12] J. Billa and all, Audio indexing of Arabic broadcast news, in Proc. IEEE\nInternational Conference on Acoustique, Speech and Signal s Processing,\npp. 5-8, Orlando 2002.\n[13] D. Povey and A. Ghoshal et al., The Kaldi Speech Recognition T oolkit,\nin IEEE ASRU, 2011.\n[14] D. Povey and G.Saon, F eature and model space speaker adaptation\nwith full covariance Gaussians, in INTERSPEECH 2006 - ICSLP , Ninth\nInternational Conference on Spoken Language Processing, P ittsburgh,\nP A, USA, September 17-21, 2006\n[15] Lukasz Laszko, W ord detection in recorded speech using textual queries,\nin Proc. Proceedings of the IEEE Federated Conference on Com puter\nScience and Information Systems, 5, pp. 849-853, Pologne 20 15.\n482 PROCEEDINGS OF THE FEDCSIS. GDA ´NSK, 2016",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7584969401359558
    },
    {
      "name": "Speech recognition",
      "score": 0.7232077121734619
    },
    {
      "name": "Acoustic model",
      "score": 0.6430697441101074
    },
    {
      "name": "Language model",
      "score": 0.5016047954559326
    },
    {
      "name": "Hidden Markov model",
      "score": 0.4801437556743622
    },
    {
      "name": "Natural language processing",
      "score": 0.40446725487709045
    },
    {
      "name": "Speech processing",
      "score": 0.36749058961868286
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35956376791000366
    }
  ]
}