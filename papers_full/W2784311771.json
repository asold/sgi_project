{
    "title": "End-to-End Quantum-like Language Models with Application to Question Answering",
    "url": "https://openalex.org/W2784311771",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A1972955069",
            "name": "Peng Zhang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2557745259",
            "name": "Jiabin Niu",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2050912551",
            "name": "Zhan Su",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2231501501",
            "name": "Benyou Wang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2104047888",
            "name": "Liqun Ma",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1980846011",
            "name": "Dawei Song",
            "affiliations": [
                "Tianjin Open University"
            ]
        },
        {
            "id": "https://openalex.org/A1972955069",
            "name": "Peng Zhang",
            "affiliations": [
                "Tianjin University of Science and Technology",
                "Tianjin University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2557745259",
            "name": "Jiabin Niu",
            "affiliations": [
                "Tianjin University of Science and Technology",
                "Tianjin University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2050912551",
            "name": "Zhan Su",
            "affiliations": [
                "Tianjin University of Technology",
                "Tianjin University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2231501501",
            "name": "Benyou Wang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2104047888",
            "name": "Liqun Ma",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A1980846011",
            "name": "Dawei Song",
            "affiliations": [
                "Tianjin University of Technology",
                "Tianjin University of Science and Technology",
                "The Open University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2189408051",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W1563364586",
        "https://openalex.org/W282425278",
        "https://openalex.org/W1492375252",
        "https://openalex.org/W6667733676",
        "https://openalex.org/W2170738476",
        "https://openalex.org/W2059059956",
        "https://openalex.org/W6702864554",
        "https://openalex.org/W6631834165",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2087746031",
        "https://openalex.org/W2291880741",
        "https://openalex.org/W2539338396",
        "https://openalex.org/W1966443646",
        "https://openalex.org/W2185337446",
        "https://openalex.org/W2128739123",
        "https://openalex.org/W2160416736",
        "https://openalex.org/W3101747393",
        "https://openalex.org/W209109747",
        "https://openalex.org/W2265289447",
        "https://openalex.org/W2251202616",
        "https://openalex.org/W2339896096",
        "https://openalex.org/W2120735855",
        "https://openalex.org/W1926201870",
        "https://openalex.org/W2163382007",
        "https://openalex.org/W6703254000",
        "https://openalex.org/W2102306213",
        "https://openalex.org/W4244716531",
        "https://openalex.org/W1915024344",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W1591825359",
        "https://openalex.org/W2211192759",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W2337120330",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W1485275430",
        "https://openalex.org/W2963053846",
        "https://openalex.org/W2950193743",
        "https://openalex.org/W2336251867",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2013784198",
        "https://openalex.org/W2251818205",
        "https://openalex.org/W4211080241",
        "https://openalex.org/W2951359136",
        "https://openalex.org/W2315127070",
        "https://openalex.org/W2070217890",
        "https://openalex.org/W2264105282",
        "https://openalex.org/W4297945192",
        "https://openalex.org/W2137852732",
        "https://openalex.org/W1532325895",
        "https://openalex.org/W2335622251",
        "https://openalex.org/W2538374209"
    ],
    "abstract": "Language Modeling (LM) is a fundamental research topic in a range of areas. Recently, inspired by quantum theory, a novel Quantum Language Model (QLM) has been proposed for Information Retrieval (IR). In this paper, we aim to broaden the theoretical and practical basis of QLM. We develop a Neural Network based Quantum-like Language Model (NNQLM) and apply it to Question Answering. Specifically, based on word embeddings, we design a new density matrix, which represents a sentence (e.g., a question or an answer) and encodes a mixture of semantic subspaces. Such a density matrix, together with a joint representation of the question and the answer, can be integrated into neural network architectures (e.g., 2-dimensional convolutional neural networks). Experiments on the TREC-QA and WIKIQA datasets have verified the effectiveness of our proposed models.",
    "full_text": "End-to-End Quantum-Like Language\nModels with Application to Question Answering\nPeng Zhang,1,∗ Jiabin Niu,1 Zhan Su,1 Benyou Wang,2 Liqun Ma,3 Dawei Song1,4,∗\n1. School of Computer Science and Technology, Tianjin University, Tianjin, China\n2. Department of Social Network Operation, Social Network Group, Tencent, Shenzhen, China\n3. School of Electrical and Information Engineering, Tianjin University, Tianjin, China\n4. Computing and Communications Department, The Open University,United Kingdom\n* Corrrespondence:{pzhang, dwsong}@tju.edu.cn\nAbstract\nLanguage Modeling (LM) is a fundamental research topic in a\nrange of areas. Recently, inspired by quantum theory, a novel\nQuantum Language Model (QLM) has been proposed for In-\nformation Retrieval (IR). In this paper, we aim to broaden the\ntheoretical and practical basis of QLM. We develop a Neural\nNetwork based Quantum-like Language Model (NNQLM)\nand apply it to Question Answering. Speciﬁcally, based on\nword embeddings, we design a new density matrix, which\nrepresents a sentence (e.g., a question or an answer) and en-\ncodes a mixture of semantic subspaces. Such a density matrix,\ntogether with a joint representation of the question and the\nanswer, can be integrated into neural network architectures\n(e.g., 2-dimensional convolutional neural networks). Experi-\nments on the TREC-QA and WIKIQA datasets have veriﬁed\nthe effectiveness of our proposed models.\nIntroduction\nLanguage Models (LM) play a fundamental role in Artiﬁcial\nIntelligence (AI) related areas, e.g., natural language pro-\ncessing, information retrieval, machine translation, speech\nrecognition and other applications. The commonly used lan-\nguage models include statistical language models and neural\nlanguage models. Generally speaking, statistical language\nmodels compute a joint probability distribution over a se-\nquence of words (Manning, Raghavan, and Sch¨utze 2008;\nZhai 2008), while neural language models can obtain a dis-\ntributed representation for each word (Bengio et al. 2003;\nMikolov et al. 2013).\nRecently, by using the mathematical formulations of\nquantum theory, a Quantum Language Model (QLM), has\nbeen proposed in Information Retrieval (IR). QLM encodes\nthe probability uncertainties of both single and compound\nterms in a density matrix, without resorting to extend the\nvocabulary artiﬁcially (Sordoni, Nie, and Bengio 2013). The\nranking of documents against a query is based on the von-\nNeumann divergence between the density matrices of the\nquery and each document. QLM shows an effective perfor-\nmance on the ad-hoc retrieval task.\nQLM is theoretically signiﬁcant, as for the ﬁrst time it\ngeneralizes LM via the formulations of Quantum theory.\nHowever, it has the following limitations. First, in QLM, the\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nrepresentation for each term is a one-hot vector, which only\nencodes the local occurrence, yet without taking into ac-\ncount the global semantic information. Second, QLM repre-\nsents a sequence of terms (e.g., a query or a document) by a\ndensity matrix, which is estimated via an ite-Lrative process,\nrather than an analytical procedure. Thus it is difﬁcult to in-\ntegrate such a matrix in an end-to-end design. Third, QLM\ndeals with the representation, estimation and ranking pro-\ncesses sequentially and separately. As a consequence, these\nthree steps cannot be jointly optimized, thus limiting QLM’s\napplicability and impact in the related research areas.\nIn this paper, we aim to broaden the theoretical and practi-\ncal basis of QLM, by addressing the above problems. Specif-\nically, we adopt the word embeddings to represent each word\nsince such a distributed representation can encode more se-\nmantic information than one-hot vectors. By treating each\nembedding vector as an observed state for each word, a sen-\ntence (e.g., a question or an answer) can correspond to a\nmixed state represented by a density matrix. Then, we can\nderive such a density matrix without an iterative estimation\nstep. This makes the density matrix representation feasible\nto be integrated into a neural network architecture and au-\ntomatically updated by a back propagation algorithm. After\ngetting the word level and sentence level representations, we\ncan have a joint representation for two sentences (e.g., ques-\ntion and answer sentences in the answer selection task).\nBased on the above ideas, we propose an end-to-end\nmodel, namely Neural Network based Quantum-like Lan-\nguage Model (NNQLM). Two different architectures are de-\nsigned. The ﬁrst is just adding a single softmax layer to the\ndiagonal values and its trace value of the joint representa-\ntion. The second is built upon a Convolutional Neural Net-\nwork (CNN), which can automatically extract more useful\npatterns from the joint representation of density matrices.\nWe clarify that our motivation of using quantum theory is\nto inspire new perspectives and formulations for the natural\nlanguage applications, instead of developing quantum com-\nputation algorithms. Indeed, one can build the analogy be-\ntween quantum theory (e.g., quantum probability) and some\nmacro-world problems (Bruza, Wang, and Busemeyer 2015;\nWang et al. 2016). For sake of applicability, our model does\nnot fully comply with the theory of quantum probability, so\nthat we will use “quantum-like” instead of “quantum” when\nreferring to the language model we propose in this paper. To\nThe Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI-18)\n5666\nthe best of our knowledge, this is the ﬁrst attempt to integrate\nthe quantum-like probability theory with the neural network\narchitecture in Natural Language Processing (NLP) tasks.\nWe apply the proposed end-to-end quantum-like language\nmodels to a typical QA task, namely Answer Selection,\nwhich aims to ﬁnd accurate answers from pre-selected set of\ncandidates (Yang, Yih, and Meek 2015). For each single sen-\ntence (question or answer), the density matrix will represent\nthe mixture of semantic subspaces spanned by embedding\nvectors. For each question-answer pair, the joint density ma-\ntrix representation encodes theinter-sentence similarity in-\nformation between the question and the answer. The neural\nnetwork architecture (e.g. 2-dimensional CNN) is adopted to\nlearn useful similarity pattensfor matching and ranking the\nanswers against the given question. A series of systematic\nexperiments on TREC\nQA and WikiQA have shown that the\nproposed NNQLM signiﬁcantly improves the performance\nover QLM on both datasets, and also outperforms a state of\nthe art end-to-end answer selection approach on TREC\nQA.\nQuantum Preliminaries\nThe mathematical formalism of quantum theory is based on\nlinear algebra. Now, we brieﬂy introduce some basic con-\ncepts and the original quantum language model.\nBasic Concepts\nIn quantum probability (V on Neumann 1955), the proba-\nbilistic space is naturally represented in a Hilbert space, de-\nnoted as H\nn. For practical reasons, the previous quantum-\ninspired models limited the problem in the real space, de-\nnoted as R\nn (Sordoni, Nie, and Bengio 2013). The Dirac’s\nnotation is often used, which denotes a unit vector⃗u ∈ Rn\nas a ket |u⟩ and its transpose ⃗uT as a bra ⟨u|. The inner\nproduct between two state vectors is denoted as⟨u|v⟩. The\nprojector onto the direction|u⟩is |u⟩⟨u|, which is anouter\nproduct (also called dyad)o f |u⟩itself. Each rank-one pro-\njector |u⟩⟨u| can represent aquantum elementary event.\nDensity matrices are a generalization of the conventional\nﬁnite probability distributions. A density matrix ρ can be\ndeﬁned as a mixture of dyads|ψi⟩⟨ψi|:\nρ =\n∑\ni\npi |ψi⟩⟨ψi| (1)\nwhere |ψi⟩ is a pure state vector with probability pi. ρ is\nsymmetric, positive semideﬁnite, and of trace 1(tr(ρ)=1).\nAccording to the Gleason’s Theorem (Gleason 1957), there\nis a bijective correspondence between the quantum probabil-\nity measure μ and the density matrixρ (i.e., μρ(|u⟩⟨u|)=\ntr(ρ|u⟩⟨u|)).\nQuantum Language Model\nA quantum language model represents a word or a com-\npound dependency between words by a quantum elemen-\ntary event. For each single wordw\ni, the corresponding pro-\njector Πi = |ei⟩⟨ei|, where |ei⟩, the standard basis vector\nassociated to a word, is an one-hot vector. Sordoni, Nie,\nand Bengio (2013) utilized the Maximum Likelihood Esti-\nmation (MLE) to estimate the density matricesρ\nq and ρd,\nSentence\nMatrix\nDensity\nMatrix\nOuter\nProducts\n࣋\nͳሬሬሬԦܶ\nʹሬሬሬԦܶ\n͵ሬሬሬԦܶ\n݊ݏሬሬሬԦܶ\nFigure 1: Single Sentence Representation\nwhich represent the queryq and document d, respectively.\nThe original MLE estimation uses a so-called RρR algo-\nrithm, which is an Expectation-Maximization (EM) iterative\nalgorithm and does not have an analytical solution. After the\nestimation of density matrices, the ranking is based on the\nnegative V on-Neumann (VN) Divergence betweenρ\nq and\nρd (i.e., −△ VN (ρq||ρd)=t r (ρq log ρd)). It turns out that\nthe original QLM cannot be directly integrated into an end-\nto-end mechanism. This motivates us to consider a Neural\nNetwork architecture.\nNeural Network based Quantum-like\nLanguage Model\nIn this section, we will describe our model in the context\nof a typical Question Answering task, namely answer se-\nlection, which aims to ﬁnd accurate answers from a set of\npre-selected candidate answers based on a question-answer\nsimilarity matching process. Note that the proposed model\nis general and can also be applied to other NLP and IR tasks\nthat involve similarity matching and ranking.\nSpeciﬁcally, we introduce our model in three steps.\nFirstly, we design an embedding based density matrix repre-\nsentation for single sentences to model the intra-sentence se-\nmantic information carried by a question/answer. Then, we\nintroduce a joint representation to model the inter-sentence\nsimilarities between a question and an answer. Finally, the\nquestion and answer are matched according to similarity\nfeatures/patterns obtained from the joint representation. All\nthese parts are integrated into a neural network structure.\nEmbedding based Density Matrix for Single\nSentence Representation\nFormally, word embeddings are encoded in an embedding\nmatrix E ∈ R|V |×d, where |V| is the length of the vo-\ncabulary and d is the dimension of the word embeddings.\nDifferent from the one-hot representation, word embeddings\nare obtained from the whole corpus or certain external large\ncorpora, and thus contain global semantic information. As\nshown in Figure 1, thei\nth word in a sentence is represented\nby a vector −→si ∈ E. Such a distributed representation for\neach word can naturally serve as an observed state for a sen-\ntence. To obtain a unit state vector, we normalize each word\n5667\nembedding vector (−→si ∈ E) as follows:\n|si⟩=\n−→si\n||−→si ||2\n(2)\nThen, a sentence (e.g., question or answer) can correspond\nto a mixed state represented by a density matrix. According\nto the deﬁnition of density matrix, we can derive it as\nρ =\n∑\ni\npiSi =\n∑\ni\npi |si⟩⟨si| (3)\nwhere ∑\ni pi =1 , and ρ is symmetric, positive semideﬁ-\nnite and of trace 1.Si is a semantic subspace spanned by\nthe embedding-based state vector |si⟩. Each outer product\n|si⟩⟨si|can be regarded as a partial Positive Operator-V alued\nMeasurement (partial POVM) (Blacoe 2014), which is more\ngeneral than the Projector-based Measurement in the origi-\nnal QLM. In addition, compared with the projectorsΠ\ni in\nQLM, Si carries more semantic information due to the em-\nbedding based state vectors|si⟩as against the one-hot vec-\ntors |ei⟩that formsΠi.\nIn Eq. 3,pi (∑\ni pi =1 ) is the corresponding probability\nof the state|si⟩with respect to theith word si in a given sen-\ntence. In practice, the values ofpi reﬂect the weights of the\nwords in different positions of the sentence, and can be con-\nsidered as a parameter automatically adjusted in the training\nprocess of the network.\nTo our best knowledge, current QA systems often directly\nalign the embedding vector for each word, but without con-\nsidering the mixture of the semantic subspaces spanned by\nthe embedding vectors. With such a mixture space, we will\nshow that some useful similarity features/patterns will be de-\nrived in our neural network based architecture. We can also\ninterpret the density matrix in Eq. 3 from the perspective\nof covariance matrix. The density matrix to some extent re-\nﬂects the covariance of different embedding dimensions for\na sentence. In other words, it represents how scattered the\nwords (in the sentence) will be in the embedded space.\nJoint Representation for Question Answer Pair\nInstead of separately modeling/projecting a text into one di-\nmensional vector and then computing a distance-based score\nbetween a pair of text fragments, two dimensional match-\ning model which uses a joint representation (a matrix or\nmulti-dimension tensor) have proven more effective (Hu et\nal. 2014; Wan et al. 2016). Based on the density matrices for\na question and an answer, we can build a joint representation\nfor modeling the interaction between two density matrices\nby their multiplication:\nM\nqa = ρqρa (4)\nwhere ρq and ρa are the density matrix representation for\nthe questionq and the answera, respectively.\nIn order to analyze the property of this joint representa-\ntion, we can ﬁrst decompose the density matrix of the query\nthrough spectral decomposition:\nρq =\n∑\ni\nλi |ri⟩⟨ri| (5)\nwhere λi is an eigenvalue and |ri⟩ is the corresponding\neigenvector. The eigenvector can be interpreted as a latent\nsemantic basis, and the eigenvalue can reﬂect how scat-\ntered the words are in the corresponding basis. Similarly,\nthe answer density matrixρ\na can be decomposed intoρa =∑\nj λj |rj⟩⟨rj|. Then the joint representation between ρq\nand ρa can be written as:\nρqρa =\n∑\ni,j\nλiλj |ri⟩⟨ri|rj⟩⟨rj|\n=\n∑\ni,j\nλiλj ⟨ri|rj⟩|ri⟩⟨rj|\n(6)\nIn Eq. 6, the more similar two bases are, the bigger the\n⟨ri|rj⟩(representing the inner product and the Cosine simi-\nlarity between|ri⟩and ⟨rj|) is. Since⟨ri|rj⟩=t r (|ri⟩⟨rj|),\nwe have\ntr(ρqρa)=\n∑\ni,j\nλiλj ⟨ri|rj⟩2 (7)\nwhich is the sum of Cosine similarities of between the la-\ntent semantic dimensions. In this way, the joint representa-\ntion can retain the distribution of similar bases and ignore\nthe dissimilar ones. More generally,tr(ρ\nqρa) is a general-\nization of inner product from vectors to matrices, which is\ncalled trace inner product (Balkır 2014). Thus, the joint rep-\nresentation matrix M\nqa encodes the similarity information\nacross the question and the answer.\nLearning to Match Density Matrices\nBased on the above ideas, we propose two Neural Network\nbased Quantum-like Language Models (NNQLM) to match\nthe question-answer pairs.\nNNQLM-I As shown in Figure 2, the ﬁrst architecture\nis designed to allow a direct and intuitive comparison with\nthe original QLM. In QLM, the similarity between a ques-\ntion and an answer is obtained by the negative VN diver-\ngence. However, because of thelog operation for the matrix,\nthe negative VN divergence is hard to be integrated into an\nend-to-end approach. In this paper, we adopt the trace inner\nproduct. Formally, the trace inner product between two den-\nsity matrices ρ\nq and ρa (for a questionq and an answera,\nrespectively) can be formulated as:\nS(ρq,ρa)=t r (ρqρa) (8)\nTrace inner product has been used to calculate the similarity\nbetween words or sentences (Blacoe, Kasheﬁ, and Lapata\n2013; Blacoe 2014) and has been proven to be an approx-\nimation of the negative VN divergence (Sordoni, Bengio,\nand Nie 2014). As aforementioned, it can be rewritten as\nx\ntrace =t r (ρqρa)= ∑\ni,j λiλj ⟨ri|rj⟩2, which can be un-\nderstood as the semantic overlaps used to compute the sim-\nilarity between the density matrices of the question and the\nanswer. In addition, the diagonal elements (forming⃗x\ndiag)\nof Mqa are also adopted to enrich the feature representation,\nsince different diagonal elements may have different degrees\nof importance for similarity measurement. Then, the feature\nrepresentation can be denoted as:\n⃗x\nfeat =[ xtrace; ⃗xdiag] (9)\n5668\nSentence\nMatrices\nOuter\n Products\nDensity\n Matrices\nJoint\nRepresentation Softmax\nSingle Sentence Representation Jo int Representation\n Matching\n݁ܿܽݎݐݔ\n݃ܽ݅݀\nܽݍࡹ\nܽ࣋ݍ࣋\nܽ࣋\nݍ࣋ࡽ\n࡭\nFigure 2: NNQLM-I. The ﬁrst three layers are to obtain the single sentence representation, the fourth layer is to obtain the joint\nrepresentation of a QA pair, and the softmax layer is to match the QA pair.\nA fully-connected layer and a softmax activation are\nadopted. The outputs of the softmax layer are the probabili-\nties of positive and negative labels of an answer. The proba-\nbility of the positive label is used as the similarity score for\nranking. The back propagation is trained with the negative\ncross entropy loss:\nL = −\nN∑\ni\n[yi log h(⃗xfeat )+( 1 −yi)l o g ( 1−h(⃗xfeat ))]\n(10)\nwhere h(⃗xfeat ) is the output from softmax. In this way, we\nextend the original QLM to an end-to-end method.\nNNQLM-II In this architecture, we will adopt a “two-\ndimensional” (2D) convolution (Hu et al. 2014) to learn rel-\natively more abstract representations, which are different\nfrom the intuitive features (e.g., trace inner productx\ntrace\nwhich is a similarity measure) in NNQLM-I. We think the\n2D convolution is more suitable for the joint representa-\ntion M\nqa,a s Mqa is not a simple concatenation of word\nembedding vectors. For the simple concatenation represen-\ntation, the convolution kernels slide only along each sin-\ngle dimension, so that the corresponding convolution can\nbe considered as “one-dimensional” (1D) convolution (Hu\net al. 2014), which is actually used in many CNN-based\nQA models (Severyn and Moschitti 2015; Y u et al. 2014;\nKim 2014).\nThe second architecture, namely NNQLM-II, is shown in\nFigure 3. Recall that in the ﬁrst architecture, only the diag-\nonal values and the trace value of the joint representation\nM\nqa are involved in the training process (see Figure 2). In\nNNQLM-II, we use 2D convolution kernels to scan all the\nlocal parts (including the diagonal elements in the ﬁrst archi-\ntecture) of the joint representation and extract/ﬁlter as many\nsimilarity patterns as possible inM\nqa.\nSuppose the number of ﬁlters is c. The ith convolution\noperation is formulated as:\nCi = δ(Mqa ∗Wi + bi) (11)\nwhere 1 ⩽ i ⩽ c, δ is the non-linear activation function, *\ndenotes the 2D convolution,Wi and bi are the weight and\nthe bias respectively for theith convolution kernel, andCi\nis the feature map. After the convolution layer obtains the\nfeature maps, we then use row-wise and column-wise max-\npooling to generate vectors⃗r\nq\ni ∈ Rd−k+1 and ⃗ra\ni ∈ Rd−k+1,\nrespectively, with the formulations as follows:\n⃗rq\ni =( qj : j =1 ,2,...,d −k +1 )\nqj =m a x\n1≤m≤d−k+1\n(Ci(j,m)) (12)\n⃗ra\ni =( aj : j =1 ,2,...,d −k +1 )\naj =m a x\n1≤m≤d−k+1\n(Ci(m,j)) (13)\nWe concatenate these vectors as follow:\n⃗xfeat =[ ⃗rq\n1; ⃗ra\n1 ; ... ; ⃗rq\ni ; ⃗ra\ni ; ... ; ⃗rq\nc ; ⃗ra\nc ] (14)\nwhere 1 ⩽ i ⩽ c . The above convolution operation aims to\nextract useful similarity patterns and each convolution ker-\nnel corresponds to a feature. We can adjust the parameters\nof the kernels when the model is being trained.\nRelated Work\nNow, we present a brief review of the related work, including\nthe recent quantum-inspired work in Information Retrieval\n(IR) and Natural Language Processing (NLP), and some rep-\nresentative work in Question Answering.\nQuantum-inspired Models for IR and NLP\nvan Rijsbergen (2004) argued that quantum theory can\nunify the logical, geometric, and probabilistic IR models\nby its mathematical formalism. After this pioneering work,\n5669\nSentence \nMatrices\nOuter \nProducts\nDensity \nMatrices\nJoint \nRepresentation Softmax\n  \nrow-pooling\ncol-pooling\nconnect\nPoolingConvolution\nSingle Sentence Representation Joint Representation\nrow\n pooling\nMatching\nܽݍࡹ\nܽ࣋ݍ࣋\nܽ࣋\nݍ࣋ࡽ\n࡭\nͳ\nʹ\nܿࡿ\n  \nFigure 3: NNQLM-II. The single sentence representation and the joint representation are the same as those in NNQLM-I, and\nthe rest layers are to match the QA pair by the similarity patterns learned by 2D-CNN.\nTable 1: Statistics of TREC-QA and WikiQA\nTREC-QA WIKIQA\nTRAIN DEV TEST TRAIN DEV TEST\n#Question 1229 82 100 873 126 243\n#Pairs 53417 1148 1517 8672 1130 2351\n%Correct 12.0 19.3 18.7 12.0 12.4 12.5\na range of quantum-inspired methods have been devel-\noped (Zuccon and Azzopardi 2010; Piwowarski et al. 2010;\nZhang et al. 2016), based on the analogy between IR rank-\ning and quantum phenomena (e.g., double-slit experiment).\nQuantum theory has been also used for semantic represen-\ntation in combination with Dependency Parsing Tree (Bla-\ncoe, Kasheﬁ, and Lapata 2013), where state vectors repre-\nsent word meanings and density matrices represent the un-\ncertainty of word meanings.\nSordoni, Nie, and Bengio (2013) successfully applied\nquantum probability in Language Modeling (LM) and pro-\nposed a Quantum Language Model (QLM), for which the\nestimation of the density matrix is crucial. It is proven that\nthe density matrix is a more general representation for texts,\nby looking at vector space model and language model in the\nquantum formalism (Sordoni and Nie 2013). Using the idea\nof quantum entropy minimization in QLM, Sordoni, Bengio,\nand Nie (2014) proposed to learn latent concept embeddings\nfor query expansion. By devising a similarity function in the\nlatent concept space, the query representation will get closer\nto the relevant document terms, thus beneﬁting the likeli-\nhood of selecting good expansion terms. Indeed, this work\ninspires us to develop QLM towards a supervised approach\nand estimating density matrix analytically. Compared with\nthis work, our proposed model targets different application\ntask, and uses different approaches to density matrix estima-\ntion and learning architectures.\nMore recently, a session-based adaptive QLM was pro-\nposed to model the evolution of density matrix and capture\nthe dynamic information need in search sessions (Li et al.\n2015; 2016). The concept of quantum entanglement has also\nbeen integrated into the quantum language model (Xie et al.\n2015). Practically, the so-called pure high-order term asso-\nciation patterns (as a reﬂection of entanglement) are selected\nas the compound terms for the input of density matrix esti-\nmation. The above variants of QLM keeps the main archi-\ntecture of QLM. In other words, they still carry out the rep-\nresentation, estimation and ranking processes sequentially,\nwithout a joint optimization strategy. Thus their potential to\nimprove the retrieval effectiveness is limited.\nAnswer Selection\nIn this paper, we apply the proposed end-to-end QLM in the\nanswer selection task. The aim of answer selection is to ﬁnd\ncorrect answer sentences from pre-selected answer candi-\ndates given a question. In the answer selection task, end-to-\nend methods represent the current state of the art.\nY u et al. (2014) used CNN to capture bigram information\nin the question/answer. Severyn and Moschitti (2015) fur-\nther developed this idea and capture n-gram of higher order\ndependency. Qiu and Huang (2015) also modeled n-gram\ninformation in a single sentence and model the interactions\n5670\nbetween sentences with a tensor layer. Later, more effec-\ntive components are added to the CNN model, such as at-\ntention mechanism (Yin et al. 2015; dos Santos et al. 2016)\nand Noise-Contrastive Estimation (NCE) (Rao, He, and Lin\n2016). Long-Short Term Memory (LSTM) has also been uti-\nlized (Wang and Nyberg 2015; Tay et al. 2017).\nOur work is the ﬁrst attempt to introduce Quantum Lan-\nguage Model (QLM) in the answer selection task. We will\nshow that the density matrix representation has a great po-\ntential to effectively encode the mixture of semantic sub-\nspaces and reﬂect how scattered the words of a sentence are\nin the embedded space. To our best knowledge, such repre-\nsentation for sentences and a further joint representation for\ntwo sentences, are different from those representations used\nin the aforementioned end-to-end QA approaches. In addi-\ntion, different from existing QLM based models, we design\na new method to obtain density matrices and propose an end-\nto-end model to integrate the density matrix representation\nand the similarity matching into neural network structures.\nExperiment\nDatasets and Evaluation Metrics\nExtensive experiments are conducted on the TREC-QA and\nWikiQA datasets. TREC-QA is a standard benchmarking\ndataset used in the Text REtrieval Conference (TREC)’s QA\ntrack (8-13) (Wang, Smith, and Mitamura 2007). WikiQA\n(Yang, Yih, and Meek 2015) is an open domain question-\nanswering dataset released by Microsoft Research, and we\nuse it for the subtask assuming there is at least one correct\nanswer for each question. The basic statistics of the datasets\nare presented in Table 1. The evaluation metrics we use are\nmean average precision (MAP) and mean reciprocal rank\n(MRR), which are commonly used in previous works for the\nsame task with the same datasets.\nMethods for Comparison and Parameter Settings\nThe methods for comparison are as follows. QLM is the\noriginal quantum language model while QLM\nT replaces\nnon-negative VN divergence with trace inner product as the\nranking function. NNQLM-I and NNQLM-II are our pro-\nposed end-to-end QLMs.\nFor QLM, we initializeρ\n0 by a diagonal matrix, in which\nthe diagonal elements are Term Frequency (TF) values of\nthe corresponding words. The initial matrices are normal-\nized with a unit trace. The size of sliding window is 5.\nFor NNQLMs, the hyper parameters are listed in Table\n2. The parameters that need to be trained are the position\nweights p\ni in density matrices (see Eq. 3), the weights of the\nsoftmax layer, and the parameters for the convolution layer.\nThe word embeddings are trained by word2vec (Mikolov et\nal. 2013) on English Wikimedia\n1. The dimensionality is 50,\nand the Out-of-V ocabulary words are randomly initialized\nby a uniform distribution in the range of (-0.25, 0.25). In\nNNQLM-I, the embeddings are updated during training. For\nNNQLM-II, we keep the embeddings static, which is found\nin our pilot experiments to outperform dynamically updating\nthe embeddings.\n1https://dumps.wikimedia.org/\nResults\nA series of experiments are carried out for a systematic eval-\nuation. Table 3 summarizes the experiment results.\nIn the ﬁrst group, we compare QLM with QLM\nT . On both\nTREC-QA and WIKIQA, QLM achieves similar results to\nQLM\nT . This is consistent with our previous explanation that\nthe trace inner product is an approximation of negative VN\ndivergence. Thus, it is reasonable to use it as a similarity\nmeasure in our ﬁrst architecture (NNQLM-I) based on the\njoint representation.\nIn the second group, we compare two NNQLMs with\nQLM. NNQLM-I can outperform QLM, which shows the\neffectiveness of our new deﬁnition of the density matrix\ntogether with the simple training algorithm. Recall that in\nNNQLM-I, only the diagonal and trace values of the joint\nrepresentation are involved in the training process.\nNNQLM-II, which uses 2D convolution neural network\n(2D-CNN for short), largely outperforms NNQLM-I. It ver-\niﬁes our earlier analysis that the 2D-CNN is able to learn\nricher similarity features than those learned from the ﬁrst ar-\nchitecture. It also implies that the joint representation of den-\nsity matrices has a great potential as a kind of representation\nfor learning effective inter-sentence similarity information.\nAs we can see from Table 3, NNQLM-II can signiﬁcantly\nimprove the original QLM on both datasets (by 11.87%\nMAP and 13.61% MRR on TREC-QA, and by 27.15% MAP\nand 28.09% on WIKIQA). The signiﬁcant test is performed\nusing the Wilcoxon signed-rank test with p<0.05.\nIn the third group, we compare NNQLM-II with the ex-\nisting neural network based approaches (Y u et al. 2014;\nSeveryn and Moschitti 2015; Yin et al. 2015; Wang and Ny-\nberg 2015; Yang et al. 2016)\n2. On TREC-QA, NNQLM-\nII achieves the best performance over the comparative ap-\nproaches. Note that NNQLM-II outperforms a strong base-\nline (Yang et al. 2016) by 2.46% MAP and 3.24% MRR. On\nWikiQA, NNQLM-II has outperformed a baseline method\nproposed (Y u et al. 2014), for which its WikiQA results are\nreported in (Yang, Yih, and Meek 2015).\nDiscussions\nNNQLM-II has not yet outperformed the other two baselines\non WikiQA. The possible reason is that although 2D-CNN\ncan learn useful similarity patterns for the QA task, there\ncan be other useful features (e.g, the structure in a sentence)\nthat could inﬂuence the performance. In fact, We can add\nsome other features and get improvements on WikiQA. In\nour future work, we will improve the network structure of\nNNQLM-II for a better model.\nIn addition, we would like to further explain the mech-\nanism of our model in comparison with the existing QA\nmodels. The difference of our proposed model stems from\nthe density matrix representation. Such a matrix can rep-\nresent the mixture of the semantic subspaces, and the joint\nrepresentation of question and answer matrices can encode\n2The WIKIQA results of the methods proposed in (Y u et al.\n2014) and (Yin et al. 2015) are extracted from (Yang, Yih, and\nMeek 2015) and (dos Santos et al. 2016), respectively, excluding\nhandcrafted features.\n5671\nTable 2: The Hyper Parameters of NNQLM\nMethod TREC-QA WIKIQA\nNNQLM-I NNQLM-II NNQLM-I NNQLM-II\nlearning rate 0.01 0.01 0.08 0.02\nbatchsize 100 100 100 140\nﬁlter number / 65 / 150\nﬁlter size / 40 / 40\nTable 3: Results on TREC-QA and WIKIQA\nMethod TREC-QA WIKIQA\nMAP MRR MAP MRR\nQLM 0.6784 0.7265 0.5109 0.5148\nQLMT 0.6683 0.7280 0.5108 0.5145\nNNQLM-I 0.6791 0.7529 0.5462 0.5574\nNNQLM-II 0.7589 0.8254 0.6496 0.6594\n(Y u et al. 2014) 0.5693 0.6613 0.6190 0.6281\n(Severyn and Moschitti, 2015, 2016) 0.6709 0.7280 0.6661 0.6851\n(Yin et al. 2015) / / 0.6600 0.6770\n(Wang and Nyberg 2015) 0.5928 0.5928 / /\n(Yang et al. 2016) 0.7407 0.7995 / /\nsimilarity patterns. By using the 2D-CNN, we can extract\nuseful similarity patterns and obtain a good performance on\nthe answer selection task. On the other hand, most existing\nneural network based QA models concatenates word embed-\nding vectors. Based on such concatenation, the 1D convo-\nlution neural networks (1D-CNN for short) can be directly\nperformed. We have carried out the above experiments for\na comparison. In the future, we will systematically analyze\nand evaluate the above two different mechanisms in-depth.\nConclusions and Future Work\nIn this paper, we have proposed an Neural-Network based\nQuantum-like Language Models (namely NNQLM), which\nsubstantially extend the original Quantum Language Model\n(QLM) to an end-to-end mechanism, with application to the\nQuestion Answering (QA) task. To the best of our knowl-\nedge, this is the ﬁrst time for the QLM to be extended\nwith neural network architectures and for the quantum or\nquantum-like models to applied to QA. We have designed a\nnew density matrix based on word embeddings, and such a\ndensity matrix for a single sentence, together with the joint\nrepresentation for sentence pairs, can be integrated into the\nneural network architectures for an effective joint training.\nSystematic experiments on TREC\nQA and WikiQA have\ndemonstrated the applicability and effectiveness of QLM\nand NNQLMs. Our proposed NNQLM-II achieves a signif-\nicant improvement over QLM on both datasets, and outper-\nforms a strong baseline (Yang et al. 2016) by 2.46% (MAP)\nand 3.24% (MRR) on TREC-QA.\nA straightforward future research direction is to explore\nother neural networks for NNQLM. Our models can also be\napplied to other tasks, especially for short text pair matching\ntasks. In addition, for those QA pairs that are not selected\nby the similarity matching, e.g., in a casual inference case,\nwe can encode an additional component (e.g., an inference\nfunction) in the quantum-like models. It is also interesting\nto explore the capability of NNQLMs using density matrices\nfor the representation learning task.\nAcknowledgements\nThis work is supported in part by the Chinese National Pro-\ngram on Key Basic Research Project (973 Program, grant\nNo. 2014CB744604), Natural Science Foundation of China\n(grant No. U1636203, 61772363, 61402324), Tianjin Natu-\nral Science Foundation (grant No. 15JCQNJC41700), and\nEuropean Union’ Horizon 2020 research and innovation\nprograme under the Marie Skłodowska-Curie grant agree-\nment No. 721321.\nReferences\nBalkır, E. 2014. Using density matrices in a compositional\ndistributional model of meaning. Ph.D. Dissertation, Mas-\nters thesis, University of Oxford.\nBengio, Y .; Ducharme, R.; Vincent, P .; and Janvin, C. 2003.\nA neural probabilistic language model.Journal of Machine\nLearning Research3:1137–1155.\nBlacoe, W.; Kasheﬁ, E.; and Lapata, M. 2013. A quantum-\ntheoretic approach to distributional semantics. In Proc. of\nHLT-NAACL, 847–857.\nBlacoe, W. 2014. Semantic composition inspired by quan-\ntum measurement. InProc. of QI, 41–53. Springer.\nBruza, P . D.; Wang, Z.; and Busemeyer, J. R. 2015. Quantum\ncognition: a new theoretical approach to psychology.Trends\nin Cognitive Sciences19(7):383 – 393.\ndos Santos, C. N.; Tan, M.; Xiang, B.; and Zhou, B. 2016.\nAttentive pooling networks.CoRR, abs/1602.03609.\nGleason, A. M. 1957. Measures on the closed subspaces\nof a hilbert space. Journal of mathematics and mechanics\n6(6):885–893.\n5672\nHu, B.; Lu, Z.; Li, H.; and Chen, Q. 2014. Convolutional\nneural network architectures for matching natural language\nsentences. In Proc. of NIPS, 2042–2050.\nKim, Y . 2014. Convolutional neural networks for sentence\nclassiﬁcation. arXiv preprint arXiv:1408.5882.\nLi, Q.; Li, J.; Zhang, P .; and Song, D. 2015. Modeling multi-\nquery retrieval tasks using density matrix transformation. In\nProc. of SIGIR, 871–874. ACM.\nLi, J.; Zhang, P .; Song, D.; and Hou, Y . 2016. An adaptive\ncontextual quantum language model.Physica A: Statistical\nMechanics and its Applications456:51–67.\nManning, C. D.; Raghavan, P .; and Sch¨utze, H. 2008. In-\ntroduction to Information Retrieval. New Y ork, NY , USA:\nCambridge University Press.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013. Distributed representations of words and\nphrases and their compositionality. InProc. of NIPS, 3111–\n3119.\nPiwowarski, B.; Frommholz, I.; Lalmas, M.; and van Rijs-\nbergen, K. 2010. What can quantum theory bring to infor-\nmation retrieval. InProc. of CIKM, 59–68.\nQiu, X., and Huang, X. 2015. Convolutional neural tensor\nnetwork architecture for community-based question answer-\ning. In Proc. of IJCAI, 1305–1311.\nRao, J.; He, H.; and Lin, J. 2016. Noise-contrastive esti-\nmation for answer selection with deep neural networks. In\nProc. of CIKM, 1913–1916. ACM.\nSeveryn, A., and Moschitti, A. 2015. Learning to rank short\ntext pairs with convolutional deep neural networks. InProc.\nof SIGIR, 373–382. ACM.\nSeveryn, A., and Moschitti, A. 2016. Modeling relational in-\nformation in question-answer pairs with convolutional neu-\nral networks. arXiv preprint arXiv:1604.01178.\nSordoni, A., and Nie, J.-Y . 2013. Looking at vector space\nand language models for ir using density matrices. InProc.\nof QI, 147–159. Springer.\nSordoni, A.; Bengio, Y .; and Nie, J.-Y . 2014. Learning con-\ncept embeddings for query expansion by quantum entropy\nminimization. In Proc. of AAAI, volume 14, 1586–1592.\nSordoni, A.; Nie, J.-Y .; and Bengio, Y . 2013. Modeling term\ndependencies with quantum language models for ir. InProc.\nof SIGIR, 653–662. ACM.\nTay, Y .; Phan, M. C.; Tuan, L. A.; and Hui, S. C. 2017.\nLearning to rank question answer pairs with holographic\ndual lstm architecture. InProc. of SIGIR, 695–704. ACM.\nV an Rijsbergen, C. J. 2004.The geometry of information\nretrieval. Cambridge University Press.\nV on Neumann, J. 1955.Mathematical foundations of quan-\ntum mechanics. Number 2. Princeton university press.\nWan, S.; Lan, Y .; Guo, J.; Xu, J.; Pang, L.; and Cheng, X.\n2016. A deep architecture for semantic matching with mul-\ntiple positional sentence representations. InProc. of AAAI,\n2835–2841.\nWang, D., and Nyberg, E. 2015. A long short-term memory\nmodel for answer sentence selection in question answering.\nIn Proc. of ACL, 707–712.\nWang, B.; Zhang, P .; Li, J.; Song, D.; Hou, Y .; and Shang,\nZ. 2016. Exploration of quantum interference in document\nrelevance judgement discrepancy.Entropy 18(4):144.\nWang, M.; Smith, N. A.; and Mitamura, T. 2007. What is\nthe jeopardy model? a quasi-synchronous grammar for qa.\nIn Proc. of EMNLP-CoNLL, volume 7, 22–32.\nXie, M.; Hou, Y .; Zhang, P .; Li, J.; Li, W.; and Song, D.\n2015. Modeling quantum entanglements in quantum lan-\nguage models. InProc. of IJCAI. AAAI Press.\nYang, L.; Ai, Q.; Guo, J.; and Croft, W. B. 2016. anmm:\nRanking short answer texts with attention-based neural\nmatching model. InProc. of CIKM, 287–296. ACM.\nYang, Y .; Yih, W.-t.; and Meek, C. 2015. Wikiqa: A chal-\nlenge dataset for open-domain question answering. InProc.\nof EMNLP, 2013–2018. Citeseer.\nYin, W.; Sch¨utze, H.; Xiang, B.; and Zhou, B. 2015. Abcnn:\nAttention-based convolutional neural network for modeling\nsentence pairs. arXiv preprint arXiv:1512.05193.\nY u, L.; Hermann, K. M.; Blunsom, P .; and Pulman, S. 2014.\nDeep learning for answer sentence selection.arXiv preprint\narXiv:1412.1632.\nZhai, C. 2008.Statistical Language Models for Information\nRetrieval. Synthesis Lectures on Human Language Tech-\nnologies. Morgan & Claypool Publishers.\nZhang, P .; Li, J.; Wang, B.; Zhao, X.; Song, D.; Hou, Y .; and\nMelucci, M. 2016. A quantum query expansion approach\nfor session search.Entropy 18(4):146.\nZuccon, G., and Azzopardi, L. 2010. Using the quantum\nprobability ranking principle to rank interdependent docu-\nments. In Proc. of ECIR, 357–369.\n5673"
}