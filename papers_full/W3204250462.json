{
  "title": "CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models",
  "url": "https://openalex.org/W3204250462",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104181513",
      "name": "Yao Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2012104819",
      "name": "Zhang Ao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2348152382",
      "name": "Zhang ZhengYan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1860873322",
      "name": "Liu Zhi-Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3005059581",
      "name": "Chua, Tat-Seng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381112680",
      "name": "Sun, Maosong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2962764817",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3177174258",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3099261920",
    "https://openalex.org/W2768661419",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W2986803748",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3172141633",
    "https://openalex.org/W3098232790",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3173909648",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2963445828",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W1861492603"
  ],
  "abstract": "Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, CPT enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/CPT.",
  "full_text": "Work in Progress\nCPT: C OLORFUL PROMPT TUNING FOR PRE-TRAINED\nVISION -LANGUAGE MODELS\nYuan Yao1∗, Ao Zhang2∗, Zhengyan Zhang1, Zhiyuan Liu1†, Tat-Seng Chua2, Maosong Sun1\n1Department of Computer Science and Technology\nInstitute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology, China\n2Sea-NExT Joint Lab, Singapore\nSchool of Computing, National University of Singapore, Singapore\nyuan-yao18@mails.tsinghua.edu.cn, aozhang@u.nus.edu\nABSTRACT\nPre-Trained Vision-Language Models (VL-PTMs) have shown promising capa-\nbilities in grounding natural language in image data, facilitating a broad variety\nof cross-modal tasks. However, we note that there exists a signiﬁcant gap be-\ntween the objective forms of model pre-training and ﬁne-tuning, resulting in a\nneed for large amounts of labeled data to stimulate the visual grounding capa-\nbility of VL-PTMs for downstream tasks. To address the challenge, we present\nCross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a\nnovel paradigm for tuning VL-PTMs, which reformulates visual grounding into\na ﬁll-in-the-blank problem with color-based co-referential markers in image and\ntext, maximally mitigating the gap. In this way, CPT enables strong few-shot\nand even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive\nexperimental results show that the prompt-tuned VL-PTMs outperform their ﬁne-\ntuned counterparts by a large margin (e.g.,17.3% absolute accuracy improvement,\nand 73.8% relative standard deviation reduction on average with one shot in Re-\nfCOCO evaluation). We make the data and code for this paper publicly available\nat https://github.com/thunlp/CPT.\n1 I NTRODUCTION\nGrounding natural language in ﬁne-grained image regions is essential for a broad variety of vision-\nlanguage tasks, such as robotic navigation (Tellex et al., 2011; Anderson et al., 2018b), visual ques-\ntion answering (Antol et al., 2015; Anderson et al., 2018a), visual dialogue (Das et al., 2017), and vi-\nsual commonsense reasoning (Zellers et al., 2019). Recently Pre-Trained Vision-Language Models\n(VL-PTMs) have shown promising capabilities in visual grounding. Typically, generic cross-modal\nrepresentations are ﬁrst pre-trained on large-scale image-caption data in a self-supervised fashion,\nand then ﬁne-tuned to adapt to downstream tasks (Lu et al., 2019; Su et al., 2019; Li et al., 2020;\nRadford et al., 2021). This pre-training-then-ﬁne-tuning paradigm of VL-PTMs has greatly pushed\nforward the state-of-the-art of many cross-modal tasks.\nDespite the success, we note that there exists a signiﬁcant gap between the objective forms of pre-\ntraining and ﬁne-tuning of VL-PTMs. As illustrated in Figure 1, during pre-training, most VL-PTMs\nare optimized based on the masked language modeling objective, trying to recover the masked token\nfrom the cross-modal context. However, during ﬁne-tuning, downstream tasks are usually conducted\nby classifying unmasked token representations into semantic labels, where task-speciﬁc parameters\nare typically introduced. The gap hinders the effective adaptation of VL-PTMs to downstream tasks.\nAs a result, a large amount of labeled data is typically required to stimulate the visual grounding\ncapabilities of VL-PTMs for downstream tasks.\n∗ indicates equal contribution\n† Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn)\n1\narXiv:2109.11797v3  [cs.CV]  20 May 2022\nWork in Progress\nMLM \nHead\n>,0*@\nUHG\nEOXH\nŏ KRUVH7KH ZDWFKHG E\\ LVWKH ZRPDQ LQ >0$6.@ FRORU\nQuery TextImage Regions Query Template\n>&/6@ >6(3@\nŏ\nŏ\nŏ>,0*@ KRUVH7KH ZDWFKHG E\\ WKH ZRPDQ>&/6@ >6(3@\nCLS\nHead\nODEHO\u001d\u0003SRVLWLYH\n Label Space\nODEHO\u001d\u0003QHJDWLYH\nŏ>,0*@ ZRPDQ$ LV >0$6.@ WKH KRUVH>&/6@ >6(3@\nMLM\nHead\nZDWFKLQJ\n Vocabulary\nULGLQJ\nŏ\nŏ\n4XHU\\\u00037H[W\u001d\u0003\nThe horse watched by the woman\n\u000bE\f\u0003)LQH\u0010WXQLQJ\n\u000bD\f\u00033UH\u0010WUDLQLQJ\n\u000bF\f\u0003&URVV\u00100RGDO\u00033URPSW\u00037XQLQJ\u0003\u000b2XU\u0003DSSURDFK\f\nŏ ŏ\n  Vocabulary\n4XHU\\\u00037H[W\u001d\u0003\nThe horse watched by the woman\nFigure 1: Illustration of (a) pre-training for VL-PTMs with masked language modeling (MLM)\nhead, (b) vanilla ﬁne-tuning with new classiﬁcation (CLS) head, and (c) our colorful cross-modal\nprompt tuning (CPT) framework that reformulates visual grounding into a ﬁll-in-the-blank problem\nwith reused MLM head. Only square parts of relevant image regions are shown for illustration.\nIn this work, inspired by recent progress in pre-trained language models in natural language pro-\ncessing (Brown et al., 2020; Schick & Sch ¨utze, 2021a; Liu et al., 2021), we present Cross-modal\nPrompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-\nPTMs. The key insight is that by adding color-based co-referential markers in both image and text,\nvisual grounding can be reformulated into a ﬁll-in-the-blank problem, maximally mitigating the gap\nbetween pre-training and ﬁne-tuning. As shown in Figure 1, to ground natural language expres-\nsions in image data, CPT consists of two components: (1) a visual sub-prompt that uniquely marks\nimage regions with colored blocks or segmentation masks, and (2) a textual sub-prompt that puts\nthe query text into a color-based query template. Explicit grounding to the target image region can\nthen be achieved by recovering the corresponding color text from the masked token in the query\ntemplate. In addition, we present a principled method to search for high-quality cross-modal prompt\nconﬁgurations (i.e., visual appearances and texts of colors) for CPT.\nBy mitigating the gap from pre-training, CPT enables strong few-shot and even zero-shot visual\ngrounding capabilities of VL-PTMs. Experimental results show that the prompt-tuned VL-PTMs\noutperform their ﬁne-tuned counterparts by a large margin. For example, using colored blocks\nas visual sub-prompts, CPT achieves 17.3% absolute accuracy improvement, and 73.8% relative\nstandard deviation reduction on average with one shot in RefCOCO evaluation. In the same setting,\nwhen equipped with colored segmentation masks as visual sub-prompts, CPT can further achieve\n20.0% absolute accuracy improvement, and 76.2% relative standard deviation reduction than the\nvanilla ﬁne-tuning approach. In addition to the object position output tasks such as visual grounding,\nwe show that CPT can also be applied to achieve strong zero- and few-shot performance for position\ninput tasks such as visual relation detection.\nOur contributions are summarized as threefold: (1) We present a novel cross-modal prompt tuning\nparadigm for VL-PTMs. To the best of our knowledge, this is the ﬁrst attempt in both cross-modal\nprompt tuning for VL-PTMs, and zero- and few-shot visual grounding independent of object types.\n(2) We present a principled approach to search for high-quality cross-modal prompt conﬁgurations\nfor CPT. (3) We conduct comprehensive experiments which demonstrate the effectiveness of CPT.\n2 P RELIMINARY\nIn the literature, visual grounding is typically formulated as a referring expression comprehension\n(REC) problem (Plummer et al., 2015; Mao et al., 2016). Given an image I and a query text of\nreferring expression q, REC aims to locate the target region in I that corresponds to q. In this\nsection, we introduce the vanilla ﬁne-tuning approach for VL-PTMs.\n2\nWork in Progress\nA common practice for REC is to ﬁrst detect a set of region proposals{v1,v2,...,v n}via object de-\ntectors, and then classify or rank the proposals to select the target region (Lu et al., 2019; Chen et al.,\n2020). Speciﬁcally, visual and textual inputs are ﬁrst transformed into a sequence of input tokens\n{[IMG],v1,v2,...,v n,[CLS],w1,w2,...,w m,[SEP]}, where {w1,w2,...,w m}are textual tokens\nof q, and [IMG], [CLS] and [SEP] are special tokens. To obtain input representations, the feature\nof image regions is extracted by visual encoders, and the embeddings of textual and special tokens\nare obtained by a lookup table. Then input representations are fed into the pre-trained transformers\nto produce the hidden representations {h[IMG],h1\nv,h2\nv,..., hn\nv ,h[CLS],h1\nw,h2\nw,..., hm\nw ,h[SEP]}.\nFinally the hidden representation of the target region is optimized against negative ones via classi-\nﬁcation or ranking loss, where new task-speciﬁc parameters are introduced. As a result, ﬁne-tuned\nVL-PTMs need a large mount of labeled instances to stimulate the visual grounding capability.\n3 C ROSS -MODAL PROMPT TUNING (CPT)\nIn this section, we introduce the framework of CPT, and how to apply CPT to zero-shot, few-shot\nand fully supervised visual grounding.\n3.1 O VERVIEW\nThe key to visual grounding is to establish ﬁne-grained connections between image regions and\ntextual expressions. Therefore, a good cross-modal prompt tuning framework should take full ad-\nvantage of co-referential signals from both image and text, and maximally mitigate the gap between\npre-training and tuning. To this end, CPT reformulates visual grounding into a ﬁll-in-the-blank prob-\nlem, as shown in Figure 1. Speciﬁcally, the CPT framework consists of two components: (1) avisual\nsub-prompt that uniquely marks the image regions with colored blocks or segmentation masks, and\n(2) a textual sub-prompt that puts the query text into a color-based query template. Equipped with\nCPT, it is then straightforward for VL-PTMs to ground the query text by ﬁlling the masked token\nwith the color text of the target image region, where the objective form is identical to pre-training.\n3.2 V ISUAL SUB-PROMPT\nGiven an imageIand its region proposalsR= {v1,v2,...,v n}, visual sub-prompt aims to uniquely\nmark the image regions with natural visual makers. Interestingly, we note that colored bounding\nboxes are widely used to uniquely mark objects in imagesfor visualization in the literature. Inspired\nby this, we bridge the image regions and query text through a set of colors C, where each color\nci = (ci\nv,ci\nw) ∈C is deﬁned by its visual appearance ci\nv (e.g., RGB (255, 0, 0)) and color text ci\nw\n(e.g., red). Then we mark each region proposalvi in the image with a unique colorci\nv for grounding,\nresulting in a set of colored image proposals Ψ(R; C), where Ψ(·) denotes visual sub-prompt.\nAs for the shape of the visual sub-prompt, in principle, there are multiple plausible choices to mark\nthe regions with colors, including colored bounding boxes, solid blocks, or solid object segmentation\nmasks. In our experiments, we ﬁnd that coloring the object with solid blocks and segmentation\nmasks yields better results than bounding boxes, since solid colors that ﬁt the outlines of objects are\nmore common in real-world images (e.g., red shirt and blue car). Note that the addition of visual\nsub-prompt to the raw image does not change the architecture or parameters of VL-PTMs.\n3.3 T EXTUAL SUB-PROMPT\nTextual sub-prompt aims to prompt VL-PTMs to establish the connections between the query text\nand image regions marked by visual sub-prompt. Speciﬁcally, the query text q (e.g., “the horse\nwatched by the woman”) is transformed into a ﬁll-in-the-blank query using a template Tg(·) as:\nTg(q) = [CLS] qis in [MASK] color [SEP]\nIn this way, VL-PTMs are prompted to decide the color of which region is more appropriate to ﬁll\nin the mask (e.g., red or blue) as follows:\nP(v = vi|R, q) = P([MASK] = ci\nw|Ψ(R; C), Tg(q)) = exp(h⊤\n[MASK]ci\nw)∑\ncj∈Cexp(h⊤\n[MASK]cj\nw)\n, (1)\n3\nWork in Progress\nZRPDQ7KH LQ JUHHQ >0$6.@FRORU LV WKH KRUVH LQ >6(3@UHG FRORU>,0*@\n ŏ >&/6@\nMLM \nHead\nZDWFKLQJ\nULGLQJŏVocabulary\nFigure 2: CPT for visual relation detection by ﬁlling-in-the-blank with reused MLM head.\nwhere vis the target region, ci\nw is the embedding of ci\nw in the pre-trained MLM head. Note that the\nprocedure does not introduce any new parameters, and also mitigates the gap between pre-training\nand tuning, and therefore improves the data efﬁciency for tuning VL-PTMs.\n3.4 T RAINING AND INFERENCE\nEquipped with CPT, VL-PTMs can readily perform zero-shot visual grounding without any labeled\ndata, since the cross-modal representations of colors and their composition with other concepts (e.g.,\nobjects, attributes and relations) have been well learned by VL-PTMs during pre-training. When a\nfew or full labeled instances are available, VL-PTMs can be further tuned by CPT using the entropy-\nbased objective: L= −∑\n(R,q,v⋆)∈Dtrain log P(v⋆|R,q), where Dtrain is the training set.\nAlthough it is appealing to bridge the image and text through a color-based prompt, we identify two\nkey challenges in its design: (1) how to determine the conﬁgurations of the color set C, and (2) how\nto deal with the large number of image regions with limited pre-trained colors.\nCross-Modal Prompt Search.Previous works in textual prompt tuning show that prompt conﬁgu-\nrations (e.g., textual templates) have a signiﬁcant inﬂuence on the performance (Jiang et al., 2020).\nIn this work, we make the ﬁrst investigation in searching the cross-modal prompt conﬁguration (i.e.,\nthe color set C). Intuitively, Cshould consist of colors to which VL-PTMs are the most sensitive.\nTo obtain a color ci = ( ci\nv,ci\nw), a naive approach is to adopt the most frequent color text in the\npre-training text as ci\nw, and its standard RGB as ci\nv (e.g., ci = ((255 ,0,0),red)). However, this\nsolution is sub-optimal, since it determines the color text without considering its visual appearance,\nand the visual appearance of a color in real-world images often differs from its standard RGB.\nTo address the challenge, we present a principled cross-modal prompt search (CPS) algorithm for\nCPT, which jointly considers visual and textual semantics in real-world cross-modal data. Speciﬁ-\ncally, we ﬁrst identify a candidate set of color texts ˆCw and visual appearances ˆCv. For each visual\nappearance candidate ˆcv ∈ ˆCv, we feed into VL-PTMs a pseudo-data instance consisting of a pure\ncolored block of ˆcv and a text: “ [CLS] a photo in [MASK] color [SEP]”. Then we compute the\ndecoding score s( ˆcv, ˆcw) for each color text candidate ˆcw ∈ ˆCw as in Equation 1, where a larger\ndecoding score indicates higher correlation between ˆcv and ˆcw. To select the color texts that are\nsensitive by VL-PTMs, we retain the color texts that achieve the largest decoding scores for visual\nappearance candidates: Cw = {cw|cw = arg maxˆcj\nw∈ˆCw\ns(ˆci\nv,ˆcj\nw),ˆci\nv ∈ ˆCv}. Similarly, we can\nobtain the visual appearances according to the largest decoding score, resulting in the color set:\nC = {(cv,cw)|cv = arg max ˆciv∈ˆCv\ns(ˆci\nv,cj\nw),cj\nw ∈ Cw}. We refer readers to Section B for the\npseudo-code of the algorithm. In experiments, we ﬁnd that the resultant colors yield better results\nthan the naive ones. To make the raw content of the colored image regions available to VL-PTMs, a\ntransparency hyperparameter α∈(0,1) is further applied to color visual appearances in practice.\nImage Region Batching.In visual grounding, the number of region proposals in an image usually\nexceeds the size ofC(∼10). Besides, we observe that heavily overlapped colored blocks can hinder\nvisual grounding. Therefore, we divide the image regions into batches, where each batch contains\na handful of moderately overlapping image regions, and mark each batch with a visual sub-prompt\nrespectively. To handle the batches that do not contain the target region, we further introduce a new\ncandidate text none in the decoding vocabulary, to indicate that there is no target region in the batch.\n3.5 CPT FOR VISUAL RELATION DETECTION\nIn the previous sections, we introduced CPT for visual grounding. In fact, CPT can also be easily\nadapted to other cross-modal tasks, such as visual relation detection. Given an object pair (including\nthe categories and bounding boxes) in an image, visual relation detection aims to classify the relation\n4\nWork in Progress\ninto a relation setP, providing structured image representations that can facilitate many cross-modal\ntasks (Johnson et al., 2015; Hudson & Manning, 2019; Shi et al., 2019). In the literature, since the\nground-truth relations cannot be exhaustively annotated during evaluation, to avoid false negatives,\nprevious works typically score the triplets and evaluate the recall of top-N triplets (Xu et al., 2017;\nZellers et al., 2018; Chen et al., 2019; Tang et al., 2019).\nVisual and Textual Sub-prompts.As shown in Figure 2, to perform visual relation detection, CPT\nﬁrst marks the image regions with visual sub-prompt as in Section 3.2, and puts the object pair in\nthe query template as follows:\nTr(s,o) = [CLS] The sw in ci\nw color is [MASK] the ow in cj\nw color [SEP]\nwhere sw is the subject text, ow is the object text, and ci\nw and cj\nw are the corresponding color texts.\nThen VL-PTMs are prompted to recover the relation texts from masked tokens in the template. To\naccommodate the varied number of tokens in relation texts (e.g.,wearing, walking on, typically 1∼3\ntokens), we introduce a variable lindicating the number of tokens in a relation text (e.g., l = 2 for\nwalking on). The template T(·; l) will have lconsecutive masked tokens for relation prediction. For\neach template T(·; l), we introduce a special NA relation consisting of ltokens, which indicates that\nthere is no relation between the entity pair under T(·; l). Speciﬁcally, in our experiments, the NA\nrelation is irrelevant, no relation, no relation with for l= 1,2,3 respectively.\nTraining. Given a relational triplet (s,r,o), after decorating the input image regions and the object\npair with visual and textual sub-prompts, VL-PTMs are optimized with the MLM loss to recover\nthe relational tokens. Speciﬁcally, denote the number of tokens in ras |r|. (1) For templates where\nl = |r|, models are asked to reconstruct the ith masked token in T(s,o; l) with the ith relational\ntoken ri using the MLM head. (2) For templates where l ̸= |r|, since there is no relation between\n(s,o) under T(s,o; l), models are asked to reconstruct the NA relation. For (s,o) that do not have\nany relation in the image, models are asked to reconstruct the NA relation for all T(s,o; l).\nInference. During inference, given an object pair (s,o), we score the relations based on their\nﬁtness to the prompt context. Speciﬁcally, the score of each relation r ∈ P∪{NA}is ob-\ntained by the aggregated MLM scores of its composing tokens under the corresponding template:\ns(r) = 1\nl\n∑l\ni=1 log P([MASK]i = ri|T(s,o; l)), where l = |r|. Intuitively, larger s(r) indicates\nthat the relation rbetter ﬁts the prompt context. Finally, the triplets (s,r,o) are ranked according to\nthe relation score s(r), where r∈P.\nCompared with visual grounding that aims to locate image regions for ungrounded texts, visual\nrelation detection represents a different series of cross-modal tasks that aim to perform semantic\nrecognition based on grounded inputs, such as visual commonsense reasoning Zellers et al. (2019),\nobject classiﬁcation (Zhao et al., 2017) and scene graph classiﬁcation (Xu et al., 2017). In addition\nto better data efﬁciency, a crucial advantage of using CPT is that the semantic labels can be produced\nfrom open-world vocabularies, instead of ﬁxed label sets.\n4 E XPERIMENTS\nIn this section, we empirically evaluate CPT in prompting VL-PTMs for visual grounding in dif-\nferent settings, including zero-shot, few-shot and fully supervised settings. We refer readers to\nSection C for the implementation details.\n4.1 E XPERIMENTAL SETTINGS\nWe ﬁrst introduce the experimental settings of the visual grounding task, including datasets, training\nsettings, evaluation protocols and baseline models in our experiments.\nDatasets. Following previous works (Rohrbach et al., 2016; Zhang et al., 2018), we adopt three\nwidely used visual grounding datasets collected from MSCOCO images (Lin et al., 2014), including\nRefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016) and RefCOCOg (Mao et al., 2016). We\nrefer readers to Section D.2 for more dataset details.\n5\nWork in Progress\nTable 1: Main results. Accuracies (%) of grounding referring expressions in zero-shot, few-shot and\nfully supervised settings. We report mean and standard deviation performance over 5 random splits.\nZS: zero-shot. Blk: colored block, Seg: colored segmentation mask.\nShot Model RefCOCO RefCOCO+ RefCOCOg\nval testA testB val testA testB val test\nZS 0 Random 15.9±0.2 19.4±0.6 13.4±0.4 16.1±0.1 13.3±0.6 20.0±0.2 18.8±0.4 19.2±0.3CPT-Blk 26.9 27 .5 27 .4 25 .4 25 .0 27 .0 32 .1 32 .3CPT-Seg 32.2 36.1 30.3 31.9 35.2 28.8 36.7 36.5\nFew-Shot\n1 Fine-tuning16.5±4.9 12.0±6.6 23.5±5.7 22.2±7.6 20.6±9.3 25.7±5.2 26.9±8.4 26.9±8.1CPT-Blk 34.1±1.3 37.7±1.7 32.2±1.5 35.9±4.1 40.4±5.4 32.2±2.6 39.7±3.4 39.9±3.0CPT-Seg 37.2±0.9 41.5±1.5 33.2±1.7 37.9±4.0 42.3±5.9 33.9±2.4 43.1±2.9 43.4±3.1\n2 Fine-tuning22.5±4.5 21.0±7.2 25.9±4.7 27.0±3.1 27.8±4.2 27.0±2.6 28.4±12.0 28.1±11.3CPT-Blk 35.3±3.2 39.6±3.0 30.9±1.7 33.3±3.6 37.5±4.8 30.3±2.5 40.1±5.1 40.0±4.7CPT-Seg 39.8±1.7 45.6±3.2 33.9±0.4 38.6±3.6 44.5±4.5 32.8±3.8 44.7±5.1 44.3±4.8\n4 Fine-tuning29.1±5.0 29.9±7.8 29.8±5.3 34.2±4.2 37.7±5.2 30.5±3.3 34.0±13.1 33.7±12.8CPT-Blk 38.3±2.1 43.6±3.3 34.0±1.6 38.8±3.8 44.4±6.4 33.5±1.5 40.6±7.9 40.9±7.9CPT-Seg 40.7±3.2 47.4±4.1 35.3±1.8 40.3±2.0 46.5±3.1 34.5±1.5 44.4±6.9 44.4±6.9\n8 Fine-tuning34.6±4.8 37.8±5.5 31.4±5.1 36.2±3.6 40.1±4.6 32.7±2.3 40.6±11.2 40.4±11.7CPT-Blk 41.0±1.5 43.9±1.7 35.8±2.2 39.3±1.5 46.1±1.8 33.2±1.3 43.4±6.5 43.6±6.4CPT-Seg 41.3±2.6 48.2±4.6 35.7±2.5 42.6±2.9 49.3±4.7 35.4±1.0 47.4±3.5 47.4±3.5\n16 Fine-tuning39.8±4.2 45.5±5.0 34.9±3.0 41.8±3.0 47.3±3.1 36.2±2.3 47.5±4.1 47.8±4.7CPT-Blk 44.8±3.3 51.4±4.1 38.2±2.3 41.5±1.3 48.2±2.1 34.7±0.9 47.8±2.1 48.2±2.8CPT-Seg 45.3±1.8 53.3±3.0 37.5±1.3 44.8±0.9 52.5±1.2 36.6±1.2 51.0±2.6 51.4±2.8\nFully Supervised\n|Dtrain|\nMAttNet 76.7 81 .1 70 .0 65 .3 71 .6 56 .0 66 .6 67 .3VL-T5 − − − − − − 71.2 71 .3ViLBERT − − − 72.3 78 .5 62 .6 − −VLBERT − − − 71.6 77 .7 61 .0 − −ERNIE-ViL − − − 74.0 80 .3 64 .7 − −UNITER 81.2 86 .5 73 .9 75.3 81.3 65.6 74.3 74 .5Fine-tuning 81.8 87.2 74.3 74 .5 80 .8 64 .3 74 .6 75.7CPT-Blk 81.9 87 .1 73 .8 74 .4 80 .4 64 .1 75.3 75.3CPT-Seg 81.9 86.4 74.4 73.9 79 .5 64 .4 74 .4 75 .2\nTraining Settings.We report experimental results of different training settings, including (1) zero-\nshot setting, where no training data is available, (2) few-shot setting, whereKtraining instances are\navailable (K = 1,2,4,8,16), and (3) fully supervised setting, where the full training set is available.\nEvaluation Protocols. (1) Evaluation metrics. Following Zhang et al. (2018); Lu et al. (2019),\nwe adopt accuracy of the grounding results as the evaluation metrics. An expression is considered\ncorrectly grounded if the IoU of the top predicted region and the ground truth is greater than 0.5.\n(2) Model validation. To better approximate the few-shot scenario where only a few labeled in-\nstances are available, inspired by Gao et al. (2021), we use a few-shot validation set (consisting of\n16 instances) for few-shot and zero-shot experiments, and use full validation set for fully supervised\nexperiments. (3) Robust evaluation. Previous works have shown that model training on limited data\ncan suffer from instability (Dodge et al., 2020; Gao et al., 2021). For a robust and comprehensive\nevaluation, we report mean results over5 random training set splits, as well as the standard deviation.\nFor fair comparisons, the training and validation sets are identical for our baselines and CPT.\nBaselines. We evaluate two variants of CPT, including CPT using colored blocks (CPT-Blk) and\ncolored segmentation masks (CPT-Seg). We adopt the widely used VinVL (Zhang et al., 2021)\nas the CPT backbone. We compare CPT with a series of strong baselines that utilize detected\nproposals, including vanilla ﬁne-tuning of VinVL and other VL-PTMs (see Section D.1 for more\nbaseline details). For fair comparisons, we adopt the base size for all VL-PTMs. We refer readers to\nSection A.1 for the results of large size VL-PTMs.\n4.2 M AIN RESULTS\nThe main results are reported in Table 1, from which we observe that: (1) CPT outperforms the\nrandom baseline and the strong ﬁne-tuning baseline by a large margin in zero-shot and few-shot\nsettings. For example, using colored blocks as visual sub-prompts, CPT achieves 17.3% absolute\naccuracy improvement on average with one shot in RefCOCO evaluation. This indicates that CPT\ncan effectively improve sample efﬁciency in tuning VL-PTMs. (2) Coloring objects with segmen-\ntation masks in visual sub-prompts (CPT-Seg) achieves even better results than blocks (CPT-Blk).\nThe reason is that solid colors that ﬁt the outlines of objects are more common in real-world im-\n6\nWork in Progress\nages, making CPT-Seg more natural visual sub-prompts (despite requiring stronger annotation to\ntrain the segmentation tools). (3) Notably, CPT achieves signiﬁcantly smaller standard deviation\nthan ﬁne-tuning. For example, CPT-Blk achieves 73.8% relative standard deviation reduction on\naverage with one shot in RefCOCO evaluation. This shows that a coherent tuning approach from\npre-training can lead to substantially more stable few-shot training, which is a crucial factor for eval-\nuating few-shot learning models (Gao et al., 2021). (4) We note that CPT-Blk slightly underperforms\nﬁne-tuning with 16 shots in RefCOCO+ evaluation. The reason is that RefCOCO+ has more color-\nbased expressions (e.g., the person in red shirt and blue hat ), which can disturb our color-based\nCPT. However, this problem can be alleviated with more tuning instances in the fully supervised\nscenario, where models can learn to better distinguish colors in the query text and prompt template.\n(5) CPT models achieve comparable performance to strong ﬁne-tuned VL-PTMs in the fully super-\nvised settings. This shows that CPT is a competitive tuning approach for VL-PTMs even in the fully\nsupervised scenario. We note that CPT-Blk slightly outperforms CPT-Seg in the fully supervised\nsetting, and we refer readers to Section A.3 for a detailed analysis. In summary, compared to the\nvanilla ﬁne-tuning approach, CPT achieves superior/comparable, and more stable performance in\nzero-shot, few-shot and fully supervised visual grounding.\n4.3 I NFLUENCE OF COLORS IN CPT’S VISUAL GROUNDING\nIn our analysis, we ﬁrst investigate the inﬂuence of colors—the key ingredients—in the visual\ngrounding performance of CPT. Speciﬁcally, we compare colors obtained from the frequency-based\nbaseline (Freq) (See Section 3.4) and our cross-modal prompt search method CPS ( Ours) in two\ndimensions, including an overall evaluation of top-N colors and a zoom-in study of individual col-\nors. Unless otherwise speciﬁed, all the following experiments are conducted based on CPT-Blk on\nthe validation set of RefCOCO in 0,2,8 shot settings.\nTable 2: Top-6 colors from the frequency-based baseline and our CPS. Visual appearances and color\ntexts are reported. Best viewed in color.\nModel Color #1 Color #2 Color #3 Color #4 Color #5 Color #6\nFreq (255,0,0), red (0,0,0), black (0,0,255), blue (0,255,0), green (255,255,0), yellow (165,42,42), brown\nOurs (240,0,30), red (155,50,210), purple (255,255,25), yellow (0,10,255), blue (255,170,230), pink (0,255,0), green\nOverall Evaluation of Top-N Colors.We ﬁrst show the top-6 colors recommended by each ap-\nproach in Table 2. To evaluate the overall performance of the top colors from different models,\nwe evaluate CPT equipped with each color from the top-6 colors respectively, and report the mean\naccuracy and standard deviation over different colors. From the experimental results in Figure 3a,\nwe observe that the top colors produced by CPS achieve both higher mean accuracy and lower stan-\ndard deviation than the baseline method in different shot-settings. The reason is that CPS jointly\nconsiders visual and textual semantics in searching cross-modal prompts, and therefore is able to\neffectively adjust and rank the colors for more accurate and stable visual grounding.\n0-Shot 2-Shot 8-Shot\nShot\n20\n25\n30\n35\n40Accuracy\nFreq\nOurs\n(a) Overall performance.\nred blue green yellow pink purple\n0-Shot\n20\n22\n24\n26\n28\n30Accuracy\nFreq\nOurs\nred blue green yellow pink purple\n2-Shot\n20\n25\n30\n35\n40\nFreq\nOurs\nred blue green yellow pink purple\n8-Shot\n20\n25\n30\n35\n40\n45\nFreq\nOurs (b) Zoom-in study of individual colors in different shots.\nFigure 3: Results of utilizing different colors for visual grounding, including (a) an overall evaluation\nof top-6 colors from different models, and (b) a zoom-in study of aligned individual colors.\nZoom-In Study of Individual Colors.To investigate the ﬁne-grained inﬂuence of speciﬁc colors in\nCPT’s visual grounding, we further perform a zoom-in study of individual colors. To align the colors\nfor comparison, we merge the top-6 colors from the baseline and CPS, and remove the colors that are\nnot included in the models’ complete color sets (e.g., black /∈C in CPS). We report the accuracies\n7\nWork in Progress\nQuery Text: right elephant in water\n(a) Correctly predicted\nQuery Text: apple on the bottom to\nthe right of the orange in middle (b) Disturbed by objects of the\nsame type, but still reasonable\nQuery Text: food in red bowl\n(c) Disturbed by colors in raw\nimage regions and text\nFigure 4: Case study. The bounding boxes given by image region proposals (olive), ground-truth\nannotation (pink), CPT (green), and ﬁne-tuning baseline (yellow) are highlighted accordingly.\nin Figure 3b, from which we observe that: (1) The performance of different colors varies greatly in\nprompting VL-PTMs in the same shot-settings, and the optimal colors are different in different shot-\nsettings. The results indicate the large inﬂuence of cross-modal prompt conﬁgurations, consistent\nwith the ﬁndings from recent studies in textual prompt tuning (Jiang et al., 2020; Gao et al., 2021).\n(2) Colors produced by CPS achieve comparable or superior performance compared to the baseline\nin individual colors. The results show that given the color texts, CPS can properly adjust the color\nvisual appearance (i.e., RGB) to improve the visual grounding performance. (3) We note that in\nsome cases, colors produced by CPS slightly underperform the baseline. We hypothesize the reason\nis that, CPS uses a single textual template to compute the decoding scores for color adjustment,\nwhich can be biased. The problem can potentially be addressed by ensembling templates as in Qin\n& Eisner (2021), which we leave for future work.\n4.4 C ASE STUDY\nTo provide a more intuitive understanding of CPT, we conduct a case study on the validation set\nof RefCOCO in 8-shot setting. From the results in Figure 4, we have the following observations:\n(1) CPT enables VL-PTMs to distinguish target objects distracted by the same of type objects using\nonly a few training instances, while the ﬁne-tuning method struggles to succeed (Figure 4a). (2) CPT\ncan be distracted by hard candidates (e.g., objects of the same type as the target that requires complex\nreasoning to identify), but will typically produce reasonable predictions. For example, in Figure 4b,\nCPT predicts a nearby apple while the ﬁne-tuning baseline predicts a bowl. The reason is that\nCPT maximally reuses the pre-trained parameters of VL-PTMs, which can help prevent outrageous\npredictions that typically happen in few-shot ﬁne-tuning. (3) However, we ﬁnd that CPT can be\ndisturbed by colors in raw image regions and text. For example, it can be difﬁcult for the model to\nidentify a red bowl when the candidate regions are colored by red blocks (Figure 4c).\n4.5 E XPERIMENTS ON VISUAL RELATION DETECTION\nTo show the generalization capability of CPT, we further evaluate CPT on visual relation detection.\nExperimental Settings.(1) Datasets. We adopt the popular Visual Genome dataset (Krishna et al.,\n2017), which contains 50 visual relations. We refer readers to Section D.2 for the dataset details.\n(2) Evaluation protocols. Following previous works (Xu et al., 2017; Chen et al., 2019), we use\nrecall@N (R@N) and mean recall@N (mR@N) as the evaluation metrics. During training, K la-\nbeled instances are provided for each relation. (3) Baselines. We adopt ﬁne-tuning of VinVL as our\nmost direct baseline model. Speciﬁcally, we feed the image regions and their categories into the\nmodel, and concatenate the visual hidden representations of the subject and object. Then the object\npair representation is fed into a softmax classiﬁer. All VL-PTMs are in base size. We also report\nthe results of strong baselines that are tailored for the task, and are fully supervised with 315,642\nlabeled triplets, including Neural Motif (Zellers et al., 2018), BGNN (Li et al., 2021a), PCPL (Yan\net al., 2020) and DT2-ACBS (Desai et al., 2021).\n8\nWork in Progress\nTable 3: Visual relation detection results on Visual Genome. ZS: zero-shot, FS: fully supervised.\nWe report the mean and standard deviation performance over 2 random splits.\nShot Model Val Test\nR@50 R@100 mR@50 mR@100 R@50 R@100 mR@50 mR@100\nZS 0 Random 1.6±0.2 1 .8±0.2 1 .1±0.2 1 .3±0.1 1 .5±0.0 1 .8±0.1 1 .2±0.1 1 .6±0.1CPT-Blk 33.6 34.7 14.8 15.5 29.3 30.5 13.0 14.5\nFew-Shot\n1 Fine-tuning 3.8±0.1 4 .2±0.1 7 .8±0.9 8 .7±1.0 4 .1±0.1 4 .7±0.0 6 .7±0.3 7 .6±0.4CPT-Blk 16.3±2.0 17.5±2.3 25.2±0.7 27.4±0.8 18.0±2.8 20.0±3.0 23.9±0.3 26.3±0.3\n4 Fine-tuning 7.1±1.9 7 .6±2.0 10 .3±0.8 11 .7±0.8 7 .3±1.5 7 .9±1.7 11 .8±1.0 13 .2±0.9CPT-Blk 14.4±0.4 15.4±0.4 30.4±1.5 32.8±1.6 17.7±0.6 19.3±0.6 28.5±1.5 32.1±1.0\n16 Fine-tuning 8.4±0.3 8 .9±0.3 20 .7±0.6 21 .7±0.6 10 .4±0.7 11 .2±0.8 19 .7±0.1 21 .7±0.1CPT-Blk 15.0±0.6 16.0±0.8 33.0±0.2 35.4±0.6 18.4±1.0 20.0±1.1 32.5±0.5 36.1±0.6\n32 Fine-tuning 9.7±1.1 10 .2±1.1 21 .9±0.6 22 .9±0.2 11 .7±0.2 12 .4±0.3 22 .0±0.1 24 .1±0.0CPT-Blk 17.2±0.4 18.2±0.4 34.6±0.2 37.9±0.1 20.8±0.1 22.3±0.1 34.0±0.1 37.7±0.3\nFS|Dtrain|\nNeural Motif - - - - 65.2 67 .0 14 .8 16 .1BGNN - - - - 59.2 61 .3 30 .4 32 .9PCPL - - - - 50.8 52 .6 35 .2 37 .8DT2-ACBS - - - - 23.3 25 .6 35 .9 39 .7\nResults. From the results in Table 3, we observe that: (1) CPT signiﬁcantly outperforms the ran-\ndom baseline and the strong ﬁne-tuning baseline in zero-shot and few-shot settings. For example,\nusing 32 shots, CPT achieves a strong mR@100 of 37.7%, outperforming ﬁne-tuning by 13.6% ab-\nsolute points, and closely approaching state-of-the-art fully supervised DT2-ACBS. This indicates\nthat CPT can improve sample efﬁciency in tuning VL-PTMs. (2) We note that while the macro per-\nformance of CPT monotonically increases as the shot number grows, the micro performance drops\nﬁrst in 1- and 4-shot settings. This is due to the distribution gap between the balanced training set\n(i.e., K shot for each relation) and the long-tail test set. Since the relations in the pre-training corpora\nalso follow a long-tail distribution, CPT can achieve a high starting point for micro performance.\n5 R ELATED WORK\nPre-trained Vision-language Models.Existing VL-PTMs can be roughly divided into three cate-\ngories according to their pre-training objectives and architectures: (1) Masked language modeling\nbased VL-PTMs are mainly pre-trained to recover the masked tokens (Lu et al., 2019; Su et al., 2019;\nTan & Bansal, 2019; Li et al., 2020; Yu et al., 2021); (2) Auto-regressive language modeling based\nVL-PTMs model image and text tokens with Transformer decoders auto-regressively (Ramesh et al.,\n2021; Wang et al., 2021); (3) Contrastive learning based VL-PTMs are pre-trained to holistically\nmatch image-text pairs (Radford et al., 2021; Li et al., 2021b). Note that our Cross-modal Prompt\nTuning (CPT) framework is orthogonal to VL-PTM design. In this work, without loss of generality,\nwe focus on prompting masked language modeling based VL-PTMs due to their prevalence and\nsuperior performance, while applying CPT to other VL-PTMs is also applicable.\nPrompt Tuning for NLP.Prompt tuning for pre-trained language models is a rapidly emerging\nﬁeld in NLP (Raffel et al., 2019; Brown et al., 2020; Liu et al., 2021). Originally designed for prob-\ning knowledge in pre-trained language models (Petroni et al., 2019), prompt tuning has now been\nextended to handle a variety of NLP tasks, including language understanding (Schick & Sch ¨utze,\n2021a;b) and generation (Li & Liang, 2021). To facilitate prompt engineering, Shin et al. (2020) pro-\npose to automatically generate prompt templates via gradient-based search. Most related to our work\nare Tsimpoukelli et al. (2021); Zhou et al. (2021); Wang et al. (2021) that present textual prompt\ntuning for VL-PTMs, achieving promising results on some vision-language tasks. However, simi-\nlar to existing works in NLP, they focus on prompt engineering in text, keeping images untouched,\nand therefore can only perform holistic implicit visual grounding. In comparison, to the best of our\nknowledge, CPT is the ﬁrst cross-modal prompt tuning framework tailored for both image and text,\nand is capable of explicitly grounding natural language to ﬁne-grained image regions.\nVisual Grounding. There is a general consensus that visual grounding plays an essential role in\nsolving vision-language tasks (Karpathy & Fei-Fei, 2015; Plummer et al., 2015; Goodfellow et al.,\n2016; Krishna et al., 2017; Lu et al., 2019). Mao et al. (2016) propose the referring expression\ncomprehension task to explicitly evaluate the visual grounding capability. To address the task, most\n9\nWork in Progress\nmodels learn to classify or rank image region candidates based on the expressions in a fully super-\nvised fashion (Mao et al., 2016; Zhang et al., 2018; Lu et al., 2019; Chen et al., 2020), requiring\nlarge amounts of costly human-annotated data. To alleviate reliance on human annotation, some\nworks have investigated zero-/few-shot grounding of new object types (Sadhu et al., 2019; Blukis\net al., 2020), whereas amounts of training data are still needed for existing object types. In com-\nparison, we prompt general VL-PTMs for zero- and few-shot visual grounding in a reformulated\nﬁll-in-the-blank paradigm independent of speciﬁc object types.\n6 C ONCLUSION AND FUTURE WORK\nIn this work, we present the ﬁrst Cross-modal Prompt Tuning (CPT) framework for VL-PTMs. To\nfacilitate prompt engineering, we present a principled approach to search for cross-modal prompt\nconﬁgurations. Comprehensive experimental results demonstrate the effectiveness of CPT on zero-\nshot, few-shot and fully supervised visual grounding. In future, we plan to address the color dis-\nturbance and improve the computation efﬁciency of CPT, and also investigate the effectiveness of\nCPT on other vision-language tasks. As the ﬁrst attempt in cross-modal prompt tuning, we propose\na color-based framework as one of the possible prompt tuning solutions. We leave exploring other\nplausible prompt tuning approaches of VL-PTMs for future work.\n7 E THICS STATEMENT\nIn this section, we discuss the main ethical considerations of CPT: (1) Intellectual property pro-\ntection. The codes and data adopted from previous works are granted for research-purpose usage.\n(2) Privacy. The data adopted in this work (i.e., the pre-training data and tuning data) is created by\nhuman annotators for research purposes, and should not cause privacy issues. (3) Potential prob-\nlems. VL-PTMs may be biased towards some objects and attributes. There are increasing efforts to\naddress the problem in the community (Ross et al., 2021; Zhao et al., 2021).\n8 R EPRODUCIBILITY STATEMENT\nTo maximize the reproducibility, we provide a clear description of the methodology in Section 3,\nthe pseudo-code of the model in Section B, implementation details in Section C, and detailed data\ncharacteristics and evaluation protocols in Section 4.1. All the data and codes will be available to\nfacilitate future research.\nREFERENCES\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and\nLei Zhang. Bottom-up and top-down attention for image captioning and visual question answer-\ning. In Proceedings of CVPR, pp. 6077–6086, 2018a.\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S ¨underhauf, Ian Reid,\nStephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments. In Proceedings of CVPR, June\n2018b.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-\nnick, and Devi Parikh. VQA: Visual question answering. InProceedings of ICCV, pp. 2425–2433,\n2015.\nValts Blukis, Ross A Knepper, and Yoav Artzi. Few-shot object grounding and mapping for natural\nlanguage robot instruction following. arXiv preprint arXiv:2011.07384, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n10\nWork in Progress\nTianshui Chen, Weihao Yu, Riquan Chen, and Liang Lin. Knowledge-embedded routing network\nfor scene graph generation. In Proceedings of CVPR, pp. 6163–6171, 2019.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: Universal image-text representation learning. In Proceedings of ECCV,\npp. 104–120. Springer, 2020.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. In Proceedings of ICML, PMLR, pp. 1931–1942, 2021.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos ´e MF Moura, Devi\nParikh, and Dhruv Batra. Visual dialog. In Proceedings of CVPR, pp. 326–335, 2017.\nAlakh Desai, Tz-Ying Wu, Subarna Tripathi, and Nuno Vasconcelos. Learning of visual relations:\nThe devil is in the tails. In Proceedings of ICCV, pp. 15404–15413, 2021.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith.\nFine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\narXiv preprint arXiv:2002.06305, 2020.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. In Proceedings of ACL, 2021.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of CVPR, pp. 770–778, 2016.\nKaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick. Mask R-CNN. In Proceedings of\nthe ICCV, pp. 2961–2969, 2017.\nDrew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine.\nIn Proceedings of NeurIPS, pp. 5901–5914, 2019.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? TACL, 8:423–438, 2020.\nJustin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and\nLi Fei-Fei. Image retrieval using scene graphs. In Proceedings of CVPR, pp. 3668–3678, 2015.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descrip-\ntions. In Proceedings of CVPR, pp. 3128–3137, 2015.\nSahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferitGame: Referring to\nobjects in photographs of natural scenes. In Proceedings of EMNLP, pp. 787–798, 2014.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. IJCV, 123(1):32–73, 2017.\nRongjie Li, Songyang Zhang, Bo Wan, and Xuming He. Bipartite graph network with adaptive\nmessage passing for unbiased scene graph generation. In Proceedings of CVPR , pp. 11109–\n11119, 2021a.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang.\nUNIMO: Towards uniﬁed-modal understanding and generation via cross-modal contrastive learn-\ning. In Proceedings of ACL, pp. 2592–2607. Association for Computational Linguistics, 2021b.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation.arXiv\npreprint arXiv:2101.00190, 2021.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language\ntasks. In Proceedings of ECCV, pp. 121–137. Springer, 2020.\n11\nWork in Progress\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proceedings\nof ECCV, pp. 740–755. Springer, 2014.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing. arXiv preprint arXiv:2107.13586, 2021.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. Proceedings of NeurIPS, 32:13–23, 2019.\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. InProceedings of CVPR, pp.\n11–20, 2016.\nFabio Petroni, Tim Rockt¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. Language models as knowledge bases? In Proceedings of EMNLP-IJCNLP,\npp. 2463–2473, 2019.\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svet-\nlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of ICCV, pp. 2641–2649, 2015.\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of NAACL, pp. 5203–5212, 2021.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object\ndetection with region proposal networks. Proceedings of NeurIPS, 28:91–99, 2015.\nAnna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding\nof textual phrases in images by reconstruction. In Proceedings of ECCV, pp. 817–834. Springer,\n2016.\nCandace Ross, Boris Katz, and Andrei Barbu. Measuring social biases in grounded vision and\nlanguage embeddings. In Proceedings of NAACL, pp. 998–1008, 2021.\nArka Sadhu, Kan Chen, and Ram Nevatia. Zero-shot grounding of objects from natural language\nqueries. In Proceedings of ICCV, pp. 4694–4703, 2019.\nTimo Schick and Hinrich Sch ¨utze. It’s not just size that matters: Small language models are also\nfew-shot learners. In Proceedings of NAACL , pp. 2339–2352. Association for Computational\nLinguistics, 2021a.\nTimo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of EACL, pp. 255–269, 2021b.\nJiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable and explicit visual reasoning over scene\ngraphs. In Proceedings of CVPR, pp. 8376–8384, 2019.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Eliciting\nknowledge from language models using automatically generated prompts. In Proceedings of\nEMNLP, pp. 4222–4235, 2020.\n12\nWork in Progress\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-\ntraining of generic visual-linguistic representations. In Proceedings of ICLR, 2019.\nHao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from trans-\nformers. In Proceedings of EMNLP-IJCNLP, pp. 5100–5111, 2019.\nKaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose\ndynamic tree structures for visual contexts. In Proceedings of CVPR, pp. 6619–6628, 2019.\nStefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller,\nand Nicholas Roy. Understanding natural language commands for robotic navigation and mobile\nmanipulation. In Proceedings of AAAI, volume 25, 2011.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multi-\nmodal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM: Sim-\nple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904,\n2021.\nDanfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative\nmessage passing. In Proceedings of CVPR, pp. 5410–5419, 2017.\nShaotian Yan, Chen Shen, Zhongming Jin, Jianqiang Huang, Rongxin Jiang, Yaowu Chen, and Xian-\nSheng Hua. Pcpl: Predicate-correlation perception learning for unbiased scene graph generation.\nIn Proceedings of ACM Multimedia, pp. 265–273, 2020.\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-ViL:\nKnowledge enhanced vision-language representations through scene graphs. In Proceedings of\nAAAI, volume 35, pp. 3208–3216, 2021.\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context\nin referring expressions. In Proceedings of ECCV, pp. 69–85. Springer, 2016.\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg.\nMAttNet: Modular attention network for referring expression comprehension. In Proceedings\nof CVPR, pp. 1307–1315, 2018.\nRowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing\nwith global context. In Proceedings of CVPR, pp. 5831–5840, 2018.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual\ncommonsense reasoning. In Proceedings of CVPR, pp. 6720–6731, 2019.\nHanwang Zhang, Yulei Niu, and Shih-Fu Chang. Grounding referring expressions in images by\nvariational context. In Proceedings of CVPR, pp. 4158–4166, 2018.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In Proceed-\nings of CVPR, pp. 5579–5588, 2021.\nBo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. A survey on deep learning-based ﬁne-grained\nobject classiﬁcation and semantic segmentation. IJAC, 14(2):119–135, 2017.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improv-\ning few-shot performance of language models. In Proceedings of the ICML, 2021.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\nlanguage models. arXiv preprint arXiv:2109.01134, 2021.\n13\nWork in Progress\nA S UPPLEMENTARY EXPERIMENTS\nA.1 R ESULTS OF LARGE SIZE VL-PTM S\nIn this section, we report the experimental results of large size VL-PTMs, including vanilla ﬁne-\ntuning of baseline VL-PTMs, CPT-Blk and CPT-Seg with large size backbone (i.e., 1,024 dimen-\nsional hidden representations and 24 layers). From the experimental results in Table 4, we observe\nthat compared with vanilla ﬁne-tuning, CPT achieves signiﬁcantly better and more stable perfor-\nmance in zero-shot and few-shot settings, and comparable results in the fully supervised settings,\nwhich is consistent with the conclusions of main experiments in Section 4.2. In summary, the results\nshow that CPT can generalize to VL-PTMs of different sizes.\nTable 4: Results of large size VL-PTMs. Accuracies (%) of grounding referring expressions in zero-\nshot, few-shot and fully supervised settings. We report mean and standard deviation performance\nover 5 random splits. ZS: zero-shot. Blk: colored block, Seg: colored segmentation mask.\nShot Model RefCOCO RefCOCO+ RefCOCOg\nval testA testB val testA testB val test\nZS 0 Random 15.9±0.2 19 .4±0.6 13 .4±0.4 16 .1±0.1 13 .3±0.6 20 .0±0.2 18 .8±0.4 19 .2±0.3CPT-Blk 25.7 25 .4 27 .0 25 .9 25 .8 25 .7 32 .9 32 .6CPT-Seg 29.5 30.6 28.7 28.8 30.3 27.4 34.6 34.8\nFew-Shot\n1 Fine-tuning18.5±3.4 13 .7±4.8 25 .0±3.7 23 .0±6.5 22 .8±8.2 23 .6±4.5 30 .6±7.3 31 .5±7.4CPT-Blk 36.4±3.5 39 .1±4.3 34 .3±2.7 34 .4±3.8 38 .7±5.4 31 .2±2.5 38 .7±4.8 38 .7±4.6CPT-Seg 39.3±4.2 43.2±5.6 35.5±2.4 35.9±3.8 41.0±5.0 31.2±2.8 40.9±6.0 41.0±6.1\n2 Fine-tuning23.4±3.5 21 .1±5.2 26 .7±4.5 28 .3±2.3 30 .1±5.3 26 .4±2.8 33 .1±8.3 33 .4±8.2CPT-Blk 38.3±2.9 40 .5±4.2 35 .3±1.2 36 .2±5.5 41 .1±7.6 31 .9±3.3 40 .6±5.9 41 .3±6.1CPT-Seg 41.4±1.5 45.8±3.6 36.6±2.0 38.7±3.8 44.7±5.2 33.5±2.6 43.2±5.9 43.4±5.8\n4 Fine-tuning27.8±4.8 26 .0±7.8 30 .1±3.4 33 .4±3.5 36 .8±5.1 28 .3±2.1 36 .9±8.9 37 .2±8.7CPT-Blk 40.9±1.8 45 .0±2.0 36 .6±1.6 37 .2±3.6 42 .4±5.4 33 .6±2.3 42 .2±6.5 42 .7±6.9CPT-Seg 41.3±5.2 45.9±7.1 36.5±3.7 39.8±3.8 45.7±5.7 34.1±1.8 45.7±7.3 45.8±7.6\n8 Fine-tuning33.3±4.2 35 .6±7.4 31 .2±2.7 38 .1±3.7 43 .5±3.9 31 .2±3.8 41 .9±8.0 42 .5±7.9CPT-Blk 42.7±4.1 48 .4±5.7 37 .3±2.4 39 .9±2.2 45 .8±3.0 34 .6±2.1 44 .8±4.1 45 .5±4.6CPT-Seg 45.2±3.6 51.4±4.9 38.7±2.4 42.4±3.8 49.0±4.9 35.7±1.8 48.1±5.4 48.6±5.8\n16 Fine-tuning38.4±2.4 42 .8±4.2 33 .4±2.5 40 .7±3.2 45 .6±3.5 34 .7±2.8 48 .7±3.5 49 .4±3.5CPT-Blk 45.7±2.5 53 .0±3.2 37 .9±1.5 41 .8±2.0 48 .8±2.6 35 .7±1.4 47 .7±2.4 48 .6±2.8CPT-Seg 48.6±3.1 55.9±3.5 40.3±2.0 43.8±2.0 50.9±2.5 36.5±1.3 50.8±3.6 51.6±3.7\nFully Supervised\n|Dtrain|\nMAttNet 76.7 81 .1 70 .0 65 .3 71 .6 52 .0 66 .6 67 .3ViLBERT - - - 72.3 78 .5 62 .6 - -VLBERT - - - 72.6 78 .6 62 .3 - -ERNIE-ViL - - - 76.0 82.1 66.9 - -UNITER 81.4 87 .0 74 .2 75 .9 81 .5 66 .7 74.9 75.8Fine-tuning 81.8 87.5 73.7 74 .8 81 .0 64 .1 74 .7 75.8CPT-Blk 81.5 87 .0 74.3 73.6 80 .1 64 .1 74 .1 75 .2CPT-Seg 81.8 87.3 74 .1 74 .1 79 .5 63 .8 73 .6 74 .7\nA.2 E FFECT OF COLOR TRANSPARENCY\n0.0 0.2 0.4 0.6 0.8 1.0\nTransparency\n15\n20\n25\n30\n35\n40\n45Accuracy\n0-Shot\n1-Shot\n2-Shot\n4-Shot\n8-Shot\n16-Shot\nFigure 5: Experimental results with dif-\nferent color transparencies.\nIn practice, the color transparency is a crucial hyperpa-\nrameter in CPT. Essentially, the choice of transparency\nis a trade-off between two factors: a small transparency\ncan establish strong connections between color texts and\nvisual appearances, but will undermine the visibility of\nthe raw image region contents, and vice versa. To inves-\ntigate the effect of color transparency, we evaluate CPT\nwith different transparency of the default color (i.e., (240,\n0, 30), red), with step size 0.1 in grid search. From\nthe results in Figure 5, we observe that: (1) The per-\nformance peaks at moderate transparencies in different\nshot-settings, which is consistent with our analysis of the\ntrade-off. (2) Interestingly, the optimal transparency in-\ncreases as the number of training shots grows. The rea-\nson is that the bottleneck of visual grounding in low shot\nsettings is to learn to utilize obvious colors in CPT to establish coarse-grained connections between\n14\nWork in Progress\nQuery Text: middle cat with tail\nhanging down\n(a) CPT: ✓\nQuery Text: the first half of the\nsandwich to the left (b) CPT: ✓\nQuery Text: back of second chair\nfrom left (c) CPT: ×\nQuery Text: blue and yellow bus \nfirst one\n(d) CPT: ✓, FT: ✓\nQuery Text: whole bear next to \nempty spot on center seat (e) CPT: ✓, FT: ×\nQuery Text: chef glancing over at\na restaurant its blurry (f) CPT: ×, FT: ✓\nFigure 7: Visualization of grounding results. First row: zero-shot setting. Second row: fully su-\npervised setting. FT: ﬁne-tuning. The bounding boxes given by image region proposals (olive),\nground-truth annotation (pink), CPT (green), and ﬁne-tuning baseline (yellow) are highlighted ac-\ncordingly. Some images are cropped for better visual effects.\nimages and text. In comparison, in many shot settings, with a better mastery of colors, ﬁne-grained\nreading and understanding of image regions become more important to handle hard instances that\nrequire complex reasoning (e.g., composition of attributes and relations).\nA.3 A NALYSIS OF VISUAL SUB-PROMPT SHAPE\nFigure 6: Performance of CPT-Blk and\nCPT-Seg with base size in different box\nareas in the fully supervised setting.\nIn the main experimental results in Table 1, we note that\nalthough CPT-Seg signiﬁcantly outperforms CPT-Blk in\nthe zero-shot and few-shot settings, it slightly underper-\nforms CPT-Blk in the fully supervised setting. To in-\nvestigate the reason, we divide target objects in the val-\nidation set of RefCOCOg into disjoint bins according to\nthe area of the bounding boxes, where each bin contains\nequal numbers of target objects (thus contributes equally\nto the overall result), and report the average performance\nof each bin in the fully supervised setting. From the re-\nsults in Figure 6, we ﬁnd that CPT-Seg outperforms CPT-\nBlk on large objects, but is inferior in grounding small ob-\njects. We hypothesize the reason is that CPT-Seg changes\nthe object outlines with imperfect colored segmentation\nmasks, hindering the understanding and reasoning of ob-\njects to some extent. The problem is exacerbated in small\nobjects, since compared with large objects, the segmentation error of small objects is essentially en-\nlarged when the object feature maps are pooled into input features of the same size for Transformers.\nA.4 V ISUALIZATION\nThe few-shot grounding results are visualized in Figure 4. In this section, we further visualize the\ngrounding results in zero-shot and fully supervised settings, as shown in Figure 7. We ﬁnd that\n15\nWork in Progress\nCPT can make reasonable zero-shot predictions. Moreover, we observe that the color disturbance\nproblem is largely alleviated in the fully supervised setting, i.e., CPT is less disturbed by colors in\nraw image and text, as shown in Figure 7d. The reason is that a capable VL-PTM can learn to largely\ndistinguish the colors of varying objects and pre-deﬁned maker blocks.\nB P SEUDO -CODE OF CROSS -MODAL PROMPT SEARCH\nHere we provide the pseudo-code of cross-modal prompt search. The algorithm aims to jointly\nconsider visual and textual semantics in real-world cross-modal data to search for the color set C\nin CPT. The algorithm is simple in its design, and we leave exploring more advanced cross-modal\nprompt search methods for future work.\nAlgorithm 1Cross-modal Prompt Search\nRequire: P(·, ·): VL-PTM with image regions Rand query\ntext q as input\nRequire: ˆCw: candidate color text set\nRequire: ˆCv: candidate color RGB set\n1: for ˆci\nv in ˆCv do\n2: R= {a pure color block of ˆci\nv}\n3: q = “[CLS] a photo of [MASK] color [SEP]”\n4: for ˆcj\nw in ˆCw do\n5: s(ˆci\nv, ˆcj\nw) = P([MASK] = ˆcj\nw|R, q)\n6: end for\n7: end for\n8: Discard color candidates with low decoding scores\n9: // Select sensitive color texts\n10: Cw = {cw|cw = arg maxˆcj\nw∈ˆCw\ns(ˆci\nv, ˆcj\nw), ˆci\nv ∈ˆCv}\n11: // Select corresponding sensitive color RGB\n12: C= {(cv, cw)|cv = arg maxˆciv∈ˆCv s(ˆci\nv, cj\nw), cj\nw ∈Cw}\nC I MPLEMENTATION DETAILS\nIn this section, we provide the implementation details about model training and inference, object\ndetection and segmentation, as well as cross-modal prompt search.\nBackbone. We adopt the widely used VinVL (Zhang et al., 2021) as the backbone, which achieves\nstrong performance on many vision-language tasks. We use the VinVL base model in the main\nexperiments, with 768 dimensional hidden representations and 12 encoding layers.\nObject Detection and Segmentation.During training and inference, we use the region proposals\npredicted by the Faster-RCNN (Ren et al., 2015) and object segmentation masks predicted by the\nMask-RCNN (He et al., 2017), which are provided by MAttNet (Yu et al., 2018). Both Faster-RCNN\nand Mask-RCNN are based on ResNet101 (He et al., 2016) with a region proposal network and a\nfully connected classiﬁer for object detection. For Mask-RCNN, an additional mask branch is added\nto conduct multi-task learning. The Faster-RCNN and Mask-RCNN provided by MAttNet (Yu et al.,\n2018) achieve 34.1 and 30.7 average precision on the COCO test set respectively.\nVisual Grounding.During training, an image region is considered as the target if its intersection-\nover-union (IoU) with the ground-truth region is greater than 0.5. During inference, we select the\ntarget region with the largest decoding score. All the hyperparameters and models are selected by\ngrid search based on the performance on the few-shot/full validation set. The learning rate is 6e−5,\nand decreases linearly towards 0, which will be achieved at the end of the training (500 and 20,000\nsteps for few-shot and fully supervised training respectively). The batch size is 32 and identical to\nshot size in fully supervised and few-shot settings respectively. The size of image region batch is 1.\nVisual Relation Detection. The hyperparameters and models are selected by grid search on the\nvalidation set. The learning rate is 3e-5, and decreases linearly towards 0, which will be achieved at\nthe end of the training (200 steps). The batch size is identical to shot size.\n16\nWork in Progress\nTable 5: Relations in Visual Genome dataset (Krishna et al., 2017). Some relations are renamed\n(shown in parentheses) to better ﬁt the query template.\nat in to on of and for has (having)\nsays (saying) over from with near wears (wearing) under above\nusing along behind riding across eating holding wearing\nbetween against playing watching carrying covering made of part of\nlying on parked on ﬂying in laying on growing on looking at walking on walking in\nsitting on covered in mounted on painted on standing on attached to belonging to hanging from\nin front of on back of\nCross-modal Prompt Search. The color text candidate set ˆCw is obtained from Wikipedia at\nen.wikipedia.org/wiki/Lists_of_colors. The color appearance candidate set ˆCv is\nobtained by grid searching RGB candidates around the standard RGBs of color texts in ˆCw. In grid\nsearching RGB candidates, the range is±30 around standard RGBs with step size5 in each channel.\nColors candidates are discarded if the decoding score is less than0.8. The speciﬁc color choice from\nCis determined based on the performance on the few-shot/full validation set. The optimal color used\nin our experiments are c= ((240,0,30),red) with transparency value 0.5 in zero-shot and few-shot\nsettings, and c= ((255,170,230),pink) with transparency value0.45 in the fully supervised setting.\nD E XPERIMENT DETAILS\nD.1 B ASELINE DETAILS\nWe provide baseline details for visual grounding. (1) Vanilla ﬁne-tuning for VinVL (Zhang et al.,\n2021). This model adopts the same backbone as CPT, and serves as the most direct baseline in few-\nshot and fully supervised experiments. Following Chen et al. (2020), the logits for all regions are fed\ninto a softmax layer, and the score of the target region is optimized using cross-entropy objective.\n(2) Vanilla ﬁne-tuning for other VL-PTMs. For fully supervised experiments, we also report previ-\nous results of ﬁne-tuning other VL-PTMs, including ViLBERT (Lu et al., 2019), VLBERT (Su et al.,\n2019), UNITER (Chen et al., 2020), ERNIE-ViL (Yu et al., 2021) and VL-T5 (Cho et al., 2021). (3)\nVisual grounding model. MAttNet (Yu et al., 2018) is a strong model tailored for visual grounding,\nand is compared in fully supervised setting. (4) Random baseline. For zero-shot experiments, we\ncompare with a random baseline that randomly guesses the target region. For fair comparisons, we\nuse the object proposals detected by MAttNet (Yu et al., 2018) for all baselines and CPT-Blk.\nD.2 D ATASET DETAILS\nVisual Grounding Datasets.(1) RefCOCO (Yu et al., 2016) is collected through a two-player ref-\nerential game (Kazemzadeh et al., 2014), and contains 142,210 referential expressions for 50,000\nobject instances in19,994 images. The dataset is split into train, validation, testA and testB sets, with\n120,624, 10,834, 5,657 and 5,095 expression-object pairs respectively. TestA set only contains peo-\nple as target objects, while testB set contains all other types of objects as targets. (2) RefCOCO+ (Yu\net al., 2016) is also collected in an interactive way, and contains 141,564 referential expressions for\n49,856 object instances in 19,992 images. The difference from RefCOCO is that RefCOCO+ fo-\ncuses on distinguishing objects using appearance-based expressions, and excludes location-based\nexpressions. The dataset is split into train, validation, testA and testB sets, with 120,191, 10,758,\n5,726 and 4,889 expression-object pairs respectively. (3) RefCOCOg (Mao et al., 2016) is collected\nin a non-interactive way, and contains 95,010 referential expressions for 49,822 object instances in\n25,799 images. The referential expressions in RefCOCOg are typically longer and more complex.\nThe train, validation and test sets contain 80,512, 4,896 and 9,602 expression-object pairs.\nVisual Relation Detection Datasets.We provide the relations of Visual Genome dataset in Table 5.\nThe dataset contains 65,651, 5,000 and 32,422 images in training, validation and test set respec-\ntively, where each image contains an average of 10.3 objects and 4.8 labeled relation instances.\nThere are 150 distinct object categories and 50 relation categories in the dataset.\n17\nWork in Progress\nE D ISCUSSION AND OUTLOOK\nIn this section, we discuss the limitations of CPT and promising directions for future research.\nLimitations. Despite its promising performance on visual grounding, we note that there are several\nlimitations in CPT: (1) Color disturbance. CPT takes advantage of colors to bridge visual and textual\nsemantics, by adding color-based sub-prompts in both images and text. As shown in Section 4.4, the\ncolor-based prompt can be disturbed by colors in raw images and text. (2) Computation efﬁciency.\nIn our experiments, to maximally avoid color disturbance and account for the limited number of\ncolor candidates, we adopt small image region batch sizes. This means that a data instance needs\nto be fed into the model multiple times in order to obtain the result. We believe addressing these\nchallenges are promising directions for improving CPT.\nOutlook. In this work, we take visual grounding as a representative example to demonstrate the\neffectiveness of CPT. In fact, CPT can be easily adapted to other vision-language tasks. Here we\ndiscuss the promising directions, as illustrated in Figure 8. The visual and textual sub-prompts in\nCPT can well capture ﬁne-grained object-level semantics for object-level tasks, such as: (1) Object\nclassiﬁcation. By coloring object proposals with visual sub-prompt, VL-PTMs can be prompted to\nproduce object labels for object classiﬁcation. (2) Scene graph classiﬁcation. Moreover, by further\ndecomposing textual sub-prompts, complex tasks involving different sub-tasks can be solved in a\nuniﬁed cross-modal prompt tuning framework. For example, VL-PTMs can be prompted to jointly\nproduce object and predicate labels for challenging scene graph classiﬁcation. In addition to data\nefﬁciency, a crucial advantage of using CPT is that the object/predicate labels can be produced from\nopen-world vocabularies, instead of ﬁxed label sets.\nMLM \nHead\n>,0*@\nKRUVH\nZRPDQ\nŏ REMHFW7KH LQ UHG DFRORU LV >0$6.@>&/6@ >6(3@\nŏ\nŏ\n\u000bD\f\u00032EMHFW\u0003&ODVVLƉFDWLRQ\nVocabulary\n\u000bE\f\u00036FHQH\u0003*UDSK\u0003&ODVVLƉFDWLRQ\n>,0*@\n ŏ >&/6@ >0$6.@7KH LQ JUHHQ >0$6.@FRORU LV WKH >0$6.@ LQ >6(3@UHG FRORU\nMLM \nHead\nZRPDQ\nPDQ\nŏ\nŏVocabulary\nMLM \nHead\nZDWFKLQJ\nULGLQJ\nŏ\nŏVocabulary\nMLM \nHead\nKRUVH\nFDW\nŏ\nŏVocabulary\nFigure 8: Outlook for adapting cross-modal prompt tuning (CPT) to other tasks.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6882343292236328
    },
    {
      "name": "Modal",
      "score": 0.6723951101303101
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6035102009773254
    },
    {
      "name": "Code (set theory)",
      "score": 0.584287166595459
    },
    {
      "name": "Shot (pellet)",
      "score": 0.578500509262085
    },
    {
      "name": "Contrast (vision)",
      "score": 0.48831722140312195
    },
    {
      "name": "Image (mathematics)",
      "score": 0.46988821029663086
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4636380076408386
    },
    {
      "name": "One shot",
      "score": 0.45120182633399963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.443266898393631
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3489796221256256
    },
    {
      "name": "Machine learning",
      "score": 0.2966437339782715
    },
    {
      "name": "Engineering",
      "score": 0.13637018203735352
    },
    {
      "name": "Chemistry",
      "score": 0.09748134016990662
    },
    {
      "name": "Programming language",
      "score": 0.08922946453094482
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}